Journal Artificial Intelligence Research 17 (2002) 379-449

Submitted 4/02; published 12/02

Specific-to-General Learning Temporal Events
Application Learning Event Definitions Video
Alan Fern
Robert Givan
Jeffrey Mark Siskind

AFERN @ PURDUE . EDU
GIVAN @ PURDUE . EDU
QOBI @ PURDUE . EDU

School Electrical Computer Engineering
Purdue University, West Lafayette, 47907 USA

Abstract
develop, analyze, evaluate novel, supervised, specific-to-general learner simple temporal logic use resulting algorithm learn visual event definitions video
sequences. First, introduce simple, propositional, temporal, event-description language called
AMA sufficiently expressive represent many events yet sufficiently restrictive support
learning. give algorithms, along lower upper complexity bounds, subsumption generalization problems AMA formulas. present positive-examplesonly
specific-to-general learning method based algorithms. present polynomialtimecomputable syntactic subsumption test implies semantic subsumption without
equivalent it. generalization algorithm based syntactic subsumption used place
semantic generalization improve asymptotic complexity resulting learning algorithm.
Finally, apply algorithm task learning relational event definitions video
show yields definitions competitive hand-coded ones.

1. Introduction
Humans conceptualize world terms objects events. reflected fact
talk world using nouns verbs. perceive events taking place objects,
interact world performing events objects, reason effects
actual hypothetical events performed us others objects. learn new
object event types novel experience. paper, present evaluate novel implemented techniques allow computer learn new event types examples. show results
application techniques learning new event types automatically constructed
relational, force-dynamic descriptions video sequences.
wish acquired knowledge event types support multiple modalities. Humans
observe someone faxing letter first time quickly able recognize future occurrences
faxing, perform faxing, reason faxing. thus appears likely humans use
learn event representations sufficiently general support fast efficient use multiple
modalities. long-term goal research allow similar cross-modal learning use
event representations. intend learned representations used vision (as described
paper), planning (something beginning investigate), robotics (something
left future).
crucial requirement event representations capture invariants event
type. Humans classify picking cup table picking dumbbell floor
picking up. suggests human event representations relational. abstract

c 2002 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiF ERN , G IVAN , & ISKIND

relational notion picking parameterized participant objects rather distinct
propositional notions instantiated specific objects. Humans classify event picking
matter whether hand moving slowly quickly, horizontally vertically, leftward
rightward, along straight path circuitous one. appears characteristics
participant-object motion distinguish picking event types. Rather, fact
object picked changes supported resting initial location
supported grasped agent. suggests primitive relations used
build event representations force dynamic (Talmy, 1988).
Another desirable property event representations perspicuous. Humans
introspect describe defining characteristics event types. introspection allows us create dictionaries. support introspection, prefer representation language
allows characteristics explicitly manifest event definitions emergent consequences distributed parameters neural networks hidden Markov models.
develop supervised learner event representation possessing desired characteristics follows. First, present simple, propositional, temporal logic called AMA
sublanguage variety familiar temporal languages (e.g. linear temporal logic, LTL Bacchus & Kabanza, 2000, event logic Siskind, 2001). logic expressive enough describe
variety interesting temporal events, restrictive enough support effective learner,
demonstrate below. proceed develop specific-to-general learner AMA logic giving algorithms complexity bounds subsumption generalization problems involving
AMA formulas. show semantic subsumption intractable, provide weaker syntactic notion subsumption implies semantic subsumption checked polynomial
time. implemented learner based upon syntactic subsumption.
next show means adapt (propositional) AMA learner learn relational concepts.
evaluate resulting relational learner complete system learning force-dynamic event
definitions positive-only training examples given real video sequences. first
system perform visual-event recognition video. review prior work compare
current work later paper. fact, two prior systems built one
authors. H OWARD (Siskind & Morris, 1996) learns classify events video using temporal,
relational representations. representations force dynamic. L EONARD (Siskind,
2001) classifies events video using temporal, relational, force-dynamic representations
learn representations. uses library hand-code representations. work adds
learning component L EONARD , essentially duplicating performance hand-coded
definitions automatically.
demonstrated utility learner visual-eventlearning domain,
note many domains interesting concepts take form structured temporal sequences events. machine planning, macro-actions represent useful temporal patterns
action. computer security, typical application behavior, represented perhaps temporal patterns system calls, must differentiated compromised application behavior (and likewise
authorized-user behavior intrusive behavior).
follows, Section 2 introduces application domain recognizing visual events
provides informal description system learning event definitions video. Section 3
introduces AMA language, syntax semantics, several concepts needed analysis
language. Section 4 develops analyzes algorithms subsumption generalization
problems language, introduces practical notion syntactic subsumption. Sec380

fiL EARNING EMPORAL E VENTS

tion 5 extends basic propositional learner handle relational data negation, control
exponential run-time growth. Section 6 presents results visual-event learning. Sections 7
8 compare related work conclude.

2. System Overview
section provides overview system learning recognize visual events video.
aim provide intuitive picture system providing technical details. formal
presentation event-description language, algorithms, theoretical empirical results appears Sections 36. first introduce application domain visual-event recognition
L EONARD system, event recognizer upon learner built. Second, describe
positive-only learner fits overall system. Third, informally introduce AMA
event-description language used learner. Finally, give informal presentation
learning algorithm.
2.1 Recognizing Visual Events
L EONARD (Siskind, 2001) system recognizing visual events video camera input
example simple visual event hand picking block. research originally
motivated problem adding learning component L EONARDallowing L EONARD
learn recognize event viewing example events type. Below, give high-level
description L EONARD system.
L EONARD three-stage pipeline depicted Figure 1. raw input consists video-frame
image sequence depicting events. First, segmentation-and-tracking component transforms
input polygon movie: sequence frames, frame set convex polygons placed
around tracked objects video. Figure 2a shows partial video sequence pick event
overlaid corresponding polygon movie. Next, model-reconstruction component
transforms polygon movie force-dynamic model. model describes changing
support, contact, attachment relations tracked objects time. Constructing
model somewhat involved process described Siskind (2000). Figure 2b shows
visual depiction force-dynamic model corresponding pick event. Finally, eventrecognition component armed library event definitions determines events occurred
model and, accordingly, video. Figure 2c shows text output input
event-recognizer pick event. first line corresponds output indicates
interval(s) pick occurred. remaining lines text encoding
event-recognizer input (model-reconstruction output), indicating time intervals various
force-dynamic relations true video.
event-recognition component L EONARD represents event types event-logic formulas following simplified example, representing x picking z .

4

P ICK U P (x; y; z ) = (S UPPORTS (z; ) ^ C ONTACTS (z; )); (S UPPORTS (x; ) ^ ATTACHED (x; ))

formula asserts event x picking z defined sequence two states
z supports way contact first state x supports way attachment
second state. UPPORTS , C ONTACTS , ATTACHED primitive force-dynamic relations.
formula specific example general class AMA formulas use
learning.
381

fiF ERN , G IVAN , & ISKIND

image
sequence

Segmentation
Tracking

polygonscene
sequence

Model
Reconstruction

training
models
event
labels

model
sequence

Event
Learner

Event
Classification

event
labels

learned event
definitions

Figure 1: upper boxes represent three primary components L EONARDs pipeline.
lower box depicts event-learning component described paper. input
learning component consists training models target events (e.g., movies pick
events) along event labels (e.g., P ICK U P (hand; red; green)) output
event definition (e.g., temporal logic formula defining P ICK U P (x; y; z )).
2.2 Adding Learning Component
Prior work reported paper, definitions L EONARD event-recognition library
hand coded. Here, add learning component L EONARD learn recognize
events. Figure 1 shows event learner fits overall system. input event
learner consists force-dynamic models model-reconstruction stage, along event
labels, output consists event definitions used event recognizer. take
supervised-learning approach force-dynamic model-reconstruction process applied
training videos target event type. resulting force-dynamic models along labels
indicating target event type given learner induces candidate definition
event type.
example, input learner might consist two models corresponding two videos,
one hand picking red block green block label P ICK U P (hand; red; green)
one hand picking green block red block label P ICK U P (hand; green; red)the
output would candidate definition P ICK U P (x; y; z ) applicable previously unseen
pick events. Note learning component positive-only sense learning
target event type uses positive training examples (where target event occurs)
use negative examples (where target event occur). positive-only setting
interest appears humans able learn many event definitions given primarily
positive examples. practical standpoint, positive-only learner removes often difficult
task collecting negative examples representative event learned
(e.g., typical non-pickup event?).
construction learner involves two primary design choices. First, must choose
event representation language serve learners hypothesis space (i.e., space event definitions may output). Second, must design algorithm selecting good event definition
hypothesis space given set training examples event type.
2.3 AMA Hypothesis Space
full event logic supported L EONARD quite expressive, allowing specification
wide variety temporal patterns (formulas). help support successful learning, use
382

fiL EARNING EMPORAL E VENTS

(a)

Frame 0

Frame 1

Frame 2

Frame 13

Frame 14

Frame 20

Frame 0

Frame 1

Frame 2

Frame 13

Frame 14

Frame 20

(b)

(PICK-UP HAND RED GREEN)@{[[0,1],[14,22])}

(c)

(SUPPORTED? RED)@{[[0:22])}
(SUPPORTED? HAND)@{[[1:13]), [[24:26])}
(SUPPORTS? RED HAND)@{[[1:13]), [[24:26])}
(SUPPORTS? HAND RED)@{[[13:22])}
(SUPPORTS? GREEN RED)@{[[0:14])}
(SUPPORTS? GREEN HAND)@{[[1:13])}
(CONTACTS? RED GREEN)@{[[0:2]), [[6:14])}
(ATTACHED? RED HAND)@{[[1:26])}
(ATTACHED? RED GREEN)@{[[1:6])}

Figure 2: L EONARD recognizes pick event. (a) Frames raw video input automatically generated polygon movie overlaid. (b) frames visual depiction
automatically generated force-dynamic properties. (c) text input/output
event classifier corresponding depicted movie. top line output
remaining lines make input encodes changing force-dynamic properties.
GREEN represents block table RED represents block picked up.

383

fiF ERN , G IVAN , & ISKIND

restrictive subset event logic, called AMA, learners hypothesis space. subset excludes
many practically useless formulas may confuse learner, still retaining substantial
expressiveness, thus allowing us represent learn many useful event types. restriction
AMA formulas form syntactic learning bias.
basic AMA formulas called states express constant properties time intervals arbitrary duration. example, UPPORTS (z; ) ^ C ONTACTS (z; ) state tells us
z must support contact . general, state conjunction number
primitive propositions (in case force-dynamic relations). Using AMA describe
sequences states. example, (S UPPORTS (z; ) ^ C ONTACTS (z; )) ; (S UPPORTS (x; ) ^
ATTACHED (x; )) sequence two states, first state given second
state indicating x must support attached . formula true whenever first
state true time interval, followed immediately second state true
time interval meeting first time interval. sequences called timelines since
Meets Ands. general, timelines contain number states. Finally,
conjoin timelines get AMA formulas (Ands MAs). example, AMA formula

[(S UPPORTS (z; y) ^ C ONTACTS (z; y)) ; (S UPPORTS (x; y) ^ ATTACHED (x; y))] ^
[(S UPPORTS (u; v) ^ ATTACHED (u; v)) ; (S UPPORTS (w; v) ^ C ONTACTS (w; v))]
defines event two timelines must true simultaneously time interval.
Using AMA formulas represent events listing various property sequences (MA timelines),
must occur parallel event unfolds. important note, however,
transitions states different timelines AMA formula occur relation one
another. example, AMA formula, transition two states first
timeline occur before, after, exactly transition states second timeline.
important assumption leveraged learner primitive propositions used construct states describe liquid properties (Shoham, 1987). purposes, say property
liquid holds time-interval holds subintervals. force-dynamic
properties produced L EONARD liquide.g., hand UPPORTS block interval
clearly hand supports block subintervals. primitive propositions
liquid, properties described states (conjunctions primitives) liquid. However, properties described AMA formulas not, general, liquid.
2.4 Specific-to-General Learning Positive Data
Recall examples wish classify learn force-dynamic models,
thought (and derived from) movies depicting temporal events. recall
learner outputs definitions AMA hypothesis space. Given AMA formula, say
covers example model true model. particular target event type (such
P ICK U P ), ultimate goal learner output AMA formula covers example
model model depicts instance target event type. understand
learner, useful define generality relationship AMA formulas. say AMA
formula 1 general (less specific) AMA formula 2 2 covers every
example 1 covers (and possibly more).1
1. formal analysis, use two different notions generality (semantic syntactic). section,
ignore distinctions. note, however, algorithm informally describe later section based
syntactic notion generality.

384

fiL EARNING EMPORAL E VENTS

learning goal find AMA formula consistent set positiveonly training data, one result trivial solution returning formula covers
examples. Rather fix problem adding negative training examples (which rule
trivial solution), instead change learning goal finding least-general
formula covers positive examples.2 learning approach pursued
variety different languages within machine-learning literature, including clausal first-order
logic (Plotkin, 1971), definite clauses (Muggleton & Feng, 1992), description logic (Cohen &
Hirsh, 1994). important choose appropriate hypothesis space bias learning
approach hypothesis returned may simply (or resemble) one two extremes, either
disjunction training examples universal hypothesis covers examples.
experiments, found that, enough training data, least-general AMA formula often
converges usefully.
take standard specific-to-general machine-learning approach finding least-general
AMA formula covers set positive examples. approach relies computation two
functions: least-general covering formula (LGCF) example model least-general
generalization (LGG) set AMA formulas. LGCF example model least general
AMA formula covers example. Intuitively, LGCF AMA formula captures
information model. LGG set AMA formulas least-general AMA
formula general formula set. Intuitively, LGG formula set
AMA formula captures largest amount common information among formulas.
Viewed differently, LGG formula set covers examples covered formulas,
covers examples possible (while remaining AMA).3
resulting specific-to-general learning approach proceeds follows. First, use LGCF
function transform positive training model AMA formula. Second, return LGG
resulting formulas. result represents least-general AMA formula covers
positive training examples. Thus, specify learner, remains provide algorithms computing LGCF LGG AMA language. informally describe
algorithms computing functions, formally derived analyzed Sections 3.4 4.
2.5 Computing AMA LGCF
increase readability presentation, follows, dispense presenting examples primitive properties meaningfully named force-dynamic relations. Rather,
examples utilize abstract propositions b. current application, propositions correspond exclusively force-dynamic properties, may applications.
demonstrate system computes LGCF example model.
Consider following example model: fa@[1; 4]; b@[3; 6]; c@[6; 6]; d@[1; 3]; d@[5; 6]g . Here,
take number (1, . . . , 6) represent time interval arbitrary (possibly varying
number) duration nothing changes, fact p@[i; j ] indicates proposition p continuously true throughout time intervals numbered j . model
depicted graphically, shown Figure 3. top four lines figure indicate time
2. avoids need negative examples corresponds finding specific boundary version space
(Mitchell, 1982).
3. existence uniqueness LGCF LGG defined formal property hypothesis space
proven AMA Sections 3.4 4, respectively.

385

fiF ERN , G IVAN , & ISKIND

1

2



3

4





b

b

5

6

b

b
c


a^d







; a^b^d ; a^b ; b^d ; b^c^d

Figure 3: LGCF Computation. top four horizontal lines figure indicate intervals propositions a; b; c true model given
fa@[1; 4]; b@[3; 6]; c@[6; 6]; d@[1; 3]; d@[5; 6]g . bottom line shows model
divided intervals transitions occur. LGCF timeline,
shown bottom figure, state no-transition intervals.
state simply contains true propositions within corresponding interval.
intervals propositions a; b; c, true model. bottom line
figure shows model divided five time intervals propositions
change truth value. division possible assumption propositions
liquid. allows us, example, break time-interval true three consecutive subintervals true. dividing model intervals transitions,
compute LGCF simply treating intervals state timeline,
states contain propositions true corresponding time interval.
resulting five-state timeline shown bottom figure. show later simple
computation returns LGCF model. Thus, see LGCF model always
timeline.
2.6 Computing AMA LGG
describe algorithm computing LGG two AMA formulasthe LGG
formulas computed via sequence 1 pairwise LGG applications, discussed later.
Consider two timelines: 1 = (a ^ b ^ c); (b ^ c ^ d); e 2 = (a ^ b ^ e); a; (e ^ d).
useful consider various ways timelines true simultaneously along
arbitrary time interval. this, look various ways two timelines
aligned along time interval. Figure 4a shows one many possible alignments
timelines. call alignments interdigitationsin general, exponentially many
interdigitations, one ordering state transitions differently. Note interdigitation
allowed constrain two transitions different timelines occur simultaneously (though
depicted figure).4
4. Thus, interdigitation provides ordering relation transitions need anti-symmetric, reflexive,
transitive, total.

386

fiL EARNING EMPORAL E VENTS

(a)

a^b^e
(b)

a^b^c

b^c^d



e^d

e

a^b^c a^b^c a^b^c b^c^d
a^b^e
a^b ;



e^d

e^d



; true ;



e
e^d
;

e

Figure 4: Generalizing timelines (a ^ b ^ c); (b ^ c ^ d); e (a ^ b ^ e); a; (e ^ d). (a)
One exponentially many interdigitations two timelines. (b) Computing
interdigitation generalization corresponding interdigitation part (a). States
formed intersecting aligned states two timelines. state true represents
state propositions.

Given interdigitation two timelines, easy construct new timeline must
true whenever either timelines true (i.e., construct generalization two timelines).
Figure 4b, give construction interdigitation given Figure 4a. top two
horizontal lines figure correspond interdigitation, divided every state
either timeline two identical states, whenever transition occurs state
timeline. resulting pair timelines simultaneous transitions viewed
sequence state pairs, one timeline. bottom horizontal line labeled
timeline one state state pair, state intersection
proposition sets state pair. Here, true represents empty set propositions, state
true anywhere.
call resulting timeline interdigitation generalization (IG) 1 2 .
clear IG true whenever either 1 2 true. particular, 1 holds along
time-interval model, sequence consecutive (meeting) subintervals
sequence states 1 true. construction, IG aligned relative 1 along
interval view states sets, states IG subsets corresponding
aligned state(s) 1 . Thus, IG states true model alignment, showing
IG true model.
general, exponentially many IGs two input timelines, one possible
interdigitation two. Clearly, since IG generalization input timelines,
conjunction IGs. conjunction AMA formula generalizes
input timelines. fact, show later paper AMA formula LGG
two timelines. show conjunction IGs 1 2 serves
LGG.
387

fiF ERN , G IVAN , & ISKIND

[(a ^ b); b; e; true; e] ^
[(a ^ b); b; true; e] ^
[(a ^ b); b; true; true; e] ^
[(a ^ b); b; true; e] ^
[(a ^ b); b; true; d; e] ^
[(a ^ b); true; true; e] ^
[(a ^ b); true; e] ^
[(a ^ b); true; d; e] ^
[(a ^ b); a; true; true; e] ^
[(a ^ b); a; true; e] ^
[(a ^ b); a; true; d; e] ^
[(a ^ b); a; d; e] ^
[(a ^ b); a; true; d; e]
formula LGG, contains redundant timelines pruned. First,
clear different IGs result timelines, remove one copy
timeline LGG. Second, note timeline 0 general timeline
, ^ 0 equivalent thus, prune away timelines generalizations
others. Later paper, show efficiently test whether one timeline general
another. performing pruning steps, left first next last
timelines formulathus, [(a ^ b); a; d; e] ^ [(a ^ b); b; e; true; e] LGG 1
2 .
demonstrated compute LGG pairs timelines. use
procedure compute LGG pairs AMA formulas. Given two AMA formulas compute
LGG simply conjoining LGGs pairs timelines (one AMA formula)
i.e., formula
m^
n
^
LGG(i ; 0j )
j

LGG two AMA formulas 1 ^ ^ 01 ^ ^ 0n , 0j
timelines.
informally described LGCF LGG operations needed carry
specific-to-general learning approach described above. follows, formally develop
operations analyze theoretical properties corresponding problems, discuss
needed extensions bring (exponential, propositional, negation-free) operations
practice.

3. Representing Events AMA
present formal account AMA hypothesis space analytical development
algorithms needed specific-to-general learning AMA. Readers primarily interested
high-level view algorithms empirical evaluation may wish skip Sections 3 4
instead proceed directly Sections 5 6, discuss several practical extensions
basic learner present empirical evaluation.
study subset interval-based logic called event logic (Siskind, 2001) utilized
L EONARD event recognition video sequences. logic interval-based explicitly rep388

fiL EARNING EMPORAL E VENTS

resenting possible interval relationships given originally Allen (1983) calculus
interval relations (e.g., overlaps, meets, during). Event-logic formulas allow definition
event types specify static properties intervals directly dynamic properties
hierarchically relating sub-intervals using Allen relations. paper, formal syntax
semantics full event logic needed Proposition 4 given Appendix A.
restrict attention much simpler subset event logic call AMA, defined
below. believe choice event logic rather first-order logic, well restriction
AMA fragment event logic, provide useful learning bias ruling large number
practically useless concepts maintaining substantial expressive power. practical utility
bias demonstrated via empirical results visual-eventrecognition application.
AMA seen restriction LTL (Bacchus & Kabanza, 2000) conjunction
Until, similar motivations. present syntax semantics AMA along
key technical properties AMA used throughout paper.
3.1 AMA Syntax Semantics
natural describe temporal events specifying sequence properties must hold
consecutive time intervals. example, hand picking block might become block
supported hand block supported hand. represent
sequences timelines5 , sequences conjunctive state restrictions. Intuitively,
timeline given sequence propositional conjunctions, separated semicolons,
taken represent set events temporally match sequence consecutive conjunctions.
AMA formula conjunction number timelines, representing events
simultaneously viewed satisfying conjoined timelines. Formally, syntax
AMA formulas given by,
state

AMA

::= true j prop j prop ^ state
::= (state) j (state);
// may omit parens
::= j ^ AMA

prop primitive proposition (sometimes called primitive event type). take
grammar formally define terms timeline, formula, AMA formula, state. k formula formula k states, k -AMA formula AMA formula
whose timelines k -MA timelines. often treat states proposition sets
true empty set AMA formulas MA-timeline sets. may treat formulas
sets statesit important note, however, formulas may contain duplicate states,
duplication significant. reason, treating timelines sets,
formally intend sets state-index pairs (where index gives states position formula).
indicate explicitly avoid encumbering notation, implicit index must
remembered whenever handling duplicate states.
semantics AMA formulas defined terms temporal models. temporal model
= hM; set PROP propositions pair mapping natural numbers
(representing time) truth assignments PROP, closed natural-number interval .
note Siskind (2001) gives continuous-time semantics event logic models
5. stands Meets/And, timeline Meet sequence conjunctively restricted intervals.

389

fiF ERN , G IVAN , & ISKIND

defined terms real-valued time intervals. temporal models defined use discrete
natural-number time-indices. However, results still apply continuous-time semantics. (That semantics bounds number state changes continuous timeline countable number.) important note natural numbers domain representing
time discretely, prescribed unit continuous time represented natural
number. Instead, number represents arbitrarily long period continuous time
nothing changed. Similarly, states timelines represent arbitrarily long periods time
conjunctive restriction given state holds. satisfiability relation AMA
formulas given follows:




state satisfied model hM; iff [x] assigns P true every x 2 P



AMA formula 1 ^ 2 ^ ^ n satisfied iff satisfied M.

2 s.

timeline s1 ; s2 ; : : : ; sn satisfied model hM; [t; t0 ]i iff exists t00
[t; t0 ] hM; [t; t00 ]i satisfies s1 either hM; [t00 ; t0 ]i hM; [t00 + 1; t0 ]i satisfies
s2 ; : : : ; sn .

condition defining satisfaction timelines may appear unintuitive first due
fact two ways s2 ; : : : ; sn satisfied. reason becomes clear recalling using natural numbers represent continuous time intervals. Intuitively,
continuous-time perspective, timeline satisfied consecutive continuous-time
intervals satisfying sequence consecutive states timeline. transition
consecutive states si si+1 occur either within interval constant truth assignment (that
happens satisfy states) exactly boundary two time intervals constant truth
value. definition, cases correspond s2 ; : : : ; sn satisfied time
intervals [t00 ; t0 ] [t00 + 1; t0 ] respectively.
satisfies say model covers M. say AMA 1
subsumes AMA 2 iff every model 2 model 1 , written 2 1 , say 1
properly subsumes 2 , written 2 < 1 , 1 6 2 . Alternatively, may state
2 1 saying 1 general (or less specific) 2 1 covers 2 . Siskind
(2001) provides method determine whether given model satisfies given AMA formula.
Finally, useful associate distinguished timeline model. projection
model = hM; [i; j ]i written MAP(M) timeline s0 ; s1 ; : : : ; sj state sk
gives true propositions (i + k ) 0 k j i. Intuitively, projection gives
sequence propositional truth assignments beginning end model. Later
show projection model viewed representing model precise
sense.
following two examples illustrate basic behaviors AMA formulas:
Example 1 (Stretchability). S1 ; S2 ; S3 , S1 ; S2 ; S2 ; : : : ; S2 ; S3 , S1 ; S1 ; S1 ; S2 ; S3 ; S3 ; S3
equivalent timelines. general, timelines property duplicating state
results formula equivalent original formula. Recall that, given model hM; i,
view truth assignment [x] representing continuous time-interval. interval
conceptually divided arbitrary number subintervals. Thus state satisfied
hM; [x; x]i, state sequence ; ; : : : ; .
390

fiL EARNING EMPORAL E VENTS

Example 2 (Infinite Descending Chains). Given propositions B , timeline =
subsumed formulas A; B , A; B ; A; B , A; B ; A; B ; A; B , . . . .
intuitively clear semantics viewed continuous-time perspective. interval
B true broken arbitrary number subintervals
B hold. example illustrates infinite descending chains AMA
formulas entire chain subsumes given formula (but member equivalent given
formula). general, AMA formula involving propositions B subsume .

(A ^ B )

3.2 Motivation AMA
timelines natural way capture stretchable sequences state constraints.
consider conjunction sequences, i.e., AMA? several reasons language enrichment. First all, show AMA least-general generalization (LGG)
uniquethis true MA. Second, informally, argue parallel conjunctive constraints important learning efficiency. particular, space formulas
length k grows size exponentially k , making difficult induce long formulas.
However, finding several shorter timelines characterize part long sequence
changes exponentially easier. (At least, space search exponentially smaller.) AMA
conjunction timelines places shorter constraints simultaneously often captures
great deal concept structure. reason, analyze AMA well and,
empirical work, consider k -AMA.
AMA language propositional. intended applications relational, first-order,
including visual-event recognition. Later paper, show propositional AMA learning algorithms develop effectively applied relational domains. approach
first-order learning distinctive automatically constructing object correspondence across examples (cf. Lavrac, Dzeroski, & Grobelnik, 1991; Roth & Yih, 2001). Similarly, though AMA
allow negative state constraints, Section 5.4 discuss extend results
incorporate negation learning algorithms, crucial visual-event recognition.
3.3 Conversion First-Order Clauses
note AMA formulas translated various ways first-order clauses.
straightforward, however, use existing clausal generalization techniques learning.
particular, capture AMA semantics clauses, appears necessary define subsumption
generalization relative background theory restricts us continuous-time first-order
model space.
example, consider AMA formulas 1 = ^ B 2 = A; B B
propositionsfrom Example 2 know 1 2 . Now, consider straightforward clausal
translation formulas giving C1 = A(I ) ^ B (I ) C2 = A(I1 ) ^ B (I2 ) ^ EETS (I1 ; I2 ) ^
= PAN (I1 ; I2 ), Ij variables represent time intervals, EETS indicates
two time intervals meet other, PAN function returns time interval equal
union two time-interval arguments. meaning intend capture satisfying
assignments C1 C2 indicate intervals 1 2 satisfied, respectively.
clear that, contrary want, C1 6 C2 (i.e., 6j= C1 ! C2 ), since easy
find unintended first-order models satisfy C1 , C2 . Thus translation,
similar translations, capture continuous-time nature AMA semantics.
391

fiF ERN , G IVAN , & ISKIND

order capture AMA semantics clausal setting, one might define first-order theory
restricts us continuous-time modelsfor example, allowing derivation property B
holds interval, property holds sub-intervals. Given theory ,
j= C1 ! C2 , desired. However, well known least-general generalizations relative background theories need exist (Plotkin, 1971), prior work clausal
generalization simply subsume results AMA language.
note particular training set, may possible compile continuous-time background theory finite adequate set ground facts. Relative ground theories,
clausal LGGs known always exist thus could used application. However,
compiling approaches look promising us require exploiting analysis similar one given paperi.e., understanding AMA generalization subsumption
problem separately clausal generalization exploiting understanding compiling
background theory. pursued compilations further.
Even given compilation procedure, problems using existing clausal generalization techniques learning AMA formulas. clausal translations
AMA found, resulting generalizations typically fall outside (clausal translations
formulas the) AMA language, language bias AMA lost. preliminary empirical work video-event recognition domain using clausal inductive-logic-programming (ILP)
systems, found learner appeared lack necessary language bias find effective
event definitions. believe would possible find ways build language bias
ILP systems, chose instead define learn within desired language bias directly,
defining class AMA formulas, studying generalization operation class.
3.4 Basic Concepts Properties AMA
use following convention naming results: propositions theorems key
results work, theorems results technical difficulty, lemmas
technical results needed later proofs propositions theorems. number
results one sequence, regardless type. Proofs theorems propositions provided
main textomitted proofs lemmas provided appendix.
give pseudo-code methods non-deterministic style. non-deterministic language functions return one value non-deterministically, either contain
non-deterministic choice points, call non-deterministic functions. Since nondeterministic function return one possible value, depending choices made
choice points encountered, specifying function natural way specify richly structured set (if function arguments) relation (if function arguments). actually
enumerate values set (or relation, arguments provided) one simply use
standard backtracking search different possible computations corresponding different
choices choice points.
3.4.1 UBSUMPTION



G ENERALIZATION



TATES

basic formulas deal states (conjunctions propositions). propositional
setting computing subsumption generalization state level straightforward. state S1
subsumes S2 (S2 S1 ) iff S1 subset S2 , viewing states sets propositions. this,
derive intersection states least-general subsumer states union
states likewise general subsumee.
392

fiL EARNING EMPORAL E VENTS

3.4.2 NTERDIGITATIONS
Given set timelines, need consider different ways model could simultaneously satisfy timelines set. start model (i.e., first time point),
initial state timeline must satisfied. time point model, one
timelines transition second state timelines must satisfied place
initial state, initial state timelines remains satisfied. sequence
transitions subsets timelines, final state timeline holds. way
choosing transition sequence constitutes different interdigitation timelines.
Viewed differently, model simultaneously satisfying timelines induces co-occurrence
relation tuples timeline states, one timeline, identifying tuples co-occur
point model. represent concept formally set tuples co-occurring states,
i.e., co-occurrence relation. sometimes think set tuples ordered sequence
transitions. Intuitively, tuples interdigitation represent maximal time intervals
timeline transition, tuples giving co-occurring states
time interval.
relation R X1 Xn simultaneously consistent orderings 1 ,. . . ,n, if,
whenever R(x1 ; : : : ; xn ) R(x01 ; : : : ; x0n ), either xi x0i , i, x0i xi , i. say
R piecewise total projection R onto component totali.e., every state Xi
appears R.
Definition 1 (Interdigitation). interdigitation set f1 ; : : : ; n g timelines cooccurrence relation 1 n (viewing timelines sets states6 ) piecewise total
simultaneously consistent state orderings . say two states 2
s0 2 j 6= j co-occur iff tuple contains s0 . sometimes refer
sequence tuples, meaning sequence lexicographically ordered state orderings.
note exponentially many interdigitations even two timelines (relative
total number states timelines). Example 3 page 396 shows interdigitation two
timelines. Pseudo-code non-deterministically generating arbitrary interdigitation set
timelines found Figure 5. Given interdigitation timelines s1 ; s2 ; : : : ; sm
t1 ; t2 ; : : : ; tn (and possibly others), following basic properties interdigitations easily
verifiable:
1. < j , si tk co-occur k 0
2.

< k, sj co-occur tk

0

.

(s1 ; t1 ) (sm ; tn ).

first use interdigitations syntactically characterize subsumption timelines.
Definition 2 (Witnessing Interdigitation). interdigitation two timelines 1 2
witness 1 2 iff every pair co-occurring states s1 2 1 s2 2 2 ,
s2 subset s1 (i.e., s1 s2 ).
following lemma proposition establish equivalence witnessing interdigitations
subsumption.
6. Recall, that, formally, timelines viewed sets state-index pairs, rather sets states. ignore
distinction notation, readability purposes, treating timelines though state duplicated.

393

fiF ERN , G IVAN , & ISKIND

1:

an-interdigitation (f1 ; 2 ; : : : ; n g)

// Input: timelines 1 ; : : : ; n
// Output: interdigitation f1 ; : : : ; n g

2:
3:

S0 := hhead(1 ); : : : ; head(n )i;
1 n; ji j = 1
return hS0 i;
0
:= ji j > 1g;
00 := a-non-empty-subset-of (T 0 );

4:
5:
6:
7:
8:

:= 1 n
2 00
0i := rest(i )
else 0i := ;

9:
10:
12:
12:

return extend-tuple (S0 ; an-interdigitation (f01 ; : : : ; 0n g));

13:

Figure 5: Pseudo-code an-interdigitation(), non-deterministically computes interdigitation set f1 ; : : : ; n g timelines. function head() returns first
state timeline . rest() returns first state removed. extend-tuple(x,I )
extends tuple adding new first element x form longer tuple. a-non-emptysubset-of(S ) non-deterministically returns arbitrary non-empty subset .
Lemma 1. timeline model M, satisfies , witnessing
interdigitation MAP(M) .
Proposition 2. timelines 1 2 , 1

1 2 .

2 iff interdigitation witnesses

Proof: show backward direction induction number states n timeline 1 .
n = 1, existence witnessing interdigitation 1 2 implies every state 2
subset single state 1 , thus model 1 model 2 1 2 .
Now, suppose induction backward direction theorem holds whenever 1 n
fewer states. Given arbitrary model n + 1 state 1 interdigitation W
witnesses 1 2 , must show model 2 conclude 1 2 desired.
Write 1 s1 ; : : : ; sn+1 2 t1 ; : : : ; tm . witnessing interdigitation, W must identify
maximal prefix t1 ; : : : ; tm 2 made states co-occur s1 thus
subsets s1 . Since = hM; [t; t0 ]i satisfies 1 , definition must exist t00 2 [t; t0 ]
hM; [t; t00 ]i satisfies s1 (and thus t1 ; : : : ; tm ) hM; 0 satisfies s2 ; : : : ; sn+1 0 equal
either [t00 ; t0 ] [t00 + 1; t0 ]. either case, straightforward construct, W , witnessing
interdigitation s2 ; : : : ; sn+1 tm +1 ; : : : ; tm use induction hypothesis show
hM; 0 must satisfy tm +1; : : : ; tm . follows satisfies 2 desired.
forward direction, assume 1 2 , let model 1 =
MAP(M). clear exists satisfies 1 . follows satisfies 2 .
Lemma 1 implies witnessing interdigitation MAP(M) 2 thus
1 2 . 2
0

0

0

0

394

fiL EARNING EMPORAL E VENTS

3.4.3 L EAST-G ENERAL C OVERING F ORMULA
logic discriminate two models contains formula satisfies one other.
turns AMA formulas discriminate two models exactly much richer internal positive event logic (IPEL) formulas so. Internal formulas define event occurrence
terms properties within defining interval. is, satisfaction hM; depends
proposition truth values given inside interval . Positive formulas
contain negation. Appendix gives full syntax semantics IPEL (which used
state prove Lemma 3 ). fact AMA discriminate models well IPEL
indicates restriction AMA formulas retains substantial expressive power leads
following result serves least-general covering formula (LGCF) component
specific-to-general learning procedure. Formally, LGCF model within formula language
L (e.g. AMA IPEL) formula L covers covering formula
L strictly less general. Intuitively, LGCF model, unique, representative
formula model. analysis uses concept model embedding. say model
embeds model M0 iff MAP(M) MAP(M0 ).
Lemma 3.

E

2 IP EL, model embeds model satisfies E , satisfies E .

Proposition 4. projection model LGCF internal positive event logic (and
hence AMA), semantic equivalence.
Proof: Consider model M. know MAP(M) covers M, remains show
MAP(M) least general formula so, semantic equivalence.
Let E IPEL formula covers M. Let M0 model covered MAP(M)
want show E covers M0 . know, Lemma 1, witnessing
interdigitation MAP(M0 ) MAP(M). Thus, Proposition 2, MAP(M0 ) MAP(M)
showing M0 embeds M. Combining facts Lemma 3 follows E covers
M0 hence MAP(M) E . 2
Proposition 4 tells us that, IPEL, LGCF model exists, unique,
timeline. Given property, AMA formula covers timelines covered
another AMA formula 0 , 0 . Thus, remainder paper, considering
subsumption formulas, abstract away temporal models deal instead
timelines. Proposition 4 tells us compute LGCF model constructing
projection model. Based definition projection, straightforward
derive LGCF algorithm runs time polynomial size model7 . note
projection may contain repeated states. practice, remove repeated states, since
change meaning resulting formula (as described Example 1).
3.4.4 C OMBINING NTERDIGITATION



G ENERALIZATION



PECIALIZATION

Interdigitations useful analyzing conjunctions disjunctions timelines.
conjoining set timelines, model conjunction induces interdigitation timelines
co-occurring states simultaneously hold model point (viewing states
sets, states resulting unioning co-occurring states must hold). constructing
7. take size model = hM; sum x 2 number true propositions (x).

395

fiF ERN , G IVAN , & ISKIND

interdigitation taking union tuple co-occurring states get sequence states,
get timeline forces conjunction timelines hold. call sequence
interdigitation specialization timelines. Dually, interdigitation generalization involving
intersections states gives timeline holds whenever disjunction set timelines
holds.
Definition 3. interdigitation generalization (specialization) set timelines
timeline s1 ; : : : ; sm , that, interdigitation tuples, sj intersection
(respectively, union) components jth tuple sequence . set interdigitation
generalizations (respectively, specializations) called IG() (respectively, IS()).
Example 3. Suppose s1 ; s2 ; s3 ; t1 ; t2 ; t3 sets propositions (i.e., states). Consider timelines = s1 ; s2 ; s3 = t1 ; t2 ; t3 . relation

f hs1; t1 ; hs2; t1 ; hs3; t2 ; hs3; t3 g
interdigitation states s1 s2 co-occur t1 , s3 co-occurs
t2 t3 . corresponding IG members

s1 \ t1 ; s2 \ t1 ; s3 \ t2 ; s3 \ t3
s1 [ t1 ; s2 [ t1 ; s3 [ t2 ; s3 [ t3

2 IG(fS; g)
2 IS(fS; g):

t1 s1 ; t1 s2 ; t2 s3 ; t3 s3 , interdigitation witnesses

T.

timeline IG() (dually, IS()) subsumes (is subsumed by) timeline
easily verified using Proposition 2. complexity analyses, note number states
member IG() IS() bounded number states
timelines bounded total number states timelines
. number interdigitations , thus members IG() IS(), exponential total number states. algorithms present later computing LGGs
require computation IG () IS(). give pseudo-code compute
quantities. Figure 6 gives pseudo-code function an-IG-member non-deterministically
computes arbitrary member IG() (an-IS-member same, except replace intersection union). Given set timelines compute IG() executing possible
deterministic computation paths function call an-IG-member(), i.e., computing set
results obtainable non-deterministic function possible decisions non-deterministic
choice points.
give useful lemma proposition concerning relationships conjunctions disjunctions concepts (the former AMA concepts). convenience here,
use disjunction concepts, producing formulas outside AMA obvious interpretation.
Lemma 5. Given formula subsumes member set formulas,
subsumes member 0 IG(). Dually, subsumed member ,
subsumed member 0 IS(). case, length 0 bounded
size .

396

fiL EARNING EMPORAL E VENTS

an-IG-member (f1 ; 2 ; : : : ; n g)

// Input: timelines 1 ; : : : ; n
// Output: member IG(f1 ; 2 ; : : : ; n g)

return map (intersect-tuple ; an-interdigitation (f1 ; : : : ; n g));
Figure 6: Pseudo-code an-IG-member, non-deterministically computes member
IG(T ) set timelines. function intersect-tuple(I ) takes tuple
sets argument returns intersection. higher-order function map(f; )
takes function f tuple arguments returns tuple length
obtained applying f element making tuple results.
Proposition 6.

following hold:

1. (and-to-or) conjunction set timelines equals disjunction timelines
IS().
2. (or-to-and) disjunction set timelines subsumed conjunction
timelines IG().
Proof: prove or-to-and, recall that, 2 0 2 IG(), 0 .
W
V
immediate ( ) ( IG()). Using dual argument, show
W
V
V
W
( IS()) ( ). remains Vto show ( ) ( ISW()), equivalent showing
timeline subsumed ( ) subsumed ( IS()) (by Proposition 4). Consider
V
timeline ( )this implies member subsumes . Lemma
W
5 implies 0 2 IS() 0 . get ( IS())
desired. 2
Using and-to-or, reduce AMA subsumption subsumption, exponential increase problem size.
Proposition 7.
2 ; 1 2 .

AMA

1



2 , 1

2 1 2 IS( 1) 2 2

Proof: forward direction show contrapositive. Assume 1 2 IS( 1 )
2 2 2 W1 6 2 . Thus, timeline
1 6 2 .
W
tells us ( IS( 1 )) 6 2 , thus ( IS( 1 )) 6 2 and-to-or get
1 6 2 .
backward direction assume 1 2 IS( 1 ) 2 2 2 1 2 .
W
tells us 1 2 IS( 1 ), 1 2 thus, 1 = ( IS( 1 )) 2 . 2

4. Subsumption Generalization
section study subsumption generalization AMA formulas. First, give
polynomial-time algorithm deciding subsumption formulas show
deciding subsumption AMA formulas coNP-complete. Second give algorithms complexity bounds construction least-general generalization (LGG) formulas based
397

fiF ERN , G IVAN , & ISKIND

MA-subsumes (1 ; 2 )
// Input: 1 = s1 ; : : : ; sm 2
// Output: 1 2

= t1 ; : : : ; tn

1. path v1;1 vm;n SG(1 ; 2 ) return TRUE. example,
(a)
(b)

(c)

Create array Reachable(i,j ) boolean values, FALSE, 0
0 j n.
:= 1 m, Reachable(i; 0) := TRUE;
j := 1 n, Reachable(0; j ) := TRUE;
:= 1
j := 1 n
Reachable(i; j ) := (ti sj ^ ( Reachable(i
Reachable(i; j
Reachable(i

Reachable(m; n) return TRUE;



1; j ) _
1) _
1; j 1));

2. Otherwise, return FALSE;
Figure 7: Pseudo-code subsumption algorithm.
defined main text.

SG(1 ; 2 ) subsumption graph

analysis subsumption, including existence, uniqueness, lower/upper bounds, algorithm
LGG AMA formulas. Third, introduce polynomial-timecomputable syntactic notion
subsumption algorithm computes corresponding syntactic LGG exponentially faster semantic LGG algorithm. Fourth, Section 4.4, give detailed example
showing steps performed LGG algorithms compute semantic syntactic LGGs
two AMA formulas.
4.1 Subsumption
methods rely critically novel algorithm deciding subsumption question 1 2
formulas 1 2 polynomial-time. note merely searching possible
interdigitations 1 2 witnessing interdigitation provides obvious decision procedure
subsumption questionhowever, are, general, exponentially many interdigitations. reduce subsumption problem finding path graph pairs states
1 2 , polynomial-time operation. Pseudo-code resulting subsumption algorithm shown Figure 7. main data structure used subsumption algorithm
subsumption graph.
Definition 4. subsumption graph two timelines 1 = s1 ; ; sm 2 = t1 ; ; tn
(written SG(1 ; 2 )) directed
graph G = hV; E V = fvi;j j 1 m; 1 j ng .

(directed) edge set E equals hvi;j ; vi ;j j si tj ; si tj ; i0 + 1; j j 0 j + 1 .
0

0

0

0

achieve polynomial-time bound one simply use polynomial-time pathfinding algorithm. case special structure subsumption graph exploited determine
398

fiL EARNING EMPORAL E VENTS

desired path exists (mn) time, example method shown pseudo-code illustrates.
following theorem asserts correctness algorithm assuming correct polynomial-time
path-finding method used.
Lemma 8. Given timelines 1 = s1 ; : : : ; sm 2 = t1 ; : : : ; tn , witnessing
interdigitation 1 2 iff path subsumption graph SG(1 ; 2 ) v1;1
vm;n .
Theorem 9.
mial time.

Given timelines 1 2 , MA-subsumes(1 ; 2 ) decides 1

2 polyno-

Proof: algorithm clearly runs polynomial time. Lemma 8 tells us line 2 algorithm
return TRUE iff witnessing interdigitation. Combining Proposition 2 shows
algorithm returns TRUE iff 1 2 . 2
Given polynomial-time algorithm subsumption, Proposition 7 immediately suggests
exponential-time algorithm deciding AMA subsumptionby computing subsumption
exponentially many timelines one formula timelines formula.
next theorem suggests cannot better worst casewe argue
AMA subsumption coNP-complete reduction boolean satisfiability. Readers uninterested
technical details argument may skip directly Section 4.2.
develop correspondence boolean satisfiability problems, include negation,
AMA formulas, lack negation, imagine boolean variable two AMA
propositions, one true one false. particular, given boolean satisfiability problem
n variables p1 ; : : : ; pn , take set PROPn set containing 2n AMA propositions
Truek Falsek k 1 n. represent truth assignment pi
variables AMA state sA given follows:

sA = fTruei j 1 n; A(pi ) = trueg [ fFalsei j 1 n; A(pi ) = falseg
Proposition 7 suggests, checking AMA subsumption critically involves exponentially
many interdigitation specializations timelines one AMA formulas. proof,
design AMA formula whose interdigitation specializations seen correspond truth
assignments8 boolean variables, shown following lemma.
Lemma 10.

Given n, let conjunction timelines
n
[
i=1

f(PROPn; Truei; Falsei; PROPn); (PROPn; Falsei; Truei; PROPn)g:

following facts truth assignments Boolean variables p1 ; : : : ; pn :
1. truth assignment A, PROPn ; sA ; PROPn semantically equivalent member
IS( ).
2. 2 IS( ) truth assignment PROPn ; sA ; PROPn .
8. truth assignment function mapping boolean variables true false.

399

fiF ERN , G IVAN , & ISKIND

lemma hand, tackle complexity AMA subsumption.
Theorem 11.

Deciding AMA subsumption coNP-complete.

Proof: first show deciding AMA-subsumption 1 2 coNP providing
polynomial-length certificate answer. certificate non-subsumption
interdigitation timelines 1 yields member IS( 1 ) subsumed 2 .
certificate checked polynomial time: given interdigitation, corresponding
member IS( 1 ) computed time polynomial size 1 , test
whether resulting timeline subsumed timeline 2 using polynomial-time MAsubsumption algorithm. Proposition 7 guarantees 1 6 2 iff timeline IS( 1 )
subsumed every timeline 2 , certificate exist exactly
answer subsumption query no.
show coNP-hardness reduce problem deciding satisfiability 3-SAT formula
= C1 ^ ^ Cm problem recognizing non-subsumption AMA formulas. Here,
Ci (li;1 _ li;2 _ li;3 ) li;j either proposition p chosen P = fp1 ; : : : ; pn g
negation :p. idea reduction construct AMA formula view
exponentially many members IS( ) representing truth assignments. construct
timeline view representing :S show satisfiable iff 6 .
Let defined Lemma 10. Let formula s1 ; : : : ; sm ,

si =

fFalsej j li;k = pj kg [
fTruej j li;k = :pj kg:

si thought asserting Ci . start showing satisfiable
6 . Assume satisfied via truth assignment Awe know Lemma 10
0 2 IS( ) semantically equivalent PROPn ; sA ; PROPn . show
PROPn ; sA ; PROPn subsumed , conclude 6 using Proposition 7, desired.
Suppose contradiction PROPn ; sA ; PROPn subsumed state sA must
subsumed state si . Consider corresponding clause Ci . Since satisfies
Ci satisfied least one literals li;k must true. Assume li;k = pj (a
dual argument holds li;k = :pj ), si contains Falsej sA contains Truej
Falsej thus, sA 6 si (since si 6 sA ), contradicting choice i.
complete proof, assume unsatisfiable show . Using
Proposition 7, consider arbitrary 0 IS( )we show 0 . Lemma 10
know truth assignment 0 PROPn ; sA ; PROPn . Since unsatisfiable
know Ci satisfied hence :Ci satisfied A. implies
primitive proposition si sA . Let W following interdigitation =
PROPn ; sA ; PROPn = s1 ; : : : ; sm :

fhPROPn; s1 hPROPn; s2 hPROPn; sii hsA; sii hPROPn; sii hPROPn; si+1i hPROPn; smig

see tuple co-occurring states given state subsumed
state . Thus W witnessing interdigitation PROPn ; sA ; PROPn ,
holds Proposition 2combining 0 PROPn ; sA ; PROPn get 0 . 2
Given hardness result later define weaker polynomial-timecomputable subsumption
notion use learning algorithms.
400

fiL EARNING EMPORAL E VENTS

4.2 Least-General Generalization.
AMA LGG set AMA formulas AMA formula general
formula set strictly general formula. existence
AMA LGG nontrivial infinite chains increasingly specific formulas
generalize given formulas. Example 2 demonstrated chains subsumee
extended AMA subsumees. example, member chain P ; Q, P ; Q; P ; Q,
P ; Q; P ; Q; P ; Q; : : : covers 1 = (P ^ Q); Q 2 = P ; (P ^ Q). Despite complications,
AMA LGG exist.
Theorem 12. LGG finite set AMA formulas subsumed
generalizations .
Proof: Let set 2 IS( 0 ). Let conjunction timelines
generalize size larger . Since finite number primitive
propositions, finite number timelines, well defined9 . show
least-general generalization . First, note timeline generalizes thus
(by Proposition 6), must generalize . Now, consider arbitrary generalization 0 .
Proposition 7 implies 0 must generalize formula . Lemma 5 implies
timeline 0 must subsume timeline longer size subsumes
timelines . must timeline , choice , every timeline
0 subsumes timeline . follows 0 subsumes , LGG subsumed
LGGs , desired. 2


0

Given AMA LGG exists unique show compute it. first step
strengthen or-to-and Proposition 6 get LGG sublanguage.
Theorem 13. set formulas, conjunction timelines IG() AMA
LGG .
Proof: Let specified conjunction. Since timeline IG() subsumes timelines
, subsumes member . show least-general formula, consider
AMA formula 0 subsumes members . Since timeline 0 must subsume
members , Lemma 5 implies timeline 0 subsumes member IG() thus
timeline 0 subsumes . implies 0 . 2
characterize AMA LGG using IG.
Theorem 14.



IG( 2 IS( )) AMA LGG set AMA formulas.

Proof: Let = f 1 ; : : : ; n g E = 1 _ _ n . know AMA LGG
must subsume E , would fail subsume one . Using and-to-or represent
W
W
E disjunction timelines given E = ( IS( 1 )) _ _ ( IS( n )). AMA
LGG must least-general formula subsumes E i.e., AMA LGG set

timelines fIS( )j 2 g. Theorem 13 tells us LGG timelines given

IG( fIS( )j 2 g). 2
9. must least one timeline, timeline state true

401

fiF ERN , G IVAN , & ISKIND

1:
2:
3:
4:
5:
6:
7:
8:
9:

10:
11:
12:
13:
14:

15:

semantic-LGG(f 1 ; 2 ; : : : ; g)

// Input: AMA formulas 1 ; : : : ;
// Output: LGG f 1 ; : : : ; g

:= fg;
:= 1
all-values(an-IS-member ( ))
(80 2 : 6 0 )
0 := f00 2 j 00 g;
:= (S 0 ) [ fg;
G := fg;
all-values(an-IG-member(S ))
(80 2 G : 0 6 )
G0 := f00 2 G j 00 g;
G := (G G0 ) [ fg;
V

return (

G)

Figure 8: Pseudo-code computing semantic AMA LGG set AMA formulas.
Theorem 14 leads directly algorithm computing AMA LGGFigure 8 gives
pseudo-code computation. Lines 4-9 pseudo-code correspond computation

fIS( )j 2 g, timelines included set subsumed timelines
already set (which checked polynomial time subsumption algorithm).
pruning, accomplished test line 7, often drastically reduces size timeline set perform subsequent IG computationthe final result affected
pruning since subsequent IG computation generalization step. remainder

pseudo-code corresponds computation IG( fIS( )j 2 g) include
timelines final result subsume timeline set. pruning step (the test
line 12) sound since one timeline subsumes another, conjunction timelines
equivalent specific one. Section 4.4.1 traces computations algorithm
example LGG calculation.
Since sizes IS() IG() exponential sizes inputs, code
Figure 8 doubly exponential input size. conjecture cannot better this,
yet proven doubly exponential lower bound AMA case. input
formulas timelines algorithm takes singly exponential time, since IS(fg) =
MA. prove exponential lower bound input formulas MA. Again,
readers uninterested technical details proof safely skip forward Section 4.3.
argument, take available primitive propositions set fpi;j j 1
n; 1 j ng, consider timelines


1 = s1; ; s2; ; : : : ; sn;
2 = s;1 ; s;2 ; : : : ; s;n ;
402



fiL EARNING EMPORAL E VENTS



si; = pi;1 ^ ^ pi;n
s;j = p1;j ^ ^ pn;j :

show AMA LGG 1 2 must contain exponential number timelines.
particular, show AMA LGG equivalent conjunction subset
IG(f1 ; 2 g), certain timelines may omitted subset.
Lemma 15. AMA LGG set
timelines IG() j 0 j j j

timelines equivalent conjunction 0

Proof: Lemma 5 implies timeline must subsume timeline 0 2 IG().
conjunction 0 0 must equivalent , since clearly covers covered
LGG . Since 0 formed taking one timeline IG() timeline ,
j 0 j j j. 2 complete argument showing exponentially many
timelines IG(f1 ; 2 g) cannot omitted conjunction remains LGG.
Notice i; j si; \s;j = pi;j . implies state IG(f1 ; 2 g)
contains exactly one proposition, since state formed intersecting state 1
2 . Furthermore, definition interdigitation, applied here, implies following two facts
timeline q1 ; q2 ; : : : ; qm IG(f1 ; 2 g):
1. q1

= p1;1 qm = pn;n.

2. consecutive states qk
= i0 j

= pi;j qk+1 = pi ;j , i0 either + 1, j 0 either j j + 1,
= j0.
0

0

Together facts imply timeline IG(f1 ; 2 g) sequence propositions starting
p1;1 ending pn;n consecutive propositions pi;j ; pi ;j different
i0 equal + 1 j 0 equal j j + 1. call timeline IG(f1 ; 2 g) square
pair consecutive propositions pi;j pi ;j either i0 = j 0 = j .
following lemma implies square timeline omitted conjunction timelines
IG(1 ; 2 ) remain LGG 1 2 .
0

0

0

0

Lemma 16. Let 1 2 given let = IG(f1 ; 2 g).
timelines subset omits square timeline, < 0 .
V

0 whose

n 2)! hence exponenThe number square timelines IG(f1 ; 2 g) equal (n (21)!(
n 1)!
tial size 1 2 . completed proof following result.

Theorem 17.

smallest LGG two formulas exponentially large.

Proof: Lemma 15, AMA LGG 0 1 2 equivalent conjunction
number timelines chosen IG(f1 ; 2 g). However, Lemma 16, conjunction
n 2)! timelines, must 0 , must exponentially
must least (n (21)!(
n 1)!
large. 2
Conjecture 18.

smallest LGG two AMA formulas doubly-exponentially large.
403

fiF ERN , G IVAN , & ISKIND

show lower-bound AMA LGG complexity merely consequence
existence large AMA LGGs. Even small LGG, expensive compute
due difficulty testing AMA subsumption:
Theorem 19. Determining whether formula AMA LGG two given AMA formulas 1
2 co-NP-hard, co-NEXP, size three formulas together.
Proof: show co-NP-hardness use straightforward reduction AMA subsumption. Given
two AMA formulas 1 2 decide 1 2 asking whether 2 AMA LGG 1
2 . Clearly 1 2 iff 2 LGG two formulas.
show co-NEXP upper bound, note check exponential time whether 1
2 using Proposition 7 polynomial-time subsumption algorithm. remains
show check whether least subsumer. Since Theorem 14 shows
LGG 1 2 IG(IS( 1 ) [ IS( 2 )), LGG 6 IG(IS( 1 ) [ IS( 2 )).
Thus, Proposition 7, least subsumer, must timelines 1 2 IS( )
2 2 IG(IS( 1 ) [ IS( 2 )) 1 6 2 . use exponentially long certificates
answers: certificate pair interdigitation I1 interdigitation I2
IS( 1 ) [ IS( 2 ), corresponding members 1 2 IS( ) 2 2 IG(IS( 1 ) [ IS( 2 ))
1 6 2 . Given pair certificates I1 I2 , 1 computed polynomial time,
2 computed exponential time, subsumption checked
polynomial time (relative size, exponential). LGG
IG(IS( 1 ) [ IS( 2 )), certificates exist. 2
4.3 Syntactic Subsumption Syntactic Least-General Generalization.
Given intractability results semantic AMA subsumption, introduce tractable generality notion, syntactic subsumption, discuss corresponding LGG problem. use
syntactic forms generality efficiency familiar ILP (Muggleton & De Raedt, 1994)
where, example, -subsumption often used place entailment generality relation.
Unlike AMA semantic subsumption, syntactic subsumption requires checking polynomially
many subsumptions, polynomial time (via Theorem 9).
Definition 5. AMA 1 syntactically subsumed AMA 2 (written 1
timeline 2 2 2 , timeline 1 2 1 1 2 .

syn 2) iff

Proposition 20. AMA syntactic subsumption decided polynomial time.
Syntactic subsumption trivially implies semantic subsumptionhowever, converse
hold general. Consider AMA formulas (A; B ) ^ (B ; A), A; B ; B
primitive propositions. (A; B ) ^ (B ; A) A; B ; A; however, neither A; B
A; B ; B ; A; B ; A, A; B ; syntactically subsume (A; B ) ^ (B ; A).
Syntactic subsumption fails recognize constraints derived interaction
timelines within formula.
Syntactic Least-General Generalization. syntactic AMA LGG syntactically least-general
AMA formula syntactically subsumes input AMA formulas. Here, least means
404

fiL EARNING EMPORAL E VENTS

formula properly syntactically subsumed syntactic LGG syntactically subsume input
formulas. Based hardness gap syntactic semantic AMA subsumption, one might
conjecture similar gap exists syntactic semantic LGG problems. Proving
gap exists requires closing gap lower upper bounds AMA LGG shown
Theorem 14 favor upper bound, suggested Conjecture 18. cannot yet
show hardness gap semantic syntactic LGG, give syntactic LGG algorithm
exponentially efficient best semantic LGG algorithm found (that
Theorem 14). First, show syntactic LGGs exist unique mutual syntactic
subsumption (and hence semantic equivalence).
Theorem 21. exists syntactic LGG AMA formula set syntactically subsumed syntactic generalizations .
Proof: Let conjunction timelines syntactically generalize
size larger . proof Theorem 12, well defined. show
syntactic LGG . First, note syntactically generalizes timeline
generalizes timeline every member , choice . consider arbitrary
syntactic generalization 0 . definition syntactic subsumption, timeline
0 must subsume timeline member . Lemma 5 implies
timeline 0 size larger subsumes subsumed .
choice , timeline 0 must timeline . follows 0 syntactically subsumes
, syntactic LGG subsumed syntactic generalizations . 2
general, know semantic syntactic LGGs different, though clearly syntactic
LGG semantic generalization must subsume semantic LGG. example, (A; B ) ^
(B ; A), A; B ; semantic LGG A; B ; A, discussed above; syntactic LGG
(A; B ; true) ^ (true; B ; A), subsumes A; B ; subsumed A; B ; A. Even
so, formulas:
Proposition 22.

AMA , syn

equivalent .

Proof: forward direction immediate since already know syntactic subsumption implies
semantic subsumption. reverse direction, note implies timeline
subsumes thus since single timeline timeline subsumes timeline
definition syntactic subsumption. 2
Proposition 23.

syntactic AMA LGG formula set semantic LGG .

Proof: Now, consider syntactic LGG . Proposition 22 implies semantic
generalization . Consider semantic LGG 0 . show 0 conclude
semantic LGG . Proposition 22 implies 0 syntactically subsumes . follows
0 ^ syntactically subsumes . But, 0 ^ syntactically subsumed , syntactic
LGG follows 0 ^ syntactically subsumes , would least syntactic
generalization . ( 0 ^ ), implies 0 , desired. 2
note stronger result stating formula syntactic LGG set formulas semantic LGG immediate consequence results above.
405

fiF ERN , G IVAN , & ISKIND

first examination, strengthening appears trivial, given equivalence syn
. However, semantically least necessarily stronger condition syntactically leastwe ruled possibility semantically least generalization may
syntactically subsume another generalization semantically (but syntactically) equivalent.
(This question open, found example phenomenon either.)
Proposition 23 together Theorem 21 nice consequence learning approach
syntactic LGG two AMA formulas semantic LGG formulas, long
original formulas syntactic LGGs sets timelines. learning approach starts training examples converted timelines using LGCF operation,
syntactic LGGs computed (whether combining training examples once, incrementally computing syntactic LGGs parts training data) always syntactic LGGs sets
timelines hence semantic LGGs, spite fact syntactic subsumption
weaker semantic subsumption. note, however, resulting semantic LGGs may
considerably larger smallest semantic LGG (which may syntactic LGG all).
Using Proposition 23, show cannot hope polynomial-time syntactic LGG
algorithm.
Theorem 24.

smallest syntactic LGG two formulas exponentially large.

Proof: Suppose always syntactic LGG two formulas exponentially large.
Since Proposition 23 formula semantic LGG, always semantic LGG
two formulas exponentially large. contradicts Theorem 17. 2
discouraging, algorithm syntactic LGG whose time complexity
matches lower-bound, unlike semantic LGG case, best algorithm
doubly exponential worst case. Theorem 14 yields exponential time method computing
semantic LGG set timelines since timeline , IS() = , simply
conjoin timelines IG(). Given set AMA formulas, syntactic LGG algorithm uses
method compute polynomially-many semantic LGGs sets timelines, one chosen
input formula, conjoins results.
Theorem 25.
1 ; : : : ; n .

formula

2 IG(f1 ; : : : ; n g) syntactic LGG AMA formulas

V





Proof: Let 2 IG(f1 ; : : : ; n g). timeline must subsume
output IG set containing timeline thus syntactically subsumes .
show syntactically least formula, consider 0 syntactically subsumes every
. show syn 0 conclude. timeline 0 0 subsumes timeline Ti 2 ,
i, assumption syn 0 . Lemma 5, 0 must subsume member
IG(fT1 ; : : : ; Tn g)and member timeline timeline 0 0 subsumes
timeline . conclude syn 0 , desired. 2
V

theorem yields algorithm computes syntactic AMA LGG exponential time
pseudo-code method given Figure 9. exponential time bound follows fact
exponentially many ways choose 1 ; : : : ; line 5,
exponentially many semantic-LGG members line 6 (since timelines)the
product two exponentials still exponential.
406

fiL EARNING EMPORAL E VENTS

1:
2:
3:
4:
5:
6:

syntactic-LGG(f 1 ; 2 ; : : : ; g)

// Input: AMA formulas f 1 ; : : : ; g
// Output: syntactic LGG f 1 ; : : : ; g

G := fg;

h1 ; : : : ; 2 1

semantic-LGG(f1 ; : : : ; g)

7:
8:
9:
10:

V

return (

(80 2 G : 0 6 )
G0 := f00 2 G j 00 g;
G := (G G0 ) [ fg;

G)

Figure 9: Pseudo-code computes syntactic AMA LGG set AMA formulas.
formula returned algorithm shown actually subset syntactic LGG given
Theorem 25. subset syntactically (and hence semantically) equivalent formula
specified theorem, possibly smaller due pruning achieved statement
lines 79. timeline pruned set (semantically) subsumed timeline
set (one timeline kept semantically equivalent group timelines, random).
pruning timelines sound, since timeline pruned output subsumes
formula outputthis fact allows easy argument pruned formula syntactically equivalent (i.e. mutually syntactically subsumed by) unpruned formula. Section 4.4.2
traces computations algorithm example LGG calculation. note empirical evaluation discussed Section 6, cost terms accuracy using
efficient syntactic vs. semantic LGG. know learned definitions made errors
direction overly specificthus, since semantic-LGG least specific
syntactic-LGG would advantage using semantic algorithm.
method exponential amount work even result small (typically
many timelines pruned output subsume remains). still
open question whether output-efficient algorithm computing syntactic AMA
LGGthis problem coNP conjecture coNP-complete. One route settling
question determine output complexity semantic LGG input formulas.
believe problem coNP-complete, proven this; problem P,
output-efficient method computing syntactic AMA LGG based Theorem 25.
summary algorithmic complexity results section found Table 3
conclusions section paper.
4.4 Examples: Least-General Generalization Calculations
work details semantic syntactic LGG calculation. consider
AMA formulas = (A; B ) ^ (B ; A) = A; B ; A, semantic LGG A; B ;
syntactic LGG (A; B ; true) ^ (true; B ; A).

407

fiF ERN , G IVAN , & ISKIND

4.4.1 EMANTIC LGG E XAMPLE
first step calculating semantic LGG, according algorithm given Figure 8,
compute interdigitation-specializations input formulas (i.e., IS() IS( )). Trivially,
IS() = = A; B ; A. calculate IS( ), must consider possible interdigitations , three,

f hA; B ; hB; B ; hB; Ai g
f hA; B ; hB; Ai g
f hA; B ; hA; Ai ; hB; Ai g
interdigitation leads corresponding member IS( ) unioning (conjoining) states
tuple, IS( )

f (A ^ B ); B ; (A ^ B );
(A ^ B );
(A ^ B ); A; (A ^ B ) g:
Lines 59 semantic LGG algorithm compute set , equal union
timelines IS( ) IS(), subsumed timelines removed. formulas, see
timeline IS( ) subsumed thus, = = A; B ; A.
computing , algorithm returns conjunction timelines IG(S ), redundant
timelines removed (i.e., subsuming timelines removed). case, IG(S ) = A; B ; A,
trivially, one timeline , thus algorithm correctly computes semantic LGG
A; B ; A.
4.4.2 YNTACTIC LGG E XAMPLE
syntactic LGG algorithm, shown Figure 9, computes series semantic LGGs
timeline sets, returning conjunction results (after pruning). Line 5 algorithm, cycles
timeline tuples cross-product input AMA formulas. case tuples
T1 = hA; B ; A; A; B T2 = hA; B ; A; B ; Aifor tuple, algorithm
computes semantic LGG tuples timelines.
semantic LGG computation tuple uses algorithm given Figure 8,
argument always set timelines rather AMA formulas. reason, lines 4
9 superfluous, timeline 0 , IS(0 ) = 0 . case tuple T1 , lines 49
algorithm compute = fA; B ; A; A; B g. remains compute interdigitationgeneralizations (i.e., IG(S )), returning conjunction timelines pruning (lines
1015 Figure 8). set interdigitations are,

f hA; Ai ; hB; Ai ; hB; B ; hB; Ai g
f hA; Ai ; hB; B ; hB; Ai g
f hA; Ai ; hA; B ; hB; B ; hB; Ai g
f hA; Ai ; hA; B ; hB; Ai g
f hA; Ai ; hA; B ; hA; Ai ; hB; Ai g
intersecting states interdigitation tuples get IG(S ),

f A; true; B ; true; A; B ; true; A; true; B ; true; A; true; true; A; true; A; true g
408

fiL EARNING EMPORAL E VENTS

Since timeline A; B ; true subsumed timelines IG(S ), timelines
pruned. Thus semantic LGG algorithm returns A; B ; true semantic LGG timelines
T1 .
Next syntactic LGG algorithm computes semantic LGG timelines T2 . Following
steps T1 , find semantic LGG timelines T2 true; B ; A. Since
A; B ; true true; B ; subsume one another, set G computed lines 59
syntactic LGG algorithm equal f A; B ; true; true; B ; g. Thus, algorithm computes
syntactic LGG (A; B ; true) ^ (true; B ; A). Note that, case, syntactic
LGG general semantic LGG.

5. Practical Extensions
implemented specific-to-general AMA learning algorithm based LGCF syntactic LGG algorithms presented earlier. implementation includes four practical extensions.
first extension aims controlling exponential complexity limiting length
timelines consider. Second describe often efficient LGG algorithm based
modified algorithm computing pairwise LGGs. third extension deals applying
propositional algorithm relational data, necessary application domain visual event
recognition. Fourth, add negation AMA language show compute corresponding LGCFs LGGs using algorithms AMA (without negation). Adding negation
AMA turns crucial achieving good performance experiments. end
section review overall complexity implemented system.
5.1 k-AMA Least-General Generalization
already indicated syntactic AMA LGG algorithm takes exponential time relative
lengths timelines AMA input formulas. motivates restricting AMA
language k -AMA practice, formulas contain timelines k states.
k increased algorithm able output increasingly specific formulas cost
exponential increase computational time. visual-eventrecognition experiments shown
later, increased k , resulting formulas became overly specific computational bottleneck reachedi.e., application best values k practically computable
ability limit k provided useful language bias.
use k -cover operator order limit syntactic LGG algorithm k -AMA. k -cover
AMA formula syntactically least general k -AMA formula syntactically subsumes
inputit easy show k -cover formula formed conjoining k -MA
timelines syntactically subsume formula (i.e., subsume timeline formula) .
Figure 10 gives pseudo-code computing k -cover AMA formula. shown
algorithm correctly computes k -cover input AMA formula. algorithm calculates
set least general k -MA timelines subsume timeline inputthe resulting k -MA
formulas conjoined redundant timelines pruned using subsumption test. note
k -cover AMA formula may exponentially larger formula; however,
practice, found k -covers exhibit undue size growth.
Given k -cover algorithm restrict learner k -AMA follows: 1) Compute
k-cover AMA input formula. 2) Compute syntactic AMA LGG resulting kAMA formulas. 3) Return k -cover resulting AMA formula. primary bottleneck
409

fiF ERN , G IVAN , & ISKIND

1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:

12:
13:
14:
15:
17:
18:
19:
20:

V

k-cover(k; 1im )
V
// Input: positive natural number k , AMA formula 1im
V
// Output: k -cover 1im
G := fg;
:= 1

:= hP1 ; : : : ; Pn all-values(a-k-partition (k; ))


:= ( P1 ); : : : ; ( Pn );
(80 2 G : 0 6 )
G0 := f00 2 G j 00 g;
G := (G G0 ) [ fg;
V
return ( G)
P

a-k-partition (k; s1 ; : : : ; sj )

// Input: positive natural number k , timeline s1 ; : : : ; sj
// Output: tuple k sets consecutive states partitions s1 ; : : : ; sj

k return hfs1g; : : : ; fsj gi;
k = 1 return hfs1 ; : : : ; sj gi;
l := a-member-of(f1; 2; : : : ; j k + 1g);
P0 = fs1 ; : : : ; sl g;
j

return extend-tuple (P0 ; a-k-partition (k

// pick next block size
// construct next block

1; sl+1 ; : : : ; sj ));

Figure 10: Pseudo-code non-deterministically computing k-cover AMA formula, along
non-deterministic helper function selecting k block partition states
timeline.

original syntactic LGG algorithm computing exponentially large set interdigitationgeneralizationsthe k -limited algorithm limits complexity computes interdigitationgeneralizations involving k -MA timelines.
5.2 Incremental Pairwise LGG Computation
implemented learner computes syntactic k-AMA LGG AMA formula setshowever,
directly use algorithm describe above. Rather compute LGG formula
sets via single call algorithm, typically efficient break computation
sequence pairwise LGG calculations. describe approach potential
efficiency gains.
straightforward show syntactic semantic subsumption
LGG( 1 ; : : : ; ) = LGG( 1 ; LGG( 2 ; : : : ; )) AMA formulas. Thus,
recursively applying transformation incrementally compute LGG AMA formulas via sequence 1 pairwise LGG calculations. Note since LGG operator
410

fiL EARNING EMPORAL E VENTS

commutative associative final result depend order process
formulas. refer incremental pairwise LGG strategy incremental approach
strategy makes single call k-AMA LGG algorithm (passing entire formula
set) direct approach.
simplify discussion consider computing LGG formula set
argument extended easily AMA formulas (and hence k-AMA). Recall syntactic
LGG algorithm Figure 9 computes LGG() conjoining timelines IG() subsume others, eliminating subsuming timelines form pruning. incremental
approach applies pruning step pair input formulas processedin contrast,
direct approach must compute interdigitation-generalization input formulas
pruning happen. resulting savings substantial, typically compensates
extra effort spent checking pruning (i.e. testing subsumption timelines
incremental LGG computed). formal approach describing savings constructed


based observation 2IG(f1 ;2 g) IG(fg[ ) 2LGG(1 ;2 ) IG(fg[ )
seen compute LGG [ f1 ; 2 g, latter possibly much cheaper
compute due pruning. is, LGG(1 ; 2 ) typically contains much smaller number
timelines IG(f1 ; 2 g).
Based observations implemented system uses incremental approach
compute LGG formula set. describe optimization used system speedup
computation pairwise LGGs, compared directly running algorithm Figure 9. Given
pair AMA formulas 1 = 1;1 ^ ^ 1;m 2 = 2;1 ^ ^ 2;n , let syntactic
LGG obtained running algorithm Figure 9. algorithm constructs computing
LGGs timeline pairs (i.e., LGG(1;i ; 2;j ) j ) conjoining results
removing subsuming timelines. turns often avoid computing many
LGGs. see consider case exists j 1;i 2;j , know
LGG(1;i ; 2;j ) = 2;j tells us 2;j considered inclusion (it may
pruned). Furthermore know LGG involving 2;j subsume 2;j thus
pruned . shows need compute LGGs involving 2;j , rather
need consider adding 2;j constructing .
observation leads modified algorithm (used system) computing
syntactic LGG pair AMA formulas. new algorithm computes LGGs
non-subsuming timelines. Given AMA formulas 1 2 , modified algorithm proceeds
follows: 1) Compute subsumer set = f 2 1 j 90 2 2 s:t: 0 g [ f 2 2 j 90 2
1 s:t: 0 g. 2) Let AMA 01 ( 02 ) result removing timelines 1 ( 2 )
. 3) Let 0 syntactic LGG 01 02 computed running algorithm Figure 9
(if either 0i empty 0 empty). 4) Let 0 conjunction timelines
subsume timeline 0 . 5) Return = 0 ^ 0 . method avoids computing LGGs
involving subsuming timelines (an exponential operation) cost performing polynomially
many subsumption tests (a polynomial operation). noticed significant advantage
using procedure experiments. particular, advantage tends grow process
training examples. due fact incrementally process training examples
resulting formulas become generalthus, general formulas likely
subsuming timelines. best case 1 syn 2 (i.e., timelines 2 subsuming), see step 2 produces empty formula thus step 3 (the expensive step) performs
workin case return set = 2 desired.
411

fiF ERN , G IVAN , & ISKIND

5.3 Relational Data
L EONARD produces relational models involve objects (force dynamic) relations
objects. Thus event definitions include variables allow generalization objects.
example, definition P ICK U P (x; y; z ) recognizes P ICK U P (hand; block; table) well
P ICK U P (man; box; floor). Despite fact k -AMA learning algorithm propositional,
still able use learn relational definitions.
take straightforward object-correspondence approach relational learning. view
models output L EONARD containing relations applied constants. Since (currently)
support supervised learning, set distinct training examples event type.
implicit correspondence objects filling role across different training models given type. example, models showing P ICK U P (hand; block; table)
P ICK U P (man; box; floor) implicit correspondences given hhand; mani, hblock; boxi,
htable; floori. outline two relational learning methods differ much objectcorrespondence information require part training data.
5.3.1 C OMPLETE BJECT C ORRESPONDENCE
first approach assumes complete object correspondence given, input, along
training examples. Given information, propositionalize training models
replacing corresponding objects unique constants. propositionalized models given
propositional k -AMA learning algorithm returns propositional k -AMA formula.
lift propositional formula replacing constant distinct variable. Lavrac et al.
(1991) taken similar approach.
5.3.2 PARTIAL BJECT C ORRESPONDENCE
approach assumes complete object-correspondence information. sometimes
possible provide correspondences (for example, color-coding objects fill identical
roles recording training movies), information always available.
partial object correspondence (or even none all) available, automatically complete
correspondence apply technique.
moment, assume evaluation function takes two relational models
candidate object correspondence, input, yields evaluation correspondence quality. Given set training examples missing object correspondences, perform greedy
search best set object-correspondence completions models. method works
storing set P propositionalized training examples (initially empty) set U unpropositionalized training examples (initially entire training set). first step, P empty,
evaluate pairs examples U , possible correspondences, select pair yields
highest score, remove examples involved pair U , propositionalize according best correspondence, add P . subsequent step, use previously
computed values pairs examples, one U one P , possible correspondences. select example U correspondence yields highest average
score relative models P example removed U , propositionalized according
winning correspondence, added P . fixed number objects, effort expended
polynomial size training set; however, number objects b appear
training example allowed grow, number correspondences must considered grows
412

fiL EARNING EMPORAL E VENTS

bb . reason, important events involved manipulate modest number
objects.
evaluation function based intuition object roles visual events (as well
events domains) often inferred considering changes initial
final moments event. Specifically, given two models object correspondence,
first propositionalize models according correspondence. Next, compute ADD
DELETE lists model. ADD list set propositions true final
moment initial moment. DELETE list set propositions true
initial moment final moment. add delete lists motivated STRIPS action
representations (Fikes & Nilsson, 1971). Given ADDi DELETEi lists models 1 2,
evaluation function returns sum cardinalities ADD1 \ ADD2 DELETE1 \
DELETE2 . heuristic measures similarity ADD DELETE lists two
models. intuition behind heuristic similar intuition behind STRIPS actiondescription languagei.e., differences initial final moments
event occurrence related target event, event effects described ADD
DELETE lists. found evaluation function works well visual-event domain.
Note, full object correspondences given learner (rather automatically
extracted learner), training examples interpreted specifying target event
took place well objects filled various event roles (e.g., P ICK U P (a,b,c)). Rather,
object correspondences provided training examples interpreted specifying
existence target event occurrence specify objects fill roles (i.e., training
example labeled P ICK U P rather P ICK U P (a,b,c)). Accordingly, rules learned
correspondences provided allow us infer target event occurred
objects filled event roles. example object correspondences manually provided
learner might produce rule,
"

4 (S UPPORTS (z; y) ^ C ONTACTS (z; y));
P ICK U P (x; y; z ) =
(S UPPORTS (x; y) ^ ATTACHED (x; y))

#

whereas learner automatically extracts correspondences would instead produce rule,
"

4 (S UPPORTS (z; y) ^ C ONTACTS (z; y));
P ICK U P =
(S UPPORTS (x; y) ^ ATTACHED (x; y))

#

worth noting, however, upon producing second rule availability single training
example correspondence information allows learner determine roles variables,
upon output first rule. Thus, assumption learner reliably
extract object correspondences, need label training examples correspondence information order obtain definitions explicitly recognize object roles.
5.4 Negative Information
AMA language allow negated propositions. Negation, however, sometimes necessary adequately define event type. section, consider language AMA ,
superset AMA, addition negated propositions. first give syntax semantics
AMA , extend AMA syntactic subsumption AMA . Next, describe approach
413

fiF ERN , G IVAN , & ISKIND

learning AMA formulas using above-presented algorithms AMA. show approach correctly computes AMA LGCF syntactic AMA LGG. Finally, discuss
alternative, related approach adding negation designed reduce overfitting appears
result full consideration negated propositions.
AMA syntax AMA, new grammar building states negated
propositions:
literal
state

::= true j prop j :3prop
::= literal j literal ^ state

prop primitive proposition. semantics AMA
state satisfaction.

AMA except



positive literal P (negative literal
true (false), every x 2 .10

:3P ) satisfied model hM; iff [x] assigns P



state l1 ^ ^ lm satisfied model hM; iff literal li satisfied hM; i.

Subsumption. important difference AMA AMA Proposition 2, establishing existence witnessing interdigitations subsumption, longer true .
words, two timelines 1 ; 2 2 AMA , 1 2 , need
interdigitation witnesses 1 2 . see this, consider AMA timelines:

1 = (a ^ b ^ c); b; a; b; (a ^ b ^ : c)
2 = b; a; c; a; b; a; : c; a; b
argue:
1. interdigitation witnesses 1 2 . see this, first show that,
witness, second fourth states 1 (each b) must interdigitate align
either first fifth, fifth ninth states 2 (also, b). either
cases, third state 1 interdigitate states 2 subsume it.
2. Even so, still 1 2 . see this, consider model hM; satisfies 1 .
must interval [i1 ; i2 ] within hM; [i1 ; i2 ]i satisfies third state 1 ,
state a. two cases:
(a) proposition c true point hM; [i1 ; i2 ]i. Then, one verify hM;
satisfies 1 2 following alignment:

1
2

=
=

(a ^ b ^ c); b;
b;

a;
a; c; a;

b;
b;

(a ^ b ^ : c)
a; : c; a; b

10. note important use notation :3P rather :P . event-logic, formula :P
satisfied model whenever P false instant model. Rather, event-logic interprets :3P
indicating P never true model (as defined above). Notice first form negation yield
liquid propertyi.e., :P true along interval necessarily subintervals. second form
negation, however, yield liquid property provided P liquid. important learning algorithms,
since assume states built liquid properties.

414

fiL EARNING EMPORAL E VENTS

(b) proposition c false everywhere hM; [i1 ; i2 ]i. Then, one verify hM;
satisfies 1 2 following alignment:

1 =
(a ^ b ^ c);
2 =
b; a; c; a;
follows 1 2 .

b;
b;

a;
a; : c; a;

b; (a ^ b ^ : c)
b

light examples, conjecture computationally hard compute AMA
subsumption even timelines. reason, extend definition syntactic subsumption AMA way provides clearly tractable subsumption test analogous
discussed AMA.
Definition 6. AMA 1 syntactically subsumed AMA 2 (written 1 syn 2 ) iff
timeline 2 2 2 , timeline 1 2 1 witnessing interdigitation
1 2 .
difference definition previous one AMA need
test witnessing interdigitations timelines rather subsumption timelines.
AMA formulas, note new old definition equivalent (due Proposition 2);
however, AMA new definition weaker, result general LGG formulas.
one might expect, AMA syntactic subsumption implies semantic subsumption tested
polynomial-time using subsumption graph described Lemma 8 test witnesses.
Learning. Rather design new LGCF LGG algorithms directly handle AMA ,
instead compute functions indirectly applying algorithms AMA transformed
problem. Intuitively, adding new propositions models (i.e., training examples) represent proposition negations. Assume training-example models
set propositions P = fp1 ; : : : ; pn g. introduce new set P = fp1 ; : : : ; pn g propositions
use construct new training models P [ P assigning true pi time
model iff pi false model time. forming new set training models (each
twice many propositions original models) compute least general AMA formula
covers new models (by computing AMA LGCFs applying syntactic AMA LGG
algorithm), resulting AMA formula propositions P [ P . Finally replace pi
:3pi resulting AMA formula 0 propositions P turns
syntactic subsumption 0 least general AMA formula covers original training
models.
show correctness transformational approach computing AMA
LGCF syntactic LGG. First, introduce notation. Let set models
P . Let set models P [ P , time, i, exactly one pi
pi true. Let following mapping M: hM; 2 M, [hM; i]
unique hM 0 ; 2 j 2 i, 0 (j ) assigns pi true iff (j ) assigns pi
true. Notice inverse functional mapping M. approach handling
negation using purely AMA algorithms begins applying original training models.
follows, consider AMA formulas propositions P , AMA formulas
propositions P [ P .
Let F mapping AMA AMA 2 AMA , F [ ] AMA formula
identical except :3pi replaced pi . Notice inverse F func415

fiF ERN , G IVAN , & ISKIND

tion AMA AMA corresponds final step approach described above.
following lemma shows one-to-one correspondence satisfaction AMA
formulas models satisfaction AMA formulas models M.
Lemma 26. model hM; 2 2 AMA ,
[hM; i].

covers hM;

iff

F [ ] covers

Using lemma, straightforward show transformational approach computes
AMA LGCF semantic subsumption (and hence syntactic subsumption).
Proposition 27.



hM; 2 M, let AMA LGCF model [hM; i].
LGCF hM; i, equivalence.

F 1 [] unique AMA

Then,

Proof: know covers [hM; i], therefore Lemma 26 know F 1 [] covers
hM; i. show F 1[] least-general formula AMA covers hM; i.
sake contradiction assume 0 2 AMA covers hM; 0 < F 1 [].
follows model hM 0 ; 0 covered F 1 [] 0 . Lemma 26
F [0 ] covers [hM; i] since unique AMA LGCF [hM; i],
equivalence, F [0 ]. However, [hM 0 ; 0 i] covered
F [0 ] gives contradiction. Thus, 0 exist. follows
AMA LGCF. uniqueness AMA LGCF equivalence follows AMA
closed conjunction; two non-equivalent LGCF formulas, could
conjoined get LGCF formula strictly less one them. 2
use fact F operator preserves syntactic subsumption. particular, given
two timelines 1 ; 2 , clear witnessing interdigitation 1 2 trivially
converted witness F [1 ] F [2 ] (and vice versa). Since syntactic subsumption defined
terms witnessing interdigitations, follows 1 ; 2 2 AMA , ( 1 syn 2 ) iff
(F [ 1 ] syn F [ 2 ]). Using property, straightforward show compute syntactic
AMA LGG using syntactic AMA LGG algorithm.
Proposition 28.

AMA

formulas

1 ; : : : ; ,

let



[ 1 ]; : : : ; F [ ]g. Then, F 1[ ] unique syntactic AMA

syntactic AMA LGG
LGG f 1 ; : : : ; g.

Proof: know i, F [ ] syn thus, since F 1 preserves syntactic subsumption,
i, syn F 1 [ ]. shows F 1 [ ] generalization inputs.
show F 1 [ ] least formula. sake contradiction assume
F 1 [ ] least. follows must 0 2 AMA 0 <syn F 1 [ ]
i, syn 0 . Combining fact F preserves syntactic subsumption, get
F [ 0 ] <syn i, F [ ] F [ 0 ]. contradicts fact LGG;
must F 1 [ ] syntactic AMA LGG. argued elsewhere, uniqueness
LGG follows fact AMA closed conjunction. 2
propositions ensure correctness transformational approach computing
syntactic LGG within AMA . case semantic subsumption, transformational approach
correctly compute AMA LGG. see this, recall given two timelines 1 ; 2 2 AMA , 1 2 , witnessing interdigitation. Clearly
416

fiL EARNING EMPORAL E VENTS

semantic subsumption, AMA LGG 1 2 2 . However, semantic AMA LGG
F [1 ] F [2 ] F [2 ]. reason since witness F [1 ] F [2 ]
(and F [i ] timelines), know Proposition 2 F [1 ] 6 F [2 ]. Thus, F [2 ]
cannot returned AMA LGG, since subsume input formulasthis shows
transformational approach return 2 = F 1 [F [2 ]]. Here, transformational
approach produce AMA formula general 2 .
computational side, note that, since transformational approach doubles number propositions training data, algorithms specifically designed AMA may
efficient. algorithms might leverage special structure transformed examples
AMA algorithms ignorein particular, exactly one pi pi true time.
Boundary Negation. experiments, actually compare two methods assigning truth
values pi propositions training data models. first method, called full negation,
assigns truth values described above, yielding syntactically least-general AMA formula
covers examples. found, however, using full negation often results learning overly
specific formulas. help alleviate problem, second method places bias use
negation. choice bias inspired idea that, often, much useful information
characterizing event type pre- post-conditions. second method, called boundary
negation, differs full negation allows pi true initial final moments
model (and pi false). pi must false times. is, allow
informative negative information beginnings ends training examples.
found boundary negation provides good trade-off negation (i.e., AMA),
often produces overly general results, full negation (i.e., AMA ), often produces overly
specific much complicated results.
5.5 Overall Complexity Scalability
review overall complexity visual event learning component discuss
scalability issues. Given training set temporal models (i.e., set movies), system
following: 1) Propositionalize training models, translating negation descried Section 5.4.
2) Compute LGCF propositional model. 3) Compute k -AMA LGG LGCFs.
4) Return lifted (variablized) version LGG. Steps two four require little computational
overhead, linear sizes input output respectively. Steps one three
computational bottlenecks systemthey encompass inherent exponential complexity
arising relational temporal problem structure.
Step One. Recall Section 5.3.2 system allows user annotate training examples object correspondence information. technique propositionalizing models
shown exponential number unannotated objects training example. Thus,
system requires number objects relatively small correspondence information
given small number objects. Often event class definitions interested
involve large number objects. true, controlled learning setting
manage relational complexity generating training examples small number (or
zero) irrelevant objects. case domains studied empirically paper.
less controlled setting, number unannotated objects may prohibit use
correspondence techniquethere least three ways one might proceed. First, try
417

fiF ERN , G IVAN , & ISKIND

develop efficient domain-specific techniques filtering objects finding correspondences.
is, particular problem may possible construct simple filter removes irrelevant
objects consideration find correspondences remaining objects. Second,
provide learning algorithm set hand-coded first-order formulas, defining set
domain-specific features (e.g., spirit Roth & Yih, 2001). features used
propositionalize training instances. Third, draw upon ideas relational learning
design truly first-order version k -AMA learning algorithm. example, one could use
existing first-order generalization algorithms generalize relational state descriptions. Effectively
approach pushes object correspondence problem k -AMA learning algorithm rather
treating preprocessing step. Since well known computing first-order LGGs
intractable (Plotkin, 1971), practical generalization algorithms retain tractability constraining
LGGs various ways (e.g., Muggleton & Feng, 1992; Morales, 1997).
Step Three. system uses ideas Section 5.2 speedup k -AMA LGG computation
set training data. Nevertheless, computational complexity still exponential k thus,
practice restricted using relatively small values k . restriction limit
performance visual event experiments, expect limit direct applicability
system complex problems. particular, many event types interest may
adequately represented via k -AMA k small. event types, however, often contain
significant hierarchical structurei.e., decomposed set short sub-event types.
interesting research direction consider using k -AMA learner component
hierarchical learning systemthere could used learn k -AMA sub-event types. note
learner alone cannot applied hierarchically requires liquid primitive events,
learns non-liquid composite event types. work required (and intended) construct
hierarchical learner based perhaps non-liquid AMA learning.
Finally, recall compute LGG examples, system uses sequence 1
pairwise LGG calculations. fixed k , pairwise calculation takes polynomial time. However, since size pairwise LGG grow least constant factor respect
inputs, worst-case time complexity computing sequence 1 pairwise LGGs exponential m. expect worst case primarily occur target event type
compact k -AMA representationin case hierarchical approach described
appropriate. compact representation, empirical experience indicates
growth occurin particular, pairwise LGG tends yield significant pruning. problems, reasonable assumptions amount pruning11 imply time
complexity computing sequence 1 pairwise LGGs polynomial m.

6. Experiments
6.1 Data Set
data set contains examples 7 different event types: pick up, put down, stack, unstack, move,
assemble, disassemble. involve hand two three blocks. detailed
description sample video sequences event types, see Siskind (2001). Key frames
sample video sequences event types shown Figure 11. results segmentation,
11. particular, assume size pairwise k-AMA LGG usually bounded sizes k-covers
inputs.

418

fiL EARNING EMPORAL E VENTS

tracking, model reconstruction overlaid video frames. recorded 30 movies
7 event classes resulting total 210 movies comprising 11946 frames.12
replaced one assemble movie (assemble-left-qobi-04), duplicate copy another (assembleleft-qobi-11) segmentation tracking errors.
event classes hierarchical occurrences events one class contain occurrences events one simpler classes. example, movie depicting
OVE (a; b; c; d) event (i.e. moves b c d) contains subintervals P ICK U P (a; b; c)
P UT (a; b; d) events occur. experiments, learning definition event
class movies event class used training. train movies
event classes may depict occurrence event class learned subevent.
However, evaluating learned definitions, wish detect events correspond
entire movie well subevents correspond portions movie. example, given
movie depicting OVE (a; b; c; d) event, wish detect OVE(a; b; c; d) event
P ICK U P (a; b; c) P UT (a; b; d) subevents well. movie type data
set, set intended events subevents detected. definition
detect intended event, deem error false negative. definition detects unintended
event, deem error false positive. example, movie depicts OVE(a; b; c; d) event,
intended events OVE(a; b; c; d), P ICK U P (a; b; c), P UT (a; b; c). definition
pick detects occurrence P ICK U P (c; b; a) P ICK U P (b; a; c), P ICK U P (a; b; c),
charged two false positives well one false negative. evaluate definitions
terms false positive negative rates describe below.
6.2 Experimental Procedure
event type, evaluate k -AMA learning algorithm using leave-one-movie-out crossvalidation technique training-set sampling. parameters learning algorithm k
degree negative information used. value either P, positive propositions
only, BN, boundary negation, N, full negation. parameters evaluation procedure
include target event type E training-set size N . Given information, evaluation
proceeds follows: movie (the held-out movie) 210 movies, apply k AMA learning algorithm randomly drawn training sample N movies 30 movies
event type E (or 29 movies one 30). Use L EONARD detect occurrences
learned event definition . Based E event type , record number false
positives false negatives , detected L EONARD . Let FP FN total number
false positives false negatives observed 210 held-out movies respectively. Repeat
entire process calculating FP FN 10 times record averages FP FN.13
Since event types occur frequently data others simpler events
occur subevents complex events vice versa, report FP FN directly.
Instead, normalize FP dividing total number times L EONARD detected target
event correctly incorrectly within 210 movies normalize FN dividing total
12. source code data used experiments available Online Appendix 1,
ftp://ftp.ecn.purdue.edu/qobi/ama.tar.Z.
13. record times experiments, system fast enough give live demos N = 29
k = 3 boundary negation, giving best results show (though dont typically record 29 training
videos live demo reasons). less favorable parameter settings (particularly k = 4 full
negation) take (real-time) hour so.

419

fiF ERN , G IVAN , & ISKIND

pick

put

stack

unstack

move

assemble

disassemble

Figure 11: Key frames sample videos 7 event types.

420

fiL EARNING EMPORAL E VENTS

number correct occurrences target event within 210 movies (i.e., human assessment
number occurrences target event). normalized value FP estimates probability target event occur given predicted occur, normalized
value FN estimates probability event predicted occur given
occur.
6.3 Results
evaluate k -AMA learning approach, ran leave-one-movie-out experiments, described
above, varying k , , N . 210 example movies recorded color-coded objects
provide complete object-correspondence information. compared learned event definitions
performance two sets hand-coded definitions. first set HD1 hand-coded definitions
appeared Siskind (2001). response subsequent deeper understanding behavior
L EONARD model-reconstruction methods, manually revised definitions yield another
set HD2 hand-coded definitions gives significantly better FN performance cost
FP performance. Appendix C gives event definitions HD1 HD2 along set
machine-generated definitions, produced k -AMA learning algorithm, given training data
k = 30 = BN.
6.3.1 BJECT C ORRESPONDENCE
evaluate algorithm finding object correspondences, ignored correspondence information provided color coding applied algorithm training models event
type. algorithm selected correct correspondence 210 training models. Thus,
data set, learning results correspondence information given identical
correspondences manually provided, except that, first case, rules
specify particular object roles, discussed section 5.3.2. Since evaluation procedure uses
role information, rest experiments use manual correspondence information, provided
color-coding, rather computing it.
correspondence technique perfect experiments, may suited
event types. Furthermore, likely produce errors noise levels increase. Since
correspondence errors represent form noise learner makes special provisions
handling noise, results likely poor errors common. example,
worst case, possible single extremely noisy example cause LGG trivial (i.e.,
formula true). cases, forced improve noise tolerance learner.
6.3.2 VARYING k

first three rows Table 1 show FP FN values 7 event types k 2 f2; 3; 4g ,
N = 29 (the maximum), = BN. Similar trends found = P = N.
general trend that, k increases, FP decreases remains FN increases remains
same. trend consequence k -cover approach. because, k increases,
k -AMA language contains strictly formulas. Thus k1 > k2 , k1 -cover formula
never general k2 -cover. strongly suggests, prove, FP
non-increasing k FN non-decreasing k .
results show 2-AMA overly general put assemble, i.e. gives high
FP. contrast, 3-AMA achieves FP = 0 event type, pays penalty FN compared
421

fiF ERN , G IVAN , & ISKIND

k
2 BN

pick

put

stack

unstack

move

assemble

disassemble

FP
FN

0
0

0.14
0.19

0
0.12

0
0.03

0
0

0.75
0

0
0

3

BN

FP
FN

0
0

0
0.2

0
0.45

0
0.10

0
0.03

0
0.07

0
0.10

4

BN

FP
FN

0
0

0
0.2

0
0.47

0
0.12

0
0.03

0
0.07

0
0.17

3

P

FP
FN

0.42
0

0.5
0.19

0
0.42

0.02
0.11

0
0.03

0
0.03

0
0.10

3

BN

FP
FN

0
0

0
0.2

0
0.45

0
0.10

0
0.03

0
0.07

0
0.10

3

N

FP
FN

0
0.04

0
0.39

0
0.58

0
0.16

0
0.13

0
0.2

0
0.2

HD1

FP
FN

0.01
0.02

0.01
0.22

0
0.82

0
0.62

0
0.03

0
1.0

0
0.5

HD2

FP
FN

0.13
0.0

0.11
0.19

0
0.42

0
0.02

0
0.0

0
0.77

0
0.0

Table 1: FP FN learned definitions, varying k , hand-coded definitions.
2-AMA. Since 3-AMA achieves FP = 0, likely advantage moving k -AMA
k > 3. is, expected result FN become larger. effect demonstrated
4-AMA table.
6.3.3 VARYING

Rows four six Table 1 show FP FN 7 event types 2 fP; BN; Ng, N = 29,
k = 3. Similar trends observed values k . general trend that,
degree negative information increases, learned event definitions become specific.
words, FP decreases FN increases. makes sense since, negative information
added training models, specific structure found data exploited
k -AMA formulas. see that, = P, definitions pick put
overly general, produce high FP. Alternatively, = N, learned definitions
overly specific, giving FP = 0, cost high FN. experiments, well others,
found = BN yields best worlds: FP = 0 event types lower FN
achieved = N.
Experiments shown demonstrated that, without negation pick put down,
increase k arbitrarily, attempt specialize learned definitions, never significantly reduce FP. indicates negative information plays particularly important role
constructing definitions event types.

422

fiL EARNING EMPORAL E VENTS

6.3.4 C OMPARISON



H -C ODED EFINITIONS

bottom two rows table 1 show results HD1 HD2 . yet attempted
automatically select parameters learning (i.e. k ). Rather, focus comparing
hand-coded definitions parameter set judged best performing across event
types. believe, however, parameters could selected reliably using cross-validation
techniques applied larger data set. case, parameters would selected perevent-type basis would likely result even favorable comparison hand-coded
definitions.
results show learned definitions significantly outperform HD1 current data
set. HD1 definitions found produce large number false negatives current
data set. Notice that, although HD2 produces significantly fewer false negatives event types,
produces false positives pick put down. hand definitions
utilize pick put macros defining events.
performance learned definitions competitive performance HD2 .
main differences performance are: (a) pick put down, learned HD2 definitions
achieve nearly FN learned definitions achieve FP = 0 whereas HD2 significant
FP, (b) unstack disassemble, learned definitions perform moderately worse HD2
respect FN, (c) learned definitions perform significantly better HD2 assemble
events.
conjecture manual revision could improve HD2 perform well (and perhaps better than) learned definitions every event class. Nonetheless, view experiment
promising, demonstrates learning technique able compete with, sometimes
outperform, significant hand-coding efforts one authors.
6.3.5 VARYING N
practical interest know training-set size affects algorithms performance.
application, important method work well fairly small data sets, tedious
collect event data. Table 2 shows FN learning algorithm event type, N
reduced 29 5. experiments, used k = 3 = BN. Note FP = 0
event types N hence shown. expect FN increase N decreased,
since, specific-to-general learning, data yields more-general definitions. Generally, FN
flat N > 20, increases slowly 10 < N < 20, increases abruptly 5 < N < 10.
see that, several event types, FN decreases slowly, N increased 20 29.
indicates larger data set might yield improved results event types.
6.3.6 P ERSPICUITY



L EARNED EFINITIONS

One motivation using logic-based event representation support perspicuityin respect
results mixed. note perspicuity fuzzy subjective concept. Realizing this,
say event definition perspicuous humans knowledge language
would find definition natural. Here, assume human detailed knowledge model-reconstruction process learner trying fit. Adding assumption
would presumably make definitions qualify perspicuous, many complex features learned definitions appear fact due idiosyncrasies model-reconstruction
process. sense, evaluating perspicuity output entire system,
423

fiF ERN , G IVAN , & ISKIND

learner itself, key route improving perspicuity sense would improve
intuitive properties model-reconstruction output without change learner.
learned hand-coded definitions similar respect accuracy, typically
learned definitions much less perspicuous. simplest event types, however, learned
definitions arguably perspicuous. look issue detail. Appendix C gives
hand-coded definitions HD1 HD2 along set machine-generated definitions.
learned definitions correspond output k -AMA learner run 30 training
movies event type k = 3 = BN (i.e., best performing configuration
respect accuracy).
Perspicuous Definitions. P ICK U P (x; y; z ) P UT (x; y; z ) definitions particular interest since short state sequences appear adequate representing event types
thus, hope perspicuous 3-AMA definitions. fact, hand-coded definitions involve short sequences. Consider hand-coded definitions P ICK U P(x; y; z )the definitions
roughly viewed 3-MA timelines form begin;trans;end.14 State begin asserts facts
indicate z held x end asserts facts indicate held
x z . State trans intended model fact L EONARDs model-reconstruction
process always handle transition begin end smoothly (so definition
begin;end work well). make similar observations P UT OWN(x; y; z ).
Figure 15 gives learned 3-AMA definitions P ICK U P (x; y; z ) P UT (x; y; z )
definitions contain six two 3-MA timelines respectively. Since definitions consists
multiple parallel timelines, may first seem perspicuous. However, closer examination
reveals that, definition, single timeline arguably perspicuouswe
placed perspicuous timelines beginning definition. perspicuous timelines
natural begin;trans;end interpretation. fact, practically equivalent definitions
P ICK U P (x; y; z ) P UT (x; y; z ) HD2 .15
mind, notice HD2 definitions overly general indicated significant
false positive rates. learned definitions, however, yield false positives without significant
increase false negatives. learned definitions improve upon HD2 essentially specializing
HD2 definitions (i.e., perspicuous timelines) conjoining non-perspicuous
timelines. non-perspicuous timelines often intuitive, capture patterns
events help rule non-events. example, learned definition P ICK U P (x; y; z )
non-perspicuous timelines indicate ATTACHED (y; z ) true transition period
event. attachment relationship make intuitive sense. Rather, represents
systematic error made model reconstruction process pick events.
summary, see learned definitions P ICK U P (x; y; z ) P UT (x; y; z )
contain perspicuous timeline one non-perspicuous timelines. perspicuous timelines give intuitive definition events, whereas non-perspicuous timelines capture nonintuitive aspects events model reconstruction process important practice.
note that, experienced users, primary difficulty hand-coding definitions L EONARD
14. Note event-logic definition P ICK U P(x; y; z ) HD2 written compact form 3-MA,
definition converted 3-MA (and hence 3-AMA). Rather, HD1 cannot translated exactly 3-MA
since uses disjunctionit disjunction two 3-MA timelines.
15. primary difference HD2 definitions contain negated propositions. learner considers
proposition negation proposition true point training movies. Many negated
propositions HD2 never appear positively, thus included learned definitions.

424

fiL EARNING EMPORAL E VENTS

determining non-perspicuous properties must included. Typically requires many
iterations trial error. automated technique relieve user task. Alternatively,
could view system providing guidance task.
Large Definitions. TACK (w; x; y; z ) U NSTACK (w; x; y; z ) events nearly identical
put pick respectively. difference picking
putting onto two block (rather single block) tower (i.e., composed blocks z ).
Thus, might expect perspicuous 3-AMA definitions. However, see
learned definitions TACK (w; x; y; z ) U NSTACK (w; x; y; z ) Figures 16 17 involve
many timelines P ICK U P (w; x; ) P UT (w; x; ). Accordingly,
definitions quite overwhelming much less perspicuous.
Despite large number timelines, definitions general structure
pick put down. particular, contain distinguished perspicuous timeline,
placed beginning definition, conjoined many non-perspicuous timelines.
clear that, above, perspicuous timelines natural begin;trans;end interpretation
and, again, similar definitions HD 2 . case, however, definitions
HD2 overly general (committing false positives). Thus, inclusion
non-perspicuous timelines detrimental effect since unnecessarily specialize definition
resulting false negatives.
suspect primary reason large number non-perspicuous timelines relative
definitions pick put stems increased difficulty constructing
force-dynamic models. inclusion two block tower examples causes modelreconstruction process produce unintended results, particularly transition periods
TACK U NSTACK . result often many unintuitive physically incorrect patterns
involving three blocks hand produced transition period. learner
captures patterns roughly via non-perspicuous timelines. likely generalizing
definitions including training examples would filter timelines, making
overall definition perspicuous. Alternatively, interest consider pruning learned
definitions. straightforward way generate negative examples. these,
could remove timelines (generalizing definition) contribute toward rejecting
negative examples. unclear prune definitions without negative examples.
Hierarchical Events. OVE(w; x; y; z ), SSEMBLE (w; x; y; z ), ISASSEMBLE (w; x; y; z )
inherently hierarchical, composed four simpler event types. hand-coded definitions leverage structure utilizing simpler definitions macros. light,
clear that, viewed non-hierarchically, (as learner does) events involve relatively
long state sequences. Thus, 3-AMA adequate writing perspicuous definitions.
spite representational shortcoming, learned 3-AMA definitions perform quite well.
performance supports one arguments using AMA section 3.2. Namely, given
easier find short rather long sequences, practical approach finding definitions long
events conjoin short sequences within events. Examining timelines learned
3-AMA definitions reveals might expect. timeline captures often understandable
property long event sequence, conjunction timelines cannot considered
perspicuous definition. future direction utilize hierarchical learning techniques
improve perspicuity definitions maintaining accuracy.
425

fiF ERN , G IVAN , & ISKIND

N

pick

put

stack

unstack

move

assemble

disassemble

29
25
20
15
10
5

0.0
0.0
0.01
0.01
0.07
0.22

0.20
0.20
0.21
0.22
0.27
0.43

0.45
0.47
0.50
0.53
0.60
0.77

0.10
0.16
0.17
0.26
0.36
0.54

0.03
0.05
0.08
0.14
0.23
0.35

0.07
0.09
0.12
0.20
0.32
0.57

0.10
0.10
0.12
0.16
0.26
0.43

Table 2: FN k

= 3, = BN, various values N .

note, however, that, level, learned definition OVE (w; x; y; z ) given Figure 18 perspicuous. particular, first 3-MA timeline naturally interpreted giving
pre- post-conditions move action. is, initially x supported hand w
empty finally x supported z hand w empty. Thus, care preand post-conditions, might consider timeline perspicuous. remaining timelines
definition capture pieces internal event structure facts indicating x moved
hand. weaker case made assemble disassemble. first timeline
learned definitions Figures 19 20 interpreted giving pre- post-conditions.
However, cases, pre(post)-conditions assemble(disassemble) quite incomplete.
incompleteness due inclusion examples model-reconstruction process
properly handle initial(final) moments.

7. Related Work
discuss two bodies related work. First, present previous work visual event recognition relates experiments here. Second, discuss previous approaches learning
temporal patterns positive data.
7.1 Visual Event Recognition
system unique combines positive-only learning temporal, relational,
force-dynamic representation recognize events real video. Prior work investigated various subsets features systembut, date, system combined pieces
together. Incorporating one pieces system significant endeavor. respect, competing approaches directly compare system against. Given this,
following representative list systems common features ours. meant
comprehensive focuses pointing primary differences systems ours, primary differences actually render systems loosely related
ours.
Borchardt (1985) presents representation temporal, relational, force-dynamic event definitions definitions neither learned applied video. Regier (1992) presents techniques learning temporal event definitions learned definitions neither relational, force
dynamic, applied video. addition learning technique truly positive-onlyrather,
extracts implicit negative examples event type positive examples event types.
426

fiL EARNING EMPORAL E VENTS

Yamoto, Ohya, Ishii (1992), Brand Essa (1995), Siskind Morris (1996), Brand, Oliver,
Pentland (1997), Bobick Ivanov (1998) present techniques learning temporal event
definitions video learned definitions neither relational force dynamic. Pinhanez
Bobick (1995) Brand (1997a) present temporal, relational event definitions recognize
events video definitions neither learned force dynamic. Brand (1997b) Mann
Jepson (1998) present techniques analyzing force dynamics video neither formulate
event definitions apply techniques recognizing events learning event definitions.
7.2 Learning Temporal Patterns
divide body work three main categories: temporal data mining, inductive logic
programming, finite-statemachine induction.
Temporal Data Mining. sequence-mining literature contains many general-to-specific (levelwise) algorithms finding frequent sequences (Agrawal & Srikant, 1995; Mannila, Toivonen,
& Verkamo, 1995; Kam & Fu, 2000; Cohen, 2001; Hoppner, 2001). explore specific-togeneral approach. previous work, researchers studied problem mining temporal
patterns using languages interpreted placing constraints partially totally ordered
sets time points, e.g., sequential patterns (Agrawal & Srikant, 1995) episodes (Mannila et al.,
1995). languages place constraints time points rather time intervals work
here. recently work mining temporal patterns using interval-based pattern
languages (Kam & Fu, 2000; Cohen, 2001; Hoppner, 2001).
Though languages learning frameworks vary among approaches, share two
central features distinguish approach. First, typically goal
finding frequent patterns (formulas) within temporal data setour approach focused
finding patterns frequency one (covering positive examples). first learning
application visual-event recognition yet required us find patterns frequency less
one. However, number ways extend method direction
becomes necessary (e.g., deal noisy training data). Second, approaches
use standard general-to-specific level-wise search techniques, whereas chose take specificto-general approach. One direction future work develop general-to-specific level-wise
algorithm finding frequent formulas compare specific-to-general approach.
Another direction design level-wise version specific-to-general algorithmwhere
example, results obtained k -AMA LGG used efficiently calculate
(k + 1)-AMA LGG. Whereas level-wise approach conceptually straightforward general-tospecific framework clear specific-to-general case. familiar
temporal data-mining systems take specific-to-general approach.
First-Order Learning Section 3.3, pointed difficulties using existing first-order
clausal generalization techniques learning AMA formulas. spite difficulties, still
possible represent temporal events first-order logic (either without capturing AMA
semantics precisely) apply general-purpose relational learning techniques, e.g., inductive
logic programming (ILP) (Muggleton & De Raedt, 1994). ILP systems require positive
negative training examples hence suitable current positive-only framework.
Exceptions include G OLEM (Muggleton & Feng, 1992), P ROGOL (Muggleton, 1995), C LAU DIEN (De Raedt & Dehaspe, 1997), among others. performed full evaluation
427

fiF ERN , G IVAN , & ISKIND

Inputs

AMA

Subsumption
Semantic
Syntactic
P
P
coNP-complete P

Semantic AMA LGG
Lower Upper Size
P
coNP EXP
coNP NEXP 2-EXP?

Syntactic AMA LGG
Lower Upper Size
P
coNP EXP
P
coNP EXP

Table 3: Complexity Results Summary. LGG complexities relative input plus output size.
size column reports worst-case smallest correct output size. ? indicates
conjecture.
systems, early experiments visual-event recognition domain confirmed belief
horn clauses, lacking special handling time, give poor inductive bias. particular, many
learned clauses find patterns simply make sense temporal perspective and,
turn, generalize poorly. believe reasonable alternative approach may incorporate
syntactic biases ILP systems done, example, Cohen (1994), Dehaspe De Raedt
(1996), Klingspor, Morik, Rieger (1996). work, however, chose work directly
temporal logic representation.
Finite-State Machines Finally, note much theoretical empirical research
learning finite-state machines (FSMs) (Angluin, 1987; Lang, Pearlmutter, & Price, 1998).
view FSMs describing properties strings (symbol sequences). case, however,
interested describing sequences propositional models rather sequences symbols.
suggests learning type factored FSM arcs labeled sets propositions
rather single symbols. Factored FSMs may natural direction extend
expressiveness current language, example allowing repetition. aware
work concerned learning factored FSMs; however, likely inspiration drawn
symbol-based FSM-learning algorithms.

8. Conclusion
presented simple logic representing temporal events called AMA shown
theoretical empirical results learning AMA formulas. Empirically, weve given first
system learning temporal, relational, force-dynamic event definitions positive-only input
applied system learn definitions real video input. resulting
performance matches event definitions hand-coded substantial effort human
domain experts. theoretical side, Table 3 summarizes upper lower bounds
shown subsumption generalization problems associated logic.
case, provided provably correct algorithm matching upper bound shown.
table shows worst-case size smallest LGG could possibly take relative input
size, AMA inputs. key results table polynomial-time
subsumption AMA syntactic subsumption, coNP lower bound AMA subsumption,
exponential size LGGs worst case, apparently lower complexity syntactic AMA
LGG versus semantic LGG. described build learner based results applied
visual-event learning domain. date, however, definitions learn neither crossmodal perspicuous. performance learned definitions matches hand428

fiL EARNING EMPORAL E VENTS

coded ones, wish surpass hand coding. future, intend address cross-modality
applying learning technique planning domain. believe addressing perspicuity
lead improved performance.

Acknowledgments
authors wish thank anonymous reviewers helping improve paper. work
supported part NSF grants 9977981-IIS 0093100-IIS, NSF Graduate Fellowship
Fern, Center Education Research Information Assurance Security
Purdue University. Part work performed Siskind NEC Research Institute,
Inc.

Appendix A. Internal Positive Event Logic
give syntax semantics event logic called Internal Positive Event Logic
(IPEL). logic used main text motivate choice small subset
logic, AMA, showing, Proposition 4, AMA define set models IPEL
define.
event type (i.e., set models) said internal whenever contains model
= hM; i, contains model agrees truth assignments [i] 2 .
Full event logic allows definition non-internal events, example, formula = 3< P
satisfied hM; interval 0 entirely preceding P satisfied
hM; 0 i, thus internal. applications considering appear require
non-internal events, thus currently consider events internal.
Call event type positive contains model = hM; [1; 1]i (1) truth
assignment assigning propositions value true. positive event type cannot require proposition false point time.
IPEL fragment full propositional event logic describe positive internal
events. conjecture, yet proven, positive internal events representable
full event logic Siskind (2001) represented IPEL formula. Formally, syntax
IPEL formulas given

E ::= true j prop j E1 _ E2 j 3R E1 j E1 ^R E2 ;
0

Ei IPEL formulas, prop primitive proposition (sometimes called primitive event
type), R subset thirteen Allen interval relations fs,f,d,b,m,o,=,si,fi,di,bi,ai,oi g (Allen,
1983), R0 subset restricted set Allen relations fs,f,d,=g, semantics
Allen relation given Table 4. difference IPEL syntax full propositional
event logic event logic allows negation operator, that, full event logic, R0
subset thirteen Allen relations. operators ^ ; used define AMA formulas
merely abbreviations IPEL operators ^f=g ^fmg respectively, AMA subset
IPEL (though distinguished subset indicated Proposition 4).
thirteen Allen interval relations binary relations set closed naturalnumber intervals. Table 4 gives definitions relations, defining [m1 ; m2 ] r [n1 ; n2 ]
Allen relation r . Satisfiability IPEL formulas defined follows,
429

fiF ERN , G IVAN , & ISKIND

I1
[m1 ; m2 ]
[m1 ; m2 ]
[m1 ; m2 ]
[m1 ; m2 ]
[m1 ; m2 ]
[m1 ; m2 ]
[m1 ; m2 ]

Relation

f

b


=

I2
[n1 ; n2 ]
[n1 ; n2 ]
[n1 ; n2 ]
[n1 ; n2 ]
[n1 ; n2 ]
[n1 ; n2 ]
[n1 ; n2 ]

English
starts
finishes


meets
overlaps
equals

Definition
m1 = n1 m2
m1 n1 m2
m1 n1 m2

n2
= n2
n2

m2 n1
m2 = n1 m2 + 1 = n1
m1 n1 m2 n2
m1 = n1 m2 = n2

Inverse
si

di
bi
mi
oi
=

Table 4: Thirteen Allen Relations (adapted semantics).

true satisfied every model.
prop satisfied model hM; iff [x] assigns prop true every x 2 .
E1 _ E2 satisfied model iff satisfies E1 satisfies E2.
3RE satisfied model hM; iff r 2 R interval 0 0 r
hM; 0 satisfies E .
E1 ^R E2 satisfied model hM; iff r 2 R exist intervals I1 I2
I1 r I2 , PAN (I1 ; I2 ) = hM; I1 satisfies E1 hM; I2 satisfies E2 .
prop primitive proposition, E Ei IPEL formulas, R set Allen relations,
PAN (I1 ; I2 ) minimal interval contains I1 I2 . definition, easy
show, induction number operators connectives formula, IPEL formulas
define internal events. One verify definition satisfiability given earlier AMA
formulas corresponds one give here.

Appendix B. Omitted Proofs
Lemma 1. timeline model M, satisfies witnessing
interdigitation MAP(M) .
Proof: Assume = hM; satisfies timeline = s1 ; : : : ; sn , let 0 =
MAP(M). straightforward argue, induction length , exists mapping
V 0 states sub-intervals ,

2 V 0 (s), [i] satisfies s,
V 0(s1) includes initial time point ,
V 0(sn) includes final time point ,
2 [1; n 1], V 0(si ) meets V 0(si+1) (see Table 4).
430

fiL EARNING EMPORAL E VENTS

Let V relation states 2 members 2 true 2 V 0 (s). Note
conditions V 0 ensure every 2 every 2 appear tuple V (not
necessarily together). use V construct witnessing interdigitation W .
Let R total, one-to-one, onto function time-points corresponding states 0 ,
noting 0 one state time-point , 0 = MAP(hM; i). Note R preserves
ordering that, j , R(i) later R(j ) 0 . Let W composition V R
relations V R.
show W interdigitation. first show state 0 appears
tuple W , W piecewise total. States must appear, trivially, appears
tuple V , R total. States 0 appear 2 appears tuple V , R
onto states 0 .
suffices show states , W (s; s0 ) W (t; t0 ) implies
s0 later t0 0 , W simultaneously consistent. conditions defining V 0
imply every number 2 V (s) less equal every j 2 V (t). order-preservation
property R, noted above, implies every state s0 2 V R(s) later state
t0 2 V R(t) 0 , desired. W interdigitation.
argue W witnesses 0 . Consider 2 2 0 W (s; t).
construction W , must 2 V 0 (s) ith state 0 . Since 0 = MAP(M),
follows set true propositions [i]. Since 2 V 0 (s), know [i] satisfies
s. follows t, s. 2

2 IPEL, model embeds model satisfies E satisfies E .
Proof: Consider models = hM; M0 = hM 0 ; 0 embeds M0 , let
= MAP(M) 0 = MAP(M0 ). Assume E 2 IPEL satisfied M0 , show
E satisfied M.
know definition embedding 0 thus witnessing interdigitation W 0 Proposition 2. know one-to-one correspondence
numbers (I 0 ) states (0 ) denote state (0 ) corresponding 2 (i0 2 0 )
Lemma 3. E

si (ti ). correspondence allows us naturally interpret W mapping V subsets
0 subsets follows: I10 0 , V (I10 ) equals set 2 i0 2 I10 ,
si co-occurs ti W . use following properties V ,
0

0

1. I10 sub-interval 0 , V (I10 ) sub-interval .

2. I10 sub-interval 0 , hM; V (I10 )i embeds hM 0 ; I10 i.

3. I10 I20 sub-intervals 0 , r Allen relation, I10 rI20 iff V (I10 )rV (I20 ).
4. I10 I20 sub-intervals 0 , V (S PAN (I10 ; I20 )) = PAN (V (I10 ); V (I20 )).

5.

V (I 0 ) = .

sketch proofs properties. 1) Use induction length I10 ,
definition interdigitation. 2) Since V (I10 ) interval, MAP(hM; V (I10 )i) well defined.
MAP(hM; V (I10 )i) MAP(hM 0 ; I10 i) follows assumption embeds M0 . 3)
Appendix A, see Allen relations defined terms relation natural
431

fiF ERN , G IVAN , & ISKIND

number endpoints intervals. show V preserves (but <) singleton sets
(i.e., every member V (fi0 g) every member V (fj 0 g) i0 j 0 ) V commutes set union. follows V preserves Allen interval relations. 4) Use fact
V preserves sense argued, along fact PAN (I10 ; I20 ) depends
minimum maximum numbers I10 I20 . 5) Follows definition interdigitation
construction V .
use induction number operators connectives E prove that, M0
satisfies E , must M. base case E = prop, prop primitive proposition,
true. Since M0 satisfies E , know prop true 0 [x0 ] x0 2 0 . Since W witnesses
0 , know that, prop true 0 [x], prop true [x], x 2 V (x0 ).
Therefore, since V (I 0 ) = , prop true 0 [x], x 2 , hence M0 satisfies E .
inductive case, assume claim holds IPEL formulas fewer N operators connectiveslet E1 ; E2 two formulas. E = E1 _ E2 , claim trivially
holds. E = 3R E1 , R must subset set relations fs,f,d,=g. Notice E
written disjunction 3r E1 formulas, r single Allen relation R. Thus,
suffices handle case R single Allen relation. Suppose E = 3fsg E1 . Since M0
satisfies E , must sub-interval I10 0 I10 0 hM 0 ; I10 satisfies E1 . Let
I1 = V (I10 ), know properties V V (I 0 ) = , and, hence, I1 . Furthermore, know hM; I1 embeds hM 0 ; I10 i, and, thus, inductive hypothesis, hM; I1
satisfies E1 . Combining facts, get E satisfied M. Similar arguments hold
remaining three Allen relations. Finally, consider case E = E1 ^R E2 , R
set Allen relations. Again, suffices handle case R single Allen relation
r. Since M0 satisfies E = E1 ^r E2 , know sub-intervals I10 I20 0
PAN (I10 ; I20 ) = 0 , I10 r I20 , hM 0 ; I10 satisfies E1 , hM 0 ; I20 satisfies E2 . facts,
properties V , easy verify satisfies E . 2
Lemma 5. Given formula subsumes member set formulas,
subsumes member 0 IG(). Dually, subsumed member ,
subsumed member 0 IS(). case, length 0
bounded size .
Proof: prove result IG(). proof IS() follows similar lines. Let

=

f1 ; : : : ; ng, = s1; : : : ; sm, assume 1 n, . Proposition 2, i, witnessing interdigitation Wi . combine Wi

interdigitation , show corresponding member IG() subsumed
. construct interdigitation , first notice that, sj , Wi specifies set
states (possibly single state least one) co-occur sj . Furthermore, since
Wi interdigitation, easy show set states corresponds consecutive subsequence states let j;i timeline corresponding subsequence.
let j = fj;i j 1 ng, ffj interdigitation j . take union
ffj , 1 j m. show interdigitation . Since state appearing
must co-occur least one state sj least one Wi , least one tuple ffj ,
and, hence, tuple piecewise total.
Now, define restriction i;j components j , < j , relation given
taking set pairs formed shortening tuples omitting components except
432

fiL EARNING EMPORAL E VENTS

ith j th. Likewise define ffi;j
k k . show interdigitation, suffices
show i;j simultaneously consistent. Consider states si sj timelines
j , respectively, i;j (si ; sj ). Suppose ti occurs si i, tj 2 j ,
i;j (ti ; tj ) holds. suffices show sj later tj j . Since i;j (si ; sj ) i;j (ti ; tj ),
i;j
0
0
must ffi;j
k (si ; sj ) ffk (ti ; tj ), respectively, k k . know k k
0
si ti Wi simultaneously consistent. k = k , sj later tj j ,
ffk must simultaneously consistent, interdigitation. Otherwise, k < k 0 . sj
later tj j , desired, Wj simultaneously consistent. simultaneously
consistent, interdigitation .
Let 0 member IG() corresponding . show 0 . know
state s0 2 0 intersection states tuple ffj say s0 derives
ffj . Consider interdigitation 0 0 , 0 (sj ; s0 ), sj 2 s0 2 0 ,
s0 derives ffj . 0 piecewise total, every tuple 0 derives ffj , ffj
empty. 0 simultaneously consistent tuples 0 deriving later ffk must later
lexicographic ordering , given simultaneous consistency Wk interdigitations used
construct ffj . Finally, know sj subsumes (i.e., subset of) state tuple
ffj , Wk witnessing interdigitation k , and, hence, subsumes (is subset
of) intersection states. Therefore, sj 2 co-occurs s0 2 0 0
s0 sj . Thus, 0 witnessing interdigitation 0 , Proposition 2 0 .
size bound 0 follows, since, pointed main text, size member
IG() upper-bounded number states . 2
0

Lemma 8. Given timelines 1 = s1 ; : : : ; sm 2 = t1 ; : : : ; tn , witnessing
interdigitation 1 2 iff path subsumption graph SG(1 ; 2 ) v1;1
vm;n .
Proof: Subsumption
graph SG(1 ; 2 ) equal hV; E V = fvi;j j 1 m; 1 j ng

E = hvi;j ; vi ;j j si tj ; si tj ; i0 + 1; j j 0 j + 1 . Note
correspondence vertices state tupleswith vertex vi;j corresponding hsi ; tj i.
forward direction, assume W witnessing interdigitation 1 2 .
know that, states si tj co-occur W , si tj since W witnesses 1 2 .
vertices corresponding tuples W called co-occurrence vertices, satisfy
first condition belonging edge E (that si tj ). follows definition
interdigitation v1;1 vm;n co-occurrence vertices. Consider co-occurrence
vertex vi;j equal vm;n , lexicographically least co-occurrence vertex vi ;j vi;j
(ordering vertices
ordering
pair subscripts). show i, j , i0 , j 0 satisfy

requirements vi;j ; vi ;j 2 E . not, either i0 > + 1 j 0 > j + 1. i0 > + 1,
co-occurrence vertex vi+1;j , contradicting W piecewise total. j 0 > j + 1,
since W piecewise total, must co-occurrence vertex vi ;j +1 : i00 <
i00 > i0 , contradicts simultaneous consistency W , i00 = i, contradicts
lexicographically least choice vi ;j . follows every co-occurrence vertex vm;n
edge another co-occurrence vertex closer Manhattan distance vm;n , thus
path v1;1 vm;n .
reverse direction assume path vertices SG(1 ; 2 ) v1;1 vm;n
given by, vi1 ;j1 ; vi2 ;j2 ; : : : ; vir ;js i1 = j1 = 1, ir = m; js = n. Let W set state
0

0

0

0

0

0

0

00

00

0

0

433

0

fiF ERN , G IVAN , & ISKIND

tuples corresponding vertices along path. W must simultaneously consistent
orderings directed edges non-decreasing orderings. W must
piecewise total edge cross one state transition either 1 2 ,
edge set definition. W interdigitation. Finally, definition edge set E ensures
tuple hsi ; tj W property si tj , W witnessing interdigitation
1 2 , showing 1 2 , desired. 2
Lemma 10. Given n, let conjunction timelines
n
[
i=1

f(PROPn; Truei; Falsei; PROPn); (PROPn; Falsei; Truei; PROPn)g:

following facts truth assignments Boolean variables p1 ; : : : ; pn :
1. truth assignment A, PROPn ; sA ; PROPn semantically equivalent member
IS( ).
2. 2 IS( ) truth assignment PROPn ; sA ; PROPn .
Proof: prove first part lemma, construct interdigitation
corresponding member IS( ) equivalent PROPn ; sA ; PROPn . Intuitively, construct
ensuring tuple consists states form Truek Falsek agree
truth assignmentthe union states tuple, taken IS( ) equal sA . Let
= fT0 ; T1 ; T2 ; T3 ; T4 g interdigitation exactly five state tuples Ti . assign
states timeline tuples follows:
1. k , 1 k




n A(pk ) true,

timeline s1 ; s2 ; s3 ; s4 = Q; ruek ; F alsek ; Q, assign state si tuple Ti ,
assign state s1 T0 well,
timeline s01 ; s02 ; s03 ; s04 = Q; F alsek ; ruek ; Q, assign state s0i tuple Ti 1 ,
state s04 tuple T4 well.

2. k , 1 k n A(pk ) false, assign states tuples item 1
interchanging roles ruek F alsek .

clear piecewise total simultaneously consistent state orderings
, interdigitation. union states T0 , T1 , T3 , T4 equal
PROPn , since PROPn included state tuples. Furthermore, see
union states T2 equal sA . Thus, member IS( ) corresponding equal
PROPn ; PROPn ; sA ; PROPn ; PROPn , semantically equivalent PROPn ; sA ; PROPn ,
desired.
prove second part lemma, let member IS( ). first argue
every state must contain either Truek Falsek 1 k n. k , since contains PROPn ; Truek ; Falsek ; PROPn , every member IS( ) must subsumed PROPn ; Truek ;
Falsek ; PROPn . So, subsumed PROPn ; Truek ; Falsek ; PROPn . every state PROPn ;
Truek ; Falsek ; PROPn contains either Truek Falsek , implying , desired.
434

fiL EARNING EMPORAL E VENTS

Next, claim 1 k n, either Truek Falsek i.e., either states
include Truek , states include Falsek (and possibly both). prove claim, assume,
sake contradiction, that, k , 6 Truek 6 Falsek . Combining assumption first claim, see must states s0 contains ruek
F alsek , s0 contains F alsek ruek , respectively. Consider interdigitation
corresponds member IS( ). know s0 equal union
states tuples 0 , respectively, . 0 must include one state timeline
s1 ; s2 ; s3 ; s4 = PROPn ; Truek ; Falsek ; PROPn s01 ; s02 ; s03 ; s04 = PROPn ; Falsek ; Truek ; PROPn .
Clearly, since include Falsek , includes states s1 s02 , likewise 0 includes
states s2 s01 . follows simultaneously consistent state orderings
s1 ; s2 ; s3 ; s4 s01 ; s02 ; s03 ; s04 , contradicting choice interdigitation. shows
either Truek Falsek .
Define truth assignment 1 k n, A(pk ) Truek .
Since,for k , Truek Falsek , follows state subsumed
sA . Furthermore, since begins ends PROPn , easy give interdigitation
PROPn ; sA ; PROPn witnesses PROPn ; sA ; PROPn . Thus,
PROPn ; sA ; PROPn . 2
Lemma 16. Let 1 2 given page 402, proof Theorem 17, let =
V
IG(f1 ; 2 g). 0 whose timelines subset omits square
timeline, < 0 .
Proof: Since timelines 0 subset timelines , know 0 . remains
show 0 6 . show constructing timeline covered 0 , .
Let = s1 ; s2 ; : : : ; s2n 1 square timeline included 0 . Recall
si single proposition proposition set P = fpi;j j 1 n; 1 j ng, that,
consecutive states si si+1 , si = pi;j , si+1 either pi+1;j pi;j +1 . Define new
timeline = s2 ; s3 ; : : : ; s2n 2 si = (P si ). show 6 (so 6 ),
that, 0 fg, 0 (so 0 ).
sake contradiction, assume must interdigitation W
witnessing . show induction that, 2, W (si ; sj ) implies j > i.
base case, = 2, know s2 6 s2 , since s2 6 s2 , W (s2 ; s2 ) false, since
W witnesses subsumption. inductive case, assume claim holds i0 < i,
W (si ; sj ). know si 6 si , thus 6= j . W piecewise total, must
W (si 1 ; sj ) j 0 , and, induction hypothesis, must j 0 > 1. Since W
simultaneously consistent sk sk state orderings, 1 < i, j 0 j .
follows j > desired. Given claim, see s2n 2 cannot co-occur W
state , contradicting fact W piecewise total. Thus 6 .
Let 0 = s01 ; : : : ; s0m timeline fg, construct interdigitation
witnesses 0 . Note assumed square, 0 need be. Let j smallest
index sj 6= s0j since s1 = s01 = p1;1 , 6= 0 , know j must exist,
range 2 j m. use index j guide construction interdigitation. Let W
interdigitation 0 , exactly following co-occurring states (i.e., state tuples):
0

0

1. 1 j

1, si+1 co-occurs s0i .
435

fiF ERN , G IVAN , & ISKIND

m, sj co-occurs s0i.
j + 1 2n 2, si co-occurs s0m .

2. j
3.

easy check W piecewise total simultaneously consistent state
orderings , interdigitation. show W witnesses 0
showing states subsumed states co-occur W . co-occurring
states si+1 s0i corresponding first item s0i = si implies s0i
contained si+1 , giving si+1 s0i . consider co-occurring states sj s0i
second item above. Since square, choose k l sj 1 = pk;l , sj either
pk+1;l pk;l+1. addition, since sj 1 = s0j 1 s0j either pk+1;l ; pk;l+1 pk+1;l+1
sj 6= s0j . cases, find state 0 s0j equal sj follows
noting proposition indices never decrease across timeline 0 16 . therefore
that, j , sj s0i . Finally, co-occurring states si s0m item three above,
si s0m , since s0m = pn;n, states . Thus, shown co-occurring
states W , state subsumed co-occurring state 0 . Therefore, W witnesses
0 , implies 0 . 2
Lemma 26. model hM; 2 2 AMA ,
[hM; i].

covers hM;

iff

F [ ] covers

Proof: Recall set models propositions set P = fp1 ; : : : ; pn g
assume AMA uses primitive propositions P (possibly negated).
set propositions P = fp1 ; : : : ; pn g, assume formulas AMA use propositions
P [ P set models P [ P , i, exactly one pi pi
true time. Note F [ ] AMA [hM; i] M. prove lemma via
straightforward induction structure proving result literals, states,
timelines, finally AMA formulas.
prove result literals, consider two cases (the third case true trivial). First,
single proposition pi , 0 = F [pi ] = pi . Consider model hM; 2 let
hM 0 ; = [hM; i]. following relationships yield desired result.

covers hM;

iff
iff
iff

2 , [i] assigns pi true
2 , 0 [i] assigns pi true
0 = pi covers [hM; i]

(by definition satisfiability)
(by definition )
(by definition satisfiability)

second case negated proposition :3pi here, get 0 = pi . Let
hM; 2 hM 0 ; = [hM; i]. following relationships yield desired result.

covers hM;

iff
iff
iff

2 , [i] assigns pi false
2 , 0 [i] assigns pi true
0 = pi covers [hM; i]

(by definition satisfiability)
(by definition )
(by definition satisfiability)

proves lemma literals.
16. Note
pk+1;l+1 .

required square possible +1 equal
0

sj

436

sj

i.e., could equal

fiL EARNING EMPORAL E VENTS

prove result states, use induction number k literals state. base
case k = 1 (the state single literal) proven above. assume lemma
holds states k fewer literals let = l1 ^ ^ lk+1 hM; 2 M.
inductive assumption know = l1 ^ ^ lk covers hM; iff F [] covers [hM; i].
base case know lk+1 covers hM; iff F [lk+1 ] covers [hM; i]. facts
definition satisfiability states, get covers hM; iff F [] ^ F [lk+1 ] covers
[hM; i]. Clearly F property F [] ^ F [lk+1 ] = F [ ], showing lemma holds
states.
prove result timelines, use induction number k states timeline.
base case k = 1 (the timeline single state) proven above. assume
lemma holds timelines k fewer states. Let = s1 ; : : : ; sk+1 hM; [t; t0 ]i 2
hM 0 ; [t; t0 ]i = [hM; [t; t0 ]i]. following relationships.

covers hM; [t; t0 ]i

iff
iff
iff
iff

exists t00 2 [t; t0 ], s1 covers hM; [t; t00 ]i
= s2 ; : : : ; sk+1 covers either hM; [t00 ; t0 ]i hM; [t00 + 1; t0 ]i
exists t00 2 [t; t0 ], F [s1 ] covers hM 0 ; [t; t00 ]i
F [] covers either hM 0 ; [t00 ; t0 ]i hM 0 ; [t00 + 1; t0 ]i
F [s1 ]; F [] covers hM 0 ; [t; t0 ]i
F [ ] covers hM 0 ; [t; t0 ]i

first iff follows definition satisfiability; second follows inductive
hypothesis, base case, fact [t; t0 ] [hM; i] = hM 0 ; i; third
follows definition satisfiability; fourth follows fact F [s1 ]; F [] =
F [ ].
Finally, prove result AMA formulas, induction number k timelines
formula. base case k = 1 (the formula single timeline) proven
above. assume lemma holds AMA formulas k fewer timelines
let = 1 ^ ^ k+1 hM; 2 M. inductive assumption, know
0 = 1 ^ ^ k covers hM; iff F [ 0 ] covers [hM; i]. base case,
know k+1 covers hM; iff F [k+1 ] covers [hM; i]. facts definition
satisfiability, get covers hM; iff F [ 0 ] ^ F [k+1 ] covers [hM; i]. Clearly F
property F [ 0 ] ^ F [k+1 ] = F [ ], showing lemma holds AMA formulas.
completes proof. 2

Appendix C. Hand-coded Learned Definitions Used Experiments
give two sets hand-coded definitions, HD1 HD2 , used experimental
evaluation. give set learned AMA event definitions seven event types.
learned definitions correspond output k -AMA learning algorithm, given available
training examples (30 examples per event type), k = 3 = BN. event definitions
written event logic, :3p denotes negation proposition p.

437

fiF ERN , G IVAN , & ISKIND

1

0

4

P ICK U P (x; y; z )

=

P UT OWN(x; y; z )

=

TACK (w; x; y; z )

=

U NSTACK (w; x; y; z )

=

OVE(w; x; y; z )
SSEMBLE(w; x; y; z )
ISASSEMBLE(w; x; y; z )

4

4

4

4
4
=
4
=
=

:3x = ^ :3z = x ^ :3z = y^
C
B UPPORTED(y ) ^ :3ATTACHED(x; z )^
B 8 2
3 9 C
C
B >
:3ATTACHED(x; y) ^ :3S UPPORTS(x; y)^
>
>
C
B >
>
7
>
C
B >
>
6
UPPORTS (z; )^
>
7
>
C
B >
>
6
>
7
>
C
B >
>
6
:
3
UPPORTED(x) ^ :3ATTACHED(y; z )^ 7 ; >
>
C
B >
>
6
>
>
C
B >
>
5
4
:3S UPPORTS(y; x) ^ :3S UPPORTS(y; z )^
>
>
C
B >
>
>
>
B >
>
:3S UPPORTS(x; z ) ^ :3S UPPORTS(z; x)
= C
C
B <
C
B
B > [2ATTACHED(x; ) _ ATTACHED(y; z )] ;
3 > C
>
C
B >
ATTACHED(x; ) ^ UPPORTS(x; )^
>
>
C
B >
>
>
6
7
>
C
B >
>
:3S UPPORTS(z; y)^
>
6
7
>
C
B >
>
>
6
7
>
C
B >
>
:
3

UPPORTED
(
x
)
^
:
3

TTACHED
(
y;
z
)
^
>
6
7
>
C
B >
>
>
>
>
4
5

@ >
:
3

UPPORTS
(
y;
x
)
^
:
3

UPPORTS
(
y;
z
)
^
>
>
>
>
;
:
:3S UPPORTS(x; z ) ^ :3S UPPORTS(z; x)
1
0
:3x = ^ :3z = x ^ :3z = y^
C
B UPPORTED(y ) ^ :3ATTACHED(x; z )^
B 8 2
3 9 C
C
B >

TTACHED
(
x; ) ^ UPPORTS (x; )^
>
>
C
B >
>
>
C
7
B >
>
6
:
3

UPPORTS
(
z;

)
^
>
>
C
7
B >
>
6
>
>
C
B >
>
6
7
:
3

UPPORTED
(
x
)
^
:
3

TTACHED
(
y;
z
)
^
;
>
>
C
B >
>
6
7
>
>
C
B >
>
4
5
:
3

UPPORTS
(
y;
x
)
^
:
3

UPPORTS
(
y;
z
)
^
>
>
C
B >
>
>
>
B >
>
:
3

UPPORTS
(
x;
z
)
^
:
3

UPPORTS
(
z;
x
)
= C
C
B <
C
B
B > [2ATTACHED(x; ) _ ATTACHED(y; z )] ;
3 > C
>
C
B >

TTACHED
(
x; ) ^ :3 UPPORTS(x; )^
:
3
>
>
> C
B >
7 >
>
C
B >
>
6 UPPORTS (z; )^
>
>
7 > C
B >
6
>
C
B >
>
6 :3S UPPORTED(x) ^ :3ATTACHED(y; z )^ 7 >
>
>
7 > C
B >
6
>
>

@ >
4 :3S UPPORTS(y; x) ^ :3S UPPORTS(y; z )^ 5 >
>
>
>
>
;
:
:3S UPPORTS(x; z ) ^ :3S UPPORTS(z; x)
3
2
:3z = w ^ :3z = x ^ :3z = y^
4 P UT OWN(w; x; ) ^ UPPORTS(z; )^ 5
:ATTACHED(z; y)


:3z = w ^ :3z = x ^ :3z = y^
P ICK U P (w; x; ) ^ UPPORTS(z; ) ^ :ATTACHED(z; )
:3y = z ^ [P ICK U P(w; x; y); P UT OWN(w; x; z )]
P UT OWN(w; y; z ) ^f g TACK (w; x; y; z )
U NSTACK(w; x; y; z ) ^f g P ICK U P (x; y; z )
<

<

Figure 12: HD1 event-logic definitions seven event types.

438

fiL EARNING EMPORAL E VENTS

0

1
:3x = ^ :3z = x ^ :3z = y^
B
C
(y) ^ :3ATTACHED (x; z )^
B UPPORTED
C
3
9 C
B 8 2
B >
C

TTACHED (x; ) ^ :3S UPPORTS (x; )^
:
3
>
>
B >
C
> 6
>
7
>
B >
C
> 6 UPPORTS (z; ) ^ C ONTACTS (z; )^
>
7
>
B >
C
>
>
7
6
>
B >
C
> 6 :3S UPPORTED (x) ^ :3ATTACHED (y; z )^ 7 ^f<;mg >
>
B >
C
>
>
7
6
>
B >
C
>
> 4 :3S UPPORTS (y; x) ^ :3S UPPORTS (y; z )^ 5
>
4
B >
C
>
>
>
P ICK U P (x; y; z ) = B >
= C
<
B
C

UPPORTS (x; z ) ^ :3S UPPORTS (z; x)
:
3
3
B
2
C
B >
C
>

TTACHED
(
x;

)
^

UPPORTS
(
x;

)
^
>
B >
C
>
> 6
>
7
B >
C
>
>
:
3

UPPORTS
(
z;

)
^
>
6
7
B >
C
>
> 6
>
7
B >
C
>
>
>
:
3

UPPORTED
(
x
)
^
:
3

TTACHED
(
y;
z
)
^
6
7
B >
C
> 6
>
>
7
B >
C
>
>
>
4
5
@ >

:
3

UPPORTS
(
y;
x
)
^
:
3

UPPORTS
(
y;
z
)
^
>
>
>
>
;
:
:3S UPPORTS(x; z) ^ :3S UPPORTS(z; x)
0
1
:3x = ^ :3z = x ^ :3z = y^
C
B
(y) ^ :3ATTACHED (x; z )^
C
B UPPORTED
3
9 C
B 8 2
C
B >

TTACHED (x; ) ^ UPPORTS (x; )^
>
>
C
B >
>
> 6
7
>
C
B >
>
>

UPPORTS
(
z;

)
^
:
3
7
6
>
C
B >
>
> 6
7
>
C
B >
>
>
:
3

UPPORTED
(
x
)
^
:
3

TTACHED
(
y;
z
)
^
^
7
6
>
f
<;

g
C
B >
>
>
7
> C
> 6
B >
>
5
4
>
:
3

UPPORTS
(
y;
x
)
^
:
3

UPPORTS
(
y;
z
)
^
4 B>
C
>
>
>
P UT (x; y; z ) = B >
= C
<
C
B
:
3

UPPORTS (x; z ) ^ :3S UPPORTS (z; x)
3
C
B
2
C
B >
>
:
3ATTACHED (x; y) ^ :3S UPPORTS(x; y)^
>
C
B >
>
>
>
6
7
C
B >
>
> 6 UPPORTS (z; ) ^ C ONTACTS (z; )^
>
7
C
B >
>
>
>
6
7
B >
C
>
> 6 :3S UPPORTED (x) ^ :3ATTACHED (y; z )^ 7
>
B >
C
>
>
>
6
7
B >
> C
>
>
4 :3S UPPORTS (y; x) ^ :3S UPPORTS (y; z )^ 5

@ >
>
>
>
>
;
:
:3S UPPORTS(x; z) ^ :3S UPPORTS(z; x)

Figure 13: Part HD2 event-logic definitions.

439

fiF ERN , G IVAN , & ISKIND

0

1

:3w = x ^ :3y = w ^ :3y = x^
B :3z = w ^ :3z = x ^ :3z = ^
C
B
C
B UPPORTED (x) ^ :3ATTACHED(w; )^
C
B 8 2
9 C
3
B >
C
ATTACHED(w; x) ^ UPPORTS (w; x)^
>
B >
>
C
> 6
>
B >
7
>
C
:
3
UPPORTS(y; x)^
>
B >
>
7
>
6
C
> 6
>
B >
7
>
C
UPPORTS(z; ) ^ C ONTACTS(z; )^
>
B >
7
>
6
>
C
> 6
>
B >
7
C
>
:
3
ATTACHED(z; )^
^
f
mg >
B >
7
6
C
>
>
> 6
>
7
B >
C
>
:
3
UPPORTED(w) ^ :3ATTACHED(x; )^ 7
>
B >
6
C
>
>
> 4
>
B >
C
>
5
:3S UPPORTS(x; w) ^ :3S UPPORTS(x; y)^
>
B >
>
>
= C
B <
C
B
C
2 :3S UPPORTS(w; ) ^ :3S UPPORTS(y; w)
3
B >
C
:3ATTACHED(w; x) ^ :3S UPPORTS(w; x)^
>
B >
C
>
>
B >
C
6
7
>
>
UPPORTS(y; x) ^ C ONTACTS (y; x)^
>
B >
C
6
7
>
>
>
B >
C
6
7
>
>
>
B >
C
6 UPPORTS(z; ) ^ C ONTACTS(z; )^
7
>
>
>
>
B >
C
6
7
>
>
B >
C
7
> 6 :3ATTACHED(z; )^
>
>
>
B >
C
6
7
>
>
B >
C
> 6 :3S UPPORTED(w) ^ :3ATTACHED(x; )^ 7
>
>
>
>
@ >

4
5
:3S UPPORTS(x; w) ^ :3S UPPORTS(x; y)^
>
>
>
>
:
;
:3S UPPORTS(w; y) ^ :3S UPPORTS(y; w)
1
0
:3w = x ^ :3y = w ^ :3y = x^
C
B :3z = w ^ :3z = x ^ :3z = ^
C
B
C
B UPPORTED(x) ^ :3ATTACHED(w; )^
9 C
B 8 2
3
C
B >
:3ATTACHED(w; x) ^ :3S UPPORTS(w; x)^
>
C
>
B >
>
C
>
6
B >
7
>
>
C
>
6 UPPORTS (y; x) ^ C ONTACTS (y; x)^
B >
7
>
>
>
C
>
6
B >
7
>
C
>
B >
7
> 6 UPPORTS (z; ) ^ C ONTACTS (z; )^
>
>
C
>
6
B >
7
C
>
6 :3ATTACHED(z; )^
B >
7 ^f mg >
>
>
>
C
>
6
B >
7
>
C
>
B >
> 6 :3S UPPORTED(w) ^ :3ATTACHED(x; )^ 7
>
>
C
>
B >
4
5
:3S UPPORTS(x; w) ^ :3S UPPORTS(x; y)^
>
C
>
B >
>
=
<
C
B
C
B
2 :3S UPPORTS(w; ) ^ :3S UPPORTS(y; w) 3
C
B >
ATTACHED(w; x) ^ UPPORTS(w; x)^
>
C
>
B >
>
>
6
C
7
>
B >
>
C
7
>
B >
> 6 :3S UPPORTS(y; x)^
>
>
6
C
7
>
B >
>
6 UPPORTS (z; ) ^ C ONTACTS (z; )^
C
7
>
B >
>
>
>
6
C
7
>
B >
>
C
7
>
B >
> 6 :3ATTACHED(z; )^
>
>
6
C
7
>
B >
>
C
>
B >
> 6 :3S UPPORTED(w) ^ :3ATTACHED(x; )^ 7
>
>
>
4

5
@ >
:3S UPPORTS(x; w) ^ :3S UPPORTS(x; y)^
>
>
>
>
;
:
:3S UPPORTS(w; y) ^ :3S UPPORTS(y; w)
:3y = z ^ [P ICK U P(w; x; y); P UT OWN(w; x; z )]
P UT OWN(w; y; z ) ^f g TACK (w; x; y; z )
U NSTACK (w; x; y; z ) ^f g P ICK U P (x; y; z )
<;

TACK(w; x; y; z )

4

=

<;

U NSTACK(w; x; y; z )

OVE(w; x; y; z )
SSEMBLE(w; x; y; z )
ISASSEMBLE(w; x; y; z )

4

=

4
4
=
4
=
=

<

<

Figure 14: Part II HD2 event-logic definitions.

440

fiL EARNING EMPORAL E VENTS

3 9
0 8 2
UPPORTED (y ) ^ UPPORTS (z; )^
>
>
>
>
>
B >
> 4 C ONTACTS (y; z ) ^ : UPPORTS(x; )^
5; >
>
>
B >
>
>
>
B >
>
: ATTACHED(x; y) ^ : ATTACHED(y; z )
=
B <
B
UPPORTED(y );
B > 2
3 >^
>
B >
UPPORTED (y ) ^ UPPORTS (x; )^
>
>
>
B >
>
>
B >
>
5
4
ATTACHED(x; ) ^ : UPPORTS(z; )^
>
>
>
B >
;
:
B
: C ONTACTS(y; z ) ^ : ATTACHED(y; z9)
B 8
B > UPPORTED(y );
>
B >

>
=
B <
UPPORTED(y ) ^ ATTACHED(x; )^
B
;
^
B >
ATTACHED(y; z )
>
B >
>
:
;
B
B 8 [S UPPORTED(y ) ^ ATTACHED(x; )] 9
B < [S UPPORTED(y ) ^ C ONTACTS (y; z )] ; =
B
B
B : [S UPPORTED(y ) ^ ATTACHED(y; z )] ; ; ^
B
B 8 [2S UPPORTED(y ) ^ ATTACHED(x; )]
3 9
B >
UPPORTED (y ) ^ UPPORTS (z; )^
>
>
B >
>
> 4 C ONTACTS (y; z ) ^ : UPPORTS(x; )^
B >
5; >
=
B <
B
:

TTACHED
(
x; ) ^ : ATTACHED(y; z )
^
B >
>
>
B >
[

UPPORTED
(

)
^

UPPORTS
(
z;

)]
;
>
>
>
B >
;
B : [S UPPORTED(y ) ^ ATTACHED(x; )]
9
B 8
B > [S UPPORTED(y ) ^ UPPORTS (z; )] ;
>
>
B >
>
>
> [S UPPORTED(y ) ^ ATTACHED(x; )] ;
B >
B < 2
3 =
B

UPPORTED
(
) ^ UPPORTS (x; )^
B >
>
5 >
@ >
>
> 4 ATTACHED(x; ) ^ : UPPORTS(z; )^
>
>
:
;

3

3

3

3

P ICK U P (x; y; z )

4

=

3

3

3

3

3

3

:3C ONTACTS(y; z ) ^ :3ATTACHED(y; z )

P UT OWN(x; y; z )

4

=

1
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C


3 9
0 8 2
UPPORTED(y ) ^ UPPORTS(x; ) ^ ATTACHED(x; )^
>
>
>
>
> 4 : UPPORTS(z; ) ^ : C ONTACTS(y; z )^
>
B >
5; >
>
B >
>
>
=
B <
: ATTACHED(y; z )
B
^
B > UPPORTED (y );
>
>
B >


>
>
>
B >
UPPORTED(y ) ^ UPPORTS (z; ) ^ C ONTACTS(z; )^
>
B >
>
>
;
B :
:

UPPORTS
(
x; ) ^ :

TTACHED
(
x; )
B 8
9

B <
B
UPPORTED (y ) ^ ATTACHED(x; ) ;
=
@
UPPORTED (y ) ^ ATTACHED(x; ) ^ ATTACHED(y; z ) ;
:
;

3
3

3

3

3

UPPORTED (y )

Figure 15: learned 3-AMA definitions P ICK U P (x; y; z ) P UT (x; y; z ).

441

1
C
C
C
C
C
C
C
C
C
C
C
C


fiF ERN , G IVAN , & ISKIND

0 8
>
>
B <
B
B >
B >
B :
B (
B
B
B
B
B (
B
B
B
B
B (
B
B
B
B
B (
B
B
B
B
B (
B
B
B
B
B 8
B <
B
B
B :
B
B (
B
B
B
B
B (
B
B
B
B
B 8
B
B <
B
B
B :
B 8
B
B <
B
B
B :
B
B (
B
B
B
B
B 8
B <
B
B
B :
B
B 8
B >
B >
B <
B
@ >
>
:

h

^

^

^

^

UPPORTED(y ) ATTACHED(w; x) UPPORTS(z; ) C ONTACTS(y; z )
UPPORTS(x; )
UPPORTS(y; x)
C ONTACTS(x; )
ATTACHED(x; )

:3

^ :3

^ :3

^ :3



;

9
>
>
=

1

C
C
>^ C
UPPORTED(y ) ^ UPPORTED(x) ^ UPPORTS(y; x) ^ C ONTACTS(x; ) ^ C ONTACTS(y; z )^
>
; C
C
:3S UPPORTS(x; y) ^ :3ATTACHED(w; x) ^ :3ATTACHED(x; y) ^ :3)ATTACHED(y; z)
C
C
[S UPPORTED(y) ^ ATTACHED(w; x)] ;
C
[S UPPORTED(y) ^ ATTACHED(x; y)] ;
^
C
C
[S UPPORTED(y) ^ UPPORTED(x) ^ UPPORTS(y; x) ^ C ONTACTS(x; y)]
)
C
C
[S UPPORTED(y) ^ ATTACHED(w; x)] ;
[S UPPORTED(y) ^ UPPORTS(x; y) ^ ATTACHED(w; x) ^ ATTACHED(x; y) ^ ATTACHED(y; z)] ; ^ C
C
C
[S UPPORTED(y) ^ UPPORTED(x)S UPPORTS(y; x)]
)
C
C
[S UPPORTED(y) ^ ATTACHED(w; x)] ;
C
[S UPPORTED(y) ^ UPPORTED(x) ^ UPPORTS(x; y) ^ UPPORTS(y; x) ^ ATTACHED(w; x)] ; ^
C
C
[S UPPORTED(y) ^ UPPORTED(x) ^ UPPORTS(y; x)]
)
C
C
[S UPPORTED(y) ^ ATTACHED(w; x) ^ UPPORTS(z; y) ^ C ONTACTS(y; z)] ;
C
[S UPPORTED(y) ^ ATTACHED(y; z)] ;
^
C
C
[S UPPORTED(y) ^ UPPORTED(x) ^ UPPORTS(y; x) ^ C ONTACTS(y; z)] )
C
C
[S UPPORTED(y) ^ ATTACHED(w; x) ^ UPPORTS(z; y) ^ C ONTACTS(y; z)] ;
C
[S UPPORTED(y) ^ ATTACHED(w; x) ^ ATTACHED(y; z)] ;
^
C
C
[hS UPPORTED(y) ^ UPPORTED(x) ^ UPPORTS(y; x)]
9
C
UPPORTED(y ) ^ ATTACHED(w; x) ^ UPPORTS(z; ) ^ C ONTACTS(y; z )^
C
=
;
C
:3S UPPORTS(x; y) ^ :3S UPPORTS(y; x) ^ :3C ONTACTS(x; y) ^ :3ATTACHED(x; y)
C
^
C
[S UPPORTED(y) ^ ATTACHED(w; x)] ;
;
C
[S UPPORTED(y) ^ UPPORTED(x) ^ UPPORTS(y; x)]
C
)
C
[S UPPORTED(y) ^ ATTACHED(w; x)] ;
C
[S UPPORTED(y) ^ ATTACHED(w; x) ^ UPPORTS(z; y) ^ C ONTACTS(y; z)] ; ^
C
C
[S UPPORTED(y) ^ UPPORTED(x)]
C
)
C
[S UPPORTED(y) ^ ATTACHED(w; x)] ;
C
[S UPPORTED(y) ^ ATTACHED(w; x) ^ UPPORTS(z; y) ^ UPPORTED(x)] ; ^
C
C
[S UPPORTED(y) ^ UPPORTED(x)]
C
9
C
[hS UPPORTED(y) ^ ATTACHED(w; x)] ;
=
C
UPPORTED(y ) ^ C ONTACTS(y; z ) ^ UPPORTS(z; ) ^ UPPORTED(x)^
C
;
^
C
:3S UPPORTS(x; y) ^ :3ATTACHED(x; y)
;
C
C
[S UPPORTED(y) ^ UPPORTED(x)]
9
C
UPPORTED(y );
C
h
=
C
UPPORTED(y ) ^ C ONTACTS(y; z ) ^ UPPORTS(z; ) ^ UPPORTED(x)^
^
;
C
:3S UPPORTS(x; y) ^ :3ATTACHED(x; y) ^ :3ATTACHED(y; z)
C
;
C
[S UPPORTED(y) ^ UPPORTED(x) ^ UPPORTS(y; x)] )
C
C
[S UPPORTED(y) ^ ATTACHED(w; x)] ;
C
[S UPPORTED(y) ^ C ONTACTS(y; z) ^ UPPORTED(x)] ; ^
C
C
[S UPPORTED(y) ^ UPPORTED(x) ^ UPPORTED(y)x]
9 C
[S UPPORTED(y) ^ ATTACHED(w; x)] ;
= C
C
[hS UPPORTED(y) ^ UPPORTED(x) ^ UPPORTS(y; x)] ;

^C
UPPORTED(y ) ^ UPPORTED(x) ^ UPPORTS(y; x) ^ C ONTACTS(x; ) ^ C ONTACTS(y; z )^
; C
C
:3S UPPORTS(x; y) ^ :3ATTACHED(w; x) ^ :3ATTACHED(x; y) ^ :3ATTACHED(y; z)
9 C

UPPORTED(y );
> C
h

>
= C
UPPORTED(y ) ^ UPPORTED(x) ^ UPPORTS(y; x) ^ UPPORTS(z; )^
C
;
C
C
ONTACTS(x; ) ^ C ONTACTS(y; z )
h
>
UPPORTED(y ) ^ UPPORTED(x) ^ UPPORTS(y; x) ^ C ONTACTS(x; ) ^ C ONTACTS(y; z )^
>
;

[S UPPORTED(y)] ;
h

:3S UPPORTS(x; y) ^ :3ATTACHED(w; x) ^ :3ATTACHED(x; y) ^ :3ATTACHED(y; z)

Figure 16: learned 3-AMA definition TACK (w; x; y; z ).

442

fiL EARNING EMPORAL E VENTS

0 8
>
>
B >
>
B >
>
B >
>
B <
B
B >
B >
>
B >
B >
>
>
B >
B :
B
B (
B
B
B
B
B (
B
B
B
B
B
B (
B
B
B
B
B (
B
B
B
B
B 8
B
B >
>
B >
>
B >
B <
B
B >
B >
>
B >
:
B >
B 8
B
B >
>
B >
>
B >
B <
B
B >
B >
>
B >
>
B :
B
B (
B
B
B
B
B 8
B
B >
<
B
B
B >
B :
B
B (
B
B
B
B
B (
B
B
B
B
B
B 8
B >
B <
B
@
>
:

"

#

9

UPPORTED(x) ^ UPPORTED(y ) ^ UPPORTS(y; x)^
>
>
>
;
C ONTACTS(x; ) ^ C ONTACTS(y; z ) ^ :3S UPPORTS(w; x)^
>
>
>
>
:3S UPPORTS(x; y) ^ :3ATTACHED(w; x) ^ :3ATTACHED(x; y)
>
=
[2S UPPORTED(x) ^ UPPORTED(y)] ;
3
^
UPPORTED(x) ^ UPPORTED(y ) ^ ATTACHED(w; x) ^ UPPORTS(z; )^
>
>
>
6 C ONTACTS(y; z ) ^ ATTACHED(w; x) ^ :3S UPPORTS(x; )^
7 >
>
4
5 >
>
:3S UPPORTS(y; x) ^ :3C ONTACTS(x; y)^
>
;
:3ATTACHED(x; y) ^ :3ATTACHED(y; z)
)
[S UPPORTED(x) ^ UPPORTED(y) ^ UPPORTS(y; x)] ;
[S UPPORTED(x) ^ UPPORTED(y) ^ ATTACHED(w; x) ^ ATTACHED(y; z)] ; ^
[S UPPORTED(x) ^ UPPORTED(y) ^ ATTACHED(w; x) ^ C ONTACTS(y; z)] )
[S UPPORTED(x) ^ UPPORTED(y) ^ UPPORTS(y; x) ^ C ONTACTS(y; z)] ;
[S UPPORTED(x) ^ UPPORTED(y) ^ ATTACHED(y; z)] ;
^
[S UPPORTED(x) ^ UPPORTED(y) ^ ATTACHED(w; x) ^ C ONTACTS(y; z)] )
[S UPPORTED(x) ^ UPPORTED(y) ^ UPPORTS(y; x) ^ C ONTACTS(x; y)] ;
[S UPPORTED(x) ^ UPPORTED(y) ^ UPPORTS(y; x) ^ ATTACHED(x; y)] ; ^
[S UPPORTED(x) ^ UPPORTED(y) ^ ATTACHED(w; x)] )
[S UPPORTED(x) ^ UPPORTED(y) ^ UPPORTS(y; x)] ;
[S UPPORTED(x) ^ UPPORTED(y) ^ C ONTACTS(y; z)] ; ^
[S UPPORTED(x) ^ UPPORTED(y) ^ ATTACHED(w; x)]
9
[S UPPORTED(x) ^ UPPORTED(y) ^ UPPORTS(y; x)] ;
>
>
>
>
[2S UPPORTED(x) ^ UPPORTED(y) ^ ATTACHED(w; x)] ;
3 >
=
UPPORTED(x) ^ UPPORTED(y ) ^ ATTACHED(w; x) ^ UPPORTS(z; )^
^
7 >
6 C ONTACTS(y; z ) ^ ATTACHED(w; x) ^ :3S UPPORTS(x; )^
5 >
4
>
>
:3S UPPORTS(y; x) ^ :3C ONTACTS(x; y)^
>
;
ATTACHED(x; ) ^ :3ATTACHED(y; z )
:
3
2
3 9
UPPORTED(x) ^ UPPORTED(y ) ^ UPPORTS(y; x)^
>
>
>
6 C ONTACTS(x; ) ^ C ONTACTS(y; z )^
7 >
>
4
5; =
:3S UPPORTS(w; x) ^ :3S UPPORTS(x; y)^
^
:3ATTACHED(w; x) ^ :3ATTACHED(x; y)
>
>
>
>
[S UPPORTED(x) ^ UPPORTED(y) ^ UPPORTS(y; x)] ;
>
;
[S UPPORTED(x) ^ UPPORTED(y) ^ ATTACHED(w; x)]
[S UPPORTED(x) ^ UPPORTED(y) ^ UPPORTS(y; x) ^ C ONTACTS(y; z)] ; )
[S UPPORTED(x) ^ UPPORTED(y) ^ UPPORTS(y; x) ^ ATTACHED(y; z)] ; ^
[S UPPORTED(x) ^ UPPORTED(y) ^ ATTACHED(w; x)]
9
[S UPPORTED(x) ^ UPPORTED(y) ^ UPPORTS(y; x)] ;

>
=
UPPORTED(x) ^ UPPORTED(y ) ^ UPPORTS(y; x) ^ ATTACHED(y; z )^
^
;
UPPORTS(x; ) ^ ATTACHED(w; x) ^ ATTACHED(x; )
>
;
[S UPPORTED(x) ^ UPPORTED(y) ^ ATTACHED(w; x)]
)
[S UPPORTED(x) ^ UPPORTED(y)] ;
[S UPPORTED(x) ^ UPPORTED(y) ^ UPPORTS(y; x) ^ ATTACHED(w; x)] ; ^
[S UPPORTED(x) ^ UPPORTED(y) ^ UPPORTS(w; x) ^ ATTACHED(w; x)] )
[S UPPORTED(x) ^ UPPORTED(y) ^ UPPORTS(y; x)] ;
[S UPPORTED(x) ^ UPPORTED(y) ^ UPPORTS(w; x) ^ ATTACHED(w; x)] ; ^
[S UPPORTED(x) ^ UPPORTED(y) ^ ATTACHED(w; x)]
9
[S UPPORTED(x) ^ UPPORTED(y) ^ UPPORTS(y; x)] ;

>
=
UPPORTED(x) ^ UPPORTED(y ) ^ C ONTACTS(y; z )^
;
:3S UPPORTS(x; y) ^ :3ATTACHED(x; y) ^ :3ATTACHED(y; z)
>
;
[S UPPORTED(x) ^ UPPORTED(y)]

Figure 17: learned 3-AMA definition U NSTACK (w; x; y; z ).

443

1
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C


fiF ERN , G IVAN , & ISKIND

0 8
>
>
>
B >
>
B >
>
B >
>
B >
B <
B
B >
B >
>
B >
>
B >
>
B >
>
B >
B :
B 8
B >
B <
B
B
B >
B :
B 8
B >
B <
B
B
B >
B :
B 8
B >
B <
B
B
B >
B :
B 8
B >
B <
B
B
B >
B :
B 8
B >
B <
B
B
B >
B :
B 8
B >
B <
B
@
>
:

2
6
4

UPPORTED (x) ^ UPPORTS (y; x) ^ C ONTACTS (y; x)^
:3S UPPORTS(w; x) ^ :3S UPPORTS(z; x) ^ :3C ONTACTS(x; z)^
:3ATTACHED(w; x) ^ :3ATTACHED (y; x) ^ :3ATTACHED (x; z)

3
7
5

UPPORTED (x);
UPPORTED (x) ^ UPPORTS (z; x) ^ C ONTACTS (x; z )^
6
:
4 3S UPPORTS (w; x) ^ :3S UPPORTS (y; x) ^ :3C ONTACTS (y; x)^
:3ATTACHED(w; x) ^ :3ATTACHED9(y; x) ^ :3ATTACHED (x; z)
[S UPPORTED (x) ^ UPPORTS (y; x)] ; >
=
[S UPPORTED (x) ^ ATTACHED (w; x)] ; > ^
;
UPPORTED (x)
9
>
UPPORTED (x);
=
[S UPPORTED (x) ^ ATTACHED (w; x) ^ ATTACHED (x; z )] ; > ^
;
UPPORTED (x)
9
>
[S UPPORTED (x)] ;
=
[S UPPORTED (x) ^ ATTACHED (x; z )] ; > ^
[S UPPORTED (x) ^ C ONTACTS (x; z )] ;
9
>
UPPORTED (x);
=
[S UPPORTED (x) ^ ATTACHED (w; x) ^ UPPORTS (w; x)] ; > ^
;
UPPORTED (x)
9
>
UPPORTED (x);
=
[S UPPORTED (x) ^ ATTACHED (w; x) ^ ATTACHED (y; x)] ; > ^
;
UPPORTED (x)
9
[S UPPORTED (x) ^ C ONTACTS (y; x)] ; >
=
[S UPPORTED (x) ^ ATTACHED (y; x)] ; >
;
UPPORTED (x)
2

Figure 18: learned 3-AMA definition OVE (w; x; y; z ).

444

3
7
5

;

9
>
>
>
>
>
>
>
>
>
>
=
>
>
>
>
>
>
>
>
>
>
;

1

^

C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C


fiL EARNING EMPORAL E VENTS

0 8
>
>
>
B >
>
>
B >
B >
>
>
B <
B
B
B >
B >
>
B >
>
B >
>
B >
>
B >
:
B
B 8
B >
B >
B >
>
B >
<
B
B
B >
B >
>
B >
>
B :
B 8
B
B >
B <
B
B >
B :
B 8
B
B >
B <
B
B >
B :
B 8
B
B >
B <
B
B >
B :
B 8
B
B >
B <
@
>
:

2

3

9

:3S UPPORTED (x) ^ :3S UPPORTS(z; y) ^ :3S UPPORTS(y; x)^ 7 >
>
>
6
>
:
4 3C ONTACTS (x; ) ^ :3C ONTACTS (z; )^
5; >
>
>
>
>
>
:3ATTACHED(w; x) ^ :3ATTACHED (z; y)
=

true
;
2
6
4
2
6
4

3

UPPORTED (x) ^ UPPORTED (y ) ^ UPPORTS (z; )^
7
UPPORTS (y; x) ^ C ONTACTS (x; )^
5
C ONTACTS (z; ) ^ :3ATTACHED (w; )
:3S UPPORTED (x) ^ :3S UPPORTS(z; y) ^ :3S UPPORTS(y; x)^
:3C ONTACTS(x; y) ^ :3C ONTACTS(z; y)^
:3ATTACHED(w; x) ^ :3ATTACHED (z; y)

ATTACHED (w; );
UPPORTED (y )

9
>
=

true;

3
7
5

;

>
>
>
>
>
>
>
>
>
>
;
9
>
>
>
>
>
=
>
>
>
>
>
;

[S UPPORTED (y) ^ :3ATTACHED (w; x) ^ :3ATTACHED (z; y)] ; > ^
;
UPPORTED (y )
9
>
true;
=
[S UPPORTED (y) ^ ATTACHED (z; y)] ; > ^
[S UPPORTED (y) ^ C ONTACTS (z; y)] ;
true;
[S UPPORTED (y) ^ UPPORTS (z; y)C ONTACTS (z; y) ^ ATTACHED (w; x)] ;
UPPORTED (y )
9
>
true;
=
[S UPPORTED (y) ^ ATTACHED (w; y)ATTACHED (z; y)] ; >
;
UPPORTED (y )
Figure 19: learned 3-AMA definition SSEMBLE (w; x; y; z ).

445

1

^

^

9
>
=
>
;

^

C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C


fiF ERN , G IVAN , & ISKIND

0 8
>
>
B >
>
B >
>
B >
>
B >
>
B <
B
B >
B >
B >
>
B >
>
B >
>
B >
>
B :
B 8
B >
B >
B <
B
B >
B >
B :
B 8
B >
B >
B <
B
B >
B >
B :
B 8
B >
B >
B <
B
B >
B >
B :
B 8
B >
B >
B <
B
B >
B >
B :
B 8
B <
B
B
B :
B
B 8
B <
B
@
:

3

2

9

UPPORTED (x) ^ UPPORTED(y ) ^ UPPORTS(y; x) ^ UPPORTS (z; )^
>
>
7 >
>
6 C ONTACTS (x; ) ^ C ONTACTS(z; ) ^ : UPPORTS(w; x)^
7; >
>
6
>
5 >
4 : UPPORTS(w; ) ^ : UPPORTS(x; ) ^ : ATTACHED(x; w)^
>
>
=
: ATTACHED(w; y) ^ : ATTACHED(x; y) ^ : ATTACHED(z; y)
^
UPPORTED(y );
>
>
2
3
>
>
UPPORTED (y ) ^ : UPPORTED(x) ^ : UPPORTS(w; x)^
>
>
>
>
4 : UPPORTS(z; ) ^ : UPPORTS(y; x) ^ : C ONTACTS(x; )^ 5 ;
>
>
;
: C ONTACTS(z; y) ^ : ATTACHED(x; w) ^ : ATTACHED9(z; y)
[ UPPORTED (x) ^ UPPORTED (y )] ;
>

>
=
UPPORTED(x) ^ UPPORTED (y ) ^ UPPORTS (w; x)^
^
;
UPPORTS(z; ) ^ C ONTACTS (z; ) ^ ATTACHED(x; w)
>
>
;
UPPORTED(y )
9


UPPORTED(x) ^ UPPORTED (y ) ^ UPPORTS (z; )^
>
>
;
=
UPPORTS(y; x) ^ C ONTACTS (x; ) ^ C ONTACTS(z; )
^
[ UPPORTED (x) ^ UPPORTED (y ) ^ UPPORTS (y; x) ^ ATTACHED (x; )] ; >
>
;
UPPORTED(y )
9
[ UPPORTED (x) ^ UPPORTED (y ) ^ UPPORTS (y; x) ^ C ONTACTS (z; )] ; >
>


=
UPPORTED(x) ^ UPPORTED (y ) ^ UPPORTS (x; )^
;
^
UPPORTS(y; z ) ^ ATTACHED(x; ) ^ ATTACHED(z; )
>
>
;
UPPORTED(y )
9
[ UPPORTED (x) ^ UPPORTED (y ) ^ UPPORTS (y; x)] ;
>
>

=
UPPORTED(x) ^ UPPORTED (y ) ^ UPPORTS (x; )^
;
^
UPPORTS(y; z ) ^ ATTACHED(x; ) ^ ATTACHED(z; ) ^ ATTACHED(x; w)
>
>
;
UPPORTED(y )
9
UPPORTED(y );
=
[ UPPORTED (y ) ^ ATTACHED (w; ) ^ ATTACHED (z; )] ;
^
;
UPPORTED(y )
9
UPPORTED(y );
=
[ UPPORTED (y ) ^ UPPORTS (w; ) ^ ATTACHED (w; )] ;
;
UPPORTED(y )

3
3

3
3

3
3

3

3
3

3

3

3
3

3
3

Figure 20: learned 3-AMA definition ISASSEMBLE (w; x; y; z ).

446

1
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C
C


fiL EARNING EMPORAL E VENTS

References
Agrawal, R., & Srikant, R. (1995). Mining sequential patterns. Proceedings Eleventh
International Conference Data Engineering, pp. 314.
Allen, J. F. (1983). Maintaining knowledge temporal intervals. Communications ACM,
26(11), 832843.
Angluin, D. (1987). Learning regular sets queries counterexamples. Information
Computation, 75, 87106.
Bacchus, F., & Kabanza, F. (2000). Using temporal logics express search control knowledge
planning. Artificial Intelligence, 16, 123191.
Bobick, A. F., & Ivanov, Y. A. (1998). Action recognition using probabilistic parsing. Proceedings IEEE Computer Society Conference Computer Vision Pattern Recognition,
pp. 196202, Santa Barbara, CA.
Borchardt, G. C. (1985). Event calculus. Proceedings Ninth International Joint Conference
Artificial Intelligence, pp. 524527, Los Angeles, CA.
Brand, M. (1997a). inverse Hollywood problem: video scripts storyboards via
causal analysis. Proceedings Fourteenth National Conference Artificial Intelligence, pp. 132137, Providence, RI.
Brand, M. (1997b). Physics-based visual understanding. Computer Vision Image Understanding, 65(2), 192205.
Brand, M., & Essa, I. (1995). Causal analysis visual gesture understanding. Proceedings
AAAI Fall Symposium Computational Models Integrating Language Vision.
Brand, M., Oliver, N., & Pentland, A. (1997). Coupled hidden Markov models complex action
recognition. Proceedings IEEE Computer Society Conference Computer Vision
Pattern Recognition.
Cohen, P. (2001). Fluent learning: Elucidating structure episodes. Proceedings
Fourth Symposium Intelligent Data Analysis.
Cohen, W. (1994). Grammatically biased learning: Learning logic programs using explicit antecedent description lanugage. Artificial Intelligence, 68, 303366.
Cohen, W., & Hirsh, H. (1994). Learning CLASSIC description logic: Theoretical experimental results. Proceedings Fourth International Conference Principles Knowledge
Representation Reasoning, pp. 121133.
De Raedt, L., & Dehaspe, L. (1997). Clausal discovery. Machine Learning, 26, 99146.
Dehaspe, L., & De Raedt, L. (1996). DLAB: declarative language bias formalism. Proceedings
Ninth International Syposium Methodologies Intelligent Systems, pp. 613622.
Fikes, R., & Nilsson, N. (1971). STRIPS: new approach application theorem proving
problem solving. Artificial Intelligence, 2(3/4).
Hoppner, F. (2001). Discovery temporal patternsLearning rules qualitative behaviour
time series. Proceedings Fifth European Conference Principles Practice
Knowledge Discovery Databases.
447

fiF ERN , G IVAN , & ISKIND

Kam, P., & Fu, A. (2000). Discovering temporal patterns interval-based events. Proceedings
Second International Conference Data Warehousing Knowledge Discovery.
Klingspor, V., Morik, K., & Rieger, A. D. (1996). Learning concepts sensor data mobile
robot. Artificial Intelligence, 23(2/3), 305332.
Lang, K., Pearlmutter, B., & Price, R. (1998). Results Abbadingo one DFA learning competition new evidence-driven state merging algorithm. Proceedings Fourth
International Colloquium Grammatical Inference.
Lavrac, N., Dzeroski, S., & Grobelnik, M. (1991). Learning nonrecursive definitions relations
LINUS. Proceedings Fifth European Working Session Learning, pp. 265
288.
Mann, R., & Jepson, A. D. (1998). Toward computational perception action. Proceedings
IEEE Computer Society Conference Computer Vision Pattern Recognition, pp.
794799, Santa Barbara, CA.
Mannila, H., Toivonen, H., & Verkamo, A. I. (1995). Discovery frequent episodes sequences.
Proceedings First International Conference Knowledge Discovery Data Mining.
Mitchell, T. (1982). Generalization search. Artificial Intelligence, 18(2), 51742.
Morales, E. (1997). Pal: pattern-based first-order inductive system. Machine Learning, 26, 227
252.
Muggleton, S. (1995). Inverting entailment Progol. Machine Intelligence, 14, 133188.
Muggleton, S., & Feng, C. (1992). Efficient induction logic programs. Muggleton, S. (Ed.),
Inductive Logic Programming, pp. 281298. Academic Press.
Muggleton, S., & De Raedt, L. (1994). Inductive logic programming: Theory methods. Journal
Logic Programming, 19/20, 629679.
Pinhanez, C., & Bobick, A. (1995). Scripts machine understanding image sequences.
Proceedings AAAI Fall Symposium Series Computational Models Integrating
Language Vision.
Plotkin, G. D. (1971). Automatic Methods Inductive Inference. Ph.D. thesis, Edinburgh University.
Regier, T. P. (1992). Acquisition Lexical Semantics Spatial Terms: Connectionist Model
Perceptual Categorization. Ph.D. thesis, University California Berkeley.
Roth, D., & Yih, W. (2001). Relational learning via propositional algorithms: information extraction case study. Proeedings Seventeenth International Joint Conference Artificial
Intelligence.
Shoham, Y. (1987). Temporal logics AI: Semantical ontological considerations. Artificial
Intelligence, 33(1), 89104.
Siskind, J. M. (2000). Visual event classification via force dynamics. Proceedings Seventeenth National Conference Artificial Intelligence, pp. 149155, Austin, TX.
Siskind, J. M. (2001). Grounding lexical semantics verbs visual perception using force
dynamics event logic. Journal Artificial Intelligence Research, 15, 3190.
448

fiL EARNING EMPORAL E VENTS

Siskind, J. M., & Morris, Q. (1996). maximum-likelihood approach visual event classification. Proceedings Fourth European Conference Computer Vision, pp. 347360,
Cambridge, UK. Springer-Verlag.
Talmy, L. (1988). Force dynamics language cognition. Cognitive Science, 12, 49100.
Yamoto, J., Ohya, J., & Ishii, K. (1992). Recognizing human action time-sequential images using
hidden Markov model. Proceedings IEEE Conference Computer Vision
Pattern Recognition, pp. 379385.

449


