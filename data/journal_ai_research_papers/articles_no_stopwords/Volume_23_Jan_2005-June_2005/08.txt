Journal Artificial Intelligence Research 23 (2005) 367-420

Submitted 07/04; published 04/05

Hybrid BDI-POMDP Framework Multiagent Teaming
Ranjit Nair

ranjit.nair@honeywell.com

Automation Control Solutions
Honeywell Laboratories, Minneapolis, MN 55416

Milind Tambe

tambe@usc.edu

Department Computer Science
University Southern California, Los Angeles, CA 90089

Abstract
Many current large-scale multiagent team implementations characterized
following belief-desire-intention (BDI) paradigm, explicit representation team
plans. Despite promise, current BDI team approaches lack tools quantitative
performance analysis uncertainty. Distributed partially observable Markov decision
problems (POMDPs) well suited analysis, complexity finding optimal
policies models highly intractable. key contribution article
hybrid BDI-POMDP approach, BDI team plans exploited improve POMDP
tractability POMDP analysis improves BDI team plan performance.
Concretely, focus role allocation, fundamental problem BDI teams:
agents allocate different roles team. article provides three key contributions. First, describe role allocation technique takes account future
uncertainties domain; prior work multiagent role allocation failed address
uncertainties. end, introduce RMTDP (Role-based Markov Team Decision Problem), new distributed POMDP model analysis role allocations.
technique gains tractability significantly curtailing RMTDP policy search; particular, BDI team plans provide incomplete RMTDP policies, RMTDP policy search
fills gaps incomplete policies searching best role allocation.
second key contribution novel decomposition technique improve RMTDP
policy search efficiency. Even though limited searching role allocations, still
combinatorially many role allocations, evaluating RMTDP identify best
extremely difficult. decomposition technique exploits structure BDI team
plans significantly prune search space role allocations. third key contribution
significantly faster policy evaluation algorithm suited BDI-POMDP hybrid approach. Finally, present experimental results two domains: mission rehearsal
simulation RoboCupRescue disaster rescue simulation.

1. Introduction
Teamwork, whether among software agents, robots (and people) critical capability
large number multiagent domains ranging mission rehearsal simulations,
RoboCup soccer disaster rescue, personal assistant teams. Already large number multiagent teams developed range domains (Pynadath & Tambe,
2003; Yen, Yin, Ioerger, Miller, Xu, & Volz, 2001; Stone & Veloso, 1999; Jennings, 1995;
Grosz, Hunsberger, & Kraus, 1999; Decker & Lesser, 1993; Tambe, Pynadath, & Chauvat,
2000; da Silva & Demazeau, 2002). existing practical approaches characterized situated within general belief-desire-intention (BDI) approach, paradigm
c
2005
AI Access Foundation. rights reserved.

fiNair & Tambe

designing multiagent systems, made increasingly popular due programming frameworks (Tambe et al., 2000; Decker & Lesser, 1993; Tidhar, 1993b) facilitate design
large-scale teams. Within approach, inspired explicitly implicitly BDI logics,
agents explicitly represent reason team goals plans (Wooldridge, 2002).
article focuses analysis BDI teams, provide feedback aid human
developers possibly agents participating team, team performance
complex dynamic domains improved. particular, focuses critical
challenge role allocation building teams (Tidhar, Rao, & Sonenberg, 1996; Hunsberger
& Grosz, 2000), i.e. agents allocate various roles team. instance,
mission rehearsal simulations (Tambe et al., 2000), need select numbers
types helicopter agents allocate different roles team. Similarly, disaster
rescue (Kitano, Tadokoro, Noda, Matsubara, Takahashi, Shinjoh, & Shimada, 1999), role
allocation refers allocating fire engines ambulances fires greatly impact
team performance. domains, performance team
linked important metrics loss human life property thus critical
analyze team performance suggest improvements.
BDI frameworks facilitate human design large scale teams, key difficulty
analyzing role allocation teams due uncertainty arises complex
domains. example, actions may fail world state may partially observable
agents owing physical properties environment imperfect sensing. Role
allocation demands future uncertainties taken account, e.g. fact
agent may fail execution may may replaced another must taken
account determining role allocation. Yet current role allocation algorithms address uncertainty (see Section 7.4). Indeed, uncertainty requires
quantitative comparison different role allocations. However, tools quantitative
evaluations BDI teams currently absent. Thus, given uncertainties, may
required experimentally recreate large number possible scenarios (in real domain
simulations) evaluate compare different role allocations.
Fortunately, emergence distributed Partially Observable Markov Decision Problems (POMDPs) provides models (Bernstein, Zilberstein, & Immerman, 2000; Boutilier,
1996; Pynadath & Tambe, 2002; Xuan, Lesser, & Zilberstein, 2001) used
quantitative analysis agent teams uncertain domains. Distributed POMDPs represent class formal models powerful enough express uncertainty
dynamic domains arising result non-determinism partial observability
principle, used generate evaluate complete policies multiagent team.
However, two shortcomings models prevents application
analysis role allocation. First, previous work analysis focused communication (Pynadath & Tambe, 2002; Xuan et al., 2001), rather role allocation
coordination decisions. Second, shown Bernstein et al. (2000), problem
deriving optimal policy generally computationally intractable (the corresponding
decision problem NEXP-complete). Thus, applying optimal policies analysis highly
intractable.
address first difficulty, derive RMTDP (Role-based Multiagent Team Decision
Problem), distributed POMDP framework quantitatively analyzing role allocations.
Using framework, show that, general, problem finding optimal role
368

fiHybrid BDI-POMDP Framework Multiagent Teaming

completed policy =
additions BDI team plan

BDI team plan
RMTDP
Search Policy Space

Incomplete policy
BDI Interpreter

Domain
RMTDP model

Figure 1: Integration BDI POMDP.

allocation policy computationally intractable (the corresponding decision problem still
NEXP-complete). shows improving tractability analysis techniques role
allocation critically important issue.
Therefore, order make quantitative analysis multiagent teams using RMTDP
tractable, second contribution provides hybrid BDI-POMDP approach
combines native strengths BDI POMDP approaches, i.e., ability BDI
frameworks encode large-scale team plans POMDP ability quantitatively
evaluate plans. hybrid approach based three key interactions improve
tractability RMTDP optimality BDI agent teams. first interaction
shown Figure 1. particular, suppose wish analyze BDI agent team (each agent
consisting BDI team plan domain independent interpreter helps coordinate
plans) acting domain. shown Figure 1, model domain via
RMTDP, rely BDI team plan interpreter providing incomplete policy
RMTDP. RMTDP model evaluates different completions incomplete
policy provides optimally completed policy feedback BDI system. Thus,
RMTDP fills gaps incompletely specified BDI team plan optimally.
gaps concentrate role allocations, method applied
key coordination decisions. restricting optimization role allocation decisions
fixing policy points, able come restricted policy
space. use RMTDPs effectively search restricted space order find
optimal role allocation.
restricted policy search one key positive interaction hybrid approach,
second interaction consists efficient policy representation used converting
BDI team plan interpreter corresponding policy (see Figure 1) new
algorithm policy evaluation. general, agents policy distributed POMDP
indexed observation history (Bernstein et al., 2000; Pynadath & Tambe, 2002).
369

fiNair & Tambe

However, BDI system, agent performs action selection based set
privately held beliefs obtained agents observations applying belief
revision function. order evaluate teams performance, sufficient RMTDP
index agents policies belief state (represented privately held beliefs)
instead observation histories. shift representation results considerable
savings amount time needed evaluate policy space required
represent policy.
third key interaction hybrid approach exploits BDI team plan structure increasing efficiency RMTDP-based analysis. Even though RMTDP
policy space restricted filling gaps incomplete policies, many policies may result
given large number possible role allocations. Thus enumerating evaluating
possible policy given domain difficult. Instead, provide branch-and-bound algorithm exploits task decomposition among sub-teams team significantly prune
search space provide correctness proof worst-case analysis algorithm.
order empirically validate approach, applied RMTDP allocation
BDI teams two concrete domains: mission rehearsal simulations (Tambe et al., 2000)
RoboCupRescue (Kitano et al., 1999). first present (significant) speed-up
gained three interactions mentioned above. Next, domains, compared
role allocations found approach state-of-the-art techniques allocate
roles without uncertainty reasoning. comparison shows importance reasoning
uncertainty determining role allocation complex multiagent domains.
RoboCupRescue domain, compared allocations found allocations chosen
humans actual RoboCupRescue simulation environment. results showed
role allocation technique presented article capable performing human
expert levels RoboCupRescue domain.
article organized follows: Section 2 presents background motivation.
Section 3, introduce RMTDP model present key complexity results. Section
4 explains BDI team plan evaluated using RMTDP. Section 5 describes
analysis methodology finding optimal role allocation, Section 6 presents
empirical evaluation methodology. Section 7, present related work
Section 8, list conclusions.

2. Background
section first describes two domains consider article: abstract
mission rehearsal domain (Tambe et al., 2000) RoboCupRescue domain (Kitano
et al., 1999). domain requires us allocate roles agents team. Next, teamoriented programming (TOP), framework describing team plans described
context two domains. focus TOP, discussed Section 7.1,
techniques would applicable frameworks tasking teams (Stone & Veloso,
1999; Decker & Lesser, 1993).
2.1 Domains
first domain consider based mission rehearsal simulations (Tambe et al.,
2000). expository purposes, intentionally simplified. scenario
370

fiHybrid BDI-POMDP Framework Multiagent Teaming

follows: helicopter team executing mission transporting valuable cargo point
X point enemy terrain (see Figure 2). three paths X
different lengths different risk due enemy fire. One scouting sub-teams must
sent (one path X Y), larger size scouting sub-team
safer is. scouts clear one path X Y, transports
move safely along path. However, scouts may fail along path, may
need replaced transport cost transporting cargo. Owing partial
observability, transports may receive observation scout failed
route cleared. wish transport amount cargo quickest
possible manner within mission deadline.
key role allocation decision given fixed number helicopters,
allocated scouting transport roles? Allocating scouts means
scouting task likely succeed, fewer helicopters left
used transport cargo consequently less reward. However, allocating
scouts could result mission failing altogether. Also, allocating scouts,
routes scouts sent on? shortest route would preferable
risky. Sending scouts route decreases likelihood failure
individual scout; however, might beneficial send different routes, e.g.
scouts risky short route others safe longer route.
Thus many role allocations consider. Evaluating difficult
role allocation must look-ahead consider future implications uncertainty, e.g. scout
helicopters fail scouting may need replaced transport. Furthermore, failure success scout may visible transport helicopters hence
transport may replace scout transports may never fly destination.
scout
transports

X

route 1

route 2



enemy gun
route 3

Figure 2: Mission rehearsal domain.

second example scenario (see Figure 3), set RoboCupRescue disaster
simulation environment (Kitano et al., 1999), consists five fire engines three different
fire stations (two stations 1 & 3 last station 2) five ambulances
stationed ambulance center. Two fires (in top left bottom right corners
map) start need extinguished fire engines. fire extinguished,
ambulance agents need save surviving civilians. number civilians
371

fiNair & Tambe

location known ahead time, although total number civilians known.
time passes, high likelihood health civilians deteriorate fires
increase intensity. Yet agents need rescue many civilians possible
minimal damage buildings. first part goal scenario therefore
first determine fire engines assign fire. fire engines gathered
information number civilians fire, transmitted ambulances.
next part goal allocate ambulances particular fire rescue
civilians trapped there. However, ambulances cannot rescue civilians fires fully
extinguished. Here, partial observability (each agent view objects within visual
range), uncertainty related fire intensity, well location civilians
health add significantly difficulty.

C1
F3
F2
F1
C2


Figure 3: RoboCupRescue Scenario: C1 C2 denote two fire locations, F1, F2
F3 denote fire stations 1, 2 3 respectively denotes ambulance
center.

2.2 Team-Oriented Programming
aim team-oriented programming (TOP) (Pynadath & Tambe, 2003; Tambe et al.,
2000; Tidhar, 1993b) framework provide human developers (or automated symbolic
planners) useful abstraction tasking teams. domains described
Section 2.1, consists three key aspects team: (i) team organization hierarchy
consisting roles; (ii) team (reactive) plan hierarchy; (iii) assignment roles
sub-plans plan hierarchy. developer need specify low-level coordination
details. Instead, TOP interpreter (the underlying coordination infrastructure) automatically enables agents decide communicate reallocate
372

fiHybrid BDI-POMDP Framework Multiagent Teaming

roles upon failure. TOP abstraction enables humans rapidly provide team plans
large-scale teams, unfortunately, qualitative assessment team performance
feasible. Thus, key TOP weakness inability quantitatively evaluate optimize
team performance, e.g., allocating roles agents qualitative matching capabilities may feasible. discussed later, hybrid BDI-POMDP model addresses
weakness providing techniques quantitative evaluation.
concrete example, consider TOP mission rehearsal domain. first
specify team organization hierarchy (see Figure 4(a)). Task Force highest level
team organization consists two roles Scouting Transport,
Scouting sub-team roles three scouting sub-sub-teams. Next specify
hierarchy reactive team plans (Figure 4(b)). Reactive team plans explicitly express
joint activities relevant team consist of: (i) pre-conditions plan
proposed; (ii) termination conditions plan ended; (iii)
team-level actions executed part plan (an example plan discussed
shortly). Figure 4(b), highest level plan Execute Mission three sub-plans:
DoScouting make one path X safe transports, DoTransport
move transports along scouted path, RemainingScouts scouts
reached destination yet get there.
Execute Mission [Task Force]
DoScouting
[Task Force]

RemainingScouts
DoTransport
[Scouting Team] [Transport Team]

Task Force
ScoutRoutes
WaitAtBase
[Transport Team] [Scouting Team]

Scouting Team

Transport Team

SctTeamA SctTeamB SctTeamC

ScoutRoute1 ScoutRoute2 ScoutRoute3
[SctTeamA] [SctTeamB] [SctTeamC]

(a)

(b)

Figure 4: TOP mission rehearsal domain a: Organization hierarchy; b: Plan hierarchy.

Figure 4(b) shows coordination relationships: relationship indicated
solid arc, relationship indicated dashed arc. Thus, WaitAtBase ScoutRoutes must done least one ScoutRoute1,
ScoutRoute2 ScoutRoute3 need performed. temporal dependence relationship among sub-plans, implies sub-teams assigned perform
DoTransport RemainingScouts cannot DoScouting plan completed. However, DoTransport RemainingScouts execute parallel. Finally,
assign roles plans Figure 4(b) shows assignment brackets adjacent plans.
instance, Task Force team assigned jointly perform Execute Mission SctTeamA assigned ScoutRoute1.
team plan corresponding Execute Mission shown Figure 5.
seen, team plan consists context, pre-conditions, post-conditions, body constraints. context describes conditions must fulfilled parent plan
pre-conditions particular conditions cause sub-plan begin exe373

fiNair & Tambe

cution. Thus, Execute Mission, pre-condition team mutually believes
(MB)1 start location. post-conditions divided Achieved,
Unachievable Irrelevant conditions sub-plan terminated.
body consists sub-plans exist within team plan. Lastly, constraints describe
temporal constraints exist sub-plans body. description
plans plan hierarchy Figure 4(b) given Appendix A.
ExecuteMission:
Context:
Pre-conditions: (MB <TaskForce> location(TaskForce) = START)
Achieved: (MB <TaskForce> (Achieved(DoScouting) Achieved(DoTransport))) (time
> (MB <TaskForce>
Achieved(RemainingScouts) ( helo ScoutingTeam, alive(helo)
location(helo) 6= END)))
Unachievable: (MB <TaskForce> Unachievable(DoScouting)) (MB <TaskForce>
Unachievable(DoTransport)
(Achieved(RemainingScouts) ( helo ScoutingTeam, alive(helo)
location(helo) 6= END)))
Irrelevant:
Body:
DoScouting
DoTransport
RemainingScouts
Constraints: DoScouting DoTransport, DoScouting RemainingScouts

Figure 5: Example team plan. MB refers mutual belief.
HTN (Dix, Muoz-Avila, Nau, & Zhang, 2003; Erol, Hendler, & Nau, 1994),
plan hierarchy TOP gives decomposition task smaller tasks. However,
language TOPs richer language early HTN planning (Erol et al., 1994)
contained simple ordering constraints. seen example, plan hierarchy
TOPs contain relationships OR. addition, recent
work HTN planning (Dix et al., 2003), sub-plans TOPs contain pre-conditions
post-conditions, thus allowing conditional plan execution. main differences
TOPs HTN planning are: (i) TOPs contain organization hierarchy addition
plan hierarchy, (ii) TOP interpreter ensures team executes plans coherently.
seen later, TOPs analyzed expressiveness including conditional
execution; however, since analysis focus fixed time horizon, loops
task description unrolled time horizon.
1. Mutual Belief (Wooldridge, 2002), shown (MB hteami x) Figure 5, refers private belief held
agent team believe fact x true, agents
team believe x true, every agent believes every agent believes x
true on. infinite levels nesting difficult realize practice. Thus, practical
BDI implementations, purposes article, mutual belief approximated private
belief held agent agents team believe x true.

374

fiHybrid BDI-POMDP Framework Multiagent Teaming

new observation
agent

Belief Update
function

Private beliefs
agent

Figure 6: Mapping observations beliefs.

execution, agent copy TOP. agent maintains set
private beliefs, set propositions agent believes true (see
Figure 6). agent receives new beliefs, i.e. observations (including communication),
belief update function used update set privately held beliefs. instance,
upon seeing last scout crashed, transport may update privately held beliefs
include belief CriticalFailure(DoScouting). practical BDI systems, belief
update computation low complexity (e.g. constant linear time). beliefs
updated, agent selects plan execute matching beliefs preconditions plans. basic execution cycle similar standard reactive planning
systems PRS (Georgeff & Lansky, 1986).
team plan execution, observations form communications often arise
coordination actions executed TOP interpreter. instance, TOP
interpreters exploited BDI theories teamwork, Levesque et al.s theory
joint intentions (Levesque, Cohen, & Nunes, 1990) require agent
comes privately believe fact terminates current team plan (i.e. matches
achievement unachievability conditions team plan), communicates fact
rest team. performing coordination actions automatically, TOP
interpreter enables coherence initiation termination team plans within TOP.
details examples TOPs seen work Pynadath
Tambe (2003), Tambe et al. (2000) Tidhar (1993b).
concretely illustrate key challenges role allocation mentioned
earlier. First, human developer must allocate available agents organization hierarchy (Figure 4(a)), find best role allocation. However, combinatorially many
allocations choose (Hunsberger & Grosz, 2000; Tambe et al., 2000). instance,
starting 6 homogeneous helicopters results 84 different ways deciding
many agents assign scouting transport sub-team. problem exacerbated fact best allocation varies significantly based domain variations.
example, Figure 7 shows three different assignments agents team organization hierarchy, found analysis best given setting failure
observation probabilities (details Section 6). example, increasing probability
failures routes resulted number transports best allocation changing
four (see Figure 7(b)) three (see Figure 7(a)), additional scout added
SctTeamB. failures possible all, number transports increased
five (see Figure 7(c)). analysis takes step towards selecting best among
allocations.
375

fiNair & Tambe

Task Force
Scouting Team

Task Force

Transport Team=3

Scouting Team

SctTeamA=2 SctTeamB=1 SctTeamC=0

Transport Team=4

SctTeamA=2 SctTeamB=0 SctTeamC=0

(a) Medium probability

(b) Low probability
Task Force

Scouting Team

Transport Team=5

SctTeamA=0 SctTeamB=0 SctTeamC=1

(c) Zero probability

Figure 7: Best role allocations different probabilities scout failure.

Figure 8 shows TOP RoboCupRescue scenario. seen, plan hierarchy scenario consists pair ExtinguishFire RescueCivilians plans
done parallel, decompose individual plans. (These individual plans get fire engines ambulances move streets using specific
search algorithms, however, individual plans relevant discussions
article; interested readers refer description RoboCupRescue team
entered RoboCup competitions 2001 (Nair, Ito, Tambe, & Marsella, 2002).)
organizational hierarchy consists Task Force comprising two Engine sub-teams, one
fire Ambulance Team, engine teams assigned extinguishing
fires ambulance team assigned rescuing civilians. particular TOP,
assignment ambulances AmbulanceTeamA AmbulanceTeamB conditioned
communication c, indicated AmbulanceTeamA|c AmbulanceTeamB|c.
c described detail figure, refers communication received fire engines describes number civilians present fire.
problem engines assign Engine Team possible value c,
ambulances assign Ambulance Team. Note engines differing
capabilities owing differing distances fires ambulances identical
capabilities.
Task Force
EngineTeamA

EngineTeamB

AmbulanceTeam

AmbulanceTeamA |c

AmbulanceTeamB |c

(a)
ExecuteMission
[Task Force]
ExtinguishFire1
[EngineTeamA]

RescueCivilians1
[AmbulanceTeamA]

ExtinguishFire2
[EngineTeamB]

RescueCivilians2
[AmbulanceTeamB]

(b)

Figure 8: TOP RoboCupRescue scenario a: Organization hierarchy; b: Plan hierarchy.

376

fiHybrid BDI-POMDP Framework Multiagent Teaming

3. Role-based Multiagent Team Decision Problem
Multiagent Team Decision Problem (MTDP) (Pynadath & Tambe, 2002) inspired
economic theory teams (Marschak & Radner, 1972; Ho, 1980; Yoshikawa, 1978).
order quantitative analysis key coordination decisions multiagent teams,
extend MTDP analysis coordination actions interest. example,
COM-MTDP (Pynadath & Tambe, 2002) extension MTDP analysis communication. article, illustrate general methodology analysis aspects
coordination present RMTDP model quantitative analysis role allocation
reallocation concrete example. contrast BDI systems introduced previous section, RMTDP enables explicit quantitative optimization team performance. Note
that, use MTDP, possible distributed POMDP models could potentially
serve basis (Bernstein et al., 2000; Xuan et al., 2001).
3.1 Multiagent Team Decision Problem
Given team n agents, MTDP (Pynadath & Tambe, 2002) defined tuple:
hS, A, P, , O, Ri. consists finite set states = 1 j ,
1 j m, feature world state. agent perform action
set actions Ai , 1in Ai = A. P (s, < a1 , . . . , >, ) gives probability
transitioning state state given agents perform actions < a1 , . . . , >
jointly. agent receives observation (1in = ) based function
O(s, < a1 , . . . , >, 1 , . . . , n ), gives probability agents receive
observations, 1 , . . . , n given world state perform < a1 , . . . , >
jointly. agents receive single joint reward R(s, < a1 , . . . , >) based state
joint action < a1 , . . . , >. joint reward shared equally members
private reward individual agents receive actions. Thus,
agents motivated behave team, taking actions jointly yield
maximum expected reward.
agent MTDP chooses actions based local policy, ,
mapping observation history actions. Thus, time t, agent perform action
(i0 , . . . , ). contrasts single-agent POMDP, index agents
policy belief state probability distribution world state (Kaelbling, Littman,
& Cassandra, 1998), shown sufficient statistic order compute
optimal policy (Sondik, 1971). Unfortunately, cannot directly use single-agent POMDP
techniques (Kaelbling et al., 1998) maintaining updating belief states (Kaelbling et al.,
1998) MTDP unlike single agent POMDP, MTDP, agents observation
depends actions, unknown actions agents. Thus,
distributed POMDP models (Bernstein et al., 2000; Xuan et al., 2001),
MTDP, local policies indexed observation histories. =< 1 , . . . , n > refers
joint policy team agents.
3.2 Extension Explicit Coordination
Beginning MTDP, next step methodology make explicit separation
domain-level actions coordination actions interest. Earlier work intro377

fiNair & Tambe

duced COM-MTDP model (Pynadath & Tambe, 2002), coordination action
fixed communication action, got separated out. However, coordination actions could separated domain-level actions order investigate
impact. Thus, investigate role allocation reallocations, actions allocating agents
roles reallocate roles separated out. end, define RMTDP
(Role-based Multiagent Team Decision Problem) tuple hS, A, P, , O, R, RLi
new component, RL. particular, RL = {r1 , . . . , rs } set roles agents
undertake. instance role rj may assigned agent fulfill it.
actions agent distinguishable two types:
Role-Taking actions: = {irj } contains role-taking actions agent i. irj
means agent takes role rj RL.

Role-Execution Actions: = rj RL irj contains execution actions agent
irj set agent actions executing role rj RL
addition define set states = 1 roles, feature roles (a vector) gives current role agent taken on. reason
introducing new feature assist us mapping BDI team plan
RMTDP. Thus time agent performs new role-taking action successfully, value
feature roles updated reflect change. key
model agents initial role-taking action subsequent role reallocation. Modeling
allocation reallocation important accurate analysis BDI teams. Note
agent observe part feature pertaining current role
may observe parts pertaining agents roles.
introduction roles allows us represent specialized behaviors associated
role, e.g. transport vs. scout role. filling particular role, rj , agent
perform role-execution actions, irj , may different roleexecution actions irl role rl . Thus, feature roles used filter actions
role-execution actions correspond agents current role permitted.
worst case, filtering affect computational complexity (see Theorem 1
below) practice, significantly improve performance trying find
optimal policy team, since number domain actions agent choose
restricted role agent taken on. Also, different roles
produce varied effects world state (modeled via transition probabilities, P )
teams reward. Thus, policies must ensure agents role capabilities
benefit team most.
MTDP, agent chooses action perform indexing local policy
observation history. epoch agents could role-taking
actions others role-execution actions. Thus, agents local policy
divided local role-taking role-execution policies observation
histories, i0 , . . . , , either (i0 , . . . , ) = null (i0 , . . . , ) = null. =<
1 , . . . , n > refers joint role-taking policy team agents =<
1 , . . . , n > refers joint role-execution policy.
378

fiHybrid BDI-POMDP Framework Multiagent Teaming

article explicitly model communicative actions special action.
Thus communication treated role-execution action communication
received agents treated observations.2
3.3 Complexity Results RMTDP
Section 2.2 qualitatively emphasized difficulty role allocation, RMTDP helps
us understanding complexity precisely. goal RMTDP come
joint policies maximize total expected reward finite horizon
. Note agents change roles according local role-taking policies.
agents role-execution policy subsequent change would contain actions pertaining
new role. following theorem illustrates complexity finding optimal joint
policies.
Theorem 1 decision problem determining exist policies, ,
RMTDP, yield expected reward least K finite horizon NEXPcomplete.
Proof sketch: Proof follows reduction MTDP (Pynadath & Tambe, 2002)
to/from RMTDP. reduce MTDP RMTDP, set RMTDPs role taking actions, ,
null set RMTDPs role-execution actions, , MTDPs set actions, A.
reduce RMTDP
MTDP, generate new MTDP set actions,

equal . Finding required policy MTDP NEXP-complete (Pynadath &
Tambe, 2002).
theorem shows us, solving RMTDP optimal joint role-taking roleexecution policies even finite horizon highly intractable. Hence, focus
complexity determining optimal role-taking policy, given fixed role-execution
policy. fixed role-execution policy, mean action selection agent
predetermined role executing.
Theorem 2 decision problem determining exists role-taking policy, ,
RMTDP, yields expected reward least K together fixed role-execution
policy , finite horizon NEXP-complete.
Proof sketch: reduce MTDP RMTDP different role-taking
role-execution action corresponding action MTDP. Hence, RMTDP
role-taking action irj agent take role rj created action aj Ai
MTDP role rj contains single role-execution action, i.e. |irj | = 1.
RMTDP, construct transition function role-taking action always
succeeds affected state feature roles . role-execution action irj ,
transition probability MTDP action, aj Ai corresponding
last role-taking action irj . fixed role-execution policy simply perform
action, irj , corresponding last successful role-taking action, irj . Thus,
decision problem RMTDP fixed role-execution policy least hard
2. explicit analysis communication please refer work done Pynadath Tambe (2002)
Goldman et al. (2003).

379

fiNair & Tambe

decision problem MTDP. Furthermore, given Theorem 1, conclude
NEXP-Completeness.
result suggests even fixing role-execution policy, solving RMTDP
optimal role-taking policy still intractable. Note Theorem 2 refers completely
general globally optimal role-taking policy, number agents change roles
point time. Given result, general globally optimal role-taking policy
likely doubly exponential complexity, may left choice run
brute-force policy search, i.e. enumerate role-taking policies evaluate
them, together determinethe run-time finding globally optimal policy.
number policies

||

||T 1
||1

n

, i.e. doubly exponential number observation

histories number agents. Thus, RMTDP enables quantitative evaluation
teams policies, computing optimal policies intractable; furthermore, given low level
abstraction, contrast TOP, difficult human understand optimal policy.
contrast RMTDP TOP root hybrid model described
following section.

4. Hybrid BDI-POMDP Approach
explained TOP RMTDP, present detailed view
hybrid methodology quantitatively evaluate TOP. first provide detailed
interpretation Figure 1. BDI team plans essentially TOP plans, BDI
interpreter TOP coordination layer. shown Figure 1, RMTDP model
constructed corresponding domain TOP interpreter converted
corresponding (incomplete) RMTDP policy. analyze TOP using
analysis techniques rely evaluating RMTDP policy using RMTDP model
domain.
Thus, hybrid approach combines strengths TOPs (enabling humans
specify TOPs coordinate large-scale teams) strengths RMTDP (enabling
quantitative evaluation different role allocations). one hand, synergistic
interaction enables RMTDPs improve performance TOP-based BDI teams.
hand, identified least six specific ways TOPs make easier
build RMTDPs efficiently search RMTDP policies: two discussed
section, four next section. particular, six ways are:
1. TOPs exploited constructing RMTDP models domain (Section 4.1);
2. TOPs exploited present incomplete policies RMTDPs, restricting RMTDP
policy search (Section 5.1);
3. TOP belief representation exploited enabling faster RMTDP policy evaluation
(Section 4.2);
4. TOP organization hierarchy exploited hierarchically grouping RMTDP policies
(Section 5.1);
5. TOP plan hierarchy exploited decomposing RMTDPs (Section 5.3);
380

fiHybrid BDI-POMDP Framework Multiagent Teaming

6. TOP plan hierarchies exploited cutting observation belief
histories RMTDPs (Section 5.3).
end result efficient policy search completed RMTDP policy improves
TOP performance. exploit TOP framework, frameworks tasking
teams, e.g. Decker Lesser (1993) Stone Veloso (1999) could benefit
similar synergistic interaction.
4.1 Guidelines Constructing RMTDP
shown Figure 1, analysis approach uses input RMTDP model domain,
well incomplete RMTDP policy. Fortunately, TOP serve
direct mapping RMTDP policy, utilized actually constructing
RMTDP model domain. particular, TOP used determine
domain features important model. addition, structure TOP
exploited decomposing construction RMTDP.
elements RMTDP tuple, hS, A, P, , O, R, RLi, defined using procedure relies TOP well underlying domain. procedure
automated, key contribution recognizing exploitation TOP structures
constructing RMTDP model. First, order determine set states, S,
critical model variables tested pre-conditions, termination conditions
context components (i.e. sub-plans) TOP. Note state needs
model features tested TOP; TOP pre-condition expresses complex test
feature, test modeled state, instead gets used defining
incomplete policy input RMTDP. Next define set roles, RL, leaf-level
roles organization hierarchy TOP. Furthermore, specified Section 3.2,
define state feature roles vector containing current role agent.
defined RL roles , define actions, follows. role rj RL,
define corresponding role-taking action, irj succeed fail depending
agent performs action state action performed in.
role-execution actions, irj agent role rj , allowed role according
TOP.
Thus, defined S, RL based TOP. illustrate steps, consider
plans Figure 4(b). pre-conditions leaf-level plan ScoutRoute1 (See
Appendix A), instance, tests start location helicopters start location X,
termination conditions test scouts end location Y. Thus, locations
helicopters modeled features set states RMTDP. Using
organization hierarchy, define set roles RL role corresponding
four different kinds leaf-level roles, i.e. RL = {memberSctT eamA, memberSctT eamB,
memberSctT eamC, memberT ransportT eam}. role-taking role-execution actions
defined follows:
role-taking action defined corresponding four roles RL, i.e.
becoming member one three scouting teams transport team.
domain specifies transport change scout thus role-taking
action, jointTransportTeam, fail agent i, current role agent scout.
381

fiNair & Tambe

Role-execution actions obtained TOP plans corresponding agents
role. mission rehearsal scenario, agent, fulfilling scout role (members
SctTeamA, SctTeamB SctTeamC), always goes forward, making current
position safe, reaches destination execution action
consider move-making-safe. agent transport role (members Transport
Team) waits X obtains observation signal one scouting sub-team
reached hence role-execution actions wait move-forward.
must define , P, O, R. obtain set observations agent
directly domain. instance, transport helos may observe status scout
helos (normal destroyed), well signal path safe. Finally, determining
functions, P, O, R requires combination human domain expertise empirical
data domain behavior. However, shown later Section 6, even approximate
model transitional observational uncertainty sufficient deliver significant benefits. Defining reward transition function may sometimes require additional state
variables modeled, implicitly modeled TOP. mission
rehearsal domain, time scouting transport mission completed
determined amount reward. Thus, time implicitly modeled TOP
needed explicitly modeled RMTDP.
Since interested analyzing particular TOP respect uncertainty,
procedure constructing RMTDP model simplified exploiting hierarchical decomposition TOP order decompose construction RMTDP
model. high-level components TOP often represent plans executed different
sub-teams, may loosely interact other. Within component,
sub-team members may exhibit tight interaction, focus loose coupling
across components, end results one component feed another,
components independently contribute team goal. Thus, procedure constructing RMTDP exploits loose coupling components plan hierarchy
order build RMTDP model represented combination smaller RMTDPs (factors). Note decomposition infeasible, approach still applies except
benefits hierarchical decomposition unavailable.
classify sibling components either parallel sequentially executed (contains temporal constraint). Components executed parallel could either independent
dependent. independent components, define RMTDPs
components sub-team executing one component cannot affect transitions, observations reward obtained sub-teams executing components. procedure determining elements RMTDP tuple component k,
hSk , Ak , Pk , k , Ok , Rk , RLk i, identical procedure described earlier constructing
overall RMTDP. However, component smaller set relevant variables
roles hence specifying elements corresponding RMTDP easier.
combine RMTDPs independent components obtain
RMTDP corresponding higher-level component. higher level component l,
whose child
components independent, set states, Sl = x FSl x
FSl = k s.t. Child(k,l)=true FSk FSl FSk sets features set
states Sl set states Sk . state sl Sl said correspond state
sk Sk x FSk , sl [x ] = sk [x ], i.e. state sl value state sk
382

fiHybrid BDI-POMDP Framework Multiagent Teaming


defined follows, Pl (sl , al , sl ) =
Q features state sk . transition function

k s.t. Child(k,l)=true Pk (sk , ak , sk ), sl sl component l corresponds states
sk sk component k ak joint action performed sub-team assigned component k corresponding joint action al performed sub-team
assigned
component l. observation function defined similarly Ol (sl , al , l ) =
Q
Ok (sk , ak , k ). reward function component l defined
k s.t. Child(k,l)=true
P
Rl (sl , al ) = k s.t. Child(k,l)=true Rk (sk , ak ).

case sequentially executed components (those connected temporal constraint), components loosely coupled since end states preceding component
specify start states succeeding component. Thus, since one component
active time, transition function defined follows, Pl (sl , al , sl ) = Pk (sk , ak , sk ),
component k active child component, sk sk represent states
component k corresponding states sl sl component l ak joint action
performed sub-team assigned component k corresponding joint action
al performed sub-team corresponding component l. Similarly, define
Ol (sl , al , l ) = Ok (sk , ak , k ) Rl (sl , al ) = Rk (sk , ak ), k active child
component.

Consider following example mission rehearsal domain components
exhibit sequential dependence parallel independence. Concretely, component
DoScouting executed first followed DoTransport RemainingScouts,
parallel independent hence, either DoScouting active DoTransport
RemainingScouts active point execution. Hence, transition, observation reward functions parent Execute Mission given corresponding
functions either DoScouting combination corresponding functions
DoTransport RemainingScouts.
use top-down approach order determine construct factored RMTDP
plan hierarchy. shown Algorithm 1, replace particular sub-plan
constituent sub-plans either independent sequentially executed. not,
RMTDP defined using particular sub-plan. process applied recursively
starting root component plan hierarchy. concrete example, consider
mission rehearsal simulation domain hierarchy illustrated Figure 4(b).
Given temporal constraints DoScouting DoTransport, DoScouting RemainingScouts, exploited sequential decomposition, DoTransport
RemainingScouts parallel independent components. Hence, replace
ExecuteMission DoScouting, DoTransport RemainingScouts. apply process DoScouting. constituent components DoScouting
neither independent sequentially executed thus DoScouting cannot replaced
constituent components. Thus, RMTDP mission rehearsal domain comprised
smaller RMTDPs DoScouting, DoTransport RemainingScouts.
Thus, using TOP identify relevant variables building factored RMTDP
utilizing structure TOP decompose construction procedure, reduce load
domain expert model construction. Furthermore, shown Section 5.3,
factored model greatly improves performance search best role allocation.
383

fiNair & Tambe

Algorithm 1 Build-RMTDP(TOP top, Sub-plan subplan)
1: children subplanchildren() {subplanchildren() returns sub-plans within subplan}
2: children = null children (loosely coupled independent)
3:
rmtdp Define-RMTDP(subplan) {not automated}
4:
return rmtdp
5: else
6:
child children
7:
factors[child] Build-RMTDP(top,child)
8:
rmtdp ConstructFromFactors(factors)
9:
return rmtdp

4.2 Exploiting TOP Beliefs Evaluation RMTDP Policies
present technique exploiting TOPs speeding evaluation RMTDP
policies. explain improvement, first describe original algorithm
determining expected reward joint policy, local policies agent
indexed entire observation histories (Pynadath & Tambe, 2002; Nair, Pynadath,
Yokoo, Tambe, & Marsella, 2003a). Here, obtain RMTDP policy TOP
follows. obtain (~
), i.e. action performed agent observation history

~i , action performed agent following TOP set privately
held beliefs corresponding observation history, ~it . compute expected reward
RMTDP policy projecting teams execution possible branches
different world states different observations. time step, compute
expected value joint policy, =< 1 , . . . , n >, team starting given state, st ,
given set past observations,
~ 1t , . . . ,
~ nt , follows:
X











~ nt ) = R(st , 1 (~
Vt (st , ~1t , . . . ,
1t ), . . . , n (~nt ) ) +
P , 1
~ 1t , . . . , n
~ nt , st+1
st+1

X









st+1, 1
~ 1t , . . . , n
~ nt , 1t+1, . . . , nt+1 Vt+1 st+1 , ~1t+1 , . . . ,
~ nt+1

(1)

t+1

expected reward joint policy given V0 (s0 , < null, . . . , null >) s0
start state. time step t, computation Vt performs summation
possible world states agent observations time complexity (|S| ||).
computation
repeated states observation histories length t, i.e.

|S| ||t times. Therefore,
given time horizon , overall complexity algo
2

+1
rithm |S| ||
.
discussed Section 2.2, team-oriented program, agents action selection
based currently held private beliefs (note mutual beliefs modeled
privately held beliefs agents per footnote 2). similar technique
exploited mapping TOP RMTDP policy. Indeed, evaluation RMTDP
policy corresponds TOP speeded agents local policy indexed
private beliefs, . refer , TOP-congruent belief state agent
384

fiHybrid BDI-POMDP Framework Multiagent Teaming

RMTDP. Note belief state probability distribution world
states single agent POMDP, rather privately held beliefs (from BDI
program) agent time t. similar idea representing policy
finite-state controller (Hansen & Zhou, 2003; Poupart & Boutilier, 2003). case,
private beliefs would map states finite-state controller.
Belief-based RMTDP policy evaluation leads speedup multiple observation
histories map belief state, . speedup key illustration exploitation
synergistic interactions TOP RMTDP. instance, belief representation techniques used TOP reflected RMTDP, resulting faster policy evaluation
help us optimize TOP performance. detailed example belief state presented later
brief explanation belief-based RMTDP policies evaluated.
evaluation using observation histories, compute expected reward
belief-based policy projecting teams execution possible branches
different world states different observations. time step, compute
expected value joint policy, =< 1 , . . . , n >, team starting given state, st ,
given team belief state, < 1t , . . . , nt > follows:





X





Vt (st , 1t . . . nt ) = R(st , 1 (1t ), . . . , n (nt ) ) + P st , 1 1t , . . . , n nt , st+1
st+1

X









st+1, 1 1t , . . . , n nt , 1t+1, . . . , nt+1 Vt+1 st+1 , 1t+1 , . . . , nt+1

t+1

(2)


t+1

= BeliefUpdateFunction



, it+1



complexity computing function (expression 2) (|S| ||) BF , BF
represents complexity belief update function, BeliefUpdateFunction.
time step computation value function done every state possible
reachable belief states. Let |i | = max1tT (|it |) represent maximum number
possible belief states agent point time, |it | number
belief states agent t. Therefore complexity algorithm
given O(|S|2 || (|1 | . . . |n |) ) BF . Note that, algorithm
exponent unlike algorithm expression 1. Thus, evaluation method
give large time savings if: (i) quantity (|1 | . . . |n |) much less ||T
(ii) belief update cost low. practical BDI systems, multiple observation histories
map often onto belief state, thus usually, (|1 | . . . |n |) much less
||T . Furthermore, since belief update function mirrors practical BDI systems,
complexity low polynomial constant. Indeed, experimental results
show significant speedups result switching TOP-congruent belief states
. However, absolute worst case, belief update function may simply append
new observation history past observations (i.e., TOP-congruent beliefs
equivalent keeping entire observation histories) thus belief-based evaluation
complexity observation history-based evaluation.
turn example belief-based policy evaluation mission rehearsal
domain. time step, transport helicopters may receive observation
385

fiNair & Tambe

whether scout failed based observation function. use observationhistory representation policy, transport agent would maintain complete
history observations could receive time step. example, setting
two scout helicopters, one route 1 route 2, particular transport
helicopter may several different observation histories length two. every time step,
transports may receive observation scout alive failed.
Thus, time = 2, transport helicopter might one following observation histories length two, < {sct1OnRoute1Alive, sct2OnRoute2Alive}1 , {sct1OnRoute1F ailed,
sct2OnRoute2F ailed}2 >, < {sct1OnRoute1Alive, sct2OnRoute2F ailed}1 , {sct1OnRoute1
F ailed}2 >, < {sct1OnRoute1F ailed, sct2OnRoute2Alive}1 , {sct2OnRoute2F ailed}2 >,
etc. However, action selection transport helicopters depends whether
critical failure (i.e. last remaining scout crashed) taken place change
role. Whether failure critical determined passing observation
belief-update function. exact order observations received
precise times failure non-failure observations received relevant
determining critical failure taken place consequently whether transport
change role scout. Thus, many observation histories map onto
belief states. example, three observation histories map belief
CriticalF ailure(DoScouting) i.e. critical failure taken place. results significant speedups using belief-based evaluation, Equation 2 needs executed
smaller number belief states, linear domains, opposed observation
history-based evaluation, Equation 1 executed exponential number observation histories (||T ). actual speedup obtained mission rehearsal domain
demonstrated empirically Section 6.

5. Optimizing Role Allocation
Section 4 focused mapping domain interest onto RMTDP algorithms
policy evaluation, section focuses efficient techniques RMTDP policy search,
service improving BDI/TOP team plans. TOP essence provides incomplete,
fixed policy, policy search optimizes decisions left open incomplete policy;
policy thus completed optimizes original TOP (see Figure 1). enabling RMTDP
focus search incomplete policies, providing ready-made decompositions,
TOPs assist RMTDPs quickly searching policy space, illustrated
section. focus, particular, problem role allocation (Hunsberger & Grosz,
2000; Modi, Shen, Tambe, & Yokoo, 2003; Tidhar et al., 1996; Fatima & Wooldridge, 2001),
critical problem teams. TOP provides incomplete policy, keeping open
role allocation decision agent, RMTDP policy search provides optimal
role-taking action role allocation decision points. contrast previous
role allocation approaches, approach determines best role allocation, taking
consideration uncertainty domain future costs. Although demonstrated
solving role allocation problem, methodology general enough apply
coordination decisions.
386

fiHybrid BDI-POMDP Framework Multiagent Teaming

5.1 Hierarchical Grouping RMTDP Policies
mentioned earlier, address role allocation, TOP provides policy complete,
except role allocation decisions. RMTDP policy search optimally fills
role allocation decisions. understand RMTDP policy search, useful gain
understanding role allocation search space. First, note role allocation focuses
deciding many types agents allocate different roles organization
hierarchy. role allocation decision may made time = 0 may made
later time conditioned available observations. Figure 9 shows partially expanded role
allocation space defined TOP organization hierarchy Figure 4(a) six helicopters.
node role allocation space completely specifies allocation agents roles
corresponding level organization hierarchy (ignore now, number
right node). instance, root node role allocation space specifies
six helicopters assigned Task Force (level one) organization hierarchy
leftmost leaf node (at level three) Figure 9 specifies one helicopter assigned
SctTeamA, zero SctTeamB, zero SctTeamC five helicopters Transport Team.
Thus, see, leaf node role allocation space complete, valid role
allocation agents roles organization hierarchy.
order determine one leaf node (role allocation) superior another evaluate
using RMTDP constructing RMTDP policy each. particular
example, role allocation specified leaf node corresponds role-taking actions
agent execute time = 0. example, case leftmost leaf
Figure 9, time = 0, one agent (recall Section 2.2 homogeneous team
hence specific agent matter) become member SctTeamA
agents become members Transport Team. Thus, one agent i, roletaking policy include (null) = joinSctT eamA agents, j, j 6= i,
include j (null) = joinT ransportT eam. case, assume rest
role-taking policy, i.e. roles reallocated scout fails, obtained role
reallocation algorithm BDI/TOP interpreter, STEAM algorithm (Tambe
et al., 2000). Thus example, role reallocation indeed performed STEAM
algorithm, STEAMs reallocation policy included incomplete policy
RMTDP initially provided. Thus, best role allocation computed keeping
mind STEAMs reallocation policy. STEAM, given failure agent playing RoleF ,
agent playing RoleR replace if:
Criticality (RoleF ) Criticality (RoleR ) > 0
Criticality (x) = 1 x critical; = 0 otherwise

Thus, based agents observations, critical failure taken place,
replacing agents decision replace computed using expression
included incomplete policy input RMTDP. Since incomplete
policy completed role allocation leaf node using technique above,
able construct policy RMTDP corresponds role allocation.
domains RoboCupRescue, allocation decisions made time
= 0. domains, possible role allocation conditioned observations
(or communication) obtained course execution. instance,
shown Figure 8(a), RoboCupRescue scenario, ambulances allocated
sub-team AmbulanceTeamA AmbulanceTeamB information location
387

fiNair & Tambe

6

0

6

[0]
6

1

6

[4167]
5

2

6

[3420]
4

3

6

[2773]
3

4

6

[1926]
2

5

6

[1179]
1

6

6

[432]
0

6 1359.57
6 2926.08
6 1500.12
6 613.81
2 4
2 4
1 5
1 5
1 1 0
0 0 2
0 0 1
1 0 0

Figure 9: Partially expanded role allocation space mission rehearsal domain(six helos).

civilians conveyed fire engines. allocation ambulances
conditioned communication, i.e. number civilians location.
Figure 10 shows partially expanded role allocation scaled-down rescue scenario
three civilians, two ambulances two fire engines (one station 1
station 2). Figure, 1;1;2 depicts fact two ambulances,
one fire engine station. shown, level allocation fire engines
EngineTeamA EngineTeamB gives number engines assigned
EngineTeam station. next level (leaf level) different leaf nodes
possible assignment ambulances AmbulanceTeamA AmbulanceTeamB depending
upon value communication c. Since three civilians exclude
case civilians present particular fire, two possible messages i.e.
one civilian fire 1 two civilians fire 1 (c = 1 2).
TaskForce=1;1;2
1;1;2

1;1;2

EngineTeamA=0;1 EngineTeamB=1;0 AmbTeam=2

c=1

EngineTeamA=1;0 EngineTeamB=0;1 AmbTeam=2

1;1;2

1;1;2

0;1 1;0 2

0;1 1;0 2

c=2

c=1

AmbTeamA=2 AmbTeamB=0 AmbTeamA=1 AmbTeamB=1

c=2

AmbTeamA=1 AmbTeamB=1 AmbTeamA=1 AmbTeamB=1

Figure 10: Partially expanded role allocation space Rescue domain (one fire engine
station 1, one fire engine station 2, two ambulances, three civilians).
thus able exploit TOP organization hierarchy create hierarchical
grouping RMTDP policies. particular, leaf node represents complete
RMTDP policy (with role allocation specified leaf node), parent node
represents group policies. Evaluating policy specified leaf node equivalent
evaluating specific role allocation taking future uncertainties account. could
388

fiHybrid BDI-POMDP Framework Multiagent Teaming

brute force search role allocations, evaluating order determine
best role allocation. However, number possible role allocations exponential
leaf roles organization hierarchy. Thus, must prune search space.
5.2 Pruning Role Allocation Space
prune space valid role allocations using upper bounds (MaxEstimates)
parents leaves role allocation space admissible heuristics (Section 5.3).
leaf role allocation space represents completely specified policy MaxEstimate upper bound maximum value policies parent node
evaluated using RMTDP. obtain MaxEstimates parent nodes (shown
brackets right parent node Figure 9), use branch-and-bound style
pruning (see Algorithm 2). discuss Algorithm 2 below, note essence
performs branch-and-bound style pruning; key novelty step 2 discuss
Section 5.3.
branch-and-bound algorithm works follows: First, sort parent nodes
estimates start evaluating children parent highest MaxEstimate (Algorithm 2: steps 3-13). Evaluate(RMTDP, child) refers evaluation
leaf-level policy, child, using RMTDP model. evaluation leaf-level policies (step
13) done using either methods described Section 4. case
role allocation space Figure 9, would start evaluating leaves parent
node one helicopter Scouting Team five Transport Team. value
evaluating leaf node shown right leaf node. obtained
value best leaf node (Algorithm 2: steps 14,15), case 1500.12, compare
MaxEstimates parents role allocation space (Algorithm 2:
steps 16-18). see Figure 9 would result pruning three parent nodes
(leftmost parent right two parents) avoid evaluation 65 84 leaf-level
policies. Next, would proceed evaluate leaf nodes parent
two helos Scouting Team four Transport Team. would result pruning
remaining unexpanded parent nodes return leaf highest value,
case node corresponding two helos allocated SctTeamA four
Transport Team. Although demonstrated 3-level hierarchy, methodology
applying deeper hierarchies straightforward.
5.3 Exploiting TOP Calculate Upper Bounds Parents
discuss upper bounds parents, called MaxEstimates, calculated parent. MaxEstimate parent defined strict upper bound
maximum expected reward leaf nodes it. necessary
MaxEstimate upper bound else might end pruning potentially useful role
allocations. order calculate MaxEstimate parent could evaluate
leaf nodes using RMTDP, would nullify benefit subsequent pruning. We, therefore, turn TOP plan hierarchy (see Figure 4(b)) break
evaluation parent node components, evaluated separately thus
decomposing problem. words, approach exploits structure BDI
program construct small-scale RMTDPs unlike decomposition techniques
389

fiNair & Tambe

Algorithm 2 Branch-and-bound algorithm policy search.
1: Parents list parent nodes
2: Compute MAXEXP(Parents) {Algorithm 3}
3: Sort Parents decreasing order MAXEXP
4: bestVal
5: parent Parents
6:
done[parent] false; pruned[parent] false
7: parent Parents
8:
done[parent] = false pruned[parent] = false
9:
child parentnextChild() {child leaf-level policy parent}
10:
child = null
11:
done[parent] true
12:
else
13:
childVal Evaluate(RMTDP,child)
14:
childVal > bestVal
15:
bestVal childVal;best child
16:
parent1 Parents
17:
MAXEXP[parent1] < bestVal
18:
pruned[parent1] true
19: return best

assume decomposition ultimately rely domain experts identify interactions
agents reward transition functions (Dean & Lin, 1995; Guestrin, Venkataraman,
& Koller, 2002).
parent role allocation space, use small-scale RMTDPs evaluate values TOP component. Fortunately, discussed Section 4.1,
exploited small-scale RMTDPs corresponding TOP components constructing larger
scale RMTDPs. put small-scale RMTDPs use again, evaluating policies within
component obtain upper bounds. Note evaluation leaf-level
policies, evaluation components parent node done using either
observation histories (see Equation 1) belief states (see Equation 2). describe
section using observation history-based evaluation method computing values
components parent, summed obtain MaxEstimate (an
upper bound childrens values). Thus, whereas parent role allocation space
represents group policies, TOP components (sub-plans) allow component-wise
evaluation group obtain upper bound expected reward policy
within group.
Algorithm 3 exploits smaller-scale RMTDP components, discussed Section 4.1,
obtain upper bounds parents. First, order evaluate MaxEstimate
parent node role allocation space, identify start states component
evaluate RMTDPs. explain step using parent node Figure 9
Scouting Team = two helos, Transport Team = four helos (see Figure 11). first
component preceding components, start states corresponds
start states policy TOP mapped onto. next
390

fiHybrid BDI-POMDP Framework Multiagent Teaming

components next component one linked sequential dependence
start states end states preceding component. However, explained later
section, significantly reduce list start states component
evaluated.
Algorithm 3 MAXEXP method calculating upper bounds parents role allocation space.
1: parent search space
2:
MAXEXP[parent] 0
3:
component corresponding factors RMTDP Section 4.1
4:
component preceding component j
5:
Obtain start states, states[i] endStates[j]
6:
states[i] removeIrrelevantFeatures(states[i]) {discard features present
Si }
7:
Obtain corresponding observation histories start OHistories[i]
endOHistories[j]
8:
OHistories[i] removeIrrelevantObservations(OHistories[i])
9:
else
10:
Obtain start states, states[i]
11:
Observation histories start OHistories[i] null
12:
maxEval[i] 0
13:
leaf-level policies parent
14:
maxEval[i] max(maxEval[i], maxsi states[i],ohi OHistories[i](Evaluate(RM DPi ,
si , ohi , )))
+
15:
MAXEXP[parent] maxEval[i]
Similarly, starting observation histories component observation histories completing preceding component (no observation history first
component). BDI plans normally refer entire observation histories rely
key beliefs typically referred pre-conditions component.
starting observation history shortened include relevant observations,
thus obtaining reduced list starting observation sequences. Divergence private observations problematic, e.g. cause agents trigger different team plans.
indicated earlier Section 2.2, TOP interpreters guarantee coherence
key aspects observation histories. instance, discussed earlier, TOP interpreter
ensures coherence key beliefs initiating terminating team plans TOP; thus
avoiding divergence observation histories.
order compute maximum value particular component, evaluate
possible leaf-level policies within component possible start states observation histories obtain maximum (Algorithm 3:steps 13-14). evaluation,
store end states ending observation histories used
evaluation subsequent components. shown Figure 11, evaluation
DoScouting component parent node two helicopters assigned
Scouting Team four helos Transport Team, leaf-level policies correspond
possible ways helicopters could assigned teams SctTeamA, SctTeamB, Sct391

fiNair & Tambe

TeamC Transport Team, e.g. one helo SctTeamB, one helo SctTeamC four
helos Transport Team, two helos SctTeamA four helos Transport Team, etc.
role allocation tells agents role take first step. remainder
role-taking policy specified role replacement policy TOP infrastructure
role-execution policy specified DoScouting component TOP.
obtain MaxEstimate parent node role allocation space, simply
sum maximum values obtained component (Algorithm 3:steps 15), e.g.
maximum values component (see right component Figure 11)
summed obtain MaxEstimate (84 + 3330 + 36 = 3420). seen Figure 9, third
node left indeed upper bound 3420.
calculation MaxEstimate parent nodes much faster
evaluating leaf nodes cases two reasons. Firstly, parent nodes
evaluated component-wise. Thus, multiple leaf-level policies within one component result
end state, remove duplicates get start states next component. Since component contains state features relevant it, number
duplicates greatly increased. duplication evaluation effort cannot avoided
leaf nodes, policy evaluated independently start finish. instance, DoScouting component, role allocations, SctTeamA=1, SctTeamB=1,
SctTeamC=0, TransportTeam=4 SctTeamA=1, SctTeamB=0, SctTeamC=1, TransportTeam=4, end states common eliminating irrelevant features
scout SctTeamB former allocation scout SctTeamC latter allocation fail. feature elimination (Algorithm 3:steps 6),
state features retained DoTransport scouted route number transports
(some transports may replaced failed scouts) shown Figure 11.
second reason computation MaxEstimates parents much faster
number starting observation sequences much less number ending observation histories preceding components. observations
observation histories component relevant succeeding components (Algorithm 3:steps 8). Thus, function removeIrrelevantObservations reduces number
starting observation histories observation histories preceding component.
refer methodology obtaining MaxEstimates parent MAXEXP. variation this, maximum expected reward failures (NOFAIL),
obtained similar fashion except assume probability agent failing 0. able make assumption evaluating parent node, since
focus obtaining upper bounds parents, obtaining exact value.
result less branching hence evaluation component proceed much
quicker. NOFAIL heuristic works evaluation policy without failures
occurring higher evaluation policy failures possible.
normally case domains. evaluation NOFAIL heuristics
role allocation space six helicopters shown square brackets Figure 9.
following theorem shows MAXEXP method finding upper bounds
indeed finds upper bound thus yields admissible search heuristic branchand-bound search role allocation space.
Theorem 3 MAXEXP method always yield upper bound.
392

fiHybrid BDI-POMDP Framework Multiagent Teaming

[84]
DoScouting
[ScoutingTeam=2,TransportTeam=4]

Alloc:
SctTeamA=2
SctTeamB=0
SctTeamC=0
TransportTeam=4

Alloc:
SctTeamA=0
SctTeamB=1
SctTeamC=1
TransportTeam=4

[3300]
DoTransport
[TransportTeam=4]

StartState:
RouteScouted=1
Transports=4

[36]
RemainingScouts
[ScoutTeam=2]

StartState:
RouteScouted=1
Transports=3

StartState:
RouteScouted=1
Transports=0

Figure 11: Component-wise decomposition parent exploiting TOP.

Proof: See Appendix C.
Theorem 3, conclude branch-and-bound policy search algorithm
always find best role allocation, since MaxEstimates parents true
upper bounds. Also, help Theorem 4, show worst case,
branch-and-bound policy search complexity brute force search.
Theorem 4 Worst-case complexity evaluating single parent node using MAXEXP
evaluating every leaf node within constant factor.
Proof sketch:
worst case complexity MAXEXP arises when:
1. Let ESj end states component j executing policy removing
features irrelevant succeeding component k. Similarly, let ESj
end states component j executing policy
Tremoving features
irrelevant succeeding component k. ESj ESj = null
duplication end states occur.
2. Let OHj ending observation histories component j executing policy
removing observations irrelevant succeeding component
k. Similarly, let OHj ending observation histories component j executing policy removing observation
histories irrelevant

succeeding component k. OHj OHj = null duplication
observation histories occur. Note belief-based evaluation used
would replace observation histories TOP congruent belief states
(see Sect 4).
case, computational advantage evaluating components
MaxEstimate separately. Thus, equivalent evaluating child node
parent. Thus, worst case, MAXEXP computation parent
evaluating children within constant factor.
addition, worst case, pruning result using MAXEXP every
leaf node need evaluated. equivalent evaluating leaf node twice.
393

fiNair & Tambe

Thus, worst case complexity branch-and-bound search using MAXEXP
finding best role allocation evaluating every leaf node. refer
brute-force approach NOPRUNE. Thus, worst case complexity MAXEXP
NOPRUNE. However, owing pruning savings decomposition computation MaxEstimates, significant savings likely average
case. Section 6 highlights savings mission rehearsal RoboCupRescue
domains.

6. Experimental Results
section presents four sets results context two domains introduced
Section 2.1, viz. mission rehearsal RoboCupRescue (Kitano et al., 1999). First,
investigated empirically speedups result using TOP-congruent belief
states (belief-based evaluation) observation history-based evaluation using
algorithm Section 5 brute-force search. focus determining
best assignment agents roles; assume fixed TOP TOP infrastructure.
Second, conducted experiments investigate benefits considering uncertainty
determining role allocations. this, compared allocations found RMTDP
role allocation algorithm (i) allocations consider kind uncertainty,
(ii) allocations consider observational uncertainty consider action
uncertainty. Third, conducted experiments domains determine sensitivity
results changes model. Fourth, compare performance allocations
found RMTDP role allocation algorithm allocations human subjects
complex domains RoboCupRescue simulations.
6.1 Results Mission Rehearsal Domain
mission rehearsal domain, TOP one discussed Section 2.2.
seen Figure 4(a), organization hierarchy requires determining number agents
allocated three scouting sub-teams remaining helos must allocated
transport sub-team. Different numbers initial helicopters attempted, varying
three ten. details RMTDP constructed domain given
Appendix B. probability failure scout time step routes 1, 2 3
0.1, 0.15 0.2, respectively. probability transport observing alive scout
routes 1, 2 3 0.95, 0.94 0.93, respectively. False positives possible,
i.e. transport observe scout alive failed. probability
transport observing scout failure routes 1, 2 3 0.98, 0.97 0.96, respectively.
too, false positives possible hence transport observe failure
unless actually taken place.
Figure 12 shows results comparing different methods searching role
allocation space. show four methods. method adds new speedup techniques
previous:
1. NOPRUNE-OBS: brute force evaluation every role allocation determine
best. Here, agent maintains complete observation history evaluation
algorithm Equation 1 used. ten agents, RMTDP projected
394

fiHybrid BDI-POMDP Framework Multiagent Teaming

order 10,000 reachable states order 100,000 observation histories
per role allocation evaluated (thus largest experiment category limited
seven agents).
2. NOPRUNE-BEL: brute force evaluation every role allocation. difference
method NOPRUNE-OBS use belief-based evaluation
algorithm (see Equation 2).
3. MAXEXP: branch-and-bound search algorithm described Section 5.2
uses upper bounds evaluation parent nodes find best allocation.
Evaluation parent leaf nodes uses belief-based evaluation.
4. NOFAIL: modification branch-and-bound heuristic mentioned Section 5.3.
essence MAXEXP, except upper bounds computed making
assumption agents fail. heuristic correct domains
total expected reward failures always less failures present
give significant speedups agent failures one primary sources
stochasticity. method, too, evaluation parent leaf nodes uses
belief-based evaluation. (Note upper bounds computed using
no-failure assumption changes assumed actual domains.)
Figure 12(a), Y-axis number nodes role allocation space evaluated
(includes leaf nodes well parent nodes), Figure 12(b) Y-axis represents
runtime seconds logarithmic scale. figures, vary number agents
X-axis. Experimental results previous work using distributed POMDPs often
restricted two agents; exploiting hybrid models, able vary number
agents three ten shown Figure 12(a). clearly seen Figure 12(a),
pruning, significant reductions obtained MAXEXP NOFAIL NOPRUNEBEL terms numbers nodes evaluated. reduction grows quadratically
10-fold ten agents.3 NOPRUNE-OBS identical NOPRUNE-BEL terms
number nodes evaluated, since methods leaf-level policies evaluated,
method evaluation differs. important note although NOFAIL
MAXEXP result number nodes evaluated domains,
necessarily true always. general, NOFAIL evaluate least many nodes
MAXEXP since estimate least high MAXEXP estimate. However,
upper bounds computed quicker NOFAIL.
Figure 12(b) shows NOPRUNE-BEL method provides significant speedup
NOPRUNE-OBS actual run-time. instance, 12-fold speedup using
NOPRUNE-BEL instead NOPRUNE-OBS seven agent case (NOPRUNE-OBS
could executed within day problem settings greater seven agents).
empirically demonstrates computational savings possible using belief-based evaluation instead observation history-based evaluation (see Section 4). reason,
use belief-based evaluation MAXEXP NOFAIL approaches
3. number nodes NOPRUNE eight agents obtained experiments, rest
calculated using formula [m]n /n! = (m + n 1) . . . m/n!, represents number
heterogeneous role types n number homogeneous agents. [m]n = (m + n 1) . . .
referred rising factorial.

395

fiNair & Tambe

remaining experiments paper. MAXEXP heuristic results 16-fold speedup
NOPRUNE-BEL eight agent case.
NOFAIL heuristic quick compute upper bounds far outperforms
MAXEXP heuristic (47-fold speedup MAXEXP ten agents). Speedups
MAXEXP NOFAIL continually increase increasing number agents. speedup
NOFAIL method MAXEXP marked because, domain, ignoring
failures results much less branching.
350

NOFAIL, MAXEXP

Number nodes

300

NOPRUNE-OBS,
NOPRUNE-BEL

250
200
150
100
50
0

3

4

5

6

7

8

9

10

Number agents
100000

MAXEXP
NOFAIL
NOPRUNE-BEL
NOPRUNE-OBS

Time secs (log scale)

10000
1000
100
10
1
0.1
0.01

3

4

5

6

7

8

9

10

Number agents

Figure 12: Performance role allocation space search mission rehearsal domain, a) (left)
Number nodes evaluated, b) (right)Run-time seconds log scale.

Next, conducted experiments illustrating importance RMTDPs reasoning
action observation uncertainties role allocations. this, compared
allocations found RMTDP role allocation algorithm allocations found using two
different methods (see Figure 13):
1. Role allocation via constraint optimization (COP) (Modi et al., 2003; Mailler & Lesser,
2004) allocation approach: COP approach4 , leaf-level sub-teams or4. Modi et al.s work (2003) focused decentralized COP, investigation emphasis
resulting role allocation generated COP, decentralization per se.

396

fiHybrid BDI-POMDP Framework Multiagent Teaming

ganization hierarchy treated variables number helicopters
domain variable (thus, domain may 1, 2, 3,..helicopters).
reward allocating agents sub-teams expressed terms constraints:
Allocating helicopter scout route assigned reward corresponding
routes distance ignoring possibility failure (i.e. ignoring transition
probability). Allocating helicopters subteam obtained proportionally higher reward.
Allocating helicopter transport role assigned large reward transporting cargo destination. Allocating helicopters subteam
obtained proportionally higher reward.
allocating least one scout role assigned reward negative infinity
Exceeding total number agents assigned reward negative infinity
2. RMTDP complete observability: approach, consider transition
probability, ignore partial observability; achieved assuming complete observability RMTDP. MTDP complete observability equivalent
Markov Decision Problem (MDP) (Pynadath & Tambe, 2002) actions
joint actions. We, thus, refer allocation method MDP method.
Figure 13(a) shows comparison RMTDP-based allocation MDP allocation COP allocation increasing number helicopters (X-axis). compare
using expected number transports get destination (Y-axis) metric
comparison since primary objective domain. seen, considering forms uncertainty (RMTDP) performs better considering transition
uncertainty (MDP) turn performs better considering uncertainty (COP).
Figure 13(b) shows actual allocations found three methods four helicopters
six helicopters. case four helicopters (first three bars), RMTDP MDP
identical, two helicopters scouting route 2 two helicopters taking transport role.
COP allocation however consists one scout route 3 three transports.
allocation proves myopic results fewer transports getting destination
safely. case six helicopters, COP chooses one scout helicopter route 3,
shortest route. MDP approach results two scouts route 1,
longest route albeit safest. RMTDP approach, considers observational
uncertainty chooses additional scout route 2, order take care cases
failures scouts go undetected transports.
noted performance RMTDP-based allocation depend
values elements RMTDP model. However, next experiment
revealed, getting values exactly correct necessary. order test sensitivity
performance allocations actual model values, introduced error
various parameters model see allocations found using incorrect model
would perform original model (without errors). emulates situation
model correctly represent domain. Figure 14 shows expected number
transports reach destination (Y-axis) mission rehearsal scenario six
helicopters error (X-axis) introduced various parameters model. instance,
397

fiNair & Tambe

7

Number transports

6
5

RMTDP
COP
MDP

4
3
2
1
0

4

5

6

7

8

Number agents
7
6 helos

RM

xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx

Rt3
Rt2
xxx
xxxRt1
xxx
Transports
xxxx
xxxx
xxxx
xxxx
xxxx

xxxx
xxxx
xxxx
xxxx
xxxx

DP

TD

P

0

xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx

xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx

xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxx



1

xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx

TD
P

xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx

xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx

RM

2

xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx

xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxxxxxx
xxxxxx
xxxxxxxxxx
xxxxxxxxxx


DP

3

xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxxxxxx
xxxxxx
xxxxxxxxxx
xxxxxxxxxx

P

4

CO

Number helos

5

P

xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx

CO

4 helos

6

Figure 13: a) Comparison performance different allocation methods, b)Allocations
found using different allocation methods.

398

fiHybrid BDI-POMDP Framework Multiagent Teaming

percentage error failure rate route 1 (route1-failure-rate) -15%
(i.e. erroneous failure rate 85% actual failure rate) 10%, difference
number transports reached destination (3.498). However
percentage error greater 10%, allocation found conservative resulting
fewer transports getting destination. Similarly, percentage error less
-15%, allocation found risky, scouts assigned, resulting
failures. general, Figure 14 shows model insensitive errors 5 10%
model parameters mission rehearsal domain, model parameters
outside range, non-optimal allocations would result. comparing non-optimal
allocations COP, find always perform better COP range
errors tested (+/-25%) failure rate well observability routes. instance,
error 25% failure rate route 1, RMTDP managed 2.554 transports
safely reach destination, COP managed get 1.997 transports reach safely.
comparing non-optimal allocations MDP, find performed better
MDP within range +/- 25% error observability routes. Thus,
although allocations found using incorrect model non-optimal performed
better COP MDP large ranges errors model. shows getting
model exactly correct necessary find good allocations. thus able
obtain benefits RMTDP even without insisting accurate model.
4

route1-failure-rate
route-2-failure-rate
route3-failure-rate
route1-observability

Number Transports

3.5
3
2.5
2
1.5
1
0.5
0
-25 -20 -15 -10 -5

0

5

10

15

20

25

Percentage error

Figure 14: Model sensitivity mission rehearsal domain.

6.2 Results RoboCupRescue Domain
6.2.1 Speedups RoboCupRescue Domain
next set experiments, highlight computational savings obtained
RoboCupRescue domain. scenario experiment consisted two fires different
locations city. fires different initially unknown number civilians
it, however total number civilians distribution locations
civilians chosen known ahead time. experiment, fix number
civilians five set distribution used choose civilians locations uniform.
number fire engines set five, located three different fire stations described
399

fiNair & Tambe

Section 2.1 vary number ambulances, co-located ambulance center,
two seven. reason chose change number ambulances
small number fire engines unable extinguish fires, changing problem completely.
goal determine fire engines allocate fire information
civilians transmitted, many ambulances send fire location.
Figure 15 highlights savings terms number nodes evaluated actual
runtime increase number agents. show results NOPRUNE-BEL
MAXEXP. NOPRUNE-OBS could run slowness. NOFAIL
heuristic identical MAXEXP since agents cannot fail scenario. RMTDP
case 30,000 reachable states.
Figures 15(a) 15(b), increase number ambulances along Xaxis. Figure 15(a), show number nodes evaluated (parent nodes + leaf nodes)5
logarithmic scale. seen, MAXEXP method results 89-fold
decrease number nodes evaluated compared NOPRUNE-BEL seven
ambulances, decrease becomes pronounced number ambulances
increased. Figure 15(b) shows time seconds logarithmic scale Y-axis
compares run-times MAXEXP NOPRUNE-BEL methods finding best
role allocation. NOPRUNE-BEL method could find best allocation within
day number ambulances increased beyond four. four ambulances (and
five fire engines), MAXEXP resulted 29-fold speedup NOPRUNE-BEL.
6.2.2 Allocation RoboCupRescue
next set experiments shows practical utility role allocation analysis
complex domains. able show significant performance improvements actual
RoboCupRescue domain using role allocations generated analysis. First,
construct RMTDP rescue scenario, described Section 2.1, taking guidance
TOP underlying domain (as described Section 4.1). use
MAXEXP heuristic determine best role allocation. compared RMTDP
allocation allocations chosen human subjects. goal comparing RMTDP
allocations human subjects mainly show RMTDP capable performing
near human expert levels domain. addition, order determine
reasoning uncertainty actually impacts allocations, compared RMTDP
allocations allocations determined two additional allocation methods:
1. RescueISI: Allocations used RoboCupRescue agents entered
RoboCupRescue competitions 2001(RescueISI) (Nair et al., 2002),
finished third place. agents used local reasoning decision making,
ignoring transitional well observational uncertainty.
2. RMTDP complete observability: discussed earlier, complete observability
RMTDP leads MDP, refer method MDP method.
5. number nodes evaluated using NOPRUNE-BEL computed (f1 + 1) (f2 + 1) (f3 + 1)
(a + 1)c+1 , f1 , f2 f3 number fire engines station 1, 2 3, respectively,
number ambulances c number civilians. node provides complete conditional
role allocation, assuming different numbers civilians fire station.

400

fiHybrid BDI-POMDP Framework Multiagent Teaming

Number nodes (log scale)

10000000

MAXEXP
NOPRUNE

1000000
100000
10000
1000
100
10
1

2

3

4

5

6

7

Number ambulances

Run time secs (log scale)

100000

MAXEXP
NOPRUNE

10000

1000

100

10

1

2

3

4

5

Number ambulances

6

7

Figure 15: Performance role allocation space search RoboCupRescue, a: (left) Number
nodes evaluated log scale, b: (right) Run-time seconds log
scale.

401

fiNair & Tambe

Note comparisons performed using RoboCupRescue simulator
multiple runs deal stochasticity6 . scenario described Section 6.2.1.
fix number fire engines, ambulances civilians five each. experiment,
consider two settings, location civilians drawn from:
Uniform distribution 25% cases four civilians fire 1 one civilian
fire 2, 25% three civilians fire 1 two fire 2, 25% two civilians
fire 1 three fire 2 remaining 25% one civilian fire 1
four civilians fire 2. speedup results Section 6.2.1 obtained using
distribution.
Skewed distribution 80% cases four civilians fire 1 one civilian
fire 2 remaining 20% one civilian fire 1 four civilians fire 2.
Note consider case civilians located fire
optimal ambulance allocation simply assign ambulances fire
civilians located. skewed distribution chosen highlight cases
becomes difficult humans reason allocation choose.
three human subjects used experiment researchers USC. three
familiar RoboCupRescue. given time study setup
given time limit provide allocations. subject told allocations
going judged first basis number civilian lives lost next
damage sustained due fire. exactly criteria used RoboCupRescue (Kitano
et al., 1999).
compared RMTDP allocation human subjects
RoboCupRescue simulator RescueISI MDP. Figure 16, compared
performance allocations basis number civilians died
average damage two buildings (lower values better criteria). two
criteria main two criteria used RoboCupRescue (Kitano et al., 1999). values shown Figure 16 obtained averaging forty simulator runs uniform
distribution twenty runs skewed distribution allocation. average
values plotted account stochasticity domain. Error bars provided
show standard error allocation method.
seen Figure 16(a), RMTDP allocation better five
allocations terms lower number civilians dead (although human3 quite close).
example, averaging forty runs, RMTDP allocation resulted 1.95 civilian deaths
human2s allocation resulted 2.55 civilian deaths. terms average building
damage, six allocations almost indifferentiable, humans actually performing marginally better. Using skewed distribution, difference allocations
much perceptible (see Figure 16(b)). particular, notice RMTDP
allocation much better humans terms number civilians dead. Here,
human3 particularly badly bad allocation fire engines. resulted
damage buildings consequently number civilians dead.
6. mission rehearsal domain, could run actual mission rehearsal simulator since
simulator public domain longer accessible, hence difference tested role
allocations mission rehearsal RoboCupRescue domains.

402

fiHybrid BDI-POMDP Framework Multiagent Teaming

Comparing RMTDP RescueISI MDP approach showed reasoning
transitional uncertainty (MDP) better static reactive allocation method
(RescueISI) well reasoning transitional observational uncertainty. uniform distribution case, found RMTDP better MDP
RescueISI, MDP method performing better RescueISI. skewed distribution case, improvement allocations using RMTDP greater. Averaging twenty
simulation runs, RMTDP allocations resulted 1.54 civilians deaths MDP resulted
1.98 RescueISI 3.52. allocation method used RescueISI often resulted
one fires allocated fire engines. allocations determined
MDP approach turned human1.
two-tailed t-test performed order test statistical significance means
allocations Figure 16. means number civilians dead RMTDP
allocation human allocations found statistically different (confidence
> 96%) uniform well skewed distributions. difference fire
damage statistically significant uniform case, however, difference
RMTDP allocation human3 fire damage statistically significant (> 96%)
skewed case.
6

Civilians casualties
Building damage

5
4
3
2
1

DP

ue







3
sc





2



hu

hu





1


hu

RM

TD

P

0

6

Civilians casualties
Building damage

5
4
3
2
1

DP





sc
ue



3


hu

2


hu

1


hu

RM
TD

P

0

Figure 16: Comparison performance RoboCupRescue, a: (left) uniform, b: (right)
skewed.

403

fiNair & Tambe

Considering average performance different allocations highlight
individual cases marked differences seen performance. Figure 17,
present comparison particular settings allocation methods showed
bigger difference RMTDP terms allocations. standard error shown
error bars allocation. Figures 17(a) 17(b) compare allocations uniform
civilian distributions setting one civilian fire 1 four civilians
fire 2 (1-4 civilian setting) four civilians fire 1 one fire 2 (4-1 civilian setting)
respectively. seen figure, RMTDP allocation results fewer civilian
casualties slightly damage buildings due fire (difference fire damage
statistically significant damage values close). Figures 17(c)
17(d) compare allocations skewed civilian distribution. key difference
arises human3. seen, human3 results damage due fire.
human3 allocated fire engines one buildings, turn resulted
building burnt completely. Consequently, civilians located fire
location could rescued ambulances. Thus, see specific instances
allocation done using RMTDP-based allocation algorithm superior allocations
human comes with.

3.5

Civilians casualties
Building damage

3

4.5

Civilians casualties
Building damage

4
3.5

2.5

3

2

2.5
2

1.5

1.5

1

3.5

DP



ue



3
sc

2






hu



hu

P




TD

Civilians casualties
Building damage

3

hu

RM



DP




3

ue




hu

sc




hu





hu

TD
RM

2

0
1

0

P

0.5
1

1
0.5

4.5

Civilians casualties
Building damage

4
3.5

2.5

3
2

2.5
2

1.5

1.5

1

1
0.5

0.5
DP




sc
ue



3



hu



2
hu

hu

P
RM
TD

DP



ue


3




sc

hu

2


hu

1


hu

TD
P
RM



1

0

0

Figure 17: Comparison performance RoboCupRescue particular settings, a: (topleft) uniform 1-4 civilian setting b:(top-right) uniform 4-1 civilian setting, c:
(bottom-left) skewed 1-4 civilian setting d:(bottom-right) skewed 4-1 civilian
setting.

404

fiHybrid BDI-POMDP Framework Multiagent Teaming

Table 1 shows allocations fire 1 (agents assigned fire 1 allocated fire
2) found RMTDP role allocation algorithm used human subjects
skewed 4-1 civilian setting (we consider case since shows difference).
particular, table highlights differences various allocators skewed
4-1 civilian setting helps account differences seen performance
actual simulator. seen Figure 17(d), main difference performance
terms number civilians saved. Recall scenario, four
civilians fire 1, one fire 2. human subjects MDP chose
send one ambulance fire 2 (number ambulances allocated f ire 2 = 5
number ambulances allocated f ire 1). lone ambulance unable rescue
civilian fire 1, resulting humans MDP saving fewer civilians. RescueISI chose
send ambulances fire 2 using greedy selection method based proximity
civilians resulting civilians fire 1 dying7 . terms fire engine allocation,
human3 sent four fire engines fire 1 civilians likely located
(number engines allocated f ire 2 = 5 number engines allocated f ire 1).
Unfortunately, backfired since lone fire engine fire 2 able extinguish
fire there, causing fire spread parts city.
Distribution
Skewed 4-1

Engines station 1
Engines station 2
Engines station 3
Ambulances

RMTDP
0
1
1
3

human1
2
1
0
4

human2
2
1
0
4

human3
1
1
2
4

RescueISI
2
1
0
0

MDP
2
1
0
4

Table 1: Allocations ambulances fire engines fire 1.
experiments show allocations found RMTDP role allocation algorithm performs significantly better allocations chosen human subjects RescueISI
MDP cases (and significantly worse case). particular
distribution civilians uniform, difficult humans come
allocation difference human allocations RMTDP allocation
becomes significant. conclude RMTDP allocation performs
near-human expertise.
last experiment done using RoboCupRescue simulator, introduced error
RMTDP model order determine sensitive model errors
parameters model. Figure 18 compares allocations found, five
ambulances, 5 fire engines 5 civilians, terms number civilian casualties (Yaxis) error (X-axis) introduced probability fire spread probability
civilian health deterioration. seen increasing error probability fire
spread 20% higher results allocations save fewer civilians fire brigades
choose concentrate effort one fires. resulting allocation
found value terms number civilians casualties used
RescueISI, consider uncertainty. Reducing error probability
fire impact allocations found. Increasing error probability
7. strategy ambulances going closest civilian worked fairly well ambulances
usually well spread

405

fiNair & Tambe

civilian health deterioration 15% higher caused civilians sacrificed.
allocation found value terms number civilians casualties
used RescueISI. Decreasing error probability civilian health deterioration
-5% lower (more negative) caused number ambulances allocated fire
number civilians fire (same human1).
3

Civilian casualties

2.5
2

fire-rate
civilian-health

1.5
1
0.5
0

-25 -20 -15 -10 -5

0

5

10

15

20

25

Percentage error

Figure 18: Model sensitivity RoboCupRescue scenario.

7. Related Work
four related areas research, wish highlight. First,
considerable amount work done field multiagent teamwork (Section 7.1).
second related area research use decision theoretic models, particular
distributed POMDPs (Section 7.2). third area related work describe (Section 7.3)
hybrid systems used Markov Decision Process BDI approaches. Finally,
Section 7.4, related work role allocation reallocation multiagent teams
described.
7.1 BDI-based Teamwork
Several formal teamwork theories Joint Intentions (Cohen & Levesque, 1991),
SharedPlans (Grosz & Kraus, 1996) proposed tried capture essence
multiagent teamwork logic Beliefs-Desires-Intentions. Next, practical models
teamwork COLLAGEN (Rich & Sidner, 1997), GRATE* (Jennings, 1995),
STEAM (Tambe, 1997) built teamwork theories (Cohen & Levesque, 1991; Grosz
& Kraus, 1996) attempted capture aspects teamwork reusable
across domains. addition, complement practical teamwork models, teamoriented programming approach (Pynadath & Tambe, 2003; Tidhar, 1993a, 1993b)
introduced allow large number agents programmed teams. approach
expanded applied variety domains (Pynadath & Tambe, 2003; Yen
et al., 2001; da Silva & Demazeau, 2002). approaches building practical multia406

fiHybrid BDI-POMDP Framework Multiagent Teaming

gent systems (Stone & Veloso, 1999; Decker & Lesser, 1993), explicitly based
team-oriented programming, could considered family.
research reported article complements research teamwork introducing hybrid BDI-POMDP models exploit synergy BDI POMDP
approaches. particular, TOP teamwork models traditionally addressed
uncertainty cost. hybrid model provides capability, illustrated
benefits reasoning via detailed experiments.
article uses team-oriented programming (Tambe et al., 2000; da Silva &
Demazeau, 2002; Tidhar, 1993a, 1993b) example BDI approach, relevant
similar techniques modeling tasking collectives agents, Decker
Lessers (1993) TAEMS approach. particular, TAEMS language provides abstraction tasking collaborative groups agents similar TOP, GPGP infrastructure used executing TAEMS-based tasks analogous TOP interpreter
infrastructure shown Figure 1. Lesser et al. explored use distributed
MDPs analyses GPGP coordination (Xuan & Lesser, 2002), exploited
use TAEMS structures decomposition abstraction searching optimal policies
distributed MDPs, suggested article. Thus, article complements Lesser
et al.s work illustrating significant avenue efficiency improvements
analyses.
7.2 Distributed POMDP Models
Distributed POMDP models represent collection formal models expressive
enough capture uncertainty domain costs rewards associated
states actions. Given group agents, problem deriving separate policies maximize joint reward modeled using distributed POMDP
models. particular, DEC-POMDP (Decentralized POMDP) (Bernstein et al., 2000)
MTDP (Multiagent Team Decision Problem) (Pynadath & Tambe, 2002) generalizations POMDPs case multiple, distributed agents, basing
actions separate observations. frameworks allow us formulate
constitutes optimal policy multiagent team principle derive policy.
However, exceptions, effective algorithms deriving policies distributed
POMDPs developed. Significant progress achieved efficient
single-agent POMDP policy generation algorithms (Monahan, 1982; Cassandra, Littman,
& Zhang, 1997; Kaelbling et al., 1998). However, unlikely research directly
carried distributed case. Finding optimal policies distributed POMDPs
NEXP-complete (Bernstein et al., 2000). contrast, finding optimal policy single
agent POMDP PSPACE-complete (Papadimitriou & Tsitsiklis, 1987). Bernstein et
al. note (Bernstein et al., 2000), suggests fundamental difference nature
problems. distributed problem cannot treated one separate POMDPs
individual policies generated individual agents possible cross-agent
interactions reward, transition observation functions. (For one action one
agent, may many different rewards possible, based actions agents
may take.)
407

fiNair & Tambe

Three approaches used solve distributed POMDPs. One approach
typically taken make simplifying assumptions domain. instance,
Guestrin et al. (2002), assumed agent completely observe world state.
addition, assumed reward function (and transition function) team
expressed sum (product) reward (transition) functions agents
team. Becker et al. (2003) assume domain factored agent
completely observable local state domain transition-independent
(one agent cannot affect another agents local state).
second approach taken simplify nature policies considered
agents. example, Chades et al. (2002) restrict agent policies memoryless
(reactive) policies, thereby simplifying problem solving multiple MDPs. Peshkin et
al. (2000) take different approach using gradient descent search find local optimum
finite-controllers bounded memory. Nair et al. (2003a) present algorithm finding
locally optimal policy space unrestricted finite-horizon policies. third
approach, taken Hansen et al. (2004), involves trying determine globally optimal
solution without making simplifying assumptions domain. approach,
attempt prune space possible complete policies eliminating dominated
policies. Although brave frontal assault problem, method expected
face significant difficulties scaling due fundamental complexity obtaining
globally optimal solution.
key difference work research focused hybrid systems
leverage advantages BDI team plans, used practical systems,
distributed POMDPs quantitatively reason uncertainty cost. particular,
use TOPs specify large-scale team plans complex domains use RMTDPs
finding best role allocation teams.
7.3 Hybrid BDI-POMDP Approaches
POMDP models used context analysis single agent (Schut,
Wooldridge, & Parsons, 2001) multiagent (Pynadath & Tambe, 2002; Xuan et al., 2001)
behavior. Schut et al. compare various strategies intention reconsideration (deciding
deliberate intentions) modeling BDI system using POMDP.
key differences work approach apply analysis single
agent case consider issues exploiting BDI system structure improving
POMDP efficiency.
Xuan Lesser (2001) Pynadath Tambe (2002), analyze multiagent
communication. Xuan Lesser dealt finding evaluating various communication policies, Pynadath Tambe used COM-MTDP model deal problem comparing various communication strategies empirically analytically.
approach general explain approach analyzing coordination actions including communication. concretely demonstrate approach analysis role
allocation. Additional key differences earlier work Pynadath Tambe (2002)
follows: (i) RMTDP, illustrate techniques exploit team plan decomposition
speeding policy search, absent COM-MTDP, (ii) introduce techniques
belief-based evaluation absent previous work. Nonetheless, combining RMTDP
408

fiHybrid BDI-POMDP Framework Multiagent Teaming

COM-MTDP interesting avenue research preliminary steps
direction presented Nair, Tambe Marsella (2003b).
Among hybrid systems focused analysis, Scerri et al. (2002) employ Markov
Decision Processes within team-oriented programs adjustable autonomy. key difference work MDPs used execute particular
sub-plan within TOPs plan hierarchy making improvements TOP.
DTGolog (Boutilier, Reiter, Soutchanski, & Thrun, 2000) provides first-order language
limits MDP policy search via logical constraints actions. Although shares
work key idea synergistic interactions MDPs Golog, differs
work focuses single agent MDPs fully observable domains,
exploit plan structure improving MDP performance. ISAAC (Nair, Tambe, Marsella,
& Raines, 2004), system analyzing multiagent teams, employs decision theoretic
methods analyzing multiagent teams. work, probabilistic finite automaton
(PFA) represents probability distribution key patterns teams behavior
learned logs teams behaviors. key difference work
analysis performed without access actual team plans agents
executing hence advice provided cannot directly applied improving team,
need human developer change team behavior per advice generated.

7.4 Role Allocation Reallocation
several different approaches problem role allocation reallocation.
example, Tidhar et al. (1996) Tambe et al. (2000) performed role allocation based
matching capabilities, Hunsberger Grosz (2000) proposed use combinatorial auctions decide roles assigned. Modi et al. (2003) showed
role allocation modeled distributed constraint optimization problem
applied problem tracking multiple moving targets using distributed sensors.
Shehory Kraus (1998) suggested use coalition formation algorithms deciding
quickly agent took role. Fatima Wooldridge (2001) use auctions
decide task allocation. important note competing techniques
free problem model problem, even though model
transition probabilities. approaches reforming team reconfiguration methods due Dunin-Keplicz Verbrugge (2001), self-adapting organizations Horling
Lesser (2001) dynamic re-organizing groups (Barber & Martin, 2001). Scerri et
al. (2003) present role (re)allocation algorithm allows autonomy role reallocation
shift human supervisor agents.
key difference prior work use stochastic models (RMTDPs)
evaluate allocations: enables us compute benefits role allocation, taking
account uncertainty costs reallocation upon failure. example, mission
rehearsal domain, uncertainties considered, one scout would
allocated, leading costly future reallocations even mission failure. Instead,
lookahead, depending probability failure, multiple scouts sent one
routes, resulting fewer future reallocations higher expected reward.
409

fiNair & Tambe

8. Conclusion
BDI approach agent teamwork provided successful applications, tools
techniques provide quantitative analyses team coordination team behaviors uncertainty lacking. emerging field distributed POMDPs provides
decision theoretic method quantitatively obtaining optimal policy team
agents, faces serious intractability challenge. Therefore, article leverages
benefits BDI POMDP approaches analyze improve key coordination
decisions within BDI-based team plans using POMDP-based methods. order demonstrate analysis methods, concentrated role allocation fundamental aspect
agent teamwork provided three key contributions. First, introduced RMTDP,
distributed POMDP based framework, analysis role allocation. Second, article
presented RMTDP-based methodology optimizing key coordination decisions within
BDI team plan given domain. Concretely, article described methodology
finding best role allocation fixed team plan. Given combinatorially many
role allocations, introduced methods exploit task decompositions among sub-teams
significantly prune search space role allocations.
Third, hybrid BDI-POMDP approach uncovered several synergistic interactions
BDI team plans distributed POMDPs:
1. TOPs useful constructing RMTDP model domain, identifying
features need modeled well decomposing model construction
according structure TOP. RMTDP model could used
evaluate TOP.
2. TOPs restricted policy search providing RMTDPs incomplete policies
limited number open decisions.
3. BDI approach helped coming novel efficient belief-based representation policies suited hybrid BDI-POMDP approach corresponding
algorithm evaluating policies. resulted faster evaluation
compact policy representation.
4. structure TOP exploited decompose problem evaluating
abstract policies, resulting significant pruning search optimal role
allocations.
constructed RMTDPs two domains RoboCupRescue mission rehearsal
simulation determined best role allocation domains. Furthermore,
illustrated significant speedups RMTDP policy search due techniques introduced
article. Detailed experiments revealed advantages approach state-ofthe-art role allocation approaches failed reason uncertainty.
key agenda future work continue scale-up RMTDPs even larger
scale agent teams. scale-up require efficiency improvements. propose
continue exploit interaction BDI POMDP approaches achieving
scale-up. instance, besides disaster rescue, distributed sensor nets large area
monitoring applications could benefit scale-up.
410

fiHybrid BDI-POMDP Framework Multiagent Teaming

Acknowledgments
research supported NSF grant #0208580. would thank Jim Blythe,
Anthony Cassandra, Hyuckchul Jung, Spiros Kapetanakis, Sven Koenig, Michael Littman,
Stacy Marsella, David Pynadath Paul Scerri discussions related article.
would thank reviewers article whose comments helped
significantly improving article.

Appendix A. TOP details
section, describe TOP helicopter scenario. details
subplan Figure 4(b) shown below:
ExecuteMission:
Context:
Pre-conditions: (MB <TaskForce> location(TaskForce) = START)
Achieved: (MB <TaskForce> (Achieved(DoScouting) Achieved(DoTransport)))
(time > (MB <TaskForce> Achieved(RemainingScouts)
( helo ScoutingTeam, alive(helo) location(helo) 6= END)))
Unachievable: (MB <TaskForce> Unachievable(DoScouting))
(MB <TaskForce> (Unachievable(DoTransport)
(Achieved(RemainingScouts)
( helo ScoutingTeam, alive(helo) location(helo) 6= END))))
Irrelevant:
Body:
DoScouting
DoTransport
RemainingScouts
Constraints:
DoScouting DoTransport
DoScouting RemainingScouts
DoScouting:
Context:ExecuteMission <TaskForce>
Pre-conditions:
Achieved:
Unachievable:
Irrelevant:
Body:
WaitAtBase
ScoutRoutes
Constraints:
WaitAtBase ScoutRoutes
WaitAtBase:
Context: DoScouting <TaskForce>
Pre-conditions:
Achieved:
Unachievable: (MB <TransportTeam> helo TransportTeam, alive(helo))
411

fiNair & Tambe

Irrelevant:
Body:
no-op



ScoutRoutes:
Context: DoScouting <TaskForce>
Achieved:
Unachievable:
Irrelevant:(MB <ScoutingTeam> helo TransportTeam, alive(helo))
Body:
ScoutRoute1
ScoutRoute2
ScoutRoute3
Constraints:
ScoutRoute1 ScoutRoute2 ScoutRoute3
ScoutRoute1:
Context: ScoutRoutes <ScoutingTeam>
Pre-conditions:
Achieved: (MB <SctTeamA> helo SctTeamA, location(helo) = END)
Unachievable: time > (MB <SctTeamA> helo SctTeamA, alive(helo))
Irrelevant:
Body:
(location(SctTeamA) = START) route(SctTeamA) 1
(location(SctTeamA) 6= END) move-forward
ScoutRoute2:
Context: ScoutRoutes <ScoutingTeam>
Pre-conditions:
Achieved: (MB <SctTeamB> helo SctTeamB, location(helo) = END)
Unachievable: time > (MB <SctTeamB> helo SctTeamB, alive(helo))
Irrelevant:
Body:
(location(SctTeamB) = START) route(SctTeamB) 2
(location(SctTeamB) 6= END) move-forward
ScoutRoute2:
Context: ScoutRoutes <ScoutingTeam>
Pre-conditions:
Achieved: (MB <SctTeamA> helo SctTeamA, location(helo) = END)
Unachievable: time > (MB <SctTeamA> helo SctTeamA, alive(helo))
Irrelevant:
Body:
(location(SctTeamA) = START) route(SctTeamA) 1
(location(SctTeamA) 6= END) move-forward
DoTransport:
Context: ExecuteMission <TaskForce>
Pre-conditions:
412

fiHybrid BDI-POMDP Framework Multiagent Teaming

Achieved: (MB <TransportTeam> location(TransportTeam) = END)
Unachievable: time > (MB <TransportTeam> helo TransportTeam, alive(helo))
Irrelevant:
Body:
(location(TransportTeam) = start)
(MB <TransportTeam> Achieved(ScoutRoute1))
route(TransportTeam) 1
elseif (MB <TransportTeam> Achieved(ScoutRoute2))
route(TransportTeam) 2
elseif (MB <TransportTeam> Achieved(ScoutRoute3))
route(TransportTeam) 3
(route(TransportTeam) 6= null) (location(TransportTeam) 6= END)
move-forward
RemainingScouts:
Context: ExecuteMission <TaskForce>
Pre-conditions:
Achieved: (MB <ScoutingTeam> location(ScoutingTeam) = END)
Unachievable: time > (MB <ScoutingTeam> ( helo ScoutingTeam
alive(helo) location(helo) 6= END))
Irrelevant:
Body:
(location(ScoutingTeam) 6= END) move-forward

predicate Achieved(tplan) true Achieved conditions tplan true. Similarly, predicates Unachievable(tplan) Irrelevant(tplan) true Unachievable conditions Irrelevant conditions tplan true, respectively. predicate
(location(team) = END) true members team END.
Figure 4(b) shows coordination relationships: relationship indicated
solid arc, relationship indicated dotted arc. coordination relationships indicate unachievability, achievability irrelevance conditions
enforced TOP infrastructure. relationship team sub-plans
means team sub-plans fail, parent team plan fail. Also,
parent team plan achieved, child sub-plans must achieved. Thus,
DoScouting, WaitAtBase ScoutRoutes must done:
Achieved: (MB <TaskForce> Achieved(WaitAtBase) Achieved(ScoutRoutes))
Unachievable: (MB <TaskForce> Unachievable(WaitAtBase)
Unachievable(ScoutRoutes))

relationship means subplans must fail parent fail success
subplans means parent plan succeeded. Thus, ScoutingRoutes,
least one ScoutRoute1, ScoutRoute2 ScoutRoute3 need performed:
(MB <ScoutingTeam> Achieved(ScoutRoute1)
Achieved(ScoutRoute2) Achieved(ScoutRoute3))
Unachievable: (MB <TaskForce> Unachievable(ScoutRoute1)
Unachievable(ScoutRoute2) Unachievable(ScoutRoute3))
Achieved:

413

fiNair & Tambe

relationship affects irrelevance conditions subplans joins.
parent unachievable subplans still executing become irrelevant.
Thus, WaitAtBase:
Irrelevant:

(MB <TaskForce> Unachievable(ScoutRoutes))

Similarly ScoutingRoutes:
Irrelevant:

(MB <TaskForce> Unachievable(ScoutRoutes))

.
Finally, assign roles plans Figure 4(b) shows assignment brackets adjacent plans. instance, Task Force team assigned jointly perform Execute
Mission.

Appendix B. RMTDP details
section, present details RMTDP constructed TOP Figure 4.
S: get features state attributes tested preconditions
achieved, unachievable irrelevant conditions body team plans
individual agent plans. Thus relevant state variables are:location
helicopter, role helicopter,route helicopter, status helicopter
(alive not) time. team n helicopters, state given tuple
< time, role1 , . . . , rolen , loc1 , . . . , locn , route1 , . . . , routen , status1 , . . . , statusn >.
A: consider actions primitive actions agent perform
within individual plans. TOP infrastructure enforces mutual belief
communication actions. Since analyzing cost focus
research consider communication implicit model effect
communication directly observation function.
consider 2 kinds actions role-taking role-execution actions. assume
initial allocation specify roles agents. specifies whether
agent scout transport scout scout team assigned to.
scout cannot become transport change team initial allocation
transport change role taking one role-taking actions.The role-taking
role-execution actions agent given by:
i,memberT ransportT eam = {joinSctT eamA, joinSctT eamB, joinSctT eamC}
i,memberSctT eamA = i,memberSctT eamB = i,memberSctT eamCx =
i,memberT ransportT eam = {chooseRoute, moveF orward}
i,memberSctT eamA = i,memberSctT eamB = i,memberSctT eamC = {moveF orward}
P : obtain transition function help human expert
simulations simulator available. domain, helicopters crash (be shot
down) START, END already scouted location. probability
scouts get shot depends route on, i.e. probability
crash route1 p1 , probability crash route2 p2 probability crash
route3 p3 many scouts spot. assume
414

fiHybrid BDI-POMDP Framework Multiagent Teaming

probability transport shot unscouted location 1
scouted location 0. probability multiple crashes obtained
multiplying probabilities individual crashes.
action, moveForward, effect routei = null loci = END
statusi = dead. cases, location agent gets incremented.
assume role-taking actions scoutRoutex always succeed role
performing agent transport assigned route already.
: transport START observe status agents
probability depending positions. helicopter particular route
observe helicopters route completely cannot observe helicopters
routes.
O: observation function gives probability group agents receive
particular joint observation. domain assume observations one agent
independent observations agents, given current state
previous joint action. Thus probability joint observation computed
multiplying probabilities individual agents observations.
probability transport START observing status alive scout
route 1 0.95. probability transport START observing nothing
alive scout 0.05 since dont false negatives. Similarly scout
route 1 crashes, probability visible transport START 0.98
probability transport doesnt see failure 0.02. Similarly
probabilities observing alive scout route 2 route 3 0.94 0.93
respectively probabilities observing crash route 2 route 3
0.97 0.96 respectively.
R: reward function obtained help human expert helps
assign value various states cost performing various actions.
analysis, assume actions moveForward chooseRoute cost.
consider negative reward (cost) replacement action, scoutRoutex,
R , negative reward failure helicopter RF , reward
scout reaching END Rscout reward transport reaching END
Rtransport . E.g. R = 10, RF = 50, Rscout = 5, Rtransport = 75.
RL: roles individual agents take TOP organization hierarchy.
RL = {transport, scoutOnRoute1, scoutOnRoute2, scoutOnRoute3}.

Appendix C. Theorems
Theorem 3 MAXEXP method always yield upper bound.
Proof sketch:
Let policy leaf-level policy highest expected reward particular parent node, i, restricted policy space.
V = maxChildren(i) V
415

(3)

fiNair & Tambe

Since reward function specified separately component, separate expected reward V rewards constituent components given
starting states starting observation histories components. Let
team plan divided components components parallel
independent sequentially executed.
X
V
maxstates[j],oHistories[j]Vj
1jm

expected value obtained component j, 1 j cannot greater
highest value obtained j using policy.
maxstates[j],oHistories[j]Vj maxChildren(i) maxstates[j],oHistories[j](Vj )

(4)

Hence,
V

X

maxChildren(i) maxstates[j],oHistories[j](Vj )

1jm

V MaxEstimate(i)

(5)



References
Barber, S., & Martin, C. (2001). Dynamic reorganization decision-making groups.
Proceedings Fifth International Conference Autonomous Agents (Agents-01),
pp. 513520.
Becker, R., Zilberstein, S., Lesser, V., & Goldman, C. V. (2003). Transition-independent
decentralized Markov decision processes. Proceedings Second International
Joint Conference Autonomous Agents Multi Agent Systems (AAMAS-03), pp.
4148.
Bernstein, D. S., Zilberstein, S., & Immerman, N. (2000). complexity decentralized control MDPs. Proceedings Sixteenth Conference Uncertainty
Artificial Intelligence(UAI-00), pp. 3237.
Boutilier, C. (1996). Planning, learning & coordination multiagent decision processes.
Proceedings Sixth Conference Theoretical Aspects Rationality Knowledge (TARK-96), pp. 195210.
Boutilier, C., Reiter, R., Soutchanski, M., & Thrun, S. (2000). Decision-theoretic, highlevel agent programming situation calculus. Proceedings Seventeenth
National Conference Artificial Intelligence (AAAI-00), pp. 355362.
Cassandra, A., Littman, M., & Zhang, N. (1997). Incremental pruning: simple, fast,
exact method partially observable Markov decision processes. Proceedings
Thirteenth Annual Conference Uncertainty Artificial Intelligence (UAI-97),
pp. 5461.
416

fiHybrid BDI-POMDP Framework Multiagent Teaming

Chades, I., Scherrer, B., & Charpillet, F. (2002). heuristic approach solving
decentralized-pomdp: Assessment pursuit problem. Proceedings 2002
ACM Symposium Applied Computing (SAC-02), pp. 5762.
Cohen, P. R., & Levesque, H. J. (1991). Teamwork. Nous, 25 (4), 487512.
da Silva, J. L. T., & Demazeau, Y. (2002). Vowels co-ordination model. Proceedings
First International Joint Conference Autonomous Agents Multiagent
Systems (AAMAS-2002), pp. 11291136.
Dean, T., & Lin, S. H. (1995). Decomposition techniques planning stochastic domains. Proceedings Fourteenth International Joint Conference Artificial
Intelligence (IJCAI-95), pp. 11211129.
Decker, K., & Lesser, V. (1993). Quantitative modeling complex computational task
environments. Proceedings Eleventh National Conference Artificial Intelligence (AAAI-93), pp. 217224.
Dix, J., Muoz-Avila, H., Nau, D. S., & Zhang, L. (2003). Impacting shop: Putting
ai planner multi-agent environment. Annals Mathematics Artificial
Intelligence, 37 (4), 381407.
Dunin-Keplicz, B., & Verbrugge, R. (2001). reconfiguration algorithm distributed
problem solving. Engineering Simulation, 18, 227246.
Erol, K., Hendler, J., & Nau, D. S. (1994). HTN planning: Complexity expressivity.
Proceedings Twelfth National Conference Artificial Intelligence (AAAI-94),
pp. 11231128.
Fatima, S. S., & Wooldridge, M. (2001). Adaptive task resource allocation multiagent systems. Proceedings Fifth International Conference Autonomous
Agents (Agents-01), pp. 537544.
Georgeff, M. P., & Lansky, A. L. (1986). Procedural knowledge. Proceedings IEEE
special issue knowledge representation, 74, 13831398.
Goldman, C. V., & Zilberstein, S. (2003). Optimizing information exchange cooperative
multi-agent systems. Proceedings Second International Joint Conference
Autonomous Agents Multi Agent Systems (AAMAS-03), pp. 137144.
Grosz, B., Hunsberger, L., & Kraus, S. (1999). Planning acting together. AI Magazine,
20 (4), 2334.
Grosz, B., & Kraus, S. (1996). Collaborative plans complex group action. Artificial
Intelligence, 86 (2), 269357.
Guestrin, C., Venkataraman, S., & Koller, D. (2002). Context specific multiagent coordination planning factored MDPs. Proceedings Eighteenth National
Conference Artificial Intelligence (AAAI-02), pp. 253259.
Hansen, E., & Zhou, R. (2003). Synthesis hierarchical finite-state controllers pomdps.
Proceedings Thirteenth International Conference Automated Planning
Scheduling (ICAPS-03), pp. 113122.
417

fiNair & Tambe

Hansen, E. A., Bernstein, D. S., & Zilberstein, S. (2004). Dynamic programming partially
observable stochastic games. Proceedings Nineteenth National Conference
Artificial Intelligence (AAAI-04), pp. 709715.
Ho, Y.-C. (1980). Team decision theory information structures. Proceedings
IEEE, 68 (6), 644654.
Horling, B., Benyo, B., & Lesser, V. (2001). Using self-diagnosis adapt organizational
structures. Proceedings Fifth International Conference Autonomous
Agents (Agents-01), pp. 529536.
Hunsberger, L., & Grosz, B. (2000). combinatorial auction collaborative planning.
Proceedings Fourth International Conference Multiagent Systems (ICMAS2000), pp. 151158.
Jennings, N. (1995). Controlling cooperative problem solving industrial multi-agent
systems using joint intentions. Artificial Intelligence, 75 (2), 195240.
Kaelbling, L., Littman, M., & Cassandra, A. (1998). Planning acting partially
observable stochastic domains. Artificial Intelligence, 101 (2), 99134.
Kitano, H., Tadokoro, S., Noda, I., Matsubara, H., Takahashi, T., Shinjoh, A., & Shimada,
S. (1999). RoboCup-Rescue: Search rescue large scale disasters domain
multiagent research. Proceedings IEEE Conference Systems, Men,
Cybernetics (SMC-99), pp. 739743.
Levesque, H. J., Cohen, P. R., & Nunes, J. (1990). acting together. Proceedings
National Conference Artificial Intelligence, pp. 9499. Menlo Park, Calif.: AAAI
press.
Mailler, R. T., & Lesser, V. (2004). Solving distributed constraint optimization problems using cooperative mediation. Proceedings Third International Joint Conference
Agents Multiagent Systems (AAMAS-04), pp. 438445.
Marschak, J., & Radner, R. (1972). Economic Theory Teams. Cowles Foundation
Yale University Press, New Haven, CT.
Modi, P. J., Shen, W.-M., Tambe, M., & Yokoo, M. (2003). asynchronous complete
method distributed constraint optimization. Proceedings Second International Joint Conference Agents Multiagent Systems (AAMAS-03), pp.
161168.
Monahan, G. (1982). survey partially observable Markov decision processes: Theory,
models algorithms. Management Science, 101 (1), 116.
Nair, R., Ito, T., Tambe, M., & Marsella, S. (2002). Task allocation rescue simulation
domain. RoboCup 2001: Robot Soccer World Cup V, Vol. 2377 Lecture Notes
Computer Science, pp. 751754. Springer-Verlag, Heidelberg, Germany.
Nair, R., Pynadath, D., Yokoo, M., Tambe, M., & Marsella, S. (2003a). Taming decentralized
POMDPs: Towards efficient policy computation multiagent settings. Proceedings
Eighteenth International Joint Conference Artificial Intelligence (IJCAI-03),
pp. 705711.
418

fiHybrid BDI-POMDP Framework Multiagent Teaming

Nair, R., Tambe, M., & Marsella, S. (2003b). Team formation reformation multiagent domains RoboCupRescue. Kaminka, G., Lima, P., & Roja, R. (Eds.),
Proceedings RoboCup-2002 International Symposium, pp. 150161. Lecture Notes
Computer Science, Springer Verlag.
Nair, R., Tambe, M., Marsella, S., & Raines, T. (2004). Automated assistants analyze
team behavior. Journal Autonomous Agents Multi-Agent Systems, 8 (1), 69
111.
Papadimitriou, C., & Tsitsiklis, J. (1987). Complexity Markov decision processes. Mathematics Operations Research, 12 (3), 441450.
Peshkin, L., Meuleau, N., Kim, K.-E., & Kaelbling, L. (2000). Learning cooperate via
policy search. Proceedings Sixteenth Conference Uncertainty Artificial
Intelligence (UAI-00), pp. 489496.
Poupart, P., & Boutilier, C. (2003). Bounded finite state controllers. Proceedings
Advances Neural Information Processing Systems 16 (NIPS).
Pynadath, D. V., & Tambe, M. (2002). communicative multiagent team decision
problem: Analyzing teamwork theories models. Journal Artificial Intelligence
Research, 16, 389423.
Pynadath, D. V., & Tambe, M. (2003). Automated teamwork among heterogeneous software agents humans. Journal Autonomous Agents Multi-Agent Systems
(JAAMAS), 7, 71100.
Rich, C., & Sidner, C. (1997). COLLAGEN: agents collaborate people.
Proceedings First International Conference Autonomous Agents (Agents97), pp. 284291.
Scerri, P., Johnson, L., Pynadath, D., Rosenbloom, P., Si, M., Schurr, N., & Tambe, M.
(2003). prototype infrastructure distributed robot, agent, person teams.
Proceedings Second International Joint Conference Agents Multiagent
Systems (AAMAS-03), pp. 433440.
Scerri, P., Pynadath, D. V., & Tambe, M. (2002). Towards adjustable autonomy
real-world. Journal Artificial Intelligence (JAIR), 17, 171228.
Schut, M. C., Wooldridge, M., & Parsons, S. (2001). Reasoning intentions uncertain domains. Proceedings Sixth European Conference Symbolic
Quantitative Approaches Reasoning Uncertainty (ECSQARU-2001), pp. 84
95.
Shehory, O., & Kraus, S. (1998). Methods task allocation via agent coalition formation.
Artificial Intelligence, 101 (1-2), 165200.
Sondik, E. J. (1971). optimal control partially observable Markov processes. Ph.D.
Thesis, Stanford.
Stone, P., & Veloso, M. (1999). Task decomposition, dynamic role assignment, lowbandwidth communication real-time strategic teamwork. Artificial Intelligence,
110 (2), 241273.
419

fiNair & Tambe

Tambe, M. (1997). Towards flexible teamwork. Journal Artificial Intelligence Research,
7, 83124.
Tambe, M., Pynadath, D., & Chauvat, N. (2000). Building dynamic agent organizations
cyberspace. IEEE Internet Computing, 4 (2), 6573.
Tidhar, G. (1993a). Team-oriented programming: Preliminary report. Tech. rep. 41, Australian Artificial Intelligence Institute.
Tidhar, G. (1993b). Team-oriented programming: Social structures. Tech. rep. 47, Australian Artificial Intelligence Institute.
Tidhar, G., Rao, A., & Sonenberg, E. (1996). Guided team selection. Proceedings
Second International Conference Multi-agent Systems (ICMAS-96), pp. 369376.
Wooldridge, M. (2002). Introduction Multiagent Systems. John Wiley & Sons.
Xuan, P., & Lesser, V. (2002). Multi-agent policies: centralized ones decentralized ones. Proceedings First International Joint Conference Agents
Multiagent Systems (AAMAS-02), pp. 10981105.
Xuan, P., Lesser, V., & Zilberstein, S. (2001). Communication decisions multiagent
cooperation. Proceedings Fifth International Conference Autonomous
Agents (Agents-01), pp. 616623.
Yen, J., Yin, J., Ioerger, T. R., Miller, M. S., Xu, D., & Volz, R. A. (2001). Cast: Collaborative agents simulating teamwork. Proceedings Seventeenth International
Joint Conference Artificial Intelligence (IJCAI-01), pp. 11351144.
Yoshikawa, T. (1978). Decomposition dynamic team decision problems. IEEE Transactions Automatic Control, AC-23 (4), 627632.

420


