Journal Artificial Intelligence Research 23 (2005) 79-122

Submitted 2/04; published 2/05

Reinforcement Learning Agents Many Sensors
Actuators Acting Categorizable Environments
Josep Porta

porta@science.uva.nl

IAS Group, Informatics Institute
University Amsterdam
Kruislaan 403, 1098SJ, Amsterdam, Netherlands

Enric Celaya

celaya@iri.upc.edu

Institut de Robotica Informatica Industrial
Spanish Council Scientific Research (CSIC)
Llorens Artigas 4-6, 08028, Barcelona, Spain

Abstract
paper, confront problem applying reinforcement learning agents
perceive environment many sensors perform parallel actions using
many actuators case complex autonomous robots. argue reinforcement
learning successfully applied case strong assumptions made
characteristics environment learning performed,
relevant sensor readings motor commands readily identified. introduction
assumptions leads strongly-biased learning systems eventually lose
generality traditional reinforcement-learning algorithms.
line, observe that, realistic situations, reward received robot
depends reduced subset executed actions reduced subset
sensor inputs (possibly different situation action) relevant
predict reward. formalize property called categorizability assumption
present algorithm takes advantage categorizability environment,
allowing decrease learning time respect existing reinforcement-learning
algorithms. Results application algorithm couple simulated realisticrobotic problems (landmark-based navigation six-legged robot gait generation)
reported validate approach compare existing flat generalizationbased reinforcement-learning approaches.

1. Introduction
division knowledge-based behavior-based artificial intelligence
fundamental achieving successful applications within field autonomous robots (Arkin,
1998). However, now, division repercussions reinforcement learning. Within artificial intelligence, reinforcement learning formalized
general way borrowing ideas dynamic programming decision-theory fields.
Within formalization, objective reinforcement-learning methods establish
correct mapping set abstract observations (formalized states) set
high level actions, without worried sets states actions
defined (for introduction reinforcement learning check Kaelbling, Littman,
& Moore, 1996; Sutton & Barto, 1998, among many others). Algorithms developed within
general framework used different fields without modification.
c
2005
AI Access Foundation. rights reserved.

fiPorta & Celaya

particular application, definition sets states actions responsibility
programmer supposed part reinforcement-learning problem.
However, clearly pointed Brooks (1991), autonomous robots major hurdles
related perception action representations. reason, robotic
task, traditional reinforcement-learning research assumes major problem
(connecting states actions) simpler assumes given (the definition
states actions). consequence existing reinforcement-learning methods
best suited problems fall symbolic artificial intelligence domain
belong robotics. Due generality existing reinforcement-learning
algorithms, robotic problem analyzed re-formulated tackled
available reinforcement-learning tools but, many cases, re-formulation
awkward introducing unnecessary complexity learning process. alternative
explore paper new reinforcement-learning algorithm applied
robotic problems are, without re-formulation.
Brooks (1991) remarked, dealing real environment necessarily problem
since real environments properties exploited reduce complexity
robots controller. Brooks works, find simple robot controllers achieve
good performance particular environments. clearly contrast generality
pursued within reinforcement learning. Following idea parallel Brooks,
paper, present new reinforcement-learning algorithm takes advantage specific
environment-related property (that call categorizability) efficiently learn achieve
given task. formalize categorizability property present representation
system (partial rules) exploit property. remarkable feature representation
system allows generalization spaces sensors actions, using
uniform mechanism. ability generalize state action spaces
fundamental successfully apply reinforcement learning autonomous robots.
paper organized follows. First, Section 2, formalize reinforcement learning point view use field autonomous robotics describe
problems make flat (and, cases, generalization-based) reinforcementlearning algorithms adequate case. Section 3 presents categorizability assumption plausible robotics environments. Then, Section 4, describe
alternative reinforcement-learning algorithm exploits categorizability assumption circumvent problems present existing approaches. Section 5, analyze
points contact proposal already existing work. Next, Section 6,
present experiments validate approach. experiments performed
simulations mimic realistic robotic applications categorizability assumption
likely valid. Finally, Section 7, conclude analyzing strengths
weaknesses proposed learning system.
Additionally, Appendix provides detailed description partial-rule learning
algorithm introduced paper, Appendix B devoted enhancement
algorithm make execution efficient, Appendix C summarizes notation
use throughout paper.
80

fiReinforcement Learning Categorizable Environments

2. Problem Formalization
simplicity, assume robot perceives environment set binary
feature detectors1 F = {fdi | = 1..nf }. feature detector devised process
identifies specific combinations present (and possibly past) sensor readings.
use feature detectors common robotics. field, feature detectors
defined programmer attending special characteristics environment,
robot sensors, task executed order extract potentially useful information
(presence landmarks obstacles, . . . ) raw sensor readings.
similar way, instead working directly space actions provided
robot motors (that define low-level way controlling robot), common
practice define set elementary actions EA = {eai |i = 1..ne }. elementary action
specific sequence/combination motor commands defined programmer attending
characteristics robot task achieved. simplify, assume
elementary actions form (mi k) (i [1..nm ]) mi motor k
value range valid inputs motor mi . framework quite flexible since
motor mi either one physical motors robot high-level, abstract
motor combines movements actual motors. formalization, given
moment, robot execute parallel many elementary actions available motors.
robot controller seen procedure executes (combinations elementary)
actions response specific situations (i.e., activation specific feature detectors)
objective achieving given task. Reinforcement-learning approaches automatically
define controller using information provided reward signal. context
reinforcement learning, controller called policy learner.
objective value-function-based reinforcement-learning algorithms (the
common reinforcement-learning algorithms) predict reward directly
indirectly obtained execution action (i.e., combination elementary
actions) possible situation, described combination active inactive feature
detectors. prediction available, action executed situation
one maximum reward expected.
predict reward, classic reinforcement-learning algorithms rely Markov
assumption, requires state signal carry enough information determine effects
actions given situation.2 Additionally, non-generalizing reinforcement-learning
algorithms assume states system must learned independently. So,
information gathered effects action given state s, denoted Q(s, a),
cannot safely transferred similar states actions. assumption, cost
reinforcement-learning algorithm general problem
(ns na ),
ns number states na number actions.
action tried least state. Since state defined observed
1. Non-binary feature detectors providing discrete range values readily binarized.
2. Non-Markovian problems, confronted, converted Markovian ones.
scope paper, although one relevant points achieve successful
real-world reinforcement-learning application.

81

fiPorta & Celaya

combination feature detectors, potential number states
ns = 2 nf ,
nf number feature detectors. Consequently,
(ns na ) = (2nf na ),
exponential number feature detectors. Since number feature detectors used robotic applications tends high, non-generalizing reinforcement learning
becomes impractical realistic problems. well known curse dimensionality
introduced Bellman (1957), whose research presaged work reinforcement
learning.
Although size action set (na ) important size state set (ns )
curse dimensionality, less attention paid actions reinforcement-learning
literature. However, robot many degrees freedom execute many elementary
actions simultaneously makes cost learning algorithms increase
exponentially number motors robot (nm ).
Suppose address task two different sets feature detectors F 1
F D2 F D1 F D2 . Using plain reinforcement-learning algorithm, cost
finding proper policy would larger using larger set features (F 2 ).
even one features F D2 F D1 stronger correlation reward
features F D1 . Non-generalizing reinforcement-learning algorithms
able take advantage situation, and, even better input information,
performance decreases. similar argument made actions addition
feature detectors.
Generalizing reinforcement-learning algorithms using gradient-descent
techniques (Widrow & Hoff, 1960), coarse codings (Hinton, McClelland, & Rumelhart,
1986), radial-basis functions (Poggio & Girosi, 1990), tile coding (Sutton, 1996) decision
trees (Chapman & Kaelbling, 1991; McCallum, 1995) partially palliate problem
since deal large state spaces. However, approach complex realistic
problems, number dimensions state-space grows point making use
generalization techniques impractical function approximation
techniques must used (Sutton & Barto, 1998, page 209).
Adding relevant inputs actions task make task easier least
difficult. methods whose complexity depends relevance available inputs actions number would scale well real domain problems.
Examples systems fulfilling property are, instance, Kanerva coding system presented Kanerva (1988) random representation method Sutton Whitehead
(1993). systems rely large collections fixed prototypes (i.e., combinations
feature detectors) selected random, proposal search appropriate prototypes, using strong bias search performed reasonable time.
strong bias based categorizability assumption plausible assumption
case autonomous robots, allows large speed learning process.
Additionally, existing systems address problem determining relevance
actions, since assume learning agent single actuator (that is, obviously,
82

fiReinforcement Learning Categorizable Environments

relevant one). simple set adequate robotics. approach (presented below), combinations feature detectors elementary actions considered
using unified framework.

3. Categorizability Assumption
experience developing controllers autonomous robots, observe that, many
realistic situations, reward received robot depends reduced subset
actions executed robot sensor inputs irrelevant
predict reward. Thus, example, value resulting action grasping
object front robot depend object is: object robot
bring user, electrified cable, unimportant object. However, result
probably whether robot moving cameras grasping
object, day night, robot is, time, checking distance
nearest wall, see red light nearby (aspects, them, may
become important circumstances).
agent observes acts environment reduced fraction available inputs actuators considered time, say agent
categorizable environment.
Categorizability binary predicate graded property. completely
categorizable case, would necessary pay attention one sensor/motor
situation. extreme spectrum, motors carefully
coordinated achieve task effect action could predicted
taking account value feature detectors, would say environment
categorizable all.
Since robots large collection sensors providing heterogeneous collection
inputs many actuators affecting quite different degrees freedom, hypothesis
that, robotic problems, environments highly categorizable and, cases,
algorithm biased categorizability assumption would result advantageous.

4. Reinforcement Learning Categorizable Environments: Partial
Rule Approach
implement algorithm able exploit potential categorizability environment,
need representation system able transfer information similar situations
similar actions.
Clustering techniques successive subdivisions state space (as, instance,
presented McCallum, 1995) focus perception side problem aim
determining reward expected given state considering
feature detectors perceived state. subset relevant feature detectors
used compute expected reward state possible action (the Q(s, a)
function). However, way posing problem curse dimensionality problem
completely avoided since features relevant one action
another produces unnecessary (from point view action)
differentiation equivalent situations, decreasing learning speed. problem
83

fiPorta & Celaya

avoided finding specific set relevant feature detectors action.
case, Q function computed Q(fs (a), a), state definition function
action consideration. technique used, instance, Mahadevan
Connell (1992). Unfortunately, problem confronting, enough since,
case, actions composed combinations elementary actions want
transfer reward information similar combinations actions. Therefore,
estimate Q(fs (a), a) taking account elementary actions compose
a. However, principle, relevance elementary actions function situation (or,
equivalently, state): given elementary action relevant situations
others. reason, function approximate becomes Q(f (a), fa (s))
cross-dependency state defined function action, f (a),
action defined function state, fa (s). proposal detail next solves
cross-dependency working Cartesian product spaces feature detectors
elementary actions combinations.
formalize proposal, introduce definitions.
say agent perceives (or observes) partial view order k, v(fd i1 , . . . , fdik ),
k nf whenever predicate fdi1 ... fdik holds.3 Obviously, many partial views
perceived time.
given moment, agent executes action issues different command
one agents motors = {ea1 , . . . , eanm }, nm number motors.
partial command order k, noted c(eai1 , . . . , eaik ), k nm , executed whenever
elementary actions {eai1 , . . . , eaik } executed simultaneously. say partial
command c action accordance c subset a. Note execution
given action supposes execution partial commands accordance
it.
partial rule w defined pair w = (v, c), v partial view c
partial command. say partial rule w = (v, c) active v observed, w
used whenever partial view v perceived partial command c executed.
partial rule covers sub-area Cartesian product feature detectors elementary
actions and, thus, defines situation-action rule used partially determine
actions robot many situations (all partial view rule
active). order partial rule defined sum order partial view
order partial command compose rule.
associate quantiy qw partial rule. qw estimation value (i.e.,
discounted cumulative reward) obtained executing c v observed
time t:

X
qw =
t+i rt+i ,
i=0

rt+i reward received learner time step + rule w used time
t. So, partial rule interpreted as: partial view v observed execution
partial command c results value qw .
3. partial view include negations feature detectors since non-detection feature
relevant detection.

84

fiReinforcement Learning Categorizable Environments

objective learning process deriving set partial rules adjusting
corresponding qw values desired task properly achieved.
apparent drawback partial-rule representation number possible
partial rules much larger number state action pairs: number
partial rules defined set nf binary feature detectors nm binary
motors 3nf +nm , number different states action pairs 2nf +nm .
arbitrary problems confronted (as case synthetic learning situations),
partial-rule approach could useful. However, problems confronted robots
arbitrary since, mentioned, environments present regularities properties (as
categorizability) exploited reduce complexity controller necessary
achieve given task.
Using partial-rule framework, categorizability assumption formally defined
as:
Definition 1 say environment/task highly categorizable exists set
low-order partial rules allows us predict reward accuracy
statistics possible state-action combination considered. lower order
rules controller higher categorizability environment/task.
extent categorizability assumption fulfilled, number partial rules
necessary control robot becomes much smaller number state-action pairs
defined using sets feature detectors elementary actions
partial views partial commands based. Additionally, categorizability implies
rules necessary controller mostly lower order
easily exploited bias search space partial rules. So, environment
categorizable, use partial-rule approach suppose important increase
learning speed reduction use memory respect traditional
non-generalizing reinforcement-learning algorithms.
following sections, describe possible estimate effect
action given fixed set partial rules. evaluation, repeated actions, used
determine best action executed given moment. Next, detail
possible adjust value predictions fixed set partial rules. Finally, describe
categorizability assumption allows us use incremental strategy generation
new partial rules. strategy results faster learning existing generalizing
non-generalizing reinforcement-learning algorithms. procedures described highlevel form make explanation clear. Details implementation found
Appendix A.
4.1 Value Prediction using Partial Rules
given situation, many partial views simultaneously active triggering subset
partial rules controller C. call subset active partial rules denote
C 0 . evaluate given action take account rules C 0
partial command accordance a. denote subset C 0 (a). Note that,
approach, refer action, mean corresponding set elementary actions
(one per motor) single element, general case reinforcement learning.
85

fiPorta & Celaya

Every rule w = (v, c) C 0 (a) provides value prediction a: qw associated
partial rule. averaged value provides information accuracy
prediction. pointed Wilson (1995), favor use partial
rules high accuracy value prediction or, say it, rules high relevance.
seems clear relevance rule (w ) depends distribution values
around qw . Distributions low dispersion indicative coherent value predictions
and, so, highly relevant rule. measure dispersion maintain error estimation
ew approximation qw . Another factor (not used Wilson, 1995) taken
account relevance determination confidence qw ew statistics: low
confidence (i.e., insufficiently sampled) measures qw ew reduce relevance
rule. confidence value prediction given rule (cw ) number
interval [0, 1], initialized 0, increasing partial rule used (i.e., rule
active partial command executed). confidence would decrease
value model given partial rule consistently wrong.
Using confidence, approximate real error value prediction partial
rule w
w = ew cw + e (1 cw ),
value e average error value prediction. Observe importance
e reduced confidence increases and, consequently, w converges ew .
definitions, relevance partial rule defined
w =

1
.
1 + w

Note exact formula relevance important far w1 w2
w1 w2 . formula provides value range [0, 1] could directly
used scale factor, necessary.
problem then, derive single value prediction using qw statistics
rules C 0 (a) corresponding relevance value, w ? Two possible solutions
come mind: using weighted sum values predicted partial rules using
relevance weighting factor, using competitive approach,
relevant partial rule used determine predicted value. weighted sum assumes
linear relation inputs (the value prediction provided individual rule)
output (the value prediction a). assumption proved powerful many
systems but, general, compatible categorizability assumption since,
although one partial rules involved sum low order, taking
account means using large set different feature detectors elementary
actions predict effect given action. reason, learning system uses
winner-take-all solution value prediction relevant partial rule
taken account predict value action. So, action determine
winner rule
w =winner (C 0 , a) =arg

max {w0 },

w0 C 0 (a)

use range likely value rule, Iw = [qw 2w , qw + 2w ], randomly
determine value prediction action a. probability distribution inside interval
depends distribution assume value.
86

fiReinforcement Learning Categorizable Environments

procedure outlined used time step obtain value prediction
action. action maximal value one want robot execute
next.
Observe obtain probabilistic value prediction: situation
statistics, get different value predictions action. way,
action obtains maximal evaluation always one maximal q w
and, consequently, favor exploration promising actions. probabilistic action selection provides exploratory mechanism uses information typical
reinforcement-learning exploration mechanisms (the error confidence value predictions available reinforcement-learning algorithms) result
sophisticated exploration schema (see Wilson, 1996, survey different exploration
mechanisms reinforcement learning).
4.2 Partial Rules Value Adjustment
adjust value predictions rules C 0 (a) last executed action.
rule adjusted, update qw , ew , cw statistics.
effect action accordance partial command c attending
partial rule w = (v, c) defined (using Bellman-like equation)

qw
= rw +

X

p(w, C 0 ) v (C 0 ),

C 0

r w average reward obtained immediately executing c v observed,
discount factor used balance importance immediate respect delayed
reward, v (C 0 ) represents goodness (or value) situation rules C 0 active,
p(w, C 0 ) probability reaching situation execution c v
observed. value situation assessed using best action executable
situation

v (C 0 ) = max
{qw
|w = winner(C 0 , a0 )},
0


since gives us information well robot perform (at most)
situation.
many existing reinforcement-learning approaches, values q w ew
rules adjusted modified using temporal difference rule
error measure. Rules direct relation
progressively approach qw
received reward would provide value prediction (qw ) coherent actually
obtained one and, consequently, statistics adjustment, prediction error
decreased. Contrariwise, rules related observed reward would predict value
different obtained one error statistics increased. way,
rule really important generation received reward, relevance increased
decreased. Rules low relevance chances used drive
robot and, extreme cases, could removed controller.
confidence cw adjusted. adjustment depends confidence measured. related number samples used qw ew
statistics, cw simply slightly incremented every time statistics rule w
87

fiPorta & Celaya

updated. However, decrease confidence value model given partial
rule consistently wrong (i.e., value observed systematically interval w ).
Observe learning rule equivalent used state-based reinforcementlearning methods. instance, Q-learning (Watkins & Dayan, 1992), Q (s, a),
state action, defined
X
p(s, a, s0 ) V (s0 ),
Q (s, a) = r w +
s0

p(s, a, s0 ) probability transition s0 executed
V (s0 ) = max
{Q (s0 , a0 )}
0


approach, set rules active given situation C 0 plays role state
instead
and, thus, v (C 0 ) V (s0 ) equivalent. hand, estimate qw

Q (s, a), rule w includes information (partial) state actions
Q (s, a) play similar role. value prediction given rule, q ,
making qw
w
corresponds average value predictions cells Cartesian product
feature detectors elementary actions covered rule. case complete
rules (i.e., rules involving feature detectors actions motors), sub-area
covered rule includes one cell Cartesian product and, therefore,
controller includes complete rules, described learning rule exactly
used Q-learning. particular case, C 0 (a) one rule that, consequently,
winner rule. statistics rule (and updated
way) Q(s, a) entry table used Q-learning. Thus, learning rule
generalization learning rule normally used reinforcement learning.
4.3 Controller Initialization Partial Rule Creation/Elimination
Since assume working categorizable environment, use incremental
strategy learn adequate set partial rules: initialize controller rules
lowest order generate new partial rules necessary (i.e., cases
correctly categorized using available set rules). So, initial controller contain,
instance, rules order two include one feature detector one elementary
action ((v(fdi ), c(aej )), (v(fdi ), c(aej )) i, j). case, sensible include
empty rule (the rule order 0, w ) initial controller. rule always active
provides average value average error value prediction. Additionally,
knowledge user task achieved easily introduced initial
controller form partial rules. available, estimation value predictions
user-defined rules included. hand-crafted rules (and value
predictions) correct learning process accelerated. correct,
learning algorithm would take care correcting them.
create new rule large error value prediction detected. new
rule defined combination two rules C 0 (a), rules forecast
effects last executed action, a, current situation. selecting couple
rules combined, favor selection value prediction close
88

fiReinforcement Learning Categorizable Environments

actually observed one, since likely involve features elementary actions
(partially) relevant value prediction try refine.
problem possible determine priori whether incorrectly
predicted value would correctly predicted rule adjustments really
necessary create new partial rule account received reward. So, create new
rules large error value prediction, possible create unnecessary
rules. existence (almost) redundant rules necessarily negative, since
provide robustness controller, called degeneracy effect introduced Edelman
(1989). must avoided generate rule twice, since useful
all. Two rules identical respect lexicographic criteria (they contain
feature detectors elementary actions) respect semantic ones (they
get active situations propose equivalent actions). identical rules
created, detected removed soon possible. Preserving
rules proved useful avoids number rules controller growing
reasonable limit.
Since create new rules significant error value prediction,
necessary, could end generating complete rules (provided limit
number rules controller). case, assuming specific
rule accurate value prediction, system would behave normal tablebased reinforcement-learning algorithm: specific rules (i.e., relevant
ones) would used evaluate actions and, explained before, statistics
rules would exactly table-based reinforcement-learning algorithms.
Thus, limit, system deal type problems non-generalizing
reinforcement-learning algorithms. However, regard limit situation improbable impose limits number rules controllers. Observe
asymptotic convergence table-based reinforcement learning possible
use winner-takes-all strategy action evaluation. weighted-sum strategy,
value estimation non-complete rules possibly present controller would
added complete rules leading action evaluation different
table-based reinforcement-learning algorithms.

5. Partial Rule Approach Context
categorizability assumption closely related complexity theory principles
Minimum Description Length (MDL) used authors Schmidhuber (2002) bias learning algorithms. complexity results try formalize
well-known Occams Razor principle enforces choosing simplest model
set otherwise equivalent models.
Boutilier, Dean, Hanks (1999) presents good review representation methods
reduce computational complexity planning algorithms exploiting particular
characteristics given environment. representation based partial rules seen
another representation systems. However, partial rule representation
formalism that, without bias introduced categorizability assumption, would
efficient enough applied realistic applications.
89

fiPorta & Celaya

partial-rule formalism seen generalization XCS classifier
systems described Wilson (1995). XCS learning system aims determining set
classifiers (that combinations features associated action) associated value relevance predictions. main difference approach
Wilsons work pursues generic learner bias learning process using
categorizability assumption. allows us use incremental rule-generation strategy
likely efficient robotic problems. Additionally, categorizability assumption modifies way value given action evaluated: Wilsons
approach uses weighted sum predictions classifier advocating action
determine expected effect action, while, fulfill categorizability assumption (i.e., minimize number feature detectors elementary actions involved
given evaluation), propose use winner-takes-all strategy. critical point
since winner-takes-all strategy takes full advantage categorizability assumption
allows partial-rule system asymptotically converge table-based
reinforcement-learning system. case weighted sum strategy used.
Furthermore, XCS formalism generalization action space and,
already commented, requirement robotic-like applications.
general, reinforcement learning pay attention necessity generalizing
space actions, although exceptions exists. instance, work Maes
Brooks (1990) includes possible execution elementary actions parallel. However
system include mechanism detecting interactions actions and,
thus, coordination actions relies sensory conditions. instance, system
difficulties detecting execution two actions results always (i.e., independently
active/inactive feature detectors) positive/negative reward.
CASCADE algorithm Kaelbling (1993) learns bit complex action
separately. algorithm presents clear sequential structure learning
given action bit depends previously learned ones. approach
predefined order learning outputs result flexible learning
schema.
multiagent learning (Claus & Boutilier, 1998; Sen, 1994; Tan, 1997) objective
learn optimal behavior group agents trying cooperatively solve given
task. Thus, field, case, multiple actions issued parallel considered. However, one main issues multiagent learning, coordination
different learners irrelevant case since one learner.
Finally, way define complex actions elementary actions
points common works reinforcement learning macro-actions defined
learner confronts different tasks (Sutton, Precup, & Singh, 1999; Drummond, 2002).
However, useful combinations elementary actions detected algorithm
guaranteed relevant task hand (although likely relevant
related tasks).

6. Experiments
show results applying learning algorithm two robotics-like simulated problems: robot landmark-based navigation legged robot walking. first problem
90

fiReinforcement Learning Categorizable Environments

Flowers

Bushes

Boat

Tree

Lake

Goal





A7
A6



A5






A4



ffff

ffff

ffff

ffff

ffff

ffff

ffff

ffff

ffff

ffff

ffff

ffff

ffff








































































fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi
A1
fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi
A2


A3
fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi
fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi








fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi




Rock

Bushes

Start

North

Bush

Figure 1: Landscape simple landmark-based navigation task. landscape divided areas (the dashed ovals) subsets landmarks visible.

simpler (although includes delayed reward) use clearly describe
workings algorithm. second problem approaches realistic robotic application,
objective long term. use two examples compare performance
learning system generalizing non-generalizing reinforcement-learning
algorithms. confronted problems different enough show generality
proposed learning system.
6.1 Simulated Landmark-Based Navigation
confront simple simulated landmark-based navigation task forest-like environment shown Figure 1. objective learner go start position
(marked cross bottom figure) goal position
food (marked cross top right corner environment). agent
neither walk lake escape depicted terrain.
agent make use binary landmark (i.e., feature) detectors identify
position environment decide action execute next. example,
landmark detectors agent are:
1. Rock detector: Active rock seen.
2. Boat detector: Active boat seen.
3. Flower detector: Active bunch flowers seen.
91

fiPorta & Celaya

4. Tree detector: Active tree seen.
5. Bush detector: Active whenever bush seen.
6. Water detector: Active water nearby.
7. Bird detector: Active bird flying agent.
8. Cow detector: Active cow nearby.
9. Sun detector: Active sun shining.
10. Cloud detector: Active cloudy.
detectors, first 5 relevant task. water detector always
active, rest landmark detectors become active random. 10 landmark
detectors differentiate 210 = 1024 situations.
simplify problem clustering possible positions learner environment 7 areas (shown Figure 1): area includes positions
set relevant landmarks seen.
far actions concerned, use three actions West-East movement
robot: move West (denoted W ), stay place (), move East (E).
three indicate movement along North-South dimension (move North
N , stay latitude , move South S). two independent groups
three actions combined giving rise 9 different actions (move North-West, North,
North-East, etc.). assume agent executes one actions,
stop nearest area terrain direction movement reached.
agent tries move lake terrain, remains
position was. Figure 1 shows possible transitions contiguous areas
environment.
described landmark detectors elementary actions maximum possible order given rule 12, define 944784 (310 42 ) syntactically different
partial rules. taking account rules one feature detector one elementary action (that ones initially included controller) 90 different
partial rules.
agent receives reward (with value 100) reaches goal. Consequently,
problem delayed reward since agent must transmit information provided reward signal actions situations directly related
observation reward.
parameters partial-rule learning algorithm used task = 0.9,
= 0.99, = 5, = 0.1, = 5, = 200 and, = 0.95 (see Appendix detailed
description parameters). Observe that, maximum number partial rules
= 200 initial controller containing 90 rules, little room left generation
rules order higher 2.
learning organized sequence trials. trial consists placing
learner starting position letting move goal reached, allowing
execution 150 actions reach goal. performing optimally, three
actions required reach objective starting position.
92

fiReinforcement Learning Categorizable Environments

180
160

Steps Goal

140
120
100
80
60
40
20
0

0

50

100

150

200

250

Trial
PR Algorithm

XCS

Figure 2: Performance landmark-based navigation task. Results shown average 10 runs.

Figure 2 shows that, 40 learning trials, agent approaches optimal behavior
(represented flat dashed line = 3).
dashed line Figure 2 performance XCS problem. perform
test, used implementation Wilsons XCS developed Butz (1999).
make XCS work search space partial-rule algorithm, modified
XCS implementation able deal non-binary actions. modification,
parameter adjustment, introduced original code. results presented
corresponds average 10 runs using set parameters gave better
result. Nominally, parameters were: learning rate = 0.1, decay rate = 0.9,
maximum number classifiers = 200 (however, initial set empty), genetic
algorithm applied average every 5 time steps, deletion experience 5, subsume
experience 15, fall rate 0.1, minimum error 0.01, prediction threshold
0.5, crossover probability 0.8, mutation probability 0.04 initial dont
care probability 1/3. prediction fitness new classifiers initialized 10
error 0. detailed explanation meaning parameters provided
Wilson (1995) comments code Butz (1999).
see XCS reaches performance partial-rule approach,
using four times trials. difference performance partially explained
XCSs lack generalization action space. However factor relevant
case since action space two dimensions. main factor explains
better performance partial-rule approach bias introduced categorizability
93

fiPorta & Celaya


1

Position
A2

V
81

2

A4

90

3

A6

100

Action
(W, N )
(W, )
(, N )
(E, N )
(E, )
(E, )

Winner Rule
w1 = (v(Rock, Boat), c(W, N ))
w2 = (v(Rock, W ater), c(W ))
w3 = (v(Boat, ree), c(N ))
w4 = (v(T ree), c(E, N ))
w5 = (v(Rock, Boat), c(E))
w6 = (v(Bush), c(E, ))

qw
80.63
79.73
89.61
90.0
86.71
100.0

ew
1.16
2.19
2.04
0.0
4.58
0.0

Guess
79.87
77.65
88.88
89.86
79.56
99.87

Table 1: Partial execution trace landmark-based navigation task. Elementary action means movement along corresponding dimension. time step
t, action highest guess executed. time step 3, goal
reached.

assumption present XCS system that, case, allows
efficient learning process. XCS powerful partial-rule approach sense
XCS makes assumption categorizability environment,
assume high. result XCS learning process includes identification
degree categorizability environment case is, sense,
pre-defined. generality XCS, however, produces slower learning process.
initialize classifiers XCS high dont care probability initialize
rules partial-rule algorithm generalization used action space
(i.e., rules include command motor), two systems become closer.
case, main (but only) difference two approaches
assumption relation inputs value: XCS assumes linear
relation, assume environment categorizable, or, same, assume
value depend inputs. Due difference, confronted
problem, two systems would learn policy values
action, values would computed using different rules different associated
values, independently parameter/rule initialization used case.
system smaller learning time would assumption closer
reality. results obtained particular example presented show
categorizability assumption valid hypothesis would case
robotics-like applications.
Table 1 shows evaluation actions different situations agent encounters path start goal 50 learning trials. Analyzing trace,
extract insight partial-rule learning algorithm works.
instance, time step 1, see rule w2 = (v(Rock, W ater), c(W )) used
determine value action (W, ). Since landmark detector Water always active,
rule equivalent w = (v(Rock), c(W )), one rules used generate w 2 .
examine statistics w find qw = 74.70 ew = 15.02. Obviously,
value distributions qw qw2 look different (74.70 vs. 79.73 15.02 vs. 2.19).
w2 generated later stages learning and, thus, statistics
updated using subsample values used adjusts statistics w.
94

fiReinforcement Learning Categorizable Environments

particular case, qw updated 250 times qw2 updated 27
times. learning continues, distributions become similar rule w 2
eventually eliminated.
Table 1, see sometimes non-optimal actions get
evaluation close optimal ones. reason, agent executes, times,
non-optimal actions increases number steps necessary reach goal.
general, adjustment statistics rules solve problem but,
particular case, need create new rules fix situation. instance, time step 2,
value rule w4 increased towards 90, value rules active time step
proposing actions accordance action rule w4 converge toward 90.
So, long term, rule proposing action (N ) get value close 90.
absence specific rules, rule used estimate value action
(, N ) and, due probabilistic nature action selection procedure,
action can, eventually, executed delaying agent reaching goal 1 time
step. However, execution (, N ) results error value prediction and, thus,
creation new rules better characterize situation. soon specific rule
action (, N ) generated, error longer repeated.
time step 3, see rule w6 = (v(Bush), c(E, )) value 100 error
0 guess rule 99.87. maximum confidence () lower
1.0 (0.99 case) makes agent keep always certain degree
exploration.
agent receives reward task totally achieved, function value
situation computed V (s) = n1 r n distance (in actions)
situation target one r reward finally obtained. Table 1, see
situations get correct evaluation: 80.63( 81 = 100 0.9 2 ) A2, 90(= 100 0.9)
A4, 100 A6.
Observe problem solved using 200 partial rules 9216
possible situation-action combinations domain. So, say problem
certainly categorizable. main conclusion extract toy example
that, particular case confronted problem categorizable, presented
algorithm able determine relevant rules adjust values (including
effect delayed reward) optimal action determined
situation.
6.2 Gait Generation Six-Legged Robot
applied algorithm task learning generate appropriate gait (i.e.,
sequence steps) six-legged robot (Figure 3). apply learning algorithm
real robot would possible, dangerous: initial phases learning robot
would fall many times damaging motors. reason used simulator
learning and, afterward, applied learned policy real robot.
problem learning walk six legged robot chosen many authors
paradigmatic robotic-learning problem. instance, Maes Brooks (1990)
implemented specific method based immediate reward derive preconditions
leg perform step. Pendrith Ryan (1996) used simplified version
95

fiPorta & Celaya

six-legged walking problem test algorithm able deal Non-Markovian spaces
states Kirchner (1998) presented hierarchical version Q-learning learn
low-level movements leg, well coordination scheme low-level
learned behaviors. Ilg, Muhlfriedel, Berns (1997) introduced learning architecture
based self-organizing neural networks, Kodjabachia Meyer (1998) proposed
evolutionary strategy develop neural network control gait robot. Vallejo
Ramos (2000) used parallel genetic algorithm architecture Parker (2000) described
evolutionary computation robot executes best controller found
given moment new optimal controller computed off-line simulation.
algorithms usually tested flat terrain aim generating periodic gaits (i.e.,
gaits sequence steps repeated cyclically). However, general locomotion
(turns, irregular terrain, etc) problem free gait generation needs considered.

Figure 3: Genghis II walking robot 2D simulation environment.
simulator (see Figure 3) allows controller command leg robot
two independent degrees freedom (horizontal vertical) able detect
robot unstable position (in robot happens two neighboring legs
air simultaneously). Using simulator, implemented behaviors described
Celaya Porta (1996) except charge gait generation. Therefore,
task learned consists deciding every moment legs must step (that is, leave
ground move advanced position), must descend stay
ground support propel body.
defined set 12 feature detectors that, due experience legged robots,
knew could useful different situations gait-generation task:
air(x): Active leg x air.
Advanced(x): Active leg x advanced neighboring leg clockwise
circuit around robot.
Attending activation non-activation 12 feature detectors,
differentiate 4096 different situations.
action side, work two different elementary actions per leg: one
issues step leg another descends leg touches ground.
96

fiReinforcement Learning Categorizable Environments

Thus, cardinality set elementary actions 12 and, time step, robot
issues action containing 6 elementary elements (one per leg). Thus, think
leg virtual motor accepts two possible values, 0 remain contact
ground 1 perform step.
reward signal includes two aspects:
Stability: action causes robot fall down, reward 50 given.
Efficiency: robot fall down, reward equal distance
advanced robot given. Observe legs descend recover contact
ground advance robot obtained movement necessary
able get reward next time steps. So, problem delayed
reward.
efficient stable gait tripod gait two sets three non-adjacent
legs step alternately. Using gait, robot would obtain reward 0 (when one group
three legs lifted advanced) followed reward 50 (when legs contact
ground move backward reaction advance legs moved previous
time step). Thus, optimal average reward 25.
experiments, robot set initial posture legs contact
ground random advance position.
Figure 4 shows results applying partial-rule algorithm compared obtained using standard Q-learning 4096 distinct states 64 different actions.
partial-rule algorithm, used following set parameters: = 0.2, =
0.99, = 22, = 0.1, = 150, = 10000 and, = 0.95 (see Appendix description
parameters). Q-learning, learning rate set = 0.5 use
action selection rule performs exploratory actions probability 0.1.
Figure 4, see stability subproblem (i.e., falling down,
corresponds getting reward greater zero) learned quickly. because,
stability subproblem, take advantage generalization provided using
separate elementary actions and, single rule, avoid executing several dangerous
actions. However, advance subproblem (i.e., getting reward close 25) learned
slowly. little generalization possible learning system must generate
specific rules. words, sub-problem less categorizable stability
one.
landmark-based navigation example discussed previous section,
observe controller contains (slightly) overly general rules responsible
non optimal performance robot. However, dont regard problem
since interested efficiently learning correct enough policy
frequent situations finding optimal behaviors particular cases.
Figure 5 shows performance Q-learning longer run using different exploration rates. shows Q-learning eventually converge optimal policy
many iterations approach (about factor 10). Observe lower
exploration rate allows algorithm achieve higher performance (around 19
learning rate 0.1 around 24 learning rate 0.01) using longer period.
careful adjustment exploration rate combine initial faster learning
97

fiPorta & Celaya

20

Average Reward

10
0
-10
-20
-30
-40
-50

0

1000

2000
3000
Time Slice

PR Algorithm

4000

5000

QLearning

Figure 4: Performance partial-rule approach compared standard Q-learning.
Results smoothed average 10 experiments.

better convergence long term. Experiments Q-learning using learning rates
0.5 showed insignificant differences compared results shown here.
advantage algorithm non-generalizing ones increased problems
sensors provide information related task. test point,
set experiment 6 feature detectors become active randomly
added 12 initial ones. new features, number possible combinations
feature activations increases, number states considered Q-learning.
Figure 6 shows comparison algorithm Q-learning problem.
Q-learning able learn reasonable gait strategy 5000 time steps shown
figure, performance partial-rule algorithm almost
before. means partial-rule algorithm able detect sets features
relevant use effectively determine robots behavior. remarkable
that, case, ratio memory used algorithm respect used
non-generalizing algorithms 0.2%. exemplifies performance
non-generalizing algorithms degrades number features increases,
necessarily case using partial-rule approach.
importance generation partial rules improvement categorization seen comparing results obtained problem without
mechanism (Figure 7). results show task cannot learned using
partial rules order 2. aspect gait-generation problem learned
rules order 2 avoid lifting leg one neighboring legs already
98

fiReinforcement Learning Categorizable Environments

20

Average Reward

10
0
-10
-20
-30
-40
-50

0

50000

Exploration 0.1

100000
150000
Time Slice

200000

Exploration 0.01

250000

References

Figure 5: Performance Q-learning algorithm different exploration rates.
reference values 19 24 upper bound performance attainable
using exploration rate 0.1 0.01.

20

Average Reward

10
0
-10
-20
-30
-40
-50

0

1000

2000
3000
Time Slice

PR Algorithm

4000

5000

QLearning

Figure 6: Performance algorithm compared Q-learning irrelevant
features.

99

fiPorta & Celaya

20

Average Reward

10
0
-10
-20
-30
-40
-50

0

1000

2000
3000
Time Slice

Without Generation

4000

5000

Generation

Figure 7: Performance without partial-rule generation procedure.
air. instance, rule
v(In air(1)) c(Step(2)),
forecasts highly relevant negative reward prevents leg 2 raised
leg 1 air.
Rules order higher 2 (i.e., provided robot initial controller)
necessary, instance, avoid raising two neighboring legs simultaneously. rule
v(In air(1)) c(Step(1), Step(2))
becomes active robot evaluates action implies raising leg 1 leg 2
time. Since value prediction rule negative relevance
high, action evaluation would discarded, preventing robot falling
down. Similar rules generated pair neighboring legs. make
robot advance, need generate rules even higher order.
Figure 8, see performance algorithm start learning
process correct rule set (i.e., rule set learned previous experiment),
statistics initialized 0. experiment, compare complexity
learning values rules compared complexity learning rules
value time. see values rules need
learned learning process two times faster normal application
algorithm.
final experiment, issue frequent changes heading direction robot
(generated randomly every 10 time steps). way, periodic gaits become suboptimal
100

fiReinforcement Learning Categorizable Environments

20

Average Reward

10
0
-10
-20
-30
-40
-50

0

500

1000

1500

2000
2500
Time Slice

PR correct rule set

3000

3500

4000

PR Algorithm

Figure 8: Performance partial-rule approach learning started correct
rule set compared standard approach rules learned.

controller produce free gait, i.e., gait includes sequence steps
without periodic repetition.
case, focus advance subproblem and, thus, introduced handcrafted rules initial controller prevent robot falling down. rules
form:
leg lifted execution action results value 50 confidence 1,
actions lift one two legs contiguous i.
set parameters used case was: = 0.2, = 0.99, = 5, = 0.1,
= 150, = 10000 and, = 0.95.
Figure 9 shows average results obtained using partial-rule learning algorithm
compared obtained best hand-coded gait-generation strategy. figure, horizontal dashed line shows average performance using best gait-generation
strategy implemented (Celaya & Porta, 1998). seen learned gaitgeneration strategy (the increasing continuous line) produces performance similar
best hand-coded strategy that, cases, even outperforms it. Figure 10
shows situation learned controller produces better behavior hand
coded one. Using hand-coded strategy, robot starts walk raising two legs (3
6) and, time steps reaches state tripod gait generated. Initially,
leg 2 advanced legs 1 4 and, general, suboptimal execute step
leg neighboring legs less advances itself. particular case
however, general rule hold. learned strategy detects exception
101

fiPorta & Celaya

25

Average Reward

20
15
10
5
0

0

1000

2000
3000
Time Slice

PR Algorithm

4000

5000

Hand Coded

Figure 9: Performance partial-rule approach learning free gait.
generates tripod gait beginning resulting larger advance robot
initial stages movement.

7. Conclusions
paper, introduced categorizability assumption states robot
driven achieve given task using simple rules: i.e., rules including reduced
set feature detectors elementary actions. assumption supported
experience within behavior-based approach controllers formed sets rules
relatively simple conditions actions. shown learning algorithm
based categorizability assumption allows large speed learning process
many realistic robotic applications respect existing algorithms.
exploit categorizability assumption observations action spaces,
introduced new representation formalism based concept partial rules
concepts independent states independent actions kernel
many existing reinforcement-learning approaches.
introduction partial-rule concept provides large flexibility problems
formalized. structure algorithms, confront problems
generalization perception side (usually considered reinforcement learning),
action side (usually considered), them.
generalization possible via partial rules, use complete rules:
rules involving available inputs outputs. case, partial-rule approach
equivalent non-generalizing reinforcement learning. algorithm presented
102

fiReinforcement Learning Categorizable Environments

Leg Numbering
1
2

0

0

Step 3,6

Step 2,3,6

25

32

3

4

5

6

Step 1,4,5

Step 1,4,5

59

82

Step 2,3,6

Step 2,3,6

109

132

Figure 10: hand-programmed gait strategy (top sequence) vs. learned one (bottom
sequence). advance position robot snapshot indicated
picture.

can, necessary, generate complete rules and, consequently, can, principle, solve
problem solved using traditional reinforcement-learning algorithm. However,
take categorizability assumption valid so, generation complete rules
extreme case likely occur limit situation. Therefore,
approach, forego generality order increase efficiently learning process
class problems want address.
Another advantage partial-rule framework allows easy robust
introduction initial knowledge learning process form rules easily understood programmer. contrast usual reinforcement-learning
algorithms introduction initial knowledge is, general, rather difficult.
partial-rule approach, subtle change emphasis main goal
learning: work reinforcement learning emphasis learning
value action state, main purpose learn relevance (subsets of)
elementary actions feature detectors. relevant subsets elementary actions
feature detectors identified, learning becomes straightforward.
103

fiPorta & Celaya

main limitation work possible know priori (except trivial cases) whether environment categorizable given robot. Non-generalizing
reinforcement learning implicitly assumes environment non-categorizable
that, consequently, possible combination features actions taken
account separately. approach assumes opposite: environment
categorizable and, so, reduced combinations features actions need taken
account. drawback using non-generalizing approach robotic tasks
become intractable curse dimensionality. generalization techniques
problem partially alleviated, enough general. approach take
radical approach order much less affected curse dimensionality:
introduce strong bias learning process drastically limit use combinations
features actions.
tested partial-rule learning algorithm many robotic-inspired problems
two discussed paper (landmark based-navigation sixlegged robot gait generation) categorizability assumption proved valid
cases tested. algorithm out-performs generalizing non-generalizing reinforcementlearning algorithms memory requirements convergence time. Additionally,
shown approach scales well number inputs increases,
performance existing algorithms largely degraded. important result
lets us think could possible use approach control complex robots,
use existing approaches discarded.
work presented paper, extract two main proposals. First,
apply reinforcement learning agents many sensors actuators,
concentrate efforts determining relevance inputs outputs and, second,
achieve efficient learning complex environments could necessary introduce
additional assumptions reinforcement-learning algorithms, even risk losing
generality.

Acknowledgments
authors would express gratitude anonymous reviewers paper.
contributions toward improving quality paper relevant enough
considered, sense, co-authors paper. shortcomings still paper
attributed nominal authors.
second author partially supported Spanish Ministerio de Ciencia Tecnologa FEDER funds, project DPI2003-05193-C02-01 Plan
Nacional de I+D+I.

104

fiReinforcement Learning Categorizable Environments

Appendix A: Partial-Rule Learning Algorithm
appendix, describe detail approach described main body
paper.

Partial Rule Learning Algorithm
(Initialize)
F Set features detectors
EA Set elementary actions
C {w } {(v(fd), c(ea)), (v(fd), c(ea))|fd F D, ea EA}
w C
qw 0
ew 0
iw 0
endfor
e0
episode
C 0 {w C|w active}
Repeat (for step episode):
(Action Selection)
Action Evaluation
(Computes guess(a0 ) a0 )
0
arg max
{guess(a )}
0


Execute
(System Update)
ra Reward generated
0
Cant
C0
0
C {w C|w active}
Statistics Update
Partial-Rule Management
terminal situation
enddo

Figure 11: partial-rule learning algorithm. Text inside parentheses comments.
Action Evaluation, Statistics Update, Partial-Rule Management procedures
described next.

partial-rule learning algorithm (whose top level form shown Figure 11) stores
following information partial rule
value (i.e., discounted cumulative reward) estimation qw ,
error estimation ew ,
confidence index iw .
105

fiPorta & Celaya

1.0
0.9





0.8
0.7

cw

0.6
0.5
0.4
0.3
0.2
0.1
1

2

3

4

5

iw

6

7



8

9

10

Figure 12: Confidence function =7 =0.8.
estimate confidence qw ew use confidence index iw that, roughly
speaking, keeps track number times partial rule used. confidence
derived iw using confidence function following way:
cw =confidence function(iw ),
confidence function non-decreasing function range [0, ].
less 1 since, way, system always keeps certain degree exploration and,
consequently, able adapt changes environment. Different confidence schemes
implemented changing confidence function. implementation, use
sigmoid-like function (see Figure 12) increases slowly low values w reducing
confidence provided first obtained rewards. way avoid premature
increase confidence (and, thus, decrease error exploration)
insufficiently-sampled rules. parameter () determines point function
reaches top value .
Additionally, confidence index used define learning rate (i.e., weight
new observed rewards statistics update). purpose implement MAM
function (Venturini, 1994) rule:
mw = max{, 1/(iw + 1)}.
Using MAM-based updating rule, that, lower confidence, higher
effect last observed rewards statistics, faster adaptation
statistics. adaptive learning rate strategy related presented Sutton (1991)
Kaelbling (1993), contrasts traditional reinforcement-learning algorithms
constant learning rate used.
initialization phase, algorithm enters continuous loop task
episode consisting estimating possible effects actions, executing promis106

fiReinforcement Learning Categorizable Environments

Action Evaluation
action a0
w winner(C 0 , a0 )
guess(a0 ) qw + 2 random(w , w )
endfor

Figure 13: Action Evaluation procedure.
ing one, updating system performance improves future. system
update includes statistics update partial-rule management.
Action Evaluation
simplest procedure get estimated value actions brute-force approach
consisting independent evaluation one them. simple cases, approach
would enough but, number valid combinations elementary actions (i.e.,
actions) large, separate evaluation action would take long time, increasing
time robot decision decreasing reactivity control. avoid this,
Appendix B presents efficient procedure get value action.
Figure 13 summarizes action-evaluation procedure using partial rules. value
action guessed using relevant rule action (i.e., winner rule).
winner rule computed
winner (C 0 , a) =arg

max {w },

wC 0 (a)

w relevance rule w
w =

1
.
1 + w

value estimation using winner rule selected random (uniformly)
interval
Iw = [qw 2w , qw + 2w ],

w = ew cw + e (1 cw ).
Here, e average error value prediction (i.e., value error prediction
empty rule, w ).
Statistics Update
statistics-update procedure (Figure 14), qw ew adjusted rules
active previous time step proposed partial command accordance
(the last executed action).
107

fiPorta & Celaya

Statistics Update
terminal situation
v0
else
v max
{qw |w = winner(C 0 , a0 )}
0


endif
q ra + v
0
w = (v, c) Cant
c accordance
q Iw
iw iw + 1
else
iw min( 1, iw 1)
endif
qw qw (1 mw ) + q mw
ew ew (1 mw ) + |qw q| mw
endif
endfor
e ew

Figure 14: Statistics update procedure.

qw ew updated using learning rate (mw ) computed using MAM
function, initially 1, consequently, initial values qw ew
influence future values variables. initial values become relevant
using constant learning rate, many existing reinforcement-learning algorithms do.
observed effects last executed action agree current estimated
interval value (Iw ), confidence index increased one unit. Otherwise,
confidence index decreased allowing faster adaptation statistics last
obtained, surprising values reward.
Partial-Rule Management
procedure (Figure 15) includes generation new partial rules removal
previously generated ones proved useless.
implementation, apply heuristic produces generation new partial
rules value prediction error exceeds e. way, concentrate efforts
improve categorization situations larger errors value prediction.
Every time wrong prediction made, new partial rules generated
0 (a). Recall set includes
combination pairs rules included set Cant
rules active previous time step accordance executed action a. Thus,
rules related situation-action whose value prediction need
improve.
108

fiReinforcement Learning Categorizable Environments

combination two partial rules w1 w2 consists new partial rule partial
view includes features included partial views either w1 w2
partial command includes elementary actions partial commands either
w1 w2 . words, feature set w1 w2 union feature sets w1
w2 elementary actions w1 w2 union w1
0 (a), simultaneously active
w2 . Note that, since w1 w2 Cant
accordance action and, thus, incompatible
(i.e., include inconsistent features elementary actions).
partial-rule creation, bias system favor combination rules
(wi ) whose value prediction (qwi ) closer observed one (q). Finally, generation
rules lexicographically equivalent already existing ones allowed.
According categorizability assumption, low-order partial rules required
achieve task hand. reason, improve efficiency, limit number
partial rules maximum . However, partial-rule generation procedure always
generating new rules (concentrating situations larger error). Therefore,
need create new rules room them, must eliminate less useful
partial rules.
partial rule removed value prediction similar rule
situations.
similarity two rules measured using normalized degree intersection value distributions number times rules used
simultaneously:
similarity(w, w 0 ) =

U (w w0 )
kIw Iw0 k
,
max{kIw k, kIw0 k} min{U (w), U (w 0 )}

U (w) indicates number times rule w actually used.
similarity assessment pair partial rules controller expensive
and, general, determining similarity rule respect
generated (that rules tried refine new rule created)
sufficient. Thus, based similarity measure, define redundancy
partial rule w = (w1 w2 ) as:
redundancy(w) = max{similarity(w, w1 ), similarity(w, w2 )}.
Observe w = (w1 w2 ), w w1 = w U (w) U (w1 ).
Therefore
U (w w1 )
U (w)
U (w)
=
=
= 1.
min{U (w), U (w1 )}
min{U (w), U (w1 )}
U (w)
reasoning done w2 and, consequently,
redundancy(w) = max{

kIw Iw2 k
kIw Iw1 k
,
}.
max{kIw k, kIw1 k} max{kIw k, kIw2 k}

need create new rules maximum number rules ()
reached, partial rules redundancy given threshold () eliminated.
Since redundancy partial rule estimated observing number
109

fiPorta & Celaya

Partial Rule Management
0
w winner(Cant
, a)
|qw q| > e
(If time create new rules)
(Partial Rule Elimination)
(Test room new rules)
kCk >
(Rule elimination based redundancy)
C C {w C | redundancy(w) > }
(Rule elimination based creation error)
kCk > (If still room)
SC partial rules C with:
- Lowest creation error(w),
- creation error(w) < |qw q|
C C SC
endif
endif
(Partial Rule Generation)
t0
kCk < <
(Create new rule w 0 )
0
(a)
Select two different rules w1 , w2 Cant
preferring minimize
|qwi q| cwi + e (1 cwi )
w0 = (w1 w2 )
creation error(w 0 ) |qw q|
(Insert new rule controller)
C C {w 0 }
tt+1
endwhile
endif

Figure 15: Partial Rule Management procedure. value q calculated Statistics
Update procedure last executed action.

times, redundancy partial rules low confidence indexes set 0,
immediately removed creation.
Observe that, compute redundancy rule w, use partial rules
w derived. reason, rule w 0 cannot removed controller C
exists rule w C w = w 0 w00 . Additionally, way eliminate
first useless rules higher order.

110

fiReinforcement Learning Categorizable Environments

Appendix B: Efficient Action Evaluation
non-generalizing reinforcement learning cost executing single learning step
neglected. However, algorithms generalization spaces sensors and/or actuators
simple execution time iteration increased substantially.
extreme case, increase limit reactivity learner
dangerous working autonomous robot.
expensive procedure algorithm computing value
actions (i.e., valid combinations elementary actions). cost procedure
especially critical since used twice step: get guess action
(in Action Evaluation procedure detailed Figure 13) get goodness
new achieved situation action execution (when computing v value
Statistics Update procedure detailed Figure 14). trivial re-order algorithm
avoid double use expensive procedure learning step: select
action executed next time evaluate goodness new
achieved situation. drawback re-order action selected without
taking account information provided last reward value (the goodness
situation assessed value adjustment). However, problem tasks
require many learning steps.
Even use action-evaluation procedure per learning step,
optimize much possible since brute-force approach described before,
evaluates action sequentially, feasible simple problems.
action-evaluation method presented next based observation many
actions would value since highest relevant partial rule given
moment would provide value actions accordance partial
command rule. separate computation value two actions would end
evaluated using rule waste time. avoided performing
action evaluation attending set active rules first place set
possible actions, brute-force approach does.
Figure 16 shows general form algorithm propose. algorithm, partial
rules considered one time, ordered relevant rule least relevant
one. partial command rule consideration (cow ) used process
actions accordance partial command. already processed sub-set
actions need considered action-evaluation procedure.
rules processed, update current situation assessment (v) action
executed next (a) attending, respectively, value prediction (qw ) guess (gw )
rules.
Observe partial rules maintained sorted relevance statistics update
procedure, since procedure rule relevance modified. relevance
rule changed, position list modified accordingly. way
re-sort list rules every time want apply procedure
described.
elementary actions form (m k) motor k value
range possible values motor, algorithm implemented
especially efficient way since need explicitly compute set actions A.
111

fiPorta & Celaya

Action Evaluation
(Initialization)
L List active rules sorted relevance.
EA Set elementary actions
Set combinations EA
v
(Situation assessment)

(Optimal action)
g
(Optimal action value prediction)
(Process)
w first element(L)

cow partial command w
gw qw + 2 random(w , w )
Aw {a A|cow accordance a}
qw > v
v qw
endif
gw > g
g gw
cow
endif
Aw
w next element(L)
6=

Figure 16: General form proposed situation-assessment action-selection procedure.

case (see Figure 17 18), construct decision tree using motors decision
attributes groups leaf actions evaluated partial
rule (all actions removed set iteration algorithm Figure 16).
internal node tree classifies action according one motor commands included action. internal nodes store following information:
Partial command: partial command accordance action classified node. partial command constructed collecting
motors whose values fixed nodes root tree node
consideration.
Motor: motor used node classify actions. node open (i.e.,
still decided motor attend) motor value set .
node closed deciding motor pay attention (and adding
corresponding subtrees) converting node leaf.
112

fiReinforcement Learning Categorizable Environments

Action Evaluation
(Initialization)
L List active rules sorted relevance.
v

g
tree new node(c )
open 1
closed 0
(Process)
w first element(L)

gw qw + 2 random(w , w )
Include Rule(tree, w, gw )
w next element(L)
closed = open

Figure 17: Top level algorithm efficient action evaluation algorithm. end
algorithm, v goodness current situation used Statistics
Update algorithm (see Figure 14), action executed next guess
expected value. Include Rule procedure detailed next figure.

Subtrees: list subtrees start node. subtree
associated value corresponds one possible actions executable motor node. actions included given subtree elementary action
(m k) motor node k value corresponding
subtree.
leaves tree information value actions classified
leaf. information represented following set attributes leaf:
Value: expected value actions classified leaf. maximum
value leaves used assess goodness, v, new achieved situation.
Guess: value altered noise exploratory reasons. leaf maximal
guess set actions select action executed next.
Relevance: relevance value predictions (of value guess).
Partial command: partial command accordance actions
classified leaf. case internal nodes, partial command
constructed collecting motors whose values fixed root
tree leaf consideration.
113

fiPorta & Celaya

Include rule(n, w, gw )
not(is leaf(n))
cow command(w)
con command(n)
motor(n) 6=
(Closed Node: Search compatible sub-nodes)
ea cow motor(ea) = motor(n)
Include Rule(get subtree(value(ea), n), w, gw )
else
subtrees(n)
Include Rule(s, w, gw )
endfor
endif
else
(Open Node: Specialize node)
cow con 6=
(Extend node)
ea action in(cow con )
set motor(n, motor(ea))
closed closed + 1
k values(motor(ea))
new subtree(n, {k, new node(con (motor(ea) k))})
open open + 1
endfor
Include Rule(n, w, gw )
else
(Transform node leaf )
transform leaf(n, qw , gw , w , cow )
closed closed + 1
qw > v
v qw
endif
gw > guess
g gw
cow
endif
endif
endif
endif

Figure 18: Include rule algorithm searches nodes node n partial command compatible partial command rule w extends nodes
insert leave tree.

given moment, inclusion new partial rule tree produces specialization open nodes compatible rule (see Figure 18). say open
node n compatible given rule w partial command node con
partial command rule cow assign different values motor.
specialization open node result extension node (i.e., new branches
114

fiReinforcement Learning Categorizable Environments

Partial rules
Partial View
Partial Command
RU Ev
(m1 v1 ) (m2 v1 )
RU Ev
(m1 v1 )
RU Ev
(m2 v1 ) (m3 v1 )
RU Ev
(m2 v1 )
RU Ev
(m1 v0 )
RU Ev
(m2 v0 )
RU Ev
(m3 v1 )
RU Ev
(m3 v0 )

q

e
5
7
8
3
2
10
1
6


0.1
0.9
2.0
3.1
3.5
3.6
4.0
4.5

0.83
0.52
0.33
0.24
0.22
0.21
0.20
0.18

guess
5.1
6.5
6.0
6.2
5.3
4.1
5.2
12.7

Table 2: Set rules controller. values q e stored guess
computed them. define partial views RU Ev indicate
active current time step.

added tree node) transformation node leaf.
node extended partial command rule affects motors included
partial command node. means motor values taken
account tree used action evaluation according
rule consideration. node extended, one motors present
layers tree used generate layer open nodes current node.
that, node considered closed inclusion rule procedure repeated
node (with different effects node closed). motors affected
partial command rule affected partial command node,
node transformed leaf storing value, guess, relevance attributes
extracted information associated rule.
process stopped soon detect nodes closed (i.e.
external nodes tree leaves). case, rules still processed
effect tree form and, consequently useful action evaluation. rule
consistently used action evaluation, removed controller.
toy-size example illustrate tree-based action-evaluation algorithm. Suppose
robot three motors accept two different values (named v 0
v1 ). produces set 8 different actions. Suppose that, given moment, robot
controller includes set rules shown Table 2. Action Evaluation algorithm
(Figure 17), rules processed least relevant one expanding
initially empty tree using algorithm Figure 18. inclusion rule tree results
extension tree (see stages B, E Figure 19) closing branches
converting open nodes leaves (stages C F). particular case tree becomes
completely closed processing 5 rules 8 active rules controller.
end process, tree five leaves. Three include two actions
two represent single action. Using tree say value
situation tree constructed, v, 8 (this given leaf circled
solid line figure). Additionally, next action executed form
115

fiPorta & Celaya

Motor: m1
Command:
TRUE c

B


Open
Node

v1

v0

Motor: m2
Command:
(m1,v1)
v0

Open
Node

v1
Value: 5
Guess: 5.1
Relevance: 0.83
Command:
(m1,v1)
(m2,v1)

Open
Node

C

Motor: m2
Command:
(m1,v1)
v0

Open
Node

Value: 7
Guess: 6.5
Relevance: 0.52
Command:
(m1,v1)
(m2,v0)

E

Open
Node

v0

v1

v1

Open
Node

Value: 5
Guess: 5.1
Relevance: 0.83
Command:
(m1,v1)
(m2,v1)

Motor: m3
Command:
(m1,v0)
(m2,v1)
v0
v1

v1
Motor: m3
Command:
(m1,v0)
(m2,v1)
v0
Value: 3
Guess: 6.2
Relevance: 0.24
Command:
(m1,v0)
(m2,v1)
(m3,v0)

v1

v1

v0

Value: 7
Guess: 6.5
Relevance: 0.52
Command:
(m1,v1)
(m2,v0)

Motor: m2
Command:
(m1,v1)
v0

Motor: m2
Command:
(m1,v0)
v1

v0
Value: 5
Guess: 5.1
Relevance: 0.83
Command:
(m1,v1)
(m2,v1)

Value: 2
Guess: 5.3
Relevance: 0.22
Command:
(m1,v0)
(m2,v0)

Value: 8
Guess: 6.0
Relevance: 0.33
Command:
(m1,v0)
(m2,v1)
(m3,v1)

Value: 5
Guess: 5.1
Relevance: 0.83
Command:
(m1,v1)
(m2,v1)

Motor: m1
Command:
TRUE c

v1
Motor: m2
Command:
(m1,v1)
v0

v1

Value: 7
Guess: 6.5
Relevance: 0.52
Command:
(m1,v1)
(m2,v0)

Value: 8
Guess: 6.0
Relevance: 0.33
Command:
(m1,v0)
(m2,v1)
(m3,v1)

Open
Node

F

Motor: m2
Command:
(m1,v0)

Motor: m2
Command:
(m1,v1)
v0

Motor: m2
Command:
(m1,v0)

Motor: m1
Command:
TRUE c
v0

v1

v0

v1

v0

v0

Motor: m1
Command:
TRUE c



Motor: m1
Command:
TRUE c

v1
Motor: m3
Command:
(m1,v0)
(m2,v1)
v0
Value: 3
Guess: 6.2
Relevance: 0.24
Command:
(m1,v0)
(m2,v1)
(m3,v0)

v1

Value: 7
Guess: 6.5
Relevance: 0.52
Command:
(m1,v1)
(m2,v0)

v1
Value: 5
Guess: 5.1
Relevance: 0.83
Command:
(m1,v1)
(m2,v1)

Value: 8
Guess: 6.0
Relevance: 0.33
Command:
(m1,v0)
(m2,v1)
(m3,v1)

Figure 19: Six different stages construction tree action evaluation.
stage corresponds insertion one rule Table 2.

116

fiReinforcement Learning Categorizable Environments

8
7

log(Time)

6
5
4
3
2
1
0

0

1

2
3
Number Void Motors

Brute Force Evaluation

4

5

TreeBased Evaluation

Figure 20: Log execution time (in seconds) brute-force approach vs. treebased one.

(m1 v1 , m2 v0 , m3 ]) ] represents possible action. optimal action
given leaf circled dashed line leaf larger guess value.
cost algorithm largely depends specific set partial rules
processed. worst case, cost algorithm is:
O(nr lnm ),
nr number rules, nm number motors and, l maximal range values
accepted motors. because, worst case, insert given rule,
visit nodes maximally expanded tree (i.e., tree node l subtrees
final nodes branches still opened). number nodes
tree
nm
X
lnm +1 1
li =
= O(lnm ).
l1
i=0

transform cost expression taking account l nm total number
possible combinations elementary actions (nc ) or, words, total amount
actions. Therefore, cost presented algorithm
O(nr nc ).
hand, cost brute-force approach always
(nr nc ).
117

fiPorta & Celaya

So, worst case, cost presented algorithm order cost
brute-force approach. However, since l rules would enough close
maximally expanded tree (one rule different values motor used last
still-open layer tree), cost tree-based algorithm would be, average,
much smaller brute-force approach.
Figure 20 exemplifies different performance brute-force action-evaluation procedure tree-based one. figure shows time taken execution
toy example Section 6.1. experiment, defined void motors motors
whose actions effect environment. seen, number void
motors increases, cost tree-based evaluation significantly less
brute-force approach.

118

fiReinforcement Learning Categorizable Environments

Appendix C: Notation
Uppercase used sets, Greek letters represent parameters algorithms.

Set states.
0
s,
Individual states. Full views.
ns
Number states.
F = {fdi | = 1..nf }
Set feature detectors.
Partial view order k.
v(fdi1 , . . . , fdik )

Set actions robot.
na
Number actions.
EA = {eai | = 1..ne }
Set elementary actions.
nm
Number motors robot.
eai = (mi k)
Elementary action assigns value k motor mi .
c(eai1 , . . . , eaik )
Partial command order k.
= (ea1 , . . . , eanm )
Action. Combination elementary actions. Full command.
w = (v, c)
Partial rule composed partial view v partial command c.
w
empty partial rule.
w1 w 2
Composition two partial rules.
C = {wi | = 1..nr }
Controller set partial rules.

Maximum number elements C.
0
0
C , Cant
Subset rules active given time step previous one.
C 0 (a)
Active rules partial command accordance a.
qw
Expected value partial rule w.
ew
Expected error value estimation partial rule w.
e
Average error value prediction.
iw
Confidence index.
cw
Confidence statistics partial rule w.

Top value confidence.

Index confidence function reaches value .
w = ew cw + e (1 cw ) Error return prediction partial rule w.
w = 1/(1 + w )
Relevance rule w.
Iw = [qw 2w ]
Value interval partial rule w.
mw
Updating ratio statistics partial rule w.

Learning rate. Top value mw .
U (w)
Number times rule w used.
0
winner(C , a)
relevant active partial rule w.r.t. action a.
guess(a)
reliable value estimation action a.
ra
Reward received execution a.

Discount factor.
v
Goodness given situation.
q = ra + v
Value executing action given situation.

Number new partial rules created time.

Redundancy threshold used partial-rule elimination.

119

fiPorta & Celaya

References
Arkin, R. C. (1998). Behavior-Based Robotics. Intelligent Robotics Autonomous Agents.
MIT Press.
Bellman, R. E. (1957). Dynamic Programming. Princeton University Press, Princeton.
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions computational leverage. Journal Artificial Intelligence Research,
11, 194.
Brooks, R. A. (1991). Intelligence without representation. Artificial Intelligence, 47, 139
159.
Butz,

M. (1999).
C-XCS: implementation
(http://www.cs.bath.ac.uk/ amb/LCSWEB/computer.htm).



XCS



C.

Celaya, E., & Porta, J. M. (1996). Control six-legged robot walking abrupt terrain.
Proceedings IEEE International Conference Robotics Automation,
pp. 27312736.
Celaya, E., & Porta, J. M. (1998). control structure locomotion legged
robot difficult terrain. IEEE Robotics Automation Magazine, Special Issue
Walking Robots, 5 (2), 4351.
Chapman, D., & Kaelbling, L. P. (1991). Input generalization delayed reinforcement
learning: algorithm performance comparisons. Proceedings International Joint Conference Artificial Intelligence, pp. 726731.
Claus, C., & Boutilier, C. (1998). dynamics reinforcement learning cooperative
multiagent systems. Proceedings Fifteenth National Conference Artificial
Intelligence, pp. 746752. American Association Artificial Intelligence.
Drummond, C. (2002). Accelerating reinforcement learning composing solutions automatically identified subtasks. Journal Artificial Intelligence Research, 16, 59104.
Edelman, G. M. (1989). Neuronal Darwinism. Oxford University Press.
Hinton, G., McClelland, J., & Rumelhart, D. (1986). Parallel Distributed Processing: Explorations Microstructure Cognition. Volume 1: Foundations, chap. Distributed
Representations. MIT Press, Cambridge, MA.
Ilg, W., Muhlfriedel, T., & Berns, K. (1997). Hybrid learning architecture based neural
networks adaptive control walking machine. Proceedings 1997 IEEE
International Conference Robotics Automation, pp. 26262631.
Kaelbling, L. P. (1993). Learning Embedded Systems. Bradford Book. MIT Press,
Cambridge MA.
Kaelbling, L. P., Littman, M. L., & Moore, A. W. (1996). Reinforcement learning: survey.
Journal Artificial Intelligence Research, 4, 237 285.
Kanerva, P. (1988). Sparse Distributed Memory. MIT Press, Cambridge, MA.
Kirchner, F. (1998). Q-learning complex behaviors six-legged walking machine.
Robotics Autonomous Systems, 25, 253262.
120

fiReinforcement Learning Categorizable Environments

Kodjabachia, J., & Meyer, J. A. (1998). Evolution development modular control
architectures 1-d locomotion six-legged animats. Connection Science, 2, 211
237.
Maes, P., & Brooks, R. A. (1990). Learning coordinate behaviors. Proceedings
AAAI-90, pp. 796802.
Mahadevan, S., & Connell, J. H. (1992). Automatic programming behavior-based robots
using reinforcement learning. Artificial Intelligence, 55, 311363.
McCallum, A. K. (1995). Reinforcement Learning Selective Perception Hidden
State. Ph.D. thesis, Department Computer Science.
Parker, G. B. (2000). Co-evolving model parameters anytime learning evolutionary
robotics. Robotics Autonomous Systems, 33, 1330.
Pendrith, M. D., & Ryan, M. R. K. (1996). C-trace: new algorithm reinforcement
learning robotic control. Proceedings 1996 International Workshop
Learning Autonomous Robots (Robotlearn96).
Poggio, T., & Girosi, F. (1990). Regularization algorithms learning equivalent
multilayer networks. Science, pp. 978982.
Schmidhuber, J. (2002). speed prior: new simplicity measure yielding near-optimal
computable predictions. Proceedings 15th Annual Conference Computational Learning Theory (COLT 2OO2). Lecture Notes Artificial Intelligence.
Springer., pp. 216228.
Sen, S. (1994). Learning coordinate without sharing information. Proceedings
Twelfth National Conference Artificial Intelligence, pp. 426431. American
Association Artificial Intelligence.
Sutton, R. S. (1991). Reinforcement learning architectures animats. Meyer, J. A., &
Wilson, S. W. (Eds.), Proceedings First International Conference Simulation Adaptive Behavior. Animals Animats, pp. 288296. MIT Press,
Bradford Books.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: Introduction. Bradford
Book. MIT Press.
Sutton, R. S., & Whitehead, S. D. (1993). Online learning random representations.
Proceedings Eleventh International Conference Machine Learning, pp.
314321. Morgan Kaufman, San Francisco, CA.
Sutton, R. (1996). Generalization reinforcement learning: Successful examples using
sparse coarse coding. Proceedings 1995 Conference Advances Neural
Information Processing, pp. 10381044.
Sutton, R., Precup, D., & Singh, S. (1999). MDPs semi-MDPs: framework
temporal abstraction reinforcement learning. Artificial Intelligence, 12, 181211.
Tan, M. (1997). Multi-agent reinforcement learning: Independent vs. cooperative agents.
Reading Agents, pp. 487494. Morgan Kaufmann Publishers Inc.
121

fiPorta & Celaya

Vallejo, E. E., & Ramos, F. (2000). distributed genetic programming architecture
evolution robust insect locomotion controllers. Meyer, J. A., Berthoz, A.,
Floreano, D., Roitblat, H. L., & Wilson, S. W. (Eds.), Supplement Proceedings
Sixth International Conference Simulation Adaptive Behavior: Animals
Animats, pp. 235244. International Society Adaptive Behavior.
Venturini, G. (1994). Apprentissage Adaptatif et Apprentissage Supervise par Algorithme
Genetique. Ph.D. thesis.
Watkins, C. J. C. H., & Dayan, P. (1992). Q-learning. Machine Learning, 8, 279292.
Widrow, B., & Hoff, M. (1960). Adaptive switching circuits. Western Electronic Show
Convention, Volume 4, pp. 96104. Institute Radio Engineers (now IEEE).
Wilson, S. W. (1995). Classifier fitness based accuracy. Evolutionary Computation, 3,
149175.
Wilson, S. W. (1996). Explore/exploit strategies autonomy. Animals Animats 4: Proceedings 4th International Conference Simulation Adaptive
Behavior, pp. 325332.

122


