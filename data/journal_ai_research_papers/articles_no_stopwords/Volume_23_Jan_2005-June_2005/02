journal artificial intelligence

submitted published

reinforcement learning agents many sensors
actuators acting categorizable environments
josep porta

porta science uva nl

ias group informatics institute
university amsterdam
kruislaan sj amsterdam netherlands

enric celaya

celaya iri upc edu

institut de robotica informatica industrial
spanish council scientific csic
llorens artigas barcelona spain

abstract
confront applying reinforcement learning agents
perceive environment many sensors perform parallel actions
many actuators case complex autonomous robots argue reinforcement
learning successfully applied case strong assumptions made
characteristics environment learning performed
relevant sensor readings motor commands readily identified introduction
assumptions leads strongly biased learning systems eventually lose
generality traditional reinforcement learning
line observe realistic situations reward received robot
depends reduced subset executed actions reduced subset
sensor inputs possibly different situation action relevant
predict reward formalize property called categorizability assumption
present takes advantage categorizability environment
allowing decrease learning time respect existing reinforcement learning
application couple simulated realisticrobotic landmark navigation six legged robot gait generation
reported validate compare existing flat generalizationbased reinforcement learning approaches

introduction
division knowledge behavior artificial intelligence
fundamental achieving successful applications within field autonomous robots arkin
however division repercussions reinforcement learning within artificial intelligence reinforcement learning formalized
general way borrowing ideas dynamic programming decision theory fields
within formalization objective reinforcement learning methods establish
correct mapping set abstract observations formalized states set
high level actions without worried sets states actions
defined introduction reinforcement learning check kaelbling littman
moore sutton barto among many others developed within
general framework used different fields without modification
c

ai access foundation rights reserved

fiporta celaya

particular application definition sets states actions responsibility
programmer supposed part reinforcement learning
however clearly pointed brooks autonomous robots major hurdles
related perception action representations reason robotic
task traditional reinforcement learning assumes major
connecting states actions simpler assumes given definition
states actions consequence existing reinforcement learning methods
best suited fall symbolic artificial intelligence domain
belong robotics due generality existing reinforcement learning
robotic analyzed formulated tackled
available reinforcement learning tools many cases formulation
awkward introducing unnecessary complexity learning process alternative
explore reinforcement learning applied
robotic without formulation
brooks remarked dealing real environment necessarily
since real environments properties exploited reduce complexity
robots controller brooks works simple robot controllers achieve
good performance particular environments clearly contrast generality
pursued within reinforcement learning following idea parallel brooks
present reinforcement learning takes advantage specific
environment related property call categorizability efficiently learn achieve
given task formalize categorizability property present representation
system partial rules exploit property remarkable feature representation
system allows generalization spaces sensors actions
uniform mechanism ability generalize state action spaces
fundamental successfully apply reinforcement learning autonomous robots
organized follows first section formalize reinforcement learning point view use field autonomous robotics describe
make flat cases generalization reinforcementlearning adequate case section presents categorizability assumption plausible robotics environments section describe
alternative reinforcement learning exploits categorizability assumption circumvent present existing approaches section analyze
points contact proposal already existing work next section
present experiments validate experiments performed
simulations mimic realistic robotic applications categorizability assumption
likely valid finally section conclude analyzing strengths
weaknesses proposed learning system
additionally appendix provides detailed description partial rule learning
introduced appendix b devoted enhancement
make execution efficient appendix c summarizes notation
use throughout


fireinforcement learning categorizable environments

formalization
simplicity assume robot perceives environment set binary
feature detectors f fdi nf feature detector devised process
identifies specific combinations present possibly past sensor readings
use feature detectors common robotics field feature detectors
defined programmer attending special characteristics environment
robot sensors task executed order extract potentially useful information
presence landmarks obstacles raw sensor readings
similar way instead working directly space actions provided
robot motors define low level way controlling robot common
practice define set elementary actions ea eai ne elementary action
specific sequence combination motor commands defined programmer attending
characteristics robot task achieved simplify assume
elementary actions form mi k nm mi motor k
value range valid inputs motor mi framework quite flexible since
motor mi one physical motors robot high level abstract
motor combines movements actual motors formalization given
moment robot execute parallel many elementary actions available motors
robot controller seen procedure executes combinations elementary
actions response specific situations e activation specific feature detectors
objective achieving given task reinforcement learning approaches automatically
define controller information provided reward signal context
reinforcement learning controller called policy learner
objective value function reinforcement learning
common reinforcement learning predict reward directly
indirectly obtained execution action e combination elementary
actions possible situation described combination active inactive feature
detectors prediction available action executed situation
one maximum reward expected
predict reward classic reinforcement learning rely markov
assumption requires state signal carry enough information determine effects
actions given situation additionally non generalizing reinforcement learning
assume states system must learned independently
information gathered effects action given state denoted q
cannot safely transferred similar states actions assumption cost
reinforcement learning general
ns na
ns number states na number actions
action tried least state since state defined observed
non binary feature detectors providing discrete range values readily binarized
non markovian confronted converted markovian ones
scope although one relevant points achieve successful
real world reinforcement learning application



fiporta celaya

combination feature detectors potential number states
ns nf
nf number feature detectors consequently
ns na nf na
exponential number feature detectors since number feature detectors used robotic applications tends high non generalizing reinforcement learning
becomes impractical realistic well known curse dimensionality
introduced bellman whose presaged work reinforcement
learning
although size action set na important size state set ns
curse dimensionality less attention paid actions reinforcement learning
literature however robot many degrees freedom execute many elementary
actions simultaneously makes cost learning increase
exponentially number motors robot nm
suppose address task two different sets feature detectors f
f f f plain reinforcement learning cost
finding proper policy would larger larger set features f
even one features f f stronger correlation reward
features f non generalizing reinforcement learning
able take advantage situation even better input information
performance decreases similar argument made actions addition
feature detectors
generalizing reinforcement learning gradient descent
techniques widrow hoff coarse codings hinton mcclelland rumelhart
radial basis functions poggio girosi tile coding sutton decision
trees chapman kaelbling mccallum partially palliate
since deal large state spaces however complex realistic
number dimensions state space grows point making use
generalization techniques impractical function approximation
techniques must used sutton barto page
adding relevant inputs actions task make task easier least
difficult methods whose complexity depends relevance available inputs actions number would scale well real domain
examples systems fulfilling property instance kanerva coding system presented kanerva random representation method sutton whitehead
systems rely large collections fixed prototypes e combinations
feature detectors selected random proposal search appropriate prototypes strong bias search performed reasonable time
strong bias categorizability assumption plausible assumption
case autonomous robots allows large speed learning process
additionally existing systems address determining relevance
actions since assume learning agent single actuator obviously


fireinforcement learning categorizable environments

relevant one simple set adequate robotics presented combinations feature detectors elementary actions considered
unified framework

categorizability assumption
experience developing controllers autonomous robots observe many
realistic situations reward received robot depends reduced subset
actions executed robot sensor inputs irrelevant
predict reward thus example value resulting action grasping
object front robot depend object object robot
bring user electrified cable unimportant object however
probably whether robot moving cameras grasping
object day night robot time checking distance
nearest wall see red light nearby aspects may
become important circumstances
agent observes acts environment reduced fraction available inputs actuators considered time say agent
categorizable environment
categorizability binary predicate graded property completely
categorizable case would necessary pay attention one sensor motor
situation extreme spectrum motors carefully
coordinated achieve task effect action could predicted
taking account value feature detectors would say environment
categorizable
since robots large collection sensors providing heterogeneous collection
inputs many actuators affecting quite different degrees freedom hypothesis
robotic environments highly categorizable cases
biased categorizability assumption would advantageous

reinforcement learning categorizable environments partial
rule
implement able exploit potential categorizability environment
need representation system able transfer information similar situations
similar actions
clustering techniques successive subdivisions state space instance
presented mccallum focus perception side aim
determining reward expected given state considering
feature detectors perceived state subset relevant feature detectors
used compute expected reward state possible action q
function however way posing curse dimensionality
completely avoided since features relevant one action
another produces unnecessary point view action
differentiation equivalent situations decreasing learning speed


fiporta celaya

avoided finding specific set relevant feature detectors action
case q function computed q fs state definition function
action consideration technique used instance mahadevan
connell unfortunately confronting enough since
case actions composed combinations elementary actions want
transfer reward information similar combinations actions therefore
estimate q fs taking account elementary actions compose
however principle relevance elementary actions function situation
equivalently state given elementary action relevant situations
others reason function approximate becomes q f fa
cross dependency state defined function action f
action defined function state fa proposal detail next solves
cross dependency working cartesian product spaces feature detectors
elementary actions combinations
formalize proposal introduce definitions
say agent perceives observes partial view order k v fd fdik
k nf whenever predicate fdi fdik holds obviously many partial views
perceived time
given moment agent executes action issues different command
one agents motors ea eanm nm number motors
partial command order k noted c eai eaik k nm executed whenever
elementary actions eai eaik executed simultaneously say partial
command c action accordance c subset note execution
given action supposes execution partial commands accordance

partial rule w defined pair w v c v partial view c
partial command say partial rule w v c active v observed w
used whenever partial view v perceived partial command c executed
partial rule covers sub area cartesian product feature detectors elementary
actions thus defines situation action rule used partially determine
actions robot many situations partial view rule
active order partial rule defined sum order partial view
order partial command compose rule
associate quantiy qw partial rule qw estimation value e
discounted cumulative reward obtained executing c v observed
time

x
qw
rt


rt reward received learner time step rule w used time
partial rule interpreted partial view v observed execution
partial command c value qw
partial view include negations feature detectors since non detection feature
relevant detection



fireinforcement learning categorizable environments

objective learning process deriving set partial rules adjusting
corresponding qw values desired task properly achieved
apparent drawback partial rule representation number possible
partial rules much larger number state action pairs number
partial rules defined set nf binary feature detectors nm binary
motors nf nm number different states action pairs nf nm
arbitrary confronted case synthetic learning situations
partial rule could useful however confronted robots
arbitrary since mentioned environments present regularities properties
categorizability exploited reduce complexity controller necessary
achieve given task
partial rule framework categorizability assumption formally defined

definition say environment task highly categorizable exists set
low order partial rules allows us predict reward accuracy
statistics possible state action combination considered lower order
rules controller higher categorizability environment task
extent categorizability assumption fulfilled number partial rules
necessary control robot becomes much smaller number state action pairs
defined sets feature detectors elementary actions
partial views partial commands additionally categorizability implies
rules necessary controller mostly lower order
easily exploited bias search space partial rules environment
categorizable use partial rule suppose important increase
learning speed reduction use memory respect traditional
non generalizing reinforcement learning
following sections describe possible estimate effect
action given fixed set partial rules evaluation repeated actions used
determine best action executed given moment next detail
possible adjust value predictions fixed set partial rules finally describe
categorizability assumption allows us use incremental strategy generation
partial rules strategy faster learning existing generalizing
non generalizing reinforcement learning procedures described highlevel form make explanation clear details implementation found
appendix
value prediction partial rules
given situation many partial views simultaneously active triggering subset
partial rules controller c call subset active partial rules denote
c evaluate given action take account rules c
partial command accordance denote subset c note
refer action mean corresponding set elementary actions
one per motor single element general case reinforcement learning


fiporta celaya

every rule w v c c provides value prediction qw associated
partial rule averaged value provides information accuracy
prediction pointed wilson favor use partial
rules high accuracy value prediction say rules high relevance
seems clear relevance rule w depends distribution values
around qw distributions low dispersion indicative coherent value predictions
highly relevant rule measure dispersion maintain error estimation
ew approximation qw another factor used wilson taken
account relevance determination confidence qw ew statistics low
confidence e insufficiently sampled measures qw ew reduce relevance
rule confidence value prediction given rule cw number
interval initialized increasing partial rule used e rule
active partial command executed confidence would decrease
value model given partial rule consistently wrong
confidence approximate real error value prediction partial
rule w
w ew cw e cw
value e average error value prediction observe importance
e reduced confidence increases consequently w converges ew
definitions relevance partial rule defined
w



w

note exact formula relevance important far w w
w w formula provides value range could directly
used scale factor necessary
derive single value prediction qw statistics
rules c corresponding relevance value w two possible solutions
come mind weighted sum values predicted partial rules
relevance weighting factor competitive
relevant partial rule used determine predicted value weighted sum assumes
linear relation inputs value prediction provided individual rule
output value prediction assumption proved powerful many
systems general compatible categorizability assumption since
although one partial rules involved sum low order taking
account means large set different feature detectors elementary
actions predict effect given action reason learning system uses
winner take solution value prediction relevant partial rule
taken account predict value action action determine
winner rule
w winner c arg

max w

w c

use range likely value rule iw qw w qw w randomly
determine value prediction action probability distribution inside interval
depends distribution assume value


fireinforcement learning categorizable environments

procedure outlined used time step obtain value prediction
action action maximal value one want robot execute
next
observe obtain probabilistic value prediction situation
statistics get different value predictions action way
action obtains maximal evaluation one maximal q w
consequently favor exploration promising actions probabilistic action selection provides exploratory mechanism uses information typical
reinforcement learning exploration mechanisms error confidence value predictions available reinforcement learning
sophisticated exploration schema see wilson survey different exploration
mechanisms reinforcement learning
partial rules value adjustment
adjust value predictions rules c last executed action
rule adjusted update qw ew cw statistics
effect action accordance partial command c attending
partial rule w v c defined bellman equation

qw
rw

x

p w c v c

c

r w average reward obtained immediately executing c v observed
discount factor used balance importance immediate respect delayed
reward v c represents goodness value situation rules c active
p w c probability reaching situation execution c v
observed value situation assessed best action executable
situation

v c max
qw
w winner c



since gives us information well robot perform
situation
many existing reinforcement learning approaches values q w ew
rules adjusted modified temporal difference rule
error measure rules direct relation
progressively qw
received reward would provide value prediction qw coherent actually
obtained one consequently statistics adjustment prediction error
decreased contrariwise rules related observed reward would predict value
different obtained one error statistics increased way
rule really important generation received reward relevance increased
decreased rules low relevance chances used drive
robot extreme cases could removed controller
confidence cw adjusted adjustment depends confidence measured related number samples used qw ew
statistics cw simply slightly incremented every time statistics rule w


fiporta celaya

updated however decrease confidence value model given partial
rule consistently wrong e value observed systematically interval w
observe learning rule equivalent used state reinforcementlearning methods instance q learning watkins dayan q
state action defined
x
p v
q r w


p probability transition executed
v max
q



set rules active given situation c plays role state
instead
thus v c v equivalent hand estimate qw

q rule w includes information partial state actions
q play similar role value prediction given rule q
making qw
w
corresponds average value predictions cells cartesian product
feature detectors elementary actions covered rule case complete
rules e rules involving feature detectors actions motors sub area
covered rule includes one cell cartesian product therefore
controller includes complete rules described learning rule exactly
used q learning particular case c one rule consequently
winner rule statistics rule updated
way q entry table used q learning thus learning rule
generalization learning rule normally used reinforcement learning
controller initialization partial rule creation elimination
since assume working categorizable environment use incremental
strategy learn adequate set partial rules initialize controller rules
lowest order generate partial rules necessary e cases
correctly categorized available set rules initial controller contain
instance rules order two include one feature detector one elementary
action v fdi c aej v fdi c aej j case sensible include
empty rule rule order w initial controller rule active
provides average value average error value prediction additionally
knowledge user task achieved easily introduced initial
controller form partial rules available estimation value predictions
user defined rules included hand crafted rules value
predictions correct learning process accelerated correct
learning would take care correcting
create rule large error value prediction detected
rule defined combination two rules c rules forecast
effects last executed action current situation selecting couple
rules combined favor selection value prediction close


fireinforcement learning categorizable environments

actually observed one since likely involve features elementary actions
partially relevant value prediction try refine
possible determine priori whether incorrectly
predicted value would correctly predicted rule adjustments really
necessary create partial rule account received reward create
rules large error value prediction possible create unnecessary
rules existence almost redundant rules necessarily negative since
provide robustness controller called degeneracy effect introduced edelman
must avoided generate rule twice since useful
two rules identical respect lexicographic criteria contain
feature detectors elementary actions respect semantic ones
get active situations propose equivalent actions identical rules
created detected removed soon possible preserving
rules proved useful avoids number rules controller growing
reasonable limit
since create rules significant error value prediction
necessary could end generating complete rules provided limit
number rules controller case assuming specific
rule accurate value prediction system would behave normal tablebased reinforcement learning specific rules e relevant
ones would used evaluate actions explained statistics
rules would exactly table reinforcement learning
thus limit system deal type non generalizing
reinforcement learning however regard limit situation improbable impose limits number rules controllers observe
asymptotic convergence table reinforcement learning possible
use winner takes strategy action evaluation weighted sum strategy
value estimation non complete rules possibly present controller would
added complete rules leading action evaluation different
table reinforcement learning

partial rule context
categorizability assumption closely related complexity theory principles
minimum description length mdl used authors schmidhuber bias learning complexity try formalize
well known occams razor principle enforces choosing simplest model
set otherwise equivalent
boutilier dean hanks presents good review representation methods
reduce computational complexity exploiting particular
characteristics given environment representation partial rules seen
another representation systems however partial rule representation
formalism without bias introduced categorizability assumption would
efficient enough applied realistic applications


fiporta celaya

partial rule formalism seen generalization xcs classifier
systems described wilson xcs learning system aims determining set
classifiers combinations features associated action associated value relevance predictions main difference
wilsons work pursues generic learner bias learning process
categorizability assumption allows us use incremental rule generation strategy
likely efficient robotic additionally categorizability assumption modifies way value given action evaluated wilsons
uses weighted sum predictions classifier advocating action
determine expected effect action fulfill categorizability assumption e minimize number feature detectors elementary actions involved
given evaluation propose use winner takes strategy critical point
since winner takes strategy takes full advantage categorizability assumption
allows partial rule system asymptotically converge table
reinforcement learning system case weighted sum strategy used
furthermore xcs formalism generalization action space
already commented requirement robotic applications
general reinforcement learning pay attention necessity generalizing
space actions although exceptions exists instance work maes
brooks includes possible execution elementary actions parallel however
system include mechanism detecting interactions actions
thus coordination actions relies sensory conditions instance system
difficulties detecting execution two actions e independently
active inactive feature detectors positive negative reward
cascade kaelbling learns bit complex action
separately presents clear sequential structure learning
given action bit depends previously learned ones
predefined order learning outputs flexible learning
schema
multiagent learning claus boutilier sen tan objective
learn optimal behavior group agents trying cooperatively solve given
task thus field case multiple actions issued parallel considered however one main issues multiagent learning coordination
different learners irrelevant case since one learner
finally way define complex actions elementary actions
points common works reinforcement learning macro actions defined
learner confronts different tasks sutton precup singh drummond
however useful combinations elementary actions detected
guaranteed relevant task hand although likely relevant
related tasks

experiments
applying learning two robotics simulated robot landmark navigation legged robot walking first


fireinforcement learning categorizable environments

flowers

bushes

boat

tree

lake

goal





















ffff

ffff

ffff

ffff

ffff

ffff

ffff

ffff

ffff

ffff

ffff

ffff

ffff








































































fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi

fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi




fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi
fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi fififi








fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi fifi




rock

bushes

start

north

bush

figure landscape simple landmark navigation task landscape divided areas dashed ovals subsets landmarks visible

simpler although includes delayed reward use clearly describe
workings second approaches realistic robotic application
objective long term use two examples compare performance
learning system generalizing non generalizing reinforcement learning
confronted different enough generality
proposed learning system
simulated landmark navigation
confront simple simulated landmark navigation task forest environment shown figure objective learner go start position
marked cross bottom figure goal position
food marked cross top right corner environment agent
neither walk lake escape depicted terrain
agent make use binary landmark e feature detectors identify
position environment decide action execute next example
landmark detectors agent
rock detector active rock seen
boat detector active boat seen
flower detector active bunch flowers seen


fiporta celaya

tree detector active tree seen
bush detector active whenever bush seen
water detector active water nearby
bird detector active bird flying agent
cow detector active cow nearby
sun detector active sun shining
cloud detector active cloudy
detectors first relevant task water detector
active rest landmark detectors become active random landmark
detectors differentiate situations
simplify clustering possible positions learner environment areas shown figure area includes positions
set relevant landmarks seen
far actions concerned use three actions west east movement
robot move west denoted w stay place move east e
three indicate movement along north south dimension move north
n stay latitude move south two independent groups
three actions combined giving rise different actions move north west north
north east etc assume agent executes one actions
stop nearest area terrain direction movement reached
agent tries move lake terrain remains
position figure shows possible transitions contiguous areas
environment
described landmark detectors elementary actions maximum possible order given rule define syntactically different
partial rules taking account rules one feature detector one elementary action ones initially included controller different
partial rules
agent receives reward value reaches goal consequently
delayed reward since agent must transmit information provided reward signal actions situations directly related
observation reward
parameters partial rule learning used task
see appendix detailed
description parameters observe maximum number partial rules
initial controller containing rules little room left generation
rules order higher
learning organized sequence trials trial consists placing
learner starting position letting move goal reached allowing
execution actions reach goal performing optimally three
actions required reach objective starting position


fireinforcement learning categorizable environments




steps goal






















trial
pr

xcs

figure performance landmark navigation task shown average runs

figure shows learning trials agent approaches optimal behavior
represented flat dashed line
dashed line figure performance xcs perform
test used implementation wilsons xcs developed butz
make xcs work search space partial rule modified
xcs implementation able deal non binary actions modification
parameter adjustment introduced original code presented
corresponds average runs set parameters gave better
nominally parameters learning rate decay rate
maximum number classifiers however initial set empty genetic
applied average every time steps deletion experience subsume
experience fall rate minimum error prediction threshold
crossover probability mutation probability initial dont
care probability prediction fitness classifiers initialized
error detailed explanation meaning parameters provided
wilson comments code butz
see xcs reaches performance partial rule
four times trials difference performance partially explained
xcss lack generalization action space however factor relevant
case since action space two dimensions main factor explains
better performance partial rule bias introduced categorizability


fiporta celaya




position


v














action
w n
w
n
e n
e
e

winner rule
w v rock boat c w n
w v rock w ater c w
w v boat ree c n
w v ree c e n
w v rock boat c e
w v bush c e

qw







ew







guess







table partial execution trace landmark navigation task elementary action means movement along corresponding dimension time step
action highest guess executed time step goal
reached

assumption present xcs system case allows
efficient learning process xcs powerful partial rule sense
xcs makes assumption categorizability environment
assume high xcs learning process includes identification
degree categorizability environment case sense
pre defined generality xcs however produces slower learning process
initialize classifiers xcs high dont care probability initialize
rules partial rule generalization used action space
e rules include command motor two systems become closer
case main difference two approaches
assumption relation inputs value xcs assumes linear
relation assume environment categorizable assume
value depend inputs due difference confronted
two systems would learn policy values
action values would computed different rules different associated
values independently parameter rule initialization used case
system smaller learning time would assumption closer
reality obtained particular example presented
categorizability assumption valid hypothesis would case
robotics applications
table shows evaluation actions different situations agent encounters path start goal learning trials analyzing trace
extract insight partial rule learning works
instance time step see rule w v rock w ater c w used
determine value action w since landmark detector water active
rule equivalent w v rock c w one rules used generate w
examine statistics w qw ew obviously
value distributions qw qw look different vs vs
w generated later stages learning thus statistics
updated subsample values used adjusts statistics w


fireinforcement learning categorizable environments

particular case qw updated times qw updated
times learning continues distributions become similar rule w
eventually eliminated
table see sometimes non optimal actions get
evaluation close optimal ones reason agent executes times
non optimal actions increases number steps necessary reach goal
general adjustment statistics rules solve
particular case need create rules fix situation instance time step
value rule w increased towards value rules active time step
proposing actions accordance action rule w converge toward
long term rule proposing action n get value close
absence specific rules rule used estimate value action
n due probabilistic nature action selection procedure
action eventually executed delaying agent reaching goal time
step however execution n error value prediction thus
creation rules better characterize situation soon specific rule
action n generated error longer repeated
time step see rule w v bush c e value error
guess rule maximum confidence lower
case makes agent keep certain degree
exploration
agent receives reward task totally achieved function value
situation computed v n r n distance actions
situation target one r reward finally obtained table see
situations get correct evaluation

observe solved partial rules
possible situation action combinations domain say
certainly categorizable main conclusion extract toy example
particular case confronted categorizable presented
able determine relevant rules adjust values including
effect delayed reward optimal action determined
situation
gait generation six legged robot
applied task learning generate appropriate gait e
sequence steps six legged robot figure apply learning
real robot would possible dangerous initial phases learning robot
would fall many times damaging motors reason used simulator
learning afterward applied learned policy real robot
learning walk six legged robot chosen many authors
paradigmatic robotic learning instance maes brooks
implemented specific method immediate reward derive preconditions
leg perform step pendrith ryan used simplified version


fiporta celaya

six legged walking test able deal non markovian spaces
states kirchner presented hierarchical version q learning learn
low level movements leg well coordination scheme low level
learned behaviors ilg muhlfriedel berns introduced learning architecture
self organizing neural networks kodjabachia meyer proposed
evolutionary strategy develop neural network control gait robot vallejo
ramos used parallel genetic architecture parker described
evolutionary computation robot executes best controller found
given moment optimal controller computed line simulation
usually tested flat terrain aim generating periodic gaits e
gaits sequence steps repeated cyclically however general locomotion
turns irregular terrain etc free gait generation needs considered

figure genghis ii walking robot simulation environment
simulator see figure allows controller command leg robot
two independent degrees freedom horizontal vertical able detect
robot unstable position robot happens two neighboring legs
air simultaneously simulator implemented behaviors described
celaya porta except charge gait generation therefore
task learned consists deciding every moment legs must step leave
ground move advanced position must descend stay
ground support propel body
defined set feature detectors due experience legged robots
knew could useful different situations gait generation task
air x active leg x air
advanced x active leg x advanced neighboring leg clockwise
circuit around robot
attending activation non activation feature detectors
differentiate different situations
action side work two different elementary actions per leg one
issues step leg another descends leg touches ground


fireinforcement learning categorizable environments

thus cardinality set elementary actions time step robot
issues action containing elementary elements one per leg thus think
leg virtual motor accepts two possible values remain contact
ground perform step
reward signal includes two aspects
stability action causes robot fall reward given
efficiency robot fall reward equal distance
advanced robot given observe legs descend recover contact
ground advance robot obtained movement necessary
able get reward next time steps delayed
reward
efficient stable gait tripod gait two sets three non adjacent
legs step alternately gait robot would obtain reward one group
three legs lifted advanced followed reward legs contact
ground move backward reaction advance legs moved previous
time step thus optimal average reward
experiments robot set initial posture legs contact
ground random advance position
figure shows applying partial rule compared obtained standard q learning distinct states different actions
partial rule used following set parameters
see appendix description
parameters q learning learning rate set use
action selection rule performs exploratory actions probability
figure see stability subproblem e falling
corresponds getting reward greater zero learned quickly
stability subproblem take advantage generalization provided
separate elementary actions single rule avoid executing several dangerous
actions however advance subproblem e getting reward close learned
slowly little generalization possible learning system must generate
specific rules words sub less categorizable stability
one
landmark navigation example discussed previous section
observe controller contains slightly overly general rules responsible
non optimal performance robot however dont regard
since interested efficiently learning correct enough policy
frequent situations finding optimal behaviors particular cases
figure shows performance q learning longer run different exploration rates shows q learning eventually converge optimal policy
many iterations factor observe lower
exploration rate allows achieve higher performance around
learning rate around learning rate longer period
careful adjustment exploration rate combine initial faster learning


fiporta celaya



average reward















time slice

pr





qlearning

figure performance partial rule compared standard q learning
smoothed average experiments

better convergence long term experiments q learning learning rates
showed insignificant differences compared shown
advantage non generalizing ones increased
sensors provide information related task test point
set experiment feature detectors become active randomly
added initial ones features number possible combinations
feature activations increases number states considered q learning
figure shows comparison q learning
q learning able learn reasonable gait strategy time steps shown
figure performance partial rule almost
means partial rule able detect sets features
relevant use effectively determine robots behavior remarkable
case ratio memory used respect used
non generalizing exemplifies performance
non generalizing degrades number features increases
necessarily case partial rule
importance generation partial rules improvement categorization seen comparing obtained without
mechanism figure task cannot learned
partial rules order aspect gait generation learned
rules order avoid lifting leg one neighboring legs already


fireinforcement learning categorizable environments



average reward













exploration



time slice



exploration



references

figure performance q learning different exploration rates
reference values upper bound performance attainable
exploration rate



average reward















time slice

pr





qlearning

figure performance compared q learning irrelevant
features



fiporta celaya



average reward















time slice

without generation





generation

figure performance without partial rule generation procedure
air instance rule
v air c step
forecasts highly relevant negative reward prevents leg raised
leg air
rules order higher e provided robot initial controller
necessary instance avoid raising two neighboring legs simultaneously rule
v air c step step
becomes active robot evaluates action implies raising leg leg
time since value prediction rule negative relevance
high action evaluation would discarded preventing robot falling
similar rules generated pair neighboring legs make
robot advance need generate rules even higher order
figure see performance start learning
process correct rule set e rule set learned previous experiment
statistics initialized experiment compare complexity
learning values rules compared complexity learning rules
value time see values rules need
learned learning process two times faster normal application

final experiment issue frequent changes heading direction robot
generated randomly every time steps way periodic gaits become suboptimal


fireinforcement learning categorizable environments



average reward



















time slice

pr correct rule set







pr

figure performance partial rule learning started correct
rule set compared standard rules learned

controller produce free gait e gait includes sequence steps
without periodic repetition
case focus advance subproblem thus introduced handcrafted rules initial controller prevent robot falling rules
form
leg lifted execution action value confidence
actions lift one two legs contiguous
set parameters used case

figure shows average obtained partial rule learning
compared obtained best hand coded gait generation strategy figure horizontal dashed line shows average performance best gait generation
strategy implemented celaya porta seen learned gaitgeneration strategy increasing continuous line produces performance similar
best hand coded strategy cases even outperforms figure
shows situation learned controller produces better behavior hand
coded one hand coded strategy robot starts walk raising two legs
time steps reaches state tripod gait generated initially
leg advanced legs general suboptimal execute step
leg neighboring legs less advances particular case
however general rule hold learned strategy detects exception


fiporta celaya



average reward













time slice

pr





hand coded

figure performance partial rule learning free gait
generates tripod gait beginning resulting larger advance robot
initial stages movement

conclusions
introduced categorizability assumption states robot
driven achieve given task simple rules e rules including reduced
set feature detectors elementary actions assumption supported
experience within behavior controllers formed sets rules
relatively simple conditions actions shown learning
categorizability assumption allows large speed learning process
many realistic robotic applications respect existing
exploit categorizability assumption observations action spaces
introduced representation formalism concept partial rules
concepts independent states independent actions kernel
many existing reinforcement learning approaches
introduction partial rule concept provides large flexibility
formalized structure confront
generalization perception side usually considered reinforcement learning
action side usually considered
generalization possible via partial rules use complete rules
rules involving available inputs outputs case partial rule
equivalent non generalizing reinforcement learning presented


fireinforcement learning categorizable environments

leg numbering







step

step













step

step





step

step





figure hand programmed gait strategy top sequence vs learned one bottom
sequence advance position robot snapshot indicated
picture

necessary generate complete rules consequently principle solve
solved traditional reinforcement learning however
take categorizability assumption valid generation complete rules
extreme case likely occur limit situation therefore
forego generality order increase efficiently learning process
class want address
another advantage partial rule framework allows easy robust
introduction initial knowledge learning process form rules easily understood programmer contrast usual reinforcement learning
introduction initial knowledge general rather difficult
partial rule subtle change emphasis main goal
learning work reinforcement learning emphasis learning
value action state main purpose learn relevance subsets
elementary actions feature detectors relevant subsets elementary actions
feature detectors identified learning becomes straightforward


fiporta celaya

main limitation work possible know priori except trivial cases whether environment categorizable given robot non generalizing
reinforcement learning implicitly assumes environment non categorizable
consequently possible combination features actions taken
account separately assumes opposite environment
categorizable reduced combinations features actions need taken
account drawback non generalizing robotic tasks
become intractable curse dimensionality generalization techniques
partially alleviated enough general take
radical order much less affected curse dimensionality
introduce strong bias learning process drastically limit use combinations
features actions
tested partial rule learning many robotic inspired
two discussed landmark navigation sixlegged robot gait generation categorizability assumption proved valid
cases tested performs generalizing non generalizing reinforcementlearning memory requirements convergence time additionally
shown scales well number inputs increases
performance existing largely degraded important
lets us think could possible use control complex robots
use existing approaches discarded
work presented extract two main proposals first
apply reinforcement learning agents many sensors actuators
concentrate efforts determining relevance inputs outputs second
achieve efficient learning complex environments could necessary introduce
additional assumptions reinforcement learning even risk losing
generality

acknowledgments
authors would express gratitude anonymous reviewers
contributions toward improving quality relevant enough
considered sense co authors shortcomings still
attributed nominal authors
second author partially supported spanish ministerio de ciencia tecnologa feder funds project dpi c plan
nacional de



fireinforcement learning categorizable environments

appendix partial rule learning
appendix describe detail described main body


partial rule learning
initialize
f set features detectors
ea set elementary actions
c w v fd c ea v fd c ea fd f ea ea
w c
qw
ew
iw
endfor
e
episode
c w c w active
repeat step episode
action selection
action evaluation
computes guess

arg max
guess



execute
system update
ra reward generated

cant
c

c w c w active
statistics update
partial rule management
terminal situation
enddo

figure partial rule learning text inside parentheses comments
action evaluation statistics update partial rule management procedures
described next

partial rule learning whose top level form shown figure stores
following information partial rule
value e discounted cumulative reward estimation qw
error estimation ew
confidence index iw


fiporta celaya











cw

















iw













figure confidence function
estimate confidence qw ew use confidence index iw roughly
speaking keeps track number times partial rule used confidence
derived iw confidence function following way
cw confidence function iw
confidence function non decreasing function range
less since way system keeps certain degree exploration
consequently able adapt changes environment different confidence schemes
implemented changing confidence function implementation use
sigmoid function see figure increases slowly low values w reducing
confidence provided first obtained rewards way avoid premature
increase confidence thus decrease error exploration
insufficiently sampled rules parameter determines point function
reaches top value
additionally confidence index used define learning rate e weight
observed rewards statistics update purpose implement mam
function venturini rule
mw max iw
mam updating rule lower confidence higher
effect last observed rewards statistics faster adaptation
statistics adaptive learning rate strategy related presented sutton
kaelbling contrasts traditional reinforcement learning
constant learning rate used
initialization phase enters continuous loop task
episode consisting estimating possible effects actions executing promis

fireinforcement learning categorizable environments

action evaluation
action
w winner c
guess qw random w w
endfor

figure action evaluation procedure
ing one updating system performance improves future system
update includes statistics update partial rule management
action evaluation
simplest procedure get estimated value actions brute force
consisting independent evaluation one simple cases
would enough number valid combinations elementary actions e
actions large separate evaluation action would take long time increasing
time robot decision decreasing reactivity control avoid
appendix b presents efficient procedure get value action
figure summarizes action evaluation procedure partial rules value
action guessed relevant rule action e winner rule
winner rule computed
winner c arg

max w

wc

w relevance rule w
w



w

value estimation winner rule selected random uniformly
interval
iw qw w qw w

w ew cw e cw
e average error value prediction e value error prediction
empty rule w
statistics update
statistics update procedure figure qw ew adjusted rules
active previous time step proposed partial command accordance
last executed action


fiporta celaya

statistics update
terminal situation
v
else
v max
qw w winner c



endif
q ra v

w v c cant
c accordance
q iw
iw iw
else
iw min iw
endif
qw qw mw q mw
ew ew mw qw q mw
endif
endfor
e ew

figure statistics update procedure

qw ew updated learning rate mw computed mam
function initially consequently initial values qw ew
influence future values variables initial values become relevant
constant learning rate many existing reinforcement learning
observed effects last executed action agree current estimated
interval value iw confidence index increased one unit otherwise
confidence index decreased allowing faster adaptation statistics last
obtained surprising values reward
partial rule management
procedure figure includes generation partial rules removal
previously generated ones proved useless
implementation apply heuristic produces generation partial
rules value prediction error exceeds e way concentrate efforts
improve categorization situations larger errors value prediction
every time wrong prediction made partial rules generated
recall set includes
combination pairs rules included set cant
rules active previous time step accordance executed action thus
rules related situation action whose value prediction need
improve


fireinforcement learning categorizable environments

combination two partial rules w w consists partial rule partial
view includes features included partial views w w
partial command includes elementary actions partial commands
w w words feature set w w union feature sets w
w elementary actions w w union w
simultaneously active
w note since w w cant
accordance action thus incompatible
e include inconsistent features elementary actions
partial rule creation bias system favor combination rules
wi whose value prediction qwi closer observed one q finally generation
rules lexicographically equivalent already existing ones allowed
according categorizability assumption low order partial rules required
achieve task hand reason improve efficiency limit number
partial rules maximum however partial rule generation procedure
generating rules concentrating situations larger error therefore
need create rules room must eliminate less useful
partial rules
partial rule removed value prediction similar rule
situations
similarity two rules measured normalized degree intersection value distributions number times rules used
simultaneously
similarity w w

u w w
kiw iw k

max kiw k kiw k min u w u w

u w indicates number times rule w actually used
similarity assessment pair partial rules controller expensive
general determining similarity rule respect
generated rules tried refine rule created
sufficient thus similarity measure define redundancy
partial rule w w w
redundancy w max similarity w w similarity w w
observe w w w w w w u w u w
therefore
u w w
u w
u w



min u w u w
min u w u w
u w
reasoning done w consequently
redundancy w max

kiw iw k
kiw iw k


max kiw k kiw k max kiw k kiw k

need create rules maximum number rules
reached partial rules redundancy given threshold eliminated
since redundancy partial rule estimated observing number


fiporta celaya

partial rule management

w winner cant

qw q e
time create rules
partial rule elimination
test room rules
kck
rule elimination redundancy
c c w c redundancy w
rule elimination creation error
kck still room
sc partial rules c
lowest creation error w
creation error w qw q
c c sc
endif
endif
partial rule generation

kck
create rule w


select two different rules w w cant
preferring minimize
qwi q cwi e cwi
w w w
creation error w qw q
insert rule controller
c c w
tt
endwhile
endif

figure partial rule management procedure value q calculated statistics
update procedure last executed action

times redundancy partial rules low confidence indexes set
immediately removed creation
observe compute redundancy rule w use partial rules
w derived reason rule w cannot removed controller c
exists rule w c w w w additionally way eliminate
first useless rules higher order



fireinforcement learning categorizable environments

appendix b efficient action evaluation
non generalizing reinforcement learning cost executing single learning step
neglected however generalization spaces sensors actuators
simple execution time iteration increased substantially
extreme case increase limit reactivity learner
dangerous working autonomous robot
expensive procedure computing value
actions e valid combinations elementary actions cost procedure
especially critical since used twice step get guess action
action evaluation procedure detailed figure get goodness
achieved situation action execution computing v value
statistics update procedure detailed figure trivial order
avoid double use expensive procedure learning step select
action executed next time evaluate goodness
achieved situation drawback order action selected without
taking account information provided last reward value goodness
situation assessed value adjustment however tasks
require many learning steps
even use action evaluation procedure per learning step
optimize much possible since brute force described
evaluates action sequentially feasible simple
action evaluation method presented next observation many
actions would value since highest relevant partial rule given
moment would provide value actions accordance partial
command rule separate computation value two actions would end
evaluated rule waste time avoided performing
action evaluation attending set active rules first place set
possible actions brute force
figure shows general form propose partial
rules considered one time ordered relevant rule least relevant
one partial command rule consideration cow used process
actions accordance partial command already processed sub set
actions need considered action evaluation procedure
rules processed update current situation assessment v action
executed next attending respectively value prediction qw guess gw
rules
observe partial rules maintained sorted relevance statistics update
procedure since procedure rule relevance modified relevance
rule changed position list modified accordingly way
sort list rules every time want apply procedure
described
elementary actions form k motor k value
range possible values motor implemented
especially efficient way since need explicitly compute set actions


fiporta celaya

action evaluation
initialization
l list active rules sorted relevance
ea set elementary actions
set combinations ea
v
situation assessment

optimal action
g
optimal action value prediction
process
w first element l

cow partial command w
gw qw random w w
aw cow accordance
qw v
v qw
endif
gw g
g gw
cow
endif
aw
w next element l


figure general form proposed situation assessment action selection procedure

case see figure construct decision tree motors decision
attributes groups leaf actions evaluated partial
rule actions removed set iteration figure
internal node tree classifies action according one motor commands included action internal nodes store following information
partial command partial command accordance action classified node partial command constructed collecting
motors whose values fixed nodes root tree node
consideration
motor motor used node classify actions node open e
still decided motor attend motor value set
node closed deciding motor pay attention adding
corresponding subtrees converting node leaf


fireinforcement learning categorizable environments

action evaluation
initialization
l list active rules sorted relevance
v

g
tree node c
open
closed
process
w first element l

gw qw random w w
include rule tree w gw
w next element l
closed open

figure top level efficient action evaluation end
v goodness current situation used statistics
update see figure action executed next guess
expected value include rule procedure detailed next figure

subtrees list subtrees start node subtree
associated value corresponds one possible actions executable motor node actions included given subtree elementary action
k motor node k value corresponding
subtree
leaves tree information value actions classified
leaf information represented following set attributes leaf
value expected value actions classified leaf maximum
value leaves used assess goodness v achieved situation
guess value altered noise exploratory reasons leaf maximal
guess set actions select action executed next
relevance relevance value predictions value guess
partial command partial command accordance actions
classified leaf case internal nodes partial command
constructed collecting motors whose values fixed root
tree leaf consideration


fiporta celaya

include rule n w gw
leaf n
cow command w
con command n
motor n
closed node search compatible sub nodes
ea cow motor ea motor n
include rule get subtree value ea n w gw
else
subtrees n
include rule w gw
endfor
endif
else
open node specialize node
cow con
extend node
ea action cow con
set motor n motor ea
closed closed
k values motor ea
subtree n k node con motor ea k
open open
endfor
include rule n w gw
else
transform node leaf
transform leaf n qw gw w cow
closed closed
qw v
v qw
endif
gw guess
g gw
cow
endif
endif
endif
endif

figure include rule searches nodes node n partial command compatible partial command rule w extends nodes
insert leave tree

given moment inclusion partial rule tree produces specialization open nodes compatible rule see figure say open
node n compatible given rule w partial command node con
partial command rule cow assign different values motor
specialization open node extension node e branches


fireinforcement learning categorizable environments

partial rules
partial view
partial command
ru ev
v v
ru ev
v
ru ev
v v
ru ev
v
ru ev
v
ru ev
v
ru ev
v
ru ev
v

q

e




























guess









table set rules controller values q e stored guess
computed define partial views ru ev indicate
active current time step

added tree node transformation node leaf
node extended partial command rule affects motors included
partial command node means motor values taken
account tree used action evaluation according
rule consideration node extended one motors present
layers tree used generate layer open nodes current node
node considered closed inclusion rule procedure repeated
node different effects node closed motors affected
partial command rule affected partial command node
node transformed leaf storing value guess relevance attributes
extracted information associated rule
process stopped soon detect nodes closed e
external nodes tree leaves case rules still processed
effect tree form consequently useful action evaluation rule
consistently used action evaluation removed controller
toy size example illustrate tree action evaluation suppose
robot three motors accept two different values named v
v produces set different actions suppose given moment robot
controller includes set rules shown table action evaluation
figure rules processed least relevant one expanding
initially empty tree figure inclusion rule tree
extension tree see stages b e figure closing branches
converting open nodes leaves stages c f particular case tree becomes
completely closed processing rules active rules controller
end process tree five leaves three include two actions
two represent single action tree say value
situation tree constructed v given leaf circled
solid line figure additionally next action executed form


fiporta celaya

motor
command
true c

b


open
node

v

v

motor
command
v
v

open
node

v
value
guess
relevance
command
v
v

open
node

c

motor
command
v
v

open
node

value
guess
relevance
command
v
v

e

open
node

v

v

v

open
node

value
guess
relevance
command
v
v

motor
command
v
v
v
v

v
motor
command
v
v
v
value
guess
relevance
command
v
v
v

v

v

v

value
guess
relevance
command
v
v

motor
command
v
v

motor
command
v
v

v
value
guess
relevance
command
v
v

value
guess
relevance
command
v
v

value
guess
relevance
command
v
v
v

value
guess
relevance
command
v
v

motor
command
true c

v
motor
command
v
v

v

value
guess
relevance
command
v
v

value
guess
relevance
command
v
v
v

open
node

f

motor
command
v

motor
command
v
v

motor
command
v

motor
command
true c
v

v

v

v

v

v

motor
command
true c



motor
command
true c

v
motor
command
v
v
v
value
guess
relevance
command
v
v
v

v

value
guess
relevance
command
v
v

v
value
guess
relevance
command
v
v

value
guess
relevance
command
v
v
v

figure six different stages construction tree action evaluation
stage corresponds insertion one rule table



fireinforcement learning categorizable environments




log time















number void motors

brute force evaluation





treebased evaluation

figure log execution time seconds brute force vs treebased one

v v represents possible action optimal action
given leaf circled dashed line leaf larger guess value
cost largely depends specific set partial rules
processed worst case cost
nr lnm
nr number rules nm number motors l maximal range values
accepted motors worst case insert given rule
visit nodes maximally expanded tree e tree node l subtrees
final nodes branches still opened number nodes
tree
nm
x
lnm
li
lnm
l


transform cost expression taking account l nm total number
possible combinations elementary actions nc words total amount
actions therefore cost presented
nr nc
hand cost brute force
nr nc


fiporta celaya

worst case cost presented order cost
brute force however since l rules would enough close
maximally expanded tree one rule different values motor used last
still open layer tree cost tree would average
much smaller brute force
figure exemplifies different performance brute force action evaluation procedure tree one figure shows time taken execution
toy example section experiment defined void motors motors
whose actions effect environment seen number void
motors increases cost tree evaluation significantly less
brute force



fireinforcement learning categorizable environments

appendix c notation
uppercase used sets greek letters represent parameters

set states


individual states full views
ns
number states
f fdi nf
set feature detectors
partial view order k
v fdi fdik

set actions robot
na
number actions
ea eai ne
set elementary actions
nm
number motors robot
eai mi k
elementary action assigns value k motor mi
c eai eaik
partial command order k
ea eanm
action combination elementary actions full command
w v c
partial rule composed partial view v partial command c
w
empty partial rule
w w
composition two partial rules
c wi nr
controller set partial rules

maximum number elements c


c cant
subset rules active given time step previous one
c
active rules partial command accordance
qw
expected value partial rule w
ew
expected error value estimation partial rule w
e
average error value prediction
iw
confidence index
cw
confidence statistics partial rule w

top value confidence

index confidence function reaches value
w ew cw e cw error return prediction partial rule w
w w
relevance rule w
iw qw w
value interval partial rule w
mw
updating ratio statistics partial rule w

learning rate top value mw
u w
number times rule w used

winner c
relevant active partial rule w r action
guess
reliable value estimation action
ra
reward received execution

discount factor
v
goodness given situation
q ra v
value executing action given situation

number partial rules created time

redundancy threshold used partial rule elimination



fiporta celaya

references
arkin r c behavior robotics intelligent robotics autonomous agents
mit press
bellman r e dynamic programming princeton university press princeton
boutilier c dean hanks decision theoretic structural assumptions computational leverage journal artificial intelligence

brooks r intelligence without representation artificial intelligence

butz


c xcs implementation
http www cs bath ac uk amb lcsweb computer htm



xcs



c

celaya e porta j control six legged robot walking abrupt terrain
proceedings ieee international conference robotics automation
pp
celaya e porta j control structure locomotion legged
robot difficult terrain ieee robotics automation magazine special issue
walking robots
chapman kaelbling l p input generalization delayed reinforcement
learning performance comparisons proceedings international joint conference artificial intelligence pp
claus c boutilier c dynamics reinforcement learning cooperative
multiagent systems proceedings fifteenth national conference artificial
intelligence pp american association artificial intelligence
drummond c accelerating reinforcement learning composing solutions automatically identified subtasks journal artificial intelligence
edelman g neuronal darwinism oxford university press
hinton g mcclelland j rumelhart parallel distributed processing explorations microstructure cognition foundations chap distributed
representations mit press cambridge
ilg w muhlfriedel berns k hybrid learning architecture neural
networks adaptive control walking machine proceedings ieee
international conference robotics automation pp
kaelbling l p learning embedded systems bradford book mit press
cambridge
kaelbling l p littman l moore w reinforcement learning survey
journal artificial intelligence
kanerva p sparse distributed memory mit press cambridge
kirchner f q learning complex behaviors six legged walking machine
robotics autonomous systems


fireinforcement learning categorizable environments

kodjabachia j meyer j evolution development modular control
architectures locomotion six legged animats connection science

maes p brooks r learning coordinate behaviors proceedings
aaai pp
mahadevan connell j h automatic programming behavior robots
reinforcement learning artificial intelligence
mccallum k reinforcement learning selective perception hidden
state ph thesis department computer science
parker g b co evolving model parameters anytime learning evolutionary
robotics robotics autonomous systems
pendrith ryan r k c trace reinforcement
learning robotic control proceedings international workshop
learning autonomous robots robotlearn
poggio girosi f regularization learning equivalent
multilayer networks science pp
schmidhuber j speed prior simplicity measure yielding near optimal
computable predictions proceedings th annual conference computational learning theory colt oo lecture notes artificial intelligence
springer pp
sen learning coordinate without sharing information proceedings
twelfth national conference artificial intelligence pp american
association artificial intelligence
sutton r reinforcement learning architectures animats meyer j
wilson w eds proceedings first international conference simulation adaptive behavior animals animats pp mit press
bradford books
sutton r barto g reinforcement learning introduction bradford
book mit press
sutton r whitehead online learning random representations
proceedings eleventh international conference machine learning pp
morgan kaufman san francisco ca
sutton r generalization reinforcement learning successful examples
sparse coarse coding proceedings conference advances neural
information processing pp
sutton r precup singh mdps semi mdps framework
temporal abstraction reinforcement learning artificial intelligence
tan multi agent reinforcement learning independent vs cooperative agents
reading agents pp morgan kaufmann publishers inc


fiporta celaya

vallejo e e ramos f distributed genetic programming architecture
evolution robust insect locomotion controllers meyer j berthoz
floreano roitblat h l wilson w eds supplement proceedings
sixth international conference simulation adaptive behavior animals
animats pp international society adaptive behavior
venturini g apprentissage adaptatif et apprentissage supervise par algorithme
genetique ph thesis
watkins c j c h dayan p q learning machine learning
widrow b hoff adaptive switching circuits western electronic
convention pp institute radio engineers ieee
wilson w classifier fitness accuracy evolutionary computation

wilson w explore exploit strategies autonomy animals animats proceedings th international conference simulation adaptive
behavior pp




