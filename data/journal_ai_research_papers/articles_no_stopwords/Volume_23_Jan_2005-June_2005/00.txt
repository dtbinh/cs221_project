Journal Artificial Intelligence Research 23 (2005) 1-40

Submitted 05/04; published 01/05

Finding Approximate POMDP Solutions Belief
Compression
Nicholas Roy

nickroy@mit.edu

Massachusetts Institute Technology,
Computer Science Artificial Intelligence Laboratory
Cambridge,

Geoffrey Gordon

ggordon@cs.cmu.edu

Carnegie Mellon University, School Computer Science
Pittsburgh, PA

Sebastian Thrun

thrun@stanford.edu

Stanford University, Computer Science Department
Stanford, CA

Abstract
Standard value function approaches finding policies Partially Observable Markov
Decision Processes (POMDPs) generally considered intractable large models.
intractability algorithms large extent consequence computing
exact, optimal policy entire belief space. However, real-world POMDP
problems, computing optimal policy full belief space often unnecessary
good control even problems complicated policy classes. beliefs experienced
controller often lie near structured, low-dimensional subspace embedded
high-dimensional belief space. Finding good approximation optimal value function
subspace much easier computing full value function.
introduce new method solving large-scale POMDPs reducing dimensionality belief space. use Exponential family Principal Components Analysis (Collins, Dasgupta, & Schapire, 2002) represent sparse, high-dimensional belief spaces
using small sets learned features belief state. plan terms
low-dimensional belief features. planning low-dimensional space, find
policies POMDP models orders magnitude larger models
handled conventional techniques.
demonstrate use algorithm synthetic problem mobile robot
navigation tasks.

1. Introduction
Decision making one central problems artificial intelligence robotics.
robots deployed world accomplish specific tasks, real world
difficult place actactions serious consequences. Figure 1(a) depicts
mobile robot, Pearl, designed operate environment shown Figure 1(b),
Longwood retirement facility Pittsburgh. Real world environments Longwood
characterized uncertainty; sensors cameras range finders noisy
entire world always observable. large number state estimation techniques
explicitly recognize impossibility correctly identifying true state world
(Gutmann, Burgard, Fox, & Konolige, 1998; Olson, 2000; Gutmann & Fox, 2002; Kanazawa,
c
2005
AI Access Foundation. rights reserved.

fiRoy, Gordon, & Thrun

Koller, & Russell, 1995; Isard & Blake, 1998) using probabilistic techniques track
location robot. state estimators Kalman filter (Leonard & DurrantWhyte, 1991) Markov localization (Fox, Burgard, & Thrun, 1999; Thrun, Fox, Burgard,
& Dellaert, 2000) provide (possibly factored, Boyen & Koller, 1998) distribution
possible states world instead single (possibly incorrect) state estimate.

(a)

Figure 1:

(b)

planner mobile robot Pearl, shown (a), must able navigate
reliably real environments Longwood Oakmont retirement facility,
shown (b). white areas map free space, black pixels
obstacles, grey areas regions map uncertainty. Notice
large open spaces, many symmetries lead ambiguity robots
position. map 53.6m 37.9m, resolution 0.1m 0.1m per pixel.

contrast, controllers motion planners, dialogue systems, etc. rarely model
notions uncertainty. state estimate full probability distribution,
controller often uses heuristic extract single best state, distributions
mean mode. planners compensate inevitable estimation errors robust control (Chen, 2000; Bagnell & Schneider, 2001), deployed systems incorporate
full probabilistic state estimate planning. Although most-likely-state method
simple used successfully real applications (Nourbakhsh, Powers, &
Birchfield, 1995), substantial control errors result distribution possible
states uncertain. single state estimate wrong, planner likely choose
unreasonable action.
Figure 2 illustrates difference conventional controllers model
uncertainty. figure, robot must navigate bottom right corner top
left, limited range sensing (up 2m) noisy dead reckoning.1 impoverished
1. purposes example sensing dead reckoning artificially poor,
phenomenon would occur naturally larger-scale environments.

2

fiFinding Approximate POMDP Solutions Belief Compression

sensor data cause robots state estimate become quite uncertain strays
far environmental structures use localize itself. left (Figure 2a)
example trajectory motion planner knowledge uncertainty
state estimate mechanism taking uncertainty account. robots
trajectory diverges desired path, robot incorrectly believes arrived
goal. shown state estimates reflect high uncertainty
robot position. right (Figure 2b) example trajectory controller
model positional uncertainty, take action keep uncertainty small following
walls, arrive reliably goal.

Goal

Goal

Measured Path

Measured Path
True Path
True Path

Start

Start

(a) Conventional controller

Figure 2:

(b) Robust controller

Two possible trajectories navigation Longwood Oakmont environment. robot limited range sensing (up 2m) poor dead-reckoning
odometry. (a) trajectory conventional motion planner uses
single state estimate, minimizes travel distance. (b) trajectory
robust controller models state uncertainty minimize travel
distance uncertainty.

controller Figure 2(b) derived representation called partially observable Markov decision process (POMDP). POMDPs technique making decisions
based probabilistic estimates state world, rather absolute knowledge
true state. POMDP uses priori model world together history
actions taken observations received order infer probability distribution,
belief, possible states world. controller chooses actions, based upon
current belief, maximize reward expects receive time.
advantage using POMDPs decision making resulting policies
handle uncertainty well. POMDP planning process take advantage actions
implicitly reduce uncertainty, even problem specification (e.g., reward function)
explicitly reward actions. disadvantage POMDPs finding
optimal policy computationally intractable. Existing techniques finding exact optimal
3

fiRoy, Gordon, & Thrun

plans POMDPs typically cannot handle problems hundred states
(Hauskrecht, 2000; Zhang & Zhang, 2001). planning problems involving real, physical
systems cannot expressed compactly; would deploy robots plan
thousands possible states world (e.g., map grid cells), thousands possible
observations (e.g., laser range measurements) actions (e.g., velocities).
paper, describe algorithm finding approximate solutions realworld POMDPs. algorithm arises insight exact POMDP policies use
unnecessarily complex, high-dimensional representations beliefs controller
expect experience. finding low-dimensional representations, planning process
becomes much tractable.
first describe find low-dimensional representations beliefs realworld POMDPs; use variant common dimensionality-reduction technique
called Principal Components Analysis. particular variant use modifies loss
function PCA order better model data probability distributions. Using
low-dimensional representations, describe plan low-dimensional space,
conclude experimental results robot control tasks.

2. Partially Observable Markov Decision Processes
partially observable Markov decision process (POMDP) model deciding
act accessible, stochastic environment known transition model (Russell
Norvig (1995), pg. 500). POMDP described following:









set states = {s1 , s2 , . . . s|S| }
set actions = {a1 , a2 , . . . , a|A| }
set observations Z = {z1 , z2 , . . . , z|Z| }
set transition probabilities (si , a, sj ) = p(sj |si , a)
set observation probabilities O(zi , a, sj ) = p(zi |sj , a)
set rewards R : 7 R
discount factor [0, 1]
initial belief p0 (s)

transition probabilities describe state evolves actions, represent Markov assumption: next state depends current (unobservable)
state action independent preceding (unobserved) states actions.
reward function describes objective control, discount factor used
ensure reasonable behaviour face unlimited time. optimal policy known
always exist discounted ( < 1) case bounded immediate reward (Howard,
1960).
POMDP policies often computed using value function belief space.
value function V (b) given policy defined long-term expected reward
controller receive starting belief b executing policy horizon time,
may infinite. optimal POMDP policy maximizes value function.
value function POMDP policy finite horizon described using piecewise linear function space beliefs. Many algorithms compute value function
iteratively, evaluating refining current value function estimate
4

fiFinding Approximate POMDP Solutions Belief Compression

refinements improve expected reward policy belief. Figure 3(a)
shows belief space three-state problem. belief space two-dimensional,
shaded simplex. point simplex corresponds particular belief (a threedimensional vector), corners simplex represent beliefs state
known 100% certainty. value function shown Figure 3(b) gives long-term
expected reward policy, starting belief simplex.

1

10
9

0.8

8
7

0.6

6
5

0.4

4
3

0.2

2
0
0

1
0
0.2

0.2
0.4
0.6
0.8
1

0

0.2

0.4

0.6

0.8

0.4

1

0.6
0.8
1

(a) belief space
Figure 3:

0

0.2

0.4

0.6

0.8

1

(b) value function

(a) belief space three-state problem two-dimensional, shaded
simplex. (b) value function defined belief space. purposes
visualization, set beliefs constitutes belief space shown (a)
projected onto XY plane (b); value function rises along
positive Z axis. point belief space corresponds specific
distribution, value function point gives expected reward
policy starting belief. belief space (and therefore value
function) one fewer dimension total number states
problem.

process evaluating refining value function core solving
POMDPs considered intractable. value function defined space
beliefs, continuous high-dimensional; belief space one fewer
dimension number states model. navigation problem map
thousands possible states, computing value function optimization problem
continuous space many thousands dimensions, feasible existing
algorithms.
However, careful consideration real-world problems suggests possible approach
finding approximate value functions. examine beliefs navigating mobile
robot encounters, beliefs share common attributes. beliefs typically
small number modes, particular shape modes fairly generic. modes
move change variance, ways modes change relatively
constrained. fact, even real world navigation problems large belief spaces,
beliefs degrees freedom.
Figure 4(a) illustrates idea: shows typical belief mobile robot might
experience navigating nursing home environment Figure 1(b). visualize
distribution sample set poses (also called particles) according distribution
5

fiRoy, Gordon, & Thrun

plot particles map. distribution unimodal probability mass
mostly concentrated small area. Figure 4(b) shows different kind belief:
probability mass spread wide area, multiple modes, locations
particles bear little relationship map. would difficult find sequence
actions observations would result belief.

particles


(a) common belief

Figure 4:

(b) unlikely belief

Two example probability distributions robot pose. small black dots
particles drawn distribution discrete grid positions. left
distribution robots location relatively certain; kind compact,
unimodal distribution common robot navigation. right
different, implausible distribution. right hand distribution sufficiently
unlikely afford ignore it; even unable distinguish
belief belief result fail identify optimal action,
quality controller unaffected.

real-world beliefs degrees freedom, concentrated near
low-dimensional subset high-dimensional belief spacethat is, beliefs experienced
controller lie near structured, low-dimensional surface embedded belief
space. find surface, representation belief state terms
small set bases features. One benefit representation need
plan terms small set features: finding value functions low-dimensional
spaces typically easier finding value functions high-dimensional spaces.
two potential disadvantages sort representation. first
contains approximation: longer finding complete, optimal POMDP policy.
Instead (as suggested Figure 5) trying find representations belief
rich enough allow good control sufficiently parsimonious make
6

fiFinding Approximate POMDP Solutions Belief Compression

planning problem tractable. second disadvantage technical one:
making nonlinear transformation belief space, POMDP planning algorithms
assume convex value function longer work. discuss problem detail
Section 6.
Conventional
Path Planner

POMDP

Tractable
Robust

Figure 5:

Intractable
Robust

useful planner lies somewhere continuum MDP-style
approximations full POMDP solution.

3. Dimensionality Reduction
order find low-dimensional representation beliefs, use statistical dimensionality reduction algorithms (Cox & Cox, 1994). algorithms search projection
original high-dimensional representation beliefs lower-dimensional compact representation. is, search low-dimensional surface, embedded
high-dimensional belief space, passes near sample beliefs. consider
evolution beliefs POMDP trajectory inside belief space, assumption trajectories large, real world POMDPs lie near low-dimensional
surface embedded belief space. Figure 6 depicts example low-dimensional surface
embedded belief space three-state POMDP described previous section.
1

0.8

0.6

0.4

0.2

0
0
0.2
0.4
0.6
0.8
1

Figure 6:

0

0.2

0.4

0.6

0.8

1

one-dimensional surface (black line) embedded two-dimensional belief space
(gray triangle). black dot represents single belief probability distribution
experienced controller. beliefs lie near low-dimensional surface.

Ideally, dimensionality reduction involves information lossall aspects data
recovered equally well low-dimensional representation highdimensional one. practice, though, see use lossy representations
belief (that is, representations may allow original data beliefs
recovered without error) still get good control. But, see finding
representations probability distributions require careful trade-off
7

fiRoy, Gordon, & Thrun

preserving important aspects distributions using dimensions possible.
measure quality representation penalizing reconstruction errors
loss function (Collins et al., 2002). loss function provides quantitative way
measure errors representing data, different loss functions result different
low-dimensional representations.
Principal Components Analysis
One common forms dimensionality reduction Principal Components Analysis (Joliffe, 1986). Given set data, PCA finds linear lower-dimensional representation data variance reconstructed data preserved. Intuitively,
PCA finds low-dimensional hyperplane that, project data onto
hyperplane, variance data changed little possible. transformation
preserves variance seems appealing maximally preserve ability distinguish beliefs far apart Euclidean norm. see below, however,
Euclidean norm appropriate way measure distance beliefs
goal preserve ability choose good actions.
first assume data set n beliefs {b1 , . . . , bn } B, belief bi
B, high-dimensional belief space. write beliefs column vectors
matrix B = [b1 | . . . |bn ], B R|S|n . use PCA compute low-dimensional
representation beliefs factoring B matrices U B,
B = U B .

(1)

equation (1), U R|S|l corresponds matrix bases span low-dimensional
space l < |S| dimensions. B Rnl represents data low-dimensional space.2
geometric perspective, U comprises set bases span hyperplane B
high-dimensional space B; B co-ordinates data hyperplane.
hyperplane dimensionality l exists contains data exactly, PCA find surface given dimensionality best preserves variance data, projecting
data onto hyperplane reconstructing it. Minimizing change variance original data B reconstruction U B equivalent minimizing
sum squared error loss:
L(B, U, B) = kB U B k2F .

(2)

PCA Performance
Figure 7 shows toy problem use evaluate success PCA finding
low-dimensional representations. abstract model two-dimensional state space:
one dimension position along one two circular corridors, one binary variable
determines corridor in. States s1 . . . s100 inclusive correspond one corridor,
states s101 . . . s200 correspond other. reward known position
different corridor; therefore, agent needs discover corridor, move
2. Many descriptions PCA based factorization U SV , U V column-orthonormal
diagonal. could enforce similar constraint identifying B = V S; case columns U
would orthonormal B would orthogonal.

8

fiFinding Approximate POMDP Solutions Belief Compression

appropriate position, declare arrived goal. goal declared
system resets (regardless whether agent actually goal). agent
4 actions: left, right, sense_corridor, declare_goal. observation
transition probabilities given discretized von Mises distributions (Mardia & Jupp,
2000; Shatkay & Kaelbling, 2002), exponential family distribution defined [ : ).
von Mises distribution wrapped analog Gaussian; accounts fact
two ends corridor connected. sum two von Mises variates
another von Mises variate, product two von Mises likelihoods
scaled von Mises likelihood, guarantee true belief distribution always
von Mises distribution corridor action observation.
instance problem consists 200 states, 4 actions 102 observations.
Actions 1 2 move controller left right (with von Mises noise) action
3 returns observation uniquely correctly identifies half maze
agent (the top half bottom half). Observations returned actions 1 2
identify current state modulo 100: probability observation von Mises
distribution mean equal true state (modulo 100). is, observations
indicate approximately agent horizontally.
Max Prob.
Obs. = #1
1

Max Prob.
Obs. = #3
2

Max Prob.
Obs. = #5

3

4

103

104

5

6

105

106

7

Reward
101

102

107

...
...

Max Prob.
Obs. = #100
100

Observation "top"
action #3 prob. 1

Reward

Observation "bottom"
action #3 prob. 1

200

Figure 7: toy maze 200 states.

maze interesting relatively large POMDP standards (200 states)
contains particular kind uncertaintythe agent must use action 3 point
uniquely identify half maze in; remaining actions result observations
contain information corridor agent in. problem large
solved conventional POMDP value iteration, structured heuristic
policies perform poorly.
collected data set 500 beliefs assessed performance PCA beliefs
problem. data collected using hand-coded controller, alternating
random exploration actions MDP solution, taking current state
maximum-likelihood state belief. Figure 8 shows 4 sample beliefs data
set. Notice beliefs essentially two discretized von Mises distributions
different weights, one half maze. starting belief state
left-most distribution Figure 8: equal probability top bottom corridors,
position along corridor following discretized von Mises distribution concentration
parameter 1.0 (meaning p(state) falls 1/e maximum value move 1/4
way around corridor likely state).
9

fiRoy, Gordon, & Thrun

Sample Belief

Sample Belief

0.04

0.04

0.035

0.035

0.03

0.03

0.025

0.025

0.02
0.015
0.01

0.02
0.015
0.01

0.005

0.005

0

0

-0.005

0

20

40

60

-0.005

80 100 120 140 160 180 200
State

Probability

0.03
0.025

Probability

Probability

Sample Belief
0.04
0.035

0.02
0.015
0.01
0.005
0

0

20

40

60

-0.005

80 100 120 140 160 180 200
State

0

20

40

60

80 100 120 140 160 180 200
State

Sample Belief
0.14
0.12

Probability

0.1
0.08
0.06
0.04
0.02
0

Figure 8:

0

20

40

60

80

100 120 140 160 180 200
State

Sample beliefs toy problem, sample set 500, different (noncontiguous) points time. left-most belief initial belief state.

Figure 9 examines performance PCA representing beliefs data set
computing average error original beliefs B reconstructions U B.3
Figure 9(a) see average squared error (squared L2 ) compared number
bases, Figure 9(b), see average Kullbach-Leibler (KL) divergence.
KL divergence belief b reconstruction r = U b low-dimensional
representation b given
KL(b k r) =

|S|
X

b(si ) ln

i=1



b(si )
r(si )



(3)

Minimizing squared L2 error explicit objective PCA, KL divergence
appropriate measure much two probability distributions differ. 4
Unfortunately, PCA performs poorly representing probability distributions. Despite
fact probability distributions collected data set 3 degrees
freedom, reconstruction error remains relatively high somewhere 10
15 basis functions. examine reconstruction sample belief, see
kinds errors PCA making. Figure 10 shows sample belief (the solid line)
reconstruction (the dotted line). Notice reconstructed belief strange
artifacts: contains ringing (multiple small modes), negative regions.
PCA purely geometric process; notion original data probability
distributions, therefore free generate reconstructions data contain
negative numbers sum 1.
3. use popular implementation PCA based Golub-Reinsche algorithm (Golub & Reinsch,
1970) available GNU Scientific Library (Galassi, Davies, Theiler, Gough, Jungman, Booth,
& Rossi, 2002).
4. Note computing KL divergence reconstruction original belief,
shift reconstruction non-negative, rescale sum 1.

10

fiFinding Approximate POMDP Solutions Belief Compression

Average L2 Error vs. Number Bases

Average KL Divergence vs. Number Bases

0.02

1.6
Average KL Divergence

1.4
Average L2 Error

0.015
0.01
0.005
0

1.2
1
0.8
0.6
0.4
0.2
0

0

5

10
15
20
Number Bases

25

30

0

(a) Average Squared L-2 Error
Figure 9:

5

10

15
20
Number Bases

25

30

(b) Average KL Divergence

average error original sample set B reconstructions U B.
(a) Squared L2 error, explicitly minimized PCA, (b) KL divergence.
error bars represent standard deviation mean error
500 beliefs.
Example Belief Reconstruction
0.045

Original Belief
Reconstructed Belief

0.04

Probability

0.035
0.03
0.025
0.02
0.015
0.01
0.005
0
-0.005

0

20

40

60

80

100 120 140 160 180 200
State

Figure 10: example belief reconstruction, using 10 bases.

Notice PCA process making significant errors lowprobability regions belief. particularly unfortunate real-world probability distributions tend characterized compact masses probability, surrounded
large regions zero probability (e.g., Figure 4a). therefore need modify
PCA ensure reconstructions probability distributions, improve representation sparse probability distributions reducing errors made low-probability
events.
question answered loss functions available instead sum
squared errors, equation (2). would loss function better reflects
need represent probability distributions.
11

fiRoy, Gordon, & Thrun

4. Exponential Family PCA
conventional view PCA geometric one, finding low-dimensional projection
minimizes squared-error loss. alternate view probabilistic one:
data consist samples drawn probability distribution, PCA algorithm
finding parameters generative distribution maximize likelihood
data. squared-error loss function corresponds assumption data
generated Gaussian distribution. Collins et al. (2002) demonstrated PCA
generalized range loss functions modeling data different exponential
families probability distributions Gaussian, binomial, Poisson.
exponential family distribution corresponds different loss function variant
PCA, Collins et al. (2002) refer generalization PCA arbitrary exponential
family data-likelihood models Exponential family PCA E-PCA.
E-PCA model represents reconstructed data using low-dimensional weight
vector b, basis matrix U , link function f :
b f (U b)

(4)

E-PCA model uses different link function, derived data
likelihood model (and corresponding error distribution loss function). link
function mapping data space another space data
linearly represented.
link function f mechanism E-PCA generalizes dimensionality reduction non-linear models. example, identity link function corresponds
Gaussian errors reduces E-PCA regular PCA, sigmoid link function
corresponds Bernoulli errors produces kind logistic PCA 0-1 valued data.
nonlinear link functions correspond non-Gaussian exponential families
distributions.
find parameters E-PCA model maximizing log-likelihood
data model, shown (Collins et al., 2002) equivalent
minimizing generalized Bregman divergence
BF (b k U b) = F (U b) b U b + F (b)

(5)

low-dimensional high-dimensional representations, solve using
convex optimization techniques. (Here F convex function whose derivative f ,
F convex dual F . ignore F purpose minimizing equation 5
since value b fixed.) relationship PCA E-PCA link
functions reminiscent relationship linear regression Generalized Linear
Models (McCullagh & Nelder, 1983).
apply E-PCA belief compression, need choose link function accurately reflects fact beliefs probability distributions. choose link
function
f (U b) = eU b
(6)
P U b
P
hard verify F (U b) =
e F (b) = b ln b b. So, equation 5
becomes
X
X
BF (b k U b) =
eU b b U b + b ln b
b
(7)
12

fiFinding Approximate POMDP Solutions Belief Compression

write b = f (U b), equation 7 becomes
BF (b k U b) = b ln b b ln b +

X

b

X

b = U KL(b k b)

U KL unnormalized KL divergence. Thus, choosing exponential link function (6) corresponds minimizing unnormalized KL divergence original
belief reconstruction. loss function intuitively reasonable choice measuring error reconstructing probability distribution.5 exponential link function
corresponds Poisson error model component reconstructed belief.
choice loss link functions two advantages: first, exponential link
function constrains low-dimensional representation eU b positive. Second, error
model predicts variance belief component proportional expected
value. Since PCA makes significant errors close 0, wish increase penalty
errors small probabilities, error model accomplishes that.
compute loss bi , ignoring terms depend data b, then6

L(B, U, B) =

|B|
X
i=1


eU bi bi U bi .

(8)

introduction link function raises question: instead using complex
machinery E-PCA, could choose non-linear function project data
space linear, use conventional PCA? difficulty
approach course identifying function; general, good link functions E-PCA
related good nonlinear functions application regular PCA. So,
might appear reasonable use PCA find low-dimensional representation log
beliefs, rather use E-PCA exponential link function find representation
beliefs directly, approach performs poorly surface locally
well-approximated log projection. E-PCA viewed minimizing weighted
least-squares chooses distance metric appropriately local. Using conventional
PCA log beliefs performs poorly situations beliefs contain extremely
small zero probability entries.
P
5. chosen link function eU b / eU b would arrived normalized KL divergence,
perhaps even intuitively reasonable way measure error reconstructing
probability distribution. more-complicated link function would made difficult
derive Newton equations following pages, impossible; experimented
resulting algorithm found produces qualitatively similar results algorithm described
here. Using normalized KL divergence one advantage: allow us get away
one fewer basis function planning, since unnormalized KL divergence E-PCA optimization
must learn basis explicitly represent normalization constant.
6. E-PCA related Lee Seungs (1999) non-negative matrix factorization. One NMF loss
functions presented Lee Seung (1999) penalizes KL-divergence matrix
reconstruction, equation 8; but, NMF loss incorporate link function
E-PCA loss. Another NMF loss function presented Lee Seung (1999) penalizes squared
error constrains factors nonnegative; resulting model example (GL) 2 M,
generalization E-PCA described Gordon (2003).

13

fiRoy, Gordon, & Thrun

Finding E-PCA Parameters
Algorithms conventional PCA guaranteed converge unique answer independent initialization. general, E-PCA property: loss function (8)
may multiple distinct local minima. However, problem finding best B given
B U convex; convex optimization problems well studied unique global
solutions (Rockafellar, 1970). Similarly, problem finding best U given B B
convex. So, possible local minima joint space U B highly constrained,
finding U B require solving general non-convex optimization problem.
Gordon (2003) describes fast, Newtons Method approach computing U B
summarize here. algorithm related Iteratively Reweighted Least Squares,
popular algorithm generalized linear regression (McCullagh & Nelder, 1983). order
use Newtons Method minimize equation (8), need derivative respect U
B:

(U B)

L(B, U, B) =
e

B U B
(9)
U
U
U
= e(U B) B B B
(10)
= (e(U B) B)B

(11)


(U B)


L(B, U, B) =
e

B U B
B
B
B
= U e(U B) U B


= U (e

(U B)

B).

(12)
(13)
(14)

set right hand side equation (14) zero, iteratively compute Bj ,
column B, Newtons method. Let us set q(Bj ) = U (e(U Bj ) Bj ), linearize
Bj find roots q(). gives
j th

Bjnew = Bj
Bjnew Bj

=

q(Bj )
q 0 (Bj )

U (e(U Bj ) Bj )
q 0 (Bj )

(15)
(16)

Note equation 15 formulation Newtons method finding roots q, typically
written
f (xn )
xn+1 = xn 0
.
(17)
f (xn )
need expression q 0 :
q
Bj

=
=


U (e(U Bj ) Bj )
Bj

U e(U Bj )
Bj

= U Dj U
14

(18)
(19)
(20)

fiFinding Approximate POMDP Solutions Belief Compression

define Dj terms diag operator returns diagonal matrix:



Dj = diag(eU Bj ),

(21)


b(s0 ) . . .
0

.. .
..
diag(b) = ...
.
.
0
. . . b(s|S| )

(22)



Combining equation (15) equation (20), get
(U Dj U )(Bjnew Bj ) = U (Bj eU Bj )

(23)

U Dj U Bjnew = (U Dj U )Bj + U Dj Dj1 (Bj eU Bj )
= U Dj (U Bj + Dj1 (Bj eU Bj ),

(24)
(25)

weighted least-squares problem solved standard linear algebra
techniques. order ensure solution numerically well-conditioned, typically
add regularizer divisor,
Bjnew =

U Dj (U Bj + Dj1 (Bj eU Bj )
(U Dj U + 105 Il )

.

(26)

Il l l identity matrix. Similarly, compute new U computing Ui ,
ith row U ,
(Ui B + (Bi eUi B )Di1 )Di B
.
(27)
Uinew =
(BDi B + 105 Il )
E-PCA Algorithm
algorithm automatically finding good low-dimensional representation
B high-dimensional belief set B. algorithm given Table 1; optimization
iterated termination condition reached, finite number iterations,
minimum error achieved.
steps 7 9 raise one issue. Although solving row U column B
separately convex optimization problem, solving two matrices simultaneously
not. therefore subject potential local minima; experiments
find problem, expect need find ways address local
minimum problem order scale even complicated domains.
bases U found, finding low-dimensional representation highdimensional belief convex problem; compute best answer iterating equation (26). Recovering full-dimensional belief b low-dimensional representation b
straightforward:
x = eU b .
(28)
definition PCA explicitly factor data U , B many
presentations do. three-part representation PCA, contains singular values
15

fiRoy, Gordon, & Thrun

1. Collect set sample beliefs high-dimensional belief space
2. Assemble samples data matrix B = [b1 | . . . |b|B| ]
3. Choose appropriate loss function, L(B, U, B)
4. Fix initial estimate B U randomly
5.
6.

column Bj B,
Compute Bjnew using current U estimate equation (26)

7.
8.

row Ui U ,

9.

Compute Uinew using new B estimate equation (27)

10. L(B, U, B) >

Table 1:

E-PCA Algorithm finding low-dimensional representation POMDP,
including Gordons Newtons method (2003).

decomposition, U B orthonormal. use two-part representation
B f (U B) quantity E-PCA decomposition corresponds
singular values PCA. result, U B general orthonormal.
desired, though, possible orthonormalize U additional step optimization
using conventional PCA adjust B accordingly.

5. E-PCA Performance
Using loss function equation (8) iterative optimization procedure described
equation (26) equation (27) find low-dimensional factorization, look
well dimensionality-reduction procedure performs POMDP examples.
Toy Problem
Recall Figure 9 unable find good representations data
fewer 10 15 bases, even though domain knowledge indicated data
3 degrees freedom (horizontal position mode along corridor, concentration
mode, probability top bottom corridor). Examining one
sample beliefs Figure 10, saw representation worst lowprobability regions. take data set toy example, use E-PCA
find low-dimensional representation compare performance PCA E-PCA.
Figure 11(a) shows E-PCA substantially efficient representing data,
see KL divergence falling close 0 4 bases. Additionally, squared L 2
error 4 bases 4.64 104 . (We need 4 bases perfect reconstruction, rather
3, since must include constant basis function. small amount reconstruction
16

fiFinding Approximate POMDP Solutions Belief Compression

error 4 bases remains stopped optimization procedure fully
converged.)

Average KL Divergence vs. Number Bases (E-PCA)

Example Belief Reconstruction Using 3 Bases

1.6

0.045
0.04
0.035
0.03

1.2
1

Probability

Average KL Divergence

1.4

0.8
0.6
0.4
0
0

5

10

15
20
Number Bases

25

30

(a) Reconstruction Performance

Probability

Probability

0.036
0.034
0.032
0.03
0.028

40

60

80

100 120 140 160 180 200
State

2e-09
1.5e-09
1e-09
5e-10

0.026

0
146

148

150
State

152

154

90

(c) Belief Reconstruction Near
Peak

Figure 11:

20

Example Belief Reconstruction Using 3 Bases
3e-09
Original Belief
Reconstructed Belief
2.5e-09

Original Belief
Reconstructed Belief

0.038

0

(b) Example Belief Reconstruction

Example Belief Reconstruction Using 3 Bases
0.04

0.025
0.02
0.015
0.01
0.005
0
-0.005

0.2

Original Belief
Reconstructed Belief

95

100
State

105

110

(d) Belief Reconstruction LowProbability Region

(a) average KL divergence original sample set reconstructions. KL divergence 0.018 4 bases. error bars represent
standard deviation mean 500 beliefs. (b) example
belief Figure 10 reconstruction using 3 bases. reconstruction
shows small errors peak mode. shown reconstruction
using 4 bases, original belief reconstruction indistinguishable naked eye. (c) (d) show fine detail original belief
reconstruction two parts state space. Although reconstruction
perfect, low-probability area, see error approximately
2 109 .

17

fiRoy, Gordon, & Thrun

Figure 11(b) shows E-PCA reconstruction example belief Figure 10.
see many artifacts present PCA reconstruction absent. Using
3 bases, see E-PCA reconstruction already substantially better PCA
using 10 bases, although small errors peaks (e.g., Figure 11c)
two modes. (Using 4 bases, E-PCA reconstruction indistinguishable naked eye
original belief.) kind accuracy 3 4 bases typical
data set.
Robot Beliefs
Although performance E-PCA finding good representations abstract problem
compelling, would ideally able use algorithm real-world problems,
robot navigation problem Figure 2. Figures 12 13 show results two
robot navigation problems, performed using physically-realistic simulation (although
artificially limited sensing dead-reckoning). collected sample set 500 beliefs
moving robot around environment using heuristic controller, computed
low-dimensional belief space B according algorithm Table 1. full state space
47.7m 17m, discretized resolution 1m 1m per pixel, total 799 states.
Figure 12(a) shows sample belief, Figure 12(b) reconstruction using 5 bases.
Figure 12(c) see average reconstruction performance E-PCA approach,
measured average KL-divergence sample belief reconstruction.
comparison, performance PCA E-PCA plotted. E-PCA error falls
0.02 5 bases, suggesting 5 bases sufficient good reconstruction.
substantial reduction, allowing us represent beliefs problem using
5 parameters, rather 799 parameters. Notice many states lie regions
outside map; is, states never receive probability mass
removed. removing states would trivial operation, E-PCA correctly
able automatically.
Figure 13, similar results shown different environment. sample set 500
beliefs collected using heuristic controller, low-dimensional belief space
B computed using E-PCA. full state space 53.6m 37.9m, resolution
.5m .5m per pixel. example belief shown Figure 13(a), reconstruction
using 6 bases shown Figure 13(b). reconstruction performance measured
average KL divergence shown Figure 13(c); error falls close 0 around 6
bases, minimal improvement thereafter.

6. Computing POMDP policies
Exponential-family Principal Components Analysis model gives us way find
low-dimensional representation beliefs occur particular problem.
two real-world navigation problems tried, algorithm proved effective
finding low-dimensional representations, showing reductions 800 states
2, 000 states 5 6 bases. 5 6 dimensional belief space allow much
tractable computation value function, able solve much
larger POMDPs could solved previously.
18

fiFinding Approximate POMDP Solutions Belief Compression

KL Divergence Sampled Beliefs Reconstructions

Particles form
bimodal distribution

45

E-PCA
PCA

40

KL Divergence

35

(a) Original Belief

30
25
20
15
10
5
0
1

2

3

4

5

6

7

8

9

Number Bases

(c) Reconstruction performance

(b) Reconstruction

Figure 12:

(a) sample belief robot navigation task. (b) reconstruction
belief learned E-PCA representation using 5 bases. (c) average
KL divergence sample beliefs reconstructions
number bases used. Notice E-PCA error falls close 0 5 bases,
whereas conventional PCA much worse reconstruction error even 9 bases,
improving rapidly.

KL Divergence Sampled Beliefs Reconstructions
4
3.5
KL Divergence

3
2.5
2
1.5
1
0.5
0

(a) sample belief

Figure 13:

(b) reconstruction

0

2

4

6
8
10
Number Bases

(c) Average
performance

12

14

16

reconstruction

(a) sample belief navigation problem Longwood, cf. Figure 2. (b)
reconstruction learned E-PCA representation using 6 bases. (c)
average KL divergence sample beliefs reconstructions
number bases used.

Unfortunately, longer use conventional POMDP value iteration find
optimal policy given low-dimensional set belief space features. POMDP value iteration depends fact value function convex belief space.
19

fiRoy, Gordon, & Thrun

compute non-linear transformation beliefs recover coordinates
low-dimensional belief surface, lose convexity value function (compare Figure 3 Figure 6 see why). result, value function cannot expressed
supremum set hyperplanes low-dimensional belief space.
So, instead using POMDP value iteration, build low-dimensional discrete
belief space MDP use MDP value iteration. Since know form
value function, turn function approximation. Gordon (1995) proved
fitted value iteration algorithm guaranteed find bounded-error approximation
(possibly discounted) MDPs value function, long use combination
function approximator averager. Averagers function approximators
non-expansions max-norm; is, exaggerate errors training data.
experiments below, use regular grids well irregular, variable-resolution grids
based 1-nearest-neighbour discretization, represented set low-dimensional beliefs
B ,
B = {b1 , b2 , . . . , b|B | }.
(29)
approximations averagers; averagers include linear interpolation, knearest-neighbours, local weighted averaging. focus detail exact
mechanism discretizing low-dimensional space, outside scope
paper. resolution regular grid cases chosen empirically; section 7
describe specific variable resolution discretization scheme worked well empirically.
reader consult Munos Moore (2002) Zhou Hansen (2001)
sophisticated representations.
fitted value iteration algorithm uses following update rule compute t-step
lookahead value function V (t 1)-step lookahead value function V t1 :


|B |
X
V (bi ) = max R (bi , a) +
(bi , a, bj ) V t1 (bj )
(30)


j=1

R approximate reward transition functions based dynamics
POMDP, result E-PCA, finite set low-dimensional belief samples
B using function approximator. Note problems described
paper, problem require discounting ( = 1). following sections describe
compute model parameters R .
Computing Reward Function
original reward function R(s, a) represents immediate reward taking action
state s. cannot know, given either low-dimensional high-dimensional belief,
immediate reward be, compute expected reward. therefore
represent reward expected value immediate reward full model,
current belief:
R (b , a) = Eb (R(s, a))

(31)

|S|

=

X
i=1

20

R(si , a)b(si ).

(32)

fiFinding Approximate POMDP Solutions Belief Compression

Equation (32) requires us recover high-dimensional belief b low-dimensional
representation b , shown equation (28).
many problems, reward function R effect giving low immediate
reward belief states high entropy. is, many problems planner
driven towards beliefs centred high-reward states low uncertainty.
property intuitively desirable: beliefs robot worry
immediate bad outcome.
Computing Transition Function
Computing low-dimensional transition function = p(bj |a, bi ) simple
computing low-dimensional reward function R : need consider pairs lowdimensional beliefs, bi bj . original high-dimensional belief space, transition
prior belief bi posterior belief bj described Bayes filter equation:
bj (s) = O(s, a, z)

|S|
X

(sk , a, s)bi (sk )

(33)

k=1

action selected z observation saw; original POMDP
transition probability distribution, original POMDP observation probability
distribution.
Equation (33) describes deterministic transition conditioned upon prior belief,
action observation. transition posterior bj stochastic observation known; is, transition bi bj occurs specific z
generated, probability transition probability generating observation
z. So, separate full transition process deterministic transition b ,
belief acting sensing, stochastic transition b j , full posterior:
ba (s) =

|S|
X

(sj , a, s)bi (sj )

(34)

j=1

bj (s) = O(s, a, z)ba (s).

(35)

Equations 34 35 describe transitions high-dimensional beliefs
original POMDP. Based high-dimensional transitions, compute transitions low-dimensional approximate belief space MDP. Figure 14 depicts process.
figure shows, start low-dimensional belief bi . bi reconstruct
high-dimensional belief b according equation (28). apply action
observation z described equation (34) equation (35) find new belief
b0 . b0 compress low-dimensional representation b0 iterating
equation (26). Finally, since b0 may member sample B low-dimensional
belief states, map b0 nearby bj B according function approximator.
function approximator grid, last step means replacing b0
prototypical bj shares grid cell. generally, function approximator may
represent b0 combination several states, putting weight w(bj , b0 ) bj . (For
example, approximator k-nearest-neighbour, w(bj , b0 ) = k1 closest k
21

fiRoy, Gordon, & Thrun

~*
bi

~
b

~*
bj
Lowdimensional

action


b

ba

b

Highdimensional

observation
z

Figure 14: process computing single transition probability.

samples B .) case replace transition bi b0 several transitions,
bi bj , scale probability one w(bj , b0 ).
transition bi b ba b0 b0 bj assign probability
p(z, j|i, a) =

p(z|ba ) w(bj , b0 )

=

w(bj , b0 )

|S|
X

p(z|sl )ba (sl )

(36)

l=1

total transition probability (bi , a, bj ) sum, observations z, p(z, j|i, a).
Step 3 Table 2 performs computation, shares work computation
(bi , a, bj ) different posterior beliefs bj reachable prior belief
bi action a.
Computing Value Function
reward transition functions computed previous sections, use
value iteration compute value function belief space MDP. full algorithm
given Table 2.

7. Solving Large POMDPs
section, present application algorithm finding policies large
POMDPs.
Toy problem
first tested E-PCA belief features using regular grid representation version
toy problem described earlier. ensure needed small set belief
samples bi , made goal region larger. used coarser discretization
underlying state space (40 states instead 200) allow us compute low-dimensional
model quickly.
Figure 15 shows comparison policies different algorithms. E-PCA
approximately twice well Maximum-Likelihood heuristic; heuristic guesses
corridor, correct half time. AMDP Heuristic algorithm
Augmented MDP algorithm reported Roy Thrun (1999). controller attempts
22

fiFinding Approximate POMDP Solutions Belief Compression

1. Generate discrete low-dimensional belief space B using E-PCA (cf. Table 1)
2. Compute low-dimensional reward function R :
b B ,
(a) Recover b b
(b) Compute R (b, a) =

P|S|

i=1 R(si , a)b(si ).

3. Compute low-dimensional transition function :
bi B ,
(a) bj : (bi , a, bj ) = 0
(b) Recover bi bi
(c) observation z
(d)

Compute bj Bayes filter equation (33) b.

(e)

Compute b0 bj iterating equation (26).

(f)

bj w(bj , b0 ) > 0
Add p(z, j|i, a) equation (36) (bi , a, bj )

(g)

4. Compute value function B
(a) = 0
(b) bi B : V 0 (bi ) = 0
(c)
(d)

change = 0

(e)

bi B :


P|B |
V (bi ) = maxa R (bi , a) + j=1 (bi , a, bj ) V t1 (bj )

change = change + V (bi ) V t1 (bi )
(f) change > 0

Table 2: Value Iteration E-PCA POMDP

find policy result lowest-entropy belief reaching goal.
controller poorly unable distinguish unimodal belief
knows corridor position within corridor, bimodal
belief knows position corridor. results Figure 15 averaged
10,000 trials.
noted problem sufficiently small conventional PCA fares
reasonably well. next sections, see problems PCA representation
poorly compared E-PCA.
23

fiRoy, Gordon, & Thrun

Average reward vs. Number Bases
120000

E-PCA
PCA

Average Reward

100000
80000
60000
40000

MDP Heuristic

20000
0
-20000

AMDP Heuristic
1

2

3

4

Number Bases

Figure 15:

comparison policy performance using different numbers bases, 10,000
trials, regular grid discretization. Policy performance given total
reward accumulated trials.

Robot Navigation
tested E-PCA POMDP algorithm simulated robot navigation problems two
example environments, Wean Hall corridor shown Figure 16 Longwood retirement facility shown Figure 1(b). model parameters given robot navigation
models (see Fox et al., 1999).
evaluated policy relatively simple problem depicted Figure 16. set
robots initial belief may one two locations corridor,
objective get within 0.1m goal state (each grid cell 0.2m0.2m).
controller received reward +1000 arriving goal state taking at_goal
action; reward 1000 given (incorrectly) taking action non-goal state.
reward 1 motion. states used planning example
500 states along corridor, actions forward backward motion.
Figure 16 shows sample robot trajectory using E-PCA policy 5 basis functions.
Notice robot drives past goal lab door order verify orientation
returning goal; robot know true position, cannot know
fact passing goal. robot started end corridor,
orientation would become apparent way goal.
Figure 17 shows average policy performance three different techniques.
Maximum-Likelihood heuristic could distinguish orientations, therefore approximately 50% time declared goal wrong place. evaluated policy
learned using best 5 bases conventional PCA. policy performed substantially
better maximum-likelihood heuristic controller incorrectly declare robot arrived goal. However, representation could detect
robot goal, chose sub-optimal (with respect E-PCA
policy) motion actions regularly. E-PCA outperformed techniques example able model belief accurately, contrast result Figure 15
PCA sufficient representation perform well better E-PCA.
24

fiFinding Approximate POMDP Solutions Belief Compression

True Position

Goal State
Final Estimated
Position

True Start State

Figure 16:

Goal Position

Start Position

example robot trajectory, using policy learned using 5 basis functions.
left start conditions goal. right robot
trajectory. Notice robot drives past goal lab door localize
itself, returning goal.

Policy perfomance Mobile Robot Navigation
400000

Average Reward

300000
200000
100000
0

-268500.0
-1000.0

33233.0

-100000
-200000
-300000

Figure 17:

ML Heuristic

PCA

E-PCA

comparison policy performance using E-PCA, conventional PCA
Maximum Likelihood heuristic, 1,000 trials.

Figure 18(a) shows second example navigation simulation. Notice initial
belief problem bi-modal; good policy take actions disambiguate
modes proceeding goal. Using sample set 500 beliefs, computed
low-dimensional belief space B. Figure 18(b) shows average KL divergence
original reconstructed beliefs. improvement KL divergence error measure
slowed substantially around 6 bases; therefore used 6 bases represent belief
space.
Figure 18(c) shows example execution policy computed using E-PCA.
reward parameters previous navigation example. robot
parameters maximum laser range 2m, high motion model variance. first
action policy chose turn robot around move closer nearest wall.
effect eliminating second distribution mode right. robot
followed essentially coastal trajectory left-hand wall order stay localized,
although uncertainty direction became relatively pronounced. see
uncertainty eventually resolved top image, robot moved
goal.
25

fiRoy, Gordon, & Thrun

KL Divergence Sampled Beliefs Reconstructions
4
3.5

True (hidden) start

3
KL Divergence

Goal

2.5
2
1.5
1
0.5
0

Start distribution modes

(a) Initial Distribution

0

2

4

6
8
10
Number Bases

12

14

16

(b) Reconstruction Performance

Positional Accuracy Goal

8
Distance goal metres

7

5

4.544

4
3
2
1
0

(c) Complete Trajectory

Figure 18:

6.030

6

1.075
E-PCA

AMDP

MDP

(d) Policy Performance

(a) sample navigation problem Longwood, cf. Figure 2. problem
involves multi-modal distributions. (c) average KL divergence
sample beliefs reconstructions number bases used, 500
samples beliefs navigating mobile robot environment. (d) comparison policy performance using E-PCA, conventional MDP AMDP
heuristic.

interesting note policy contains similar coastal attribute
heuristic policies (e.g., Entropy heuristic AMDP, Cassandra, Kaelbling, &
Kurien, 1996; Roy & Thrun, 1999). However, unlike heuristics, E-PCA representation able reach goal accurately (that is, get closer goal).
representation successful able accurately represent beliefs
effects actions beliefs.
26

fiFinding Approximate POMDP Solutions Belief Compression

Finding People

(a) Original Belief

(b) Reconstruction PCA

(c) Reconstruction E-PCA

Figure 19:

performance PCA E-PCA sample belief. map 238 85
grid cells, 0.2m resolution. (a) sample belief. (b) PCA reconstruction,
using 40 bases. (c) E-PCA reconstruction, using 6 bases.

addition synthetic problem robot navigation problems described
previous sections, tested algorithm complicated POMDP problem,
finding person object moving around environment. problem
motivated Nursebot domain, residents experiencing cognitive decline
sometimes become disoriented start wander. order make better use
health-care providers time, would use robot Pearl (Figure 1a) find
residents quickly. assume person adversarial.
state space problem much larger previous robot navigation problems: cross-product persons position robots position. However,
assume simplicity robots position known, therefore belief distribution persons position. transitions person state feature
modelled Brownian motion fixed, known velocity, models persons
motion random, independent robot position. (If person moving avoid
captured robot, different transition model would required.) assume
position person unobservable robot close enough see
person (when robot line-of-sight person, maximum range, usually
3 metres); observation model 1% false negatives false positives. reward
function maximal person robot location.
27

fiRoy, Gordon, & Thrun

Figure 19(a) shows example probability distribution occur problem
(not shown robots position). grey dots particles drawn distribution
person could environment. distribution initially uniform
reachable areas (inside black walls). robot receives sensor data,
probability mass extinguished within sensor range robot. robot
moves around, probability mass extinguished, focusing distribution
remaining places person be. However, probability distribution starts
recover mass places robot visits leaves. particle filter,
visualized particles leaking areas previously emptied out.
collected set 500 belief samples using heuristic controller given driving
robot maximum likelihood location person, used E-PCA find good
low-dimensional representation beliefs. Figure 19(b) shows reconstruction
example belief Figure 19(a), using conventional PCA 40 bases. figure
reinforce idea PCA performs poorly representing probability distributions. Figure 19(c) shows reconstruction using E-PCA 6 bases, qualitatively better
representation original belief.
Recall section 6 use function approximator representing value
function. preceding examples used regular grid low-dimensional surface
performed well finding good policies. However, problem finding people empirically requires finer resolution representation would computationally tractable
regular grid. therefore turn different function approximator, 1-nearestneighbour variable resolution representation. add new low-dimensional belief states
model periodically re-evaluating model grid cell, splitting gridcell smaller discrete cells statistic predicted model disagrees
statistic computed experience. number different statistics suggested
testing model data real world (Munos & Moore, 1999),
reduction reward variance, value function disagreement. opted instead
simpler criterion transition probability disagreement. examine policy computed
using fixed representation, policy computed using incrementally refined
representation. Note fully explored effect different variable resolution representations value function, e.g., using k-nearest-neighbour interpolations
described Hauskrecht (2000). experiments beyond scope
paper, focus utility E-PCA decomposition. variable resolution
representation value function shown scale effectively beyond tens
dimensions best (Munos & Moore, 2002).
problem shares many attributes robot navigation problem, see
Figure 19 figures 20 21 problem generates spatial distributions higher
complexity. somewhat surprising E-PCA able find good representation
beliefs using 6 bases, indeed average KL divergence generally higher
robot navigation task. Regardless, able find good controllers,
example problem PCA performs poorly even large number
bases.
Figure 20 shows example trajectory heuristic control strategy, driving
robot maximum likelihood location person time step. open circle
robot position, starting far right. solid black circle position
28

fiFinding Approximate POMDP Solutions Belief Compression

Figure 20:

(a)

(b)

(c)

(d)

(e)

(f)

example suboptimal person finding policy. grey particles drawn
distribution person might be, initially uniformly distributed (a). black dot true (unobservable) position person.
open circle observable position robot. robots poor
action selection, person able escape previously explored areas.

person, unobservable robot within 3m range. person starts
room corridor (a), moves corridor robot
moved far end corridor (b). robot returns search inside room (c)
(d), person moves unobserved previously searched corridor (e). Although
deliberately chosen example heuristic performs poorly, person
following unlikely adversarial trajectory: times solid black circle remains
regions high probability. robots belief accurately reflects possibility
person slip past, heuristic control algorithm way take possibility
account.
Using policy found low-dimensional belief space described previous
sections, able find much better controller. sample trajectory controller
29

fiRoy, Gordon, & Thrun

Figure 21:

(a)

(b)

(c)

(d)

(e)

(f)

policy computed using E-PCA representation. initial conditions
panel (a) Figure 20. Notice that, unlike previous figure,
strategy ensures probability mass located one place, allowing
robot find person significantly higher probability.

shown Figure 21. robot travels right-most position corridor (a)
part-way corridor (b), returns explore room (c)
(d). example, persons starting position different one given
previous examplethe E-PCA policy would find person point, starting
initial conditions previous example). exploring room eliminating
possibility person inside room (e), policy reduced possible
locations person left-hand end corridor, able find
person reliably location.
Note figures 20 21 target person worst-case start position
planner. person start position Figure 21 Figure 20,
policy would found person panel (d). Similarly, person started
30

fiFinding Approximate POMDP Solutions Belief Compression

end corridor Figure 21, policy shown Figure 20 would found
person panel (b).
Performance Different Policies

Average # Actions Find Person

250

200

150

100
Fully Observable Policy
50

0

Figure 22:

Closest

Densest

MDP

PCA

E-PCA Refined E-PCA

comparison 6 policies person finding simple environment.
baseline fully-observable, i.e., cheating, solution (the solid line). EPCA policy fixed (variable resolution) discretization. Refined E-PCA
discretization additional belief samples added. PCA
policy approximately 6 times worse best E-PCA policy.

Figure 22 shows quantitative comparison performance E-PCA
number heuristic controllers simulation, comparing average time find
person different controllers. solid line depicts baseline performance,
using controller access true state person times (i.e., fully
observable lower bound best possible performance). travel time case
solely function distance person; searching necessary performed.
course, realizable controller reality. controllers are:
Closest: robot driven nearest state non-zero probability.
Densest: robot driven location probability mass
visible.
MDP: robot driven maximum-likelihood state.
PCA: controller found using PCA representation fixed discretization
low-dimensional surface.
E-PCA: E-PCA controller using fixed discretization low-dimensional surface
compute value function.
Refined E-PCA: E-PCA controller using incrementally refined variable resolution
discretization surface computing value function.
performance best E-PCA controller surprisingly close theoretical best
performance, terms time find person, result demonstrates need
careful choice discretization belief space computing value function.
31

fiRoy, Gordon, & Thrun

initial variable resolution representation proved poor function approximator, however, using iteratively-refined variable resolution discretization, able improve
performance substantially. controller using conventional PCA representation
case computed fixed discretization low-dimensional representation using
40 bases 500 grid points. quality belief representation PCA poor
investigate complex policy approximators.

8. Discussion
experiments demonstrate E-PCA algorithm scale finding low-dimensional surfaces embedded high-dimensional spaces.
Time Complexity
algorithm iterative therefore simple expression total running time
available. data set |B| samples dimensionality n, computing surface size
l, iteration algorithm O(|B|nl 2 + |B|l3 + nl3 ). step Newtons
algorithm dominated set matrix multiplies final step inverting l l
matrix, O(l3 ). U step consists |B| iterations, iteration O(nl)
multiplies O(l3 ) inversion. V step consists n iterations, iteration
O(|B|l) multiplies O(l 3 ) inversion, leading total complexity given above.
Figure 23 shows time compute E-PCA bases 500 sample beliefs,
20,230 states. implementation used Java 1.4.0 Colt 1.0.2, 1 GHz Athlon
CPU 900M RAM. shown computation times conventional PCA
decomposition. small state space problems, E-PCA decomposition faster
PCA small number bases, implementation PCA always computes
full decomposition (l = n, l reduced dimensionality n full
dimensionality).
Exponential Family PCA Running Time
35000
30000
Time secs

25000
20000
15000
10000
Conventional PCA = 4151sec

5000
0

Figure 23:

0

2

4

6
8
Number Bases

10

12

time compute E-PCA representations different discretizations
state space.

32

fiFinding Approximate POMDP Solutions Belief Compression

far dominant term running time algorithm time compute
E-PCA bases. bases found low-dimensional space
discretized, running time required value iteration converge policy
problems described order 50 100ms.
Sample Belief Collection
example problems addressed, used standard sample size 500
sample beliefs. Additionally, used hand-coded heuristic controllers sample beliefs
model. practice, found 500 sample beliefs collected using semi-random controller sufficient example problems. However, may able improve overall
performance algorithm future problems iterating phases building
belief space representation (i.e., collecting beliefs generating low-dimensional
representation) computing good controller. initial set beliefs
collected used build initial set bases corresponding policy, continue
evaluate error representation (e.g., K-L divergence current belief
low-dimensional representation). initial representation learned
beliefs, representation may over-fit beliefs; detect situation
noticing representation poor job representing new beliefs. Validation
techniques cross-validation may useful determining enough beliefs
acquired.
Model Selection
One open questions addressed far choosing appropriate
number bases representation. Unless problem-specific information,
true number degrees freedom belief space (as toy example
section 3), difficult identify appropriate dimensionality underlying surface
control. One common approach examine eigenvalues decomposition,
recovered using orthonormalization step algorithm Table 1.
(This assumes particular link function capable expressing surface
data lies on.) eigenvalues conventional PCA often used determine
appropriate dimensionality underlying surface; certainly reconstruction
lossless use many bases non-zero eigenvalues.
Unfortunately, recall description E-PCA section 4 generate
set singular values, eigenvalues. non-linear projection introduced link
function causes eigenvalues U matrix uninformative contribution
basis representation. Instead using eigenvalues choose appropriate
surface dimensionality, use reconstruction quality, Figure 11. Using reconstruction
quality estimate appropriate dimensionality common choice PCA
dimensionality reduction techniques (Tenenbaum, de Silva, & Langford, 2000). One
alternate choice would evaluate reward policies computed different dimensionalities choose compact representation achieves highest reward,
essentially using control error rather reconstruction quality determine dimensionality.
33

fiRoy, Gordon, & Thrun

Recall discussion section 2 using dimensionality reduction
represent beliefs POMDPs specific kind structure. particular, E-PCA
representation useful representing beliefs relatively sparse
small number degrees freedom. However, E-PCA unable find good lowdimensional representations POMDP models exhibit kind structure
is, beliefs cannot represented lying low-dimensional hyperplane linked
full belief space via appropriate link function. One additional problem
know priori whether specific POMDP appropriate structure. unlikely
general technique determine usefulness E-PCA,
take advantage model selection techniques determine whether E-PCA
find usefully low dimensional representation specific POMDP. example,
KL divergence set sample beliefs reconstructions large even using
large number bases, problem may right structure.

9. Related Work
Many attempts made use reachability analysis constrain set beliefs
planning (Washington, 1997; Hauskrecht, 2000; Zhou & Hansen, 2001; Pineau, Gordon,
& Thrun, 2003a). reachable set beliefs relatively small, forward search
find set perfectly reasonable approach. policy computed beliefs course optimal, although relatively rare real world problems able
enumerate reachable beliefs. Reachability analysis used
success heuristic guiding search methods, especially focusing computation
finding function approximators (Washington, 1997; Hansen, 1998). approach,
problem still remains compute low-dimensional representation given finite
set representative beliefs. Discretization belief space explored
number times, regular grid-based discretization (Lovejoy, 1991), regular variable
resolution approaches (Zhou & Hansen, 2001) non-regular variable resolution representations (Brafman, 1997; Hauskrecht, 2000). vein, state abstraction (Boutilier &
Poole, 1996) explored take advantage factored state spaces, particular
interest algorithm Hansen Feng (2000) perform state abstraction
absence prior factorization. far, however, approaches fallen
victim curse dimensionality failed scale dozen
states most.
value-directed POMDP compression algorithm Poupart Boutilier (2002)
dimensionality-reduction technique closer spirit ours, technique.
algorithm computes low-dimensional representation POMDP directly model
parameters R, , finding Krylov subspace reward function belief
propagation. Krylov subspace vector matrix smallest subspace
contains vector closed multiplication matrix. POMDPs,
authors use smallest subspace contains immediate reward vector closed
set linear functions defined state transitions observation model.
major advantage approach optimizes correct criterion: value-directed
compression distinguish beliefs different value. major
disadvantage approach Krylov subspace constrained linear. Using
34

fiFinding Approximate POMDP Solutions Belief Compression

algorithm PCA instead E-PCA, realize much compression
Poupart Boutilier (2002) method: take advantage regularities
transition matrices a,z reward function R. Unfortunately,
seen, beliefs unlikely lie low-dimensional hyperplane, results reported
section 3 indicate linear compression scale size problems wish
address.
Possibly promising approaches finding approximate value-functions
point-based methods, instead optimizing value function entire
belief space, specific beliefs. Cheng (1988) described method backing
value function specific belief points procedure called point-based dynamic
programming (PB-DP). PB-DP steps interleaved standard backups
full value iteration. Zhang Zhang (2001) improved method choosing Witness
points backup belief points, iteratively increasing number points.
essential idea point-based backups significantly cheaper full backup steps.
Indeed, algorithm described Zhang Zhang (2001) out-performs Hansens exact
policy-search method order magnitude small problems. However, need
periodic backups across full belief space still limits applicability algorithms
small abstract problems.
recently, Pineau et al. (2003a) abandoned full value function backups
favour point-based backups point-based value iteration (PBVI) algorithm.
backing discrete belief points, backup operator polynomial instead
exponential (as value iteration), and, even importantly, complexity
value function remains constant. PBVI uses fundamentally different approach finding
POMDP policies, still remains constrained curse dimensionality large state
spaces. However, applied successfully problems least order magnitude
larger predecessors, another example algorithms used make
large POMDPs tractable.
E-PCA possible technique non-linear dimensionality reduction;
exists large body work containing different techniques Self-Organizing Maps (Kohonen, 1982), Generative Topographic Mapping (Bishop, Svensen, & Williams, 1998),
Stochastic Neighbour Embedding (Hinton & Roweis, 2003). Two successful
algorithms emerge recently Isomap (Tenenbaum et al., 2000) Locally Linear Embedding (Roweis & Saul, 2000). Isomap extends PCA-like methods non-linear
surfaces using geodesic distances distance metric data samples, rather
Euclidean distances. Locally Linear Embedding (LLE) considered local alternative
global reduction Isomap represents point weighted combination neighbours operates two phases: computing weights k nearest
neighbours high-dimensional point, reconstructing data lowdimensional co-ordinate frame weights. However, algorithms contain
explicit models kind data (e.g., probability distributions) attempting
model. One interesting line research, however, may extend algorithms using
different loss functions manner PCA extended E-PCA.
35

fiRoy, Gordon, & Thrun

10. Conclusion
Partially Observable Markov Decision Processes considered intractable finding
good controllers real world domains. particular, best algorithms date
finding approximate value function full belief space scaled beyond
hundred states (Pineau et al., 2003a). However, demonstrated real world
POMDPs contain structured belief spaces; finding using structure,
able solve POMDPs order magnitude larger solved conventional
value iteration techniques. Additionally, able solve different kinds POMDPs,
simple highly-structured synthetic problem robot navigation problem
problem factored belief space relatively complicated probability distributions.
algorithm used find structure related Principal Components Analysis
loss function specifically chosen representing probability distributions. real
world POMDPs able solve characterized sparse distributions,
Exponential family PCA algorithm particularly effective compressing data.
exist POMDP problems structure,
dimensionality reduction technique work well; however, question
investigation other, related dimensionality-reduction techniques (e.g., Isomap LocallyLinear Embedding, Tenenbaum et al., 2000; Roweis, Saul, & Hinton, 2002) applied.
number interesting possibilities extending algorithm order
improve efficiency increase domain applicability. loss function
chose dimensionality reduction based reconstruction error,
L(B, U, B) = e(U B) B U B,

(37)

(cf. equation 8). Minimizing reconstruction error allow near-optimal policies
learned. However, would ideally find compact representation
minimizes control errors. could possibly better approximated taking advantage
transition probability structure. example, dimensionality reduction minimizes
prediction errors would correspond loss function:
L(B, U, B, ) = e(U b) B U b + kB,2...n B,1...n1 k2

(38)

B,1...n1 l n 1 matrix first n 1 column vectors B, B,2...n
l n 1 matrix n 1 column vectors V starting second vector.
effect finding representation allows bt+1 predicted bt ,
caveat B must arranged action. plan address issue
future work.
Another shortcoming approach described work contains
assumption beliefs described using low-dimensional representation.
However, relatively easy construct example problem generates beliefs
lie two distinct low-dimensional surfaces, current formulation would make
apparent dimensionality beliefs appear much higher set beliefs sampled
one surface alone.
work largely motivated finding better representations beliefs,
approach solving large POMDPs. Policy search methods (Meuleau,
36

fiFinding Approximate POMDP Solutions Belief Compression

Peshkin, Kim, & Kaelbling, 1999) hierarchical methods (Pineau, Gordon, & Thrun,
2003b) able solve large POMDPs. interesting note controllers
based E-PCA representations often essentially independent policy complexity
strongly dependent belief complexity, whereas policy search hierarchical
methods strongly dependent policy complexity largely independent belief
space complexity. seems likely progress solving large POMDPs general lie
combination approaches.
E-PCA algorithm finds low-dimensional representation B full belief space B
sampled data. demonstrated reliance sampled data obstacle
real world problems. Furthermore, using sampled beliefs could asset
large problems generating tracking beliefs considerably easier
planning. may however preferable try compute low-dimensional representation
directly model parameters. Poupart Boutilier (2002) use notion Krylov
subspace this. subspace computed algorithm may correspond exactly
conventional PCA seen instances PCA poor job finding
low-dimensional representations. likely explanation real-world beliefs
lie low-dimensional planes problems, instead curved surfaces.
extremely useful algorithm would one finds subset belief space closed
transition observation function, constrained find planes.

Acknowledgements
Thanks Tom Mitchell, Leslie Kaelbling, Reid Simmons, Drew Bagnell, Aaron Courville,
Mike Montemerlo Joelle Pineau useful comments insight work. Nicholas
Roy funded National Science Foundation ITR grant # IIS-0121426. Geoffrey Gordon funded AFRL contract F3060201C0219, DARPAs MICA program,
AFRL contract F306029820137, DARPAs CoABS program.

References
Bagnell, J. A., & Schneider, J. (2001). Autonomous helicopter control using reinforcement
learning policy search methods. Proceedings IEEE International Conference
Robotics Automation (ICRA), pp. 16151620, Seoul, South Korea. IEEE Press.
Bishop, C., Svensen, M., & Williams, C. (1998). GTM: generative topographic mapping.
Neural Computation, 10 (1), 215234.
Boutilier, C., & Poole, D. (1996). Computing optimal policies partially observable
Markov decision processes using compact representations. Proceedings 13th
National Conference Artificial Intelligence (AAAI-96), pp. 11681175.
Boyen, X., & Koller, D. (1998). Tractable inference complex stochastic processes.
Proceedings 14th Annual Conference Uncertainty AI (UAI), pp. 3342,
Madison, Wisconsin.
Brafman, R. I. (1997). heuristic variable grid solution method POMDPs. Kuipers,
B. K., & Webber, B. (Eds.), Proceedings 14th National Conference Artificial
Intelligence (AAAI), pp. 727733, Providence, RI.
37

fiRoy, Gordon, & Thrun

Cassandra, A. R., Kaelbling, L., & Kurien, J. A. (1996). Acting uncertainty: Discrete Bayesian models mobile-robot navigation. Proceedings IEEE/RSJ
International Conference Intelligent Robots Systems.
Chen, B. M. (2000). Robust H- Control. Springer-Verlag.
Cheng, H.-T. (1988). Algorithms Partially Observable Markov Decision Processes. Ph.D.
thesis, University British Columbia, Vancouver, Canada.
Collins, M., Dasgupta, S., & Schapire, R. (2002). generalization principal components
analysis exponential family. Dietterich, T. G., Becker, S., & Ghahramani, Z.
(Eds.), Advances Neural Information Processing Systems 14 (NIPS), Cambridge,
MA. MIT Press.
Cox, T., & Cox, M. (1994). Multidimensional Scaling. Chapman & Hall, London.
Fox, D., Burgard, W., & Thrun, S. (1999). Markov localization mobile robots dynamic
environments. Journal Artificial Intelligence Research, 11, 391427.
Galassi, M., Davies, J., Theiler, J., Gough, B., Jungman, G., Booth, M., & Rossi,
F. (2002).
GNU Scientific Library Reference Manual (3rd Edition edition).
http://www.gnu.org/software/gsl/.
Golub, G., & Reinsch, C. (1970). Singular value decomposition least squares solutions.
Numerische Mathematik, pp. 403420.
Gordon, G. (1995). Stable function approximation dynamic programming. Prieditis,
A., & Russell, S. (Eds.), Proceedings 12 International Conference Machine
Learning (ICML), pp. 261268, San Francisco, CA. Morgan Kaufmann.
Gordon, G. (2003). Generalized2 linear2 models. Becker, S., Thrun, S., & Obermayer, K.
(Eds.), Advances Neural Information Processing Systems 15 (NIPS). MIT Press.
Gutmann, J.-S., Burgard, W., Fox, D., & Konolige, K. (1998). experimental comparison
localization methods. Proceedings IEEE/RSJ International Conference
Intelligent Robots Systems, Victoria, Canada.
Gutmann, J.-S., & Fox, D. (2002). experimental comparison localization methods
continued. Proceedings IEEE/RSJ International Conference Intelligent
Robots Systems, Lausanne, Switzerland.
Hansen, E., & Feng, Z. (2000). Dynamic programming POMDPs using factored state
representation. Proceedings Fifth International Conference Artificial
Intelligence Planning Scheduling (AIPS-00), Breckenridge, CO.
Hansen, E. (1998). Solving POMDPs searching policy space. Proceedings
14th Conference Uncertainty Artifical Intelligence (UAI), pp. 211219, Madison,
WI.
Hauskrecht, M. (2000). Value-function approximations partially observable Markov
decision processes. Journal Artificial Intelligence Research, 13, 3394.
Hinton, G., & Roweis, S. (2003). Stochastic neighbor embedding. Becker, S., Thrun,
S., & Obermayer, K. (Eds.), Advances Neural Information Processing Systems 15
(NIPS). MIT Press.
38

fiFinding Approximate POMDP Solutions Belief Compression

Howard, R. A. (1960). Dynamic Programming Markov Processes. MIT.
Isard, M., & Blake, A. (1998). CONDENSATION conditional density propagation
visual tracking. International Journal Computer Vision, 29 (1), 528.
Joliffe, I. T. (1986). Principal Component Analysis. Springer-Verlag.
Kanazawa, K., Koller, D., & Russell, S. (1995). Stochastic simulation algorithms dynamic
probabilistic networks. Proceedings 11th Annual Conference Uncertainty
AI (UAI), pp. 346351, Montreal, Canada.
Kohonen, T. (1982). Self-organized formation topologically correct feature maps. Biological Cybernetics, 48, 5969.
Lee, D. D., & Seung, H. S. (1999). Learning parts objects non-negative matrix
factorization. Nature, 401, 788791.
Leonard, J., & Durrant-Whyte, H. (1991). Mobile robot localization tracking geometric
beacons. IEEE Transactions Robotics Automation, 7 (3), 376382.
Lovejoy, W. S. (1991). Computationally feasible bounds partially observable Markov
decison processes. Operations Research, 39, 192175.
Mardia, K. V., & Jupp, P. E. (2000). Directional Statistics (2nd edition). Wiley, Chichester,
NY.
McCullagh, P., & Nelder, J. A. (1983). Generalized Linear Models (2nd edition). Chapman
Hall, London.
Meuleau, N., Peshkin, L., Kim, K.-E., & Kaelbling, L. P. (1999). Learning finite-state controllers partially observable environments. Laskey, K. B., & Prade, H. (Eds.),
Proceedings Fifteenth International Conference Uncertainty Artificial Intelligence, pp. 427436, Stockholm, Sweden. Morgan Kaufmann.
Munos, R., & Moore, A. (1999). Variable resolution discretization high-accuracy solutions optimal control problems. Dean, T. (Ed.), Proceedings 16th International Joint Conference Artificial Intelligence (IJCAI), pp. 13481355, Stockholm
Sweden. Morgan Kaufmann.
Munos, R., & Moore, A. (2002). Variable resolution discretization optimal control. Machine Learning, 49 (2-3), 291323.
Nourbakhsh, I., Powers, R., & Birchfield, S. (1995). DERVISH office-navigating robot.
AI Magazine, 16 (2), 5360.
Olson, C. F. (2000). Probabilistic self-localization mobile robots. IEEE Transactions
Robotics Automation, 16 (1), 5566.
Pineau, J., Gordon, G., & Thrun, S. (2003a). Point-based value iteration: anytime
algorithm POMDPs. Proceedings 18th International Joint Conference
Artificial Intelligence (IJCAI 2003), Acapulco, Mexico.
Pineau, J., Gordon, G., & Thrun, S. (2003b). Policy-contingent abstraction robust robot
control. Meek, C., & Kjlruff, U. (Eds.), Proceedings 19th Annual Conference
Uncertainty Artificial Intelligence (UAI), Acapulco, Mexico.
39

fiRoy, Gordon, & Thrun

Poupart, P., & Boutilier, C. (2002). Value-directed compression POMDPs. Becker,
S., Thrun, S., & Obermayer, K. (Eds.), Advances Neural Information Processing
Systems 15 (NIPS), Vancouver, Canada. MIT Press.
Rockafellar, R. T. (1970). Convex Analysis. Princeton University Press, New Jersey.
Roweis, S., & Saul, L. (2000). Nonlinear dimensionality reduction locally linear embedding.. Science, 290 (5500), 23232326.
Roweis, S. T., Saul, L. K., & Hinton, G. E. (2002). Global coordination local linear
models. Dietterich, T. G., Becker, S., & Ghahramani, Z. (Eds.), Advances
Neural Information Processing Systems, Vol. 14, Cambridge, MA. MIT Press.
Roy, N., & Thrun, S. (1999). Coastal navigation mobile robots. Solla, S. A., todd
K. Leen, & Muller, K. R. (Eds.), Advances Neural Processing Systems 12 (NIPS),
pp. 10431049, Denver, CO. MIT Press.
Russell, S., & Norvig, P. (1995). Artificial Intelligence: Modern Approach. Prentice Hall.
Shatkay, H., & Kaelbling, L. P. (2002). Learning geometrically-constrained hidden markov
models robot navigation: Bridging geometrical-topological gap. Journal AI
Research.
Tenenbaum, J. B., de Silva, V., & Langford, J. C. (2000). global geometric framework
nonlinear dimensionality reduction. Science, 290 (5500), 23192323.
Thrun, S., Fox, D., Burgard, W., & Dellaert, F. (2000). Robust Monte Carlo localization
mobile robots. Artificial Intelligence, 128 (1-2), 99141.
Washington, R. (1997). BI-POMDP: Bounded, incremental partially-observable Markovmodel planning. Proceedings 4th European Conference Planning (ECP).
Zhang, N. L., & Zhang, W. (2001). Speeding convergence value iteration partially observable Markov decision processes. Journal Artificial Intelligence Research,
14, 128.
Zhou, R., & Hansen, E. (2001). improved grid-based approximation algorithm
POMDPs. Nebel, B. (Ed.), Proceedings 17th International Joint Conference Artificial Intelligence (IJCAI), pp. 707716, Seattle, Washington. Morgan
Kaufmann.

40


