journal artificial intelligence

submitted published

hybrid bdi pomdp framework multiagent teaming
ranjit nair

ranjit nair honeywell com

automation control solutions
honeywell laboratories minneapolis mn

milind tambe

tambe usc edu

department computer science
university southern california los angeles ca

abstract
many current large scale multiagent team implementations characterized
following belief desire intention bdi paradigm explicit representation team
plans despite promise current bdi team approaches lack tools quantitative
performance analysis uncertainty distributed partially observable markov decision
pomdps well suited analysis complexity finding optimal
policies highly intractable key contribution article
hybrid bdi pomdp bdi team plans exploited improve pomdp
tractability pomdp analysis improves bdi team plan performance
concretely focus role allocation fundamental bdi teams
agents allocate different roles team article provides three key contributions first describe role allocation technique takes account future
uncertainties domain prior work multiagent role allocation failed address
uncertainties end introduce rmtdp role markov team decision distributed pomdp model analysis role allocations
technique gains tractability significantly curtailing rmtdp policy search particular bdi team plans provide incomplete rmtdp policies rmtdp policy search
fills gaps incomplete policies searching best role allocation
second key contribution novel decomposition technique improve rmtdp
policy search efficiency even though limited searching role allocations still
combinatorially many role allocations evaluating rmtdp identify best
extremely difficult decomposition technique exploits structure bdi team
plans significantly prune search space role allocations third key contribution
significantly faster policy evaluation suited bdi pomdp hybrid finally present experimental two domains mission rehearsal
simulation robocuprescue disaster rescue simulation

introduction
teamwork whether among software agents robots people critical capability
large number multiagent domains ranging mission rehearsal simulations
robocup soccer disaster rescue personal assistant teams already large number multiagent teams developed range domains pynadath tambe
yen yin ioerger miller xu volz stone veloso jennings
grosz hunsberger kraus decker lesser tambe pynadath chauvat
da silva demazeau existing practical approaches characterized situated within general belief desire intention bdi paradigm
c

ai access foundation rights reserved

finair tambe

designing multiagent systems made increasingly popular due programming frameworks tambe et al decker lesser tidhar b facilitate design
large scale teams within inspired explicitly implicitly bdi logics
agents explicitly represent reason team goals plans wooldridge
article focuses analysis bdi teams provide feedback aid human
developers possibly agents participating team team performance
complex dynamic domains improved particular focuses critical
challenge role allocation building teams tidhar rao sonenberg hunsberger
grosz e agents allocate roles team instance
mission rehearsal simulations tambe et al need select numbers
types helicopter agents allocate different roles team similarly disaster
rescue kitano tadokoro noda matsubara takahashi shinjoh shimada role
allocation refers allocating fire engines ambulances fires greatly impact
team performance domains performance team
linked important metrics loss human life property thus critical
analyze team performance suggest improvements
bdi frameworks facilitate human design large scale teams key difficulty
analyzing role allocation teams due uncertainty arises complex
domains example actions may fail world state may partially observable
agents owing physical properties environment imperfect sensing role
allocation demands future uncertainties taken account e g fact
agent may fail execution may may replaced another must taken
account determining role allocation yet current role allocation address uncertainty see section indeed uncertainty requires
quantitative comparison different role allocations however tools quantitative
evaluations bdi teams currently absent thus given uncertainties may
required experimentally recreate large number possible scenarios real domain
simulations evaluate compare different role allocations
fortunately emergence distributed partially observable markov decision pomdps provides bernstein zilberstein immerman boutilier
pynadath tambe xuan lesser zilberstein used
quantitative analysis agent teams uncertain domains distributed pomdps represent class formal powerful enough express uncertainty
dynamic domains arising non determinism partial observability
principle used generate evaluate complete policies multiagent team
however two shortcomings prevents application
analysis role allocation first previous work analysis focused communication pynadath tambe xuan et al rather role allocation
coordination decisions second shown bernstein et al
deriving optimal policy generally computationally intractable corresponding
decision nexp complete thus applying optimal policies analysis highly
intractable
address first difficulty derive rmtdp role multiagent team decision
distributed pomdp framework quantitatively analyzing role allocations
framework general finding optimal role


fihybrid bdi pomdp framework multiagent teaming

completed policy
additions bdi team plan

bdi team plan
rmtdp
search policy space

incomplete policy
bdi interpreter

domain
rmtdp model

figure integration bdi pomdp

allocation policy computationally intractable corresponding decision still
nexp complete shows improving tractability analysis techniques role
allocation critically important issue
therefore order make quantitative analysis multiagent teams rmtdp
tractable second contribution provides hybrid bdi pomdp
combines native strengths bdi pomdp approaches e ability bdi
frameworks encode large scale team plans pomdp ability quantitatively
evaluate plans hybrid three key interactions improve
tractability rmtdp optimality bdi agent teams first interaction
shown figure particular suppose wish analyze bdi agent team agent
consisting bdi team plan domain independent interpreter helps coordinate
plans acting domain shown figure model domain via
rmtdp rely bdi team plan interpreter providing incomplete policy
rmtdp rmtdp model evaluates different completions incomplete
policy provides optimally completed policy feedback bdi system thus
rmtdp fills gaps incompletely specified bdi team plan optimally
gaps concentrate role allocations method applied
key coordination decisions restricting optimization role allocation decisions
fixing policy points able come restricted policy
space use rmtdps effectively search restricted space order
optimal role allocation
restricted policy search one key positive interaction hybrid
second interaction consists efficient policy representation used converting
bdi team plan interpreter corresponding policy see figure
policy evaluation general agents policy distributed pomdp
indexed observation history bernstein et al pynadath tambe


finair tambe

however bdi system agent performs action selection set
privately held beliefs obtained agents observations applying belief
revision function order evaluate teams performance sufficient rmtdp
index agents policies belief state represented privately held beliefs
instead observation histories shift representation considerable
savings amount time needed evaluate policy space required
represent policy
third key interaction hybrid exploits bdi team plan structure increasing efficiency rmtdp analysis even though rmtdp
policy space restricted filling gaps incomplete policies many policies may
given large number possible role allocations thus enumerating evaluating
possible policy given domain difficult instead provide branch bound exploits task decomposition among sub teams team significantly prune
search space provide correctness proof worst case analysis
order empirically validate applied rmtdp allocation
bdi teams two concrete domains mission rehearsal simulations tambe et al
robocuprescue kitano et al first present significant speed
gained three interactions mentioned next domains compared
role allocations found state art techniques allocate
roles without uncertainty reasoning comparison shows importance reasoning
uncertainty determining role allocation complex multiagent domains
robocuprescue domain compared allocations found allocations chosen
humans actual robocuprescue simulation environment showed
role allocation technique presented article capable performing human
expert levels robocuprescue domain
article organized follows section presents background motivation
section introduce rmtdp model present key complexity section
explains bdi team plan evaluated rmtdp section describes
analysis methodology finding optimal role allocation section presents
empirical evaluation methodology section present related work
section list conclusions

background
section first describes two domains consider article abstract
mission rehearsal domain tambe et al robocuprescue domain kitano
et al domain requires us allocate roles agents team next teamoriented programming top framework describing team plans described
context two domains focus top discussed section
techniques would applicable frameworks tasking teams stone veloso
decker lesser
domains
first domain consider mission rehearsal simulations tambe et al
expository purposes intentionally simplified scenario


fihybrid bdi pomdp framework multiagent teaming

follows helicopter team executing mission transporting valuable cargo point
x point enemy terrain see figure three paths x
different lengths different risk due enemy fire one scouting sub teams must
sent one path x larger size scouting sub team
safer scouts clear one path x transports
move safely along path however scouts may fail along path may
need replaced transport cost transporting cargo owing partial
observability transports may receive observation scout failed
route cleared wish transport amount cargo quickest
possible manner within mission deadline
key role allocation decision given fixed number helicopters
allocated scouting transport roles allocating scouts means
scouting task likely succeed fewer helicopters left
used transport cargo consequently less reward however allocating
scouts could mission failing altogether allocating scouts
routes scouts sent shortest route would preferable
risky sending scouts route decreases likelihood failure
individual scout however might beneficial send different routes e g
scouts risky short route others safe longer route
thus many role allocations consider evaluating difficult
role allocation must look ahead consider future implications uncertainty e g scout
helicopters fail scouting may need replaced transport furthermore failure success scout may visible transport helicopters hence
transport may replace scout transports may never fly destination
scout
transports

x

route

route



enemy gun
route

figure mission rehearsal domain

second example scenario see figure set robocuprescue disaster
simulation environment kitano et al consists five fire engines three different
fire stations two stations last station five ambulances
stationed ambulance center two fires top left bottom right corners
map start need extinguished fire engines fire extinguished
ambulance agents need save surviving civilians number civilians


finair tambe

location known ahead time although total number civilians known
time passes high likelihood health civilians deteriorate fires
increase intensity yet agents need rescue many civilians possible
minimal damage buildings first part goal scenario therefore
first determine fire engines assign fire fire engines gathered
information number civilians fire transmitted ambulances
next part goal allocate ambulances particular fire rescue
civilians trapped however ambulances cannot rescue civilians fires fully
extinguished partial observability agent view objects within visual
range uncertainty related fire intensity well location civilians
health add significantly difficulty

c
f
f
f
c


figure robocuprescue scenario c c denote two fire locations f f
f denote fire stations respectively denotes ambulance
center

team oriented programming
aim team oriented programming top pynadath tambe tambe et al
tidhar b framework provide human developers automated symbolic
planners useful abstraction tasking teams domains described
section consists three key aspects team team organization hierarchy
consisting roles ii team reactive plan hierarchy iii assignment roles
sub plans plan hierarchy developer need specify low level coordination
details instead top interpreter underlying coordination infrastructure automatically enables agents decide communicate reallocate


fihybrid bdi pomdp framework multiagent teaming

roles upon failure top abstraction enables humans rapidly provide team plans
large scale teams unfortunately qualitative assessment team performance
feasible thus key top weakness inability quantitatively evaluate optimize
team performance e g allocating roles agents qualitative matching capabilities may feasible discussed later hybrid bdi pomdp model addresses
weakness providing techniques quantitative evaluation
concrete example consider top mission rehearsal domain first
specify team organization hierarchy see figure task force highest level
team organization consists two roles scouting transport
scouting sub team roles three scouting sub sub teams next specify
hierarchy reactive team plans figure b reactive team plans explicitly express
joint activities relevant team consist pre conditions plan
proposed ii termination conditions plan ended iii
team level actions executed part plan example plan discussed
shortly figure b highest level plan execute mission three sub plans
doscouting make one path x safe transports dotransport
move transports along scouted path remainingscouts scouts
reached destination yet get
execute mission task force
doscouting
task force

remainingscouts
dotransport
scouting team transport team

task force
scoutroutes
waitatbase
transport team scouting team

scouting team

transport team

sctteama sctteamb sctteamc

scoutroute scoutroute scoutroute
sctteama sctteamb sctteamc



b

figure top mission rehearsal domain organization hierarchy b plan hierarchy

figure b shows coordination relationships relationship indicated
solid arc relationship indicated dashed arc thus waitatbase scoutroutes must done least one scoutroute
scoutroute scoutroute need performed temporal dependence relationship among sub plans implies sub teams assigned perform
dotransport remainingscouts cannot doscouting plan completed however dotransport remainingscouts execute parallel finally
assign roles plans figure b shows assignment brackets adjacent plans
instance task force team assigned jointly perform execute mission sctteama assigned scoutroute
team plan corresponding execute mission shown figure
seen team plan consists context pre conditions post conditions body constraints context describes conditions must fulfilled parent plan
pre conditions particular conditions cause sub plan begin exe

finair tambe

cution thus execute mission pre condition team mutually believes
mb start location post conditions divided achieved
unachievable irrelevant conditions sub plan terminated
body consists sub plans exist within team plan lastly constraints describe
temporal constraints exist sub plans body description
plans plan hierarchy figure b given appendix
executemission
context
pre conditions mb taskforce location taskforce start
achieved mb taskforce achieved doscouting achieved dotransport time
mb taskforce
achieved remainingscouts helo scoutingteam alive helo
location helo end
unachievable mb taskforce unachievable doscouting mb taskforce
unachievable dotransport
achieved remainingscouts helo scoutingteam alive helo
location helo end
irrelevant
body
doscouting
dotransport
remainingscouts
constraints doscouting dotransport doscouting remainingscouts

figure example team plan mb refers mutual belief
htn dix muoz avila nau zhang erol hendler nau
plan hierarchy top gives decomposition task smaller tasks however
language tops richer language early htn erol et al
contained simple ordering constraints seen example plan hierarchy
tops contain relationships addition recent
work htn dix et al sub plans tops contain pre conditions
post conditions thus allowing conditional plan execution main differences
tops htn tops contain organization hierarchy addition
plan hierarchy ii top interpreter ensures team executes plans coherently
seen later tops analyzed expressiveness including conditional
execution however since analysis focus fixed time horizon loops
task description unrolled time horizon
mutual belief wooldridge shown mb hteami x figure refers private belief held
agent team believe fact x true agents
team believe x true every agent believes every agent believes x
true infinite levels nesting difficult realize practice thus practical
bdi implementations purposes article mutual belief approximated private
belief held agent agents team believe x true



fihybrid bdi pomdp framework multiagent teaming

observation
agent

belief update
function

private beliefs
agent

figure mapping observations beliefs

execution agent copy top agent maintains set
private beliefs set propositions agent believes true see
figure agent receives beliefs e observations including communication
belief update function used update set privately held beliefs instance
upon seeing last scout crashed transport may update privately held beliefs
include belief criticalfailure doscouting practical bdi systems belief
update computation low complexity e g constant linear time beliefs
updated agent selects plan execute matching beliefs preconditions plans basic execution cycle similar standard reactive
systems prs georgeff lansky
team plan execution observations form communications often arise
coordination actions executed top interpreter instance top
interpreters exploited bdi theories teamwork levesque et al theory
joint intentions levesque cohen nunes require agent
comes privately believe fact terminates current team plan e matches
achievement unachievability conditions team plan communicates fact
rest team performing coordination actions automatically top
interpreter enables coherence initiation termination team plans within top
details examples tops seen work pynadath
tambe tambe et al tidhar b
concretely illustrate key challenges role allocation mentioned
earlier first human developer must allocate available agents organization hierarchy figure best role allocation however combinatorially many
allocations choose hunsberger grosz tambe et al instance
starting homogeneous helicopters different ways deciding
many agents assign scouting transport sub team exacerbated fact best allocation varies significantly domain variations
example figure shows three different assignments agents team organization hierarchy found analysis best given setting failure
observation probabilities details section example increasing probability
failures routes resulted number transports best allocation changing
four see figure b three see figure additional scout added
sctteamb failures possible number transports increased
five see figure c analysis takes step towards selecting best among
allocations


finair tambe

task force
scouting team

task force

transport team

scouting team

sctteama sctteamb sctteamc

transport team

sctteama sctteamb sctteamc

medium probability

b low probability
task force

scouting team

transport team

sctteama sctteamb sctteamc

c zero probability

figure best role allocations different probabilities scout failure

figure shows top robocuprescue scenario seen plan hierarchy scenario consists pair extinguishfire rescuecivilians plans
done parallel decompose individual plans individual plans get fire engines ambulances move streets specific
search however individual plans relevant discussions
article interested readers refer description robocuprescue team
entered robocup competitions nair ito tambe marsella
organizational hierarchy consists task force comprising two engine sub teams one
fire ambulance team engine teams assigned extinguishing
fires ambulance team assigned rescuing civilians particular top
assignment ambulances ambulanceteama ambulanceteamb conditioned
communication c indicated ambulanceteama c ambulanceteamb c
c described detail figure refers communication received fire engines describes number civilians present fire
engines assign engine team possible value c
ambulances assign ambulance team note engines differing
capabilities owing differing distances fires ambulances identical
capabilities
task force
engineteama

engineteamb

ambulanceteam

ambulanceteama c

ambulanceteamb c


executemission
task force
extinguishfire
engineteama

rescuecivilians
ambulanceteama

extinguishfire
engineteamb

rescuecivilians
ambulanceteamb

b

figure top robocuprescue scenario organization hierarchy b plan hierarchy



fihybrid bdi pomdp framework multiagent teaming

role multiagent team decision
multiagent team decision mtdp pynadath tambe inspired
economic theory teams marschak radner ho yoshikawa
order quantitative analysis key coordination decisions multiagent teams
extend mtdp analysis coordination actions interest example
com mtdp pynadath tambe extension mtdp analysis communication article illustrate general methodology analysis aspects
coordination present rmtdp model quantitative analysis role allocation
reallocation concrete example contrast bdi systems introduced previous section rmtdp enables explicit quantitative optimization team performance note
use mtdp possible distributed pomdp could potentially
serve basis bernstein et al xuan et al
multiagent team decision
given team n agents mtdp pynadath tambe defined tuple
hs p ri consists finite set states j
j feature world state agent perform action
set actions ai ai p gives probability
transitioning state state given agents perform actions
jointly agent receives observation function
n gives probability agents receive
observations n given world state perform
jointly agents receive single joint reward r state
joint action joint reward shared equally members
private reward individual agents receive actions thus
agents motivated behave team taking actions jointly yield
maximum expected reward
agent mtdp chooses actions local policy
mapping observation history actions thus time agent perform action
contrasts single agent pomdp index agents
policy belief state probability distribution world state kaelbling littman
cassandra shown sufficient statistic order compute
optimal policy sondik unfortunately cannot directly use single agent pomdp
techniques kaelbling et al maintaining updating belief states kaelbling et al
mtdp unlike single agent pomdp mtdp agents observation
depends actions unknown actions agents thus
distributed pomdp bernstein et al xuan et al
mtdp local policies indexed observation histories n refers
joint policy team agents
extension explicit coordination
beginning mtdp next step methodology make explicit separation
domain level actions coordination actions interest earlier work intro

finair tambe

duced com mtdp model pynadath tambe coordination action
fixed communication action got separated however coordination actions could separated domain level actions order investigate
impact thus investigate role allocation reallocations actions allocating agents
roles reallocate roles separated end define rmtdp
role multiagent team decision tuple hs p r rli
component rl particular rl r rs set roles agents
undertake instance role rj may assigned agent fulfill
actions agent distinguishable two types
role taking actions irj contains role taking actions agent irj
means agent takes role rj rl

role execution actions rj rl irj contains execution actions agent
irj set agent actions executing role rj rl
addition define set states roles feature roles vector gives current role agent taken reason
introducing feature assist us mapping bdi team plan
rmtdp thus time agent performs role taking action successfully value
feature roles updated reflect change key
model agents initial role taking action subsequent role reallocation modeling
allocation reallocation important accurate analysis bdi teams note
agent observe part feature pertaining current role
may observe parts pertaining agents roles
introduction roles allows us represent specialized behaviors associated
role e g transport vs scout role filling particular role rj agent
perform role execution actions irj may different roleexecution actions irl role rl thus feature roles used filter actions
role execution actions correspond agents current role permitted
worst case filtering affect computational complexity see theorem
practice significantly improve performance trying
optimal policy team since number domain actions agent choose
restricted role agent taken different roles
produce varied effects world state modeled via transition probabilities p
teams reward thus policies must ensure agents role capabilities
benefit team
mtdp agent chooses action perform indexing local policy
observation history epoch agents could role taking
actions others role execution actions thus agents local policy
divided local role taking role execution policies observation
histories null null
n refers joint role taking policy team agents
n refers joint role execution policy


fihybrid bdi pomdp framework multiagent teaming

article explicitly model communicative actions special action
thus communication treated role execution action communication
received agents treated observations
complexity rmtdp
section qualitatively emphasized difficulty role allocation rmtdp helps
us understanding complexity precisely goal rmtdp come
joint policies maximize total expected reward finite horizon
note agents change roles according local role taking policies
agents role execution policy subsequent change would contain actions pertaining
role following theorem illustrates complexity finding optimal joint
policies
theorem decision determining exist policies
rmtdp yield expected reward least k finite horizon nexpcomplete
proof sketch proof follows reduction mtdp pynadath tambe
rmtdp reduce mtdp rmtdp set rmtdps role taking actions
null set rmtdps role execution actions mtdps set actions
reduce rmtdp
mtdp generate mtdp set actions

equal finding required policy mtdp nexp complete pynadath
tambe
theorem shows us solving rmtdp optimal joint role taking roleexecution policies even finite horizon highly intractable hence focus
complexity determining optimal role taking policy given fixed role execution
policy fixed role execution policy mean action selection agent
predetermined role executing
theorem decision determining exists role taking policy
rmtdp yields expected reward least k together fixed role execution
policy finite horizon nexp complete
proof sketch reduce mtdp rmtdp different role taking
role execution action corresponding action mtdp hence rmtdp
role taking action irj agent take role rj created action aj ai
mtdp role rj contains single role execution action e irj
rmtdp construct transition function role taking action
succeeds affected state feature roles role execution action irj
transition probability mtdp action aj ai corresponding
last role taking action irj fixed role execution policy simply perform
action irj corresponding last successful role taking action irj thus
decision rmtdp fixed role execution policy least hard
explicit analysis communication please refer work done pynadath tambe
goldman et al



finair tambe

decision mtdp furthermore given theorem conclude
nexp completeness
suggests even fixing role execution policy solving rmtdp
optimal role taking policy still intractable note theorem refers completely
general globally optimal role taking policy number agents change roles
point time given general globally optimal role taking policy
likely doubly exponential complexity may left choice run
brute force policy search e enumerate role taking policies evaluate
together determinethe run time finding globally optimal policy
number policies






n

e doubly exponential number observation

histories number agents thus rmtdp enables quantitative evaluation
teams policies computing optimal policies intractable furthermore given low level
abstraction contrast top difficult human understand optimal policy
contrast rmtdp top root hybrid model described
following section

hybrid bdi pomdp
explained top rmtdp present detailed view
hybrid methodology quantitatively evaluate top first provide detailed
interpretation figure bdi team plans essentially top plans bdi
interpreter top coordination layer shown figure rmtdp model
constructed corresponding domain top interpreter converted
corresponding incomplete rmtdp policy analyze top
analysis techniques rely evaluating rmtdp policy rmtdp model
domain
thus hybrid combines strengths tops enabling humans
specify tops coordinate large scale teams strengths rmtdp enabling
quantitative evaluation different role allocations one hand synergistic
interaction enables rmtdps improve performance top bdi teams
hand identified least six specific ways tops make easier
build rmtdps efficiently search rmtdp policies two discussed
section four next section particular six ways
tops exploited constructing rmtdp domain section
tops exploited present incomplete policies rmtdps restricting rmtdp
policy search section
top belief representation exploited enabling faster rmtdp policy evaluation
section
top organization hierarchy exploited hierarchically grouping rmtdp policies
section
top plan hierarchy exploited decomposing rmtdps section


fihybrid bdi pomdp framework multiagent teaming

top plan hierarchies exploited cutting observation belief
histories rmtdps section
end efficient policy search completed rmtdp policy improves
top performance exploit top framework frameworks tasking
teams e g decker lesser stone veloso could benefit
similar synergistic interaction
guidelines constructing rmtdp
shown figure analysis uses input rmtdp model domain
well incomplete rmtdp policy fortunately top serve
direct mapping rmtdp policy utilized actually constructing
rmtdp model domain particular top used determine
domain features important model addition structure top
exploited decomposing construction rmtdp
elements rmtdp tuple hs p r rli defined procedure relies top well underlying domain procedure
automated key contribution recognizing exploitation top structures
constructing rmtdp model first order determine set states
critical model variables tested pre conditions termination conditions
context components e sub plans top note state needs
model features tested top top pre condition expresses complex test
feature test modeled state instead gets used defining
incomplete policy input rmtdp next define set roles rl leaf level
roles organization hierarchy top furthermore specified section
define state feature roles vector containing current role agent
defined rl roles define actions follows role rj rl
define corresponding role taking action irj succeed fail depending
agent performs action state action performed
role execution actions irj agent role rj allowed role according
top
thus defined rl top illustrate steps consider
plans figure b pre conditions leaf level plan scoutroute see
appendix instance tests start location helicopters start location x
termination conditions test scouts end location thus locations
helicopters modeled features set states rmtdp
organization hierarchy define set roles rl role corresponding
four different kinds leaf level roles e rl membersctt eama membersctt eamb
membersctt eamc membert ransportt eam role taking role execution actions
defined follows
role taking action defined corresponding four roles rl e
becoming member one three scouting teams transport team
domain specifies transport change scout thus role taking
action jointtransportteam fail agent current role agent scout


finair tambe

role execution actions obtained top plans corresponding agents
role mission rehearsal scenario agent fulfilling scout role members
sctteama sctteamb sctteamc goes forward making current
position safe reaches destination execution action
consider move making safe agent transport role members transport
team waits x obtains observation signal one scouting sub team
reached hence role execution actions wait move forward
must define p r obtain set observations agent
directly domain instance transport helos may observe status scout
helos normal destroyed well signal path safe finally determining
functions p r requires combination human domain expertise empirical
data domain behavior however shown later section even approximate
model transitional observational uncertainty sufficient deliver significant benefits defining reward transition function may sometimes require additional state
variables modeled implicitly modeled top mission
rehearsal domain time scouting transport mission completed
determined amount reward thus time implicitly modeled top
needed explicitly modeled rmtdp
since interested analyzing particular top respect uncertainty
procedure constructing rmtdp model simplified exploiting hierarchical decomposition top order decompose construction rmtdp
model high level components top often represent plans executed different
sub teams may loosely interact within component
sub team members may exhibit tight interaction focus loose coupling
across components end one component feed another
components independently contribute team goal thus procedure constructing rmtdp exploits loose coupling components plan hierarchy
order build rmtdp model represented combination smaller rmtdps factors note decomposition infeasible still applies except
benefits hierarchical decomposition unavailable
classify sibling components parallel sequentially executed contains temporal constraint components executed parallel could independent
dependent independent components define rmtdps
components sub team executing one component cannot affect transitions observations reward obtained sub teams executing components procedure determining elements rmtdp tuple component k
hsk ak pk k ok rk rlk identical procedure described earlier constructing
overall rmtdp however component smaller set relevant variables
roles hence specifying elements corresponding rmtdp easier
combine rmtdps independent components obtain
rmtdp corresponding higher level component higher level component l
whose child
components independent set states sl x fsl x
fsl k child k l true fsk fsl fsk sets features set
states sl set states sk state sl sl said correspond state
sk sk x fsk sl x sk x e state sl value state sk


fihybrid bdi pomdp framework multiagent teaming


defined follows pl sl al sl
q features state sk transition function

k child k l true pk sk ak sk sl sl component l corresponds states
sk sk component k ak joint action performed sub team assigned component k corresponding joint action al performed sub team
assigned
component l observation function defined similarly ol sl al l
q
ok sk ak k reward function component l defined
k child k l true
p
rl sl al k child k l true rk sk ak

case sequentially executed components connected temporal constraint components loosely coupled since end states preceding component
specify start states succeeding component thus since one component
active time transition function defined follows pl sl al sl pk sk ak sk
component k active child component sk sk represent states
component k corresponding states sl sl component l ak joint action
performed sub team assigned component k corresponding joint action
al performed sub team corresponding component l similarly define
ol sl al l ok sk ak k rl sl al rk sk ak k active child
component

consider following example mission rehearsal domain components
exhibit sequential dependence parallel independence concretely component
doscouting executed first followed dotransport remainingscouts
parallel independent hence doscouting active dotransport
remainingscouts active point execution hence transition observation reward functions parent execute mission given corresponding
functions doscouting combination corresponding functions
dotransport remainingscouts
use top order determine construct factored rmtdp
plan hierarchy shown replace particular sub plan
constituent sub plans independent sequentially executed
rmtdp defined particular sub plan process applied recursively
starting root component plan hierarchy concrete example consider
mission rehearsal simulation domain hierarchy illustrated figure b
given temporal constraints doscouting dotransport doscouting remainingscouts exploited sequential decomposition dotransport
remainingscouts parallel independent components hence replace
executemission doscouting dotransport remainingscouts apply process doscouting constituent components doscouting
neither independent sequentially executed thus doscouting cannot replaced
constituent components thus rmtdp mission rehearsal domain comprised
smaller rmtdps doscouting dotransport remainingscouts
thus top identify relevant variables building factored rmtdp
utilizing structure top decompose construction procedure reduce load
domain expert model construction furthermore shown section
factored model greatly improves performance search best role allocation


finair tambe

build rmtdp top top sub plan subplan
children subplanchildren subplanchildren returns sub plans within subplan
children null children loosely coupled independent

rmtdp define rmtdp subplan automated

return rmtdp
else

child children

factors child build rmtdp top child

rmtdp constructfromfactors factors

return rmtdp

exploiting top beliefs evaluation rmtdp policies
present technique exploiting tops speeding evaluation rmtdp
policies explain improvement first describe original
determining expected reward joint policy local policies agent
indexed entire observation histories pynadath tambe nair pynadath
yokoo tambe marsella obtain rmtdp policy top
follows obtain
e action performed agent observation history

action performed agent following top set privately
held beliefs corresponding observation history compute expected reward
rmtdp policy projecting teams execution possible branches
different world states different observations time step compute
expected value joint policy n team starting given state st
given set past observations

nt follows
x











nt r st
vt st
n nt
p
n
nt st
st

x









st
n
nt nt vt st
nt





expected reward joint policy given v null null
start state time step computation vt performs summation
possible world states agent observations time complexity
computation
repeated states observation histories length e

times therefore
given time horizon overall complexity algo



rithm

discussed section team oriented program agents action selection
currently held private beliefs note mutual beliefs modeled
privately held beliefs agents per footnote similar technique
exploited mapping top rmtdp policy indeed evaluation rmtdp
policy corresponds top speeded agents local policy indexed
private beliefs refer top congruent belief state agent


fihybrid bdi pomdp framework multiagent teaming

rmtdp note belief state probability distribution world
states single agent pomdp rather privately held beliefs bdi
program agent time similar idea representing policy
finite state controller hansen zhou poupart boutilier case
private beliefs would map states finite state controller
belief rmtdp policy evaluation leads speedup multiple observation
histories map belief state speedup key illustration exploitation
synergistic interactions top rmtdp instance belief representation techniques used top reflected rmtdp resulting faster policy evaluation
help us optimize top performance detailed example belief state presented later
brief explanation belief rmtdp policies evaluated
evaluation observation histories compute expected reward
belief policy projecting teams execution possible branches
different world states different observations time step compute
expected value joint policy n team starting given state st
given team belief state nt follows





x





vt st nt r st n nt p st n nt st
st

x









st n nt nt vt st nt








beliefupdatefunction







complexity computing function expression bf bf
represents complexity belief update function beliefupdatefunction
time step computation value function done every state possible
reachable belief states let max tt represent maximum number
possible belief states agent point time number
belief states agent therefore complexity
given n bf note
exponent unlike expression thus evaluation method
give large time savings quantity n much less
ii belief update cost low practical bdi systems multiple observation histories
map often onto belief state thus usually n much less
furthermore since belief update function mirrors practical bdi systems
complexity low polynomial constant indeed experimental
significant speedups switching top congruent belief states
however absolute worst case belief update function may simply append
observation history past observations e top congruent beliefs
equivalent keeping entire observation histories thus belief evaluation
complexity observation history evaluation
turn example belief policy evaluation mission rehearsal
domain time step transport helicopters may receive observation


finair tambe

whether scout failed observation function use observationhistory representation policy transport agent would maintain complete
history observations could receive time step example setting
two scout helicopters one route route particular transport
helicopter may several different observation histories length two every time step
transports may receive observation scout alive failed
thus time transport helicopter might one following observation histories length two sct onroute alive sct onroute alive sct onroute f ailed
sct onroute f ailed sct onroute alive sct onroute f ailed sct onroute
f ailed sct onroute f ailed sct onroute alive sct onroute f ailed
etc however action selection transport helicopters depends whether
critical failure e last remaining scout crashed taken place change
role whether failure critical determined passing observation
belief update function exact order observations received
precise times failure non failure observations received relevant
determining critical failure taken place consequently whether transport
change role scout thus many observation histories map onto
belief states example three observation histories map belief
criticalf ailure doscouting e critical failure taken place significant speedups belief evaluation equation needs executed
smaller number belief states linear domains opposed observation
history evaluation equation executed exponential number observation histories actual speedup obtained mission rehearsal domain
demonstrated empirically section

optimizing role allocation
section focused mapping domain interest onto rmtdp
policy evaluation section focuses efficient techniques rmtdp policy search
service improving bdi top team plans top essence provides incomplete
fixed policy policy search optimizes decisions left open incomplete policy
policy thus completed optimizes original top see figure enabling rmtdp
focus search incomplete policies providing ready made decompositions
tops assist rmtdps quickly searching policy space illustrated
section focus particular role allocation hunsberger grosz
modi shen tambe yokoo tidhar et al fatima wooldridge
critical teams top provides incomplete policy keeping open
role allocation decision agent rmtdp policy search provides optimal
role taking action role allocation decision points contrast previous
role allocation approaches determines best role allocation taking
consideration uncertainty domain future costs although demonstrated
solving role allocation methodology general enough apply
coordination decisions


fihybrid bdi pomdp framework multiagent teaming

hierarchical grouping rmtdp policies
mentioned earlier address role allocation top provides policy complete
except role allocation decisions rmtdp policy search optimally fills
role allocation decisions understand rmtdp policy search useful gain
understanding role allocation search space first note role allocation focuses
deciding many types agents allocate different roles organization
hierarchy role allocation decision may made time may made
later time conditioned available observations figure shows partially expanded role
allocation space defined top organization hierarchy figure six helicopters
node role allocation space completely specifies allocation agents roles
corresponding level organization hierarchy ignore number
right node instance root node role allocation space specifies
six helicopters assigned task force level one organization hierarchy
leftmost leaf node level three figure specifies one helicopter assigned
sctteama zero sctteamb zero sctteamc five helicopters transport team
thus see leaf node role allocation space complete valid role
allocation agents roles organization hierarchy
order determine one leaf node role allocation superior another evaluate
rmtdp constructing rmtdp policy particular
example role allocation specified leaf node corresponds role taking actions
agent execute time example case leftmost leaf
figure time one agent recall section homogeneous team
hence specific agent matter become member sctteama
agents become members transport team thus one agent roletaking policy include null joinsctt eama agents j j
include j null joint ransportt eam case assume rest
role taking policy e roles reallocated scout fails obtained role
reallocation bdi top interpreter steam tambe
et al thus example role reallocation indeed performed steam
steams reallocation policy included incomplete policy
rmtdp initially provided thus best role allocation computed keeping
mind steams reallocation policy steam given failure agent playing rolef
agent playing roler replace
criticality rolef criticality roler
criticality x x critical otherwise

thus agents observations critical failure taken place
replacing agents decision replace computed expression
included incomplete policy input rmtdp since incomplete
policy completed role allocation leaf node technique
able construct policy rmtdp corresponds role allocation
domains robocuprescue allocation decisions made time
domains possible role allocation conditioned observations
communication obtained course execution instance
shown figure robocuprescue scenario ambulances allocated
sub team ambulanceteama ambulanceteamb information location


finair tambe

































































figure partially expanded role allocation space mission rehearsal domain six helos

civilians conveyed fire engines allocation ambulances
conditioned communication e number civilians location
figure shows partially expanded role allocation scaled rescue scenario
three civilians two ambulances two fire engines one station
station figure depicts fact two ambulances
one fire engine station shown level allocation fire engines
engineteama engineteamb gives number engines assigned
engineteam station next level leaf level different leaf nodes
possible assignment ambulances ambulanceteama ambulanceteamb depending
upon value communication c since three civilians exclude
case civilians present particular fire two possible messages e
one civilian fire two civilians fire c
taskforce




engineteama engineteamb ambteam

c

engineteama engineteamb ambteam









c

c

ambteama ambteamb ambteama ambteamb

c

ambteama ambteamb ambteama ambteamb

figure partially expanded role allocation space rescue domain one fire engine
station one fire engine station two ambulances three civilians
thus able exploit top organization hierarchy create hierarchical
grouping rmtdp policies particular leaf node represents complete
rmtdp policy role allocation specified leaf node parent node
represents group policies evaluating policy specified leaf node equivalent
evaluating specific role allocation taking future uncertainties account could


fihybrid bdi pomdp framework multiagent teaming

brute force search role allocations evaluating order determine
best role allocation however number possible role allocations exponential
leaf roles organization hierarchy thus must prune search space
pruning role allocation space
prune space valid role allocations upper bounds maxestimates
parents leaves role allocation space admissible heuristics section
leaf role allocation space represents completely specified policy maxestimate upper bound maximum value policies parent node
evaluated rmtdp obtain maxestimates parent nodes shown
brackets right parent node figure use branch bound style
pruning see discuss note essence
performs branch bound style pruning key novelty step discuss
section
branch bound works follows first sort parent nodes
estimates start evaluating children parent highest maxestimate steps evaluate rmtdp child refers evaluation
leaf level policy child rmtdp model evaluation leaf level policies step
done methods described section case
role allocation space figure would start evaluating leaves parent
node one helicopter scouting team five transport team value
evaluating leaf node shown right leaf node obtained
value best leaf node steps case compare
maxestimates parents role allocation space
steps see figure would pruning three parent nodes
leftmost parent right two parents avoid evaluation leaf level
policies next would proceed evaluate leaf nodes parent
two helos scouting team four transport team would pruning
remaining unexpanded parent nodes return leaf highest value
case node corresponding two helos allocated sctteama four
transport team although demonstrated level hierarchy methodology
applying deeper hierarchies straightforward
exploiting top calculate upper bounds parents
discuss upper bounds parents called maxestimates calculated parent maxestimate parent defined strict upper bound
maximum expected reward leaf nodes necessary
maxestimate upper bound else might end pruning potentially useful role
allocations order calculate maxestimate parent could evaluate
leaf nodes rmtdp would nullify benefit subsequent pruning therefore turn top plan hierarchy see figure b break
evaluation parent node components evaluated separately thus
decomposing words exploits structure bdi
program construct small scale rmtdps unlike decomposition techniques


finair tambe

branch bound policy search
parents list parent nodes
compute maxexp parents
sort parents decreasing order maxexp
bestval
parent parents

done parent false pruned parent false
parent parents

done parent false pruned parent false

child parentnextchild child leaf level policy parent

child null

done parent true

else

childval evaluate rmtdp child

childval bestval

bestval childval best child

parent parents

maxexp parent bestval

pruned parent true
return best

assume decomposition ultimately rely domain experts identify interactions
agents reward transition functions dean lin guestrin venkataraman
koller
parent role allocation space use small scale rmtdps evaluate values top component fortunately discussed section
exploited small scale rmtdps corresponding top components constructing larger
scale rmtdps put small scale rmtdps use evaluating policies within
component obtain upper bounds note evaluation leaf level
policies evaluation components parent node done
observation histories see equation belief states see equation describe
section observation history evaluation method computing values
components parent summed obtain maxestimate
upper bound childrens values thus whereas parent role allocation space
represents group policies top components sub plans allow component wise
evaluation group obtain upper bound expected reward policy
within group
exploits smaller scale rmtdp components discussed section
obtain upper bounds parents first order evaluate maxestimate
parent node role allocation space identify start states component
evaluate rmtdps explain step parent node figure
scouting team two helos transport team four helos see figure first
component preceding components start states corresponds
start states policy top mapped onto next


fihybrid bdi pomdp framework multiagent teaming

components next component one linked sequential dependence
start states end states preceding component however explained later
section significantly reduce list start states component
evaluated
maxexp method calculating upper bounds parents role allocation space
parent search space

maxexp parent

component corresponding factors rmtdp section

component preceding component j

obtain start states states endstates j

states removeirrelevantfeatures states discard features present
si

obtain corresponding observation histories start ohistories
endohistories j

ohistories removeirrelevantobservations ohistories

else

obtain start states states

observation histories start ohistories null

maxeval

leaf level policies parent

maxeval max maxeval maxsi states ohi ohistories evaluate rm dpi
si ohi


maxexp parent maxeval
similarly starting observation histories component observation histories completing preceding component observation history first
component bdi plans normally refer entire observation histories rely
key beliefs typically referred pre conditions component
starting observation history shortened include relevant observations
thus obtaining reduced list starting observation sequences divergence private observations problematic e g cause agents trigger different team plans
indicated earlier section top interpreters guarantee coherence
key aspects observation histories instance discussed earlier top interpreter
ensures coherence key beliefs initiating terminating team plans top thus
avoiding divergence observation histories
order compute maximum value particular component evaluate
possible leaf level policies within component possible start states observation histories obtain maximum steps evaluation
store end states ending observation histories used
evaluation subsequent components shown figure evaluation
doscouting component parent node two helicopters assigned
scouting team four helos transport team leaf level policies correspond
possible ways helicopters could assigned teams sctteama sctteamb sct

finair tambe

teamc transport team e g one helo sctteamb one helo sctteamc four
helos transport team two helos sctteama four helos transport team etc
role allocation tells agents role take first step remainder
role taking policy specified role replacement policy top infrastructure
role execution policy specified doscouting component top
obtain maxestimate parent node role allocation space simply
sum maximum values obtained component steps e g
maximum values component see right component figure
summed obtain maxestimate seen figure third
node left indeed upper bound
calculation maxestimate parent nodes much faster
evaluating leaf nodes cases two reasons firstly parent nodes
evaluated component wise thus multiple leaf level policies within one component
end state remove duplicates get start states next component since component contains state features relevant number
duplicates greatly increased duplication evaluation effort cannot avoided
leaf nodes policy evaluated independently start finish instance doscouting component role allocations sctteama sctteamb
sctteamc transportteam sctteama sctteamb sctteamc transportteam end states common eliminating irrelevant features
scout sctteamb former allocation scout sctteamc latter allocation fail feature elimination steps
state features retained dotransport scouted route number transports
transports may replaced failed scouts shown figure
second reason computation maxestimates parents much faster
number starting observation sequences much less number ending observation histories preceding components observations
observation histories component relevant succeeding components steps thus function removeirrelevantobservations reduces number
starting observation histories observation histories preceding component
refer methodology obtaining maxestimates parent maxexp variation maximum expected reward failures nofail
obtained similar fashion except assume probability agent failing able make assumption evaluating parent node since
focus obtaining upper bounds parents obtaining exact value
less branching hence evaluation component proceed much
quicker nofail heuristic works evaluation policy without failures
occurring higher evaluation policy failures possible
normally case domains evaluation nofail heuristics
role allocation space six helicopters shown square brackets figure
following theorem shows maxexp method finding upper bounds
indeed finds upper bound thus yields admissible search heuristic branchand bound search role allocation space
theorem maxexp method yield upper bound


fihybrid bdi pomdp framework multiagent teaming


doscouting
scoutingteam transportteam

alloc
sctteama
sctteamb
sctteamc
transportteam

alloc
sctteama
sctteamb
sctteamc
transportteam


dotransport
transportteam

startstate
routescouted
transports


remainingscouts
scoutteam

startstate
routescouted
transports

startstate
routescouted
transports

figure component wise decomposition parent exploiting top

proof see appendix c
theorem conclude branch bound policy search
best role allocation since maxestimates parents true
upper bounds help theorem worst case
branch bound policy search complexity brute force search
theorem worst case complexity evaluating single parent node maxexp
evaluating every leaf node within constant factor
proof sketch
worst case complexity maxexp arises
let esj end states component j executing policy removing
features irrelevant succeeding component k similarly let esj
end states component j executing policy
tremoving features
irrelevant succeeding component k esj esj null
duplication end states occur
let ohj ending observation histories component j executing policy
removing observations irrelevant succeeding component
k similarly let ohj ending observation histories component j executing policy removing observation
histories irrelevant

succeeding component k ohj ohj null duplication
observation histories occur note belief evaluation used
would replace observation histories top congruent belief states
see sect
case computational advantage evaluating components
maxestimate separately thus equivalent evaluating child node
parent thus worst case maxexp computation parent
evaluating children within constant factor
addition worst case pruning maxexp every
leaf node need evaluated equivalent evaluating leaf node twice


finair tambe

thus worst case complexity branch bound search maxexp
finding best role allocation evaluating every leaf node refer
brute force noprune thus worst case complexity maxexp
noprune however owing pruning savings decomposition computation maxestimates significant savings likely average
case section highlights savings mission rehearsal robocuprescue
domains

experimental
section presents four sets context two domains introduced
section viz mission rehearsal robocuprescue kitano et al first
investigated empirically speedups top congruent belief
states belief evaluation observation history evaluation
section brute force search focus determining
best assignment agents roles assume fixed top top infrastructure
second conducted experiments investigate benefits considering uncertainty
determining role allocations compared allocations found rmtdp
role allocation allocations consider kind uncertainty
ii allocations consider observational uncertainty consider action
uncertainty third conducted experiments domains determine sensitivity
changes model fourth compare performance allocations
found rmtdp role allocation allocations human subjects
complex domains robocuprescue simulations
mission rehearsal domain
mission rehearsal domain top one discussed section
seen figure organization hierarchy requires determining number agents
allocated three scouting sub teams remaining helos must allocated
transport sub team different numbers initial helicopters attempted varying
three ten details rmtdp constructed domain given
appendix b probability failure scout time step routes
respectively probability transport observing alive scout
routes respectively false positives possible
e transport observe scout alive failed probability
transport observing scout failure routes respectively
false positives possible hence transport observe failure
unless actually taken place
figure shows comparing different methods searching role
allocation space four methods method adds speedup techniques
previous
noprune obs brute force evaluation every role allocation determine
best agent maintains complete observation history evaluation
equation used ten agents rmtdp projected


fihybrid bdi pomdp framework multiagent teaming

order reachable states order observation histories
per role allocation evaluated thus largest experiment category limited
seven agents
noprune bel brute force evaluation every role allocation difference
method noprune obs use belief evaluation
see equation
maxexp branch bound search described section
uses upper bounds evaluation parent nodes best allocation
evaluation parent leaf nodes uses belief evaluation
nofail modification branch bound heuristic mentioned section
essence maxexp except upper bounds computed making
assumption agents fail heuristic correct domains
total expected reward failures less failures present
give significant speedups agent failures one primary sources
stochasticity method evaluation parent leaf nodes uses
belief evaluation note upper bounds computed
failure assumption changes assumed actual domains
figure axis number nodes role allocation space evaluated
includes leaf nodes well parent nodes figure b axis represents
runtime seconds logarithmic scale figures vary number agents
x axis experimental previous work distributed pomdps often
restricted two agents exploiting hybrid able vary number
agents three ten shown figure clearly seen figure
pruning significant reductions obtained maxexp nofail noprunebel terms numbers nodes evaluated reduction grows quadratically
fold ten agents noprune obs identical noprune bel terms
number nodes evaluated since methods leaf level policies evaluated
method evaluation differs important note although nofail
maxexp number nodes evaluated domains
necessarily true general nofail evaluate least many nodes
maxexp since estimate least high maxexp estimate however
upper bounds computed quicker nofail
figure b shows noprune bel method provides significant speedup
noprune obs actual run time instance fold speedup
noprune bel instead noprune obs seven agent case noprune obs
could executed within day settings greater seven agents
empirically demonstrates computational savings possible belief evaluation instead observation history evaluation see section reason
use belief evaluation maxexp nofail approaches
number nodes noprune eight agents obtained experiments rest
calculated formula n n n n represents number
heterogeneous role types n number homogeneous agents n n
referred rising factorial



finair tambe

remaining experiments maxexp heuristic fold speedup
noprune bel eight agent case
nofail heuristic quick compute upper bounds far outperforms
maxexp heuristic fold speedup maxexp ten agents speedups
maxexp nofail continually increase increasing number agents speedup
nofail method maxexp marked domain ignoring
failures much less branching


nofail maxexp

number nodes



noprune obs
noprune bel
























number agents


maxexp
nofail
noprune bel
noprune obs

time secs log scale

























number agents

figure performance role allocation space search mission rehearsal domain left
number nodes evaluated b right run time seconds log scale

next conducted experiments illustrating importance rmtdps reasoning
action observation uncertainties role allocations compared
allocations found rmtdp role allocation allocations found two
different methods see figure
role allocation via constraint optimization cop modi et al mailler lesser
allocation cop leaf level sub teams modi et al work focused decentralized cop investigation emphasis
resulting role allocation generated cop decentralization per se



fihybrid bdi pomdp framework multiagent teaming

ganization hierarchy treated variables number helicopters
domain variable thus domain may helicopters
reward allocating agents sub teams expressed terms constraints
allocating helicopter scout route assigned reward corresponding
routes distance ignoring possibility failure e ignoring transition
probability allocating helicopters subteam obtained proportionally higher reward
allocating helicopter transport role assigned large reward transporting cargo destination allocating helicopters subteam
obtained proportionally higher reward
allocating least one scout role assigned reward negative infinity
exceeding total number agents assigned reward negative infinity
rmtdp complete observability consider transition
probability ignore partial observability achieved assuming complete observability rmtdp mtdp complete observability equivalent
markov decision mdp pynadath tambe actions
joint actions thus refer allocation method mdp method
figure shows comparison rmtdp allocation mdp allocation cop allocation increasing number helicopters x axis compare
expected number transports get destination axis metric
comparison since primary objective domain seen considering forms uncertainty rmtdp performs better considering transition
uncertainty mdp turn performs better considering uncertainty cop
figure b shows actual allocations found three methods four helicopters
six helicopters case four helicopters first three bars rmtdp mdp
identical two helicopters scouting route two helicopters taking transport role
cop allocation however consists one scout route three transports
allocation proves myopic fewer transports getting destination
safely case six helicopters cop chooses one scout helicopter route
shortest route mdp two scouts route
longest route albeit safest rmtdp considers observational
uncertainty chooses additional scout route order take care cases
failures scouts go undetected transports
noted performance rmtdp allocation depend
values elements rmtdp model however next experiment
revealed getting values exactly correct necessary order test sensitivity
performance allocations actual model values introduced error
parameters model see allocations found incorrect model
would perform original model without errors emulates situation
model correctly represent domain figure shows expected number
transports reach destination axis mission rehearsal scenario six
helicopters error x axis introduced parameters model instance


finair tambe



number transports




rmtdp
cop
mdp

















number agents

helos

rm

xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx

rt
rt
xxx
xxxrt
xxx
transports
xxxx
xxxx
xxxx
xxxx
xxxx

xxxx
xxxx
xxxx
xxxx
xxxx

dp

td

p



xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx

xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx

xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxx





xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx

td
p

xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx

xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx

rm



xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx
xxxxxxxxxx

xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxxxxxx
xxxxxx
xxxxxxxxxx
xxxxxxxxxx


dp



xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxx
xxxxxxxxxx
xxxxxx
xxxxxxxxxx
xxxxxxxxxx

p



co

number helos



p

xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx
xxxxxxxxx

co

helos



figure comparison performance different allocation methods b allocations
found different allocation methods



fihybrid bdi pomdp framework multiagent teaming

percentage error failure rate route route failure rate
e erroneous failure rate actual failure rate difference
number transports reached destination however
percentage error greater allocation found conservative resulting
fewer transports getting destination similarly percentage error less
allocation found risky scouts assigned resulting
failures general figure shows model insensitive errors
model parameters mission rehearsal domain model parameters
outside range non optimal allocations would comparing non optimal
allocations cop perform better cop range
errors tested failure rate well observability routes instance
error failure rate route rmtdp managed transports
safely reach destination cop managed get transports reach safely
comparing non optimal allocations mdp performed better
mdp within range error observability routes thus
although allocations found incorrect model non optimal performed
better cop mdp large ranges errors model shows getting
model exactly correct necessary good allocations thus able
obtain benefits rmtdp even without insisting accurate model


route failure rate
route failure rate
route failure rate
route observability

number transports























percentage error

figure model sensitivity mission rehearsal domain

robocuprescue domain
speedups robocuprescue domain
next set experiments highlight computational savings obtained
robocuprescue domain scenario experiment consisted two fires different
locations city fires different initially unknown number civilians
however total number civilians distribution locations
civilians chosen known ahead time experiment fix number
civilians five set distribution used choose civilians locations uniform
number fire engines set five located three different fire stations described


finair tambe

section vary number ambulances co located ambulance center
two seven reason chose change number ambulances
small number fire engines unable extinguish fires changing completely
goal determine fire engines allocate fire information
civilians transmitted many ambulances send fire location
figure highlights savings terms number nodes evaluated actual
runtime increase number agents noprune bel
maxexp noprune obs could run slowness nofail
heuristic identical maxexp since agents cannot fail scenario rmtdp
case reachable states
figures b increase number ambulances along xaxis figure number nodes evaluated parent nodes leaf nodes
logarithmic scale seen maxexp method fold
decrease number nodes evaluated compared noprune bel seven
ambulances decrease becomes pronounced number ambulances
increased figure b shows time seconds logarithmic scale axis
compares run times maxexp noprune bel methods finding best
role allocation noprune bel method could best allocation within
day number ambulances increased beyond four four ambulances
five fire engines maxexp resulted fold speedup noprune bel
allocation robocuprescue
next set experiments shows practical utility role allocation analysis
complex domains able significant performance improvements actual
robocuprescue domain role allocations generated analysis first
construct rmtdp rescue scenario described section taking guidance
top underlying domain described section use
maxexp heuristic determine best role allocation compared rmtdp
allocation allocations chosen human subjects goal comparing rmtdp
allocations human subjects mainly rmtdp capable performing
near human expert levels domain addition order determine
reasoning uncertainty actually impacts allocations compared rmtdp
allocations allocations determined two additional allocation methods
rescueisi allocations used robocuprescue agents entered
robocuprescue competitions rescueisi nair et al
finished third place agents used local reasoning decision making
ignoring transitional well observational uncertainty
rmtdp complete observability discussed earlier complete observability
rmtdp leads mdp refer method mdp method
number nodes evaluated noprune bel computed f f f
c f f f number fire engines station respectively
number ambulances c number civilians node provides complete conditional
role allocation assuming different numbers civilians fire station



fihybrid bdi pomdp framework multiagent teaming

number nodes log scale



maxexp
noprune





















number ambulances

run time secs log scale



maxexp
noprune



















number ambulances





figure performance role allocation space search robocuprescue left number
nodes evaluated log scale b right run time seconds log
scale



finair tambe

note comparisons performed robocuprescue simulator
multiple runs deal stochasticity scenario described section
fix number fire engines ambulances civilians five experiment
consider two settings location civilians drawn
uniform distribution cases four civilians fire one civilian
fire three civilians fire two fire two civilians
fire three fire remaining one civilian fire
four civilians fire speedup section obtained
distribution
skewed distribution cases four civilians fire one civilian
fire remaining one civilian fire four civilians fire
note consider case civilians located fire
optimal ambulance allocation simply assign ambulances fire
civilians located skewed distribution chosen highlight cases
becomes difficult humans reason allocation choose
three human subjects used experiment researchers usc three
familiar robocuprescue given time study setup
given time limit provide allocations subject told allocations
going judged first basis number civilian lives lost next
damage sustained due fire exactly criteria used robocuprescue kitano
et al
compared rmtdp allocation human subjects
robocuprescue simulator rescueisi mdp figure compared
performance allocations basis number civilians died
average damage two buildings lower values better criteria two
criteria main two criteria used robocuprescue kitano et al values shown figure obtained averaging forty simulator runs uniform
distribution twenty runs skewed distribution allocation average
values plotted account stochasticity domain error bars provided
standard error allocation method
seen figure rmtdp allocation better five
allocations terms lower number civilians dead although human quite close
example averaging forty runs rmtdp allocation resulted civilian deaths
human allocation resulted civilian deaths terms average building
damage six allocations almost indifferentiable humans actually performing marginally better skewed distribution difference allocations
much perceptible see figure b particular notice rmtdp
allocation much better humans terms number civilians dead
human particularly badly bad allocation fire engines resulted
damage buildings consequently number civilians dead
mission rehearsal domain could run actual mission rehearsal simulator since
simulator public domain longer accessible hence difference tested role
allocations mission rehearsal robocuprescue domains



fihybrid bdi pomdp framework multiagent teaming

comparing rmtdp rescueisi mdp showed reasoning
transitional uncertainty mdp better static reactive allocation method
rescueisi well reasoning transitional observational uncertainty uniform distribution case found rmtdp better mdp
rescueisi mdp method performing better rescueisi skewed distribution case improvement allocations rmtdp greater averaging twenty
simulation runs rmtdp allocations resulted civilians deaths mdp resulted
rescueisi allocation method used rescueisi often resulted
one fires allocated fire engines allocations determined
mdp turned human
two tailed test performed order test statistical significance means
allocations figure means number civilians dead rmtdp
allocation human allocations found statistically different confidence
uniform well skewed distributions difference fire
damage statistically significant uniform case however difference
rmtdp allocation human fire damage statistically significant
skewed case


civilians casualties
building damage







dp

ue








sc









hu

hu








hu

rm

td

p





civilians casualties
building damage







dp





sc
ue






hu




hu




hu

rm
td

p



figure comparison performance robocuprescue left uniform b right
skewed



finair tambe

considering average performance different allocations highlight
individual cases marked differences seen performance figure
present comparison particular settings allocation methods showed
bigger difference rmtdp terms allocations standard error shown
error bars allocation figures b compare allocations uniform
civilian distributions setting one civilian fire four civilians
fire civilian setting four civilians fire one fire civilian setting
respectively seen figure rmtdp allocation fewer civilian
casualties slightly damage buildings due fire difference fire damage
statistically significant damage values close figures c
compare allocations skewed civilian distribution key difference
arises human seen human damage due fire
human allocated fire engines one buildings turn resulted
building burnt completely consequently civilians located fire
location could rescued ambulances thus see specific instances
allocation done rmtdp allocation superior allocations
human comes



civilians casualties
building damage





civilians casualties
building damage





















dp



ue




sc








hu



hu

p




td

civilians casualties
building damage



hu

rm



dp






ue




hu

sc




hu





hu

td
rm








p









civilians casualties
building damage






















dp




sc
ue







hu




hu

hu

p
rm
td

dp



ue







sc

hu




hu




hu

td
p
rm









figure comparison performance robocuprescue particular settings topleft uniform civilian setting b top right uniform civilian setting c
bottom left skewed civilian setting bottom right skewed civilian
setting



fihybrid bdi pomdp framework multiagent teaming

table shows allocations fire agents assigned fire allocated fire
found rmtdp role allocation used human subjects
skewed civilian setting consider case since shows difference
particular table highlights differences allocators skewed
civilian setting helps account differences seen performance
actual simulator seen figure main difference performance
terms number civilians saved recall scenario four
civilians fire one fire human subjects mdp chose
send one ambulance fire number ambulances allocated f ire
number ambulances allocated f ire lone ambulance unable rescue
civilian fire resulting humans mdp saving fewer civilians rescueisi chose
send ambulances fire greedy selection method proximity
civilians resulting civilians fire dying terms fire engine allocation
human sent four fire engines fire civilians likely located
number engines allocated f ire number engines allocated f ire
unfortunately backfired since lone fire engine fire able extinguish
fire causing fire spread parts city
distribution
skewed

engines station
engines station
engines station
ambulances

rmtdp





human





human





human





rescueisi





mdp





table allocations ambulances fire engines fire
experiments allocations found rmtdp role allocation performs significantly better allocations chosen human subjects rescueisi
mdp cases significantly worse case particular
distribution civilians uniform difficult humans come
allocation difference human allocations rmtdp allocation
becomes significant conclude rmtdp allocation performs
near human expertise
last experiment done robocuprescue simulator introduced error
rmtdp model order determine sensitive model errors
parameters model figure compares allocations found five
ambulances fire engines civilians terms number civilian casualties yaxis error x axis introduced probability fire spread probability
civilian health deterioration seen increasing error probability fire
spread higher allocations save fewer civilians fire brigades
choose concentrate effort one fires resulting allocation
found value terms number civilians casualties used
rescueisi consider uncertainty reducing error probability
fire impact allocations found increasing error probability
strategy ambulances going closest civilian worked fairly well ambulances
usually well spread



finair tambe

civilian health deterioration higher caused civilians sacrificed
allocation found value terms number civilians casualties
used rescueisi decreasing error probability civilian health deterioration
lower negative caused number ambulances allocated fire
number civilians fire human


civilian casualties




fire rate
civilian health




















percentage error

figure model sensitivity robocuprescue scenario

related work
four related areas wish highlight first
considerable amount work done field multiagent teamwork section
second related area use decision theoretic particular
distributed pomdps section third area related work describe section
hybrid systems used markov decision process bdi approaches finally
section related work role allocation reallocation multiagent teams
described
bdi teamwork
several formal teamwork theories joint intentions cohen levesque
sharedplans grosz kraus proposed tried capture essence
multiagent teamwork logic beliefs desires intentions next practical
teamwork collagen rich sidner grate jennings
steam tambe built teamwork theories cohen levesque grosz
kraus attempted capture aspects teamwork reusable
across domains addition complement practical teamwork teamoriented programming pynadath tambe tidhar b
introduced allow large number agents programmed teams
expanded applied variety domains pynadath tambe yen
et al da silva demazeau approaches building practical multia

fihybrid bdi pomdp framework multiagent teaming

gent systems stone veloso decker lesser explicitly
team oriented programming could considered family
reported article complements teamwork introducing hybrid bdi pomdp exploit synergy bdi pomdp
approaches particular top teamwork traditionally addressed
uncertainty cost hybrid model provides capability illustrated
benefits reasoning via detailed experiments
article uses team oriented programming tambe et al da silva
demazeau tidhar b example bdi relevant
similar techniques modeling tasking collectives agents decker
lessers taems particular taems language provides abstraction tasking collaborative groups agents similar top gpgp infrastructure used executing taems tasks analogous top interpreter
infrastructure shown figure lesser et al explored use distributed
mdps analyses gpgp coordination xuan lesser exploited
use taems structures decomposition abstraction searching optimal policies
distributed mdps suggested article thus article complements lesser
et al work illustrating significant avenue efficiency improvements
analyses
distributed pomdp
distributed pomdp represent collection formal expressive
enough capture uncertainty domain costs rewards associated
states actions given group agents deriving separate policies maximize joint reward modeled distributed pomdp
particular dec pomdp decentralized pomdp bernstein et al
mtdp multiagent team decision pynadath tambe generalizations pomdps case multiple distributed agents basing
actions separate observations frameworks allow us formulate
constitutes optimal policy multiagent team principle derive policy
however exceptions effective deriving policies distributed
pomdps developed significant progress achieved efficient
single agent pomdp policy generation monahan cassandra littman
zhang kaelbling et al however unlikely directly
carried distributed case finding optimal policies distributed pomdps
nexp complete bernstein et al contrast finding optimal policy single
agent pomdp pspace complete papadimitriou tsitsiklis bernstein et
al note bernstein et al suggests fundamental difference nature
distributed cannot treated one separate pomdps
individual policies generated individual agents possible cross agent
interactions reward transition observation functions one action one
agent may many different rewards possible actions agents
may take


finair tambe

three approaches used solve distributed pomdps one
typically taken make simplifying assumptions domain instance
guestrin et al assumed agent completely observe world state
addition assumed reward function transition function team
expressed sum product reward transition functions agents
team becker et al assume domain factored agent
completely observable local state domain transition independent
one agent cannot affect another agents local state
second taken simplify nature policies considered
agents example chades et al restrict agent policies memoryless
reactive policies thereby simplifying solving multiple mdps peshkin et
al take different gradient descent search local optimum
finite controllers bounded memory nair et al present finding
locally optimal policy space unrestricted finite horizon policies third
taken hansen et al involves trying determine globally optimal
solution without making simplifying assumptions domain
attempt prune space possible complete policies eliminating dominated
policies although brave frontal assault method expected
face significant difficulties scaling due fundamental complexity obtaining
globally optimal solution
key difference work focused hybrid systems
leverage advantages bdi team plans used practical systems
distributed pomdps quantitatively reason uncertainty cost particular
use tops specify large scale team plans complex domains use rmtdps
finding best role allocation teams
hybrid bdi pomdp approaches
pomdp used context analysis single agent schut
wooldridge parsons multiagent pynadath tambe xuan et al
behavior schut et al compare strategies intention reconsideration deciding
deliberate intentions modeling bdi system pomdp
key differences work apply analysis single
agent case consider issues exploiting bdi system structure improving
pomdp efficiency
xuan lesser pynadath tambe analyze multiagent
communication xuan lesser dealt finding evaluating communication policies pynadath tambe used com mtdp model deal comparing communication strategies empirically analytically
general explain analyzing coordination actions including communication concretely demonstrate analysis role
allocation additional key differences earlier work pynadath tambe
follows rmtdp illustrate techniques exploit team plan decomposition
speeding policy search absent com mtdp ii introduce techniques
belief evaluation absent previous work nonetheless combining rmtdp


fihybrid bdi pomdp framework multiagent teaming

com mtdp interesting avenue preliminary steps
direction presented nair tambe marsella b
among hybrid systems focused analysis scerri et al employ markov
decision processes within team oriented programs adjustable autonomy key difference work mdps used execute particular
sub plan within tops plan hierarchy making improvements top
dtgolog boutilier reiter soutchanski thrun provides first order language
limits mdp policy search via logical constraints actions although shares
work key idea synergistic interactions mdps golog differs
work focuses single agent mdps fully observable domains
exploit plan structure improving mdp performance isaac nair tambe marsella
raines system analyzing multiagent teams employs decision theoretic
methods analyzing multiagent teams work probabilistic finite automaton
pfa represents probability distribution key patterns teams behavior
learned logs teams behaviors key difference work
analysis performed without access actual team plans agents
executing hence advice provided cannot directly applied improving team
need human developer change team behavior per advice generated

role allocation reallocation
several different approaches role allocation reallocation
example tidhar et al tambe et al performed role allocation
matching capabilities hunsberger grosz proposed use combinatorial auctions decide roles assigned modi et al showed
role allocation modeled distributed constraint optimization
applied tracking multiple moving targets distributed sensors
shehory kraus suggested use coalition formation deciding
quickly agent took role fatima wooldridge use auctions
decide task allocation important note competing techniques
free model even though model
transition probabilities approaches reforming team reconfiguration methods due dunin keplicz verbrugge self adapting organizations horling
lesser dynamic organizing groups barber martin scerri et
al present role allocation allows autonomy role reallocation
shift human supervisor agents
key difference prior work use stochastic rmtdps
evaluate allocations enables us compute benefits role allocation taking
account uncertainty costs reallocation upon failure example mission
rehearsal domain uncertainties considered one scout would
allocated leading costly future reallocations even mission failure instead
lookahead depending probability failure multiple scouts sent one
routes resulting fewer future reallocations higher expected reward


finair tambe

conclusion
bdi agent teamwork provided successful applications tools
techniques provide quantitative analyses team coordination team behaviors uncertainty lacking emerging field distributed pomdps provides
decision theoretic method quantitatively obtaining optimal policy team
agents faces serious intractability challenge therefore article leverages
benefits bdi pomdp approaches analyze improve key coordination
decisions within bdi team plans pomdp methods order demonstrate analysis methods concentrated role allocation fundamental aspect
agent teamwork provided three key contributions first introduced rmtdp
distributed pomdp framework analysis role allocation second article
presented rmtdp methodology optimizing key coordination decisions within
bdi team plan given domain concretely article described methodology
finding best role allocation fixed team plan given combinatorially many
role allocations introduced methods exploit task decompositions among sub teams
significantly prune search space role allocations
third hybrid bdi pomdp uncovered several synergistic interactions
bdi team plans distributed pomdps
tops useful constructing rmtdp model domain identifying
features need modeled well decomposing model construction
according structure top rmtdp model could used
evaluate top
tops restricted policy search providing rmtdps incomplete policies
limited number open decisions
bdi helped coming novel efficient belief representation policies suited hybrid bdi pomdp corresponding
evaluating policies resulted faster evaluation
compact policy representation
structure top exploited decompose evaluating
abstract policies resulting significant pruning search optimal role
allocations
constructed rmtdps two domains robocuprescue mission rehearsal
simulation determined best role allocation domains furthermore
illustrated significant speedups rmtdp policy search due techniques introduced
article detailed experiments revealed advantages state ofthe art role allocation approaches failed reason uncertainty
key agenda future work continue scale rmtdps even larger
scale agent teams scale require efficiency improvements propose
continue exploit interaction bdi pomdp approaches achieving
scale instance besides disaster rescue distributed sensor nets large area
monitoring applications could benefit scale


fihybrid bdi pomdp framework multiagent teaming

acknowledgments
supported nsf grant would thank jim blythe
anthony cassandra hyuckchul jung spiros kapetanakis sven koenig michael littman
stacy marsella david pynadath paul scerri discussions related article
would thank reviewers article whose comments helped
significantly improving article

appendix top details
section describe top helicopter scenario details
subplan figure b shown
executemission
context
pre conditions mb taskforce location taskforce start
achieved mb taskforce achieved doscouting achieved dotransport
time mb taskforce achieved remainingscouts
helo scoutingteam alive helo location helo end
unachievable mb taskforce unachievable doscouting
mb taskforce unachievable dotransport
achieved remainingscouts
helo scoutingteam alive helo location helo end
irrelevant
body
doscouting
dotransport
remainingscouts
constraints
doscouting dotransport
doscouting remainingscouts
doscouting
context executemission taskforce
pre conditions
achieved
unachievable
irrelevant
body
waitatbase
scoutroutes
constraints
waitatbase scoutroutes
waitatbase
context doscouting taskforce
pre conditions
achieved
unachievable mb transportteam helo transportteam alive helo


finair tambe

irrelevant
body
op



scoutroutes
context doscouting taskforce
achieved
unachievable
irrelevant mb scoutingteam helo transportteam alive helo
body
scoutroute
scoutroute
scoutroute
constraints
scoutroute scoutroute scoutroute
scoutroute
context scoutroutes scoutingteam
pre conditions
achieved mb sctteama helo sctteama location helo end
unachievable time mb sctteama helo sctteama alive helo
irrelevant
body
location sctteama start route sctteama
location sctteama end move forward
scoutroute
context scoutroutes scoutingteam
pre conditions
achieved mb sctteamb helo sctteamb location helo end
unachievable time mb sctteamb helo sctteamb alive helo
irrelevant
body
location sctteamb start route sctteamb
location sctteamb end move forward
scoutroute
context scoutroutes scoutingteam
pre conditions
achieved mb sctteama helo sctteama location helo end
unachievable time mb sctteama helo sctteama alive helo
irrelevant
body
location sctteama start route sctteama
location sctteama end move forward
dotransport
context executemission taskforce
pre conditions


fihybrid bdi pomdp framework multiagent teaming

achieved mb transportteam location transportteam end
unachievable time mb transportteam helo transportteam alive helo
irrelevant
body
location transportteam start
mb transportteam achieved scoutroute
route transportteam
elseif mb transportteam achieved scoutroute
route transportteam
elseif mb transportteam achieved scoutroute
route transportteam
route transportteam null location transportteam end
move forward
remainingscouts
context executemission taskforce
pre conditions
achieved mb scoutingteam location scoutingteam end
unachievable time mb scoutingteam helo scoutingteam
alive helo location helo end
irrelevant
body
location scoutingteam end move forward

predicate achieved tplan true achieved conditions tplan true similarly predicates unachievable tplan irrelevant tplan true unachievable conditions irrelevant conditions tplan true respectively predicate
location team end true members team end
figure b shows coordination relationships relationship indicated
solid arc relationship indicated dotted arc coordination relationships indicate unachievability achievability irrelevance conditions
enforced top infrastructure relationship team sub plans
means team sub plans fail parent team plan fail
parent team plan achieved child sub plans must achieved thus
doscouting waitatbase scoutroutes must done
achieved mb taskforce achieved waitatbase achieved scoutroutes
unachievable mb taskforce unachievable waitatbase
unachievable scoutroutes

relationship means subplans must fail parent fail success
subplans means parent plan succeeded thus scoutingroutes
least one scoutroute scoutroute scoutroute need performed
mb scoutingteam achieved scoutroute
achieved scoutroute achieved scoutroute
unachievable mb taskforce unachievable scoutroute
unachievable scoutroute unachievable scoutroute
achieved



finair tambe

relationship affects irrelevance conditions subplans joins
parent unachievable subplans still executing become irrelevant
thus waitatbase
irrelevant

mb taskforce unachievable scoutroutes

similarly scoutingroutes
irrelevant

mb taskforce unachievable scoutroutes


finally assign roles plans figure b shows assignment brackets adjacent plans instance task force team assigned jointly perform execute
mission

appendix b rmtdp details
section present details rmtdp constructed top figure
get features state attributes tested preconditions
achieved unachievable irrelevant conditions body team plans
individual agent plans thus relevant state variables location
helicopter role helicopter route helicopter status helicopter
alive time team n helicopters state given tuple
time role rolen loc locn route routen status statusn
consider actions primitive actions agent perform
within individual plans top infrastructure enforces mutual belief
communication actions since analyzing cost focus
consider communication implicit model effect
communication directly observation function
consider kinds actions role taking role execution actions assume
initial allocation specify roles agents specifies whether
agent scout transport scout scout team assigned
scout cannot become transport change team initial allocation
transport change role taking one role taking actions role taking
role execution actions agent given
membert ransportt eam joinsctt eama joinsctt eamb joinsctt eamc
membersctt eama membersctt eamb membersctt eamcx
membert ransportt eam chooseroute movef orward
membersctt eama membersctt eamb membersctt eamc movef orward
p obtain transition function help human expert
simulations simulator available domain helicopters crash shot
start end already scouted location probability
scouts get shot depends route e probability
crash route p probability crash route p probability crash
route p many scouts spot assume


fihybrid bdi pomdp framework multiagent teaming

probability transport shot unscouted location
scouted location probability multiple crashes obtained
multiplying probabilities individual crashes
action moveforward effect routei null loci end
statusi dead cases location agent gets incremented
assume role taking actions scoutroutex succeed role
performing agent transport assigned route already
transport start observe status agents
probability depending positions helicopter particular route
observe helicopters route completely cannot observe helicopters
routes
observation function gives probability group agents receive
particular joint observation domain assume observations one agent
independent observations agents given current state
previous joint action thus probability joint observation computed
multiplying probabilities individual agents observations
probability transport start observing status alive scout
route probability transport start observing nothing
alive scout since dont false negatives similarly scout
route crashes probability visible transport start
probability transport doesnt see failure similarly
probabilities observing alive scout route route
respectively probabilities observing crash route route
respectively
r reward function obtained help human expert helps
assign value states cost performing actions
analysis assume actions moveforward chooseroute cost
consider negative reward cost replacement action scoutroutex
r negative reward failure helicopter rf reward
scout reaching end rscout reward transport reaching end
rtransport e g r rf rscout rtransport
rl roles individual agents take top organization hierarchy
rl transport scoutonroute scoutonroute scoutonroute

appendix c theorems
theorem maxexp method yield upper bound
proof sketch
let policy leaf level policy highest expected reward particular parent node restricted policy space
v maxchildren v




finair tambe

since reward function specified separately component separate expected reward v rewards constituent components given
starting states starting observation histories components let
team plan divided components components parallel
independent sequentially executed
x
v
maxstates j ohistories j vj
jm

expected value obtained component j j cannot greater
highest value obtained j policy
maxstates j ohistories j vj maxchildren maxstates j ohistories j vj



hence
v

x

maxchildren maxstates j ohistories j vj

jm

v maxestimate





references
barber martin c dynamic reorganization decision making groups
proceedings fifth international conference autonomous agents agents
pp
becker r zilberstein lesser v goldman c v transition independent
decentralized markov decision processes proceedings second international
joint conference autonomous agents multi agent systems aamas pp

bernstein zilberstein immerman n complexity decentralized control mdps proceedings sixteenth conference uncertainty
artificial intelligence uai pp
boutilier c learning coordination multiagent decision processes
proceedings sixth conference theoretical aspects rationality knowledge tark pp
boutilier c reiter r soutchanski thrun decision theoretic highlevel agent programming situation calculus proceedings seventeenth
national conference artificial intelligence aaai pp
cassandra littman zhang n incremental pruning simple fast
exact method partially observable markov decision processes proceedings
thirteenth annual conference uncertainty artificial intelligence uai
pp


fihybrid bdi pomdp framework multiagent teaming

chades scherrer b charpillet f heuristic solving
decentralized pomdp assessment pursuit proceedings
acm symposium applied computing sac pp
cohen p r levesque h j teamwork nous
da silva j l demazeau vowels co ordination model proceedings
first international joint conference autonomous agents multiagent
systems aamas pp
dean lin h decomposition techniques stochastic domains proceedings fourteenth international joint conference artificial
intelligence ijcai pp
decker k lesser v quantitative modeling complex computational task
environments proceedings eleventh national conference artificial intelligence aaai pp
dix j muoz avila h nau zhang l impacting shop putting
ai planner multi agent environment annals mathematics artificial
intelligence
dunin keplicz b verbrugge r reconfiguration distributed
solving engineering simulation
erol k hendler j nau htn complexity expressivity
proceedings twelfth national conference artificial intelligence aaai
pp
fatima wooldridge adaptive task resource allocation multiagent systems proceedings fifth international conference autonomous
agents agents pp
georgeff p lansky l procedural knowledge proceedings ieee
special issue knowledge representation
goldman c v zilberstein optimizing information exchange cooperative
multi agent systems proceedings second international joint conference
autonomous agents multi agent systems aamas pp
grosz b hunsberger l kraus acting together ai magazine

grosz b kraus collaborative plans complex group action artificial
intelligence
guestrin c venkataraman koller context specific multiagent coordination factored mdps proceedings eighteenth national
conference artificial intelligence aaai pp
hansen e zhou r synthesis hierarchical finite state controllers pomdps
proceedings thirteenth international conference automated
scheduling icaps pp


finair tambe

hansen e bernstein zilberstein dynamic programming partially
observable stochastic games proceedings nineteenth national conference
artificial intelligence aaai pp
ho c team decision theory information structures proceedings
ieee
horling b benyo b lesser v self diagnosis adapt organizational
structures proceedings fifth international conference autonomous
agents agents pp
hunsberger l grosz b combinatorial auction collaborative
proceedings fourth international conference multiagent systems icmas pp
jennings n controlling cooperative solving industrial multi agent
systems joint intentions artificial intelligence
kaelbling l littman cassandra acting partially
observable stochastic domains artificial intelligence
kitano h tadokoro noda matsubara h takahashi shinjoh shimada
robocup rescue search rescue large scale disasters domain
multiagent proceedings ieee conference systems men
cybernetics smc pp
levesque h j cohen p r nunes j acting together proceedings
national conference artificial intelligence pp menlo park calif aaai
press
mailler r lesser v solving distributed constraint optimization cooperative mediation proceedings third international joint conference
agents multiagent systems aamas pp
marschak j radner r economic theory teams cowles foundation
yale university press ct
modi p j shen w tambe yokoo asynchronous complete
method distributed constraint optimization proceedings second international joint conference agents multiagent systems aamas pp

monahan g survey partially observable markov decision processes theory
management science
nair r ito tambe marsella task allocation rescue simulation
domain robocup robot soccer world cup v vol lecture notes
computer science pp springer verlag heidelberg germany
nair r pynadath yokoo tambe marsella taming decentralized
pomdps towards efficient policy computation multiagent settings proceedings
eighteenth international joint conference artificial intelligence ijcai
pp


fihybrid bdi pomdp framework multiagent teaming

nair r tambe marsella b team formation reformation multiagent domains robocuprescue kaminka g lima p roja r eds
proceedings robocup international symposium pp lecture notes
computer science springer verlag
nair r tambe marsella raines automated assistants analyze
team behavior journal autonomous agents multi agent systems

papadimitriou c tsitsiklis j complexity markov decision processes mathematics operations
peshkin l meuleau n kim k e kaelbling l learning cooperate via
policy search proceedings sixteenth conference uncertainty artificial
intelligence uai pp
poupart p boutilier c bounded finite state controllers proceedings
advances neural information processing systems nips
pynadath v tambe communicative multiagent team decision
analyzing teamwork theories journal artificial intelligence

pynadath v tambe automated teamwork among heterogeneous software agents humans journal autonomous agents multi agent systems
jaamas
rich c sidner c collagen agents collaborate people
proceedings first international conference autonomous agents agents pp
scerri p johnson l pynadath rosenbloom p si schurr n tambe
prototype infrastructure distributed robot agent person teams
proceedings second international joint conference agents multiagent
systems aamas pp
scerri p pynadath v tambe towards adjustable autonomy
real world journal artificial intelligence jair
schut c wooldridge parsons reasoning intentions uncertain domains proceedings sixth european conference symbolic
quantitative approaches reasoning uncertainty ecsqaru pp

shehory kraus methods task allocation via agent coalition formation
artificial intelligence
sondik e j optimal control partially observable markov processes ph
thesis stanford
stone p veloso task decomposition dynamic role assignment lowbandwidth communication real time strategic teamwork artificial intelligence



finair tambe

tambe towards flexible teamwork journal artificial intelligence

tambe pynadath chauvat n building dynamic agent organizations
cyberspace ieee internet computing
tidhar g team oriented programming preliminary report tech rep australian artificial intelligence institute
tidhar g b team oriented programming social structures tech rep australian artificial intelligence institute
tidhar g rao sonenberg e guided team selection proceedings
second international conference multi agent systems icmas pp
wooldridge introduction multiagent systems john wiley sons
xuan p lesser v multi agent policies centralized ones decentralized ones proceedings first international joint conference agents
multiagent systems aamas pp
xuan p lesser v zilberstein communication decisions multiagent
cooperation proceedings fifth international conference autonomous
agents agents pp
yen j yin j ioerger r miller xu volz r cast collaborative agents simulating teamwork proceedings seventeenth international
joint conference artificial intelligence ijcai pp
yoshikawa decomposition dynamic team decision ieee transactions automatic control ac




