Journal Artificial Intelligence Research 10 (1999) 271-289

Submitted 11/98; published 5/99

Issues Stacked Generalization
Kai Ming Ting

kmting@deakin.edu.au

Ian H. Witten

ihw@cs.waikato.ac.nz

School Computing Mathematics
Deakin University, Australia.
Department Computer Science
University Waikato, New Zealand.

Abstract

Stacked generalization general method using high-level model combine lowerlevel models achieve greater predictive accuracy. paper address two crucial
issues considered `black art' classification tasks ever since
introduction stacked generalization 1992 Wolpert: type generalizer
suitable derive higher-level model, kind attributes used
input. find best results obtained higher-level model combines
confidence (and predictions) lower-level ones.
demonstrate effectiveness stacked generalization combining three different
types learning algorithms classification tasks. compare performance
stacked generalization majority vote published results arcing bagging.

1. Introduction
Stacked generalization way combining multiple models learned
classification task (Wolpert, 1992), used regression (Breiman, 1996a)
even unsupervised learning (Smyth & Wolpert, 1997). Typically, different learning
algorithms learn different models task hand, common form
stacking first step collect output model new set data.
instance original training set, data set represents every model's prediction
instance's class, along true classification. step, care taken ensure
models formed batch training data include instance
question, way ordinary cross-validation. new data treated
data another learning problem, second step learning algorithm
employed solve problem. Wolpert's terminology, original data models
constructed first step referred level-0 data level-0 models,
respectively, set cross-validated data second-stage learning algorithm
referred level-1 data level-1 generalizer.
paper, show make stacked generalization work classification tasks
addressing two crucial issues Wolpert (1992) originally described `black art'
resolved since. two issues (i) type attributes
used form level-1 data, (ii) type level-1 generalizer order get improved
accuracy using stacked generalization method.
Breiman (1996a) demonstrated success stacked generalization setting
ordinary regression. level-0 models regression trees different sizes linear
c 1999 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiTing & Witten

regressions using different number variables. instead selecting single model
works best judged (for example) cross-validation, Breiman used different level0 regressors' output values member training set form level-1 data.
used least-squares linear regression, constraint regression coecients
non-negative, level-1 generalizer. non-negativity constraint turned
crucial guarantee predictive accuracy would better achieved
selecting single best predictor.
show stacked generalization made work reliably classification
tasks. using output class probabilities generated level-0 models
form level-1 data. level-1 generalizer use version least squares linear
regression adapted classification tasks. find use class probabilities crucial
successful application stacked generalization classification tasks. However,
non-negativity constraints found necessary Breiman regression found
irrelevant improved predictive accuracy classification situation.
Section 2, formally introduce technique stacked generalization describe
pertinent details learning algorithm used experiments. Section 3 describes
results stacking three different types learning algorithms. Section 4 compares
stacked generalization arcing bagging, two recent methods employ sampling
techniques modify data distribution order produce multiple models single
learning algorithm. following section describes related work, paper ends
summary conclusions.

2. Stacked Generalization

Given data set L = f(y ; x ); n = 1; : : : ; N g, class value x vector
representing attribute values nth instance, randomly split data J almost
equal parts L1 ; : : : ; L . Define L L(, ) = L , L test training sets
j th fold J -fold cross-validation. Given K learning algorithms, call level-0
generalizers, invoke kth algorithm data training set L(, ) induce
model M(, ) , k = 1; : : : ; K . called level-0 models.
instance x L , test set j th cross-validation fold, let z denote
prediction model M(, ) x . end entire cross-validation process,
data set assembled outputs K models
n

n

n

J

j

j

n

j

j

j

k

n

j

kn

j

n

k

L = f(y ; z1 ; : : : ; z ); n = 1; : : : ; N g:
CV

n

n

Kn

level-1 data. Use learning algorithm call level-1 generalizer
~ function (z1 ; : : : ; z ). level-1
derive data model
model. Figure 1 illustrates cross-validation process. complete training process,
final level-0 models , k = 1; : : : ; K , derived using data L.
let us consider classification process, uses models , k = 1; : : : ; K ,
~ . Given new instance, models produce vector (z1 ; : : : ; z ).
conjunction
~ , whose output final classification result
vector input level-1 model
instance. completes stacked generalization method proposed Wolpert
(1992), used Breiman (1996a) LeBlanc & Tibshirani (1993).
K

k

k

k

272

K

fiIssues Stacked Generalization

~


L CV
Level-1
Level-0

(-j)

(-j)

M1

(-j)

Mk

MK

(-j)

L

Figure 1: figure illustrates j -fold cross-validation process level-0; level-1
~.
data set L end process used produce level-1 model
CV

~ ,
well situation described above, results level-1 model
present paper considers situation output level-0 models
set class probabilities rather single class prediction. model M(, ) used
classify instance x L , let P (x) denote probability ith output class,
vector
P = (P 1 (x ); : : : ; P (x ); : : : ; P (x ))
gives model's class probabilities nth instance, assuming classes.
level-1 data, assemble together class probability vector K models, along
actual class:
L0 = f(y ; P1 ; : : : ; P ; : : : ; P ); n = 1; : : : ; N g:
~ 0 contrast
~.
Denote level-1 model derived
following two subsections describe algorithms used level-0 level-1 generalizers experiments reported Section 3.
j

k

j

ki

kn

CV

n

k

n

n

ki

kn

n

kI

n

Kn

2.1 Level-0 Generalizers

Three learning algorithms used level-0 generalizers: C4.5, decision tree learning
algorithm (Quinlan, 1993); NB, re-implementation Naive Bayesian classifier (Cestnik,
1990); IB1, variant lazy learning algorithm (Aha, Kibler & Albert, 1991)
employs p-nearest-neighbor method using modified value-difference metric nominal
binary attributes (Cost & Salzberg, 1993). learning algorithms
show formula use
P estimated output class probabilities P (x)
instance x (where, cases, P (x) = 1).
C4.5: Consider leaf decision tree instance x falls. Let
number (training) instances class leaf, suppose majority class








273

fiTing & Witten

P
leaf I^. Let E = 6= ^ . Then, using Laplace estimator,






P ^(x) = 1 , PEm+ +1 2 ;






P (x) = (1 , P ^(x)) ; 6= I^:






Note pruned trees default settings C4.5 used experiments.
NB: Let P (ijx) posterior probability class i, given instance x.
P (x) = PP P(ij(xij)x) :




Note NB uses Laplacian estimate estimating conditional probabilities
nominal attribute compute P (ijx). continuous-valued attribute,
normal distribution assumed case conditional probabilities
conveniently represented entirely terms mean variance observed
values class.
IB1: Suppose p nearest neighbors used; denote f(y ; x ); = 1; : : : ; pg
instance x. (We use p = 3 experiments.)




P f (y )=d(x; x )
P (x) = P=1 1=d(x; x ) ;
p









p

=1





f (y ) = 1 = 0 otherwise, Euclidean distance function.




three learning algorithms, predicted class level-0 model, given instance
x, I^
P ^(x) > P (x) 6= I^:




2.2 Level-1 Generalizers

compare effect four different learning algorithms level-1 generalizer: C4.5,
IB1(using p = 21 nearest neighbors),1 NB, multi-response linear regression algorithm,
MLR. last needs explanation.
MLR adaptation least-squares linear regression algorithm Breiman (1996a)
used regression settings. classification problem real-valued attributes
transformed multi-response regression problem. original classification problem
classes, converted separate regression problems, problem
class ` instances responses equal one class ` zero otherwise.
input MLR level-1 data, need consider situation model
0
~
, attributes probabilities, separately model M~ ,
1. large value used following Wolpert's (1992) advice \ reasonable `relatively global,
smooth ' level-1 generalizers perform well."
p

:::

:::

274

fiIssues Stacked Generalization

classes. former case, attributes already real-valued, linear
regression class ` simply

X
LR (x) =
K

`

k`

P (x):
k`

k

latter case, classes unordered nominal attributes. map binary
values obvious way, setting P (x) 1 class instance x ` zero otherwise;
use linear regression.
Choose linear regression coecients fff g minimize
k`

X X
j

(

yn ;xn

)2Lj

k`

(y ,
n

Xff

P (, )(x ))2 :
j

k`

k`

n

k

coecients fff g constrained non-negative, following Breiman's (1996a) discovery necessary successful application stacked generalization regression problems. non-negative-coecient least-squares algorithm described Lawson
& Hanson (1995) employed derive linear regression class. show
later that, fact, non-negative constraint unnecessary classification tasks.
place, describe working MLR. classify new instance
x, compute LR (x) classes assign instance class `
greatest value:2
LR (x) > LR (x) `0 6= `:
next section investigate stacking C4.5, NB IB1.
k`

`

`

`0

3. Stacking C4.5, NB IB1

3.1 Stacked Generalization Work?

experiments section show
successful stacked generalization necessary use output class prob~ 0 rather
~;
abilities rather class predictions|that is,
MLR algorithm suitable level-1 generalizer, among four
algorithms used.
use two artificial datasets eight real-world datasets UCI Repository
machine learning databases (Blake, Keogh & Merz, 1998). Details given
Table 1.
artificial datasets|Led24 Waveform|each training dataset L size 200
300, respectively, generated using different seed. algorithms used
experiments tested separate dataset 5000 instances. Results expressed
average error rate ten repetitions entire procedure.
real-world datasets, W -fold cross-validation performed. fold
cross-validation, training dataset used L, models derived evaluated
2. pattern recognition community calls type classifier linear machine (Duda & Hart, 1973).

275

fiTing & Witten

Datasets # Samples # Classes # Attr & Type
Led24
200/5000
10
10N
Waveform 300/5000
3
40C
Horse
368
2
3B+12N+7C
Credit
690
2
4B+5N+6C
Vowel
990
11
10C
Euthyroid
3163
2
18B+7C
Splice
3177
3
60N
Abalone
4177
3
1N+7C
Nettalk(s)
5438
5
7N
Coding
20000
2
15N

N-nominal; B-binary; C-Continuous.

Table 1: Details datasets used experiment.
test dataset. result expressed average error rate W -fold crossvalidation. Note cross-validation used evaluation entire procedure,
whereas J -fold cross-validation mentioned Section 2 internal operation
stacked generalization. However, W J set 10 experiments.
~
section, present results model combination using level-1 models
0
~
, well model selection method, employing J -fold cross-validation procedure. Note difference model combination model selection
whether level-1 learning employed not.
Table 2 shows average error rates, obtained using W -fold cross-validation, C4.5,
NB IB1, BestCV, best three, selected using J -fold crossvalidation. expected, BestCV almost always classifier lowest error rate.3
~ ,
Table 3 shows result stacked generalization using level-1 model
~ 0 ,
level-1 data comprise classifications generated level-0 models,
level-1 data comprise probabilities generated level-0 models. Results
shown four level-1 generalizers case, along BestCV. lowest error
rate dataset given bold.
Table 4 summarizes results Table 3 terms comparison level-1
~ 0 derived
model BestCV totaled datasets. Clearly, best level-1 model
using MLR. performs better BestCV nine datasets equally well tenth.
~ derived NB, performs better BestCV seven
best performing
datasets significantly worse two (Waveform Vowel). regard difference
two standard errors significant (95% confidence). standard error figures
omitted table increase readability.
datasets shown order increasing size. MLR performs significantly
better BestCV four largest datasets. indicates stacked generalization
likely give significant improvements predictive accuracy volume data
large|a direct consequence accurate estimation using cross-validation.
3. Note BestCV always select classifier folds. error rate
always equal lowest error rate among three classifiers.
W

276

fiIssues Stacked Generalization

Datasets

Level-0 Generalizers
C4.5 NB
IB1
Led24
35.4 35.4
32.2
Waveform 31.8 17.1
26.2
Horse
15.8 17.9
15.8
Credit
17.4 17.3
28.1
Vowel
22.7 51.0
2.6
Euthyroid 1.9 9.8
8.6
Splice
5.5 4.5
4.7
Abalone
41.4 42.1
40.5
Nettalk(s) 17.0 15.9
12.7
Coding
27.6 28.8
25.0

BestCV
32.8 0.6
17.1 0.3
17.1 1.6
17.4 1.2
2.6 0.2
1.9 0.3
4.5 0.4
40.1 0.6
12.7 0.4
25.0 0.3

Table 2: Average error rates C4.5, NB IB1, BestCV|the best among
selected using J -fold cross-validation. standard errors shown last
column.
Datasets

~
Level-1 model,
C4.5 NB IB1 MLR
34.0 32.4 35.0 33.3
17.7 19.2 18.7 17.2
16.9 14.9 17.6 16.3
18.4 16.1 16.9 17.4
2.6 3.8 3.6
2.6

BestCV
Led24
32.8
Waveform
17.1
Horse
17.1
Credit
17.4
Vowel
2.6
Euthyroid
1.9 1.9 1.9 1.9
Splice
4.5 3.9 3.9 3.8
Abalone
40.1 38.5 38.5 38.2
Nettalk(s)
12.7 12.4 11.9 12.4
Coding
25.0 23.2 23.1 23.2

1.9
3.8

38.1
12.6
23.2

~0
Level-1 model,
C4.5 NB IB1 MLR
41.7 35.7 32.1 31.3
20.6 17.6 17.8 16.8
18.0 18.5 17.7 15.2
15.4 15.9 14.3 16.2
2.7 7.2 3.3 2.5
2.2 4.3 2.0 1.9
4.0 3.9 3.8 3.8
43.3 37.1 39.2 38.3
14.0 14.6 12.0 11.5
22.3 21.2 21.2 20.7

Table 3: Average error rates stacking C4.5, NB IB1.
~
~0
Level-1 model,
Level-1 model,
C4.5 NB IB1 MLR C4.5 NB IB1 MLR
#Win vs. #Loss 3-5 2-7 4-5 2-5 7-3 6-4 4-6 0-9
~
~ 0.
Table 4: Summary Table 3|Comparison BestCV

277

fiTing & Witten

one level-0 models performs significantly much better rest,
Euthyroid Vowel datasets, MLR performs either good BestCV selecting
best performing level-0 model, better BestCV.
MLR advantage three level-1 generalizers model
easily interpreted. Examples combination weights derives (for probability~ 0 ) appear Table 5 Horse, Credit, Splice, Abalone, Waveform, Led24
based model
Vowel datasets. weights indicate relative importance level-0 generalizers
prediction class. example, Splice dataset (in Table 5(b)), NB
dominant generalizer predicting class 2, NB IB1 good predicting class
3, three generalizers make worthwhile contribution prediction class 1.
contrast, Abalone dataset three generalizers contribute substantially
prediction three classes. Note weights class sum one
constraint imposed MLR.

3.2 Non-negativity Constraints Necessary?
Breiman (1996a) LeBlanc & Tibshirani (1993) use stacked generalization
method regression setting report necessary constrain regression
coecients non-negative order guarantee stacked regression improves predictive accuracy. investigate finding domain classification tasks.
assess effect non-negativity constraint performance, three versions
~ 0:
MLR employed derive level-1 model
i. linear regression MLR calculated intercept constant (that is,
+ 1 weights classes) without constraints;
ii. linear regression derived neither intercept constant (I weights
classes) constraints;
iii. linear regression derived without intercept constant, nonnegativity constraints (I non-negative weights classes).
third version one used results presented earlier. Table 6 shows
results three versions. almost indistinguishable error rates. conclude
classification tasks, non-negativity constraints necessary guarantee
stacked generalization improves predictive accuracy.
However, another reason good idea employ non-negativity constraints. Table 7 shows example weights derived three versions MLR
Led24 dataset. third version, shown column (iii), supports perspicuous
interpretation level-0 generalizer's contribution class predictions
two. dataset, IB1 dominant generalizer predicting classes 4, 5 8,
NB IB1 make worthwhile contribution predicting class 2, evidenced
high weights. However, negative weights used predicting classes render
interpretation two versions much less clear.
278

fiIssues Stacked Generalization

Horse
Credit
Class C4.5 NB IB1 C4.5 NB IB1
1
0.36 0.20 0.42 0.63 0.30 0.04
2
0.39 0.19 0.41 0.65 0.28 0.07
C4.5 ff1 ; NB ff2 ; IB1 ff3 .
~ 0 ) Horse Credit datasets.
Table 5: (a) Weights generated MLR (model
Class
1
2
3

C4.5
0.23
0.15
0.08

Splice
NB
0.43
0.72
0.52

IB1
0.36
0.12
0.40

Abalone
C4.5 NB IB1
0.25 0.25 0.39
0.27 0.20 0.25
0.30 0.18 0.39

Waveform
C4.5 NB IB1
0.16 0.59 0.34
0.14 0.72 0.07
0.04 0.65 0.23

~ 0 ) Splice, Abalone Waveform
Table 5: (b) Weights generated MLR (model
datasets.
Vowel
C4.5 NB IB1
0.04 0.00 0.96
0.03 0.00 0.97
0.01 0.00 1.00
0.05 0.25 0.86
0.01 0.08 0.97
0.15 0.00 0.92
0.03 0.01 1.02
0.04 0.01 0.96
0.03 0.00 1.02
0.08 0.01 0.93
0.00 0.04 0.96
~ 0 ) Led24 Vowel datasets.
Table 5: (c) Weights generated MLR (model
Class
1
2
3
4
5
6
7
8
9
10
11

C4.5
0.46
0.00
0.47
0.00
0.00
0.35
0.15
0.00
0.00
0.00
{

Led24
NB
0.65
0.37
0.00
0.13
0.19
0.14
0.43
0.00
0.38
0.50
{

IB1
0.00
0.43
0.54
0.65
0.64
0.35
0.36
0.68
0.29
0.24
{

279

fiTing & Witten

Datasets

MLR
Constraints Intercept Non-Negativity
Led24
31.4
31.4
31.3
Waveform
16.8
16.8
16.8
Horse
15.8
15.8
15.2
Credit
16.2
16.2
16.2
Vowel
2.4
2.4
2.5
Euthyroid
1.9
1.9
1.9
Splice
3.7
3.8
3.8
Abalone
38.3
38.3
38.3
Nettalk(s)
11.6
11.5
11.5
Coding
20.7
20.7
20.7
Table 6: Average error rates three versions MLR.
Class
1
2
3
4
5
6
7
8
9
10

ff0

0.00
0.02
0.00
0.04
0.03
0.01
0.01
0.02
0.04
0.04

ff1

(i)

ff2

ff3

ff1

(ii)

ff2

ff3

ff1

0.45 0.65 0.00 0.46 0.65 0.00 0.46
{0.42 0.47 0.56 {0.40 0.49 0.56 0.00
0.46 {0.01 0.54 0.47 {0.01 0.54 0.47
{0.33 0.15 0.84 {0.29 0.21 0.81 0.00
{0.37 0.26 0.84 {0.32 0.26 0.84 0.00
0.35 0.12 0.35 0.36 0.14 0.35 0.35
0.15 0.43 0.36 0.15 0.43 0.36 0.15
{0.05 {0.25 0.72 {0.03 {0.19 0.72 0.00
{0.08 0.32 0.32 {0.05 0.40 0.30 0.00
{0.06 0.43 0.25 {0.01 0.50 0.24 0.00

(iii)

ff2

0.65
0.37
0.00
0.13
0.19
0.14
0.43
0.00
0.38
0.50

ff3

0.00
0.43
0.54
0.65
0.64
0.35
0.36
0.68
0.29
0.24

Table 7: Weights generated three versions MLR: (i) constraints, (ii) intercept,
(iii) non-negativity constraints, LED24 dataset.

280

fiIssues Stacked Generalization

Dataset
#SE BestCV Majority MLR
Horse
0.5
17.1
15.0 15.2
Splice
2.5
4.5
4.0 3.8
Abalone
3.3
40.1
39.0 38.3
Led24
8.7
32.8
31.8 31.3
Credit
8.9
17.4
16.1 16.2
Nettalk(s) 10.8
12.7
12.2 11.5
Coding
12.7
25.0
23.1 20.7
Waveform 18.7
17.1
19.5 16.8
Euthyroid 26.3
1.9
8.1 1.9
Vowel
242.0
2.6
13.0 2.5
~ 0 ), along
Table 8: Average error rates BestCV, Majority Vote MLR (model
number standard error (#SE) BestCV worst performing
level-0 generalizers.

3.3 Stacked Generalization Compare Majority Vote?
~ 0 , derived MLR, majority vote,
Let us compare error rate

simple decision combination method requires neither cross-validation level1 learning. Table 8 shows average error rates BestCV, majority vote MLR.
order see whether relative performances level-0 generalizers effect
methods, number standard errors (#SE) error rates
worst performing level-0 generalizer BestCV given, datasets re-ordered
according measure. Since BestCV almost always selects best performing level-0
generalizer, small values #SE indicate level-0 generalizers perform comparably
one another, vice versa.
MLR compares favorably majority vote, eight wins versus two losses.
eight wins, six significant differences (the two exceptions Splice
Led24 datasets); whereas losses (for Horse Credit datasets) insignificant
differences. Thus extra computation cross-validation level-1 learning seems
paid off.
interesting note performance majority vote related size
#SE. Majority vote compares favorably BestCV first seven datasets,
values #SE small. last three, #SE large, majority vote performs
worse. indicates level-0 generalizers perform comparably, worth
using cross-validation determine best one, result majority vote|which
far cheaper|is significantly different. Although small values #SE necessary
condition majority vote rival BestCV, sucient condition|see Matan
(1996) example. applies majority vote compared MLR. MLR
performs significantly better five datasets large #SE values,
one cases.
281

fiTing & Witten

M~ versus M~ 0

C4.5 NB IB1 MLR
#Win vs. #Loss 8-2 5-4 3-6 1-7
~ versus
~ 0 generalizer|summarized results Table 3.
Table 9:
worth mentioning method averages P (x) level-0 models,
yielding P (x), predicts class I^ P^(x) > P (x) 6= I^: According
Breiman (1996b), method produces error rate almost identical majority
vote.








3.4 Stacked Generalization Work Best M~ 0 Generated
MLR?
shown stacked generalization works best output class probabilities
(rather class predictions) used MLR algorithm (rather C4.5, IB1,
NB). retrospect, surprising, explained intuitively follows.
level-1 model provide simple way combining evidence available.
evidence includes predictions, confidence level-0 model
predictions. linear combination simplest way pooling level-0 models'
confidence, MLR provides that.
alternative methods NB, C4.5, IB1 shortcomings. Bayesian approach could form basis suitable alternative way pooling level-0 models' confidence, independence assumption central Naive Bayes hampers performance
datasets evidence provided individual level-0 models certainly
independent. C4.5 builds trees model interaction amongst attributes|particularly
tree large|but desirable combining confidences. Nearest neighbor methods really give way combining confidences; also, similarity metric
employed could misleadingly assume two different sets confidence levels similar.
~
~ 0 level-1
Table 9 summarizes results Table 3 comparing
generalizer, across datasets. C4.5 clearly better label-based representation,
discretizing continuous-valued attributes creates intra-attribute interaction addition interactions different attributes. evidence Table 9 NB
indifferent use labels confidences: normal distribution assumption
embodies latter case could another reason unsuitable combining
confidence measures. MLR IB1 handle continuous-valued attributes better
label-based ones, since domain designed work.
Summary

summarize findings section follows.

None four learning algorithms used obtain model M~ perform satisfactorily.
282

fiIssues Stacked Generalization

MLR best four learning algorithms use level-1 generalizer
~ 0.
obtaining model
obtained using MLR, M~ 0 lower predictive error rate best model
selected J -fold cross-validation, almost datasets used experiments.

Another advantage MLR three level-1 generalizers interpretability.
weights indicate different contributions level-0 model k makes
prediction classes `.
k`

Model M~ 0 derived MLR without non-negativity constraints.
constraints make little difference model's predictive accuracy.

use non-negativity constraints MLR advantage interpretability. Non-

negative weights support easier interpretation extent model
contributes prediction class.
k`

derived using MLR, model M~ 0 compares favorably majority vote.
MLR provides method combining confidence generated level-0 models
final decision. various reasons, NB, C4.5, IB1 suitable task.

4. Comparison Arcing Bagging
section compares results stacking C4.5, NB IB1 results arcing
(called boosting originator, Schapire, 1990) bagging reported Breiman
(1996b; 1996c). arcing bagging employ sampling techniques modify data
distribution order produce multiple models single learning algorithm.
combine decisions individual models, arcing uses weighted majority vote
bagging uses unweighted majority vote. Breiman reports arcing bagging
substantially improve predictive accuracy single model derived using base
learning algorithm.

4.1 Experimental Results

First describe differences experimental procedures. results
stacking averaged ten-fold cross-validation datasets except Waveform,
averaged ten repeated trials. Standard errors shown. Results arcing
bagging obtained Breiman (1996b; 1996c), averaged 100 trials.
Breiman's experiments, trial uses random 9:1 split form training test
sets datasets except Waveform. note Waveform dataset used 19
irrelevant attributes, Breiman used version without irrelevant attributes (which would
expected degrade performance level-0 generalizers experiments).
cases 300 training instances used dataset, used 5000 test instances
whereas Breiman used 1800. Arcing bagging done 50 decision tree models
derived CART (Breiman et al., 1984) trial.
283

fiTing & Witten

Dataset
#Samples stacking arcing bagging
Waveform
300
16.8 0.2 17.8
19.3
Glass
214
28.4 2.9 22.0
23.2
Ionosphere
351
9.7 1.5
6.4
7.9
Soybean
683
4.3 1.1
5.8
6.8
Breast Cancer
699
2.7 0.8
3.2
3.7
Diabetes
768
24.2 1.2 26.6
23.9
Table 10: Comparing stacking arcing bagging classifiers.
results six datasets given Table 10, indicate three methods
competitive.4 Stacking performs better arcing bagging three
datasets (Waveform, Soybean Breast Cancer), better arcing worse
bagging Diabetes dataset. Note stacking performs poorly Glass
Ionosphere, two small real-world datasets. surprising, cross-validation
inevitably produces poor estimates small datasets.

4.2 Discussion

bagging, stacking ideal parallel computation. construction level-0
model proceeds independently, communication modeling processes
necessary.
Arcing bagging require considerable number member models
rely varying data distribution get diverse set models single learning
algorithm. Using level-1 generalizer, stacking work two three level-0
models.
Suppose computation time required learning algorithm C , arcing
bagging needs h models. learning time required = hC . Suppose stacking requires
g models model employs J -fold cross-validation. Assuming time C needed
derive g level-0 models level-1 model, learning time stacking
= (g(J + 1) + 1)C . results given Table 10, h = 50, J = 10, g = 3; thus
= 50C = 34C . However, practice learning time required level-0
level-1 generalizers may different.
Users stacking free choice level-0 models. may either derived
single learning algorithm, variety different algorithms. example Section
3 uses different types learning algorithms, bag-stacking|stacking bagged models
(Ting & Witten, 1997)|uses data variation obtain diverse set models single
learning algorithm. former case, performance may vary substantially
level-0 models|for example NB performs poorly Vowel Euthyroid datasets
compared two models (see Table 2). Stacking copes well situation.
performance variation among member models bagging rather small
derived learning algorithm using bootstrap samples. Section 3.3








4. heart dataset used Breiman (1996b; 1996c) omitted much modified
original one.

284

fiIssues Stacked Generalization

shows small performance variation among member models necessary condition
majority vote (as employed bagging) work well.
worth noting arcing bagging incorporated framework
stacked generalization using arced bagged models level-0 models. Ting & Witten
(1997) show one possible way incorporating bagged models level-1 learning, employing MLR instead voting. implementation, L used test set
bagged models derive level-1 data rather cross-validated data.
viable bootstrap sample leaves 37% examples. Ting & Witten
(1997) show bag-stacking almost always higher predictive accuracy bagging
models derived either C4.5 NB. Note difference whether
adaptive level-1 model simple majority vote employed
According Breiman (1996b; 1996c), arcing bagging improve predictive accuracy learning algorithms `unstable.'5 unstable learning algorithm
one small perturbations training set produce large changes
derived model. Decision trees neural networks unstable; NB IB1 stable.
Stacking works both.
MLR successful candidate level-1 learning found,
algorithms might work equally well. One candidate neural networks. However,
experimented back-propagation neural networks purpose found
much slower learning rate MLR. example, MLR took 2.9
seconds compare 4790 seconds neural network nettalk dataset;
error rate. possible candidates multinomial logit model
(Jordan & Jacobs, 1994), special case generalized linear models (McCullagh
& Nelder, 1983), supra Bayesian procedure (Jacobs, 1995) treats level-0
models' confidence data may combined prior distribution level-0 models
via Bayes' rule.

5. Related Work

analysis stacked generalization motivated Breiman (1996a), discussed
earlier, LeBlanc & Tibshirani (1993). LeBlanc & Tibshirani (1993) examine stacking
linear discriminant nearest neighbor classifier show that, one artificial
dataset, method similar MLR performs better non-negativity constraints
without. results Section 3.2 show constraints irrelevant MLR's
predictive accuracy classification situation.
LeBlanc & Tibshirani (1993) Ting & Witten (1997) use version MLR
employs class probabilities level-0 model induce linear regression.
case, linear regression class `

LR (x) =
`

XXff
K



k



ki`

P (x):
ki

implementation requires fitting KI parameters, compared K parameters
version used paper (see corresponding formula Section 2.2).
5. Schapire, R.E., Y. Freund, P. Bartlett, & W.S. Lee (1997) provide alternative explanation
effectiveness arcing bagging.

285

fiTing & Witten

versions give comparable results terms predictive accuracy, version used
paper runs considerably faster needs fit fewer parameters.
limitations MLR well-known (Duda & Hart, 1973). -class problem,
divides description space convex decision regions. Every region must singly
connected, decision boundaries linear hyperplanes. means MLR
suitable problems unimodal probability densities. Despite limitations,
MLR still performs better level-1 generalizer IB1, nearest competitor deriving
M~ 0. limitations may hold key fuller understanding behavior stacked
generalization. Jacobs (1995) reviews linear combination methods used MLR.
Previous work stacked generalization, especially applied classification tasks,
limited several ways. applies particular dataset (e.g., Zhang,
Mesirov & Waltz, 1992). Others report results less convincing (Merz, 1995).
Still others different focus evaluate results datasets (LeBlanc
& Tibshirani, 1993; Chan & Stolfo, 1995; Kim & Bartlett, 1995; Fan et al., 1996).
One might consider degenerate form stacked generalization use crossvalidation produce data level-1 learning. Then, level-1 learning done `on
y' training process (Jacobs et al., 1991). another approach, level-1 learning
takes place batch mode, level-0 models derived (Ho et al., 1994).
Several researchers worked still degenerate form stacked generalization
without cross-validation learning level 1. Examples neural network ensembles
(Hansen & Salamon, 1990; Perrone & Cooper, 1993; Krogh & Vedelsby, 1995), multiple
decision tree combination (Kwok & Carter, 1990; Buntine, 1991; Oliver & Hand, 1995),
multiple rule combination (Kononenko & Kovacic, 1992). methods used level 1
majority voting, weighted averaging Bayesian combination. possible methods
distribution summation likelihood combination. various forms re-ordering
class rank, Ali & Pazzani (1996) study methods rule learner. Ting
(1996) uses confidence prediction combine nearest neighbor classifier
Naive Bayesian classifier.

6. Conclusions
addressed two crucial issues successful implementation stacked generalization classification tasks. First, class probabilities used instead single
predicted class input attributes higher-level learning. class probabilities serve
confidence measure prediction made. Second, multi-response least squares
linear regression technique employed high-level generalizer. technique
provides method combining level-0 models' confidence. three learning algorithms either algorithmic limitations suitable aggregating confidences.
combining three different types learning algorithms, implementation
stacked generalization found achieve better predictive accuracy model
selection based cross-validation majority vote; found competitive arcing bagging. Unlike stacked regression, non-negativity constraints
least-squares regression necessary guarantee improved predictive accuracy
classification tasks. However, constraints still preferred increase
interpretability level-1 model.
286

fiIssues Stacked Generalization

implication successful implementation stacked generalization earlier
model combination methods employing (weighted) majority vote, averaging, computations make use level-1 learning, apply learning improve
predictive accuracy.

Acknowledgment

authors grateful New Zealand Marsden Fund financial support
research. work conducted first author Department Computer
Science, University Waikato. authors grateful J. Ross Quinlan providing
C4.5 David W. Aha providing IB1. anonymous reviewers editor
provided many helpful comments.

References

Aha, D.W., D. Kibler & M.K. Albert (1991). Instance-Based Learning Algorithms. Machine Learning, 6, pp. 37-66.
Ali, K.M. & M.J. Pazzani (1996). Error Reduction Learning Multiple Descriptions. Machine Learning, Vol. 24, No. 3, pp. 173-206.
Blake, C., E. Keogh & C.J. Merz (1998). UCI Repository machine learning databases
[http:// www.ics.uci.edu/ mlearn/MLRepository.html]. Irvine, CA: University California, Department Information Computer Science.
Breiman, L. (1996a). Stacked Regressions. Machine Learning, Vol. 24, pp. 49-64.
Breiman, L. (1996b). Bagging Predictors. Machine Learning, Vol. 24, No. 2, pp. 123-140.
Breiman, L. (1996c). Bias, Variance, Arcing Classifiers. Technical Report 460. Department Statistics, University California, Berkeley, CA.
Breiman, L., J.H. Friedman, R.A. Olshen & C.J. Stone (1984). Classification Regression Trees. Belmont, CA: Wadsworth.
Cestnik, B. (1990). Estimating Probabilities: Crucial Task Machine Learning.
Proceedings European Conference Artificial Intelligence, pp. 147-149.
Chan, P.K. & S.J. Stolfo (1995). Comparative Evaluation Voting Meta-learning
Partitioned Data. Proceedings Twelfth International Conference Machine Learning, pp. 90-98, Morgan Kaufmann.
Cost, & S. Salzberg (1993). Weighted Nearest Neighbor Algorithm Learning
Symbolic Features. Machine Learning, 10, pp. 57-78.
Fan, D.W., P.K. Chan, S.J. Stolfo (1996). Comparative Evaluation Combiner
Stacked Generalization. Proceedings AAAI-96 workshop Integrating Multiple
Learned Models, pp. 40-46.
Hansen, L.K. & P. Salamon (1990). Neural Network Ensembles. IEEE Transactions
Pattern Analysis Machine Intelligence, 12, pp. 993-1001.
287

fiTing & Witten

Ho, T.K., J.J. Hull & S.N. Srihari (1994). Decision Combination Multiple Classifier
Systems. IEEE Transactions Pattern Analysis Machine Intelligence, Vol. 16,
No. 1, pp. 66-75.
Jacobs, R.A. (1995). Methods Combining Experts' Probability Assessments. Neural
Computation 7, pp. 867-888, MIT Press.
Jacobs, R.A., M.I. Jordan, S.J. Nowlan & G.E. Hinton (1991). Adaptive Mixtures Local
Experts. Neural Computation 3, pp. 79-87.
Jacobs, R.A. & M.I. Jordan (1994). Hierachical Mixtures Experts EM Algorithms. Neural Computation 6, pp. 181-214.
Kim, K. & E.B. Bartlett (1995). Error Estimation Series Association Neural Network
Systems. Neural Computation 7, pp. 799-808, MIT Press.
Kononenko, I. & M. Kovacic (1992). Learning Optimization: Stochastic Generation
Multiple Knowledge. Proceedings Ninth International Conference
Machine Learning, pp. 257-262, Morgan Kaufmann.
Krogh, A. & J. Vedelsby (1995). Neural Network Ensembles, Cross Validation, Active
Learning. Advances Neural Information Processing Systems 7, G. Tesauro, D.S.
Touretsky & T.K. Leen (Editors), pp. 231-238, MIT Press.
Kwok, S. & C. Carter (1990). Multiple Decision Trees. Uncertainty Artificial Intelligence 4, R. Shachter, T. Levitt, L. Kanal J. Lemmer (Editors), pp. 327-335,
North-Holland.
Lawson C.L. & R.J. Hanson (1995). Solving Least Squares Problems. SIAM Publications.
LeBlanc, M. & R. Tibshirani (1993). Combining Estimates Regression Classification. Technical Report 9318. Department Statistics, University Toronto.
Matan, O. (1996). Voting Ensembles Classifiers (extended abstract). Proceedings
AAAI-96 workshop Integrating Multiple Learned Models, pp. 84-88.
McCullagh, P. & J.A. Nelder (1983). Generalized Linear Models. London: Chapman
Hall.
Merz, C.J. (1995). Dynamic Learning Bias Selection. Proceedings Fifth International Workshop Artificial Intelligence Statistics, Ft. Lauderdale, FL:
Unpublished, pp. 386-395.
Oliver, J.J. & D.J. Hand (1995). Pruning Averaging Decision Trees. Proceedings
Twelfth International Conference Machine Learning, pp. 430-437, Morgan
Kaufmann.
Perrone, M.P. & L.N. Cooper (1993). Networks Disagree: Ensemble Methods
Hybrid Neural Networks. Artificial Neural Networks Speech Vision, R.J.
Mammone (Editor). Chapman-Hall.
Quinlan, J.R. (1993). C4.5: Program machine learning. Morgan Kaufmann.
288

fiIssues Stacked Generalization

Schapire, R.E. (1990). Strength Weak Learnability. Machine Learning, 5, pp.
197-227, Kluwer Academic Publishers.
Schapire, R.E., Y. Freund, P. Bartlett, & W.S. Lee (1997). Boosting margin: new
explanation effectiveness voting methods. Proceedings Fourteenth
International Conference Machine Learning, pages 322-330, Morgan Kaufmann.
Smyth, P. & D. Wolpert (1997). Stacked Density Estimation. Advances Neural Information Processing Systems.
Ting, K.M. (1996). Characterisation Predictive Accuracy Decision Combination. Proceedings Thirteenth International Conference Machine Learning,
pp. 498-506, Morgan Kaufmann.
Ting, K.M. & I.H. Witten (1997). Stacking Bagged Dagged Models. Proceedings
Fourteenth International Conference Machine Learning, pp. 367-375, Morgan
Kaufmann.
Weiss S. M. & C. A. Kulikowski (1991). Computer Systems Learns. Morgan Kaufmann.
Wolpert, D.H. (1992). Stacked Generalization. Neural Networks, Vol. 5, pp. 241-259,
Pergamon Press.
Zhang, X., J.P. Mesirov & D.L. Waltz (1992). Hybrid System Protein Secondary
Structure Prediction. Journal Molecular Biology, 225, pp. 1049-1063.

289


