Journal Artificial Intelligence Research 10 (1999) 375-397

Submitted 2/99; published 6/99

Ecient Heuristic Hypothesis Ranking
steve.chien@jpl.nasa.gov
andre.stechert@jpl.nasa.gov
darren.mutz@jpl.nasa.gov

Steve Chien
Andre Stechert
Darren Mutz
Jet Propulsion Laboratory
California Institute Technology
4800 Oak Grove Drive, M/S 126-347
Pasadena, CA 91109-8099

Abstract

paper considers problem learning ranking set stochastic alternatives based upon incomplete information (i.e., limited number samples). describe
system that, decision cycle, outputs either complete ordering hypotheses
decides gather additional information (i.e., observations) cost. ranking
problem generalization previously studied hypothesis selection problem|in selection, algorithm must select single best hypothesis, ranking, algorithm
must order hypotheses.
central problem address achieving desired ranking quality minimizing cost acquiring additional samples. describe two algorithms hypothesis
ranking application probably approximately correct (PAC) expected
loss (EL) learning criteria. Empirical results provided demonstrate effectiveness
ranking procedures synthetic real-world datasets.
1. Introduction

many applications, cost information quite high, imposing requirement
learning algorithms glean much usable information possible minimum
data. example:



Data may scarce, making learning possible limited training data
key.



speedup learning, minimizing processing time critical. Here, reducing number
necessary training examples key since expense processing example
significant (Tadepalli, 1992).



decision tree learning, cost using available training examples evaluating potential attributes partitioning computationally expensive (Musick,
Catlett, & Russell, 1993).



evaluating medical treatment policies, acquiring additional training examples might
imply human subjects exposed experimental treatment longer
period necessary.

one wishes sort guarantee quality solution, statistical decision
theoretic framework useful. framework answers questions: much information

c 1999 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiChien, Stechert, & Mutz
enough? point adequate information rank alternatives
requested confidence?
paper focuses parametric ranking problems, general class statistical machine learning problems goal rank set alternative hypotheses
goodness hypothesis function set parameters whose values unknown
(e.g., Chien, Stechert, & Mutz, 1998; Gratch, 1992; Greiner & Jurisica, 1992; Kaelbling,
1993; Moore & Lee, 1994; Musick et al., 1993). learning system determines refines estimates parameters using training examples, secondary goal
minimizing learning cost.
principal contributions paper are:



define two families hypothesis ranking algorithms, based recursive selection
adjacency, respectively. provide specific details apply using
probably approximately correct (PAC) expected loss (EL) decision criteria.



provide empirical results demonstrating effectiveness algorithms
achieving requested decision criteria synthetic data.



provide empirical results showing algorithms significantly outperform
existing statistical methods real-world data spacecraft design optimization
image compression applications.

remainder paper structured follows. First, describe hypothesis ranking problem formally, including definitions probably approximately
correct (PAC) expected loss (EL) decision criteria. define two algorithms
establishing criteria hypothesis ranking problem|a recursive hypothesis selection algorithm adjacent comparison algorithm. Next, describe empirical tests
demonstrating effectiveness algorithms well documenting improved
performance standard algorithm statistical ranking literature. Finally,
describe related work future extensions algorithms.
2. Hypothesis Ranking Problems

Hypothesis ranking problems abstract class learning problems algorithm
given set hypotheses rank. ranking desired orders hypotheses
expected utility, determined hypothesis' underlying probability
distribution. expected utilities unknown algorithm must estimated
training data.
Hypothesis ranking problems extension hypothesis selection problems (Chien,
Gratch, & Burl, 1995), learning system attempts select best alternative
set hypotheses. distinction hypothesis ranking hypothesis selection selection learning algorithm interested single best hypothesis,
ranking learning algorithm must determine relative order hypotheses1 .
Hypothesis selection ranking important aspect many machine learning
problems. example, utility problem speedup learning viewed selection
1. algorithms results described paper extend straightforward fashion hybrid rankingselection problems system must select rank top N hypotheses.

376

fiEfficient Heuristic Hypothesis Ranking
problem single problem-solving heuristic strategy chosen larger set
candidates. case, expected utility typically defined average time solve
problem (Gratch, 1992; Greiner & Jurisica, 1992; Minton, 1988). attribute selection
problem machine learning viewed hypothesis selection problem
one must select best attribute split set possible attribute splits utility
often measured information gain (Musick et al., 1993). reinforcement learning,
system must learn appropriate action context, utility interpreted
expected reward (Kaelbling, 1993).2
key observation regarding problems (and learning problems, general) could viewed optimization problem, utility
function optimized. Then, application traditional (or non-traditional)
optimization methods yield good results within guarantees provided algorithm depending features landscape optimized. However,
addition model sampling cost, new degree freedom added problem.
cost samples high, traditional optimization algorithms fare poorly.
Additionally, many mentioned applications system chooses single
alternative never revisits decision, many cases system
want investigate several prioritized options (either serially parallel), hence
ranking useful. Motivation provided following scenarios:



Upper lower bounds, span: Minimax search algorithms use metaknowledge

(such upper lower bounds node) pruning parts tree. Also,
times knowing span expected utilities candidate set
useful (e.g., checking convergence conditions adaptive algorithm
GA).



Augmenting external knowledge: Another area hypothesis ranking may



entire ranking: cases, entire ranking significant. instance,

important applications hypothesis selection human supervision.
stochastic objective function (i.e., hypothesis) represents part problem, ranking used augment external knowledge problem.
example, engineering simulations usually capture physical properties candidate designs, usually choose forego details manufacturing, logistics,
economics.
evolutionary algorithms, individuals propagated future generations
often selected likelihood proportionate rank current
generation (Goldberg, 1989). Another example arises case search algorithms
take advantage node ordering heuristics, beam search iterative
broadening (Ginsberg & Harvey, 1992).

hypothesis evaluation problem, always achieving correct ranking impossible
practice, exact underlying probability distributions unknown. Thus,
always (perhaps vanishingly) small chance algorithms unlucky
2. Note analogous reinforcement learning problem one learning appropriate action immediate feedback rather delayed feedback.

377

fiChien, Stechert, & Mutz
finite number samples taken. Consequently, rather always
requiring algorithm output correct ranking, impose probabilistic criteria
rankings produced. several families requirements exist, paper
examine two criteria: probably approximately correct (PAC) model selecting
hypothesis function approximates well target function (Valiant, 1984)
expected loss (EL) requirement frequently used decision theory gaming problems
(Russell & Wefald, 1992). Informally, satisfy PAC requirement, algorithm must
produce result high probability close correct (e.g., incorrect orderings
likely occur hypotheses similar expected utilities). satisfy
EL requirement, hand, bound must established expected loss
result, loss difference utilities two incorrectly ordered hypothese
incorrect ranking.
expected utility hypothesis estimated observing values
finite set training examples. However, satisfy decision criteria, algorithm must
able reason potential difference estimated true utilities
hypotheses. Let Ui denote true expected utility hypothesis let U^i
estimated expected utility hypothesis i. Without loss generality, let us presume
proposed ranking hypotheses U1 > U2 >; :::; > Uk 1 > Uk .
PAC requirement states that, user-specified , probability 1 :
k^1

[(Ui + ) > MAX (Ui+1 ; :::; Uk )]

(1)

i=1

context PAC criterion, number called indifference interval

overall ranking error total error rate. 3

issue allocate overall ranking error among many possible pairwise
comparisons hypotheses discussed next section.
Correspondingly, selecting hypothesis H1 best set k hypotheses H1 ; :::; Hk , let selection loss L follows.
L(H1 ; fH1 ; :::; Hk g) = MAX (0; MAX (U2 ; :::; Uk )

U1 )

(2)

Then, ranking loss RL ranking H1 ; :::; Hk would be:
RL(H1 ; :::; Hk ) =

k 1
X

L(Hi ; fHi+1 ; :::; Hk g)

(3)

i=1

3. distinction betwen true means estimated means (for use sample means)
confusing one. assessing validity ranking produced algorithm, one would use
true means distributions (if available, test distributions) accurate estimation
possible (such edxtremely large sampling distribution). However, ranking algorithm
uses estimated parameters (including sample mean) estimate error. estimation single
mean estimate mean normally distributed around true mean usage
justified. However, proven (and indeed unsure) whether using estimate
complex ranking selection contexts guaranteed correct (see later section heuristic nture
algorithms).

378

fiEfficient Heuristic Hypothesis Ranking
hypothesis ranking algorithm obeys expected loss requirement must produce
rankings average less ranking loss requested expected loss bound.
policy loss allocation discussed next section.
example, consider ranking hypotheses expected utilities: U1 = 1:0; U2 =
0:95; U3 = 0:86. ranking U2 > U1 > U3 valid PAC ranking indifference
interval = 0:06 = 0:01 observed ranking loss 0:05 + 0 = 0:05.
However, confidence pairwise comparison two hypotheses well
understood complement probability comparison's result
error, less clear define ensure desired confidence met set
comparisons required selection even complex set comparisons required
ranking. Equation 4 defines confidence Ui + > Uj , utilities
normally distributed unknown unequal variances.

pn



= (U^i

j + )
^

Si

(4)

j

represents cumulative standard normal distribution function, n, U^i j ,
S^i j size, sample mean, sample standard deviation blocked differential
distribution4, respectively.
Likewise, computation expected loss asserting ordering pair
hypotheses well understood, estimation expected loss entire ranking
less clear. Equation 5 defines expected loss drawing conclusion Ui > Uj ,
assumption normality (see Chien et al., 1995, details).
EL[Ui > Uj ] =

S^i

je

U^i j 2
)
j

0:5n( ^
Si

p

2n

+

U^i

p

j

2

Z

1

U^i j pn
S^i j

e

0:5z 2

dz

(5)

next two subsections, describe two interpretations estimating likelihood
overall ranking satisfies PAC EL requirements estimating combining
pairwise PAC errors EL estimates. interpretations lends directly
algorithmic implementation described below.
2.1 Ranking Recursive Selection

One obvious way determine ranking H1 ; :::; Hk view ranking recursive selection
set remaining candidate hypotheses. view, overall ranking error,
specified desired confidence PAC algorithms loss threshold EL
algorithms, first distributed among k 1 selection errors subdivided
pairwise comparison errors (Figure 1). Data sampled estimates
pairwise comparison error (as dictated equation 4 5) satisfy bounds set
algorithm.
4. Note approach block, match, examples reduce sampling complexity. Blocking
makes estimates using difference utility competing hypotheses observed example. Blocking significantly reduce variance data hypotheses independent.
differential distribution formed taking differences blocked individual samples form
new distribution. trivial modify formulas address cases possible
block data (see Moore & Lee, 1994; Chien et al., 1995, details).

379

fiChien, Stechert, & Mutz

H1

H2

H3

H4

H5

H2

H3

H4

H5

H3

H4

H5

H4

H5


*

Figure 1: Computing overall error recursive ranking. per-comparison errors
summed level recursion, overall sum (across levels)
compared specified total error, .

Thus, another degree freedom design recursive ranking algorithms
method overall ranking error ultimately distributed among individual pairwise comparisons hypotheses. Two factors uence way compute
error distribution. First, model error combination determines error allocated
individual comparisons selections combines overall ranking error therefore
many candidates available distribution error.
Using Bonferroni's inequality, asserts probability union events
greater sum probabilities individual events5 , one would inclined
combine errors additively. However, following conservative approach, one
assert predicted \best" hypothesis may change sampling
worst case, conclusion might dependon possible pairwise comparisons
error distributed among n2 pairs hypotheses.6
Second, policy respect allocation error among candidate comparisons
selections determines samples distributed. example, contexts,
consequences early selections far outweigh later selections. scenarios,
implemented ranking algorithms divide overall ranking error unequally
5. Note simplest Bonferonni inequalities, fall clean correspondence
terms expansion probability union events according principle
inclusion exclusion natural way.
6. discussion issue, see pp. 18-20 (Gratch, 1993).

380

fiEfficient Heuristic Hypothesis Ranking
favor earlier selections.7 Also, possible divide selection error pairwise error
unequally based estimates hypothesis parameters order reduce sampling cost
(for example, Gratch, Chien, & DeJong, 1994, allocates error rationally).
Within scope paper, consider algorithms that: (i) combine pairwise
error selection error additively, (ii) combine selection error overall ranking error
additively, (iii) allocate error equally level.
One disadvantage recursive selection hypothesis selected,
removed pool candidate hypotheses. issue rare cases when,
sampling increase confidence later selection, estimate hypothesis'
mean changes enough previously selected hypothesis longer dominates it.
However, remains original hypotheses shown dominate others
specified level certainty, .
assumptions result following formulations (where (U1 fU2 ; :::; Uk g)
used denote error due action selecting hypothesis 1 Equation 1
set fH1 ; :::; Hk g (U1 fU2 ; :::; Uk g) denotes error due selection loss
situations Equation 2 applies):
rec (U1 > U2 > ::: > Uk ) =

rec (U2 > U3 > ::: > Uk )
+ (U1 fU2 ; :::; Uk g)

(6)

rec (Uk ) = 0 (the base case recursion) selection error defined
(Chien et al., 1995):
(U1 fU2 ; :::; Uk g) =

k
X

1;i

(7)

i=2

using Equation 4 compute pairwise confidence.

Algorithmically, implement following pseudo-code:

ensure n0 samples per hypothesis
distribute error individual selections
(stopping criteria met)
take samples
(means ordered differently ranking)
restart algorithm
analogous recursive selection algorithm based expected loss defined follows
ELrec (U1 > U2 > ::: > Uk ) =

ELrec (U2 > U3 > ::: > Uk )
+EL(U1 fU2 ; :::; Uk g)

(8)

ELrec(Uk ) = 0 selection EL defined (Chien et al., 1995):
EL(U1 fU2 ; :::; Uk g) =

k
X
i=2

7. Space constraints preclude description here.

381

EL(U1 ; Ui )

(9)

fiChien, Stechert, & Mutz

*



1,2

H1

2,3

H2

k-1,k

H3

Hk-1



Hk

Figure 2: Computing overall error adjacent ranking. Per-comparison errors neighboring hypotheses proposed ranking summed compared
required total error, .

2.2 Ranking Adjacency Comparison

Another interpretation ranking confidence (or loss) adjacent elements
ranking need compared. case, overall ranking error divided directly
k 1 pairwise comparison errors (Figure 2). leads following confidence equation
PAC criteria:
adj (U1 > U2 > ::: > Uk ) =

k 1
X

i;i+1

(10)

i=1

following equation EL criteria.
ELadj (U1 > U2 > ::: > Uk ) =

k 1
X

EL(Ui ; Ui+1 )

(11)

i=1

ranking comparison adjacent hypotheses establish dominance
loss bounds non-adjacent hypotheses (where hypotheses ordered
observed mean utility), advantage requiring fewer comparisons recursive
selection (and thus may require fewer samples recursive selection). However,
reason, adjacency algorithms may less likely recursive selection algorithms
bound probability correct ranking (or average loss) correctly. case
PAC algorithms, -dominance necessarily transitive. case
EL algorithms, expected loss necessarily additive considering two
hypothesis comparisons sharing common hypothesis.8
8. example ranking loss non-adjacent hypotheses exceeds desired loss bound
ranking, even though sum adjacent losses not, occurs blocked differential
distribution induced two non-adjacent hypotheses high variance relative hypothesis adjacent

382

fiEfficient Heuristic Hypothesis Ranking
2.3 Heuristic Nature Algorithms

recusrsive selection adjacency algorithms heuristic sense
proven statistically meet specified decision criteria (i.e., PAC criteria
select ranking satisfies equation (1) probability 1 similarly EL
criteria average ranking loss specified equation (3) less requested bound.
Indeed, several aspects algorithms make extremely dicult prove
would (probabilistically) achieve corresponding decision criteria. aspects include:



Sharing samples: order n1 samples differential distribution (i.e.



Heuristic error combination: recursive selection adjacency error com-



Ignorance lead switches multiple comparison paths: sampling pro-



blocking) H1 H2 , takes n1 samples H1 n1 samples
problems H2 . algorithms reduce sampling cost reusing
samples differential distributions comparing H1 hypotheses H2
hypotheses. makes errors derived samples independent.
Hence traded accuracy ease analysis algorithms heuristic
eciency. Particularly recursive selection approach, samples lowest
ranking hypothesis would used k 1 differential comparisons.
bination models heuristic means combining pairwise errors.
pairwise errors independent (see above). Empirically observed
pairwise errors tend overestimated error combination function
tends under-combine. Overall empirically combined error estimates tend
reasonably accurate, remaining sections show.
cess, ordering hypotheses may change (e.g., ordering sample means
may change). means implicitly, decision depended additional
pairwise comparison may ected final set comparisons contributing pairwise error. complexity could avoided fixing order
hypotheses n0 samples. However, would require samples would
involve showing -dominance hypothesis higher sample mean hypothesis
(indeed, may never converge). choose ignore complexity base
combined error used stopping condition final ordering.

Use non-normal distributions: many applications described re-

mainder article, real-world data distributed manner simlar
normal distributions (we investigate issue later article).
algorithms describe heuristic presume data normally
distributed even though case.

(i.e., currently ranked them). variance differential distribution makes
maximum contribution sample set small, so, e.g., 1 2 = 2, 1 2 = 2, n1 2 = 2,
2 3 = 2, 2 3 = 2, n2 3 = 2, exists configuration 1 3 = 4, 1 3 = 8.
expected losses EL(H1 ; H2 ) = 2:05, EL(H2 ; H3 ) = 2:05, EL(H1 ; H3 ) = 4:80 > 4:10.

383

fiChien, Stechert, & Mutz
2.4 Relevant Approaches

standard statistical ranking/selection approaches make strong assumptions
form problem (e.g., variances associated underlying utility distribution
hypotheses might assumed known equal). Among these, method Turnbull
Weiss (Turnbull & Weiss, 1984) comparable PAC-based approach.9
Turnbull Weiss' algorithm sequential interval-based procedure selecting
member population largest mean. treat hypotheses normally
distributed random variables unknown mean unknown possibly unequal
variance. algorithm carries additional stipulation hypotheses
independent. procedure consists taking initial sample n0 observations
hypotheses taking samples sequentially according stopping criteria.
stopping criteria satisfied, hypothesis highest sample mean
2
chosen. stopping criteria inequality Snii n1 satisfied, Si
ni sample mean number samples ith hypothesis n chosen
2
according indifference
interval confidence level . particular, n = d2
R1
chosen satisfy 1 (F (y + d))k 1f (y)dy = F (y) f (y) cumulative
distribution function probability density function standard normal distribution.
still reasonable use approach candidate hypotheses
independent, excessive statistical error unnecessarily large training set sizes may result.
case hypotheses truly independent, Turnbull Weiss' technique
able exploit knowledge outperform methods adopt
assumption.
3. Empirical Performance Evaluation

turn empirical evaluation hypothesis ranking techniques synthetic
real-world datasets. evaluation serves three purposes. First, demonstrates
techniques perform predicted (in terms bounding probability incorrect selection expected loss). Second, validates performance techniques compared
standard algorithms statistical literature. Third, evaluation demonstrates
robustness new approaches real-world hypothesis ranking problems.
experimental trial consists solving hypothesis ranking problem given
technique given set problem control parameters. measure performance
(1) well algorithms satisfy respective criteria; (2) number
samples taken or, alternatively, cost (in seconds) executing algorithm. Since
performance statistical algorithms single trial provides little information
overall behavior, trial repeated multiple times results averaged
across trials. Synthetic experimental trials repeated 500 times, trials
real-world data repeated 100 times. PAC expected loss criteria
directly comparable, approaches analyzed separately.
9. PAC-based approaches investigated extensively statistical ranking selection literature topic confidence interval based algorithms (see Haseeb, 1985, review recent
literature).

384

fiEfficient Heuristic Hypothesis Ranking
Hk

H4

H3

H2

H1

-(k-1)

-3

-2

-



utility

Figure 3: stepped means hypothesis configuration.
3.1 Evaluation Synthetic Datasets

Evaluation synthetic data used show that: (1) techniques correctly bound probability incorrect ranking expected loss predicted underlying assumptions
valid even underlying utility distributions inherently hard rank 10 ,
(2) PAC techniques compare favorably algorithm Turnbull Weiss
wide variety circumstances.
synthetic datasets, utility distributions hypotheses modeled
random variables defined underlying parameterized distribution. Thus, characterizing ranking problem consists choosing number hypotheses rank
assigning values parameters representing utility distributions hypotheses. case, model utilities independent normal random variables
mean standard deviation. Thus, let k number hypotheses, hypothesis ranking problem described 2k parameters specifying expected utility
utility standard deviation hypothesis. general, several parameters may required characterize ranking problem fully11, number hypotheses
choices parameters utility distributions underlying hypotheses
characterize overall diculty ranking problem.
statistical ranking selection community uses standard family selection
problems known diculty analyze performance hypothesis selection strategies.
method, called least favorable configuration (LFC) population means
assignment parameters distributions likely cause technique
choose wrong hypothesis thus provides severe test technique's abilities.
configuration, utilities independent normally distributed variables equal
variance. k 1 hypotheses utilities equal expectation, , remaining
hypothesis expected utility + .
interested hypothesis ranking problems rather selection problems,
use generalization LFC call stepped means. configuration, one
hypotheses assigned expected utility successive hypotheses assigned
expected utility 1; :::; k 1 (Figure 3).
general, problems based least favorable configuration become dicult
(i.e., require samples) number hypotheses k increases, common utility
variance 2 increases, difference means utility distributions decreases.
standard methodology, technique evaluated ability achieve confidence
10. Configurations contain hypotheses high variance relative separation means
dicult rank.
11. instance, samples allocated rationally (Chien et al., 1995), becomes necessary assign
parameters cost distribution well, candidate hypotheses ranked,
number hypotheses rank would another problem parameter.

385

fiChien, Stechert, & Mutz
correct selection using several settings k . last ratio combines
single quantity which, increases, makes problem dicult. methodology
extends stepped means directly.
hypothesis ranking strategies algorithm control parameters
govern attack problem. PAC techniques three control parameters:
initial sample size n0 , desired confidence correct ranking indifference setting
12 . expected loss techniques two control parameters: initial sample size n0
loss threshold H .
observed number samples required achieved accuracy PAC techniques
stepped means configuration shown Table 3.1. results indicate
systems roughly comparable number examples required choose hypotheses.
expected, number examples increases k, , . P ACadj algorithm
required least number samples inconsistent meeting desired accuracy
bound (as indicated failure meet prescribed error bound several cases).
interesting Turnbull Weiss method significantly outperform PAC
techniques despite fact algorithm assumes hypotheses independent
(as case stepped means configuration), PAC approaches make
assumption. comparison, principal performance metric number
samples required achieve requested ranking, methods effective achieving
requested accuracy.
expected loss experiments, ran expected loss hypothesis ranking algorithms
stepped means configurations described range expected loss
bounds. Table 3.1 shows results experiment, displaying number samples
required produce ranking average observed loss configuration.
results show ELrec algorithm correctly bounded loss ELadj algorithm required less samples ELrec algorithm, correctly bound
expected loss (since observed loss greater loss bound H .13
3.2 Evaluation Real Datasets

test real-world applicability based data drawn several datasets relating
spacecraft design processing science data gathered context planetary
exploration. first two datasets investigate relate spacecraft design optimization
problems hypotheses wish rank candidate solutions design
problem. third last dataset examine involves ranking various lossless image
compression approaches based performance large set terrestrial images collected spacecraft Galileo. Cost evaluation given seconds empirical data
12. Note formulation stepped means test PAC approaches, difference
expected mean successive hypotheses indifference interval algorithm. Thus,
plays roles problem parameter control parameter here.
13. One confusing point identical hypothesis ranking algorithm settings, one observe
lower loss ranking larger number hypotheses. algorithm first divides
loss number pirwise comparisons. Thus, overall error (or expected loss bound),
hypotheses, pairwise expected error (or loss) smaller hypotheses.
ranking loss defined previously. Thus, possible observed loss increase decrease
compared settings fewer hypotheses.

386

fiEfficient Heuristic Hypothesis Ranking

k
3
3
3
3
3
3
5
5
5
5
5
5
10
10
10
10
10
10



0.75
0.75
0.90
0.90
0.95
0.95
0.75
0.75
0.90
0.90
0.95
0.95
0.75
0.75
0.90
0.90
0.95
0.95




2
3
2
3
2
3
2
3
2
3
2
3
2
3
2
3
2
3

TURNBULL
62 (0.88)
117 (0.89)
97 (0.96)
183 (0.99)
130 (0.97)
231 (0.96)
177 (0.83)
321 (0.95)
245 (0.98)
445 (0.98)
299 (0.98)
541 (0.98)
558 (0.92)
1,015 (0.94)
700 (0.97)
1,254 (0.97)
821 (1.00)
1,462 (0.99)

P ACrec

55 (0.95)
101 (0.86)
86 (0.94)
152 (0.96)
122 (0.97)
204 (0.95)
165 (0.95)
314 (0.93)
245 (0.97)
409 (0.91)
294 (0.98)
538 (0.98)
624 (0.91)
1,042 (0.95)
742 (0.96)
1,359 (0.97)
877 (0.97)
1,569 (0.98)

P ACadj

38 (0.78)
49 (0.80)
58 (0.92)
96 (0.89)
89 (0.97)
146 (0.94)
105 (0.87)
161 (0.75)
163 (0.91)
290 (0.92)
216 (1.00)
377 (0.92)
345 (0.85)
635 (0.83)
523 (0.91)
883 (0.90)
661 (0.94)
1,164 (0.93)

Table 1: Estimated expected total number observations PAC algorithms
stepped means configuration. Achieved probability correct ranking shown
parenthesis.

Parameters
k H
3 2 1.0
3 2 0.75
3 2 0.5
3 2 0.25
5 2 1.0
5 2 0.75
5 2 0.5
5 2 0.25
10 2 1.0
10 2 0.75
10 2 0.5
10 2 0.25

ELrec

Samples
96
102
139
235
320
343
464
575
1,136
1,325
1,533
1,856

Loss
0.6
0.5
0.2
0.1
0.7
0.4
0.4
0.2
0.5
0.5
0.3
0.1

ELadj

Samples
43
56
73
139
140
169
247
350
572
668
872
1,153

Loss
1.2
1.0
0.6
0.4
1.3
1.2
0.7
0.5
1.4
1.1
0.7
0.4

Table 2: Estimated expected total number observations EL algorithms stepped
means configuration. Observed average loss produced rankings.

387

fiChien, Stechert, & Mutz
because, unlike synthetic problems, cost sampling hypothesis constant
domains. Table 3 gives summary three ranking problems considered.
Dataset
DS-2 Penetrator

fixed parameters
penetrator diameter
penetrator length

DS-2 Aeroshell

fore body overlap
nose cone angle
bluntness ratio
fillet radius
outer diameter
tail geometry
compression method

Lossless Image Comp.

random variables
impact orientation
impact velocity
soil density
stagnation pressure coef.

optimization criteria
maximize penetration probability
maximize penetration depth

randomly selected test image

maximize compression ratio

minimize weight
achieve target entry velocity

Table 3: Description datasets used algorithm evaluation.

3.2.1 DS-2 Penetrator

goal New Millennium Deep Space Two (DS-2) mission deliver pair
microprobes planet Mars scientific study Martian soil. probes
released orbit, travel Martian atmosphere, embed
soil near southern polar ice cap. primary science objectives mission
(Balacuit., 1997):





determine ice present surface Mars,
measure local atmospheric pressure,
characterize thermal properties Martian subsurface soil.

goal spacecraft design problem determine good set physical dimensions penetrator|a small, robust probe designed impact surface extremely
high velocity operate extreme cold. Specifically, use design simulation
data DS-2 mission penetrator design.
casting design problem, hold shape penetrator constant
generate design candidates based different values variables penetrator diameter
length. specific design sample taken acquiring impact orientation, impact
velocity, soil density parameterized multivariate distribution calling
complex physical simulation determine depth penetrator bored
Martian surface. goal penetrator design problem determine physical
dimensions penetrator maximize probability penetration, cases
penetration, maximize penetration depth.
Tables 4 5 show results applying PAC-based, Turnbull, expected loss
algorithms ranking problem system requested rank 10 penetrator
designs.14 problem utility function depth penetration penetrator,
14. \True" expected utility values computed performing 20,000 samples using sample mean
large sample ground truth. expected utilities used compute PAC -validity
rankings observed loss using provided definitions.

388

fiEfficient Heuristic Hypothesis Ranking
cases penetrator penetrate assigned zero utility.
shown Table 4, PAC algorithms significantly outperformed Turnbull algorithm,
expected hypotheses somewhat correlated (via impact orientations soil densities). Table 5 shows ELrec expected loss algorithm effectively
bounded actual loss ELadj algorithm inconsistent.
k
10
10
10



0.75
0.90
0.95




2
2
2

TURNBULL
534 (0.96)
667 (0.98)
793 (0.99)

P ACrec

144 (1.00)
160 (1.00)
177 (1.00)

P ACadj

92 (0.98)
98 (1.00)
103 (0.99)

Table 4: Estimated expected total number observations rank DS-2 spacecraft designs.
Achieved probability correct ranking shown parenthesis.

Parameters
k
H
10
0.10
10
0.05
10
0.02

ELrec

Samples
152
200
378

Loss
0.05
0.03
0.03

ELadj

Samples
77
90
139

Loss
0.14
0.06
0.03

Table 5: Estimated expected total number observations expected loss incorrect
ranking DS-2 penetrator designs.

3.2.2 DS-2 Aeroshell Design Ranking

objective problem design aeroshell soil penetrator described
previous section gives appropriate entry velocity minimum weight. Design
candidates defined six continuous variables represent various geometric quantities: extent fore body overlaps aftbody, nose cone angle, bluntness
ratio, fillet radius, outer diameter, tail geometry. Candidate designs (hypotheses)
evaluated running simple physical simulation aeroshell's behavior.
sample taken running simulation fixed design variables hypothesis
value stagnation pressure coecient taken normal distribution.
simulation computes values achieved entry velocity mass aeroshell;
weighted sum reciprocals values maximized.
give results ranking three, five, ten hypotheses using Turnbull, PAC,
expected loss algorithms Tables 6 7.15
previous experiment, PAC-based algorithms outperformed Turnbull
algorithm cases. P ACadj algorithm represents significant increase
15. Again, deep sampling (500 samples) performed obtain \correct" ranking,
algorithms compared.

389

fiChien, Stechert, & Mutz
performance here, note achieve desired level confidence cases;
Turnbull P ACrec algorithms achieve required confidence.

k
3
3
3
3
3
3
5
5
5
5
5
5
10
10
10
10
10
10



0.75
0.75
0.90
0.90
0.95
0.95
0.75
0.75
0.90
0.90
0.95
0.95
0.75
0.75
0.90
0.90
0.95
0.95




2
3
2
3
2
3
2
3
2
3
2
3
2
3
2
3
2
3

TURNBULL
8.9 (1.00)
22.9 (1.00)
17.1 (1.00)
38.2 (1.00)
22.6 (1.00)
52.0 (1.00)
29.1 (0.92)
69.0 (1.00)
42.4 (0.99)
94.7 (0.99)
51.9 (0.98)
117.9 (0.99)
84.0 (0.99)
196.6 (1.00)
112.0 (0.98)
252.9 (0.99)
129.1 (1.00)
315.7 (1.00)

P ACrec

8.4 (1.00)
11.3 (1.00)
14.0 (1.00)
18.6 (1.00)
21.6 (1.00)
32.1 (1.00)
20.1 (0.94)
33.9 (0.96)
30.0 (0.93)
54.8 (0.96)
43.6 (1.00)
81.5 (0.99)
42.0 (0.94)
57.9 (0.96)
53.8 (0.98)
85.5 (1.00)
61.3 (0.97)
125.7 (1.00)

P ACadj

3.5 (1.00)
3.8 (1.00)
7.1 (1.00)
7.2 (1.00)
7.1 (1.00)
7.3 (1.00)
11.8 (0.91)
11.7 (0.91)
11.7 (0.91)
11.8 (0.84)
11.7 (0.91)
11.5 (0.92)
22.1 (0.92)
22.1 (0.90)
22.6 (0.89)
21.6 (0.91)
20.6 (0.90)
20.4 (0.92)

Table 6: Estimated expected cost (in seconds) rank aeroshell designs. Achieved probability correct ranking shown parenthesis.

Parameters
k
H
3
20
3
30
3
40
5
20
5
30
5
40
10
20
10
30
10
40

ELrec

Execution Cost
9.5
7.6
7.3
21.7
18.1
15.0
55.3
42.6
38.2

Loss
4.3
3.4
4.1
7.0
12.0
9.3
9.7
8.9
10.4

ELadj

Execution Cost
7.9
7.3
6.9
7.2
6.4
10.5
18.3
14.2
13.1

Loss
3.4
3.7
2.7
8.6
12.4
8.5
7.9
9.8
9.6

Table 7: Estimated expected cost (in seconds) expected loss incorrect ranking
DS-2 aeroshell designs.

390

fiEfficient Heuristic Hypothesis Ranking
3.2.3 Lossless Image Compression Galileo Image Data

problem utilizes large set raw image data acquired Galileo spacecraft.
images 256 256 size made greyscale pixels ranging 0 255
intensity. goal select lossless compression method16 performs best
class images. performance image compression algorithm particular image
could measured number ways. example, execution time, compression ratio,
image quality (in case lossy compression methods considered) could
define algorithm performance. tests chose consider compression ratio
achieved given compression method utility function. sample method
(hypothesis), image randomly selected, method applied image,
achieved compression ratio recorded.
Given (Tables 8 9) results ranking three, five, seven hypotheses
using Turnbull, PAC, expected loss algorithms. Ranking correctness determined
comparison \correct" ranking established sampling compression method
set 1500 distinct images.
note substantial performance improvement PAC-based algorithms
Turnbull algorithm. Although Turnbull algorithm PAC
algorithms (Table 8) achieved desired confidence level, adjacent version EL
algorithm (Table 9) failed bound loss specified level half cases.
interesting consider results presented section light fact
statistical techniques used makes form normality assumption.
fact, three problem domains investigate number hypotheses whose
utility functions normally distributed. past experience known utility
functions DS-2 Penetrator domain (Section 3.2.1) highly non-normal; Figure 4
illustrates difference data normally distributed data not.
0.08

0.12

0.07
0.1
0.06
0.08
0.05

0.04

0.06

0.03
0.04
0.02
0.02
0.01

0
0.2

0
0.3

0.4

0.5

0.6

0.7

0.8

0

50

100

150

200

250

300

350

400

450

500

Figure 4: comparison (a) data normally distributed high likelihood (b)
data likely normally distributed. case, histogram
experimental data shown solid boxes; data drawn normal distribution
mean standard deviation shown dashed lines.
determine extent utilities hypotheses remaining two domains normally distributed applied Kolmogorov-Smirnov test (see Appendix
16. seven compression methods considered were: CALIC, lossless JPEG, GIF, TIFF, pack, gzip,
compress.

391

fiChien, Stechert, & Mutz

k
3
3
3
3
3
3
5
5
5
5
5
5
7
7
7
7
7
7



0.90
0.90
0.95
0.95
0.99
0.99
0.90
0.90
0.95
0.95
0.99
0.99
0.90
0.90
0.95
0.95
0.99
0.99




2
3
2
3
2
3
2
3
2
3
2
3
2
3
2
3
2
3

TURNBULL
62.8 (1.00)
150.8 (1.00)
84.6 (1.00)
206.8 (1.00)
142.0 (1.00)
359.4 (1.00)
134.7 (1.00)
329.9 (1.00)
176.1 (1.00)
399.8 (1.00)
249.3 (1.00)
598.1 (1.00)
210.8 (1.00)
499.3 (1.00)
250.3 (1.00)
608.7 (1.00)
339.6 (1.00)
813.7 (1.00)

P ACrec

30.1 (1.00)
30.5 (1.00)
28.6 (1.00)
29.0 (1.00)
30.1 (1.00)
30.6 (1.00)
39.5 (1.00)
39.9 (1.00)
39.3 (1.00)
39.3 (1.00)
39.2 (1.00)
39.2 (1.00)
35.6 (1.00)
35.7 (1.00)
37.4 (1.00)
36.0 (1.00)
36.5 (1.00)
37.2 (1.00)

P ACadj

14.8 (1.00)
14.8 (1.00)
15.0 (1.00)
20.5 (1.00)
23.3 (1.00)
23.2 (1.00)
29.9 (1.00)
30.0 (1.00)
29.8 (1.00)
29.6 (1.00)
29.9 (1.00)
30.7 (1.00)
37.2 (1.00)
34.5 (1.00)
35.6 (1.00)
35.0 (1.00)
34.5 (1.00)
35.3 (1.00)

Table 8: Estimated expected cost (in seconds) rank lossless image compression approaches Galileo image data. Achieved probability correct ranking shown
parenthesis.

Parameters
k
H
3
10
3
5
3
1
5
10
5
5
5
1
7
10
7
5
7
1

ELrec

Execution Cost
31.7
32.5
33.7
80.6
83.5
101.0
99.5
105.7
119.8

Loss
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0

ELadj

Execution Cost
24.9
24.9
24.9
32.7
33.7
32.3
42.3
33.3
30.4

Loss
0.0
0.0
0.0
17.4
69.4
49.4
17.4
34.7
86.8

Table 9: Estimated expected cost (in seconds) expected loss incorrect ranking
DS-2 penetrator designs.

392

fiEfficient Heuristic Hypothesis Ranking
details). test determined none ten hypotheses DS-2 Aeroshell
domain (Section 3.2.2) normally distributed utility. Additionally, two seven
hypotheses image compression domain (Section 3.2.3) shown greater
90% likelihood normally distributed utility functions17. reasons,
evaluating ranking strategies datasets provides particularly strong test
applicability techniques.
draw reader's attention particularly large disparity performance
Turnbull algorithm PAC-based algorithms image compression domain,
especially apparent number hypotheses, confidence level, high.
Additionally, problem domain two hypotheses normally distributed utility
five non-normal. observations suggest PAC-based algorithms
perform better (in relative terms) faced domain violates assumption
normality.
4. Discussion Conclusions

number areas related work. First, considerable analysis
hypothesis selection problems. Selection problems formalized using Bayesian
framework (Moore & Lee, 1994; Rivest & Sloan, 1988) require initial
sample, uses rigorous encoding prior knowledge. Howard (Howard, 1970)
details Bayesian framework analyzing learning cost selection problems. one
uses hypothesis selection framework ranking, allocation pairwise errors
performed rationally (Gratch et al., 1994). Reinforcement learning work (Kaelbling, 1993)
immediate feedback viewed hypothesis selection problem.
framework presented invites future work number directions. Currently,
stopping criteria used relaxations ranking requirement. Another approach
could used bound resources available ranking. Limiting number
samples sample cost high limiting time computation (so
anytime algorithm) two straightforward application areas.
Another area future work discovery composite strategies hypotheses. Thus
far examined ranking (and articles, selection) hypothesis highest expected value entire distribution. example, learning scheduling control
strategy well distribution problems. However, likely
distributions problems, exists composite strategy would outperform
single strategy. example, single strategy might apply method solve
problem. composite strategy would be, test problem feature X, X true apply method A, else apply method B. composite strategies correspond algorithm
portfolios named Operations Research. Indeed results applying methods could
viewed strategies. One might composite strategy trying method
10 CPU seconds, fails trying method B. course, composition portfolio approaches, diculty iseciently proposing evaluating plausible
compositions. even small set base strategies number copositions enormous.
17. reference, data Figure 4 (a) normally distributed 97.5% likelihood, according
Kolmogorov-Smirnov test.

393

fiChien, Stechert, & Mutz
summary, paper described hypothesis ranking problem, extension
hypothesis selection problem. defined application two decision criteria, probably approximately correct expected loss, problem. defined two families
algorithms, recursive selection adjacency, solution hypothesis ranking problems.
Finally, demonstrated effectiveness algorithms synthetic realworld datasets, documenting improved performance existing statistical approaches.
Acknowledgments

work performed Jet Propulsion Laboratory, California Institute Technology, contract National Aeronautics Space Administration.
Appendix A. Applying K-S Test Real Datasets

Kolmogorov-Smirnov Test statistical means accepting, certain level
confidence, hypothesis sampleset fits parametric distribution given
set parameters. method compares CDF generated empirical distribution
corresponding parametric distribution (i.e., estimated parameters).
K-S test gives confidence based maximum, D, discrepancies
two CDFs:
= maxjF1 (x)

F2 (x)j

purposes wish determine, hypothesis given domain, whether
values utility function normally distributed not. case, half
utility samples taken used compute mean standard deviation normal;
remaining half used compute CDF.
A.1 DS-2 Penetrator

20000 samples taken.
design number
1
2
3
4
5
6
7
8
9
10

maxjF1 (x)

F2 (x)j
0.1415
0.1202
0.1020
0.1261
0.1207
0.1261
0.1020
0.1493
0.1461
0.1261

394

normally distributed?
90% likely
90% likely
90% likely
90% likely
90% likely
90% likely
90% likely
90% likely
90% likely
90% likely

fiEfficient Heuristic Hypothesis Ranking
A.2 DS-2 Aeroshell Design Ranking

500 samples taken.
design number
1
2
3
4
5
6
7
8
9
10

maxjF1 (x)

F2 (x)j

0.08
0.08
0.08
0.08
0.08
0.08
0.08
0.08
0.08
0.08

normally distributed?
< 90% likely
< 90% likely
< 90% likely
< 90% likely
< 90% likely
< 90% likely
< 90% likely
< 90% likely
< 90% likely
< 90% likely

A.3 Lossless Image Compression Galileo Image Data

200 samples taken.
compression method
gif
compress
calic
gzip
jpegls
pack
tiff

maxjF1 (x)

F2 (x)j

0.10
0.14
0.19
0.09
0.18
0.12
0.11

normally distributed?
90% likely
< 90% likely
90% likely
97.5% likely
90% likely
< 90% likely
< 90% likely

References

Balacuit., C. P. (1997). Deep Space 2 { Mars Microprobe Home Page (mission objectives
statement). Tech. rep. http://nmp.jpl.nasa.gov/ds2, NASA/JPL.
Chien, S. A., Gratch, J. M., & Burl, M. C. (1995). Ecient Allocation Resources
Hypothesis Evaluation: Statistical Approach. IEEE Trans. Pattern Analysis
Machine Intelligence, 17 (7), 652{665.
Chien, S. A., Stechert, A. D., & Mutz, D. H. (1998). Ecient Heuristic Ranking Hypotheses. Advances Neural Information Processing Systems 10 (Jordan, Kearns,
Solla eds.), pp. 444{450 Denver, Colorado. NIPS.
Ginsberg, M., & Harvey, W. (1992). Iterative Broadening. Artificial Intelligence Journal,
55, 367{383.
395

fiChien, Stechert, & Mutz
Goldberg, D. (1989). Genetic Algorithms Search, Optimization, Machine Learning.
Addison-Wesley.
Gratch, J. (1992). COMPOSER: Probabilistic Solution Utility Problem Speed-up
Learning. Proceedings Tenth National Conference Artificial Intelligence,
pp. 235{240 San Jose, CA. AAAI.
Gratch, J. (1993). COMPOSER: Decision-theoretic Approach Adaptive Problem Solving. Tech. rep. UIUCDCS-R-93-1806, Department Computer Science, University
Illinois.
Gratch, J., Chien, S., & DeJong, G. (1994). Improving Learning Performance
Rational Resource Allocation. Proceedings Twelfth National Conference
Artificial Intelligence, pp. 576{582 Seattle, WA. AAAI.
Greiner, R., & Jurisica, I. (1992). Statistical Approach Solving EBL Utility
Problem. Proceedings Tenth National Conference Artificial Intelligence,
pp. 241{248 San Jose, CA. AAAI.
Haseeb, R. M. (1985). Modern Statistical Selection. American Sciences Press, Columbus,
OH.
Howard, R. A. (1970). Decision Analysis: Perspectives Inference, Decision, Experimentation. Proceedings IEEE, 58 (5), 823{834.
Kaelbling, L. P. (1993). Learning Embedded Systems. MIT Press, Cambridge, MA.
Minton, S. (1988). Learning Search Control Knowledge: Explanation-Based Approach.
Kluwer Academic Publishers, Norwell, MA.
Moore, A. W., & Lee, M. S. (1994). Ecient Algorithms Minimizing Cross-Validation
Error. Proceedings International Conference Machine Learning New
Brunswick, MA.
Musick, R., Catlett, J., & Russell, S. (1993). Decision Theoretic Subsampling Induction Large Databases. Proceedings International Conference Machine
Learning, pp. 212{219 Amherst, MA.
Rivest, R. L., & Sloan, R. (1988). New Model Inductive Inference. Proceedings
Second Conference Theoretical Aspects Reasoning Knowledge.
Russell, S., & Wefald, E. (1992). Right Thing: Studies Limited Rationality. MIT
Press, Cambridge, MA.
Tadepalli, P. (1992). theory unsupervised speedup learning. Proc. Tenth
National Conference Artificial Intelligence, pp. 229{234 San Jose, CA. AAAI.
Turnbull, B. W., & Weiss, L. I. (1984). Class Sequential Procedures k-sample
Problems Concerning Normal Means Unknown Equal Variances. Santner,
T. J., & Tamhane, A. C. (Eds.), Design Experiments: Ranking Selection, pp.
225{240. Marcel Dekker.
396

fiEfficient Heuristic Hypothesis Ranking
Valiant, L. G. (1984). Theory Learnable. Communications ACM, 27,
1134{1142.

397


