Journal Artificial Intelligence Research 10 (1999) 291-322

Submitted 10/98; published 5/99

Variational Probabilistic Inference
QMR-DT Network

Tommi S. Jaakkola

tommi@ai.mit.edu

Artificial Intelligence Laboratory,
Massachusetts Institute Technology,
Cambridge, 02139 USA

Michael I. Jordan

Computer Science Division Department Statistics,
University California,
Berkeley, CA 94720-1776 USA

jordan@cs.berkeley.edu

Abstract

describe variational approximation method ecient inference large-scale
probabilistic models. Variational methods deterministic procedures provide approximations marginal conditional probabilities interest. provide alternatives approximate inference methods based stochastic sampling search. describe
variational approach problem diagnostic inference \Quick Medical Reference" (QMR) network. QMR network large-scale probabilistic graphical model
built statistical expert knowledge. Exact probabilistic inference infeasible
model small set cases. evaluate variational inference algorithm
large set diagnostic test cases, comparing algorithm state-of-the-art stochastic
sampling method.

1. Introduction
Probabilistic models become increasingly prevalent AI recent years. Beyond
significant representational advantages probability theory, including guarantees
consistency naturalness combining diverse sources knowledge (Pearl, 1988),
discovery general exact inference algorithms principally responsible
rapid growth probabilistic AI (see, e.g., Lauritzen & Spiegelhalter, 1988; Pearl, 1988;
Shenoy, 1992). exact inference methods greatly expand range models
treated within probabilistic framework provide unifying perspective
general problem probabilistic computation graphical models.
Probability theory viewed combinatorial calculus instructs us
merge probabilities sets events probabilities composites. key operation marginalization, involves summing (or integrating) values
variables. Exact inference algorithms essentially find ways perform sums
possible marginalization operations. terms graphical representation
probability distributions|in random variables correspond nodes conditional
independencies expressed missing edges nodes|exact inference algorithms
define notion \locality" (for example cliques appropriately defined graph),
attempt restrict summation operators locally defined sets nodes.
c 1999 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiJaakkola & Jordan

approach manages stave exponential explosion exact probabilistic
computation, exponential explosion inevitable calculus explicitly
performs summations sets nodes. is, models interest
\local" overly large (see Jordan, et al., press). point view, perhaps
surprising exact inference NP-hard (Cooper, 1990).
paper discuss inference problem particular large-scale graphical
model, Quick Medical Reference (QMR) model.1 QMR model consists combination statistical expert knowledge approximately 600 significant diseases
approximately 4000 findings. probabilistic formulation model (the QMR-DT),
diseases findings arranged bi-partite graph, diagnosis problem
infer probability distribution diseases given subset findings. Given
finding generally relevant wide variety diseases, graph underlying
QMR-DT dense, ecting high-order stochastic dependencies. computational complexity treating dependencies exactly characterized terms size
maximal clique \moralized" graph (see, e.g., Dechter, 1998; Lauritzen & Spiegelhalter, 1988). particular, running time exponential measure size.
QMR-DT, considering standardized \clinocopathologic conference" (CPC) cases
discuss below, find median size maximal clique moralized graph
151.5 nodes. rules use general exact algorithms QMR-DT.
general algorithms take advantage particular parametric form
probability distributions nodes graph, conceivable additional
factorizations might found take advantage particular choice made
QMR-DT. factorization fact found Heckerman (1989); \Quickscore
algorithm" provides exact inference algorithm tailored QMR-DT. Unfortunately, however, run time algorithm still exponential number positive
findings. CPC cases, estimate algorithm would require average
50 years solve inference problem current computers.
Faced apparent infeasibility exact inference large-scale models
QMR-DT, many researchers investigated approximation methods. One general
approach developing approximate algorithms perform exact inference,
partially. One consider partial sets node instantiations, partial sets hypotheses,
partial sets nodes. point view led development algorithms
approximate inference based heuristic search. Another approach developing approximation algorithms exploit averaging phenomena dense graphs. particular, laws
large numbers tell us sums random variables behave simply, converging
predictable numerical results. Thus, may need perform sums explicitly, either
exactly partially. point view leads variational approach approximate
inference. Finally, yet another approach approximate inference based stochastic
sampling. One sample simplified distributions obtain information
complex distribution interest. discuss methods turn.
Horvitz, Suermondt Cooper (1991) developed partial evaluation algorithm
known \bounded conditioning" works considering partial sets node instan1. acronym \QMR-DT" use paper refers \decision-theoretic" reformulation
QMR Shwe, et al. (1991). Shwe, et al. replaced heuristic representation employed
original QMR model (Miller, Fasarie, & Myers, 1986) probabilistic representation.

292

fiVariational Probabilistic Inference QMR-DT

tiations. algorithm based notion \cutset"; subset nodes whose
removal renders remaining graph singly-connected. Ecient exact algorithms exist
singly-connected graphs (Pearl, 1988). Summing instantiations cutset, one
calculate posterior probabilities general graphs using ecient algorithm
subroutine. Unfortunately, however, exponentially many cutset instantiations. bounded conditioning algorithm aims forestalling exponential growth
considering partial sets instantiations. Although algorithm promise graphs
\nearly singly-connected," seems unlikely provide solution dense graphs
QMR-DT. particular, median cutset size QMR-DT across
CPC cases 106.5, yielding unmanageably large number 2106:5 cutset instantiations.
Another approach approximate inference provided \search-based" methods,
consider node instantiations across entire graph (Cooper, 1985; Henrion, 1991;
Peng & Reggia, 1987). general hope methods relatively small fraction
(exponentially many) node instantiations contains majority probability mass,
exploring high probability instantiations (and bounding unexplored
probability mass) one obtain reasonable bounds posterior probabilities. QMRDT search space huge, containing approximately 2600 disease hypotheses. If, however,
one considers cases small number diseases, hypotheses involving
small number diseases contain high probability posteriors, may
possible search significant fraction relevant portions hypothesis space.
Henrion (1991) fact able run search-based algorithm QMR-DT inference
problem, set cases characterized small number diseases. cases,
however, exact Quickscore algorithm ecient. general corpus
CPC cases discuss current paper characterized small number
diseases per case. general, even impose assumption patients limited
number N diseases, cannot assume priori model show sharp cutoff
posterior probability disease N . Finally, high-dimensional search problems
often necessary allow paths limited target hypothesis subspace;
particular, one would able arrive hypothesis containing diseases
pruning hypotheses containing additional diseases (Peng & Reggia, 1987). Imposing
limitation lead failure search.
recent partial evaluation methods include \localized partial evaluation" method
Draper Hanks (1994), \incremental SPI" algorithm D'Ambrosio (1993),
\probabilistic partial evaluation" method Poole (1997), \mini-buckets" algorithm
Dechter (1997). former algorithm considers partial sets nodes, latter three
consider partial evaluations sums emerge exact inference run.
promising methods, partial evaluation methods yet clear
restrict exponential growth complexity ways yield realistic accuracy/time
tradeoffs large-scale models QMR-DT.2
Variational methods provide alternative approach approximate inference.
similar spirit partial evaluation methods (in particular incremental SPI
mini-buckets algorithms), aim avoid performing sums exponentially
2. D'Ambrosio (1994) reports \mixed" results using incremental SPI QMR-DT, somewhat
dicult set cases Heckerman (1989) Henrion (1991), still restricted number
positive findings.

293

fiJaakkola & Jordan

many summands, come problem different point view.
variational point view, sum avoided contains sucient number terms
law large numbers invoked. variational approach inference
replaces quantities expected beneficiary averaging process
surrogates known \variational parameters." inference algorithm manipulates
parameters directly order find good approximation marginal probability
interest. QMR-DT model turns particularly appealing architecture
development variational methods. show, variational methods simple
graphical interpretation case QMR-DT.
final class methods performing approximate inference stochastic sampling methods. Stochastic sampling large family, including techniques rejection
sampling, importance sampling, Markov chain Monte Carlo methods (MacKay, 1998).
Many methods applied problem approximate probabilistic inference graphical models analytic results available (Dagum & Horvitz, 1993).
particular, Shwe Cooper (1991) proposed stochastic sampling method known
\likelihood-weighted sampling" QMR-DT model. results promising results date inference QMR-DT|they able produce reasonably
accurate approximations reasonable time two dicult CPC cases. consider
Shwe Cooper algorithm later paper; particular compare algorithm
empirically variational algorithm across entire corpus CPC cases.
Although important compare approximation methods, emphasized
outset think goal identify single champion
approximate inference technique. Rather, different methods exploit different structural
features large-scale probability models, expect optimal solutions involve
combination methods. return point discussion section,
consider various promising hybrids approximate exact inference algorithms.
general problem approximate inference NP-hard (Dagum & Luby, 1993)
provides additional reason doubt existence single champion approximate
inference technique. think important stress, however, hardness result,
together Cooper's (1990) hardness result exact inference cited above,
taken suggest exact inference approximate inference \equally hard."
take example related field, exist large domains solid uid mechanics
exact solutions infeasible approximate techniques (finite element
methods) work well. Similarly, statistical physics, models exactly solvable,
exist approximate methods (mean field methods, renormalization group methods)
work well many cases. feel goal research probabilistic inference
similarly identifying effective approximate techniques work well
large classes problems.

2. QMR-DT Network
QMR-DT network (Shwe et al., 1991) two-level bi-partite graphical model (see
Figure 1). top level graph contains nodes diseases , bottom level
contains nodes findings .
294

fiVariational Probabilistic Inference QMR-DT

number conditional independence assumptions ected bi-partite
graphical structure. particular, diseases assumed marginally independent.
(I.e., independent absence findings. Note diseases assumed
mutually exclusive; patient multiple diseases). Also, given states
disease nodes, findings assumed conditionally independent. (For
discussion regarding medical validity diagnostic consequences
assumptions embedded QMR-DT belief network, see Shwe et al., 1991).
diseases

d1

f1

dn

fm
findings

Figure 1: QMR belief network two-level graph dependencies
diseases associated findings modeled via noisy-OR gates.
state precisely probability model implied QMR-DT model, write
joint probability diseases findings as:

P (f; d) = P (f jd)P (d) =

"





3
#2

P (fi jd) 4 P (dj )5

j

(1)

f binary (1/0) vectors referring presence/absence states diseases
positive/negative states outcomes findings, respectively. conditional
probabilities P (fi jd) represented \noisy-OR model" (Pearl, 1988):

P (fi = 0jd) = P (fi = 0jL) P (fi = 0jdj )
(2)
= (1 , qi0 )





j 2i

(1 , qij )dj

j 2
P
,
i0 , j2i ij dj
e
;

(3)

(4)
set diseases parents finding QMR graph, qij =
P (fi = 1jdj = 1) probability disease j , present, could alone cause
finding positive outcome, qi0 = P (fi = 1jL) \leak" probability, i.e.,
probability finding caused means diseases included
QMR model. final line, reparameterize noisy-OR probability model
using exponentiated notation. notation, model parameters given
ij = , log(1 , qij ).
295

fiJaakkola & Jordan

3. Inference
Carrying diagnostic inference QMR model involves computing posterior
marginal probabilities diseases given set observed positive (fi = 1) negative
(fi0 = 0) findings. Note set observed findings considerably smaller set
possible findings; note moreover (from bi-partite structure QMR-DT graph)
unobserved findings effect posterior probabilities diseases.
brevity adopt notation fi+ corresponds event = 1, fi, refers
= 0 (positive negative findings respectively). Thus posterior probabilities
interest P (dj jf + ; f , ), f + f , vectors positive negative findings.
negative findings f , benign respect inference problem|they
incorporated posterior probability linear time number associated diseases
number negative findings. discuss below, seen
fact probability negative finding Eq. (4) exponential expression
linear dj . positive findings, hand, problematic.
worst case exact calculation posterior probabilities exponentially costly
number positive findings (Heckerman, 1989; D'Ambrosio, 1994). Moreover, practical
diagnostic situations number positive findings often exceeds feasible limit
exact calculations.
Let us consider inference calculations detail. find posterior probability
P (djf + ; f ,), first absorb evidence negative findings, i.e., compute P (djf ,).
P (f , jd)P (d) normalization. Since P (f , jd) P (d) factorize
diseases (see Eq. (1) Eq. (2) above), posterior P (djf , ) must factorize well.
normalization P (f , jd)P (d) therefore reduces independent normalizations
disease carried time linear number diseases (or negative
findings). remainder paper, concentrate solely positive findings
pose real computational challenge. Unless otherwise stated, assume
prior distribution diseases already contains evidence negative findings.
words, presume updates P (dj ) P (dj jf , ) already made.
turn question computing P (dj jf + ), posterior marginal probability
based positive findings. Formally, obtaining posterior involves marginalizing
P (f + jd)P (d) across remaining diseases:

P (dj jf + ) /

X

dndj

P (f + jd)P (d)

(5)

summation possible configurations disease variables
dj (we use shorthand summation index n dj this). QMR model
P (f + jd)P (d) form:

P (f + jd)P (d) =
=

"
"





3
#2

P (fi+ jd) 4 P (dj )5

j

3
2
#
1 , e ,i0 , j ij dj 4 P (dj )5





(6)

P

j

296

(7)

fiVariational Probabilistic Inference QMR-DT

follows Eq. (4) fact P (fi+ jd) = 1 , P (f , jd). perform
summation Eq. (5) diseases, would multiply terms 1 , efg
corresponding conditional probabilities positive finding. number
terms exponential number positive findings. algorithms exist
attempt find exploit factorizations expression, based particular pattern
observed evidence (cf. Heckerman, 1989; D'Ambrosio, 1994), algorithms limited
roughly 20 positive findings current computers. seems unlikely sucient
latent factorization QMR-DT model able handle full CPC corpus,
median number 36 positive findings per case maximum number 61 positive
findings.

4. Variational Methods
Exact inference algorithms perform many millions arithmetic operations applied
complex graphical models QMR-DT. proliferation terms expresses
symbolic structure model, necessarily express numeric structure
model. particular, many sums QMR-DT inference problem sums
large numbers random variables. Laws large numbers suggest sums
may yield predictable numerical results ensemble summands, fact
might enable us avoid performing sums explicitly.
exploit possibility numerical regularity dense graphical models develop
variational approach approximate probabilistic inference. Variational methods
general class approximation techniques wide application throughout applied mathematics. Variational methods particularly useful applied highly-coupled systems. introducing additional parameters, known \variational parameters"|which
essentially serve low-dimensional surrogates high-dimensional couplings
system|these methods achieve decoupling system. mathematical machinery
variational approach provides algorithms finding values variational parameters decoupled system good approximation original coupled
system.
case probabilistic graphical models variational methods allow us simplify
complicated joint distribution one Eq. (7). achieved via parameterized transformations individual node probabilities. see later, node
transformations interpreted graphically delinking nodes graph.
find appropriate transformations? variational methods consider
come convex analysis (see Appendix 6). Let us begin considering methods
obtaining upper bounds probabilities. well-known fact convex analysis
concave function represented solution minimization problem:

f (x) = min
f x , f ( ) g


(8)

f ( ) conjugate function f (x). function f ( ) obtained
solution minimization problem:

f ( ) = min
x f x , f (x) g:

297

(9)

fiJaakkola & Jordan

formal identity pair minimization problems expresses \duality" f
conjugate f .
representation f Eq. (8) known variational transformation. parameter known variational parameter. relax minimization fix
variational parameter arbitrary value, obtain upper bound:

f (x) x , f ( ):

(10)

bound better values variational parameter others,
particular value bound exact.
want obtain lower bounds conditional probabilities. straightforward
way obtain lower bounds appeal conjugate duality express functions terms maximization principle. representation, however, applies convex
functions|in current paper require lower bounds concave functions. concave functions, however, special form allows us exploit conjugatePduality
different way. particular, require bounds functions form f (a + j zj ),
f concave function, zj 2 f1; 2; : : :; ng non-negative variables,
constant. variables zj expression effectively coupled|the impact
changing one variable contingent settings remaining variables. use
Jensen's inequality, however, obtain lower bound variables decoupled.3
particular:

f( +

X

j

qj zqj )
j
j
X

qj f ( + zqj )
j
j

zj ) = f ( +

X

(11)
(12)

qj viewed defining probability distribution variables zj .
variational parameter case theP probability distribution q . optimal setting
parameter given qj = zj = k zk . easily verified substitution
Eq. (12), demonstrates lower bound tight.

4.1 Variational Upper Lower Bounds Noisy-OR

Let us return problem computing posterior probabilities QMR
model. Recall conditional probabilities corresponding positive findings
need simplified. end, write
P

P (fi+ jd) = 1 , e ,i0 ,

j ij dj

= e log(1,e,x )

(13)

P

x = i0 + j ij dj . Consider exponent f (x) = log(1 , e,x ). noisy-OR,
well many conditional models involving compact representations (e.g., logistic
regression), exponent f (x) concave function x. Based discussion
P

P

3. Jensen's inequality, states f (a + j qj xj ) j qj f (a + xj ), concave
f ,
P
0 qj 1, simple consequence Eq. (8), x taken + j qj xj .

298

P

qj

= 1,

fiVariational Probabilistic Inference QMR-DT

previous section, know must exist variational upper bound function
linear x:

f (x) x , f ()

(14)

Using Eq. (9) evaluate conjugate function f ( ) noisy-OR, obtain:

f () = , log + ( + 1) log( + 1)

(15)

desired
bound obtained substituting Eq. (13) (and recalling definition
x = i0 + Pj ij dj ):

P (fi+ jd)

=




P

e f (i0 + Pj ij dj )

e (i0+ j ij dj ),f (i)
P (fi+ jd; i):

(16)
(17)
(18)

Note \variational evidence" P (fi+ jd; i) exponential term linear
disease vector d. negative findings, implies variational
evidence incorporated posterior time linear number diseases
associated finding.
graphical way understand effect transformation. rewrite
variational evidence follows:
P



P (fi+jd; i) = e i(i0 + j ij dj ),f (i)
id
Yh
= e i0 ,f (i) e iij j :
j

(19)
(20)

Note first term constant, note moreover product factorized
across diseases. latter factors multiplied pre-existing
prior corresponding disease (possibly modulated factors negative
evidence). constant term viewed associated delinked finding node .
Indeed, effect variational transformation delink finding node
graph, altering priors disease nodes connected finding node.
graphical perspective important presentation variational algorithm|
able view variational transformations simplifying graph point
exact methods run.
turn
lower bounds conditional probabilities P (fi+ jd). expoP
nent f (i0 + j ij dj ) exponential representation form applied
Jensen's inequality previous section. Indeed, since f concave need identify
non-negative variables zj , case ij dj , constant a,
i0. Applying bound Eq. (12) have:

P (fi+ jd)

=

P

e f ( i0 + j ij dj )

e

ij dj
j qjji f io + qjji

P



299

(21)
(22)

fiJaakkola & Jordan

= e
= e

h

P





ij
j qjji dj f io + qjji +(1,dj ) f ( io )




ij
j qjji dj f io + qjji ,f ( io ) +f ( io )

P

h

(23)

(24)

(25)
allowed different variational distribution qji finding. Note
bound linear exponent. case upper bound,
implies variational evidence incorporated posterior distribution
time linear number diseases. Moreover, view variational
transformation terms delinking finding node graph.

P (fi+ jd; qji)

4.2 Approximate Inference QMR

previous section described variational transformations derived individual findings QMR model; discuss utilize transformations
context overall inference algorithm.
Conceptually overall approach straightforward. transformation involves
replacing exact conditional probability finding lower bound upper
bound:
P (fi+ jd; qji) P (fi+ jd) P (fi+ jd; i):
(26)
Given transformations viewed delinking ith finding node
graph, see transformations yield bounds, yield simplified graphical structure. imagine introducing transformations sequentially
graph sparse enough exact methods become feasible. point stop
introducing transformations run exact algorithm.
problem approach, however. need decide step
node transform, requires assessment effect overall accuracy
transforming node. might imagine calculating change probability interest
given transformation, choosing transform node
yields least change target probability. Unfortunately unable calculate
probabilities original untransformed graph, thus unable assess effect
transforming one node. unable get algorithm started.
Suppose instead work backwards. is, introduce transformations
findings, reducing graph entirely decoupled set nodes. optimize
variational parameters fully transformed graph (more optimization
variational parameters below). graph inference trivial. Moreover, easy
calculate effect reinstating single exact conditional one node: choose
reinstate node yields change.
Consider particular case upper bounds (lower bounds analogous).
transformation introduces upper bound conditional probability P (fi+ jd). Thus
likelihood observing (positive) findings P (f + ) upper bounded variational
counterpart P (f + j ):
X
X
P (f + ) = P (f + jd)P (d) P (f + jd; )P (d) P (f + j )
(27)




300

fiVariational Probabilistic Inference QMR-DT

assess accuracy variational transformation introducing optimizing variational transformations positive findings. Separately
positive finding replace variationally transformed conditional probability P (fi+ jd; i)
corresponding exact conditional P (fi+ jd) compute difference
resulting bounds likelihood observations:

= P (f + j ) , P (f + j n )

(28)

P (f + j n ) computed without transforming ith positive finding. larger
difference is, worse ith variational transformation is. therefore
introduce transformations ascending order s. Put another way,
treat exactly (not transform) conditional probabilities whose measure large.
practice, intelligent method ordering transformations critical. Figure 2
compares calculation likelihoods based measure opposed method
chooses ordering transformations random. plot corresponds representative diagnostic case, shows upper bounds log-likelihoods observed
findings function number conditional probabilities left intact (i.e.
transformed). Note upper bound must improve (decrease) fewer transformations. results striking|the choice ordering large effect accuracy
(note plot log-scale).
30

loglikelihood

35
40
45
50
55
60
0

2

4
6
8
10
# exactly treated findings

12

Figure 2: upper bound log-likelihood delta method removing transformations (solid line) method bases choice random ordering
(dashed line).
Note curve proposed ranking convex; thus bound improves
less fewer transformations left. first remove worst
transformations, replacing exact conditionals. remaining transformations better indicated delta measure thus bound improves less
replacements.
make claims optimality delta method; simply useful heuristic
allows us choose ordering variational transformations computationally
ecient way. Note implementation method optimizes variational
parameters outset chooses ordering transformations
based fixed parameters. parameters suboptimal graphs
301

fiJaakkola & Jordan

substantial numbers nodes reinstated, found practice
simplified algorithm still produces reasonable orderings.
decided nodes reinstate, approximate inference algorithm
run. introduce transformations nodes left transformed
ordering algorithm. product exact conditional probabilities graph
transformed conditional probabilities yields upper lower bound overall
joint probability associated graph (the product bounds bound). Sums
bounds still bounds, thus likelihood (the marginal probability findings)
bounded summing across bounds joint probability. particular, upper
bound likelihood obtained via:
X
X
P (f + ) = P (f + jd)P (d) P (f + jd; )P (d) P (f + j )
(29)








dndj

dndj

corresponding lower bound likelihood obtained similarly:
X
X
P (f + ) = P (f +jd)P (d) P (f + jd; q )P (d) P (f + jq )

(30)

cases assume graph suciently simplified variational
transformations sums performed eciently.
expressions Eq. (29) Eq. (30) yield upper lower bounds arbitrary
values variational parameters q . wish obtain tightest possible bounds,
thus optimize expressions respect q . minimize respect
maximize respect q. Appendix 6 discusses optimization problems
detail. turns upper bound convex thus adjustment
variational parameters upper bound reduces convex optimization problem
carried eciently reliably (there local minima). lower bound
turns maximization carried via EM algorithm.
Finally, although bounds likelihood useful, ultimate goal approximate
marginal posterior probabilities P (dj jf + ). two basic approaches utilizing
variational bounds Eq. (29) Eq. (30) purpose. first method,
emphasis current paper, involves using transformed probability model (the
model based either upper lower bounds) computationally ecient surrogate
original probability model. is, tune variational parameters transformed
model requiring model give tightest possible bound likelihood.
use tuned transformed model inference engine provide approximations
probabilities interest, particular marginal posterior probabilities P (dj jf + ).
approximations found manner bounds, computationally ecient
approximations. provide empirical data following section show
approach indeed yields good approximations marginal posteriors QMR-DT
network.
ambitious goal obtain interval bounds marginal posterior probabilities themselves. end, let P (f + ; dj j ) denote combined event QMR-DT
model generates observed findings f + j th disease takes value dj .
bounds follow directly from:
X
X
P (f + ; dj ) = P (f + jd)P (d) P (f + jd; )P (d) P (f + ; dj j)
(31)
302

fiVariational Probabilistic Inference QMR-DT

P (f + jd; ) product upper-bound transformed conditional probabilities
exact (untransformed) conditionals. Analogously compute lower bound P (f + ; dj jq )
applying lower bound transformations:

P (f + ; dj ) =

X

dndj

P (f +jd)P (d)

X

dndj

P (f + jd; q )P (d) P (f + ; dj jq )

(32)

Combining bounds obtain interval bounds posterior marginal probabilities diseases (cf. Draper & Hanks 1994):

P (f +; dj j)
P (f + ; dj jq)
+)

P
(

j
f
j
P (f +; dj j) + P (f + ; dj jq )
P (f + ; dj j ) + P (f + ; dj jq) ;
dj binary complement dj .

(33)

5. Experimental Evaluation

diagnostic cases used evaluating performance variational techniques cases abstracted clinocopathologic conference (\CPC") cases. cases
generally involve multiple diseases considered clinically dicult cases.
cases Middleton et al. (1990) find importance sampling method
work satisfactorily.
evaluation variational methodology consists three parts. first part
exploit fact subset CPC cases (4 48 cases)
suciently small number positive findings calculate exact values
posterior marginals using Quickscore algorithm. is, four cases
able obtain \gold standard" comparison. provide assessment accuracy
eciency variational methods four CPC cases. present variational
upper lower bounds likelihood well scatterplots compare variational
approximations posterior marginals exact values. present comparisons
likelihood-weighted sampler Shwe Cooper (1991).
second section present results remaining, intractable CPC cases.
use lengthy runs Shwe Cooper sampling algorithm provide surrogate
gold standard cases.
Finally, third section consider problem obtaining interval bounds
posterior marginals.

5.1 Comparison Exact Marginals

Four CPC cases 20 fewer positive findings (see Table 1), cases
possible calculate exact values likelihood posterior marginals
reasonable amount time. used Heckerman's \Quickscore" algorithm (Heckerman 1989)|an algorithm tailored QMR-DT architecture|to perform exact
calculations.
Figure 3 shows log-likelihood four tractable CPC cases. figure shows
variational lower upper bounds. calculated variational bounds twice,
differing numbers positive findings treated exactly two cases (\treated exactly"
303

fiJaakkola & Jordan

case # pos. findings # neg. findings
1
20
14
2
10
21
3
19
19
4
19
33

20

20

30

30

40

loglikelihood

loglikelihood

Table 1: Description cases evaluated exact posterior marginals.

50

40

50

60
60
70
70

(a)

0

1

2
3
sorted cases

4

(b)

5

0

1

2
3
sorted cases

4

5

Figure 3: Exact values variational upper lower bounds log-likelihood
log P (f + j ) four tractable CPC cases. (a) 8 positive findings
treated exactly, (b) 12 positive findings treated exactly.
simply means finding transformed variationally). panel (a) 8
positive findings treated exactly, (b) 12 positive findings treated exactly.
expected, bounds tighter positive findings treated exactly.4
average running time across four tractable CPC cases 26.9 seconds
exact method, 0.11 seconds variational method 8 positive findings treated
exactly, 0.85 seconds variational method 12 positive findings treated exactly.
(These results obtained 433 MHz DEC Alpha computer).
Although likelihood important quantity approximate (particularly applications parameters need estimated), interest QMR-DT setting
posterior marginal probabilities individual diseases. discussed
previous section, simplest approach obtaining variational estimates quantities define approximate variational distribution based either distribution
P (f + j ), upper-bounds likelihood, distribution P (f + jq ), lowerbounds likelihood. fixed values variational parameters (chosen provide
tight bound likelihood), distributions provide partially factorized approximations joint probability distribution. factorized forms exploited
4. Given significant fraction positive findings treated exactly simulations, one
may wonder additional accuracy due variational transformations. address
concern later section demonstrate variational transformations fact responsible
significant portion accuracy cases.

304

fiVariational Probabilistic Inference QMR-DT

1

1

0.8

0.8
variational estimates

variational estimates

ecient approximate inference engines general posterior probabilities, particular
use provide approximations posterior marginals individual diseases.
practice found distribution P (f + j ) yielded accurate posterior
marginals distribution P (f + jq ), restrict presentation P (f + j ). Figure 4 displays scatterplot approximate posterior marginals, panel (a) corre-

0.6

0.4

0.2

(a)

0
0

0.6

0.4

0.2

0.2

0.4
0.6
exact marginals

0.8

(b)

1

0
0

0.2

0.4
0.6
exact marginals

0.8

1

Figure 4: Scatterplot variational posterior estimates exact marginals.
(a) 8 positive findings treated exactly (b) 12 positive findings
treated exactly.
sponding case 8 positive findings treated exactly panel (b) case
12 positive findings treated exactly. plots obtained first extracting
50 highest posterior marginals case using exact methods computing
approximate posterior marginals corresponding diseases. approximate
marginals fact correct points figures align along diagonals
shown dotted lines. see reasonably good correspondence|the variational
algorithm appears provide good approximation largest posterior marginals. (We
quantify correspondence ranking measure later section).
current state-of-the-art algorithm QMR-DT enhanced version likelihoodweighted sampling proposed Shwe Cooper (1991). Likelihood-weighted sampling
stochastic sampling method proposed Fung Chang (1990) Shachter Peot
(1990). Likelihood-weighted sampling basically simple forward sampling method
weights samples likelihoods. enhanced improved utilizing \selfimportance sampling" (see Shachter & Peot, 1990), version importance sampling
importance sampling distribution continually updated ect current
estimated posterior distribution. Middleton et al. (1990) utilized likelihood-weighted sampling self-importance sampling (as well heuristic initialization scheme known
\iterative tabular Bayes") QMR-DT model found work satisfactorily. Subsequent work Shwe Cooper (1991), however, used additional
enhancement algorithm known `Markov blanket scoring" (see Shachter & Peot,
1990), distributes fractions samples positive negative values node
proportion probability values conditioned Markov blanket
node. combination Markov blanket scoring self-importance sampling yielded
305

fiJaakkola & Jordan

effective algorithm.5 particular, modifications place, Shwe Cooper
reported reasonable accuracy two dicult CPC cases.
re-implemented likelihood-weighted sampling algorithm Shwe Cooper,
incorporating Markov blanket scoring heuristic self-importance sampling. (We
utilize \iterative tabular Bayes" instead utilized related initialization scheme{
\heuristic tabular Bayes"{also discussed Shwe Cooper). section discuss
results running algorithm four tractable CPC cases, comparing
results variational inference.6 following section present fuller comparative
analysis two algorithms CPC cases.
Likelihood-weighting sampling, indeed sampling algorithm, realizes timeaccuracy tradeoff|taking additional samples requires time improves accuracy.
comparing sampling algorithm variational algorithm, ran sampling
algorithm several different total time periods, accuracy achieved
sampling algorithm roughly covered range achieved variational algorithm.
results shown Figure 5, right-hand curve corresponding sampling runs.
figure displays mean correlations approximate exact posterior
marginals across ten independent runs algorithm (for four tractable CPC cases).
1

mean correlation

0.98
0.96
0.94
0.92
0.9
0.88
0.86 1
10

0

1

10
10
execution time seconds

2

10

Figure 5: mean correlation approximate exact posterior marginals
function execution time (in seconds). Solid line: variational estimates;
dashed line: likelihood-weighting sampling. lines sampling result represent standard errors mean based ten independent
runs sampler.
Variational algorithms characterized time-accuracy tradeoff. particular,
accuracy method generally improves findings treated exactly,
cost additional computation. Figure 5 shows results variational
algorithm (the left-hand curve). three points curve correspond 8, 12
5. initialization method proved little effect inference results.
6. investigated Gibbs sampling (Pearl, 1988). results Gibbs sampling good
results likelihood-weighted sampling, report latter results remainder
paper.

306

fiVariational Probabilistic Inference QMR-DT

16 positive findings treated exactly. Note variational estimates deterministic
thus single run made.
figure shows achieve roughly equivalent levels accuracy, sampling
algorithm requires significantly computation time variational method.
Although scatterplots correlation measures provide rough indication accuracy approximation algorithm, deficient several respects. particular,
diagnostic practice interest ability algorithm rank diseases correctly,
avoid false positives (diseases fact significant included
set highly ranked diseases) false negatives (significant diseases omitted set highly ranked diseases). defined ranking measure follows (see
Middleton et al., 1990). Consider set N highest ranking disease hypotheses,
ranking based correct posterior marginals. Corresponding set
diseases find smallest set N 0 approximately ranked diseases includes
N significant ones. words, N \true positives" approximate method
produces N 0 , N \false positives." Plotting false positives function true positives
provides meaningful useful measure accuracy approximation scheme.
extent method provides nearly correct ranking true positives plot
increases slowly area curve small. significant disease appears
late approximate ordering plot increases rapidly near true rank missed
disease area curve large.
plot number \false negatives" set top N highly ranked diseases.
False negatives refer number diseases, N highest ranking diseases,
appear set N approximately ranked diseases. Note unlike
previous measure, measure reveal severity misplacements,
frequency.
improved diagnostic measure hand, let us return evaluation
inference algorithms, beginning variational algorithm. Figure 6 provides plots
60

7

50

6

false negatives

false positives

5
40
30
20

4
3
2

10

(a)

0
0

1

10

20
30
true positives

40

(b)

50

0
0

10

20
30
approximate ranking

40

50

Figure 6: (a) Average number false positives function true positives variational method (solid lines) partially-exact method (dashed line). (b) False
negatives set top N approximately ranked diseases. figures 8
positive findings treated exactly.
false positives (panel a) false negatives (panel b) true positives
307

fiJaakkola & Jordan

40

4

35

3.5

(a)

3
false negatives

false positives

30
25
20
15

2.5
2
1.5

10

1

5

0.5

0
0

10

20
30
true positives

40

(b)

50

0
0

10

20
30
approximate ranking

40

50

Figure 7: (a) Average number false positives function true positives variational method (solid line) partially-exact method (dashed line). (b) False
negatives set top N approximately ranked diseases. figures 12
positive findings treated exactly.
tractable CPC cases. Eight positive findings treated exactly simulation shown
figure. Figure 7 displays results 12 positive finding treated exactly.
noted earlier, 8 12 positive findings comprise significant fraction
total positive findings tractable CPC cases, thus important verify
variational transformations fact contributing accuracy posterior
approximations beyond exact calculations. comparing
variational method method call \partially-exact" method
posterior probabilities obtained using findings treated exactly
variational calculations (i.e., using findings transformed).
variational transformations contribute accuracy approximation,
performance partially-exact method comparable
variational method.7 Figure 6 Figure 7 clearly indicate case.
difference accuracy methods substantial computational load
comparable (about 0.1 seconds 433MHz Dec Alpha).
believe accuracy portrayed false positive plots provides good indication potential variational algorithm providing practical solution
approximate inference problem QMR-DT. figures show, number
false positives grows slowly number true positives. example, shown
Figure 6 eight positive findings treated exactly, find 20 likely diseases
would need entertain top 23 diseases list approximately ranked
diseases (compared 50 partially-exact method).
ranking plot likelihood-weighted sampler shown Figure 8,
curve variational method Figure 7 included comparison. make
plots, ran likelihood-weighted sampler amount time (6.15 seconds)
7. noted conservative comparison, partially-exact method fact
benefits variational transformation|the set exactly treated positive findings selected
basis accuracy variational transformations, accuracies correlate
diagnostic relevance findings.

308

fiVariational Probabilistic Inference QMR-DT

40
35

false positives

30
25
20
15
10
5
0
0

10

20
30
true positives

40

50

Figure 8: Average number false positives function true positives likelihoodweighted sampler (dashed line) variational method (solid line) 12
positive findings treated exactly.
comparable time allocated slowest variational method (3.17 seconds;
case 16 positive findings treated exactly. Recall time required
variational algorithm 12 positive findings treated exactly 0.85 seconds.)
plots show, tractable CPC cases, variational method significantly
accurate sampling algorithm comparable computational loads.

5.2 Full CPC Corpus

consider full CPC corpus. majority cases (44 48 cases),
20 positive findings thus appear beyond reach exact methods.
important attraction sampling methods mathematical guarantee accurate
estimates limit suciently large sample size (Gelfand & Smith, 1990). Thus
sampling methods promise providing general methodology approximate
inference, two caveats: (1) number samples needed dicult
diagnosis, (2) many samples may required obtain accurate estimates.
real-time applications, latter issue rule sampling solutions. However, long-term
runs sampler still provide useful baseline evaluation accuracy faster
approximation algorithms. begin considering latter possibility context
likelihood-weighted sampling QMR-DT. turn comparative evaluation
likelihood-weighted sampling variational methods time-limited setting.
explore viability likelihood-weighted sampler providing surrogate
gold standard, carried two independent runs consisting 400,000 samples.
Figure 9(a) shows estimates log-likelihood first sampling run
CPC cases. show variational upper lower bounds cases
(the cases sorted according lower bound). Note bounds
rigorous bounds true log-likelihood, thus provide direct indication
accuracy sampling estimates. Although see many estimates lie
bounds, see many cases sampling estimates deviate substantially
bounds. suggests posterior marginal estimates obtained
samples likely unreliable well. Indeed, Figure 9(b) presents scatterplot
309

fiJaakkola & Jordan

20
1

40
0.8
sampling estimates 2

loglikelihood

60
80
100
120
140
160

0.6

0.4

0.2

180

(a)

200
0

10

20
30
sorted cases

40

50

(b)

0
0

0.2

0.4
0.6
sampling estimates 1

0.8

1

Figure 9: (a) Upper lower bounds (solid lines) corresponding sampling estimates (dashed line) log-likelihood observed findings CPC cases.
(b) correlation plot posterior marginal estimates two independent sampling runs.
estimated posterior marginals two independent runs sampler. Although
see many cases results lie diagonal, indicating agreement
two runs, see many pairs posterior estimates far diagonal.
results cast doubt viability likelihood-weighted sampler
general approximator full set CPC cases. Even problematically appear
without reliable surrogate gold standard cases, making dicult
evaluate accuracy real-time approximations variational method. Note,
however, estimates Figure 9(a) seem fall two classes|estimates
lie within variational bounds estimates rather far bounds.
suggests possibility distribution sampled multi-modal,
estimates falling within correct mode providing good approximations
others falling spurious modes providing seriously inaccurate approximations.
situation holds, accurate surrogate gold standard might obtained using
variational bounds filter sampling results retaining estimates
lie bounds given variational approach.
Figure 10 provides evidence viability approach. 24 48
CPC cases independent runs sampler resulted estimates loglikelihood lying approximately within variational bounds. recomputed posterior
marginal estimates selected cases plotted figure.
scatterplot shows high degree correspondence posterior estimates
cases. thus tentatively assume estimates accurate enough serve
surrogate gold standard proceed evaluate real-time approximations.
Figure 11 plots false positives true positives 24 selected CPC
cases variational method. Twelve positive findings treated exactly
simulation. Obtaining variational estimates took 0.29 seconds computer time per
case. Although curve increases rapidly tractable CPC cases,
variational algorithm still appears provide reasonably accurate ranking posterior
marginals, within reasonable time frame.
310

fiVariational Probabilistic Inference QMR-DT

1

sampling estimates 2

0.8

0.6

0.4

0.2

0
0

0.2

0.4
0.6
sampling estimates 1

0.8

1

Figure 10: correlation plot selected posterior marginal estimates two
independent sampling runs, selection based variational
upper lower bounds.
70
60

false positives

50
40
30
20
10
0
0

10

20
30
true positives

40

50

Figure 11: Average number false positives function true positives variational method (solid line) likelihood-weighted sampler (dashed line).
variational method 12 positive findings treated exactly,
sampler results averages across ten runs.
compare variational algorithm time-limited version likelihood-weighted
sampler ran latter algorithm period time (8.83 seconds per case) roughly comparable running time variational algorithm (0.29 seconds per case). Figure 11
shows corresponding plot false positives true positives, averaged ten independent runs. see curve increases significantly steeply
variational curve. find 20 likely diseases variational method
would need entertain top 30 diseases list approximately ranked
diseases. sampling method would need entertain top 70 approximately
ranked diseases.

5.3 Interval Bounds Marginal Probabilities

Thus far utilized variational approach produce approximations posterior marginals. approximations discussed originate upper lower
311

fiJaakkola & Jordan

bounds likelihood, bounds. is, guaranteed lie true posteriors, see Figure 4. discussed
Section 4.1, however, possible induce upper lower bounds posterior
marginals upper lower bounds likelihood (cf. Eq. 33). section
evaluate interval bounds QMR-DT posterior marginals.
Figure 12 displays histogram interval bounds four tractable CPC cases,
24 selected CPC cases previous section, CPC cases. histograms
include diseases QMR-DT network. case tractable cases
0.8

0.8

0.8

0.6

0.6

0.4

0
0

0.6

0.4

0.2

(a)

Frequency

1

Frequency

1

Frequency

1

0.4

0.2

0.2

0.4

0.6

Interval size

0.8

1

(b)

0
0

0.2

0.2

0.4

0.6

Interval size

0.8

1

(c)

0
0

0.2

0.4

0.6

0.8

Interval size

Figure 12: Histograms size interval bounds diseases QMRDT network (a) four tractable CPC cases, (b) 24 selected CPC cases
previous section, (c) CPC cases.
variational method run 12 positive findings treated exactly. remaining
CPC cases variational method run 16 positive findings treated exactly.
running time algorithm less 10 seconds computer time per CPC case.
tractable CPC cases, interval bounds tight nearly diseases
network. However, (1) positive findings treated variationally
cases, (2) need practice compute variational bounds cases.
get somewhat better picture viability variational interval bounds
Figure 12(b) Figure 12(c), picture decidedly mixed. 24 selected
cases, tight bounds provided approximately half diseases. bounds
vacuous approximately quarter diseases, range diseases
between. consider CPC cases, approximately third bounds
tight nearly half vacuous.
Although results may indicate limitations variational approximation,
another immediate problem appears responsible looseness
bounds many cases. particular, recall use Quickscore algorithm
(Heckerman, 1989) handle exact calculations within framework variational
algorithm. Unfortunately Quickscore suffers vanishing numerical precision large
numbers positive findings, general begin run numerical problems,
resulting vacuous bounds, 16 positive findings incorporated exactly
variational approximation. Thus, although clearly interest run variational
algorithm longer durations, thereby improve bounds, unable
within current implementation exact subroutine.
312

1

fiVariational Probabilistic Inference QMR-DT

clearly worth studying methods Quickscore treating exact findings within variational algorithm, interest consider combining
variational methods methods, search-based partial evaluation
methods, based intervals. methods may help simplifying posterior
obviating need improving exact calculations.
worth emphasizing positive aspect results potential
practical utility. previous section showed variational method provide accurate approximations posterior marginals. Combined interval bounds
section|which calculated eciently|the user obtain guarantees approximately third approximations. Given relatively benign rate increase false
positives function true positives (Figure 11), guarantees may suce. Finally,
diseases bounds loose perturbation methods available
(Jaakkola, 1997) help validate approximations diseases.

6. Discussion
Let us summarize variational inference method evaluate results
obtained.
variational method begins parameterized upper lower bounds individual conditional probabilities nodes model. QMR-DT, bounds
exponentials linear functions, introducing model corresponds
delinking nodes graph. Sums products bounds yield bounds, thus
readily obtain parameterized bounds marginal probabilities, particular upper
lower bounds likelihood.
exploited likelihood bounds evaluating output likelihood-weighted
sampling algorithm. Although sampling algorithm yield reliable results across
corpus CPC cases, utilized variational upper lower bounds select
among samples able obtain sampling results consistent
runs. suggests general procedure variational bounds used assess
convergence sampling algorithm. (One imagine intimate relationship
algorithms variational bounds used adjust on-line
course sampler).
fact bounds likelihood (or marginal probabilities)
critical|the bounding property allows us find optimizing values variational parameters minimizing upper-bounding variational distribution maximizing
lower-bounding variational distribution. case QMR-DT network (a bipartite noisy-OR graph), minimization problem convex optimization problem
maximization problem solved via EM algorithm.
variational parameters optimized, resulting variational distribution
exploited inference engine calculating approximations posterior probabilities.
technique focus paper. Graphically, variationally transformed
model viewed sub-graph original model finding
nodes delinked. sucient number findings delinked variationally
possible run exact algorithm resulting graph. approach yields
approximations posterior marginals disease nodes.
313

fiJaakkola & Jordan

found empirically approximations appeared provide good approximations true posterior marginals. case tractable set CPC cases
(cf. Figure 7) and|subject assumption obtained good surrogate
gold standard via selected output sampler|also case full CPC
corpus (cf. Figure 11).
compared variational algorithm state-of-the-art algorithm QMRDT, likelihood-weighted sampler Shwe Cooper (1991). found variational algorithm outperformed likelihood-weighted sampler tractable cases
full corpus. particular, fixed accuracy requirement variational algorithm significantly faster (cf. Figure 5), fixed time allotment variational
algorithm significantly accurate (cf. Figure 8 Figure 11).
results less satisfactory interval bounds posterior marginals.
Across full CPC corpus found approximately one third disease
bounds tight half diseases bounds vacuous. major impediment
obtaining tighter bounds appears lie variational approximation per se
rather exact subroutine, investigating exact methods improved
numerical properties.
Although focused detail QMR-DT model paper, worth
noting variational probabilistic inference methodology considerably general.
Specifically, methods described limited bi-partite
graphical structure QMR-DT model, necessary employ noisy-OR nodes
(Jaakkola & Jordan, 1996). case type transformations
exploited QMR-DT setting extend larger class dependence relations
based generalized linear models (Jaakkola, 1997). Finally, review applications
variational methods variety graphical model architectures, see Jordan, et al.
(1998).
promising direction future research appears integration various
kinds approximate exact methods (see, e.g., Dagum & Horvitz, 1992; Jensen, Kong,
& Kjrulff, 1995). particular, search-based methods (Cooper, 1985; Peng & Reggia,
1987, Henrion, 1991) variational methods yield bounds probabilities, and,
indicated introduction, seem exploit different aspects structure complex probability distributions. may possible combine bounds
algorithm|the variational bounds might used guide search, searchbased bounds might used aid variational approximation. Similar comments
made respect localized partial evaluation methods bounded conditioning
methods (Draper & Hanks, 1994; Horvitz, et al., 1989). Also, seen variational
bounds used assessing whether estimates Monte Carlo sampling algorithms
converged. interesting hybrid would scheme variational approximations refined treating initial conditions sampler.
Even without extensions results paper appear quite promising.
presented algorithm runs real time large-scale graphical model
exact algorithms general infeasible. results obtained appear
reasonably accurate across corpus dicult diagnostic cases. work
needed, believe results indicate promising role variational inference
developing, critiquing exploiting large-scale probabilistic models QMR-DT.
314

fiVariational Probabilistic Inference QMR-DT

Acknowledgements
would thank University Pittsburgh Randy Miller use
QMR-DT database. want thank David Heckerman suggesting attack
QMR-DT variational methods, providing helpful counsel along way.

Appendix A. Duality
upper lower bounds individual conditional probability distributions form
basis variational method based \dual" \conjugate" representations
convex functions. present brief description convex duality appendix,
refer reader Rockafellar (1970) extensive treatment.
Let f (x) real-valued, convex function defined convex set X (for example,
X = Rn ). simplicity exposition, assume f well-behaved (differentiable)
function. Consider graph f , i.e., points (x; f (x)) n + 1 dimensional space.
fact function f convex translates convexity set f(x; ) : f (x)g
called epigraph f denoted epi(f ) (Figure 13). elementary property
f(x)
epi(f)

x - - f*() 0

x - - f*() 0

x-y-0

Figure 13: Half-spaces containing convex set epi(f ). conjugate function f ( )
defines critical half-spaces whose intersection epi(f ), or, equivalently,
defines tangent planes f (x).
convex sets represented intersection half-spaces
contain (see Figure 13). parameterizing half-spaces obtain dual
representations convex functions. end, define half-space condition:
(x; ) xT , , 0

(34)

parameterize (non-vertical) half-spaces. interested characterizing half-spaces contain epigraph f . require therefore points
epigraph must satisfy half-space condition: (x; ) 2 epi(f ), must
xT , , 0. holds whenever xT , f (x) , 0 points epigraph
property f (x). Since condition must satisfied x 2 X , follows
315

fiJaakkola & Jordan


max
f xT , f (x) , g 0;
x2X

(35)

well. Equivalently,

max
f xT , f (x) g
x2X

(36)

right-hand side equation defines function , known
\dual" \conjugate" function f ( ). function, convex function, defines
critical half-spaces needed representation epi(f ) intersection
half-spaces (Figure 13).
clarify duality f (x) f (x), let us drop maximum rewrite
inequality as:

xT f (x) + f ( )

(37)

equation, roles two functions interchangeable may suspect
f (x) obtained dual function f (x) optimization procedure.
fact case have:

f (x) = max
f xT , f () g
2

(38)

equality states dual dual gives back original function. provides
computational tool calculating dual functions.
concave (convex down) functions results analogous; replace max
min, lower bounds upper bounds.

Appendix B. Optimization Variational Parameters

variational method described involves replacing selected local conditional
probabilities either upper-bounding lower-bounding variational transformations.
product bounds bound, variationally transformed joint probability
distribution bound (upper lower) true joint probability distribution. Moreover, sums bounds bound sum, obtain bounds marginal
probabilities marginalizing variationally transformed joint probability distribution.
particular, provides method obtaining bounds likelihood (the marginal
probability evidence).
Note variationally transformed distributions bounds arbitrary values
variational parameters (because individually transformed node conditional probability bound arbitrary values variational parameter). obtain optimizing
values variational parameters, take advantage fact transformed
distribution bound, either minimize (in case upper bounds) maximize
(in case lower bounds) transformed distribution respect variational
parameters. optimization process provides tight bound marginal
probability interest (e.g., likelihood) thereby picks particular variational
distribution subsequently used approximate inference.
316

fiVariational Probabilistic Inference QMR-DT

appendix discuss optimization problems must solve case
noisy-OR networks. consider upper lower bounds separately, beginning
upper bound.

Upper Bound Transformations

goal isPto compute tight upper bound likelihood observed findings:
P (f + ) = P (f + jd)P (d). discussed Section 4.2, obtain upper bound
P (f + jd) introducing upper bounds individual node conditional probabilities.
represent upper bound P (f + jd; ), product across individual variational transformations may contain contributions due findings treated
exactly (i.e., transformed). Marginalizing across obtain bound:

P (f + )

X



P (f + jd; )P (d) P (f + j):

(39)

latter quantity wish minimize respect variational parameters
.
simplify notation assume first positive findings transformed (and therefore need optimized) remaining conditional probabilities
treated exactly. notation P (f + j ) given

P (f + j )

=

/

2
X
4

3"

#


+
+
P (fi jd; i)5
P (fi jd) P (dj )
i>m
j
im
8
9
<Y
=
E : P (fi+ jd; i); ;
im

(40)
(41)

expectation taken respect posterior distribution diseases
given positive findings plan treat exactly. Note proportionality
constant depend variational parameters (it likelihood exactly
treated positive findings). insert explicit forms transformed conditional
probabilities (see Eq. (17)) Eq. (41) find:

P (f + j) /
=

8
9
< ( +P ),f ( ) =

E : e i0 j ij j
;
im
P

P

e im (i i0 ,f (i)) E e j; im ij dj

(42)
(43)

simply converted products sums exponent pulled
terms constants respect expectation. log-scale,
proportionality becomes equivalence constant:
P

X




ij
j
+

j;i


log P (f j ) = C + (i i0 , f (i)) + log E e
im

317

(44)

fiJaakkola & Jordan

Several observations order. Recall f (i ) conjugate concave function
f (the exponent), therefore concave; reason ,f (i ) convex.
Appendix C prove remaining term:
P

log E e

j;im iij dj



(45)

convex function variational parameters. Now, since sum convex
functions convex, conclude log P (f + j ) convex function variational
parameters. means local minima optimization problem.
may safely employ standard Newton-Raphson procedure solve r log P (f + j ) = 0.
Alternatively utilize fixed-point iterations. particular, calculate derivatives
variational form iteratively solve individual variational parameters k
derivatives zero. derivatives given follows:
8

9

@ log P (f + j) = + log k + E <X =
k0
kj j ;
:
@k
1 + k
j

(46)

@ 2 log P (f + j) = 1 , 1 + Var <X = ;
: j kj j ;
@ 2 k
k 1 + k

(47)

8

9

expectation variance respect posterior approximation
P (djf + ; ), derivatives computed time linear number associated diseases finding. benign scaling variance calculations comes
exploiting special properties noisy-OR dependence marginal independence
diseases.
Calculating expectations Eq. (7) exponentially costly number exactly
treated positive findings. large number positive findings,
recourse simplified procedure optimize variational parameters
transformed positive findings. resulting variational parameters
suboptimal, found practice incurred loss accuracy typically quite
small. simulations reported paper, optimized variational parameters
approximately half exactly treated findings introduced. (To precise,
case 8, 12 16 total findings treated exactly, optimized parameters
4, 8, 8 findings, respectively, introduced).

Lower Bound Transformations

Mimicking case upper bounds, replace individual conditional probabilities
findings lower-bounding transformations, resulting lower-bounding expression
P (f + jd; q). Taking product P (d) marginalizing yields lower bound
likelihood:
X
P (f + ) P (f + jd; q )P (d) P (f + jq):
(48)


wish maximize P (f + jq ) respect variational parameters q obtain
tightest possible bound.
318

fiVariational Probabilistic Inference QMR-DT

problem mapped onto standard optimization problem statistics.
particular, treating latent variable, f observed variable, q parameter
vector, optimization P (f + jq ) (or logarithm) viewed standard maximum
likelihood estimation problem latent variable model. solved using EM
algorithm (Dempster, Laird, & Rubin, 1977). algorithm yields sequence variational
parameters monotonically increase objective function log P (f + jq ). Within EM
framework, obtain update variational parameters maximizing expected
complete log-likelihood:




E log P (f + jd; q )P (d) =

X



n



E log P (fi+ jd; qji) + constant;

(49)

q old denotes vector variational parameters update, constant term independent variational parameters q expectation
respect posterior distribution P (djf + ; q old ) / P (f + jd; q old)P (d). Since variational
parameters associated conditional probabilities P (fi+ jd; qji) independent one
another, maximize term sum separately. Recalling form
variational transformation (see Eq. (24)), have:
!

"

#

E
=
qjji E fdjg f io + qij , f ( io )
j ji
j
+f ( io )
(50)
maximize respect qj ji keeping expectations E fdj g fixed.
n

log P (fi+ jd; qji)



X

optimization problem solved iteratively monotonically performing
following synchronous updates normalization:

qj ji

!

"

!

E fdj g qjji f io + qij , ij f 0 io + qij , qjji f ( io )
j ji
j ji

#

(51)

f 0 denotes derivative f . (The update guaranteed non-negative).
algorithm easily extended handle case positive
findings transformed. new feature conditional
probabilities products P (f + jd; q old) P (f + jd; q ) left intact, i.e.,
transformed; optimization respect variational parameters corresponding
transformed conditionals proceeds before.

Appendix C. Convexity

purpose appendix demonstrate function:
P

log E e

j;im iij dj



(52)

convex function variational parameters . note first
ane transformaP
tions change convexity properties. Thus convexity X = j;im ij dj implies
319

fiJaakkola & Jordan

convexity variational parameters . remains show
n



log E e X = log

X



pi e Xi = f (X~ )

(53)

convex function vector X~ = fX1 : : :Xn gT ; indicated discrete
values range random variable X Xi denoted probability measure
values pi . Taking gradient f respect Xk gives:

@ f (X~ ) = Ppk e Xk q
k
@Xk
pi e Xi

(54)

2
Hkl = @X@ @X f (X~ ) = kl qk , qk ql

(55)

X
X
X
Z~ HZ~ = qk Zk2 , ( qk Zk )( ql Zl) = VarfZ g 0

(56)

qk defines probability distribution. convexity revealed positive semidefinite Hessian H, whose components case
k

l

see H positive semi-definite, consider
k

k

l

VarfZ g variance discrete random variable Z takes values Zi
probability qi .

References

D'Ambrosio, B. (1993). Incremental probabilistic inference. Proceedings Ninth
Conference Uncertainty Artificial Intelligence. San Mateo, CA: Morgan Kaufmann.
D'Ambrosio, B. (1994). Symbolic probabilistic inference large BN20 networks. Proceedings Tenth Conference Uncertainty Artificial Intelligence. San Mateo,
CA: Morgan Kaufmann.
Cooper, G. (1985). NESTOR: computer-based medical diagnostic aid integrates
causal probabilistic knowledge. Ph.D. Dissertation, Medical Informatics Sciences,
Stanford University, Stanford, CA. (Available UMI
http://wwwlib.umi.com/dissertations/main).
Cooper, G. (1990). computational complexity probabilistic inference using Bayesian
belief networks. Artificial Intelligence, 42, 393{405.
Dagum, P., & Horvitz, E. (1992). Reformulating inference problems selective
conditioning. Proceedings Eighth Annual Conference Uncertainty
Artificial Intelligence.
Dagum, P., & Horvitz, E. (1993). Bayesian analysis simulation algorithms inference
Belief networks. Networks, 23, 499{516.
320

fiVariational Probabilistic Inference QMR-DT

Dagum, P., & Luby, M. (1993). Approximate probabilistic reasoning Bayesian belief
networks NP-hard. Artificial Intelligence, 60, 141{153.
Dechter, R. (1997). Mini-buckets: general scheme generating approximations automated reasoning. Proceedings Fifteenth International Joint Conference
Artificial Intelligence.
Dechter, R. (1998). Bucket elimination: unifying framework probabilistic inference.
M. I. Jordan (Ed.), Learning Graphical Models. Cambridge, MA: MIT Press.
Dempster, A., Laird, N., & Rubin, D. (1977). Maximum likelihood incomplete data
via EM algorithm. Journal Royal Statistical Society B, 39, 1{38.
Draper, D., & Hanks, S. (1994). Localized partial evaluation belief networks. Proceedings Tenth Annual Conference Uncertainty Artificial Intelligence.
Fung, R., & Chang, K. C. (1990). Weighting integrating evidence stochastic simulation Bayesian networks. Proceedings Fifth Conference Uncertainty
Artificial Intelligence. Amsterdam: Elsevier Science.
Gelfand, A., & Smith, A. (1990). Sampling-based approaches calculating marginal Densities. Journal American Statistical Association, 85, 398{409.
Heckerman, D. (1989). tractable inference algorithm diagnosing multiple diseases.
Proceedings Fifth Conference Uncertainty Artificial Intelligence.
Henrion, M. (1991). Search-based methods bound diagnostic probabilities large
belief nets. Proceedings Seventh Conference Uncertainty Artificial Intelligence.
Horvitz, E. Suermondt, H., & Cooper, G. (1989). Bounded conditioning: Flexible inference
decisions scarce resources. Proceedings Fifth Conference Uncertainty Artificial Intelligence.
Jaakkola, T. (1997). Variational methods inference learning graphical models.
PhD thesis, Department Brain Cognitive Sciences, Massachusetts Institute
Technology.
Jaakkola, T., & Jordan, M. (1996). Recursive algorithms approximating probabilities
graphical models. Advances Neural Information Processing Systems 9. Cambridge, MA: MIT Press.
Jensen, C. S., Kong, A., & Kjrulff, U. (1995). Blocking-Gibbs sampling large
probabilistic expert systems. International Journal Human-Computer Studies, 42,
647{666.
Jensen, F. (1996). Introduction Bayesian networks. New York: Springer.
321

fiJaakkola & Jordan

Jordan, M., Ghaharamani, Z. Jaakkola, T., & Saul, L. (in press). introduction
variational methods graphical models. Machine Learning.
Lauritzen, S., & Spiegelhalter, D. (1988). Local computations probabilities graphical structures application expert systems (with discussion). Journal
Royal Statistical Society B, 50, 157{224.
MacKay, D. J. C. (1998). Introduction Monte Carlo methods. M. I. Jordan (Ed.),
Learning Graphical Models. Cambridge, MA: MIT Press.
Middleton, B., Shwe, M., Heckerman, D., Henrion, M., Horvitz, E., Lehmann, H., & Cooper,
G. (1990). Probabilistic diagnosis using reformulation INTERNIST-1/QMR
knowledge base II. Evaluation diagnostic performance. Section Medical Informatics Technical report SMI-90-0329, Stanford University.
Miller, R. A., Fasarie, F. E., & Myers, J. D. (1986). Quick medical reference (QMR)
diagnostic assistance. Medical Computing, 3, 34{48.
Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems. San Mateo, CA: Morgan
Kaufmann.
Peng, Y., & Reggia, J. (1987). probabilistic causal model diagnostic problem solving {
Part 2: Diagnostic strategy. IEEE Trans. Systems, Man, Cybernetics: Special
Issue Diagnosis, 17, 395{406.
Poole, D. (1997). Probabilistic partial evaluation: Exploiting rule structure probabilistic
inference. Proceedings Fifteenth International Joint Conference Artificial
Intelligence.
Rockafellar, R. (1972). Convex Analysis. Princeton University Press.
Shachter, R. D., & Peot, M. (1990). Simulation approaches general probabilistic inference
belief networks. Proceedings Fifth Conference Uncertainty Artificial
Intelligence. Elsevier Science: Amsterdam.
Shenoy, P. P. (1992). Valuation-based systems Bayesian decision analysis. Operations
Research, 40, 463{484.
Shwe, M., & Cooper, G. (1991). empirical analysis likelihood { weighting simulation
large, multiply connected medical belief network. Computers Biomedical
Research, 24, 453-475.
Shwe, M., Middleton, B., Heckerman, D., Henrion, M., Horvitz, E., Lehmann, H., & G.
Cooper (1991). Probabilistic diagnosis using reformulation INTERNIST1/QMR knowledge base I. probabilistic model inference algorithms. Methods
Information Medicine, 30, 241{255.

322


