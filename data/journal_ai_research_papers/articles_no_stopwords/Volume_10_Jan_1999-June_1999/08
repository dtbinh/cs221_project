journal artificial intelligence

submitted published

issues stacked generalization
kai ming ting

kmting deakin edu au

ian h witten

ihw cs waikato ac nz

school computing mathematics
deakin university australia
department computer science
university waikato zealand

abstract

stacked generalization general method high level model combine lowerlevel achieve greater predictive accuracy address two crucial
issues considered black art classification tasks ever since
introduction stacked generalization wolpert type generalizer
suitable derive higher level model kind attributes used
input best obtained higher level model combines
confidence predictions lower level ones
demonstrate effectiveness stacked generalization combining three different
types learning classification tasks compare performance
stacked generalization majority vote published arcing bagging

introduction
stacked generalization way combining multiple learned
classification task wolpert used regression breiman
even unsupervised learning smyth wolpert typically different learning
learn different task hand common form
stacking first step collect output model set data
instance original training set data set represents every model prediction
instance class along true classification step care taken ensure
formed batch training data include instance
question way ordinary cross validation data treated
data another learning second step learning
employed solve wolpert terminology original data
constructed first step referred level data level
respectively set cross validated data second stage learning
referred level data level generalizer
make stacked generalization work classification tasks
addressing two crucial issues wolpert originally described black art
resolved since two issues type attributes
used form level data ii type level generalizer order get improved
accuracy stacked generalization method
breiman demonstrated success stacked generalization setting
ordinary regression level regression trees different sizes linear
c ai access foundation morgan kaufmann publishers rights reserved

fiting witten

regressions different number variables instead selecting single model
works best judged example cross validation breiman used different level regressors output values member training set form level data
used least squares linear regression constraint regression coecients
non negative level generalizer non negativity constraint turned
crucial guarantee predictive accuracy would better achieved
selecting single best predictor
stacked generalization made work reliably classification
tasks output class probabilities generated level
form level data level generalizer use version least squares linear
regression adapted classification tasks use class probabilities crucial
successful application stacked generalization classification tasks however
non negativity constraints found necessary breiman regression found
irrelevant improved predictive accuracy classification situation
section formally introduce technique stacked generalization describe
pertinent details learning used experiments section describes
stacking three different types learning section compares
stacked generalization arcing bagging two recent methods employ sampling
techniques modify data distribution order produce multiple single
learning following section describes related work ends
summary conclusions

stacked generalization

given data set l f x n n g class value x vector
representing attribute values nth instance randomly split data j almost
equal parts l l define l l l l test training sets
j th fold j fold cross validation given k learning call level
generalizers invoke kth data training set l induce
model k k called level
instance x l test set j th cross validation fold let z denote
prediction model x end entire cross validation process
data set assembled outputs k
n

n

n

j

j

j

n

j

j

j

k

n

j

kn

j

n

k

l f z z n n g
cv

n

n

kn

level data use learning call level generalizer
function z z level
derive data model
model figure illustrates cross validation process complete training process
final level k k derived data l
let us consider classification process uses k k
given instance produce vector z z
conjunction
whose output final classification
vector input level model
instance completes stacked generalization method proposed wolpert
used breiman leblanc tibshirani
k

k

k

k



k

fiissues stacked generalization




l cv
level
level

j

j



j

mk

mk

j

l

figure figure illustrates j fold cross validation process level level

data set l end process used produce level model
cv


well situation described level model
present considers situation output level
set class probabilities rather single class prediction model used
classify instance x l let p x denote probability ith output class
vector
p p x p x p x
gives model class probabilities nth instance assuming classes
level data assemble together class probability vector k along
actual class
l f p p p n n g
contrast

denote level model derived
following two subsections describe used level level generalizers experiments reported section
j

k

j

ki

kn

cv

n

k

n

n

ki

kn

n

ki

n

kn

level generalizers

three learning used level generalizers c decision tree learning
quinlan nb implementation naive bayesian classifier cestnik
ib variant lazy learning aha kibler albert
employs p nearest neighbor method modified value difference metric nominal
binary attributes cost salzberg learning
formula use
p estimated output class probabilities p x
instance x cases p x
c consider leaf decision tree instance x falls let
number training instances class leaf suppose majority class










fiting witten

p
leaf let e laplace estimator






p x pem






p x p x






note pruned trees default settings c used experiments
nb let p ijx posterior probability class given instance x
p x pp p ij xij x




note nb uses laplacian estimate estimating conditional probabilities
nominal attribute compute p ijx continuous valued attribute
normal distribution assumed case conditional probabilities
conveniently represented entirely terms mean variance observed
values class
ib suppose p nearest neighbors used denote f x pg
instance x use p experiments




p f x x
p x p x x
p









p







f otherwise euclidean distance function




three learning predicted class level model given instance
x
p x p x




level generalizers

compare effect four different learning level generalizer c
ib p nearest neighbors nb multi response linear regression
mlr last needs explanation
mlr adaptation least squares linear regression breiman
used regression settings classification real valued attributes
transformed multi response regression original classification
classes converted separate regression
class instances responses equal one class zero otherwise
input mlr level data need consider situation model


attributes probabilities separately model
large value used following wolpert advice reasonable relatively global
smooth level generalizers perform well
p







fiissues stacked generalization

classes former case attributes already real valued linear
regression class simply

x
lr x
k



k

p x
k

k

latter case classes unordered nominal attributes map binary
values obvious way setting p x class instance x zero otherwise
use linear regression
choose linear regression coecients fff g minimize
k

x x
j



yn xn

lj

k


n

xff

p x
j

k

k

n

k

coecients fff g constrained non negative following breiman discovery necessary successful application stacked generalization regression non negative coecient least squares described lawson
hanson employed derive linear regression class
later fact non negative constraint unnecessary classification tasks
place describe working mlr classify instance
x compute lr x classes assign instance class
greatest value
lr x lr x
next section investigate stacking c nb ib
k







stacking c nb ib

stacked generalization work

experiments section
successful stacked generalization necessary use output class prob rather

abilities rather class predictions
mlr suitable level generalizer among four
used
use two artificial datasets eight real world datasets uci repository
machine learning databases blake keogh merz details given
table
artificial datasets led waveform training dataset l size
respectively generated different seed used
experiments tested separate dataset instances expressed
average error rate ten repetitions entire procedure
real world datasets w fold cross validation performed fold
cross validation training dataset used l derived evaluated
pattern recognition community calls type classifier linear machine duda hart



fiting witten

datasets samples classes attr type
led


n
waveform

c
horse


b n c
credit


b n c
vowel


c
euthyroid


b c
splice


n
abalone


n c
nettalk


n
coding


n

n nominal b binary c continuous

table details datasets used experiment
test dataset expressed average error rate w fold crossvalidation note cross validation used evaluation entire procedure
whereas j fold cross validation mentioned section internal operation
stacked generalization however w j set experiments

section present model combination level


well model selection method employing j fold cross validation procedure note difference model combination model selection
whether level learning employed
table shows average error rates obtained w fold cross validation c
nb ib bestcv best three selected j fold crossvalidation expected bestcv almost classifier lowest error rate

table shows stacked generalization level model

level data comprise classifications generated level
level data comprise probabilities generated level
shown four level generalizers case along bestcv lowest error
rate dataset given bold
table summarizes table terms comparison level
derived
model bestcv totaled datasets clearly best level model
mlr performs better bestcv nine datasets equally well tenth
derived nb performs better bestcv seven
best performing
datasets significantly worse two waveform vowel regard difference
two standard errors significant confidence standard error figures
omitted table increase readability
datasets shown order increasing size mlr performs significantly
better bestcv four largest datasets indicates stacked generalization
likely give significant improvements predictive accuracy data
large direct consequence accurate estimation cross validation
note bestcv select classifier folds error rate
equal lowest error rate among three classifiers
w



fiissues stacked generalization

datasets

level generalizers
c nb
ib
led


waveform

horse


credit


vowel


euthyroid

splice


abalone


nettalk

coding



bestcv











table average error rates c nb ib bestcv best among
selected j fold cross validation standard errors shown last
column
datasets


level model
c nb ib mlr







bestcv
led

waveform

horse

credit

vowel

euthyroid

splice

abalone

nettalk

coding










level model
c nb ib mlr











table average error rates stacking c nb ib


level model
level model
c nb ib mlr c nb ib mlr
win vs loss


table summary table comparison bestcv



fiting witten

one level performs significantly much better rest
euthyroid vowel datasets mlr performs good bestcv selecting
best performing level model better bestcv
mlr advantage three level generalizers model
easily interpreted examples combination weights derives probability appear table horse credit splice abalone waveform led
model
vowel datasets weights indicate relative importance level generalizers
prediction class example splice dataset table b nb
dominant generalizer predicting class nb ib good predicting class
three generalizers make worthwhile contribution prediction class
contrast abalone dataset three generalizers contribute substantially
prediction three classes note weights class sum one
constraint imposed mlr

non negativity constraints necessary
breiman leblanc tibshirani use stacked generalization
method regression setting report necessary constrain regression
coecients non negative order guarantee stacked regression improves predictive accuracy investigate finding domain classification tasks
assess effect non negativity constraint performance three versions

mlr employed derive level model
linear regression mlr calculated intercept constant
weights classes without constraints
ii linear regression derived neither intercept constant weights
classes constraints
iii linear regression derived without intercept constant nonnegativity constraints non negative weights classes
third version one used presented earlier table shows
three versions almost indistinguishable error rates conclude
classification tasks non negativity constraints necessary guarantee
stacked generalization improves predictive accuracy
however another reason good idea employ non negativity constraints table shows example weights derived three versions mlr
led dataset third version shown column iii supports perspicuous
interpretation level generalizer contribution class predictions
two dataset ib dominant generalizer predicting classes
nb ib make worthwhile contribution predicting class evidenced
high weights however negative weights used predicting classes render
interpretation two versions much less clear


fiissues stacked generalization

horse
credit
class c nb ib c nb ib




c nb ib
horse credit datasets
table weights generated mlr model
class




c




splice
nb




ib




abalone
c nb ib




waveform
c nb ib




splice abalone waveform
table b weights generated mlr model
datasets
vowel
c nb ib











led vowel datasets
table c weights generated mlr model
class












c












led
nb












ib














fiting witten

datasets

mlr
constraints intercept non negativity
led



waveform



horse



credit



vowel



euthyroid



splice



abalone



nettalk



coding



table average error rates three versions mlr
class


































ii


















iii



























table weights generated three versions mlr constraints ii intercept
iii non negativity constraints led dataset



fiissues stacked generalization

dataset
se bestcv majority mlr
horse



splice



abalone



led



credit



nettalk


coding



waveform


euthyroid


vowel



along
table average error rates bestcv majority vote mlr model
number standard error se bestcv worst performing
level generalizers

stacked generalization compare majority vote
derived mlr majority vote
let us compare error rate

simple decision combination method requires neither cross validation level learning table shows average error rates bestcv majority vote mlr
order see whether relative performances level generalizers effect
methods number standard errors se error rates
worst performing level generalizer bestcv given datasets ordered
according measure since bestcv almost selects best performing level
generalizer small values se indicate level generalizers perform comparably
one another vice versa
mlr compares favorably majority vote eight wins versus two losses
eight wins six significant differences two exceptions splice
led datasets whereas losses horse credit datasets insignificant
differences thus extra computation cross validation level learning seems
paid
interesting note performance majority vote related size
se majority vote compares favorably bestcv first seven datasets
values se small last three se large majority vote performs
worse indicates level generalizers perform comparably worth
cross validation determine best one majority vote
far cheaper significantly different although small values se necessary
condition majority vote rival bestcv sucient condition see matan
example applies majority vote compared mlr mlr
performs significantly better five datasets large se values
one cases


fiting witten

versus

c nb ib mlr
win vs loss
versus
generalizer summarized table
table
worth mentioning method averages p x level
yielding p x predicts class p x p x according
breiman b method produces error rate almost identical majority
vote








stacked generalization work best generated
mlr
shown stacked generalization works best output class probabilities
rather class predictions used mlr rather c ib
nb retrospect surprising explained intuitively follows
level model provide simple way combining evidence available
evidence includes predictions confidence level model
predictions linear combination simplest way pooling level
confidence mlr provides
alternative methods nb c ib shortcomings bayesian could form basis suitable alternative way pooling level confidence independence assumption central naive bayes hampers performance
datasets evidence provided individual level certainly
independent c builds trees model interaction amongst attributes particularly
tree large desirable combining confidences nearest neighbor methods really give way combining confidences similarity metric
employed could misleadingly assume two different sets confidence levels similar

level
table summarizes table comparing
generalizer across datasets c clearly better label representation
discretizing continuous valued attributes creates intra attribute interaction addition interactions different attributes evidence table nb
indifferent use labels confidences normal distribution assumption
embodies latter case could another reason unsuitable combining
confidence measures mlr ib handle continuous valued attributes better
label ones since domain designed work
summary

summarize findings section follows

none four learning used obtain model perform satisfactorily


fiissues stacked generalization

mlr best four learning use level generalizer

obtaining model
obtained mlr lower predictive error rate best model
selected j fold cross validation almost datasets used experiments

another advantage mlr three level generalizers interpretability
weights indicate different contributions level model k makes
prediction classes
k

model derived mlr without non negativity constraints
constraints make little difference model predictive accuracy

use non negativity constraints mlr advantage interpretability non

negative weights support easier interpretation extent model
contributes prediction class
k

derived mlr model compares favorably majority vote
mlr provides method combining confidence generated level
final decision reasons nb c ib suitable task

comparison arcing bagging
section compares stacking c nb ib arcing
called boosting originator schapire bagging reported breiman
b c arcing bagging employ sampling techniques modify data
distribution order produce multiple single learning
combine decisions individual arcing uses weighted majority vote
bagging uses unweighted majority vote breiman reports arcing bagging
substantially improve predictive accuracy single model derived base
learning

experimental

first describe differences experimental procedures
stacking averaged ten fold cross validation datasets except waveform
averaged ten repeated trials standard errors shown arcing
bagging obtained breiman b c averaged trials
breiman experiments trial uses random split form training test
sets datasets except waveform note waveform dataset used
irrelevant attributes breiman used version without irrelevant attributes would
expected degrade performance level generalizers experiments
cases training instances used dataset used test instances
whereas breiman used arcing bagging done decision tree
derived cart breiman et al trial


fiting witten

dataset
samples stacking arcing bagging
waveform



glass



ionosphere




soybean




breast cancer




diabetes



table comparing stacking arcing bagging classifiers
six datasets given table indicate three methods
competitive stacking performs better arcing bagging three
datasets waveform soybean breast cancer better arcing worse
bagging diabetes dataset note stacking performs poorly glass
ionosphere two small real world datasets surprising cross validation
inevitably produces poor estimates small datasets

discussion

bagging stacking ideal parallel computation construction level
model proceeds independently communication modeling processes
necessary
arcing bagging require considerable number member
rely varying data distribution get diverse set single learning
level generalizer stacking work two three level

suppose computation time required learning c arcing
bagging needs h learning time required hc suppose stacking requires
g model employs j fold cross validation assuming time c needed
derive g level level model learning time stacking
g j c given table h j g thus
c c however practice learning time required level
level generalizers may different
users stacking free choice level may derived
single learning variety different example section
uses different types learning bag stacking stacking bagged
ting witten uses data variation obtain diverse set single
learning former case performance may vary substantially
level example nb performs poorly vowel euthyroid datasets
compared two see table stacking copes well situation
performance variation among member bagging rather small
derived learning bootstrap samples section








heart dataset used breiman b c omitted much modified
original one



fiissues stacked generalization

shows small performance variation among member necessary condition
majority vote employed bagging work well
worth noting arcing bagging incorporated framework
stacked generalization arced bagged level ting witten
one possible way incorporating bagged level learning employing mlr instead voting implementation l used test set
bagged derive level data rather cross validated data
viable bootstrap sample leaves examples ting witten
bag stacking almost higher predictive accuracy bagging
derived c nb note difference whether
adaptive level model simple majority vote employed
according breiman b c arcing bagging improve predictive accuracy learning unstable unstable learning
one small perturbations training set produce large changes
derived model decision trees neural networks unstable nb ib stable
stacking works
mlr successful candidate level learning found
might work equally well one candidate neural networks however
experimented back propagation neural networks purpose found
much slower learning rate mlr example mlr took
seconds compare seconds neural network nettalk dataset
error rate possible candidates multinomial logit model
jordan jacobs special case generalized linear mccullagh
nelder supra bayesian procedure jacobs treats level
confidence data may combined prior distribution level
via bayes rule

related work

analysis stacked generalization motivated breiman discussed
earlier leblanc tibshirani leblanc tibshirani examine stacking
linear discriminant nearest neighbor classifier one artificial
dataset method similar mlr performs better non negativity constraints
without section constraints irrelevant mlr
predictive accuracy classification situation
leblanc tibshirani ting witten use version mlr
employs class probabilities level model induce linear regression
case linear regression class

lr x


xxff
k



k



ki

p x
ki

implementation requires fitting ki parameters compared k parameters
version used see corresponding formula section
schapire r e freund p bartlett w lee provide alternative explanation
effectiveness arcing bagging



fiting witten

versions give comparable terms predictive accuracy version used
runs considerably faster needs fit fewer parameters
limitations mlr well known duda hart class
divides description space convex decision regions every region must singly
connected decision boundaries linear hyperplanes means mlr
suitable unimodal probability densities despite limitations
mlr still performs better level generalizer ib nearest competitor deriving
limitations may hold key fuller understanding behavior stacked
generalization jacobs reviews linear combination methods used mlr
previous work stacked generalization especially applied classification tasks
limited several ways applies particular dataset e g zhang
mesirov waltz others report less convincing merz
still others different focus evaluate datasets leblanc
tibshirani chan stolfo kim bartlett fan et al
one might consider degenerate form stacked generalization use crossvalidation produce data level learning level learning done
training process jacobs et al another level learning
takes place batch mode level derived ho et al
several researchers worked still degenerate form stacked generalization
without cross validation learning level examples neural network ensembles
hansen salamon perrone cooper krogh vedelsby multiple
decision tree combination kwok carter buntine oliver hand
multiple rule combination kononenko kovacic methods used level
majority voting weighted averaging bayesian combination possible methods
distribution summation likelihood combination forms ordering
class rank ali pazzani study methods rule learner ting
uses confidence prediction combine nearest neighbor classifier
naive bayesian classifier

conclusions
addressed two crucial issues successful implementation stacked generalization classification tasks first class probabilities used instead single
predicted class input attributes higher level learning class probabilities serve
confidence measure prediction made second multi response least squares
linear regression technique employed high level generalizer technique
provides method combining level confidence three learning algorithmic limitations suitable aggregating confidences
combining three different types learning implementation
stacked generalization found achieve better predictive accuracy model
selection cross validation majority vote found competitive arcing bagging unlike stacked regression non negativity constraints
least squares regression necessary guarantee improved predictive accuracy
classification tasks however constraints still preferred increase
interpretability level model


fiissues stacked generalization

implication successful implementation stacked generalization earlier
model combination methods employing weighted majority vote averaging computations make use level learning apply learning improve
predictive accuracy

acknowledgment

authors grateful zealand marsden fund financial support
work conducted first author department computer
science university waikato authors grateful j ross quinlan providing
c david w aha providing ib anonymous reviewers editor
provided many helpful comments

references

aha w kibler k albert instance learning machine learning pp
ali k j pazzani error reduction learning multiple descriptions machine learning vol pp
blake c e keogh c j merz uci repository machine learning databases
http www ics uci edu mlearn mlrepository html irvine ca university california department information computer science
breiman l stacked regressions machine learning vol pp
breiman l b bagging predictors machine learning vol pp
breiman l c bias variance arcing classifiers technical report department statistics university california berkeley ca
breiman l j h friedman r olshen c j stone classification regression trees belmont ca wadsworth
cestnik b estimating probabilities crucial task machine learning
proceedings european conference artificial intelligence pp
chan p k j stolfo comparative evaluation voting meta learning
partitioned data proceedings twelfth international conference machine learning pp morgan kaufmann
cost salzberg weighted nearest neighbor learning
symbolic features machine learning pp
fan w p k chan j stolfo comparative evaluation combiner
stacked generalization proceedings aaai workshop integrating multiple
learned pp
hansen l k p salamon neural network ensembles ieee transactions
pattern analysis machine intelligence pp


fiting witten

ho k j j hull n srihari decision combination multiple classifier
systems ieee transactions pattern analysis machine intelligence vol
pp
jacobs r methods combining experts probability assessments neural
computation pp mit press
jacobs r jordan j nowlan g e hinton adaptive mixtures local
experts neural computation pp
jacobs r jordan hierachical mixtures experts em neural computation pp
kim k e b bartlett error estimation series association neural network
systems neural computation pp mit press
kononenko kovacic learning optimization stochastic generation
multiple knowledge proceedings ninth international conference
machine learning pp morgan kaufmann
krogh j vedelsby neural network ensembles cross validation active
learning advances neural information processing systems g tesauro
touretsky k leen editors pp mit press
kwok c carter multiple decision trees uncertainty artificial intelligence r shachter levitt l kanal j lemmer editors pp
north holland
lawson c l r j hanson solving least squares siam publications
leblanc r tibshirani combining estimates regression classification technical report department statistics university toronto
matan voting ensembles classifiers extended abstract proceedings
aaai workshop integrating multiple learned pp
mccullagh p j nelder generalized linear london chapman
hall
merz c j dynamic learning bias selection proceedings fifth international workshop artificial intelligence statistics ft lauderdale fl
unpublished pp
oliver j j j hand pruning averaging decision trees proceedings
twelfth international conference machine learning pp morgan
kaufmann
perrone p l n cooper networks disagree ensemble methods
hybrid neural networks artificial neural networks speech vision r j
mammone editor chapman hall
quinlan j r c program machine learning morgan kaufmann


fiissues stacked generalization

schapire r e strength weak learnability machine learning pp
kluwer academic publishers
schapire r e freund p bartlett w lee boosting margin
explanation effectiveness voting methods proceedings fourteenth
international conference machine learning morgan kaufmann
smyth p wolpert stacked density estimation advances neural information processing systems
ting k characterisation predictive accuracy decision combination proceedings thirteenth international conference machine learning
pp morgan kaufmann
ting k h witten stacking bagged dagged proceedings
fourteenth international conference machine learning pp morgan
kaufmann
weiss c kulikowski computer systems learns morgan kaufmann
wolpert h stacked generalization neural networks vol pp
pergamon press
zhang x j p mesirov l waltz hybrid system protein secondary
structure prediction journal molecular biology pp




