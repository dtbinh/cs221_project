journal artificial intelligence

submitted published

hierarchical reinforcement learning maxq value
function decomposition
thomas g dietterich

department computer science oregon state university
corvallis

abstract

tgd cs orst edu

presents hierarchical reinforcement learning decomposing target markov decision process mdp hierarchy smaller mdps
decomposing value function target mdp additive combination
value functions smaller mdps decomposition known maxq decomposition procedural semantics subroutine hierarchy declarative
semantics representation value function hierarchical policy maxq unifies
extends previous work hierarchical reinforcement learning singh kaelbling
dayan hinton assumption programmer identify useful
subgoals define subtasks achieve subgoals defining subgoals
programmer constrains set policies need considered reinforcement
learning maxq value function decomposition represent value function
policy consistent given hierarchy decomposition creates opportunities exploit state abstractions individual mdps within hierarchy
ignore large parts state space important practical application
method defines maxq hierarchy proves formal representational power establishes five conditions safe use state abstractions
presents online model free learning maxq q proves converges
probability kind locally optimal policy known recursively optimal policy
even presence five kinds state abstraction evaluates maxq
representation maxq q series experiments three domains shows
experimentally maxq q state abstractions converges recursively optimal
policy much faster q learning fact maxq learns representation
value function important benefit makes possible compute execute
improved non hierarchical policy via procedure similar policy improvement
step policy iteration demonstrates effectiveness non hierarchical
execution experimentally finally concludes comparison related work
discussion design tradeoffs hierarchical reinforcement learning

c ai access foundation morgan kaufmann publishers rights reserved

fidietterich

introduction
area reinforcement learning bertsekas tsitsiklis sutton barto
studies methods agent learn optimal near optimal plans interacting
directly external environment basic methods reinforcement learning
classical dynamic programming developed late
bellman howard however reinforcement learning methods offer two
important advantages classical dynamic programming first methods online
permits focus attention parts state space important
ignore rest space second methods employ function approximation e g neural networks represent knowledge allows
generalize across state space learning time scales much better
despite recent advances reinforcement learning still many shortcomings
biggest lack fully satisfactory method incorporating hierarchies
reinforcement learning classical shown hierarchical methods hierarchical task networks currie tate macro actions
fikes hart nilsson korf state abstraction methods sacerdoti
knoblock provide exponential reductions computational cost finding
good plans however basic probabilistic reinforcement learning methods treat state space one huge search space
means paths start state goal state long
length paths determines cost learning information
future rewards must propagated backward along paths
many researchers singh lin kaelbling dayan hinton
hauskrecht et al parr russell sutton precup singh experimented different methods hierarchical reinforcement learning hierarchical
probabilistic explored many different points design space
hierarchical methods several systems designed specific situations
lack crisp definitions main approaches clear understanding relative
merits different methods
formalizes clarifies one attempts understand
compares techniques called maxq method provides
hierarchical decomposition given reinforcement learning set subproblems simultaneously provides decomposition value function given
set value functions subproblems hence declarative
semantics value function decomposition procedural semantics subroutine
hierarchy
decomposition subproblems many advantages first policies learned
subproblems shared reused multiple parent tasks second value functions
learned subproblems shared subproblem reused task
learning overall value function task accelerated third state abstractions applied overall value function represented compactly
sum separate terms depends subset state variables
compact representation value function require less data learn hence
learning faster


fimaxq hierarchical reinforcement learning

previous shows several important design decisions must
made constructing hierarchical reinforcement learning system provide
overview let us review issues see maxq
method approaches
first issue specify subtasks hierarchical reinforcement learning involves
breaking target markov decision hierarchy subproblems subtasks
three general approaches defining subtasks one define
subtask terms fixed policy provided programmer
learned separate process option method sutton precup singh
takes second define subtask terms nondeterministic finite state controller hierarchy abstract machines ham method
parr russell takes method permits programmer
provide partial policy constrains set permitted actions point
specify complete policy subtask third define
subtask terms termination predicate local reward function define
means subtask completed final reward completing
subtask maxq method described follows building
upon previous work singh kaelbling dayan hinton dean
lin
advantage option partial policy approaches subtask
defined terms amount effort course action rather terms
achieving particular goal condition however option least
simple form described requires programmer provide complete policies
subtasks dicult programming task real world
hand termination predicate method requires programmer guess relative
desirability different states subtask might terminate
dicult although dean lin guesses revised automatically
learning
potential drawback hierarchical methods learned policy may
suboptimal hierarchy constrains set possible policies considered
constraints poorly chosen resulting policy suboptimal nonetheless
learning developed option partial policy approaches
guarantee learned policy best possible policy consistent
constraints
termination predicate method suffers additional source suboptimality
learning described converges form local optimality
call recursive optimality means policy subtask locally optimal
given policies children might exist better hierarchical policies
policy subtask must locally suboptimal overall policy optimal
example subtask buying milk might performed suboptimally distant
store larger involves buying film store
avoided careful definition termination predicates local reward functions
added burden programmer interesting note
recursive optimality noticed previously previous work


fidietterich

focused subtasks single terminal state cases
arise
second design issue whether employ state abstractions within subtasks
subtask employs state abstraction ignores aspects state environment
example many robot navigation choices route take
reach goal location independent robot currently carrying
exceptions state abstraction explored previously see maxq
method creates many opportunities exploit state abstraction abstractions
huge impact accelerating learning see important
design tradeoff successful use state abstraction requires subtasks defined
terms termination predicates rather option partial policy methods
maxq method must employ termination predicates despite
create
third design issue concerns non hierarchical execution learned hierarchical policy kaelbling first point value function learned
hierarchical policy could evaluated incrementally yield potentially much
better non hierarchical policy dietterich sutton et al generalized
arbitrary subroutines could executed non hierarchically yield improved
policies however order support non hierarchical execution extra learning
required ordinarily hierarchical reinforcement learning states learning
required higher levels hierarchy states one subroutines could terminate plus possible initial states support non hierarchical
execution learning required states levels hierarchy general
requires additional exploration well additional computation memory
consequence hierarchical decomposition value function maxq method
able support form execution see many
improvement non hierarchical execution worth added cost
fourth final issue form learning employ important advantage reinforcement learning typically operate online
however finding online work general hierarchical reinforcement learning
dicult particularly within termination predicate family methods singh
method relied subtask unique terminal state kaelbling employed mix
online batch train hierarchy work within options framework usually assumes policies subproblems given need
learned best previous online hamq q learning
parr russell partial policy method feudal q dayan
hinton unfortunately hamq method requires attening hierarchy
several undesirable consequences feudal q tailored specific kind
converge well defined optimal policy
present general called maxq q fully online learning
hierarchical value function enables subtasks within hierarchy
learned simultaneously online experimentally theoretically
converges recursively optimal policy substantially
faster e non hierarchical q learning state abstractions employed


fimaxq hierarchical reinforcement learning

remainder organized follows introducing notation
section define maxq value function decomposition section illustrate
simple example markov decision section presents analytically
tractable version maxq q learning called maxq
proves convergence recursively optimal policy shows extend maxq produce maxq q shows extend theorem similarly
section takes issue state abstraction formalizes series five conditions
state abstractions safely incorporated maxq representation
state abstraction give rise hierarchical credit assignment
brie discusses one solution finally section presents experiments
three example domains experiments give idea generality maxq
representation provide relative importance temporal state
abstractions importance non hierarchical execution concludes
discussion design issues brie described particular
addresses tradeoff method defining subtasks via termination predicates
ability exploit state abstractions
readers may disappointed maxq provides way learning structure hierarchy philosophy developing maxq share
reinforcement learning researchers notably parr russell draw inspiration
development belief networks pearl belief networks first introduced
formalism knowledge engineer would describe structure networks domain experts would provide necessary probability estimates subsequently
methods developed learning probability values directly observational data
recently several methods developed learning structure belief
networks data dependence knowledge engineer reduced
likewise require programmer provide structure
hierarchy programmer need make several important design decisions
see maxq representation much computer program
rely programmer design modules indicate permissible
ways modules invoke learning fill
implementations module way overall program work well
believe provide practical tool solving large real world
mdps believe help us understand structure hierarchical learning
hope subsequent able automate
work currently requiring programmer

formal definitions
begin introducing definitions markov decision semi markov decision

markov decision
employ standard definition markov decision known markov
decision processes restrict attention situations agent


fidietterich

interacting fully observable stochastic environment situation modeled
markov decision mdp hs p r p defined follows
finite set states environment point time agent
observe complete state environment
finite set actions technically set available actions depends
current state suppress dependence notation
p action performed environment makes probabilistic transition current state resulting state according probability
distribution p js
r similarly action performed environment makes transition
agent receives real valued possibly stochastic reward r whose
expected value r js simplify notation customary treat
reward given time action initiated even though may
general depend well
p starting state distribution mdp initialized state
probability p
policy mapping states actions tells action perform
environment state
consider two settings episodic infinite horizon
episodic setting rewards finite least one zero cost absorbing
terminal state absorbing terminal state state actions lead back
state probability zero reward technical reasons consider
deterministic policies proper deterministic policies
non zero probability reaching terminal state started arbitrary state
believe condition relaxed verified formally
episodic setting goal agent policy maximizes expected
cumulative reward special case rewards non positive
referred stochastic shortest path rewards viewed
costs e lengths policy attempts move agent along path minimum
expected cost
infinite horizon setting rewards finite addition discount
factor agent goal policy minimizes infinite discounted sum
future rewards
value function v policy function tells state
expected cumulative reward executing policy starting state let rt
random variable tells reward agent receives time step following
policy define value function episodic setting
v e frt rt rt jst g
discounted setting value function


n



v e rt rt rt fifi st


fimaxq hierarchical reinforcement learning

see equation reduces previous one however infinitehorizon mdps sum may converge
value function satisfies bellman equation fixed policy

v

x



p js r js v




quantity right hand side called backed value performing action
state possible successor state computes reward would received
value resulting state weights according probability
ending
optimal value function v value function simultaneously maximizes
expected cumulative reward states bellman proved unique
solution known bellman equation

v max


x



p js r js v






may many optimal policies achieve value policy chooses
achieve maximum right hand side equation optimal policy
denote optimal policy note optimal policies greedy
respect backed value available actions
closely related value function called action value function q function
watkins function q gives expected cumulative reward performing action state following policy thereafter q function satisfies
bellman equation

q

x



p js r js q




optimal action value function written q satisfies equation
x
q p js







r js max q






note policy greedy respect q optimal policy may
many optimal policies differ break ties actions
identical q values
action order denoted total order actions within mdp
anti symmetric transitive relation true iff strictly preferred
ordered greedy policy greedy policy breaks ties example
suppose two best actions state q q
ordered greedy policy choose note
although may many optimal policies given mdp ordered greedy policy
unique


fidietterich

semi markov decision processes

order introduce prove properties maxq decomposition
need consider simple generalization mdps semi markov decision process
discrete time semi markov decision process smdp generalization markov
decision process actions take variable amount time complete
particular let random variable n denote number time steps action takes
executed state extend state transition probability function
joint distribution states number time steps n action
performed state p n js similarly expected reward changed
r n js
straightforward modify bellman equation define value function
fixed policy
h

x
v p n js r n js n v
n

change expected value right hand side taken respect
n raised power n ect variable amount time
may elapse executing action
note expectation linear operator write bellman
equations sum expected reward performing action expected value
resulting state example rewrite equation
x

v r p n js n v
n

r expected reward performing action state expectation taken respect n
given generalized apply discrete time semimarkov decision processes consequence whenever talks
executing primitive action could easily talk executing hand coded openloop subroutine subroutines would learned could execution
interrupted discussed section many applications e g robot
control limited sensors open loop controllers useful e g hide partialobservability example see kalmar szepesvari lorincz
note episodic case difference mdp semi markov
decision process discount factor therefore neither optimal policy
optimal value function depend amount time action takes

reinforcement learning

reinforcement learning tries construct optimal policy
unknown mdp given access unknown mdp via following
formalization slightly different standard formulation smdps separates
p js f tjs f cumulative distribution function probability
terminate time units real valued rather integer valued case important
consider joint distribution n need consider actions arbitrary
real valued durations



fimaxq hierarchical reinforcement learning

reinforcement learning protocol time step told current state
mdp set actions executable state
chooses action mdp executes action causes
move state returns real valued reward r absorbing terminal state
set actions contains special action reset causes mdp move
one initial states drawn according p
make use two well known learning q learning
watkins watkins dayan sarsa rummery niranjan
apply case action value function q represented
table one entry pair state action every entry table
initialized arbitrarily
q learning observed chosen received r observed
performs following update

qt fft qt fft r max
q

fft learning rate parameter
jaakkola jordan singh bertsekas tsitsiklis prove
agent follows exploration policy tries every action every state infinitely often



x
x
lim




lim









qt converges optimal action value function q probability proof
holds settings discussed episodic infinite horizon
sarsa similar observing choosing observing r
observing choosing performs following update

qt fft qt fft r qt
fft learning rate parameter key difference q value chosen
action q appears right hand side place q learning uses
q value best action singh et al provide two important convergence
first fixed policy employed choose actions sarsa converge
value function policy provided fft decreases according equations second
called glie policy employed choose actions sarsa converge value
function optimal policy provided fft decreases according equations
glie policy defined follows

definition glie greedy limit infinite exploration policy policy

satisfying

action executed infinitely often every state visited infinitely often
limit policy greedy respect q value function probability



fidietterich



r

g







b








figure taxi domain

maxq value function decomposition
center maxq method hierarchical reinforcement learning maxq
value function decomposition maxq describes decompose overall value function
policy collection value functions individual subtasks subsubtasks
recursively

motivating example

make discussion concrete let us consider following simple example figure
shows grid world inhabited taxi agent four specially designated
locations world marked r ed b lue g reen ellow taxi
episodic episode taxi starts randomly chosen square
passenger one four locations chosen randomly passenger wishes
transported one four locations chosen randomly taxi must go
passenger location source pick passenger go destination location
destination put passenger keep things uniform taxi
must pick drop passenger even already located destination
episode ends passenger deposited destination location
six primitive actions domain four navigation actions move
taxi one square north south east west b pickup action c putdown action
reward action additional reward successfully
delivering passenger reward taxi attempts execute
putdown pickup actions illegally navigation action would cause taxi hit
wall action op usual reward
simplify examples throughout section make six primitive actions deterministic later make actions stochastic order create greater
challenge learning
seek policy maximizes total reward per episode possible
states squares locations passenger counting four starting locations
taxi destinations
task simple hierarchical structure two main sub tasks
get passenger deliver passenger subtasks turn involves


fimaxq hierarchical reinforcement learning

subtask navigating one four locations performing pickup putdown
action
task illustrates need support temporal abstraction state abstraction
subtask sharing temporal abstraction obvious example process navigating passenger location picking passenger temporally extended
action take different numbers steps complete depending distance
target top level policy get passenger deliver passenger expressed
simply temporal abstractions employed
need state abstraction perhaps less obvious consider subtask getting
passenger subtask solved destination passenger
completely irrelevant cannot affect nagivation pickup decisions perhaps
importantly navigating target location source destination
location passenger target location important fact
cases taxi carrying passenger cases irrelevant
finally support subtask sharing critical system could learn solve
navigation subtask solution could shared get passenger
deliver passenger subtasks maxq method provides
value function representation learning supports temporal abstraction
state abstraction subtask sharing
construct maxq decomposition taxi must identify set
individual subtasks believe important solving overall task
case let us define following four tasks
navigate subtask goal move taxi current location
one four target locations indicated formal parameter
get subtask goal move taxi current location
passenger current location pick passenger
put goal subtask move taxi current location
passenger destination location drop passenger
root whole taxi task
subtasks defined subgoal subtask terminates
subgoal achieved
defining subtasks must indicate subtask subtasks
primitive actions employ reach goal example navigate subtask
use four primitive actions north south east west get subtask
use navigate subtask pickup primitive action
information summarized directed acyclic graph called task
graph shown figure graph node corresponds subtask
primitive action edge corresponds potential way one subtask
call one child tasks notation formal actual e g source tells formal
parameter bound actual parameter
suppose subtasks write policy e g computer
program achieve subtask refer policy subtask subroutine view parent subroutine invoking child subroutine via ordinary


fidietterich

root

get

put
source

pickup

destination

navigate

north

south

east

putdown

west

figure task graph taxi
subroutine call return semantics policy subtask gives
us overall policy taxi mdp root subtask executes policy calling
subroutines policies get put subtasks get policy calls subroutines
navigate subtask pickup primitive action call
collection policies hierarchical policy hierarchical policy subroutine executes
enters terminal state subtask

definitions

let us formalize discussion far
maxq decomposition takes given mdp decomposes finite set
subtasks fm mn g convention root subtask e solving
solves entire original mdp
definition unparameterized subtask three tuple hti ai r defined follows
ti termination predicate partitions set active states si set
terminal states ti policy subtask mi executed current
state si time subtask mi executed mdp enters
state ti mi terminates immediately even still executing subtask see

ai set actions performed achieve subtask mi actions
primitive actions set primitive actions mdp
subtasks denote indexes refer
actions children subtask sets ai define directed graph
subtasks mn graph may contain cycles stated another way
subtask invoke recursively directly indirectly
child subtask mj formal parameters interpreted subtask
occurred multiple times ai one occurrence possible tuple actual


fimaxq hierarchical reinforcement learning

values could bound formal parameters set actions ai may differ
one state another one set actual parameter values another
technically ai function actual parameters however suppress
dependence notation
r pseudo reward function specifies deterministic pseudo reward
transition terminal state ti pseudo reward tells desirable
terminal states subtask typically employed give goal
terminal states pseudo reward non goal terminal states negative
reward definition pseudo reward r zero non terminal states
pseudo reward used learning mentioned
section
primitive action primitive subtask maxq decomposition
executable terminates immediately execution
pseudo reward function uniformly zero

subtask formal parameters possible binding actual values
formal parameters specifies distinct subtask think values formal
parameters part name subtask practice course implement
parameterized subtask parameterizing components task b specifies
actual parameter values task mi define parameterized termination
predicate ti b parameterized pseudo reward function r b simplify notation
rest usually omit parameter bindings however
noted parameter subtask takes large number possible values
equivalent creating large number different subtasks need
learned create large number candidate actions parent task
make learning dicult parent task well

definition hierarchical policy set containing policy subtasks
f n g
subtask policy takes state returns name primitive action
execute name subroutine bindings formal parameters invoke
terminology sutton precup singh subtask policy deterministic
option probability terminating state denote
si ti
parameterized task policy must parameterized well takes
state bindings formal parameters returns chosen action bindings
formal parameters
table gives pseudo code description procedure executing hierarchical
policy hierarchical policy executed stack discipline similar ordinary
programming languages let kt denote contents pushdown stack time
subroutine invoked name actual parameters pushed onto stack
subroutine terminates name actual parameters popped stack
notice line subroutine stack terminates subroutines


fidietterich

table pseudo code execution hierarchical policy






















procedure executehierarchicalpolicy

st state world time
kt state execution stack time
let kt empty stack observe st
push nil onto stack kt invoke root task parameters

repeat
top kt primitive action

let top kt
name current subroutine
gives parameter bindings
let fa
action fa gives parameter bindings chosen policy
push fa onto stack kt
end
let nil pop kt primitive action top stack
execute primitive action observe st receive reward r st jst
subtask kt terminated st
let terminated subtask highest closest root stack
top kt pop kt
pop kt
kt kt resulting execution stack
kt empty
end executehierarchicalpolicy

immediately aborted control returns subroutine invoked
terminated subroutine
sometimes useful think contents stack additional part
state space hence hierarchical policy implicitly defines mapping
current state st current stack contents kt primitive action action
executed yields resulting state st resulting stack contents kt
added state information stack hierarchical policy non markovian
respect original mdp
hierarchical policy maps states stack contents k actions
value function hierarchical policy must assign values combinations states
stack contents k

definition hierarchical value function denoted v hs k gives expected cumu

lative reward following hierarchical policy starting state stack contents
k

hierarchical value function exactly learned ron parr b hamq
discuss however focus learning
projected value functions subtasks mn hierarchy


fimaxq hierarchical reinforcement learning

definition projected value function hierarchical policy subtask mi denoted
v expected cumulative reward executing policies descendents
mi starting state mi terminates
purpose maxq value function decomposition decompose v
projected value function root task terms projected value functions v
subtasks maxq decomposition

decomposition projected value function

defined hierarchical policy projected value function
value function decomposed hierarchically decomposition
following theorem

theorem given task graph tasks mn hierarchical policy
subtask mi defines semi markov decision process states si actions ai probability
transition function pi n js expected reward function r v
v projected value function child task state primitive
action
v defined expected immediate reward executing v
p


p js r js
proof consider subroutines descendents task mi task graph

subroutines executing fixed policies specified hierarchical policy
probability transition function pi n js well defined stationary distribution
child subroutine set states si set actions ai obvious
interesting part theorem fact expected reward function r
smdp projected value function child task
see let us write value v

v e frt rt rt jst g

sum continues subroutine task mi enters state ti
let us suppose first action chosen subroutine subroutine
invoked executes number steps n terminates state according
pi n js rewrite equation
v e



nx

u

u rt u




x

u n




u fifi

ur

st





first summation right hand side equation discounted sum rewards
executing subroutine starting state terminates words v
projected value function child task second term right hand side
equation value current task v discounted n
current state subroutine terminates write form
bellman equation
x

v v pi n js n v
n



fidietterich

form equation bellman equation smdp
first term expected reward r q e
obtain hierarchical decomposition projected value function let us switch
action value q representation first need extend q notation
handle task hierarchy let q expected cumulative reward subtask
mi performing action state following hierarchical policy subtask
mi terminates action may primitive action child subtask
notation state equation follows

q v

x

n

pi n js n q



right term equation expected discounted reward completing task

mi executing action state term depends
summation marginalizes away dependence n let us define c
equal term

definition completion function c expected discounted cumulative

reward completing subtask mi invoking subroutine subtask state
reward discounted back point time begins execution

c

x

n

pi n js n q



definition express q function recursively

q v c



finally express definition v

v




q
composite
p


p js r js primitive



refer equations decomposition equations
maxq hierarchy fixed hierarchical policy equations recursively decompose
projected value function root v projected value functions
individual subtasks mn individual completion functions c j
j n fundamental quantities must stored represent value
function decomposition c values non primitive subtasks v values
primitive actions
make easier programmers design debug maxq decompositions
developed graphical representation call maxq graph maxq graph
taxi domain shown figure graph contains two kinds nodes max nodes
q nodes max nodes correspond subtasks task decomposition
one max node primitive action one max node subtask including
root task primitive max node stores value v q nodes correspond
actions available subtask q node parent task state


fimaxq hierarchical reinforcement learning

maxroot

qpickup

qget

qput

maxget

maxput

qnavigateforput

qnavigateforget

source

qputdown

destination

pickup

putdown

maxnavigate

qnorth

qeast

qsouth

qwest

north

east

south

west

figure maxq graph taxi domain
subtask stores value c children node unordered
order drawn figure imply anything order
executed indeed child action may executed multiple times
parent subtask completed
addition storing information max nodes q nodes viewed performing parts computation described decomposition equations specifically
max node viewed computing projected value function v
subtask primitive max nodes information stored node composite
max nodes information obtained asking q node corresponding
q node parent task child task viewed computing value
q asking child task projected value function v
adding completion function c


fidietterich

example consider situation shown figure denote
suppose passenger r wishes go b let hierarchical policy
evaluating optimal policy denoted omit superscript reduce
clutter notation value state cost
unit move taxi r unit pickup passenger units move taxi b
unit putdown passenger total units reward
passenger delivered agent gets reward net value
figure shows maxq hierarchy computes value compute value
v root maxroot consults policy finds root get hence asks
q node qget compute q root get completion cost root task
performing get c root get cost units deliver
customer net reward completing get subtask however
reward completing get must ask maxget estimate expected
reward performing get
policy maxget dictates navigate subroutine invoked
bound r maxget consults q node qnavigateforget compute expected
reward qnavigateforget knows completing navigate r task one action
pickup required complete get c maxget navigate r
asks maxnavigate r compute expected reward performing navigate
location r
policy maxnavigate chooses north action maxnavigate asks qnorth
compute value qnorth looks completion cost finds c navigate north
e navigate task completed performing north action consults
maxnorth determine expected cost performing north action
maxnorth primitive action looks expected reward
series recursive computations conclude follows

q navigate r north
v navigate r
q get navigate r
perform navigate plus complete get
v get
q root get
perform get plus complete root task collect final reward
end value v root decomposed sum
c terms plus expected reward chosen primitive action

v root v north c navigate r north
c get navigate r c root get




fimaxq hierarchical reinforcement learning


maxroot



qget

qput


maxget

maxput


qpickup

qnavigateforput

qnavigateforget

qputdown



pickup

putdown

maxnavigate



qnorth

qeast

qsouth

qwest

east

south

west


north

figure computing value state maxq hierarchy c value
q node shown left node numbers values
returned graph
general maxq value function decomposition form

v v c c c
path max nodes chosen hierarchical policy going
root primitive leaf node summarized graphically figure
summarize presentation section following theorem

theorem let ng hierarchical policy defined given maxq
graph subtasks mn let root node graph
exist values c internal max nodes v primitive leaf max


fidietterich



v x

xxxxx




x
v
p
pppp



v



zz

zz


v c
r

r

r

r

r

c


r

r

c

r r r r r

figure maxq decomposition r r denote sequence rewards received
primitive actions times
nodes v computed decomposition equations
expected discounted cumulative reward following policy starting state

proof proof induction number levels task graph

level compute values c v primitive according
decomposition equations apply decomposition equations compute
q apply equation theorem conclude q gives
value function level obtain value function entire
hierarchical policy q e
important note representation theorem mention pseudoreward function pseudo reward used learning theorem
captures representational power maxq decomposition address
question whether learning given policy
subject next section

learning maxq decomposition
section presents central contributions first discuss optimality criteria employed hierarchical reinforcement learning introduce
maxq learning learn value functions policies maxq
hierarchies pseudo rewards e pseudo rewards zero
central theoretical maxq converges recursively optimal
policy given maxq hierarchy followed brief discussion ways
accelerating maxq learning section concludes description maxq q
learning handles non zero pseudo reward functions


fimaxq hierarchical reinforcement learning

two kinds optimality

order develop learning maxq decomposition must consider
exactly hoping achieve course mdp would
optimal policy however maxq method hierarchical reinforcement
learning general programmer imposes hierarchy hierarchy
constrains space possible policies may possible represent
optimal policy value function
maxq method constraints take two forms first within subtask
possible primitive actions may permitted example taxi task
navigate north south east west actions available pickup
putdown actions allowed second consider max node mj child nodes
fmj mjk g policy learned mj must involve executing learned policies
child nodes policy child node mji executed run enters
state tji hence policy learned mj must pass subset
terminal state sets ftj tjk g
ham method shares two constraints addition imposes
partial policy node policy subtask mi must deterministic
refinement given non deterministic initial policy node
option policy even constrained
two non primitive levels hierarchy subtasks lower level e
whose children primitive actions given complete policies programmer
hence learned policy upper level must constructed concatenating
given lower level policies order
purpose imposing constraints policy incorporate prior knowledge
thereby reduce size space must searched good policy
however constraints may make impossible learn optimal policy
learn optimal policy next best target would learn best
policy consistent e represented given hierarchy




definition hierarchically optimal policy mdp policy achieves
highest cumulative reward among policies consistent given hierarchy

parr b proves hamq learning converges probability
hierarchically optimal policy similarly given fixed set options sutton precup
singh prove smdp learning converges hierarchically
optimal value function incidentally primitive actions
made available trivial options smdp method converges optimal
policy however case hard say anything formal options speed
learning process may fact hinder hauskrecht et al
maxq decomposition represent value function hierarchical
policy could easily construct modified version hamq apply
learn hierarchically optimal policies maxq hierarchy however decided
pursue even weaker form optimality reasons become clear proceed
form optimality called recursive optimality


fidietterich

maxroot

g

qexit

qgotogoal

maxexit

maxgotogoal





qexitnorth

qexitsouth

qexiteast

north

qnorthg

south

qsouthg

qeastg

east

figure simple mdp left associated maxq graph right policy shown
left diagram recursively optimal hierarchically optimal shaded
cells indicate points locally optimal policy globally optimal

definition recursively optimal policy markov decision process maxq
decomposition fm mk g hierarchical policy f k g

subtask mi corresponding policy optimal smdp defined set states
si set actions ai state transition probability function p n js
reward function given sum original reward function r js pseudoreward function r

note state transition probability distribution p n js subtask mi
defined locally optimal policies fj g subtasks descendents mi
maxq graph hence recursive optimality kind local optimality
policy node optimal given policies children
reason seek recursive optimality rather hierarchical optimality recursive optimality makes possible solve subtask without reference context
executed context free property makes easier share use
subtasks turn essential successful use state abstraction
proceed describe learning recursive optimality let us see
recursive optimality differs hierarchical optimality
easy construct examples policies recursively optimal hierarchically optimal vice versa consider simple maze associated
maxq graph shown figures suppose robot starts somewhere left room
must reach goal g right room robot three actions north south
east actions deterministic robot receives reward move
let us define two subtasks


fimaxq hierarchical reinforcement learning

exit task terminates robot exits left room set pseudo

reward function r two terminal states e two states indicated

gotogoal task terminates robot reaches goal g
arrows figure locally optimal policy within room arrows
left seek exit left room shortest path specified
set pseudo reward function arrows right follow shortest
path goal fine however resulting policy neither hierarchically optimal
optimal
exists hierarchical policy would exit left room upper
door maxq value function decomposition represent value function
policy policy would locally optimal example states
shaded region would follow shortest path doorway hence
example illustrates recursively optimal policy hierarchically optimal
hierarchically optimal policy recursively optimal
consider moment see way fix value
upper starred state optimal hierarchical policy value lower
starred state hence changed r values instead zero
recursively optimal policy would hierarchically optimal globally optimal
words programmer guess right values terminal states
subtask recursively optimal policy hierarchically optimal
basic idea first pointed dean lin describe
makes initial guesses values starred states updates
guesses computed values starred states resulting recursivelyoptimal policy proved converge hierarchically optimal policy
drawback method requires repeated solution resulting hierarchical
learning yield speedup solving original

parr proposed interesting constructs set different r functions computes recursively optimal policy subtask
method chooses r functions way hierarchically optimal policy
approximated desired degree unfortunately method quite expensive
relies solving series linear programming requires time
polynomial several parameters including number states jsi j within subtask
discussion suggests principle possible learn good values
pseudo reward function practice must rely programmer specify single
pseudo reward function r subtask programmer wishes consider small
number alternative pseudo reward functions handled defining small
number subtasks identical except r functions permitting
learning choose one gives best recursively optimal policy
experiments employed following simplified defining
r subtask mi define two predicates termination predicate ti
goal predicate gi goal predicate defines subset terminal states goal
states pseudo reward terminal states fixed constant


fidietterich

pseudo reward e g set better terminate goal state
non goal state tested maxq method
worked well
experiments maxq found easy make mistakes
defining ti gi goal defined carefully easy create set subtasks
lead infinite looping example consider figure suppose
permit fourth action west mdp let us define termination goal
predicates right hand room satisfied iff robot reaches goal
exits room natural definition since quite similar definition
left hand room however resulting locally optimal policy room
attempt move nearest three locations goal upper door
lower door easily see states near goal policies
constructed maxroot loop forever first trying leave left room
entering right room trying leave right room entering left room
easily fixed defining goal predicate gi right room true
robot reaches goal g avoiding undesired termination bugs
hard complex domains
worst case possible programmer specify pseudo rewards
recursively optimal policy made arbitrarily worse hierarchically optimal
policy example suppose change original mdp figure state
immediately left upper doorway gives large negative reward l whenever
robot visits square rewards everywhere else hierarchicallyoptimal policy exits room lower door suppose programmer chosen
instead force robot exit upper door e g assigning pseudo reward
l leaving via lower door case recursively optimal policy leave
upper door suffer large l penalty making l arbitrarily large
make difference hierarchically optimal policy recursively optimal
policy arbitrarily large

maxq learning
understanding recursively optimal policies present two learning
first one called maxq applies case pseudo reward
function r zero first prove convergence properties
extended give second maxq q works general
pseudo reward functions
table gives pseudo code maxq maxq recursive function executes
current exploration policy starting max node state performs actions
reaches terminal state point returns count total number primitive
actions executed execute action maxq calls recursively
line recursive call returns updates value completion function
node uses count number primitive actions appropriately discount
value resulting state leaf nodes maxq updates estimated one step
expected reward v value fft learning rate parameter
gradually decreased zero limit


fimaxq hierarchical reinforcement learning

table maxq learning



















function maxq maxnode state
primitive maxnode
execute receive r observe state
vt fft vt fft rt
return
else
let count
ti false
choose action according current exploration policy x
let n maxq recursive call
observe state
ct fft ct fft n vt




count count n


end
return count

end maxq

main program
initialize v c j arbitrarily
maxq root node starting state

three things must specified order make description
complete
first keep pseudo code readable table ancestor termination handled recall action termination predicates
subroutines calling stack checked termination predicate one
satisfied calling stack unwound highest terminated subroutine cases c values updated subroutines interrupted
except follows subroutine invoked subroutine j j termination condition
satisfied subroutine update value c j
second must specify compute vt line since stored
max node computed following modified versions decomposition
equations


maxa qt composite

vt
primitive
qt vt ct

equations ect two important changes compared equations
first equation vt defined terms q value best action rather
action chosen fixed hierarchical policy second superscripts
current value function vt fixed hierarchical policy
compute vt equations must perform complete search
paths maxq graph starting node ending leaf nodes table

vt



fidietterich

table pseudo code greedy execution maxq graph
function evaluatemaxnode








primitive max node
return hvt ii
else
j ai
let hvt j aj evaluatemaxnode j
let j hg argmaxj vt j ct j
return hvt j hg ajhg
end evaluatemaxnode

gives pseudo code recursive function evaluatemaxnode implements depthfirst search addition returning vt evaluatemaxnode returns action
leaf node achieves value information needed maxq
useful later consider non hierarchical execution learned recursivelyoptimal policy
search computationally expensive future
develop ecient methods computing best path graph one
perform best first search use bounds values within subtrees
prune useless paths maxq graph better would make
computation incremental state environment changes
nodes whose values changed state change considered
possible develop ecient bottom method similar rete
successors used soar architecture forgy tambe rosenbloom

third thing must specified complete definition maxq
exploration policy x require x ordered glie policy

definition ordered glie policy glie policy greedy limit infinite

exploration converges limit ordered greedy policy greedy policy
imposes arbitrary fixed order available actions breaks ties favor
action appears earliest order

need property order ensure maxq converges uniquely defined
recursively optimal policy fundamental recursive optimality
general max node choice many different locally optimal policies given
policies adopted descendent nodes different locally optimal policies
achieve locally optimal value function give rise different probability transition functions p n js semi markov decision
defined next level node maxq graph differ depending
locally optimal policies chosen node differences may
lead better worse policies higher levels maxq graph even though make
difference inside subtask practice designer maxq graph need
design pseudo reward function subtask ensure locally optimal policies


fimaxq hierarchical reinforcement learning

equally valuable parent subroutine carry formal analysis
rely arbitrary tie breaking mechanism establish fixed ordering
max nodes maxq graph e g left right depth first numbering break ties
favor lowest numbered action defines unique policy max node
consequently induction defines unique policy entire maxq graph let
us call policy r use r subscript denote recursively optimal quantities
ordered greedy policy hence corresponding value function vr cr
qr denote corresponding completion function action value function prove
maxq converges r

theorem let hs p r p episodic mdp deterministic

policies proper discounted infinite horizon mdp discount factor let h
maxq graph defined subtasks fm mk g pseudo reward function
r zero let fft sequence constants max node



x
x
lim







lim









let x ordered glie policy node state assume
immediate rewards bounded probability maxq converges
r unique recursively optimal policy consistent h x

proof proof follows argument similar introduced prove convergence

q learning sarsa bertsekas tsitsiklis jaakkola et al
employ following stochastic approximation theory state without
proof

lemma proposition bertsekas tsitsiklis consider iteration
rt fft rt fft urt wt ut
let ft fr rt w wt fft ig entire history
iteration


fft satisfy conditions
b every noise terms wt satisfy e wt jft
c given norm jj jj rn exist constants b e wt jft
b jjrtjj
exists vector r positive vector scalar


jjurt rjj jjrt rjj

alternatively could break ties stochastic policy chose randomly among tied
actions



fidietterich

e exists nonnegative random sequence converges zero probability

jut j jjrt jj
rt converges r probability notation jj jj denotes weighted maximum
norm
ja j
jjajj max


structure proof theorem inductive starting leaves
maxq graph working toward root employ different time clock
node count number update steps performed maxq node
variable refer time clock current node
prove base case primitive max node note line maxq
standard stochastic approximation computing expected reward
performing action state therefore converges conditions given

prove recursive case consider composite max node child node j let
pt n js j transition probability distribution performing child action j state
time e following exploration policy descendent nodes node j
inductive assumption maxq applied j converge unique recursively optimal value function vr j probability furthermore maxq
following ordered glie policy j descendents converge executing greedy policy respect value functions pt n js j converge
pr n js j unique transition probability function executing child j
locally optimal policy r remains shown update assignment c
line maxq converges optimal cr function probability

prove apply lemma identify x lemma
state action pair vector rt completion cost table ct
fixed update steps vector r optimal completion cost
cr fixed define mapping u
uc

x









pr n js n max
c vr


c update mdp mi assuming descendent value functions
vr transition probabilities pr n js converged
apply lemma must first express c update formula form
update rule lemma let state performing state line
written

ct

fft ct fft n





max ct vt


fft ct fft uct wt ut


fimaxq hierarchical reinforcement learning







wt n max
c vt

x

n

ut

x

n
x

n









pt n js n max
c vt




pt n js n max
ct vt








pr n js n max
c vr


wt difference update node single sample
point drawn according pt n js update full distribution
pt n js value ut captures difference update
current probability transitions pt n js current value functions children
vt update optimal probability transitions pr n js
optimal values children vr
verify conditions lemma
condition assumed conditions theorem fft fft
condition b satisfied sampled pt n js expected value
difference zero
condition c follows directly fact jct j jvt j bounded
bounded episodic case discounted case
follows episodic case assumed policies proper hence trajectories
terminate finite time finite total reward discounted case infinite sum
future rewards bounded one step rewards bounded values c v
computed temporal averages cumulative rewards received finite number
bounded updates hence means variances maximum values
bounded
condition condition u weighted max norm pseudo contraction
derive starting weighted max norm q learning well known
q weighted max norm pseudo contraction bertsekas tsitsiklis
episodic case deterministic policies proper discount factor
infinite horizon discounted case exists positive
vector scalar

jjtqt qjj jjqt qjj



operator
tq

x

n

p n js n r js max
q


derive pseudo contraction c update operator u
plan first express u operator learning c terms operator
updating q values replace tq pseudo contraction equation q


fidietterich

learning uc u weighted max norm pseudo contraction
weights
recall eqn q c v furthermore u operator
performs updates optimal value functions child nodes write
qt ct v children node converged
q function version bellman equation mdp mi written

q

x

n

pr n js n vr max
q


noted vr plays role immediate reward function mi
therefore node operator rewritten
tq

x

n

pr js n vr max
q


replace q c vr obtain
tq

x

n

pr n js n vr max
c vr


note vr depend n move outside expectation
obtain
tq vr

x

n

pr n js n max
c vr


vr uc

abusing notation slightly express vector form tq vr uc
similarly write qt ct vr vector form qt ct vr
substitute two formulas max norm pseudo contraction formula
eqn obtain

jjvr uct cr vr jj jjvr ct cr vr jj
thus u weighted max norm pseudo contraction

jjuct cr jj jjct cr jj
condition satisfied
finally easy verify e important condition assumption
ordered glie policies child nodes converge probability locally optimal
policies children therefore pt n js converges pr n js n
probability vt converges probability vr child
actions therefore jut j converges zero probability trivially construct
sequence jut j bounds convergence

jut j jjct jj


fimaxq hierarchical reinforcement learning

verified conditions lemma conclude ct converges
cr probability induction conclude holds nodes
maxq including root node value function represented maxq graph
converges unique value function recursively optimal policy r q e
important aspect theorem proves q learning take
place levels maxq hierarchy simultaneously higher levels need
wait lower levels converged begin learning necessary
lower levels eventually converge locally optimal policies

techniques speeding maxq

maxq extended accelerate learning higher nodes graph
technique call states updating action chosen max node
state execution move environment sequence states
sn sn subroutines markovian resulting
state would reached started executing action state
state including sn hence execute version line maxq
intermediate states shown replacement pseudo code

j n
b
ct sj fft ct sj fft n j maxa qt
c
end
implementation composite action executed maxq constructs
linked list sequence primitive states visited list returned
composite action terminates parent max node process state
list shown parent max node concatenates state lists receives
children passes parent terminates experiments
employ states updating
kaelbling introduced related powerful method accelerating hierarchical reinforcement learning calls goals updating understand
method suppose primitive action several composite tasks could
invoked primitive action goals updating whenever primitive action
executed equivalent line maxq applied every composite task could
invoked primitive action sutton precup singh prove
composite tasks converge optimal q values goals updating furthermore point exploration policy employed choosing primitive
actions different policies subtasks learned
straightforward implement simple form goals updating within maxq
hierarchy case composite tasks invoke primitive actions whenever one
primitive actions executed state update c value parent
tasks invoke
however additional care required implement goals updating non primitive
actions suppose executing exploration policy following sequence world
states actions obtained ak sk ak sk let j composite task terminated state sk let sk n ak n ak ak sequence
actions could executed subtask j children words suppose












fidietterich

possible parse state action sequence terms series subroutine calls
returns one invocation subtask j possible parent task invokes j
update value c sk n j course order updates useful
exploration policy must ordered glie policy converge recursively
optimal policy subtask j descendents cannot follow arbitrary exploration
policy would produce accurate samples states drawn according
p n js j hence unlike simple case described sutton precup singh
exploration policy cannot different policies subtasks learned
although considerably reduces usefulness goals updating
completely eliminate simple way implementing non primitive goals updating
would perform maxq q learning usual whenever subtask j invoked
state returned could update value c j potential calling subtasks
implemented however complexity involved identifying
possible actual parameters potential calling subroutines

maxq q learning
shown convergence maxq let us design learning
work arbitrary pseudo reward functions r could add pseudoreward maxq would effect changing mdp
different reward function pseudo rewards contaminate values
completion functions computed hierarchy resulting learned policy
recursively optimal original mdp
solved learning one completion function use inside
max node separate completion function use outside max node quantities used inside node written tilde r c q quantities used
outside node written without tilde
outside completion function c completion function
discussing far computes expected reward completing task
mi performing action state following learned policy mi
computed without reference r completion function used parent
tasks compute v expected reward performing action starting state
second completion function c completion function use
inside node order discover locally optimal policy task mi function
incorporate rewards real reward function r js
pseudo reward function r used evaluatemaxnode line
choose best action j hg execute note however evaluatemaxnode still
return external value vt j hg chosen action
employ two different update rules learn two completion functions
c function learned update rule similar q learning rule line
maxq c function learned update rule similar sarsa
purpose learn value function policy discovered optimizing c
pseudo code resulting maxq q shown table
key step lines line maxq q first updates c value
greedy action resulting state update includes pseudo reward r


fimaxq hierarchical reinforcement learning

table maxq q learning
























function maxq q maxnode state
let seq sequence states visited executing
primitive maxnode
execute receive r observe state
vt fft vt fft rt
push onto beginning seq
else
let count
ti false
choose action according current exploration policy x
let childseq maxq q childseq sequence states visited


executing action reverse order
observe state
let argmaxa c vt
let n
childseq
c fft c fft n r c vt
ct fft ct fft n ct vt
n n
end
append
childseq onto front seq

end
end else
return seq
end maxq q


line maxq q updates c greedy action even would
greedy action according uncontaminated value function update
course include pseudo reward function
important note whereever vt appears pseudo code refers
uncontaminated value function state executing max node
computed recursively exactly way maxq
finally note pseudo code incorporates states updating call
maxq q returns list states visited execution
updates lines performed states list states
ordered recent first states updated starting last state visited
working backward starting state helps speed
maxq q converged resulting recursively optimal policy computed
node choosing action maximizes q c v breaking
ties according fixed ordering established ordered glie policy
reason gave name max nodes nodes represent subtasks
learned policies within maxq graph q node j parent node stores
c j c j computes q j q j invoking child
max node j max node takes maximum q values computes
v computes best action q


fidietterich

corollary conditions theorem maxq q converges unique

recursively optimal policy mdp defined maxq graph h pseudo reward functions
r ordered glie exploration policy x
proof argument identical tedious proof theorem
proof convergence c values identical original proof c values
relies proving convergence c values well follows
weighted max norm pseudo contraction argument q e

state abstraction

many reasons introduce hierarchical reinforcement learning perhaps
important reason create opportunities state abstraction introduced
simple taxi figure pointed within subtask ignore
certain aspects state space example performing maxnavigate
taxi make navigation decisions regardless whether passenger
taxi purpose section formalize conditions safe
introduce state abstractions convergence proofs maxq q
extended prove convergence presence state abstraction specifically
identify five conditions permit safe introduction state abstractions
throughout section use taxi running example
see five conditions permit us reduce number distinct values
must stored order represent maxq value function decomposition
establish starting point let us compute number values must stored
taxi without state abstraction
maxq representation must tables c functions internal
nodes v functions leaves first six leaf nodes store v
must store values node states locations possible
destinations passenger possible current locations passenger four
special locations inside taxi second root node two children
requires values third maxget maxput nodes
actions one requires values total finally maxnavigate
four actions must consider target parameter take
four possible values hence effectively combinations states values
action total values must represented total therefore maxq
representation requires separate quantities represent value function
place number perspective consider q learning representation must
store separate value six primitive actions possible states
total values hence see without state abstraction maxq
representation requires four times memory q table

five conditions permit state abstraction

introduce five conditions permit introduction state abstractions
condition give definition prove lemma states condition satisfied value function corresponding class policies


fimaxq hierarchical reinforcement learning

represented abstractly e abstract versions v c functions condition provide rules identifying condition satisfied
give examples taxi domain
begin introducing definitions notation

definition let mdp h maxq graph defined suppose

state written vector values set state variables max
node suppose state variables partitioned two sets xi yi let
function projects state onto values variables xi h combined
called state abstracted maxq graph

cases state variables partitioned often write x
mean state represented vector values state variables x
vector values state variables similarly sometimes write
p x n jx v x r x place p n js v r
respectively

definition abstract policy abstract hierarchical policy mdp state

abstracted maxq graph h associated abstraction functions hierarchical policy
policy corresponding subtask mi satisfies condition two
states stochastic policy
exploration policy interpreted mean probability distributions
choosing actions states

order maxq q converge presence state abstractions require
times instantaneous exploration policy abstract hierarchical policy
one way achieve construct exploration policy uses information relevant state variables deciding action perform boltzmann exploration state abstracted q values greedy exploration counter
exploration abstracted states abstract exploration policies counter
exploration full state space abstract exploration policy
introduced notation let us describe analyze five abstraction conditions identified three different kinds conditions
abstractions introduced first kind involves eliminating irrelevant variables
within subtask maxq graph form abstraction nodes toward
leaves maxq graph tend relevant variables nodes higher
graph relevant variables hence kind abstraction useful
lower levels maxq graph
second kind abstraction arises funnel actions macro actions
move environment large number initial states small number
resulting states completion cost subtasks represented number
values proportional number resulting states funnel actions tend appear higher
maxq graph form abstraction useful near root graph
third kind abstraction arises structure maxq graph
exploits fact large parts state space subtask may reachable
termination conditions ancestors maxq graph


fidietterich

begin describing two abstraction conditions first type present
two conditions second type finally describe one condition third type
condition max node irrelevance

first condition arises set state variables irrelevant max node

definition max node irrelevance let mi max node maxq graph h

mdp set state variables irrelevant node state variables
partitioned two sets x stationary abstract hierarchical
policy executed descendents following two properties hold

state transition probability distribution p n js node factored
product two distributions

p x n jx p jx p x n jx



give values variables x x give values
variables x

pair states x x x
child action v v r r

note two conditions must hold stationary abstract policies executed
descendents subtask discuss rather strong
requirements satisfied practice first however prove conditions
sucient permit c v tables represented state abstractions

lemma let mdp full state maxq graph h suppose state vari

ables yi irrelevant max node let x associated abstraction function
projects onto remaining relevant variables xi let abstract hierarchical
policy action value function q node represented compactly
one value completion function c j equivalence class states
share values relevant variables
specifically q j computed follows

q j v j c j


c x j

x

x n

p x n jx j n v x x r x c x x

v j x v j x r x r x x x arbitrary
value irrelevant state variables yi


fimaxq hierarchical reinforcement learning

proof define mdp mi node follows
states x fx j x g
actions
transition probabilities p x n jx
reward function v x r x

abstract policy decisions states x
x therefore well defined policy mi action value function
mi unique solution following bellman equation
x
q x j v j x p x n jx j n r x q x x

x n

compare bellman equation mi
x
q j v j p n js j n r q
n



note v j v j v j x r r r x furthermore know distribution p factored separate distributions yi
xi hence rewrite
x
x
q j v j x p jx j p x n jx j n r x q


x n

right sum depend sum evaluates
eliminated give
x
q j v j x p x n jx j n r x q

x n

finally note equations identical except expressions
q values since solution bellman equation unique must conclude
q j q j
rewrite right hand side obtain
q j v j c j

x
c x j p x n jx j n v x x r x c x x

q e

x n

course primarily interested able discover represent optimal
policy node following corollary shows optimal policy abstract
policy hence represented abstractly


fidietterich

corollary consider conditions lemma change ab

stract hierarchical policy executed descendents node node
let ordering actions optimal ordered policy node
abstract policy action value function represented abstractly

proof define policy optimal ordered policy abstract mdp

let q x j corresponding optimal action value function
argument given q solution optimal bellman equation
original mdp means policy defined optimal

ordered policy construction abstract policy q e
stated max node irrelevance condition appears quite dicult satisfy since
requires state transition probability distribution factor x components
possible abstract hierarchical policies however practice condition often
satisfied
example let us consider navigate subtask source destination
passenger irrelevant achievement subtask policy successfully completes subtask value function regardless source
destination locations passenger abstracting away passenger source destination obtain huge savings space instead requiring values represent
c functions task require values actions locations possible
values
advantages form abstraction similar obtained boutilier
dearden goldszmidt belief network actions exploited
simplify value iteration stochastic indeed one way understanding
conditions definition express form decision diagram shown
figure diagram shows irrelevant variables affect rewards
directly indirectly therefore affect value function
optimal policy
one rule noticing cases abstraction condition holds examine
subgraph rooted given max node set state variables irrelevant leaf
state transition probabilities reward functions pseudo reward functions
termination conditions subgraph variables satisfy max node
irrelevance condition

lemma let mdp associated maxq graph h let max node

h let xi yi partition state variables set state variables yi
irrelevant node
primitive leaf node descendent
p x jx p jx p x jx
r x jx r x jx

internal node j equal node descendent r j x
r j x termination predicate tj x true iff tj x


fimaxq hierarchical reinforcement learning

j

v

x

x





figure dynamic decision diagram represents conditions definition
probabilistic nodes x represent state variables time nodes
x represent state variables later time n square action
node j chosen child subroutine utility node v represents value
function v j x child action note x may uence
cannot affect x therefore cannot affect v

proof must abstract hierarchical policy give rise smdp

node whose transition probability distribution factors whose reward function depends
xi definition abstract hierarchical policy choose actions
upon information xi primitive probability transition functions factor
independent component xi since termination conditions nodes
variables xi probability transition function pi x n jx
must factor pi jx pi x n jx similarly reward functions
v j x must equal v j x rewards received within subtree
leaves pseudo rewards depend variables xi therefore
variables yi irrelevant max node q e
taxi task primitive navigation actions north south east west
depend location taxi location passenger pseudoreward function termination condition maxnavigate node depend
location taxi parameter hence lemma applies passenger
source destination irrelevant maxnavigate node
condition leaf irrelevance

second abstraction condition describes situations apply state abstractions leaf nodes maxq graph leaf nodes obtain stronger
lemma slightly weaker definition irrelevance


fidietterich

definition leaf irrelevance set state variables irrelevant primitive
action maxq graph states expected value reward function
x
v p js r js


depend values state variables words
pair states differ values variables
x



p js r js

x



p js r js

condition satisfied leaf following lemma shows
represent value function v compactly

lemma let mdp full state maxq graph h suppose state vari

ables irrelevant leaf node let x associated abstraction function
projects onto remaining relevant variables x represent v
state abstracted value function v v x

proof according definition leaf irrelevance two states differ

irrelevant state variables value v hence represent
unique value v x q e
two rules finding cases leaf irrelevance applies first rule shows
probability distribution factors leaf irrelevance

lemma suppose probability transition function primitive action p js factors p x jx p jx p x jx reward function satisfies r js
r x jx variables irrelevant leaf node
proof plug definition v simplify
x
v
p js r js






x

x
x



x

x

p jx p x jx r x jx

p jx

x

x

p x jx r x jx

p x jx r x jx

hence expected reward action depends variables x
variables q e
second rule shows reward function primitive action constant
apply state abstractions even p js factor

lemma suppose r js reward function action mdp equal

constant ra entire state irrelevant primitive action


fimaxq hierarchical reinforcement learning

proof
v

x



p js r js

x


p js ra


ra
depend entire state irrelevant primitive action q e
lemma satisfied four leaf nodes north south east west taxi
task one step reward constant hence instead requiring
values store v functions need values one action similarly
expected rewards pickup putdown actions require values depending
whether corresponding actions legal illegal hence together require
values instead values
condition distribution irrelevance

consider condition funnel actions

definition distribution irrelevance set state variables yj irrelevant distribution action j abstract policies executed node j
descendents maxq hierarchy following holds pairs states
differ values state variables yj

p n js j p n js j
n

condition satisfied subtask j c value parent task
represented compactly

lemma let mdp full state maxq graph h suppose set

state variables yj irrelevant distribution action j child max
node let ij associated abstraction function ij x define
abstract completion cost function c ij j states

c j c ij j

proof completion function fixed policy defined follows
x
c j p n js j n q
n



consider two states ij ij x distribution irrelevance transition probability distributions hence
right hand sides value conclude

c j c j


fidietterich

therefore define abstract completion function c x j represent quantity q e
undiscounted cumulative reward definition distribution irrelevance weakened eliminate n number steps needed
pairs states differ irrelevant state variables
p js j p js j undiscounted case lemma still holds
revised definition
might appear distribution irrelevance condition would rarely satisfied often cases condition true consider example get
subroutine taxi task matter location taxi state taxi
passenger starting location get finishes executing e
taxi completed picking passenger hence starting location
irrelevant resulting location taxi p js get p js get
states differ taxi location
note however maximizing discounted reward taxi location would
irrelevant probability get terminate exactly n steps would
depend location taxi could differ states different values
n produce different amounts discounting hence cannot ignore
taxi location representing completion function get
undiscounted case applying lemma represent c root get
distinct values equivalence classes states source locations
times destination locations much less quantities unabstracted
representation
note although state variables may irrelevant distribution
subtask j may important within subtask j taxi task location
taxi critical representing value v get irrelevant state
distribution get therefore irrelevant representing c root get hence
maxq decomposition essential obtaining benefits distribution irrelevance
funnel actions arise many hierarchical reinforcement learning example abstract actions move robot doorway move car onto entrance
ramp freeway property distribution irrelevance condition
applicable situations long undiscounted setting
condition termination

fourth condition closely related funnel property applies subtask
guaranteed cause parent task terminate goal state sense subtask
funneling environment set states described goal predicate
parent task

lemma termination let mi task maxq graph states
goal predicate gi true pseudo reward function r suppose
child task state hierarchical policies

pi n js gi


fimaxq hierarchical reinforcement learning

e every possible state applying make goal predicate
gi true
policy executed node completion cost c zero
need explicitly represented

proof action executed state guaranteed state

gi true definition goal states satisfy termination predicate ti
task terminate gi true terminal pseudo reward zero
hence completion function zero q e
example taxi task states taxi holding passenger
put subroutine succeed goal terminal state root
termination predicate put e passenger destination location
implies goal condition root means c root put
uniformly zero states put terminated
easy detect cases termination condition satisfied need
compare termination predicate ta subtask goal predicate gi parent
task first implies second termination lemma satisfied
condition shielding

shielding condition arises structure maxq graph
lemma shielding let mi task maxq graph state
paths root graph node mi subtask j possibly equal
whose termination predicate tj true q nodes mi need represent
c values state
proof order task executed state must exist path ancestors
task leading root graph ancestor tasks
terminated condition lemma guarantees false hence task
cannot executed state therefore c values need represented q e
termination condition shielding condition verified analyzing
structure maxq graph identifying nodes whose ancestor tasks terminated
taxi domain simple example arises put task terminated
states passenger taxi means need
represent c root put states combined
termination condition need explicitly represent completion function
put
dicussion

applying five abstraction conditions obtain following safe state abstractions taxi task
north south east west terminal nodes require one quantity
total four values leaf irrelevance


fidietterich

pickup putdown require values legal illegal states total four
leaf irrelevance

qnorth qsouth qeast qwest require values four values
locations max node irrelevance

qnavigateforget requires values four possible source locations passenger destination max node irrelevant maxget taxi starting location
distribution irrelevant navigate action

qpickup requires possible values possible source locations possible taxi
locations passenger destination max node irrelevant maxget

qget requires possible values source locations destination locations
distribution irrelevance

qnavigateforput requires values four possible destination locations

passenger source destination max node irrelevant maxput taxi
location distribution irrelevant navigate action

qputdown requires possible values taxi locations possible destination locations passenger source max node irrelevant maxput

qput requires values termination shielding
gives total distinct values much less values required
q learning hence see applying state abstractions maxq
representation give much compact representation value function
key thing note state abstractions value function decomposed sum terms single term depends entire state mdp
even though value function whole depend entire state mdp
example consider state described figures showed
value state passenger r destination b taxi
decomposed

v root v north c navigate r north
c get navigate r c root get
state abstractions see term right hand side depends
subset features

v north constant
c navigate r north depends taxi location passenger source
location

c get navigate r depends source location
c root get depends passenger source destination


fimaxq hierarchical reinforcement learning

without maxq decomposition features irrelevant value function depends entire state
prior knowledge required part programmer order identify
state abstractions suces know qualitative constraints one step
reward functions one step transition probabilities termination predicates goal
predicates pseudo reward functions within maxq graph specifically max
node irrelevance leaf irrelevance conditions require simple analysis one step
transition function reward pseudo reward functions opportunities apply
distribution irrelevance condition found identifying funnel effects
definitions termination conditions operators similarly
shielding termination conditions require analysis termination predicates
subtasks hence applying five conditions introduce state abstractions
straightforward process model one step transition reward functions
learned abstraction conditions checked see satisfied

convergence maxq q state abstraction

shown state abstractions safely introduced maxq value
function decomposition five conditions described however conditions guarantee value function fixed abstract hierarchical policy
represented recursively optimal policies represented
maxq q learning recursively optimal policy
forced use state abstractions goal section prove two
ordered recursively optimal policy abstract policy hence
represented state abstractions b maxq q converge
policy applied maxq graph safe state abstractions

lemma let mdp full state maxq graph h abstract state maxq
graph h abstractions satisfy five conditions given let
ordering actions maxq graph following statements true
unique ordered recursively optimal policy r defined h abstract policy e depends relevant state variables node see
definition
c v functions h represent projected value function r

proof five abstraction lemmas tell us ordered recursively optimal policy
abstract c v functions h represent value function hence
heart lemma first claim last two forms abstraction shielding
termination place restrictions abstract policies ignore
proof
proof induction levels maxq graph starting leaves
base case let us consider max node whose children primitive actions
case policies executed within children max node hence variables
yi irrelevant node apply abstraction lemmas represent
value function policy node abstract policies consequently value


fidietterich

function optimal policy node represented property

q q

states
let us impose action ordering compute optimal ordered policy consider
two actions e prefers suppose
tie q function state values

q q
two actions maximize q state optimal ordered
policy must choose states
established q values hence tie exist
hence optimal ordered policy must make choice
states hence optimal ordered policy node abstract policy
let us turn recursive case max node make inductive assumption
ordered recursively optimal policy abstract within descendent nodes consider
locally optimal policy node set state variables irrelevant
node corollary tells us q j q j states
similarly set variables irrelevant distribution
particular action j lemma tells us thing hence ordering
argument given ordered optimal policy node must abstract induction
proves lemma q e
lemma established combination mdp abstract
maxq graph h action ordering defines unique recursively optimal ordered abstract policy ready prove maxq q converge policy

theorem let hs p r p episodic mdp deterministic

policies proper discounted infinite horizon mdp discount factor let h
unabstracted maxq graph defined subtasks fm mk g pseudo reward
functions r let h state abstracted maxq graph defined applying state
abstractions node h five conditions given let x
abstract ordered glie exploration policy node state whose decisions
depend relevant state variables node let r unique recursivelyoptimal hierarchical policy defined x r probability
maxq q applied h converges r provided learning rates fft satisfy
equation one step rewards bounded

proof rather repeating entire proof maxq q describe

must change state abstraction last two forms state abstraction refer states
whose values inferred structure maxq graph therefore
need represented since values updated maxq q
ignore consider first three forms state abstraction turn
begin considering primitive leaf nodes let leaf node let set
state variables leaf irrelevant let x x two states


fimaxq hierarchical reinforcement learning

differ values leaf irrelevance probability transitions
p js p js need expected reward performing
states must maxq q visits abstract state x
know value part state abstracted away nonetheless
draws sample according p jx receives reward r jx updates
estimate v x line maxq q let pt probability maxq q
visiting x given unabstracted part state x line maxq q
computing stochastic approximation
x

n

write

x



pt pt n jx r jx

pt

x

n

pt n jx r jx

according leaf irrelevance inner sum value states
x call value r x gives
x



pt r x

equal r x distribution pt hence maxq q converges leaf
irrelevance abstractions
let us turn two forms abstraction apply internal nodes max node
irrelevance distribution irrelevance consider smdp defined node
abstracted maxq graph time maxq q would ordinary smdp
transition probability function pt x n jx reward function vt x r x
except maxq q draws samples state transitions drawn according
distribution pt n js original state space prove theorem must
drawing n according second distribution equivalent drawing
x n according first distribution
max node irrelevance know abstract policies applied node
descendents transition probability distribution factors

p n js p jx p x n jx
exploration policy abstract policy pt n js factors way
means yi components state cannot affect xi components hence
sampling pt n js discarding yi values gives samples pt x n jx
therefore maxq q converge max node irrelevance abstractions
finally consider distribution irrelevance let j child node suppose
yj set state variables irrelevant distribution j
smdp node wishes draw sample pt x n jx j know
current value irrelevant part current state however
matter distribution irrelevance means possible values
pt x n jx j hence maxq q converge distribution
irrelevance abstractions


fidietterich

three cases maxq q converge locally optimal ordered policy
node maxq graph lemma produces locally optimal ordered
policy unabstracted smdp node hence induction maxq q converge
unique ordered recursively optimal policy r defined maxq q h mdp
ordered exploration policy x q e

hierarchical credit assignment

still situations would introduce state abstractions
five properties described permit consider following
modification taxi suppose taxi fuel tank time
taxi moves one square costs one unit fuel taxi runs fuel
delivering passenger destination receives reward trial
ends fortunately filling station taxi execute fillup action fill
fuel tank
solve modified maxq hierarchy introduce another
subtask refuel goal moving taxi filling station filling
tank maxrefuel child maxroot invokes navigate bound
location filling station move taxi filling station
introduction fuel possibility might run fuel means
must include current amount fuel feature representing every c value
internal nodes v value leaf nodes throughout maxq graph
unfortunate intuition tells us amount fuel uence
decisions inside navigate subtask taxi enough
fuel reach target case chosen navigation actions depend
fuel else taxi enough fuel hence fail reach regardless
navigation actions taken words navigate subtask
need worry amount fuel even enough fuel
action navigate take get fuel instead top level subtasks
monitoring amount fuel deciding whether go refuel go pick
passenger go deliver passenger
given intuition natural try abstracting away amount remaining
fuel within navigate subtask however work taxi
runs fuel reward given qnorth qsouth qeast qwest nodes
cannot explain reward received consistent way
setting c tables predict negative reward occur c
values ignore amount fuel tank stated formally diculty
max node irrelevance condition satisfied one step reward function
r js actions depends amount fuel
call hierarchical credit assignment fundamental issue
maxq decomposition information rewards stored leaf nodes
hierarchy would separate basic rewards received navigation
e action reward received exhausting fuel make
reward leaves depend location taxi max node irrelevance
condition satisfied


fimaxq hierarchical reinforcement learning

one way programmer manually decompose reward function

indicate nodes hierarchy receive reward let r js
p


r js decomposition reward function r js specifies
part reward must handled max node modified taxi
example decompose reward leaf nodes receive original
penalties fuel rewards must handled maxroot lines
maxq q easily modified include r js
domains believe easy designer hierarchy decompose
reward function straightforward studied
however interesting future develop
solve hierarchical credit assignment autonomously

non hierarchical execution maxq hierarchy

point focused exclusively representing learning
hierarchical policies however often optimal policy mdp strictly hierarchical kaelbling first introduced idea deriving non hierarchical policy
value function hierarchical policy section exploit maxq decomposition
generalize ideas apply recursively levels hierarchy
describe two methods non hierarchical execution
first method dynamic programming known policy
iteration policy iteration starts initial policy repeats
following two steps policy converges policy evaluation step computes
value function v k current policy k policy improvement step
computes policy k according rule

k argmax


x



p js r js v k



howard proved k optimal policy k guaranteed
improvement note order apply method need know transition
probability distribution p js reward function r js
know p js r js use maxq representation value
function perform one step policy iteration start hierarchical policy
represent value function maxq hierarchy e g could learned via
maxq q perform one step policy improvement applying equation
v computed maxq hierarchy compute v

corollary let g argmaxa ps p js r js v v
value function computed maxq hierarchy primitive action
optimal policy g strictly better least one state

proof direct consequence howard policy improvement theorem q e
unfortunately iterate policy improvement process policy

g unlikely hierarchical policy e unlikely representable


fidietterich

table procedure executing one step greedy policy
procedure executehgpolicy

repeat

let hv ai evaluatemaxnode



execute primitive action
let resulting state
end executehgpolicy

terms local policies node maxq graph nonetheless one step policy
improvement give significant improvements
non hierarchical execution ignores internal structure maxq
graph effect maxq hierarchy viewed way represent v
representation would give one step improved policy g
second non hierarchical execution borrows idea q learning
one great beauties q representation value functions compute
one step policy improvement without knowing p js simply taking policy
g argmaxa q gives us one step greedy policy
computed one step lookahead maxq decomposition perform
policy improvement steps levels hierarchy
already defined function need table presented function
evaluatemaxnode given current state conducts search along paths
given max node leaves maxq graph finds path
best value e maximum sum c values along path plus v value
leaf equivalent computing best action greedily level
maxq graph addition evaluatemaxnode returns primitive action end
best path action would first primitive action executed
learned hierarchical policy executed starting current state second method
non hierarchical execution maxq graph call evaluatemaxnode
state execute primitive action returned pseudo code shown
table
call policy computed executehgpolicy hierarchical greedy policy
denote hg superscript indicates computing greedy
action time step following theorem shows give better policy
original hierarchical policy

theorem let g maxq graph representing value function hierarchical policy

e terms c j computed j let v hg value
computed executehgpolicy line let hg resulting policy define
v hg value function hg states case
v v hg v hg


proof sketch left inequality equation satisfied construction line
evaluatemaxnode see consider original hierarchical policy


fimaxq hierarchical reinforcement learning

viewed choosing path maxq graph running root one
leaf nodes v sum c values along chosen path plus
v value leaf node contrast evaluatemaxnode performs traversal
paths maxq graph finds best path path largest
sum c leaf v values hence v hg must least large v
establish right inequality note construction v hg value function
policy call hg chooses one action greedily level maxq graph
recursively follows thereafter consequence fact line
evaluatemaxnode c right hand side c represents cost
completing subroutine following following greedier policy
table c written ct however execute executehgpolicy
hence execute hg opportunity improve upon hg time step
hence v hg underestimate actual value hg q e
note theorem works one direction says state
v hg v greedy policy hg strictly better
however could optimal policy yet structure maxq
graph prevents us considering action primitive composite would
improve hence unlike policy improvement theorem howard primitive
actions eligible chosen guarantee suboptimal
hierarchically greedy policy strict improvement
contrast perform one step policy improvement discussed start
section corollary guarantees improve policy see
general neither two methods non hierarchical execution better
nonetheless first method operates level individual primitive
actions able produce large improvements policy contrast
hierarchical greedy method obtain large improvements policy changing
actions e subroutines chosen near root hierarchy hence general
hierarchical greedy execution probably better method course value functions
methods could computed one better estimated value could
executed
sutton et al simultaneously developed closely related method nonhierarchical execution macros method equivalent executehgpolicy
special case maxq hierarchy one level subtasks interesting
aspect executehgpolicy permits greedy improvements levels
tree uence action chosen
care must taken applying theorem maxq hierarchy whose c values
learned via maxq q online maxq q correctly learned values states nodes maxq graph example
taxi value c put qputdown learned well except
four special locations r g b put subtask cannot
executed passenger taxi usually means get
completed taxi passenger source location exploration children put tried states putdown usually fail receive negative
reward whereas navigate eventually succeed perhaps lengthy exploration


fidietterich

take taxi destination location states updating values
c put navigate learned states along path
passenger destination c values putdown action learned
passenger source destination locations hence train maxq representation hierarchical execution maxq q switch hierarchically greedy
execution quite bad particular need introduce hierarchicallygreedy execution early enough exploration policy still actively exploring
theory glie exploration policy never ceases explore practice want
good policy quickly asymptotically
course alternative would use hierarchically greedy execution
beginning learning however remember higher nodes maxq hierarchy
need obtain samples p n js child action hierarchical greedy
execution interrupts child reached terminal state e state
along way another subtask appears better evaluatemaxnode samples
cannot obtained hence important begin purely hierarchical execution
training make transition greedy execution point
taken implement maxq q way
specify number primitive actions l taken hierarchically hierarchical execution interrupted control returns top level action
chosen greedily start l set large execution completely
hierarchical child action invoked committed execute action
terminates however gradually reduce l becomes point
hierarchical greedy execution time reaches time
boltzmann exploration cools temperature exploration effectively
halted experimental generally gives excellent
little added exploration cost

experimental evaluation maxq method
performed series experiments maxq method three goals
mind understand expressive power value function decomposition b
characterize behavior maxq q learning c assess relative
importance temporal abstraction state abstraction non hierarchical execution
section describe experiments present

fickle taxi task
first experiments performed modified version taxi task version
incorporates two changes task described section first four
navigation actions noisy probability moves intended direction
probability instead moves right intended direction
probability moves left purpose change create realistic
dicult challenge learning second change
taxi picked passenger moved one square away passenger source
location passenger changes destination location probability


fimaxq hierarchical reinforcement learning

purpose change create situation optimal policy hierarchical
policy effectiveness non hierarchical execution measured
compared four different configurations learning q learning
b maxq q learning without form state abstraction c maxq q learning
state abstraction maxq q learning state abstraction greedy execution
configurations controlled many parameters include following
initial values q c functions b learning rate employed fixed
learning rate c cooling schedule boltzmann exploration glie policy
employed non hierarchical execution schedule decreasing l number
steps consecutive hierarchical execution optimized settings separately
configuration goal matching exceeding primitive training
actions possible best policy could code hand boltzmann exploration
established initial temperature cooling rate separate temperature
maintained max node maxq graph temperature reduced
multiplying cooling rate time subtask terminates goal state
process optimizing parameter settings time consuming
q learning maxq q critical parameter schedule
cooling temperature boltzmann exploration cooled rapidly
converge suboptimal policy case tested nine different
cooling rates choose different cooling rates subtasks started
fixed policies e g random hand coded subtasks except subtasks
closest leaves chosen schedules subtasks allowed
parent tasks learn policies tuned cooling rates one
nice effect method cooling temperature subtask terminates
naturally causes subtasks higher maxq graph cool slowly meant
good could often obtained cooling rate max
nodes
choice learning rate easier since determined primarily degree
stochasticity environment tested three four different rates
configuration initial values q c functions set knowledge
experiments required
took care tuning parameters experiments one would
normally take real application wanted ensure method
compared best possible conditions general form particularly
speed learning wide ranges cooling rate learning rate
parameter settings
following parameters selected tuning experiments q
learning initial q values states learning rate boltzmann exploration
initial temperature cooling rate use initial values
end signature debugging detect weight modified
maxq q learning without state abstraction used initial values learning rate boltzmann exploration initial temperature cooling
rates maxroot maxput maxget maxnavigate


fidietterich


maxq abstract

mean cumulative reward


maxq
abstract
greedy



maxq
abstract



flat q















primitive actions





figure comparison performance hierarchical maxq q learning without state abstractions state abstractions state abstractions combined
hierarchical greedy evaluation q learning
maxq q learning state abstraction used initial values learning
rate boltzmann exploration initial temperature cooling rates
maxroot maxput maxget maxnavigate
maxq q learning non hierarchical execution used settings
state abstraction addition initialized l decreased
trial reached trials execution completely greedy
figure shows averaged training runs training run involves
performing repeated trials convergence different trials execute different
numbers primitive actions plotted number primitive actions
horizontal axis rather number trials
first thing note forms maxq learning better initial performance
q learning constraints introduced maxq hierarchy
example agent executing navigate subtask never attempt pickup
putdown passenger actions available navigate similarly
agent never attempt putdown passenger first picked passenger
vice versa termination conditions get put subtasks
second thing notice without state abstractions maxq q learning actually takes longer converge flat q curve crosses maxq abstraction


fimaxq hierarchical reinforcement learning

curve shows without state abstraction cost learning huge number
parameters maxq representation really worth benefits suspect
consequence model free nature maxq q maxq decomposition represents information redundantly example cost performing
put subtask computed c root get v put model
could compute learned model maxq q must learn
separately experience
third thing notice state abstractions maxq q converges
quickly hierarchically optimal policy seen clearly figure
focuses range reward values neighborhood optimal policy
see maxq abstractions attains hierarchically optimal policy
approximately steps whereas q learning requires roughly twice long reach
level however q learning course continue onward reach optimal
performance whereas maxq hierarchy best hierarchical policy slow
respond fickle behavior passenger changes destination
last thing notice greedy execution maxq policy able
attain optimal performance execution becomes greedy
temporary drop performance maxq q must learn c values regions
state space visited recursively optimal policy despite
drop performance greedy maxq q recovers rapidly reaches hierarchically optimal
performance faster purely hierarchical maxq q learning hence added
cost terms exploration introducing greedy execution
experiment presents evidence favor three claims first hierarchical reinforcement learning much faster q learning second state abstraction
required maxq q learning good performance third non hierarchical
execution produce significant improvements performance little added
exploration cost

kaelbling hdg method
second task consider simple maze task introduced leslie kaelbling
shown figure trial task agent starts randomlychosen state must move randomly chosen goal state usual north south
east west operators employed deterministic operators small cost
move agent must minimize undiscounted sum costs
goal state different locations actually
different mdps kaelbling hdg method starts choosing arbitrary set landmark
states defining voronoi partition state space manhattan distances
landmarks e two states belong voronoi cell iff
nearest landmark method defines one subtask landmark l subtask
move state current voronoi cell neighboring voronoi cell
landmark l optimal policies subtasks computed
hdg policies subtasks solve abstract markov decision
moving landmark state landmark state subtask
solutions macro actions subroutines computes value function mdp


fidietterich



maxq abstract greedy

mean cumulative reward



optimal policy
flat q
hier optimal policy


maxq abstract

maxq abstract














primitive actions





figure close view previous figure figure shows two horizontal lines
indicating optimal performance hierarchically optimal performance
domain make figure readable applied step moving
average data points average runs
finally possible destination location g within voronoi cell landmark l
hdg method computes optimal policy getting l g
combining subtasks hdg method construct good approximation
optimal policy follows addition value functions discussed
agent maintains two functions nl name landmark nearest state
n l list landmarks cells immediate neighbors cell l
combining agent build list state current landmark
landmarks neighboring cells landmark agent computes sum
three terms
expected cost reaching landmark
expected cost moving landmark landmark goal cell
expected cost moving goal cell landmark goal state
note terms exact estimates term computed
landmark subtasks subroutines means corresponding path must pass
intermediate landmark states rather going directly goal landmark


fimaxq hierarchical reinforcement learning































figure kaelbling navigation task circled state landmark state
heavy lines boundaries voronoi cells episode
start state goal state chosen random figure start state
shown black square goal state shown black hexagon

hence term typically overestimate required distance note
choices intermediate landmarks need explicitly
included computation best action agent enters cell containing
goal
given information agent chooses move toward best landmarks
unless agent already goal voronoi cell case agent moves toward
goal state example figure term cost reaching landmark
row column term cost getting row column
landmark row column going one landmark another case
best landmark landmark path go directly row column row column
hence term term cost getting row column goal
sum comparison optimal path
length
kaelbling experiments employed variation q learning learn terms
computed regular intervals via floyd warshall sources
shortest paths
figure shows maxq solving overall task root
takes one argument g specifies goal cell three subtasks


fidietterich

maxroot g
gl nl g

qgotogoallmk gl

qgotogoal g

maxgotogoallmk gl

qgotolmk l gl

maxgotolmk l

qnorthlmk l

qsouthlmk l

maxgotogoal g

qeastlmk l

north

qwestlmk l

south

qnorthg g

east

qsouthg g

qeastg g

qwestg g

west

figure maxq graph hdg navigation task

gotogoallmk go landmark nearest goal location termination
predicate subtask true agent reaches landmark nearest
goal goal predicate termination predicate

gotolmk l go landmark l termination predicate true

agent reaches landmark l b agent outside region defined
voronoi cell l neighboring voronoi cells n l goal predicate
subtask true condition

gotogoal g go goal location g termination predicate subtask

true agent goal location agent outside voronoi
cell nl g contains g goal predicate subtask true agent
goal location


fimaxq hierarchical reinforcement learning

maxq decomposition essentially kaelbling method somewhat
redundant consider state agent inside voronoi cell goal
g states hdg decomposes value function three terms
similarly maxq decomposes three terms
v gotolmk l cost getting landmark l represented sum
v c gotolmk l
c gotogoallmk gl maxgotolmk l cost getting landmark l
landmark gl nearest goal
c root gotogoallmk gl cost getting goal location reaching gl
e cost completing root task reaching gl
agent inside goal voronoi cell hdg maxq store
essentially information hdg stores q gotogoal g maxq breaks
two terms c gotogoal g v sums two quantities
compute q value
note maxq decomposition stores information twice specifically
cost getting goal landmark gl goal stored c root gotogoallmk gl
c gotogoal g v
let us compare amount memory required q learning hdg maxq
locations possible actions possible goal states q learning
must store values
compute quantity hdg must store q values four actions
state respect landmark landmarks n nl gives
total values must stored
compute quantity hdg must store landmark information
shortest path every landmark landmarks consider landmark
row column neighboring landmarks constitute five macro actions
agent perform move another landmark nearest landmark
goal cell could landmarks gives total q values
must stored similar computations landmarks give total values
must stored
finally compute quantity hdg must store information square inside
voronoi cell get squares inside voronoi
cell requires values
hence grand total hdg huge savings q learning
let consider maxq hierarchy without state abstractions
v expected reward primitive action state
states primitive actions requires values however
reward constant apply leaf irrelevance store single value
c gotolmk l one four primitive actions requires
amount space kaelbling representation indeed combined
v represents exactly information requires values
state abstractions applied


fidietterich

c gotogoallmk gl gotolmk l cost completing gotogoallmk

task going landmark l primitive actions deterministic
gotolmk l terminate location l hence need store
pair l gl exactly kaelbling quantity
requires values however primitive actions stochastic
kaelbling original must store value possible
terminal state gotolmk action actions could terminate
target landmark l one states bordering set voronoi cells
neighbors cell l requires values kaelbling stores
values effectively making assumption gotolmk l
never fail reach landmark l approximation introduce
maxq representation choice state abstraction node

c gotogoal cost completing gotogoal task executing one
primitive actions quantity hdg representation
requires amount space values

c root gotogoallmk cost reaching goal reached

landmark nearest goal maxq must represent combinations
goal landmarks goals requires values note values
values c gotogoal g v primitive actions
means maxq representation stores information twice whereas
hdg representation stores term

c root gotogoal cost completing root task exe

cuted gotogoal task primitive action deterministic zero
gotogoal reached goal hence apply termination
condition store values however primitive actions stochastic must store value possible state borders voronoi cell
contains goal requires different values kaelbling hdg
representation value function ignoring probability gotogoal
terminate non goal state maxq exact representation value
function ignore possibility incorrectly apply termination
condition case maxq representation becomes function approximation

stochastic case without state abstractions maxq representation requires
values safe state abstractions requires values approximations employed kaelbling equivalently primitive actions deterministic
maxq representation state abstractions requires values numbers
summarized table see unsafe state abstractions maxq
representation requires slightly space hdg representation
redundancy storing c root gotogoallmk
example shows hdg task start fully general formulation provided maxq impose assumptions obtain method similar
hdg maxq formulation guarantees value function hierarchical
policy represented exactly assumptions introduce approximations


fimaxq hierarchical reinforcement learning

table comparison number values must stored represent value
function hdg maxq methods
hdg maxq
hdg maxq maxq maxq
item item
values abs safe abs unsafe abs
v




c gotolmk l



c gotogoallmk gotolmk l



c gotogoal g



c root gotogoallmk




c root gotogoal




total number values required


value function representation might useful general design methodology
building application specific hierarchical representations long term goal develop
methods application require inventing set techniques instead shelf tools e g maxq could specialized imposing
assumptions state abstractions produce ecient special purpose systems
one important contributions hdg method introduced
form non hierarchical execution soon agent crosses one voronoi cell
another current subtask reaching landmark cell interrupted
agent recomputes current target landmark effect
reaches goal voronoi cell agent aiming landmark outside
current voronoi cell hence although agent aims sequence landmark states
typically visit many states way goal states provide
convenient set intermediate targets taking shortcuts hdg compensates
fact general overestimated cost getting goal
computed value function policy agent goes one landmark
another
effect obtained hierarchical greedy execution maxq graph
directly inspired hdg method note storing nl nearest landmark
function kaelbing hdg method detect eciently current subtask
interrupted technique works navigation space
distance metric contrast executehgpolicy performs kind polling
checks primitive action whether interrupt current subroutine
invoke one important goal future maxq general
purpose mechanism avoiding unnecessary polling mechanism
discover eciently evaluable interrupt conditions
figure shows experiments hdg maxq q learning employed following parameters flat q learning initial values
learning rate initial temperature cooling rate
maxq q without state abstractions initial values learning rate initial


fidietterich


flat q

maxq
abstract



mean cumulative reward



maxq abstract














primitive actions

e

e

e

figure comparison flat q learning maxq q learning without state
abstraction average runs
temperature cooling rates maxroot maxgotogoallmk
maxgotogoal maxgotolmk maxq q state abstractions
initial values learning rate initial temperature cooling rates
maxroot maxgotogoal maxgotogoallmk
maxgotolmk hierarchical greedy execution introduced starting primitive actions per trial reducing every trial actions trials
execution completely greedy
figure confirms observations made experiments fickle taxi task
without state abstractions maxq q converges much slowly q learning
state abstractions converges roughly three times fast figure shows close
view figure allows us compare differences final levels performance
methods see maxq q state abstractions able
reach quality hand coded hierarchical policy presumably even exploration
would required achieve whereas state abstractions maxq q able
slightly better hand coded policy hierarchical greedy execution maxq q
able reach goal one fewer action average approaches
performance best hierarchical greedy policy computed value iteration notice
however best performance obtained hierarchical greedy execution
best recursively optimal policy cannot match optimal performance hence flat q


fimaxq hierarchical reinforcement learning


optimal policy

mean cumulative reward

hierarchical greedy optimal policy
maxq abstract greedy
maxq abstract





flat q

hierarchical hand coded policy

maxq abstract













primitive actions

e

e

e

figure expanded view comparing flat q learning maxq q learning
without state abstraction without hierarchical greedy execution
average runs
learning achieves policy reaches goal state average one fewer
primitive action finally notice taxi domain added exploration
cost shifting greedy execution
kaelbling hdg work recently extended generalized moore baird
kaelbling sparse mdp overall task get given
start state desired goal state key success
landmark subtask guaranteed terminate single resulting state makes
possible identify sequence good intermediate landmark states assemble
policy visits sequence moore baird kaelbling construct
hierarchy landmarks airport hierarchy makes process ecient
note subtask terminate single state general mdps
airport method would work would combinatorial explosion
potential intermediate states would need considered

parr russell hierarchies abstract machines

b dissertation work ron parr considered hierarchical reinforcement learning programmer encodes prior knowledge form hierarchy
finite state controllers called ham hierarchy abstract machines hierarchy


fidietterich

intersection
vertical hallway
horizontal hallway
goal

figure parr maze left start state upper left corner
states lower right hand room terminal states smaller diagram
right shows hallway intersection structure maze
executed procedure call return discipline provides partial policy
task policy partial machine include non deterministic choice
machine states machine lists several options action specify
one chosen programmer puts choice states point
know action performed given partial policy parr
goal best policy making choices choice states words
goal learn hierarchical value function v hs mi state external
environment contains internal state hierarchy e contents
procedure call stack values current machine states machines
appearing stack key observation necessary learn value
function choice states hs mi parr learn decomposition value
function instead attens hierarchy create markov decision
choice states hs mi hence hierarchical primarily sense programmer
structures prior knowledge hierarchically advantage parr method
optimal hierarchical policy subject constraints provided programmer
disadvantage method cannot executed non hierarchically produce
better policy
parr illustrated work maze shown figure maze large scale
structure series hallways intersections small scale structure series
obstacles must avoided order move hallways intersections


fimaxq hierarchical reinforcement learning

trial agent starts top left corner must move state
bottom right corner room agent usual four primitive actions north south
east west actions stochastic probability succeed
probability action move left probability action
move right instead e g north action move east probability
west probability action would collide wall obstacle
effect
maze structured series rooms containing block states
obstacles rooms parts hallways connected
two rooms opposite sides rooms intersections two
hallways meet
test representational power maxq hierarchy want see well
represent prior knowledge parr able represent ham begin
describing parr ham maze task present maxq hierarchy
captures much prior knowledge
parr top level machine mroot consists loop single choice state
chooses among four possible child machines mgo east mgo south mgo west
mgo north loop terminates agent reaches goal state mroot
invoke particular machine hallway specified direction hence
start state consider mgo south mgo east
mgo machine begins executing agent intersection first
thing tries exit intersection hallway specified direction
attempts traverse hallway reaches another intersection first
invoking mexitintersection machine machine returns invokes
mexithallway machine machine returns mgo returns
mexitintersection mexithallway machines identical except termination conditions machines consist loop one choice state chooses among
four possible subroutines simplify description suppose mgo east chosen mexitintersection east four possible subroutines msniff east north
msniff east south mback east north mback east south
msniff p machine moves direction encounters wall
part obstacle part walls maze moves perpendicular
direction p reaches end wall wall end two ways
agent trapped corner walls directions p else
longer wall direction first case msniff machine terminates second
case resumes moving direction
mback p machine moves one step backwards direction opposite
moves five steps direction p moves may may succeed
actions stochastic may walls blocking way actions carried
case mback machine returns
msniff mback machines terminate reach end hall
end intersection
author thanks ron parr providing details ham task



fidietterich

finite state controllers define highly constrained partial policy mback
msniff mgo machines contain choice states choice points
mroot must choose direction move mexitintersection
mexithall must decide call msniff call mback
perpendicular direction tell machines try cannot move forward

maxroot

go
r room
maxgo r

qexitinter r

qexithall r

maxexitinter r

maxexithall r

qsniffei p

qbackei p

qsniffeh p
x x

qbackeh p
x x



maxsniff p

maxback p x

qfollowwall p

qtowall

qbackone

qperpthree p

maxfollowwall p

maxtowall

maxbackone

maxperpthree p

p



qmovefw

inv

qmovetw

qmovebo

p
qmovep

maxmove

figure maxq graph parr maze task
figure shows maxq graph encodes similar set constraints policy
subtasks defined follows


fimaxq hierarchical reinforcement learning

root exactly mroot machine must choose direction

invoke go terminates agent enters terminal state
goal condition course

go r go direction leaving room r parameter r bound identi

fication number corresponding current room agent
located go terminates agent enters room end hallway
direction leaves desired hallway e g wrong direction
goal condition go satisfied agent reaches desired intersection

exitinter r terminates agent exited room r goal condition
agent exit room r direction

exithall r terminates agent exited current hall

intersection goal condition agent entered desired intersection
direction

sniff r encodes subtask equivalent msniff machine however

sniff must two child subtasks towall followwall simply internal
states msniff necessary subtask maxq framework cannot

contain internal state whereas finite state controller ham representation
contain many internal states necessary particular one state
moving forward another state following wall sideways

towall equivalent one part msniff terminates

wall front agent direction goal condition
termination condition

followwall p equivalent part msniff moves direction

p wall direction ends stuck corner walls
directions p goal condition termination condition

back p x attempts encode information mback machine

case maxq hierarchy cannot capture information

mback simply executes sequence primitive actions one step back five steps
direction p mback must internal states maxq
allow instead back subtask subgoal moving agent least

one square backwards least squares direction p order determine
whether achieved subgoal must remember x position
started execute bound parameters back back terminates
achieves desired change position runs walls prevent
achieving subgoal goal condition termination condition

backone x moves agent one step backwards direction opposite

needs starting x position order tell succeeded
terminates moved least one unit direction wall
direction goal condition termination condition


fidietterich

perpthree p x moves agent three steps direction p needs

starting x positions order tell succeeded terminates
moved least three units direction p wall direction
goal condition termination condition

move parameterized primitive action executes one primitive move
direction terminates immediately

see three major differences maxq representation ham representation first ham finite state controller contain
internal states convert maxq subtask graph must make separate
subtask internal state ham second ham terminate
amount effort e g performing actions whereas maxq subtask must terminate
change state world impossible define maxq subtask performs k steps terminate regardless effects steps e
without adding kind counter state mdp third dicult
formulate termination conditions maxq subtasks ham machines
example ham necessary specify mexithallway machine terminates entered different intersection one mgo executed
however important maxq method maxq subtask learns
value function policy independent parent tasks example without
requirement enter different intersection learning maxq
prefer maxexithall take one step backward return room
go action started much easier terminal state reach
arise ham policy learned subtask
depends whole attened hierarchy machines returning state
go action started help solve overall reaching goal state
lower right corner
construct maxq graph introduced three programming
tricks binding parameters aspects current state order serve kind
local memory subtask began executing b parameterized
primitive action order able pass parameter value specifies primitive
action perform c employing inheritance termination conditions
subtask maxq graph others inherits termination
conditions ancestor tasks hence agent middle executing towall
action leaves intersection towall subroutine terminates exitinter
termination condition satisfied behavior similar standard behavior
maxq ordinarily ancestor task terminates descendent tasks forced
return without updating c values inheritance termination conditions
hand descendent tasks forced terminate updating c
values words termination condition child task logical disjuntion
termination conditions ancestors plus termination condition
inheritance made easier write maxq graph parents need
pass children information necessary children define
complete termination goal predicates


fimaxq hierarchical reinforcement learning

essentially opportunities state abstraction task
irrelevant features state opportunities apply shielding
termination properties however particular exithall guaranteed cause
parent task maxgo terminate require stored c values
many states subtasks terminated e g go east state
wall east side room c values need stored
nonetheless even applying state elimination conditions maxq representation task requires much space representation exact
computation dicult applying maxq q learning maxq representation
required values whereas q learning requires fewer values parr
states method requires values
test relative effectiveness maxq representation compare maxq q
learning q learning large negative values states
acquire particularly early phases learning unable get boltzmann
exploration work well one bad experience would cause action receive
low q value would never tried hence experimented
greedy exploration counter exploration greedy exploration policy
ordered abstract glie policy random action chosen probability
gradually decreased time counter exploration policy keeps track
many times action executed state choose action state
selects action executed fewest times actions
executed times switches greedy execution hence genuine glie
policy parr employed counter exploration policies experiments task
domains conducted several experimental runs e g testing boltzmann greedy counter exploration determine best parameters
flat q learning chose following parameters learning rate greedy exploration initial value decreased successful
execution max node initial q values maxq q learning chose
following parameters counter exploration learning rate equal
reciprocal number times action performed initial values c
values selected carefully provide underestimates true c values example
initial values qexitinter worst case completing
exitinter task takes steps complete subsequent exithall task hence
complete go parent task performance quite sensitive initial c values
potential drawback maxq
figure plots see maxq q learning converges
times faster flat q learning know whether maxq q converged
recursively optimal policy comparison performance hierarchical
policy coded hand hand coded policy used knowledge contextual
information choose operators policy surely better best recursively
optimal policy hamq learning converge policy equal slightly better
hand coded policy
experiment demonstrates maxq representation capture
prior knowledge represented hamq hierarchy


fidietterich




mean reward per trial



hand coded hierarchical policy





maxq q learning

flat q learning






e

e

e
primitive steps

e

e

e

figure comparison flat q learning maxq q learning parr maze task
shows maxq representation requires much care design goal
conditions subtasks

domains
addition three domains discussed developed maxq graphs
singh ag task treasure hunter task described tadepalli dietterich
dayan hinton feudal q learning task tasks
easily naturally placed maxq framework indeed fit easily
parr russell maze task
maxq able exactly duplicate singh work decomposition value
function exactly amount space represent value function
maxq duplicate tadepalli dietterich however
maxq explanation method considerably slower requires substantially space represent value function
feudal q task maxq able give better performance feudal q learning
reason feudal q learning subroutine makes decisions q
function learned level hierarchy without information
estimated costs actions descendents contrast maxq value function
decomposition permits max node make decisions sum completion
function c j costs estimated descendents v j course maxq


fimaxq hierarchical reinforcement learning

supports non hierarchical execution possible feudal q
learn value function decomposition

discussion

concluding wish discuss two issues design tradeoffs hierarchical reinforcement learning b methods automatically learning least
improving maxq hierarchies

design tradeoffs hierarchical reinforcement learning

introduction discussed four issues concerning design hierarchical reinforcement learning architectures method defining subtasks b
use state abstraction c non hierarchical execution design learning subsection want highlight tradeoff first two
issues
maxq defines subtasks termination predicate ti pseudo reward function
r least two drawbacks method first hard programmer define ti r correctly since essentially requires guessing value function
optimal policy mdp states subtask terminates second
leads us seek recursively optimal policy rather hierarchically optimal policy
recursively optimal policies may much worse hierarchically optimal ones
may giving substantial performance
however return two drawbacks maxq obtains important benefit
policies value functions subtasks become context free words
depend parent tasks larger context invoked
understand point consider mdp shown figure clear
optimal policy exiting left hand room exit subtask depends location
goal top right hand room agent prefer
exit via upper door whereas bottom right hand room agent
prefer exit lower door however define subtask exiting
left hand room pseudo reward zero doors obtain policy
optimal case policy use cases furthermore
policy depend location goal hence apply max node
irrelevance solve exit subtask location robot ignore
location goal
example shows obtain benefits subtask reuse state abstraction define subtask termination predicate pseudo reward
function termination predicate pseudo reward function provide barrier
prevents communication value information exit subtask context
compare parr ham method hamq finds best policy
consistent hierarchy achieve must permit information propagate
exit subtask e exit finite state controller environment
means state reached leaving exit subtask different
values depending location goal different values propagate
back exit subtask represent different values exit subtask must know


fidietterich

location goal short achieve hierarchically optimal policy within exit
subtask must general represent value function entire state space state
abstractions cannot employed without losing hierarchical optimality
see therefore direct tradeoff achieving hierarchical
optimality employing state abstractions methods hierarchical optimality
freedom defining subtasks e g partial policies ham
cannot safely employ state abstractions within subtasks general cannot
reuse solution one subtask multiple contexts methods recursive optimality
hand must define subtasks method pseudo reward functions
maxq fixed policies options framework isolates subtask
context return apply state abstraction learned policy
reused many contexts less optimal
interesting iterative method described dean lin
viewed method moving along tradeoff dean lin method
programmer makes initial guess values terminal states subtask
e doorways figure initial guess locally optimal policies
subtasks computed locally optimal policy parent task
computed holding subtask policies fixed e treating options
point computed recursively optimal solution original
given initial guesses instead solving subproblems sequentially
via oine dean lin suggested could use maxq q learning

method dean lin stop instead computes values
terminal states subtask learned value function entire
allows update guesses values terminal states
entire solution process repeated obtain recursively optimal solution
guesses prove process iterated indefinitely
converge hierarchically optimal policy provided course state abstractions
used within subtasks
suggests extension maxq q learning adapts r values online
time subtask terminates could update r function computed value
terminated state precise j subtask j terminates
state update r j equal v maxa q however
work r j represented full state subtask j employing state
abstractions x r j x need average value v
average taken states x weighted probability
visiting states easily accomplished performing stochastic approximation
update form
r j x fft r j x fftv
time subtask j terminates could expected converge
best hierarchical policy consistent given state abstractions
suggests may worthwhile first learn recursively
optimal policy aggressive state abstractions use learned value
function initialize maxq representation detailed representation
states progressive refinements state space could guided monitoring


fimaxq hierarchical reinforcement learning

degree values v x vary abstract state x large
variance means state abstractions failing make important distinctions
values states refined
kinds adaptive take longer converge basic
maxq method described tasks agent must solve many times
lifetime worthwhile learning provide initial useful
solution gradually improve solution optimal important goal
future methods diagnosing repairing errors sub optimalities
initial hierarchy ultimately optimal policy discovered

automated discovery abstractions

taken rely upon programmer design
maxq hierarchy including termination conditions pseudo reward functions state
abstractions particularly concerning state abstraction suggest
ways might able automate construction hierarchy
main purpose hierarchy create opportunities subtask sharing
state abstraction actually closely related order subtask shared
two different regions state space must case value function
two different regions identical except additive offset maxq framework
additive offset would difference c values parent task one way
reusable subtasks would look regions state space value function
exhibits additive offsets
second way would search structure one step probability transition
function p js subtask useful enables state abstractions max
node irrelevance formulate identifying region
state space conditioned region p js factors according
equation top divide conquer similar decision tree
might able
third way would search funnel actions looking bottlenecks state
space policies must travel would useful discovering cases
distribution irrelevance
ways dicult kinds state abstractions discover
arbitrary subgoals introduced constrain policy sacrifice optimality
example could automatically decide impose landmarks onto
hdg task perhaps detecting large region state space without bottlenecks
variations reward function
discovering hierarchies important challenge future
least provided guidelines constitute good state abstractions
serve objective functions guiding automated search abstractions

concluding remarks

introduced representation value function hierarchical reinforcement learning maxq value function decomposition proved
maxq decomposition represent value function hierarchical policy


fidietterich

finite horizon undiscounted cumulative reward criterion infinite horizon
discounted reward criterion representation supports subtask sharing use overall value function decomposed value functions individual subtasks
introduced learning maxq q learning proved
converges probability recursively optimal policy argued although
recursive optimality weaker hierarchical optimality global optimality
important form optimality permits subtask learn locally optimal
policy ignoring behavior ancestors maxq graph increases
opportunities subtask sharing state abstraction
shown maxq decomposition creates opportunities state abstraction identified set five properties max node irrelevance leaf irrelevance
distribution irrelevance shielding termination allow us ignore large
parts state space within subtasks proved maxq q still converges
presence forms state abstraction showed experimentally state abstraction important practice successful application maxq q learning
least taxi hdg tasks
presented two different methods deriving improved non hierarchical policies maxq value function representation formalized conditions
methods improve hierarchical policy verified
experimentally non hierarchical execution gives improved performance fickle
taxi task achieves optimal performance hdg task gives
substantial improvement
finally argued tradeoff governing design hierarchical
reinforcement learning methods one end design spectrum context free
methods maxq q learning provide good support state abstraction
subtask sharing learn recursively optimal policies end
spectrum context sensitive methods hamq options framework
early work dean lin methods discover hierarchically optimal
policies cases globally optimal policies drawback cannot
easily exploit state abstractions share subtasks great speedups
enabled state abstraction argued context free
preferred relaxed needed obtain improved policies

acknowledgements
author gratefully acknowledges support national science foundation
grant number iri oce naval grant number n air force oce scientific grant number f
spanish government program estancias de investigadores extranjeros en
regimen de sabatico en espa na addition author indebted many colleagues
helping develop clarify ideas including valentina zubek leslie
kaelbling bill langford wes pinchot rich sutton prasad tadepalli sebastian thrun
particularly want thank eric chown encouraging study feudal reinforcement
learning ron parr providing details ham machines sebastian thrun
encouraging write single comprehensive thank andrew moore


fimaxq hierarchical reinforcement learning

action editor valentina zubek two sets anonymous reviewers previous
drafts suggestions careful reading improved
immeasurably

references

bellman r e dynamic programming princeton university press
bertsekas p tsitsiklis j n neuro dynamic programming athena scientific
belmont
boutilier c dearden r goldszmidt exploiting structure policy construction proceedings fourteenth international joint conference artificial
intelligence pp
currie k tate plan open architecture artificial intelligence
dayan p hinton g feudal reinforcement learning advances neural
information processing systems pp morgan kaufmann san francisco
ca
dean lin h decomposition techniques stochastic domains
tech rep cs department computer science brown university providence
rhode island
dietterich g maxq method hierarchical reinforcement learning
fifteenth international conference machine learning pp morgan kaufmann
fikes r e hart p e nilsson n j learning executing generalized robot
plans artificial intelligence
forgy c l rete fast many pattern many object pattern
match artificial intelligence
hauskrecht meuleau n kaelbling l p dean boutilier c hierarchical
solution markov decision processes macro actions proceedings
fourteenth annual conference uncertainty artificial intelligence uai pp
san francisco ca morgan kaufmann publishers
howard r dynamic programming markov processes mit press cambridge
jaakkola jordan singh p convergence stochastic iterative
dynamic programming neural computation
kaelbling l p hierarchical reinforcement learning preliminary proceedings tenth international conference machine learning pp san
francisco ca morgan kaufmann


fidietterich

kalmar z szepesvari c lorincz module reinforcement learning
real robot machine learning
knoblock c learning abstraction hierarchies solving proceedings
eighth national conference artificial intelligence pp boston
aaai press
korf r e macro operators weak method learning artificial intelligence

lin l j reinforcement learning robots neural networks ph thesis
carnegie mellon university department computer science pittsburgh pa
moore w baird l kaelbling l p multi value functions ecient automatic action hierarchies multiple goal mdps proceedings international joint conference artificial intelligence pp san francisco morgan kaufmann
parr r flexible decomposition weakly coupled markov decision
proceedings fourteenth annual conference uncertainty
artificial intelligence uai pp san francisco ca morgan kaufmann
publishers
parr r b hierarchical control learning markov decision processes ph
thesis university california berkeley california
parr r russell reinforcement learning hierarchies machines advances neural information processing systems vol pp cambridge
mit press
pearl j probabilistic inference intelligent systems networks plausible inference morgan kaufmann san mateo ca
rummery g niranjan online q learning connectionist systems
tech rep cued finfeng tr cambridge university engineering department cambridge england
sacerdoti e hierarchy abstraction spaces artificial intelligence

singh jaakkola littman l szepesvari c convergence
single step policy reinforcement learning tech rep university
colorado department computer science boulder co appear machine
learning
singh p transfer learning composing solutions elemental sequential
tasks machine learning
sutton r singh precup ravindran b improved switching among
temporally abstract actions advances neural information processing systems
vol pp mit press


fimaxq hierarchical reinforcement learning

sutton r barto g introduction reinforcement learning mit press
cambridge
sutton r precup singh mdps semi mdps learning
representing knowledge multiple temporal scales tech rep university massachusetts department computer information sciences amherst
appear artificial intelligence
tadepalli p dietterich g hierarchical explanation reinforcement
learning proceedings fourteenth international conference machine
learning pp san francisco ca morgan kaufmann
tambe rosenbloom p investigating production system representations
non combinatorial match artificial intelligence
watkins c j c h learning delayed rewards ph thesis king college
oxford reprinted mit press
watkins c j dayan p technical note q learning machine learning




