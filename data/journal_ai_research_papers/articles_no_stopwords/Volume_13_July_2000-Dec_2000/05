Journal Artificial Intelligence Research 13 (2000) 227-303

Submitted 11/99; published 11/00

Hierarchical Reinforcement Learning MAXQ Value
Function Decomposition
Thomas G. Dietterich

Department Computer Science, Oregon State University
Corvallis, 97331

Abstract

tgd@cs.orst.edu

paper presents new approach hierarchical reinforcement learning based decomposing target Markov decision process (MDP) hierarchy smaller MDPs
decomposing value function target MDP additive combination
value functions smaller MDPs. decomposition, known MAXQ decomposition, procedural semantics|as subroutine hierarchy|and declarative
semantics|as representation value function hierarchical policy. MAXQ unifies
extends previous work hierarchical reinforcement learning Singh, Kaelbling,
Dayan Hinton. based assumption programmer identify useful
subgoals define subtasks achieve subgoals. defining subgoals,
programmer constrains set policies need considered reinforcement
learning. MAXQ value function decomposition represent value function
policy consistent given hierarchy. decomposition creates opportunities exploit state abstractions, individual MDPs within hierarchy
ignore large parts state space. important practical application
method. paper defines MAXQ hierarchy, proves formal results representational power, establishes five conditions safe use state abstractions. paper
presents online model-free learning algorithm, MAXQ-Q, proves converges
probability 1 kind locally-optimal policy known recursively optimal policy,
even presence five kinds state abstraction. paper evaluates MAXQ
representation MAXQ-Q series experiments three domains shows
experimentally MAXQ-Q (with state abstractions) converges recursively optimal
policy much faster Q learning. fact MAXQ learns representation
value function important benefit: makes possible compute execute
improved, non-hierarchical policy via procedure similar policy improvement
step policy iteration. paper demonstrates effectiveness non-hierarchical
execution experimentally. Finally, paper concludes comparison related work
discussion design tradeoffs hierarchical reinforcement learning.

c 2000 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiDietterich

1. Introduction
area Reinforcement Learning (Bertsekas & Tsitsiklis, 1996; Sutton & Barto, 1998)
studies methods agent learn optimal near-optimal plans interacting
directly external environment. basic methods reinforcement learning
based classical dynamic programming algorithms developed late
1950s (Bellman, 1957; Howard, 1960). However, reinforcement learning methods offer two
important advantages classical dynamic programming. First, methods online.
permits focus attention parts state space important
ignore rest space. Second, methods employ function approximation algorithms (e.g., neural networks) represent knowledge. allows
generalize across state space learning time scales much better.
Despite recent advances reinforcement learning, still many shortcomings.
biggest lack fully satisfactory method incorporating hierarchies
reinforcement learning algorithms. Research classical planning shown hierarchical methods hierarchical task networks (Currie & Tate, 1991), macro actions
(Fikes, Hart, & Nilsson, 1972; Korf, 1985), state abstraction methods (Sacerdoti, 1974;
Knoblock, 1990) provide exponential reductions computational cost finding
good plans. However, basic algorithms probabilistic planning reinforcement learning \ at" methods|they treat state space one huge search space.
means paths start state goal state long,
length paths determines cost learning planning, information
future rewards must propagated backward along paths.
Many researchers (Singh, 1992; Lin, 1993; Kaelbling, 1993; Dayan & Hinton, 1993;
Hauskrecht, et al., 1998; Parr & Russell, 1998; Sutton, Precup, & Singh, 1998) experimented different methods hierarchical reinforcement learning hierarchical
probabilistic planning. research explored many different points design space
hierarchical methods, several systems designed specific situations.
lack crisp definitions main approaches clear understanding relative
merits different methods.
paper formalizes clarifies one approach attempts understand
compares techniques. approach, called MAXQ method, provides
hierarchical decomposition given reinforcement learning problem set subproblems. simultaneously provides decomposition value function given
problem set value functions subproblems. Hence, declarative
semantics (as value function decomposition) procedural semantics (as subroutine
hierarchy).
decomposition subproblems many advantages. First, policies learned
subproblems shared (reused) multiple parent tasks. Second, value functions
learned subproblems shared, subproblem reused new task,
learning overall value function new task accelerated. Third, state abstractions applied, overall value function represented compactly
sum separate terms depends subset state variables.
compact representation value function require less data learn, hence,
learning faster.
228

fiMAXQ Hierarchical Reinforcement Learning

Previous research shows several important design decisions must
made constructing hierarchical reinforcement learning system. provide
overview results paper, let us review issues see MAXQ
method approaches them.
first issue specify subtasks. Hierarchical reinforcement learning involves
breaking target Markov decision problem hierarchy subproblems subtasks.
three general approaches defining subtasks. One approach define
subtask terms fixed policy provided programmer (or
learned separate process). \option" method Sutton, Precup, Singh
(1998) takes approach. second approach define subtask terms nondeterministic finite-state controller. Hierarchy Abstract Machines (HAM) method
Parr Russell (1998) takes approach. method permits programmer
provide \partial policy" constrains set permitted actions point,
specify complete policy subtask. third approach define
subtask terms termination predicate local reward function. define
means subtask completed final reward completing
subtask. MAXQ method described paper follows approach, building
upon previous work Singh (1992), Kaelbling (1993), Dayan Hinton (1993), Dean
Lin (1995).
advantage \option" partial policy approaches subtask
defined terms amount effort course action rather terms
achieving particular goal condition. However, \option" approach (at least
simple form described paper), requires programmer provide complete policies
subtasks, dicult programming task real-world problems.
hand, termination predicate method requires programmer guess relative
desirability different states subtask might terminate.
dicult, although Dean Lin show guesses revised automatically
learning algorithm.
potential drawback hierarchical methods learned policy may
suboptimal. hierarchy constrains set possible policies considered.
constraints poorly chosen, resulting policy suboptimal. Nonetheless,
learning algorithms developed \option" partial policy approaches
guarantee learned policy best possible policy consistent
constraints.
termination predicate method suffers additional source suboptimality.
learning algorithm described paper converges form local optimality
call recursive optimality. means policy subtask locally optimal
given policies children. might exist better hierarchical policies
policy subtask must locally suboptimal overall policy optimal.
example, subtask buying milk might performed suboptimally (at distant
store) larger problem involves buying film (at store). problem
avoided careful definition termination predicates local reward functions,
added burden programmer. (It interesting note problem
recursive optimality noticed previously. previous work
229

fiDietterich

focused subtasks single terminal state, cases, problem
arise.)
second design issue whether employ state abstractions within subtasks.
subtask employs state abstraction ignores aspects state environment.
example, many robot navigation problems, choices route take
reach goal location independent robot currently carrying.
exceptions, state abstraction explored previously. see MAXQ
method creates many opportunities exploit state abstraction, abstractions
huge impact accelerating learning. see important
design tradeoff: successful use state abstraction requires subtasks defined
terms termination predicates rather using option partial policy methods.
MAXQ method must employ termination predicates, despite problems
create.
third design issue concerns non-hierarchical \execution" learned hierarchical policy. Kaelbling (1993) first point value function learned
hierarchical policy could evaluated incrementally yield potentially much
better non-hierarchical policy. Dietterich (1998) Sutton, et al. (1999) generalized
show arbitrary subroutines could executed non-hierarchically yield improved
policies. However, order support non-hierarchical execution, extra learning
required. Ordinarily, hierarchical reinforcement learning, states learning
required higher levels hierarchy states one subroutines could terminate (plus possible initial states). support non-hierarchical
execution, learning required states (and levels hierarchy). general,
requires additional exploration well additional computation memory.
consequence hierarchical decomposition value function, MAXQ method
able support either form execution, see many problems
improvement non-hierarchical execution worth added cost.
fourth final issue form learning algorithm employ. important advantage reinforcement learning algorithms typically operate online.
However, finding online algorithms work general hierarchical reinforcement learning
dicult, particularly within termination predicate family methods. Singh's
method relied subtask unique terminal state; Kaelbling employed mix
online batch algorithms train hierarchy; work within \options" framework usually assumes policies subproblems given need
learned all. best previous online algorithms HAMQ Q learning algorithm
Parr Russell (for partial policy method) Feudal Q algorithm Dayan
Hinton. Unfortunately, HAMQ method requires \ attening" hierarchy,
several undesirable consequences. Feudal Q algorithm tailored specific kind
problem, converge well-defined optimal policy.
paper, present general algorithm, called MAXQ-Q, fully-online learning
hierarchical value function. algorithm enables subtasks within hierarchy
learned simultaneously online. show experimentally theoretically
algorithm converges recursively optimal policy. show substantially
faster \ at" (i.e., non-hierarchical) Q learning state abstractions employed.
230

fiMAXQ Hierarchical Reinforcement Learning

remainder paper organized follows. introducing notation
Section 2, define MAXQ value function decomposition Section 3 illustrate
simple example Markov decision problem. Section 4 presents analytically
tractable version MAXQ-Q learning algorithm called MAXQ-0 algorithm
proves convergence recursively optimal policy. shows extend MAXQ0 produce MAXQ-Q algorithm, shows extend theorem similarly.
Section 5 takes issue state abstraction formalizes series five conditions
state abstractions safely incorporated MAXQ representation.
State abstraction give rise hierarchical credit assignment problem, paper
brie discusses one solution problem. Finally, Section 7 presents experiments
three example domains. experiments give idea generality MAXQ
representation. provide results relative importance temporal state
abstractions importance non-hierarchical execution. paper concludes
discussion design issues brie described above, particular,
addresses tradeoff method defining subtasks (via termination predicates)
ability exploit state abstractions.
readers may disappointed MAXQ provides way learning structure hierarchy. philosophy developing MAXQ (which share
reinforcement learning researchers, notably Parr Russell) draw inspiration
development Belief Networks (Pearl, 1988). Belief networks first introduced
formalism knowledge engineer would describe structure networks domain experts would provide necessary probability estimates. Subsequently,
methods developed learning probability values directly observational data.
recently, several methods developed learning structure belief
networks data, dependence knowledge engineer reduced.
paper, likewise require programmer provide structure
hierarchy. programmer need make several important design decisions.
see MAXQ representation much computer program,
rely programmer design modules indicate permissible
ways modules invoke other. learning algorithms fill
\implementations" module way overall program work well.
believe approach provide practical tool solving large real-world
MDPs. believe help us understand structure hierarchical learning
algorithms. hope subsequent research able automate
work currently requiring programmer do.

2. Formal Definitions
begin introducing definitions Markov Decision Problems Semi-Markov Decision Problems.

2.1 Markov Decision Problems
employ standard definition Markov Decision Problems (also known Markov
decision processes). paper, restrict attention situations agent
231

fiDietterich

interacting fully-observable stochastic environment. situation modeled
Markov Decision Problem (MDP) hS; A; P; R; P0 defined follows:
: finite set states environment. point time, agent
observe complete state environment.
A: finite set actions. Technically, set available actions depends
current state s, suppress dependence notation.
P : action 2 performed, environment makes probabilistic transition current state resulting state s0 according probability
distribution P (s0 js; a).
R: Similarly, action performed environment makes transition
s0 , agent receives real-valued (possibly stochastic) reward r whose
expected value R(s0 js; a). simplify notation, customary treat
reward given time action initiated, even though may
general depend s0 well a.
P0 : starting state distribution. MDP initialized, state
probability P0 (s).
policy, , mapping states actions tells action = (s) perform
environment state s.
consider two settings: episodic infinite-horizon.
episodic setting, rewards finite least one zero-cost absorbing
terminal state. absorbing terminal state state actions lead back
state probability 1 zero reward. technical reasons, consider
problems deterministic policies \proper"|that is, deterministic policies
non-zero probability reaching terminal state started arbitrary state.
(We believe condition relaxed, verified formally.)
episodic setting, goal agent find policy maximizes expected
cumulative reward. special case rewards non-positive, problems
referred stochastic shortest path problems, rewards viewed
costs (i.e., lengths), policy attempts move agent along path minimum
expected cost.
infinite horizon setting, rewards finite. addition, discount
factor , agent's goal find policy minimizes infinite discounted sum
future rewards.
value function V policy function tells, state s,
expected cumulative reward executing policy starting state s. Let rt
random variable tells reward agent receives time step following
policy . define value function episodic setting
V (s) = E frt + rt+1 + rt+2 + jst = s; g :
discounted setting, value function


n



V (s) = E rt + rt+1 + 2 rt+2 + fifi st = s; :
232

fiMAXQ Hierarchical Reinforcement Learning

see equation reduces previous one = 1. However, infinitehorizon MDPs sum may converge = 1.
value function satisfies Bellman equation fixed policy:

V (s) =

X

s0

P (s0 js; (s)) R(s0 js; (s)) + V (s0 ) :




quantity right-hand side called backed-up value performing action
state s. possible successor state s0 , computes reward would received
value resulting state weights according probability
ending s0 .
optimal value function V value function simultaneously maximizes
expected cumulative reward states 2 . Bellman (1957) proved unique
solution known Bellman equation:

V (s) = max


X

s0

P (s0 js; a) R(s0 js; a) + V (s0 ) :




(1)

may many optimal policies achieve value. policy chooses
achieve maximum right-hand side equation optimal policy.
denote optimal policy . Note optimal policies \greedy"
respect backed-up value available actions.
Closely related value function so-called action-value function, Q function
(Watkins, 1989). function, Q (s; a), gives expected cumulative reward performing action state following policy thereafter. Q function satisfies
Bellman equation:

Q (s; a) =

X

s0

P (s0 js; a) R(s0 js; a) + Q (s0 ; (s0 )) :




optimal action-value function written Q (s; a), satisfies equation
X
Q (s; a) = P (s0 js; a)

s0





R(s0 js; a) + max Q(s0 ; a0 )
a0

:

(2)

Note policy greedy respect Q optimal policy. may
many optimal policies|they differ break ties actions
identical Q values.
action order, denoted !, total order actions within MDP. is, !
anti-symmetric, transitive relation !(a1 ; a2 ) true iff a1 strictly preferred
a2 . ordered greedy policy, ! greedy policy breaks ties using !. example,
suppose two best actions state a1 a2 , Q(s; a1 ) = Q(s; a2 ),
!(a1 ; a2 ). ordered greedy policy ! choose a1 : ! (s) = a1 . Note
although may many optimal policies given MDP, ordered greedy policy,
! , unique.
233

fiDietterich

2.2 Semi-Markov Decision Processes

order introduce prove properties MAXQ decomposition,
need consider simple generalization MDPs|the semi-Markov decision process.
discrete-time semi-Markov Decision Process (SMDP) generalization Markov
Decision Process actions take variable amount time complete.
particular, let random variable N denote number time steps action takes
executed state s. extend state transition probability function
joint distribution result states s0 number time steps N action
performed state s: P (s0 ; N js; a). Similarly, expected reward changed
R(s0 ; N js; a).1
straightforward modify Bellman equation define value function
fixed policy
h

X
V (s) = P (s0; N js; (s)) R(s0 ; N js; (s)) + N V (s0 ) :
s0 ;N

change expected value right-hand side taken respect
s0 N , raised power N ect variable amount time
may elapse executing action a.
Note expectation linear operator, write Bellman
equations sum expected reward performing action expected value
resulting state s0 . example, rewrite equation
X
(3)
V (s) = R(s; (s)) + P (s0 ; N js; (s)) N V (s0 ):
s0 ;N

R(s; (s)) expected reward performing action (s) state s, expectation taken respect s0 N .
results given paper generalized apply discrete-time semiMarkov Decision Processes. consequence whenever paper talks
executing primitive action, could easily talk executing hand-coded openloop \subroutine". subroutines would learned, could execution
interrupted discussed Section 6. many applications (e.g., robot
control limited sensors), open-loop controllers useful (e.g., hide partialobservability). example, see Kalmar, Szepesvari, A. Lorincz (1998).
Note episodic case, difference MDP Semi-Markov
Decision Process, discount factor 1, therefore neither optimal policy
optimal value function depend amount time action takes.

2.3 Reinforcement Learning Algorithms

reinforcement learning algorithm algorithm tries construct optimal policy
unknown MDP. algorithm given access unknown MDP via following
1. formalization slightly different standard formulation SMDPs, separates
P (s0js; a) F (tjs; a), F cumulative distribution function probability
terminate time units, real-valued rather integer-valued. case, important
consider joint distribution s0 N , need consider actions arbitrary
real-valued durations.

234

fiMAXQ Hierarchical Reinforcement Learning

reinforcement learning protocol. time step t, algorithm told current state
MDP set actions A(s) executable state.
algorithm chooses action 2 A(s), MDP executes action (which causes
move state s') returns real-valued reward r. absorbing terminal state,
set actions A(s) contains special action reset, causes MDP move
one initial states, drawn according P0 .
paper, make use two well-known learning algorithms: Q learning
(Watkins, 1989; Watkins & Dayan, 1992) SARSA(0) (Rummery & Niranjan, 1994).
apply algorithms case action value function Q(s; a) represented
table one entry pair state action. Every entry table
initialized arbitrarily.
Q learning, algorithm observed s, chosen a, received r, observed s0 ,
performs following update:

Qt (s; a) := (1 , fft )Qt,1 (s; a) + fft [r + max
Q (s0 ; a0 )];
a0 t,1
fft learning rate parameter.
Jaakkola, Jordan Singh (1994) Bertsekas Tsitsiklis (1996) prove
agent follows \exploration policy" tries every action every state infinitely often



X
X
lim

=
1

lim
ff2t < 1
(4)

!1
T!1
t=1

t=1

Qt converges optimal action-value function Q probability 1. proof
holds settings discussed paper (episodic infinite-horizon).
SARSA(0) algorithm similar. observing s, choosing a, observing r,
observing s0 , choosing a0 , algorithm performs following update:

Qt (s; a) := (1 , fft )Qt,1 (s; a) + fft [r + Qt,1 (s0 ; a0 )];
fft learning rate parameter. key difference Q value chosen
action a0 , Q(s0 ; a0 ), appears right-hand side place Q learning uses
Q value best action. Singh, et al. (1998) provide two important convergence results:
First, fixed policy employed choose actions, SARSA(0) converge
value function policy provided fft decreases according Equations (4). Second,
so-called GLIE policy employed choose actions, SARSA(0) converge value
function optimal policy, provided fft decreases according Equations (4).
GLIE policy defined follows:

Definition 1 GLIE (Greedy Limit Infinite Exploration) policy policy

satisfying

1. action executed infinitely often every state visited infinitely often.
2. limit, policy greedy respect Q-value function probability
1.
235

fiDietterich

4

R

G

3
2
1
0
0

B
1

2

3

4

Figure 1: Taxi Domain.

3. MAXQ Value Function Decomposition
center MAXQ method hierarchical reinforcement learning MAXQ
value function decomposition. MAXQ describes decompose overall value function
policy collection value functions individual subtasks (and subsubtasks,
recursively).

3.1 Motivating Example

make discussion concrete, let us consider following simple example. Figure 1
shows 5-by-5 grid world inhabited taxi agent. four specially-designated
locations world, marked R(ed), B(lue), G(reen), Y(ellow). taxi problem
episodic. episode, taxi starts randomly-chosen square.
passenger one four locations (chosen randomly), passenger wishes
transported one four locations (also chosen randomly). taxi must go
passenger's location (the \source"), pick passenger, go destination location
(the \destination"), put passenger there. (To keep things uniform, taxi
must pick drop passenger even he/she already located destination!)
episode ends passenger deposited destination location.
six primitive actions domain: (a) four navigation actions move
taxi one square North, South, East, West, (b) Pickup action, (c) Putdown action.
reward ,1 action additional reward +20 successfully
delivering passenger. reward ,10 taxi attempts execute
Putdown Pickup actions illegally. navigation action would cause taxi hit
wall, action no-op, usual reward ,1.
simplify examples throughout section, make six primitive actions deterministic. Later, make actions stochastic order create greater
challenge learning algorithms.
seek policy maximizes total reward per episode. 500 possible
states: 25 squares, 5 locations passenger (counting four starting locations
taxi), 4 destinations.
task simple hierarchical structure two main sub-tasks:
Get passenger Deliver passenger. subtasks turn involves
236

fiMAXQ Hierarchical Reinforcement Learning

subtask navigating one four locations performing Pickup Putdown
action.
task illustrates need support temporal abstraction, state abstraction,
subtask sharing. temporal abstraction obvious|for example, process navigating passenger's location picking passenger temporally extended
action take different numbers steps complete depending distance
target. top level policy (get passenger; deliver passenger) expressed
simply temporal abstractions employed.
need state abstraction perhaps less obvious. Consider subtask getting
passenger. subtask solved, destination passenger
completely irrelevant|it cannot affect nagivation pickup decisions. Perhaps
importantly, navigating target location (either source destination
location passenger), target location important. fact
cases taxi carrying passenger cases irrelevant.
Finally, support subtask sharing critical. system could learn solve
navigation subtask once, solution could shared \Get passenger"
\Deliver passenger" subtasks. show MAXQ method provides
value function representation learning algorithm supports temporal abstraction,
state abstraction, subtask sharing.
construct MAXQ decomposition taxi problem, must identify set
individual subtasks believe important solving overall task.
case, let us define following four tasks:
Navigate(t). subtask, goal move taxi current location
one four target locations, indicated formal parameter t.
Get. subtask, goal move taxi current location
passenger's current location pick passenger.
Put. goal subtask move taxi current location
passenger's destination location drop passenger.
Root. whole taxi task.
subtasks defined subgoal, subtask terminates
subgoal achieved.
defining subtasks, must indicate subtask subtasks
primitive actions employ reach goal. example, Navigate(t) subtask
use four primitive actions North, South, East, West. Get subtask
use Navigate subtask Pickup primitive action, on.
information summarized directed acyclic graph called task
graph, shown Figure 2. graph, node corresponds subtask
primitive action, edge corresponds potential way one subtask
\call" one child tasks. notation formal=actual (e.g., t=source) tells formal
parameter bound actual parameter.
suppose subtasks, write policy (e.g., computer
program) achieve subtask. refer policy subtask \subroutine", view parent subroutine invoking child subroutine via ordinary
237

fiDietterich

Root

Get

Put
t/source

Pickup

t/destination

Navigate(t)

North

South

East

Putdown

West

Figure 2: task graph Taxi problem.
subroutine-call-and-return semantics. policy subtask, gives
us overall policy Taxi MDP. Root subtask executes policy calling
subroutines policies Get Put subtasks. Get policy calls subroutines
Navigate(t) subtask Pickup primitive action. on. call
collection policies hierarchical policy. hierarchical policy, subroutine executes
enters terminal state subtask.

3.2 Definitions

Let us formalize discussion far.
MAXQ decomposition takes given MDP decomposes finite set
subtasks fM0 ; M1 ; : : : ; Mn g convention M0 root subtask (i.e., solving
M0 solves entire original MDP ).
Definition 2 unparameterized subtask three-tuple, hTi; Ai ; R~i i, defined follows:
1. Ti termination predicate partitions set active states, Si , set
terminal states, Ti : policy subtask Mi executed current
state Si . If, time subtask Mi executed, MDP enters
state Ti , Mi terminates immediately (even still executing subtask, see
below).
2. Ai set actions performed achieve subtask Mi . actions
either primitive actions A, set primitive actions MDP,
subtasks, denote indexes i. refer
actions \children" subtask i. sets Ai define directed graph
subtasks M0 ; : : : ; Mn , graph may contain cycles. Stated another way,
subtask invoke recursively either directly indirectly.
child subtask Mj formal parameters, interpreted subtask
occurred multiple times Ai , one occurrence possible tuple actual
238

fiMAXQ Hierarchical Reinforcement Learning

values could bound formal parameters. set actions Ai may differ
one state another one set actual parameter values another,
technically, Ai function actual parameters. However, suppress
dependence notation.
3. R~ (s0 ) pseudo-reward function, specifies (deterministic) pseudo-reward
transition terminal state s0 2 Ti . pseudo-reward tells desirable
terminal states subtask. typically employed give goal
terminal states pseudo-reward 0 non-goal terminal states negative
reward. definition, pseudo-reward R~ (s) zero non-terminal states
s. pseudo-reward used learning, mentioned
Section 4.
primitive action primitive subtask MAXQ decomposition
always executable, always terminates immediately execution,
pseudo-reward function uniformly zero.

subtask formal parameters, possible binding actual values
formal parameters specifies distinct subtask. think values formal
parameters part \name" subtask. practice, course, implement
parameterized subtask parameterizing various components task. b specifies
actual parameter values task Mi , define parameterized termination
predicate Ti (s; b) parameterized pseudo-reward function R~ (s0 ; b). simplify notation
rest paper, usually omit parameter bindings. However,
noted parameter subtask takes large number possible values,
equivalent creating large number different subtasks, need
learned. create large number candidate actions parent task,
make learning problem dicult parent task well.

Definition 3 hierarchical policy, , set containing policy subtasks
problem: = f0 ; : : : ; n g:
subtask policy takes state returns name primitive action
execute name subroutine (and bindings formal parameters) invoke.
terminology Sutton, Precup, Singh (1998), subtask policy deterministic
\option", probability terminating state (which denote (s)) 0
2 Si , 1 2 Ti .
parameterized task, policy must parameterized well takes
state bindings formal parameters returns chosen action bindings
(if any) formal parameters.
Table 1 gives pseudo-code description procedure executing hierarchical
policy. hierarchical policy executed using stack discipline, similar ordinary
programming languages. Let Kt denote contents pushdown stack time t.
subroutine invoked, name actual parameters pushed onto stack.
subroutine terminates, name actual parameters popped stack.
Notice (line 16) subroutine stack terminates, subroutines
239

fiDietterich

Table 1: Pseudo-Code Execution Hierarchical Policy.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21

Procedure ExecuteHierarchicalPolicy()

st state world time
Kt state execution stack time
Let = 0; Kt = empty stack; observe st
push (0; nil) onto stack Kt (invoke root task parameters)

repeat
top(Kt ) primitive action

Let (i; ) := top(Kt),
name \current" subroutine,
gives parameter bindings
Let (a; fa ) := (s; ),
action fa gives parameter bindings chosen policy
push (a; fa ) onto stack Kt
end //
Let (a; nil) := pop(Kt) primitive action top stack.
Execute primitive action a, observe st+1, receive reward R(st+1jst ; a)
subtask Kt terminated st+1
Let 0 terminated subtask highest (closest root) stack.
top(Kt) 6= 0 pop(Kt)
pop(Kt)
Kt+1 := Kt resulting execution stack.
Kt+1 empty
end ExecuteHierarchicalPolicy

immediately aborted, control returns subroutine invoked
terminated subroutine.
sometimes useful think contents stack additional part
state space problem. Hence, hierarchical policy implicitly defines mapping
current state st current stack contents Kt primitive action a. action
executed, yields resulting state st+1 resulting stack contents Kt+1 .
added state information stack, hierarchical policy non-Markovian
respect original MDP.
hierarchical policy maps states stack contents K actions,
value function hierarchical policy must assign values combinations states
stack contents K .

Definition 4 hierarchical value function, denoted V (hs; K i), gives expected cumu-

lative reward following hierarchical policy starting state stack contents
K.

hierarchical value function exactly learned Ron Parr's (1998b) HAMQ
algorithm, discuss below. However, paper, focus learning
projected value functions subtasks M0 ; : : : ; Mn hierarchy.
240

fiMAXQ Hierarchical Reinforcement Learning

Definition 5 projected value function hierarchical policy subtask Mi, denoted
V (i; s), expected cumulative reward executing (and policies descendents
Mi ) starting state Mi terminates.
purpose MAXQ value function decomposition decompose V (0; s) (the
projected value function root task) terms projected value functions V (i; s)
subtasks MAXQ decomposition.

3.3 Decomposition Projected Value Function

defined hierarchical policy projected value function, show
value function decomposed hierarchically. decomposition based
following theorem:

Theorem 1 Given task graph tasks M0 ; : : : ; Mn hierarchical policy ,
subtask Mi defines semi-Markov decision process states Si , actions Ai , probability
transition function Pi (s0 ; N js; a), expected reward function R(s; a) = V (a; s),
V (a; s) projected value function child task state s. primitive
action,
V (a; s) defined expected immediate reward executing s: V (a; s) =
P
0
0
s0 P (s js; a)R(s js; a).
Proof: Consider subroutines descendents task Mi task graph.

subroutines executing fixed policies (specified hierarchical policy
), probability transition function Pi (s0 ; N js; a) well defined, stationary distribution
child subroutine a. set states Si set actions Ai obvious.
interesting part theorem fact expected reward function R(s; a)
SMDP projected value function child task .
see this, let us write value V (i; s):

V (i; s) = E frt + rt+1 + 2 rt+2 + jst = s; g
(5)
sum continues subroutine task Mi enters state Ti .
let us suppose first action chosen subroutine a. subroutine
invoked, executes number steps N terminates state s0 according
Pi (s0 ; N js; a). rewrite Equation (5)
V (i; s) = E

(

NX
,1
u=0

u rt+u

+

1
X

u=N




t+u fifi

ur

st = s;

)

(6)

first summation right-hand side Equation (6) discounted sum rewards
executing subroutine starting state terminates, words, V (a; s),
projected value function child task . second term right-hand side
equation value s0 current task i, V (i; s0 ), discounted N ,
s0 current state subroutine terminates. write form
Bellman equation:
X
(7)
V (i; s) = V (i (s); s) + Pi (s0 ; N js; (s)) N V (i; s0 )
s0 ;N

241

fiDietterich

form Equation (3), Bellman equation SMDP,
first term expected reward R(s; (s)). Q.E.D.
obtain hierarchical decomposition projected value function, let us switch
action-value (or Q) representation. First, need extend Q notation
handle task hierarchy. Let Q (i; s; a) expected cumulative reward subtask
Mi performing action state following hierarchical policy subtask
Mi terminates. Action may either primitive action child subtask.
notation, re-state Equation (7) follows:

Q (i; s; a) = V (a; s) +

X

s0 ;N

Pi (s0 ; N js; a) N Q (i; s0 ; (s0 ));

(8)

right-most term equation expected discounted reward completing task

Mi executing action state s. term depends i, s, a,
summation marginalizes away dependence s0 N . Let us define C (i; s; a)
equal term:

Definition 6 completion function, C (i; s; a), expected discounted cumulative

reward completing subtask Mi invoking subroutine subtask state s.
reward discounted back point time begins execution.

C (i; s; a) =

X

s0 ;N

Pi (s0; N js; a) N Q (i; s0 ; (s0 ))

(9)

definition, express Q function recursively

Q (i; s; a) = V (a; s) + C (i; s; a):

(10)

Finally, re-express definition V (i; s)

V (i; s) =

(

(i; s; (s))
Q
composite
P
0
0
s0 P (s js; i)R(s js; i) primitive

(11)

refer equations (9), (10), (11) decomposition equations
MAXQ hierarchy fixed hierarchical policy . equations recursively decompose
projected value function root, V (0; s) projected value functions
individual subtasks, M1 ; : : : ; Mn individual completion functions C (j; s; a)
j = 1; : : : ; n. fundamental quantities must stored represent value
function decomposition C values non-primitive subtasks V values
primitive actions.
make easier programmers design debug MAXQ decompositions,
developed graphical representation call MAXQ graph. MAXQ graph
Taxi domain shown Figure 3. graph contains two kinds nodes, Max nodes
Q nodes. Max nodes correspond subtasks task decomposition|there
one Max node primitive action one Max node subtask (including
Root) task. primitive Max node stores value V (i; s). Q nodes correspond
actions available subtask. Q node parent task i, state
242

fiMAXQ Hierarchical Reinforcement Learning

MaxRoot

QPickup

QGet

QPut

MaxGet

MaxPut

QNavigateForPut

QNavigateForGet

t/source

QPutdown

t/destination

Pickup

Putdown

MaxNavigate(t)

QNorth(t)

QEast(t)

QSouth(t)

QWest(t)

North

East

South

West

Figure 3: MAXQ graph Taxi Domain.
subtask stores value C (i; s; a). children node unordered|that
is, order drawn Figure 3 imply anything order
executed. Indeed, child action may executed multiple times
parent subtask completed.
addition storing information, Max nodes Q nodes viewed performing parts computation described decomposition equations. Specifically,
Max node viewed computing projected value function V (i; s)
subtask. primitive Max nodes, information stored node. composite
Max nodes, information obtained \asking" Q node corresponding (s).
Q node parent task child task viewed computing value
Q (i; s; a). \asking" child task projected value function V (a; s)
adding completion function C (i; s; a).
243

fiDietterich

example, consider situation shown Figure 1, denote s1 .
Suppose passenger R wishes go B. Let hierarchical policy
evaluating optimal policy denoted (we omit superscript * reduce
clutter notation). value state 10, cost 1
unit move taxi R, 1 unit pickup passenger, 7 units move taxi B,
1 unit putdown passenger, total 10 units (a reward ,10).
passenger delivered, agent gets reward +20, net value +10.
Figure 4 shows MAXQ hierarchy computes value. compute value
V (Root; s1 ), MaxRoot consults policy finds Root (s1) Get. Hence, \asks"
Q node, QGet compute Q (Root; s1 ; Get). completion cost Root task
performing Get, C (Root; s1 ; Get), 12, cost 8 units deliver
customer (for net reward 20 , 8 = 12) completing Get subtask. However,
reward completing Get, must ask MaxGet estimate expected
reward performing Get itself.
policy MaxGet dictates s1 , Navigate subroutine invoked
bound R, MaxGet consults Q node, QNavigateForGet compute expected
reward. QNavigateForGet knows completing Navigate(R) task, one action
(the Pickup) required complete Get, C (MaxGet; s1 ; Navigate(R)) = ,1.
asks MaxNavigate(R) compute expected reward performing Navigate
location R.
policy MaxNavigate chooses North action, MaxNavigate asks QNorth
compute value. QNorth looks completion cost, finds C (Navigate; s1 ; North)
0 (i.e., Navigate task completed performing North action). consults
MaxNorth determine expected cost performing North action itself.
MaxNorth primitive action, looks expected reward, ,1.
series recursive computations conclude follows:

Q (Navigate(R); s1 ; North) = ,1 + 0
V (Navigate(R); s1 ) = ,1
Q (Get; s1 ; Navigate(R)) = ,1 + ,1
(,1 perform Navigate plus ,1 complete Get.
V (Get; s1) = ,2
Q (Root; s1; Get) = ,2 + 12
(,2 perform Get plus 12 complete Root task collect final reward).
end result value V (Root; s1 ) decomposed sum
C terms plus expected reward chosen primitive action:

V (Root; s1 ) = V (North; s1 ) + C (Navigate(R); s1 ; North) +
C (Get; s1 ; Navigate(R)) + C (Root; s1; Get)
= ,1 + 0 + ,1 + 12
= 10
244

fiMAXQ Hierarchical Reinforcement Learning

10
MaxRoot
10
12

QGet

QPut

-2
MaxGet

MaxPut
-2

QPickup

QNavigateForPut

QNavigateForGet

QPutdown

-1
-1
Pickup

Putdown

MaxNavigate(t)
-1
0

QNorth(t)

QEast(t)

QSouth(t)

QWest(t)

East

South

West

-1
North

Figure 4: Computing value state using MAXQ hierarchy. C value
Q node shown left node. numbers show values
returned graph.
general, MAXQ value function decomposition form

V (0; s) = V (am ; s) + C (am,1 ; s; ) + : : : + C (a1 ; s; a2 ) + C (0; s; a1 ); (12)
a0 ; a1 ; : : : ; \path" Max nodes chosen hierarchical policy going
Root primitive leaf node. summarized graphically Figure 5.
summarize presentation section following theorem:

Theorem 2 Let = fi; = 0; : : : ; ng hierarchical policy defined given MAXQ
graph subtasks M0 ; : : : ; Mn ; let = 0 root node graph.
exist values C (i; s; a) (for internal Max nodes) V (i; s) (for primitive, leaf Max
245

fiDietterich



V (0X; s)

XXXXX




X
V (a ; s)
P
PPPP
1

.
V (am,1 ; s)

. .

ZZ

ZZ


V (am ; s) C (am,1 ; s; )
r1

r2

r3

r4

r5

C (a1 ; s; a2 )
. . .

r8

r9

C (0; s; a1 )

r10 r11 r12 r13 r14

Figure 5: MAXQ decomposition; r1 ; : : : ; r14 denote sequence rewards received
primitive actions times 1; : : : ; 14.
nodes) V (0; s) (as computed decomposition equations (9), (10), (11))
expected discounted cumulative reward following policy starting state s.

Proof: proof induction number levels task graph.

level i, compute values C (i; s; (s)) (or V (i; s); primitive) according
decomposition equations. apply decomposition equations compute
Q (i; s; (s)) apply Equation (8) Theorem 1 conclude Q (i; s; (s)) gives
value function level i. = 0, obtain value function entire
hierarchical policy. Q. E. D.
important note representation theorem mention pseudoreward function, pseudo-reward used learning. theorem
captures representational power MAXQ decomposition, address
question whether learning algorithm find given policy.
subject next section.

4. Learning Algorithm MAXQ Decomposition
section presents central contributions paper. First, discuss optimality criteria employed hierarchical reinforcement learning. introduce
MAXQ-0 learning algorithm, learn value functions (and policies) MAXQ
hierarchies pseudo-rewards (i.e., pseudo-rewards zero).
central theoretical result paper MAXQ-0 converges recursively optimal
policy given MAXQ hierarchy. followed brief discussion ways
accelerating MAXQ-0 learning. section concludes description MAXQ-Q
learning algorithm, handles non-zero pseudo-reward functions.
246

fiMAXQ Hierarchical Reinforcement Learning

4.1 Two Kinds Optimality

order develop learning algorithm MAXQ decomposition, must consider
exactly hoping achieve. course, MDP , would find
optimal policy . However, MAXQ method (and hierarchical reinforcement
learning general), programmer imposes hierarchy problem. hierarchy
constrains space possible policies may possible represent
optimal policy value function.
MAXQ method, constraints take two forms. First, within subtask,
possible primitive actions may permitted. example, taxi task,
Navigate(t), North, South, East, West actions available|the Pickup
Putdown actions allowed. Second, consider Max node Mj child nodes
fMj ; : : : ; Mjk g. policy learned Mj must involve executing learned policies
child nodes. policy child node Mji executed, run enters
state Tji . Hence, policy learned Mj must pass subset
terminal state sets fTj ; : : : ; Tjk g.
HAM method shares two constraints addition, imposes
partial policy node, policy subtask Mi must deterministic
refinement given non-deterministic initial policy node i.
\option" approach, policy even constrained. approach,
two non-primitive levels hierarchy, subtasks lower level (i.e.,
whose children primitive actions) given complete policies programmer.
Hence, learned policy upper level must constructed \concatenating"
given lower level policies order.
purpose imposing constraints policy incorporate prior knowledge
thereby reduce size space must searched find good policy.
However, constraints may make impossible learn optimal policy.
can't learn optimal policy, next best target would learn best
policy consistent (i.e., represented by) given hierarchy.
1

1

Definition 7 hierarchically optimal policy MDP policy achieves
highest cumulative reward among policies consistent given hierarchy.

Parr (1998b) proves HAMQ learning algorithm converges probability 1
hierarchically optimal policy. Similarly, given fixed set options, Sutton, Precup,
Singh (1998) prove SMDP learning algorithm converges hierarchically
optimal value function. Incidentally, show primitive actions
made available \trivial" options, SMDP method converges optimal
policy. However, case, hard say anything formal options speed
learning process. may fact hinder (Hauskrecht et al., 1998).
MAXQ decomposition represent value function hierarchical
policy, could easily construct modified version HAMQ algorithm apply
learn hierarchically optimal policies MAXQ hierarchy. However, decided
pursue even weaker form optimality, reasons become clear proceed.
form optimality called recursive optimality.
247

fiDietterich

MaxRoot

G

QExit

QGotoGoal

MaxExit

MaxGotoGoal

*

*

QExitNorth

QExitSouth

QExitEast

North

QNorthG

South

QSouthG

QEastG

East

Figure 6: simple MDP (left) associated MAXQ graph (right). policy shown
left diagram recursively optimal hierarchically optimal. shaded
cells indicate points locally-optimal policy globally optimal.

Definition 8 recursively optimal policy Markov decision process MAXQ
decomposition fM0 ; : : : ; Mk g hierarchical policy = f0 ; : : : ; k g

subtask Mi , corresponding policy optimal SMDP defined set states
Si , set actions Ai , state transition probability function P (s0 ; N js; a),
reward function given sum original reward function R(s0 js; a) pseudoreward function R~ (s0 ).

Note state transition probability distribution, P (s0 ; N js; a) subtask Mi
defined locally optimal policies fj g subtasks descendents Mi
MAXQ graph. Hence, recursive optimality kind local optimality
policy node optimal given policies children.
reason seek recursive optimality rather hierarchical optimality recursive optimality makes possible solve subtask without reference context
executed. context-free property makes easier share re-use
subtasks. turn essential successful use state abstraction.
proceed describe learning algorithm recursive optimality, let us see
recursive optimality differs hierarchical optimality.
easy construct examples policies recursively optimal hierarchically optimal (and vice versa). Consider simple maze problem associated
MAXQ graph shown Figures 6. Suppose robot starts somewhere left room,
must reach goal G right room. robot three actions, North, South,
East, actions deterministic. robot receives reward ,1 move.
Let us define two subtasks:
248

fiMAXQ Hierarchical Reinforcement Learning

Exit. task terminates robot exits left room. set pseudo-

reward function R~ 0 two terminal states (i.e., two states indicated
*'s).
GotoGoal. task terminates robot reaches goal G.
arrows Figure 6 show locally optimal policy within room. arrows
left seek exit left room shortest path, specified
set pseudo-reward function 0. arrows right follow shortest
path goal, fine. However, resulting policy neither hierarchically optimal
optimal.
exists hierarchical policy would always exit left room upper
door. MAXQ value function decomposition represent value function
policy, policy would locally optimal (because, example, states
\shaded" region would follow shortest path doorway). Hence,
example illustrates recursively optimal policy hierarchically optimal
hierarchically optimal policy recursively optimal.
consider moment, see way fix problem. value
upper starred state optimal hierarchical policy ,2 value lower
starred state ,6. Hence, changed R~ values (instead zero),
recursively-optimal policy would hierarchically optimal (and globally optimal).
words, programmer guess right values terminal states
subtask, recursively optimal policy hierarchically optimal.
basic idea first pointed Dean Lin (1995). describe algorithm
makes initial guesses values starred states updates
guesses based computed values starred states resulting recursivelyoptimal policy. proved converge hierarchically optimal policy.
drawback method requires repeated solution resulting hierarchical
learning problem, always yield speedup solving original,
problem.
Parr (1998a) proposed interesting approach constructs set different R~ functions computes recursively optimal policy subtask.
method chooses R~ functions way hierarchically optimal policy
approximated desired degree. Unfortunately, method quite expensive,
relies solving series linear programming problems requires time
polynomial several parameters, including number states jSi j within subtask.
discussion suggests while, principle, possible learn good values
pseudo-reward function, practice, must rely programmer specify single
pseudo-reward function, R~ , subtask. programmer wishes consider small
number alternative pseudo-reward functions, handled defining small
number subtasks identical except R~ functions, permitting
learning algorithm choose one gives best recursively-optimal policy.
experiments, employed following simplified approach defining
R~ . subtask Mi, define two predicates: termination predicate, Ti ,
goal predicate, Gi . goal predicate defines subset terminal states \goal
states", pseudo-reward 0. terminal states fixed constant
249

fiDietterich

pseudo-reward (e.g., ,100) set always better terminate goal state
non-goal state. problems tested MAXQ method,
worked well.
experiments MAXQ, found easy make mistakes
defining Ti Gi . goal defined carefully, easy create set subtasks
lead infinite looping. example, consider problem Figure 6. Suppose
permit fourth action, West, MDP let us define termination goal
predicates right hand room satisfied iff either robot reaches goal
exits room. natural definition, since quite similar definition
left-hand room. However, resulting locally-optimal policy room
attempt move nearest three locations: goal, upper door,
lower door. easily see states near goal, policies
constructed MaxRoot loop forever, first trying leave left room
entering right room, trying leave right room entering left room.
problem easily fixed defining goal predicate Gi right room true
robot reaches goal G. avoiding \undesired termination" bugs
hard complex domains.
worst case, possible programmer specify pseudo-rewards
recursively optimal policy made arbitrarily worse hierarchically optimal
policy. example, suppose change original MDP Figure 6 state
immediately left upper doorway gives large negative reward ,L whenever
robot visits square. rewards everywhere else ,1, hierarchicallyoptimal policy exits room lower door. suppose programmer chosen
instead force robot exit upper door (e.g., assigning pseudo-reward
,10L leaving via lower door). case, recursively-optimal policy leave
upper door suffer large ,L penalty. making L arbitrarily large,
make difference hierarchically-optimal policy recursively-optimal
policy arbitrarily large.

4.2 MAXQ-0 Learning Algorithm
understanding recursively optimal policies, present two learning
algorithms. first one, called MAXQ-0, applies case pseudo-reward
function R~ always zero. first prove convergence properties show
extended give second algorithm, MAXQ-Q, works general
pseudo-reward functions.
Table 2 gives pseudo-code MAXQ-0. MAXQ-0 recursive function executes
current exploration policy starting Max node state s. performs actions
reaches terminal state, point returns count total number primitive
actions executed. execute action, MAXQ-0 calls recursively
(line 9). recursive call returns, updates value completion function
node i. uses count number primitive actions appropriately discount
value resulting state s0 . leaf nodes, MAXQ-0 updates estimated one-step
expected reward, V (i; s). value fft (i) \learning rate" parameter
gradually decreased zero limit.
250

fiMAXQ Hierarchical Reinforcement Learning

Table 2: MAXQ-0 learning algorithm.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18

function MAXQ-0(MaxNode i, State s)
primitive MaxNode
execute i, receive r, observe result state s0
Vt (i; s) := (1 , fft (i)) Vt (i; s) + fft (i) rt
return 1
else
let count = 0
Ti (s) false
choose action according current exploration policy x(i; s)
let N = MAXQ-0(a; 0s) (recursive call)
observe result state
Ct (i; s; a) := (1 , fft (i)) Ct (i; s; a) + fft (i) N Vt (i; s0 )
+1

+1

count := count + N
:= s0

end
return count

end MAXQ-0

// Main program
initialize V (i; s) C (i; s; j ) arbitrarily
MAXQ-0(root node 0, starting state s0 )

three things must specified order make algorithm description
complete.
First, keep pseudo-code readable, Table 2 show \ancestor termination" handled. Recall action, termination predicates
subroutines calling stack checked. termination predicate one
satisfied, calling stack unwound highest terminated subroutine. cases, C values updated subroutines interrupted
except follows. subroutine invoked subroutine j , j 's termination condition
satisfied, subroutine update value C (i; s; j ).
Second, must specify compute Vt (i; s0 ) line 11, since stored
Max node. computed following modified versions decomposition
equations:
(

maxa Qt (i; s; a) composite
(13)
Vt (i; s)
primitive
Qt(i; s; a) = Vt(a; s) + Ct (i; s; a):
(14)
equations ect two important changes compared Equations (10) (11).
First, Equation (13), Vt (i; s) defined terms Q value best action a, rather
action chosen fixed hierarchical policy. Second, superscripts,
current value function, Vt (i; s), based fixed hierarchical policy .
compute Vt (i; s) using equations, must perform complete search
paths MAXQ graph starting node ending leaf nodes. Table 3

Vt (i; s) =

251

fiDietterich

Table 3: Pseudo-code Greedy Execution MAXQ Graph.
function EvaluateMaxNode(i; s)
1
2
3
4
5
6
7

primitive Max node
return hVt (i; s); ii
else
j 2 Ai ,
let hVt (j; s); aj = EvaluateMaxNode(j; s)
let j hg = argmaxj Vt(j; s) + Ct (i; s; j )
return hVt (j hg ; s); ajhg
end // EvaluateMaxNode

gives pseudo-code recursive function, EvaluateMaxNode, implements depthfirst search. addition returning Vt (i; s), EvaluateMaxNode returns action
leaf node achieves value. information needed MAXQ-0,
useful later consider non-hierarchical execution learned recursivelyoptimal policy.
search computationally expensive, problem future research
develop ecient methods computing best path graph. One
approach perform best-first search use bounds values within subtrees
prune useless paths MAXQ graph. better approach would make
computation incremental, state environment changes,
nodes whose values changed result state change re-considered.
possible develop ecient bottom-up method similar RETE algorithm (and
successors) used SOAR architecture (Forgy, 1982; Tambe & Rosenbloom,
1994).
third thing must specified complete definition MAXQ-0
exploration policy, x . require x ordered GLIE policy.

Definition 9 ordered GLIE policy GLIE policy (Greedy Limit Infinite

Exploration) converges limit ordered greedy policy, greedy policy
imposes arbitrary fixed order ! available actions breaks ties favor
action appears earliest order.

need property order ensure MAXQ-0 converges uniquely-defined
recursively optimal policy. fundamental problem recursive optimality
general, Max node choice many different locally optimal policies given
policies adopted descendent nodes. different locally optimal policies
achieve locally optimal value function, give rise different probability transition functions P (s0 ; N js; i). result Semi-Markov Decision
Problems defined next level node MAXQ graph differ depending
various locally optimal policies chosen node i. differences may
lead better worse policies higher levels MAXQ graph, even though make
difference inside subtask i. practice, designer MAXQ graph need
design pseudo-reward function subtask ensure locally optimal policies
252

fiMAXQ Hierarchical Reinforcement Learning

equally valuable parent subroutine. carry formal analysis,
rely arbitrary tie-breaking mechanism.2 establish fixed ordering
Max nodes MAXQ graph (e.g., left-to-right depth-first numbering), break ties
favor lowest-numbered action, defines unique policy Max node.
consequently, induction, defines unique policy entire MAXQ graph. Let
us call policy r . use r subscript denote recursively optimal quantities
ordered greedy policy. Hence, corresponding value function Vr , Cr
Qr denote corresponding completion function action-value function. prove
MAXQ-0 algorithm converges r .

Theorem 3 Let = hS; A; P; R; P0 either episodic MDP deterministic

policies proper discounted infinite horizon MDP discount factor . Let H
MAXQ graph defined subtasks fM0 ; : : : ; Mk g pseudo-reward function
R~ (s0 ) zero s0. Let fft (i) > 0 sequence constants Max node



X
X
lim

(

)
=
1

lim
ff2t (i) < 1
(15)

!1
!1
t=1

t=1

Let x (i; s) ordered GLIE policy node state assume
immediate rewards bounded. probability 1, algorithm MAXQ-0 converges
r , unique recursively optimal policy consistent H x.

Proof: proof follows argument similar introduced prove convergence

Q learning SARSA(0) (Bertsekas & Tsitsiklis, 1996; Jaakkola et al., 1994).
employ following result stochastic approximation theory, state without
proof:

Lemma 1 (Proposition 4.5 Bertsekas Tsitsiklis, 1996) Consider iteration
rt+1 (i) := (1 , fft (i))rt (i) + fft (i)((Urt )(i) + wt (i) + ut(i)):
Let Ft = fr0 (i); : : : ; rt (i); w0 (i); : : : ; wt,1 (i); ff0 (i); : : : ; fft (i); 8ig entire history
iteration.


(a) fft (i) 0 satisfy conditions (15)
(b) every t, noise terms wt (i) satisfy E [wt (i)jFt ] = 0
(c) Given norm jj jj Rn , exist constants B E [wt2 (i)jFt ]
+ B jjrtjj2 .
(d) exists vector r , positive vector , scalar 2 [0; 1),
t,

jjUrt , rjj jjrt , rjj

2. Alternatively, could break ties using stochastic policy chose randomly among tied
actions.

253

fiDietterich

(e) exists nonnegative random sequence converges zero probability
1
jut (i)j t(jjrt jj + 1)
rt converges r probability 1. notation jj jj denotes weighted maximum
norm
jA(i)j :
jjAjj = max
(i)

structure proof Theorem 3 inductive, starting leaves
MAXQ graph working toward root. employ different time clock
node count number update steps performed MAXQ-0 node.
variable always refer time clock current node i.
prove base case primitive Max node, note line 3 MAXQ-0
standard stochastic approximation algorithm computing expected reward
performing action state s, therefore converges conditions given
above.
prove recursive case, consider composite Max node child node j . Let
Pt (s0; N js; j ) transition probability distribution performing child action j state
time (i.e., following exploration policy descendent nodes node j ).
inductive assumption, MAXQ-0 applied j converge (unique) recursively optimal value function Vr (j; s) probability 1. Furthermore, MAXQ-0
following ordered GLIE policy j descendents, converge executing greedy policy respect value functions, Pt (s0 ; N js; j ) converge
Pr (s0 ; N js; j ), unique transition probability function executing child j
locally optimal policy r . remains shown update assignment C
(line 11 MAXQ-0 algorithm) converges optimal Cr function probability
1.
prove this, apply Lemma 1. identify x lemma
state-action pair (s; a). vector rt completion-cost table Ct (i; s; a)
s; fixed update steps. vector r optimal completion-cost
Cr (i; s; a) (again, fixed i). Define mapping U
(UC )(i; s; a) =

X

s0





0 0
0 0
Pr (s0 ; N js; a) N max
0 [C (i; ; ) + Vr (a ; )]


C update MDP Mi assuming descendent value functions,
Vr (a; s), transition probabilities, Pr(s0 ; N js; a), converged.
apply lemma, must first express C update formula form
update rule lemma. Let state results performing state s. Line
11 written

Ct+1 (i; s; a) :=

(1 , fft (i)) Ct (i; s; a) + fft (i) N





max[Ct (i; s; a0 ) + Vt (a0 ; s)]
a0

:= (1 , fft (i)) Ct (i; s; a) + fft (i) [(UCt )(i; s; a) + wt (i; s; a) + ut (i; s; a)]
254

fiMAXQ Hierarchical Reinforcement Learning







wt (i; s; a) = N max
[C (i; s; a0 ) + Vt (a0 ; s)] ,
a0
X

s0 ;N

ut (i; s; a) =

X

s0 ;N
X

s0 ;N









Pt (s0 ; N js; a) N max
[C (i; s0 ; a0 ) + Vt (a0 ; s0 )]
a0

0 0
0 0
Pt (s0 ; N js; a) N max
0 [Ct (i; ; ) + Vt (a ; )]




,



Pr (s0; N js; a) N max
[C (i; s0 ; a0 ) + Vr (a0 ; s0 )]
a0

wt (i; s; a) difference update node using single sample
point drawn according Pt (s0 ; N js; a) update using full distribution
Pt (s0; N js; a). value ut (i; s; a) captures difference update using
current probability transitions Pt (s0 ; N js; a) current value functions children
Vt (a0; s0 ) update using optimal probability transitions Pr (s0 ; N js; a)
optimal values children Vr (a0 ; s0 ).
verify conditions Lemma 1.
Condition (a) assumed conditions theorem fft (s; a) = fft (i).
Condition (b) satisfied sampled Pt (s0 ; N js; a), expected value
difference zero.
Condition (c) follows directly fact jCt (i; s; a)j jVt (i; s)j bounded.
show bounded episodic case discounted case
follows. episodic case, assumed policies proper. Hence, trajectories
terminate finite time finite total reward. discounted case, infinite sum
future rewards bounded one-step rewards bounded. values C V
computed temporal averages cumulative rewards received finite number
(bounded) updates, hence, means, variances, maximum values
bounded.
Condition (d) condition U weighted max norm pseudo-contraction.
derive starting weighted max norm Q learning. well known
Q weighted max norm pseudo-contraction (Bertsekas & Tsitsiklis, 1996)
episodic case deterministic policies proper (and discount factor = 1)
infinite horizon discounted case (with < 1). is, exists positive
vector scalar 2 [0; 1), t,

jjTQt , Qjj jjQt , Qjj ;

(16)

operator
(TQ)(s; a) =

X

s0 ;N

P (s0; N js; a) N [R(s0 js; a) + max
Q(s0 ; a0 )]:
a0

show derive pseudo-contraction C update operator U .
plan show first express U operator learning C terms operator
updating Q values. replace TQ pseudo-contraction equation Q
255

fiDietterich

learning UC , show U weighted max-norm pseudo-contraction
weights .
Recall Eqn. (10) Q(i; s; a) = C (i; s; a) + V (a; s). Furthermore, U operator
performs updates using optimal value functions child nodes, write
Qt (i; s; a) = Ct (i; s; a) + V (a; s). children node converged,
Q-function version Bellman equation MDP Mi written

Q(i; s; a) =

X

s0 ;N

Pr(s0 ; N js; a) N [Vr (a; s) + max
Q(i; s0 ; a0 )]:
a0

noted before, Vr (a; s) plays role immediate reward function Mi .
Therefore, node i, operator rewritten
(TQ)(i; s; a) =

X

s0 ;N

Pr (s0 js; a) N [Vr(a; s) + max
Q(i; s0 ; a0 )]:
a0

replace Q(i; s; a) C (i; s; a) + Vr (a; s), obtain
(TQ)(i; s; a) =

X

s0 ;N

Pr (s0; N js; a) N (Vr (a; s) + max
[C (i; s0 ; a0 ) + Vr (a0 ; s0 )]):
a0

Note Vr (a; s) depend s0 N , move outside expectation
obtain
(TQ)(i; s; a) = Vr (a; s) +

X

s0 ;N

Pr (s0 ; N js; a) N (max
[C (i; s0 ; a0 ) + Vr (a0 ; s0 )])
a0

= Vr (a; s) + (UC )(i; s; a)

Abusing notation slightly, express vector form TQ(i) = Vr + UC (i).
Similarly, write Qt (i; s; a) = Ct (i; s; a)+ Vr (a; s) vector form Qt (i) = Ct (i)+ Vr .
substitute two formulas max norm pseudo-contraction formula
, Eqn. (16) obtain

jjVr + UCt (i) , (Cr(i) + Vr)jj jjVr + Ct (i) , (Cr(i) + Vr)jj :
Thus, U weighted max-norm pseudo-contraction,

jjUCt (i) , Cr(i)jj jjCt (i) , Cr(i)jj ;
condition (d) satisfied.
Finally, easy verify (e), important condition. assumption,
ordered GLIE policies child nodes converge probability 1 locally optimal
policies children. Therefore Pt (s0 ; N js; a) converges Pr (s0 ; N js; a) s0; N; s;
probability 1 Vt (a; s) converges probability 1 Vr (a; s) child
actions a. Therefore, jut j converges zero probability 1. trivially construct
sequence = jut j bounds convergence,

jut (s; a)j (jjCt (s; a)jj + 1):
256

fiMAXQ Hierarchical Reinforcement Learning

verified conditions Lemma 1, conclude Ct (i) converges
Cr(i) probability 1. induction, conclude holds nodes
MAXQ including root node, value function represented MAXQ graph
converges unique value function recursively optimal policy r . Q.E.D.
important aspect theorem proves Q learning take
place levels MAXQ hierarchy simultaneously|the higher levels need
wait lower levels converged begin learning. necessary
lower levels eventually converge (locally) optimal policies.

4.3 Techniques Speeding MAXQ-0

Algorithm MAXQ-0 extended accelerate learning higher nodes graph
technique call \all-states updating". action chosen Max node
state s, execution move environment sequence states
= s1; : : : ; sN ; sN +1 = s0. subroutines Markovian, resulting
state s0 would reached started executing action state s2 , s3 ,
state including sN . Hence, execute version line 11 MAXQ-0
intermediate states shown replacement pseudo-code:
11a
j 1 N
11b
Ct (i; sj ; a) := (1 , fft (i)) Ct(i; sj ; a) + fft (i) N ,j maxa Qt (i; s0 ; a0 )
11c
end //
implementation, composite action executed MAXQ-0, constructs
linked list sequence primitive states visited. list returned
composite action terminates. parent Max node process state
list shown above. parent Max node concatenates state lists receives
children passes parent terminates. experiments paper
employ all-states updating.
Kaelbling (1993) introduced related, powerful, method accelerating hierarchical reinforcement learning calls \all-goals updating." understand
method, suppose primitive action, several composite tasks could
invoked primitive action. all-goals updating, whenever primitive action
executed, equivalent line 11 MAXQ-0 applied every composite task could
invoked primitive action. Sutton, Precup, Singh (1998) prove
composite tasks converge optimal Q values all-goals updating. Furthermore, point exploration policy employed choosing primitive
actions different policies subtasks learned.
straightforward implement simple form all-goals updating within MAXQ
hierarchy case composite tasks invoke primitive actions. Whenever one
primitive actions executed state s, update C (i; s; a) value parent
tasks invoke a.
However, additional care required implement all-goals updating non-primitive
actions. Suppose executing exploration policy, following sequence world
states actions obtained: s0 ; a0 ; s1 ; : : : ; ak,1 ; sk,1 ; ak ; sk+1 . Let j composite task terminated state sk+1 , let sk,n; ak,n ; : : : ; ak,1 ; ak sequence
actions could executed subtask j children. words, suppose
(

+1

257

+1

)

0

fiDietterich

possible \parse" state-action sequence terms series subroutine calls
returns one invocation subtask j . possible parent task invokes j ,
update value C (i; sk,n ; j ). course, order updates useful,
exploration policy must ordered GLIE policy converge recursively
optimal policy subtask j descendents. cannot follow arbitrary exploration
policy, would produce accurate samples result states drawn according
P (s0 ; N js; j ). Hence, unlike simple case described Sutton, Precup, Singh,
exploration policy cannot different policies subtasks learned.
Although considerably reduces usefulness all-goals updating,
completely eliminate it. simple way implementing non-primitive all-goals updating
would perform MAXQ-Q learning usual, whenever subtask j invoked
state returned, could update value C (i; s; j ) potential calling subtasks
i. implemented this, however, complexity involved identifying
possible actual parameters potential calling subroutines.

4.4 MAXQ-Q Learning Algorithm
shown convergence MAXQ-0, let us design learning algorithm
work arbitrary pseudo-reward functions, R~ (s0 ). could add pseudoreward MAXQ-0, would effect changing MDP
different reward function. pseudo-rewards \contaminate" values
completion functions computed hierarchy. resulting learned policy
recursively optimal original MDP.
problem solved learning one completion function use \inside"
Max node separate completion function use \outside" Max node. quantities used \inside" node written tilde: R~ , C~ , Q~ . quantities used
\outside" node written without tilde.
\outside" completion function, C (i; s; a) completion function
discussing far paper. computes expected reward completing task
Mi performing action state following learned policy Mi .
computed without reference R~ . completion function used parent
tasks compute V (i; s), expected reward performing action starting state s.
second completion function C~ (i; s; a) completion function use
\inside" node order discover locally optimal policy task Mi . function
incorporate rewards \real" reward function, R(s0 js; a),
pseudo-reward function, R~ (s0 ). used EvaluateMaxNode line 6
choose best action j hg execute. Note, however, EvaluateMaxNode still
return \external" value Vt (j hg ; s) chosen action.
employ two different update rules learn two completion functions.
C~ function learned using update rule similar Q learning rule line 11
MAXQ-0. C function learned using update rule similar SARSA(0)|
purpose learn value function policy discovered optimizing C~ .
Pseudo-code resulting algorithm, MAXQ-Q shown Table 4.
key step lines 15 16. line 15, MAXQ-Q first updates C~ using value
greedy action, , resulting state. update includes pseudo-reward R~ .
258

fiMAXQ Hierarchical Reinforcement Learning

Table 4: MAXQ-Q learning algorithm.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23

function MAXQ-Q(MaxNode i, State s)
let seq = () sequence states visited executing
primitive MaxNode
execute i, receive r, observe result state s0
Vt (i; s) := (1 , fft (i)) Vt (i; s) + fft (i) rt
push onto beginning seq
else
let count = 0
Ti (s) false
choose action according current exploration policy x(i; s)
let childSeq = MAXQ-Q(a;s), childSeq sequence states visited
+1

executing action a. (in reverse order)
observe result state s0
let = argmaxa [C~t (i; s0 ; a0 ) + Vt (a0 ; s0 )]
let N = 1
childSeq
C~t+1 (i; s; a) := (1 , fft (i)) C~t (i; s; a) + fft (i) N [R~ (s0 ) + C~t (i; s0 ; ) + Vt(a ; s)]
Ct+1 (i; s; a) := (1 , fft (i)) Ct (i; s; a) + fft (i) N [Ct (i; s0 ; ) + Vt(a ; s0 )]
N := N + 1
end //
append
childSeq onto front seq
:= s0
end //
end // else
return seq
end MAXQ-Q
0

line 16, MAXQ-Q updates C using greedy action , even would
greedy action according \uncontaminated" value function. update,
course, include pseudo-reward function.
important note whereever Vt (a; s) appears pseudo-code, refers
\uncontaminated" value function state executing Max node a.
computed recursively exactly way MAXQ-0.
Finally, note pseudo-code incorporates all-states updating, call
MAXQ-Q returns list states visited execution,
updates lines 15 16 performed states. list states
ordered most-recent-first, states updated starting last state visited
working backward starting state, helps speed algorithm.
MAXQ-Q converged, resulting recursively optimal policy computed
node choosing action maximizes Q~ (i; s; a) = C~ (i; s; a)+ V (a; s) (breaking
ties according fixed ordering established ordered GLIE policy).
reason gave name \Max nodes" nodes represent subtasks (and
learned policies) within MAXQ graph. Q node j parent node stores
C~ (i; s; j ) C (i; s; j ), computes Q~ (i; s; j ) Q(i; s; j ) invoking child
Max node j . Max node takes maximum Q values computes either
V (i; s) computes best action, using Q~ .
259

fiDietterich

Corollary 1 conditions Theorem 3, MAXQ-Q converges unique

recursively optimal policy MDP defined MAXQ graph H , pseudo-reward functions
R~ , ordered GLIE exploration policy x.
Proof: argument identical to, tedious than, proof Theorem 3.
proof convergence C~ values identical original proof C values,
relies proving convergence \new" C values well, follows
weighted max norm pseudo-contraction argument. Q.E.D.

5. State Abstraction

many reasons introduce hierarchical reinforcement learning, perhaps
important reason create opportunities state abstraction. introduced
simple taxi problem Figure 1, pointed within subtask, ignore
certain aspects state space. example, performing MaxNavigate(t),
taxi make navigation decisions regardless whether passenger
taxi. purpose section formalize conditions safe
introduce state abstractions show convergence proofs MAXQ-Q
extended prove convergence presence state abstraction. Specifically,
identify five conditions permit \safe" introduction state abstractions.
Throughout section, use taxi problem running example,
see five conditions permit us reduce number distinct values
must stored order represent MAXQ value function decomposition.
establish starting point, let us compute number values must stored
taxi problem without state abstraction.
MAXQ representation must tables C functions internal
nodes V functions leaves. First, six leaf nodes, store V (i; s),
must store 500 values node, 500 states; 25 locations, 4 possible
destinations passenger, 5 possible current locations passenger (the four
special locations inside taxi itself). Second, root node, two children,
requires 2 500 = 1000 values. Third, MaxGet MaxPut nodes, 2
actions each, one requires 1000 values, total 2000. Finally, MaxNavigate(t),
four actions, must consider target parameter t, take
four possible values. Hence, effectively 2000 combinations states values
action, 8000 total values must represented. total, therefore, MAXQ
representation requires 14,000 separate quantities represent value function.
place number perspective, consider Q learning representation must
store separate value six primitive actions 500 possible states,
total 3,000 values. Hence, see without state abstraction, MAXQ
representation requires four times memory Q table!

5.1 Five Conditions Permit State Abstraction

introduce five conditions permit introduction state abstractions.
condition, give definition prove lemma states condition satisfied, value function corresponding class policies
260

fiMAXQ Hierarchical Reinforcement Learning

represented abstractly (i.e., abstract versions V C functions). condition, provide rules identifying condition satisfied
give examples taxi domain.
begin introducing definitions notation.

Definition 10 Let MDP H MAXQ graph defined . Suppose

state written vector values set state variables. Max
node i, suppose state variables partitioned two sets Xi Yi , let
function projects state onto values variables Xi . H combined
called state-abstracted MAXQ graph.

cases state variables partitioned, often write = (x; y)
mean state represented vector values state variables X
vector values state variables . Similarly, sometimes write
P (x0 ; y0; N jx; y; a), V (a; x; y), R~ (x0 ; y0 ) place P (s0; N js; a), V (a; s), R~ (s0 ),
respectively.

Definition 11 (Abstract Policy) abstract hierarchical policy MDP state-

abstracted MAXQ graph H associated abstraction functions , hierarchical policy
policy (corresponding subtask Mi ) satisfies condition two
states s1 s2 (s1 ) = (s2 ), (s1 ) = (s2 ). (When stochastic policy,
exploration policy, interpreted mean probability distributions
choosing actions states.)

order MAXQ-Q converge presence state abstractions, require
times (instantaneous) exploration policy abstract hierarchical policy.
One way achieve construct exploration policy uses information relevant state variables deciding action perform. Boltzmann exploration based (state-abstracted) Q values, -greedy exploration, counter-based
exploration based abstracted states abstract exploration policies. Counter-based
exploration based full state space abstract exploration policy.
introduced notation, let us describe analyze five abstraction conditions. identified three different kinds conditions
abstractions introduced. first kind involves eliminating irrelevant variables
within subtask MAXQ graph. form abstraction, nodes toward
leaves MAXQ graph tend relevant variables, nodes higher
graph relevant variables. Hence, kind abstraction useful
lower levels MAXQ graph.
second kind abstraction arises \funnel" actions. macro actions
move environment large number initial states small number
resulting states. completion cost subtasks represented using number
values proportional number resulting states. Funnel actions tend appear higher
MAXQ graph, form abstraction useful near root graph.
third kind abstraction arises structure MAXQ graph itself.
exploits fact large parts state space subtask may reachable
termination conditions ancestors MAXQ graph.
261

fiDietterich

begin describing two abstraction conditions first type. present
two conditions second type. finally, describe one condition third type.
5.1.1 Condition 1: Max Node Irrelevance

first condition arises set state variables irrelevant Max node.

Definition 12 (Max Node Irrelevance) Let Mi Max node MAXQ graph H

MDP . set state variables irrelevant node state variables
partitioned two sets X stationary abstract hierarchical
policy executed descendents i, following two properties hold:

state transition probability distribution P (s0; N js; a) node factored
product two distributions:

P (x0 ; y0 ; N jx; y; a) = P (y0jx; y; a) P (x0 ; N jx; a);

(17)

y0 give values variables , x x0 give values
variables X .

pair states s1 = (x; y1 ) s2 = (x; y2 ) (s1) = (s2) = x,
child action a, V (a; s1 ) = V (a; s2 ) R~ (s1 ) = R~ (s2 ).

Note two conditions must hold stationary abstract policies executed
descendents subtask i. discuss rather strong
requirements satisfied practice. First, however, prove conditions
sucient permit C V tables represented using state abstractions.

Lemma 2 Let MDP full-state MAXQ graph H , suppose state vari-

ables Yi irrelevant Max node i. Let (s) = x associated abstraction function
projects onto remaining relevant variables Xi . Let abstract hierarchical
policy. action-value function Q node represented compactly,
one value completion function C (i; s; j ) equivalence class states
share values relevant variables.
Specifically Q (i; s; j ) computed follows:

Q (i; s; j ) = V (j; (s)) + C (i; (s); j )


C (i; x; j ) =

X

x0 ;N

P (x0 ; N jx; j ) N [V ((x0 ); x0 ) + R~ (x0 ) + C (i; x0 ; (x0 ))];

V (j 0 ; x0 ) = V (j 0 ; x0 ; y0 ), R~ (x0 ) = R~ (x0 ; y0 ), (x) = (x; y0 ) arbitrary
value y0 irrelevant state variables Yi .
262

fiMAXQ Hierarchical Reinforcement Learning

Proof: Define new MDP i(Mi ) node follows:
States: X = fx j i(s) = x; 2 g.
Actions: A.
Transition probabilities: P (x0 ; N jx; a)
Reward function: V (a; x) + R~ i(x0)

abstract policy, decisions states (s) = x
x. Therefore, well-defined policy (Mi ). action-value function
(Mi ) unique solution following Bellman equation:
X
Q (i; x; j ) = V (j; x) + P (x0 ; N jx; j ) N [R~ i(x0 ) + Q (i; x0 ; (x0 ))]
(18)
x0 ;N

Compare Bellman equation Mi :
X
Q (i; s; j ) = V (j; s) + P (s0; N js; j ) N [R~i (s0) + Q (i; s0 ; (s0 ))]
s0 ;N

(19)

note V (j; s) = V (j; (s)) = V (j; x) R~ (s0 ) = R~ ((s0 )) = R~ (x0 ). Furthermore, know distribution P factored separate distributions Yi
Xi . Hence, rewrite (19)
X
X
Q (i; s; j ) = V (j; x) + P (y0 jx; y; j ) P (x0 ; N jx; j ) N [R~ (x0 ) + Q (i; s0 ; (s0 ))]
y0

x0 ;N

right-most sum depend y0 , sum y0 evaluates 1,
eliminated give
X
Q (i; s; j ) = V (j; x) + P (x0 ; N jx; j ) N [R~ i(x0 ) + Q (i; s0 ; (s0 ))]:
(20)
x0 ;N

Finally, note equations (18) (20) identical except expressions
Q values. Since solution Bellman equation unique, must conclude
Q (i; s; j ) = Q (i; (s); j ):
rewrite right-hand side obtain
Q (i; s; j ) = V (j; (s)) + C (i; (s); j );

X
C (i; x; j ) = P (x0; N jx; j ) N [V ((x0 ); x0 ) + R~ (x0 ) + C (i; x0 ; (x0 ))]:

Q.E.D.

x0 ;N

course primarily interested able discover represent optimal
policy node i. following corollary shows optimal policy abstract
policy, hence, represented abstractly.
263

fiDietterich

Corollary 2 Consider conditions Lemma 2, change ab-

stract hierarchical policy executed descendents node i, node
i. Let ! ordering actions. optimal ordered policy ! node
abstract policy, action-value function represented abstractly.

Proof: Define policy ! optimal ordered policy abstract MDP

(M ), let Q (i; x; j ) corresponding optimal action-value function.
argument given above, Q solution optimal Bellman equation
original MDP. means policy ! defined ! (s) = ((s)) optimal

ordered policy, construction, abstract policy. Q.E.D.
stated, Max node irrelevance condition appears quite dicult satisfy, since
requires state transition probability distribution factor X components
possible abstract hierarchical policies. However, practice, condition often
satisfied.
example, let us consider Navigate(t) subtask. source destination
passenger irrelevant achievement subtask. policy successfully completes subtask value function regardless source
destination locations passenger. abstracting away passenger source destination, obtain huge savings space. Instead requiring 8000 values represent
C functions task, require 400 values (4 actions, 25 locations, 4 possible
values t).
advantages form abstraction similar obtained Boutilier,
Dearden Goldszmidt (1995) belief network models actions exploited
simplify value iteration stochastic planning. Indeed, one way understanding
conditions Definition 12 express form decision diagram, shown
Figure 7. diagram shows irrelevant variables affect rewards
either directly indirectly, therefore, affect either value function
optimal policy.
One rule noticing cases abstraction condition holds examine
subgraph rooted given Max node i. set state variables irrelevant leaf
state transition probabilities reward functions pseudo-reward functions
termination conditions subgraph, variables satisfy Max Node
Irrelevance condition:

Lemma 3 Let MDP associated MAXQ graph H , let Max node

H . Let Xi Yi partition state variables . set state variables Yi
irrelevant node
primitive leaf node descendent i,
P (x0 ; y0 jx; y; a) = P (y0 jx; y; a)P (x0 jx; a)
R(x0; y0 jx; y; a) = R(x0 jx; a),

internal node j equal node descendent , R~ j (x0 ; y0) =
R~j (x0) termination predicate Tj (x0 ; y0 ) true iff Tj (x0).
264

fiMAXQ Hierarchical Reinforcement Learning

j

V

X

X





Figure 7: dynamic decision diagram represents conditions Definition 12.
probabilistic nodes X represent state variables time t, nodes
X 0 0 represent state variables later time + N . square action
node j chosen child subroutine, utility node V represents value
function V (j; x) child action. Note X may uence 0 ,
cannot affect X 0 , therefore, cannot affect V .

Proof: must show abstract hierarchical policy give rise SMDP

node whose transition probability distribution factors whose reward function depends
Xi . definition, abstract hierarchical policy choose actions based
upon information Xi . primitive probability transition functions factor
independent component Xi since termination conditions nodes
based variables Xi , probability transition function Pi (x0 ; y0 ; N jx; y; a)
must factor Pi (y0 jx; y; a) Pi (x0 ; N jx; a). Similarly, reward functions
V (j; x; y) must equal V (j; x), rewards received within subtree (either
leaves pseudo-rewards) depend variables Xi . Therefore,
variables Yi irrelevant Max node i. Q.E.D.
Taxi task, primitive navigation actions, North, South, East, West
depend location taxi location passenger. pseudoreward function termination condition MaxNavigate(t) node depend
location taxi (and parameter t). Hence, lemma applies, passenger
source destination irrelevant MaxNavigate node.
5.1.2 Condition 2: Leaf Irrelevance

second abstraction condition describes situations apply state abstractions leaf nodes MAXQ graph. leaf nodes, obtain stronger result
Lemma 2 using slightly weaker definition irrelevance.
265

fiDietterich

Definition 13 (Leaf Irrelevance) set state variables irrelevant primitive
action MAXQ graph states expected value reward function,
X
V (a; s) = P (s0 js; a)R(s0 js; a)
s0

depend values state variables . words,
pair states s1 s2 differ values variables ,
X

s01

P (s01 js1 ; a)R(s01 js1 ; a) =

X

s02

P (s02 js2 ; a)R(s02 js2 ; a):

condition satisfied leaf a, following lemma shows
represent value function V (a; s) compactly.

Lemma 4 Let MDP full-state MAXQ graph H , suppose state vari-

ables irrelevant leaf node a. Let (s) = x associated abstraction function
projects onto remaining relevant variables X . represent V (a; s)
state abstracted value function V (a; (s)) = V (a; x).

Proof: According definition Leaf Irrelevance, two states differ

irrelevant state variables value V (a; s). Hence, represent
unique value V (a; x). Q.E.D.
two rules finding cases Leaf Irrelevance applies. first rule shows
probability distribution factors, Leaf Irrelevance.

Lemma 5 Suppose probability transition function primitive action a, P (s0js; a), factors P (x0 ; y0 jx; y; a) = P (y0 jx; y; a)P (x0 jx; a) reward function satisfies R(s0 js; a) =
R(x0jx; a). variables irrelevant leaf node a.
Proof: Plug definition V (a; s) simplify.
X
V (a; s) =
P (s0 js; a)R(s0 js; a)
=
=
=

s0

X

x0 ;y0
X

y0

X

x0

P (y0 jx; y; a)P (x0 jx; a)R(x0 jx; a)

P (y0 jx; y; a)

X

x0

P (x0 jx; a)R(x0 jx; a)

P (x0 jx; a)R(x0 jx; a)

Hence, expected reward action depends variables X
variables . Q.E.D.
second rule shows reward function primitive action constant,
apply state abstractions even P (s0 js; a) factor.

Lemma 6 Suppose R(s0js; a) (the reward function action MDP ) always equal

constant ra . entire state irrelevant primitive action a.
266

fiMAXQ Hierarchical Reinforcement Learning

Proof:
V (a; s) =

X

s0

P (s0 js; a)R(s0 js; a)

X

=
P (s0 js; a)ra
0

= ra :
depend s, entire state irrelevant primitive action a. Q.E.D.
lemma satisfied four leaf nodes North, South, East, West taxi
task, one-step reward constant (,1). Hence, instead requiring 2000
values store V functions, need 4 values|one action. Similarly,
expected rewards Pickup Putdown actions require 2 values, depending
whether corresponding actions legal illegal. Hence, together, require 4
values, instead 1000 values.
5.1.3 Condition 3: Result Distribution Irrelevance

consider condition results \funnel" actions.

Definition 14 (Result Distribution Irrelevance). set state variables Yj irrelevant result distribution action j if, abstract policies executed node j
descendents MAXQ hierarchy, following holds: pairs states s1
s2 differ values state variables Yj ,

P (s0 ; N js1 ; j ) = P (s0 ; N js2 ; j )
s0 N .

condition satisfied subtask j , C value parent task
represented compactly:

Lemma 7 Let MDP full-state MAXQ graph H , suppose set

state variables Yj irrelevant result distribution action j , child Max
node i. Let ij associated abstraction function: ij (s) = x. define
abstract completion cost function C (i; ij (s); j ) states s,

C (i; s; j ) = C (i; ij (s); j ):

Proof: completion function fixed policy defined follows:
X
C (i; s; j ) = P (s0 ; N js; j ) N Q (i; s0 ):
s0 ;N

(21)

Consider two states s1 s2 , ij (s1 ) = ij (s2 ) = x. Result Distribution Irrelevance, transition probability distributions same. Hence,
right-hand sides (21) value, conclude

C (i; s1 ; j ) = C (i; s2 ; j ):
267

fiDietterich

Therefore, define abstract completion function, C (i; x; j ) represent quantity. Q.E.D.
undiscounted cumulative reward problems, definition result distribution irrelevance weakened eliminate N , number steps. needed
pairs states s1 s2 differ irrelevant state variables,
P (s0 js1 ; j ) = P (s0 js2; j ) (for s0 ). undiscounted case, Lemma 7 still holds
revised definition.
might appear result distribution irrelevance condition would rarely satisfied, often find cases condition true. Consider, example, Get
subroutine taxi task. matter location taxi state s, taxi
passenger's starting location Get finishes executing (i.e.,
taxi completed picking passenger). Hence, starting location
irrelevant resulting location taxi, P (s0 js1 ; Get) = P (s0 js2 ; Get)
states s1 s2 differ taxi's location.
Note, however, maximizing discounted reward, taxi's location would
irrelevant, probability Get terminate exactly N steps would
depend location taxi, could differ states s1 s2 . Different values
N produce different amounts discounting (21), hence, cannot ignore
taxi location representing completion function Get.
undiscounted case, applying Lemma 7, represent C (Root; s; Get)
using 16 distinct values, 16 equivalence classes states (4 source locations
times 4 destination locations). much less 500 quantities unabstracted
representation.
Note although state variables may irrelevant result distribution
subtask j , may important within subtask j . Taxi task, location
taxi critical representing value V (Get; s), irrelevant result state
distribution Get, therefore irrelevant representing C (Root; s; Get). Hence,
MAXQ decomposition essential obtaining benefits result distribution irrelevance.
\Funnel" actions arise many hierarchical reinforcement learning problems. example, abstract actions move robot doorway move car onto entrance
ramp freeway property. Result Distribution Irrelevance condition
applicable situations long undiscounted setting.
5.1.4 Condition 4: Termination

fourth condition closely related \funnel" property. applies subtask
guaranteed cause parent task terminate goal state. sense, subtask
funneling environment set states described goal predicate
parent task.

Lemma 8 (Termination). Let Mi task MAXQ graph states
goal predicate Gi (s) true, pseudo-reward function R~ (s) = 0. Suppose
child task state hierarchical policies ,

8 s0 Pi (s0; N js; a) > 0 ) Gi(s0 ):
268

fiMAXQ Hierarchical Reinforcement Learning

(i.e., every possible state s0 results applying make goal predicate,
Gi , true.)
policy executed node i, completion cost C (i; s; a) zero
need explicitly represented.

Proof: action executed state s, guaranteed result state s0

Gi (s) true. definition, goal states satisfy termination predicate Ti (s),
task terminate. Gi(s) true, terminal pseudo-reward zero,
hence, completion function always zero. Q.E.D.
example, Taxi task, states taxi holding passenger,
Put subroutine succeed result goal terminal state Root.
termination predicate Put (i.e., passenger destination location)
implies goal condition Root (which same). means C (Root; s; Put)
uniformly zero, states Put terminated.
easy detect cases Termination condition satisfied. need
compare termination predicate Ta subtask goal predicate Gi parent
task. first implies second, termination lemma satisfied.
5.1.5 Condition 5: Shielding

shielding condition arises structure MAXQ graph.
Lemma 9 (Shielding). Let Mi task MAXQ graph state
paths root graph node Mi subtask j (possibly equal i)
whose termination predicate Tj (s) true, Q nodes Mi need represent
C values state s.
Proof: order task executed state s, must exist path ancestors
task leading root graph ancestor tasks
terminated. condition lemma guarantees false, hence task
cannot executed state s. Therefore, C values need represented. Q.E.D.
Termination condition, Shielding condition verified analyzing
structure MAXQ graph identifying nodes whose ancestor tasks terminated.
Taxi domain, simple example arises Put task, terminated
states passenger taxi. means need
represent C (Root; s; Put) states. result that, combined
Termination condition above, need explicitly represent completion function
Put all!
5.1.6 Dicussion

applying five abstraction conditions, obtain following \safe" state abstractions Taxi task:
North, South, East, West. terminal nodes require one quantity each,
total four values. (Leaf Irrelevance).
269

fiDietterich

Pickup Putdown require 2 values (legal illegal states), total four.
(Leaf Irrelevance.)

QNorth(t), QSouth(t), QEast(t), QWest(t) require 100 values (four values
25 locations). (Max Node Irrelevance.)

QNavigateForGet requires 4 values (for four possible source locations). (The passenger destination Max Node Irrelevant MaxGet, taxi starting location
Result Distribution Irrelevant Navigate action.)

QPickup requires 100 possible values, 4 possible source locations 25 possible taxi
locations. (Passenger destination Max Node Irrelevant MaxGet.)

QGet requires 16 possible values (4 source locations, 4 destination locations). (Result
Distribution Irrelevance.)

QNavigateForPut requires 4 values (for four possible destination locations).

(The passenger source destination Max Node Irrelevant MaxPut; taxi
location Result Distribution Irrelevant Navigate action.)

QPutdown requires 100 possible values (25 taxi locations, 4 possible destination locations). (Passenger source Max Node Irrelevant MaxPut.)

QPut requires 0 values. (Termination Shielding.)
gives total 632 distinct values, much less 3000 values required
Q learning. Hence, see applying state abstractions, MAXQ
representation give much compact representation value function.
key thing note state abstractions, value function decomposed sum terms single term depends entire state MDP,
even though value function whole depend entire state MDP.
example, consider state described Figures 1 4. There, showed
value state s1 passenger R, destination B, taxi (0,3)
decomposed

V (Root; s1 ) = V (North; s1 ) + C (Navigate(R); s1 ; North) +
C (Get; s1 ; Navigate(R)) + C (Root; s1 ; Get)
state abstractions, see term right-hand side depends
subset features:

V (North; s1) constant
C (Navigate(R); s1 ; North) depends taxi location passenger's source
location.

C (Get; s1; Navigate(R)) depends source location.
C (Root; s1 ; Get) depends passenger's source destination.
270

fiMAXQ Hierarchical Reinforcement Learning

Without MAXQ decomposition, features irrelevant, value function depends entire state.
prior knowledge required part programmer order identify
state abstractions? suces know qualitative constraints one-step
reward functions, one-step transition probabilities, termination predicates, goal
predicates, pseudo-reward functions within MAXQ graph. Specifically, Max
Node Irrelevance Leaf Irrelevance conditions require simple analysis one-step
transition function reward pseudo-reward functions. Opportunities apply
Result Distribution Irrelevance condition found identifying \funnel" effects
result definitions termination conditions operators. Similarly,
Shielding Termination conditions require analysis termination predicates
various subtasks. Hence, applying five conditions introduce state abstractions
straightforward process, model one-step transition reward functions
learned, abstraction conditions checked see satisfied.

5.2 Convergence MAXQ-Q State Abstraction

shown state abstractions safely introduced MAXQ value
function decomposition five conditions described above. However, conditions guarantee value function fixed abstract hierarchical policy
represented|they show recursively optimal policies represented,
show MAXQ-Q learning algorithm find recursively optimal policy
forced use state abstractions. goal section prove two
results: (a) ordered recursively-optimal policy abstract policy (and, hence,
represented using state abstractions) (b) MAXQ-Q converge
policy applied MAXQ graph safe state abstractions.

Lemma 10 Let MDP full-state MAXQ graph H abstract-state MAXQ
graph (H ) abstractions satisfy five conditions given above. Let !
ordering actions MAXQ graph. following statements true:
unique ordered recursively-optimal policy r defined , H , ! abstract policy (i.e., depends relevant state variables node; see
Definition 11),
C V functions (H ) represent projected value function r.

Proof: five abstraction lemmas tell us ordered recursively-optimal policy
abstract, C V functions (H ) represent value function. Hence,
heart lemma first claim. last two forms abstraction (Shielding
Termination) place restrictions abstract policies, ignore
proof.
proof induction levels MAXQ graph, starting leaves.
base case, let us consider Max node whose children primitive actions.
case, policies executed within children Max node. Hence variables
Yi irrelevant node i, apply abstraction lemmas represent
value function policy node i|not abstract policies. Consequently, value
271

fiDietterich

function optimal policy node represented, property

Q(i; s1 ; a) = Q (i; s2 ; a)
(22)
states s1 s2 (s1 ) = (s2 ).
let us impose action ordering ! compute optimal ordered policy. Consider
two actions a1 a2 !(a1 ; a2 ) (i.e., ! prefers a1 ), suppose
\tie" Q function state s1 values

Q (i; s1 ; a1 ) = Q (i; s1 ; a2 )
two actions maximize Q state. optimal ordered
policy must choose a1 . states s2 (s1 ) = (s2 ),
established (22) Q values same. Hence, tie exist
a1 a2 , hence, optimal ordered policy must make choice
states. Hence, optimal ordered policy node abstract policy.
let us turn recursive case Max node i. Make inductive assumption
ordered recursively-optimal policy abstract within descendent nodes consider
locally optimal policy node i. set state variables irrelevant
node i, Corollary 2 tells us Q (i; s1 ; j ) = Q (i; s2 ; j ) states s1 s2
i(s1) = (s2 ). Similarly, set variables irrelevant result distribution
particular action j , Lemma 7 tells us thing. Hence, ordering
argument given above, ordered optimal policy node must abstract. induction,
proves lemma. Q.E.D.
lemma, established combination MDP , abstract
MAXQ graph H , action ordering defines unique recursively-optimal ordered abstract policy. ready prove MAXQ-Q converge policy.

Theorem 4 Let = hS; A; P; R; P0 either episodic MDP deterministic

policies proper discounted infinite horizon MDP discount factor < 1. Let H
unabstracted MAXQ graph defined subtasks fM0 ; : : : ; Mk g pseudo-reward
functions R~ (s0 ). Let (H ) state-abstracted MAXQ graph defined applying state
abstractions node H five conditions given above. Let x (i; (s))
abstract ordered GLIE exploration policy node state whose decisions
depend \relevant" state variables node i. Let r unique recursivelyoptimal hierarchical policy defined x , , R~ . probability 1, algorithm
MAXQ-Q applied (H ) converges r provided learning rates fft (i) satisfy
Equation (15) one-step rewards bounded.

Proof: Rather repeating entire proof MAXQ-Q, describe

must change state abstraction. last two forms state abstraction refer states
whose values inferred structure MAXQ graph, therefore
need represented all. Since values updated MAXQ-Q,
ignore them. consider first three forms state abstraction turn.
begin considering primitive leaf nodes. Let leaf node let set
state variables Leaf Irrelevant a. Let s1 = (x; y1 ) s2 = (x; y2 ) two states
272

fiMAXQ Hierarchical Reinforcement Learning

differ values . Leaf Irrelevance, probability transitions
P (s01 js1 ; a) P (s02 js2 ; a) need same, expected reward performing
states must same. MAXQ-Q visits abstract state x,
\know" value y, part state abstracted away. Nonetheless,
draws sample according P (s0 jx; y; a), receives reward R(s0 jx; y; a), updates
estimate V (a; x) (line 4 MAXQ-Q). Let Pt (y) probability MAXQ-Q
visiting (x; y) given unabstracted part state x. Line 4 MAXQ-Q
computing stochastic approximation
X

s0 ;N;y

write

X



Pt (y)Pt (s0 ; N jx; y; a)R(s0 jx; y; a):

Pt (y)

X

s0 ;N

Pt (s0 ; N jx; y; a)R(s0 jx; y; a):

According Leaf Irrelevance, inner sum value states
(s) = x. Call value r0 (x). gives
X



Pt (y)r0 (x);

equal r0 (x) distribution Pt (y). Hence, MAXQ-Q converges Leaf
Irrelevance abstractions.
let us turn two forms abstraction apply internal nodes: Max Node
Irrelevance Result Distribution Irrelevance. Consider SMDP defined node
abstracted MAXQ graph time MAXQ-Q. would ordinary SMDP
transition probability function Pt (x0 ; N jx; a) reward function Vt (a; x) + R~ (x0 )
except MAXQ-Q draws samples state transitions, drawn according
distribution Pt (s0 ; N js; a) original state space. prove theorem, must
show drawing (s0 ; N ) according second distribution equivalent drawing
(x0 ; N ) according first distribution.
Max Node Irrelevance, know abstract policies applied node
descendents, transition probability distribution factors

P (s0 ; N js; a) = P (y0 jx; y; a)P (x0 ; N jx; a):
exploration policy abstract policy, Pt (s0 ; N js; a) factors way.
means Yi components state cannot affect Xi components, hence,
sampling Pt (s0 ; N js; a) discarding Yi values gives samples Pt (x0 ; N jx; a).
Therefore, MAXQ-Q converge Max Node Irrelevance abstractions.
Finally, consider Result Distribution Irrelevance. Let j child node i, suppose
Yj set state variables irrelevant result distribution j .
SMDP node wishes draw sample Pt (x0 ; N jx; j ), \know"
current value y, irrelevant part current state. However,
matter, Result Distribution Irrelevance means possible values y,
Pt (x0 ; y0; N jx; y; j ) same. Hence, MAXQ-Q converge Result Distribution
Irrelevance abstractions.
273

fiDietterich

three cases, MAXQ-Q converge locally-optimal ordered policy
node MAXQ graph. Lemma 10, produces locally-optimal ordered
policy unabstracted SMDP node i. Hence, induction, MAXQ-Q converge
unique ordered recursively optimal policy r defined MAXQ-Q H , MDP ,
ordered exploration policy x . Q.E.D.

5.3 Hierarchical Credit Assignment Problem

still situations would introduce state abstractions
five properties described permit them. Consider following
modification taxi problem. Suppose taxi fuel tank time
taxi moves one square, costs one unit fuel. taxi runs fuel
delivering passenger destination, receives reward ,20, trial
ends. Fortunately, filling station taxi execute Fillup action fill
fuel tank.
solve modified problem using MAXQ hierarchy, introduce another
subtask, Refuel, goal moving taxi filling station filling
tank. MaxRefuel child MaxRoot, invokes Navigate(t) (with bound
location filling station) move taxi filling station.
introduction fuel possibility might run fuel means
must include current amount fuel feature representing every C value
(for internal nodes) V value (for leaf nodes) throughout MAXQ graph.
unfortunate, intuition tells us amount fuel uence
decisions inside Navigate(t) subtask. is, either taxi enough
fuel reach target (in case, chosen navigation actions depend
fuel), else taxi enough fuel, hence, fail reach regardless
navigation actions taken. words, Navigate(t) subtask
need worry amount fuel, even enough fuel,
action Navigate(t) take get fuel. Instead, top-level subtasks
monitoring amount fuel deciding whether go refuel, go pick
passenger, go deliver passenger.
Given intuition, natural try abstracting away \amount remaining
fuel" within Navigate(t) subtask. However, doesn't work, taxi
runs fuel ,20 reward given, QNorth, QSouth, QEast, QWest nodes
cannot \explain" reward received|that is, consistent way
setting C tables predict negative reward occur, C
values ignore amount fuel tank. Stated formally, diculty
Max Node Irrelevance condition satisfied one-step reward function
R(s0js; a) actions depends amount fuel.
call hierarchical credit assignment problem. fundamental issue
MAXQ decomposition information rewards stored leaf nodes
hierarchy. would separate basic rewards received navigation
(i.e., ,1 action) reward received exhausting fuel (,20). make
reward leaves depend location taxi, Max Node Irrelevance
condition satisfied.
274

fiMAXQ Hierarchical Reinforcement Learning

One way programmer manually decompose reward function

indicate nodes hierarchy \receive" reward. Let R(s0 js; a) =
P
0
0
R(i; js; a) decomposition reward function, R(i; js; a) specifies
part reward must handled Max node i. modified taxi problem,
example, decompose reward leaf nodes receive original
penalties, out-of-fuel rewards must handled MaxRoot. Lines 15 16
MAXQ-Q algorithm easily modified include R(i; s0 js; a).
domains, believe easy designer hierarchy decompose
reward function. straightforward problems studied.
However, interesting problem future research develop algorithm
solve hierarchical credit assignment problem autonomously.

6. Non-Hierarchical Execution MAXQ Hierarchy

point paper, focused exclusively representing learning
hierarchical policies. However, often optimal policy MDP strictly hierarchical. Kaelbling (1993) first introduced idea deriving non-hierarchical policy
value function hierarchical policy. section, exploit MAXQ decomposition
generalize ideas apply recursively levels hierarchy.
describe two methods non-hierarchical execution.
first method based dynamic programming algorithm known policy
iteration. policy iteration algorithm starts initial policy 0 . repeats
following two steps policy converges. policy evaluation step, computes
value function V k current policy k . Then, policy improvement step,
computes new policy, k+1 according rule

k+1(s) := argmax


X

s0

P (s0 js; a)[R(s0 js; a) + V k (s0 )]:

(23)

Howard (1960) proved k optimal policy, k+1 guaranteed
improvement. Note order apply method, need know transition
probability distribution P (s0 js; a) reward function R(s0 js; a).
know P (s0 js; a) R(s0 js; a), use MAXQ representation value
function perform one step policy iteration. start hierarchical policy
represent value function using MAXQ hierarchy (e.g., could learned via
MAXQ-Q). Then, perform one step policy improvement applying Equation (23)
using V (0; s0 ) (computed MAXQ hierarchy) compute V (s0 ).

Corollary 3 Let g (s) = argmaxa Ps0 P (s0js; a)[R(s0 js; a) + V (0; s)], V (0; s)
value function computed MAXQ hierarchy primitive action. Then,
optimal policy, g strictly better least one state .

Proof: direct consequence Howard's policy improvement theorem. Q.E.D.
Unfortunately, can't iterate policy improvement process, new policy,

g unlikely hierarchical policy (i.e., unlikely representable
275

fiDietterich

Table 5: procedure executing one-step greedy policy.
procedure ExecuteHGPolicy(s)
1
repeat
2
Let hV (0; s); ai := EvaluateMaxNode(0; s)
3
4

execute primitive action
Let resulting state
end // ExecuteHGPolicy

terms local policies node MAXQ graph). Nonetheless, one step policy
improvement give significant improvements.
approach non-hierarchical execution ignores internal structure MAXQ
graph. effect, MAXQ hierarchy viewed way represent V |any
representation would give one-step improved policy g .
second approach non-hierarchical execution borrows idea Q learning.
One great beauties Q representation value functions compute
one step policy improvement without knowing P (s0 js; a), simply taking new policy
g (s) := argmaxa Q(s; a). gives us one-step greedy policy
computed using one-step lookahead. MAXQ decomposition, perform
policy improvement steps levels hierarchy.
already defined function need. Table 3 presented function
EvaluateMaxNode, which, given current state s, conducts search along paths
given Max node leaves MAXQ graph finds path
best value (i.e., maximum sum C values along path, plus V value
leaf). equivalent computing best action greedily level
MAXQ graph. addition, EvaluateMaxNode returns primitive action end
best path. action would first primitive action executed
learned hierarchical policy executed starting current state s. second method
non-hierarchical execution MAXQ graph call EvaluateMaxNode
state, execute primitive action returned. pseudo-code shown
Table 5.
call policy computed ExecuteHGPolicy hierarchical greedy policy,
denote hg , superscript * indicates computing greedy
action time step. following theorem shows give better policy
original, hierarchical policy.

Theorem 5 Let G MAXQ graph representing value function hierarchical policy

(i.e., terms C (i; s; j ), computed i; s, j ). Let V hg (0; s) value
computed ExecuteHGPolicy (line 2), let hg resulting policy. Define
V hg value function hg. states s, case
V (s) V hg (0; s) V hg (s):
(24)

Proof: (sketch) left inequality Equation (24) satisfied construction line 6
EvaluateMaxNode. see this, consider original hierarchical policy, ,
276

fiMAXQ Hierarchical Reinforcement Learning

viewed choosing \path" MAXQ graph running root one
leaf nodes, V (0; s) sum C values along chosen path (plus
V value leaf node). contrast, EvaluateMaxNode performs traversal
paths MAXQ graph finds best path, is, path largest
sum C (and leaf V ) values. Hence, V hg (0; s) must least large V (0; s).
establish right inequality, note construction V hg (0; s) value function
policy, call hg , chooses one action greedily level MAXQ graph
(recursively), follows thereafter. consequence fact line
6 EvaluateMaxNode C right-hand side, C represents cost
\completing" subroutine following , following other, greedier, policy.
(In Table 3, C written Ct .) However, execute ExecuteHGPolicy (and
hence, execute hg ), opportunity improve upon hg time step.
Hence, V hg (0; s) underestimate actual value hg . Q.E.D.
Note theorem works one direction. says find state
V hg (0; s) > V (s), greedy policy, hg , strictly better .
However, could optimal policy yet structure MAXQ
graph prevents us considering action (either primitive composite) would
improve . Hence, unlike policy improvement theorem Howard (where primitive
actions always eligible chosen), guarantee suboptimal,
hierarchically greedy policy strict improvement.
contrast, perform one-step policy improvement discussed start
section, Corollary 3 guarantees improve policy. see
general, neither two methods non-hierarchical execution always better
other. Nonetheless, first method operates level individual primitive
actions, able produce large improvements policy. contrast,
hierarchical greedy method obtain large improvements policy changing
actions (i.e., subroutines) chosen near root hierarchy. Hence, general,
hierarchical greedy execution probably better method. (Of course, value functions
methods could computed, one better estimated value could
executed.)
Sutton, et al. (1999) simultaneously developed closely-related method nonhierarchical execution macros. method equivalent ExecuteHGPolicy
special case MAXQ hierarchy one level subtasks. interesting
aspect ExecuteHGPolicy permits greedy improvements levels
tree uence action chosen.
care must taken applying Theorem 5 MAXQ hierarchy whose C values
learned via MAXQ-Q. online algorithm, MAXQ-Q correctly learned values states nodes MAXQ graph. example,
taxi problem, value C (Put; s; QPutdown) learned well except
four special locations R, G, B, Y. Put subtask cannot
executed passenger taxi, usually means Get
completed, taxi passenger's source location. exploration, children Put tried states. PutDown usually fail (and receive negative
reward), whereas Navigate eventually succeed (perhaps lengthy exploration)
277

fiDietterich

take taxi destination location. all-states updating, values
C (Put; s; Navigate(t)) learned states along path
passenger's destination, C values Putdown action learned
passenger's source destination locations. Hence, train MAXQ representation using hierarchical execution (as MAXQ-Q), switch hierarchically-greedy
execution, results quite bad. particular, need introduce hierarchicallygreedy execution early enough exploration policy still actively exploring. (In
theory, GLIE exploration policy never ceases explore, practice, want find
good policy quickly, asymptotically).
course alternative would use hierarchically-greedy execution
beginning learning. However, remember higher nodes MAXQ hierarchy
need obtain samples P (s0 ; N js; a) child action a. hierarchical greedy
execution interrupts child reached terminal state (i.e., state
along way, another subtask appears better EvaluateMaxNode), samples
cannot obtained. Hence, important begin purely hierarchical execution
training, make transition greedy execution point.
approach taken implement MAXQ-Q way
specify number primitive actions L taken hierarchically hierarchical execution \interrupted" control returns top level (where new action
chosen greedily). start L set large, execution completely
hierarchical|when child action invoked, committed execute action
terminates. However, gradually, reduce L becomes 1, point
hierarchical greedy execution. time reaches 1 time
Boltzmann exploration cools temperature 0.1 (which exploration effectively
halted). experimental results show, generally gives excellent results
little added exploration cost.

7. Experimental Evaluation MAXQ Method
performed series experiments MAXQ method three goals
mind: (a) understand expressive power value function decomposition, (b)
characterize behavior MAXQ-Q learning algorithm, (c) assess relative
importance temporal abstraction, state abstraction, non-hierarchical execution.
section, describe experiments present results.

7.1 Fickle Taxi Task
first experiments performed modified version taxi task. version
incorporates two changes task described Section 3.1. First, four
navigation actions noisy, probability 0.8 moves intended direction,
probability 0.1 instead moves right (of intended direction)
probability 0.1 moves left. purpose change create realistic
dicult challenge learning algorithms. second change
taxi picked passenger moved one square away passenger's source
location, passenger changes destination location probability 0.3.
278

fiMAXQ Hierarchical Reinforcement Learning

purpose change create situation optimal policy hierarchical
policy effectiveness non-hierarchical execution measured.
compared four different configurations learning algorithm: (a) Q learning,
(b) MAXQ-Q learning without form state abstraction, (c) MAXQ-Q learning
state abstraction, (d) MAXQ-Q learning state abstraction greedy execution.
configurations controlled many parameters. include following: (a)
initial values Q C functions, (b) learning rate (we employed fixed
learning rate), (c) cooling schedule Boltzmann exploration (the GLIE policy
employed), (d) non-hierarchical execution, schedule decreasing L, number
steps consecutive hierarchical execution. optimized settings separately
configuration goal matching exceeding (with primitive training
actions possible) best policy could code hand. Boltzmann exploration,
established initial temperature cooling rate. separate temperature
maintained Max node MAXQ graph, temperature reduced
multiplying cooling rate time subtask terminates goal state.
process optimizing parameter settings algorithm time-consuming,
Q learning MAXQ-Q. critical parameter schedule
cooling temperature Boltzmann exploration: cooled rapidly,
algorithms converge suboptimal policy. case, tested nine different
cooling rates. choose different cooling rates various subtasks, started
using fixed policies (e.g., either random hand-coded) subtasks except subtasks
closest leaves. Then, chosen schedules subtasks, allowed
parent tasks learn policies tuned cooling rates, on. One
nice effect method cooling temperature subtask terminates
naturally causes subtasks higher MAXQ graph cool slowly. meant
good results could often obtained using cooling rate Max
nodes.
choice learning rate easier, since determined primarily degree
stochasticity environment. tested three four different rates
configuration. initial values Q C functions set based knowledge
problems|no experiments required.
took care tuning parameters experiments one would
normally take real application, wanted ensure method
compared best possible conditions. general form results (particularly
speed learning) wide ranges cooling rate learning rate
parameter settings.
following parameters selected based tuning experiments. Q
learning: initial Q values 0.123 states, learning rate 0.25, Boltzmann exploration
initial temperature 50 cooling rate 0.9879. (We use initial values
end .123 \signature" debugging detect weight modified.)
MAXQ-Q learning without state abstraction, used initial values 0.123, learning rate 0.50, Boltzmann exploration initial temperature 50 cooling
rates 0.9996 MaxRoot MaxPut, 0.9939 MaxGet, 0.9879 MaxNavigate.
279

fiDietterich

200
MAXQ Abstract

Mean Cumulative Reward

0
MAXQ
Abstract+
Greedy

-200

MAXQ
Abstract

-400

Flat Q

-600

-800

-1000
0

20000

40000

60000
80000
100000
Primitive Actions

120000

140000

Figure 8: Comparison performance hierarchical MAXQ-Q learning (without state abstractions, state abstractions, state abstractions combined
hierarchical greedy evaluation) Q learning.
MAXQ-Q learning state abstraction, used initial values 0.123, learning
rate 0.25, Boltzmann exploration initial temperature 50 cooling rates
0.9074 MaxRoot, 0.9526 MaxPut, 0.9526 MaxGet, 0.9879 MaxNavigate.
MAXQ-Q learning non-hierarchical execution, used settings
state abstraction. addition, initialized L 500 decreased 10
trial reached 1. 50 trials, execution completely greedy.
Figure 8 shows averaged results 100 training runs. training run involves
performing repeated trials convergence. different trials execute different
numbers primitive actions, plotted number primitive actions
horizontal axis rather number trials.
first thing note forms MAXQ learning better initial performance
Q learning. constraints introduced MAXQ hierarchy.
example, agent executing Navigate subtask, never attempt pickup
putdown passenger, actions available Navigate. Similarly,
agent never attempt putdown passenger first picked passenger
(and vice versa) termination conditions Get Put subtasks.
second thing notice without state abstractions, MAXQ-Q learning actually takes longer converge, Flat Q curve crosses MAXQ/no abstraction
280

fiMAXQ Hierarchical Reinforcement Learning

curve. shows without state abstraction, cost learning huge number
parameters MAXQ representation really worth benefits. suspect
consequence model-free nature MAXQ-Q algorithm. MAXQ decomposition represents information redundantly. example, cost performing
Put subtask computed C (Root; s; Get) V (Put; s). model-based
algorithm could compute learned model, MAXQ-Q must learn
separately experience.
third thing notice state abstractions, MAXQ-Q converges
quickly hierarchically optimal policy. seen clearly Figure 9,
focuses range reward values neighborhood optimal policy.
see MAXQ abstractions attains hierarchically optimal policy
approximately 40,000 steps, whereas Q learning requires roughly twice long reach
level. However, Q learning, course, continue onward reach optimal
performance, whereas MAXQ hierarchy, best hierarchical policy slow
respond \fickle" behavior passenger he/she changes destination.
last thing notice greedy execution, MAXQ policy able
attain optimal performance. execution becomes \more greedy",
temporary drop performance, MAXQ-Q must learn C values new regions
state space visited recursively optimal policy. Despite
drop performance, greedy MAXQ-Q recovers rapidly reaches hierarchically optimal
performance faster purely-hierarchical MAXQ-Q learning. Hence, added
cost|in terms exploration|for introducing greedy execution.
experiment presents evidence favor three claims: first, hierarchical reinforcement learning much faster Q learning; second, state abstraction
required MAXQ-Q learning good performance; third, non-hierarchical
execution produce significant improvements performance little added
exploration cost.

7.2 Kaelbling's HDG Method
second task consider simple maze task introduced Leslie Kaelbling
(1993) shown Figure 11. trial task, agent starts randomlychosen state must move randomly-chosen goal state using usual North, South,
East, West operators (we employed deterministic operators). small cost
move, agent must minimize undiscounted sum costs.
goal state 100 different locations, actually 100
different MDPs. Kaelbling's HDG method starts choosing arbitrary set landmark
states defining Voronoi partition state space based Manhattan distances
landmarks (i.e., two states belong Voronoi cell iff
nearest landmark). method defines one subtask landmark l. subtask
move state current Voronoi cell neighboring Voronoi cell
landmark l. Optimal policies subtasks computed.
HDG policies subtasks, solve abstract Markov Decision
Problem moving landmark state landmark state using subtask
solutions macro actions (subroutines). computes value function MDP.
281

fiDietterich

10

MAXQ Abstract+Greedy

Mean Cumulative Reward

5

Optimal Policy
Flat Q
Hier-Optimal Policy

0
MAXQ Abstract

MAXQ Abstract

-5

-10

-15
0

50000

100000

150000
200000
Primitive Actions

250000

300000

Figure 9: Close-up view previous figure. figure shows two horizontal lines
indicating optimal performance hierarchically optimal performance
domain. make figure readable, applied 100-step moving
average data points (which average 100 runs).
Finally, possible destination location g within Voronoi cell landmark l,
HDG method computes optimal policy getting l g.
combining subtasks, HDG method construct good approximation
optimal policy follows. addition value functions discussed above,
agent maintains two functions: NL(s), name landmark nearest state s,
N (l), list landmarks cells immediate neighbors cell l.
combining these, agent build list state current landmark
landmarks neighboring cells. landmark, agent computes sum
three terms:
(t1) expected cost reaching landmark,
(t2) expected cost moving landmark landmark goal cell,
(t3) expected cost moving goal-cell landmark goal state.
Note terms (t1) (t3) exact estimates, term (t2) computed using
landmark subtasks subroutines. means corresponding path must pass
intermediate landmark states rather going directly goal landmark.
282

fiMAXQ Hierarchical Reinforcement Learning

10
9
8
7
6
5
4
3
2
1
1

2

3

4

5

6

7

8

9

10

Figure 10: Kaelbling's 10-by-10 navigation task. circled state landmark state,
heavy lines show boundaries Voronoi cells. episode,
start state goal state chosen random. figure, start state
shown black square, goal state shown black hexagon.

Hence, term (t2) typically overestimate required distance. (Also note (t3)
choices intermediate landmarks, need explicitly
included computation best action agent enters cell containing
goal.)
Given information, agent chooses move toward best landmarks
(unless agent already goal Voronoi cell, case agent moves toward
goal state). example, Figure 10, term (t1) cost reaching landmark
row 6, column 6, 4. Term (t2) cost getting row 6, column 6
landmark row 1 column 4 (by going one landmark another). case,
best landmark-to-landmark path go directly row 6 column 6 row 1 column
4. Hence, term (t2) 6. Term (t3) cost getting row 1 column 4 goal,
1. sum 4 + 6 + 1 = 11. comparison, optimal path
length 9.
Kaelbling's experiments, employed variation Q learning learn terms (t1)
(t3), computed (t2) regular intervals via Floyd-Warshall all-sources
shortest paths algorithm.
Figure 11 shows MAXQ approach solving problem. overall task Root,
takes one argument g, specifies goal cell. three subtasks:
283

fiDietterich

MaxRoot(g)
gl/NL(g)

QGotoGoalLmk(gl)

QGotoGoal(g)

MaxGotoGoalLmk(gl)

QGotoLmk(l,gl)

MaxGotoLmk(l)

QNorthLmk(l)

QSouthLmk(l)

MaxGotoGoal(g)

QEastLmk(l)

North

QWestLmk(l)

South

QNorthG(g)

East

QSouthG(g)

QEastG(g)

QWestG(g)

West

Figure 11: MAXQ graph HDG navigation task.

GotoGoalLmk, go landmark nearest goal location. termination
predicate subtask true agent reaches landmark nearest
goal. goal predicate termination predicate.

GotoLmk(l), go landmark l. termination predicate true either (a)

agent reaches landmark l (b) agent outside region defined
Voronoi cell l neighboring Voronoi cells, N (l). goal predicate
subtask true condition (a).

GotoGoal(g), go goal location g. termination predicate subtask

true either agent goal location agent outside Voronoi
cell NL(g) contains g. goal predicate subtask true agent
goal location.
284

fiMAXQ Hierarchical Reinforcement Learning

MAXQ decomposition essentially Kaelbling's method, somewhat
redundant. Consider state agent inside Voronoi cell goal
g. states, HDG decomposes value function three terms (t1), (t2), (t3).
Similarly, MAXQ decomposes three terms:
V (GotoLmk(l); s; a) cost getting landmark l. represented sum
V (a; s) C (GotoLmk(l); s; a).
C (GotoGoalLmk(gl); s; MaxGotoLmk(l)) cost getting landmark l
landmark gl nearest goal.
C (Root; s; GotoGoalLmk(gl)) cost getting goal location reaching gl
(i.e., cost completing Root task reaching gl).
agent inside goal Voronoi cell, HDG MAXQ store
essentially information. HDG stores Q(GotoGoal(g); s; a), MAXQ breaks
two terms: C (GotoGoal(g); s; a) V (a; s) sums two quantities
compute Q value.
Note MAXQ decomposition stores information twice|specifically,
cost getting goal landmark gl goal stored C (Root; s; GotoGoalLmk(gl))
C (GotoGoal(g); s; a) + V (a; s).
Let us compare amount memory required Q learning, HDG, MAXQ.
100 locations, 4 possible actions, 100 possible goal states, Q learning
must store 40,000 values.
compute quantity (t1), HDG must store 4 Q values (for four actions)
state respect landmark landmarks N (NL(s)). gives
total 2,028 values must stored.
compute quantity (t2), HDG must store, landmark, information
shortest path every landmark. 12 landmarks. Consider landmark
row 6, column 1. 5 neighboring landmarks constitute five macro actions
agent perform move another landmark. nearest landmark
goal cell could 11 landmarks, gives total 55 Q values
must stored. Similar computations 12 landmarks give total 506 values
must stored.
Finally, compute quantity (t3), HDG must store information, square inside
Voronoi cell, get squares inside Voronoi
cell. requires 3,536 values.
Hence, grand total HDG 6,070, huge savings Q learning.
let's consider MAXQ hierarchy without state abstractions.
V (a; s): expected reward primitive action state.
100 states 4 primitive actions, requires 400 values. However,
reward constant (,1), apply Leaf Irrelevance store single value.
C (GotoLmk(l); s; a), one four primitive actions. requires
amount space (t1) Kaelbling's representation|indeed, combined
V (a; s), represents exactly information (t1). requires 2,028 values.
state abstractions applied.
285

fiDietterich

C (GotoGoalLmk(gl); s; GotoLmk(l)): cost completing GotoGoalLmk

task going landmark l. primitive actions deterministic,
GotoLmk(l) always terminate location l, hence, need store
pair l gl. exactly Kaelbling's quantity (t2),
requires 506 values. However, primitive actions stochastic|as
Kaelbling's original paper|then must store value possible
terminal state GotoLmk action. actions could terminate
target landmark l one states bordering set Voronoi cells
neighbors cell l. requires 6,600 values. Kaelbling stores
values (t2), effectively making assumption GotoLmk(l)
never fail reach landmark l. approximation introduce
MAXQ representation choice state abstraction node.

C (GotoGoal; s; a): cost completing GotoGoal task executing one
primitive actions a. quantity (t3) HDG representation,
requires amount space: 3,536 values.

C (Root; s; GotoGoalLmk): cost reaching goal reached

landmark nearest goal. MAXQ must represent combinations
goal landmarks goals. requires 100 values. Note values
values C (GotoGoal(g); s; a) + V (a; s) primitive actions.
means MAXQ representation stores information twice, whereas
HDG representation stores (as term (t3)).

C (Root; s; GotoGoal). cost completing Root task exe-

cuted GotoGoal task. primitive action deterministic, always zero,
GotoGoal reached goal. Hence, apply Termination
condition store values all. However, primitive actions stochastic, must store value possible state borders Voronoi cell
contains goal. requires 96 different values. Again, Kaelbling's HDG
representation value function, ignoring probability GotoGoal
terminate non-goal state. MAXQ exact representation value
function, ignore possibility. (incorrectly) apply Termination
condition case, MAXQ representation becomes function approximation.

stochastic case, without state abstractions, MAXQ representation requires
12,760 values. safe state abstractions, requires 12,361 values. approximations employed Kaelbling (or equivalently, primitive actions deterministic),
MAXQ representation state abstractions requires 6,171 values. numbers
summarized Table 6. see that, unsafe state abstractions, MAXQ
representation requires slightly space HDG representation (because
redundancy storing C (Root; s; GotoGoalLmk).
example shows HDG task, start fully-general formulation provided MAXQ impose assumptions obtain method similar
HDG. MAXQ formulation guarantees value function hierarchical
policy represented exactly. assumptions introduce approximations
286

fiMAXQ Hierarchical Reinforcement Learning

Table 6: Comparison number values must stored represent value
function using HDG MAXQ methods.
HDG MAXQ
HDG MAXQ MAXQ MAXQ
item item
values abs safe abs unsafe abs
V (a; s)
0
400
1
1
(t1) C (GotoLmk(l); s; a)
2,028 2,028
2,028
2,028
(t2) C (GotoGoalLmk; s; GotoLmk(l))
506 6,600
6,600
506
(t3) C (GotoGoal(g); s; a)
3,536 3,536
3,536
3,536
C (Root; s; GotoGoalLmk)
0
100
100
100
C (Root; s; GotoGoal)
0
96
96
0
Total Number Values Required
6,070 12,760 12,361
6,171
value function representation. might useful general design methodology
building application-specific hierarchical representations. long-term goal develop
methods new application require inventing new set techniques. Instead, off-the-shelf tools (e.g., based MAXQ) could specialized imposing
assumptions state abstractions produce ecient special-purpose systems.
One important contributions HDG method introduced
form non-hierarchical execution. soon agent crosses one Voronoi cell
another, current subtask reaching landmark cell \interrupted",
agent recomputes \current target landmark". effect (until
reaches goal Voronoi cell), agent always aiming landmark outside
current Voronoi cell. Hence, although agent \aims for" sequence landmark states,
typically visit many states way goal. states provide
convenient set intermediate targets. taking \shortcuts", HDG compensates
fact that, general, overestimated cost getting goal,
computed value function based policy agent goes one landmark
another.
effect obtained hierarchical greedy execution MAXQ graph (which
directly inspired HDG method). Note storing NL (nearest landmark)
function, Kaelbing's HDG method detect eciently current subtask
interrupted. technique works navigation problems space
distance metric. contrast, ExecuteHGPolicy performs kind \polling",
checks primitive action whether interrupt current subroutine
invoke new one. important goal future research MAXQ find general
purpose mechanism avoiding unnecessary \polling"|that is, mechanism
discover eciently-evaluable interrupt conditions.
Figure 12 shows results experiments HDG using MAXQ-Q learning algorithm. employed following parameters: Flat Q learning, initial values
0.123, learning rate 1.0, initial temperature 50, cooling rate 0.9074;
MAXQ-Q without state abstractions: initial values ,25:123, learning rate 1.0, initial
287

fiDietterich

0
Flat Q

MAXQ +
Abstract

-20

Mean Cumulative Reward

-40

MAXQ Abstract

-60
-80
-100
-120
-140
0

200000

400000

600000
800000
Primitive Actions

1e+06

1.2e+06

1.4e+06

Figure 12: Comparison Flat Q learning MAXQ-Q learning without state
abstraction. (Average 100 runs.)
temperature 50, cooling rates 0.9074 MaxRoot, 0.9999 MaxGotoGoalLmk,
0.9074 MaxGotoGoal, 0.9526 MaxGotoLmk; MAXQ-Q state abstractions:
initial values ,20:123, learning rate 1.0, initial temperature 50, cooling rates
0.9760 MaxRoot, 0.9969 MaxGotoGoal, 0.9984 MaxGotoGoalLmk, 0.9969
MaxGotoLmk. Hierarchical greedy execution introduced starting 3000 primitive actions per trial, reducing every trial 2 actions, 1500 trials,
execution completely greedy.
figure confirms observations made experiments Fickle Taxi task.
Without state abstractions, MAXQ-Q converges much slowly Q learning.
state abstractions, converges roughly three times fast. Figure 13 shows close-up
view Figure 12 allows us compare differences final levels performance
methods. Here, see MAXQ-Q state abstractions able
reach quality hand-coded hierarchical policy|presumably even exploration
would required achieve this, whereas state abstractions, MAXQ-Q able
slightly better hand-coded policy. hierarchical greedy execution, MAXQ-Q
able reach goal using one fewer action, average|so approaches
performance best hierarchical greedy policy (as computed value iteration). Notice
however, best performance obtained hierarchical greedy execution
best recursively-optimal policy cannot match optimal performance. Hence, Flat Q
288

fiMAXQ Hierarchical Reinforcement Learning

-6
Optimal Policy

Mean Cumulative Reward

Hierarchical Greedy Optimal Policy
MAXQ Abstract + Greedy
MAXQ + Abstract

-8

-10

Flat Q

Hierarchical Hand-coded Policy

MAXQ Abstract

-12

-14

0

200000

400000

600000
800000
Primitive Actions

1e+06

1.2e+06

1.4e+06

Figure 13: Expanded view comparing Flat Q learning MAXQ-Q learning
without state abstraction without hierarchical greedy execution.
(Average 100 runs.)
learning achieves policy reaches goal state, average, one fewer
primitive action. Finally notice taxi domain, added exploration
cost shifting greedy execution.
Kaelbling's HDG work recently extended generalized Moore, Baird
Kaelbling (1999) sparse MDP overall task get given
start state desired goal state. key success approach
landmark subtask guaranteed terminate single resulting state. makes
possible identify sequence good intermediate landmark states assemble
policy visits sequence. Moore, Baird Kaelbling show construct
hierarchy landmarks (the \airport" hierarchy) makes planning process ecient.
Note subtask terminate single state (as general MDPs),
airport method would work, would combinatorial explosion
potential intermediate states would need considered.

7.3 Parr Russell: Hierarchies Abstract Machines

(1998b) dissertation work, Ron Parr considered approach hierarchical reinforcement learning programmer encodes prior knowledge form hierarchy
finite-state controllers called HAM (Hierarchy Abstract Machines). hierarchy
289

fiDietterich

Intersection
Vertical Hallway
Horizontal Hallway
Goal

Figure 14: Parr's maze problem (on left). start state upper left corner,
states lower right-hand room terminal states. smaller diagram
right shows hallway intersection structure maze.
executed using procedure-call-and-return discipline, provides partial policy
task. policy partial machine include non-deterministic \choice"
machine states, machine lists several options action specify
one chosen. programmer puts \choice" states point
he/she know action performed. Given partial policy, Parr's
goal find best policy making choices choice states. words,
goal learn hierarchical value function V (hs; mi), state (of external
environment) contains internal state hierarchy (i.e., contents
procedure call stack values current machine states machines
appearing stack). key observation necessary learn value
function choice states hs; mi. Parr's algorithm learn decomposition value
function. Instead, \ attens" hierarchy create new Markov decision problem
choice states hs; mi. Hence, hierarchical primarily sense programmer
structures prior knowledge hierarchically. advantage Parr's method
find optimal hierarchical policy subject constraints provided programmer.
disadvantage method cannot executed \non-hierarchically" produce
better policy.
Parr illustrated work using maze shown Figure 14. maze large-scale
structure (as series hallways intersections), small-scale structure (a series
obstacles must avoided order move hallways intersections).
290

fiMAXQ Hierarchical Reinforcement Learning

trial, agent starts top left corner, must move state
bottom right corner room. agent usual four primitive actions, North, South,
East, West. actions stochastic: probability 0.8, succeed,
probability 0.1 action move \left" probability 0.1 action
move \right" instead (e.g., North action move east probability 0.1
west probability 0.1). action would collide wall obstacle,
effect.
maze structured series \rooms", containing 12-by-12 block states
(and various obstacles). rooms parts \hallways", connected
two rooms opposite sides. rooms \intersections", two
hallways meet.
test representational power MAXQ hierarchy, want see well
represent prior knowledge Parr able represent using HAM. begin
describing Parr's HAM maze task, present MAXQ hierarchy
captures much prior knowledge.3
Parr's top level machine, MRoot, consists loop single choice state
chooses among four possible child machines: MGo(East), MGo(South), MGo(West),
MGo(North). loop terminates agent reaches goal state. MRoot
invoke particular machine hallway specified direction. Hence,
start state, consider MGo(South) MGo(East).
MGo(d) machine begins executing agent intersection. first
thing tries exit intersection hallway specified direction d.
attempts traverse hallway reaches another intersection. first
invoking MExitIntersection(d) machine. machine returns, invokes
MExitHallway(d) machine. machine returns, MGo returns.
MExitIntersection MExitHallway machines identical except termination conditions. machines consist loop one choice state chooses among
four possible subroutines. simplify description, suppose MGo(East) chosen MExitIntersection(East). four possible subroutines MSniff (East; North),
MSniff (East; South), MBack(East; North), MBack(East; South).
MSniff (d; p) machine always moves direction encounters wall (either
part obstacle part walls maze). moves perpendicular
direction p reaches end wall. wall \end" two ways: either
agent trapped corner walls directions p else
longer wall direction d. first case, MSniff machine terminates; second
case, resumes moving direction d.
MBack(d; p) machine moves one step backwards (in direction opposite d)
moves five steps direction p. moves may may succeed,
actions stochastic may walls blocking way. actions carried
case, MBack machine returns.
MSniff MBack machines terminate reach end hall
end intersection.
3. author thanks Ron Parr providing details HAM task.

291

fiDietterich

finite-state controllers define highly constrained partial policy. MBack,
MSniff, MGo machines contain choice states all. choice points
MRoot, must choose direction move, MExitIntersection
MExitHall, must decide call MSniff, call MBack,
\perpendicular" direction tell machines try cannot move forward.

MaxRoot

Go(d)
r/ROOM
MaxGo(d,r)

QExitInter(d,r)

QExitHall(d,r)

MaxExitInter(d,r)

MaxExitHall(d,r)

QSniffEI(d,p)

QBackEI(d,p)

QSniffEH(d,p)
x/X

QBackEH(d,p)
x/X
y/Y

y/Y
MaxSniff(d,p)

MaxBack(d,p,x,y)

QFollowWall(d,p)

QToWall(d)

QBackOne(d)

QPerpThree(p)

MaxFollowWall(d,p)

MaxToWall(d)

MaxBackOne(d)

MaxPerpThree(p)

d/p

d/d

QMoveFW(d)

d/Inv(d)

QMoveTW(d)

QMoveBO(d)

d/p
QMoveP3(d)

MaxMove(d)

Figure 15: MAXQ graph Parr's maze task.
Figure 15 shows MAXQ graph encodes similar set constraints policy.
subtasks defined follows:
292

fiMAXQ Hierarchical Reinforcement Learning

Root. exactly MRoot machine. must choose direction

invoke Go. terminates agent enters terminal state.
goal condition (of course).

Go(d; r). (Go direction leaving room r.) parameter r bound identi-

fication number corresponding current 12-by-12 \room" agent
located. Go terminates agent enters room end hallway
direction leaves desired hallway (e.g., wrong direction).
goal condition Go satisfied agent reaches desired intersection.

ExitInter(d; r). terminates agent exited room r. goal condition
agent exit room r direction d.

ExitHall(d; r). terminates agent exited current hall (into

intersection). goal condition agent entered desired intersection
direction d.

Sniff (d; r). encodes subtask equivalent MSniff machine. However,

Sniff must two child subtasks, ToWall FollowWall, simply internal
states MSniff. necessary, subtask MAXQ framework cannot

contain internal state, whereas finite-state controller HAM representation
contain many internal states necessary. particular, one state
moving forward another state following wall sideways.

ToWall(d). equivalent one part MSniff. terminates

wall \front" agent direction d. goal condition
termination condition.

FollowWall(d; p). equivalent part MSniff. moves direction

p wall direction ends (or stuck corner walls
directions p). goal condition termination condition.

Back(d; p; x; y). attempts encode information MBack machine,

case MAXQ hierarchy cannot capture information.

MBack simply executes sequence 6 primitive actions (one step back, five steps
direction p). this, MBack must 6 internal states, MAXQ
allow. Instead, Back subtask subgoal moving agent least

one square backwards least 3 squares direction p. order determine
whether achieved subgoal, must remember x position
started execute, bound parameters Back. Back terminates
achieves desired change position runs walls prevent
achieving subgoal. goal condition termination condition.

BackOne(d; x; y). moves agent one step backwards (in direction opposite

d. needs starting x position order tell succeeded.
terminates moved least one unit direction wall
direction. goal condition termination condition.
293

fiDietterich

PerpThree(p; x; y). moves agent three steps direction p. needs

starting x positions order tell succeeded. terminates
moved least three units direction p wall direction.
goal condition termination condition.

Move(d). \parameterized primitive" action. executes one primitive move
direction terminates immediately.

this, see three major differences MAXQ representation HAM representation. First, HAM finite-state controller contain
internal states. convert MAXQ subtask graph, must make separate
subtask internal state HAM. Second, HAM terminate based
\amount effort" (e.g., performing 5 actions), whereas MAXQ subtask must terminate
based change state world. impossible define MAXQ subtask performs k steps terminate regardless effects steps (i.e.,
without adding kind \counter" state MDP). Third, dicult
formulate termination conditions MAXQ subtasks HAM machines.
example, HAM, necessary specify MExitHallway machine terminates entered different intersection one MGo executed.
However, important MAXQ method, MAXQ, subtask learns
value function policy|independent parent tasks. example, without
requirement enter different intersection, learning algorithms MAXQ
always prefer MaxExitHall take one step backward return room
Go action started (because much easier terminal state reach).
problem arise HAM approach, policy learned subtask
depends whole \ attened" hierarchy machines, returning state
Go action started help solve overall problem reaching goal state
lower right corner.
construct MAXQ graph problem, introduced three programming
tricks: (a) binding parameters aspects current state (in order serve kind
\local memory" subtask began executing), (b) parameterized
primitive action (in order able pass parameter value specifies primitive
action perform), (c) employing \inheritance termination conditions"|that is,
subtask MAXQ graph (but others paper) inherits termination
conditions ancestor tasks. Hence, agent middle executing ToWall
action leaves intersection, ToWall subroutine terminates ExitInter
termination condition satisfied. behavior similar standard behavior
MAXQ. Ordinarily, ancestor task terminates, descendent tasks forced
return without updating C values. inheritance termination conditions,
hand, descendent tasks forced terminate, updating C
values. words, termination condition child task logical disjuntion
termination conditions ancestors (plus termination condition).
inheritance made easier write MAXQ graph, parents need
pass children information necessary children define
complete termination goal predicates.
294

fiMAXQ Hierarchical Reinforcement Learning

essentially opportunities state abstraction task,
irrelevant features state. opportunities apply Shielding
Termination properties, however. particular, ExitHall(d) guaranteed cause
parent task, MaxGo(d), terminate, require stored C values.
many states subtasks terminated (e.g., Go(East) state
wall east side room), C values need stored.
Nonetheless, even applying state elimination conditions, MAXQ representation task requires much space representation. exact
computation dicult, applying MAXQ-Q learning, MAXQ representation
required 52,043 values, whereas Q learning requires fewer 16,704 values. Parr
states method requires 4,300 values.
test relative effectiveness MAXQ representation, compare MAXQ-Q
learning Q learning. large negative values states
acquire (particularly early phases learning), unable get Boltzmann
exploration work well|one bad experience would cause action receive
low Q value, would never tried again. Hence, experimented
-greedy exploration counter-based exploration. -greedy exploration policy
ordered, abstract GLIE policy random action chosen probability ,
gradually decreased time. counter-based exploration policy keeps track
many times action executed state s. choose action state
s, selects action executed fewest times actions
executed times. switches greedy execution. Hence, genuine GLIE
policy. Parr employed counter-based exploration policies experiments task.
domains, conducted several experimental runs (e.g., testing Boltzmann, -greedy, counter-based exploration) determine best parameters
algorithm. Flat Q learning, chose following parameters: learning rate 0.50, greedy exploration initial value 1.0, decreased 0.001 successful
execution Max node, initial Q values ,200:123. MAXQ-Q learning, chose
following parameters: counter-based exploration = 10, learning rate equal
reciprocal number times action performed, initial values C
values selected carefully provide underestimates true C values. example,
initial values QExitInter ,40:123, worst case, completing
ExitInter task, takes 40 steps complete subsequent ExitHall task hence,
complete Go parent task. Performance quite sensitive initial C values,
potential drawback MAXQ approach.
Figure 16 plots results. see MAXQ-Q learning converges 10
times faster Flat Q learning. know whether MAXQ-Q converged
recursively optimal policy. comparison, show performance hierarchical
policy coded hand, hand-coded policy, used knowledge contextual
information choose operators, policy surely better best recursively
optimal policy. HAMQ learning converge policy equal slightly better
hand-coded policy.
experiment demonstrates MAXQ representation capture most|but
all|of prior knowledge represented HAMQ hierarchy.
295

fiDietterich

-100
-150

Mean Reward Per Trial

-200

Hand-coded hierarchical policy

-250
-300
-350

MAXQ Q Learning

Flat Q Learning

-400
-450
-500
0

1e+06

2e+06

3e+06
Primitive Steps

4e+06

5e+06

6e+06

Figure 16: Comparison Flat Q learning MAXQ-Q learning Parr maze task.
shows MAXQ representation requires much care design goal
conditions subtasks.

7.4 Domains
addition three domains discussed above, developed MAXQ graphs
Singh's (1992) \ ag task", treasure hunter task described Tadepalli Dietterich
(1997), Dayan Hinton's (1993) Feudal-Q learning task. tasks
easily naturally placed MAXQ framework|indeed, fit easily
Parr Russell maze task.
MAXQ able exactly duplicate Singh's work decomposition value
function|while using exactly amount space represent value function.
MAXQ duplicate results Tadepalli Dietterich|however,
MAXQ explanation-based method, considerably slower requires substantially space represent value function.
Feudal-Q task, MAXQ able give better performance Feudal-Q learning.
reason Feudal-Q learning, subroutine makes decisions using Q
function learned level hierarchy|that is, without information
estimated costs actions descendents. contrast, MAXQ value function
decomposition permits Max node make decisions based sum completion
function, C (i; s; j ), costs estimated descendents, V (j; s). course, MAXQ
296

fiMAXQ Hierarchical Reinforcement Learning

supports non-hierarchical execution, possible Feudal-Q,
learn value function decomposition.

8. Discussion

concluding paper, wish discuss two issues: (a) design tradeoffs hierarchical reinforcement learning (b) methods automatically learning (or least
improving) MAXQ hierarchies.

8.1 Design Tradeoffs Hierarchical Reinforcement Learning

introduction paper, discussed four issues concerning design hierarchical reinforcement learning architectures: (a) method defining subtasks, (b)
use state abstraction, (c) non-hierarchical execution, (d) design learning algorithms. subsection, want highlight tradeoff first two
issues.
MAXQ defines subtasks using termination predicate Ti pseudo-reward function
R~ . least two drawbacks method. First, hard programmer define Ti R~ correctly, since essentially requires guessing value function
optimal policy MDP states subtask terminates. Second,
leads us seek recursively optimal policy rather hierarchically optimal policy.
Recursively optimal policies may much worse hierarchically optimal ones,
may giving substantial performance.
However, return two drawbacks, MAXQ obtains important benefit:
policies value functions subtasks become context-free. words,
depend parent tasks larger context invoked.
understand point, consider MDP shown Figure 6. clear
optimal policy exiting left-hand room (the Exit subtask) depends location
goal. top right-hand room, agent prefer
exit via upper door, whereas bottom right-hand room, agent
prefer exit lower door. However, define subtask exiting
left-hand room using pseudo-reward zero doors, obtain policy
optimal either case, policy re-use cases. Furthermore,
policy depend location goal. Hence, apply Max node
irrelevance solve Exit subtask using location robot ignore
location goal.
example shows obtain benefits subtask reuse state abstraction define subtask using termination predicate pseudo-reward
function. termination predicate pseudo-reward function provide barrier
prevents \communication" value information Exit subtask context.
Compare Parr's HAM method. HAMQ algorithm finds best policy
consistent hierarchy. achieve this, must permit information propagate
\into" Exit subtask (i.e., Exit finite-state controller) environment.
means state reached leaving Exit subtask different
values depending location goal, different values propagate
back Exit subtask. represent different values, Exit subtask must know
297

fiDietterich

location goal. short, achieve hierarchically optimal policy within Exit
subtask, must (in general) represent value function using entire state space. State
abstractions cannot employed without losing hierarchical optimality.
see, therefore, direct tradeoff achieving hierarchical
optimality employing state abstractions. Methods hierarchical optimality
freedom defining subtasks (e.g., using partial policies, HAM approach).
cannot (safely) employ state abstractions within subtasks, general, cannot
reuse solution one subtask multiple contexts. Methods recursive optimality,
hand, must define subtasks using method (such pseudo-reward functions
MAXQ fixed policies options framework) isolates subtask
context. return, apply state abstraction learned policy
reused many contexts (where less optimal).
interesting iterative method described Dean Lin (1995)
viewed method moving along tradeoff. Dean Lin method,
programmer makes initial guess values terminal states subtask
(i.e., doorways Figure 6). Based initial guess, locally optimal policies
subtasks computed. locally optimal policy parent task
computed|while holding subtask policies fixed (i.e., treating options).
point, algorithm computed recursively optimal solution original
problem, given initial guesses. Instead solving various subproblems sequentially
via oine algorithm Dean Lin suggested, could use MAXQ-Q learning
algorithm.
method Dean Lin stop here. Instead, computes new values
terminal states subtask based learned value function entire
problem. allows update \guesses" values terminal states.
entire solution process repeated obtain new recursively optimal solution,
based new guesses. prove process iterated indefinitely,
converge hierarchically optimal policy (provided, course, state abstractions
used within subtasks).
suggests extension MAXQ-Q learning adapts R~ values online.
time subtask terminates, could update R~ function based computed value
terminated state. precise, j subtask i, j terminates
state s0 , update R~ j (s0 ) equal V~ (i; s0 ) = maxa0 Q~ (i; s0 ; a0 ). However,
work R~ j (s0 ) represented using full state s0. subtask j employing state
abstractions, x = (s), R~ j (x0 ) need average value V~ (i; s0 ),
average taken states s0 x0 = (s0 ) (weighted probability
visiting states). easily accomplished performing stochastic approximation
update form
R~j (x0 ) = (1 , fft )R~ j (x0 ) + fftV~ (i; s0 )
time subtask j terminates. algorithm could expected converge
best hierarchical policy consistent given state abstractions.
suggests problems, may worthwhile first learn recursively
optimal policy using aggressive state abstractions use learned value
function initialize MAXQ representation detailed representation
states. progressive refinements state space could guided monitoring
298

fiMAXQ Hierarchical Reinforcement Learning

degree values V~ (i; x0 ) vary abstract state x0 . large
variance, means state abstractions failing make important distinctions
values states, refined.
kinds adaptive algorithms take longer converge basic
MAXQ method described paper. tasks agent must solve many times
lifetime, worthwhile learning algorithms provide initial useful
solution gradually improve solution optimal. important goal
future research find methods diagnosing repairing errors (or sub-optimalities)
initial hierarchy ultimately optimal policy discovered.

8.2 Automated Discovery Abstractions

approach taken paper rely upon programmer design
MAXQ hierarchy including termination conditions, pseudo-reward functions, state
abstractions. results paper, particularly concerning state abstraction, suggest
ways might able automate construction hierarchy.
main purpose hierarchy create opportunities subtask sharing
state abstraction. actually closely related. order subtask shared
two different regions state space, must case value function
two different regions identical except additive offset. MAXQ framework,
additive offset would difference C values parent task. one way
find reusable subtasks would look regions state space value function
exhibits additive offsets.
second way would search structure one-step probability transition
function P (s0 js; a). subtask useful enables state abstractions Max
Node Irrelevance. formulate problem identifying region
state space that, conditioned region, P (s0 js; a) factors according
Equation 17. top-down divide-and-conquer algorithm similar decision-tree algorithms
might able this.
third way would search funnel actions looking bottlenecks state
space policies must travel. would useful discovering cases
Result Distribution Irrelevance.
ways, dicult kinds state abstractions discover
arbitrary subgoals introduced constrain policy (and sacrifice optimality).
example, could algorithm automatically decide impose landmarks onto
HDG task? Perhaps detecting large region state space without bottlenecks
variations reward function?
problem discovering hierarchies important challenge future,
least paper provided guidelines constitute good state abstractions,
serve objective functions guiding automated search abstractions.

9. Concluding Remarks

paper introduced new representation value function hierarchical reinforcement learning|the MAXQ value function decomposition. proved
MAXQ decomposition represent value function hierarchical policy
299

fiDietterich

finite-horizon undiscounted, cumulative reward criterion infinite-horizon
discounted reward criterion. representation supports subtask sharing re-use, overall value function decomposed value functions individual subtasks.
paper introduced learning algorithm, MAXQ-Q learning, proved
converges probability 1 recursively optimal policy. paper argued although
recursive optimality weaker either hierarchical optimality global optimality,
important form optimality permits subtask learn locally optimal
policy ignoring behavior ancestors MAXQ graph. increases
opportunities subtask sharing state abstraction.
shown MAXQ decomposition creates opportunities state abstraction, identified set five properties (Max Node Irrelevance, Leaf Irrelevance,
Result Distribution Irrelevance, Shielding, Termination) allow us ignore large
parts state space within subtasks. proved MAXQ-Q still converges
presence forms state abstraction, showed experimentally state abstraction important practice successful application MAXQ-Q learning|at
least Taxi HDG tasks.
paper presented two different methods deriving improved non-hierarchical policies MAXQ value function representation, formalized conditions
methods improve hierarchical policy. paper verified
experimentally non-hierarchical execution gives improved performance Fickle
Taxi Task (where achieves optimal performance) HDG task (where gives
substantial improvement).
Finally, paper argued tradeoff governing design hierarchical
reinforcement learning methods. one end design spectrum \context free"
methods MAXQ-Q learning. provide good support state abstraction
subtask sharing learn recursively optimal policies. end
spectrum \context-sensitive" methods HAMQ, options framework,
early work Dean Lin. methods discover hierarchically optimal
policies (or, cases, globally optimal policies), drawback cannot
easily exploit state abstractions share subtasks. great speedups
enabled state abstraction, paper argued context-free approach
preferred|and relaxed needed obtain improved policies.

Acknowledgements
author gratefully acknowledges support National Science Foundation
grant number IRI-9626584, Oce Naval Research grant number N00014-95-10557, Air Force Oce Scientific Research grant number F49620-98-1-0375,
Spanish government program Estancias de Investigadores Extranjeros en
Regimen de A~no Sabatico en Espa~na. addition, author indebted many colleagues
helping develop clarify ideas paper including Valentina Zubek, Leslie
Kaelbling, Bill Langford, Wes Pinchot, Rich Sutton, Prasad Tadepalli, Sebastian Thrun.
particularly want thank Eric Chown encouraging study Feudal reinforcement
learning, Ron Parr providing details HAM machines, Sebastian Thrun
encouraging write single comprehensive paper. thank Andrew Moore
300

fiMAXQ Hierarchical Reinforcement Learning

(the action editor), Valentina Zubek, two sets anonymous reviewers previous
drafts paper suggestions careful reading, improved paper
immeasurably.

References

Bellman, R. E. (1957). Dynamic Programming. Princeton University Press.
Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientific,
Belmont, MA.
Boutilier, C., Dearden, R., & Goldszmidt, M. (1995). Exploiting structure policy construction. Proceedings Fourteenth International Joint Conference Artificial
Intelligence, pp. 1104{1111.
Currie, K., & Tate, A. (1991). O-plan: open planning architecture. Artificial Intelligence, 52 (1), 49{86.
Dayan, P., & Hinton, G. (1993). Feudal reinforcement learning. Advances Neural
Information Processing Systems, 5, pp. 271{278. Morgan Kaufmann, San Francisco,
CA.
Dean, T., & Lin, S.-H. (1995). Decomposition techniques planning stochastic domains.
Tech. rep. CS-95-10, Department Computer Science, Brown University, Providence,
Rhode Island.
Dietterich, T. G. (1998). MAXQ method hierarchical reinforcement learning.
Fifteenth International Conference Machine Learning, pp. 118{126. Morgan Kaufmann.
Fikes, R. E., Hart, P. E., & Nilsson, N. J. (1972). Learning executing generalized robot
plans. Artificial Intelligence, 3, 251{288.
Forgy, C. L. (1982). Rete: fast algorithm many pattern/many object pattern
match problem. Artificial Intelligence, 19 (1), 17{37.
Hauskrecht, M., Meuleau, N., Kaelbling, L. P., Dean, T., & Boutilier, C. (1998). Hierarchical
solution Markov decision processes using macro-actions. Proceedings
Fourteenth Annual Conference Uncertainty Artificial Intelligence (UAI{98), pp.
220{229 San Francisco, CA. Morgan Kaufmann Publishers.
Howard, R. A. (1960). Dynamic Programming Markov Processes. MIT Press, Cambridge, MA.
Jaakkola, T., Jordan, M. I., & Singh, S. P. (1994). convergence stochastic iterative
dynamic programming algorithms. Neural Computation, 6 (6), 1185{1201.
Kaelbling, L. P. (1993). Hierarchical reinforcement learning: Preliminary results. Proceedings Tenth International Conference Machine Learning, pp. 167{173 San
Francisco, CA. Morgan Kaufmann.
301

fiDietterich

Kalmar, Z., Szepesvari, C., & Lorincz, A. (1998). Module based reinforcement learning
real robot. Machine Learning, 31, 55{85.
Knoblock, C. A. (1990). Learning abstraction hierarchies problem solving. Proceedings
Eighth National Conference Artificial Intelligence, pp. 923{928 Boston, MA.
AAAI Press.
Korf, R. E. (1985). Macro-operators: weak method learning. Artificial Intelligence,
26 (1), 35{77.
Lin, L.-J. (1993). Reinforcement learning robots using neural networks. Ph.D. thesis,
Carnegie Mellon University, Department Computer Science, Pittsburgh, PA.
Moore, A. W., Baird, L., & Kaelbling, L. P. (1999). Multi-value-functions: Ecient automatic action hierarchies multiple goal MDPs. Proceedings International Joint Conference Artificial Intelligence, pp. 1316{1323 San Francisco. Morgan Kaufmann.
Parr, R. (1998a). Flexible decomposition algorithms weakly coupled Markov decision
problems. Proceedings Fourteenth Annual Conference Uncertainty
Artificial Intelligence (UAI{98), pp. 422{430 San Francisco, CA. Morgan Kaufmann
Publishers.
Parr, R. (1998b). Hierarchical control learning Markov decision processes. Ph.D.
thesis, University California, Berkeley, California.
Parr, R., & Russell, S. (1998). Reinforcement learning hierarchies machines. Advances Neural Information Processing Systems, Vol. 10, pp. 1043{1049 Cambridge,
MA. MIT Press.
Pearl, J. (1988). Probabilistic Inference Intelligent Systems. Networks Plausible Inference. Morgan Kaufmann, San Mateo, CA.
Rummery, G. A., & Niranjan, M. (1994). Online Q-learning using connectionist systems.
Tech. rep. CUED/FINFENG/TR 166, Cambridge University Engineering Department, Cambridge, England.
Sacerdoti, E. D. (1974). Planning hierarchy abstraction spaces. Artificial Intelligence,
5 (2), 115{135.
Singh, S., Jaakkola, T., Littman, M. L., & Szepesvari, C. (1998). Convergence results
single-step on-policy reinforcement-learning algorithms. Tech. rep., University
Colorado, Department Computer Science, Boulder, CO. appear Machine
Learning.
Singh, S. P. (1992). Transfer learning composing solutions elemental sequential
tasks. Machine Learning, 8, 323{339.
Sutton, R. S., Singh, S., Precup, D., & Ravindran, B. (1999). Improved switching among
temporally abstract actions. Advances Neural Information Processing Systems,
Vol. 11, pp. 1066{1072. MIT Press.
302

fiMAXQ Hierarchical Reinforcement Learning

Sutton, R., & Barto, A. G. (1998). Introduction Reinforcement Learning. MIT Press,
Cambridge, MA.
Sutton, R. S., Precup, D., & Singh, S. (1998). MDPs Semi-MDPs: Learning,
planning, representing knowledge multiple temporal scales. Tech. rep., University Massachusetts, Department Computer Information Sciences, Amherst,
MA. appear Artificial Intelligence.
Tadepalli, P., & Dietterich, T. G. (1997). Hierarchical explanation-based reinforcement
learning. Proceedings Fourteenth International Conference Machine
Learning, pp. 358{366 San Francisco, CA. Morgan Kaufmann.
Tambe, M., & Rosenbloom, P. S. (1994). Investigating production system representations
non-combinatorial match. Artificial Intelligence, 68 (1), 155{199.
Watkins, C. J. C. H. (1989). Learning Delayed Rewards. Ph.D. thesis, King's College,
Oxford. (To reprinted MIT Press.).
Watkins, C. J., & Dayan, P. (1992). Technical note Q-Learning. Machine Learning, 8, 279.

303


