Journal Artificial Intelligence Research 13 (2000) 155-188

Submitted 6/00; published 10/00

AIS-BN: Adaptive Importance Sampling Algorithm
Evidential Reasoning Large Bayesian Networks
Jian Cheng
Marek J. Druzdzel

jcheng@sis.pitt.edu
marek@sis.pitt.edu

Decision Systems Laboratory
School Information Sciences Intelligent Systems Program
University Pittsburgh, Pittsburgh, PA 15260 USA

Abstract
Stochastic sampling algorithms, attractive alternative exact algorithms
large Bayesian network models, observed perform poorly evidential
reasoning extremely unlikely evidence. address problem, propose adaptive importance sampling algorithm, AIS-BN, shows promising convergence rates
even extreme conditions seems outperform existing sampling algorithms
consistently. Three sources performance improvement (1) two heuristics
initialization importance function based theoretical properties importance sampling finite-dimensional integrals structural advantages Bayesian
networks, (2) smooth learning method importance function, (3) dynamic
weighting function combining samples different stages algorithm.
tested performance AIS-BN algorithm along two state art
general purpose sampling algorithms, likelihood weighting (Fung & Chang, 1989; Shachter
& Peot, 1989) self-importance sampling (Shachter & Peot, 1989). used
tests three large real Bayesian network models available scientific community:
CPCS network (Pradhan et al., 1994), PathFinder network (Heckerman, Horvitz,
& Nathwani, 1990), ANDES network (Conati, Gertner, VanLehn, & Druzdzel,
1997), evidence unlikely 1041 . AIS-BN algorithm always performed
better two algorithms, majority test cases achieved orders
magnitude improvement precision results. Improvement speed given desired
precision even dramatic, although unable report numerical results here,
algorithms almost never achieved precision reached even first
iterations AIS-BN algorithm.

1. Introduction
Bayesian networks (Pearl, 1988) increasingly popular tools modeling uncertainty
intelligent systems. practical models reaching size several hundreds variables
(e.g., Pradhan et al., 1994; Conati et al., 1997), becomes increasingly important address problem feasibility probabilistic inference. Even though several ingenious
exact algorithms proposed, large models stumble theoretically demonstrated NP-hardness inference (Cooper, 1990). significance
result observed practice exact algorithms applied large, densely connected
practical networks require either prohibitive amount memory prohibitive amount
computation unable complete. approximating inference desired
precision shown NP-hard well (Dagum & Luby, 1993), comc
2000
AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiCheng & Druzdzel

plex networks alternative produce result all. Furthermore,
obtaining result crucial applications, precision guarantees may critical
types problems traded speed computation.
prominent subclass approximate algorithms family stochastic sampling
algorithms (also called stochastic simulation Monte Carlo algorithms). precision
obtained stochastic sampling generally increases number samples generated
fairly unaffected network size. Execution time fairly independent
topology network linear number samples. Computation
interrupted time, yielding anytime property algorithms, important timecritical applications.
stochastic sampling performs well predictive inference, diagnostic reasoning, i.e., reasoning observed evidence nodes ancestors network often
exhibits poor convergence. number observations increases, especially
observations unlikely a-priori, stochastic sampling often fails converge reasonable estimates posterior probabilities. Although problem known since
first sampling algorithm proposed Henrion (1988), little done
address effectively. Furthermore, various sampling algorithms proposed tested
simple small networks, networks special topology, without presence
extremely unlikely evidence practical significance problem underestimated. Given typical number samples used real-time feasible
todays hardware, say 106 samples, behavior stochastic sampling algorithm
drastically different different size networks. network consisting 10 nodes
observations, may possible converge exact probabilities, large
networks negligibly small fraction total sample space probed. One
practical Bayesian network models used tests, subset CPCS
network (Pradhan et al., 1994), consists 179 nodes. total sample space larger
1061 . 106 samples, sample 1055 fraction sample space.
believe crucial (1) study feasibility convergence properties
sampling algorithms large practical networks, (2) develop sampling algorithms show good convergence extreme, yet practical conditions,
evidential reasoning given extremely unlikely evidence. all, small networks
updated using existing exact algorithms precisely large networks
stochastic sampling useful. likelihood evidence, know
stochastic sampling generally perform well high (Henrion, 1988). So,
important look cases evidence unlikely. paper, test
two existing state art stochastic sampling algorithms Bayesian networks, likelihood weighting (Fung & Chang, 1989; Shachter & Peot, 1989) self-importance sampling
(Shachter & Peot, 1989), subset CPCS network extremely unlikely evidence. show exhibit similarly poor convergence rates. propose new
sampling algorithm, call adaptive importance sampling Bayesian networks
(AIS-BN), suitable evidential reasoning large multiply-connected Bayesian
networks. AIS-BN algorithm based importance sampling, widely
applied method variance reduction simulation applied Bayesian networks (e.g., Shachter & Peot, 1989). demonstrate empirically three large
practical Bayesian network models AIS-BN algorithm consistently outperforms
156

fiAdaptive Importance Sampling Bayesian Networks

two algorithms. majority test cases, achieved two orders
magnitude improvement convergence. Improvement speed given desired precision
even dramatic, although unable report numerical results here,
algorithms never achieved precision reached even first iterations
AIS-BN algorithm. main sources improvement are: (1) two heuristics
initialization importance function based theoretical properties importance sampling finite-dimensional integrals structural advantages Bayesian
networks, (2) smooth learning method updating importance function, (3)
dynamic weighting function combining samples different stages algorithm.
study value two heuristics used AIS-BN algorithm: (1) initialization
probability distributions parents evidence nodes uniform distribution
(2) adjusting small probabilities conditional probability tables, show
play important role AIS-BN algorithm moderate role
existing algorithms.
remainder paper structured follows. Section 2 first gives general
introduction importance sampling domain finite-dimensional integrals,
originally proposed. show importance sampling used compute probabilities Bayesian networks draw additional benefits graphical
structure network. develop generalized sampling scheme aid us
reviewing previously proposed sampling algorithms describing AIS-BN
algorithm. Section 3 describes AIS-BN algorithm. propose two heuristics initialization importance function discuss theoretical foundations. describe
smooth learning method importance function dynamic weighting function
combining samples different stages algorithm. Section 4 describes empirical evaluation AIS-BN algorithm. Finally, Section 5 suggests several possible
improvements AIS-BN algorithm, possible applications learning scheme,
directions future work.

2. Importance Sampling Algorithms Bayesian Networks
feel useful go back theoretical roots importance sampling order
able understand source speedup AIS-BN algorithm relative
existing state art importance sampling algorithms Bayesian networks. first
review general idea importance sampling finite-dimensional integrals
reduce sampling variance. discuss application importance sampling
Bayesian networks. Readers interested details directed literature
Monte Carlo methods computation finite integrals, excellent exposition
Rubinstein (1981) essentially following first section.
2.1 Mathematical Foundations
Let g(X) function variables X = (X1 , ..., Xm ) domain Rm ,
computing g(X) X feasible. Consider problem approximate computation
integral
Z
I=

g(X) dX .


157

(1)

fiCheng & Druzdzel

Importance sampling approaches problem writing integral (1)
Z

I=


g(X)
f (X) dX ,
f (X)

f (X), often referred importance function, probability density function
. f (X) used importance sampling exists algorithm generating
samples f (X) importance function zero original function
zero, i.e., g(X) 6= 0 = f (X) 6= 0.
independently sampled n points s1 , s2 , . . . , sn , si , according
probability density function f (X), estimate integral
n
1X
g(si )
=
n i=1 f (si )

(2)

estimate variance
b (In ) =

2

n
X
1
g(si )

n (n 1) i=1 f (si )



2

.

(3)

straightforward show estimator following properties:
1. E(In ) =
2. limn =

n
3. n (In I) Normal(0, f2(X) ),
f2(X)


Z

=


g(X)

f (X)

2

f (X) dX

(4)



b 2 (In ) = 2 (In ) = f2 (X) /n
4. E

variance proportional f2(X) inversely proportional number
samples. minimize variance , either increase number samples
try decrease f2(X) . respect latter, Rubinstein (1981) reports following
useful theorem corollary.
Theorem 1 minimum f2(X) equal
f2(X)

2

Z

|g(X)| dX

=

I2



occurs X distributed according following probability density function
f (X) = R

|g(X)|
.
|g(X)| dX

158

fiAdaptive Importance Sampling Bayesian Networks

Corollary 1 g(X) >0, optimal probability density function
f (X) =

g(X)


f2(X) = 0.
Although practice sampling precisely f (X) = g(X)/I occur rarely, expect
functions close enough still reduce variance effectively. Usually,
closer shape function f (X) shape function g(X), smaller
f2(X) . high-dimensional integrals, selection importance function, f (X), far
critical increasing number samples, since former dramatically
affect f2(X) . seems prudent put energy choosing importance function
whose shape close possible g(X) apply brute force method
increasing number samples.
worth noting f (X) uniform, importance sampling becomes general
Monte Carlo sampling. Another noteworthy property importance sampling
derived Equation 4 avoid f (X) |g(X) f (X)| part
domain sampling, even f (X) matches well g(X)/I important regions.
f (X) |g(X) f (X)|, variance become large even infinite.
avoid adjusting f (X) larger unimportant regions domain X.
section discussed importance sampling continuous variables,
results stated valid discrete variables well, case integration
substituted summation.
2.2 Generic Importance Sampling Algorithm Bayesian Networks
following discussion, random variables used multiple-valued, discrete variables.
Capital letters, A, B, C, denote random variables. Bold capital letters,
A, B, C, denote sets variables. Bold capital letter E usually used denote
set evidence variables. Lower case letters a, b, c denote particular instantiations
variables A, B, C respectively. Bold lower case letters, a, b, c, denote
particular instantiations sets A, B, C respectively. Bold lower case letter e,
particular, used denote observations, i.e., instantiations set evidence
variables E. Anc(A) denotes set ancestors node A. Pa(A) denotes set
parents (direct ancestors) node A. pa(A) denotes particular instantiation Pa(A). \
denotes set difference. Pa(A)|E=e denotes use extended vertical bar indicate
substitution e E A.
know joint probability distribution variables Bayesian network model, Pr(X), product probability distributions nodes
conditional parents, i.e.,
Pr(X) =

n


Pr(Xi |Pa(Xi )) .

(5)

i=1

order calculate Pr(E = e), need sum Pr(X\E, E = e).
Pr(E = e) =

X

Pr(X\E, E = e)

X\E

159

(6)

fiCheng & Druzdzel

see Equation 6 almost identical Equation 1 except integration
replaced summation domain replaced X\E. theoretical results
derived importance sampling reviewed previous section thus
directly applied computing probabilities Bayesian networks.
previous work importance sampling-based algorithms Bayesian networks, postpone discussion work next section.
present generic stochastic sampling algorithm help us reviewing
prior work presenting algorithm.
posterior probability Pr(a|e) obtained first computing Pr(a, e) Pr(e)
combining based definition conditional probability
Pr(a|e) =

Pr(a, e)
.
Pr(e)

(7)

order increase accuracy results importance sampling computing posterior probabilities different network variables given evidence, general use
different importance functions Pr(a, e) Pr(e). increases computation time linearly gain accuracy may significant given obtaining
desired accuracy exponential nature. often, common practice use
importance function (usually Pr(e)) sample probabilities. difference
1. Order nodes according topological order.
2. Initialize importance function Pr0 (X\E), desired number samples
m, updating interval l, score arrays every node.
3. k 0,
4. 1
5.

(i mod l == 0)

6.

k k+1

7.

Update importance function Prk (X\E) based .
end

8.

si generate sample according Prk (X\E)

9.

{si }

10.

Calculate Score(si , Pr(X\E, e), Prk (X\E)) add corresponding entry every score array according instantiated states.
end

11. Normalize score arrays every node.
Figure 1: generic importance sampling algorithm.
160

fiAdaptive Importance Sampling Bayesian Networks

optimal importance functions two quantities large, perforc
c
mance may deteriorate significantly. Although Pr(a,
e) Pr(e)
unbiased estimators
c
according Property 1 (Section 2.1), Pr(a|e)
obtained means Equation 7
unbiased estimator. However, number samples increases, bias decreases
ignored altogether sample size large enough (Fishman, 1995).
Figure 1 presents generic stochastic sampling algorithm captures
existing sampling algorithms. Without loss generality, restrict
description so-called forward sampling, i.e., generation samples topological
order nodes network. forward sampling order accomplished
initialization performed Step 1, parents node placed node
itself. forward sampling, Step 8 algorithm, actual generation samples, works
follows. (i) evidence node instantiated observed state omitted
sample generation; (ii) root node randomly instantiated one possible
states, according importance prior probability node, derived
Prk (X\E); (iii) node whose parents instantiated randomly instantiated
one possible states, according importance conditional probability distribution
node given values parents, derived Prk (X\E); (iv)
procedure followed nodes instantiated. complete instantiation si
network based method one sample joint importance probability distribution
Prk (X\E) variables network. scoring Step 10 amounts calculating
Pr(si , e)/Prk (si ), required Equation 2. ratio total score sum
number samples unbiased estimator Pr(e). Step 10, count score
sum condition = a, i.e., unobserved variables values a,
ratio score sum number samples unbiased estimator
Pr(a, e).
existing algorithms focus posterior probability distributions individual
nodes. mentioned above, sake efficiency count score sum corresponding Pr(A = a, e), X\E, record score array node A.
entry array corresponds specified state A. method introduces additional
variance, opposed using importance function derived Prk (X\E) sample
Pr(A = a, e), X\E, directly.
2.3 Existing Importance Sampling Algorithms Bayesian Networks
main difference various stochastic sampling algorithms process
Steps 2, 7, 8 generic importance sampling algorithm Figure 1.
Probabilistic logic sampling (Henrion, 1988) simplest first proposed sampling algorithm Bayesian networks. importance function initialized Step 2
Pr(X) never updated (Step 7 null). Without evidence, Pr(X) optimal importance function evidence set, empty anyway. escapes authors
Pr(X) may optimal importance function Pr(A = a), X,
root node. mismatch optimal actually used importance
function may result large variance. sampling process evidence
without evidence except Step 10 count scores samples
inconsistent observed evidence, amounts discarding them.
161

fiCheng & Druzdzel

evidence unlikely, large difference Pr(X) optimal
importance function. Effectively, samples discarded performance logic
sampling deteriorates badly.
Likelihood weighting (LW) (Fung & Chang, 1989; Shachter & Peot, 1989) enhances
logic sampling never discards samples. likelihood weighting, importance
function Step 2



Pr(X\E) =
Pr(xi |Pa(Xi ))fifi

xi e
/


.
E=e

Likelihood weighting update importance function Step 7. Although likelihood weighting improvement logic sampling, convergence rate still
slow large difference optimal importance function Pr(X\E),
especially situations evidence unlikely. simplicity,
likelihood weighting algorithm commonly used simulation method
Bayesian network inference. often matches performance other, sophisticated
schemes simple able increase precision generating samples
algorithms amount time.
Backward sampling (Fung & del Favero, 1994) changes Step 1 generic algorithm
allows generating samples evidence nodes direction opposite
topological order nodes network. Step 2, backward sampling uses likelihood observed evidence instantiated nodes calculate Pr0 (X\E).
Although Fung del Favero mentioned possibility dynamic node ordering,
propose scheme updating importance function Step 7. Backward
sampling suffers problems similar likelihood weighting, i.e., possible mismatch importance function optimal importance function
lead poor convergence.
Importance sampling (Shachter & Peot, 1989) generic sampling algorithm. Shachter Peot introduced two variants importance sampling: self-importance
(SIS) heuristic importance. importance function used first step
self-importance algorithm



0
Pr (X\E) =
Pr(xi |Pa(Xi ))fifi

xi e
/


.
E=e

function updated Step 7. algorithm tries revise conditional probability
tables (CPTs) periodically order make sampling distribution gradually approach
posterior distribution. Since data used update importance function
compute estimator, process introduces bias estimator. Heuristic
importance first removes edges network becomes polytree,
uses modified version polytree algorithm (Pearl, 1986) compute likelihood
functions unobserved nodes. Pr0 (X\E) combination likelihood
functions Pr(X\E, e). Step 7 heuristic importance update Prk (X\E).
Shachter Peot (1989) point out, heuristic importance function still lead
bad approximation optimal importance function. exist algorithms
combination self-importance heuristic importance (Shachter & Peot, 1989;
162

fiAdaptive Importance Sampling Bayesian Networks

Shwe & Cooper, 1991). Although researchers suggested may promising
direction work sampling algorithms, seen results would
follow this.
separate group stochastic sampling methods formed so-called Markov Chain
Monte Carlo (MCMC) methods divided Gibbs sampling, Metropolis sampling,
Hybrid Monte Carlo sampling (Geman & Geman, 1984; Gilks, Richardson, & Spiegelhalter, 1996; MacKay, 1998). Roughly speaking, methods draw random samples
unknown target distribution f (X) biasing search distribution towards
higher probability regions. applied Bayesian networks (Pearl, 1987; Chavez &
Cooper, 1990) approach determines sampling distribution variable
previous sample given Markov blanket (Pearl, 1988). corresponds updating
Prk (X\E) sampling every node. Prk (X\E) converge optimal importance
function Pr(e) Pr0 (X\E) satisfies ergodic properties (York, 1992). Since
convergence limiting distribution slow calculating updates sampling distribution costly, algorithms used practice often simple
likelihood weighting scheme.
simulation algorithms, bounded variance algorithm
(Dagum & Luby, 1997) AA algorithm (Dagum et al., 1995), essentially
based LW algorithm Stopping-Rule Theorem (Dagum et al., 1995). Cano
et al. (1996) proposed another importance sampling algorithm performed somewhat
better LW cases extreme probability distributions, but, authors state,
general cases produced similar results likelihood weighting algorithm. Hernandez
et al. (1998) applied importance sampling reported moderate improvement
likelihood weighting.
2.4 Practical Performance Existing Sampling Algorithms
largest network tested using sampling algorithms QMR-DT (Quick
Medical Reference Decision Theoretic) (Shwe et al., 1991; Shwe & Cooper, 1991),
contains 534 adult diseases 4,040 findings, 40,740 arcs depicting disease-to-finding
dependencies. QMR-DT network belongs class special bipartite networks
structure often referred BN2O (Henrion, 1991), two-layer
composition: disease nodes top layer finding nodes bottom layer. Shwe
colleagues used algorithm combining self-importance heuristic importance
tested convergence properties QMR-DT network. since heuristic method
iterative tabular Bayes (ITB) makes use version Bayes rule designed
BN2O networks, cannot generalized arbitrary networks. Although Shwe
colleagues concluded Markov blanket scoring self-importance sampling significantly
improve convergence rate model, cannot extend conclusion general
networks. computation Markov blanket scoring complex general multiconnected network BN2O network. Also, experiments conducted lacked
gold-standard posterior probability distribution could serve judge convergence
rate.
Pradhan Dagum (1996) tested efficient version LW algorithm bounded
variance algorithm (Dagum & Luby, 1997) AA algorithm (Dagum et al., 1995)
163

fiCheng & Druzdzel

146 node, multiply connected medical diagnostic Bayesian network. One limitation
tests probability evidence cases selected testing rather
high. Although 10% cases probability evidence order
108 smaller, simple calculation based reported mean = 34.5 number
evidence nodes, shows average probability observed state evidence node
conditional direct predecessors order (108 )1/34.5 0.59. Given
algorithm essentially based LW algorithm, based tests suspect
performance deteriorate cases evidence unlikely.
algorithms focus marginal probability one hypothesis node. many
queried nodes, efficiency may deteriorate.
tested algorithms discussed Section 2.3 several large networks.
experimental results show cases unlikely evidence, none algorithms
converges reasonable estimates posterior probabilities within reasonable amount
time. convergence becomes worse number evidence nodes increases. Thus,
using algorithms large networks, simply cannot trust results.
present results tests LW SIS algorithms detail Section 4.

3. AIS-BN: Adaptive Importance Sampling Bayesian Networks
main reason existing stochastic sampling algorithms converge slowly
fail learn good importance function sampling process and, effectively,
fail reduce sampling variance. importance function optimal,
probabilistic logic sampling without evidence, algorithms capable
converging fairly good estimates posterior probabilities within relatively
samples. example, assuming posterior probabilities extreme (i.e., larger
say 0.01), 1,000 samples may sufficient obtain good estimates.
section, present adaptive importance sampling algorithm Bayesian networks
(AIS-BN) that, demonstrate next section, performs well
tests. first describe details algorithm prove two theorems
useful learning optimal importance sampling function.
3.1 Basic Algorithm AIS-BN
Compared importance sampling used normal finite-dimensional integrals, importance sampling used Bayesian networks several significant advantages. First,
network joint probability distribution Pr(X) decomposable factored
component parts. Second, network clear structure, represents many conditional independence relationships. properties helpful estimating
optimal importance function.
basic AIS-BN algorithm presented Figure 2. main differences
AIS-BN algorithm basic importance sampling algorithm Figure 1
introduce monotonically increasing weight function wk two effective heuristic
initialization methods Step 2. introduce special learning component Step 7
let updating process run smoothly, avoiding oscillation parameters.
164

fiAdaptive Importance Sampling Bayesian Networks

1. Order nodes according topological order.
2. Initialize importance function Pr0 (X\E) using heuristic methods, initialize weight w0 , set desired number samples updating
interval l, initialize score arrays every node.
3. k 0, , wT Score 0, wsum 0
4. 1
5.

(i mod l == 0)

6.

k k+1

7.

Update importance function Prk (X\E) wk based .
end

8.

si generate sample according Prk (X\E)

9.

{si }

10.

wiScore Score (si , Pr(X\E, e), Prk (X\E), wk )

11.

wT Score wT Score + wiScore
(Optional: add wiScore corresponding entry every score array)

12.

wsum wsum + wk
end

13. Output estimate Pr(E) wT Score /wsum
(Optional: Normalize score arrays every node)

Figure 2: adaptive importance sampling Bayesian Networks (AIS-BN) algorithm.
score processing Step 10
Pr(si , e)
wiScore = wk k
.
Pr (si )
Note respect algorithm Figure 1 becomes special case AIS-BN
wk = 1. reason use wk want give different weights
sampling results obtained different stages algorithm. stage updates
importance function, different distance optimal importance
b k ,
b k standard deviation estimated
function. recommend wk 1/
1
k
stage k using Equation 3. order keep w monotonically increasing, wk smaller
wk1 , adjust value wk1 . weighting scheme may introduce bias
1. similar weighting scheme based variance apparently developed independently Ortiz
Kaelbling (2000), recommend weight wk 1/(
bk )2 .

165

fiCheng & Druzdzel

final result. Since initial importance sampling functions often inefficient
introduce big variance results, recommend wk = 0 first
stages algorithm. designed weighting scheme reflect fact
practice estimates small estimated variance usually good estimates.
3.2 Modifying Sampling Distribution AIS-BN
Based theoretical considerations Section 2.1, know crucial element
algorithm converging good approximation optimal importance function.
follows, first give optimal importance function calculating Pr(E = e)
discuss use structural advantages Bayesian networks approximate
function. sequel, use symbol denote importance sampling
function denote optimal importance sampling function.
Since Pr(X\E, E = e) > 0, Corollary 1
(X\E) =

Pr(X\E, E = e)
= Pr(X|E = e) .
Pr(E = e)

following corollary captures result.
Corollary 2 optimal importance sampling function (X\E) calculating Pr(E = e)
Equation 6 Pr(X|E = e).
Although know mathematical expression optimal importance sampling
function, difficult obtain function exactly. algorithm, use following
importance sampling function
(X\E) =

n


Pr(Xi |Pa(Xi ), E) .

(8)

i=1

function partially considers effect evidence every node
sampling process. network structure network
absorbed evidence, function optimal importance sampling function.
easy learn and, experimental results show, good approximation
optimal importance sampling function. Theoretically, posterior structure
model changes drastically result observed evidence, importance sampling
function may perform poorly. tried find practical networks would
happen, day encountered drastic example effect.
Section 2.2, know score sums corresponding {xi , pa(Xi ), e}
yield unbiased estimator Pr(xi , pa(Xi ), e). According definition conditional
probability, get estimator Pr0 (xi |pa(Xi ), e). achieved maintaining updating table every node, structure mimicks structure
CPT. tables allow us decompose importance function components learned individually. call tables importance conditional
probability tables (ICPT).
Definition 1 importance conditional probability table (ICPT) node X table
posterior probabilities Pr(X|Pa(X), E = e) conditional evidence indexed
immediate predecessors, Pa(X).
166

fiAdaptive Importance Sampling Bayesian Networks

ICPT tables modified process learning importance function.
prove useful theorem lead considerable savings learning
process.
Theorem 2
Xi X, Xi
/ Anc(E) Pr(Xi |Pa(Xi ), E) = Pr(Xi |Pa(Xi )) .

(9)

Proof: Suppose set values parents node Xi pa(Xi ). Node Xi
dependent evidence E given pa(Xi ) Xi d-connecting E given pa(Xi )
(Pearl, 1988). According definition d-connectivity, happens
exists member Xi descendants belongs set evidence nodes E.
words Xi
/ Anc(E).
2
Theorem 2 important AIS-BN algorithm. states essentially
ICPT tables nodes ancestors evidence nodes equal
CPT tables throughout learning process. need learn ICPT tables
ancestors evidence nodes. often lead significant savings
computation. If, example, evidence nodes root nodes, ICPT tables
every node already AIS-BN algorithm becomes identical likelihood weighting
algorithm. Without evidence, AIS-BN algorithm becomes identical probabilistic
logic sampling algorithm.
worth pointing Xi , Pr(Xi |Pa (Xi ), E) (i.e., ICPT table
Xi ), easily calculated using exact methods. example, Xi parent
evidence node Ej Ej child Xi , posterior probability distribution
Xi straightforward compute exactly. Since focus current paper
Input: Initialized importance function Pr0 (X\E), learning rate (k).
Output: estimated importance function PrS (X\E).
stage k 0
1. Sample l points sk1 , sk2 , . . . , skl independently according current importance function Prk (X\E).
2. every node Xi Xi X\E Xi
/ Anc(E) count score sums
corresponding {xi , pa(Xi ), e} estimate Pr0 (xi |pa(Xi ), e) based sk1 ,
sk2 , . . . , skl .
3. Update Prk (X\E) according following formula:
Prk+1 (xi |pa(Xi ), e) =




Prk (xi |pa(Xi ), e) + (k) Pr0 (xi |pa(Xi ), e) Prk (xi |pa(Xi ), e)
end

Figure 3: AIS-BN algorithm learning optimal importance function.
167

fiCheng & Druzdzel

sampling, test results reported paper include improvement
AIS-BN algorithm.
Figure 3 lists algorithm implements Step 7 basic AIS-BN algorithm listed
Figure 2. estimate Pr0 (xi |pa(Xi ), e), use samples obtained
current stage. One reason information obtained previous stages
absorbed Prk (X\E). reason principle, successive iteration
accurate previous one importance function closer optimal
importance function. Thus, samples generated Prk+1 (X\E) better
generated Prk (X\E). Pr0 (Xi |pa(Xi ), e) Prk (Xi |pa(Xi ), e) corresponds vector
first partial derivatives direction maximum decrease error. (k)
positive function determines learning rate. (k) = 0 (lower bound),
update importance function. (k) = 1 (upper bound), stage
discard old function. convergence speed directly related (k). small,
convergence slow due large number updating steps needed
reach local minimum. hand, large, convergence rate initially
fast, algorithm eventually start oscillate thus may reach
minimum. many papers field neural network learning discuss
choose learning rate let estimated importance function converge quickly
destination function. method improve learning rate applicable
algorithm. Currently, use following function proposed Ritter et al. (1991)
k/kmax

(k) =

b


,

(10)

initial learning rate b learning rate last step. function
reported perform well neural network learning (Ritter et al., 1991).
3.3 Heuristic Initialization AIS-BN
dimensionality problem Bayesian network inference equal number
variables network, networks considered paper high.
result, learning space optimal importance function large. Choice
initial importance function Pr0 (X\E) important factor affecting learning
initial value importance function close optimal importance function
greatly affect speed convergence. section, present two heuristics
help achieve goal.
Due explicit encoding structure decomposable joint probability distribution, Bayesian networks offer computational advantages compared finite-dimensional
integrals. possible first approximation optimal importance function prior
probability distribution network variables, Pr(X). propose improvement
initialization. know effect evidence nodes node attenuated
path length node evidence nodes increased (Henrion, 1989)
affected nodes direct ancestors evidence nodes. Initializing ICPT
tables parents evidence nodes uniform distributions experience improves convergence rate. Furthermore, CPT tables parents evidence
node E may favorable observed state e probability E = e without
168

fiAdaptive Importance Sampling Bayesian Networks

condition less small value, Pr(E = e) < 1/(2 nE ), nE
number outcomes node E. Based observation, change CPT tables
parents evidence node E uniform distributions experiment
Pr(E = e) < 1/(2 nE ), otherwise leave unchanged. kind initialization
involves knowledge Pr(E = e), marginal probability without evidence. Probabilistic logic sampling (Henrion, 1988) enhanced Latin hypercube sampling (Cheng &
Druzdzel, 2000b) quasi-Monte Carlo methods (Cheng & Druzdzel, 2000a) produce
good estimate Pr(E = e). one-time effort made model
building stage worth pursuing desired precision.
Another serious problem related sampling extremely small probabilities. Suppose
exists root node state prior probability Pr(s) = 0.0001. Let
posterior probability state given evidence Pr(s|E) = 0.8. simple calculation
shows update importance function every 1, 000 samples, expect
hit every 10 updates. Thus ss convergence rate slow.
overcome problem setting threshold replacing every probability p <
network .2 time, subtract ( p) largest probability
conditional probability distribution. example, value = 10/l, l
updating interval, allow us sample 10 times often first stage
algorithm. state turns likely (having large weight), increase
probability even order converge correct answer faster. Considering
avoid f (X) |g(X) f (X)| unimportant region discussed
Section 2.1, need make threshold larger. found convergence
rate quite sensitive threshold. Based empirical tests, suggest use
= 0.04 networks whose maximum number outcomes per node exceed five.
smaller threshold might lead fast convergence cases slow convergence
others. one threshold work, changing specific network usually improve
convergence rate.
3.4 Selection Parameters
several tunable parameters AIS-BN algorithm. base choice
parameters Central Limit Theorem (CLT). According CLT, Z1 , Z2 , . . . ,
Zn independent identically distributed random variables E(Zi ) = Z
Var(Zi ) = Z2 , = 1, ..., n, Z = (Z1 +...+Zn )/n approximately normally distributed
n sufficiently large. Thus,
lim P (

n





fiZ z

z


Z
2
Z / n
2
ex /2 dx .
t) =

z
2

(11)

Although approximation holds n approaches infinity, CLT known
robust lead excellent approximations even small n. formula Equation 11
(r , ) Relative Approximation, estimate satisfies
P(

| |
r ) .


2. initialization heuristic apparently developed independently Ortiz Kaelbling (2000).

169

fiCheng & Druzdzel

fixed,


Z / n

r =
1
Z ( ),
z
2

Z (z) = 12 z ex /2 dx. Since sampling problem, z (corresponding

Pr(E) Figure 2) fixed, setting r smaller value amounts letting Z / n

smaller. So, adjust parameters based Z / n, estimated
bk
using Equation 3. theoretical intuition behind recommendation wk 1/
Section 3.1. expect work well networks, guarantees
given exist always extreme cases sampling algorithms
good estimate variance obtained.
R

2

3.5 Generalization AIS-BN: Problem Estimating Pr(a|e)
typical focus systems based Bayesian networks posterior probability various
outcomes individual variables given evidence, Pr(a|e). generalized
computation posterior probability particular instantiation set variables
given evidence, i.e., Pr(A = a|e). two methods capable performing
computation. first method efficient expense precision. second
method less efficient, offers general better convergence rates. methods
based Equation 7.
first method reuses samples generated estimate Pr(e) estimating Pr(a, e).
Estimation Pr(a, e) amounts counting scored sum condition = a.
main advantage method efficiency use set samples
estimate posterior probability state subset network given evidence.
main disadvantage variance estimated Pr(a, e) large, especially
numerical value Pr(a|e) extreme. method widely used
approach existing stochastic sampling algorithms.
second method, used much rarely (e.g., Cano et al., 1996; Pradhan & Dagum,
1996; Dagum & Luby, 1997), calls estimating Pr(e) Pr(a, e) separately.
estimating Pr(e), additional call algorithm made instantiation
set variables interest A. Pr(a, e) estimated sampling network
set observations e extended = a. main advantage method
much better reducing variance first method. main disadvantage
computational cost associated sampling possibly many combinations states
nodes interest.
Cano et al. (1996) suggested modified version second method. Suppose
interested posterior distribution Pr(ai |e) possible values ai A, = 1,
2, . . . , k. estimate Pr(ai , e) = 1, . . . , k separately, use value
Pk
i=1 Pr(ai , e) estimate Pr(e). assumption behind approach
estimate Pr(e) accurate large sample drawn.
However, even guarantee small variance every Pr(ai , e), cannot guarantee
sum small variance. So, AIS-BN algorithm use
pure form methods. algorithm listed Figure 2 based first
method optional computations Steps 12 13 performed. algorithm
170

fiAdaptive Importance Sampling Bayesian Networks

corresponding second method skips optional steps calls basic AIS-BN
algorithm twice estimate Pr(e) Pr(a, e) separately.
first method attractive simplicity possible computational
efficiency. However, shown Section 2.2, performance sampling algorithm uses one set samples (as first method above) estimate Pr(a|e)
deteriorate difference optimal importance functions Pr(a,e)
Pr(e) large. main focus computation high accuracy posterior probability distribution small number nodes, strongly recommend use algorithm
based second method. Also, algorithm easily used estimate confidence
intervals solution.

4. Experimental Results
section, first describe experimental method used tests. tests focus
CPCS network, one largest realistic networks available
know precisely nodes observable. were, therefore, able
generate realistic test cases. Since AIS-BN algorithm uses two initialization
heuristics, designed experiment studies contribution two
heuristics performance algorithm. probe extent AIS-BN algorithms
excellent performance, test several real large networks.
4.1 Experimental Method
performed empirical tests comparing AIS-BN algorithm likelihood weighting
(LW) self-importance sampling (SIS) algorithms. two algorithms basically
state art general purpose belief updating algorithms. AA (Dagum et al.,
1995) bounded variance (Dagum & Luby, 1997) algorithms, suggested
reviewer, essentially enhanced special purpose versions basic LW algorithm.
implementation three algorithms relied essentially code separate
functions algorithms differed. fair assume, therefore, observed
differences purely due theoretical differences among algorithms due
efficiency implementation. order make comparison AIS-BN algorithm
LW SIS fair, used first method computation (Section 3.5), i.e., one
relies single sampling rather calling basic AIS-BN algorithm twice.
measured accuracy approximation achieved simulation terms
Mean Square Error (MSE), i.e., square root sum square differences Pr0 (xij )
Pr(xij ), sampled exact marginal probabilities state j (j = 1, 2, . . . , ni )
node i, Xi
/ E. precisely,
v
u
u
MSE = P

1

Xi N\E ni

X

ni
X

(Pr0 (xij ) Pr(xij ))2 ,

Xi N\E j=1

N set nodes, E set evidence nodes, ni number
outcomes node i. diagrams, reported MSE averaged 10 runs. used
clustering algorithm (Lauritzen & Spiegelhalter, 1988) compute gold standard
171

fiCheng & Druzdzel

results comparisons mean square error. performed experiments
Pentium II, 333 MHz Windows computer.
MSE perfect, simplest way capturing error lends
theoretical analysis. example, possible derive analytically idealized
convergence rate terms MSE, which, turn, used judge quality
algorithm. MSE used virtually previous tests sampling algorithms,
allows interested readers tie current results past studies. reviewer offered
interesting suggestion using cross-entropy technique weights small
changes near zero much strongly equivalent size change middle
[0, 1] interval. measure would penalize algorithm imprecisions possibly
several orders magnitude small probabilities. idea interesting,
aware theoretical reasons measure would make difference
comparisons AIS-BN, LW SIS algorithms. MSE, mentioned above,
allow us compare empirically determined convergence rate theoretically
derived ideal convergence rate. Theoretically, MSE inversely proportional
square root sample size.
Since several tunable parameters used AIS-BN algorithm, list
values parameters used test: l = 2, 500; wk = 0 k 9 wk = 1
otherwise. stopped updating process Step 7 Figure 2 k 10.
words, used samples collected last step algorithm. learning
parameters used algorithm kmax = 10, = 0.4, b = 0.14 (see Equation 10).
used empirically determined value threshold = 0.04 (Section 3.3).
change CPT tables parents special evidence node uniform distributions
Pr(A = a) < 1/(2 nA ). parameters matter design decision
(e.g., number samples tests), others chosen empirically. Although
found parameters may different optimal values different Bayesian
networks, used values tests AIS-BN algorithm described
paper. Since set parameters led spectacular improvement accuracy
tested networks, fair say superiority AIS-BN algorithm
algorithms sensitive values parameters.
SIS algorithm, wk = 1 design algorithm. used l = 2, 500.
updating function Step 7 Figure 1 (Shwe et al., 1991; Cousins, Chen, &
Frisse, 1993):
Prknew (xi |pa(Xi ), e) =

c
Pr(xi |pa(Xi )) + k Pr
current (xi |pa(Xi ), e) ,
1+k

c
Pr(xi |pa(Xi )) original sampling distribution, Pr
current (xi |pa(Xi ), e)
equivalent ICPT tables estimator based currently available information,
k updating step.

4.2 Results CPCS Network
main network used tests subset CPCS (Computer-based Patient Case
Study) model (Pradhan et al., 1994), large multiply-connected multi-layer network consisting 422 multi-valued nodes covering subset domain internal medicine.
172

fiAdaptive Importance Sampling Bayesian Networks

Among 422 nodes, 14 nodes describe diseases, 33 nodes describe history risk factors, remaining 375 nodes describe various findings related diseases.
CPCS network among largest real networks available research community
present time. CPCS network contains many extreme probabilities, typically
order 104 . analysis based subset 179 nodes CPCS network,
created Max Henrion Malcolm Pradhan. used smaller version order
able compute exact solution purpose measuring approximation error
sampling algorithms.
AIS-BN algorithm learning overhead. following comparison execution time vs. number samples may give reader idea overhead. Updating
CPCS network 20 evidence nodes system takes AIS-BN algorithm
total 8.4 seconds learn. generates subsequently 3,640 samples per second,
SIS algorithm generates 2,631 samples per second, LW algorithm generates 4,167
samples per second. order remain conservative towards AIS-BN algorithm,
experiments fixed execution time algorithms (our limit 60 seconds)
rather number samples. CPCS network 20 evidence nodes, 60
seconds, AIS-BN generates 188,000 samples, SIS generates 158,000 samples
LW generates 250,000 samples.

12
100%
90%

10

80%

Frequency

8

70%
60%

6

50%
40%

4
30%
20%

2

10%
0

0%
1E-40

1E-34

1E-28

1E-22

1E-16

1E-10

Probability evidence

Figure 4: probability distribution evidence Pr(E = e) experiments.
generated total 75 test cases consisting five sequences 15 test cases each.
ran test case 10 times, time different setting random number seed.
sequence progressively higher number evidence nodes: 15, 20, 25, 30,
35 evidence nodes respectively. evidence nodes chosen randomly (equiprobable
sampling without replacement) nodes described various plausible medical
173

fiCheng & Druzdzel

findings. Almost nodes leaf nodes network. believe
constituted realistic test cases algorithms. distribution prior probability evidence, Pr(E = e), across test runs experiments shown Figure 4.
least likely evidence 5.54 1042 , likely evidence 1.37 109 ,
median 7 1024 .

0.30

AIS-BN

SIS

LW

Mean Square Error

0.25

0.20

0.15

0.10

0.05

0.00
15

30

45

60

75

90

105

120

135

150

Sample time (seconds)

Figure 5: typical plot convergence tested sampling algorithms experiments
Mean Square Error function execution time subset
CPCS network 20 evidence nodes chosen randomly among plausible medical
observations (Pr(E = e) = 3.33 1026 particular case) AIS-BN,
SIS, LW algorithms. curve AIS-BN algorithm
close horizontal axis.

Figures 5 6 show typical plot convergence tested sampling algorithms
experiments. case illustrated involves updating CPCS network 20 evidence
nodes. plot MSE initial 15 seconds algorithms start
converging. particular, learning step AIS-BN algorithm usually completed
within first 9 seconds. ran three algorithms case 150 seconds rather
60 seconds actual experiment order able observe wider range
convergence. plot MSE AIS-BN algorithm almost touches X axis
Figure 5. Figure 6 shows plot finer scale order show detail
AIS-BN convergence curve. clear AIS-BN algorithm dramatically improves
convergence rate. see results AIS-BN converge exact results
fast sampling time increases. case captured Figures 5 6, tenfold
increase sampling time (after subtracting overhead AIS-BN algorithm,
174

fiAdaptive Importance Sampling Bayesian Networks

0.0025

AIS-BN

Mean Square Error

0.0020

0.0015

0.0010

0.0005

0.0000
15

30

45

60

75

90

105

120

135

150

Sample time (seconds)

Figure 6: lower part plot Figure 5 showing convergence AIS-BN
algorithm correct posterior probabilities.

corresponds 21.5-fold increase number samples) results 4.55-fold decrease
MSE (to MSE 0.00048). observed convergence SIS LW algorithms
poor. tenfold increase sampling time practically effect accuracy. Please
note typical case observed experiments.

Absent
Mild
Moderate
Severe

Original CPT
0.99631
0.00183
0.00093
0.00093

Exact ICPT
0.0037
0.1560
0.1190
0.7213

Learned ICPT
0.015
0.164
0.131
0.690

Table 1: fragment conditional probability table node CPCS network
(node gasAcute, parents hepAcute=Mild wbcTotTho=False) Figure 6.

Figure 7 illustrates ICPT learning process AIS-BN algorithm sample
case shown Figure 6. displayed conditional probabilities belong node gasAcute
parent two evidence nodes, difInfGasMuc abdPaiExaMea. node
gasAcute four states: absent, mild, moderate, severe, two parents.
randomly chose combination parents states displayed configuration.
original CPT configuration without evidence, exact ICPT evidence
learned ICPT evidence summarized numerically Table 1. Figure 7 illustrates
175

fiCheng & Druzdzel

0.8

Absent

Mild

Moderate

Severe

0.7

Probability

0.6
0.5
0.4
0.3
0.2
0.1
0
0

1

2

3

4

5

6

7

8

9

10

Updating step

Figure 7: Convergence conditional probabilities example run AISBN algorithm captured Figure 6. displayed fragment conditional
probability table belongs node gasAcute parent one evidence
nodes.

learned importance conditional probabilities begin converge exact results
stably three updating steps. learned probabilities Step 10 close
exact results. example, difference Pr(xi |pa(Xi ), e) Pr(xi |pa(Xi ))
large. Sampling Pr(xi |pa(Xi )) instead Pr(xi |pa(Xi ), e) would introduce large
variance results.



min
median
max

AIS-BN
0.00082
0.00022
0.00049
0.00078
0.00184

SIS
0.110
0.076
0.0016
0.105
0.316

LW
0.148
0.093
0.0031
0.154
0.343

Table 2: Summary simulation results 75 simulation cases CPCS
network. Figure 8 shows 75 cases graphically.

Figure 8 shows MSE 75 test cases experiments summary
statistics Table 2. paired one-tailed t-test resulted statistically highly significant
differences AIS-BN SIS algorithms (p < 3.1 1020 ),
176

fiAdaptive Importance Sampling Bayesian Networks

1

AIS-BN

SIS

LW

Mean Square Error

0.1

0.01

0.001

0.0001
1.4E-09 2.1E-14 3.1E-18 5.7E-22 6.7E-24 1.4E-27 1.3E-32 3.8E-39

Probability evidence

Figure 8: Performance AIS-BN, SIS, LW algorithms: Mean Square Error
75 individual test cases plotted probability evidence.
sampling time 60 seconds.

SIS LW algorithms (p < 1.7 108 ). far magnitude difference
concerned, AIS-BN significantly better SIS. SIS better LW,
difference small. mean MSEs SIS LW algorithms greater
0.1, suggests neither algorithms suitable large Bayesian networks.
graph Figure 9 shows MSE ratio AIS-BN SIS algorithms.
see percentage cases whose ratio greater 100 (two orders
magnitude improvement!) 60%. words, obtained two orders magnitude
improvement MSE half cases. 80% cases, ratio greater
50. smallest ratio experiments 2.67, happened posterior
probabilities dominated prior probabilities. case, even though LW
SIS algorithms converged fast, MSE still far larger AIS-BN.
next experiment aimed showing close AIS-BN algorithm approach
best possible sampling results. know optimal importance sampling function,
convergence AIS-BN algorithm forward sampling
without evidence. words, results probabilistic logic sampling algorithm
without evidence approach limit well stochastic sampling perform. ran
logic sampling algorithm CPCS network without evidence mimicking test
runs AIS-BN algorithm, i.e., 5 blocks 15 runs, repeated 10 times
different random number seed. number samples generated equal average
number samples generated AIS-BN algorithm series 15 test runs.
177

fiCheng & Druzdzel

18

100%

16

90%
80%

14

70%

Frequency

12

60%
10
50%
8
40%
6

30%

4

20%

2
0

10%



fffiff

0%

ratio MSE SIS AIS-BN

Figure 9: ratio MSE SIS AIS-BN versus percentage.

obtained average MSE = 0.00057, = 0.000025, min = 0.00052,
max = 0.00065. best results around range. Table 2,
see minimum MSE AIS-BN algorithm 0.00049, within range
optimal result. mean MSE AIS-BN 0.00082, far optimal
results. standard deviation, , significantly larger AIS-BN algorithm,
understandable given process learning optimal importance function
heuristic nature. difficult understand exist difference
AIS-BN results optimal results. First, AIS-BN algorithm tests updated
sampling distribution 10 times, may times let converge
optimal importance distribution. Second, even algorithm converged
optimal importance distribution, sampling algorithm still let parameter
oscillate around distribution always small differences two
distributions.
Figure 10 shows convergence rate tested cases four-fold increase
sampling time (between 15 60 seconds). adjusted convergence ratio
AIS-BN algorithm dividing constant. According Equation 3, theoretically
expected convergence ratio four-fold increase number samples
around two. 96% cases among AIS-BN runs whose ratio lays
interval (1.75, 2.25], sharp contrast 11% 13% cases SIS LW
algorithms. ratios remaining 4% cases AIS-BN lay interval [2.25, 2.5].
SIS LW algorithms, percentage cases whose ratio smaller 1.5
71% 77% respectively. Less 1.5 means number samples
small estimate variance results cannot trusted. ratio greater 2.25
178

fiAdaptive Importance Sampling Bayesian Networks

70%

AIS-BN

SIS

LW
59%

60%

Frequency

50%

40%

37%

36%
35%

30%
20%

20%

20%
19%

11%

9%9%

10%
5%

8%
5%

5%5%

3%
0%

0%

4%

3%

3%

4%

0%

0%
0.5 - 0.75 0.75 - 1.0 1.0 - 1.25 1.25 - 1.5 1.5 - 1.75 1.75 - 2.0 2.0 - 2.25 2.25 - 2.5 2.5 - 2.75 2.75 - 3.0



Convergence rate

Figure 10: distribution convergence ratio AIS-BN, SIS, LW algorithms number samples increases four times.

means possibly 60 seconds long enough estimate variance, 15 seconds
short.
4.3 Role AIS-BN Heuristics Performance Improvement
experimental results see AIS-BN algorithm improve
sampling performance significantly. next series tests focused studying role
two AIS-BN initialization heuristics. first initializing ICPT tables
parents evidence uniform distributions, denoted U. second adjusting small
probabilities, denoted S. denote AIS-BN without heuristic initialization method
AIS algorithm. AIS+U+S equals AIS-BN. compared following versions
algorithms: SIS, AIS, SIS+U, AIS+U, SIS+S, AIS+S, SIS+U+S, AIS+U+S.
algorithms SIS used number samples SIS. algorithms AIS used
number samples AIS-BN. tested algorithms 75 test
cases used previous experiment. Figure 11 shows MSE sampling
algorithms summary statistics Table 3. Even though AIS algorithm better
SIS algorithm, difference large case AIS+U, AIS+S,
AIS-BN algorithms. seems heuristic initialization methods help much. results
SIS+S, SIS+U, SIS+U+S algorithms suggest although heuristic initialization
methods improve performance, alone cannot improve much. fair say
significant performance improvement AIS-BN algorithm coming
combination AIS heuristic methods, method alone. difficult
179

fiCheng & Druzdzel

understand that, good heuristic initialization methods possible let
learning process quickly exit oscillation areas. Although U methods alone
improve performance, improvement moderate compared combination
two.

0.12

0.110

Mean Square Error

0.10

0.075

0.08

0.060
0.06

0.050

0.050

0.04

0.02

0.008
0.00151
0.00







fiff

0.00082


Different Algorithms

Figure 11: comparison different algorithms CPCS network. bar based
75 test cases. dotted bar shows MSE SIS algorithm
gray bar shows MSE AIS algorithm.



min
median
max

SIS
0.110
0.076
0.0016
0.105
0.316

AIS
0.060
0.049
0.00074
0.045
0.207

SIS+U
0.050
0.052
0.0011
0.031
0.212

AIS+U
0.0084
0.025
0.00058
0.0014
0.208

SIS+S
0.075
0.074
0.00072
0.052
0.279

AIS+S
0.0015
0.0016
0.00056
0.00087
0.0085

SIS+U+S
0.050
0.059
0.00086
0.028
0.265

AIS-BN
0.00082
0.00022
0.00049
0.00078
0.0018

Table 3: Summary simulation results different algorithms CPCS network.

4.4 Results Networks
order make sure AIS-BN algorithm performs well general, tested
two large networks.
first network used tests PathFinder network (Heckerman
et al., 1990), core element expert system assists surgical pathologists
180

fiAdaptive Importance Sampling Bayesian Networks

diagnosis lymph-node diseases. two versions network. used
larger version, consisting 135 nodes. contrast CPCS network, PathFinder
contains many conditional probabilities equal 1, reflects deterministic
relationships certain settings. make sampling challenging, randomly selected
20 evidence nodes among leaf nodes. observable node (David
Heckerman, personal communication). verified case probability
selected evidence equal zero.
fixed execution time algorithms 60 seconds. learning overhead
AIS-BN algorithm PathFinder network 3.5 seconds. 60
seconds, AIS-BN generated 366,000 samples, SIS generated 250,000 samples
LW generated 2,700,000 samples. reason LW could generate
10 times many samples SIS within amount time LW algorithm
terminates sample generation early stage many samples, weight
sample becomes zero. result determinism probability tables, mentioned
above. see LW benefits greatly generating samples.
parameters used AIS-BN used CPCS network.
tested 20 cases, randomly selected 20 evidence nodes. reported MSE
case averaged 10 runs. runs SIS LW algorithms
manage generate effective samples (the weight score sum equal zero). SIS
75% effective runs LW 89% effective runs, means
runs SIS LW unable yield information posterior distributions.
cases, discarded run averaged effective runs.
runs AIS-BN algorithm effective. report experimental results
summary statistics Table 4. data, see AIS-BN algorithm
still significantly better SIS LW algorithms. Since LW algorithm
generate ten times number samples SIS algorithm, performance
better SIS algorithm.



min
median
max
effective runs

AIS-BN
0.00050
0.00037
0.00025
0.00037
0.0017
200

SIS
0.166
0.107
0.00116
0.184
0.467
150

LW
0.089
0.0707
0.00080
0.0866
0.294
178

Table 4: Summary simulation results 20 simulation cases
PathFinder network.

second network tested one ANDES networks (Conati et al.,
1997). ANDES intelligent tutoring system classical Newtonian physics
developed team researchers Learning Research Development Center
University Pittsburgh researchers United States Naval Academy.
student model ANDES uses Bayesian network longterm knowledge assessment,
181

fiCheng & Druzdzel

plan recognition, prediction students actions problem solving. selected
largest ANDES network available us, consisting 223 nodes.
contrast previous two networks, depth ANDES network significantly larger connectivity. 22 leaf nodes. quite
predictable kind networks pose difficulties learning. selected 20
evidence nodes randomly potential evidence nodes tested 20 cases. parameters used CPCS network. fixed execution time
algorithms 60 seconds. learning overhead AIS-BN algorithm
ANDES network 13.4 seconds. 60 seconds, AIS-BN generated 114,000
samples, SIS generated 98,000 samples LW generated 180,000 samples.
network, LW still generate almost two times number samples generated
SIS algorithm.
report experimental results summary statistics Table 5. results
show ANDES network AIS-BN algorithm significantly better
SIS LW algorithms. Since LW generated almost two times number samples
generated SIS algorithm, performance better SIS
algorithm.



min
median
max

AIS-BN
0.0059
0.0049
0.0023
0.0045
0.0237

SIS
0.0628
0.102
0.0028
0.0190
0.321

LW
0.0404
0.0539
0.0028
0.0198
0.221

Table 5: Summary simulation results 20 simulation cases ANDES
network.

AIS-BN algorithm average order magnitude precise
two algorithms, performance improvement smaller
two networks. reason performance improvement AIS-BN algorithm
SIS LW algorithms ANDES network smaller compared
CPCS PathFinder networks that: (1) ANDES network used tests
apparently challenging enough sampling algorithms general. ANDES
network, SIS LW perform well cases. minimum MSE SIS
LW tested cases almost AIS-BN. (2) number samples
generated AIS-BN network significantly smaller previous
two networks AIS-BN needs time learn. Although increasing number
samples improve performance three algorithms, improves performance
AIS-BN since convergence ratio AIS-BN algorithm usually larger
SIS LW (see Figure 10). (3) parameters used network
tuned CPCS network. (4) large depth fewer leaf nodes ANDES
network pose difficulties learning.
182

fiAdaptive Importance Sampling Bayesian Networks

5. Discussion
fundamental trade-off AIS-BN algorithm time spent
learning importance function time spent sampling. current approach,
believe reasonable, stop learning point importance
function good enough. experiments stopped learning 10 iterations.
several ways improving initialization conditional probability
tables outset AIS-BN algorithm. current version algorithm,
initialize ICPT table every parent N evidence node E (N Pa(E), E E)
uniform distribution Pr(E = e) < 1/(2 nE ). improved further.
extend initialization nodes severely affected evidence.
identified examining network structure local CPTs.
view learning process AIS-BN algorithm network rebuilding
process. algorithm constructs new network whose structure original
network (except delete evidence nodes corresponding arcs). constructed
network models joint probability distribution (X\E) Equation 8, approaches
optimal importance function. use learned 0 approximate distribution.
0 approximates Pr(X|E) accurately enough, use new network solve
approximate tasks, problem computing Maximum A-Posterior assignment
(MAP) (Pearl, 1988), finding k likely scenarios (Seroussi & Golmard, 1994), etc.
large advantage approach solve problems network
evidence nodes.
know Markov blanket scoring improve convergence rates sampling
algorithms (Shwe & Cooper, 1991). may applied AIS-BN algorithm
improve convergence rate. According Property 4 (Section 2.1), technique
2
c
reduce variance Pr
reduce variance Pr(e)
correspondingly improve
(e)
sampling performance. Since variance stratified sampling (Rubinstein, 1981)
never much worse random sampling, much better, improve
convergence rate. expect variance reduction methods statistics, as:
(i) expected value random variable; (ii) antithetic variants correlations (stratified
sampling, Latin hypercube sampling, etc.); (iii) systematic sampling, improve
sampling performance.
Current learning algorithm used simple approach. heuristic learning methods,
adjusting learning rates according changes error (Jacobs, 1988),
applicable algorithm. several tunable parameters AIS-BN
algorithm. Finding optimal values parameters given network another
interesting research topic.
worth observing plots presented Figure 8 fairly flat. words,
tests convergence sampling algorithms depend strongly
probability evidence. seems contradict common belief forward sampling
schemes suffer unlikely evidence. AIS-BN one shows fairly flat plot.
convergence SIS LW algorithms seems decrease slightly unlikely evidence.
possible three algorithms perform much worse probability
evidence drops threshold value, tests failed approach.
183

fiCheng & Druzdzel

relationship studied carefully, conjecture probability evidence
good measure difficulty approximate inference.
Given problem approximating probabilistic inference NP-hard, exist
networks challenging algorithm doubt even
AIS-BN algorithm perform poorly them. day, found
networks. one characteristic networks may challenging AIS-BN
algorithm. general, number parameters need learned AISBN algorithm increases, performance deteriorate. Nodes many parents,
example, challenging AIS-BN learning algorithm, update ICPT
tables combinations parent nodes. possible conditional probability
distributions causal independence properties, Noisy-OR distributions (Pearl,
1988; Henrion, 1989; Diez, 1993; Srinivas, 1993; Heckerman & Breese, 1994), common
large practical networks, treated differently lead considerable savings
learning time.
One direction testing approximate algorithms, suggested us reviewer, use
large networks exact solution cannot computed all. case, one
try infer difference variance various stages algorithm whether
converging not. interesting idea worth exploring, especially
combined theoretical work stopping criteria line work Dagum
Luby (1997).

6. Conclusion
Computational complexity remains major problem application probability theory
decision theory knowledge-based systems. important develop schemes
improve performance updating algorithms even though theoretically demonstrated worst case remain NPhard, many practical cases may become tractable.
paper, studied importance sampling Bayesian networks. reviewing
important theoretical results related importance sampling finite-dimensional
integrals, proposed new algorithm importance sampling Bayesian networks
call adaptive importance sampling (AIS-BN). process learning optimal
importance function AIS-BN algorithm computationally intractable, based
theory importance sampling finite-dimensional integrals proposed several heuristics
seem work well practice. proposed heuristic methods initializing
importance function shown accelerate learning process, smooth learning method updating importance function using structural advantages Bayesian
networks, dynamic weighting function combining samples different stages
algorithm. methods help AIS-BN algorithm get fairly accurate
estimates posterior probabilities limited time. two applied heuristics,
adjustment small probabilities, seems lead largest improvement performance,
although largest decrease MSE achieved combination two heuristics
AIS-BN algorithm.
AIS-BN algorithm lead dramatic improvement convergence rates
large Bayesian networks evidence compared existing state art algorithms.
compared performance AIS-BN algorithm performance likelihood
184

fiAdaptive Importance Sampling Bayesian Networks

weighting self-importance sampling large practical model, CPCS network,
evidence unlikely 5.54 1042 typically 7 1.024 . experiments,
observed AIS-BN algorithm always better likelihood weighting selfimportance sampling 60% cases reached two orders magnitude
improvement accuracy. Tests performed two networks, PathFinder
ANDES, yielded similar results.
Although may exist approximate algorithms prove superior AISBN networks special structure distribution, AIS-BN algorithm simple
robust general evidential reasoning problems large multiply-connected Bayesian
networks.

Acknowledgments
thank anonymous referees several insightful comments led substantial
improvement paper. research supported National Science Foundation
Faculty Early Career Development (CAREER) Program, grant IRI9624629,
Air Force Office Scientific Research grants F496209710225 F49620001
0112. earlier version paper received 2000 School Information Sciences
Robert R. Korfhage Award, University Pittsburgh. Malcolm Pradhan Max Henrion
Institute Decision Systems Research shared us CPCS network kind
permission developers Internist system University Pittsburgh.
thank David Heckerman PathFinder network Abigail Gerner ANDES
network used tests. experimental data obtained using SMILE,
Bayesian inference engine developed Decision Systems Laboratory available
http://www2.sis.pitt.edu/genie.

References
Cano, J. E., Hernandez, L. D., & Moral, S. (1996). Importance sampling algorithms
propagation probabilities belief networks. International Journal Approximate
Reasoning, 15, 7792.
Chavez, M. R., & Cooper, G. F. (1990). randomized approximation algorithm probabilistic inference Bayesian belief networks. Networks, 20 (5), 661685.
Cheng, J., & Druzdzel, M. J. (2000a). Computational investigations low-discrepancy
sequences simulation algorithms Bayesian networks. Proceedings Sixteenth Annual Conference Uncertainty Artificial Intelligence (UAI2000), pp.
7281 San Francisco, CA. Morgan Kaufmann Publishers.
Cheng, J., & Druzdzel, M. J. (2000b). Latin hypercube sampling Bayesian networks.
Proceedings 13th International Florida Artificial Intelligence Research Symposium Conference (FLAIRS-2000), pp. 287292 Orlando, Florida.
Conati, C., Gertner, A. S., VanLehn, K., & Druzdzel, M. J. (1997). On-line student modeling
coached problem solving using Bayesian networks. Proceedings Sixth
185

fiCheng & Druzdzel

International Conference User Modeling (UM96), pp. 231242 Vienna, New York.
Springer Verlag.
Cooper, G. F. (1990). computational complexity probabilistic inference using Bayesian belief networks. Artificial Intelligence, 42 (23), 393405.
Cousins, S. B., Chen, W., & Frisse, M. E. (1993). tutorial introduction stochastic
simulation algorithm belief networks. Artificial Intelligence Medicine, chap. 5,
pp. 315340. Elsevier Science Publishers B.V.
Dagum, P., Karp, R., Luby, M., & Ross, S. (1995). optimal algorithm Monte
Carlo estimation (extended abstract). Proceedings 36th IEEE Symposium
Foundations Computer Science, pp. 142149 Portland, Oregon.
Dagum, P., & Luby, M. (1993). Approximating probabilistic inference Bayesian belief
networks NP-hard. Artificial Intelligence, 60 (1), 141153.
Dagum, P., & Luby, M. (1997). optimal approximation algorithm Bayesian inference.
Artificial Intelligence, 93, 127.
Diez, F. J. (1993). Parameter adjustment Bayes networks. generalized noisy ORgate. Proceedings Ninth Annual Conference Uncertainty Artificial
Intelligence (UAI93), pp. 99105 San Francisco, CA. Morgan Kaufmann Publishers.
Fishman, G. S. (1995). Monte Carlo: concepts, algorithms, applications. SpringerVerlag.
Fung, R., & Chang, K.-C. (1989). Weighing integrating evidence stochastic simulation Bayesian networks. Uncertainty Artificial Intelligence 5, pp. 209219
New York, N. Y. Elsevier Science Publishing Company, Inc.
Fung, R., & del Favero, B. (1994). Backward simulation Bayesian networks. Proceedings
Tenth Annual Conference Uncertainty Artificial Intelligence (UAI94),
pp. 227234 San Francisco, CA. Morgan Kaufmann Publishers.
Geman, S., & Geman, D. (1984). Stochastic relaxations, Gibbs distributions Bayesian restoration images. IEEE Transactions Pattern Analysis Machine
Intelligence, 6 (6), 721742.
Gilks, W., Richardson, S., & Spiegelhalter, D. (1996). Markov chain Monte Carlo practice. Chapman Hall.
Heckerman, D., & Breese, J. S. (1994). new look causal independence. Proceedings
Tenth Annual Conference Uncertainty Artificial Intelligence (UAI94),
pp. 286292 San Mateo, CA. Morgan Kaufmann Publishers, Inc.
Heckerman, D. E., Horvitz, E. J., & Nathwani, B. N. (1990). Toward normative expert
systems: Pathfinder project. Tech. rep. KSL9008, Medical Computer Science
Group, Section Medical Informatics, Stanford University, Stanford, CA.
186

fiAdaptive Importance Sampling Bayesian Networks

Henrion, M. (1988). Propagating uncertainty Bayesian networks probabilistic logic
sampling. Uncertainty Artificial Intellgience 2, pp. 149163 New York, N. Y.
Elsevier Science Publishing Company, Inc.
Henrion, M. (1989). practical issues constructing belief networks. Kanal, L.,
Levitt, T., & Lemmer, J. (Eds.), Uncertainty Artificial Intelligence 3, pp. 161173.
Elsevier Science Publishers B.V., North Holland.
Henrion, M. (1991). Search-based methods bound diagnostic probabilities large
belief nets. Proceedings Seventh Annual Conference Uncertainty Artificial Intelligence (UAI91), pp. 142150 San Mateo, California. Morgan Kaufmann
Publishers.
Hernandez, L. D., Moral, S., & Antonio, S. (1998). Monte Carlo algorithm probabilistic
propagation belief networks based importance sampling stratified simulation
techniques. International Journal Approximate Reasoning, 18, 5391.
Jacobs, R. A. (1988). Increased rates convergence learning rate adaptation.
Neural Networks, 1, 295307.
Lauritzen, S. L., & Spiegelhalter, D. J. (1988). Local computations probabilities
graphical structures application expert systems. Journal Royal
Statistical Society, Series B (Methodological), 50 (2), 157224.
MacKay, D. (1998). Intro Monte Carlo methods. Jordan, M. I. (Ed.), Learning
Graphical Models. MIT Press, Cambridge, Massachusetts.
Ortiz, L. E., & Kaelbling, L. P. (2000). Adaptive importance sampling estimation
structured domains. Proceedings Sixteenth Annual Conference Uncertainty Artificial Intelligence (UAI2000), pp. 446454 San Francisco, CA. Morgan
Kaufmann Publishers.
Pearl, J. (1986). Fusion, propagation, structuring belief networks. Artificial Intelligence, 29 (3), 241288.
Pearl, J. (1987). Evidential reasoning using stochastic simulation causal models. Artifical
Intelligence, 32, 245257.
Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems: Networks Plausible
Inference. Morgan Kaufmann Publishers, Inc., San Mateo, CA.
Pradhan, M., & Dagum, P. (1996). Optimal Monte Carlo inference. Proceedings
Twelfth Annual Conference Uncertainty Artificial Intelligence (UAI96), pp.
446453 San Francisco, CA. Morgan Kaufmann Publishers.
Pradhan, M., Provan, G., Middleton, B., & Henrion, M. (1994). Knowledge engineering
large belief networks. Proceedings Tenth Annual Conference Uncertainty Artificial Intelligence (UAI94), pp. 484490 San Francisco, CA. Morgan
Kaufmann Publishers.
187

fiCheng & Druzdzel

Ritter, H., Martinetz, T., & Schulten, K. (1991). Neuronale Netze. Addison-Wesley,
Munchen.
Rubinstein, R. Y. (1981). Simulation Monte Carlo Method. John Wiley & Sons.
Seroussi, B., & Golmard, J. L. (1994). algorithm directly finding K probable
configurations Bayesian networks. International Journal Approximate Reasoning,
11, 205233.
Shachter, R. D., & Peot, M. A. (1989). Simulation approaches general probabilistic
inference belief networks. Uncertainty Artificial Intelligence 5, pp. 221231
New York, N. Y. Elsevier Science Publishing Company, Inc.
Shwe, M. A., & Cooper, G. F. (1991). empirical analysis likelihood-weighting simulation large, multiply-connected medical belief network. Computers Biomedical
Research, 24 (5), 453475.
Shwe, M., Middleton, B., Heckerman, D., Henrion, M., Horvitz, E., & Lehmann, H. (1991).
Probabilistic diagnosis using reformulation INTERNIST1/QMR knowledge
base: I. probabilistic model inference algorithms. Methods Information
Medicine, 30 (4), 241255.
Srinivas, S. (1993). generalization noisy-OR model. Proceedings Ninth
Annual Conference Uncertainty Artificial Intelligence (UAI93), pp. 208215
San Francisco, CA. Morgan Kaufmann Publishers.
York, J. (1992). Use Gibbs sampler expert systems. Artificial Intelligence, 56,
115130.

188


