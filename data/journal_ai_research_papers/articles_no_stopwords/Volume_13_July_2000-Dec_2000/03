journal artificial intelligence

submitted published

ais bn adaptive importance sampling
evidential reasoning large bayesian networks
jian cheng
marek j druzdzel

jcheng sis pitt edu
marek sis pitt edu

decision systems laboratory
school information sciences intelligent systems program
university pittsburgh pittsburgh pa usa

abstract
stochastic sampling attractive alternative exact
large bayesian network observed perform poorly evidential
reasoning extremely unlikely evidence address propose adaptive importance sampling ais bn shows promising convergence rates
even extreme conditions seems outperform existing sampling
consistently three sources performance improvement two heuristics
initialization importance function theoretical properties importance sampling finite dimensional integrals structural advantages bayesian
networks smooth learning method importance function dynamic
weighting function combining samples different stages
tested performance ais bn along two state art
general purpose sampling likelihood weighting fung chang shachter
peot self importance sampling shachter peot used
tests three large real bayesian network available scientific community
cpcs network pradhan et al pathfinder network heckerman horvitz
nathwani andes network conati gertner vanlehn druzdzel
evidence unlikely ais bn performed
better two majority test cases achieved orders
magnitude improvement precision improvement speed given desired
precision even dramatic although unable report numerical
almost never achieved precision reached even first
iterations ais bn

introduction
bayesian networks pearl increasingly popular tools modeling uncertainty
intelligent systems practical reaching size several hundreds variables
e g pradhan et al conati et al becomes increasingly important address feasibility probabilistic inference even though several ingenious
exact proposed large stumble theoretically demonstrated np hardness inference cooper significance
observed practice exact applied large densely connected
practical networks require prohibitive amount memory prohibitive amount
computation unable complete approximating inference desired
precision shown np hard well dagum luby comc

ai access foundation morgan kaufmann publishers rights reserved

ficheng druzdzel

plex networks alternative produce furthermore
obtaining crucial applications precision guarantees may critical
types traded speed computation
prominent subclass approximate family stochastic sampling
called stochastic simulation monte carlo precision
obtained stochastic sampling generally increases number samples generated
fairly unaffected network size execution time fairly independent
topology network linear number samples computation
interrupted time yielding anytime property important timecritical applications
stochastic sampling performs well predictive inference diagnostic reasoning e reasoning observed evidence nodes ancestors network often
exhibits poor convergence number observations increases especially
observations unlikely priori stochastic sampling often fails converge reasonable estimates posterior probabilities although known since
first sampling proposed henrion little done
address effectively furthermore sampling proposed tested
simple small networks networks special topology without presence
extremely unlikely evidence practical significance underestimated given typical number samples used real time feasible
todays hardware say samples behavior stochastic sampling
drastically different different size networks network consisting nodes
observations may possible converge exact probabilities large
networks negligibly small fraction total sample space probed one
practical bayesian network used tests subset cpcs
network pradhan et al consists nodes total sample space larger
samples sample fraction sample space
believe crucial study feasibility convergence properties
sampling large practical networks develop sampling good convergence extreme yet practical conditions
evidential reasoning given extremely unlikely evidence small networks
updated existing exact precisely large networks
stochastic sampling useful likelihood evidence know
stochastic sampling generally perform well high henrion
important look cases evidence unlikely test
two existing state art stochastic sampling bayesian networks likelihood weighting fung chang shachter peot self importance sampling
shachter peot subset cpcs network extremely unlikely evidence exhibit similarly poor convergence rates propose
sampling call adaptive importance sampling bayesian networks
ais bn suitable evidential reasoning large multiply connected bayesian
networks ais bn importance sampling widely
applied method variance reduction simulation applied bayesian networks e g shachter peot demonstrate empirically three large
practical bayesian network ais bn consistently outperforms


fiadaptive importance sampling bayesian networks

two majority test cases achieved two orders
magnitude improvement convergence improvement speed given desired precision
even dramatic although unable report numerical
never achieved precision reached even first iterations
ais bn main sources improvement two heuristics
initialization importance function theoretical properties importance sampling finite dimensional integrals structural advantages bayesian
networks smooth learning method updating importance function
dynamic weighting function combining samples different stages
study value two heuristics used ais bn initialization
probability distributions parents evidence nodes uniform distribution
adjusting small probabilities conditional probability tables
play important role ais bn moderate role
existing
remainder structured follows section first gives general
introduction importance sampling domain finite dimensional integrals
originally proposed importance sampling used compute probabilities bayesian networks draw additional benefits graphical
structure network develop generalized sampling scheme aid us
reviewing previously proposed sampling describing ais bn
section describes ais bn propose two heuristics initialization importance function discuss theoretical foundations describe
smooth learning method importance function dynamic weighting function
combining samples different stages section describes empirical evaluation ais bn finally section suggests several possible
improvements ais bn possible applications learning scheme
directions future work

importance sampling bayesian networks
feel useful go back theoretical roots importance sampling order
able understand source speedup ais bn relative
existing state art importance sampling bayesian networks first
review general idea importance sampling finite dimensional integrals
reduce sampling variance discuss application importance sampling
bayesian networks readers interested details directed literature
monte carlo methods computation finite integrals excellent exposition
rubinstein essentially following first section
mathematical foundations
let g x function variables x x xm domain rm
computing g x x feasible consider approximate computation
integral
z


g x dx






ficheng druzdzel

importance sampling approaches writing integral
z




g x
f x dx
f x

f x often referred importance function probability density function
f x used importance sampling exists generating
samples f x importance function zero original function
zero e g x f x
independently sampled n points sn si according
probability density function f x estimate integral
n
x
g si

n f si



estimate variance
b



n
x

g si

n n f si









straightforward estimator following properties
e
limn

n
n normal f x
f x


z




g x

f x



f x dx





b f x n
e

variance proportional f x inversely proportional number
samples minimize variance increase number samples
try decrease f x respect latter rubinstein reports following
useful theorem corollary
theorem minimum f x equal
f x



z

g x dx







occurs x distributed according following probability density function
f x r

g x

g x dx



fiadaptive importance sampling bayesian networks

corollary g x optimal probability density function
f x

g x


f x
although practice sampling precisely f x g x occur rarely expect
functions close enough still reduce variance effectively usually
closer shape function f x shape function g x smaller
f x high dimensional integrals selection importance function f x far
critical increasing number samples since former dramatically
affect f x seems prudent put energy choosing importance function
whose shape close possible g x apply brute force method
increasing number samples
worth noting f x uniform importance sampling becomes general
monte carlo sampling another noteworthy property importance sampling
derived equation avoid f x g x f x part
domain sampling even f x matches well g x important regions
f x g x f x variance become large even infinite
avoid adjusting f x larger unimportant regions domain x
section discussed importance sampling continuous variables
stated valid discrete variables well case integration
substituted summation
generic importance sampling bayesian networks
following discussion random variables used multiple valued discrete variables
capital letters b c denote random variables bold capital letters
b c denote sets variables bold capital letter e usually used denote
set evidence variables lower case letters b c denote particular instantiations
variables b c respectively bold lower case letters b c denote
particular instantiations sets b c respectively bold lower case letter e
particular used denote observations e instantiations set evidence
variables e anc denotes set ancestors node pa denotes set
parents direct ancestors node pa denotes particular instantiation pa
denotes set difference pa e e denotes use extended vertical bar indicate
substitution e e
know joint probability distribution variables bayesian network model pr x product probability distributions nodes
conditional parents e
pr x

n


pr xi pa xi





order calculate pr e e need sum pr x e e e
pr e e

x

pr x e e e

x e





ficheng druzdzel

see equation almost identical equation except integration
replaced summation domain replaced x e theoretical
derived importance sampling reviewed previous section thus
directly applied computing probabilities bayesian networks
previous work importance sampling bayesian networks postpone discussion work next section
present generic stochastic sampling help us reviewing
prior work presenting
posterior probability pr e obtained first computing pr e pr e
combining definition conditional probability
pr e

pr e

pr e



order increase accuracy importance sampling computing posterior probabilities different network variables given evidence general use
different importance functions pr e pr e increases computation time linearly gain accuracy may significant given obtaining
desired accuracy exponential nature often common practice use
importance function usually pr e sample probabilities difference
order nodes according topological order
initialize importance function pr x e desired number samples
updating interval l score arrays every node
k



mod l



k k



update importance function prk x e
end



si generate sample according prk x e



si



calculate score si pr x e e prk x e add corresponding entry every score array according instantiated states
end

normalize score arrays every node
figure generic importance sampling


fiadaptive importance sampling bayesian networks

optimal importance functions two quantities large perforc
c
mance may deteriorate significantly although pr
e pr e
unbiased estimators
c
according property section pr e
obtained means equation
unbiased estimator however number samples increases bias decreases
ignored altogether sample size large enough fishman
figure presents generic stochastic sampling captures
existing sampling without loss generality restrict
description called forward sampling e generation samples topological
order nodes network forward sampling order accomplished
initialization performed step parents node placed node
forward sampling step actual generation samples works
follows evidence node instantiated observed state omitted
sample generation ii root node randomly instantiated one possible
states according importance prior probability node derived
prk x e iii node whose parents instantiated randomly instantiated
one possible states according importance conditional probability distribution
node given values parents derived prk x e iv
procedure followed nodes instantiated complete instantiation si
network method one sample joint importance probability distribution
prk x e variables network scoring step amounts calculating
pr si e prk si required equation ratio total score sum
number samples unbiased estimator pr e step count score
sum condition e unobserved variables values
ratio score sum number samples unbiased estimator
pr e
existing focus posterior probability distributions individual
nodes mentioned sake efficiency count score sum corresponding pr e x e record score array node
entry array corresponds specified state method introduces additional
variance opposed importance function derived prk x e sample
pr e x e directly
existing importance sampling bayesian networks
main difference stochastic sampling process
steps generic importance sampling figure
probabilistic logic sampling henrion simplest first proposed sampling bayesian networks importance function initialized step
pr x never updated step null without evidence pr x optimal importance function evidence set empty anyway escapes authors
pr x may optimal importance function pr x
root node mismatch optimal actually used importance
function may large variance sampling process evidence
without evidence except step count scores samples
inconsistent observed evidence amounts discarding


ficheng druzdzel

evidence unlikely large difference pr x optimal
importance function effectively samples discarded performance logic
sampling deteriorates badly
likelihood weighting lw fung chang shachter peot enhances
logic sampling never discards samples likelihood weighting importance
function step



pr x e
pr xi pa xi fifi

xi e




e e

likelihood weighting update importance function step although likelihood weighting improvement logic sampling convergence rate still
slow large difference optimal importance function pr x e
especially situations evidence unlikely simplicity
likelihood weighting commonly used simulation method
bayesian network inference often matches performance sophisticated
schemes simple able increase precision generating samples
amount time
backward sampling fung del favero changes step generic
allows generating samples evidence nodes direction opposite
topological order nodes network step backward sampling uses likelihood observed evidence instantiated nodes calculate pr x e
although fung del favero mentioned possibility dynamic node ordering
propose scheme updating importance function step backward
sampling suffers similar likelihood weighting e possible mismatch importance function optimal importance function
lead poor convergence
importance sampling shachter peot generic sampling shachter peot introduced two variants importance sampling self importance
sis heuristic importance importance function used first step
self importance




pr x e
pr xi pa xi fifi

xi e




e e

function updated step tries revise conditional probability
tables cpts periodically order make sampling distribution gradually
posterior distribution since data used update importance function
compute estimator process introduces bias estimator heuristic
importance first removes edges network becomes polytree
uses modified version polytree pearl compute likelihood
functions unobserved nodes pr x e combination likelihood
functions pr x e e step heuristic importance update prk x e
shachter peot point heuristic importance function still lead
bad approximation optimal importance function exist
combination self importance heuristic importance shachter peot


fiadaptive importance sampling bayesian networks

shwe cooper although researchers suggested may promising
direction work sampling seen would
follow
separate group stochastic sampling methods formed called markov chain
monte carlo mcmc methods divided gibbs sampling metropolis sampling
hybrid monte carlo sampling geman geman gilks richardson spiegelhalter mackay roughly speaking methods draw random samples
unknown target distribution f x biasing search distribution towards
higher probability regions applied bayesian networks pearl chavez
cooper determines sampling distribution variable
previous sample given markov blanket pearl corresponds updating
prk x e sampling every node prk x e converge optimal importance
function pr e pr x e satisfies ergodic properties york since
convergence limiting distribution slow calculating updates sampling distribution costly used practice often simple
likelihood weighting scheme
simulation bounded variance
dagum luby aa dagum et al essentially
lw stopping rule theorem dagum et al cano
et al proposed another importance sampling performed somewhat
better lw cases extreme probability distributions authors state
general cases produced similar likelihood weighting hernandez
et al applied importance sampling reported moderate improvement
likelihood weighting
practical performance existing sampling
largest network tested sampling qmr dt quick
medical reference decision theoretic shwe et al shwe cooper
contains adult diseases findings arcs depicting disease finding
dependencies qmr dt network belongs class special bipartite networks
structure often referred bn henrion two layer
composition disease nodes top layer finding nodes bottom layer shwe
colleagues used combining self importance heuristic importance
tested convergence properties qmr dt network since heuristic method
iterative tabular bayes itb makes use version bayes rule designed
bn networks cannot generalized arbitrary networks although shwe
colleagues concluded markov blanket scoring self importance sampling significantly
improve convergence rate model cannot extend conclusion general
networks computation markov blanket scoring complex general multiconnected network bn network experiments conducted lacked
gold standard posterior probability distribution could serve judge convergence
rate
pradhan dagum tested efficient version lw bounded
variance dagum luby aa dagum et al


ficheng druzdzel

node multiply connected medical diagnostic bayesian network one limitation
tests probability evidence cases selected testing rather
high although cases probability evidence order
smaller simple calculation reported mean number
evidence nodes shows average probability observed state evidence node
conditional direct predecessors order given
essentially lw tests suspect
performance deteriorate cases evidence unlikely
focus marginal probability one hypothesis node many
queried nodes efficiency may deteriorate
tested discussed section several large networks
experimental cases unlikely evidence none
converges reasonable estimates posterior probabilities within reasonable amount
time convergence becomes worse number evidence nodes increases thus
large networks simply cannot trust
present tests lw sis detail section

ais bn adaptive importance sampling bayesian networks
main reason existing stochastic sampling converge slowly
fail learn good importance function sampling process effectively
fail reduce sampling variance importance function optimal
probabilistic logic sampling without evidence capable
converging fairly good estimates posterior probabilities within relatively
samples example assuming posterior probabilities extreme e larger
say samples may sufficient obtain good estimates
section present adaptive importance sampling bayesian networks
ais bn demonstrate next section performs well
tests first describe details prove two theorems
useful learning optimal importance sampling function
basic ais bn
compared importance sampling used normal finite dimensional integrals importance sampling used bayesian networks several significant advantages first
network joint probability distribution pr x decomposable factored
component parts second network clear structure represents many conditional independence relationships properties helpful estimating
optimal importance function
basic ais bn presented figure main differences
ais bn basic importance sampling figure
introduce monotonically increasing weight function wk two effective heuristic
initialization methods step introduce special learning component step
let updating process run smoothly avoiding oscillation parameters


fiadaptive importance sampling bayesian networks

order nodes according topological order
initialize importance function pr x e heuristic methods initialize weight w set desired number samples updating
interval l initialize score arrays every node
k wt score wsum



mod l



k k



update importance function prk x e wk
end



si generate sample according prk x e



si



wiscore score si pr x e e prk x e wk



wt score wt score wiscore
optional add wiscore corresponding entry every score array



wsum wsum wk
end

output estimate pr e wt score wsum
optional normalize score arrays every node

figure adaptive importance sampling bayesian networks ais bn
score processing step
pr si e
wiscore wk k

pr si
note respect figure becomes special case ais bn
wk reason use wk want give different weights
sampling obtained different stages stage updates
importance function different distance optimal importance
b k
b k standard deviation estimated
function recommend wk

k
stage k equation order keep w monotonically increasing wk smaller
wk adjust value wk weighting scheme may introduce bias
similar weighting scheme variance apparently developed independently ortiz
kaelbling recommend weight wk
bk



ficheng druzdzel

final since initial importance sampling functions often inefficient
introduce big variance recommend wk first
stages designed weighting scheme reflect fact
practice estimates small estimated variance usually good estimates
modifying sampling distribution ais bn
theoretical considerations section know crucial element
converging good approximation optimal importance function
follows first give optimal importance function calculating pr e e
discuss use structural advantages bayesian networks approximate
function sequel use symbol denote importance sampling
function denote optimal importance sampling function
since pr x e e e corollary
x e

pr x e e e
pr x e e
pr e e

following corollary captures
corollary optimal importance sampling function x e calculating pr e e
equation pr x e e
although know mathematical expression optimal importance sampling
function difficult obtain function exactly use following
importance sampling function
x e

n


pr xi pa xi e





function partially considers effect evidence every node
sampling process network structure network
absorbed evidence function optimal importance sampling function
easy learn experimental good approximation
optimal importance sampling function theoretically posterior structure
model changes drastically observed evidence importance sampling
function may perform poorly tried practical networks would
happen day encountered drastic example effect
section know score sums corresponding xi pa xi e
yield unbiased estimator pr xi pa xi e according definition conditional
probability get estimator pr xi pa xi e achieved maintaining updating table every node structure mimicks structure
cpt tables allow us decompose importance function components learned individually call tables importance conditional
probability tables icpt
definition importance conditional probability table icpt node x table
posterior probabilities pr x pa x e e conditional evidence indexed
immediate predecessors pa x


fiadaptive importance sampling bayesian networks

icpt tables modified process learning importance function
prove useful theorem lead considerable savings learning
process
theorem
xi x xi
anc e pr xi pa xi e pr xi pa xi



proof suppose set values parents node xi pa xi node xi
dependent evidence e given pa xi xi connecting e given pa xi
pearl according definition connectivity happens
exists member xi descendants belongs set evidence nodes e
words xi
anc e

theorem important ais bn states essentially
icpt tables nodes ancestors evidence nodes equal
cpt tables throughout learning process need learn icpt tables
ancestors evidence nodes often lead significant savings
computation example evidence nodes root nodes icpt tables
every node already ais bn becomes identical likelihood weighting
without evidence ais bn becomes identical probabilistic
logic sampling
worth pointing xi pr xi pa xi e e icpt table
xi easily calculated exact methods example xi parent
evidence node ej ej child xi posterior probability distribution
xi straightforward compute exactly since focus current
input initialized importance function pr x e learning rate k
output estimated importance function prs x e
stage k
sample l points sk sk skl independently according current importance function prk x e
every node xi xi x e xi
anc e count score sums
corresponding xi pa xi e estimate pr xi pa xi e sk
sk skl
update prk x e according following formula
prk xi pa xi e




prk xi pa xi e k pr xi pa xi e prk xi pa xi e
end

figure ais bn learning optimal importance function


ficheng druzdzel

sampling test reported include improvement
ais bn
figure lists implements step basic ais bn listed
figure estimate pr xi pa xi e use samples obtained
current stage one reason information obtained previous stages
absorbed prk x e reason principle successive iteration
accurate previous one importance function closer optimal
importance function thus samples generated prk x e better
generated prk x e pr xi pa xi e prk xi pa xi e corresponds vector
first partial derivatives direction maximum decrease error k
positive function determines learning rate k lower bound
update importance function k upper bound stage
discard old function convergence speed directly related k small
convergence slow due large number updating steps needed
reach local minimum hand large convergence rate initially
fast eventually start oscillate thus may reach
minimum many papers field neural network learning discuss
choose learning rate let estimated importance function converge quickly
destination function method improve learning rate applicable
currently use following function proposed ritter et al
k kmax

k

b






initial learning rate b learning rate last step function
reported perform well neural network learning ritter et al
heuristic initialization ais bn
dimensionality bayesian network inference equal number
variables network networks considered high
learning space optimal importance function large choice
initial importance function pr x e important factor affecting learning
initial value importance function close optimal importance function
greatly affect speed convergence section present two heuristics
help achieve goal
due explicit encoding structure decomposable joint probability distribution bayesian networks offer computational advantages compared finite dimensional
integrals possible first approximation optimal importance function prior
probability distribution network variables pr x propose improvement
initialization know effect evidence nodes node attenuated
path length node evidence nodes increased henrion
affected nodes direct ancestors evidence nodes initializing icpt
tables parents evidence nodes uniform distributions experience improves convergence rate furthermore cpt tables parents evidence
node e may favorable observed state e probability e e without


fiadaptive importance sampling bayesian networks

condition less small value pr e e ne ne
number outcomes node e observation change cpt tables
parents evidence node e uniform distributions experiment
pr e e ne otherwise leave unchanged kind initialization
involves knowledge pr e e marginal probability without evidence probabilistic logic sampling henrion enhanced latin hypercube sampling cheng
druzdzel b quasi monte carlo methods cheng druzdzel produce
good estimate pr e e one time effort made model
building stage worth pursuing desired precision
another serious related sampling extremely small probabilities suppose
exists root node state prior probability pr let
posterior probability state given evidence pr e simple calculation
shows update importance function every samples expect
hit every updates thus ss convergence rate slow
overcome setting threshold replacing every probability p
network time subtract p largest probability
conditional probability distribution example value l l
updating interval allow us sample times often first stage
state turns likely large weight increase
probability even order converge correct answer faster considering
avoid f x g x f x unimportant region discussed
section need make threshold larger found convergence
rate quite sensitive threshold empirical tests suggest use
networks whose maximum number outcomes per node exceed five
smaller threshold might lead fast convergence cases slow convergence
others one threshold work changing specific network usually improve
convergence rate
selection parameters
several tunable parameters ais bn base choice
parameters central limit theorem clt according clt z z
zn independent identically distributed random variables e zi z
var zi z n z z zn n approximately normally distributed
n sufficiently large thus
lim p

n





fiz z

z


z

z n

ex dx


z




although approximation holds n approaches infinity clt known
robust lead excellent approximations even small n formula equation
r relative approximation estimate satisfies
p


r


initialization heuristic apparently developed independently ortiz kaelbling



ficheng druzdzel

fixed


z n

r

z
z


z z z ex dx since sampling z corresponding

pr e figure fixed setting r smaller value amounts letting z n

smaller adjust parameters z n estimated
bk
equation theoretical intuition behind recommendation wk
section expect work well networks guarantees
given exist extreme cases sampling
good estimate variance obtained
r



generalization ais bn estimating pr e
typical focus systems bayesian networks posterior probability
outcomes individual variables given evidence pr e generalized
computation posterior probability particular instantiation set variables
given evidence e pr e two methods capable performing
computation first method efficient expense precision second
method less efficient offers general better convergence rates methods
equation
first method reuses samples generated estimate pr e estimating pr e
estimation pr e amounts counting scored sum condition
main advantage method efficiency use set samples
estimate posterior probability state subset network given evidence
main disadvantage variance estimated pr e large especially
numerical value pr e extreme method widely used
existing stochastic sampling
second method used much rarely e g cano et al pradhan dagum
dagum luby calls estimating pr e pr e separately
estimating pr e additional call made instantiation
set variables interest pr e estimated sampling network
set observations e extended main advantage method
much better reducing variance first method main disadvantage
computational cost associated sampling possibly many combinations states
nodes interest
cano et al suggested modified version second method suppose
interested posterior distribution pr ai e possible values ai
k estimate pr ai e k separately use value
pk
pr ai e estimate pr e assumption behind
estimate pr e accurate large sample drawn
however even guarantee small variance every pr ai e cannot guarantee
sum small variance ais bn use
pure form methods listed figure first
method optional computations steps performed


fiadaptive importance sampling bayesian networks

corresponding second method skips optional steps calls basic ais bn
twice estimate pr e pr e separately
first method attractive simplicity possible computational
efficiency however shown section performance sampling uses one set samples first method estimate pr e
deteriorate difference optimal importance functions pr e
pr e large main focus computation high accuracy posterior probability distribution small number nodes strongly recommend use
second method easily used estimate confidence
intervals solution

experimental
section first describe experimental method used tests tests focus
cpcs network one largest realistic networks available
know precisely nodes observable therefore able
generate realistic test cases since ais bn uses two initialization
heuristics designed experiment studies contribution two
heuristics performance probe extent ais bn
excellent performance test several real large networks
experimental method
performed empirical tests comparing ais bn likelihood weighting
lw self importance sampling sis two basically
state art general purpose belief updating aa dagum et al
bounded variance dagum luby suggested
reviewer essentially enhanced special purpose versions basic lw
implementation three relied essentially code separate
functions differed fair assume therefore observed
differences purely due theoretical differences among due
efficiency implementation order make comparison ais bn
lw sis fair used first method computation section e one
relies single sampling rather calling basic ais bn twice
measured accuracy approximation achieved simulation terms
mean square error mse e square root sum square differences pr xij
pr xij sampled exact marginal probabilities state j j ni
node xi
e precisely
v
u
u
mse p



xi n e ni

x

ni
x

pr xij pr xij

xi n e j

n set nodes e set evidence nodes ni number
outcomes node diagrams reported mse averaged runs used
clustering lauritzen spiegelhalter compute gold standard


ficheng druzdzel

comparisons mean square error performed experiments
pentium ii mhz windows computer
mse perfect simplest way capturing error lends
theoretical analysis example possible derive analytically idealized
convergence rate terms mse turn used judge quality
mse used virtually previous tests sampling
allows interested readers tie current past studies reviewer offered
interesting suggestion cross entropy technique weights small
changes near zero much strongly equivalent size change middle
interval measure would penalize imprecisions possibly
several orders magnitude small probabilities idea interesting
aware theoretical reasons measure would make difference
comparisons ais bn lw sis mse mentioned
allow us compare empirically determined convergence rate theoretically
derived ideal convergence rate theoretically mse inversely proportional
square root sample size
since several tunable parameters used ais bn list
values parameters used test l wk k wk
otherwise stopped updating process step figure k
words used samples collected last step learning
parameters used kmax b see equation
used empirically determined value threshold section
change cpt tables parents special evidence node uniform distributions
pr na parameters matter design decision
e g number samples tests others chosen empirically although
found parameters may different optimal values different bayesian
networks used values tests ais bn described
since set parameters led spectacular improvement accuracy
tested networks fair say superiority ais bn
sensitive values parameters
sis wk design used l
updating function step figure shwe et al cousins chen
frisse
prknew xi pa xi e

c
pr xi pa xi k pr
current xi pa xi e
k

c
pr xi pa xi original sampling distribution pr
current xi pa xi e
equivalent icpt tables estimator currently available information
k updating step

cpcs network
main network used tests subset cpcs computer patient case
study model pradhan et al large multiply connected multi layer network consisting multi valued nodes covering subset domain internal medicine


fiadaptive importance sampling bayesian networks

among nodes nodes describe diseases nodes describe history risk factors remaining nodes describe findings related diseases
cpcs network among largest real networks available community
present time cpcs network contains many extreme probabilities typically
order analysis subset nodes cpcs network
created max henrion malcolm pradhan used smaller version order
able compute exact solution purpose measuring approximation error
sampling
ais bn learning overhead following comparison execution time vs number samples may give reader idea overhead updating
cpcs network evidence nodes system takes ais bn
total seconds learn generates subsequently samples per second
sis generates samples per second lw generates
samples per second order remain conservative towards ais bn
experiments fixed execution time limit seconds
rather number samples cpcs network evidence nodes
seconds ais bn generates samples sis generates samples
lw generates samples









frequency





















e

e

e

e

e

e

probability evidence

figure probability distribution evidence pr e e experiments
generated total test cases consisting five sequences test cases
ran test case times time different setting random number seed
sequence progressively higher number evidence nodes
evidence nodes respectively evidence nodes chosen randomly equiprobable
sampling without replacement nodes described plausible medical


ficheng druzdzel

findings almost nodes leaf nodes network believe
constituted realistic test cases distribution prior probability evidence pr e e across test runs experiments shown figure
least likely evidence likely evidence
median



ais bn

sis

lw

mean square error
































sample time seconds

figure typical plot convergence tested sampling experiments
mean square error function execution time subset
cpcs network evidence nodes chosen randomly among plausible medical
observations pr e e particular case ais bn
sis lw curve ais bn
close horizontal axis

figures typical plot convergence tested sampling
experiments case illustrated involves updating cpcs network evidence
nodes plot mse initial seconds start
converging particular learning step ais bn usually completed
within first seconds ran three case seconds rather
seconds actual experiment order able observe wider range
convergence plot mse ais bn almost touches x axis
figure figure shows plot finer scale order detail
ais bn convergence curve clear ais bn dramatically improves
convergence rate see ais bn converge exact
fast sampling time increases case captured figures tenfold
increase sampling time subtracting overhead ais bn


fiadaptive importance sampling bayesian networks



ais bn

mean square error






























sample time seconds

figure lower part plot figure showing convergence ais bn
correct posterior probabilities

corresponds fold increase number samples fold decrease
mse mse observed convergence sis lw
poor tenfold increase sampling time practically effect accuracy please
note typical case observed experiments

absent
mild
moderate
severe

original cpt





exact icpt





learned icpt





table fragment conditional probability table node cpcs network
node gasacute parents hepacute mild wbctottho false figure

figure illustrates icpt learning process ais bn sample
case shown figure displayed conditional probabilities belong node gasacute
parent two evidence nodes difinfgasmuc abdpaiexamea node
gasacute four states absent mild moderate severe two parents
randomly chose combination parents states displayed configuration
original cpt configuration without evidence exact icpt evidence
learned icpt evidence summarized numerically table figure illustrates


ficheng druzdzel



absent

mild

moderate

severe



probability






























updating step

figure convergence conditional probabilities example run aisbn captured figure displayed fragment conditional
probability table belongs node gasacute parent one evidence
nodes

learned importance conditional probabilities begin converge exact
stably three updating steps learned probabilities step close
exact example difference pr xi pa xi e pr xi pa xi
large sampling pr xi pa xi instead pr xi pa xi e would introduce large
variance



min
median
max

ais bn






sis






lw






table summary simulation simulation cases cpcs
network figure shows cases graphically

figure shows mse test cases experiments summary
statistics table paired one tailed test resulted statistically highly significant
differences ais bn sis p


fiadaptive importance sampling bayesian networks



ais bn

sis

lw

mean square error








e e e e e e e e

probability evidence

figure performance ais bn sis lw mean square error
individual test cases plotted probability evidence
sampling time seconds

sis lw p far magnitude difference
concerned ais bn significantly better sis sis better lw
difference small mean mses sis lw greater
suggests neither suitable large bayesian networks
graph figure shows mse ratio ais bn sis
see percentage cases whose ratio greater two orders
magnitude improvement words obtained two orders magnitude
improvement mse half cases cases ratio greater
smallest ratio experiments happened posterior
probabilities dominated prior probabilities case even though lw
sis converged fast mse still far larger ais bn
next experiment aimed showing close ais bn
best possible sampling know optimal importance sampling function
convergence ais bn forward sampling
without evidence words probabilistic logic sampling
without evidence limit well stochastic sampling perform ran
logic sampling cpcs network without evidence mimicking test
runs ais bn e blocks runs repeated times
different random number seed number samples generated equal average
number samples generated ais bn series test runs


ficheng druzdzel














frequency























fffiff



ratio mse sis ais bn

figure ratio mse sis ais bn versus percentage

obtained average mse min
max best around range table
see minimum mse ais bn within range
optimal mean mse ais bn far optimal
standard deviation significantly larger ais bn
understandable given process learning optimal importance function
heuristic nature difficult understand exist difference
ais bn optimal first ais bn tests updated
sampling distribution times may times let converge
optimal importance distribution second even converged
optimal importance distribution sampling still let parameter
oscillate around distribution small differences two
distributions
figure shows convergence rate tested cases four fold increase
sampling time seconds adjusted convergence ratio
ais bn dividing constant according equation theoretically
expected convergence ratio four fold increase number samples
around two cases among ais bn runs whose ratio lays
interval sharp contrast cases sis lw
ratios remaining cases ais bn lay interval
sis lw percentage cases whose ratio smaller
respectively less means number samples
small estimate variance cannot trusted ratio greater


fiadaptive importance sampling bayesian networks



ais bn

sis

lw




frequency


















































convergence rate

figure distribution convergence ratio ais bn sis lw number samples increases four times

means possibly seconds long enough estimate variance seconds
short
role ais bn heuristics performance improvement
experimental see ais bn improve
sampling performance significantly next series tests focused studying role
two ais bn initialization heuristics first initializing icpt tables
parents evidence uniform distributions denoted u second adjusting small
probabilities denoted denote ais bn without heuristic initialization method
ais ais u equals ais bn compared following versions
sis ais sis u ais u sis ais sis u ais u
sis used number samples sis ais used
number samples ais bn tested test
cases used previous experiment figure shows mse sampling
summary statistics table even though ais better
sis difference large case ais u ais
ais bn seems heuristic initialization methods help much
sis sis u sis u suggest although heuristic initialization
methods improve performance alone cannot improve much fair say
significant performance improvement ais bn coming
combination ais heuristic methods method alone difficult


ficheng druzdzel

understand good heuristic initialization methods possible let
learning process quickly exit oscillation areas although u methods alone
improve performance improvement moderate compared combination
two





mean square error




























fiff




different

figure comparison different cpcs network bar
test cases dotted bar shows mse sis
gray bar shows mse ais



min
median
max

sis






ais






sis u






ais u






sis






ais






sis u






ais bn






table summary simulation different cpcs network

networks
order make sure ais bn performs well general tested
two large networks
first network used tests pathfinder network heckerman
et al core element expert system assists surgical pathologists


fiadaptive importance sampling bayesian networks

diagnosis lymph node diseases two versions network used
larger version consisting nodes contrast cpcs network pathfinder
contains many conditional probabilities equal reflects deterministic
relationships certain settings make sampling challenging randomly selected
evidence nodes among leaf nodes observable node david
heckerman personal communication verified case probability
selected evidence equal zero
fixed execution time seconds learning overhead
ais bn pathfinder network seconds
seconds ais bn generated samples sis generated samples
lw generated samples reason lw could generate
times many samples sis within amount time lw
terminates sample generation early stage many samples weight
sample becomes zero determinism probability tables mentioned
see lw benefits greatly generating samples
parameters used ais bn used cpcs network
tested cases randomly selected evidence nodes reported mse
case averaged runs runs sis lw
manage generate effective samples weight score sum equal zero sis
effective runs lw effective runs means
runs sis lw unable yield information posterior distributions
cases discarded run averaged effective runs
runs ais bn effective report experimental
summary statistics table data see ais bn
still significantly better sis lw since lw
generate ten times number samples sis performance
better sis



min
median
max
effective runs

ais bn







sis







lw







table summary simulation simulation cases
pathfinder network

second network tested one andes networks conati et al
andes intelligent tutoring system classical newtonian physics
developed team researchers learning development center
university pittsburgh researchers united states naval academy
student model andes uses bayesian network longterm knowledge assessment


ficheng druzdzel

plan recognition prediction students actions solving selected
largest andes network available us consisting nodes
contrast previous two networks depth andes network significantly larger connectivity leaf nodes quite
predictable kind networks pose difficulties learning selected
evidence nodes randomly potential evidence nodes tested cases parameters used cpcs network fixed execution time
seconds learning overhead ais bn
andes network seconds seconds ais bn generated
samples sis generated samples lw generated samples
network lw still generate almost two times number samples generated
sis
report experimental summary statistics table
andes network ais bn significantly better
sis lw since lw generated almost two times number samples
generated sis performance better sis




min
median
max

ais bn






sis






lw






table summary simulation simulation cases andes
network

ais bn average order magnitude precise
two performance improvement smaller
two networks reason performance improvement ais bn
sis lw andes network smaller compared
cpcs pathfinder networks andes network used tests
apparently challenging enough sampling general andes
network sis lw perform well cases minimum mse sis
lw tested cases almost ais bn number samples
generated ais bn network significantly smaller previous
two networks ais bn needs time learn although increasing number
samples improve performance three improves performance
ais bn since convergence ratio ais bn usually larger
sis lw see figure parameters used network
tuned cpcs network large depth fewer leaf nodes andes
network pose difficulties learning


fiadaptive importance sampling bayesian networks

discussion
fundamental trade ais bn time spent
learning importance function time spent sampling current
believe reasonable stop learning point importance
function good enough experiments stopped learning iterations
several ways improving initialization conditional probability
tables outset ais bn current version
initialize icpt table every parent n evidence node e n pa e e e
uniform distribution pr e e ne improved
extend initialization nodes severely affected evidence
identified examining network structure local cpts
view learning process ais bn network rebuilding
process constructs network whose structure original
network except delete evidence nodes corresponding arcs constructed
network joint probability distribution x e equation approaches
optimal importance function use learned approximate distribution
approximates pr x e accurately enough use network solve
approximate tasks computing maximum posterior assignment
map pearl finding k likely scenarios seroussi golmard etc
large advantage solve network
evidence nodes
know markov blanket scoring improve convergence rates sampling
shwe cooper may applied ais bn
improve convergence rate according property section technique

c
reduce variance pr
reduce variance pr e
correspondingly improve
e
sampling performance since variance stratified sampling rubinstein
never much worse random sampling much better improve
convergence rate expect variance reduction methods statistics
expected value random variable ii antithetic variants correlations stratified
sampling latin hypercube sampling etc iii systematic sampling improve
sampling performance
current learning used simple heuristic learning methods
adjusting learning rates according changes error jacobs
applicable several tunable parameters ais bn
finding optimal values parameters given network another
interesting topic
worth observing plots presented figure fairly flat words
tests convergence sampling depend strongly
probability evidence seems contradict common belief forward sampling
schemes suffer unlikely evidence ais bn one shows fairly flat plot
convergence sis lw seems decrease slightly unlikely evidence
possible three perform much worse probability
evidence drops threshold value tests failed


ficheng druzdzel

relationship studied carefully conjecture probability evidence
good measure difficulty approximate inference
given approximating probabilistic inference np hard exist
networks challenging doubt even
ais bn perform poorly day found
networks one characteristic networks may challenging ais bn
general number parameters need learned aisbn increases performance deteriorate nodes many parents
example challenging ais bn learning update icpt
tables combinations parent nodes possible conditional probability
distributions causal independence properties noisy distributions pearl
henrion diez srinivas heckerman breese common
large practical networks treated differently lead considerable savings
learning time
one direction testing approximate suggested us reviewer use
large networks exact solution cannot computed case one
try infer difference variance stages whether
converging interesting idea worth exploring especially
combined theoretical work stopping criteria line work dagum
luby

conclusion
computational complexity remains major application probability theory
decision theory knowledge systems important develop schemes
improve performance updating even though theoretically demonstrated worst case remain nphard many practical cases may become tractable
studied importance sampling bayesian networks reviewing
important theoretical related importance sampling finite dimensional
integrals proposed importance sampling bayesian networks
call adaptive importance sampling ais bn process learning optimal
importance function ais bn computationally intractable
theory importance sampling finite dimensional integrals proposed several heuristics
seem work well practice proposed heuristic methods initializing
importance function shown accelerate learning process smooth learning method updating importance function structural advantages bayesian
networks dynamic weighting function combining samples different stages
methods help ais bn get fairly accurate
estimates posterior probabilities limited time two applied heuristics
adjustment small probabilities seems lead largest improvement performance
although largest decrease mse achieved combination two heuristics
ais bn
ais bn lead dramatic improvement convergence rates
large bayesian networks evidence compared existing state art
compared performance ais bn performance likelihood


fiadaptive importance sampling bayesian networks

weighting self importance sampling large practical model cpcs network
evidence unlikely typically experiments
observed ais bn better likelihood weighting selfimportance sampling cases reached two orders magnitude
improvement accuracy tests performed two networks pathfinder
andes yielded similar
although may exist approximate prove superior aisbn networks special structure distribution ais bn simple
robust general evidential reasoning large multiply connected bayesian
networks

acknowledgments
thank anonymous referees several insightful comments led substantial
improvement supported national science foundation
faculty early career development career program grant iri
air force office scientific grants f f
earlier version received school information sciences
robert r korfhage award university pittsburgh malcolm pradhan max henrion
institute decision systems shared us cpcs network kind
permission developers internist system university pittsburgh
thank david heckerman pathfinder network abigail gerner andes
network used tests experimental data obtained smile
bayesian inference engine developed decision systems laboratory available
http www sis pitt edu genie

references
cano j e hernandez l moral importance sampling
propagation probabilities belief networks international journal approximate
reasoning
chavez r cooper g f randomized approximation probabilistic inference bayesian belief networks networks
cheng j druzdzel j computational investigations low discrepancy
sequences simulation bayesian networks proceedings sixteenth annual conference uncertainty artificial intelligence uai pp
san francisco ca morgan kaufmann publishers
cheng j druzdzel j b latin hypercube sampling bayesian networks
proceedings th international florida artificial intelligence symposium conference flairs pp orlando florida
conati c gertner vanlehn k druzdzel j line student modeling
coached solving bayesian networks proceedings sixth


ficheng druzdzel

international conference user modeling um pp vienna york
springer verlag
cooper g f computational complexity probabilistic inference bayesian belief networks artificial intelligence
cousins b chen w frisse e tutorial introduction stochastic
simulation belief networks artificial intelligence medicine chap
pp elsevier science publishers b v
dagum p karp r luby ross optimal monte
carlo estimation extended abstract proceedings th ieee symposium
foundations computer science pp portland oregon
dagum p luby approximating probabilistic inference bayesian belief
networks np hard artificial intelligence
dagum p luby optimal approximation bayesian inference
artificial intelligence
diez f j parameter adjustment bayes networks generalized noisy orgate proceedings ninth annual conference uncertainty artificial
intelligence uai pp san francisco ca morgan kaufmann publishers
fishman g monte carlo concepts applications springerverlag
fung r chang k c weighing integrating evidence stochastic simulation bayesian networks uncertainty artificial intelligence pp
york n elsevier science publishing company inc
fung r del favero b backward simulation bayesian networks proceedings
tenth annual conference uncertainty artificial intelligence uai
pp san francisco ca morgan kaufmann publishers
geman geman stochastic relaxations gibbs distributions bayesian restoration images ieee transactions pattern analysis machine
intelligence
gilks w richardson spiegelhalter markov chain monte carlo practice chapman hall
heckerman breese j look causal independence proceedings
tenth annual conference uncertainty artificial intelligence uai
pp san mateo ca morgan kaufmann publishers inc
heckerman e horvitz e j nathwani b n toward normative expert
systems pathfinder project tech rep ksl medical computer science
group section medical informatics stanford university stanford ca


fiadaptive importance sampling bayesian networks

henrion propagating uncertainty bayesian networks probabilistic logic
sampling uncertainty artificial intellgience pp york n
elsevier science publishing company inc
henrion practical issues constructing belief networks kanal l
levitt lemmer j eds uncertainty artificial intelligence pp
elsevier science publishers b v north holland
henrion search methods bound diagnostic probabilities large
belief nets proceedings seventh annual conference uncertainty artificial intelligence uai pp san mateo california morgan kaufmann
publishers
hernandez l moral antonio monte carlo probabilistic
propagation belief networks importance sampling stratified simulation
techniques international journal approximate reasoning
jacobs r increased rates convergence learning rate adaptation
neural networks
lauritzen l spiegelhalter j local computations probabilities
graphical structures application expert systems journal royal
statistical society series b methodological
mackay intro monte carlo methods jordan ed learning
graphical mit press cambridge massachusetts
ortiz l e kaelbling l p adaptive importance sampling estimation
structured domains proceedings sixteenth annual conference uncertainty artificial intelligence uai pp san francisco ca morgan
kaufmann publishers
pearl j fusion propagation structuring belief networks artificial intelligence
pearl j evidential reasoning stochastic simulation causal artifical
intelligence
pearl j probabilistic reasoning intelligent systems networks plausible
inference morgan kaufmann publishers inc san mateo ca
pradhan dagum p optimal monte carlo inference proceedings
twelfth annual conference uncertainty artificial intelligence uai pp
san francisco ca morgan kaufmann publishers
pradhan provan g middleton b henrion knowledge engineering
large belief networks proceedings tenth annual conference uncertainty artificial intelligence uai pp san francisco ca morgan
kaufmann publishers


ficheng druzdzel

ritter h martinetz schulten k neuronale netze addison wesley
munchen
rubinstein r simulation monte carlo method john wiley sons
seroussi b golmard j l directly finding k probable
configurations bayesian networks international journal approximate reasoning

shachter r peot simulation approaches general probabilistic
inference belief networks uncertainty artificial intelligence pp
york n elsevier science publishing company inc
shwe cooper g f empirical analysis likelihood weighting simulation large multiply connected medical belief network computers biomedical

shwe middleton b heckerman henrion horvitz e lehmann h
probabilistic diagnosis reformulation internist qmr knowledge
base probabilistic model inference methods information
medicine
srinivas generalization noisy model proceedings ninth
annual conference uncertainty artificial intelligence uai pp
san francisco ca morgan kaufmann publishers
york j use gibbs sampler expert systems artificial intelligence





