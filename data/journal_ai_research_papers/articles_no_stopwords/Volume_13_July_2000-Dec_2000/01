journal artificial intelligence

submitted published

value function approximations partially observable
markov decision processes

milos hauskrecht

milos cs brown edu

computer science department brown university
box brown university providence ri usa

abstract

partially observable markov decision processes pomdps provide elegant mathematical framework modeling complex decision stochastic
domains states system observable indirectly via set imperfect
noisy observations modeling advantage pomdps however comes price
exact methods solving computationally expensive thus applicable
practice simple focus ecient approximation heuristic
methods attempt alleviate computational trade accuracy
speed two objectives first survey approximation methods
analyze properties relations provide insights differences
second present number approximation methods novel refinements existing techniques theoretical supported experiments
agent navigation domain
introduction

making decisions dynamic environments requires careful evaluation cost benefits immediate action choices may future
evaluation becomes harder effects actions stochastic must pursue evaluate many possible outcomes parallel typically becomes
complex look future situation becomes even worse
outcomes observe imperfect unreliable indicators underlying process
special actions needed obtain reliable information unfortunately many
real world decision fall category
consider example patient management patient comes
hospital initial set complaints rarely allow physician decisionmaker diagnose underlying disease certainty number disease options
generally remain open initial evaluation physician multiple choices
managing patient choose nothing wait see order additional tests
learn patient state disease proceed radical treatment
e g surgery making right decision easy task disease patient suffers
progress time may become worse window opportunity particular
effective treatment missed hand selection wrong treatment may
make patient condition worse may prevent applying correct treatment later
treatment typically non deterministic outcomes possible
addition treatment investigative choices come different costs thus
c ai access foundation morgan kaufmann publishers rights reserved

fihauskrecht

course patient management decision maker must carefully evaluate costs
benefits current future choices well interaction ordering
decision similar characteristics complex temporal cost benefit tradeoffs
stochasticity partial observability underlying controlled process include robot
navigation target tracking machine mantainance replacement
sequential decision modeled markov decision processes mdps
bellman howard puterman boutilier dean hanks
extensions model choice similar patient management partially
observable markov decision process pomdp drake astrom sondik
lovejoy b pomdp represents two sources uncertainty stochasticity
underlying controlled process e g disease dynamics patient management
imperfect observability states via set noisy observations e g symptoms
findings tests addition lets us model uniform way control
information gathering investigative actions well effects cost benefit tradeoffs partial observability ability model reason information gathering
actions main features distinguish pomdp widely known fully
observable markov decision process bellman howard
although useful modeling perspective pomdps disadvantage hard solve papadimitriou tsitsiklis littman mundhenk goldsmith
lusena allender madani hanks condon optimal optimal solutions obtained practice low complexity challenging goal
area exploit additional structural properties domain suitable
approximations heuristics used obtain good solutions eciently
focus heuristic approximation methods particular approximations
value functions important issues area design ecient
well better understanding existing techniques relations
advantages disadvantages address issues first
survey value function approximations analyze properties relations
provide insights differences second present number methods
novel refinements existing techniques theoretical findings
supported empirically agent navigation domain
partially observable markov decision processes

partially observable markov decision process pomdp describes stochastic control
process partially observable hidden states formally corresponds tuple
r set states set actions set observations
set transition probabilities describe dynamic behavior
modeled environment set observation probabilities
describe relationships among observations states actions r ir
denotes reward model assigns rewards state transitions payoffs associated transitions instances definition pomdp includes
priori probability distribution set initial states



fivalue function approximations pomdps







ot





st











r



figure part uence diagram describing pomdp model rectangles correspond
decision nodes actions circles random variables states diamonds
reward nodes links represent dependencies among components st ot
rt denote state action observation reward time note action
time depends past observations actions states

objective function
given pomdp goal construct control policy maximizes objective value
function objective function combines partial stepwise rewards multiple steps
kinds decision typically cumulative
expectations two frequently used practice

finite horizon model maximize e ptt rt rt reward obtained
time

infinite horizon discounted model maximize e p rt
discount factor

note pomdps cumulative decision provide rich language modeling
control objectives example one easily model goal achievement tasks
specific goal must reached giving large reward transition state
zero smaller rewards transitions
focus primarily discounted infinite horizon model however
easily applied finite horizon case

information state
pomdp process states hidden cannot observe making
decision next action thus action choices information available us quantities derived information illustrated
uence diagram figure action time depends previous
observations actions states quantities summarizing information called
information states complete information states represent trivial case


fihauskrecht





st













rt

rt

figure uence diagram pomdp information states corresponding
information state mdp information states represented
double circled nodes action choice rectangle depends current
information state

definition complete information state complete information state time denoted itc consists




prior belief b states time
complete history actions observations fo ot ot g starting time

sequence information states defines controlled markov process call
information state markov decision process information state mdp policy
information state mdp defined terms control function mapping
information state space actions information state deterministic function
previous state last action observation ot

ot
update function mapping information state space observations
actions back information space easy see one convert
original pomdp information state mdp complete information states
relation components two sketch reduction
pomdp information state mdp shown figure
bellman equations pomdps
information state mdp infinite horizon discounted case fully observable
mdp satisfies standard fixed point bellman equation




x
v max p ji v





denotes generic update function thus use symbol even information
state space different


fivalue function approximations pomdps

p

v denotes optimal value function maximizing e
rt state
expected one step reward equals
x
xx
p sji
r p js p sji



denotes expected one step reward state action
since next information state deterministic function previous
information state action observation equation rewritten
compactly summing possible observations
v max




x


p sji

x




p oji v



optimal policy control function selects value maximizing action




x
x
arg max
p sji p oji v





value control functions expressed terms action value functions
q functions
v max q
arg max q


x
x

q p sji p oji v



q function corresponds expected reward chosing fixed action first
step acting optimally afterwards
sufficient statistics

derive equations implicitly used complete information states however
remarked earlier information available decision maker summarized
quantities call sucient information states states must preserve
necessary information content markov property information state
decision process

definition sucient information state process let information state space
update function defining information process
ot process sucient regard optimal control
time step satisfies
p st jit p st jitc
p ot jit p ot jitc
itc itc complete information states
easy see equations complete information states must hold
sucient information states key benefit sucient statistics often


fihauskrecht

easier manipulate store since unlike complete histories may expand
time example standard pomdp model sucient work belief states
assign probabilities every possible process state astrom case
bellman equation reduces


v b max


x


b

xx




p ojs b v b



next step belief state b
x
b b fip ojs
p sja b



p ojb normalizing constant defines belief state mdp
special case continuous state mdp belief state mdps primary focus
investigation
value function mappings properties

bellman equation belief state mdp rewritten value function
mapping form let v space real valued bounded functions v ir defined
belief information space let h b ir defined

h b v

x



b

xx



p ojs b v b

defining value function mapping h v v hv b maxa h b v
bellman equation information states written v hv well
known h mdps isotone mapping contraction
supremum norm see heyman sobel puterman

definition mapping h isotone v u

v v u implies hv hu

definition let k k supremum norm mapping h contraction
supremum norm v u v khv hu k kv u k holds
value iteration
optimal value function equation approximation computed dynamic programming techniques simplest value iteration bellman
shown figure case optimal value function v determined
limit performing sequence value iteration steps vi hvi vi
ith approximation value function ith value function sequence estimates
belief states sucient include pomdps observation action channel
lags see hauskrecht
note update v hv applied solve finite horizon
standard way difference v stands steps go value function v represents
value function rewards end states








fivalue function approximations pomdps

value iteration p omdp
initialize v b
repeat
v v
update v hv b
supb j v b v b j
return v
figure value iteration procedure
converges unique fixed point solution direct consequence banach
theorem contraction mappings see example puterman
practice stop iteration well reaches limit solution stopping
criterion use figure examines maximum difference value
functions obtained two consecutive steps called bellman error puterman
littman stops quantity falls threshold
accuracy approximate solution ith value function regard v expressed
terms bellman error

theorem let supb jvi b vi b j kvi vi k magnitude bellman
error kvi v k kvi v k hold
obtain approximation v precision bellman error fall

piecewise linear convex approximations value function

major diculty applying value iteration dynamic programming beliefstate mdps belief space infinite need compute update vi hvi
poses following threats value function ith step may
representable finite means computable finite number steps
address sondik sondik smallwood sondik showed
one guarantee computability ith value function well finite description
belief state mdp considering piecewise linear convex representations
value function estimates see figure particular sondik showed piecewise
linear convex representation vi vi hvi computable remains piecewise
linear convex

theorem piecewise linear convex functions let v initial value function
piecewise linear convex ith value function obtained finite
number update steps belief state mdp finite piecewise linear convex
equal
x
vi b max b ffi
ffi

b ffi vectors size js j finite set vectors linear functions ffi


fihauskrecht

vi b





b

figure piecewise linear convex function pomdp two process states
fs g note b b holds belief state

key part proof express update ith value function
terms linear functions defining vi

x

vi b max




b

x

max

ffi




x x







p ojs b ffi



leads piecewise linear convex value function vi represented
finite set linear functions ffi one linear function every combination actions
j
permutations ffi vectors size jj let w fo ffji g fo ffji g fojj ffi j j g
combination linear function corresponding defined
xx
ffw
p ojs ffji





theorem basis dynamic programming finding optimal
solution finite horizon value iteration finding nearoptimal approximations v discounted infinite horizon model note however
imply piecewise linearity optimal fixed point solution v
computing value function updates

key part value iteration computation value function updates
vi hvi assume ith value function vi represented finite number linear
segments vectors total number possible linear functions jajj jjj one
every combination actions permutations ffi vectors size jj
enumerated jajjs j j jjj time however complete set linear functions
rarely needed linear functions dominated others omission
change resulting piecewise linear convex function illustrated
figure



fivalue function approximations pomdps

vi b

redundant linear
function




b

figure redundant linear function function dominate regions
belief space excluded

linear function eliminated without changing resulting value function
solution called redundant conversely linear function singlehandedly achieves
optimal value least one point belief space called useful
sake computational eciency important make size linear
function set small possible keep useful linear functions value iteration steps
two main approaches computing useful linear functions first
generate test paradigm due sondik monahan
idea enumerate possible linear functions first test usefulness
linear functions set prune redundant vectors recent extensions
method interleave generate test stages early pruning set partially
constructed linear functions zhang liu cassandra littman zhang
zhang lee
second builds sondik idea computing useful linear function
single belief state sondik smallwood sondik done eciently
key locate belief points seed useful linear functions
different methods address differently methods implement idea
sondik one two pass sondik cheng methods cheng
witness kaelbling littman cassandra littman cassandra

limitations complexity

major diculty solving belief state mdp complexity piecewise
linear convex function grow extremely fast number update steps
specifically size linear function set defining function grow exponentially
number observations single update step assuming initial
value function
linear number linear functions defining ith value function
jajjj


defining redundant useful linear functions assume linear function duplicates
e one copy linear function kept set




fihauskrecht

potential growth size linear function set bad news
remarked earlier piecewise linear convex value function usually less complex
worst case many linear functions pruned away updates however
turned task identifying useful linear functions computationally
intractable well littman means one faces potential
super exponential growth number useful linear functions ineciencies
related identification vectors significant drawback makes
exact methods applicable relatively simple
analysis suggests solving pomdp intrinsically hard
task indeed finding optimal solution finite horizon pspace hard
papadimitriou tsitsiklis finding optimal solution discounted infinitehorizon criterion even harder corresponding decision shown
undecidable madani et al thus optimal solution may computable
structural refinements basic

standard pomdp model uses state space full transition reward matrices
however practice often exhibit structure represented
compactly example graphical pearl lauritzen often
dynamic belief networks dean kanazawa kjaerulff dynamic uence
diagrams howard matheson tatman schachter many ways
take advantage structure modify improve exact
example refinement basic monahan compact transition reward
studied boutilier poole hybrid framework combines
mdp pomdp solving techniques take advantage perfectly partially observable components model subsequent value function decomposition
proposed hauskrecht similar perfect information
region subset states containing actual underlying state discussed
zhang liu b finally casta non yost explore techniques
solving large pomdps consist set smaller resource coupled otherwise
independent pomdps

extracting control strategy
value iteration allow us compute ith approximation value function vi however
ulimate goal optimal control strategy close approximation
thus focus extraction control strategies
value iteration
lookahead design

simplest way define control function value function vi via
greedy one step lookahead


b arg max


x


b

x




p ojb vi b

see survey boutilier dean hanks different ways represent structured mdps


fivalue function approximations pomdps

vi b








b



b

figure direct control design every linear function defining vi associated
action action selected linear function q function maximal
vi represents ith approximation optimal value function question
arises good resulting controller really following theorem puterman
williams baird littman relates accuracy lookahead controller
bellman error

theorem let kvi vi k magnitude bellman error let vila
expected reward lookahead controller designed vi kvila v k
bound used construct value iteration routine yields lookahead
strategy minimum required precision extended kstep lookahead design straightforward way k steps error bound becomes
kvila k v k
k

direct design

extract control action via lookahead essentially requires computing one full update
obviously lead unwanted delays reaction times general speed
response remembering additional information particular every linear
function defining vi associated choice action see equation action
byproduct methods computing linear functions extra computation required
action corresponding best linear function selected directly
belief state idea illustrated figure
bound accuracy direct controller infinite horizon case
derived terms magnitude bellman error

theorem let kvi vi k magnitude bellman error let vidr
expected reward direct controller designed vi kvidr v k
direct action choice closely related notion action value function
q function analogously equation ith q function satisfies
vi b max qi b


note control action extracted via lookahead v optimal steps go
finite horizon model main difference v optimal value function steps go






fihauskrecht




































figure policy graph finite state machine obtained two value iteration steps
nodes correspond linear functions states finite state machine
links dependencies linear functions transitions states every
linear function node associated action ensure policy
applied infinite horizon add cycle last state
dashed line

qi b r b

x


p ojb vi b

perspective direct strategy selects action best maximum qfunction given belief state
finite state machine design

complex refinement technique remember every linear function
vi action choice choice linear function previous
step observations see equation idea applied
recursively linear functions previous steps obtain relatively complex
dependency structure relating linear functions vi vi v observations actions
represents control strategy kaelbling et al
see model structure graphical terms figure different nodes
represent linear functions actions associated nodes correspond optimizing actions
links emanating nodes correspond different observations successor nodes correspond linear functions paired observations graphs called policy graphs
kaelbling et al littman cassandra one interpretation dependency structure represents collection finite state machines fsms many
possible initial states implement pomdp controller nodes correspond states
controller actions controls outputs links transitions conditioned inputs
williams baird give relating accuracy direct q function controller
bellman error q functions



fivalue function approximations pomdps

observations start state fsm controller chosen greedily selecting
linear function controller state optimizing value initial belief state
advantage finite state machine representation strategy
first steps works observations directly belief state updates needed
contrasts two policy lookahead direct must keep
track current belief state update time order extract appropriate
control drawback fsm controller limited steps
correspond number value iteration steps performed however infinitehorizon model controller expected run infinite number steps one way
remedy deficiency extend fsm structure create cycles let us
visit controller states repeatedly example adding cycle transition end state
fsm controller figure dashed line ensures controller applicable
infinite horizon

policy iteration
alternative method finding solution discounted infinite horizon
policy iteration howard sondik policy iteration searches policy space
gradually improves current control policy one belief states method
consists two steps performed iteratively




policy evaluation computes expected value current policy
policy improvement improves current policy

saw section many ways represent control policy
pomdp restrict attention finite state machine model observations
correspond inputs actions outputs platzman hansen b kaelbling
et al
finite state machine controller

finite state machine fsm controller c pomdp described
set memory states controller set observations inputs set
actions outputs transition function mapping states fsm
next memory states given observation output function mapping
memory states actions function selects initial memory state given
initial information state initial information state corresponds prior
posterior belief state time depending availability initial observation
policy evaluation

first step policy iteration policy evaluation important property
fsm model value function specific fsm strategy computed
eciently number controller states key ecient computability
policy iteration policies defined regions belief space described
first sondik


fihauskrecht

x







x











x







x

figure example four state fsm policy nodes represent states links transitions states conditioned observations every memory state
associated control action output

fact value function executing fsm strategy memory state x
linear platzman

theorem let c finite state machine controller set memory states
value function applying c memory state x v c x b linear value
functions x found solving system linear equations js jjm j
variables
illustrate main idea example assume fsm controller four memory
states fx x x x g figure stochastic process two hidden states
fs g value policy augmented state space satisfies system
linear equations
v x x
v x x
v x x



v x x

xx



xx



xx



xx


p sjs x v x
p sjs x v x
p sjs x v x
p sjs x v x

x action executed x x state one transits
seeing input observation assuming start policy memory state x
value policy
x
v c x b v x b

idea linearity ecient computability value functions fixed fsm strategy
addressed recently different contexts number researchers littman cassandra
hauskrecht hansen b kaelbling et al however origins idea
traced earlier work platzman


fivalue function approximations pomdps

thus value function linear computed eciently solving system
linear equations
since general fsm controller start memory state
choose initial memory state greedily maximizing expected value
case optimal choice function defined
b arg max v c x b
x
value fsm policy c belief state b

v c b max v c x b v c b b
x

note resulting value function strategy c piecewise linear convex
represents expected rewards following c since strategy perform better
optimal strategy v c v must hold
policy improvement

policy iteration method searching space controllers starts arbitrary initial policy improves gradually refining finite state machine fsm description
particular one keeps modifying structure controller adding removing controller states memory transitions let c c old fsm controller
improvement step must satisfy


v c b v c b b

b v c b v c b
guarantee improvement hansen b proposed policy iteration relies exact value function updates obtain improved policy structure basic idea improvement observation one switch
back forth fsm policy description piecewise linear convex
representation value function particular

value function fsm policy piecewise linear convex every linear
function describing corresponds memory state controller

individual linear functions comprising value function update
viewed memory states fsm policy described section

allows us improve policy adding memory states corresponding linear
functions value function obtained exact update technique
refined removing linear functions memory states whenever fully
dominated one linear functions
policy iteration exploits exact value function updates works policies defined
belief space used earlier sondik



fihauskrecht

b







b



b

figure two step decision tree rectangles correspond decision nodes moves
decision maker circles chance nodes moves environment
black rectangles represent leaves tree reward specific path
associated every leaf tree decision nodes associated
information states obtained following action observation choices along
path root tree example b belief state obtained
performing action initial belief state b observing observation

forward decision tree methods
methods discussed far assume prior knowledge initial belief state treat
belief states equally likely however initial state known fixed methods
often modified take advantage fact example finite horizon
finite number belief states reached given initial state
case often easier enumerate possible histories sequences actions
observations represent stochastic decision trees raiffa
example two step decision tree shown figure
solving stochastic decision tree basically mimics value function
updates restricted situations reached initial belief state
key diculty number possible trajectories grows exponentially
horizon interest
combining dynamic programming decision tree techniques

solve pomdp fixed initial belief state apply two strategies one constructs decision tree first solves solves backward
fashion via dynamic programming unfortunately techniques inecient one
suffering exponential growth decision tree size super exponential
growth value function complexity however two techniques combined


fivalue function approximations pomdps

way least partially eliminates disadvantages idea fact
two techniques work solution two different sides one forward
backward complexity worsens gradually solution
compute complete kth value function dynamic programming value iteration
cover remaining steps forward decision tree expansion
modifications idea possible example one often replace
exact dynamic programming two ecient approximations providing upper
lower bounds value function decision tree must expanded
bounds sucient determine optimal action choice number search
techniques developed ai literature korf combined branch bound
pruning satia lave applied type several researchers
experimented solve pomdps washington hauskrecht
hansen b methods applicable monte carlo
sampling kearns mansour ng mcallester singh real time dynamic
programming barto bradtke singh dearden boutilier bonet geffner

classical framework

pomdp fixed initial belief states solutions closely related
work classical extensions handle stochastic partially observable
domains particularly work buridan c buridan planners kushmerick
hanks weld draper hanks weld objective planners
maximize probability reaching goal state however task similar
discounted reward task terms complexity since discounted reward model
converted goal achievement model introducing absorbing state condon

heuristic approximations

key obstacle wider application pomdp framework computational
complexity pomdp particular finding optimal solution finitehorizon case pspace hard papadimitriou tsitsiklis discounted infinitehorizon case may even computable madani et al one
approximate solution precision unfortunately even
remains intractable general pomdps cannot approximated eciently burago
rougemont slissenko lusena goldsmith mundhenk madani et al
reason simple solved optimally
near optimally practice
alleviate complexity pomdp area focused
heuristic methods approximations without error parameter ecient
heuristic methods focus thus referring approximations mean
heuristics unless specifically stated otherwise
quality heuristic approximation tested bellman error requires one exact
update step however heuristic methods per se contain precision parameter



fihauskrecht

many approximation methods combinations divided two often
closely related classes value function approximations policy approximations

value function approximations
main idea value function approximation approximate optimal
value function v ir function vb ir defined information
space typically function lower complexity recall optimal nearoptimal value function may consist large set linear functions easier compute
exact solution approximations often formulated dynamic programming
expressed terms approximate value function updates hb thus
understand differences advantages approximations exact methods
often sucient analyze compare update rules
value function bounds

although heuristic approximations guaranteed precision many cases
able say whether overestimate underestimate optimal value function
information bounds used multiple ways example upper lowerbounds help narrowing range optimal value function elimination
suboptimal actions subsequent speed ups exact methods alternatively one
use knowledge value function bounds determine accuracy controller
generated one bounds see section instances lower
bound alone sucient guarantee control choice achieves expected
reward least high one given bound section
bound property different methods determined examining updates
bound relations

definition upper bound let h exact value function mapping hb apb b hv b holds
proximation say hb upper bounds h v hv
every b
analogous definition constructed lower bound
convergence approximate value iteration

let hb value function mapping representing approximate update approximate value iteration computes ith value function vbi hb vbi fixed point
solution vc hb vb close approximation would represent intended output
approximation routine main iteration method general
converge unique multiple solutions diverge oscillate depending hb
initial function vb therefore unique convergence cannot guaranteed arbitrary
mapping hb convergence specific approximation method must proved

definition convergence hb value iteration hb converges value function v limn hb n v exists



fivalue function approximations pomdps

definition unique convergence hb value iteration converges uniquely v
every v v limn hb n v exists pairs v u v limn hb n v
limn hb n u
sucient condition unique convergence hb contraction
contraction bound properties hb combined additional conditions
convergence iterative approximation method bound address
issue present theorem comparing fixed point solutions two value function mappings
theorem let h h two value function mappings defined v v
h h contractions fixed points v v
v v h v h v v

h isotone mapping
v v holds

note theorem require v v cover space value
functions example v cover possible value functions belief state mdp
v restricted space piecewise linear convex value functions
gives us exibility design iterative approximation computing
value function bounds analogous theorem holds lower bound
control

approximation value function available used generate
control strategy general control solutions correspond options presented section
include lookahead direct q function finite state machine designs
drawback control strategies heuristic approximations
precision guarantee one way accuracy strategies one exact
update value function approximation adopt theorems
bellman error alternative solution bound accuracy
controllers upper lower bound approximations optimal value
function illustrate present prove appendix following
theorem relates quality bounds quality lookahead controller

theorem let vbu vbl upper lower bounds optimal value function
discounted infinite horizon let supb jvbu b vbl b j kvbu vbl k
maximum bound difference expected reward lookahead controller vb la
constructed vbu vbl satisfies kvb la v k
policy approximation
alternative value function approximation policy approximation shown earlier
strategy controller pomdp represented finite state machine fsm
model policy iteration searches space possible policies fsms optimal near optimal solution space usually enormous bottleneck


fihauskrecht

method thus instead searching complete policy space restrict attention
subspace believe contain optimal solution good approximation memoryless policies platzman white scherer littman singh
jaakkola jordan policies truncated histories platzman white
scherer mccallum finite state controllers fixed number memory
states platzman hauskrecht hansen b examples
policy space restriction following consider finite state machine model
see section quite general viewed special cases
states fsm policy model represent memory controller general
summarize information past activities observations thus best viewed
approximations information states feature states transition model
controller approximates update function information state mdp
output function fsm approximates control function mapping
information states actions important property model shown section
value function fixed controller fixed initial memory state
obtained eciently solving system linear equations platzman
apply policy approximation first need decide restrict
space policies judge policy quality
restriction frequently used consider controllers fixed number
states say k structural restrictions narrowing space policies
restrict output function choice actions different controller states
transitions current next states general heuristic domain related
insight may help selecting right biases
two different policies yield value functions better different regions
belief space thus order decide policy best need define
importance different regions combinations multiple solutions
example platzman considers worst case measure optimizes worst
minimal value initial belief states let c space fsm controllers satisfying
given restrictions quality policy worst case measure
max min max v c x b
c c b x
c

another option consider distribution initial belief states maximize
expectation value function values however common objective choose
policy leads best value single initial belief state b
max max v c x b
c c x
c

finding optimal policy case reduces combinatorial optimization
unfortunately trivial cases even computationally intractable
example finding optimal policy memoryless case current observations considered np hard littman thus heuristics
typically applied alleviate diculty littman



fivalue function approximations pomdps

valuefunction
approximations

gridbased linear
function methods
section

fully observable mdp
approximations
section

fast informed bound
approximations
section

fixed strategy
approximations
section

curvefitting
approximations
section

gridbased value interpolation
extrapolation methods
section

unobservable mdp
approximations
section

figure value function approximation methods
randomized policies

restricting space policies simplify policy optimization
hand simultaneously give opportunity best optimal policy replacing best restricted policy point considered deterministic
policies fixed number internal controller states policies deterministic
output transition functions however finding best deterministic policy best option randomized policies randomized output transition functions
usually lead far better performance application randomized stochastic
policies pomdps introduced platzman essentially deterministic
policy represented randomized policy single action transition
best randomized policy worse best deterministic policy difference control performance two policies shows often cases number
states controller relatively small compared optimal strategy
advantage stochastic policies space larger parameters
policy continuous therefore finding optimal stochastic policy
becomes non linear optimization variety optimization methods
applied solve example gradient see meuleau et al
value function approximation methods

section discuss depth value function approximation methods focus approximations belief information space survey known techniques
include number methods modifications existing methods figure
summarizes methods covered describe methods means update rules
alternative value function approximations may work complete histories past actions observations approximation methods used white scherer example



fihauskrecht









































moves

sensors

figure test example maze navigation maze
implement simplifies analysis theoretical comparison focus following properties complexity dynamic programming value iteration updates
complexity value functions method uses ability methods bound
exact update convergence value iteration approximate update rules
control performance related controllers theoretical analysis illustrated empirically agent navigation domain addition use
agent navigation illustrate give intuitions characteristics
methods theoretical underpinning thus generalized
used rank different methods
agent navigation

maze maze navigation states six actions eight observations
maze figure consists partially connected rooms states robot
operates collects rewards robot move four directions north south east
west check presence walls sensors neither move
actions sensor inputs perfect robot end moving unintended
directions robot moves unintended direction probability
neighboring directions move wall keeps robot
position investigative actions help robot navigate activating sensor inputs two
investigative actions allow robot check inputs presence wall northsouth east west directions sensor accuracy detecting walls two wall
case e g north south wall one wall case north south
wall case smaller probabilities wrong perceptions
control objective maximize expected discounted rewards discount
factor small reward given every action leading bumping wall
points move points investigative action one large reward
points given achieving special target room indicated circle figure
recognizing performing one move actions collecting
reward robot placed random start position
although maze moderate complexity regard size
state action observation spaces exact solution beyond reach current
exact methods exact methods tried include witness
kaelbling et al incremental pruning cassandra et al
many thanks anthony cassandra running


fivalue function approximations pomdps

vmdp


vmdp



vmdp


vqmdp

q mdp

q mdp

q mdp

q mdp


vpomdp



vpomdp


b







b

b

figure approximations fully observable version two state pomdp
states mdp approximation b qmdp approximation
values extreme points belief space solutions fully observable
mdp
policy iteration fsm model hansen b main obstacle preventing
obtaining optimal close optimal solution complexity
value function number linear functions needed describe subsequent
running times memory

approximations fully observable mdp
perhaps simplest way approximate value function pomdp assume
states process fully observable astrom lovejoy case
optimal value function v pomdp approximated
vb b

x




b vmdp



optimal value function state fully observable version
vmdp
process refer approximation mdp approximation idea
approximation illustrated figure resulting value function linear
fully defined values extreme points belief simplex correspond
optimal values fully observable case main advantage approximation
fully observable mdp fomdp solved eciently finitehorizon discounted infinite horizon update step fully
observable mdp



vimdp

max


x





p js vimdp

solution finite state fully observable mdp discounted infinite horizon criterion
found eciently formulating equivalent linear programming task bertsekas



fihauskrecht

mdp approximation

mdp approximation equation described terms valuefunction updates belief space mdp although step strictly speaking redundant
simplifies analysis comparison approximations
let vbi linear value function described vector ffmdp
corresponding values

vimdp states th value function vbi

vbi b

x




b max


hmdp vbi b

x




p js ffmdp



vbi described linear function components
mdp
ffmdp
vi max






x




p js ffmdp



mdp rule hmdp rewritten general form starts
arbitrary piecewise linear convex value function vi represented set linear
functions

vbi b

x





b max


x





p js max ffi
ffi



application hmdp mapping leads linear value function
update easy compute takes jajjs j j jjs j time reduces jajjs j
time mdp updates strung together remarked earlier optimal
solution infinite horizon discounted solved eciently via linear
programming
update mdp approximation upper bounds exact update h vbi
hmdp vbi property later theorem covers cases intuition
cannot get better solution less information thus fully observable
mdp must upper bound partially observable case
approximation q functions qmdp

variant approximation fully observable mdp uses q functions littman
cassandra kaelbling
x
vb b max b qmdp


x

qmdp
p js vmdp

optimal action value function q function fully observable mdp qmdp
approximation vb piecewise linear convex jaj linear functions corresponding


fivalue function approximations pomdps

one action figure b qmdp update rule belief state mdp vbi
linear functions ffki

vbi b max

x





b

hqmdp vbi b

x




p js max ffi
ffi



hqmdp generates value function jaj linear functions time complexity
update mdp approximation case jajjs j j jjs j reduces
jajjs j time qmdp updates used hqmdp contraction mapping
fixed point solution found solving corresponding fully observable mdp
qmdp update upper bounds exact update bound tighter
mdp update h vbi hqmdp vbi hmdp vbi prove later theorem
inequalities hold fixed point solutions theorem
illustrate difference quality bounds mdp approximation
qmdp method use maze navigation measure quality
bound use mean value function values since belief states equally important
assume uniformly distributed approximate measure
average values fixed set n belief points points set
selected uniformly random beginning set chosen fixed
remained tests later figure shows
experiment include fast informed bound method presented
next section figure shows running times methods methods
implemented common lisp run sun ultra workstation
control

mdp qmdp value function approximations used construct controllers one step lookahead addition qmdp approximation suitable
direct control strategy selects action corresponding best highest
value q function thus method special case q function discussed
section advantage direct qmdp method faster
lookahead designs hand lookahead tends improve control performance
shown figure compares control performance different controllers
maze
quality policy b preference towards particular initial belief state
measured mean value function values b uniformly distributed initial
belief states approximate measure average discounted rewards

confidence interval limits probability level range respective
average scores holds bound experiments relatively small
include graphs
pointed littman et al instances direct qmdp controller never selects
investigative actions actions try gain information underlying process
state note however observation true general qmdp controller
direct action selection may select investigative actions even though fully observable version
investigative actions never chosen


fihauskrecht

bound quality
mdp
approximation

qmdp
approximation

running times

fast informed
bound






time sec

score














mdp
approximation



qmdp
approximation

fast informed
bound

figure comparison mdp qmdp fast informed bound approximations
bound quality left running times right bound quality score
average value approximation set belief points chosen uniformly random methods upper bound optimal value function
ip bound quality graph longer bars indicate better approximations
control trajectories obtained fixed set n initial belief states selected
uniformly random beginning trajectories obtained simulation
steps long
validate comparison along averaged performance scores must
scores randomness methods indeed statistically
significantly different rely pairwise significance tests summarize
obtained score differences two methods
later sucient reject method lower score
better performer significance levels respectively error bars
figure ect critical score difference significance level
figure shows average reaction times different controllers
experiments clear dominance direct qmdp controller
need lookahead order extract action compared two mdpbased controllers

fast informed bound method
mdp qmdp approaches ignore partial observability use fully
observable mdp surrogate improve approximations account least
length trajectories steps maze chosen ensure estimates
discounted cumulative rewards far actual rewards infinite number steps
alternative way compare two methods compute confidence limits scores inspect
overlaps however case ability distinguish two methods reduced due
uctuations scores different initializations maze confidence interval limits probability
level range respective average scores covers control experiments
later pairwise tests eliminate dependency examining differences individual values
thus improve discriminative power
critical score differences listed cover worst case combination thus may pairs
smaller difference would suce


fivalue function approximations pomdps

reaction times

control performance




lookahead


lookahead

lookahead

time sec

score




direct


lookahead

direct











mdp
approximation

direct

fast informed
bound

qmdp
approximation

lookahead

lookahead

mdp
approximation

direct

qmdp
approximation

fast informed
bound

figure comparison control performance mdp qmdp fast informed bound
methods quality control left reaction times right quality control
score average discounted rewards control trajectories obtained
fixed set initial belief states selected uniformly random
error bars critical score difference value two methods become statistically different significance level
degree partial observability propose method fast informed bound
method let vbi piecewise linear convex value function represented set linear
functions update defined

x

xx




x

vbi b max b
max
p ojs b ffi












x



x

x

max b
max


hf ib vbi b






p ojs ffi

fast informed bound update obtained exact update following
derivation

x

x

xx




x




h vbi b max b
max
p ojs b ffi








x

max
b



x



xx



max

ffi

x

x

p ojs b ffi


max b
max
p ojs ffi







x



max b ffai

hf ib vbi b

value function vbi hf ib vbi one obtains update piecewise linear
convex consists jaj different linear functions corresponding one


fihauskrecht

action

ffai

x

x
max
p ojs ffi








hf ib update ecient computed jajjs j jjj j time method
outputs jaj linear functions computation done jaj js j jj time
many hf ib updates strung together significant complexity reduction
compared exact latter lead function consisting jajj jjj
linear functions exponential number observations worst case
takes jajjs j j jjj time
hf ib updates polynomial complexity one approximation
finite horizon case eciently open issue remains finding solution
infinite horizon discounted case complexity address establish
following theorem

theorem solution fast informed bound approximation found solving
mdp js jjajjj states jaj actions discount factor
full proof theorem deferred appendix key part proof
construction equivalent mdp js jjajjj states representing hf ib updates
since finite state mdp solved linear program conversion fixed point
solution fast informed bound update computable eciently
fast informed bound versus fully observable mdp approximations

fast informed update upper bounds exact update tighter mdp
qmdp approximation updates

theorem let vbi corresponds piecewise linear convex value function defined
linear functions h vbi hf ib vbi hqmdp vbi hmdp vbi
key trick deriving swap max sum operators
proof appendix thus obtain upper bound inequalities
subsequent reduction complexity update rules compared exact update
shown figure umdp approximation included figure
discussed later section thus difference among methods boils
simple mathematical manipulations note inequality relations derived
updates hold fixed point solutions theorem
figure illustrates improvement bound mdp approximations
maze note however improvement paid increased
running time complexity figure b
control

fast informed bound outputs piecewise linear convex function one
linear function per action allows us build pomdp controller selects action
associated best highest value linear function directly figure compares
control performance direct lookahead controllers mdp qmdp
controllers see fast informed bound leads tighter bounds


fivalue function approximations pomdps

umdp update


v b ax b ax
aa






exact update


v b ax b
aa


p b



ax





fast informed bound update



v b ax b
aa



ax




p ax







mdp approx update


v b b ax
aa




p












p b




qmdp approx update


v b ax b














p ax



figure relations exact update umdp fast informed bound
qmdp mdp updates
improved control average however stress currently theoretical
underpinning observation thus may true belief states

extensions fast informed bound method

main idea fast informed bound method select best linear function
every observation every current state separately differs exact update
seek linear function gives best every observation
combination states however observe great deal middle ground
two extremes indeed one design update rule chooses optimal
maximal linear functions disjoint sets states separately illustrate idea
assume partitioning fs sm g state space update

vbi b max




x


b

x


max

x x

ffi

p ojs b ffi

x x
max
p ojs b ffi






max

x x

ffi




p ojs b ffi

easy see update upper bounds exact update exploration
partitioning heuristics remains interesting open issue


fihauskrecht

approximation unobservable mdp
mdp approximation assumes full observability pomdp states obtain simpler
ecient updates extreme discard observations available
decision maker mdp observations called unobservable mdp umdp
one may choose value function solution alternative approximation
solution unobservable mdp derive corresponding update
rule humdp similarly update partially observable case humdp preserves
piecewise linearity convexity value function contraction update
equals

x




xx

vbi b max b max
p js b ffi


humdp vbi b




set linear functions describing vbi vbi remains piecewise linear convex
consists j jjaj linear functions contrast exact update
number possible vectors next step grow exponentially number
observations leads jajj jjj possible vectors time complexity update
jajjs j j j thus starting vb one linear function running time complexity
k updates bounded jajk js j finding optimal solution
unobservable mdp remains intractable finite horizon case np hard burago et al
discounted infinite horizon case undecidable madani et al thus
usually useful approximation
update humdp lower bounds exact update intuitive ecting
fact one cannot better less information provide insight
two updates related following derivation proves bound
property elegant way

x

x




xx

h vbi b max b
max
p ojs b ffi









x

b max
max





x







xxx



xx



p ojs b ffi



max b max
p js b ffi


humdp vbi b




see difference exact umdp updates max
sum next step observations exchanged causes choice vectors
humdp become independent observations sum max operations
exchanged observations marginalized recall idea swaps leads
number approximation updates see figure summary


fivalue function approximations pomdps

fixed strategy approximations
finite state machine fsm model used primarily define control strategy
strategy require belief state updates since directly maps sequences observations
sequences actions value function fsm strategy piecewise linear convex
found eciently number memory states section
policy iteration policy approximation contexts value function specific strategy
used quantify goodness policy first place value function alone
used substitute optimal value function case value function
defined belief space equals
v c b max v c x b
x

p

v c x b v c x b obtained solving set js jjm j linear equations
section remarked earlier value fixed strategy lower bounds
optimal value function v c v
simplify comparison fixed strategy approximation approximations
rewrite solution terms fixed strategy updates

x




xxx

vbi b max x b
p js x b ffi x
x



x



xx





max b x
p js x ffi x
x

hf sm vbi b

value function vbi piecewise linear convex consists jm j linear functions
ffi x infinite horizon discounted case ffi x represents ith approximation
v c x note update applied finite horizon case straightforward
way
quality control

assume fsm strategy would use substitute optimal
control policy three different ways use extract control
first simply execute strategy represented fsm need
update belief states case second possibility choose linear functions
corresponding different memory states associated actions repeatedly every
step refer controller direct dr controller requires
updating belief states every step hand control performance
worse fsm control final strategy discards information
actions extracts policy value function vb b one step lookahead
method la requires belief state updates lookaheads leads worst
reactive time dr however strategy guaranteed worse fsm
controller following theorem relates performances three controllers


fihauskrecht

control performance

reaction times








time sec

score

















dr controller

fsm controller

fsm controller

la controller

dr controller

la controller

figure comparison three different controllers fsm dr la maze
collection one action policies control quality left response time right error bars control performance graph indicate
critical score difference two methods become statistically different
significance level

theorem let cf sm fsm controller let cdr cla direct
one step lookahead controllers constructed cf sm v c b v c b
v c b v c b hold belief states b
though prove direct controller lookahead controller
better underlying fsm controller see appendix full proof
theorem cannot similar property first two controllers initial
belief states however lookahead typically tends dominate ecting
usual trade control quality response time illustrate trade
running maze example collection jaj one action policies generating
sequence action control quality response time shown figure
see controller fsm fastest three
worst terms control quality hand direct controller slower needs
update belief states every step delivers better control finally lookahead
controller slowest best control performance
f sm

f sm

dr

la

selecting fsm model

quality fixed strategy approximation depends strongly fsm model used
model provided priori constructed automatically techniques automatic
construction fsm policies correspond search complete
restricted space policies examined optimal near optimal policy
space search process equivalent policy approximations policy iteration
techniques discussed earlier sections



fivalue function approximations pomdps

grid approximations value interpolation extrapolation
value function continuous belief space approximated finite set grid
points g interpolation extrapolation rule estimates value arbitrary
point belief space relying points grid associated values
definition interpolation extrapolation rule let f ir real valued function
defined information space g fbg bg bgk g set grid points g
f bg f bg bg f bg bgk f bgk g set point value pairs function rg
ir jgj ir estimates f point information space
values associated grid points called interpolation extrapolation rule
main advantage interpolation extrapolation model estimating true value
function requires us compute value updates finite set grid points
g let vbi approximation ith value function approximation
th value function vbi obtained
vbi b rg b gi
values associated every grid point bgj g included gi

bgj h vbi bgj max




b

x


p ojb vbi bgj







grid update described terms value function mapping hg
vbi hg vbi complexity update jgjjajjs j jjceval rg jgj
ceval rg jgj computational cost evaluating interpolation extrapolation rule
rg jgj grid points later section instances need
evaluate interpolation extrapolation rule every step eliminated
family convex rules

number possible interpolation extrapolation rules enormous focus
set convex rules relatively small important subset interpolationextrapolation rules

definition convex rule let f function defined space g fbg bg bgk g
set grid points g f bg f bg bg f bg bgk f bgk g set pointvalue pairs rule rg estimating f g called convex every b
value fb b
fb b rg b g
bj every j jgj

jgj
x
bj f bj

j

pjgj

b
j j



note convex rules used work special case averagers introduced gordon
difference minor definition averager includes constant independent grid points
values added convex combination


fihauskrecht

key property convex rules corresponding grid update hg
contraction max norm gordon thus approximate value iteration
hg converges unique fixed point solution addition hg convex rules
isotone
examples convex rules

family convex rules includes approaches commonly used practice
nearest neighbor kernel regression linear point interpolations many others
take example nearest neighbor function belief point b
estimated value grid point closest terms distance metric
defined belief space point b exactly one nonzero parameter
bj k b bgj km k b bgi km holds k
zero assuming euclidean distance metric nearest neighbor leads
piecewise constant approximation regions equal values correspond regions
common nearest grid point
nearest neighbor estimates function value taking account one
grid point value kernel regression expands upon grid points
adds weights contributions values according distance target
point example assuming gaussian kernels weight grid point bgj

bj exp kb

bg
k
j

p
normalizing constant ensuring jjg j bj parameter
attens narrows weight functions euclidean metric kernel regression
rule leads smooth approximation function
linear point interpolations subclass convex rules addition constraints
definition satisfy
jgj
x
b bj bgj
j

belief point b convex combination grid points corresponding coecients optimal value function pomdp convex
constraint sucient prove upper bound property approximation
general many different linear point interpolations given grid challenging rule best approximation discuss issues
section
conversion grid mdp

assume would approximation value function gridbased convex rule grid update equation view process
process finding sequence values bgj bgj bgj grid points
bgj g instances sequence values computed without
applying interpolation extrapolation rule every step cases


fivalue function approximations pomdps

converted fully observable mdp states corresponding grid points g
call mdp grid mdp

theorem let g finite set grid points rg convex rule parameters bj fixed values bgj bgj g found solving fully
observable mdp jgj states discount factor
proof grid point bgj write


bgj max bgj





x


x

p ojbgj vbig bgj




jgj

x
g
p ojbgj


b
j k k


max bgj


k



jgj
h


x
x
max bgj gi bgk
p ojbgj
j k

k

p

g g
denoting p ojbj g
j k p bk jbj construct fully observable
mdp states corresponding grid points g discount factor
update step equals




bgj max bgj


jgj
x
k




p bgk jbgj gi bgk

p
prerequisite bj every j jgj jjg j bj guarantees
p bgk jbgj interpreted true probabilities thus one compute values bgj
solving equivalent fully observable mdp
solving grid approximations

idea converting grid approximation grid mdp basis
simple powerful approximation brie key
parameters transition probabilities rewards mdp model solve
process relatively easy parameters used interpolate extrapolate
value non grid point fixed assumption theorem case
determine parameters mdp eciently one step grid set g
nearest neighbor kernel regression examples rules property note
leads polynomial time finding values grid points recall
mdp solved eciently finite discounted infinite horizon criteria
solving grid approximation arises parameters
used interpolation extrapolation fixed subject optimization
happens example multiple ways interpolating value
note similar proved independently gordon


fihauskrecht

point belief space would best interpolation leading
best values grid points g case corresponding optimal
grid mdp cannot found single step iterative approximation solving
sequence grid mdps usually needed worst case complexity
remains open question
constructing grids

issue touched far selection grids multiple ways
select grids divide two classes regular non regular grids
regular grids lovejoy partition belief space evenly equal size regions
main advantage regular grids simplicity locate grid points
neighborhood belief point disadvantage regular grids
restricted specific number points increase grid resolution paid
exponential increase grid size example sequence regular grids
dimensional belief space corresponds pomdp states consists
grid points prevents one method higher
grid resolutions larger state spaces
non regular grids unrestricted thus provide exibility grid resolution must increased adaptively hand due irregularities methods
locating grid points adjacent arbitrary belief point usually complex
compared regular grids
linear point interpolation

fact optimal value function v convex belief state mdps used
approximation linear point interpolation upper bounds
exact solution lovejoy neither kernel regression nearest neighbor
guarantee us bound

theorem upper bound property grid point interpolation update let vbi
convex value function h vbi hg vbi
upper bound property hg update convex value functions follows directly
jensen inequality convergence upper bound follows theorem
note point interpolation update imposes additional constraint choice
grid points particular easy see valid grid must include extreme points belief simplex extreme points correspond
regular grids used lovejoy freudenthal triangulation eaves essentially idea used partition evenly n dimensional subspace ir fact
ane transform allows us map isomorphically grid points belief space grid points
n dimensional space lovejoy
number points regular grid sequence given lovejoy
n

js j

jgj
js j

grid refinement parameter



fivalue function approximations pomdps

etc without extreme points one would unable cover whole belief space via
interpolation nearest neighbor kernel regression impose restrictions grid
finding best interpolation

general multiple ways interpolate point belief space objective
best interpolation one leads tightest upper bound
optimal value function
let b belief point f bj f bj jbj gg set grid value pairs best
interpolation point b
jgj
x
fb b min j f bj
j
p
jgj jjg j j

p
subject j j
b jjg j j bgj
linear optimization although solved polynomial time
linear programming techniques computational cost still relatively
large especially considering fact optimization must repeated many times
alleviate seek ecient ways finding interpolation sacrificing
optimality
one way suboptimal interpolation quickly apply regular grids proposed
lovejoy case value belief point approximated
convex combination grid points closest approximation leads piecewise linear
convex value functions interpolations fixed finding
approximation converted equivalent grid mdp solved
finite state mdp however pointed previous section regular grids must use
specific number grid points increase resolution grid paid
exponential increase grid size feature makes method less attractive
large state space need achieve high grid resolution
present work focus non regular arbitrary grids propose interpolation searches limited space interpolations guaranteed run
time linear size grid idea interpolate point
b belief space dimension js j set grid points consists arbitrary
grid point b g js j extreme points belief simplex coecients
interpolation found eciently search best interpolation let
b g grid point defining one interpolation value point b satisfies


bb
vbi b min
vi b
b g

vbib value interpolation grid point b figure illustrates
resulting approximation function characterized sawtooth shape
uenced choice interpolating set
best value function solution close approximation apply value
iteration procedure search best interpolation every update step
one solution may use adaptive regular grids grid resolution increased
parts belief space leave idea future work


fihauskrecht

v b

v b


v b

b


b

b

b

b b


figure value function approximation linear time interpolation
two dimensional case interpolating sets restricted single internal
point belief space
drawback interpolations may remain unchanged many
update steps thus slowing solution process alternative solve
sequence grid mdps instead particular every stage best
minimum value interpolations belief points reachable grid points one step fix
coecients interpolations construct grid mdp solve exactly
approximately process repeated improvement improvement
larger threshold seen values different grid points
improving grids adaptively

quality approximation bound depends strongly points used grid
objective provide good approximation smallest possible set grid
points however task impossible achieve since cannot known advance
solving belief points pick way address build grids
incrementally starting small set grid points adding others adaptively
places greater chance improvement key part
heuristic choosing grid points added next
one heuristic method developed attempts maximize improvements bound
values via stochastic simulations method builds fact every interpolation
grid must include extreme points otherwise cannot cover entire belief space
extreme points values affect grid points try improve
values first place general value grid point b improves
precise values used successor belief points belief states correspond
b choice observation current optimal action choice b
incorporating points grid makes larger improvement value
initial grid point b likely assuming initial point extreme point
heuristic tends improve value point naturally one proceed
selection incorporating successor points first level successors
grid well forth


fivalue function approximations pomdps

generate grid points g vb g
set gnew fg
extreme points b
repeat b g gnew
n

p
set arg maxa b p ojb vb g b
select observation according p ojb
update b b
add b gnew
return gnew
figure procedure generating additional grid points bound improvement heuristic

bound quality
mdp

fast interpolation
qmdp
informed regular grid

interpolation
adaptive grid

interpolation
random grid






score
















figure improvement upper bound quality grid point interpolations
adaptive grid method method compared randomly
refined grid regular grid points upper bound approximations mdp qmdp fast informed bound methods included
comparison
capture idea generate grid points via simulation starting one
extremes belief simplex continuing belief point currently
grid reached implements bound improvement heuristic
expands current grid g set js j grid points relying current
value function approximation vb g shown figure
figure illustrates performance bound quality adaptive grid method
maze use combination adaptive grids linear time
interpolation method gradually expands grid point increments
grid points figure shows performance random grid method


fihauskrecht

running times





time sec
















































mdp qmdp fast
informed

interpolation
regular grid

interpolation
adaptive grid

interpolation
random grid

figure running times grid point interpolation methods methods tested include adaptive grid random grid regular grid grid
points running times adaptive grid cumulative ecting dependencies higher grid resolutions lower level resolutions running
time mdp qmdp fast informed bound approximations
shown comparison
points grid selected iniformly random grid point increments
shown addition figure gives regular grid interpolation
lovejoy belief points upper bound methods mdp
qmdp fast informed bound approximations
see dramatic improvement quality bound adaptive method
contrast uniformly sampled grid random grid hardly changes
bound two reasons uniformly sampled grid points likely
concentrated center belief simplex transition matrix maze
relatively sparse belief points one obtains extreme points one
step boundary simplex since grid points center simplex
never used interpolate belief states reachable extremes one step cannot
improve values extremes bound change
one drawback adaptive method running time every grid size need
solve sequence grid mdps figure compares running times different
methods maze grid expansion adaptive method depends
value function obtained previous steps plot cumulative running times
see relatively large increase running time especially larger grid sizes ecting
trade bound quality running time however note
adaptive grid method performs quite well initial steps grid
points outperforms regular grid points bound quality
finally note heuristic approaches constructing adaptive grids point
interpolation possible example different refines grid ex



fivalue function approximations pomdps

control performance


score



























fast
interpolation interpolation
mdp qmdp informed regular grid adaptive grid

interpolation
random grid

nearest neighbor nearest neighbor
adaptive grid
random grid

figure control performance lookahead controllers grid point interpolation nearest neighbor methods varying grid sizes
compared mdp qmdp fast informed bound controllers
amining differences values current grid points recently proposed brafman

control

value functions obtained different grid methods define variety controllers figure compares performances lookahead controllers point interpolation
nearest neighbor methods run two versions approaches one adaptive grid random grid obtained
grid points addition compare performances interpolation regular
grids grid points mdp qmdp fast informed bound approaches
overall performance interpolation extrapolation techniques tested
maze bit disappointing particular better scores achieved
simpler qmdp fast informed bound methods see although heuristics
improved bound quality approximations lead similar improvement
qmdp fast informed bound methods terms control
shows bad bound terms absolute values imply bad control
performance main reason control performance uenced mostly
relative rather absolute value function values words shape
function interpolation extrapolation techniques use except regular grid
interpolation approximate value function functions piecewise linear
convex interpolations linear time interpolation technique
sawtooth shaped function nearest neighbor leads piecewise constant function
allow match shape optimal function correctly
factor affects performance large sensitivity methods selection grid
points documented example comparison heuristic random grids



fihauskrecht

tests focused lookahead controllers however alternative way
define controller grid interpolation extrapolation methods use q function
approximations instead value functions direct lookahead designs qfunction approximations found solving grid mdp keeping
values functions different actions separate end

approximations value functions curve fitting least squares
fit
alternative way approximate function continuous space use curve fitting
techniques relies predefined parametric model value function
set values associated finite set grid belief points g
similar interpolation extrapolation techniques relies set belief value
pairs difference curve fitting instead remembering belief value pairs
tries summarize terms given parametric function model strategy seeks
best possible match model parameters observed point values best
match defined criteria often least squares fit criterion
objective minimize
error f

x

j j

f bj

bj yj correspond belief point associated value index j ranges
points sample set g
combining dynamic programming least squares fit

least squares approximation function used construct dynamic programming
update step vbi hlsf vbi two steps first
obtain values set sample points g

b h vbi b max




x


b

xx



b
p ojs b vi b

second fit parameters value function model vbi sample value pairs
square error cost function complexity update jgjjajjs j jjceval vbi
cfit vbi jgj time ceval vbi computational cost evaluating vbi
cfit vbi jgj cost fitting parameters vbi jgj belief value pairs
advantage approximation least squares fit requires us
compute updates finite set belief states drawback
combined value iteration method lead instability
divergence shown mdps several researchers bertsekas boyan
moore baird tsitsiklis roy
similar qmdp method allows lookahead greedy designs fact qmdp
viewed special case grid method q function approximations grid
points correspond extremes belief simplex


fivalue function approximations pomdps

line version least squares fit

finding set parameters best fit solved available
optimization procedure includes line instance version gradient
descent method corresponds well known delta rule rumelhart hinton
williams
let f denote parametric value function belief space adjustable weights
w fw w wk g line update weight wi computed

wi

wi ffi f bj yj

f
j
wi b
j

ffi learning constant bj yj last seen point value note
gradient descent method requires function differentiable regard
adjustable weights
solve discounted infinite horizon stochastic line version
least squares fit combined parallel synchronous incremental gaussseidel point updates first case value function previous step fixed
value function computed scratch set belief point samples
values computed one step expansion parameters stabilized
attenuating learning rates newly acquired function fixed process proceeds
another iteration incremental version single value function model
time updated used compute values sampled points littman et al
parr russell implement asynchronous reinforcement
learning backups sample points updated next obtained via stochastic
simulation stress versions subject threat instability divergence
remarked
parametric function

apply least squares must first select appropriate value function
model examples simple convex functions linear quadratic functions
complex possible well
one interesting relatively simple least squares approximation linear action value functions q functions littman et al
value function vbi approximated piecewise linear convex combination qb
functions
vbi b max qb b

qb b least squares fit linear function set sample points g
values points g obtained

ai b b

x



p ojb vbi b

method leads approximation jaj linear functions coecients
functions found eciently solving set linear equations recall two
approximations qmdp fast informed bound approximations work


fihauskrecht

jaj linear functions main differences methods qmdp

fast informed bound methods update linear functions directly guarantee upper
bounds unique convergence
sophisticated parametric model convex function softmax model parr
russell


vb b


x x





k
b

k



set linear functions adaptive parameters fit k temperature parameter provides better fit underlying piecewise linear convex function
larger values function represents soft approximation piecewise linear convex
function parameter k smoothing approximation
control

tested control performance least squares linear q function
model littman et al softmax model parr russell softmax
model varied number linear functions trying cases linear functions
respectively first set experiments used parallel synchronous updates
samples fixed set belief points applied stochastic gradient descent techniques
best fit cases tested control performance value function
approximations obtained updates starting qmdp solution
second set experiments applied incremental stochastic update scheme
gauss seidel style updates method acquired every grid point
updated times learning rates decreasing linearly range
started qmdp solution lookahead controllers
summarized figure shows control performance direct q function
controller comparison qmdp method
linear q function model performed well lookahead design
better qmdp method difference quite apparent
direct approaches general good performance method attributed
choice function model let us match shape optimal value function
reasonably well contrast softmax linear functions
perform expected probably softmax model linear functions
updated every sample point leads situations multiple linear functions
try track belief point update circumstances hard capture
structure optimal value function accurately negative feature
effects line changes linear functions added softmax approximation
thus could bias incremental update schemes ideal case would identify
one vector responsible specific belief point update modify vector
linear q function avoids updating single linear
function corresponding action



fivalue function approximations pomdps

control performance



lookahead


iter iter stoch
iter


iter iter


iter iter iter

score





stoch


iter stoch


iter
iter


iter

stoch

direct




qmdp
approximation

linear q function
lookahead

linear q function
direct

softmax
linear functions

softmax
linear functions

figure control performance least squares fit methods tested include linear
q function model direct lookahead control softmax linear functions lookahead control value functions
obtained synchronous updates value functions obtained
incremental stochastic update scheme used define different
controllers comparison include two qmdp controllers

grid approximations linear function updates
alternative grid approximation method constructed applying sondik
computing derivatives linear functions points grid lovejoy
let vbi piecewise linear convex function described set linear functions
linear function belief point b action computed eciently
smallwood sondik littman
ffb


xx


p ojs ffi b

b indexes linear function ffi set linear functions
maximizes expression

x x






defining vbi



p ojs b ffi

fixed combination b optimizing function b acquired choosing
vector best overall value action vectors assuming bi
set candidate linear functions resulting functions satisfies
arg max x ffb b
ffb


collection linear functions obtained set belief points combined
piecewise linear convex value function idea behind number exact
b


b




fihauskrecht

v b


v b
v b
linear function



b



b

figure incremental version grid linear function method piecewise
linear lower bound improved linear function computed belief
point b sondik method
see section however exact case set points cover
linear functions defining value function must located first hard task
contrast approximation method uses incomplete set belief points
fixed least easy locate example via random heuristic selection use
hgl denote value function mapping grid
advantage grid method leads ecient updates
time complexity update polynomial equals jgjjajjs j jj yields set
jgj linear functions compared jajj jjj possible functions exact update
since set grid points incomplete resulting approximation lower bounds
value function one would obtain performing exact update lovejoy

theorem lower bound property grid linear function update let vbi
piecewise linear value function g set grid points used compute linear function
updates hgl vbi h vbi
incremental linear function

drawback grid linear function method hgl contraction
discounted infinite horizon case therefore value iteration method
mapping may converge lovejoy remedy propose
incremental version grid linear function method idea refinement
prevent instability gradually improving piecewise linear convex lower bound
value function
assume vbi v convex piecewise linear lower bound optimal value
function defined linear function set let ffb linear function point b
computed vbi sondik method one construct improved
value function vbi vbi simply adding linear function ffb
ffb idea incremental update illustrated figure similar
incremental methods used cheng lovejoy method


fivalue function approximations pomdps

running times

bound quality














score








































time sec



















































standard

qmdp

incremental

fast
informed

standard

incremental

figure bound quality running times standard incremental version
grid linear function method fixed point grid cumulative
running times including previous update cycles shown methods
running times qmdp fast informed bound methods included
comparison
extended handle set grid points g straightforward way note
adding one linear functions previous linear functions may
become redundant removed value function techniques redundancy
checking applied exact approaches monahan eagle
incremental refinement stable converges fixed set grid points
price paid feature linear function set grow size iteration
steps although growth linear number iterations compared
potentially exponential growth exact methods linear function set describing
piecewise linear approximation become huge thus practice usually stop
incremental updates well method converges question remains open
complexity hardness finding fixed point solution fixed set
grid points g
figure illustrates trade offs involved applying incremental updates
compared standard fixed grid maze use
grid points techniques initial value function
update cycles shown see incremental method longer running times
standard method since number linear functions grow every update
hand bound quality incremental method improves rapidly
never become worse update steps
minimum expected reward

incremental method improves lower bound value function value function say vbi used create controller lookahead direct action
choice general case cannot say anything performance quality
controllers regard vbi however certain conditions performance
controllers guaranteed never fall vbi following theorem proved
appendix establishes conditions

theorem let vbi value function obtained via incremental linear function method
starting vb corresponds fixed strategy c let cla cdr two


fihauskrecht

controllers vbi lookahead controller direct action controller v c
vc
respective value functions vbi v c
vbi v c
hold
note property holds incremental version exact value iteration
lookahead direct controllers perform worse vi obtained
incremental updates v corresponding fsm controller c
la

dr

la

dr

selecting grid points

incremental version grid linear function approximation exible
works arbitrary grid moreover grid need fixed changed
line thus finding grids reduces selecting belief points
updated next one apply strategies example one use
fixed set grid points update repeatedly one select belief points line
heuristics
incremental linear function method guarantees value function
improved linear functions previous steps kept unless found redundant
quality linear function added next depends strongly quality
linear functions obtained previous steps therefore objective select order
points better chances larger improvement designed two heuristic
strategies selecting ordering belief points
first strategy attempts optimize updates extreme points belief simplex
ordering heuristically idea heuristic fact states
higher expected rewards e g designated goal states backpropagate effects
rewards locally therefore desirable states neighborhood highest
reward state updated first distant ones later apply idea order
extreme points belief simplex relying current estimate value function
identify highest expected reward states pomdp model determine
neighbor states
second strategy idea stochastic simulation strategy generates
sequence belief points likely reached fixed initial belief point
points sequence used reverse order generate updates intent
heuristic maximize improvement value function initial fixed
point run heuristic need initial belief point set initial belief
points address use first heuristic allows us order
extreme points belief simplex points used initial beliefs
simulation part thus two tier strategy top level strategy orders extremes
belief simplex lower level strategy applies stochastic simulation generate
sequence belief states likely reachable specific extreme point
tested order heuristics two tier heuristics maze
compared two simple point selection strategies fixed grid strategy
set grid points updated repeatedly random grid strategy
points chosen uniformly random figure shows bound quality
restriction grid points must included grid required
example linear point interpolation scheme use extreme points belief
simplex


fivalue function approximations pomdps

bound quality



score













































fixed grid

random grid

order heuristic

tier heuristic

figure improvements bound quality incremental linear function method
four different grid selection heuristics cycle includes grid point
updates
methods update cycles cycle consists grid point updates
maze see differences quality value function approximations
different strategies even simple ones relatively small note
observed similar maze
relatively small improvement heuristics explained fact
every linear function uences larger portion belief space thus method
less sensitive choice specific point however another plausible explanation heuristics good accurate heuristics combinations
heuristics could constructed ecient strategies locating grid points used
exact methods e g witness kaelbling et al cheng methods cheng potentially applied remains open area

control

grid linear function leads piecewise linear convex approximation every linear function comes natural action choice lets us choose
action greedily thus run lookahead direct controllers figure
compares performance four different controllers fixed grid points combining standard incremental updates lookahead direct greedy control
update cycles see figure illustrate trade offs
computational time obtaining solution quality see incremental
lookahead controller design tend improve control performance
prices paid worse running reaction times respectively
small sensitivity incremental method selection grid points would suggest one
could many instances replace exact updates simpler point selection strategies could
increase speed exact value iteration methods least initial stages suffer
ineciencies associated locating complete set grid points updated every step however
issue needs investigated


fihauskrecht

control performance











lookahead

lookahead
















score


direct


direct




qmdp

fast
informed

direct
standard

lookahead
standard

direct
incremental

lookahead
incremental

figure control performance four different controllers grid linear function updates update cycles point grid controllers represent combinations two update strategies standard incremental two action extraction techniques direct lookahead running
times two update strategies presented figure comparison include performances qmdp fast informed bound
methods direct lookahead designs
control performance



score































qmdp

fast
informed

fixed grid

random grid

order heuristic

tier heuristic

figure control performances lookahead controllers incremental linearfunction different point selection heuristics improvement cycles comparison scores qmdp fast informed
bound approximations shown well
figure illustrates effect point selection heuristics control compare
lookahead control approximations obtained improvement cycles cycle consists grid point updates test



fivalue function approximations pomdps

bound quality big differences among heuristics suggesting
small sensitivity control selection grid points

summary value function approximations
heuristic value function approximations methods allow us replace hard compute exact
methods trade solution quality speed numerous methods employ different properties different trade offs quality versus speed tables
summarize main theoretical properties approximation methods covered
majority methods polynomial complexity least ecient polynomial bellman updates makes good candidates complex
pomdp reach exact methods
methods heuristic approximations give solutions
guaranteed precision despite fact proved solutions methods
worse others terms value function quality see figure one
main contributions however currently minimal theoretical
relating methods terms control performance exception
fsm controllers fsm approximations key observation
quality control lookahead control important approximate shape
derivatives value function correctly illustrated empirically gridbased interpolation extrapolation methods section non convex
value functions main challenges ways analyzing comparing
control performance different approximations theoretically identify classes
pomdps certain methods dominate others
finally note list methods complete value function approximation methods refinements existing methods possible example white
scherer investigate methods truncated histories lead upper
lower bound estimates value function complete information states complete
histories additional restrictions methods change properties
generic method example possible additional assumptions
able ensure convergence least squares fit approximation
conclusions

pomdps offers elegant mathematical framework representing decision processes
stochastic partially observable domains despite modeling advantages however
pomdp hard solve exactly thus complexity solvingprocedures becomes key aspect sucessful application model real world
even expense optimality recent complexity
approximability pomdp encouraging lusena et al madani
et al focus heuristic approximations particular approximations value
functions



fihauskrecht

method
mdp approximation
qmdp approximation
fast informed bound
umdp approximation
fixed strategy method
grid interpolation extrapolation
nearest neighbor
kernel regression
linear point interpolation
curve fitting least squares fit
linear q function
grid linear function method
incremental version start lower bound

bound
upper
upper
upper
lower
lower
upper
lower
lower

isotonicity

p
p
p
p
p
p
p
p

contraction

p



p
p
p
p
p
p
p
p

table properties different value function approximation methods bound property
isotonicity contraction property underlying mappings
although incremental version grid linear function method
contraction converges
method
mdp approximation
qmdp approximation
fast informed bound
umdp approximation
fixed strategy method
grid interpolation extrapolation
nearest neighbor
kernel regression
linear point interpolation
fixed interpolation
best interpolation
curve fitting least squares fit
linear q function
grid linear function method
incremental version

finite horizon
p
p
p
np hard
p
varies
p
p
p
p
p
varies
p
p
na

discounted infinite horizon
p
p
p
undecidable
p
na
p
p
varies
p

na
na
na


table complexity value function approximation methods finite horizon
discounted infinite horizon objective discounted infinitehorizon case corresponding fixed point solution complexity
take account addition components pomdps
approximation specific parameters e g size grid g grid methods indicates open instances na methods applicable one
e g possible divergence



fivalue function approximations pomdps

contributions
surveys known value function approximation methods solving pomdps
focus primarily theoretical analysis comparison methods findings supported experimentally moderate size agent
navigation domain analyze methods different perspectives computational complexity capability bound optimal value function convergence properties
iterative implementations quality derived controllers analysis includes
theoretical deriving properties individual approximations relations
exact methods general relations trade offs among different methods
well understood provide insights issues analyzing
corresponding updates example showed differences among exact
mdp qmdp fast informed bound umdp methods boil simple
mathematical manipulations subsequent effect value function approximation allowed us determine relations among different methods terms quality
respective value functions one main
presented number methods heuristic refinements existing
techniques primary contributions area include fast informed bound gridbased point interpolation methods including adaptive grid approaches stochastic sampling incremental linear function method showed
instances solutions obtained eciently converting original approximation equivalent finite state mdp example grid approximations
convex rules often solved via conversion grid mdp grid points
correspond states leading polynomial complexity finite discounted infinite horizon cases section dramatically
improve run time performance grid approaches similar conversion
equivalent finite state mdp allowing polynomial time solution discounted
infinite horizon shown fast informed bound method section
challenges future directions
work pomdps approximations far complete complexity
remain open particular complexity grid seeking best interpolation complexity finding fixed point solution incremental version
grid linear function method another interesting issue needs investigation convergence value iteration least squares approximation although
method unstable general case possible certain restrictions
converge
use single pomdp maze support theoretical
findings illustrate intuitions therefore supported theoretically related mostly control cannot generalized used rank different methods
since performance may vary general area pomdps
pomdp approximations suffers shortage larger scale experimental work
multiple different complexities broad range methods experimental
work especially needed study compare different methods regard control
quality main reason theoretical relating


fihauskrecht

control performance studies help focus theoretical exploration discovering
interesting cases possibly identifying classes certain approximations less suitable preliminary experimental
significant differences control performance among different methods
may suitable approximate control policies example grid
nearest neighbor piecewise constant approximation typically inferior
outperformed simpler ecient value function methods
present work focused heuristic approximation methods investigated general pomdps take advantage additional structural refinements
however real world usually offer structure exploited devise
perhaps lead speed ups possible
restricted versions pomdps additional structural assumptions solved
approximated eciently even though general complexity pomdps approximations encouraging papadimitriou tsitsiklis littman
mundhenk et al lusena et al madani et al challenge
identify allow ecient solutions time interesting enough
point application
finally number interesting issues arise move large state
action observation spaces complexity value function updates
belief state updates becomes issue general partial observability hidden
process states allow us factor decompose belief states updates
even transitions great deal structure represented compactly
promising directions deal issues include monte carlo approaches isard
blake kanazawa koller russell doucet kearns et al
methods approximating belief states via decomposition boyen koller
combination two approaches mcallester singh
acknowledgements

anthony cassandra thomas dean leslie kaelbling william long peter szolovits
anonymous reviewers provided valuable feedback comments work
supported grant ro lm grant lm national library
medicine dod advanced project agency arpa contract number
n darpa rome labs initiative grant f
appendix theorems proofs

convergence bound
theorem let h h two value function mappings defined v v
h h contractions fixed points v v
v v h v h v v

h isotone mapping
v v holds


fivalue function approximations pomdps

proof applying h condition expanding condition
get h v h v h v v repeating get limit v h n v
h v h v h v v proves
accuracy lookahead controller bounds
theorem let vbu vbl upper lower bounds optimal value function
discounted infinite horizon let supb jvbu b vbl b j kvbu vbl k
maximum bound difference expected reward lookahead controller vb la
constructed vbu vbl satisfies kvb la v k

proof let vb denotes upper lower bound approximation v h la
value function mapping corresponding lookahead policy vb note since
lookahead policy optimizes actions regard vb h vb h la vb must hold
error vb la bounded triangle inequality

kvb la v k kvb la vb k kvb v k
first component satisfies

kvb la vb k kh la vb la vb k
kh la vb la h vb k kh vb vb k
kh la vb la h la vb k kh vb vb k
kvb la vb k
inequality kh vb vb k follows isotonicity h fact vb
upper lower bound rearranging inequalities obtain kvb la vb k
bound second term kvb v k trivial

therefore kvb la v k

mdp qmdp fast informed bounds
theorem solution fast informed bound approximation found solving
mdp js jjajjj states jaj actions discount factor
proof let ffai linear function action defining vbi let ffi denote parameters
function parameters vbi satisfy
ffi
let

x

x


max
p ojs ffi




ffi max


x





p ojs ffi

fihauskrecht

rewrite ffi every

x



x

ffi max
p ojs




x

max








ffi

p ojs

x x







p ojs ffi

equations define mdp state space action space discount
factor thus solution fast informed bound update found solving
equivalent finite state mdp

theorem let vbi corresponds piecewise linear convex value function defined
linear functions h vbi hf ib vbi hqmdp vbi hmdp vbi
proof

x

x






xx

max b
max
p ojs b ffi






hvi b


max


x




b

hf ib vi b

max


x


b









hmdp vbi b



p ojs ffi


x




b max

max

x

ffi



hqmdp vbi b
x

x



p js max ffi
ffi





x


p js max ffi
ffi



fixed strategy approximations
theorem let cf sm fsm controller let cdr cla direct
one step lookahead controllers constructed cf sm v c b v c b
v c b v c b hold belief states b
proof value function fsm controller cf sm satisfies
f sm

f sm

la

vc

f sm



b max v x b v b b
x

v x b b x

x


p ojb x v x b x


dr

fivalue function approximations pomdps

direct controller cdr selects action greedily every step
chooses according b arg maxx v x b lookahead controller cla selects
action v x b one step away

la b arg max






x


b p ojb max
v x b
x


expanding value function cf sm one step get

vc

f sm

b max v x b
x




x

max b x p ojb x v x b x
x

b b

b b


x



x





p ojb b v x b b

p ojb b max
v x b b

x

x






max
b p ojb max
v x b

x

x
la

la
b b p ojb la b max
v x b b
x




iteratively expanding maxx v x expression substituing improved
higher value expressions back obtain value functions direct
lookahead controllers expansions lead value direct controller
expansions value lookahead controller thus v c
vc
c
c
v
v must hold note however action choices b la b
expressions different leading different next step belief states
subsequently different expansion sequences therefore imply
v dr b v la b b
f sm

f sm

dr

la

grid linear function method
theorem let vbi value function obtained via incremental linear function method
starting vb corresponds fixed strategy c let cla cdr two
controllers vbi lookahead controller direct action controller v c
vc
respective value functions vbi v c
vbi v c
hold
proof initializing method value function fsm controller c
incremental updates interpreted additions states fsm controller
linear function corresponds state fsm let ci controller
step v c
vbi holds inequalities follow theorem
la

dr

la

f sm



dr

fihauskrecht

references

astrom k j optimal control markov decision processes incomplete state
estimation journal mathematical analysis applications
baird l c residual reinforcement learning function approximation proceedings twelfth international conference machine learning
pp
barto g bradtke j singh p learning act real time dynamic
programming artificial intelligence
bellman r e dynamic programming princeton university press princeton nj
bertsekas p counter example temporal differences learning neural computation
bertsekas p dynamic programming optimal control athena scientific
bonet b geffner h learning sorting classification pomdps
proceedings fifteenth international conference machine learning
boutilier c dean hanks decision theoretic structural assumptions computational leverage artificial intelligence
boutilier c poole exploiting structure policy construction proceedings
thirteenth national conference artificial intelligence pp
boyan j moore generalization reinforcement learning safely
approximating value function advances neural information processing
systems mit press
boyen x koller tractable inference complex stochastic processes
proceedings fourteenth conference uncertainty artificial intelligence pp

boyen x koller exploiting architecture dynamic systems proceedings sixteenth national conference artificial intelligence pp
brafman r heuristic variable grid solution method pomdps proceedings fourteenth national conference artificial intelligence pp
burago rougemont slissenko complexity partially
observed markov decision processes theoretical computer science
cassandra r exact approximate partially observable markov
decision processes ph thesis brown university
cassandra r littman l zhang n l incremental pruning simple
fast exact partially observable markov decision processes proceedings
thirteenth conference uncertainty artificial intelligence pp


fivalue function approximations pomdps

casta non approximate dynamic programming sensor management
proceedings conference decision control
cheng h partially observable markov decision processes ph
thesis university british columbia
condon complexity stochastic games information computation

dean kanazawa k model reasoning persistence causation
computational intelligence
dearden r boutilier c abstraction approximate decision theoretic artificial intelligence
doucet sequential simulation methods bayesian filtering tech
rep cued f infeng tr department engineering cambridge university
drake observation markov process noisy channel ph thesis
massachusetts institute technology
draper hanks weld probabilistic information gathering
contingent execution proceedings second international conference
ai systems pp
eagle j n optimal search moving target search path constrained
operations
eaves b course triangulations soving differential equations deformations springer verlag berlin
gordon g j stable function approximation dynamic programming proceedings twelfth international conference machine learning
hansen e improved policy iteration partially observable mdps
advances neural information processing systems mit press
hansen e b solving pomdps searching policy space proceedings
fourteenth conference uncertainty artificial intelligence pp
hauskrecht control stochastic domains imperfect information ph thesis massachusetts institute technology
hauskrecht fraser h medical therapy partially observable
markov decision processes proceedings ninth international workshop
principles diagnosis dx pp
hauskrecht fraser h treatment ischemic heart disease
partially observable markov decision processes artificial intelligence medicine



fihauskrecht

heyman sobel stochastic methods operations stochastic
optimization mcgraw hill
howard r dynamic programming markov processes mit press cambridge
howard r matheson j uence diagrams principles applications
decision analysis
isard blake contour tracking stochastic propagation conditional
density proccedings europian conference computer vision pp
kaelbling l p littman l cassandra r acting
partially observable stochastic domains artificial intelligence
kanazawa k koller russell j stochastic simulation
dynamic probabilistic networks proceedings eleventh conference uncertainty artificial intelligence pp
kearns mansour ng sparse sampling near
optimal large markov decision processes proceedings sixteenth
international joint conference artificial intelligence pp
kjaerulff u computational scheme reasoning dynamic probabilistic networks proceedings eighth conference uncertainty artificial intelligence pp
korf r depth first iterative deepening optimal admissible tree search artificial
intelligence
kushmerick n hanks weld probabilistic
artificial intelligence
lauritzen l graphical clarendon press
littman l memoryless policies theoretical limitations practical
cliff husbands p meyer j wilson eds animals animats proceedings third international conference simulation adaptive
behavior mit press cambridge
littman l sequential decision making ph thesis brown
university
littman l cassandra r kaelbling l p learning policies partially
observable environments scaling proceedings twelfth international
conference machine learning pp
lovejoy w computationally feasible bounds partially observed markov
decision processes operations


fivalue function approximations pomdps

lovejoy w b survey algorithmic methods partially observed markov
decision processes annals operations
lovejoy w suboptimal policies bounds parameter adaptive decision
processes operations
lusena c goldsmith j mundhenk nonapproximability markov
decision processes tech rep university kentucky
madani hanks condon undecidability probabilistic
infinite horizon partially observable markov decision processes proceedings
sixteenth national conference artificial intelligence
mcallester singh p approximate factored pomdps
belief state simplification proceedings fifteenth conference uncertainty
artificial intelligence pp
mccallum r instance utile distinctions reinforcement learning
hidden state proceedings twelfth international conference machine
learning
monahan g e survey partially observable markov decision processes theory
management science
mundhenk goldsmith j lusena c allender e encyclopaedia complexity finite horizon markov decision process tech rep cs
dept tr university kentucky
papadimitriou c h tsitsiklis j n complexity markov decision processes mathematics operations
parr r russell approximating optimal policies partially observable
stochastic domains proceedings fourteenth international joint conference
artificial intelligence pp
pearl j probabilistic reasoning intelligent systems morgan kaufman
platzman l k finite memory estimation control finite probabilistic systems
ph thesis massachusetts institute technology
platzman l k feasible computational infinite horizon partiallyobserved markov decision tech rep georgia institute technology
puterman l markov decision processes discrete stochastic dynamic programming john wiley york
raiffa h decision analysis introductory lectures choices uncertainty
addison wesley
rumelhart hinton g e williams r j learning internal representations
error propagation parallel distributed processing pp


fihauskrecht

satia j lave r markovian decision processes probabilistic observation
states management science
singh p jaakkola jordan learning without state estimation
partially observable markovian decision processes proceedings eleventh
international conference machine learning pp
smallwood r sondik e j optimal control partially observable
processes finite horizon operations
sondik e j optimal control partially observable markov decision processes
ph thesis stanford university
sondik e j optimal control partially observable processes infinite
horizon discounted costs operations
tatman j schachter r dynamic programming uence diagrams
ieee transactions systems man cybernetics
tsitsiklis j n roy b v feature methods large scale dynamic
programming machine learning
washington r incremental markov model proceedings eight
ieee international conference tools artificial intelligence pp
white c c scherer w finite memory suboptimal design partially
observed markov decision processes operations
williams r j baird l c tight performance bounds greedy policies
imperfect value functions proceedings tenth yale workshop adaptive
learning systems yale university
yost k solution large scale allocation partially observable
outcomes ph thesis naval postgraduate school monterey ca
zhang n l lee partially observable markov decision
processes advances exact solution method proceedings fourteenth conference uncertainty artificial intelligence pp
zhang n l liu w model approximation scheme partially
observable stochastic domains journal artificial intelligence
zhang n l liu w b region approximations stochastic
domains proceedings thirteenth conference uncertainty artificial
intelligence pp




