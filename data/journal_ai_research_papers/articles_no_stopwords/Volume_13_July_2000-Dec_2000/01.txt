Journal Artificial Intelligence Research 13 (2000) 33{94

Submitted 9/99; published 8/00

Value-Function Approximations Partially Observable
Markov Decision Processes

Milos Hauskrecht

milos@cs.brown.edu

Computer Science Department, Brown University
Box 1910, Brown University, Providence, RI 02912, USA

Abstract

Partially observable Markov decision processes (POMDPs) provide elegant mathematical framework modeling complex decision planning problems stochastic
domains states system observable indirectly, via set imperfect
noisy observations. modeling advantage POMDPs, however, comes price |
exact methods solving computationally expensive thus applicable
practice simple problems. focus ecient approximation (heuristic)
methods attempt alleviate computational problem trade accuracy
speed. two objectives here. First, survey various approximation methods,
analyze properties relations provide new insights differences.
Second, present number new approximation methods novel refinements existing techniques. theoretical results supported experiments problem
agent navigation domain.
1. Introduction

Making decisions dynamic environments requires careful evaluation cost benefits immediate action choices may future.
evaluation becomes harder effects actions stochastic, must pursue evaluate many possible outcomes parallel. Typically, problem becomes
complex look future. situation becomes even worse
outcomes observe imperfect unreliable indicators underlying process
special actions needed obtain reliable information. Unfortunately, many
real-world decision problems fall category.
Consider, example, problem patient management. patient comes
hospital initial set complaints. rarely allow physician (decisionmaker) diagnose underlying disease certainty, number disease options
generally remain open initial evaluation. physician multiple choices
managing patient. He/she choose nothing (wait see), order additional tests
learn patient state disease, proceed radical treatment
(e.g. surgery). Making right decision easy task. disease patient suffers
progress time may become worse window opportunity particular
effective treatment missed. hand, selection wrong treatment may
make patient's condition worse, may prevent applying correct treatment later.
result treatment typically non-deterministic outcomes possible.
addition, treatment investigative choices come different costs. Thus,
c 2000 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiHauskrecht

course patient management, decision-maker must carefully evaluate costs
benefits current future choices, well interaction ordering.
decision problems similar characteristics | complex temporal cost-benefit tradeoffs,
stochasticity, partial observability underlying controlled process | include robot
navigation, target tracking, machine mantainance replacement, like.
Sequential decision problems modeled Markov decision processes (MDPs)
(Bellman, 1957; Howard, 1960; Puterman, 1994; Boutilier, Dean, & Hanks, 1999)
extensions. model choice problems similar patient management partially
observable Markov decision process (POMDP) (Drake, 1962; Astrom, 1965; Sondik, 1971;
Lovejoy, 1991b). POMDP represents two sources uncertainty: stochasticity
underlying controlled process (e.g. disease dynamics patient management problem),
imperfect observability states via set noisy observations (e.g. symptoms,
findings, results tests). addition, lets us model uniform way control
information-gathering (investigative) actions, well effects cost-benefit tradeoffs. Partial observability ability model reason information-gathering
actions main features distinguish POMDP widely known fully
observable Markov decision process (Bellman, 1957; Howard, 1960).
Although useful modeling perspective, POMDPs disadvantage hard solve (Papadimitriou & Tsitsiklis, 1987; Littman, 1996; Mundhenk, Goldsmith,
Lusena, & Allender, 1997; Madani, Hanks, & Condon, 1999), optimal -optimal solutions obtained practice problems low complexity. challenging goal
research area exploit additional structural properties domain and/or suitable
approximations (heuristics) used obtain good solutions eciently.
focus heuristic approximation methods, particular approximations based
value functions. Important research issues area design new ecient
algorithms, well better understanding existing techniques relations,
advantages disadvantages. paper address issues. First,
survey various value-function approximations, analyze properties relations
provide insights differences. Second, present number new methods
novel refinements existing techniques. theoretical results findings
supported empirically problem agent navigation domain.
2. Partially Observable Markov Decision Processes

partially observable Markov decision process (POMDP) describes stochastic control
process partially observable (hidden) states. Formally, corresponds tuple
(S; A; ; T; O; R) set states, set actions, set observations,
: ! [0; 1] set transition probabilities describe dynamic behavior
modeled environment, : ! [0; 1] set observation probabilities
describe relationships among observations, states actions, R : ! IR
denotes reward model assigns rewards state transitions models payoffs associated transitions. instances definition POMDP includes
priori probability distribution set initial states .

34

fiValue-Function Approximations POMDPs

o0

t2



ot

t1

t+1

st

a0

t2

t+1



t1

r



Figure 1: Part uence diagram describing POMDP model. Rectangles correspond
decision nodes (actions), circles random variables (states) diamonds
reward nodes. Links represent dependencies among components. st ; ; ot
rt denote state, action, observation reward time t. Note action
time depends past observations actions, states.

2.1 Objective Function
Given POMDP, goal construct control policy maximizes objective (value)
function. objective function combines partial (stepwise) rewards multiple steps
using various kinds decision models. Typically, models cumulative based
expectations. Two models frequently used practice:

finite-horizon model maximize E (PTt=0 rt ), rt reward obtained
time t.

infinite-horizon discounted model maximize E (P1t=0 rt ), 0 <
< 1 discount factor.

Note POMDPs cumulative decision models provide rich language modeling
various control objectives. example, one easily model goal-achievement tasks (a
specific goal must reached) giving large reward transition state
zero smaller rewards transitions.
paper focus primarily discounted infinite-horizon model. However,
results easily applied finite-horizon case.

2.2 Information State
POMDP process states hidden cannot observe making
decision next action. Thus, action choices based information available us quantities derived information. illustrated
uence diagram Figure 1, action time depends previous
observations actions, states. Quantities summarizing information called
information states. Complete information states represent trivial case.
35

fiHauskrecht

t+1



st

t+1



t+1

t+1





rt

rt

Figure 2: uence diagram POMDP information states corresponding
information-state MDP. Information states (It It+1 ) represented
double-circled nodes. action choice (rectangle) depends current
information state.

Definition 1 (Complete information state). complete information state time (denoted ItC ) consists of:




prior belief b0 states time 0;
complete history actions observations fo0 ; a0 ; o1 ; a1 ; ; ot 1 ; 1 ; ot g starting time = 0.

sequence information states defines controlled Markov process call
information-state Markov decision process information-state MDP. policy
information-state MDP defined terms control function : ! mapping
information state space actions. new information state (It ) deterministic function
previous state (It 1 ), last action (at 1 ) new observation (ot ):

= (It 1 ; ot ; 1 ):
: ! update function mapping information state space, observations
actions back information space.1 easy see one always convert
original POMDP information-state MDP using complete information states.
relation components two models sketch reduction
POMDP information-state MDP, shown Figure 2.
2.3 Bellman Equations POMDPs
information-state MDP infinite-horizon discounted case fully-observable
MDP satisfies standard fixed-point (Bellman) equation:
(

)

X
V (I ) = max (I; a) + P (I 0 jI; a)V (I 0 ) :
a2A
I0

(1)

1. paper, denotes generic update function. Thus use symbol even information
state space different.
36

fiValue-Function Approximations POMDPs

P

Here, V (I ) denotes optimal value function maximizing E ( 1
t=0 rt ) state . (I; a)
expected one-step reward equals
X
XX
(I; a) = (s; a)P (sjI ) =
R(s; a; s0 )P (s0 js; a)P (sjI ):
s2S
s2 s0 2

(s; a) denotes expected one-step reward state action a.
Since next information state 0 = (I; o; a) deterministic function previous
information state , action a, observation o, Equation 1 rewritten
compactly summing possible observations :
V (I ) = max
a2A

(

X
s2S

(s; a)P (sjI ) +

X
o2

)

P (ojI; a)V ( (I; o; a)) :

(2)

optimal policy (control function) : ! selects value-maximizing action
(

)

X
X
(I ) = arg max
(s; a)P (sjI ) + P (ojI; a)V ( (I; o; a)) :
a2A s2S
o2

(3)

value control functions expressed terms action-value functions
(Q-functions)
V (I ) = max Q (I; a)
(I ) = arg max Q (I; a);
a2A
a2A
X
X

Q (I; a) = (s; a)P (sjI ) + P (ojI; a)V ( (I; o; a)):
(4)
s2S
o2
Q-function corresponds expected reward chosing fixed action (a) first
step acting optimally afterwards.
2.3.1 Sufficient Statistics

derive Equations 1|3 implicitly used complete information states. However,
remarked earlier, information available decision-maker summarized
quantities. call sucient information states. states must preserve
necessary information content Markov property information-state
decision process.

Definition 2 (Sucient information state process). Let information state space
: ! update function defining information process =
(It 1 ; 1 ; ot ). process sucient regard optimal control when,
time step t, satisfies
P (st jIt ) = P (st jItC )
P (ot jIt 1 ; 1 ) = P (ot jItC 1 ; 1 );
ItC ItC 1 complete information states.
easy see Equations 1 | 3 complete information states must hold
sucient information states. key benefit sucient statistics often
37

fiHauskrecht

easier manipulate store, since unlike complete histories, may expand
time. example, standard POMDP model sucient work belief states
assign probabilities every possible process state (Astrom, 1965).2 case
Bellman equation reduces to:
(

V (b) = max
a2A

X
s2S

(s; a)b(s) +

XX
o2 s2S

)

P (ojs; a)b(s)V ( (b; o; a)) ;

(5)

next-step belief state b0
X
b0 (s) = (b; o; a)(s) = fiP (ojs; a)
P (sja; s0 )b(s0 ):
0
2S

= 1=P (ojb; a) normalizing constant. defines belief-state MDP
special case continuous-state MDP. Belief-state MDPs primary focus
investigation paper.
2.3.2 Value-Function Mappings Properties

Bellman equation 2 belief-state MDP rewritten value-function
mapping form. Let V space real-valued bounded functions V : ! IR defined
belief information space , let h : B ! IR defined

h(b; a; V ) =

X

s2S

(s; a)b(s) +

XX

o2 s2S

P (ojs; a)b(s)V ( (b; o; a)):

defining value function mapping H : V ! V (HV )(b) = maxa2A h(b; a; V ),
Bellman equation 2 information states written V = HV : well
known H (for MDPs) isotone mapping contraction
supremum norm (see (Heyman & Sobel, 1984; Puterman, 1994)).

Definition 3 mapping H isotone, V; U

2 V V U implies HV HU .

Definition 4 Let k:k supremum norm. mapping H contraction
supremum norm, V; U 2 V , kHV HU k kV U k holds 0 < 1.
2.4 Value Iteration
optimal value function (Equation 2) approximation computed using dynamic programming techniques. simplest approach value iteration (Bellman,
1957) shown Figure 3. case, optimal value function V determined
limit performing sequence value-iteration steps Vi = HVi 1 , Vi
ith approximation value function (ith value function).3 sequence estimates
2. Models belief states sucient include POMDPs observation action channel
lags (see Hauskrecht (1997)).
3. note update V = HV 1 applied solve finite-horizon problem
standard way. difference V stands i-steps-to-go value function V0 represents
value function (rewards) end states.






38

fiValue-Function Approximations POMDPs

Value iteration (P OMDP , )
initialize V b 2 ;
repeat
V0 V;
update V HV 0 b 2 ;
supb j V (b) V 0 (b) j
return V;
Figure 3: Value iteration procedure.
converges unique fixed-point solution direct consequence Banach's
theorem contraction mappings (see, example, Puterman (1994)).
practice, stop iteration well reaches limit solution. stopping
criterion use algorithm (Figure 3) examines maximum difference value
functions obtained two consecutive steps | so-called Bellman error (Puterman, 1994;
Littman, 1996). algorithm stops quantity falls threshold .
accuracy approximate solution (ith value function) regard V expressed
terms Bellman error .

Theorem 1 Let = supb jVi (b) Vi 1 (b)j = kVi Vi 1 k magnitude Bellman
error. kVi V k 1 kVi 1 V k 1 hold.
Then, obtain approximation V precision Bellman error fall
(1 ) .
2.4.1 Piecewise Linear Convex Approximations Value Function

major diculty applying value iteration (or dynamic programming) beliefstate MDPs belief space infinite need compute update Vi = HVi 1
it. poses following threats: value function ith step may
representable finite means and/or computable finite number steps.
address problem Sondik (Sondik, 1971; Smallwood & Sondik, 1973) showed
one guarantee computability ith value function well finite description
belief-state MDP considering piecewise linear convex representations
value function estimates (see Figure 4). particular, Sondik showed piecewise
linear convex representation Vi 1 , Vi = HVi 1 computable remains piecewise
linear convex.

Theorem 2 (Piecewise linear convex functions). Let V0 initial value function
piecewise linear convex. ith value function obtained finite
number update steps belief-state MDP finite, piecewise linear convex,
equal to:
X
Vi (b) = max b(s)ffi (s);
ffi 2 s2S

b ffi vectors size jS j finite set vectors (linear functions) ffi .
39

fiHauskrecht

Vi (b)

0

1

b(s1 )

Figure 4: piecewise linear convex function POMDP two process states
fs1 ; s2g. Note b(s1) = 1 b(s2 ) holds belief state.

key part proof express update ith value function
terms linear functions 1 defining Vi 1 :
8
<X

Vi (b) = max :
a2A

s2S

(s; a)b(s) +

X

max

o2 ffi 1 2



"
X X
1 s0 2S s2S

#

9
=

P (s0 ; ojs; a)b(s) ffi 1 (s0 ); :

(6)

leads piecewise linear convex value function Vi represented
finite set linear functions ffi , one linear function every combination actions
j
permutations ffi 1 vectors size jj. Let W = (a; fo1 ; ffji 1 1 g; fo2 ; ffji 2 1 g; fojj ; ffi j1j g)
combination. linear function corresponding defined
XX
ffW
P (s0 ; ojs; a)ffji 1 (s0 ):
(7)
(s) = (s; a) +
o2 s0 2S


Theorem 2 basis dynamic programming algorithm finding optimal
solution finite-horizon models value-iteration algorithm finding nearoptimal approximations V discounted, infinite-horizon model. Note, however,
result imply piecewise linearity optimal (fixed-point) solution V .
2.4.2 Algorithms Computing Value-Function Updates

key part value-iteration algorithm computation value-function updates
Vi = HVi 1 . Assume ith value function Vi represented finite number linear
segments (ff vectors). total number possible linear functions jAjj 1 jjj (one
every combination actions permutations ffi 1 vectors size jj)
enumerated O(jAjjS j2 j 1 jjj ) time. However, complete set linear functions
rarely needed: linear functions dominated others omission
change resulting piecewise linear convex function. illustrated
Figure 5.

40

fiValue-Function Approximations POMDPs

Vi (b)

redundant linear
function
0

1

b(s1 )

Figure 5: Redundant linear function. function dominate regions
belief space excluded.

linear function eliminated without changing resulting value function
solution called redundant. Conversely, linear function singlehandedly achieves
optimal value least one point belief space called useful.4
sake computational eciency important make size linear
function set small possible (keep useful linear functions) value-iteration steps.
two main approaches computing useful linear functions. first approach
based generate-and-test paradigm due Sondik (1971) Monahan (1982).
idea enumerate possible linear functions first, test usefulness
linear functions set prune redundant vectors. Recent extensions
method interleave generate test stages early pruning set partially
constructed linear functions (Zhang & Liu, 1997a; Cassandra, Littman, & Zhang, 1997;
Zhang & Lee, 1998).
second approach builds Sondik's idea computing useful linear function
single belief state (Sondik, 1971; Smallwood & Sondik, 1973), done eciently.
key problem locate belief points seed useful linear functions
different methods address problem differently. Methods implement idea
Sondik's one- two-pass algorithms (Sondik, 1971), Cheng's methods (Cheng, 1988),
Witness algorithm (Kaelbling, Littman, & Cassandra, 1999; Littman, 1996; Cassandra,
1998).
2.4.3 Limitations Complexity

major diculty solving belief-state MDP complexity piecewise
linear convex function grow extremely fast number update steps.
specifically, size linear function set defining function grow exponentially (in
number observations) single update step. Then, assuming initial
value function
linear, number linear functions defining ith value function
O(jAjjj 1 ).


4. defining redundant useful linear functions assume linear function duplicates,
i.e. one copy linear function kept set .


41

fiHauskrecht

potential growth size linear function set bad news.
remarked earlier, piecewise linear convex value function usually less complex
worst case many linear functions pruned away updates. However,
turned task identifying useful linear functions computationally
intractable well (Littman, 1996). means one faces potential
super-exponential growth number useful linear functions, ineciencies
related identification vectors. significant drawback makes
exact methods applicable relatively simple problems.
analysis suggests solving POMDP problem intrinsically hard
task. Indeed, finding optimal solution finite-horizon problem PSPACE-hard
(Papadimitriou & Tsitsiklis, 1987). Finding optimal solution discounted infinitehorizon criterion even harder. corresponding decision problem shown
undecidable (Madani et al., 1999), thus optimal solution may computable.
2.4.4 Structural Refinements Basic Algorithm

standard POMDP model uses state space full transition reward matrices.
However, practice, problems often exhibit structure represented
compactly, example, using graphical models (Pearl, 1988; Lauritzen, 1996), often
dynamic belief networks (Dean & Kanazawa, 1989; Kjaerulff, 1992) dynamic uence
diagrams (Howard & Matheson, 1984; Tatman & Schachter, 1990).5 many ways
take advantage problem structure modify improve exact algorithms.
example, refinement basic Monahan algorithm compact transition reward
models studied Boutilier Poole (1996). hybrid framework combines
MDP-POMDP problem-solving techniques take advantage perfectly partially observable components model subsequent value function decomposition
proposed Hauskrecht (1997, 1998, 2000). similar approach perfect information
region (subset states) containing actual underlying state discussed
Zhang Liu (1997b, 1997a). Finally, Casta~non (1997) Yost (1998) explore techniques
solving large POMDPs consist set smaller, resource-coupled otherwise
independent POMDPs.

2.5 Extracting Control Strategy
Value iteration allow us compute ith approximation value function Vi . However,
ulimate goal find optimal control strategy : ! close approximation.
Thus focus problem extraction control strategies results
value iteration.
2.5.1 Lookahead Design

simplest way define control function : ! value function Vi via
greedy one-step lookahead:
(

(b) = arg max
a2A

X
s2S

(s; a)b(s) +

X
o2

)

P (ojb; a)Vi ( (b; o; a)) :

5. See survey Boutilier, Dean Hanks (1999) different ways represent structured MDPs.
42

fiValue-Function Approximations POMDPs

Vi (b)
a1
a3

a2
a1

0

b

1

b(s1 )

Figure 6: Direct control design. Every linear function defining Vi associated
action. action selected linear function (or Q-function) maximal.
Vi represents ith approximation optimal value function, question
arises good resulting controller really is.6 following theorem (Puterman, 1994;
Williams & Baird, 1994; Littman, 1996) relates accuracy (lookahead) controller
Bellman error.

Theorem 3 Let = kVi Vi 1 k magnitude Bellman error. Let ViLA
expected reward lookahead controller designed Vi . kViLA V k 12 .
bound used construct value-iteration routine yields lookahead
strategy minimum required precision. result extended kstep lookahead design straightforward way; k steps, error bound becomes
kViLA(k) V k (12 ) .
k

2.5.2 Direct Design

extract control action via lookahead essentially requires computing one full update.
Obviously, lead unwanted delays reaction times. general, speed
response remembering using additional information. particular, every linear
function defining Vi associated choice action (see Equation 7). action
byproduct methods computing linear functions extra computation required
find it. action corresponding best linear function selected directly
belief state. idea illustrated Figure 6.
bound accuracy direct controller infinite-horizon case
derived terms magnitude Bellman error.

Theorem 4 Let = kVi Vi 1 k magnitude Bellman error. Let ViDR
expected reward direct controller designed Vi . kViDR V k 12 .
direct action choice closely related notion action-value function (or
Q-function). Analogously Equation 4, ith Q-function satisfies
Vi (b) = max Qi (b; a);
a2A

6. Note control action extracted via lookahead V optimal (i + 1) steps-to-go
finite-horizon model. main difference V optimal value function steps go.




43

fiHauskrecht

a1

o2
o1

a2

o1 ,

2

o2

o1 ,

a2

o1 , o2

2

o2
a2

a1

o1

o1

a2

o1 , o2
a1

Figure 7: policy graph (finite-state machine) obtained two value iteration steps.
Nodes correspond linear functions (or states finite-state machine)
links dependencies linear functions (transitions states). Every
linear function (node) associated action. ensure policy
applied infinite-horizon problem, add cycle last state
(dashed line).

Qi (b; a) = R(b; a) +

X
o2

P (ojb; a)Vi 1 ( (b; a; o)):

perspective, direct strategy selects action best (maximum) Qfunction given belief state.7
2.5.3 Finite-State Machine Design

complex refinement technique remember, every linear function
Vi , action choice choice linear function previous
step observations (see Equation 7). idea applied
recursively linear functions previous steps, obtain relatively complex
dependency structure relating linear functions Vi ; Vi 1 ; V0 , observations actions
represents control strategy (Kaelbling et al., 1999).
see this, model structure graphical terms (Figure 7). different nodes
represent linear functions, actions associated nodes correspond optimizing actions,
links emanating nodes correspond different observations, successor nodes correspond linear functions paired observations. graphs called policy graphs
(Kaelbling et al., 1999; Littman, 1996; Cassandra, 1998). One interpretation dependency structure represents collection finite-state machines (FSMs) many
possible initial states implement POMDP controller: nodes correspond states
controller, actions controls (outputs), links transitions conditioned inputs
7. Williams Baird (1994) give results relating accuracy direct Q-function controller
Bellman error Q-functions.

44

fiValue-Function Approximations POMDPs

(observations). start state FSM controller chosen greedily selecting
linear function (controller state) optimizing value initial belief state.
advantage finite-state machine representation strategy
first steps works observations directly; belief-state updates needed.
contrasts two policy models (lookahead direct models), must keep
track current belief state update time order extract appropriate
control. drawback approach FSM controller limited steps
correspond number value iteration steps performed. However, infinitehorizon model controller expected run infinite number steps. One way
remedy deficiency extend FSM structure create cycles let us
visit controller states repeatedly. example, adding cycle transition end state
FSM controller Figure 7 (dashed line) ensures controller applicable
infinite-horizon problem.

2.6 Policy Iteration
alternative method finding solution discounted infinite-horizon problem
policy iteration (Howard, 1960; Sondik, 1978). Policy iteration searches policy space
gradually improves current control policy one belief states. method
consists two steps performed iteratively:




policy evaluation: computes expected value current policy;
policy improvement: improves current policy.

saw Section 2.5, many ways represent control policy
POMDP. restrict attention finite-state machine model observations
correspond inputs actions outputs (Platzman, 1980; Hansen, 1998b; Kaelbling
et al., 1999).8
2.6.1 Finite-State Machine Controller

finite-state machine (FSM) controller C = (M; ; A; ; ; ) POMDP described
set memory states controller, set observations (inputs) , set
actions (outputs) A, transition function : ! mapping states FSM
next memory states given observation, output function : ! mapping
memory states actions. function : I0 ! selects initial memory state given
initial information state. initial information state corresponds either prior
posterior belief state time t0 depending availability initial observation.
2.6.2 Policy Evaluation

first step policy iteration policy evaluation. important property
FSM model value function specific FSM strategy computed
eciently number controller states . key ecient computability
8. policy-iteration algorithm policies defined regions belief space described
first Sondik (1978).
45

fiHauskrecht

x2
o1
o2

a2

a1

x1

o2


o1
a1

o2

o1

x3

2

a2

o1

x4

Figure 8: example four-state FSM policy. Nodes represent states, links transitions states (conditioned observations). Every memory state
associated control action (output).

fact value function executing FSM strategy memory state x
linear (Platzman, 1980).9

Theorem 5 Let C finite-state machine controller set memory states .
value function applying C memory state x 2 , V C (x; b), linear. Value
functions x 2 found solving system linear equations jS jjM j
variables.
illustrate main idea example. Assume FSM controller four memory
states fx1 ; x2 ; x3 ; x4 g, Figure 8, stochastic process two hidden states =
fs1 ; s2g. value policy augmented state space satisfies system
linear equations
V (x1 ; s1 ) = (s1 ; (x1 )) +
V (x1 ; s2 ) = (s2 ; (x1 )) +
V (x2 ; s1 ) = (s1 ; (x2 )) +



V (x4 ; s2 ) = (s2 ; (x4 )) +

XX

o2 s2S

XX

o2 s2S

XX

o2 s2S

XX
o2 s2S

P (o; sjs1 ; (x1 ))V ((x1 ; o); s)
P (o; sjs2 ; (x1 ))V ((x1 ; o); s)
P (o; sjs1 ; (x2 ))V ((x2 ; o); s)
P (o; sjs2 ; (x4 ))V ((x4 ; o); s);

(x) action executed x (x; o) state one transits
seeing input (observation) o. Assuming start policy memory state x1 ,
value policy is:
X
V C (x1 ; b) = V (x1 ; s)b(s):
s2S
9. idea linearity ecient computability value functions fixed FSM-based strategy
addressed recently different contexts number researchers (Littman, 1996; Cassandra,
1998; Hauskrecht, 1997; Hansen, 1998b; Kaelbling et al., 1999). However, origins idea
traced earlier work Platzman (1980).
46

fiValue-Function Approximations POMDPs

Thus value function linear computed eciently solving system
linear equations.
Since general FSM controller start memory state, always
choose initial memory state greedily, maximizing expected value result.
case optimal choice function defined as:
(b) = arg max V C (x; b);
x2M
value FSM policy C belief state b is:

V C (b) = max V C (x; b) = V C ( (b); b):
x2M

Note resulting value function strategy C piecewise linear convex
represents expected rewards following C . Since strategy perform better
optimal strategy, V C V must hold.
2.6.3 Policy Improvement

policy-iteration method, searching space controllers, starts arbitrary initial policy improves gradually refining finite-state machine (FSM) description.
particular, one keeps modifying structure controller adding removing controller states (memory) transitions. Let C C 0 old new FSM controller.
improvement step must satisfy
0

V C (b) V C (b) b 2 ;

9b 2 V C 0 (b) > V C (b):
guarantee improvement, Hansen (1998a, 1998b) proposed policy-iteration algorithm relies exact value function updates obtain new improved policy structure.10 basic idea improvement based observation one switch
back forth FSM policy description piecewise-linear convex
representation value function. particular:

value function FSM policy piecewise-linear convex every linear
function describing corresponds memory state controller;

individual linear functions comprising new value function update
viewed new memory states FSM policy, described Section 2.5.3.

allows us improve policy adding new memory states corresponding linear
functions new value function obtained exact update. technique
refined removing linear functions (memory states) whenever fully
dominated one linear functions.
10. policy-iteration algorithm exploits exact value function updates works policies defined
belief space used earlier Sondik (1978).

47

fiHauskrecht

b 1,1

o1

o2

a1

b

a2

b 1,2

Figure 9: two-step decision tree. Rectangles correspond decision nodes (moves
decision-maker) circles chance nodes (moves environment).
Black rectangles represent leaves tree. reward specific path
associated every leaf tree. Decision nodes associated
information states obtained following action observation choices along
path root tree. example, b1;1 belief state obtained
performing action a1 initial belief state b observing observation o1 .

2.7 Forward (Decision Tree) Methods
methods discussed far assume prior knowledge initial belief state treat
belief states equally likely. However, initial state known fixed, methods
often modified take advantage fact. example, finite-horizon
problem, finite number belief states reached given initial state.
case often easier enumerate possible histories (sequences actions
observations) represent problem using stochastic decision trees (Raiffa, 1970).
example two-step decision tree shown Figure 9.
algorithm solving stochastic decision tree basically mimics value-function
updates, restricted situations reached initial belief state.
key diculty number possible trajectories grows exponentially
horizon interest.
2.7.1 Combining Dynamic-Programming Decision-Tree Techniques

solve POMDP fixed initial belief state, apply two strategies: one constructs decision tree first solves it, solves problem backward
fashion via dynamic programming. Unfortunately, techniques inecient, one
suffering exponential growth decision tree size, super-exponential
growth value function complexity. However, two techniques combined
48

fiValue-Function Approximations POMDPs

way least partially eliminates disadvantages. idea based fact
two techniques work solution two different sides (one forward
backward) complexity worsens gradually. solution
compute complete kth value function using dynamic programming (value iteration)
cover remaining steps forward decision-tree expansion.
Various modifications idea possible. example, one often replace
exact dynamic programming two ecient approximations providing upper
lower bounds value function. decision tree must expanded
bounds sucient determine optimal action choice. number search
techniques developed AI literature (Korf, 1985) combined branch-and-bound
pruning (Satia & Lave, 1973) applied type problem. Several researchers
experimented solve POMDPs (Washington, 1996; Hauskrecht, 1997;
Hansen, 1998b). methods applicable problem based Monte-Carlo
sampling (Kearns, Mansour, & Ng, 1999; McAllester & Singh, 1999) real-time dynamic
programming (Barto, Bradtke, & Singh, 1995; Dearden & Boutilier, 1997; Bonet & Geffner,
1998).
2.7.2 Classical Planning Framework

POMDP problems fixed initial belief states solutions closely related
work classical planning extensions handle stochastic partially observable
domains, particularly work BURIDAN C-BURIDAN planners (Kushmerick,
Hanks, & Weld, 1995; Draper, Hanks, & Weld, 1994). objective planners
maximize probability reaching goal state. However, task similar
discounted reward task terms complexity, since discounted reward model
converted goal-achievement model introducing absorbing state (Condon,
1992).
3. Heuristic Approximations

key obstacle wider application POMDP framework computational
complexity POMDP problems. particular, finding optimal solution finitehorizon case PSPACE-hard (Papadimitriou & Tsitsiklis, 1987) discounted infinitehorizon case may even computable (Madani et al., 1999). One approach
problems approximate solution -precision. Unfortunately, even
remains intractable general POMDPs cannot approximated eciently (Burago,
Rougemont, & Slissenko, 1996; Lusena, Goldsmith, & Mundhenk, 1998; Madani et al.,
1999). reason simple problems solved optimally
near-optimally practice.
alleviate complexity problem, research POMDP area focused various
heuristic methods (or approximations without error parameter) ecient.11
Heuristic methods focus here. Thus, referring approximations, mean
heuristics, unless specifically stated otherwise.
11. quality heuristic approximation tested using Bellman error, requires one exact
update step. However, heuristic methods per se contain precision parameter.

49

fiHauskrecht

many approximation methods combinations divided two often
closely related classes: value-function approximations policy approximations.

3.1 Value-Function Approximations
main idea value-function approximation approach approximate optimal
value function V : ! IR function Vb : ! IR defined information
space. Typically, new function lower complexity (recall optimal nearoptimal value function may consist large set linear functions) easier compute
exact solution. Approximations often formulated dynamic programming
problems expressed terms approximate value-function updates Hb . Thus,
understand differences advantages various approximations exact methods,
often sucient analyze compare update rules.
3.1.1 Value-Function Bounds

Although heuristic approximations guaranteed precision, many cases
able say whether overestimate underestimate optimal value function.
information bounds used multiple ways. example, upper- lowerbounds help narrowing range optimal value function, elimination
suboptimal actions subsequent speed-ups exact methods. Alternatively, one
use knowledge value-function bounds determine accuracy controller
generated based one bounds (see Section 3.1.3). Also, instances, lower
bound alone sucient guarantee control choice always achieves expected
reward least high one given bound (Section 4.7.2).
bound property different methods determined examining updates
bound relations.

Definition 5 (Upper bound). Let H exact value-function mapping Hb apb )(b) (HV )(b) holds
proximation. say Hb upper-bounds H V (HV
every b 2 .
analogous definition constructed lower bound.
3.1.2 Convergence Approximate Value Iteration

Let Hb value-function mapping representing approximate update. approximate value iteration computes ith value function Vbi = Hb Vbi 1 . fixed-point
solution Vc = Hb Vb close approximation would represent intended output
approximation routine. main problem iteration method general
converge unique multiple solutions, diverge, oscillate, depending Hb
initial function Vb0 . Therefore, unique convergence cannot guaranteed arbitrary
mapping Hb convergence specific approximation method must proved.

Definition 6 (Convergence Hb ). value iteration Hb converges value function V0 limn!1(Hb n V0 ) exists.

50

fiValue-Function Approximations POMDPs

Definition 7 (Unique convergence Hb ). value iteration converges uniquely V
every V 2 V , limn!1(Hb n V ) exists pairs V; U 2 V , limn!1(Hb n V ) =
limn!1(Hb n U ).
sucient condition unique convergence show Hb contraction.
contraction bound properties Hb combined, additional conditions,
show convergence iterative approximation method bound. address
issue present theorem comparing fixed-point solutions two value-function mappings.
Theorem 6 Let H1 H2 two value-function mappings defined V1 V2
1. H1 , H2 contractions fixed points V1 , V2 ;
2. V1 2 V2 H2 V1 H1 V1 = V1 ;

3. H2 isotone mapping.
V2 V1 holds.

Note theorem require V1 V2 cover space value
functions. example, V2 cover possible value functions belief-state MDP,
V1 restricted space piecewise linear convex value functions.
gives us exibility design iterative approximation algorithms computing
value-function bounds. analogous theorem holds lower bound.
3.1.3 Control

approximation value-function available, used generate
control strategy. general, control solutions correspond options presented Section
2.5 include lookahead, direct (Q-function) finite-state machine designs.
drawback control strategies based heuristic approximations
precision guarantee. One way find accuracy strategies one exact
update value function approximation adopt result Theorems 1 3
Bellman error. alternative solution problem bound accuracy
controllers using upper- lower-bound approximations optimal value
function. illustrate approach, present prove (in Appendix) following
theorem relates quality bounds quality lookahead controller.

Theorem 7 Let VbU VbL upper lower bounds optimal value function
discounted infinite-horizon problem. Let = supb jVbU (b) VbL (b)j = kVbU VbL k
maximum bound difference. expected reward lookahead controller Vb LA ,
constructed either VbU VbL , satisfies kVb LA V k (1(2 )) .
3.2 Policy Approximation
alternative value-function approximation policy approximation. shown earlier,
strategy (controller) POMDP represented using finite-state machine (FSM)
model. policy iteration searches space possible policies (FSMs) optimal near-optimal solution. space usually enormous, bottleneck
51

fiHauskrecht

method. Thus, instead searching complete policy space, restrict attention
subspace believe contain optimal solution good approximation. Memoryless policies (Platzman, 1977; White & Scherer, 1994; Littman, 1994; Singh,
Jaakkola, & Jordan, 1994), policies based truncated histories (Platzman, 1977; White &
Scherer, 1994; McCallum, 1995), finite-state controllers fixed number memory
states (Platzman, 1980; Hauskrecht, 1997; Hansen, 1998a, 1998b) examples
policy-space restriction. following consider finite-state machine model
(see Section 2.6.1), quite general; models viewed special cases.
States FSM policy model represent memory controller and, general,
summarize information past activities observations. Thus, best viewed
approximations information states, feature states. transition model
controller () approximates update function information-state MDP
( ) output function FSM () approximates control function () mapping
information states actions. important property model, shown Section
2.6.2, value function fixed controller fixed initial memory state
obtained eciently solving system linear equations (Platzman, 1980).
apply policy approximation approach first need decide (1) restrict
space policies (2) judge policy quality.
restriction frequently used consider controllers fixed number
states, say k. structural restrictions narrowing space policies
restrict either output function (choice actions different controller states),
transitions current next states. general, heuristic domain-related
insight may help selecting right biases.
Two different policies yield value functions better different regions
belief space. Thus, order decide policy best, need define
importance different regions combinations. multiple solutions this.
example, Platzman (1980) considers worst-case measure optimizes worst
(minimal) value initial belief states. Let C space FSM controllers satisfying
given restrictions. quality policy worst case measure is:
max min max V C (x; b):
C 2C b2I x2M
C

Another option consider distribution initial belief states maximize
expectation value function values. However, common objective choose
policy leads best value single initial belief state b0 :
max max V C (x; b0 ):
C 2C x2M
C

Finding optimal policy case reduces combinatorial optimization problem.
Unfortunately, trivial cases, even problem computationally intractable.
example, problem finding optimal policy memoryless case (only current observations considered) NP-hard (Littman, 1994). Thus, various heuristics
typically applied alleviate diculty (Littman, 1994).

52

fiValue-Function Approximations POMDPs

Valuefunction
approximations

Gridbased linear
function methods
Section 4.7

Fully observable MDP
approximations
Section 4.1

Fast informed bound
approximations
Section 4.2

Fixed strategy
approximations
Section 4.4

Curvefitting
approximations
Section 4.6

Gridbased value interpolation
extrapolation methods
Section 4.5

Unobservable MDP
approximations
Section 4.3

Figure 10: Value-function approximation methods.
3.2.1 Randomized Policies

restricting space policies simplify policy optimization problem.
hand, simultaneously give opportunity find best optimal policy, replacing best restricted policy. point, considered deterministic
policies fixed number internal controller states, is, policies deterministic
output transition functions. However, finding best deterministic policy always best option: randomized policies, randomized output transition functions,
usually lead far better performance. application randomized (or stochastic)
policies POMDPs introduced Platzman (1980). Essentially, deterministic
policy represented randomized policy single action transition,
best randomized policy worse best deterministic policy. difference control performance two policies shows often cases number
states controller relatively small compared optimal strategy.
advantage stochastic policies space larger parameters
policy continuous. Therefore problem finding optimal stochastic policy
becomes non-linear optimization problem variety optimization methods
applied solve it. example gradient-based approach (see Meuleau et al., 1999).
4. Value-Function Approximation Methods

section discuss depth value-function approximation methods. focus approximations belief information space.12 survey known techniques,
include number new methods modifications existing methods. Figure 10
summarizes methods covered. describe methods means update rules
12. Alternative value-function approximations may work complete histories past actions observations. Approximation methods used White Scherer (1994) example.

53

fiHauskrecht

15

16

17

18

19

10

11

12

13

14

5

6

7

8

9

0

1

2

3

4

Moves

Sensors

Figure 11: Test example. maze navigation problem: Maze20.
implement, simplifies analysis theoretical comparison. focus following properties: complexity dynamic-programming (value-iteration) updates;
complexity value functions method uses; ability methods bound
exact update; convergence value iteration approximate update rules;
control performance related controllers. results theoretical analysis illustrated empirically problem agent-navigation domain. addition, use
agent navigation problem illustrate give intuitions characteristics
methods theoretical underpinning. Thus, results generalized
problems used rank different methods.
Agent-Navigation Problem

Maze20 maze-navigation problem 20 states, six actions eight observations.
maze (Figure 11) consists 20 partially connected rooms (states) robot
operates collects rewards. robot move four directions (north, south, east
west) check presence walls using sensors. But, neither \move"
actions sensor inputs perfect, robot end moving unintended
directions. robot moves unintended direction probability 0.3 (0.15
neighboring directions). move wall keeps robot
position. Investigative actions help robot navigate activating sensor inputs. Two
investigative actions allow robot check inputs (presence wall) northsouth east-west directions. Sensor accuracy detecting walls 0.75 two-wall
case (e.g. north south wall), 0.8 one-wall case (north south) 0.89
no-wall case, smaller probabilities wrong perceptions.
control objective maximize expected discounted rewards discount
factor 0.9. small reward given every action leading bumping wall
(4 points move 2 points investigative action), one large reward (150
points) given achieving special target room (indicated circle figure)
recognizing performing one move actions. collecting
reward, robot placed random new start position.
Although Maze20 problem moderate complexity regard size
state, action observation spaces, exact solution beyond reach current
exact methods. exact methods tried problem include Witness algorithm
(Kaelbling et al., 1999), incremental pruning algorithm (Cassandra et al., 1997)13
13. Many thanks Anthony Cassandra running algorithms.
54

fiValue-Function Approximations POMDPs

VMDP

*
VMDP
(s 1 )

*
VMDP
(s 2 )

VQMDP

Q *MDP(s 2 ,a 1 )

Q *MDP(s 1 ,a 1)

Q *MDP(s 2 ,a 2)

Q *MDP(s 1 ,a 2)

*
VPOMDP
0

*
VPOMDP
1

b(s1 )

0

(a)

1

b(s1 )

(b)

Figure 12: Approximations based fully observable version two state POMDP
(with states s1 ; s2 ): (a) MDP approximation; (b) QMDP approximation.
Values extreme points belief space solutions fully observable
MDP.
policy iteration FSM model (Hansen, 1998b). main obstacle preventing
algorithms obtaining optimal close-to-optimal solution complexity
value function (the number linear functions needed describe it) subsequent
running times memory problems.

4.1 Approximations Fully Observable MDP
Perhaps simplest way approximate value function POMDP assume
states process fully observable (Astrom, 1965; Lovejoy, 1993). case
optimal value function V POMDP approximated as:
Vb (b) =

X

s2S

(s);
b(s)VMDP

(8)

(s) optimal value function state fully observable version
VMDP
process. refer approximation MDP approximation. idea
approximation illustrated Figure 12a. resulting value function linear
fully defined values extreme points belief simplex. correspond
optimal values fully observable case. main advantage approximation
fully observable MDP (FOMDP) solved eciently finitehorizon problem discounted infinite-horizon problems.14 update step (fully
observable) MDP is:
8
<

ViMDP
(s; a) +
+1 (s) = max
:

X
s0 2S

9
=

P (s0 js; a)ViMDP (s0 ); :

14. solution finite-state fully observable MDP discounted infinite-horizon criterion
found eciently formulating equivalent linear programming task (Bertsekas, 1995)

55

fiHauskrecht

4.1.1 MDP Approximation

MDP-approximation approach (Equation 8) described terms valuefunction updates belief-space MDP. Although step strictly speaking redundant
here, simplifies analysis comparison approach approximations.
Let Vbi linear value function described vector ffMDP
corresponding values

ViMDP (s0 ) states s0 2 . (i + 1)th value function Vbi+1

Vbi+1 (b) =

X
s2S

2

b(s) max 4(s; a) +
a2A

= (HMDP Vbi )(b):

X
s0 2S

3

P (s0 js; a)ffMDP
(s0 )5


Vbi+1 described linear function components
MDP
ffMDP
i+1 (s) = Vi+1 (s) = max


(

(s; a) +

X
s2S

)

P (s0 js; a)ffMDP
(s0 ) :


MDP-based rule HMDP rewritten general form starts
arbitrary piecewise linear convex value function Vi , represented set linear
functions :

Vbi+1 (b) =

X
s2S

8
<

b(s) max :(s; a) +
a2A

X
s0 2S

9
=

P (s0 js; a) max ffi (s0 ); :
ffi 2



application HMDP mapping always leads linear value function.
update easy compute takes O(jAjjS j2 + j jjS j) time. reduces O(jAjjS j2 )
time MDP-based updates strung together. remarked earlier, optimal
solution infinite-horizon, discounted problem solved eciently via linear
programming.
update MDP approximation upper-bounds exact update, is, H Vbi
HMDP Vbi . show property later Theorem 9, covers cases. intuition
cannot get better solution less information, thus fully observable
MDP must upper-bound partially observable case.
4.1.2 Approximation Q-Functions (QMDP)

variant approximation based fully observable MDP uses Q-functions (Littman,
Cassandra, & Kaelbling, 1995):
X
Vb (b) = max b(s)QMDP (s; a);
a2A s2S

X
(s0 )
QMDP (s; a) = (s; a) +
P (s0 js; a)VMDP
s0 2S
optimal action-value function (Q-function) fully observable MDP. QMDP
approximation Vb piecewise linear convex jAj linear functions, corresponding
56

fiValue-Function Approximations POMDPs

one action (Figure 12b). QMDP update rule (for belief state MDP) Vbi
linear functions ffki 2 is:

Vbi+1 (b) = max

X

a2A s2S

2

b(s) 4(s; a) +

= (HQMDP Vbi )(b):

X
s0 2S

3

P (s0 js; a) max ffi (s0 )5
ffi 2



HQMDP generates value function jAj linear functions. time complexity
update MDP-approximation case { O(jAjjS j2 + j jjS j), reduces
O(jAjjS j2 ) time QMDP updates used. HQMDP contraction mapping
fixed-point solution found solving corresponding fully observable MDP.
QMDP update upper-bounds exact update. bound tighter
MDP update; is, H Vbi HQMDP Vbi HMDP Vbi , prove later Theorem 9.
inequalities hold fixed-point solutions (through Theorem 6).
illustrate difference quality bounds MDP approximation
QMDP method, use Maze20 navigation problem. measure quality
bound use mean value-function values. Since belief states equally important
assume uniformly distributed. approximate measure using
average values fixed set N = 2000 belief points. points set
selected uniformly random beginning. set chosen, fixed
remained tests (here later). Figure 13 shows results
experiment; include results fast informed bound method presented
next section.15 Figure 13 shows running times methods. methods
implemented Common Lisp run Sun Ultra 1 workstation.
4.1.3 Control

MDP QMDP value-function approximations used construct controllers based one-step lookahead. addition, QMDP approximation suitable
direct control strategy, selects action corresponding best (highest
value) Q-function. Thus, method special case Q-function approach discussed
Section 3.1.3.16 advantage direct QMDP method faster
lookahead designs. hand, lookahead tends improve control performance.
shown Figure 14, compares control performance different controllers
Maze20 problem.
quality policy b , preference towards particular initial belief state,
measured mean value-function values b uniformly distributed initial
belief states. approximate measure using average discounted rewards

15. confidence interval limits probability level 0.95 range (0:45; 0:62) respective
average scores holds bound experiments paper. relatively small
include graphs.
16. pointed Littman et al. (1995), instances, direct QMDP controller never selects
investigative actions, is, actions try gain information underlying process
state. Note, however, observation true general QMDP-based controller
direct action selection may select investigative actions, even though fully observable version
problem investigative actions never chosen.
57

fiHauskrecht

bound quality
MDP
approximation

QMDP
approximation

running times

fast informed
bound

60

140

50
time [sec]

score

120
100

40
30
20

80
10

60

0
MDP
approximation

40

QMDP
approximation

fast informed
bound

Figure 13: Comparison MDP, QMDP fast informed bound approximations:
bound quality (left); running times (right). bound-quality score
average value approximation set 2000 belief points (chosen uniformly random). methods upper-bound optimal value function,
ip bound-quality graph longer bars indicate better approximations.
2000 control trajectories obtained fixed set N = 2000 initial belief states (selected
uniformly random beginning). trajectories obtained simulation
60 steps long.17
validate comparison along averaged performance scores, must show
scores result randomness methods indeed statistically
significantly different. rely pairwise significance tests.18 summarize
obtained results, score differences 1.54, 2.09 2.86 two methods (here
later paper) sucient reject method lower score
better performer significance levels 0.05, 0.01 0.001 respectively.19 Error-bars
Figure 14 ect critical score difference significance level 0.05.
Figure 14 shows average reaction times different controllers
experiments. results show clear dominance direct QMDP controller,
need lookahead order extract action, compared two MDPbased controllers.

4.2 Fast Informed Bound Method
MDP QMDP approaches ignore partial observability use fully
observable MDP surrogate. improve approximations account (at least
17. length trajectories (60 steps) Maze20 problem chosen ensure estimates
(discounted) cumulative rewards far actual rewards infinite number steps.
18. alternative way compare two methods compute confidence limits scores inspect
overlaps. However, case, ability distinguish two methods reduced due
uctuations scores different initializations. Maze20, confidence interval limits probability
level 0.95 range (1:8; 2:3) respective average scores. covers control experiments
later. Pairwise tests eliminate dependency examining differences individual values
thus improve discriminative power.
19. critical score differences listed cover worst case combination. Thus, may pairs
smaller difference would suce.
58

fiValue-Function Approximations POMDPs

reaction times

control performance
0.025

70

lookahead
0.02

lookahead

lookahead

time [sec]

score

60
50

direct
40

lookahead

direct

0.015
0.01

30

0.005

20

0
MDP
approximation

direct

fast informed
bound

QMDP
approximation

lookahead

lookahead

MDP
approximation

direct

QMDP
approximation

fast informed
bound

Figure 14: Comparison control performance MDP, QMDP fast informed bound
methods: quality control (left); reaction times (right). quality-of-control
score average discounted rewards 2000 control trajectories obtained
fixed set 2000 initial belief states (selected uniformly random).
Error-bars show critical score difference value (1.54) two methods become statistically different significance level 0.05.
degree) partial observability propose new method { fast informed bound
method. Let Vbi piecewise linear convex value function represented set linear
functions . new update defined
8
<X

XX

9
=

X

Vbi+1 (b) = max : (s; a)b(s) +
max
P (s0 ; ojs; a)b(s)ffi (s0 );
a2A s2S

2
0
o2 s2S
2S
8
9


2

<X



X

X

= max : b(s) 4(s; a) +
max
a2A s2S
o2 2 s0 2S
= (HF IB Vbi )(b):




3
=
P (s0 ; ojs; a)ffi (s0 )5;

fast informed bound update obtained exact update following
derivation:
8
<X

X

XX

9
=

X

9
=

(H Vbi )(b) = max : (s; a)b(s) +
max
P (s0 ; ojs; a)b(s)ffi (s0 );
a2A s2S

2
o2
s0 2S s2S


8
<X

max
(s; a)b(s) +
a2A :
s2S

X

2

XX



max

o2 s2S ffi 2 s0 2S

X

X

P (s0 ; ojs; a)b(s)ffi (s0 );
3

= max b(s) 4(s; a) +
max
P (s0 ; ojs; a)ffi (s0 )5
a2A s2S

2
o2
s0 2S


X



= max b(s)ffai+1 (s)
a2A s2S
= (HF IB Vbi )(b):

value function Vbi+1 = HF IB Vbi one obtains update piecewise linear
convex consists jAj different linear functions, corresponding one
59

fiHauskrecht

action

ffai+1 (s) = (s; a) +

X

X
max
P (s0 ; ojs; a)ffi (s0 ):

2
o2
s0 2S




HF IB update ecient computed O(jAjjS j2 jjj j) time. method
always outputs jAj linear functions, computation done O(jAj2 jS j2 jj) time,
many HF IB updates strung together. significant complexity reduction
compared exact approach: latter lead function consisting jAjj jjj
linear functions, exponential number observations worst case
takes O(jAjjS j2 j jjj) time.
HF IB updates polynomial complexity one find approximation
finite-horizon case eciently. open issue remains problem finding solution
infinite-horizon discounted case complexity. address establish
following theorem.

Theorem 8 solution fast informed bound approximation found solving
MDP jS jjAjjj states, jAj actions discount factor .
full proof theorem deferred Appendix. key part proof
construction equivalent MDP jS jjAjjj states representing HF IB updates.
Since finite-state MDP solved linear program conversion, fixed-point
solution fast informed bound update computable eciently.
4.2.1 Fast Informed Bound versus Fully-Observable MDP Approximations

fast informed update upper-bounds exact update tighter MDP
QMDP approximation updates.

Theorem 9 Let Vbi corresponds piecewise linear convex value function defined
linear functions. H Vbi HF IB Vbi HQMDP Vbi HMDP Vbi :
key trick deriving result swap max sum operators (the
proof Appendix) thus obtain upper-bound inequalities
subsequent reduction complexity update rules compared exact update.
shown Figure 15. UMDP approximation, included Figure 15,
discussed later Section 4.3. Thus, difference among methods boils
simple mathematical manipulations. Note inequality relations derived
updates hold fixed-point solutions (through Theorem 6).
Figure 13a illustrates improvement bound MDP-based approximations
Maze20 problem. Note, however, improvement paid increased
running-time complexity (Figure 13b).
4.2.2 Control

fast informed bound always outputs piecewise linear convex function, one
linear function per action. allows us build POMDP controller selects action
associated best (highest value) linear function directly. Figure 14 compares
control performance direct lookahead controllers MDP QMDP
controllers. see fast informed bound leads tighter bounds
60

fiValue-Function Approximations POMDPs

UMDP update:


V + 1 ( b ) = ax b ( ) ( , ) + ax
aA






exact update:


V + 1 ( b ) = ax b ( ) ( , ) +
aA


P ( ' | , )b ( )

'

ax

'



fast informed bound update:



V + 1 ( b ) = ax b ( ) ( , ) +
aA



ax




P ( ' | , ) ax ( ' )







MDP approx. update:


V + 1 ( b ) = b ( ) ax ( , ) +
aA




P ( ' , | , ) ( ' )



'

'


( s' )



P ( ' , | , )b ( ) ( ' )




QMDP approx. update:


V + 1 ( b ) = ax b ( ) ( , ) +











'S


P ( ' | , ) ax ( ' )



Figure 15: Relations exact update UMDP, fast informed bound,
QMDP MDP updates.
improved control average. However, stress currently theoretical
underpinning observation thus may true belief states
problem.
4.2.3 Extensions Fast Informed Bound Method

main idea fast informed bound method select best linear function
every observation every current state separately. differs exact update
seek linear function gives best result every observation
combination states. However, observe great deal middle ground
two extremes. Indeed, one design update rule chooses optimal
(maximal) linear functions disjoint sets states separately. illustrate idea,
assume partitioning = fS1 ; S2 ; ; Sm g state space . new update is:

Vbi+1 (b) = max
a2A

(

X
s2S

(s; a)b(s) +

X

2
4 max

X X

o2 ffi 2 s2S1 s0 2S

P (s0 ; ojs; a)b(s)ffi (s0 )+

X X
max
P (s0 ; ojs; a)b(s)ffi (s0 ) + +
2 s2S s0 2S
2




max

X X

ffi 2 s2S s0 2S


39
=
P (s0 ; ojs; a)b(s)ffi (s0 )5;

easy see update upper-bounds exact update. Exploration
approach various partitioning heuristics remains interesting open research issue.
61

fiHauskrecht

4.3 Approximation Unobservable MDP
MDP-approximation assumes full observability POMDP states obtain simpler
ecient updates. extreme discard observations available
decision maker. MDP observations called unobservable MDP (UMDP)
one may choose value-function solution alternative approximation.
find solution unobservable MDP, derive corresponding update
rule, HUMDP , similarly update partially observable case. HUMDP preserves
piecewise linearity convexity value function contraction. update
equals:
8
<X

9
=

XX

Vbi+1 (b) = max : (s; a)b(s) + max
P (s0 js; a)b(s)ffi (s0 );
a2A s2S
2 s2S s0 2S
= (HUMDP Vbi )(b);




set linear functions describing Vbi . Vbi+1 remains piecewise linear convex
consists j jjAj linear functions. contrast exact update,
number possible vectors next step grow exponentially number
observations leads jAjj jjj possible vectors. time complexity update
O(jAjjS j2 j j). Thus, starting Vb0 one linear function, running-time complexity
k updates bounded O(jAjk jS j2 ). problem finding optimal solution
unobservable MDP remains intractable: finite-horizon case NP-hard(Burago et al.,
1996), discounted infinite-horizon case undecidable (Madani et al., 1999). Thus,
usually useful approximation.
update HUMDP lower-bounds exact update, intuitive result ecting
fact one cannot better less information. provide insight
two updates related, following derivation, proves bound
property elegant way:
8
<X

X

9
=

XX

(H Vbi )(b) = max : (s; a)b(s) +
max
P (s0 ; ojs; a)b(s)ffi (s0 );
a2A s2S

2
o2
s0 2S s2S
8
9


<X

(s; a)b(s) + max
max
a2A :
ff2
s2S

8
<X







XXX

o2 s2S s0 2S

XX

=

P (s0 ; ojs; a)b(s)ffi (s0 );
9
=

= max : (s; a)b(s) + max
P (s0 js; a)b(s)ffi (s0 );
a2A s2S
2 s2S s0 2S
= (HUMDP Vbi )(b):




see difference exact UMDP updates max
sum next-step observations exchanged. causes choice vectors
HUMDP become independent observations. sum max operations
exchanged, observations marginalized out. Recall idea swaps leads
number approximation updates; see Figure 15 summary.
62

fiValue-Function Approximations POMDPs

4.4 Fixed-Strategy Approximations
finite-state machine (FSM) model used primarily define control strategy.
strategy require belief state updates since directly maps sequences observations
sequences actions. value function FSM strategy piecewise linear convex
found eciently number memory states (Section 2.6.1).
policy iteration policy approximation contexts value function specific strategy
used quantify goodness policy first place, value function alone
used substitute optimal value function. case, value function
(defined belief space) equals
V C (b) = max V C (x; b);
x2M

P

V C (x; b) = s2S V C (x; s)b(s) obtained solving set jS jjM j linear equations
(Section 2.6.2). remarked earlier, value fixed strategy lower-bounds
optimal value function, V C V .
simplify comparison fixed-strategy approximation approximations,
rewrite solution terms fixed-strategy updates
8
<X

9
=

XXX

Vbi+1 (b) = max : (s; (x))b(s) +
P (o; s0 js; (x))b(s)ffi ((x; o); s0 ); ;
x2M s2S
o2 s2S s0 2S
8
9
<X

2

XX

3

=

= max : b(s) 4(s; (x)) +
P (o; s0 js; (x))ffi ((x; o); s0 )5;
x2M s2S
o2 s0 2S
= (HF SM Vbi )(b):

value function Vbi piecewise linear convex consists jM j linear functions
ffi (x; :). infinite-horizon discounted case ffi (x; s) represents ith approximation
V C (x; s). Note update applied finite-horizon case straightforward
way.
4.4.1 Quality Control

Assume FSM strategy would use substitute optimal
control policy. three different ways use extract control.
first simply execute strategy represented FSM. need
update belief states case. second possibility choose linear functions
corresponding different memory states associated actions repeatedly every
step. refer controller direct (DR) controller. approach requires
updating belief states every step. hand control performance
worse FSM control. final strategy discards information
actions extracts policy using value function Vb (b) one-step lookahead.
method (LA) requires belief state updates lookaheads leads worst
reactive time. DR, however, strategy guaranteed worse FSM
controller. following theorem relates performances three controllers.
63

fiHauskrecht

control performance

reaction times

70

0.025
60

50

time [sec]

score

0.02

40

0.015
0.01

30

0.005

20

0

0.0001
DR controller

FSM controller

FSM controller

LA controller

DR controller

LA controller

Figure 16: Comparison three different controllers (FSM, DR LA) Maze20
problem collection one-action policies: control quality (left) response time (right). Error-bars control performance graph indicate
critical score difference two methods become statistically different
significance level 0.05.

Theorem 10 Let CF SM FSM controller. Let CDR CLA direct
one-step-lookahead controllers constructed based CF SM . V C (b) V C (b)
V C (b) V C (b) hold belief states b 2 .
Though prove direct controller lookahead controller
always better underlying FSM controller (see Appendix full proof
theorem), cannot show similar property first two controllers initial
belief states. However, lookahead approach typically tends dominate, ecting
usual trade-off control quality response time. illustrate trade-off
running Maze20 example collection jAj one-action policies, generating
sequence action. Control quality response time results shown Figure
16. see controller based FSM fastest three,
worst terms control quality. hand, direct controller slower (it needs
update belief states every step) delivers better control. Finally, lookahead
controller slowest best control performance.
F SM

F SM

DR

LA

4.4.2 Selecting FSM Model

quality fixed-strategy approximation depends strongly FSM model used.
model provided priori constructed automatically. Techniques automatic
construction FSM policies correspond search problem either complete
restricted space policies examined find optimal near-optimal policy
space. search process equivalent policy approximations policy-iteration
techniques discussed earlier Sections 2.6 3.2.

64

fiValue-Function Approximations POMDPs

4.5 Grid-Based Approximations Value Interpolation-Extrapolation
value function continuous belief space approximated finite set grid
points G interpolation-extrapolation rule estimates value arbitrary
point belief space relying points grid associated values.
Definition 8 (Interpolation-extrapolation rule) Let f : ! IR real-valued function
defined information space , G = fbG1 ; bG2 ; bGk g set grid points G =
f(bG1 ; f (bG1 )); (bG2 ; f (bG2 )); ; (bGk ; f (bGk))g set point-value pairs. function RG :
(I IR)jGj ! IR estimates f point information space using
values associated grid points called interpolation-extrapolation rule.
main advantage interpolation-extrapolation model estimating true value
function requires us compute value updates finite set grid points
G. Let Vbi approximation ith value function. approximation
(i + 1)th value function Vbi+1 obtained
Vbi+1 (b) = RG (b; Gi+1 );
values associated every grid point bGj 2 G (and included Gi+1 ) are:

'i+1 (bGj ) = (H Vbi )(bGj ) = max
a2A

(

(b; a) +

X
2

P (ojb; a)Vbi ( (bGj ; o; a))

)

:

(9)

grid-based update described terms value-function mapping HG :
Vbi+1 = HG Vbi . complexity update O(jGjjAjjS j2 jjCEval (RG ; jGj))
CEval (RG ; jGj) computational cost evaluating interpolation-extrapolation rule
RG jGj grid points. show later (Section 4.5.3), instances, need
evaluate interpolation-extrapolation rule every step eliminated.
4.5.1 Family Convex Rules

number possible interpolation-extrapolation rules enormous. focus
set convex rules relatively small important subset interpolationextrapolation rules.20

Definition 9 (Convex rule) Let f function defined space , G = fbG1 ; bG2 ; bGk g
set grid points, G = f(bG1 ; f (bG1 )); (bG2 ; f (bG2 )); ; (bGk ; f (bGk ))g set pointvalue pairs. rule RG estimating f using G called convex every b 2 ,
value fb(b) is:
fb(b) = RG (b; G ) =
0 bj 1 every j = 1; ; jGj,

jGj
X
bj f (bj );

j =1

PjGj

b
j =1 j

= 1.

20. note convex rules used work special case averagers introduced Gordon (1995).
difference minor; definition averager includes constant (independent grid points
values) added convex combination.
65

fiHauskrecht

key property convex rules corresponding grid-based update HG
contraction max norm (Gordon, 1995). Thus, approximate value iteration based
HG converges unique fixed-point solution. addition, HG based convex rules
isotone.
4.5.2 Examples Convex Rules

family convex rules includes approaches commonly used practice,
nearest neighbor, kernel regression, linear point interpolations many others.
Take, example, nearest-neighbor approach. function belief point b
estimated using value grid point closest terms distance metric
defined belief space. Then, point b, exactly one nonzero parameter
bj = 1 k b bGj kM k b bGi kM holds = 1; 2; ; k.
zero. Assuming Euclidean distance metric, nearest-neighbor approach leads
piecewise constant approximation, regions equal values correspond regions
common nearest grid point.
nearest neighbor estimates function value taking account one
grid point value. Kernel regression expands upon using grid points.
adds weights contributions (values) according distance target
point. example, assuming Gaussian kernels, weight grid point bGj

bj = exp kb

bG
k2M =22 ;
j

P
normalizing constant ensuring jjG=1j bj = 1 parameter
attens narrows weight functions. Euclidean metric, kernel-regression
rule leads smooth approximation function.
Linear point interpolations subclass convex rules addition constraints
Definition 9 satisfy
jGj
X
b = bj bGj :
j =1

is, belief point b convex combination grid points corresponding coecients. optimal value function POMDP convex,
new constraint sucient prove upper-bound property approximation.
general, many different linear point-interpolations given grid. challenging problem find rule best approximation. discuss issues
Section 4.5.7.
4.5.3 Conversion Grid-Based MDP

Assume would find approximation value function using gridbased convex rule grid-based update (Equation 9). view process
process finding sequence values '1 (bGj ); '2 (bGj ); ; 'i (bGj ); grid-points
bGj 2 G. show instances sequence values computed without
applying interpolation-extrapolation rule every step. cases, problem
66

fiValue-Function Approximations POMDPs

converted fully observable MDP states corresponding grid-points G.21
call MDP grid-based MDP.

Theorem 11 Let G finite set grid points RG convex rule parameters bj fixed. values '(bGj ) bGj 2 G found solving fully
observable MDP jGj states discount factor .
Proof grid point bGj write:
(

'i+1 (bGj ) = max (bGj ; a) +
a2A

8
<

X
o2

X

P (ojbGj ; a)VbiG ( (bGj ; a; o))

)

39
jGj
=
X
G )5
P (ojbGj ; a) 4 o;a
'
(
b
j;k k ;
2

= max :(bGj ; a) +
a2A
o2
k=1
8
"
#9
jGj
<h
=

X
X
= max : (bGj ; a) + 'Gi (bGk )
P (ojbGj ; a)o;a
j;k ;
a2A
k=1
o2
P

G G
denoting [ o2 P (ojbj ; a)G o;a
j;k ] P (bk jbj ; a), construct fully observable
MDP problem states corresponding grid points G discount factor .
update step equals:

8
<

'i+1 (bGj ) = max :(bGj ; a) +
a2A

jGj
X
k=1

9
=

P (bGk jbGj ; a)'Gi (bGk ); :

P
prerequisite 0 bj 1 every j = 1; ; jGj jjG=1j bj = 1 guarantees
P (bGk jbGj ; a) interpreted true probabilities. Thus, one compute values '(bGj )
solving equivalent fully-observable MDP. 2
4.5.4 Solving Grid-Based Approximations

idea converting grid-based approximation grid-based MDP basis
simple powerful approximation algorithm. Brie y, key find
parameters (transition probabilities rewards) new MDP model solve
it. process relatively easy parameters used interpolate-extrapolate
value non-grid point fixed (the assumption Theorem 11). case,
determine parameters new MDP eciently one step, grid set G.
nearest neighbor kernel regression examples rules property. Note
leads polynomial-time algorithms finding values grid points (recall
MDP solved eciently finite discounted, infinite-horizon criteria).
problem solving grid-based approximation arises parameters
used interpolation-extrapolation fixed subject optimization
itself. happens, example, multiple ways interpolating value
21. note similar result proved independently Gordon (1995).
67

fiHauskrecht

point belief space would find best interpolation (leading
best values) grid points G. case, corresponding \optimal"
grid-based MDP cannot found single step iterative approximation, solving
sequence grid-based MDPs, usually needed. worst-case complexity problem
remains open question.
4.5.5 Constructing Grids

issue touched far selection grids. multiple ways
select grids. divide two classes { regular non-regular grids.
Regular grids (Lovejoy, 1991a) partition belief space evenly equal-size regions.22
main advantage regular grids simplicity locate grid points
neighborhood belief point. disadvantage regular grids
restricted specific number points, increase grid resolution paid
exponential increase grid size. example, sequence regular grids
20-dimensional belief space (corresponds POMDP 20 states) consists 20, 210,
1540, 8855, 42504, grid points.23 prevents one using method higher
grid resolutions problems larger state spaces.
Non-regular grids unrestricted thus provide exibility grid resolution must increased adaptively. hand, due irregularities, methods
locating grid points adjacent arbitrary belief point usually complex
compared regular grids.
4.5.6 Linear Point Interpolation

fact optimal value function V convex belief-state MDPs used
show approximation based linear point interpolation always upper-bounds
exact solution (Lovejoy, 1991a, 1993). Neither kernel regression nearest neighbor
guarantee us bound.

Theorem 12 (Upper bound property grid-based point interpolation update). Let Vbi
convex value function. H Vbi HG Vbi .
upper-bound property HG update convex value functions follows directly
Jensen's inequality. convergence upper-bound follows Theorem 6.
Note point-interpolation update imposes additional constraint choice
grid points. particular, easy see valid grid must include extreme points belief simplex (extreme points correspond (1; 0; 0; ); (0; 1; 0; ),
22. Regular grids used Lovejoy (1991a) based Freudenthal triangulation (Eaves, 1984). Essentially, idea used partition evenly n-dimensional subspace IR . fact,
ane transform allows us map isomorphically grid points belief space grid points
n-dimensional space (Lovejoy, 1991a).
23. number points regular grid sequence given (Lovejoy, 1991a):
n

+ jS j 1)!
;
jGj = (M
!(jS j 1)!

= 1; 2; grid refinement parameter.

68

fiValue-Function Approximations POMDPs

etc.). Without extreme points one would unable cover whole belief space via
interpolation. Nearest neighbor kernel regression impose restrictions grid.
4.5.7 Finding best interpolation

general, multiple ways interpolate point belief space. objective
find best interpolation, is, one leads tightest upper bound
optimal value function.
Let b belief point f(bj ; f (bj ))jbj 2 Gg set grid-value pairs. best
interpolation point b is:
jGj
X
fb(b) = min j f (bj )
j =1
P
= 1; ; jGj, jjG=1j j

P
subject 0 j 1 j
= 1, b = jjG=1j j bGj .
linear optimization problem. Although solved polynomial time
(using linear programming techniques), computational cost still relatively
large, especially considering fact optimization must repeated many times.
alleviate problem seek ecient ways finding interpolation, sacrificing
optimality.
One way find (suboptimal) interpolation quickly apply regular grids proposed
Lovejoy (1991a). case value belief point approximated using
convex combination grid points closest it. approximation leads piecewise linear
convex value functions. interpolations fixed here, problem finding
approximation converted equivalent grid-based MDP solved
finite-state MDP. However, pointed previous section, regular grids must use
specific number grid points increase resolution grid paid
exponential increase grid size. feature makes method less attractive
problem large state space need achieve high grid resolution.24
present work focus non-regular (or arbitrary) grids. propose interpolation approach searches limited space interpolations guaranteed run
time linear size grid. idea approach interpolate point
b belief space dimension jS j set grid points consists arbitrary
grid point b0 2 G jS j 1 extreme points belief simplex. coecients
interpolation found eciently search best interpolation. Let
b0 2 G grid point defining one interpolation. value point b satisfies
0

bb
Vbi (b) = min
0 Vi (b);
b 2G

Vbib0 value interpolation grid point b0 . Figure 17 illustrates
resulting approximation. function characterized \sawtooth" shape,
uenced choice interpolating set.
find best value-function solution close approximation apply value
iteration procedure search best interpolation every update step.
24. One solution problem may use adaptive regular grids grid resolution increased
parts belief space. leave idea future work.
69

fiHauskrecht

V(b)

V(b)
*

V(b)

0 b
0

b1

b2

b3

b 4 1 b(s )
1

Figure 17: Value-function approximation based linear-time interpolation approach
(a two-dimensional case). Interpolating sets restricted single internal
point belief space.
drawback approach interpolations may remain unchanged many
update steps, thus slowing solution process. alternative approach solve
sequence grid-based MDPs instead. particular, every stage find best
(minimum value) interpolations belief points reachable grid points one step, fix
coecients interpolations (s), construct grid-based MDP solve (exactly
approximately). process repeated improvement (or improvement
larger threshold) seen values different grid points.
4.5.8 Improving Grids Adaptively

quality approximation (bound) depends strongly points used grid.
objective provide good approximation smallest possible set grid
points. However, task impossible achieve, since cannot known advance
(before solving) belief points pick. way address problem build grids
incrementally, starting small set grid points adding others adaptively,
places greater chance improvement. key part approach
heuristic choosing grid points added next.
One heuristic method developed attempts maximize improvements bound
values via stochastic simulations. method builds fact every interpolation
grid must include extreme points (otherwise cannot cover entire belief space).
extreme points values affect grid points, try improve
values first place. general, value grid point b improves
precise values used successor belief points, is, belief states correspond
(b; ; o) choice observation o. current optimal action choice b.
Incorporating points grid makes larger improvement value
initial grid point b likely. Assuming initial point extreme point,
heuristic tends improve value point. Naturally, one proceed
selection incorporating successor points first-level successors
grid well, forth.
70

fiValue-Function Approximations POMDPs

generate new grid points (G; Vb G )
set Gnew = fg
extreme points b
repeat b 2= G [ Gnew
n

P
set = arg maxa (b; a) + o2 P (ojb; a)Vb G ( (b; a; o))
select observation according P (ojb; )
update b = (b; ; o)
add b Gnew
return Gnew
Figure 18: Procedure generating additional grid points based bound improvement heuristic.

bound quality
MDP

fast interpolation
QMDP
informed regular grid

interpolation
adaptive grid

interpolation
random grid

140
40 80 120 160 200 240 280 320 360 400

120

score

40
100

80

80

120 160 200

240 280 320 360 400

60

40

Figure 19: Improvement upper bound quality grid-based point-interpolations
based adaptive-grid method. method compared randomly
refined grid regular grid 210 points. upper-bound approximations (the MDP, QMDP fast informed bound methods) included
comparison.
capture idea, generate new grid points via simulation, starting one
extremes belief simplex continuing belief point currently
grid reached. algorithm implements bound improvement heuristic
expands current grid G set jS j new grid points relying current
value-function approximation Vb G shown Figure 18.
Figure 19 illustrates performance (bound quality) adaptive grid method
Maze20 problem. use combination adaptive grids linear-time
interpolation approach. method gradually expands grid 40 point increments
400 grid points. Figure 19 shows performance random-grid method
71

fiHauskrecht

running times
5000
4500
4000
400

time [sec]

3500
3000

360

2500
320

400

2000
360

280

1500

280 320

240

1000

240

200

500
1.26

1.26

50.02

40 80

120

200

160
120
40 80

160

0
MDP QMDP fast
informed

interpolation
regular grid

interpolation
adaptive grid

interpolation
random grid

Figure 20: Running times grid-based point-interpolation methods. Methods tested include adaptive grid, random grid, regular grid 210 grid
points. Running times adaptive-grid cumulative, ecting dependencies higher grid resolutions lower-level resolutions. running
time results MDP, QMDP, fast informed bound approximations
shown comparison.
new points grid selected iniformly random (results 40 grid point increments
shown). addition, figure gives results regular grid interpolation (based
Lovejoy (1991a)) 210 belief points upper-bound methods: MDP,
QMDP fast informed bound approximations.
see dramatic improvement quality bound adaptive method.
contrast this, uniformly sampled grid (random-grid approach) hardly changes
bound. two reasons this: (1) uniformly sampled grid points likely
concentrated center belief simplex; (2) transition matrix Maze20
problem relatively sparse, belief points one obtains extreme points one
step boundary simplex. Since grid points center simplex
never used interpolate belief states reachable extremes one step cannot
improve values extremes bound change.
One drawback adaptive method running time (for every grid size need
solve sequence grid-based MDPs). Figure 20 compares running times different
methods Maze20 problem. grid-expansion adaptive method depends
value function obtained previous steps, plot cumulative running times.
see relatively large increase running time, especially larger grid sizes, ecting
trade-off bound quality running time. However, note
adaptive-grid method performs quite well initial steps, 80 grid
points outperforms regular grid (with 210 points) bound quality.
Finally, note heuristic approaches constructing adaptive grids point
interpolation possible. example, different approach refines grid ex-

72

fiValue-Function Approximations POMDPs

control performance
70

score

60

400

50

200
40 200 400

40
40

200
40

400
200

400

40

30

20

fast
interpolation interpolation
MDP QMDP informed (regular grid) (adaptive grid)

interpolation
(random grid)

nearest-neighbor nearest-neighbor
(adaptive grid)
(random grid)

Figure 21: Control performance lookahead controllers based grid-based point interpolation nearest neighbor methods varying grid sizes. results
compared MDP, QMDP fast informed bound controllers.
amining differences values current grid points recently proposed Brafman
(1997).
4.5.9 Control

Value functions obtained different grid-based methods define variety controllers. Figure 21 compares performances lookahead controllers based point-interpolation
nearest-neighbor methods. run two versions approaches, one adaptive grid, random grid, show results obtained 40, 200 400
grid points. addition, compare performances interpolation regular
grids (with 210 grid points), MDP, QMDP fast informed bound approaches.
Overall, performance interpolation-extrapolation techniques tested
Maze20 problem bit disappointing. particular, better scores achieved
simpler QMDP fast informed bound methods. see that, although heuristics
improved bound quality approximations, lead similar improvement
QMDP fast informed bound methods terms control. result
shows bad bound (in terms absolute values) always imply bad control
performance. main reason control performance uenced mostly
relative rather absolute value-function values (or, words, shape
function). interpolation-extrapolation techniques use (except regular grid
interpolation) approximate value function functions piecewise linear
convex; interpolations based linear-time interpolation technique
sawtooth-shaped function, nearest-neighbor leads piecewise-constant function.
allow match shape optimal function correctly.
factor affects performance large sensitivity methods selection grid
points, documented, example, comparison heuristic random grids.

73

fiHauskrecht

tests focused lookahead controllers only. However, alternative way
define controller grid-based interpolation-extrapolation methods use Q-function
approximations instead value functions, either direct lookahead designs.25 Qfunction approximations found solving grid-based MDP, keeping
values (functions) different actions separate end.

4.6 Approximations Value Functions Using Curve Fitting (Least-Squares
Fit)
alternative way approximate function continuous space use curve-fitting
techniques. approach relies predefined parametric model value function
set values associated finite set (grid) belief points G. approach
similar interpolation-extrapolation techniques relies set belief-value
pairs. difference curve fitting, instead remembering belief-value pairs,
tries summarize terms given parametric function model. strategy seeks
best possible match model parameters observed point values. best
match defined using various criteria, often least-squares fit criterion,
objective minimize
Error(f ) =

1X
[y
2 j j

f (bj )]2 :

bj yj correspond belief point associated value. index j ranges
points sample set G.
4.6.1 Combining Dynamic Programming Least-Squares Fit

least-squares approximation function used construct dynamic-programming
algorithm update step: Vbi+1 = HLSF Vbi . approach two steps. First,
obtain new values set sample points G:

'i+1 (b) = (H Vbi )(b) = max
a2A

(

X
s2S

(s; a)b(s) +

XX
o2 s2S

)
b
P (ojs; a)b(s)Vi ( (b; a; o)) :

Second, fit parameters value-function model Vbi+1 using new sample-value pairs
square-error cost function. complexity update O(jGjjAjjS j2 jjCEval (Vbi )+
CFit (Vbi+1 ; jGj)) time, CEval(Vbi ) computational cost evaluating Vbi
CFit (Vbi+1 ; jGj) cost fitting parameters Vbi+1 jGj belief-value pairs.
advantage approximation based least-squares fit requires us
compute updates finite set belief states. drawback approach
that, combined value-iteration method, lead instability and/or
divergence. shown MDPs several researchers (Bertsekas, 1994; Boyan
& Moore, 1995; Baird, 1995; Tsitsiklis & Roy, 1996).
25. similar QMDP method, allows lookahead greedy designs. fact, QMDP
viewed special case grid-based method Q-function approximations, grid
points correspond extremes belief simplex.
74

fiValue-Function Approximations POMDPs

4.6.2 On-line Version Least-Squares Fit

problem finding set parameters best fit solved available
optimization procedure. includes on-line (or instance-based) version gradient
descent method, corresponds well-known delta rule (Rumelhart, Hinton, &
Williams, 1986).
Let f denote parametric value function belief space adjustable weights
w = fw1 ; w2 ; ; wk g. on-line update weight wi computed as:

wi

wi ffi (f (bj ) yj )

@f
j ;
@wi b
j

ffi learning constant, bj yj last-seen point value. Note
gradient descent method requires function differentiable regard
adjustable weights.
solve discounted infinite-horizon problem, stochastic (on-line) version
least-squares fit combined either parallel (synchronous) incremental (GaussSeidel) point updates. first case, value function previous step fixed
new value function computed scratch using set belief point samples
values computed one-step expansion. parameters stabilized (by
attenuating learning rates), newly acquired function fixed, process proceeds
another iteration. incremental version, single value-function model
time updated used compute new values sampled points. Littman et al. (1995)
Parr Russell (1995) implement approach using asynchronous reinforcement
learning backups sample points updated next obtained via stochastic
simulation. stress versions subject threat instability divergence,
remarked above.
4.6.3 Parametric Function Models

apply least-squares approach must first select appropriate value function
model. Examples simple convex functions linear quadratic functions,
complex models possible well.
One interesting relatively simple approach based least-squares approximation linear action-value functions (Q-functions) (Littman et al., 1995).
value function Vbi+1 approximated piecewise linear convex combination Qb i+1
functions:
Vbi+1 (b) = max Qb i+1 (b; a);
a2A
Qb i+1 (b; a) least-squares fit linear function set sample points G.
Values points G obtained

'ai+1 (b) = (b; a) +

X

o2

P (ojb; a)Vbi ( (b; o; a)):

method leads approximation jAj linear functions coecients
functions found eciently solving set linear equations. Recall two
approximations (the QMDP fast informed bound approximations) work
75

fiHauskrecht

jAj linear functions. main differences methods QMDP

fast informed bound methods update linear functions directly, guarantee upper
bounds unique convergence.
sophisticated parametric model convex function softmax model (Parr
& Russell, 1995):
2

Vb (b) = 4

"
X X

ff2

s2S

#k 3 1
ff(s)b(s) 5

k

;

set linear functions adaptive parameters fit k \temperature" parameter provides better fit underlying piecewise linear convex function
larger values. function represents soft approximation piecewise linear convex
function, parameter k smoothing approximation.
4.6.4 Control

tested control performance least-squares approach linear Q-function
model (Littman et al., 1995) softmax model (Parr & Russell, 1995). softmax
model varied number linear functions, trying cases 10 15 linear functions
respectively. first set experiments used parallel (synchronous) updates
samples fixed set 100 belief points. applied stochastic gradient descent techniques
find best fit cases. tested control performance value-function
approximations obtained 10, 20 30 updates, starting QMDP solution.
second set experiments, applied incremental stochastic update scheme
Gauss-Seidel-style updates. results method acquired every grid point
updated 150 times, learning rates decreasing linearly range 0:2
0:001. started QMDP solution. results lookahead controllers
summarized Figure 22, shows control performance direct Q-function
controller and, comparison, results QMDP method.
linear-Q function model performed well results lookahead design
better results QMDP method. difference quite apparent
direct approaches. general, good performance method attributed
choice function model let us match shape optimal value function
reasonably well. contrast, softmax models (with 10 15 linear functions)
perform expected. probably softmax model linear functions
updated every sample point. leads situations multiple linear functions
try track belief point update. circumstances hard capture
structure optimal value function accurately. negative feature
effects on-line changes linear functions added softmax approximation,
thus could bias incremental update schemes. ideal case, would identify
one vector responsible specific belief point update (modify) vector.
linear Q-function approach avoids problem always updating single linear
function (corresponding action).

76

fiValue-Function Approximations POMDPs

control performance
70

60
lookahead

10 20
iter iter 30 stoch
iter

10 20
iter iter

10 20 30
iter iter iter

score

50

40

stoch

30
iter stoch

20
10 iter
iter

30
iter

stoch

direct

30

20
QMDP
approximation

linear Q-function
lookahead

linear Q-function
direct

softmax
(10 linear functions)

softmax
(15 linear functions)

Figure 22: Control performance least-squares fit methods. Models tested include: linear
Q-function model (with direct lookahead control) softmax models 10 15 linear functions (lookahead control only). Value functions
obtained 10, 20 30 synchronous updates value functions obtained
incremental stochastic update scheme used define different
controllers. comparison, include results two QMDP controllers.

4.7 Grid-Based Approximations Linear Function Updates
alternative grid-based approximation method constructed applying Sondik's
approach computing derivatives (linear functions) points grid (Lovejoy, 1991a,
1993). Let Vbi piecewise linear convex function described set linear functions .
new linear function belief point b action computed eciently
(Smallwood & Sondik, 1973; Littman, 1996)
ffb;a
i+1 (s) = (s; a) +

XX
o2 s0 2S

P (s0 ; ojs; a)ffi(b;a;o) (s0 );

(b; a; o) indexes linear function ffi set linear functions
maximizes expression
"
X X

s0 2S s2S

(10)


(defining Vbi )

#

P (s0 ; ojs; a)b(s) ffi (s0 )

fixed combination b; a; o. optimizing function b acquired choosing
vector best overall value action vectors. is, assuming bi+1
set candidate linear functions, resulting functions satisfies
= arg max X ffb (s)b(s):
ffb;i+1
i+1
+1 2 +1 s2S
collection linear functions obtained set belief points combined
piecewise linear convex value function. idea behind number exact
b


b


77

fiHauskrecht

V(b)
*

V(b)
V(b)
new linear function

0

b

1

b(s1 )

Figure 23: incremental version grid-based linear function method. piecewise
linear lower bound improved new linear function computed belief
point b using Sondik's method.
algorithms (see Section 2.4.2). However, exact case, set points cover
linear functions defining new value function must located first, hard task
itself. contrast, approximation method uses incomplete set belief points
fixed least easy locate, example via random heuristic selection. use
HGL denote value-function mapping grid approach.
advantage grid-based method leads ecient updates.
time complexity update polynomial equals O(jGjjAjjS j2 jj). yields set
jGj linear functions, compared jAjj jjj possible functions exact update.
Since set grid-points incomplete, resulting approximation lower-bounds
value function one would obtain performing exact update (Lovejoy, 1991a).

Theorem 13 (Lower-bound property grid-based linear function update). Let Vbi
piecewise linear value function G set grid points used compute linear function
updates. HGL Vbi H Vbi .
4.7.1 Incremental Linear-Function Approach

drawback grid-based linear function method HGL contraction
discounted infinite-horizon case, therefore value iteration method based
mapping may converge (Lovejoy, 1991a). remedy problem, propose
incremental version grid-based linear function method. idea refinement
prevent instability gradually improving piecewise linear convex lower bound
value function.
Assume Vbi V convex piecewise linear lower bound optimal value
function defined linear function set , let ffb linear function point b
computed Vbi using Sondik's method. one construct new improved
value function Vbi+1 Vbi simply adding new linear function ffb . is:
i+1 = [ ffb . idea incremental update, illustrated Figure 23, similar
incremental methods used Cheng (1988) Lovejoy (1993). method
78

fiValue-Function Approximations POMDPs

running times

bound quality

2500

65
9

2000

60

10

8

score

55
50
45
40

2

3

4

5

6

7

8

9

10

2

3

4

5

6

7

8

9

time [sec]

7

10

1

6

1500

10

5

9
8
1000

4

7
6

3

5
4

500

1

2

3

35
1.26

50.02

1

2

1

0

30
standard approach

QMDP

incremental approach

fast
informed

standard approach

incremental approach

Figure 24: Bound quality running times standard incremental version
grid-based linear-function method fixed 40-point grid. Cumulative
running times (including previous update cycles) shown methods.
Running times QMDP fast informed bound methods included
comparison.
extended handle set grid points G straightforward way. Note
adding one new linear functions , previous linear functions may
become redundant removed value function. Techniques redundancy
checking applied exact approaches (Monahan, 1982; Eagle, 1984).
incremental refinement stable converges fixed set grid points.
price paid feature linear function set grow size iteration
steps. Although growth linear number iterations, compared
potentially exponential growth exact methods, linear function set describing
piecewise linear approximation become huge. Thus, practice usually stop
incremental updates well method converges. question remains open
complexity (hardness) problem finding fixed-point solution fixed set
grid points G.
Figure 24 illustrates trade-offs involved applying incremental updates
compared standard fixed-grid approach Maze20 problem. use
grid 40 points techniques initial value function. Results 1-10
update cycles shown. see incremental method longer running times
standard method, since number linear functions grow every update.
hand, bound quality incremental method improves rapidly
never become worse update steps.
4.7.2 Minimum Expected Reward

incremental method improves lower bound value function. value function, say Vbi , used create controller (with either lookahead direct-action
choice). general case, cannot say anything performance quality
controllers regard Vbi . However, certain conditions performance
controllers guaranteed never fall Vbi . following theorem (proved
Appendix) establishes conditions.

Theorem 14 Let Vbi value function obtained via incremental linear function method,
starting Vb0 , corresponds fixed strategy C0 . Let CLA;i CDR;i two
79

fiHauskrecht

controllers based Vbi : lookahead controller direct action controller, V C ,
VC
respective value functions. Vbi V C
Vbi V C
hold.
note property holds incremental version exact value iteration.
is, lookahead direct controllers perform worse Vi obtained
incremental updates V0 corresponding FSM controller C0 .
LA;i

DR;i

LA;i

DR;i

4.7.3 Selecting Grid Points

incremental version grid-based linear-function approximation exible
works arbitrary grid.26 Moreover, grid need fixed changed
line. Thus, problem finding grids reduces problem selecting belief points
updated next. One apply various strategies this. example, one use
fixed set grid points update repeatedly, one select belief points line
using various heuristics.
incremental linear function method guarantees value function always
improved (all linear functions previous steps kept unless found redundant).
quality new linear function (to added next) depends strongly quality
linear functions obtained previous steps. Therefore, objective select order
points better chances larger improvement. designed two heuristic
strategies selecting ordering belief points.
first strategy attempts optimize updates extreme points belief simplex
ordering heuristically. idea heuristic based fact states
higher expected rewards (e.g. designated goal states) backpropagate effects
(rewards) locally. Therefore, desirable states neighborhood highest
reward state updated first, distant ones later. apply idea order
extreme points belief simplex, relying current estimate value function
identify highest expected reward states POMDP model determine
neighbor states.
second strategy based idea stochastic simulation. strategy generates
sequence belief points likely reached (fixed) initial belief point.
points sequence used reverse order generate updates. intent
heuristic \maximize" improvement value function initial fixed
point. run heuristic, need find initial belief point set initial belief
points. address problem, use first heuristic allows us order
extreme points belief simplex. points used initial beliefs
simulation part. Thus, two-tier strategy: top-level strategy orders extremes
belief simplex, lower-level strategy applies stochastic simulation generate
sequence belief states likely reachable specific extreme point.
tested order heuristics two-tier heuristics Maze20 problem,
compared two simple point selection strategies: fixed-grid strategy,
set 40 grid points updated repeatedly, random-grid strategy,
points always chosen uniformly random. Figure 25 shows bound quality
26. restriction grid points must included grid, required
example linear point-interpolation scheme, use extreme points belief
simplex.
80

fiValue-Function Approximations POMDPs

bound quality
65
60

score

55
2

50

5 6 7 8 9 10
3 4
2

1

3

7
4 5 6

8 9 10
2

4

3

2

7 8 9 10
5 6

3

1

1

1

45

7 8 9 10
4 5 6

40
35
30
fixed grid

random grid

order heuristic

2-tier heuristic

Figure 25: Improvements bound quality incremental linear-function method
four different grid-selection heuristics. cycle includes 40 grid-point
updates.
methods 10 update cycles (each cycle consists 40 grid point updates)
Maze20 problem. see differences quality value-function approximations
different strategies (even simple ones) relatively small. note
observed similar results problems, Maze20.
relatively small improvement heuristics explained fact
every new linear function uences larger portion belief space thus method
less sensitive choice specific point.27 However, another plausible explanation heuristics good accurate heuristics combinations
heuristics could constructed. Ecient strategies locating grid points used
exact methods, e.g. Witness algorithm (Kaelbling et al., 1999) Cheng's methods (Cheng, 1988) potentially applied problem. remains open area
research.
4.7.4 Control

grid-based linear-function approach leads piecewise linear convex approximation. Every linear function comes natural action choice lets us choose
action greedily. Thus run lookahead direct controllers. Figure 26
compares performance four different controllers fixed grid 40 points, combining standard incremental updates lookahead direct greedy control 1,
5 10 update cycles. results (see Figure 24) illustrate trade-offs
computational time obtaining solution quality. see incremental
approach lookahead controller design tend improve control performance.
prices paid worse running reaction times, respectively.
27. small sensitivity incremental method selection grid points would suggest one
could, many instances, replace exact updates simpler point selection strategies. could
increase speed exact value-iteration methods (at least initial stages), suffer
ineciencies associated locating complete set grid points updated every step. However,
issue needs investigated.
81

fiHauskrecht

control performance
70

5

60

10

10
1

lookahead

lookahead

5

10

5

10

1

5
1

1

score

50
direct
40

direct

30

20
QMDP

fast
informed

direct
standard

lookahead
standard

direct
incremental

lookahead
incremental

Figure 26: Control performance four different controllers based grid-based linear function updates 1, 5 10 update cycles 40-point grid. Controllers represent combinations two update strategies (standard incremental) two action-extraction techniques (direct lookahead). Running
times two update strategies presented Figure 24. comparison include performances QMDP fast informed bound
methods (with direct lookahead designs).
control performance
70
5

score

5

10

10

1

1

60

1

5

10
1

5

10

50

40

30

20
QMDP

fast
informed

fixed grid

random grid

order heuristic

2-tier heuristic

Figure 27: Control performances lookahead controllers based incremental linearfunction approach different point-selection heuristics 1, 5 10 improvement cycles. comparison, scores QMDP fast informed
bound approximations shown well.
Figure 27 illustrates effect point selection heuristics control. compare
results lookahead control only, using approximations obtained 1, 5 10 improvement cycles (each cycle consists 40 grid point updates). test results show that,

82

fiValue-Function Approximations POMDPs

bound quality, big differences among various heuristics, suggesting
small sensitivity control selection grid points.

4.8 Summary Value-Function Approximations
Heuristic value-function approximations methods allow us replace hard-to-compute exact
methods trade solution quality speed. numerous methods employ, different properties different trade-offs quality versus speed. Tables 1
2 summarize main theoretical properties approximation methods covered
paper. majority methods polynomial complexity least ecient (polynomial) Bellman updates. makes good candidates complex
POMDP problems reach exact methods.
methods heuristic approximations give solutions
guaranteed precision. Despite fact proved solutions methods
worse others terms value function quality (see Figure 15). one
main contributions paper. However, currently minimal theoretical results
relating methods terms control performance; exception results
FSM-controllers FSM-based approximations. key observation
quality control (lookahead control) important approximate shape
(derivatives) value function correctly. illustrated empirically gridbased interpolation-extrapolation methods Section 4.5.9 based non-convex
value functions. main challenges find ways analyzing comparing
control performance different approximations theoretically identify classes
POMDPs certain methods dominate others.
Finally, note list methods complete value-function approximation methods refinements existing methods possible. example, White
Scherer (1994) investigate methods based truncated histories lead upper
lower bound estimates value function complete information states (complete
histories). Also, additional restrictions methods change properties
generic method. example, possible additional assumptions
able ensure convergence least-squares fit approximation.
5. Conclusions

POMDPs offers elegant mathematical framework representing decision processes
stochastic partially observable domains. Despite modeling advantages, however,
POMDP problems hard solve exactly. Thus, complexity problem solvingprocedures becomes key aspect sucessful application model real-world
problems, even expense optimality. recent complexity results
approximability POMDP problems encouraging (Lusena et al., 1998; Madani
et al., 1999), focus heuristic approximations, particular approximations value
functions.

83

fiHauskrecht

Method
MDP approximation
QMDP approximation
Fast informed bound
UMDP approximation
Fixed-strategy method
Grid-based interpolation-extrapolation
Nearest neighbor
Kernel regression
Linear point interpolation
Curve-fitting (least-squares fit)
linear Q-function
Grid-based linear function method
Incremental version (start lower bound)

Bound
upper
upper
upper
lower
lower
upper
lower
lower

Isotonicity

p
p
p
p
p
-p
p
p

Contraction

-p

-*

p
p
p
p
p
-p
p
p

Table 1: Properties different value-function approximation methods: bound property,
isotonicity contraction property underlying mappings 0 < 1.
(*) Although incremental version grid-based linear-function method
contraction always converges.
Method
MDP approximation
QMDP approximation
Fast informed bound
UMDP approximation
Fixed-strategy method
Grid-based interpolation-extrapolation
Nearest neighbor
Kernel regression
Linear point interpolation
Fixed interpolation
Best interpolation
Curve-fitting (least-squares fit)
linear Q-function
Grid-based linear function method
Incremental version

Finite-horizon
P
P
P
NP-hard
P
varies
P
P
P
P
P
varies
P
P
NA

Discounted infinite-horizon
P
P
P
undecidable
P
NA
P
P
varies
P
?
NA
NA
NA
?

Table 2: Complexity value-function approximation methods finite-horizon problem
discounted infinite-horizon problem. objective discounted infinitehorizon case find corresponding fixed-point solution. complexity
results take account, addition components POMDPs,
approximation specific parameters, e.g., size grid G grid-based methods. ? indicates open instances NA methods applicable one
problems (e.g. possible divergence).

84

fiValue-Function Approximations POMDPs

5.1 Contributions
paper surveys new known value-function approximation methods solving POMDPs.
focus primarily theoretical analysis comparison methods, findings results supported experimentally problem moderate size agent
navigation domain. analyze methods different perspectives: computational complexity, capability bound optimal value function, convergence properties
iterative implementations, quality derived controllers. analysis includes new
theoretical results, deriving properties individual approximations, relations
exact methods. general, relations trade-offs among different methods
well understood. provide new insights issues analyzing
corresponding updates. example, showed differences among exact,
MDP, QMDP, fast-informed bound, UMDP methods boil simple
mathematical manipulations subsequent effect value-function approximation. allowed us determine relations among different methods terms quality
respective value functions one main results paper.
presented number new methods heuristic refinements existing
techniques. primary contributions area include fast-informed bound, gridbased point interpolation methods (including adaptive grid approaches based stochastic sampling), incremental linear-function method. showed
instances solutions obtained eciently converting original approximation equivalent finite-state MDP. example, grid-based approximations
convex rules often solved via conversion grid-based MDP (in grid points
correspond new states), leading polynomial-complexity algorithm finite discounted infinite-horizon cases (Section 4.5.3). result dramatically
improve run-time performance grid-based approaches. similar conversion
equivalent finite-state MDP, allowing polynomial-time solution discounted
infinite-horizon problem, shown fast informed bound method (Section 4.2).
5.2 Challenges Future Directions
Work POMDPs approximations far complete. complexity results
remain open, particular, complexity grid-based approach seeking best interpolation, complexity finding fixed-point solution incremental version
grid-based linear-function method. Another interesting issue needs investigation convergence value iteration least-squares approximation. Although
method unstable general case, possible certain restrictions
converge.
paper use single POMDP problem (Maze20) support theoretical
findings illustrate intuitions. Therefore, results supported theoretically (related mostly control) cannot generalized used rank different methods,
since performance may vary problems. general, area POMDPs
POMDP approximations suffers shortage larger-scale experimental work
multiple problems different complexities broad range methods. Experimental
work especially needed study compare different methods regard control
quality. main reason theoretical results relating
85

fiHauskrecht

control performance. studies help focus theoretical exploration discovering
interesting cases possibly identifying classes problems certain approximations less suitable. preliminary experimental results show
significant differences control performance among different methods
may suitable approximate control policies. example, grid-based
nearest-neighbor approach piecewise-constant approximation typically inferior
outperformed simpler (and ecient) value-function methods.
present work focused heuristic approximation methods. investigated general ( at) POMDPs take advantage additional structural refinements.
However, real-world problems usually offer structure exploited devise
new algorithms perhaps lead speed-ups. possible
restricted versions POMDPs (with additional structural assumptions) solved
approximated eciently, even though general complexity results POMDPs approximations encouraging (Papadimitriou & Tsitsiklis, 1987; Littman, 1996;
Mundhenk et al., 1997; Lusena et al., 1998; Madani et al., 1999). challenge
identify models allow ecient solutions time interesting enough
point application.
Finally, number interesting issues arise move problems large state,
action, observation spaces. Here, complexity value-function updates
belief state updates becomes issue. general, partial observability hidden
process states allow us factor decompose belief states (and updates),
even transitions great deal structure represented compactly.
Promising directions deal issues include various Monte-Carlo approaches (Isard
& Blake, 1996; Kanazawa, Koller, & Russell, 1995; Doucet, 1998; Kearns et al., 1999)),
methods approximating belief states via decomposition (Boyen & Koller, 1998, 1999),
combination two approaches (McAllester & Singh, 1999).
Acknowledgements

Anthony Cassandra, Thomas Dean, Leslie Kaelbling, William Long, Peter Szolovits
anonymous reviewers provided valuable feedback comments work. research
supported grant RO1 LM 04493 grant 1T15LM07092 National Library
Medicine, DOD Advanced Research Project Agency (ARPA) contract number
N66001-95-M-1089 DARPA/Rome Labs Planning Initiative grant F30602-95-1-0020.
Appendix A. Theorems proofs

A.1 Convergence Bound
Theorem 6 Let H1 H2 two value-function mappings defined V1 V2 s.t.
1. H1 , H2 contractions fixed points V1 , V2 ;
2. V1 2 V2 H2 V1 H1 V1 = V1 ;

3. H2 isotone mapping.
V2 V1 holds.
86

fiValue-Function Approximations POMDPs

Proof applying H2 condition 2 expanding result condition 2
get: H22 V1 H2 V1 H1 V1 = V1 . Repeating get limit V2 H2n V1
H22 V1 H2V1 H1V1 = V1 , proves result. 2
A.2 Accuracy Lookahead Controller Based Bounds
Theorem 7 Let VbU VbL upper lower bounds optimal value function
discounted infinite-horizon problem. Let = supb jVbU (b) VbL (b)j = kVbU VbL k
maximum bound difference. expected reward lookahead controller Vb LA ,
constructed either VbU VbL , satisfies kVb LA V k (1(2 )) .

Proof Let Vb denotes either upper lower bound approximation V H LA
value function mapping corresponding lookahead policy Vb . Note, since
lookahead policy always optimizes actions regard Vb , H Vb = H LA Vb must hold.
error Vb LA bounded using triangle inequality

kVb LA V k kVb LA Vb k + kVb V k:
first component satisfies:

kVb LA Vb k = kH LA Vb LA Vb k
kH LA Vb LA H Vb k + kH Vb Vb k
= kH LA Vb LA H LA Vb k + kH Vb Vb k
kVb LA Vb k +
inequality: kH Vb Vb k follows isotonicity H fact Vb either
upper lower bound. Rearranging inequalities, obtain: kVb LA Vb k = (1 ) .
bound second term kVb V k trivial.
(2 )
Therefore, kVb LA V k [ (1 1 ) + 1] = (1
) . 2
A.3 MDP, QMDP Fast Informed Bounds
Theorem 8 solution fast informed bound approximation found solving
MDP jS jjAjjj states, jAj actions discount factor .
Proof Let ffai linear function action defining Vbi . Let ffi (s; a) denote parameters
function. parameters Vbi+1 satisfy:
ffi+1 (s; a) = (s; a) +
Let

X

X
0
0 0
max
0 2A 0 P (s ; ojs; a)ffi (s ; ):

o2
2S

ffi+1 (s; a; o) = max
0

X

2 s0 2

87

P (s0 ; ojs; a)ffi (s0 ; a0 ):

fiHauskrecht

Now, rewrite ffi+1 (s; a; o) every s; a; as:
8
<X

2

X

ffi+1 (s; a; o) = max
P (s0 ; ojs; a) 4(s0 ; a0 ) +
a0 2A :s0 2S
o0 2
8
2
< X
4
= max
a0 2A : 0

3

2

39
=
ffi (s0 ; a0 ; o0 )5;

P (s0 ; ojs; a)(s0 ; a0 )5 + 4

X X

o0 2 s0 2S

2S

39
=
P (s0 ; ojs; a)ffi (s0 ; a0 ; o0 )5;

equations define MDP state space , action space discount
factor . Thus, solution fast informed bound update found solving
equivalent finite-state MDP. 2

Theorem 9 Let Vbi corresponds piecewise linear convex value function defined
linear functions. H Vbi HF IB Vbi HQMDP Vbi HMDP Vbi :
Proof
8
<X

X



9
=

XX

max : (s; a)b(s) +
max
P (s0 ; ojs; a)b(s)ffi (s0 );
a2A s2S

2
0
o2
2S s2S
= (HVi )(b)


max
a2A

X
s2S

2

b(s) 4(s; a) +

= (HF IB Vi )(b)

max
a2A

X
s2S

b(s) 4(s; a) +



s2S

a2A

2

= (HMDP Vbi )(b)

3

P (s0 ; ojs; a)ffi (s0 )5
3

X
s0 2S

2

b(s) max 4(s; a) +

max

X

o2 ffi 2 s0 2S

2

= (HQMDP Vbi )(b)
X

X



P (s0 js; a) max ffi (s0 )5
ffi 2



3

X
s0 2S

P (s0 js; a) max ffi (s0 )5
ffi 2



A.4 Fixed-Strategy Approximations
Theorem 10 Let CF SM FSM controller. Let CDR CLA direct
one-step-lookahead controllers constructed based CF SM . V C (b) V C (b)
V C (b) V C (b) hold belief states b 2 .
Proof value function FSM controller CF SM satisfies:
F SM

F SM

LA

VC

F SM



(b) = max V (x; b) = V ( (b); b)
x2M

V (x; b) = (b; (x)) +

X
o2

P (ojb; (x))V ((x; o); (b; (x); o)):
88

DR

fiValue-Function Approximations POMDPs

direct controller CDR selects action greedily every step, is, always
chooses according (b) = arg maxx2M V (x; b). lookahead controller CLA selects
action based V (x; b) one step away:

LA (b) = arg max
a2A

"

#

X

0
(b; a) + P (ojb; a) max
0 2M V (x ; (b; a; o)) :
x
o2

expanding value function CF SM one step get:

VC

F SM

(b) = max V (x; b)
x2M
"

#

X

= max (b; (x)) + P (ojb; (x))V ((x; o); (b; (x); o))
x2M
o2
= (b; ( (b))) +

(b; ( (b))) +
"

X

o2

X

o2

(1)

P (ojb; ( (b)))V ((x; o); (b; ( (b)); o))

P (ojb; ( (b))) max
V (x0 ; (b; ( (b)); o))
0
x 2M

X

(2)

#

0
max
(b; a) + P (ojb; a) max
0 2M V (x ; (b; a; o))
a2A
x
o2
X
LA
0
LA
= (b; (b)) + P (ojb; LA (b)) max
0 2M V (x ; (b; (b); o))
x
o2

(3)

Iteratively expanding maxx0 2M V (x; :) 2 3 expression 1 substituing improved
(higher value) expressions 2 3 back obtain value functions direct
lookahead controllers. (Expansions 2 lead value direct controller
expansions 3 value lookahead controller.) Thus V C
VC
C
C
V
V must hold. Note, however, action choices (b) LA(b)
expressions 2 3 different leading different next step belief states
subsequently different expansion sequences. Therefore, result imply
V DR (b) V LA (b) b 2 . 2
F SM

F SM

DR

LA

A.5 Grid-Based Linear-Function Method
Theorem 14 Let Vbi value function obtained via incremental linear function method,
starting Vb0 , corresponds fixed strategy C0 . Let CLA;i CDR;i two
controllers based Vbi : lookahead controller direct action controller, V C ,
VC
respective value functions. Vbi V C
Vbi V C
hold.
Proof initializing method value function FSM controller C0 ,
incremental updates interpreted additions new states FSM controller (a
new linear function corresponds new state FSM). Let Ci controller
step i. V C
= Vbi holds inequalities follow Theorem 10. 2
LA;i

DR;i

LA;i

F SM;i

89

DR;i

fiHauskrecht

References

Astrom, K. J. (1965). Optimal control Markov decision processes incomplete state
estimation. Journal Mathematical Analysis Applications, 10, 174{205.
Baird, L. C. (1995). Residual algorithms: Reinforcement learning function approximation. Proceedings Twelfth International Conference Machine Learning,
pp. 30{37.
Barto, A. G., Bradtke, S. J., & Singh, S. P. (1995). Learning act using real-time dynamic
programming. Artificial Intelligence, 72, 81{138.
Bellman, R. E. (1957). Dynamic programming. Princeton University Press, Princeton, NJ.
Bertsekas, D. P. (1994). counter-example temporal differences learning. Neural Computation, 7, 270{279.
Bertsekas, D. P. (1995). Dynamic programming optimal control. Athena Scientific.
Bonet, B., & Geffner, H. (1998). Learning sorting classification POMDPs.
Proceedings Fifteenth International Conference Machine Learning.
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions computational leverage. Artificial Intelligence, 11, 1{94.
Boutilier, C., & Poole, D. (1996). Exploiting structure policy construction. Proceedings
Thirteenth National Conference Artificial Intelligence, pp. 1168{1175.
Boyan, J. A., & Moore, A. A. (1995). Generalization reinforcement learning: safely
approximating value function. Advances Neural Information Processing
Systems 7. MIT Press.
Boyen, X., & Koller, D. (1998). Tractable inference complex stochastic processes.
Proceedings Fourteenth Conference Uncertainty Artificial Intelligence, pp.
33{42.
Boyen, X., & Koller, D. (1999). Exploiting architecture dynamic systems. Proceedings Sixteenth National Conference Artificial Intelligence, pp. 313{320.
Brafman, R. I. (1997). heuristic variable grid solution method POMDPs. Proceedings Fourteenth National Conference Artificial Intelligence, pp. 727{233.
Burago, D., Rougemont, M. D., & Slissenko, A. (1996). complexity partially
observed Markov decision processes. Theoretical Computer Science, 157, 161{183.
Cassandra, A. R. (1998). Exact approximate algorithms partially observable Markov
decision processes. Ph.D. thesis, Brown University.
Cassandra, A. R., Littman, M. L., & Zhang, N. L. (1997). Incremental pruning: simple,
fast, exact algorithm partially observable Markov decision processes. Proceedings
Thirteenth Conference Uncertainty Artificial Intelligence, pp. 54{61.
90

fiValue-Function Approximations POMDPs

Casta~non, D. (1997). Approximate dynamic programming sensor management.
Proceedings Conference Decision Control.
Cheng, H.-T. (1988). Algorithms partially observable Markov decision processes. Ph.D.
thesis, University British Columbia.
Condon, A. (1992). complexity stochastic games. Information Computation,
96, 203{224.
Dean, T., & Kanazawa, K. (1989). model reasoning persistence causation.
Computational Intelligence, 5, 142{150.
Dearden, R., & Boutilier, C. (1997). Abstraction approximate decision theoretic planning. Artificial Intelligence, 89, 219{283.
Doucet, A. (1998). sequential simulation-based methods Bayesian filtering. Tech.
rep. CUED/F-INFENG/TR 310, Department Engineering, Cambridge University.
Drake, A. (1962). Observation Markov process noisy channel. Ph.D. thesis,
Massachusetts Institute Technology.
Draper, D., Hanks, S., & Weld, D. (1994). Probabilistic planning information gathering
contingent execution. Proceedings Second International Conference
AI Planning Systems, pp. 31{36.
Eagle, J. N. (1984). optimal search moving target search path constrained.
Operations Research, 32, 1107{1115.
Eaves, B. (1984). course triangulations soving differential equations deformations. Springer-Verlag, Berlin.
Gordon, G. J. (1995). Stable function approximation dynamic programming. Proceedings Twelfth International Conference Machine Learning.
Hansen, E. (1998a). improved policy iteration algorithm partially observable MDPs.
Advances Neural Information Processing Systems 10. MIT Press.
Hansen, E. (1998b). Solving POMDPs searching policy space. Proceedings
Fourteenth Conference Uncertainty Artificial Intelligence, pp. 211{219.
Hauskrecht, M. (1997). Planning control stochastic domains imperfect information. Ph.D. thesis, Massachusetts Institute Technology.
Hauskrecht, M., & Fraser, H. (1998). Planning medical therapy using partially observable
Markov decision processes. Proceedings Ninth International Workshop
Principles Diagnosis (DX-98), pp. 182{189.
Hauskrecht, M., & Fraser, H. (2000). Planning treatment ischemic heart disease
partially observable Markov decision processes. Artificial Intelligence Medicine, 18,
221{244.
91

fiHauskrecht

Heyman, D., & Sobel, M. (1984). Stochastic methods operations research: stochastic
optimization. McGraw-Hill.
Howard, R. A. (1960). Dynamic Programming Markov Processes. MIT Press, Cambridge.
Howard, R. A., & Matheson, J. (1984). uence diagrams. Principles Applications
Decision Analysis, 2.
Isard, M., & Blake, A. (1996). Contour tracking stochastic propagation conditional
density. Proccedings Europian Conference Computer Vision, pp. 343{356.
Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1999). Planning acting
partially observable stochastic domains. Artificial Intelligence, 101, 99{134.
Kanazawa, K., Koller, D., & Russell, S. J. (1995). Stochastic simulation algorithms
dynamic probabilistic networks. Proceedings Eleventh Conference Uncertainty Artificial Intelligence, pp. 346{351.
Kearns, M., Mansour, Y., & Ng, A. Y. (1999). sparse sampling algorithm near
optimal planning large Markov decision processes. Proceedings Sixteenth
International Joint Conference Artificial Intelligence, pp. 1324{1331.
Kjaerulff, U. (1992). computational scheme reasoning dynamic probabilistic networks. Proceedings Eighth Conference Uncertainty Artificial Intelligence, pp. 121{129.
Korf, R. (1985). Depth-first iterative deepening: optimal admissible tree search. Artificial
Intelligence, 27, 97{109.
Kushmerick, N., Hanks, S., & Weld, D. (1995). algorithm probabilistic planning.
Artificial Intelligence, 76, 239{286.
Lauritzen, S. L. (1996). Graphical models. Clarendon Press.
Littman, M. L. (1994). Memoryless policies: Theoretical limitations practical results.
Cliff, D., Husbands, P., Meyer, J., & Wilson, S. (Eds.), Animals Animats 3: Proceedings Third International Conference Simulation Adaptive
Behavior. MIT Press, Cambridge.
Littman, M. L. (1996). Algorithms sequential decision making. Ph.D. thesis, Brown
University.
Littman, M. L., Cassandra, A. R., & Kaelbling, L. P. (1995). Learning policies partially
observable environments: scaling up. Proceedings Twelfth International
Conference Machine Learning, pp. 362{370.
Lovejoy, W. S. (1991a). Computationally feasible bounds partially observed Markov
decision processes. Operations Research, 39, 192{175.
92

fiValue-Function Approximations POMDPs

Lovejoy, W. S. (1991b). survey algorithmic methods partially observed Markov
decision processes. Annals Operations Research, 28, 47{66.
Lovejoy, W. S. (1993). Suboptimal policies bounds parameter adaptive decision
processes. Operations Research, 41, 583{599.
Lusena, C., Goldsmith, J., & Mundhenk, M. (1998). Nonapproximability results Markov
decision processes. Tech. rep., University Kentucky.
Madani, O., Hanks, S., & Condon, A. (1999). undecidability probabilistic planning
infinite-horizon partially observable Markov decision processes. Proceedings
Sixteenth National Conference Artificial Intelligence.
McAllester, D., & Singh, S. P. (1999). Approximate planning factored POMDPs using
belief state simplification. Proceedings Fifteenth Conference Uncertainty
Artificial Intelligence, pp. 409{416.
McCallum, R. (1995). Instance-based utile distinctions reinforcement learning
hidden state. Proceedings Twelfth International Conference Machine
Learning.
Monahan, G. E. (1982). survey partially observable Markov decision processes: theory,
models, algorithms. Management Science, 28, 1{16.
Mundhenk, M., Goldsmith, J., Lusena, C., & Allender, E. (1997). Encyclopaedia complexity results finite-horizon Markov decision process problems. Tech. rep., CS
Dept TR 273-97, University Kentucky.
Papadimitriou, C. H., & Tsitsiklis, J. N. (1987). complexity Markov decision processes. Mathematics Operations Research, 12, 441{450.
Parr, R., & Russell, S. (1995). Approximating optimal policies partially observable
stochastic domains. Proceedings Fourteenth International Joint Conference
Artificial Intelligence, pp. 1088{1094.
Pearl, J. (1988). Probabilistic reasoning intelligent systems. Morgan Kaufman.
Platzman, L. K. (1977). Finite memory estimation control finite probabilistic systems.
Ph.D. thesis, Massachusetts Institute Technology.
Platzman, L. K. (1980). feasible computational approach infinite-horizon partiallyobserved Markov decision problems. Tech. rep., Georgia Institute Technology.
Puterman, M. L. (1994). Markov decision processes: discrete stochastic dynamic programming. John Wiley, New York.
Raiffa, H. (1970). Decision analysis. Introductory lectures choices uncertainty.
Addison-Wesley.
Rumelhart, D., Hinton, G. E., & Williams, R. J. (1986). Learning internal representations
error propagation. Parallel Distributed Processing, pp. 318{362.
93

fiHauskrecht

Satia, J., & Lave, R. (1973). Markovian decision processes probabilistic observation
states. Management Science, 20, 1{13.
Singh, S. P., Jaakkola, T., & Jordan, M. I. (1994). Learning without state-estimation
partially observable Markovian decision processes. Proceedings Eleventh
International Conference Machine Learning, pp. 284{292.
Smallwood, R. D., & Sondik, E. J. (1973). optimal control partially observable
processes finite horizon. Operations Research, 21, 1071{1088.
Sondik, E. J. (1971). optimal control partially observable Markov decision processes.
Ph.D. thesis, Stanford University.
Sondik, E. J. (1978). optimal control partially observable processes infinite
horizon: Discounted costs. Operations Research, 26, 282{304.
Tatman, J., & Schachter, R. D. (1990). Dynamic programming uence diagrams.
IEEE Transactions Systems, Man Cybernetics, 20, 365{379.
Tsitsiklis, J. N., & Roy, B. V. (1996). Feature-based methods large-scale dynamic
programming. Machine Learning, 22, 59{94.
Washington, R. (1996). Incremental Markov model planning. Proceedings Eight
IEEE International Conference Tools Artificial Intelligence, pp. 41{47.
White, C. C., & Scherer, W. T. (1994). Finite memory suboptimal design partially
observed Markov decision processes. Operations Research, 42, 439{455.
Williams, R. J., & Baird, L. C. (1994). Tight performance bounds greedy policies based
imperfect value functions. Proceedings Tenth Yale Workshop Adaptive
Learning Systems Yale University.
Yost, K. A. (1998). Solution large-scale allocation problems partially observable
outcomes. Ph.D. thesis, Naval Postgraduate School, Monterey, CA.
Zhang, N. L., & Lee, S. S. (1998). Planning partially observable Markov decision
processes: Advances exact solution method. Proceedings Fourteenth Conference Uncertainty Artificial Intelligence, pp. 523{530.
Zhang, N. L., & Liu, W. (1997a). model approximation scheme planning partially
observable stochastic domains. Journal Artificial Intelligence Research, 7, 199{230.
Zhang, N. L., & Liu, W. (1997b). Region-based approximations planning stochastic
domains. Proceedings Thirteenth Conference Uncertainty Artificial
Intelligence, pp. 472{480.

94


