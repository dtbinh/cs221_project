Journal Artificial Intelligence Research 46 (2013) 89-127

Submitted 03/12; published 01/13

Automatic Aggregation Joint Modeling
Aspects Values
Christina Sauper
Regina Barzilay

csauper@csail.mit.edu
regina@csail.mit.edu

Computer Science Artificial Intelligence Laboratory
Massachusetts Institute Technology
32 Vassar St.
Cambridge, 02139 USA

Abstract
present model aggregation product review snippets joint aspect identification sentiment analysis. model simultaneously identifies underlying set
ratable aspects presented reviews product (e.g., sushi miso Japanese
restaurant) determines corresponding sentiment aspect. approach
directly enables discovery highly-rated inconsistent aspects product. generative model admits efficient variational mean-field inference algorithm. easily
extensible, describe several modifications effects model structure
inference. test model two tasks, joint aspect identification sentiment analysis set Yelp reviews aspect identification alone set medical summaries.
evaluate performance model aspect identification, sentiment analysis,
per-word labeling accuracy. demonstrate model outperforms applicable
baselines considerable margin, yielding 32% relative error reduction aspect
identification 20% relative error reduction sentiment analysis.

1. Introduction
Online product reviews become increasingly valuable influential source information consumers. ability explore range opinions allows consumers
form general opinion product gather information positive negative
aspects (e.g., packaging battery life). However, reviews added time,
problem information overload gets progressively worse. example, hundreds
reviews restaurant, consumers read handful making decision.
work, goal summarize large number reviews discovering
informational product aspects associated user sentiment.
address need, online retailers often use simple aggregation mechanisms represent spectrum user sentiment. Many sites, Amazon, simply present
distribution user-assigned star ratings, approach lacks reasoning
products given rating. retailers use breakdowns specific
predefined domain-specific aspects, food, service, atmosphere restaurant.
breakdowns continue assist effective aggregation; however, aspects
predefined, generic particular domain explanation one aspect rated well poorly. Instead, truly informative aggregation,
c
2013
AI Access Foundation. rights reserved.

fiSauper & Barzilay

product needs assigned set fine-grained aspects specifically tailored
product.
goal work provide mechanism effective unsupervised content
aggregation able discover specific, fine-grained aspects associated values. Specifically,
represent data set collection entities; instance, represent
products domain online reviews. interested discovering fine-grained
aspects entity (e.g., sandwiches dessert restaurant). Additionally, would
recover value associated aspect (e.g., sentiment product reviews).
summary input output found Figure 1. input consists short
text snippets multiple reviews several products. restaurant domain,
Figure 1, restaurants. assume snippet opinion-bearing
discusses one aspects relevant particular product. output
consists set dynamic (i.e., pre-specified) aspects product, snippets labeled
aspect discuss, sentiment values snippet individually
aspect whole. Figure 1, aspects identified Tasca Spanish Tapas include
chicken, dessert, drinks, snippets labeled aspects describe
correct polarity.
One way approach problem treat multi-class classification problem.
Given set predefined domain-specific aspects, would fairly straightforward
humans identify aspect particular snippet describes. However, task
discovering fine-grained entity-specific aspects, way know priori
aspects may present across entire data set provide training data each; instead,
must select aspects dynamically. Intuitively, one potential solution cluster
input snippets, grouping lexically similar without prior knowledge
aspects represent. However, without knowledge words represent
aspect given snippet, clusters may align ones useful cross-review analysis.
Consider, example, two clusters restaurant review snippets shown Figure 2.
clusters share many words among members, first describes
coherent aspect cluster, namely drinks aspect. snippets second cluster
discuss single product aspect, instead share expressions sentiment.
successfully navigate challenge, must distinguish words indicate aspect, words indicate sentiment, extraneous words neither.
aspect identification sentiment analysis, crucial know words within
snippet relevant task. Distinguishing straightforward, however.
work sentiment analysis relies predefined lexicon WordNet provide
hints, way anticipate every possible expression aspect sentiment,
especially user-generated data (e.g., use slang deeeeeee-lish delicious).
lieu explicit lexicon, attempt use information proxy,
part speech; example, aspect words likely nouns, value words
likely adjectives. However, show later paper, additional information
sufficient tasks hand.
Instead, propose approach analyze collection product review snippets
jointly induce set learned aspects, respective value (e.g., sentiment).
capture idea using generative Bayesian topic model set aspects
corresponding values represented hidden variables. model takes collection
90

fiAutomatic Aggregation Joint Modeling Aspects Values

Input

Output

Tasca Spanish Tapas

Tasca Spanish Tapas

Review 1
chicken cooked perfectly
dessert good

Chicken
+ chicken cooked perfectly
chicken tough tasty
+ Moist delicious chicken

Review 2
red wines cheap
excellent creme brulee
Review 3
used frozen small shrimp
chicken tough tasty
Pitcher sangria pretty good

Douzo Sushi Bar
Review 1
sushi creative pretty good
ponzu overpowering
Review 2
Real wasabi thats fresh!
torched roll tasted rather bland


Dessert
+ dessert good
+ excellent creme brulee





Drinks
red wines cheap
+ Pitcher sangria pretty good

Douzo Sushi Bar
Sushi
+ sushi creative pretty good
torched roll tasted rather bland
Condiments
ponzu overpowering
+ Real wasabi thats fresh!



Figure 1: example desired input output system restaurant
domain. input consists collection review snippets several restaurants.
output aggregation snippets aspect (e.g., chicken dessert) along
associated sentiment snippet. Note input data completely unannotated;
information given snippets describe restaurant.

snippets input explains observed text arises latent variables,
thereby connecting text fragments corresponding aspects values.
Specifically, begin defining sets sentiment word distributions aspect word
distributions. expect types sentiment words consistent across
products (e.g., product may labeled great terrible), allow positive
negative sentiment word distributions shared across products.
hand, case restaurant reviews similar domains, aspect words expected
quite distinct products. Therefore, assign product set aspect
word distributions. addition word distributions, model takes account
several factors. First, model idea particular aspect product
underlying quality; is, already 19 snippets praising particular aspect,
likely 20th snippet positive well. Second, account common
patterns language using transition distribution types words. example,
common see pattern Value Aspect, phrases great pasta.
Third, model distributions parts speech type distribution.
91

fiSauper & Barzilay

Coherent aspect cluster
+

martinis
good.
:::::::::
drinks

wine :::::::::
martinis - tasty.
:::::::
:::::

-

wine
list pricey.
:::::::::
:::::
wine:::::::::::
selection horrible.

Incoherent aspect cluster

+

sushi :::::
best :::::
Ive :::::
ever:::::
had.
Best
paella
Id
ever
had.
:::::
::::::::::::::
fillet best
steak wed ever had.
:::::::::::::::::::::::::::
best
soup
Ive
ever had.
::::::::::::::::::::::::::

Figure 2: Example clusters restaurant review snippets generated lexical clustering
algorithm; words relevant clustering highlighted. first cluster represents coherent aspect underlying product, namely drinks aspect. latter cluster simply
shares common sentiment expression represent snippets discussing
product aspect. work, aim produce first type aspect cluster along
corresponding values.

covers intuition aspect words frequently nouns, whereas value words often
adjectives. describe factors model whole detail Section 4.
formulation provides several advantages: First, model require set
predefined aspects. Instead, capable assigning latent variables discover
appropriate aspects based data. Second, joint analysis aspect value
allows us leverage several pieces information determine words relevant
aspect identification used sentiment analysis, including part
speech global entity-specific distributions words. Third, Bayesian model
admits efficient mean-field variational inference procedure parallelized
run quickly even large numbers entities snippets.
evaluate approach domain restaurant reviews. Specifically, use set
snippets automatically extracted restaurant reviews Yelp. collection consists
average 42 snippets 328 restaurants Boston area, representing
wide spectrum opinions several aspects restaurant. demonstrate
model accurately identify clusters review fragments describe
aspect, yielding 32.5% relative error reduction (9.9 absolute F1 ) standalone clustering
baseline. show model effectively identify snippet sentiment,
19.7% relative error reduction (4.3% absolute accuracy) applicable baselines. Finally,
test models ability correctly label aspect sentiment words, discovering
aspect identification high-precision, sentiment identification highrecall.
Additionally, apply slimmed-down version model focuses exclusively
aspect identification set lab- exam-related snippets medical summaries
provided Pediatric Environmental Health Clinic (PEHC) Childrens Hospital
Boston. summaries represent concise overviews patient information par92

fiAutomatic Aggregation Joint Modeling Aspects Values

ticular visit, relayed PEHC doctor childs referring physician. model
achieves 7.4% (0.7 absolute F1 ) standalone clustering baseline.
remainder paper structured follows. Section 2 compares work
previous work aspect identification sentiment analysis. Section 3 describes
specific problem formulation task setup concretely. Section 4 presents details
full model various model extensions, Section 5 describes inference procedure necessary adjustments extension. details data sets,
experimental formulation, results presented Section 6. summarize findings
consider directions future work Section 7. code data used paper
available online http://groups.csail.mit.edu/rbg/code/review-aggregation.

2. Related Work
work falls area multi-aspect sentiment analysis. section, first
describe approaches toward document-level sentence-level sentiment analysis (Section
2.1), provide foundation future work, including own. Then, describe
three common directions multi-aspect sentiment analysis; specifically, use
data-mining fixed-aspect analysis (Section 2.2.1), incorporate sentiment
analysis multi-document summarization (Section 2.2.2), finally, focused
topic modeling additional sentiment components (Section 2.2.3).
2.1 Single-Aspect Sentiment Analysis
Early sentiment analysis focused primarily identification coarse document-level sentiment (Pang, Lee, & Vaithyanathan, 2002; Turney, 2002; Pang & Lee, 2008). Specifically,
approaches attempted determine overall polarity documents. approaches included rule-based machine learning approaches: Turney (2002) used
rule-based method extract potentially sentiment-bearing phrases compared
sentiment known-polarity words, Pang et al. (2002) used discriminative
methods features unigrams, bigrams, part-of-speech tags, word position
information.
document-level sentiment analysis give us overall view opinion, looking individual sentences within document yields fine-grained analysis.
work sentence-level sentiment analysis focuses first identifying sentiment-bearing sentences determining polarity (Yu & Hatzivassiloglou, 2003; Dave, Lawrence,
& Pennock, 2003; Kim & Hovy, 2005, 2006; Pang & Lee, 2008). identification
sentiment-bearing sentences polarity analysis performed supervised
classifiers (Yu & Hatzivassiloglou, 2003; Dave et al., 2003) similarity known text (Yu
& Hatzivassiloglou, 2003; Kim & Hovy, 2005), measures based distributional
similarity using WordNet relationships.
recognizing connections parts document, sentiment analysis
improved (Pang & Lee, 2004; McDonald, Hannan, Neylon, Wells, & Reynar, 2007;
Pang & Lee, 2008). Pang Lee (2004) leverage relationship sentences
improve document-level sentiment analysis. Specifically, utilize subjectivity
individual sentences information strength connection sentences
min cut formulation provide better sentiment-focused summaries text. McDonald
93

fiSauper & Barzilay

et al. (2007) examine different connection, instead constructing hierarchical model
sentiment sentences documents. model uses complete labeling
subset data learn generalized set parameters improve classification accuracy
document-level sentence-level.
none approaches attempt identify aspects analyze sentiment
aspect-based fashion, intuitions provide key insight approaches take
work. example, importance distinguishing opinion sentences follows
intuition necessity identifying sentiment-bearing words within snippet.
2.2 Aspect-Based Sentiment Analysis
Following work single-aspect document-level sentence-level sentiment analysis
came intuition modeling aspect-based (also called feature-based) sentiment review analysis. divide approaches roughly three types systems based
techniques: systems use fixed-aspect approaches data-mining techniques
aspect selection sentiment analysis, systems adapt techniques multi-document
summarization, systems jointly model aspect sentiment probabilistic
topic models. Here, examine avenue work relevant examples contrast
work.
2.2.1 Data-Mining Fixed-Aspect Techniques Sentiment Analysis
One set approaches toward aspect-based sentiment analysis follow traditional techniques data mining (Hu & Liu, 2004; Liu, Hu, & Cheng, 2005; Popescu, Nguyen, &
Etzioni, 2005). systems may operate full documents snippets, generally require rule-based templates additional resources WordNet identify
aspects determine sentiment polarity. Another approach fix predetermined
relevant set aspects, focus learning optimal opinion assignment
aspects (Snyder & Barzilay, 2007). Below, summarize approach compare
contrast work.
One set work relies combination association mining rule-based extraction
nouns noun phrases aspect identification. Hu Liu (2004) Liu et al. (2005)
developed three-step system: First, initial aspects selected association miner
pruned series rules. Second, related opinions aspect identified
rule-based fashion using word positions, polarity determined WordNet
search based set seed words. Third, additional aspects identified similar
fashion based position selected polarity words. steps, part-ofspeech information provides key role extraction rules. later work,
additional component identify implicit aspects deterministic fashion; e.g., heavy
maps deterministically <weight> (Liu et al., 2005). task similar
utilize part-of-speech information important feature well, additionally
leverage distributional information identify aspects sentiment. Furthermore,
avoid reliance WordNet predefined rule mappings order preserve
generality system. Instead, joint modeling allows us recover relationships
without need additional information.
94

fiAutomatic Aggregation Joint Modeling Aspects Values

approaches rely WordNet relationships identify sentiment
polarity, aspects, using parts properties particular product class.
Popescu et al. (2005) first use relations generate set aspects given
product class (e.g., camera). Following that, apply relaxation labeling sentiment
analysis. procedure gradually expands sentiment individual words aspects
sentences, similar Cascade pattern mentioned work McDonald et al. (2007).
system Liu et al. (2005), system requires set manual rules several
outside resources. model require seed words, require
manual rules additional resources due joint formulation.
separate direction work relies predefined aspects focusing improvement
sentiment analysis prediction. Snyder Barzilay (2007) define set aspects specific
restaurant domain. Specifically define individual rating model aspect,
plus overall agreement model attempts determine whether resulting ratings
agree disagree. models jointly trained supervised fashion using
extension PRanking algorithm (Crammer & Singer, 2001) find best overall
star rating aspect. problem formulation differs significantly work
several dimensions: First, desire refined analysis using fine-grained aspects
instead coarse predefined features. Second, would use little supervised
training data possible, rather supervised training required PRanking
algorithm.
work, attempt capture intuitions approaches reducing
need outside resources rule-based components. example, rather supplying
rule-based patterns extraction aspect sentiment, instead leverage distributional patterns across corpus infer relationships words different types.
Likewise, rather relying WordNet relationships synonymy, antonymy, hyponymy, hypernymy (Hu & Liu, 2004; Liu et al., 2005; Popescu et al., 2005), bootstrap
model small set seed words.
2.2.2 Multi-Document Summarization Application Sentiment
Analysis
Multi-document summarization techniques generally look repetition across documents
signal important information (Radev & McKeown, 1998; Barzilay, McKeown, & Elhadad,
1999; Radev, Jing, & Budzikowska, 2000; Mani, 2001). aspect-based sentiment analysis,
work focused augmenting techniques additional components sentiment
analysis (Seki, Eguchi, Kanodo, & Aono, 2005, 2006; Carenini, Ng, & Pauls, 2006; Kim &
Zhai, 2009). general, end goal approaches task forming coherent text
summaries using either text extraction natural language generation. Unlike work,
many approaches explicitly identify aspects; instead, extracted
repeated information. Additionally, model explicitly looks connection
content sentiment, rather treating secondary computation
information selected.
One technique incorporating sentiment analysis follows previous work identification opinion-bearing sentences. Seki et al. (2005, 2006) present DUC summarization
95

fiSauper & Barzilay

systems designed create opinion-focused summaries task topics.1 system,
employ subjectivity component using supervised SVM lexical features, similar
work Yu Hatzivassiloglou (2003) Dave et al. (2003). component
used identify subjective sentences and, work Seki et al. (2006), polarity,
task sentences selected response summary. However,
previous work unlike task, aspect-based analysis summarization
task. fully supervised, relying hand-annotated set 10,000 sentences
train SVM.
Another line work focuses augmenting summarization system aspect
selection similar data-mining approaches Hu Liu (2004), rather using
single-aspect analysis. Carenini, Ng, Zwart (2005) Carenini et al. (2006) augment
previous aspect selection user-defined hierarchical organization aspects; e.g.,
digital zoom part lens. Polarity aspect assumed given previous
work. aspects incorporated existing summarization systems MEAD*
sentence extraction (Radev et al., 2000) SEA natural language generation (Carenini &
Moore, 2006) form final summaries. work Seki et al. (2005, 2006), work
create new techniques aspect identification sentiment analysis; instead,
focus process integrating sources information summarization systems.
aspects produced comparable across reviews particular product,
highly-supervised nature means approach feasible large set products
corpus reviews many types restaurants. Instead, must able
dynamically identify relevant aspects.
final line related work relies traditional summarization technique identifying contrastive contradictory sentences. Kim Zhai (2009) focus generating
contrastive summaries identifying pairs sentences express differing opinions
particular product feature. this, define metrics representativeness (coverage opinions) contrastiveness (alignment quality) using semantic similarity
WordNet matches word overlap. comparison work, approach follows orthogonal goal, try find defining aspects instead
contradictory ones. Additionally, selected pairs hint disagreements rating,
identification many people agree side overall rating
particular aspect. work, aim produce concrete set aspects
user sentiment each, whether unanimous shows disagreement.
Overall, methods designed produce output summaries focus
subjective information, specifically targeted aspect-based analysis. Instead,
aspects identified supervised fashion (Carenini et al., 2005, 2006) defined
(Seki et al., 2005, 2006; Kim & Zhai, 2009). work, crucial
dynamically-selected aspects feasible preselect aspects supervised
fashion.
2.2.3 Probabilistic Topic Modeling Sentiment Analysis
work closest direction aspect-based analysis focuses use
probabilistic topic modeling techniques identification aspects. may aggre1. task examples, see work Dang (2005, 2006).

96

fiAutomatic Aggregation Joint Modeling Aspects Values

gated without specific sentiment polarity (Lu & Zhai, 2008) combined additional
sentiment modeling either jointly (Mei, Ling, Wondra, Su, & Zhai, 2007; Blei & McAuliffe,
2008; Titov & McDonald, 2008a) separate post-processing step (Titov & McDonald,
2008b). work, approaches share intuition aspects may represented
topics.
Several approaches focus extraction topics sentiment blog articles.
one approach, used expert articles aspect extraction combination
larger corpus user reviews. Lu Zhai (2008) introduce model semi-supervised
probabilistic latent semantic analysis (PLSA) identifies sentiment-bearing aspects
segmentation expert review. Then, model extracts compatible supporting
supplementary text aspect set user reviews. Aspect selection
constrained rule-based approaches; specifically, aspect words required
nouns. work differs work significantly. share common goal
identifying aggregating opinion-bearing aspects, additionally desire identify
polarity opinions, task addressed work. addition, obtaining aspects
expert review unnecessarily constraining; practice, expert reviewers may
mention key aspects, mention every aspect. crucial discover
aspects based entire set articles.
work direction aspect identification blog posts. example,
Mei et al. (2007) use variation latent Dirichlet allocation (LDA) similar
explicitly model topics sentiment, use hidden Markov model discover
sentiment dynamics across topic life cycles. general sentiment polarity distribution
computed combining distributions several separate labeled data sets (e.g., movies,
cities, etc.). However, work, sentiment measured document-level, rather
topic-level. Additionally, topics discovered model broad; example, processing query Da Vinci Code, returned topics may labeled
book, movie, religion, rather fine-grained aspects desire model,
representing major characters events. model expands work discovering fine-grained aspects associating particular sentiment individual
aspect. addition, tying sentiment aspects, able identify sentiment-bearing
words associated polarities without additional annotation required train
external sentiment model.
Sentiment may combined LDA using additional latent variables
document order predict document-level sentiment. Blei McAuliffe (2008) propose
form supervised LDA (sLDA) incorporates additional response variable,
used represent sentiment star rating movie. jointly
model documents responses order find latent topics best predict
response variables future unlabeled documents. work significantly different
work, supervised predict multi-aspect framework.
Building approaches comes work fine-grained aspect identification sentiment analysis. Titov McDonald (2008a, 2008b) introduce multi-grain unsupervised
topic model, specifically built extension LDA. technique yields mixture
global local topics. Word distributions topics (both global local) drawn
global level, however; unlike model. consequence topics
easy compare across products corpus; however, topics gen97

fiSauper & Barzilay

eral less dynamic hope achieve must shared among every
product. One consequence defining global topics difficulty finding relevant topics
every product little overlap. example, case restaurant reviews,
Italian restaurants completely different set aspects Indian restaurants.
course, factors known, would possible run algorithm separately
subset restaurants, distinctions immediately clear priori. Increasing number topics could assist recovering additional aspects; however
aspects still global, still difficult identify restaurant-specific aspects.
sentiment analysis, PRanking algorithm Snyder Barzilay (2007) incorporated two ways: First, PRanking algorithm trained pipeline fashion
topics generated (Titov & McDonald, 2008b); later, incorporated model
inference joint formulation (Titov & McDonald, 2008a). However, cases,
original algorithm, set aspects fixed aspects corresponds
fixed set topics found model. Additionally, learning problem supervised.
fixed aspects, necessary additional supervision, global topic distribution, model formulation sufficient problem domain, requires
fine-grained aspects.
approaches structural similarity work present,
variations LDA. None, however, intent model. Mei et al. (2007)
model aspect sentiment jointly; however aspects vague, treat
sentiment document level rather aspect level. Likewise, Titov McDonald
(2008b, 2008a) model fine-grained aspects, still coarser aspects
require, even increase number aspects, distributions shared
globally. Finally, Lu Zhai (2008), Blei McAuliffe (2008), Titov McDonald
(2008b, 2008a) require supervised annotation supervised expert review
have. attempt solve issues joint formulation order proceed
minimal supervision discover truly fine-grained aspects.

3. Problem Formulation
explaining model details, describe random variables abstractions
model, well intuitions assumptions.2 visual explanation model
components shown Figure 3. present complete details generative story
Section 4.
3.1 Model Components
model composed five component types: entities, snippets, aspects, values,
word topics. Here, describe type provide examples.

2. Here, explain complete model value selection sentiment restaurant domain.
simplified case medical domain would use aspects, may simply ignore
value-related components model.

98

fiAutomatic Aggregation Joint Modeling Aspects Values

Tasca Spanish Tapas
Entity
Aspects

Chicken
+

+

chicken cooked perfectly
chicken tough tasty
Moist delicious chicken
Snippets

Values

Dessert
+
+

dessert good
excellent creme brulee


Douzo Sushi Bar
Sushi
+


sushi creative pretty good
torched roll tasted rather bland



Figure 3: Labeled model components example Figure 1. Note aspects
never given explicit labels, ones shown presented purely ease
understanding; aspects exist simply groups snippets share common subject.
Also, word topics pictured here; word topic (Aspect, Value, Background)
assigned word snippet. model components described high level
Section 3.1 depth Section 4.

3.1.1 Entity
entity represents single object described review. restaurant
domain, represent individual restaurants, Tasca Spanish Tapas, Douzo Sushi
Bar, Outback Steakhouse.
3.1.2 Snippet
snippet user-generated short sequence words describing entity. snippets
provided user (for example, quick reaction box) extracted
complete reviews phrase extraction system one Sauper,
Haghighi, Barzilay (2010). assume snippet contains one single
aspect (e.g., pizza) one single value type (e.g., positive). restaurant domain,
corresponds giving opinion one particular dish category dishes. Examples
restaurant domain include pasta dishes perfection ,
fantastic drinks, lasagna rustica cooked perfectly.
99

fiSauper & Barzilay

3.1.3 Aspect
aspect corresponds one several properties entity. restaurant domain
entities represent restaurants, aspects may correspond individual dishes categories dishes, pizza alcoholic drinks. domain, entity
unique set aspects. allows us model aspects appropriate granularity.
example, Italian restaurant may dessert aspect pertains information variety cakes, pies, gelato. However, bakerys menu would
fall dessert aspect. Instead, present useful aspect-based summary,
would require separate aspects cakes, pies, on. aspects
entity-specific rather shared, ties restaurants aspects
common (e.g., sushi restaurants sashimi aspect); consider
point potential future work. Note still possible compare aspects across
entities (e.g., find best restaurant burger ) comparing respective word
distributions.
3.1.4 Value
Values represent information associated aspect. review domain, two
value types represent positive negative sentiment respectively. general, possible
use value represent distinctions; example, domain aspects
associated numeric value others associated text description,
set value type. intended distinctions may encouraged
use seed words (see Section 3.2), may left unspecified model assign
whatever finds best fit data. number value types must prespecified;
however, possible use either many types.
3.1.5 Word Topic
words snippet observed, word associated underlying
latent topic. possible latent topics correspond aspect, value, background
topic. example, review domain, latent topic words great terrible would
Value, words represent entity aspects pizza would Aspect,
stop words in-domain white noise food would Background.
3.2 Problem Setup
work, assume snippet words always observed, correlation
snippets entities known (i.e., know entity given snippet describes).
addition, assume part speech tags word snippet. final source
supervision, may optionally include small sets seed words lexical distribution,
order bias distribution toward intended meaning. example, sentiment
case, add seed words order bias one value distribution toward positive one
toward negative. Seed words certainly required; simply tool constrain
models use distributions fit prior expectations.
Note formulation, relevant aspects restaurant observed;
instead, represented lexical distributions induced inference time.
100

fiAutomatic Aggregation Joint Modeling Aspects Values

system output, aspects represented unlabeled clusters snippets.3 Given
formulation, goal work induce latent aspect value underlying
snippet.

4. Model
model generative formulation snippets corpus. section,
first describe detail general formulation notation model, discuss
novel changes enhancements particular corpora types. Inference model
discussed Section 5. mentioned previously, describe complete model
including aspect values.
4.1 General Formulation
model, assume collection snippet words entities, s. use si,j,w
denote wth word jth snippet ith entity. assume fixed vocabulary
words W .
present summary notation Table 1, concise summary model
Figure 4, model diagram Figure 5. three levels model design:
global distributions common snippets entities collection, entity-level
distributions common snippets describing single entity, snippet- word-level
random variables. Here, describe turn.
4.1.1 Global Distributions
global level, draw set distributions common entities corpus.
include everything shared across domain, background stop-word distribution,
value types, word topic transitions.
Background Distribution global background word distribution B drawn represent stop-words in-domain white noise (e.g., food becomes white noise corpus
restaurant reviews). distribution drawn symmetric Dirichlet concentration parameter B ; experiments, set 0.2.
Value Distributions value word distribution Vv drawn value type v.
example, review domain positive negative sentiment types,
distribution words positive type one negative type. Seed words
Wseedv given additional probability mass value priors type v; specifically,
non-seed word receives hyperparameter, seed word receives + V ;
experiments, set 0.15.
Transition Distribution transition distribution drawn represent transition
probabilities underlying word topics. example, may likely
Value Aspect transition review domain, fits phrases great pizza.
experiments, distribution given slight prior bias toward helpful transitions;
3. label desired, automatically extract one selecting highest probability words
particular aspect. simplicity exactness, provide manual cluster labels examples
paper.

101

fiSauper & Barzilay

Data Set

si,j,w
ti,j,w
W
Wseedv

Collection snippet words entities
wth word jth snippet ith entity
Part-of-speech tag corresponding si,j,w
Fixed vocabulary
Seed words value type v

Lexical Distributions
B
i,a
)

(A
v
V


Background word distribution
Aspect word distribution aspect entity
Value word distribution type v
Ignored words distribution

Distributions

i,a (a )
( )


Transition distribution word topics
Aspect-value multinomial aspect entity
Aspect multinomial entity
Part-of-speech tag distribution

Latent Variables
i,j
ZA
ZVi,j
i,j,w
ZW

Aspect selected si,j
Value type selected si,j
Word topic (A, V, B, ) selected si,j,w

Notation
K

V
B


Number aspects
Indicator corresponding
Indicator corresponding
Indicator corresponding
Indicator corresponding






aspect word
value word
background word
ignored word

Table 1: Notation used paper. Items marked
Section 4.2.



relate extensions mentioned

example, encouraging sticky behavior providing small boost self-transitions.
bias easily overridden data; however, provides useful starting point.
4.1.2 Entity-Specific Distributions
naturally variations aspects snippets describe many snippets
describe aspect. example, mobile device popular long battery life likely
snippets describing battery device known large screen.
domains may enormous variation aspect vocabulary; example, restaurant
reviews, two restaurants may serve food items compare. account
102

fiAutomatic Aggregation Joint Modeling Aspects Values

Global Level:
Draw background word distribution B Dirichlet(B W )
value type v,
Draw value word distribution Vv Dirichlet(W + V Wseedv )
Entity Level:
entity i,
i,a
Draw aspect word distributions
Dirichlet(A W ) = 1, . . . , K

Draw aspect value multinomial i,a Dirichlet(AV N ) = 1, . . . , K
Draw aspect multinomial Dirichlet(M K)
Snippet Level:
snippet j describing ith entity,
i,j

Draw snippet aspect ZA
i,j

Draw snippet value ZVi,j i,ZA

i,j,w1
i,j,w
|ZW
Draw sequence word topic indicators ZW
i,j
value ZVi,j
Draw snippet word given aspect ZA

si,j,w


i,Z i,j

,

Z i,j

V V ,



B ,

i,j,w
=A
ZW
i,j,w
=V
ZW
i,j,w
ZW = B

Figure 4: summary generative model presented Section 4.1. use Dirichlet(W ) denote finite Dirichlet prior hyper-parameter counts scalar
times unit vector vocabulary items. global value word distribution, prior
hyper-parameter counts vocabulary items V Wseedv , vector
vocabulary items set seed words value v.

variations, define set entity-specific distributions generate
aspect vocabulary popularity, well distribution value types aspect.
i,a
Aspect Distributions aspect word distribution
drawn aspect a.
represents distribution unigrams particular aspect. example,
domain restaurant reviews, aspects may correspond menu items pizza,
reviews cell phones, may correspond details battery life.

103

fiSauper & Barzilay

Value v
Background word
distribution

Transition
distribution

Value word
distributions

B



Vv

Entity

Aspect
Aspect
multinomial

Aspect word
distributions

Aspect-value
multinomial



Ai,a

i,a

Snippet j

Snippet aspect

Snippet value

ZAi,j

ZVi,j

HMM snippet words



i,j,w1
ZW

i,j,w
ZW

i,j,w+1
ZW

si,j,w1

si,j,w

si,j,w+1

ZAi,j , Ai,a
ZVi,j , Vv
B
Figure 5: graphical description model presented Section 4.1. written description generative process located Figure 4. Curved arrows indicate additional links
present model drawn readability.

104

fiAutomatic Aggregation Joint Modeling Aspects Values

aspect word distribution drawn symmetric Dirichlet prior hyperparameter
; experiments, set 0.075.
Aspect-Value Multinomials Aspect-value multinomials i,a determine likelihood
value type v corresponding aspect a. example, value types represent
positive negative sentiment, corresponds agreement sentiment across snippets.
Likewise, value types represent formatting integers, decimals, text, aspect
generally prefers type value. multinomials drawn symmetric
Dirichlet prior using hyperparameter AV ; experiments, set 1.0.
Aspect Multinomial aspect multinomial controls likelihood aspect
discussed given snippet. encodes intuition certain aspects
likely discussed others given entity. example, particular Italian
restaurant famous pizza, likely pizza aspect frequently
discussed reviews, drinks aspect may mentioned occasionally.
aspect multinomial encode higher likelihood choosing pizza snippet
aspect drinks. multinomial drawn symmetric Dirichlet distribution
hyperparameter ; experiments, set 1.0.
4.1.3 Snippet- Word-Specific Random Variables
Using distributions described above, draw random variables snippet
determine aspect value type described, well sequence
underlying word topics words.
i,j
snippet describe drawn aspect
Aspect single aspect ZA

multinomial . aspect words snippet (e.g., pizza corpus restaurant
i,j
i,ZA

reviews) drawn corresponding aspect word distribution

.

Value Type single value type ZVi,j drawn conditioned selected aspect
i,j
corresponding aspect-value multinomial i,ZA . value words snippet (e.g., great
Z i,j

review domain) drawn corresponding value word distribution V V .
i,j,1
i,j,m
generWord Topic Indicators sequence word topic indicators ZW
, . . . , ZW
ated using first-order Markov model parameterized transition matrix .
indicators determine unigram distribution generates word snippet.
i,j,w
example, ZW
= B, wth word snippet generated background word
distribution B .

4.2 Model Extensions
optional components model may improve performance
cases. briefly list here, present necessary modifications model
detail case. Modifications inference procedure presented Section 5.2.
First, corpora contain irrelevant snippets, may introduce additional word
distribution word topic Ignore allow model ignore certain snippets
pieces snippets altogether. Second, possible acquire part speech tags
105

fiSauper & Barzilay

snippets, using extra piece information quite beneficial. Finally, corpora
every entity expected share aspects, model altered use
set aspect distributions entities.
4.2.1 Ignoring Snippets
snippet data automatically extracted, may noisy, snippets may
violate initial assumptions one aspect one value. example, find
snippets mistakenly extracted neither aspect value.
extraneous snippets may difficult identify priori. compensate this, modify
model allow partial entire snippets ignored addition global
unigram distribution, namely Ignore distribution . distribution drawn
symmetric Dirichlet concentration parameter .
Ignore distribution differs Background distribution includes
common uncommon words. intended select whole snippets large portions
snippets, words may overlap Background distribution distributions. order successfully incorporate distribution model, must allow
i,j,w
consider Ignore topic I. Additionally, ensure
word topic indicator ZW
selects long segments text, give large boost prior Ignore Ignore
sequence transition distribution , similar boost self-transitions.
4.2.2 Part-of-Speech Tags
Part-of-speech tags provide valuable evidence determining snippet words
drawn distribution. example, aspect words often nouns, represent
concrete properties concepts domain. Likewise, domains, value words
describe aspects therefore tend expressed numbers adjectives.
intuition directly incorporated model form additional
outputs. Specifically, modify HMM produce words tags. Additionally,
, v , , similar corresponding unigram
define distributions tags
V
B
distributions.
4.2.3 Shared Aspects
domains regular, every entity expected express aspects
consistent set, beneficial share aspect information across entities. example,
medical domain, general set lab tests physical exam categories run
patients. Note quite unlike restaurant review case, restaurants
aspects completely different (e.g., pizza, curry, scones, on).
Sharing aspects way accomplished modifying aspect distributions
i,a
. Likewise, aspect-value multinomials i,a become

become global distributions
shared across entities . Treatment aspect multinomials depend
domain properties. distribution aspects expected across
entities, made global; however, individual entity expected exhibit
variation number snippets related aspect, kept entityspecific. example, reviews set cell phones may expected focus varying
106

fiAutomatic Aggregation Joint Modeling Aspects Values

Value v
Background word
distribution

Transition
distribution

Value word
distributions

B



Vv

Entity

Aspect
Aspect
multinomial

Aspect word
distributions

Aspect-value
multinomial



Aa



Snippet j

Snippet aspect

Snippet value

ZAi,j

ZVi,j

HMM snippet words



i,j,w1
ZW

i,j,w
ZW

i,j,w+1
ZW

si,j,w1

si,j,w

si,j,w+1

ZAi,j , Ai,a
ZVi,j , Vv
B
Figure 6: graphical description model shared aspects presented Section 4.2.
Note similarities Figure 5; however version, aspects shared entire
corpus, rather entity-specific. would possible share aspect
multinomial corpus-wide; case would indicate entities share
general distribution aspects, version individual entities allowed
completely different distributions.

parts, depending unique problematic phones. graphical
description changes compared original model shown Figure 6.
107

fiSauper & Barzilay

Mean-field Factorization
Q (B , V , , , , , Z)
= q (B ) q ()

N


!
q (Vv )

v=1




q





K



i,a
q
q i,a

!

a=1



!
i,j i,j i,j,w

q ZV q ZA
q ZW
w

j

Snippet Aspect Indicator
i,j
log q(ZA
= a) Eq(i ) log (a) +

X

i,j,w
i,a i,j,w
q(ZW
= A)Eq(i,a ) log
(s
)+


w

N
X

q(ZVi,j = v)Eq(i,a ) log i,a (v)

v=1

Snippet Value Type Indicator
X
X
i,j
i,j,w
log q(ZVi,j = v)
q(ZA
= a)Eq(i,a ) log i,a (v) +
q(ZW
= V )Eq(Vv ) log Vv (si,j,w )


w

Word Topic Indicator
X





i,j,w+1
i,j
i,j i,j,w
i,j,w1
i,j,w
+
= Eq(i,a ) log
, A, ZW
q ZA

log q ZW
= log P ZW = + Eq() log ZW

















i,j,w+1

i,j,w1
i,j,w
= V log P ZW = V + Eq() log ZW
, V V, ZW
log q ZW



+

X

q ZVi,j = v Eq(Vv ) log Vv si,j,w


v

log q

i,j,w
ZW

= B log P ZW = B + Eq() log


i,j,w1
,B
ZW

i,j,w+1
B, ZW



+ Eq(B ) log B si,j,w



Figure 7: mean-field variational algorithm used learning inference obtain posterior predictions snippet properties attributes, described Section 5.
Mean-field inference consists updating latent variable factors well
straightforward update latent parameters round robin fashion.

5. Inference
goal inference model predict aspect value snippet
product j, given text observed snippets, marginalizing remaining
hidden parameters:
i,j
P (ZA
, ZVi,j |s)
accomplish task using variational inference (Blei, Ng, & Jordan, 2003). Specifically, goal variational inference find tractable approximation Q() full
posterior model.
P (B , V , , , , , Z|s) Q(B , V , , , , , Z)
model, assume full mean-field factorization variational distribution,
shown Figure 7. variational approximation defined product factors q(),
assumed independent. approximation allows tractable inference
factor individually. obtain closest possible approximation, attempt set
108



fiAutomatic Aggregation Joint Modeling Aspects Values

q() factors minimize KL divergence true model posterior:
arg min KL(Q(B , V , , , , , Z)kP (B , V , , , , , Z|s))
Q()

5.1 Optimization
optimize objective using coordinate descent q() factors. Concretely,
update factor optimizing criterion factors fixed
current values:
q() EQ/q() log P (B , V , , , , , Z, s)
summary variational update equations given Figure 7, graphical
representation involved variables step presented Figure 8. Here,
present update factor.
5.1.1 Snippet Aspect Indicator
i,j
First, consider update snippet aspect indicator, ZA
(Figure 8a):
i,j
log q(ZA
= a) Eq(i ) log (a)
X
i,j,w
i,a i,j,w
+
q(ZW
= A)Eq(i,a ) log
(s
)

+

(1b)



w
N
X

(1a)

q(ZVi,j = v)Eq(i,a ) log i,a (v)

(1c)

v=1

optimal aspect particular snippet depends three factors. First, include
likelihood discussing aspect (Eqn. 1a). mentioned earlier, encodes
prior probability aspects discussed frequently others. Second,
examine likelihood particular aspect based words snippet (Eqn. 1b).
word identified aspect word, add probability discusses
aspect. Third, determine compatibility chosen aspect type
current aspect (Eqn. 1c). example, know value type likely integer,
assigned aspect accept integers.
5.1.2 Snippet Value Type Indicator
Next, consider update snippet value type indicator, ZVi,j (Figure 8b):
X
i,j
log q(ZVi,j = v)
q(ZA
= a)Eq(i,a ) log i,a (v)

(2a)



+

X

i,j,w
q(ZW
= V )Eq(Vv ) log Vv (si,j,w )

(2b)

w

best value type snippet depends two factors. First, snippet aspect
indicator, must take consideration compatibility snippet aspect
value type (Eqn. 2a). Second, word identified value word, include
likelihood comes given value type.
109

fiSauper & Barzilay

v
B

v
Vv








i,a




j



ZV , V
ZA ,
B



j

ZVi,j

w1
ZW

w
ZW

w+1
ZW

sw1

sw

sw+1

ZV , V
ZA ,
B



Vv



j

i,j
ZA

w1
ZW



ZA
sw1

Z





j





w
ZW

w+1
ZW

sw1

sw

sw+1

Vv

i,a


w+1
ZW



w1
ZW


sw

sw+1

Z

ZV
sw1 V





Vv

i,a


i,a

B


i,j
ZA

w
ZW

w
i. ZW
=A

w1
ZW

v



i,a

ZVi,j



B


i,a


ZVi,j

v




i,a

(b) Inference procedure snippet value, ZVi,j

v


i,a


i,j
ZA



i,j
(a) Inference procedure snippet aspect, ZA

B

Vv



i,a

i,j
ZA



B



i,a

ZVi,j



j

w
ZW

w+1
ZW



sw

sw+1

Z



i,j
ZA

w1
ZW

ZVi,j
w
ZW

w+1
ZW

sw

sw+1



w
ii. ZW
=V

sw1 B

w
iii. ZW
=B

i,j,w
(c) Inference procedure word topic, ZW

Figure 8: Variational inference update steps latent variable. latent variable
currently updated shown double circle, variables relevant
update highlighted black. variables impact update
grayed out. Note snippet aspect (a) snippet value type (b), update takes
form possible aspect value type. However, word topic (c),
update symmetric relevant variables different possible word topic.

110

fiAutomatic Aggregation Joint Modeling Aspects Values

5.1.3 Word Topic Indicator
i,j,w
Finally, consider update word topic indicators, ZW
(Figure 8c). Unlike
previous indicators, possible topic slightly different equation, must
marginalize possible aspects value types.






i,j,w
i,j,w1
i,j,w+1
log q ZW
= log P ZW = + Eq() log ZW
, A, ZW
X

i,j
i,j i,j,w
+
q ZA
= Eq(i,a ) log


(3a)










i,j,w
i,j,w1
i,j,w+1
log q ZW
= V log P ZW = V + Eq() log ZW
, V V, ZW
X


+
q ZVi,j = v Eq(Vv ) log Vv si,j,w

(3b)

v






i,j,w+1
i,j,w1
i,j,w
, B B, ZW
= B log P ZW = B + Eq() log ZW
log q ZW

+ Eq(B ) log B si,j,w

(3c)

update topic composed prior probability topic, transition probabilities using topic, probability word coming appropriate unigram distribution, marginalized possibilities snippet aspect value
indicators.
5.1.4 Parameter Factors
Updates parameter factors variational inference derived simple
counts latent variables ZA , ZV , ZW . Note include partial counts;
i,j
= a1 ) = 0.35, would contribute 0.35
particular snippet aspect probability P (ZA

count (a1 ).
5.1.5 Algorithm Details
Given set update equations, update procedure straightforward. First, iterate
corpus computing updated values random variable, batch
update factors simultaneously. update algorithm run convergence.
practice, convergence achieved 50th iteration, algorithm quite efficient.
Note batch update means update computed using values
previous iteration, unlike Gibbs sampling uses updated values runs
corpus. difference allows variational update algorithm parallelized, yielding
nice efficiency boost. Specifically, parallelize algorithm, simply split set
entities evenly among processors. Updates entity-specific factors variables
computed pass data, updates global factors collected
combined end pass.
111

fiSauper & Barzilay

5.2 Inference Model Extensions
discussed Section 4.2, add additional components model improve
performance data certain attributes. Here, briefly discuss modifications
inference equations extension.
5.2.1 Ignoring Snippets
main modifications model extension addition unigram
distribution word topic I, chosen ZW . update equation ZW
modified addition following:
i,j,w
log q(ZW
= I) log P (ZW = I) + Eq(I ) log (si,j,w )

pieces equation (Eqn. 3), composed prior probability
word topic likelihood word generated .
addition, transition distribution must updated include transition probabilities I. mentioned earlier, II transition receives high weight,
transitions receive low weight.
5.2.2 Part-of-Speech Tags
add part speech tags, model updated include part-of-speech distributions
i,a
, V , B , one word topic. Note unlike unigram distributions
Vv , corresponding tag distributions dependent snippet entity, aspect,
value. included referenced updates ZW follows:





i,j,w+1
i,j,w
i,j,w1
, A, ZW
log q ZW
= log P ZW = + Eq() log ZW
X

i,j i,j,w
i,j
+ Eq(A ) log ti,j,w +
= Eq(i,a ) log

q ZA









i,j,w+1
i,j,w1
i,j,w
, V V, ZW
= V log P ZW = V + Eq() log ZW
log q ZW
X


q ZVi,j = v Eq(Vv ) log Vv si,j,w
+ Eq(V ) log V ti,j,w +
v






i,j,w
i,j,w1
i,j,w+1
log q ZW
= B log P ZW = B + Eq() log ZW
, B B, ZW


+ Eq(B ) log B ti,j,w + Eq(B ) log B si,j,w
Here, define set tags ti,j,w tag corresponding word si,j,w .
5.2.3 Shared Aspects
global set shared aspects simplification model reduces total
aspect-value
number parameters. model redefines aspect distributions
multinomials . Depending domain, may redefine aspect multinomial
. resulting latent variable update equations same; parameter
112

fiAutomatic Aggregation Joint Modeling Aspects Values

factor updates changed. Rather collecting counts snippets describing single
entity, counts collected across corpus.

6. Experiments
perform experiments two tasks. First, test full model joint prediction
aspect sentiment corpus review data. Second, use simplified version
model designed identify aspects corpus medical summary data.
domains structured quite differently, therefore present different challenges
model.
6.1 Joint Identification Aspect Sentiment
first task test full model jointly predicting aspect sentiment
collection restaurant review data. Specifically, would dynamically select
set relevant aspects restaurant, identify snippets correspond
aspect, recover polarity snippet individually aspect whole.
perform three experiments evaluate models effectiveness. First, test
quality learned aspects evaluating predicted snippet clusters. Second, assess
quality polarity classification. Third, examine per-word labeling accuracy.
6.1.1 Data Set
data set task consists snippets selected Yelp restaurant reviews
previous system (Sauper et al., 2010). system trained extract snippets containing
short descriptions user sentiment towards aspect restaurant.4 purpose
experiment, select snippets labeled system referencing food.
order ensure enough data meaningful analysis, ignore restaurants
fewer 20 snippets across reviews. model easily operate
restaurants fewer snippets, want ensure cases select evaluation
nontrivial; i.e., sufficient number snippets cluster make
valid comparison. 13,879 snippets total, taken 328 restaurants
around Boston/Cambridge area. average snippet length 7.8 words,
average 42.1 snippets per restaurant. use MXPOST tagger (Ratnaparkhi,
1996) gather POS tags data. Figure 9 shows example snippets.
domain, value distributions consist one positive one negative distribution. seeded using 42 33 seed words respectively. Seed words hand-selected
based restaurant review domain; therefore, include domain-specific words
delicious gross. complete list seed words included Table 2.
6.1.2 Domain Challenges Modeling Techniques
domain presents two challenging characteristics model. First, wide
variety restaurants within domain, including everything high-end Asian fusion
cuisine greasy burger fast food places. try represent using single
4. exact training procedures, please reference paper.

113

fiSauper & Barzilay

Positive
amazing
delightful
extraordinary
flavorful
generous
heaven
inexpensive
perfect
recommend
stimulating
wonderful

Negative
awesome
divine
fantastic
free
good
huge
love
phenomenal
rich
strong
yummy

best
enjoy
fav
fresh
great
incredible
nice
pleasant
sleek
tasty

delicious
excellent
favorite
fun
happy
interesting
outstanding
quality
stellar
tender

average
bland
disappointed
expensive
gross
lame
meh
poor
tacky
tiny
uninspiring

awful
boring
disgusting
fatty
horrible
less
mushy
pricey
tasteless
unappetizing
worse

bad
confused
dry
greasy
inedible
mediocre
overcooked
salty
terrible
underwhelming
worst

Table 2: Seed words used model restaurant corpus, 42 positive words 33
negative words total. words manually selected data set.
shared set aspects, number aspects required would immense, would
extremely difficult model make fine-grained distinctions them.
defining aspects separately restaurant mentioned Section 4, achieve
proper granularity aspects individual restaurant without overwhelming
overlapping selection choices. example, model able distinguish
Italian restaurant may need single dessert aspect, bakery requires separate
pie, cake, cookie aspects.
Second, usually fairly cohesive set words refer particular
aspect (e.g., pizza aspect might commonly seen words slice, pepperoni,
cheese), near-unlimited set potential sentiment words. especially
pronounced social media domain many novel words used express
sentiment (e.g., deeeeeeeelish substitute delicious). mentioned Section 4,
part-of-speech transition components model helps identify unknown
words likely sentiment words; however, additionally need identify polarity sentiment. this, leverage aspect-value multinomial,
represents likelihood positive negative sentiment particular aspect.
snippets given aspect positive, likely word deeeeeeeelish
represents positive sentiment well.
6.1.3 Cluster Prediction
i,j
goal task evaluate quality aspect clusters; specifically ZA
variable
Section 4. ideal clustering, predicted clusters cohesive (i.e., snippets
predicted discuss given aspect related other) comprehensive (i.e.,
snippets discuss aspect selected such). example, snippet
assigned aspect pizza snippet mentions aspect pizza,
crust, cheese, toppings.

Annotation experiment, use set gold clusters complete sets
snippets 20 restaurants, 1026 snippets total (an average 51.3 snippets per
restaurant). Cluster annotations provided graduate students fluent English.
annotator provided complete set snippets particular restaurant,
asked cluster naturally. 199 clusters total, yields average
114

fiAutomatic Aggregation Joint Modeling Aspects Values

noodles meat actually +:::::::
pretty ::::::
good.
+:::::::::::::
recommend chicken noodle pho.
soggy.
noodles ::::::
chicken pho + good.
:::::
though.
spring rolls coffee + good,
:::::

spring roll wrappers little
dry tasting.
::::::::::::::::::
crispy
spring
rolls.
+ favorites


:::::::::
Crispy Tuna Spring Rolls + fantastic!
:::::::::





lobster roll mother ordered ::::
dry scant.
:::::
portabella mushroom + go-to
sandwich.
:::::
bread sandwich stale.
:::::
rather ::::::::
measly.
slice tomato :::::::

shumai california maki sushi +::::::::
decent.
+
spicy tuna roll eel roll perfect.
:::::::
:::
so::::::
great.
rolls spicy mayo ::::
+
love Thai rolls.
:::::

Figure 9: Example snippets data set, grouped according aspect. Aspect words
underlined colored blue, negative value words labeled - colored red,
positive value words labeled + colored green. grouping labeling
given data set must learned model.

10.0 clusters per restaurant. annotations high-quality; average annotator
agreement 81.9 MUC evaluation metric (described detail below). could
define different number clusters restaurant varying number aspect
distributions, simplicity ask baseline systems full model produce
10 aspect clusters per restaurant, matching average annotated number. Varying
number clusters simply cause existing clusters merge split; large
surprising changes clustering.
Baseline use two baselines task, using clustering algorithm weighted
TF*IDF implemented publicly available CLUTO package (Karypis, 2002),5
using agglomerative clustering cosine similarity distance metric (Chen, Branavan,
Barzilay, & Karger, 2009; Chen, Benson, Naseem, & Barzilay, 2011).
first baseline, Cluster-All, clusters entire snippets data set.
baseline put strong connection things lexically similar.
model uses aspect words tie together clusters, baseline may capture correlations
words model correctly identify aspect words.
5. Available http://glaros.dtc.umn.edu/gkhome/cluto/cluto/overview.

115

fiSauper & Barzilay

Cluster-All
Cluster-Noun
model

Precision

Recall

57.3
68.6
74.3

60.1
70.5
85.3

F1
58.7
69.5
79.4

Table 3: Results using MUC metric cluster prediction joint aspect value
identification task. MUC deficiency putting everything single
cluster artificially inflate score, models set use number clusters.
Note task, Cluster-Noun significantly outperforms Cluster-All
baseline, indicating part speech crucial piece information task.

second baseline, Cluster-Noun, works nouns snippets.
snippet POS-tagged using MXPOST (Ratnaparkhi, 1996),6 non-noun (i.e.,
NN, NNS, NNP, NNPS) words removed. expect aspects contain
least one noun, acts proxy aspect identification model.
Metric use MUC cluster evaluation metric task (Vilain, Burger, Aberdeen,
Connolly, & Hirschman, 1995). metric measures number cluster merges
splits required recreate gold clusters given models output. Therefore,
concisely show accurate clusters whole. would possible
artificially inflate score putting everything single cluster, parameters
model likelihood objective model prefers use available
clusters, number baseline system.
Results Results cluster prediction task Table 3. model shows strong
performance baseline, total error reduction 32% Cluster-Noun
baseline 50% Cluster-All baseline. common cause poor cluster
choices baseline systems inability distinguish words relevant
aspect words. example, Cluster-All baseline, many snippets use word
delicious, may end cluster based alone. Cluster-Noun
baseline able avoid pitfalls thanks built-in filter. able
avoid common value words adjectives focus seems
concrete portion aspect (e.g., blackened chicken); however, still cannot make
correct distinctions assumptions broken. model capable
distinguishing words aspect words (i.e., words relevant clustering),
choose clusters make sense overall.
6.1.4 Sentiment Analysis
evaluate systems predictions snippet sentiment using predicted posterior
i,j
value distributions snippet (i.e., ZA
). task, consider binary
i,j
judgment simply one higher value q(ZA
) (see Section 5). goal
task evaluate whether model correctly distinguishes sentiment value words.
6. Available http://www.inf.ed.ac.uk/resources/nlp/local_doc/MXPOST.html.

116

fiAutomatic Aggregation Joint Modeling Aspects Values

Majority
Discriminative-Small
Seed
Discriminative-Large
model

Accuracy
60.7
74.1
78.2
80.4
82.5

Table 4: Sentiment prediction accuracy model compared Discriminative
Seed baselines, well Majority representing majority class (Positive) baseline.
One advantage system ability distinguish aspect words sentiment words
order restrict judgment relevant terms; another leverage gains
biasing unknown sentiment words follow polarity observed snippets
relating aspect.
Annotation task, use set 662 randomly selected snippets Yelp
reviews express opinions. get clear result, set specifically excludes neutral,
mixed, potentially ambiguous snippets fries salty tasty
blackened chicken spicy, make 10% overall data. set
split training set 550 snippets test set 112 snippets, snippet
manually labeled positive negative. one baseline, use set positive
negative seed words manually chosen model, shown Table 2. Note
before, model access full corpus unlabeled data plus seed words,
labeled examples.
Baseline use two baselines task, one based standard discriminative
classifier one based seed words model.
Discriminative baseline task standard maximum entropy discriminative binary classifier7 unigrams. Given enough snippets enough unrelated aspects,
classifier able identify words great indicate positive sentiment
bad indicate negative sentiment, words chicken neutral
effect. illustrate effect training size, include results DiscriminativeSmall, uses 100 training examples, Discriminative-Large, uses 550
training examples.
Seed baseline simply counts number words positive
negative seed lists used model, Vseed+ Vseed , listed Table 2.
words Vseed+ , snippet labeled positive, words
Vseed , snippet labeled negative. tie seed words, split
prediction. seed word lists manually selected specifically restaurant
reviews (i.e., contain food-related sentiment words delicious), baseline
perform well.
Results overall sentiment classification accuracy system shown Table 4). model outperforms baselines. obvious flaw Seed baseline
7. Available https://github.com/lzhang10/maxent.

117

fiSauper & Barzilay

85

Accuracy

82.5
79.5

80

80.4

78.2 77.7
76.8

78.6

75
Discriminative
Seed
model

74.1
70

0

100 200 300 400 500 600
Number snippets training data

Figure 10: Discriminative baseline performance number training examples increases. performance generally increases, inconsistencies. main
issue baseline needs see examples words training data
improve; phenomenon seen plateau graph.

inability pre-specify every possible sentiment word. perform highly, due
tailoring restaurant domain good coverage frequent words (e.g.,
delicious, good, great), performance model indicates generalize
beyond seed words.
Discriminative-Large outperforms Seed baseline test set; however,
given smaller training set Discriminative-Small, performs worse. training
curve Discriminative baseline shown Figure 10. Discriminative
baseline system correctly identify polarity statements containing information
seen past, two main weaknesses. First, every sentiment word must
present training data. example, test data, rancid appears negative
sentence; however, appear training data, model labels example
incorrectly. problematic, way find training data every possible
sentiment word, especially social media data novel words typos frequent
occurrence. models ability generalize polarity snippets describing
particular aspect allows predict sentiment values words unknown polarity.
example, already several positive snippets describing particular aspect,
system guess snippet unknown polarity likely positive.
6.1.5 Per-Word Labeling Accuracy
goal task evaluate whether word correctly identified aspect
word, value word, background word. distinction crucial order achieve
correctness clustering sentiment analysis, errors may help us identify
weaknesses model.
118

fiAutomatic Aggregation Joint Modeling Aspects Values

rolls nt
well made .
:::::::: :::: ::::::
pita ::::::::
beyond dry
:::::::
tasted
cardboard !
:::
:::::::::::::::
falafel !
Falafel King best
::::
rolls spicy mayo
good .
::: :: :::::
!
Ordered spicy tuna california roll amazing
:::::::::

Table 5: Correct annotation set phrases containing elements may confusing,
annotators tested allowed annotate actual test data. Aspect words colored blue underlined; value words colored orange underlined
wavy line. common mistakes include: annotating nt background (because
attached background word was), annotating cardboard aspect
noun, annotating Falafel King aspect subject position.
Annotation Per-word annotation acquired Mechanical Turk. per-word labeling task seems difficult Turk annotators, implement filtering procedure
ensure high-quality annotators allowed submit results. Specifically,
ask annotators produce labels set difficult phrases known labels (shown
Table 5). annotators successfully produced correct mostly-correct annotations allowed access annotation tasks containing new phrases.
unknown tasks presented 3 annotators, majority label taken word.
total, test 150 labeled phrases, total 7,401 labeled words.
Baseline baseline task relies intuition part-of-speech
useful proxy aspect value identification. know aspects usually represent
concrete entities, often nouns, value words descriptive counting,
often adjectives adverbs. Therefore, use MXPOST tagger find
POS word snippet. main baseline, Tags-Full, assign noun
(NN*) aspect label, numeral, adjective, adverb, verb participle (CD, RB*,
JJ*, VBG, VBN) value label. comparison, present results smaller tagset,
Small-Tags, labeling nouns (NN*) aspect adjectives (JJ*) values. Note
tags added Tags-Full baseline beneficial baselines score.
Tree expansion full model baselines designed pick
relevant individual words rather phrases, may correspond well phrases
humans selected relevant. Therefore, evaluate set expanded
labels identified parse trees Stanford Parser (Klein & Manning, 2003).8 Specifically, non-background word, identify largest containing noun phrase (for
aspects values) adjective adverb phrase (for values only)
contain oppositely-labeled words. example, noun phrase blackened chicken,
chicken labeled aspect word blackened labeled background word,
labeled aspect words. However, noun phrase tasty chicken
tasty already labeled value, label changed expansion
attempted. final heuristic step, punctuation, determiners, conjunctions
8. Available http://nlp.stanford.edu/software/lex-parser.shtml.

119

fiSauper & Barzilay

Tree expansion procedure aspect words:
1. Find noun phrases contain aspect word (pork).


+ innovative

::::::::::::

appetizers pork apple glaze
NP1
NP2
NP3

+ highlights
:::::::::::

2. Select largest noun phrase contain value (sentiment) words.
NP1 valid; contain value words. However, largest valid NP.
NP2 valid; contain value words. largest valid NP, selected.
NP3 contains value word (+ innovative),
invalid.
::::::::::::

3. Convert background words within selected noun phrase aspect words
except punctuation, determiners, conjunctions.


+ innovative

::::::::::::

appetizers pork apple glaze

+ highlights
:::::::::::

Figure 11: tree expansion procedure value words, example snippet.
procedure similar aspect words, except adjective phrases adverb phrases
considered expansion.
Aspect
Precision Recall

F1

Value
Precision Recall

F1

Tags-Small
Tree

79.9
74.0

79.5
83.0

79.7
78.2

78.5
79.2

45.0
57.4

57.2
66.5

Tags-Full
Tree

79.9
75.6

79.5
81.4

79.7
78.4

78.1
77.1

68.7
70.1

73.1
73.4

85.2
79.5

52.6
71.9

65.0
75.5

70.5
76.7

61.6
70.9

65.7
73.7

model
Tree

Table 6: Per-word labeling precision recall model compared Tags-Small
Tags-Full baselines, without expansion trees. model
precise aspect better recall value. Note general process expanding labels tree structure increases recall expense precision.
would newly labeled aspect value words ignored kept background
words. steps procedure illustrative example shown Figure 11.
Results evaluate systems precision recall aspect value separately.
Results systems shown Table 6. model without tree expansion
highly precise expense recall; however expansion performed, recall
improves tremendously, especially value words.
result initially disappointing, possible adjust model parameters
increase performance task; example, aspect words could put additional
120

fiAutomatic Aggregation Joint Modeling Aspects Values

moqueca delicious
:::::::
perfect winter food , ::::::
warm , filling hearty :::
:::
::::::
heavy .
:::::::::
bacon wrapped almond dates ::::::::
amazing plantains cheese boring
.
::::::
artichoke homemade pasta appetizers :::::
great

Table 7: High-precision, low-recall aspect word labeling full model. Note
human would likely identify complete phrases bacon wrapped almond dates
homemade pasta appetizers; however, additional noise degrades performance
clustering task.

start

V
B



0.06
0.19
0.02
0.22
0.00

V
0.00
0.03
0.32
0.26
0.00

B
0.94
0.77
0.47
0.43
0.01


0.00
0.01
0.01
0.17
0.99

end
0.00
0.00
0.18
0.06
0.00

Table 8: Learned transition distribution model. pattern high-precision
aspect words represented preference continuing string several aspect
words, causing model prefer single, precise aspect words. Likewise, better recall
value words indicated higher value V V transition, encourage
several words row marked value words.
i,j,w
mass prior ZW
= increase Dirichlet hyperparameter . However,
increases performance word labeling task, decreases performance
correspondingly clustering task. examination data, correlation
perfectly reasonable. order succeed clustering task, model selects
relevant portions snippet aspect words. entire aspect value
identified, clustering becomes noisy. Table 7 shows examples high-precision
labeling achieves high clustering performance, Table 8 shows example
learned transition distribution creates labeling.

6.2 Aspect Identification Shared Aspects
second task uses simplified version model designed aspect identification
only. task, use corpus medical visit summaries. domain,
summary expected contain similar relevant information; therefore, set aspects
shared corpus-wide. evaluate model formulation, examine predicted
clusters snippets, full model.
6.2.1 Data Set
data set task consists phrases selected dictated patient summaries
Pediatric Environmental Health Clinic (PEHC) Childrens Hospital Boston, specializing
treatment children lead poisoning. Specifically, patients office visit lab
results completed, PEHC doctor dictates letter referring physician containing
121

fiSauper & Barzilay

information previous visits, current developmental family status, in-office exam
results, lab results, current diagnosis, plan future.
experiment, select phrases in-office exam lab results sections
summaries. Phrases separated heuristically commas semicolons. domain contains significant amount extraneous information, restaurant
domain, must extract phrases believe bear relevance task hand.
However, medical text dense nearly relevant, heuristic separation
sufficient extract relevant phrases. 6198 snippets total, taken 271
summaries. average snippet length 4.5 words, average 23 snippets
per summary. Yelp domain, use MXPOST tagger (Ratnaparkhi, 1996)
gain POS tags. Figure 12 shows example snippets. domain,
values; simply concentrate aspect-identification task. Unlike restaurant
domain, use seed words.
6.2.2 Domain Challenges Modeling Techniques
contrast restaurant domain, medical domain uses single global set aspects.
represent either individual lab tests (e.g., lead level, white blood cell count) particular body systems (e.g., lungs cardiovascular ). aspects far common
others, uncommon summary include one two snippets
given aspect. Therefore, mentioned Section 4.2, model aspect word
distributions aspect multinomial shared entities corpus.
contrast restaurant domain, aspects defined words taken
entire snippet. Rather aspects associated names measurements
(e.g., weight), units descriptions measurement (e.g., kilograms)
relevant aspect definition. property extends numeric written measurements; example, aspect lungs commonly described clear auscultation
bilaterally. order achieve high performance, model must leverage
clues provide proper aspect identification name measurement missing
(e.g., patient 100 cm). part speech still important factor model,
predict greater importance additional parts speech
nouns.
Finally, data set noisy contains irrelevant snippets, section
headings (e.g., Physical examination review systems) extraneous information.
described Section 4.2, modify model ignore partial complete
snippets.
6.2.3 Cluster Prediction
joint aspect sentiment prediction, goal task evaluate quality
aspect identification. aspects shared across documents, clusters
generally much larger, set annotated snippets represents fraction
cluster.
Annotation experiment, use set gold clusters gathered 1,200 snippets, annotated doctor expert domain Pediatric Environmental Health Clinic Childrens Hospital Boston. Note mentioned before, clusters
122

fiAutomatic Aggregation Joint Modeling Aspects Values

113 cm height
Patients height 146.5 cm
Lungs: Clear bilaterally auscultation
lungs normal
Heart regular rate rhythm; murmurs
Heart normal S1 S2

Figure 12: Example snippets medical data set, grouped according aspect.
Aspect words underlined colored blue. grouping labeling given
data set must learned model.

Cluster-All
Cluster-Noun
model

Precision

Recall

88.2
88.4
89.1

93.0
83.9
93.4

F1
90.5
86.1
91.2

Table 9: Results using MUC metric cluster prediction aspect identification
task. Note Cluster-All baseline significantly outperforms Cluster-Noun,
opposite observe joint aspect value prediction task. due
dependence aspect identification name lab test,
units description test results, mentioned Section 6.2.2.

global domain (e.g., many patients snippets representing blood lead
level, grouped one cluster). doctor asked cluster 100
snippets time (spanning several patients), clustering entire set would
infeasible human annotator. 12 sets snippets clustered, resulting
clusters manually combined match similar clusters set. example, blood lead level cluster first set 100 snippets combined
corresponding blood lead level clusters set snippets. cluster
final set fewer 5 members removed. total, yields gold set
30 clusters. 1,053 snippets total, average 35.1 snippets per cluster.
match this, baseline systems full model asked produce 30 clusters across
full data set.
Baselines & Metric keep results consistent previous task,
use baselines evaluation metric. baselines rely TF*IDFweighted clustering algorithm, specifically implemented CLUTO package (Karypis,
2002) using agglomerative clustering cosine similarity distance metric. before,
Cluster-All represents baseline using unigrams snippets entire data set,
Cluster-Noun works nouns snippets. use
MUC cluster evaluation metric task. details baselines
evaluation metric, please see Section 6.1.3.
123

fiSauper & Barzilay

Results experiment, system demonstrates improvement 7%
Cluster-All baseline. Absolute performance relatively high systems
medical domain, indicating lexical clustering task less misleading
restaurant domain. interesting note unlike restaurant domain,
Cluster-All baseline outperforms Cluster-Noun baseline. mentioned Section 6.2.2, medical data notable relevance entire snippet clustering
(e.g., weight kilograms useful identify weight aspect).
property, using nouns cluster Cluster-Noun baseline hurts performance
significantly.

7. Conclusions Future Work
paper, presented approach fine-grained content aggregation using
probabilistic topic modeling techniques discover structure individual text snippets.
model able successfully identify clusters snippets data set discuss
aspect entity well associated values (e.g., sentiment). requires
annotation, small list seed vocabulary bias positive negative
distributions proper direction.
results demonstrate delving structure snippet assist
identifying key words important unique domain hand.
values learned, joint identification aspect value help improve
quality results. word labeling analysis reveals model learns different
type labeling task; specifically, strict, high-precision labeling clustering
task high-recall labeling sentiment. follows intuition important
identify specific main points clustering, sentiment analysis task,
may often several descriptions conflicting opinions presented need
weighed together determine overall sentiment.
model admits fast, parallelized inference procedure. Specifically, entire inference procedure takes roughly 15 minutes run restaurant corpus less 5
minutes medical corpus. Additionally, model neatly extensible adjustable
fit particular characteristics given domain.
limitations model improved future work:
First, model makes attempt explicitly model negation word interactions,
increasing difficulty aspect sentiment analysis model. performing error analysis, find negation common source error sentiment
analysis task. Likewise, aspect side, model make errors attempting
differentiate aspects ice cream cream cheese share common aspect
word cream, despite phrases occurring bigrams. using connections
stronger way, indicator variable negation higher-order HMM,
model could make informed decisions.
Second, defining aspects per-entity restaurant domain advantages
possible get fine-grained set applicable aspects, fails leverage
potential information data set. Specifically, know restaurants sharing
type (e.g., Italian, Indian, Bakery, etc.) share common aspects;
however, ties current model. Likewise, even global
124

fiAutomatic Aggregation Joint Modeling Aspects Values

level, may aspects tie across restaurants. hierarchical version
model would able tie together identify different types aspects:
global (e.g., presentation), type-level (e.g., pasta Italian type), restaurant-level
(e.g., restaurants special dish).

Bibliographic Note
Portions paper published previously conference publication (Sauper,
Haghighi, & Barzilay, 2011); however paper significantly extends work. describe several model generalizations extensions (Section 4.2) effects
inference procedure (Section 5.2). present new experimental results, including additional baseline comparisons additional experiment (Section 6.1). introduce
new domain, medical summary text, quite different domain restaurant
reviews therefore requires several fundamental changes model (Section 6.2).

Acknowledgments
authors acknowledge support NSF (CAREER grant IIS-0448168), NIH (grant
5-R01-LM009723-02), Nokia, DARPA Machine Reading Program (AFRL prime
contract no. FA8750-09-C-0172). Thanks Peter Szolovits MIT NLP group
helpful comments. opinions, findings, conclusions, recommendations expressed
paper authors, necessarily reflect views funding
organizations.

References
Barzilay, R., McKeown, K. R., & Elhadad, M. (1999). Information fusion context
multi-document summarization. Proceedings ACL, pp. 550557.
Blei, D. M., & McAuliffe, J. (2008). Supervised topic models. Advances NIPS, pp.
121128.
Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent Dirichlet allocation. Journal
Machine Learning Research, 3, 9931022.
Carenini, G., & Moore, J. D. (2006). Generating evaluating evaluative arguments.
Artificial Intelligence, 170, 925952.
Carenini, G., Ng, R., & Pauls, A. (2006). Multi-document summarization evaluative text.
Proceedings EACL, pp. 305312.
Carenini, G., Ng, R. T., & Zwart, E. (2005). Extracting knowledge evaluative text.
Proceedings K-CAP, pp. 1118.
Chen, H., Benson, E., Naseem, T., & Barzilay, R. (2011). In-domain relation discovery
meta-constraints via posterior regularization. Proceedings ACL, pp. 530540.
Chen, H., Branavan, S. R. K., Barzilay, R., & Karger, D. R. (2009). Global models
document structure using latent permutations. Proceedings ACL/HLT, pp. 371
379.
125

fiSauper & Barzilay

Crammer, K., & Singer, Y. (2001). Pranking ranking. Advances NIPS, pp.
641647. MIT Press.
Dang, H. T. (2005). Overview DUC 2005. Proceedings DUC EMNLP/HLT.
Dang, H. T. (2006). Overview DUC 2006. Proceedings DUC NAACL/HLT.
Dave, K., Lawrence, S., & Pennock, D. M. (2003). Mining peanut gallery: opinion
extraction semantic classification product reviews. Proceedings WWW,
pp. 519528.
Hu, M., & Liu, B. (2004). Mining summarizing customer reviews. Proceedings
SIGKDD, pp. 168177.
Karypis, G. (2002). CLUTO clustering toolkit. Tech. rep. 02-017, Dept. Computer
Science, University Minnesota. Available http://www.cs.umn.educluto.
Kim, H. D., & Zhai, C. (2009). Generating comparative summaries contradictory opinions
text. Proceedings CIKM, pp. 385394.
Kim, S., & Hovy, E. (2005). Automatic detection opinion bearing words sentences.
Proceedings IJCNLP, pp. 6166.
Kim, S.-M., & Hovy, E. (2006). Automatic identification pro con reasons online
reviews. Proceedings COLING ACL, pp. 483490.
Klein, D., & Manning, C. D. (2003). Accurate unlexicalized parsing. Proceedings ACL,
pp. 423430.
Liu, B., Hu, M., & Cheng, J. (2005). Opinion observer: Analyzing comparing opinions
web. Proceedings WWW, pp. 342351.
Lu, Y., & Zhai, C. (2008). Opinion integration semi-supervised topic modeling.
Proceedings WWW, pp. 121130.
Mani, I. (2001). Automatic summarization, Vol. 3. John Benjamins Pub Co.
McDonald, R., Hannan, K., Neylon, T., Wells, M., & Reynar, J. (2007). Structured models
fine-to-coarse sentiment analysis. Proceedings ACL, pp. 432439.
Mei, Q., Ling, X., Wondra, M., Su, H., & Zhai, C. (2007). Topic sentiment mixture: modeling
facets opinions weblogs. Proceedings WWW, pp. 171180.
Pang, B., & Lee, L. (2004). sentimental education: Sentiment analysis using subjectivity
summarization based minimum cuts. Proceedings ACL, pp. 271278.
Pang, B., & Lee, L. (2008). Opinion mining sentiment analysis. Foundations Trends
Information Retrieval, 2, 1135.
Pang, B., Lee, L., & Vaithyanathan, S. (2002). Thumbs up? Sentiment classification using
machine learning techniques. Proceedings EMNLP, pp. 7986.
Popescu, A.-M., Nguyen, B., & Etzioni, O. (2005). OPINE: Extracting product features
opinions reviews. Proceedings EMNLP/HLT, pp. 339346.
Radev, D., & McKeown, K. (1998). Generating natural language summaries multiple
on-line sources. Computational Linguistics, 24 (3), 469500.
126

fiAutomatic Aggregation Joint Modeling Aspects Values

Radev, D. R., Jing, H., & Budzikowska, M. (2000). Centroid-based summarization multiple documents: sentence extraction, utility-based evaluation, user studies.
Proceedings NAACL-ANLP Workshop Automatic Summarization, pp. 21
30.
Ratnaparkhi, A. (1996). maximum entropy model part-of-speech tagging. Proceedings EMNLP, pp. 133142.
Sauper, C., Haghighi, A., & Barzilay, R. (2010). Incorporating content structure text
analysis applications. Proceedings EMNLP, pp. 377387.
Sauper, C., Haghighi, A., & Barzilay, R. (2011). Content models attitude. Proceedings ACL, pp. 350358.
Seki, Y., Eguchi, K., Kanodo, N., & Aono, M. (2005). Multi-document summarization
subjectivity analysis DUC 2005. Proceedings DUC EMNLP/HLT.
Seki, Y., Eguchi, K., Kanodo, N., & Aono, M. (2006). Opinion-focused summarization
analysis DUC 2006. Proceedings DUC NAACL/HLT, pp. 122130.
Snyder, B., & Barzilay, R. (2007). Multiple aspect ranking using good grief algorithm.
Proceedings NAACL/HLT, pp. 300307.
Titov, I., & McDonald, R. (2008a). joint model text aspect ratings sentiment
summarization. Proceedings ACL, pp. 308316.
Titov, I., & McDonald, R. (2008b). Modeling online reviews multi-grain topic models.
Proceedings WWW, pp. 111120.
Turney, P. D. (2002). Thumbs thumbs down?: semantic orientation applied unsupervised classification reviews. Proceedings ACL, pp. 417424.
Vilain, M., Burger, J., Aberdeen, J., Connolly, D., & Hirschman, L. (1995). modeltheoretic coreference scoring scheme. Proceedings MUC, pp. 4552.
Yu, H., & Hatzivassiloglou, V. (2003). Towards answering opinion questions: Separating
facts opinions identifying polarity opinion sentences. Proceedings
EMNLP, pp. 129136. Association Computational Linguistics.

127


