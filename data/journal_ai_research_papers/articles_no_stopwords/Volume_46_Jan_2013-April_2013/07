journal artificial intelligence

submitted published

incremental clustering expansion faster
optimal decentralized pomdps
frans oliehoek

frans oliehoek maastrichtuniversity nl

maastricht university
maastricht netherlands

matthijs j spaan

j spaan tudelft nl

delft university technology
delft netherlands

christopher amato

camato csail mit edu

massachusetts institute technology
cambridge usa

shimon whiteson

whiteson uva nl

university amsterdam
amsterdam netherlands

abstract
article presents state art optimal solution methods decentralized
partially observable markov decision processes dec pomdps general
collaborative multiagent uncertainty building generalized multiagent gmaa reduces tree one shot collaborative
bayesian games cbgs describe several advances greatly expand range decpomdps solved optimally first introduce lossless incremental clustering
cbgs solved gmaa achieves exponential speedups without sacrificing
optimality second introduce incremental expansion nodes gmaa search
tree avoids need expand children number worst case
doubly exponential nodes depth particularly beneficial little clustering
possible addition introduce hybrid heuristic representations
compact thereby enable solution larger dec pomdps provide theoretical
guarantees suitable heuristic used incremental clustering incremental expansion yield complete search equivalent finally
present extensive empirical demonstrating gmaa ice
synthesizes advances optimally solve dec pomdps unprecedented size

introduction
key goal artificial intelligence development intelligent agents interact
environment order solve achieve goals maximize utility
agents sometimes act alone researchers increasingly interested collaborative multiagent
systems teams agents work together perform manner tasks multiagent
systems appealing tackle inherently distributed
facilitate decomposition complex tackled single
c

ai access foundation rights reserved

fioliehoek spaan amato whiteson

agent huhns sycara panait luke vlassis busoniu babuska
de schutter
one primary challenges multiagent systems presence uncertainty even
single agent systems outcome action may uncertain e g action may fail
probability furthermore many state environment may
uncertain due limited noisy sensors however multiagent settings
often greatly exacerbated since agents access sensors typically
small fraction complete system ability predict agents
act limited complicating cooperation uncertainties properly addressed
arbitrarily bad performance may
principle agents use communication synchronize beliefs coordinate
actions however due bandwidth constraints typically infeasible agents
broadcast necessary information agents addition many realistic
scenarios communication may unreliable precluding possibility eliminating uncertainty agents actions
especially recent years much focused approaches collaborative
multiagent systems deal uncertainty principled way yielding wide variety
solution methods pynadath tambe goldman zilberstein
seuken zilberstein article focuses decentralized partially observable
markov decision process dec pomdp general model collaborative multiagent uncertainty unfortunately solving dec pomdp e computing optimal
plan generally intractable nexp complete bernstein givan immerman zilberstein
fact even computing solutions absolutely bounded error e approximate
solutions nexp complete rabinovich goldman rosenschein particular
number joint policies grows exponentially number agents observations
doubly exponentially respect horizon though complexity preclude methods efficient developing better optimal
solution methods dec pomdps nonetheless important goal several reasons
first since complexity describe worst case still great potential
improve performance optimal methods practice fact evidence
many solved much faster worst case complexity bound indicates
allen zilberstein article present experiments clearly demonstrate
point many methods propose scale vastly beyond would
expected doubly exponential dependence horizon
second computer speed memory capacity increase growing set small
medium sized solved optimally arise naturally
others decomposition larger instance may possible
extrapolate optimal solutions shorter horizons
starting point policy search longer horizon work eker
akn use shorter horizon communication solutions inside
communication nair roth yohoo goldman zilberstein generally
optimal policies smaller potentially used good solutions larger
instance transfer oliehoek oliehoek whiteson spaan
surprisingly number states dec pomdp less important e g brute force search depends
number states via policy evaluation routine scales linearly number states



fiincremental clustering expansion faster optimal dec pomdps

employs optimal solutions agents better solve
many agents performing approximate influence abstraction influence search
witwicki oliehoek witwicki kaelbling optimal solutions component
potentially used near optimal solutions larger
third optimal methods offer important insights nature specific dec pomdp
solutions instance methods introduced article enabled
discovery certain properties broadcastchannel benchmark make
much easier solve
fourth optimal methods provide critical inspiration principled approximation methods fact almost successful approximate dec pomdp methods optimal
ones see e g seuken zilberstein b dibangoye mouaddib chai draa
amato dibangoye zilberstein wu zilberstein chen oliehoek
locally optimal ones velagapudi varakantham scerri sycara clustering technique presented article forms basis recently introduced approximate
clustering technique wu zilberstein chen
finally optimal methods essential benchmarking approximate methods recent
years huge advances approximate solution dec pomdps leading
development solution methods deal large horizons hundreds agents
many states e g seuken zilberstein b amato et al wu et al
oliehoek velagapudi et al
however since computing even approximate
solutions nexp complete method whose complexity doubly exponential cannot
guarantees absolute error solution assuming exp nexp
existing effective approximate methods quality guarantees
consequently difficult meaningfully interpret empirical performance without
upper bounds optimal methods supply approximate methods benchmarked lower bounds e g approximate methods comparisons cannot
detect method fails good solutions requires benchmarking
upper bounds unfortunately upper bounds easier compute qmdp
qpomdp loose helpful oliehoek spaan vlassis
benchmarking respect optimal solutions important part verification
approximate since existing optimal methods tackle small
scaling optimal solutions larger critical goal
contributions
article presents state art optimal solution methods dec pomdps
particular describes several advances greatly expand horizon many decpomdps solved optimally addition proposes evaluates complete
synthesizes advances generalized multiagent
gmaa oliehoek spaan vlassis makes possible reduce
tree one shot collaborative bayesian games cbgs appeal
method velagapudi et al repeatedly computes best responses way similar dp jesp
nair tambe yokoo pynadath marsella best response computation however exploits
sparsity interactions
note refer methods without quality guarantees approximate rather heuristic avoid
confusion heuristic search used throughout article exact



fioliehoek spaan amato whiteson

abstraction layer introduces led insights decpomdps turn improved solution methods describe
specific contributions article
introduce lossless clustering cbgs technique reduce size cbgs
gmaa enumerates possible solutions preserving optimality
exponentially reduce number child nodes gmaa search tree leading
huge increases efficiency addition applying incremental clustering ic
gmaa gmaa ic method avoid clustering exponentially sized cbgs
introduce incremental expansion ie nodes gmaa search tree although
clustering may reduce number children search node number
worst case still doubly exponential nodes depth gmaa ice applies
ie gmaa ic addresses creating next child node
candidate expansion
provide theoretical guarantees gmaa ic gmaa ice particular suitable heuristic complete
search equivalent
introduce improved heuristic representation tight heuristics
underlying pomdp solution qpomdp value function resulting
assuming step delayed communication qbg essential heuristic search methods gmaa oliehoek spaan vlassis however space needed
store heuristics grows exponentially horizon introduce hybrid representations compact thereby enable solution larger

present extensive empirical substantial improvements
current state art whereas seuken zilberstein argued gmaa
best optimally solve dec pomdps one horizon brute force
search demonstrate gmaa ice much better addition
provide comparative overview competitive optimal solution methods
literature
primary aim techniques introduced article improve scalability
respect horizon empirical confirm techniques highly
successful regard added bonus experiments demonstrate improvement
scalability respect number agents particular present first optimal
general non special case dec pomdps three agents extensions
techniques achieve improvements respect number agents
well promising ways combine ideas behind methods state art
approximate approaches discussed future work section
article synthesizes extends already reported two conference papers oliehoek
whiteson spaan spaan oliehoek amato



fiincremental clustering expansion faster optimal dec pomdps

organization
article organized follows section provides background dec pomdp model
gmaa heuristic search solution method well suitable heuristics section
introduce lossless clustering cbgs integration gmaa section introduces incremental expansion search nodes empirical evaluation proposed
techniques reported section give treatment related work section future
work discussed section conclusions drawn section

background
dec pomdp multiple agents must collaborate maximize sum common
rewards receive multiple timesteps actions affect immediate
rewards state transition current state known
agents timestep agent receives private observation correlated
state



definition dec pomdp tuple r b h
n finite set agents


finite set states

ai set joint actions ha ai finite set actions
available agent
transition function specifying state transition probabilities pr
oi finite set joint observations every stage one joint observation
ho received agent observes component oi
observation function specifies observation probabilities pr
r immediate reward function mapping pairs real numbers
b initial state distribution time denotes infinite
set probability distributions finite set
h horizon e number stages consider case h finite
stage h agent takes individual action receives individual
observation
example recycling robots illustrate dec pomdp model consider team robots tasked
removing trash office building depicted fig robots sensors marked
trash cans motors move around order look cans well gripper arms grasp carry
small trash cans light compact enough single robot carry large trash cans
require multiple robots carry together people use larger trash
cans fill quickly robot must ensure battery remains charged moving
charging station expires battery level robot degrades due distance
robot travels weight item carried robot knows battery level
robots location robots within sensor range goal
remove much trash possible given time period
represented dec pomdp natural way states consist
different locations robot battery levels different amounts trash
cans actions ai robot consist movements different directions well decisions


fioliehoek spaan amato whiteson

figure illustration recycling robots example two robots remove
trash office environment three small blue trash cans two large yellow ones
situation left robot might observe large trash next full
robot small trash empty however none sure trash
cans state due limited sensing capabilities see state trash cans
away particular one robot knowledge regarding observations robot
pick trash recharge battery range charging station
observations oi robot consist battery level location locations
robots sensor range amount trash cans within range rewards r could consist
large positive value pair robots emptying large full trash small positive value
single robot emptying small trash negative values robot depleting battery
trash overflowing optimal solution joint policy leads expected behavior given
rewards properly specified ensures robots cooperate empty
large trash cans appropriate small ones individually considering battery usage

explanatory purposes consider much simpler called decentralized tiger nair et al
example dec tiger dec tiger concerns two agents hallway two doors behind one door treasure behind tiger state
describes door tiger behindleft sl right sr occurring probability
e initial state distribution b uniform agent perform three actions open left
door aol open right door aor listen ali clearly opening door treasure
yield reward opening door tiger severe penalty greater reward
given agents opening correct door time good strategy
probably involve listening first listen actions however minor cost negative reward
every stage agents get observation agents hear tiger behind left
ohl right ohr door agent chance hearing incorrectly getting wrong
observation moreover observation informative agents listen agent opens
door agents receive uninformative uniformly drawn observation resets
sl sr equal probability point continues agents may
able open door treasure multiple times note since two observations
agents get ohl ohr agents way detecting reset
one agent opens door listens agent able tell
door opened complete specification see discussion nair et al

given dec pomdp agents common goal maximize expected cumulative
reward return task entails finding joint policy h n
space joint policies specifies individual policy agent


fiincremental clustering expansion faster optimal dec pomdps

individual policy general specifies individual action action observation history
aoh ait oti e g ati however possible restrict
attention deterministic pure policies case maps observation history
action e g number policies
oh oti oit



h



ai
number joint policies therefore

n h


denote largest individual action observation sets quality
particular joint policy expressed expected cumulative reward induces referred
value
definition value v joint policy
v e

h
hx





r st b



expectation sequences states actions observations
dec pomdp optimal joint policy e joint
policy maximizes value arg max v
individual policy depends local information oi available
agent line execution phase truly decentralized communication takes place
modeled via actions observations however may take place
line phase centralized scenario consider article
detailed introduction dec pomdps see e g work seuken zilberstein
oliehoek
heuristic search methods
recent years numerous dec pomdp solution methods proposed
methods fall one two categories dynamic programming heuristic search methods
dynamic programming methods take backwards bottom perspective first considering policies last time step h construct policies stage
h etc contrast heuristic search methods take forward top perspective
first constructing plans extending later stages
article focus heuristic search shown state art
make clear section method interpreted searching
tree collaborative bayesian games cbgs cbgs provide convenient abstraction
layer facilitates explanation techniques introduced article
section provides concise background heuristic search methods
detailed description see work oliehoek spaan vlassis description dynamic programming methods relationship heuristic search methods
see work oliehoek
multiagent
szer charpillet zilberstein introduced heuristically guided policy search method
called multiagent maa performs search partially specified joint policies


fioliehoek spaan amato whiteson









ali




ohr

ohl
aol



aol

ohl

ohr

ohl

ohr

ali

ali

aol

ali




figure arbitrary policy dec tiger figure illustrates different
types partial policies used shown past policy consists two decision
rules shown two sub tree policies introduced section
pruning joint policies guaranteed worse best fully specified joint policy
found far oliehoek spaan vlassis generalized making explicit
expand selection operators performed heuristic search resulting
generalized maa gmaa offers unified perspective maa forward sweep
policy computation method emery montemerlo differ implement
gmaa expand operator forward sweep policy computation solves e finds best
policy collaborative bayesian games maa finds policies collaborative
bayesian games describe section
gmaa considers joint policies partially specified respect
time partially specified policies formalized follows
definition decision rule agent decision stage mapping action ai
observation histories stage actions

article consider deterministic policies since policies need condition
actions observation histories made decision rules map length ai joint decision rule h nt specifies
observation histories actions


decision rule agent fig illustrates concept well past policy
introduce shortly discussed decision rules allow partial policies
defined play crucial role gmaa developed article
definition partial past policy stage ti specifies part agent policy
relates stages specifies decision rules first stages
ti past policy stage h regular fully specified policy
hi past joint policy specifies joint decision rules first
stages
gmaa performs heuristic search partial joint policies constructing
search tree illustrated fig node q ht vi search tree specifies
past joint policy heuristic value v heuristic value v node represents
optimistic estimate past joint policy vb computed via
vb v h h




fiincremental clustering expansion faster optimal dec pomdps

b





















b









b

b



maa perspective



b





b

b cbg perspective

figure generalized maa associated every node heuristic value search
trees two perspectives shown equivalent certain assumptions heuristic
explained section
h h heuristic value remaining h stages v actual
expected reward achieves first stages definition see appendix
clearly h h admissible heuristica guaranteed overestimationso vb
illustrates gmaa starts creating node q completely unspecified joint policy placing open list l selects nodes
expands repeating process certain found
optimal joint policy
select operator returns highest ranked node defined following comparison operator
definition node comparison operator defined two nodes q ht vi q
ht v follows



v v
v v
q q depth q depth q otherwise depth q depth q





otherwise
comparison operator first compares heuristic values equal
compares depth nodes finally nodes equal value equal depth
lexically compares past joint policies ranking leads behavior e selecting
node open list highest heuristic value gmaa well guaranteeing
selection order incremental expansion technique introduced section
ranking nodes greater depth higher case equal heuristic value helps tight
lower bounds early first expanding deeper nodes szer et al useful
incremental expansion
formally h underestimate value note unlike classical applications
path planningin admissible heuristic overestimatein setting maximize reward
rather minimize cost



fioliehoek spaan amato whiteson

generalized multiagent
input dec pomdp admissible heuristic h empty open list l
output optimal joint policy
vgm aa
q h v
l insert q
repeat

q select l

qexpand expand q h

depth q h

qexpand contains fully specified joint policies interested best one

h vi bestjointpolicyandvalue qexpand

v vgm aa


found best joint policy

vgm aa v

l prune vgm aa
optionally prune open list

end

else


add expanded children open list

l insert q qexpand q v vgm aa

end

postprocessnode q l
l empty
return

select l return highest ranked node open list
input open list l total order nodes
output highest ranked node q
q q l q l q q q q
return q

expand operator constructs qexpand set child nodes given node
contains partial joint policy constructs set
appending possible joint decision rules next time
step heuristic value computed node constructed
expansion checks line expansion resulted fully specified
joint policies children sufficient heuristic value placed open list
expand q h expand operator plain maa
input q ht vi search node expand h admissible heuristic
output qexpand set containing expanded child nodes
qexpand



vb v h

q ht vb

qexpand insert q
end
return qexpand



create child node

fiincremental clustering expansion faster optimal dec pomdps

postprocessnode q l
input q expanded parent node l open list
output expanded node removed
l pop q

line children fully specified bestjointpolicyandvalue returns best
joint policy value qexpand see appendix details
bestjointpolicyandvalue gmaa maintains lower bound vgm aa corresponds actual value best fully specified joint policy found far newly
found joint policy higher value lower bound updated lines
nodes partial joint policies upper bound lower best solution
far vb vgm aa pruned line pruning takes additional time
save memory finally postprocessnode simply removes parent node open
list procedure augmented incremental expansion section search ends
list becomes empty point optimal joint policy found
gmaa complete e search finds solution therefore theory
gmaa guaranteed eventually produce optimal joint policy szer et al
however practice often infeasible larger major source complexity
full expansion search node number joint decision rules stage
form children node depth search tree



n

doubly exponential comparing see worst case
complexity expanding node deepest level tree h comparable
brute force search entire dec pomdp consequently seuken zilberstein
conclude maa best solve whose horizon greater
already solved nave brute force search
bayesian game perspective
gmaa makes possible interpret maa solution collection collaborative
bayesian games cbgs employ throughout article facilitates
improvements gmaa introduce significant advances
state art dec pomdp solutions
bayesian game bg one shot interaction number agents
extension well known strategic game known normal form game
agent holds private information osborne rubinstein cbg bg
agents receive identical payoffs bayesian game perspective node q
gmaa search tree along corresponding partial joint policy defines
cbg oliehoek spaan vlassis given state distribution b
possible construct cbg b b represents decision making
stage given followed first stages starting b clear
b simply write b
follow convention root depth



fioliehoek spaan amato whiteson

definition collaborative bayesian game cbg b b hd pr ui modeling
stage dec pomdp given initial state distribution b past joint policy consists

set agents n
set joint actions
set joint types specifies type agent
h n
pr probability distribution joint types
u heuristic payoff function mapping joint type action real number u
bayesian game type agent represents private information holds
instance bayesian game modeling job recruitment scenario type agent
may indicate whether agent hard worker cbg dec pomdp agents
private information individual aoh therefore type agent corresponds
history actions observations similarly joint type corresponds
joint aoh
consequently u provide heuristic estimate long term payoff
pair words payoff function corresponds heuristic q value u
b discuss compute heuristics section given b
q
correspondence joint types aohs probability distribution joint types
pr pr b



latter probability marginal pr b defined used
computation value partial joint policy v appendix note due
correspondence types aohs size cbg b b stage
exponential
cbg agent uses bayesian game policy maps individual types actions
ai correspondence types aohs joint policy
cbg corresponds joint decision rule remainder article
assume deterministic past joint policies implies one non zero
probability given observation history thus effectively maps observation histories
actions number b b given value joint cbg
policy cbg b b
x
b
pr b q

vb



hi ii n denotes joint action application
individual cbg policies individual aoh specified
example consider cbg dec tiger given past joint policy specifies listen first two stages stage agent four possible observation histories
ohl ohl ohl ohr ohr ohl ohr ohr correspond directly possible types


probabilities joint types given listed fig since joint ohs together
determine joint aohs correspond called joint beliefs probability distributions
states introduced formally section fig b shows joint beliefs serve
basis heuristic payoff function discussed section


fiincremental clustering expansion faster optimal dec pomdps


ohl ohl
ohl ohr
ohr ohl
ohr ohr


ohl ohl





ohl ohr





ohr ohl





ohr ohr





joint type probabilities


ohl ohl
ohl ohr
ohr ohl
ohr ohr


ohl ohl





ohl ohr





ohr ohl





ohr ohr





b induced joint beliefs listed probability pr sl b
tiger behind left door

figure illustration dec tiger past joint policy specifies
listen actions first two stages

expand cbg q h expand operator gmaa makes use cbgs
input q ht vi search node expand
b
input h admissible heuristic form q
output qexpand set containing expanded child nodes
b
b b constructbg b q
qexpand generateallchildrenforcbg b b
return qexpand

explained section

solution cbg maximizes cbg equivalent team
decision process finding solution np complete tsitsiklis athans however
bayesian game perspective gmaa illustrated fig b issue solving
cbg e finding highest payoff relevant need expand
expand operator enumerates appends form set
extended joint policies


joint cbg policy b b
uses set construct qexpand set child nodes heuristic value
child node q qexpand specifies given
vb v vb



expand operator makes use cbgs summarized uses
generateallchildrenforcbg subroutine appendix fig b illustrates
bayesian game perspective gmaa


fioliehoek spaan amato whiteson

heuristics
perform heuristic search gmaa defines heuristic value vb contrast bayesian game perspective uses two formulations equivalent
b faithfully represents expected immediate reward oliehoek spaan vlasthe heuristic q
sis consequence gmaa via cbgs complete thus finds optimal
solutions stated following theorem
theorem heuristic form
b est r st e vb
q




vb q overestimation value optimal joint
policy gmaa via cbgs complete
proof see appendix
theorem q q value e expected future cumulative reward
performing joint policy oliehoek spaan p
vlassis expectation


immediate reward written r ss r pr b
computed pr b quantity refer joint belief resulting
denote b joint belief computed via repeated application
bayes rule kaelbling littman cassandra conditional
rest subsection reviews several heuristics used gmaa
qmdp
b solve underlying mdp e
one way obtain admissible heuristic q
assume joint action chosen single puppeteer agent observe true
state known qmdp littman cassandra kaelbling uses

mdp value function qt
computed standard dynamic programming

b
techniques puterman order transform qt
values qm values
compute
x
b

q
q pr b


ss

solving underlying mdp time complexity linear h makes
especially compared dec pomdp easy compute addition necessary
store value pair stage however bound provides
optimal dec pomdp q value function loose oliehoek vlassis
qpomdp
similar underlying mdp one define underlying pomdp dec pomdp
e assuming joint action chosen single agent access joint observation
alternatively one view pomdp multiagent pomdp agents instantaneously
broadcast private observations



fiincremental clustering expansion faster optimal dec pomdps

tree

vector





figure visual comparison tree vector q representations

resulting solution used heuristic called qpomdp szer et al roth
simmons veloso optimal qpomdp value function satisfies

qp bt r bt

x

p ot bt max qp bt


ot



p
bt joint belief r bt ss r bt immediate reward bt
joint belief resulting bt action joint observation ot use qpomdp
b p qt b
directly use value induced joint belief q
p

two approaches computing qpomdp one construct belief mdp
tree joint beliefs illustrated fig left starting b corresponding
empty joint aoh compute resulting corresponding

belief b continue recursively given tree possible compute values
nodes standard dynamic programming
another possibility apply vector pomdp techniques see fig right
q value function stage qtp b represented set vectors joint
kaelbling et al qt b defined maximum
action v v v
p
inner product
qtp b max b vat
v
va


given v h vector representation last stage compute v h etc order
limit growth number vectors dominated vectors pruned
since qmdp upper bound pomdp value function hauskrecht qpomdp
provides tighter upper bound q qmdp however costly compute
store tree vector may need store number
values exponential h


fioliehoek spaan amato whiteson

qbg
third heuristic called qbg assumes agent team access
individual observation communicate step delay define qbg
x
qb r max
pr ot qb ot



ot


h ot
n tuple individual policies oi ai cbg

constructed qpomdp qbg represented vectors varaiya
walrand hsu marcus oliehoek spaan vlassis two
manners computation tree vector apply yields tighter heuristic
qpomdp computation additional exponential dependence maximum
number individual observations oliehoek spaan vlassis particularly
troubling vector computation since precludes effective application incremental pruning cassandra littman zhang overcome oliehoek
spaan introduce novel tree pruning methods

clustering
gmaa solves dec pomdps repeatedly constructing cbgs expanding joint
bg policies however number equal number regular
maa child nodes given thus grows doubly exponentially horizon h
section propose improving scalability respect h
clustering individual aohs reduces number therefore number
constructed child nodes gmaa search tree
previous investigated clustering emery montemerlo gordon
schneider thrun propose clustering types profiles payoff
functions cbgs however resulting method ad hoc even given bounds
error clustering two types cbg guarantees made quality
dec pomdp solution bound respect heuristic payoff function
contrast propose cluster histories probability histories induce
histories agents states critical advantage criterion
call probabilistic equivalence pe resulting clustering lossless
solution clustered cbg used construct solution original cbg
values two cbgs identical thus criterion allows clustering aohs
cbgs represent dec pomdps preserving optimality
section describe histories dec pomdps clustered
notions probabilistic best response equivalence allows histories clustered
name qbg stems fact step delayed communication scenario modeled
cbg note however cbgs used compute qbg different form b b
discussed section latter types correspond length action observation histories
former types correspond length observation histories
cbgs essential clustering provide convenient level abstraction simplifies
exposition techniques moreover level abstraction makes possible employ
concerning cbgs outside context dec pomdps
probabilistic equivalence criterion lossless clustering introduced oliehoek et al
article presents simpler proof optimality clustering pe



fiincremental clustering expansion faster optimal dec pomdps

rational choose action section describe application
gmaa section introduces improved heuristic representations
allow computation longer horizons
lossless clustering dec pomdps
section discuss lossless clustering notion probabilistic equivalence
clustering lossless demonstrating probabilistic equivalence implies
best response equivalence describes conditions rational agent select
action two types prove implication best response
depends multiagent belief e probability distribution states policies
agents two probabilistically equivalent histories relations
equivalence notions discussed section
probabilistic equivalence criterion
first introduce probabilistic equivalence criterion used decide whether
two individual histories ia ib clustered without loss value
criterion probabilistic equivalence two aohs ia ib agent probabilistically
equivalent pe written p e ia ib following holds


pr
ia pr
ib



probabilities computed conditional pr b defined
subsections formally prove pe sufficient criterion guarantee
clustering lossless remainder section discuss key properties
pe criterion order build intuition
note criterion decomposed following two criteria



pr
ia pr
ib



pr
ia pr
ib



criteria give natural interpretation first says probability distribution
agents aohs must identical ia ib second demands
resulting joint beliefs identical
probabilities well defined without initial state distribution b
past joint policy however since consider clustering histories within particular cbg
stage constructed particular b implicitly specified therefore
drop arguments clarifying notation
example example types ohl ohr ohr ohl agent pe see note
rows columns second agent histories identical fig
fig b thus specify distribution histories agents cf equation
induced joint beliefs cf equation

probabilistic equivalence convenient property exploit holds
particular pair histories hold identical extensions
histories e propagates forwards regardless policies agents


fioliehoek spaan amato whiteson

definition identical extensions given two aohs ia ib respective extensions
ai oi b b called identical extensions






ai ai oi oi







lemma propagation pe given ia ib pe regardless decision rule
agents use identical extensions pe
ati ot st








b
ai oi
pr st
ia ati ot
pr

proof proof listed appendix holds intuitively probabilities
described taking action
seeing observation
note probabilities defined superficially resemble beliefs used
pomdps substantially different pomdp single agent compute
individual belief aoh use belief determine value
future policy sufficient statistic history predict future rewards
kaelbling et al bertsekas thus trivial equivalence aohs
induce individual belief pomdp unfortunately dec pomdps
problematic next section elaborates issue discussing relation multiagent
beliefs
sub tree policies multiagent beliefs expected future value
describe relationship multiagent beliefs probabilistic equivalence
must first discuss policies agent may follow resulting values begin
introducing concept sub tree policies illustrated fig page
deterministic policy represented tree nodes labeled actions
edges labeled observations root node corresponds first action taken
nodes specify action observation history encoded path root node
possible define sub tree policies correspond sub trees agent
policy illustrated fig particular write
w
ht



sub tree policy corresponding w
observation history oit specifies actions
last h stages refer policy consumption operator since
w
consumes part policy corresponding oit similarly write k l kl

note h steps go sub tree policy use similar notation
k joint sub tree policies extensive treatment different forms
policy refer discussion oliehoek
given concepts define value k stages go joint policy starting
state
xx
w
pr v k

v k r




joint action specified roots individual sub tree policies specified
k stage h k


fiincremental clustering expansion faster optimal dec pomdps

definition follows directly probability distribution states
sub tree policies agents sufficient predict value sub tree policy
fact distribution known multiagent belief bi hansen bernstein
zilberstein value given
xx
v bi max
bi v hi







refer maximizing agent best response bi illustrates
multiagent belief sufficient statistic contains sufficient information predict value
sub tree policy
possible connect action observation histories multiagent beliefs fixing
policies agents given agents act according profile
policies agent multiagent belief first stage dec pomdp bi
b moreover agent maintain multiagent belief execution
given history induces multiagent belief write bi
make dependence explicit multiagent belief history defined
bi pr b



induces best response via
br arg max


xx


bi v





conclude two aohs ia ib clustered together induce
multiagent belief
however notion multiagent belief clearly quite different distributions
used notion pe particular establish whether two aohs induce
multiagent belief need full specification nevertheless two aohs
pe best response equivalent therefore cluster
crux criterion satisfied aohs induce
multiagent beliefs consistent current past joint policy
best response equivalence allows lossless clustering histories
relate probabilistic equivalence multiagent belief follows
lemma pe implies multiagent belief equivalence probabilistic equivalence
implies multiagent belief equivalence


p e ia ib bi ia bi ib

proof see appendix
lemma shows two aohs pe produce multiagent belief
intuitively gives us justification cluster aohs together since multiagent
belief sufficient statistic act multiagent belief
since lemma shows ia ib induces multiagent beliefs
pe conclude act histories formally
prove ia ib best response equivalent pe


fioliehoek spaan amato whiteson

theorem pe implies best response equivalence probabilistic equivalence implies bestresponse equivalence


p e ia ib br ia br ib
proof assume arbitrary
br ia arg max

xx

bi ia v

arg max

xx

bi ib v br ib













lemma employed assert equality bi ia bi ib
theorem key demonstrates two aohs ia ib agent
pe agent need discriminate future thus
searching space joint policies restrict search assign
sub tree policy ia ib directly provides intuition lossless
clustering possible formally define clustered joint policy space follows
definition clustered joint policy space let c subset joint policies
clustered e part c assigns sub tree policy action
observation histories probabilistically equivalent
corollary existence optimal clustered joint policy exists optimal joint
policy clustered joint policy space
max v max v

c





proof clear left hand side upper bounded right hand side
since c suppose arg max v strictly higher value
best clustered joint policy least one agent one pair pe histories ia ib must
assign different sub tree policies ia ib otherwise would clustered without loss
generality assume one pair follows directly theorem
policy construct clustered policy c c assigning ia ib
ia ib guaranteed value less thereby contradicting
assumption strictly higher value best clustered joint policy
formally proves restrict search c space clustered joint
policies without sacrificing optimality
clustering commitment cbgs
though clear two aohs pe clustered making
operational requires additional step end use abstraction layer provided
bayesian games recall cbg stage aohs correspond types


fiincremental clustering expansion faster optimal dec pomdps

therefore want cluster types cbg accomplish clustering two
types ia ib introduce type ic replace defining
pr ic pr ia pr ib
j

u hic




pr ia u hia pr ib u ib


pr ia pr ib




theorem reduction commitment given agent collaborative bayesian
game b committed selecting policy assigns action two types
ia ib e selecting policy ia ib cbg reduced without
loss value agents cbg b agent employs
policy reflects clustering whose expected payoff original

cbg v b v b
proof see appendix
theorem shows given agent committed taking action
types ia ib reduce collaborative bayesian game b smaller one b

translate joint cbg policy found
b back joint cbg policy b
necessarily mean solution b best response
agent may select action ia ib rather best response
given action needs taken ia ib
even though theorem gives conditional statement depends agent
committed select action two types previous subsection discussed
rational agent make commitment combining gives
following corollary
corollary lossless clustering pe probabilistically equivalent histories ia ib
clustered without loss heuristic value merging single type cbg
proof theorem shows given agent committed take action
two types types clustered without loss value since ia ib pe
best response equivalent means agent committed use
sub tree policy hence action ai therefore directly apply clustering
without loss expected payoff cbg stage dec pomdp means loss
expected heuristic value given
intuitively maximizing action ia ib regardless future
joint policies agents use hence cluster without loss
heuristic value note depend heuristic used hence
holds optimal heuristic e optimal q value function gives
true value directly relates probabilistic equivalence equivalence optimal value
although focus cbgs generalize bgs individual payoff functions thus
could potentially exploited general payoff bgs developing methods
interesting avenue future work
proof originally provided oliehoek et al showing histories pe
induce identical q values



fioliehoek spaan amato whiteson

clustercbg b
input cbg b
output losslessly clustered cbg b
agent

individual type b

pr

b b

continue

end

individual type b

isprobabilisticallyequivalent true

hs

pr pr

isprobabilisticallyequivalent false

break

end

end

isprobabilisticallyequivalent

b b





u min u u

pr pr pr

pr

end

end

end

end

end
end
return b

prune b

prune b
take lowest upper bound

note establishes sufficient necessary condition lossless clustering
particular given policies agents many types best response equivalent
clustered however far know criterion must hold order guarantee
two histories best response policy agents
gmaa incremental clustering
knowing individual histories clustered together without loss value
potential speed many dec pomdp methods article focus application
within gmaa framework
emery montemerlo et al showed clustering incorporated every stage
cbg stage constructed clustering individual
histories types performed first afterwards reduced cbg solved
employed within gmaa modifying expand procedure
cluster cbg calling generateallchildrenforcbg
shows clustering takes input cbg returns
clustered cbg performs clustering performing pairwise comparison types


fiincremental clustering expansion faster optimal dec pomdps

b
constructextendedbg b q

input cbg b stage joint bg policy followed
b
input admissible heuristic form q

output cbg b stage
b b
make copy b subsequently alter
agent

b constructextendedtypeset
overwrite individual type sets
end
b id
joint type set explicitly stored
joint type ot b

state st

compute pr st
pr st via bayes rule

end

pr pr ot pr



q

history represented
b
b take lowest upper bound

q min q q
q q

end

b u q

end
end
return b

agent see satisfy criterion yielding comparisons agent
comparison involves looping hs line many states efficiency
could gained first checking checking rather taking
average line take lowest payoff done
upper bound heuristic values
following theorem demonstrates incorporating clustering gmaa
resulting still guaranteed optimal solution
theorem heuristic form clustering cbgs gmaa
pe criterion resulting search method complete
proof applying clustering alter computation lower bound values
heuristic values computed expanded nodes admissible fact unaltered
guaranteed corollary therefore difference regular gmaa
class considered joint policies restricted c class clustered joint policies
possible child nodes expanded clustering effectively prunes away policies
would specify different actions aohs pe thus clustered however corollary
guarantees exists optimal joint policy restricted class
modification expand proposed rather naive construct b b
must first construct oi possible aohs agent given past policy ti
subsequent clustering involves pairwise comparison exponentially many types
clearly tractable later stages
however pe aohs propagates forwards e identical extensions pe histories pe efficient possible instead clustering exponentially


fioliehoek spaan amato whiteson

expand ic q h expand operator gmaa ic
input q ht vi search node expand
b
input h admissible heuristic form q
output qexpand set containing expanded child nodes
b cbg
retrieve previous cbg note
b


b constructextendedbg b
q
b clusterbg b
cbg b
store pointer cbg
qexpand generateallchildrenforcbg b
return qexpand

growing set types simply extend already clustered types previous stages
cbg shown given set types agent previous
stage policy agent took stage set types stage
constructed


oti oti oi

means size newly constructed set oi type set
previous stage much smaller set histories oi
type set much smaller oi way bootstrap clustering
stage spend significantly less time clustering refer
implements type clustering gmaa incremental clustering gmaa ic
possible perform exact value preserving clustering
lemma guarantees identical extensions clustered without loss
value performing procedure lossy clustering scheme e g emerymontemerlo et al errors might accumulate better option might cluster
scratch every stage
expansion gmaa ic node takes exponential time respect number
agents types n joint cbg policies thus child nodes
gmaa ic search tree largest action set largest type set clustering
involves pairwise comparison types agent comparisons needs
check n numbers equality verify total cost clustering
therefore written
n n
polynomial number types clustering decreases number
types therefore significantly reduce number child nodes thereby
overall time needed however clustering possible overhead incurred
improved heuristic representation
since clustering reduce number types gmaa ic potential scale
larger horizons however important consequences computation
heuristics previous shown upper bound provided qmdp often
loose effective heuristic search oliehoek spaan vlassis however
space needed store tighter heuristics qpomdp qbg grows exponentially
horizon recall section see fig two approaches computing


fiincremental clustering expansion faster optimal dec pomdps

b minimum size
compute hybrid q















qh r r
z
h


z
v vectorbackup qt
v prune v
qt v
z v
end
z
qt treebackup qt
end
end

vector representation last stage
size vectors
size aoh representation

z

qpomdp qbg first constructs tree joint aohs heuristic values
simple implement requires storing value pair number
grows exponentially second maintains vector representation
common pomdps though pruning provide leverage worst case pruning
possible number maintained vectors grows doubly exponentially h
number stages go similarly initial belief subsequently reachable beliefs
used reduce number vectors retained stage number reachable
beliefs exponential horizon exponential complexity remains
oliehoek spaan vlassis used tree representation qpomdp qbg heuristics since
computational cost solving dec pomdp bottleneck inefficiencies representation could overlooked however longer feasible
longer horizons made possible gmaa ic

hybrid



mitigate propose hybrid represent
tation heuristics illustrated fig main
insight exponential growth two existing representations occurs opposite directions therefore

use low space complexity side representations
later stages fewer vectors use vector representation earlier stages fewer histot
ries use history representation similar
idea utilizing reachable beliefs reduce size figure illustration
vector representation described rather stor hybrid representation
ing vectors appropriate aohs step
values needed tree representation
shows mild assumptions minimally sized representation
computed starting last stage performs vector backups switching
tree backups become smaller option last time step h represent


fioliehoek spaan amato whiteson

qt set immediate reward vectors variable z initialized line keeps track
number parameters needed represent qt vectors time step hand
note z depends effective vector pruning e large parsimonious
representation piecewise linear convex value function since
dependent z updated pruning actually performed line
contrast number parameters tree representation computed directly
dec pomdp line z switches tree backups

incremental expansion
clustering technique presented previous section potential significantly
speed much clustering possible however little clustering possible
number children gmaa search tree still grow super exponentially section
presents incremental expansion complementary technique deal
incremental expansion exploits recent improvements effectively solving cbgs first
note expansion last stage h particular h
interested best child h h corresponds optimal solution
bayesian game h last stage use methods solving
cbgs kumar zilberstein b oliehoek spaan dibangoye amato
provide speedups multiple orders magnitude brute force search enumeration
unfortunately improvements gmaa afforded limited order
guarantee optimality still relies expansion child nodes corresponding
joint cbg policies intermediate stages thus necessitating brute force
however many expanded child nodes may low heuristic values vb may therefore
never selected expansion
incremental expansion overcomes exploits following key observation generate children decreasing heuristic order admissible
heuristic expand children search performed
partially specified policies cbg constructed extending cbg
parent node however rather fully expanding e enumerating cbg policies
thereby constructing children search node instantiate incremental
cbg solver corresponding cbg incremental solver returns one joint cbg
policy time used construct single child revisiting
nodes promising child nodes expanded incrementally
describe gmaa ice combines gmaa ic incremental expansion establish theoretical guarantees describe modifications
bagabab cbg solver gmaa ice employs necessary deliver
child nodes decreasing order
exceptional cases short horizon combined large state action spaces representing last time step vectors minimal cases trivially adapted
assumes vector representation shrink earlier stages although unlikely
practice cases would prevent computing minimal representation
kumar zilberstein b tackle slightly different introduce weighted constraint satisfaction solving point backup dynamic programming dec pomdps however
point backup interpreted collection cbgs oliehoek et al



fiincremental clustering expansion faster optimal dec pomdps

gmaa incremental clustering expansion
begin formalizing incremental expansion incorporating gmaa ic yielding gmaa incremental clustering expansion gmaa ice core
incremental expansion lies following lemma
lemma given two joint cbg policies cbg b b vb vb
corresponding child nodes vb vb
proof holds directly definition vb given
vb v vb
v vb vb

follows directly b b use cbg solver generate sequence
policies
vb vb

sequence corresponding children

vb vb

exploiting knowledge expand first child compute heuristic
value vb since unexpanded siblings heuristic values less
equal modify gmaa ic reinsert node q open list l
act placeholder non expanded children
definition placeholder node least one child expanded
placeholder heuristic value equal last expanded child
thus expansion search node qs child update q v heuristic value
node vb value expanded child e set q v vb
reinsert q l placeholder mentioned correct
unexpanded siblings parent node q placeholder heuristic values
lower equal vb therefore next sibling q represented placeholder
expanded time q created nodes lower heuristic value
selected expansion keep track whether node previously expanded
placeholder
gmaa ice performs search partially specified policies
gmaa ic cbg constructed extending cbg parent node
applying lossless clustering however rather expanding children gmaa ice
requests next solution incremental cbg solver single child
constructed principle gmaa ice use cbg solver able
incrementally deliver descending order vb propose modification
bagabab oliehoek et al briefly discussed section
fig illustrates process incremental expansion gmaa ice indexed
letters first cbg solver root node ha created optimal solution
computed value child hb root replaced placeholder
node ha per definition node comparison operator b appears


fioliehoek spaan amato whiteson

legend

v









root node







b




b



b vb
c



ht vi
open list

ha




b b vb

ha
hc
hb

hb
ha

b


c




next solution
b vb

hd
ha
hc
hb

figure illustration incremental expansion nodes open list bottom
past joint policies indexed letters placeholder nodes indicated dashes
open list hence selected expansion best child hc added hb replaced
placeholder hb search returns root node second best solution
obtained cbg solver leading child hd placeholder nodes retained
long unexpanded children values updated
gmaa ice derive lower upper bounds cbg solution
exploited incremental cbg solver incremental cbg solver
b initialized lower bound
vcbg vgm aa v



vgm aa value current best solution v true expected
value first stages therefore vcbg minimum value candidate
must generate remaining h stages order beat current best solution note
time incremental cbg solver queried solution vcbg evaluated
vgm aa may changed
used heuristic faitfully represents immediate reward e form
last stage h specify upper bound solution
cbg
vcbg vb h v h h

upper bound attained solutions required cbg solver
upper bound holds since
vb vb h v h h

v h v h h
vb h v h h

first step vb h v h h fully specified policy heuristic value
given equals actual value heuristic faithfully represents expected


fiincremental clustering expansion faster optimal dec pomdps

expand ice q h expand operator gmaa ice
input q ht vi search node expand
b
input h admissible heuristic form q
output qexpand set containing expanded child nodes
isplaceholder q

b cbg
reuse stored cbg
else

b cbg
retrieve previous cbg note

b

b constructextendedbg b q



b clusterbg b

b solver createsolver b

cbg b
store pointer cbg
end
set lower bound cbg solution
vcbg vgm aa v
h

vcbg vb h v h h
upper bound used last stage cbg
else

vcbg
end
b b solver nextsolution vcbg vcbg
compute next cbg solution
h v




create partial joint policy




b
b

v v
v
compute heuristic value

q ht vb
create child node

qexpand q
else

qexpand
fully expanded exists solution v h vcbg
end
return qexpand

postprocessnode ice q l post processing node gmaa ice
input q last expanded node l open list
output q removed updated
l pop q
q fully expanded depth q h

cleanup q
delete node associated cbg solver

return
else

c last expanded child q

q v c v
update heuristic value parent node

isplaceholder q true
remember q placeholder

l insert q
reinsert appropriate position
end



fioliehoek spaan amato whiteson

immediate reward used implies vb lower bound second step
v h vb h vb h admissible therefore stop expanding
lower bound heuristic value equal upper bound vcbg applies
last stage first step valid
gmaa ice implemented replacing expand postprocessnode
procedures respectively expand ice first
determines placeholder used reuses previously constructed incremental cbg solver constructs one bounds calculated next
cbg solution obtained subsequently single child node generated rather
expanding children postprocessnode ice removes last node
returned select children expanded otherwise
updates nodes heuristic value reinserts open list see appendix
gmaa ice shown single
theoretical guarantees
section prove gmaa ic gmaa ice search equivalent direct
establish gmaa ice complete means integrating incremental
expansion preserves optimality guarantees gmaa ic
definition call two gmaa variants search equivalent select exactly
sequence non placeholder nodes corresponding past joint policies expand
search tree select operator
gmaa ic gmaa ice set selected nodes
however set expanded nodes different fact precisely differences
incremental expansion exploits
theorem gmaa ice gmaa ic search equivalent
proof proof listed section appendix
note theorem imply computational space requirements
gmaa ice gmaa ic identical contrary expansion
gmaa ice generates one child node stored open list contrast
gmaa ic generates number child nodes worst case doubly exponential
depth selected node however gmaa ice guaranteed
efficient gmaa ic example case child nodes still
generated gmaa ice slower due overhead incurs
corollary heuristic form gmaa ice complete
proof stated conditions gmaa ic complete see theorem
gmaa ice search equivalent gmaa ic complete

since

allows clustering number child nodes grows less dramatically see section



fiincremental clustering expansion faster optimal dec pomdps

incremental cbg solvers
implementing gmaa ice requires cbg solver incrementally deliver
descending order vb end propose modify bayesian game branch
bound bagabab oliehoek et al bagabab performs search
partially specified cbg policies thus applied within gmaa ice performs
second nested search expand node gmaa search tree nested
search computes next cbg solution section briefly summarizes main ideas
behind bagabab information see oliehoek et al modifications
bagabab works creating search tree nodes correspond partially
specified joint cbg policies particular represents joint action vector vector
h joint actions specifies joint type node g
bagabab search tree represents partially specified vector thus partially specified
joint cbg policy example completely unspecified vector h corresponds
root node internal node

g depth root beingff depth specifies joint
actions first joint types g value node v g
value best joint cbg policy consistent since value known
advance bagabab performs search guided optimistic heuristic
particular compute upper bound value achievable
partially specified vector computing maximum value complete information joint
policy consistent e non admissible joint policy selects maximizing
joint actions remaining joint types since value guaranteed upper bound
maximum value achievable consistent joint cbg policy admissible heuristic
propose modification bagabab allow solutions incrementally delivered
main idea retain search tree first call bagabab particular cbg
b update subsequent calls thereby saving computational effort
standard search terminates single optimal solution found
behavior incremental bagabab called first time b
however standard nodes whose upper bound lower best known lower
bound safely deleted never lead optimal solution contrast
incremental setting nodes cannot pruned could possibly k th
best solution therefore might need expanded subsequent calls bagabab
nodes returned solutions pruned order avoid returning solution
twice modification requires memory affect search process
otherwise
asked k th solution bagabab resets internal lower bound value
next best solution previously found returned vcbg defined
solution found starts search initialized search
tree resulting k th solution essence method similar searching
best k solutions k incremented demand recently shown
fixed k modification preserves theoretical guarantees soundness completeness
gmaa ice could use incremental cgb solver avoid enumerating
providing first thus potential work incrementally exception may
method kumar zilberstein b employs branch bound search
edac heuristic thus limited two agent case heuristic search method may
amenable incremental implementation though knowledge attempted



fioliehoek spaan amato whiteson

optimal efficiency dechter flerova marinescu
trivially transfer setting k allowed increase

experiments
section empirically test validate proposed techniques lossless clustering
joint histories incremental expansion search nodes hybrid heuristic representations
introducing experimental setup compare performance gmaa ic
gmaa ice gmaa suite benchmark literature
next compare performance proposed methods state art optimal
approximate dec pomdp methods followed case study scaling behavior
respect number agents finally compare memory requirements
hybrid heuristic representation tree vector representations
experimental setup
well known dec pomdp benchmarks dec tiger nair et al
broadcastchannel hansen et al dec tiger discussed extensively
section broadcastchannel two agents transmit messages communication channel agents transmit time collision occurs
noisily observed agents firefighting team n firefighters
extinguish fires row nh houses oliehoek spaan vlassis
agent choose move houses fight fires location two agents
house completely extinguish fire negative reward
team firefighters depends intensity fire house fires
extinguished reward zero received hotel spaan melo
travel agents need assign customers hotels limited capacity send
customer resort yields lower reward addition use following recycling robots amato bernstein zilberstein scaled version
described section gridsmall two observations amato bernstein
zilberstein cooperative box pushing seuken zilberstein larger
two robot benchmark table summarizes numerically listing number
joint policies different horizons
experiments run intel core cpu running linux gmaa gmaa ic
gmaa ice implemented code base madp toolbox c
spaan oliehoek vector qbg representation computed variation incremental pruning adapted computing q functions instead regular value functions corresponding naiveip method described oliehoek spaan
implement pruning employ cassandras pomdp solve software r cassandra

sections limited process gb ram
maximum cpu time reported cpu times averaged independent runs
resolution timings given maa search processes since


fiincremental clustering expansion faster optimal dec pomdps

primitives

dec tiger

num h

n



ai

oi















e

e

e

broadcastchannel









e

e

e

gridsmall









e

e

e

cooperative box pushing









e

e

e

recycling robots









e

e

e

hotel









e

e

e

firefighting









e

e

e

table benchmark sizes number joint policies different horizons
computation heuristic methods amortized multiple
runs definitions available via http masplan org
comparing gmaa gmaa ic gmaa ice
compared gmaa gmaa ic gmaa ice hybrid qbg representation methods compute optimal policy expect gmaa ic efficient
gmaa lossless clustering possible furthermore expect gmaa ice
provide improvements terms speedup scaling longer horizons
shown table entries report qbg heuristics
could computed thanks hybrid representation consequently performance
gmaa ic much better previously reported including oliehoek
et al often required resort qmdp larger horizons
entries marked limits qmdp instead qbg
reach longer horizons qbg firefighting gmaa ice
qmdp compute solutions higher h possible qbg hence missing
showing gmaa ice efficient loose heuristic gmaa ic
furthermore entries indicate horizon solve
tree qbg representation often much shorter
clearly illustrate gmaa ic leads significant improvement
performance gmaa ic able produce solution quickly
increase largest solvable horizon gmaa cases gmaa ic able
drastically increase solvable horizon
furthermore clearly demonstrate incremental expansion allows significant additional improvements fact table demonstrates gmaa ice significantly outperforms gmaa ic especially little clustering possible
table illustrate efficacy hybrid representation
gridsmall cooperative box pushing firefighting hotel neither
tree vector representation able provide compact qbg heuristic longer hori heuristics computation time ranges less second hours high h difficult
table presents heuristic computation time



fioliehoek spaan amato whiteson

h







v tgmaa
dec tiger










tic

tice

h






























firefighting hnh nf





















gridsmall

























hotel




























cooperative box pushing






































v tgmaa tic tice
recycling robots










































broadcastchannel




























































table experimental comparing regular gmaa gmaa ic gmaa ice
listed computation times gmaa tgmaa gmaa ic tic
gmaa ice tice hybrid qbg representation use following symbols
memory limit violations time limit overruns heuristic computation exceeded memory time limits maximum horizon qmdp maximum horizon
tree qbg bold entries indicate methods proposed article
computed
zons apart dec tiger firefighting computing storing qbg another
tight heuristic longer horizons bottleneck scalability
together algorithmic improvements lead first optimal solutions many
horizons fact vast majority tested provide
longer horizons previous work bold entries improvements quite sub

fiincremental clustering expansion faster optimal dec pomdps

h























bgh

cbgt
dec tiger




firefighting hnh nf





gridsmall



hotel




e
e
e
e
cooperative box pushing



h



























bgh cbgt
recycling robots
remaining stages
remaining stages
remaining stages
remaining stages
remaining stages
e remaining stages
e remaining stages
e remaining stages
e remaining stages
remaining stages
remaining stages
remaining stages
broadcastchannel







e
e
e


















table experimental detailing effectiveness clustering listed size
cbgs h without clustering bgh average cbg size stages
clustering cbgt
stantial especially given lengthening horizon one increases difficulty
exponentially cf table
analysis clustering histories
table provides additional details performance gmaa ic listing
number joint types gmaa ic search cbgt stage averages
since forms cbgs different past policies leading clusterings different
sizes see impact clustering table lists bgh number joint types
cbgs constructed last stage without clustering constant
dec tiger time needed gmaa ic orders magnitude less
gmaa horizon h h test e joint
policies method able optimally solve gmaa ic however
able reasonable time dec tiger clear symmetries
note domains report smaller clusterings oliehoek et al due
implementation mistake clustering overly conservative cases treat two histories
probabilistically equivalent fact



fioliehoek spaan amato whiteson

observations allow clustering demonstrated fig another key property
opening door resets may facilitate clustering
firefighting short horizons lossless clustering possible
stage clustering incurs overhead however gmaa ic still faster
gmaa constructing bgs bootstrapping previous cbg
takes less time constructing cbg scratch interesting counterintuitive
occur h solved within memory limits contrast h fact
qmdp could compute optimal values v h turns equal
h reason optimal joint policy guaranteed extinguish
fires stages subsequent stages rewards
influence clustering analysis table reveals cbg instances encountered
h search happen cluster much better h possible
heuristics vary horizon fact h sends agents
middle house h agents dispatched different houses
agents fight fires house fire extinguished completely resulting joint
observations provide information different joint types lead
joint belief means clustered agents visit different houses
observations convey information leading different possible joint beliefs cannot
clustered
hotel allows large amount clustering gmaa ic outperforms gmaa
large margin former reaching h latter h
transition observation independent becker zilberstein lesser goldman
nair varakantham tambe yokoo varakantham marecki yabu tambe yokoo
facilitates clustering discuss section unlike methods
specifically designed exploit transition observation independence gmaa ic exploits
structure without requiring predefined explicit representation scalability
limited computation heuristic
broadcastchannel gmaa ic achieves even dramatic increase performance allowing solution horizon h analysis reveals cbgs
constructed stages fully clustered contain one type agent
reason follows constructing cbg one joint type
previous cbg given solution previous cbg uncertainty
respect previous joint action crucial property broadcastchannel
joint observation reveals nothing state joint action
taken e g collision agents chose send different individual
histories clustered cbg constructed stage one joint
type previous game therefore given past policy actions agents
perfectly predicted observation conveys information process repeats thus special property could described non observable
given past joint policy gmaa ic automatically exploits property consequently
time needed solve cbg grow horizon solution time however still increases super linearly increased amount backtracking
firefighting performance monotonic horizon case however
clustering clearly responsible difference rather explanation
certain horizons many near optimal joint policies leading backtracking
higher search cost


fiincremental clustering expansion faster optimal dec pomdps



nodes depth



dectiger h full exp
dectiger h inc exp
gridsmall h full exp
gridsmall h inc exp
firefighting h full exp
firefighting h inc exp




















figure number expanded partial joint policies intermediate stages h
log scale

analysis incremental expansion
dec tiger h gmaa ice achieves speedup three orders magnitude
compute solution h unlike gmaa ic gridsmall achieves large
speedup h fast solutions h gmaa ic runs memory similar positive obtained firefighting cooperative box pushing
recycling robots fact qmdp gmaa ice able compute
solutions well beyond h firefighting stands stark contrast gmaa ic computes solutions h heuristic note
broadcastchannel gmaa ic slightly faster
gmaa ice exhibits clustering single joint type overhead
incremental expansion pay
analyze incremental expansion examined impact number nodes
expanded intermediate stages h fig shows number nodes expanded
gmaa ice number would expanded gmaa ic
easily computed since search tree equivalent clear relationship
fig table illustrating e g gmaa ic runs memory
gridsmall h plots confirm hypothesis practice small number
child nodes queried
analysis hybrid heuristic representation
fig illustrates memory requirements terms number parameters e real numbers tree vector hybrid representations qbg latter computed
following vector representation omitted representations grew beyond limits effectiveness vector pruning depends
complexity value function increase suddenly instance happens fig c several benchmark dec pomdps hybrid
representation allows significant savings memory allowing computation tight
heuristics longer horizons


fioliehoek spaan amato whiteson

h

milp

dp lpc

dp ipg

gmaa qbg
ic

ice

heur

broadcastchannel ice solvable h





























dec tiger ice


















solvable h













firefighting agents houses firelevels ice













gridsmall ice solvable h















recycling robots ice solvable h


















hotel








ice solvable h














cooperative box pushing















solvable h




































qpomdp ice solvable h










table comparison runtimes methods total time gmaa methods
given taking time method column ic ice adding heuristic
computation time heur use following symbols memory limit violations
time limit overruns heuristic computation exceeded memory time limits



fiincremental clustering expansion faster optimal dec pomdps



memory required





















horizon







dec tiger













horizon







recycling robots






horizon





tree
vector
hybrid












horizon

tree
vector
hybrid

c hotel



tree
vector
hybrid

memory required

memory required











b firefighting







tree
vector
hybrid




memory required

memory required



tree
vector
hybrid

memory required





tree
vector
hybrid












horizon

e broadcastchannel









horizon





f gridsmall

figure hybrid heuristic representation axis shows number real numbers stored
different representations qbg several benchmark log scale
comparing methods
section compare gmaa ic gmaa ice methods literature begin comparing runtimes methods following state ofthe art optimal dec pomdp methods milp aras dutech converts decpomdp mixed integer linear program numerous solvers available
used mosek version dp lpc boularias chaib draa performs dynamic programming lossless policy compression cplex lp solver
dp ipg amato et al performs exact dynamic programing incremental policy
reported deviate reported aras dutech number
aras et al employed solution method solves milp series tree smaller
milps branching continuous realization weight variables earlier stages past
joint policy stage solve different milp involving subset consistent sequences
additionally firefighting gridsmall use benchmark versions standard literature
oliehoek spaan vlassis amato et al whereas aras dutech use non standard
versions explains difference ones reported article personal
communication raghav aras
goal boularias chaib draa non dominated joint policies initial beliefs
previously reported concerned run time compute non dominated joint policies without
performing pruning full length joint policies contrast report time needed compute
actual optimal dec pomdp policy given b additionally requires final round pruning
subsequently computing value remaining joint policies initial belief additional overhead explains differences run time report previously
reported personal communication abdeslam boularias



fioliehoek spaan amato whiteson


dec tiger
cooperative box pushing
gridsmall

h









vmbdp




v




table comparison optimal v approximate vmbdp values

generation exploits known start state knowledge states reachable
dp backup
table shows comparison demonstrates almost cases
total time gmaa ice given sum heuristic computation time time
gmaa phase significantly less state art methods
moreover demonstrated table gmaa ice compute solutions longer horizons except cooperative box pushing hotel
possible compute qbg longer horizons overcoming
could enable gmaa ice scale horizons well
dp lpc proposed boularias chaib draa improves
efficiency optimal solutions form compression performance
however weaker gmaa ic two main explanations performance difference first dp lpc uses compression compactly represent values
sets useful sub tree policies sequence form representation policies however compressed still specify actions every possible observation
history policy needs select exponential amount sequences make
policy hence cannot compute solutions long horizons second gmaa ic
exploit knowledge initial state distribution b
overall gmaa ice substantially improves state art optimally solving
dec pomdps previous methods typically improved feasible solution horizon
one provided speed ups horizons could already solved contrast
gmaa ice dramatically extends feasible solution horizon many
consider mbdp approaches leading family approximate
table reports vmbdp values produced pbip ipg amato et al
typical maxtrees parameter setting demonstrates optimal solutions produced
gmaa ic gmaa ice higher quality pbip ipg chosen
mbdp parameters achieve value
exhaustive comparison illustrates even best approximate dec pomdp methods
practice provide inferior joint policies conducting analysis
possible optimal solutions computed clearly data becomes
available thorough comparisons made therefore scalable optimal
solution methods gmaa ice critical improving analyses


fiincremental clustering expansion faster optimal dec pomdps

primitives

num h

n























e

e











e

e









e

e

e









e

e

e









e

e

e

table firefightinggraph number joint policies different numbers agents
horizons possible fire levels
scaling agents
benchmark presented far limited two agents
present case study firefightinggraph oliehoek spaan whiteson vlassis
variation firefighting allowing agents agent
fight fires two houses instead table highlights size
including total number joint policies different horizons compared
gmaa gmaa ic gmaa ice qmdp heuristic bruteforcesearch
dp ipg maximum run time hours running intel core cpu
averaged runs bruteforcesearch simple optimal enumerates
evaluates joint policies implemented codebase gmaa
variations dp ipg use original implementation run intel xeon
computer hence timing directly comparable overall trends
apparent since dp ipg implementation limited agents shown
agents
fig shows computation times firefightinggraph across different numbers
agents horizons table lists optimal values obtained expected
baseline bruteforcesearch performs poorly scaling beyond h
agents dp ipg reach h hand regular gmaa performs
relatively well scaling maximum agents however gmaa ic gmaa ice
improve efficiency gmaa orders magnitude substantially
outperform three methods scale agents benefit incremental expansion clear n gmaa ice reach higher horizon gmaa ic
hence although article focuses scalability horizon
methods propose improve scalability number agents
discussion
overall empirical demonstrate incremental clustering expansion offers
dramatic performance gains diverse set addition broad hotel dp ipg performs particularly well structure limited reachability
agent fully observe local state agent local states
except one one action dominates others dp ipg generate small number
possibly optimal policies



fioliehoek spaan amato whiteson



































































computation time

computation time

computation time







































h

agents





















































h

agents

gmaa





















h

agents

b gmaa ic



c gmaa ice































































computation time



computation time
































h

agents



























h

agents

bruteforcesearch

e dp ipg

figure comparison gmaa gmaa ic gmaa ice bruteforcesearch
dp ipg firefightinggraph shown computation time log
scale number agents horizons missing bars indicate method
exceeded time memory limits however dp ipg implementation supports
agents

h






n






n




n




n


n


table value v optimal solutions firefightinggraph different
horizons numbers agents

castchannel illustrate key advantage possesses property makes large amount clustering possible clustering method exploits
property automatically without requiring predefined explicit representation


fiincremental clustering expansion faster optimal dec pomdps

course admit great reductions via clustering one domain property
allows clustering past joint policy encountered gmaa makes
observations superfluous broadcastchannel firefighting dec tiger
see certain symmetries lead clustering however clustering occur even
without properties fact nearly horizons tested
size cbgs reduced moreover accordance analysis section
improvements efficiency huge even modest reductions cbg size
one class say something priori amount clustering
possible class dec pomdps transition observation independence
becker et al agents local states transitions
independent two agents expressed
pr pr pr



similarly observations assumed independent means agent
observation probability depends action local state pr oi ai si
probabilistic equivalence criterion factors particular due
transition observation independence holds true ia ib moreover
factors product pr pr pr thus holds pr
pr b two histories clustered induce local belief
size cbgs directly corresponds product number reachable local
beliefs since transition observation independent hotel locally
fully observable local state spaces consist four states four possible
local beliefs consistent cbg size table moreover see
maximum size typically reached end search good
policies defer sending customers hotel thus visit local states hotel
filled earlier stages
general classes even weakly coupled e g becker
zilberstein lesser witwicki durfee criterion factor
hence direct correspondence number local beliefs
applying clustering determine well clusters
analogous e g state aggregation mdps e g discussed givan dean
greig known predict priori large minimized model
fortunately empirical demonstrate domains admit little
clustering overhead small
expected incremental expansion helpful allow
much clustering however e g dec tiger illustrate limit
amount scaling method currently provide bottleneck solution
large cbgs later stages cbg solver solve large cbgs
returning first solution order guarantee optimality takes takes long time
expect improvements cbg solvers directly add efficacy
incremental expansion
experiments clearly demonstrate dec pomdp complexity
important worst case fact scalability demonstrated experiments
clearly many successfully scale dramatically beyond would
assumes external state variable



fioliehoek spaan amato whiteson

expected doubly exponential dependence horizon even smallest
doubly exponential scaling horizon implies impossible compute solutions
beyond h indicated following simple calculation let n ai
actions oi observations


ai n oi





ai n oi



e

thus even simplest possible case see increase factor e h
h similarly next increment h h increases size search
space factor e however experiments clearly indicate almost
cases things dire even though matters look bleak light
complexity many cases able perform substantially better worst
case

related work
section discuss number methods related proposed
article methods already discussed earlier sections section
indicated clustering method closely related emery montemerlo
et al fundamentally different method lossless section
discussed connections boularias chaib draa clusters
policy values contrasts clusters histories thus
policies leading greater scalability
section discussed relationship notion probabilistic equivalence pe multiagent belief however yet another notion belief employed
jesp solution method nair et al superficially similar pe
distribution jesp belief aoh probability distribution pr b
states observation histories agents given deterministic full policy
agents sufficient statistic since induces multiagent belief thus
allows clustering histories crucial difference utility pe lies
fact pe criterion specified states aohs given past joint policy
induce multiagent belief
clustering resembles number methods employ equivalence
notions first several approaches exploit notion behavioral equivalence pynadath
marsella zeng et al zeng doshi consider perspective
protagonist agent possible another agent j since j affects
actions e behavior agent cluster together agent j lead
policy j agent cluster agent j
behaviorally equivalent contrast cluster agents j histories
agent agents well environment guaranteed behave
expectation thus leading best response agent method
could seen clustering histories expected environmental behavior equivalent
notion utility equivalence pynadath marsella zeng et al closer
pe takes account value best response agent particular
clusters two mj mj br mj best response mj achieves
value mj however remains form behavior equivalence
clusters agents histories protagonist agent


fiincremental clustering expansion faster optimal dec pomdps

connections pe work influence abstraction becker et
al witwicki durfee witwicki oliehoek et al since influence
point parameter space becker et al compact representation
agents policies agents clustered lead influence
agent however though fine grained ultimately still form behavioral
equivalence
final relation equivalence notion work dekel fudenberg morris
constructs distance measure topology space types
goal approximating infinite universal type space space possible beliefs
beliefs beliefs etc one shot bayesian games setting however considers
simple finite type space types directly correspond private histories
form aohs sequential thus need approximate universal
type space instead want know histories lead future dynamics
perspective agent dekel et al topology address question
incremental expansion technique related approaches extending deal
large branching factors context multiple sequence alignment ikeda imai
yoshizumi miura ishida however different
discard unpromising nodes rather provide mechanism generate necessary
ones proposing maa szer et al developed superficially similar could applied last stage particular proposed generating
child nodes one one time checking child found value equal
parents heuristic value since value child specifies full policy value
lower bound therefore expansion remaining child nodes skipped unfortunately number issues prevent providing substantial leverage
practice first cannot applied intermediate stages h since lower bound
values expanded children available second many unlikely
child node exists third even szer et al specify efficient way
finding incremental expansion overcomes issues yielding
experiments demonstrate significantly increases size dec pomdps
solved optimally
article focuses optimal solutions dec pomdps finite horizon part
evaluation compare milp aras dutech dpilp boularias chaib draa dp ipg amato et al extension
exact dynamic programming hansen et al finite horizon decpomdps considered many approaches bounded approximations amato
carlin zilberstein locally optimal solutions nair et al varakantham nair
tambe yokoo approximate methods without guarantees seuken zilberstein
b carlin zilberstein eker akn oliehoek kooi vlassis
dibangoye et al kumar zilberstein b wu et al wu zilberstein
chen b
particular much considered optimal approximate solution
subclasses dec pomdps one subclass contains dec pomdps
agents local states agents cannot influence resulting
toi dec mdp becker et al dibangoye amato doniec charpillet ndpomdp nair et al varakantham et al marecki gupta varakantham tambe
yokoo kumar zilberstein interpreted independent po mdps


fioliehoek spaan amato whiteson

agent coupled reward function possibly unaffectable state
feature hand event driven interaction becker et al consider
agents individual rewards influence others transitions
recently allow limited transition reward dependence
introduced examples interaction driven markov games spaan melo decmdps sparse interactions melo veloso distributed pomdps coordination locales varakantham et al velagapudi et al event driven interactions
complex rewards edi cr mostafa lesser transition decoupled dec pomdps
witwicki durfee witwicki methods developed often exhibit better scaling behavior methods standard dec po mdps typically
suitable agents extended interactions e g collaborate transporting
item specialized consider timing actions whose
ordering already determined marecki tambe beynier mouaddib
another body work addresses infinite horizon amato bernstein zilberstein amato bonet zilberstein bernstein amato hansen zilberstein
kumar zilberstein pajarinen peltonen possible
represent policy tree approaches represent policies finite state controllers
optimized ways since infinite horizon case undecidable
bernstein et al approaches approximate optimal given particular controller size exists boundedly optimal theoretically construct
controller within optimal feasible small large
bernstein et al
great interest dec pomdps explicitly take account communication approaches try optimize meaning communication actions without
semantics xuan lesser zilberstein goldman zilberstein spaan gordon
vlassis goldman allen zilberstein others use fixed semantics e g
broadcasting local observations ooi wornell pynadath tambe nair et
al roth et al oliehoek spaan vlassis roth simmons veloso
spaan oliehoek vlassis goldman zilberstein becker carlin lesser zilberstein williamson gerding jennings wu et al since used
first category e g dec pomdp com converted normal dec pomdps
seuken zilberstein contributions article applicable settings
finally numerous closely related dec pomdps posgs
hansen et al interactive pomdps pomdps gmytrasiewicz doshi
graphical counterparts doshi zeng chen general sense consider self interested settings agent individual
reward function pomdps conjectured require doubly exponential time seuken
zilberstein however pomdp number recent advances
doshi gmytrasiewicz current makes clear link best response
equivalence histories notion best response equivalence beliefs pomdps
particular article demonstrates two pe action observation histories aohs induce given past joint policy distribution states aohs agents
therefore induce multiagent belief future policies agents
induced multiagent beliefs turn interpreted special cases pomdp beliefs
model agents sub intentional form fixed policy
tree rabinovich rosenschein introduced method rather optimizing


fiincremental clustering expansion faster optimal dec pomdps

expected value joint policy selects coordinated actions uncertainty tracking
dynamics environment however requires model ideal system
dynamics input many considered article identifying
dynamics difficult

future work
several avenues future work made possible presented article
perhaps promising development approximate dec pomdp
article focused optimal methods gmaa ice seen framework approximate methods methods could derived limiting amount
backtracking employing approximate cbg solvers emery montemerlo gordon schneider
thrun kumar zilberstein b wu et al integrating gmaa methods factored dec pomdps oliehoek spaan whiteson vlassis oliehoek
oliehoek et al performing lossy clustering emery montemerlo wu et al
bounded approximations heuristics particular seems promising combine approximate clustering approximate factored gmaa methods
lossy clustering could achieved generalizing probabilistic equivalence criterion
currently strict little clustering may possible many
obvious cluster histories distributions states histories
agents merely similar measured e g kullback leibler divergence alternately
histories could clustered induce individual belief states
pr

x

pr







individual beliefs sufficient statistics history hypothesize
constitute effective metrics approximate clustering since individual belief simply
marginalizes agents histories probabilities used probabilistic
equivalence criterion intuitive heuristic metric approximate clustering
article focuses increasing scalability respect horizon developing
techniques deal larger number agents important direction future work
plan explore performing gmaa factored representations oliehoek spaan
whiteson vlassis previous work could exploit factorization
last stage since earlier stages required full expansions guarantee optimality however
larger number joint bg policies e number child nodes
directly large earlier stages tightly coupled therefore incremental expansion
crucial improving scalability optimal solution methods respect number
agents
another avenue future work generalize gmaa ice particular
may possible flatten two nested searches single search
could lead significant savings would obviate need solve entire cbg
expanding next one work employed plain basis
promising direction future work investigate enhancements literature
edelkamp schrodl benefit gmaa particular described
experiments different past joint policies lead cbgs different sizes one idea


fioliehoek spaan amato whiteson

first expand parts search tree lead small cbgs biasing selection
operator pruning operator maintain optimality
yet another important direction future work development tighter heuristics
though researchers addressing topic presented article underscore important heuristics solving larger currently heuristic
bottleneck four seven considered moreover two
bottleneck already solved long h horizons
therefore believe computing tight heuristics longer horizons single
important direction improving scalability optimal dec pomdp
solution methods respect horizon
different direction employ theoretical clustering beyond decpomdp setting develop solution methods cbgs instance well known
method computing local optimum alternating maximization starting
arbitrary joint policy compute best response agent given agents keep
policies fixed select another agents policy improve etc one idea start
completely clustered cbg agents types clustered together thus
random joint cbg policy simple form agent selects single action
improving policy agent consider actual possible types compute
best response subsequently cluster together types agent selects
action proceed next agent addition since clustering
restricted collaborative setting may possible employ similar
develop solution methods general payoff bgs
finally two contributions significant impact beyond
optimally solving dec pomdps first idea incrementally expanding nodes introduced
gmaa ice applied search methods incremental expansion
useful children generated order decreasing heuristic value without prohibitive
computational effort large branching factor multiple sequence
alignment computational biology carrillo lipman ikeda imai
second representing pwlc value functions hybrid tree set vectors
wider impact well e g online search pomdps ross pineau paquet chaib draa


conclusions
article presented set methods advance state art optimal solution
methods dec pomdps particular presented several advances aim extend
horizon optimal solutions found advances build gmaa
heuristic search include lossless incremental clustering cbgs solved
gmaa incremental expansion nodes gmaa search tree hybrid heuristic
representations provided theoretical guarantees suitable heuristic used
incremental clustering incremental expansion yield complete search equivalent finally presented extensive empirical demonstrating
gmaa ice optimally solve dec pomdps unprecedented size significanty
increase horizons tackledin cases order
magnitude given increase horizon one exponentially larger
search space constitutes large improvement moreover techniques im

fiincremental clustering expansion faster optimal dec pomdps

prove scalability respect number agents leading first ever solutions
general dec pomdps three agents demonstrated
optimal techniques yield insights particular dec pomdps incremental
clustering revealed properties broadcastchannel make much easier solve
addition facilitating optimal solutions hope advances inspire principled
approximation methods incremental clustering already done wu et al
enable meaningfully benchmarked

acknowledgments
thank raghav aras abdeslam boularias making code available us supported part afosr muri project fa part nwo
catch project funded fp marie curie actions individual
fellowship fp people ief

appendix appendix
auxiliary
implements bestjointpolicyandvalue function prunes child
nodes fully specified generates children particular cbg
bestjointpolicyandvalue qexpand prune fully expanded nodes set
nodes qexpand returning best one value
input qexpand set nodes fully specified joint policies
output best full joint policy input set value
v
q qexpand

qexpand remove q

h vi q

v v

v v



end
end
return h v

detailed gmaa ice
complete gmaa ice shown
computation v
quantity v defined recursively via
v v est r st b




fioliehoek spaan amato whiteson

generateallchildrenforcbg b
input cbg b
output qexpand set containing expanded child nodes cbg
qexpand
jointp
cbg policies b

vb pr u


create partial joint policy




b
b

v v
v
compute heuristic value

q ht vb
create child node

qexpand insert q
end
return qexpand

expectation taken respect joint probability distribution states
joint aohs induced
x
pr st b
pr ot st pr st st pr pr st b
st


ot pr probability specifies
aoh case deterministic past joint policy
proofs
proof theorem
substituting yields
x
b
vb vb
pr b q





x





pr b est r st e vb

est r st b e vb b

est r st b e q b
est r st b h h

h optimal admissible heuristic substituting obtain
vb v est r st b e vb b
v est r st b h h

via v h h
demonstrates heuristic value vb used gmaa via cbgs heuristic
form admissible lower bounded actual value first plus
admissible heuristic since performs heuristic search admissible heuristic
complete


fiincremental clustering expansion faster optimal dec pomdps

gmaa ice
















































vgm aa

v
q h vi
lie q
repeat
q select lie
q ht vi
ie
l pop q
isplaceholder q
b cbg
reuse stored cbg
else
construct extended bg solver
b cbg
note



b constructextendedbg b
b clusterbg b
b solver createsolver b
cbg b
end
expand single child
vcbg vgm aa v
vcbg
last stage h
vcbg vb h v h h
end
h vb b solver nextsolution vcbg vcbg

fully expanded solution v h vcbg
delete q cbg solver
continue
e goto line
end

vb v vb
last stage h
note v vb
v vgm aa
vgm aa v
found lower bound

lie prune vgm aa
end
delete q cbg solver
else
q ht vb
lie insert q
q ht vb
update parent node q placeholder
lie insert q
end
lie empty



fioliehoek spaan amato whiteson

proof lemma




proof assume arbitrary ati ot
ot






pr st
ot
ai
x




ia
pr ot
pr st st ati pr
pr st

ai
st



x





ib
pr st st ati pr
pr st
pr ot
ai

st


b
pr st
ot
ai


assumed arbitrary st
ot

st ot





b

oi ai
pr st
ot
ai pr



general


pr






ati ot



pr st
ot
ai
pr ot












pr st
ot
ai
p




pr
oi ai






numerator denominator substituting
ia ib equation consequently conclude


b
ai oi
pr st
ia ati ot
pr




finally ati ot
arbitrarily chosen conclude

holds

proof lemma
proof assume arbitrary
bi ia pr ia b
x

pr
ia b



factoring joint distribution



x

pr
ia b pr
ia b






fiincremental clustering expansion faster optimal dec pomdps

depends


x

pr
ia b pr



depend

x

pr
ia b pr


due pe

x

pr
ib b pr











pr ib b bi ib
conclude holds
proof theorem search equivalence
prove search equivalence explicitly write node tuple q ht v phi
past joint policy v nodes heuristic value ph boolean indicating whether
placeholder consider equivalence maintained open lists open list
l maintained gmaa ic contains non expanded nodes q contrast open
list lie gmaa ice contains non expanded nodes q placeholders previously
expanded nodes q denote ordered subset lie containing non expanded nodes
q containing placeholders q treat open lists ordered sets
heuristic values associated nodes
definition l lie equivalent l lie
q l
qs ordering l remove l q q
nodes q l q placeholder q parent higher ranked
q
q ht vq falsei l q

q ht vq trueiq q q

placeholders
fig illustrates two equivalent lists past joint policies indexed letters
note placeholders lie ranked higher nodes l represent
let us write ic l ice lie one iteration e one loop main repeat
respective let ice denote operation repeats
ice long placeholder selected ends q expanded
lemma l lie executing ic l ice lie leads open lists
equivalent l lie
proof ice selects placeholder q generates child q already present
l due properties definition inserts insertion occurs
relative location ic use comparison operator
definition together facts guarantee insertion preserves properties
remove b removes elements b without changing ordering



fioliehoek spaan amato whiteson

lie

l
q
vb







c

e







f
g
h

j

q

vb










f
g

vb







b



placeholder c e j



nodes position




placeholder h
consistent ordering
equal values

figure illustration equivalent lists past joint policies indexed letters
example b expanded earlier yet fully expanded ice case
remaining unexpanded children q ice reinserts q updated heuristic
value q v q v guaranteed upper bound value unexpanded siblings
q since q v vb q vb q q v preserving properties
ice finally selects non placeholder q guaranteed q
selected ic due properties expansion ice generates one child q
inserted relative location ic inserts placeholder q hq q v truei
siblings q preserving properties
proof theorem fact gmaa ice gmaa ic search equivalent follows directly lemma search equivalence means select
non placeholders q expand since begin identical therefore trivially equivalent open lists maintain equivalent open lists throughout search
property definition ensures every time ice selects non placeholder ic
selects

references
allen zilberstein agent influence predictor difficulty decentralized
solving proceedings twenty second aaai conference artificial
intelligence
amato c bernstein zilberstein optimal fixed size controllers
decentralized pomdps proc aamas workshop multi agent sequential
decision making uncertain domains
amato c bernstein zilberstein optimizing memory bounded controllers
decentralized pomdps proc uncertainty artificial intelligence
amato c bernstein zilberstein optimizing fixed size stochastic controllers pomdps decentralized pomdps autonomous agents multi agent
systems


fiincremental clustering expansion faster optimal dec pomdps

amato c bonet b zilberstein finite state controllers mealy
machines centralized decentralized pomdps proceedings twentyfourth aaai conference artificial intelligence
amato c carlin zilberstein bounded dynamic programming decentralized pomdps proc aamas workshop multi agent sequential
decision making uncertain domains
amato c dibangoye j zilberstein incremental policy generation
finite horizon dec pomdps proc international conference automated
scheduling
aras r dutech investigation mathematical programming finite
horizon decentralized pomdps journal artificial intelligence

becker r carlin lesser v zilberstein analyzing myopic approaches
multi agent communication computational intelligence
becker r zilberstein lesser v decentralized markov decision processes
event driven interactions proc international conference autonomous
agents multi agent systems
becker r zilberstein lesser v goldman c v transition independent
decentralized markov decision processes proc international conference
autonomous agents multi agent systems
bernstein amato c hansen e zilberstein policy iteration
decentralized control markov decision processes journal artificial intelligence

bernstein givan r immerman n zilberstein complexity
decentralized control markov decision processes mathematics operations

bertsekas p dynamic programming optimal control rd ed vol athena
scientific
beynier mouaddib solving efficiently decentralized mdps temporal
resource constraints autonomous agents multi agent systems

boularias chaib draa b exact dynamic programming decentralized
pomdps lossless policy compression proc international conference
automated scheduling
busoniu l babuska r de schutter b comprehensive survey multi agent
reinforcement learning ieee transactions systems man cybernetics part c
applications reviews
carlin zilberstein value observation compression dec pomdps
proc international conference autonomous agents multi agent systems


fioliehoek spaan amato whiteson

carrillo h lipman multiple sequence alignment biology
siam journal applied mathematics
cassandra littman l zhang n l incremental pruning simple fast
exact method partially observable markov decision processes proc uncertainty
artificial intelligence
cassandra r exact approximate partially observable markov
decision processes unpublished doctoral dissertation brown university
dechter r flerova n marinescu r search best solutions
graphical proceedings twenty sixth aaai conference artificial
intelligence
dekel e fudenberg morris topologies types theoretical economics

dibangoye j amato c doniec charpillet f producing efficient errorbounded solutions transition independent decentralized mdps proc international conference autonomous agents multi agent systems submitted
publication
dibangoye j mouaddib chai draa b point incremental pruning heuristic solving finite horizon dec pomdps proc international
conference autonomous agents multi agent systems
doshi p gmytrasiewicz p monte carlo sampling methods approximating
interactive pomdps journal artificial intelligence
doshi p zeng chen q graphical interactive pomdps representations solutions autonomous agents multi agent systems
edelkamp schrodl heuristic search theory applications morgan
kaufmann
eker b akn h l evolution strategies solve dec pomdp
soft computinga fusion foundations methodologies applications

eker b akn h l solving decentralized pomdp genetic
autonomous agents multi agent systems
emery montemerlo r game theoretic control robot teams unpublished
doctoral dissertation carnegie mellon university
emery montemerlo r gordon g schneider j thrun approximate solutions partially observable stochastic games common payoffs proc
international conference autonomous agents multi agent systems
emery montemerlo r gordon g schneider j thrun game theoretic
control robot teams proc ieee international conference robotics
automation
givan r dean greig equivalence notions model minimization
markov decision processes artificial intelligence


fiincremental clustering expansion faster optimal dec pomdps

gmytrasiewicz p j doshi p framework sequential multi agent
settings journal artificial intelligence
goldman c v allen zilberstein learning communicate decentralized environment autonomous agents multi agent systems
goldman c v zilberstein optimizing information exchange cooperative
multi agent systems proc international conference autonomous agents
multi agent systems
goldman c v zilberstein decentralized control cooperative systems
categorization complexity analysis journal artificial intelligence

goldman c v zilberstein communication decomposition mechanisms
decentralized mdps journal artificial intelligence
hansen e bernstein zilberstein dynamic programming partially observable stochastic games proc national conference artificial
intelligence
hauskrecht value function approximations partially observable markov decision processes journal artificial intelligence
hsu k marcus decentralized control finite state markov processes ieee
transactions automatic control
huhns n ed distributed artificial intelligence pitman publishing ltd
ikeda imai h enhanced multiple alignments optimal
alignments several sequences k opt approximate alignments large cases theoretical computer science
kaelbling l p littman l cassandra r acting partially
observable stochastic domains artificial intelligence
kumar zilberstein constraint dynamic programming decentralized
pomdps structured interactions proc international conference
autonomous agents multi agent systems
kumar zilberstein anytime decentralized pomdps
expectation maximization proc uncertainty artificial intelligence
kumar zilberstein b point backup decentralized pomdps complexity proc international conference autonomous
agents multi agent systems
littman cassandra kaelbling l learning policies partially observable environments scaling proc international conference machine
learning
marecki j gupta varakantham p tambe yokoo agents
equal scaling distributed pomdps agent networks proc international
conference autonomous agents multi agent systems


fioliehoek spaan amato whiteson

marecki j tambe opportunistic techniques solving decentralized
markov decision processes temporal constraints proc international
conference autonomous agents multi agent systems
melo f veloso decentralized mdps sparse interactions artificial
intelligence
mostafa h lesser v compact mathematical formulation
structured agent interactions proc aamas workshop multi agent sequential decision making uncertain domains
nair r roth yohoo communication improving policy computation
distributed pomdps proc international conference autonomous agents
multi agent systems
nair r tambe yokoo pynadath v marsella taming decentralized pomdps towards efficient policy computation multiagent settings proc
international joint conference artificial intelligence
nair r varakantham p tambe yokoo networked distributed pomdps
synthesis distributed constraint optimization pomdps proc national conference artificial intelligence
oliehoek f value teams agents stochastic partially observable environments amsterdam university press doctoral dissertation university
amsterdam
oliehoek f decentralized pomdps wiering van otterlo eds
reinforcement learning state art vol springer berlin heidelberg
oliehoek f kooi j f vlassis n cross entropy method policy search
decentralized pomdps informatica
oliehoek f spaan j tree solution methods multiagent
pomdps delayed communication proceedings twenty sixth aaai conference artificial intelligence
oliehoek f spaan j dibangoye j amato c heuristic search identical payoff bayesian games proc international conference autonomous
agents multi agent systems
oliehoek f spaan j vlassis n dec pomdps delayed communication proc aamas workshop multi agent sequential decision making
uncertain domains
oliehoek f spaan j vlassis n optimal approximate q value
functions decentralized pomdps journal artificial intelligence

oliehoek f spaan j whiteson vlassis n exploiting locality
interaction factored dec pomdps proc international conference
autonomous agents multi agent systems
oliehoek f vlassis n q value functions decentralized pomdps proc
international conference autonomous agents multi agent systems


fiincremental clustering expansion faster optimal dec pomdps

oliehoek f whiteson spaan j lossless clustering histories
decentralized pomdps proc international conference autonomous
agents multi agent systems
oliehoek f whiteson spaan j approximate solutions factored dec pomdps many agents proc international conference
autonomous agents multi agent systems submitted publication
oliehoek f witwicki kaelbling l p influence abstraction
multiagent systems proceedings twenty sixth aaai conference artificial
intelligence
ooi j wornell g w decentralized control multiple access broadcast
channel performance bounds proc th conference decision control
osborne j rubinstein course game theory mit press
pajarinen j peltonen j efficient factored infinite horizon decpomdps proc international joint conference artificial intelligence
panait l luke cooperative multi agent learning state art
autonomous agents multi agent systems
puterman l markov decision processesdiscrete stochastic dynamic programming john wiley sons inc
pynadath v marsella c minimal mental proceedings
twenty second aaai conference artificial intelligence
pynadath v tambe communicative multiagent team decision
analyzing teamwork theories journal artificial intelligence

rabinovich z goldman c v rosenschein j complexity multiagent
systems price silence proc international conference autonomous
agents multi agent systems
rabinovich z rosenschein j multiagent coordination extended markov
tracking proc international conference autonomous agents multi
agent systems
ross pineau j paquet chaib draa b online
pomdps journal artificial intelligence
roth simmons r veloso reasoning joint beliefs executiontime communication decisions proc international conference autonomous
agents multi agent systems
roth simmons r veloso exploiting factored representations decentralized execution multi agent teams proc international conference
autonomous agents multi agent systems
seuken zilberstein improved memory bounded dynamic programming
decentralized pomdps proc uncertainty artificial intelligence


fioliehoek spaan amato whiteson

seuken zilberstein b memory bounded dynamic programming decpomdps proc international joint conference artificial intelligence
seuken zilberstein formal decentralized decision
making uncertainty autonomous agents multi agent systems

spaan j gordon g j vlassis n decentralized uncertainty teams communicating agents proc international conference
autonomous agents multi agent systems
spaan j melo f interaction driven markov games decentralized
multiagent uncertainty proc international conference
autonomous agents multi agent systems
spaan j oliehoek f multiagent decision process toolbox
software decision theoretic multiagent systems proc aamas
workshop multi agent sequential decision making uncertain domains
spaan j oliehoek f amato c scaling optimal heuristic search
dec pomdps via incremental expansion proc international joint conference
artificial intelligence
spaan j oliehoek f vlassis n multiagent uncertainty
stochastic communication delays proc international conference
automated scheduling
sycara k p multiagent systems ai magazine
szer charpillet f zilberstein maa heuristic search
solving decentralized pomdps proc uncertainty artificial intelligence
tsitsiklis j athans complexity decentralized decision making
detection ieee transactions automatic control
varaiya p walrand j delayed sharing patterns ieee transactions
automatic control
varakantham p kwak j young taylor e marecki j scerri p tambe
exploiting coordination locales distributed pomdps via social model shaping
proc international conference automated scheduling
varakantham p marecki j yabu tambe yokoo letting loose
spider network pomdps generating quality guaranteed policies proc
international conference autonomous agents multi agent systems
varakantham p nair r tambe yokoo winning back cup distributed pomdps continuous belief spaces proc international
conference autonomous agents multi agent systems
velagapudi p varakantham p scerri p sycara k distributed model shaping
scaling decentralized pomdps hundreds agents proc international conference autonomous agents multi agent systems
vlassis n concise introduction multiagent systems distributed artificial
intelligence morgan claypool publishers


fiincremental clustering expansion faster optimal dec pomdps

williamson gerding e h jennings n r reward shaping valuing communications multi agent coordination proc international conference
autonomous agents multi agent systems
witwicki j abstracting influences efficient multiagent coordination
uncertainty unpublished doctoral dissertation university michigan ann arbor
michigan usa
witwicki j durfee e h influence policy abstraction weakly coupled
dec pomdps proc international conference automated
scheduling
wu f zilberstein chen x point policy generation decentralized
pomdps proc international conference autonomous agents multi
agent systems
wu f zilberstein chen x b rollout sampling policy iteration decentralized
pomdps proc uncertainty artificial intelligence
wu f zilberstein chen x online multi agent systems
bounded communication artificial intelligence
xuan p lesser v zilberstein communication decisions multi agent cooperation model experiments proc international conference autonomous
agents
yoshizumi miura ishida partial expansion large branching
factor proc national conference artificial intelligence
zeng doshi p exploiting model equivalences solving interactive dynamic
influence diagrams journal artificial intelligence
zeng doshi p pan mao h chandrasekaran luo j utilizing
partial policies identifying equivalence behavioral proceedings
twenty fifth aaai conference artificial intelligence




