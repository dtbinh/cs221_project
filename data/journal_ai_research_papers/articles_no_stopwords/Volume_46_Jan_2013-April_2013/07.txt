Journal Artificial Intelligence Research 46 (2013) 449509

Submitted 9/12; published 03/13

Incremental Clustering Expansion Faster
Optimal Planning Decentralized POMDPs
Frans A. Oliehoek

frans.oliehoek@maastrichtuniversity.nl

Maastricht University
Maastricht, Netherlands

Matthijs T.J. Spaan

m.t.j.spaan@tudelft.nl

Delft University Technology
Delft, Netherlands

Christopher Amato

camato@csail.mit.edu

Massachusetts Institute Technology
Cambridge, MA, USA

Shimon Whiteson

s.a.whiteson@uva.nl

University Amsterdam
Amsterdam, Netherlands

Abstract
article presents state-of-the-art optimal solution methods decentralized
partially observable Markov decision processes (Dec-POMDPs), general models
collaborative multiagent planning uncertainty. Building generalized multiagent A* ( GMAA*) algorithm, reduces problem tree one-shot collaborative
Bayesian games (CBGs), describe several advances greatly expand range DecPOMDPs solved optimally. First, introduce lossless incremental clustering
CBGs solved GMAA*, achieves exponential speedups without sacrificing
optimality. Second, introduce incremental expansion nodes GMAA* search
tree, avoids need expand children, number worst case
doubly exponential nodes depth. particularly beneficial little clustering
possible. addition, introduce new hybrid heuristic representations
compact thereby enable solution larger Dec-POMDPs. provide theoretical
guarantees that, suitable heuristic used, incremental clustering incremental expansion yield algorithms complete search equivalent. Finally,
present extensive empirical results demonstrating GMAA*-ICE, algorithm
synthesizes advances, optimally solve Dec-POMDPs unprecedented size.

1. Introduction
key goal artificial intelligence development intelligent agents interact
environment order solve problems, achieve goals, maximize utility.
agents sometimes act alone, researchers increasingly interested collaborative multiagent
systems, teams agents work together perform manner tasks. Multiagent
systems appealing, tackle inherently distributed problems,
facilitate decomposition problems complex tackled single
c
2013
AI Access Foundation. rights reserved.

fiOliehoek, Spaan, Amato, & Whiteson

agent (Huhns, 1987; Sycara, 1998; Panait & Luke, 2005; Vlassis, 2007; Busoniu, Babuska, &
De Schutter, 2008).
One primary challenges multiagent systems presence uncertainty. Even
single-agent systems, outcome action may uncertain, e.g., action may fail
probability. Furthermore, many problems state environment may
uncertain due limited noisy sensors. However, multiagent settings problems
often greatly exacerbated. Since agents access sensors, typically
small fraction complete system, ability predict agents
act limited, complicating cooperation. uncertainties properly addressed,
arbitrarily bad performance may result.
principle, agents use communication synchronize beliefs coordinate
actions. However, due bandwidth constraints, typically infeasible agents
broadcast necessary information agents. addition, many realistic
scenarios, communication may unreliable, precluding possibility eliminating uncertainty agents actions.
Especially recent years, much research focused approaches (collaborative)
multiagent systems deal uncertainty principled way, yielding wide variety
models solution methods (Pynadath & Tambe, 2002; Goldman & Zilberstein, 2004;
Seuken & Zilberstein, 2008). article focuses decentralized partially observable
Markov decision process (Dec-POMDP), general model collaborative multiagent planning uncertainty. Unfortunately, solving Dec-POMDP, i.e., computing optimal
plan, generally intractable (NEXP-complete) (Bernstein, Givan, Immerman, & Zilberstein,
2002) fact even computing solutions absolutely bounded error (i.e., -approximate
solutions) NEXP-complete (Rabinovich, Goldman, & Rosenschein, 2003). particular,
number joint policies grows exponentially number agents observations
doubly exponentially respect horizon problem.1 Though complexity results preclude methods efficient problems, developing better optimal
solution methods Dec-POMDPs nonetheless important goal, several reasons.
First, since complexity results describe worst case, still great potential
improve performance optimal methods practice. fact, evidence
many problems solved much faster worst-case complexity bound indicates
(Allen & Zilberstein, 2007). article, present experiments clearly demonstrate
point: many problems, methods propose scale vastly beyond would
expected doubly-exponential dependence horizon.
Second, computer speed memory capacity increase, growing set small
medium-sized problems solved optimally. problems arise naturally
others result decomposition larger problems. instance, may possible
extrapolate optimal solutions problems shorter planning horizons, using
starting point policy search longer-horizon problems work Eker
Akn (2013), use shorter-horizon, no-communication solutions inside problems
communication (Nair, Roth, & Yohoo, 2004; Goldman & Zilberstein, 2008). generally,
optimal policies smaller problems potentially used find good solutions larger
problems. instance, transfer planning (Oliehoek, 2010; Oliehoek, Whiteson, & Spaan,
1. Surprisingly, number states Dec-POMDP less important, e.g., brute-force search depends
number states via policy evaluation routine, scales linearly number states.

450

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

2013) employs optimal solutions problems agents better solve problems
many agents. performing (approximate) influence-based abstraction influence search
(Witwicki, 2011; Oliehoek, Witwicki, & Kaelbling, 2012), optimal solutions component
problems potentially used find (near-)optimal solutions larger problems.
Third, optimal methods offer important insights nature specific Dec-POMDP
problems solutions. instance, methods introduced article enabled
discovery certain properties BroadcastChannel benchmark problem make
much easier solve.
Fourth, optimal methods provide critical inspiration principled approximation methods. fact, almost successful approximate Dec-POMDP methods based optimal
ones (see, e.g., Seuken & Zilberstein, 2007b, 2007a; Dibangoye, Mouaddib, & Chai-draa, 2009;
Amato, Dibangoye, & Zilberstein, 2009; Wu, Zilberstein, & Chen, 2010a; Oliehoek, 2010)
locally optimal ones (Velagapudi, Varakantham, Scerri, & Sycara, 2011)2 , clustering technique presented article forms basis recently introduced approximate
clustering technique (Wu, Zilberstein, & Chen, 2011).
Finally, optimal methods essential benchmarking approximate methods. recent
years, huge advances approximate solution Dec-POMDPs, leading
development solution methods deal large horizons, hundreds agents
many states (e.g., Seuken & Zilberstein, 2007b; Amato et al., 2009; Wu et al., 2010a;
Oliehoek, 2010; Velagapudi et al., 2011).
However, since computing even -approximate
solutions NEXP-complete, method whose complexity doubly exponential cannot
guarantees absolute error solution (assuming EXP6=NEXP). such,
existing effective approximate methods quality guarantees.3
Consequently, difficult meaningfully interpret empirical performance without
upper bounds optimal methods supply. approximate methods benchmarked lower bounds (e.g., approximate methods), comparisons cannot
detect method fails find good solutions. requires benchmarking
upper bounds and, unfortunately, upper bounds easier compute, QMDP
QPOMDP, loose helpful (Oliehoek, Spaan, & Vlassis, 2008). such,
benchmarking respect optimal solutions important part verification
approximate algorithm. Since existing optimal methods tackle small problems,
scaling optimal solutions larger problems critical goal.
1.1 Contributions
article presents state-of-the-art optimal solution methods Dec-POMDPs.
particular, describes several advances greatly expand horizon many DecPOMDPs solved optimally. addition, proposes evaluates complete algorithm
synthesizes advances. approach based generalized multiagent A*
(GMAA*) algorithm (Oliehoek, Spaan, & Vlassis, 2008), makes possible reduce
problem tree one-shot collaborative Bayesian games (CBGs). appeal
2. method Velagapudi et al. (2011) repeatedly computes best responses way similar DP-JESP
(Nair, Tambe, Yokoo, Pynadath, & Marsella, 2003). best response computation, however, exploits
sparsity interactions.
3. Note refer methods without quality guarantees approximate rather heuristic avoid
confusion heuristic search, used throughout article exact.

451

fiOliehoek, Spaan, Amato, & Whiteson

approach abstraction layer introduces, led various insights DecPOMDPs and, turn, improved solution methods describe.
specific contributions article are:4
1. introduce lossless clustering CBGs, technique reduce size CBGs
GMAA* enumerates possible solutions, preserving optimality.
exponentially reduce number child nodes GMAA* search tree, leading
huge increases efficiency. addition, applying incremental clustering (IC)
GMAA*, GMAA*-IC method avoid clustering exponentially sized CBGs.
2. introduce incremental expansion (IE) nodes GMAA* search tree. Although
clustering may reduce number children search node, number
worst case still doubly exponential nodes depth. GMAA*-ICE, applies
IE GMAA*-IC, addresses problem creating next child node
candidate expansion.
3. provide theoretical guarantees GMAA*-IC GMAA*-ICE. particular, show that, using suitable heuristic, algorithms complete
search equivalent.
4. introduce improved heuristic representation. Tight heuristics based
underlying POMDP solution (QPOMDP ) value function resulting
assuming 1-step-delayed communication (QBG ) essential heuristic search methods GMAA* (Oliehoek, Spaan, & Vlassis, 2008). However, space needed
store heuristics grows exponentially problem horizon. introduce hybrid representations compact thereby enable solution larger
problems.
5. present extensive empirical results show substantial improvements
current state-of-the-art. Whereas Seuken Zilberstein (2008) argued GMAA*
best optimally solve Dec-POMDPs one horizon brute-force
search, results demonstrate GMAA*-ICE much better. addition,
provide comparative overview results competitive optimal solution methods
literature.
primary aim techniques introduced article improve scalability
respect horizon. empirical results confirm techniques highly
successful regard. added bonus, experiments demonstrate improvement
scalability respect number agents. particular, present first optimal
results general (non-special case) Dec-POMDPs three agents. Extensions
techniques achieve improvements respect number agents,
well promising ways combine ideas behind methods state-of-the-art
approximate approaches, discussed future work Section 7.
4. article synthesizes extends research already reported two conference papers (Oliehoek,
Whiteson, & Spaan, 2009; Spaan, Oliehoek, & Amato, 2011).

452

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

1.2 Organization
article organized follows. Section 2 provides background Dec-POMDP model,
GMAA* heuristic search solution method, well suitable heuristics. Section 3,
introduce lossless clustering CBGs integration GMAA*. Section 4 introduces incremental expansion search nodes. empirical evaluation proposed
techniques reported Section 5. give treatment related work Section 6. Future
work discussed Section 7 conclusions drawn Section 8.

2. Background
Dec-POMDP, multiple agents must collaborate maximize sum common
rewards receive multiple timesteps. actions affect immediate
rewards state transition. current state known
agents, timestep agent receives private observation correlated
state.



Definition 1. Dec-POMDP tuple D, S, A, T, O, O, R, b0 , h ,
= {1, . . . ,n} finite set agents.


= s1 , . . . ,s|S| finite set states.

= Ai set joint actions = ha1 , . . . , i, Ai finite set actions
available agent i.
transition function specifying state transition probabilities Pr(s |s,a).
= Oi finite set joint observations. every stage one joint observation
= ho1 ,...,on received. agent observes component oi .
observation function, specifies observation probabilities Pr(o|a,s ).
R(s,a) immediate reward function mapping (s,a)-pairs real numbers.
b0 (S) initial state distribution time = 0, (S) denotes infinite
set probability distributions finite set S.
h horizon, i.e., number stages. consider case h finite.
stage = 0 . . . h 1, agent takes individual action receives individual
observation.
Example 1 (Recycling Robots). illustrate Dec-POMDP model, consider team robots tasked
removing trash office building, depicted Fig. 1. robots sensors find marked
trash cans, motors move around order look cans, well gripper arms grasp carry
can. Small trash cans light compact enough single robot carry, large trash cans
require multiple robots carry together. people use them, larger trash
cans fill quickly. robot must ensure battery remains charged moving
charging station expires. battery level robot degrades due distance
robot travels weight item carried. robot knows battery level
robots location robots within sensor range. goal
problem remove much trash possible given time period.
problem represented Dec-POMDP natural way. states, S, consist
different locations robot, battery levels different amounts trash
cans. actions, Ai , robot consist movements different directions well decisions
453

fiOliehoek, Spaan, Amato, & Whiteson

Figure 1: Illustration Recycling Robots example, two robots remove
trash office environment three small (blue) trash cans two large (yellow) ones.
situation, left robot might observe large trash next full,
robot small trash empty. However, none sure trash
cans state due limited sensing capabilities, see state trash cans
away. particular, one robot knowledge regarding observations robot.
pick trash recharge battery (when range charging station).
observations, Oi , robot consist battery level, location, locations
robots sensor range amount trash cans within range. rewards, R, could consist
large positive value pair robots emptying large (full) trash can, small positive value
single robot emptying small trash negative values robot depleting battery
trash overflowing. optimal solution joint policy leads expected behavior (given
rewards properly specified). is, ensures robots cooperate empty
large trash cans appropriate small ones individually considering battery usage.

explanatory purposes, consider much simpler problem, so-called decentralized tiger problem (Nair et al., 2003).
Example 2 (Dec-Tiger). Dec-Tiger problem concerns two agents find hallway two doors. Behind one door, treasure behind tiger. state
describes door tiger behindleft (sl ) right (sr )each occurring 0.5 probability
(i.e., initial state distribution b0 uniform). agent perform three actions: open left
door (aOL ), open right door (aOR ) listen (aLi ). Clearly, opening door treasure
yield reward, opening door tiger result severe penalty. greater reward
given agents opening correct door time. such, good strategy
probably involve listening first. listen actions, however, minor cost (negative reward).
every stage agents get observation. agents either hear tiger behind left
(oHL ) right (oHR ) door, agent 15% chance hearing incorrectly (getting wrong
observation). Moreover, observation informative agents listen; either agent opens
door, agents receive uninformative (uniformly drawn) observation problem resets
sl sr equal probability. point problem continues, agents may
able open door treasure multiple times. note that, since two observations
agents get oHL , oHR , agents way detecting problem reset:
one agent opens door listens, agent able tell
door opened. complete specification, see discussion Nair et al. (2003).

Given Dec-POMDP, agents common goal maximize expected cumulative
reward return. planning task entails finding joint policy = h1 , . . . ,n
space joint policies , specifies individual policy agent i.
454

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

individual policy general specifies individual action action-observation history
(AOH) ~it = (a0i ,o1i , . . . ,ait1 ,oti ), e.g., (~it ) = ati . However, possible restrict
attention deterministic pure policies, case maps observation history
~ action, e.g., (~o ) = . number policies
(OH) (o1i , . . . ,oti ) = ~oit



h 1)/(|O |1)
(|O
|

|Ai |
number joint policies therefore

n|O |h 1
|A | |O |1 ,
(2.1)
denote largest individual action observation sets. quality
particular joint policy expressed expected cumulative reward induces, referred
value.
Definition 2. value V () joint policy
V () , E

h1
hX
t=0




R(st ,at )fi,b0 ,

(2.2)

expectation sequences states, actions observations.
planning problem Dec-POMDP find optimal joint policy , i.e., joint
policy maximizes value: = arg max V ().
individual policy depends local information ~oi available
agent, on-line execution phase truly decentralized: communication takes place
modeled via actions observations. planning however, may take place
off-line phase centralized. scenario consider article.
detailed introduction Dec-POMDPs see, e.g., work Seuken Zilberstein
(2008) Oliehoek (2012).
2.1 Heuristic Search Methods
recent years, numerous Dec-POMDP solution methods proposed.
methods fall one two categories: dynamic programming heuristic search methods.
Dynamic programming methods take backwards bottom-up perspective first considering policies last time step = h 1 using construct policies stage
= h 2, etc. contrast, heuristic search methods take forward top-down perspective
first constructing plans = 0 extending later stages.
article, focus heuristic search approach shown state-of-the-art
results. make clear section, method interpreted searching
tree collaborative Bayesian games (CBGs). CBGs provide convenient abstraction
layer facilitates explanation techniques introduced article.
section provides concise background heuristic search methods.
detailed description, see work Oliehoek, Spaan, Vlassis (2008). description dynamic programming methods relationship heuristic search methods,
see work Oliehoek (2012).
2.1.1 Multiagent A*
Szer, Charpillet, Zilberstein (2005) introduced heuristically guided policy search method
called multiagent A* (MAA*). performs A* search partially specified joint policies,
455

fiOliehoek, Spaan, Amato, & Whiteson

t=0

t=1

t=2

i0

aLi
=2

2i

oHR

oHL
aOL

i1

aOL

oHL

oHR

oHL

oHR

aLi

aLi

aOL

aLi

i2

=1
Figure 2: arbitrary policy Dec-Tiger problem. figure illustrates different
types partial policies used paper. shown past policy 2i consists two decision
rules i0 , i1 . shown two sub-tree policies =1 , =2 (introduced Section 3.1.2).
pruning joint policies guaranteed worse best (fully specified) joint policy
found far. Oliehoek, Spaan, Vlassis (2008) generalized algorithm making explicit
expand selection operators performed heuristic search. resulting algorithm,
generalized MAA* (GMAA*) offers unified perspective MAA* forward sweep
policy computation method (Emery-Montemerlo, 2005), differ implement
GMAA*s expand operator: forward sweep policy computation solves (i.e., finds best
policy for) collaborative Bayesian games, MAA* finds policies collaborative
Bayesian games, describe Section 2.1.2.
GMAA* algorithm considers joint policies partially specified respect
time. partially specified policies formalized follows.
Definition 3. decision rule agent decision stage mapping action~ Ai .
observation histories stage actions :

article, consider deterministic policies. Since policies need condition
actions observation histories, made decision rules map length~ Ai . joint decision rule = h , . . . ,nt specifies
observation histories actions: :
1

decision rule agent. Fig. 2 illustrates concept, well past policy,
introduce shortly. discussed below, decision rules allow partial policies
defined play crucial role GMAA* algorithms developed article.
Definition 4. partial past policy stage t, ti , specifies part agent policy
relates stages < t. is, specifies decision rules first stages:
ti = (i0 ,i1 , . . . ,it1 ). past policy stage h regular, fully specified, policy
hi = . past joint policy = ( 0 , 1 , . . . , t1 ) specifies joint decision rules first
stages.
GMAA* performs heuristic search partial joint policies constructing
search tree illustrated Fig. 3a. node q = ht , vi search tree specifies
past joint policy heuristic value v. heuristic value v node represents
optimistic estimate past joint policy Vb (t ), computed via
Vb (t ) = V 0...t1 (t ) + H t...h1 (t ),
456

(2.3)

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

B(0 )

0
0

0

0

0
0

0
1

1
1

2

...

B(1 )

1

1

1

...

B(1 )

B(2 )

2

(a) MAA* perspective.

...

B(1 )

1

...

B(2 )

(b) CBG perspective.

Figure 3: Generalized MAA*. Associated every node heuristic value. search
trees two perspectives shown equivalent certain assumptions heuristic,
explained Section 2.2.
H t...h1 heuristic value remaining h stages V 0...t1 (t ) actual
expected reward achieves first stages (for definition, see Appendix A.3).
Clearly, H t...h1 admissible heuristica guaranteed overestimationso Vb (t ).5
Algorithm 1 illustrates GMAA*. starts creating node q 0 completely unspecified joint policy 0 placing open list L. Then, selects nodes (Algorithm 2)
expands (Algorithm 3), repeating process certain found
optimal joint policy.
Select operator returns highest ranked node, defined following comparison operator.
Definition 5. node comparison operator < defined two nodes q = ht ,vi, q =
ht ,v follows:



, v 6= v
v < v
q < q = depth(q) < depth(q ) , otherwise depth(q) 6= depth(q )
(2.4)



<
, otherwise.
is, comparison operator first compares heuristic values. equal,
compares depth nodes. Finally, nodes equal value equal depth,
lexically compares past joint policies. ranking leads A* behavior (i.e., selecting
node open list highest heuristic value) GMAA*, well guaranteeing
selection order incremental expansion technique (introduced Section 4).
Ranking nodes greater depth higher case equal heuristic value helps find tight
lower bounds early first expanding deeper nodes (Szer et al., 2005) useful
incremental expansion.
5. formally, H underestimate value. Note that, unlike classical A* applications
path planningin admissible heuristic overestimatein setting maximize reward,
rather minimize cost.

457

fiOliehoek, Spaan, Amato, & Whiteson

Algorithm 1 Generalized multiagent A*.
Input: Dec-POMDP, admissible heuristic H, empty open list L
Output: optimal joint policy
1: vGM AA
2: q 0 h0 = (), v = +i
3: L.insert(q 0 )
4: repeat
5:
q Select(L)
6:
QExpand Expand(q, H)
7:
depth(q) = h 1
8:
{ QExpand contains fully specified joint policies, interested best one }
9:
h, vi BestJointPolicyAndValue(QExpand )
10:
v > vGM AA
11:

{found new best joint policy}
12:
vGM AA v
13:
L.Prune(vGM AA )
{(optionally) prune open list}
14:
end
15:
else


{add expanded children open list}
16:
L.insert( q QExpand | q .v > vGM AA )
17:
end
18:
PostProcessNode(q, L)
19: L empty
20: return

Algorithm 2 Select(L): Return highest ranked node open list.
Input: open list L, total order nodes <
Output: highest ranked node q
1: q q L s.t. q L (q 6= q = q < q)
2: return q

Expand operator constructs QExpand , set child nodes. is, given node
contains partial joint policy = ( 0 , 1 , . . . , t1 ), constructs t+1 , set
t+1 = ( 0 , 1 , . . . , t1 , ), appending possible joint decision rules next time
step t. t+1 , heuristic value computed node constructed.
expansion, algorithm checks (line 7) expansion resulted fully specified
joint policies. not, children sufficient heuristic value placed open list
Algorithm 3 Expand(q, H). expand operator plain MAA*.
Input: q = ht , vi search node expand, H admissible heuristic.
Output: QExpand set containing expanded child nodes.
1: QExpand {}
2: t+1 {t+1 | t+1 = (t , )}
3: t+1 t+1
4:
Vb (t+1 ) V 0...t (t+1 ) + H(t+1 )
5:
q ht+1 , Vb (t+1 )i
6:
QExpand .Insert(q )
7: end
8: return QExpand

458

{create child node}

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

Algorithm 4 PostProcessNode(q,L)
Input: q expanded parent node, L open list.
Output: expanded node removed.
1: L.Pop(q)

(line 16). children fully specified, BestJointPolicyAndValue returns best
joint policy (and value) QExpand (see Algorithm 12 Appendix A.1 details
BestJointPolicyAndValue). GMAA* maintains lower bound vGM AA corresponds actual value best fully-specified joint policy found far. newly
found joint policy higher value lower bound updated (lines 11 12). Also,
nodes partial joint policies t+1 upper bound lower best solution
far, Vb (t+1 ) < vGM AA , pruned (line 13). pruning takes additional time,
save memory. Finally, PostProcessNode simply removes parent node open
list (this procedure augmented incremental expansion Section 4). search ends
list becomes empty, point optimal joint policy found.
GMAA* complete, i.e., search finds solution. Therefore, theory,
GMAA* guaranteed eventually produce optimal joint policy (Szer et al., 2005).
However, practice, often infeasible larger problems. major source complexity
full expansion search node. number joint decision rules stage
form children node depth search tree6



|A |n(|O | ) ,
(2.5)
doubly exponential t. Comparing (2.1) (2.5), see worst case
complexity expanding node deepest level tree = h 1 comparable
brute force search entire Dec-POMDP. Consequently, Seuken Zilberstein
(2008) conclude MAA* best solve problems whose horizon 1 greater
already solved nave brute force search.
2.1.2 Bayesian Game Perspective
GMAA* makes possible interpret MAA* solution collection collaborative
Bayesian games (CBGs). employ approach throughout article, facilitates
improvements GMAA* introduce, results significant advances
state-of-the-art Dec-POMDP solutions.
Bayesian game (BG) models one-shot interaction number agents.
extension well-known strategic game (also known normal form game)
agent holds private information (Osborne & Rubinstein, 1994). CBG BG
agents receive identical payoffs. Bayesian game perspective, node q
GMAA* search tree, along corresponding partial joint policy , defines
CBG (Oliehoek, Spaan, & Vlassis, 2008). is, given state distribution b0 , ,
possible construct CBG B(b0 ,t ) represents decision-making problem
stage given followed first stages starting b0 . clear
b0 is, simply write B(t ).
6. follow convention root depth 0.

459

fiOliehoek, Spaan, Amato, & Whiteson

Definition 6. collaborative Bayesian game (CBG) B(b0 ,t ) = hD, A, , Pr(), ui modeling
stage Dec-POMDP, given initial state distribution b0 past joint policy , consists
of:
D, set agents {1 . . . n},
A, set joint actions,
, set joint types, specifies type agent =
h1 , . . . ,n i,
Pr(), probability distribution joint types,
u, (heuristic) payoff function mapping joint type action real number: u(,a).
Bayesian game, type agent represents private information holds.
instance, Bayesian game modeling job recruitment scenario, type agent
may indicate whether agent hard worker. CBG Dec-POMDP, agents
private information individual AOH. Therefore, type agent corresponds
~it , history actions observations: ~it . Similarly, joint type corresponds
joint AOH: ~ .
Consequently, u provide (heuristic) estimate long-term payoff
(~ ,a)-pair. words, payoff function corresponds heuristic Q-value: u(,a)
b ~ ,a). discuss compute heuristics Section 2.2. Given , b0 ,
Q(
correspondence joint types AOHs, probability distribution joint types is:
Pr() , Pr(~ |b0 ,t ),

(2.6)

latter probability marginal Pr(s,~ |b0 ,t ) defined (A.2) used
computation value partial joint policy V 0...t1 (t ) Appendix A.3. Note due
correspondence types AOHs, size CBG B(b0 ,t ) stage
exponential t.
CBG, agent uses Bayesian game policy maps individual types actions:
(i ) = ai . correspondence types AOHs, (joint) policy
CBG corresponds (joint) decision rule: . remainder article,
assume deterministic past joint policies , implies one ~ non-zero
probability given observation history ~o . Thus, effectively maps observation histories
actions. number B(b0 ,t ) given (2.5). value joint CBG
policy CBG B(b0 ,t ) is:
X
b ~ ,(~ )),
Pr(~ |b0 ,t )Q(
(2.7)
Vb () =
~


(~ ) = hi (~it )ii=1...n denotes joint action results application
individual CBG-policies individual AOH ~it specified ~ .
Example 3. Consider CBG Dec-Tiger given past joint policy 2 specifies listen first two stages. stage = 2, agent four possible observation histories:
~ 2 = {(oHL ,oHL ), (oHL ,oHR ), (oHR ,oHL ), (oHR ,oHR )} correspond directly possible types.


probabilities joint types given 2 listed Fig. 4a. Since joint OHs together
2 determine joint AOHs, correspond so-called joint beliefs: probability distributions
states (introduced formally Section 2.2). Fig. 4b shows joint beliefs, serve
basis heuristic payoff function (as discussed Section 2.2).
460

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

~o12
(oHL ,oHL )
(oHL ,oHR )
(oHR ,oHL )
(oHR ,oHR )

~o22
(oHL ,oHL )
0.261
0.047
0.047
0.016

(oHL ,oHR )
0.047
0.016
0.016
0.047

(oHR ,oHL )
0.047
0.016
0.016
0.047

(oHR ,oHR )
0.016
0.047
0.047
0.261

(a) joint type probabilities.

~o12
(oHL ,oHL )
(oHL ,oHR )
(oHR ,oHL )
(oHR ,oHR )

~o22
(oHL ,oHL )
0.999
0.970
0.970
0.5

(oHL ,oHR )
0.970
0.5
0.5
0.030

(oHR ,oHL )
0.970
0.5
0.5
0.030

(oHR ,oHR )
0.5
0.030
0.030
0.001

(b) induced joint beliefs. Listed probability Pr(sl |~ 2 ,b0 )
tiger behind left door.

Figure 4: Illustration Dec-Tiger problem past joint policy 2 specifies
listen actions first two stages.

Algorithm 5 Expand-CBG(q, H). expand operator GMAA* makes use CBGs.
Input: q = ht , vi search node expand.
b ~,a).
Input: H admissible heuristic form Q(
Output: QExpand set containing expanded child nodes.
b
1: B(b0 ,t ) ConstructBG(b0 ,t , Q)
2: QExpand GenerateAllChildrenForCBG(B(b0 ,t ))
3: return QExpand

{as explained Section 2.1.2}

solution CBG maximizes (2.7). CBG equivalent team
decision process finding solution NP-complete (Tsitsiklis & Athans, 1985). However,
Bayesian game perspective GMAA*, illustrated Fig. 3b, issue solving
CBG (i.e., finding highest payoff ) relevant need expand .
is, Expand operator enumerates appends form set
extended joint policies


t+1 = (t , ) | joint CBG policy B(b0 ,t )
uses set construct QExpand , set child nodes. heuristic value
child node q QExpand specifies t+1 = (t , ) given
Vb (t+1 ) = V 0...t1 (t ) + Vb ().

(2.8)

Expand operator makes use CBGs summarized Algorithm 5, uses
GenerateAllChildrenForCBG subroutine (Algorithm 13 Appendix A.1). Fig. 3b illustrates
Bayesian game perspective GMAA*.
461

fiOliehoek, Spaan, Amato, & Whiteson

2.2 Heuristics
perform heuristic search, GMAA* defines heuristic value Vb (t ) using (2.3). contrast, Bayesian game perspective uses (2.8). two formulations equivalent
b faithfully represents expected immediate reward (Oliehoek, Spaan, & Vlasthe heuristic Q
sis, 2008). consequence GMAA* via CBGs complete (and thus finds optimal
solutions) stated following theorem.
Theorem 1. using heuristic form
b ~ ,a) = Est [R(st ,a) | ~ ] + E~ t+1 [Vb (~ t+1 ) | ~ , a],
Q(


(2.9)

Vb (~ t+1 ) Q (~ t+1 , (~ t+1 )) overestimation value optimal joint
policy , GMAA* via CBGs complete.
Proof. See appendix.
theorem, Q (~ ,a) Q-value, i.e., expected future cumulative reward
performing ~ joint policy (Oliehoek, Spaan,P
& Vlassis, 2008). expectation

~
immediate reward written R( ,a) = sS R(s,a) Pr(s|~ ,b0 ).
computed using Pr(s|~ ,b0 ), quantity refer joint belief resulting ~
denote b. joint belief computed via repeated application
Bayes rule (Kaelbling, Littman, & Cassandra, 1998), conditional (A.2).
rest subsection reviews several heuristics used GMAA*.
2.2.1 QMDP
b ~,a) solve underlying MDP, i.e.,
One way obtain admissible heuristic Q(
assume joint action chosen single puppeteer agent observe true
state. approach, known QMDP (Littman, Cassandra, & Kaelbling, 1995), uses

MDP value function Qt,
(s ,a), computed using standard dynamic programming

b ~t
techniques (Puterman, 1994). order transform Qt,
(s ,a)-values QM ( ,a)-values,
compute:
X t,
b (~ ,a) =
(2.10)
Q
Q (s,a) Pr(s|~ ,b0 ).


sS

Solving underlying MDP time complexity linear h, makes it,
especially compared Dec-POMDP, easy compute. addition, necessary
store value (s,a)-pair, stage t. However, bound provides
optimal Dec-POMDP Q -value function loose (Oliehoek & Vlassis, 2007).
2.2.2 QPOMDP
Similar underlying MDP, one define underlying POMDP Dec-POMDP,
i.e., assuming joint action chosen single agent access joint observation.7
7. Alternatively one view POMDP multiagent POMDP agents instantaneously
broadcast private observations.

462

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

Tree

Vector

t=0
t=1
t=2
t=3
Figure 5: Visual comparison tree vector-based Q representations.

resulting solution used heuristic, called QPOMDP (Szer et al., 2005; Roth,
Simmons, & Veloso, 2005). optimal QPOMDP value function satisfies:

QP (bt , a) = R(bt ,a) +

X

P (ot+1 |bt ,a) max QP (bt+1 ,at+1 ),
at+1

ot+1

(2.11)

P
bt joint belief, R(bt ,a) = sS R(s,a)bt (s) immediate reward, bt+1
joint belief resulting bt action joint observation ot+1 . use QPOMDP ,
b P (~ ,a) , Qt (b~t ,a).
~ , directly use value induced joint belief: Q
P

two approaches computing QPOMDP . One construct belief MDP
tree joint beliefs, illustrated Fig. 5 (left). Starting b0 (corresponding
empty joint AOH ~ 0 ), compute resulting ~ 1 corresponding
~1
belief b continue recursively. Given tree, possible compute values
nodes standard dynamic programming.
Another possibility apply vector-based POMDP techniques (see Fig. 5 (right)).
Q-value function stage QtP (b,a) represented using set vectors joint
} (Kaelbling et al., 1998). Qt (b,a) defined maximum
action V = {V1t , . . . ,V|A|
P
inner product:
QtP (b,a) , max b vat .
V
va


Given V h1 , vector representation last stage, compute V h2 , etc. order
limit growth number vectors, dominated vectors pruned.
Since QMDP upper bound POMDP value function (Hauskrecht, 2000), QPOMDP
provides tighter upper bound Q QMDP . However, costly compute
store: tree-based vector-based approach may need store number
values exponential h.
463

fiOliehoek, Spaan, Amato, & Whiteson

2.2.3 QBG
third heuristic, called QBG , assumes agent team access
individual observation communicate 1-step delay.8 define QBG
X
QB (~ ,a) = R(~ ,a) + max
Pr(ot+1 |~ ,a)QB (~ t+1 ,(ot+1 )),
(2.12)


ot+1

t+1
= h1 (ot+1
1 ),...,n (on )i tuple individual policies : Oi Ai CBG

constructed ~ ,a. QPOMDP , QBG represented using vectors (Varaiya &
Walrand, 1978; Hsu & Marcus, 1982; Oliehoek, Spaan, & Vlassis, 2008) two
manners computation (tree vector based) apply. yields tighter heuristic
QPOMDP , computation additional exponential dependence maximum
number individual observations (Oliehoek, Spaan, & Vlassis, 2008), particularly
troubling vector-based computation, since precludes effective application incremental pruning (A. Cassandra, Littman, & Zhang, 1997). overcome problem, Oliehoek
Spaan (2012) introduce novel tree-based pruning methods.

3. Clustering
GMAA* solves Dec-POMDPs repeatedly constructing CBGs expanding joint
BG policies them. However, number equal number regular
MAA* child nodes given (2.5) thus grows doubly exponentially horizon h.
section, propose new approach improving scalability respect h
clustering individual AOHs. reduces number therefore number
constructed child nodes GMAA* search tree.9
Previous research investigated clustering: Emery-Montemerlo, Gordon,
Schneider, Thrun (2005) propose clustering types based profiles payoff
functions CBGs. However, resulting method ad hoc. Even given bounds
error clustering two types CBG, guarantees made quality
Dec-POMDP solution, bound respect heuristic payoff function.
contrast, propose cluster histories based probability histories induce
histories agents states. critical advantage criterion,
call probabilistic equivalence (PE), resulting clustering lossless:
solution clustered CBG used construct solution original CBG
values two CBGs identical. Thus, criterion allows clustering AOHs
CBGs represent Dec-POMDPs preserving optimality.10
Section 3.1, describe histories Dec-POMDPs clustered using
notions probabilistic best-response equivalence. allows histories clustered
8. name QBG stems fact 1-step delayed communication scenario modeled
CBG. Note, however, CBGs used compute QBG different form B(b0 ,t )
discussed Section 2.1.2: latter, types correspond length-t (action-) observation histories;
former, types correspond length-1 observation histories.
9. CBGs essential clustering, provide convenient level abstraction simplifies
exposition techniques. Moreover, level abstraction makes possible employ results
concerning CBGs outside context Dec-POMDPs.
10. probabilistic equivalence criterion lossless clustering introduced Oliehoek et al. (2009).
article presents new, simpler proof optimality clustering based PE.

464

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

rational always choose action. Section 3.2, describe application
results GMAA*. Section 3.3 introduces improved heuristic representations
allow computation longer horizons.
3.1 Lossless Clustering Dec-POMDPs
section, discuss lossless clustering based notion probabilistic equivalence.
show clustering lossless demonstrating probabilistic equivalence implies
best response equivalence, describes conditions rational agent select
action two types. prove implication, show best response
depends multiagent belief (i.e., probability distribution states policies
agents), two probabilistically equivalent histories. Relations
equivalence notions discussed Section 6.
3.1.1 Probabilistic Equivalence Criterion
first introduce probabilistic equivalence criterion, used decide whether
two individual histories ~ia ,~ib clustered without loss value.
Criterion 1 (Probabilistic Equivalence). Two AOHs ~ia ,~ib agent probabilistically
equivalent (PE), written P E(~ia ,~ib ), following holds:
~6=i

Pr(s,~
6=i |~ia ) = Pr(s,~
6=i |~ib ).

(3.1)

probabilities computed conditional Pr(s,~ |b0 ,t ), defined (A.2).
subsections 3.1.23.1.4, formally prove PE sufficient criterion guarantee
clustering lossless. remainder Section 3.1.1 discuss key properties
PE criterion order build intuition.
Note criterion decomposed following two criteria:
~6=i
~6=i

Pr(~
6=i |~ia ) = Pr(~
6=i |~ib ),

(3.2)

Pr(s|~
6=i ,~ia ) = Pr(s|~
6=i ,~ib ).

(3.3)

criteria give natural interpretation: first says probability distribution
agents AOHs must identical ~ia ~ib . second demands
resulting joint beliefs identical.
probabilities well defined without initial state distribution b0
past joint policy . However, since consider clustering histories within particular CBG
(for stage t) constructed particular b0 ,t , implicitly specified. Therefore
drop arguments, clarifying notation.
Example 4. Example 3, types (oHL ,oHR ) (oHR ,oHL ) agent PE. see this, note
rows (columns second agent) histories identical Fig. 4a
Fig. 4b. Thus, specify distribution histories agents (cf. equation (3.2))
induced joint beliefs (cf. equation (3.3)).

Probabilistic equivalence convenient property algorithms exploit: holds
particular pair histories, hold identical extensions
histories, i.e., propagates forwards regardless policies agents.
465

fiOliehoek, Spaan, Amato, & Whiteson

Definition 7 (Identical extensions). Given two AOHs ~ia,t ,~ib,t , respective extensions
~ a,t+1 = (~ a,t ,ai ,oi ) ~ b,t+1 = (~ b,t ,a ,o ) called identical extensions






ai = ai oi = oi .







Lemma 1 (Propagation PE). Given ~ia,t ,~ib,t PE, regardless decision rule
agents use ( t6=i ), identical extensions PE:
ati ot+1 st+1 ~t+1


6=i

6=i

t+1

t+1 ~ t+1 ~ b,t t+1
, 6=i |i ,ai ,oi , 6=i ) (3.4)
Pr(st+1 ,~
6=i |~ia,t ,ati ,ot+1
, 6=i ) = Pr(s

Proof. proof listed appendix, holds intuitively probabilities
described before, taking action
seeing observation.
Note that, probabilities defined (3.1) superficially resemble beliefs used
POMDPs, substantially different. POMDP, single agent compute
individual belief using AOH. use belief determine value
future policy, sufficient statistic history predict future rewards
(Kaelbling et al., 1998; Bertsekas, 2005). Thus, trivial show equivalence AOHs
induce individual belief POMDP. Unfortunately, Dec-POMDPs
problematic. next section elaborates issue discussing relation multiagent
beliefs.
3.1.2 Sub-Tree Policies, Multiagent Beliefs Expected Future Value
describe relationship multiagent beliefs probabilistic equivalence,
must first discuss policies agent may follow resulting values. begin
introducing concept sub-tree policies. illustrated Fig. 2 (on page 456),
(deterministic) policy represented tree nodes labeled using actions
edges labeled using observations: root node corresponds first action taken,
nodes specify action observation history encoded path root node.
such, possible define sub-tree policies, , correspond sub-trees agent
policy (also illustrated Fig. 2). particular, write
w
~o = =ht
(3.5)


sub-tree policy corresponding w
observation history ~oit specifies actions
last = h stages. refer policy consumption operator, since
w
consumes part policy corresponding ~oit . Similarly write =k ~o l = =kl

(note (3.5), = h-steps-to-go sub-tree policy) use similar notation,
=k , joint sub-tree policies. extensive treatment different forms
policy, refer discussion Oliehoek (2012).
Given concepts, define value = k-stages-to-go joint policy starting
state s:
XX
w
Pr(s ,o|s,a)V (s , =k ).
(3.6)
V (s, =k ) = R(s,a) +




Here, joint action specified roots individual sub-tree policies specified
=k stage = h k.
466

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

definition, follows directly probability distribution states
sub-tree policies agents 6=i sufficient predict value sub-tree policy .
fact, distribution known multiagent belief bi (s, 6=i ) (Hansen, Bernstein, &
Zilberstein, 2004). value given
XX
V (bi ) = max
bi (s, 6=i )V (s,hi , 6=i i),
(3.7)




6=i

refer maximizing agent best response bi . illustrates
multiagent belief sufficient statistic: contains sufficient information predict value
sub-tree policy .
possible connect action observation histories multiagent beliefs fixing
policies agents. Given agents act according profile
policies 6=i , agent multiagent belief first stage Dec-POMDP: bi (s, 6=i ) =
b0 (s). Moreover, agent maintain multiagent belief execution. such,
given 6=i , history ~i induces multiagent belief, write bi (s, 6=i |~i , 6=i )
make dependence ~i , 6=i explicit. multiagent belief history defined
bi (s, 6=i |~i , 6=i ) , Pr(s, 6=i |~i , b0 , 6=i ),

(3.8)

induces best response via (3.7):
BR(~i | 6=i ) , arg max


XX


bi (s, 6=i |~i , 6=i )V (s, 6=i ,i ).

(3.9)

6=i

conclude two AOHs ~ia ,~ib clustered together induce
multiagent belief.
However, notion multiagent belief clearly quite different distributions
used notion PE. particular, establish whether two AOHs induce
multiagent belief, need full specification 6=i . Nevertheless, show two AOHs
PE best response equivalent therefore cluster them.
crux show that, Criterion 1 satisfied, AOHs always induce
multiagent beliefs 6=i (consistent current past joint policy 6=i ).
3.1.3 Best-Response Equivalence Allows Lossless Clustering Histories
relate probabilistic equivalence multiagent belief follows.
Lemma 2 (PE implies multiagent belief equivalence). 6=i , probabilistic equivalence
implies multiagent belief equivalence:


P E(~ia ,~ib ) s,6=i bi (s, 6=i |~ia , 6=i ) = bi (s, 6=i |~ib , 6=i )
(3.10)
Proof. See appendix.
lemma shows two AOHs PE, produce multiagent belief.
Intuitively, gives us justification cluster AOHs together: since multiagent
belief sufficient statistic act multiagent belief,
since Lemma 2 shows ~ia ,~ib induces multiagent beliefs 6=i
PE, conclude always act histories. Formally,
prove ~ia ,~ib best-response equivalent PE.
467

fiOliehoek, Spaan, Amato, & Whiteson

Theorem 2 (PE implies best-response equivalence). Probabilistic equivalence implies bestresponse equivalence.


P E(~ia ,~ib ) 6=i BR(~ia | 6=i ) = BR(~ib | 6=i )
Proof. Assume arbitrary 6=i ,
BR(~ia | 6=i ) = arg max

XX

bi (s, 6=i |~ia )V (s, 6=i ,i )

= arg max

XX

bi (s, 6=i |~ib )V (s, 6=i ,i ) = BR(~ib | 6=i ),









6=i

6=i

Lemma 2 employed assert equality bi (|~ia ) bi (|~ib ).
theorem key demonstrates two AOHs ~ia ,~ib agent
PE, agent need discriminate future. Thus,
searching space joint policies, restrict search assign
sub-tree policy ~ia ~ib . such, directly provides intuition lossless
clustering possible. Formally, define clustered joint policy space follows.
Definition 8 (Clustered joint policy space). Let C subset joint policies
clustered: i.e., part C assigns sub-tree policy action
observation histories probabilistically equivalent.
Corollary 1 (Existence optimal clustered joint policy). exists optimal joint
policy clustered joint policy space:
max V () = max V ()

C



(3.11)

Proof. clear left hand side (3.11) upper bounded right hand side,
since C . suppose = arg max V () strictly higher value
best clustered joint policy. least one agent one pair PE histories ~ia , ~ib , must
assign different sub-tree policies ia 6= ib (otherwise would clustered). Without loss
generality assume one pair. follows directly Theorem 2
policy construct clustered policy C C (by assigning either ia ib
~ia , ~ib ) guaranteed value less , thereby contradicting
assumption strictly higher value best clustered joint policy.
formally proves restrict search C , space clustered joint
policies, without sacrificing optimality.
3.1.4 Clustering Commitment CBGs
Though clear two AOHs PE clustered, making result
operational requires additional step. end, use abstraction layer provided
Bayesian games. Recall CBG stage, AOHs correspond types.
468

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

Therefore, want cluster types CBG. accomplish clustering two
types ia ,ib , introduce new type ic replace them, defining:
6=i Pr(ic , 6=i ) , Pr(ia , 6=i ) + Pr(ib , 6=i )
j

u(hic , 6=i ,a)




Pr(ia , 6=i )u(hia , 6=i ,a) + Pr(ib , 6=i )u( ib , 6=i ,a)
.
,
Pr(ia , 6=i ) + Pr(ib , 6=i )

(3.12)
(3.13)

Theorem 3 (Reduction commitment). Given agent collaborative Bayesian
game B committed selecting policy assigns action two types
ia ,ib , i.e., selecting policy (ia ) = (ib ), CBG reduced without
loss value agents. is, result new CBG B agent employs
policy reflects clustering whose expected payoff original

CBG: V B (i , 6=i ) = V B (i , 6=i ).
Proof. See appendix.
theorem shows that, given agent committed taking action
types ia ,ib , reduce collaborative Bayesian game B smaller one B

translate joint CBG-policy found
B back joint CBG-policy B.
necessarily mean = , 6=i solution B, best-response
agent 6=i may select action ia ,ib . Rather best-response
6=i given action needs taken ia ,ib .11
Even though Theorem 3 gives conditional statement depends agent
committed select action two types, previous subsection discussed
rational agent make commitment. Combining results gives
following corollary.
Corollary 2 (Lossless Clustering PE). Probabilistically equivalent histories ~ia ,~ib
clustered without loss heuristic value merging single type CBG.
Proof. Theorem 3 shows that, given agent committed take action
two types, types clustered without loss value. Since ~ia ,~ib PE,
best-response equivalent, means agent committed use
sub-tree policy hence action ai . Therefore directly apply clustering
without loss expected payoff, CBG stage Dec-POMDP means loss
expected heuristic value given (2.7).
Intuitively, maximizing action ~ia ~ib regardless (future)
joint policies 6=i agents use hence cluster without loss
heuristic value. Note depend heuristic used hence
holds optimal heuristic (i.e., using optimal Q-value function gives
true value). directly relates probabilistic equivalence equivalence optimal value.12
11. Although focus CBGs, results generalize BGs individual payoff functions. Thus,
could potentially exploited algorithms general-payoff BGs. Developing methods
interesting avenue future work.
12. proof originally provided Oliehoek et al. (2009) based showing histories PE
induce identical Q-values.

469

fiOliehoek, Spaan, Amato, & Whiteson

Algorithm 6 ClusterCBG(B)
Input: CBG B
Output: Losslessly clustered CBG B
1: agent
2:
individual type B.i
3:
Pr(i ) = 0
4:
B.i B.i \i
5:
continue
6:
end
7:
individual type B.i
8:
isProbabilisticallyEquivalent true
9:
hs, 6=i
10:
Pr(s, 6=i |i ) 6= Pr(s, 6=i |i )
11:
isProbabilisticallyEquivalent false
12:
break
13:
end
14:
end
15:
isProbabilisticallyEquivalent
16:
B.i B.i \i
17:

18:
6=i
19:
u(i , 6=i ,a) min(u(i , 6=i ,a),u(i , 6=i ,a))
20:
Pr(i , 6=i ) Pr(i , 6=i ) + Pr(i , 6=i )
21:
Pr(i , 6=i ) 0
22:
end
23:
end
24:
end
25:
end
26:
end
27: end
28: return B

{Prune B:}

{Prune B:}
{ take lowest upper bound }

Note result establishes sufficient, necessary condition lossless clustering.
particular, given policies agents, many types best-response equivalent
clustered. However, far know, criterion must hold order guarantee
two histories best-response policy agents.
3.2 GMAA* Incremental Clustering
Knowing individual histories clustered together without loss value
potential speed many Dec-POMDP methods. article, focus application
within GMAA* framework.
Emery-Montemerlo et al. (2005) showed clustering incorporated every stage
algorithm: CBG stage constructed, clustering individual
histories (types) performed first afterwards (reduced) CBG solved.
approach employed within GMAA* modifying Expand procedure (Algorithm 5)
cluster CBG calling GenerateAllChildrenForCBG.
Algorithm 6 shows clustering algorithm. takes input CBG returns
clustered CBG. performs clustering performing pairwise comparison types
470

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

b
Algorithm 7 ConstructExtendedBG(B, t1 , Q)

Input: CBG B stage 1, joint BG policy followed t1 .
b ~,a).
Input: admissible heuristic form Q(

Output: CBG B stage t.
1: B B
{make copy B subsequently alter}
2: agent
3:
B .i = ConstructExtendedTypeSet(i)
{overwrite individual type sets}
4: end
5: B . iD
{the new joint type set (does explicitly stored)}
6: joint type = ( t1 ,at1 ,ot ) B .
7:
state st
8:
Compute Pr(st |)
{from Pr(st1 | t1 ) via Bayes rule }
9:
end
10:
Pr() Pr(ot | t1 ,at1 ) Pr( t1 )
11:

12:
q
13:
history ~ represented
b ~ ,a))
b take lowest upper bound }
14:
q min(q,Q(
{ Q Q
15:
end
16:
B .u(,a) q
17:
end
18: end
19: return B

agent see satisfy criterion, yielding O(|i |2 ) comparisons agent i.
comparison involves looping hs, 6=i (line 9). many states, efficiency
could gained first checking (3.2) checking (3.3). Rather taking
average (3.13), line 19 take lowest payoff, done using
upper bound heuristic values.
following theorem demonstrates that, incorporating clustering GMAA*,
resulting algorithm still guaranteed find optimal solution.
Theorem 4. using heuristic form (2.9) clustering CBGs GMAA*
using PE criterion, resulting search method complete.
Proof. Applying clustering alter computation lower bound values. Also,
heuristic values computed expanded nodes admissible fact unaltered
guaranteed Corollary 2. Therefore, difference regular GMAA*
class considered joint policies restricted C , class clustered joint policies:
possible child nodes expanded, clustering effectively prunes away policies
would specify different actions AOHs PE thus clustered. However, Corollary 1
guarantees exists optimal joint policy restricted class.
modification Expand proposed rather naive. construct B(b0 ,t )
must first construct |Oi |t possible AOHs agent (given past policy ti ).
subsequent clustering involves pairwise comparison exponentially many types.
Clearly, tractable later stages.
However, PE AOHs propagates forwards (i.e., identical extensions PE histories PE), efficient approach possible. Instead clustering exponentially
471

fiOliehoek, Spaan, Amato, & Whiteson

Algorithm 8 Expand-IC(q, H). expand operator GMAA*-IC.
Input: q = ht , vi search node expand.
b ~,a).
Input: H admissible heuristic form Q(
Output: QExpand set containing expanded child nodes.
1: B(t1 ) t1 .CBG
{retrieve previous CBG, note = (t1 , t1 )}
t1 b

t1
2: B( ) ConstructExtendedBG(B(
), , Q)
3: B(t ) ClusterBG(B(t ))
4: .CBG B(t )
{store pointer CBG}
5: QExpand GenerateAllChildrenForCBG(B(t ))
6: return QExpand

growing set types, simply extend already clustered types previous stages
CBG, shown Algorithm 7. is, given , set types agent previous
stage 1, it1 policy agent took stage, set types stage t, ,
constructed


= = (i ,it1 (i ),oti ) | ,oti Oi .
(3.14)
means size newly constructed set |i | = |i | |Oi | . type set
previous stage 1 much smaller set histories |i | |Oi |t1 ,
new type set much smaller: |i | |Oi |t . way, bootstrap clustering
stage spend significantly less time clustering. refer algorithm
implements type clustering GMAA* Incremental Clustering (GMAA*-IC).
approach possible perform exact, value-preserving clustering
Lemma 1 guarantees identical extensions clustered without loss
value. performing procedure lossy clustering scheme (e.g., EmeryMontemerlo et al., 2005), errors might accumulate, better option might re-cluster
scratch every stage.
Expansion GMAA*-IC node takes exponential time respect number
agents types, O(|A |n| | ) joint CBG-policies thus child nodes
GMAA*-IC search tree (A largest action set largest type set). Clustering
involves pairwise comparison types agent comparisons needs
check O(| |n1 |S|) numbers equality verify (3.1). total cost clustering
therefore written
O(n | |2 | |n1 |S|),
polynomial number types. clustering decreases number
types | |, therefore significantly reduce number child nodes thereby
overall time needed. However, clustering possible, overhead incurred.
3.3 Improved Heuristic Representation
Since clustering reduce number types, GMAA*-IC potential scale
larger horizons. However, important consequences computation
heuristics. Previous research shown upper bound provided QMDP often
loose effective heuristic search (Oliehoek, Spaan, & Vlassis, 2008). However,
space needed store tighter heuristics QPOMDP QBG grows exponentially
horizon. Recall Section 2.2.2 (see Fig. 5) two approaches computing
472

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

b minimum size.
Algorithm 9 Compute Hybrid Q
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:

Qh1 {R1 , . . . ,R|A| }
z |A| |S|
= h 2 0
~ | |A|
|
z <
V VectorBackup(Qt+1 )
V Prune(V)
Qt V
z |V | |S|
end
z
Qt TreeBackup(Qt+1 )
end
end

{vector representation last stage}
{the size |A| vectors}
{size AOH representation}

{From z y}

QPOMDP QBG . first constructs tree joint AOHs heuristic values,
simple implement requires storing value (~ , a)-pair, number
grows exponentially t. second approach maintains vector-based representation,
common POMDPs. Though pruning provide leverage, worst case, pruning
possible number maintained vectors grows doubly exponentially h t,
number stages-to-go. Similarly, initial belief subsequently reachable beliefs
used reduce number vectors retained stage, number reachable
beliefs exponential horizon exponential complexity remains.
Oliehoek, Spaan, Vlassis (2008) used tree-based representation QPOMDP QBG heuristics. Since
computational cost solving Dec-POMDP bottleneck, inefficiencies representation could overlooked. However, approach longer feasible
longer horizons made possible GMAA*-IC.

Hybrid

t=0

mitigate problem, propose hybrid represent=1
tation heuristics, illustrated Fig. 6. main
insight exponential growth two existing representations occurs opposite directions. Therefore,
t=2
use low space-complexity side representations:
later stages, fewer vectors, use vector-based representation, earlier stages, fewer histot=3
ries, use history-based representation. similar
idea utilizing reachable beliefs reduce size Figure 6: illustration
vector representation described but, rather stor- hybrid representation.
ing vectors appropriate AOHs step,
values needed using tree-based representation.
Algorithm 9 shows how, mild assumptions, minimally-sized representation
computed. Starting last stage, algorithm performs vector backups, switching
tree backups become smaller option. last time step h 1, represent
473

fiOliehoek, Spaan, Amato, & Whiteson

Qt set immediate reward vectors13 , variable z (initialized line 2) keeps track
number parameters needed represent Qt vectors time step hand.
Note z depends effective vector pruning is, i.e., large parsimonious
representation piecewise linear convex value function is. Since problem
dependent, z updated pruning actually performed (line 9).
contrast y, number parameters tree representation, computed directly
Dec-POMDP (line 4). z > y, algorithm switches tree backups.14

4. Incremental Expansion
clustering technique presented previous section potential significantly
speed planning much clustering possible. However, little clustering possible,
number children GMAA* search tree still grow super-exponentially. section
presents incremental expansion, complementary technique deal problem.
Incremental expansion exploits recent improvements effectively solving CBGs. First
note expansion last stage = h 1 particular h1 ,
interested best child (h1 , h1, ), corresponds optimal solution
Bayesian game h1, . such, last stage, use new methods solving
CBGs (Kumar & Zilberstein, 2010b; Oliehoek, Spaan, Dibangoye, & Amato, 2010)
provide speedups multiple orders magnitude brute force search (enumeration).15
Unfortunately, improvements GMAA* afforded approach limited: order
guarantee optimality, still relies expansion (child nodes corresponding all)
joint CBG-policies intermediate stages, thus necessitating brute-force approach.
However, many expanded child nodes may low heuristic values Vb may therefore
never selected expansion.
Incremental expansion overcomes problem exploits following key observation: generate children decreasing heuristic order using admissible
heuristic, expand children. before, A* search performed
partially specified policies new CBG constructed extending CBG
parent node. However, rather fully expanding (i.e., enumerating CBG policies
thereby constructing children for) search node, instantiate incremental
CBG solver corresponding CBG. incremental solver returns one joint CBG
policy time, used construct single child t+1 = (t , ). revisiting
nodes, promising child nodes expanded incrementally.
Below, describe GMAA*-ICE, algorithm combines GMAA*-IC incremental expansion. establish theoretical guarantees describe modifications
BaGaBaB, CBG solver GMAA*-ICE employs, necessary deliver
child nodes decreasing order.
13. exceptional cases short horizon combined large state action spaces representing last time step vectors minimal. cases, algorithm trivially adapted.
14. assumes vector representation shrink earlier stages. Although unlikely
practice, cases would prevent algorithm computing minimal representation.
15. Kumar Zilberstein (2010b) tackle slightly different problem; introduce weighted constraint satisfaction approach solving point-based backup dynamic programming Dec-POMDPs. However,
point-based backup interpreted collection CBGs (Oliehoek et al., 2010).

474

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

4.1 GMAA* Incremental Clustering Expansion
begin formalizing incremental expansion incorporating GMAA*-IC, yielding GMAA* incremental clustering expansion (GMAA*-ICE). core
incremental expansion lies following lemma:
Lemma 3. Given two joint CBG policies , CBG B(b0 ,t ), Vb () Vb ( ),
corresponding child nodes Vb (t+1 ) Vb (t+1 ).
Proof. holds directly definition Vb (t ) given (2.8):
Vb (t+1 ) = V 0...(t1) (t ) + Vb ()
V 0...(t1) (t ) + Vb ( ) = Vb (t+1 ).

follows directly that, B(b0 ,t ) use CBG solver generate sequence
policies , , . . .
Vb () Vb ( ) . . .

then, sequence corresponding children

Vb (t+1 ) Vb (t+1 ) . . . .

Exploiting knowledge, expand first child t+1 compute heuristic
value Vb (t+1 ) using (2.8). Since unexpanded siblings heuristic values less
equal that, modify GMAA*-IC reinsert node q open list L
act placeholder non-expanded children.
Definition 9. placeholder node least one child expanded.
placeholder heuristic value equal last expanded child.
Thus, expansion search node qs child, update q.v, heuristic value
node, Vb (t+1 ), value expanded child, i.e., set q.v Vb (t+1 ). such,
reinsert q L placeholder. mentioned above, correct
unexpanded siblings (for parent node q placeholder) heuristic values
lower equal Vb (t+1 ). Therefore next sibling q represented placeholder
always expanded time: q always created nodes lower heuristic value
selected expansion. keep track whether node previously expanded
placeholder not.
before, GMAA*-ICE performs A* search partially specified policies.
GMAA*-IC, new CBG constructed extending CBG parent node
applying lossless clustering. However, rather expanding children, GMAA*-ICE
requests next solution incremental CBG solver, single child
t+1 = (t , ) constructed. principle GMAA*-ICE use CBG solver able
incrementally deliver descending order Vb (). propose modification
BaGaBaB algorithm (Oliehoek et al., 2010), briefly discussed Section 4.3.
Fig. 7 illustrates process incremental expansion GMAA*-ICE, indexed
letters. First, CBG solver root node ha, 7i created, optimal solution
computed, value 6. results child hb, 6i, root replaced placeholder
node ha, 6i. per Definition 5 (the node comparison operator), b appears
475

fiOliehoek, Spaan, Amato, & Whiteson

Legend:

v




7


6

Root node


6




b
6

t+1

b
4


New B(a), Vb =6
c
4

t+2
ht , vi
open list

ha, 7i


5.5

New B(b), Vb =4

ha, 6i
hc, 4i
hb, 4i

hb, 6i
ha, 6i

b
4

c
4


5.5
Next solution
B(a), Vb =5.5

hd, 5.5i
ha, 5.5i
hc, 4i
hb, 4i

Figure 7: Illustration incremental expansion, nodes open list bottom.
Past joint policies indexed letters. Placeholder nodes indicated dashes.
open list hence selected expansion. best child hc, 4i added hb, 6i replaced
placeholder hb, 4i. search returns root node, second best solution
obtained CBG solver, leading child hd, 5.5i. Placeholder nodes retained
long unexpanded children; values updated.
using GMAA*-ICE, derive lower upper bounds CBG solution,
exploited incremental CBG solver. incremental CBG solver
B(t ) initialized lower bound
vCBG = vGM AA V 0...(t1) (t ),

(4.1)

vGM AA value current best solution, V 0...(t1) (t ) true expected
value first stages. Therefore, vCBG minimum value candidate
must generate remaining h stages order beat current best solution. Note
time incremental CBG solver queried solution, vCBG re-evaluated
(using (4.1)), vGM AA may changed.
used heuristic faitfully represents immediate reward (i.e., form
(2.9)), then, last stage = h 1, specify upper bound solution
CBG
vCBG = Vb (h1 ) V 0...(h2) (h1 ).
(4.2)
upper bound attained, solutions required CBG solver.
upper bound holds since (2.8)
Vb () , Vb (h ) V 0...(h2) (h1 )

= V (h ) V 0...(h2) (h1 )
Vb (h1 ) V 0...(h2) (h1 ).

first step, Vb (h ) = V (h ), h fully specified policy heuristic value
given (2.8) equals actual value heuristic faithfully represents expected
476

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

Algorithm 10 Expand-ICE(q, H). expand operator GMAA*-ICE.
Input: q = ht , vi search node expand.
b ~,a).
Input: H admissible heuristic form Q(
Output: QExpand set containing 0 1 expanded child nodes.
1: IsPlaceholder(q)
2:
B(t ) .CBG
{reuse stored CBG}
3: else
4:
B(t1 ) t1 .CBG
{retrieve previous CBG, note = (t1 , t1 )}
t1
b
5:
B(t ) ConstructExtendedBG(B(t1 ), , Q)


6:
B( ) ClusterBG(B( ))
7:
B(t ).Solver CreateSolver(B(t ))
8:
.CBG B(t )
{store pointer CBG}
9: end
{set lower bound CBG solution}
10: vCBG = vGM AA V 0...(t1) (t )
11: = h 1
12:
vCBG = Vb (h1 ) V 0...(h2) (h1 )
{upper bound used last stage CBG}
13: else
14:
vCBG = +
15: end
b ( )i B(t ).Solver.NextSolution(vCBG ,vCBG )
{compute next CBG solution}
16: h , V

17:
18:
t+1 (t , )
{create partial joint policy}

t+1
0...t1

b
b
19:
V ( ) V
( ) + V ( )
{compute heuristic value}
20:
q ht+1 , Vb (t+1 )i
{create child node}
21:
QExpand {q }
22: else
23:
QExpand
{fully expanded: exists solution s.t. V ( h1 ) vCBG }
24: end
25: return QExpand

Algorithm 11 PostProcessNode-ICE(q, L): Post processing node GMAA*-ICE.
Input: q last expanded node, L open list.
Output: q either removed updated.
1: L.Pop(q)
2: q fully expanded depth(q) = h 1
3:
Cleanup q
{delete node associated CBG Solver}
4:
return
5: else
6:
c last expanded child q
7:
q.v c.v
{update heuristic value parent node}
8:
IsPlaceholder(q) true
{remember q placeholder}
9:
L.Insert(q)
{reinsert appropriate position}
10: end

477

fiOliehoek, Spaan, Amato, & Whiteson

immediate reward used. implies Vb () lower bound. second step
V (h ) Vb (h1 ), Vb (h1 ) admissible. Therefore, stop expanding
find (lower bound) heuristic value equal upper bound vCBG . applies
last stage first step valid.
GMAA*-ICE implemented replacing Expand PostProcessNode
procedures Algorithms 8 4 Algorithms 10 11, respectively. Expand-ICE first
determines placeholder used either reuses previously constructed incremental CBG solver constructs new one. Then, new bounds calculated next
CBG solution obtained. Subsequently, single child node generated (rather
expanding children Algorithm 13). PostProcessNode-ICE removes last node
returned Select children expanded. Otherwise,
updates nodes heuristic value reinserts open list. See Appendix A.2
GMAA*-ICE shown single algorithm.
4.2 Theoretical Guarantees
section, prove GMAA*-IC GMAA*-ICE search-equivalent. direct
result establish GMAA*-ICE complete, means integrating incremental
expansion preserves optimality guarantees GMAA*-IC.
Definition 10. call two GMAA* variants search-equivalent select exactly
sequence non-placeholder nodes corresponding past joint policies expand
search tree using Select operator.
GMAA*-IC GMAA*-ICE show set selected nodes same.
However, set expanded nodes different; fact, precisely differences
incremental expansion exploits.
Theorem 5. GMAA*-ICE GMAA*-IC search-equivalent.
Proof. Proof listed Section A.4 appendix.
Note Theorem 5 imply computational space requirements
GMAA*-ICE GMAA*-IC identical. contrary, expansion,
GMAA*-ICE generates one child node stored open list. contrast,
GMAA*-IC generates number child nodes is, worst case, doubly exponential
depth selected node.16 However, GMAA*-ICE guaranteed
efficient GMAA*-IC. example, case child nodes still
generated, GMAA*-ICE slower due overhead incurs.
Corollary 3. using heuristic form (2.9) GMAA*-ICE complete.
Proof. stated conditions, GMAA*-IC complete (see Theorem 4).
GMAA*-ICE search equivalent GMAA*-IC, complete.

Since

16. problem allows clustering, number child nodes grows less dramatically (see Section 3).

478

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

4.3 Incremental CBG Solvers
Implementing GMAA*-ICE requires CBG solver incrementally deliver
descending order Vb (). end, propose modify Bayesian game Branch
Bound (BaGaBaB) algorithm (Oliehoek et al., 2010). BaGaBaB performs A*-search
(partially specified) CBG policies. Thus, applied within GMAA*-ICE, performs
second, nested A* search. expand node GMAA* search tree, nested A*
search computes next CBG solution.17 section briefly summarizes main ideas
behind BaGaBaB (for information, see Oliehoek et al., 2010) modifications.
BaGaBaB works creating search tree nodes correspond partially
specified joint CBG policies. particular, represents joint action vector, vector
h( 1 ), . . . ,( || )i joint actions specifies joint type. node g
BaGaBaB search tree represents partially specified vector thus partially specified
joint CBG policy. example, completely unspecified vector h, , . . . ,i corresponds
root node, internal node

g depth (root beingff depth 0) specifies joint
actions first joint types g = ( 1 ), . . . , ( ), , , . . . , . value node V (g)
value best joint CBG-policy consistent it. Since value known
advance, BaGaBaB performs A* search guided optimistic heuristic.
particular, compute upper bound value achievable
partially specified vector computing maximum value complete information joint
policy consistent (i.e., non-admissible joint policy selects maximizing
joint actions remaining joint types). Since value guaranteed upper bound
maximum value achievable consistent joint CBG policy, admissible heuristic.
propose modification BaGaBaB allow solutions incrementally delivered.
main idea retain search tree first call BaGaBaB particular CBG
B(t ) update subsequent calls, thereby saving computational effort.
Standard A* search terminates single optimal solution found.
behavior incremental BaGaBaB called first time B(t ).
However, standard A*, nodes whose upper bound lower best known lower
bound safely deleted, never lead optimal solution. contrast,
incremental setting nodes cannot pruned, could possibly result k-th
best solution therefore might need expanded subsequent calls BaGaBaB.
nodes returned solutions pruned order avoid returning solution
twice. modification requires memory affect A* search process
otherwise.
asked k-th solution, BaGaBaB resets internal lower bound value
next-best solution previously found returned (or vCBG defined
(4.1) solution found). starts A* search initialized using search
tree resulting (k 1)-th solution. essence, method similar searching
best k solutions, k incremented demand. Recently shown that,
fixed k, modification preserves theoretical guarantees (soundness, completeness,
17. GMAA*-ICE could use incremental CGB solver, avoid enumerating
providing first result thus potential work incrementally. exception may
method Kumar Zilberstein (2010b), employs AND/OR branch bound search
EDAC heuristic (and thus limited two-agent case). heuristic search method, may
amenable incremental implementation though knowledge attempted.

479

fiOliehoek, Spaan, Amato, & Whiteson

optimal efficiency) A* algorithm (Dechter, Flerova, & Marinescu, 2012), results
trivially transfer setting k allowed increase.

5. Experiments
section, empirically test validate proposed techniques: lossless clustering
joint histories, incremental expansion search nodes, hybrid heuristic representations.
introducing experimental setup, compare performance GMAA*-IC
GMAA*-ICE GMAA* suite benchmark problems literature.
Next, compare performance proposed methods state-of-the-art optimal
approximate Dec-POMDP methods, followed case study scaling behavior
respect number agents. Finally, compare memory requirements
hybrid heuristic representation tree vector representations.
5.1 Experimental Setup
well-known Dec-POMDP benchmarks Dec-Tiger (Nair et al., 2003)
BroadcastChannel (Hansen et al., 2004) problems. Dec-Tiger discussed extensively
Section 2. BroadcastChannel, two agents transmit messages communication channel, agents transmit time collision occurs
noisily observed agents. FireFighting problem models team n firefighters
extinguish fires row nh houses (Oliehoek, Spaan, & Vlassis, 2008).
agent choose move houses fight fires location; two agents
house, completely extinguish fire there. (negative) reward
team firefighters depends intensity fire house; fires
extinguished, reward zero received. Hotel 1 problem (Spaan & Melo, 2008),
travel agents need assign customers hotels limited capacity. send
customer resort yields lower reward. addition, use following problems: Recycling Robots (Amato, Bernstein, & Zilberstein, 2007), scaled-down version
problem described Section 2; GridSmall two observations (Amato, Bernstein, &
Zilberstein, 2006) Cooperative Box Pushing (Seuken & Zilberstein, 2007a), larger
two-robot benchmark. Table 1 summarizes problems numerically, listing number
joint policies different planning horizons.
Experiments run Intel Core i5 CPU running Linux, GMAA*, GMAA*-IC,
GMAA*-ICE implemented code-base using MADP Toolbox (C++)
(Spaan & Oliehoek, 2008). vector-based QBG representation computed using variation Incremental Pruning (adapted computing Q-functions instead regular value functions), corresponding NaiveIP method described Oliehoek Spaan (2012).
implement pruning, employ Cassandras POMDP-solve software (A. R. Cassandra,
1998).
results Sections 5.2 5.3, limited process 2Gb RAM
maximum CPU time 3,600s. Reported CPU times averaged 10 independent runs
resolution 0.01s. Timings given MAA* search processes, since
480

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

problem primitives

Dec-Tiger

num. h

n

|S|

|Ai |

|Oi |

2

4

6

2

2

3

2

7.29e2

2.06e14

1.31e60

BroadcastChannel

2

4

2

2

6.40e1

1.07e9

8.51e37

GridSmall

2

16

5

2

1.563e4

9.313e20

1.175e88

Cooperative Box Pushing

2

100

4

5

1.68e7

6.96e187

1.96e4703

Recycling Robots

2

4

3

2

7.29e2

2.06e14

1.31e60

Hotel 1

2

16

3

4

5.90e4

1.29e81

3.48e1302

FireFighting

2

432

3

2

7.29e2

2.06e14

1.31e60

Table 1: Benchmark problem sizes number joint policies different horizons.
computation heuristic methods amortized multiple
runs.18 problem definitions available via http://masplan.org.
5.2 Comparing GMAA*, GMAA*-IC, GMAA*-ICE
compared GMAA*, GMAA*-IC, GMAA*-ICE using hybrid QBG representation. methods compute optimal policy, expect GMAA*-IC efficient
GMAA* lossless clustering possible. Furthermore, expect GMAA*-ICE
provide improvements terms speedup scaling longer planning horizons.
results shown Table 2. entries report results, QBG heuristics
could computed, thanks hybrid representation. Consequently, performance
GMAA*-IC much better previously reported results, including Oliehoek
et al. (2009), often required resort QMDP larger problems and/or horizons.
entries marked show limits using QMDP instead QBG :
problems reach longer horizons QBG . FireFighting GMAA*-ICE
QMDP compute solutions higher h possible QBG (hence missing ,
showing GMAA*-ICE efficient using loose heuristic GMAA*-IC).
Furthermore, entries indicate horizon solve problem
tree-based QBG representation often much shorter.
results clearly illustrate GMAA*-IC leads significant improvement
performance. problems, GMAA*-IC able produce solution quickly
increase largest solvable horizon GMAA*. cases, GMAA*-IC able
drastically increase solvable horizon.
Furthermore, results clearly demonstrate incremental expansion allows significant additional improvements. fact, table demonstrates GMAA*-ICE significantly outperforms GMAA*-IC, especially problems little clustering possible.
results Table 2 illustrate efficacy hybrid representation. problems
GridSmall, Cooperative Box Pushing, FireFighting Hotel 1 neither
tree vector representation able provide compact QBG heuristic longer hori18. heuristics computation time ranges less second hours (for high h difficult
problems). Table 4 presents heuristic computation time results.

481

fiOliehoek, Spaan, Amato, & Whiteson

h
2
3
4
5
6
7

V TGMAA* (s)
Dec-Tiger
4.000000
0.01
5.190812
0.01
4.802755
563.09
7.026451

10.381625

TIC (s)

TICE (s)

h

0.01
0.01
0.27
21.03


0.01
0.01
0.01
0.02
46.43


2
3
4
5
6
10
15
18
20
30
40
50
60
70
80

FireFighting hnh = 3,nf = 3i
2 4.383496
0.09
0.01
0.01
3 5.736969
3.05
0.11
0.10
4 6.578834
1001.53 950.51
1.00
5 7.069874


4.40
6 7.175591
0.08
0.07
7
#
#
GridSmall
2
0.910000
0.01
0.01
0.01
3
1.550444
0.90
0.10
0.01
4
2.241577
*
1.77 0.01
5
2.970496

0.02
6
3.717168

0.04
7
#
#
Hotel 1
2 10.000000
0.0
0.01
0.01
3 16.875000
*
0.01
0.01
4 22.187500
0.01 0.01
5 27.187500
0.01
0.01
6 32.187500
0.01
0.01
7 37.187500
0.01
0.01
8 42.187500
0.01
0.01
9 47.187500
0.02
0.01
10
#
#
Cooperative Box Pushing
2
17.600000
0.02
0.01
0.01
3
66.081000

0.11 0.01
4
98.593613
313.07
5
#
#

2
3
4
5
6
7
10
20
25
30
40
50
53
100
250
500
600
700
800
900
1000

V TGMAA* (s) TIC (s) TICE (s)
Recycling Robots
7.000000
0.01 0.01
0.01
10.660125
0.01 0.01
0.01
13.380000
713.41 0.01
0.01
16.486000
0.01 0.01
19.554200
0.01
0.01
31.863889
0.01
0.01
47.248521
0.01
0.01
56.479290
0.01 0.01
62.633136
0.01
0.01
93.402367
0.08
0.05
124.171598
0.42
0.25
154.940828
2.02
1.27
185.710059
9.70
6.00
216.479290

28.66


BroadcastChannel
2.000000
0.01 0.01
0.01
2.990000
0.01 0.01
0.01
3.890000
0.01 0.01
0.01
4.790000
1.27 0.01
0.01
5.690000
0.01
0.01
6.590000
0.01 0.01
9.290000
0.01
0.01
18.313228
0.01
0.01
22.881523
0.01
0.01
27.421850
0.01
0.01
36.459724
0.01
0.01
45.501604
0.01
0.01
48.226420
0.01 0.01
90.760423
0.01
0.01
226.500545
0.06
0.07
452.738119
0.81
0.94
543.228071
11.63
13.84
633.724279
0.52
0.63


814.709393
9.57
11.11



Table 2: Experimental results comparing regular GMAA*, GMAA*-IC, GMAA*-ICE.
Listed computation times GMAA* (TGMAA* ), GMAA*-IC (TIC ),
GMAA*-ICE (TICE ), using hybrid QBG representation. use following symbols:
memory limit violations, time limit overruns, # heuristic computation exceeded memory time limits, maximum planning horizon using QMDP , maximum planning horizon
using tree-based QBG . Bold entries indicate methods proposed article
computed results.
zons. Apart Dec-Tiger FireFighting, computing storing QBG (or another
tight heuristic) longer horizons bottleneck scalability.
Together, algorithmic improvements lead first optimal solutions many
problem horizons. fact, vast majority problems tested, provide results
longer horizons previous work (the bold entries). improvements quite sub482

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

h
2
3
4
5
2
3
4
5
6
2
3
4
2
3
4
5
6
7
8
9
2
3

|BGh1 |

|cBGt |
Dec-Tiger
4 1.0, 4.0
16 1.0, 4.0, 9.0
64 1.0, 4.0, 9.0, 23.14
256 1.0, 4.0, 9.0, 16.0, 40.43
FireFighting hnh = 3,nf = 3i
4 1.0, 4.0
16 1.0, 4.0, 16.0
64 1.0, 4.0, 16.0, 64.0
256 1.0, 4.0, 16.0, 64.0, 256.0
1024 1.0, 1.0, 2.0, 3.0, 6.0, 10.0
GridSmall
4 1.0, 4.0
16 1.0, 4.0, 10.50
64 1.0, 4.0, 10.50, 20.0
Hotel 1
16 1.0, 4.0
256 1.0, 4.0, 16.0
4096 1.0, 4.0, 8.0, 16.0
65536 1.0, 4.0, 4.0, 8.0, 16.0
1.05e6 1.0, 4.0, . . . , 4.0, 8.0, 16.0
1.68e7 1.0, 4.0, . . . , 4.0, 8.0, 16.0
2.68e8 1.0, 4.0, . . . , 4.0, 8.0, 16.0
4.29e9 1.0, 4.0, . . . , 4.0, 8.0, 16.0
Cooperative Box Pushing
25 1.0, 4.0
625 1.0, 4.0, 25.0

h
2
3
4
5
10
15
18
20
30
40
50
60
2
3
4
5
6
7
10
20
25
30
40
50
100
900

|BGh1 | |cBGt |
Recycling Robots
4 1.0, remaining stages
16 1.0, remaining stages
64 1.0, remaining stages
256 1.0, remaining stages
262144 1.0, remaining stages
2.68e8 1.0, remaining stages
1.72e10 1.0, remaining stages
2.75e11 1.0, remaining stages
2.88e17 1.0, remaining stages
1.0, remaining stages
1.0, remaining stages
1.0, remaining stages
BroadcastChannel
4 1.0 (for t)
16 1.0 (for t)
64 1.0 (for t)
256 1.0 (for t)
1024 1.0 (for t)
4096 1.0 (for t)
262144 1.0 (for t)
2.75e11 1.0 (for t)
2.81e14 1.0 (for t)
2.88e17 1.0 (for t)
1.0 (for t)
1.0 (for t)
1.0 (for t)
1.0 (for t)

4.0
4.0
4.0
4.0
4.0
4.0
4.0
4.0
4.0
4.0
4.0
4.0

Table 3: Experimental results detailing effectiveness clustering. Listed size
CBGs = h 1 without clustering (|BGh1 |), average CBG size stages
clustering (|cBGt |).
stantial, especially given lengthening horizon one increases problem difficulty
exponentially (cf. Table 1).
5.2.1 Analysis Clustering Histories
Table 3 provides additional details performance GMAA*-IC, listing
number joint types GMAA*-IC search, |cBGt |, stage t. averages
since algorithm forms CBGs different past policies, leading clusterings different
sizes.19 see impact clustering, table lists |BGh1 |, number joint types
CBGs constructed last stage without clustering, constant.
Dec-Tiger, time needed GMAA*-IC 3 orders magnitude less
GMAA* horizon h = 4. h = 5, test problem 3.82e29 joint
policies, method able optimally solve it. GMAA*-IC, however,
able reasonable time. Dec-Tiger, clear symmetries
19. Note problem domains report smaller clusterings Oliehoek et al. (2009). Due
implementation mistake, clustering overly conservative, cases treat two histories
probabilistically equivalent, fact were.

483

fiOliehoek, Spaan, Amato, & Whiteson

observations allow clustering, demonstrated Fig. 4. Another key property
opening door resets problem, may facilitate clustering.
FireFighting, short planning horizons lossless clustering possible
stage, such, clustering incurs overhead. However, GMAA*-IC still faster
GMAA* constructing BGs using bootstrapping previous CBG
takes less time constructing CBG scratch. Interesting counterintuitive results
occur h = 6, solved within memory limits, contrast h = 5. fact, using
QMDP could compute optimal values V h > 6, turns equal
h = 6. reason optimal joint policy guaranteed extinguish
fires 6 stages. subsequent stages, rewards 0.
influence clustering, analysis Table 3 reveals CBG instances encountered
h = 6 search happen cluster much better h = 5, possible
heuristics vary horizon. fact, h = 6 sends agents
middle house = 0, h = 5, agents dispatched different houses.
agents fight fires house, fire extinguished completely, resulting joint
observations provide new information. result, different joint types lead
joint belief, means clustered. agents visit different houses,
observations convey information, leading different possible joint beliefs (which cannot
clustered).
Hotel 1 allows large amount clustering, GMAA*-IC outperforms GMAA*
large margin, former reaching h = 9 latter h = 2. problem
transition observation independent (Becker, Zilberstein, Lesser, & Goldman, 2003;
Nair, Varakantham, Tambe, & Yokoo, 2005; Varakantham, Marecki, Yabu, Tambe, & Yokoo,
2007), facilitates clustering, discuss Section 5.5. Unlike methods
specifically designed exploit transition observation independence, GMAA*-IC exploits
structure without requiring predefined explicit representation it. scalability
limited computation heuristic.
BroadcastChannel, GMAA*-IC achieves even dramatic increase performance, allowing solution horizon h = 900. Analysis reveals CBGs
constructed stages fully clustered: contain one type agent.
reason follows. constructing CBG = 1, one joint type
previous CBG so, given 0 , solution previous CBG, uncertainty
respect previous joint action a0 . crucial property BroadcastChannel
(joint) observation reveals nothing new state, joint action
taken (e.g., collision agents chose send). result, different individual
histories clustered. CBG constructed stage = 2, one joint
type previous game. Therefore, given past policy, actions agents
perfectly predicted. observation conveys information process repeats. Thus, problem special property could described non-observable
given past joint policy. GMAA*-IC automatically exploits property. Consequently,
time needed solve CBG grow horizon. solution time, however, still increases super-linearly increased amount backtracking.
FireFighting, performance monotonic planning horizon. case however,
clustering clearly responsible difference. Rather, explanation
certain horizons, many near-optimal joint policies, leading backtracking
higher search cost.
484

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

10

Nodes depth

10

DecTiger, h=6 Full Exp.
DecTiger, h=6 Inc. Exp.
GridSmall, h=6 Full Exp.
GridSmall, h=6 Inc. Exp.
FireFighting, h=5 Full Exp.
FireFighting, h=5 Inc. Exp.

5

10

0

10

0

1

2


3

4

Figure 8: Number expanded partial joint policies intermediate stages = 0, . . . ,h 2
(in log scale).

5.2.2 Analysis Incremental Expansion
Dec-Tiger h = 5, GMAA*-ICE achieves speedup three orders magnitude
compute solution h = 6, unlike GMAA*-IC. GridSmall, achieves large
speedup h = 4 fast solutions h = 5 6, GMAA*-IC runs memory. Similar positive results obtained FireFighting, Cooperative Box Pushing
Recycling Robots. fact, using QMDP , GMAA*-ICE able compute
solutions well beyond h = 1000 FireFighting problem, stands stark contrast GMAA*-IC computes solutions h = 3 heuristic. Note
BroadcastChannel problem GMAA*-IC (slightly) faster
GMAA*-ICE. problem exhibits clustering single joint type, overhead
incremental expansion pay off.
analyze incremental expansion, examined impact number nodes
expanded intermediate stages = 0, . . . ,h 2. Fig. 8 shows number nodes expanded
GMAA*-ICE number would expanded GMAA*-IC (which
easily computed since search-tree equivalent). clear relationship
results Fig. 8 Table 2, illustrating, e.g., GMAA*-IC runs memory
GridSmall h = 6. plots confirm hypothesis that, practice, small number
child nodes queried.
5.2.3 Analysis Hybrid Heuristic Representation
Fig. 9 illustrates memory requirements terms number parameters (i.e., real numbers) tree, vector, hybrid representations QBG , latter computed
following Algorithm 9. Results vector representation omitted representations grew beyond limits. effectiveness vector pruning depends problem
complexity value function, increase suddenly, instance happens Fig. 9c. results show that, several benchmark Dec-POMDPs, hybrid
representation allows significant savings memory, allowing computation tight
heuristics longer horizons.
485

fiOliehoek, Spaan, Amato, & Whiteson

h

MILP

DP-LPC

DP-IPG

GMAA QBG
IC

ICE

heur

BroadcastChannel, ICE solvable h = 900
2
0.38
0.01
0.09
0.01
3
1.83
0.50
56.66
0.01
4
34.06

*
0.01
5
48.94
0.01

0.01
0.01
0.01
0.01

0.01
0.01
0.01
0.01

Dec-Tiger, ICE
2
0.69
3
23.99
4

5

0.01
0.01
0.01
0.02

0.01
0.01
0.03
0.09

solvable h = 6
0.05
0.32
60.73
55.46

2286.38


0.01
0.01
0.27
21.03

FireFighting (2 agents, 3 houses, 3 firelevels), ICE
2
4.45
8.13
10.34
0.01
3


569.27
0.11
4

950.51
GridSmall, ICE solvable h = 6
2
6.64
11.58
0.18
3


4.09
4
77.44

0.01
0.10
1.77

Recycling Robots, ICE solvable h = 70
2
1.18
0.05
0.30
0.01
3
*
2.79
1.07
0.01
4
2136.16
42.02
0.01
5

1812.15
0.01
Hotel
2
3
4
5
9
10
15

1, ICE solvable h = 9
1.92
6.14
0.22
315.16
2913.42
0.54


0.73
1.11
8.43
17.40
283.76

Cooperative Box Pushing
2
3.56
15.51
3
2534.08
4


0.01
0.01
0.01
0.01
0.02
#

solvable h 1000
0.01 0.01
0.10
0.07
1.00
0.65
0.01
0.01
0.01

0.01
0.42
67.39

0.01
0.01
0.01
0.01

0.01
0.01
0.02
0.02

0.01
0.01
0.01
0.01
0.01
#

0.03
1.51
3.74
4.54
20.26

(QPOMDP ), ICE solvable h = 4
1.07
0.01 0.01 0.01
6.43
0.91
0.02
0.15
1138.61
*
328.97 0.63

Table 4: Comparison runtimes methods. Total time GMAA* methods
given taking time method column (IC ICE) adding heuristic
computation time (heur). use following symbols: memory limit violations, *
time limit overruns, # heuristic computation exceeded memory time limits.

486

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

10

Memory required

5

10

0

10

5

10

0

1

2

3
4
Horizon

5

10

6

(a) Dec-Tiger.

5

10

0

10

2

3
4
Horizon

5

10

6

(d) Recycling Robots.

10

0

1 2 3 4 5 6 7 8 9
Horizon

15

10

Tree
Vector
Hybrid

10

10

5

10

0

1 2 3 4 5 6 7 8 9 10
Horizon

Tree
Vector
Hybrid

(c) Hotel 1.

15

Tree
Vector
Hybrid

Memory required

Memory required

10

10

1

10

20

(b) FireFighting.

15

10

10

Tree
Vector
Hybrid

10

10
Memory required

Memory required

10

Tree
Vector
Hybrid

Memory required

10

10

Tree
Vector
Hybrid

10

10

5

10

0

1 2 3 4 5 6 7 8 9 10
Horizon

(e) BroadcastChannel.

10

1

2

3
4
Horizon

5

6

(f) GridSmall.

Figure 9: Hybrid heuristic representation. y-axis shows number real numbers stored
different representations QBG several benchmark problems (in log scale).
5.3 Comparing Methods
section, compare GMAA*-IC GMAA*-ICE methods literature. begin comparing runtimes methods following state-ofthe-art optimal Dec-POMDP methods: MILP20 (Aras & Dutech, 2010) converts DecPOMDP mixed integer linear program, numerous solvers available.
used MOSEK version 6.0. DP-LPC21 (Boularias & Chaib-draa, 2008) performs dynamic programming lossless policy compression, CPLEX 12.4 LP solver.
DP-IPG (Amato et al., 2009) performs exact dynamic programing incremental policy
20. results reported deviate reported Aras Dutech (2010). number
problems, Aras et al. employed solution method solves MILP series (a tree) smaller
MILPs branching continuous realization weight variables earlier stages. is, past
joint policy stage t, solve different MILP involving subset consistent sequences.
Additionally, FireFighting GridSmall, use benchmark versions standard literature
(Oliehoek, Spaan, & Vlassis, 2008; Amato et al., 2006), whereas Aras Dutech (2010) use non-standard
versions. explains difference results ones reported article (personal
communication, Raghav Aras).
21. goal Boularias Chaib-draa (2008) find non-dominated joint policies initial beliefs.
previously reported results concerned run-time compute non-dominated joint policies, without
performing pruning full-length joint policies. contrast, report time needed compute
actual optimal Dec-POMDP policy (given b0 ). additionally requires final round pruning
subsequently computing value remaining joint policies initial belief. additional overhead explains differences run time report previously
reported (personal communication, Abdeslam Boularias).

487

fiOliehoek, Spaan, Amato, & Whiteson

Problem
Dec-Tiger
Cooperative Box Pushing
GridSmall

h
6
3
5


7
3
3

VMBDP
9.91
53.04
2.32

V
10.38
66.08
2.97

Table 5: Comparison optimal (V ) approximate (VMBDP ) values.

generation exploits known start state knowledge states reachable
DP backup.
Table 4, shows results comparison, demonstrates that, almost cases,
total time GMAA*-ICE (given sum heuristic computation time time
GMAA*-phase) significantly less state-of-the-art methods.
Moreover, demonstrated Table 2, GMAA*-ICE compute solutions longer horizons problems, except Cooperative Box Pushing Hotel 1.22
problems, possible compute QBG longer horizons. Overcoming
problem could enable GMAA*-ICE scale horizons well.
DP-LPC algorithm proposed Boularias Chaib-draa (2008) improves
efficiency optimal solutions form compression. performance algorithm,
however, weaker GMAA*-IC. two main explanations performance difference. First, DP-LPC uses compression compactly represent values
sets useful sub-tree policies, using sequence form representation. policies themselves, however, compressed: still specify actions every possible observation
history (for policy needs select exponential amount sequences make
policy). Hence, cannot compute solutions long horizons. Second, GMAA*-IC
exploit knowledge initial state distribution b0 .
Overall, GMAA*-ICE substantially improves state-of-the-art optimally solving
Dec-POMDPs. Previous methods typically improved feasible solution horizon
one (or provided speed-ups horizons could already solved). contrast,
GMAA*-ICE dramatically extends feasible solution horizon many problems.
consider MBDP-based approaches, leading family approximate algorithms.
Table 5, reports VMBDP values produced PBIP-IPG (Amato et al., 2009) (with
typical maxTrees parameter setting m), demonstrates optimal solutions produced
GMAA*-IC GMAA*-ICE higher quality. PBIP-IPG chosen
MBDP algorithms parameters achieve value.
exhaustive, comparison illustrates even best approximate Dec-POMDP methods
practice provide inferior joint policies problems. Conducting analysis
possible optimal solutions computed. Clearly, data becomes
available, thorough comparisons made. Therefore, scalable optimal
solution methods GMAA*-ICE critical improving analyses.
488

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

problem primitives

num. h

n

|S|

|A|

|O|

2

4

6

2

27

4

4

64

1.07e9

8.50e37

3

81

8

8

512

3.51e13

7.84e56

4

243

16

16

4.09e3

1.15e18

7.23e75

5

729

32

32

3.27e4

3.77e22

6.67e94

6

2187

64

64

2.62e5

9.80e55

6.15e113

Table 6: FireFightingGraph: number joint policies different numbers agents
horizons, 3 possible fire levels.
5.4 Scaling Agents
benchmark problems results presented far limited two agents. Here,
present case study FireFightingGraph (Oliehoek, Spaan, Whiteson, & Vlassis,
2008), variation FireFighting allowing agents, agent
fight fires two houses, instead them. Table 6 highlights size
problems, including total number joint policies different horizons. compared
GMAA*, GMAA*-IC, GMAA*-ICE (all using QMDP heuristic), BruteForceSearch,
DP-IPG, maximum run-time 12 hours running Intel Core i7 CPU,
averaged 10 runs. BruteForceSearch simple optimal algorithm enumerates
evaluates joint policies, implemented codebase GMAA*
variations. DP-IPG results use original implementation run Intel Xeon
computer. Hence, timing results directly comparable, overall trends
apparent. Also, since DP-IPG implementation limited 2 agents, results shown
agents.
Fig. 10 shows computation times FireFightingGraph across different numbers
agents planning horizons, Table 7 lists optimal values obtained. expected,
baseline BruteForceSearch performs poorly, scaling beyond h = 2 2
agents, DP-IPG reach h = 4. hand, regular GMAA* performs
relatively well, scaling maximum 5 agents. However, GMAA*-IC GMAA*-ICE
improve efficiency GMAA* 12 orders magnitude. such, substantially
outperform three methods, scale 6 agents. benefit incremental expansion clear n = 3,4, GMAA*-ICE reach higher horizon GMAA*-IC.
Hence, although article focuses scalability horizon, results show
methods propose improve scalability number agents.
5.5 Discussion
Overall, empirical results demonstrate incremental clustering expansion offers
dramatic performance gains diverse set problems. addition, results Broad22. Hotel 1, DP-IPG performs particularly well problem structure limited reachability.
is, agent fully observe local state (but agent) local states
except one one action dominates others. result, DP-IPG generate small number
possibly optimal policies.

489

fiOliehoek, Spaan, Amato, & Whiteson

4

4

10

10

3

2

10

1

10

0

10

1

10

2

10

3

10

6

5

4

3

2

2

3

4

5

6

7

8

9

10

3

10

computation time (s)

computation time (s)

computation time (s)

4

10

3

10

2

10

1

10

0

10

1

10

2

10

3

10

6

5

4

h

#agents

3

2

2

3

5

4

7

6

8

9

10

10

2

10

1

10

0

10

1

10

2

10

3

10

6

5

h

#agents

(a) GMAA* results.

3

2

2

3

4

5

7

8

9

10

h

#agents

(b) GMAA*-IC results.

4

(c) GMAA*-ICE results.

4

10

10

3

10

2

10

1

10

0

10

1

10

2

10

3

10

6

5

4

3

2

2

3

4

5

6

7

8

9

10

computation time (s)

3

computation time (s)

4

6

10

2

10

1

10

0

10

1

10

2

10

3

10

6
h

#agents

5

4

3

2

2

3

4

5

6

7

8

9

10

h

#agents

(d) BruteForceSearch results.

(e) DP-IPG results.

Figure 10: Comparison GMAA*, GMAA*-IC, GMAA*-ICE, BruteForceSearch,
DP-IPG FireFightingGraph problem. Shown computation time (in log
scale) various number agents horizons. Missing bars indicate method
exceeded time memory limits. However, DP-IPG implementation supports 2
agents.

h
2
3
4
5
6

n=2
4.394252
5.806354
6.626555
7.093975
7.196444

n=3
5.213685
6.654551
7.472568

n=4
6.027319
7.391423
8.000277

n=5
6.846752

n=6
7.666185

Table 7: Value V optimal solutions FireFightingGraph problem, different
horizons numbers agents.

castChannel illustrate key advantage approach: problem possesses property makes large amount clustering possible, clustering method exploits
property automatically, without requiring predefined explicit representation it.
490

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

course, problems admit great reductions via clustering. One domain property
allows clustering past joint policy encountered GMAA* makes
observations superfluous, BroadcastChannel FireFighting. Dec-Tiger,
see certain symmetries lead clustering. However clustering occur even
without properties. fact, problems nearly horizons tested,
size CBGs reduced. Moreover, accordance analysis Section 3.2,
improvements planning efficiency huge, even modest reductions CBG size.
One class problems say something priori amount clustering
possible class Dec-POMDPs transition observation independence
(Becker et al., 2003). problems, agents local states transitions
independent, two agents expressed
Pr(s1 , s2 |s1 , s2 , a1 , a2 ) = Pr(s1 |s1 , a1 ) Pr(s2 |s2 , a2 ).

(5.1)

Similarly, observations assumed independent, means agent
observation probability depends action local state: Pr(oi |ai , si ).
problems, probabilistic equivalence criterion (3.1) factors too. particular, due
transition observation independence23 , (3.2) holds true ~ia ,~ib . Moreover, (3.3)
factors product Pr(s1 , s2 |~1 , ~2 ) = Pr(s1 |~1 ) Pr(s2 |~2 ) thus holds Pr(s1 |~1a ) =
Pr(s1 |~1b ). is, two histories clustered induce local belief.
such, size CBGs directly corresponds product number reachable local
beliefs. Since transition observation independent Hotel 1 problem locally
fully observable, local state spaces consist four states, four possible
local beliefs (which consistent CBG size 16 Table 3). Moreover, see
maximum size typically reached end search. good
policies defer sending customers hotel thus visit local states hotel
filled earlier stages.
general classes problems, even weakly coupled models (e.g., Becker,
Zilberstein, & Lesser, 2004; Witwicki & Durfee, 2010), criterion (3.1) factor,
hence direct correspondence number local beliefs. such,
applying clustering algorithm determine well problem clusters.
analogous to, e.g., state aggregation MDPs (e.g., discussed Givan, Dean, &
Greig, 2003) known predict priori large minimized model
be. Fortunately, empirical results demonstrate that, domains admit little
clustering, overhead small.
expected, incremental expansion helpful problems allow
much clustering. However, results for, e.g., Dec-Tiger illustrate limit
amount scaling method currently provide. bottleneck solution
large CBGs later stages: CBG solver solve large CBGs
returning first solution order guarantee optimality, takes takes long time.
expect improvements CBG solvers directly add efficacy
incremental expansion.
experiments clearly demonstrate Dec-POMDP complexity results,
important, worst-case results. fact, scalability demonstrated experiments
clearly show many problems successfully scale dramatically beyond would
23. assumes external state variable s0 .

491

fiOliehoek, Spaan, Amato, & Whiteson

expected doubly-exponential dependence horizon. Even smallest problems,
doubly-exponential scaling horizon implies impossible compute solutions
beyond h = 4 all, indicated following simple calculation: let n = 2, |Ai | = 2
actions, |Oi | = 2| observations,
5

|Ai |(n(|Oi |

))

4

/|Ai |(n(|Oi |

))

= 4.2950e9.

Thus, even simplest possible case, see increase factor 4.2950e09 h = 4
h = 5. Similarly, next increment, h = 5 h = 6, increases size search
space factor 1.8447e19. However, experiments clearly indicate almost
cases, things dire. is, even though matters look bleak light
complexity results, many cases able perform substantially better worst
case.

6. Related Work
section, discuss number methods related proposed
article. methods already discussed earlier sections. Section 3,
indicated clustering method closely related approach Emery-Montemerlo
et al. (2005) fundamentally different method lossless. Section 5.3,
discussed connections approach Boularias Chaib-draa (2008) clusters
policy values. contrasts approach clusters histories thus
policies themselves, leading greater scalability.
Section 3.1.2, discussed relationship notion probabilistic equivalence (PE) multiagent belief. However, yet another notion belief, employed
JESP solution method (Nair et al., 2003), superficially similar PE
distribution. JESP belief AOH ~i probability distribution Pr(s,~o6=i |~i , b0 , 6=i )
states observation histories agents given (deterministic) full policy
agents. sufficient statistic, since induces multiagent belief, thus
allows clustering histories. crucial difference with, utility of, PE lies
fact PE criterion specified states AOHs given past joint policy.
is, (3.1) induce multiagent belief.
clustering approach resembles number methods employ equivalence
notions. First, several approaches exploit notion behavioral equivalence (Pynadath &
Marsella, 2007; Zeng et al., 2011; Zeng & Doshi, 2012). consider, perspective
protagonist agent i, possible models another agent j. Since j affects
actions, i.e., behavior, agent cluster together models agent j lead
policy j agent. is, cluster models agent j
behaviorally equivalent. contrast, cluster models agents j, histories
agent agents, well environment, guaranteed behave
expectation, thus leading best response agent i. is, method
could seen clustering histories expected environmental behavior equivalent.
notion utility equivalence (Pynadath & Marsella, 2007; Zeng et al., 2011) closer
PE takes account (value the) best-response agent (in particular,
clusters two models mj mj using BR(mj )the best response mj achieves
value mj ). However, remains form behavior equivalence
clusters models agents, histories protagonist agent.
492

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

connections PE work influence-based abstraction (Becker et
al., 2003; Witwicki & Durfee, 2010; Witwicki, 2011; Oliehoek et al., 2012), since influence
(or point parameter space, Becker et al., 2003) compact representation
agents policies. Models agents clustered lead influence
agent i. However, though fine-grained, ultimately still form behavioral
equivalence.
final relation equivalence notion work Dekel, Fudenberg, Morris
(2006), constructs distance measure topology space types
goal approximating infinite universal type space (the space possible beliefs
beliefs beliefs, etc.) one-shot Bayesian games. setting, however, considers
simple finite type space types directly correspond private histories (in
form AOHs) sequential problem. Thus, need approximate universal
type space; instead want know histories lead future dynamics
perspective agent. Dekel et al.s topology address question.
incremental expansion technique related approaches extending deal
large branching factors context multiple sequence alignment (Ikeda & Imai, 1999;
Yoshizumi, Miura, & Ishida, 2000). However, approach different
discard unpromising nodes rather provide mechanism generate necessary
ones. Also, proposing MAA*, Szer et al. (2005) developed superficially similar approach could applied last stage. particular, proposed generating
child nodes one one, time checking child found value equal
parents heuristic value. Since value child specifies full policy, value
lower bound therefore expansion remaining child nodes skipped. Unfortunately, number issues prevent approach providing substantial leverage
practice. First, cannot applied intermediate stages 0 < h 1 since lower bound
values expanded children available. Second, many problems unlikely
child node exists. Third, even does, Szer et al. specify efficient way
finding it. Incremental expansion overcomes issues, yielding approach that,
experiments demonstrate, significantly increases size Dec-POMDPs
solved optimally.
article focuses optimal solutions Dec-POMDPs finite horizon. part
evaluation, compare MILP approach (Aras & Dutech, 2010), DPILP (Boularias & Chaib-draa, 2008) DP-IPG (Amato et al., 2009), extension
exact dynamic programming algorithm (Hansen et al., 2004). Research finite-horizon DecPOMDPs considered many approaches bounded approximations (Amato,
Carlin, & Zilberstein, 2007), locally optimal solutions (Nair et al., 2003; Varakantham, Nair,
Tambe, & Yokoo, 2006) approximate methods without guarantees (Seuken & Zilberstein,
2007b, 2007a; Carlin & Zilberstein, 2008; Eker & Akn, 2010; Oliehoek, Kooi, & Vlassis, 2008;
Dibangoye et al., 2009; Kumar & Zilberstein, 2010b; Wu et al., 2010a; Wu, Zilberstein, &
Chen, 2010b).
particular, much research considered optimal and/or approximate solution
subclasses Dec-POMDPs. One subclass contains Dec-POMDPs
agents local states agents cannot influence. resulting models,
TOI-Dec-MDP (Becker et al., 2003; Dibangoye, Amato, Doniec, & Charpillet, 2013) NDPOMDP (Nair et al., 2005; Varakantham et al., 2007; Marecki, Gupta, Varakantham, Tambe,
& Yokoo, 2008; Kumar & Zilberstein, 2009), interpreted independent (PO)MDPs
493

fiOliehoek, Spaan, Amato, & Whiteson

agent coupled reward function (and possibly unaffectable state
feature). hand, event-driven interaction models (Becker et al., 2004) consider
agents individual rewards influence others transitions.
recently, models allow limited transition reward dependence
introduced. Examples interaction-driven Markov games (Spaan & Melo, 2008), DecMDPs sparse interactions (Melo & Veloso, 2011), distributed POMDPs coordination locales (Varakantham et al., 2009; Velagapudi et al., 2011), event-driven interactions
complex rewards (EDI-CR) (Mostafa & Lesser, 2011), transition decoupled Dec-POMDPs
(Witwicki & Durfee, 2010; Witwicki, 2011). methods developed models often exhibit better scaling behavior methods standard Dec-(PO)MDPs, typically
suitable agents extended interactions, e.g., collaborate transporting
item. Also, specialized models consider timing actions whose
ordering already determined (Marecki & Tambe, 2007; Beynier & Mouaddib, 2011).
Another body work addresses infinite-horizon problems (Amato, Bernstein, & Zilberstein, 2010; Amato, Bonet, & Zilberstein, 2010; Bernstein, Amato, Hansen, & Zilberstein,
2009; Kumar & Zilberstein, 2010a; Pajarinen & Peltonen, 2011), possible
represent policy tree. approaches represent policies using finite-state controllers
optimized various ways. Also, since infinite-horizon case undecidable
(Bernstein et al., 2002), approaches approximate optimal given particular controller size. exists boundedly optimal approach theoretically construct
controller within optimal, feasible small problems large
(Bernstein et al., 2009).
great interest Dec-POMDPs explicitly take account communication. approaches try optimize meaning communication actions without
semantics (Xuan, Lesser, & Zilberstein, 2001; Goldman & Zilberstein, 2003; Spaan, Gordon,
& Vlassis, 2006; Goldman, Allen, & Zilberstein, 2007) others use fixed semantics (e.g.,
broadcasting local observations) (Ooi & Wornell, 1996; Pynadath & Tambe, 2002; Nair et
al., 2004; Roth et al., 2005; Oliehoek, Spaan, & Vlassis, 2007; Roth, Simmons, & Veloso, 2007;
Spaan, Oliehoek, & Vlassis, 2008; Goldman & Zilberstein, 2008; Becker, Carlin, Lesser, & Zilberstein, 2009; Williamson, Gerding, & Jennings, 2009; Wu et al., 2011). Since models used
first category (e.g., Dec-POMDP-Com) converted normal Dec-POMDPs
(Seuken & Zilberstein, 2008), contributions article applicable settings.
Finally, numerous models closely related Dec-POMDPs, POSGs
(Hansen et al., 2004), interactive POMDPs (I-POMDPs) (Gmytrasiewicz & Doshi, 2005),
graphical counterparts (Doshi, Zeng, & Chen, 2008). models general sense consider self-interested settings agent individual
reward function. I-POMDPs conjectured require doubly exponential time (Seuken
& Zilberstein, 2008). However, I-POMDP number recent advances
(Doshi & Gmytrasiewicz, 2009). current paper makes clear link best-response
equivalence histories notion best-response equivalence beliefs I-POMDPs.
particular, article demonstrates two PE action-observation histories (AOHs) induce, given past joint policy, distribution states AOHs agents,
therefore induce multiagent belief future policies agents.
induced multiagent beliefs, turn, interpreted special cases I-POMDP beliefs
model agents sub-intentional models form fixed policy
tree. Rabinovich Rosenschein (2005) introduced method that, rather optimizing
494

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

expected value joint policy, selects coordinated actions uncertainty tracking
dynamics environment. approach, however, requires model ideal system
dynamics input many problems, considered article, identifying
dynamics difficult.

7. Future Work
Several avenues future work made possible research presented article.
Perhaps promising development new approximate Dec-POMDP algorithms.
article focused optimal methods, GMAA*-ICE seen framework approximate methods. methods could derived limiting amount
backtracking, employing approximate CBG solvers (Emery-Montemerlo, Gordon, Schneider,
& Thrun, 2004; Kumar & Zilberstein, 2010b; Wu et al., 2010a), integrating GMAA* methods factored Dec-POMDPs (Oliehoek, Spaan, Whiteson, & Vlassis, 2008; Oliehoek, 2010;
Oliehoek et al., 2013), performing lossy clustering (Emery-Montemerlo, 2005; Wu et al., 2011)
using bounded approximations heuristics. particular, seems promising combine approximate clustering approximate factored GMAA* methods.
Lossy clustering could achieved generalizing probabilistic equivalence criterion,
currently strict little clustering may possible many problems.
obvious approach cluster histories distributions states histories
agents merely similar, measured by, e.g., Kullback-Leibler divergence. Alternately,
histories could clustered induce individual belief states:
Pr(s|~i ) =

X

Pr(s,~
6=i |~i ).

(7.1)

~
6=i

individual beliefs sufficient statistics history, hypothesize
constitute effective metrics approximate clustering. Since individual belief simply
marginalizes agents histories probabilities used probabilistic
equivalence criterion, intuitive heuristic metric approximate clustering.
article focuses increasing scalability respect horizon, developing
techniques deal larger number agents important direction future work.
plan explore performing GMAA* using factored representations (Oliehoek, Spaan,
Whiteson, & Vlassis, 2008). previous work, could exploit factorization
last stage, since earlier stages required full expansions guarantee optimality. However,
larger problems, number joint BG policies (i.e., number child nodes)
directly large (earlier stages tightly coupled); therefore incremental expansion
crucial improving scalability optimal solution methods respect number
agents.
Another avenue future work generalize GMAA*-ICE. particular,
may possible flatten two nested searches single search.
could lead significant savings would obviate need solve entire CBG
expanding next one. work, employed plain algorithm basis,
promising direction future work investigate enhancements literature
(Edelkamp & Schrodl, 2012) benefit GMAA* most. particular, described
experiments, different past joint policies lead CBGs different sizes. One idea
495

fiOliehoek, Spaan, Amato, & Whiteson

first expand parts search tree lead small CBGs, biasing selection
operator (but pruning operator, maintain optimality).
Yet another important direction future work development tighter heuristics.
Though researchers addressing topic, results presented article underscore important heuristics solving larger problems. Currently, heuristic
bottleneck four seven problems considered. Moreover, two
problems bottleneck already solved long (h > 50) horizons.
Therefore, believe computing tight heuristics longer horizons single
important research direction improving scalability optimal Dec-POMDP
solution methods respect horizon.
different direction employ theoretical results clustering beyond DecPOMDP setting develop new solution methods CBGs. instance, well-known
method computing local optimum alternating maximization (AM): starting
arbitrary joint policy, compute best response agent given agents keep
policies fixed select another agents policy improve, etc. One idea start
completely clustered CBG, agents types clustered together thus
random joint CBG policy simple form: agent selects single action.
improving policy agent consider actual possible types compute
best response. Subsequently, cluster together types agent selects
action proceed next agent. addition, since clustering results
restricted collaborative setting, may possible employ them, using similar
approach, develop new solution methods general-payoff BGs.
Finally, two contributions significant impact beyond problem
optimally solving Dec-POMDPs. First, idea incrementally expanding nodes introduced
GMAA*-ICE applied search methods. Incremental expansion
useful children generated order decreasing heuristic value without prohibitive
computational effort, problems large branching factor multiple sequence
alignment problems computational biology (Carrillo & Lipman, 1988; Ikeda & Imai, 1999).
Second, representing PWLC value functions hybrid tree set vectors
wider impact well, e.g., online search POMDPs (Ross, Pineau, Paquet, & Chaib-draa,
2008).

8. Conclusions
article presented set methods advance state-of-the-art optimal solution
methods Dec-POMDPs. particular, presented several advances aim extend
horizon optimal solutions found. advances build GMAA*
heuristic search approach include lossless incremental clustering CBGs solved
GMAA*, incremental expansion nodes GMAA* search tree, hybrid heuristic
representations. provided theoretical guarantees that, suitable heuristic used,
incremental clustering incremental expansion yield algorithms complete search equivalent. Finally, presented extensive empirical results demonstrating
GMAA*-ICE optimally solve Dec-POMDPs unprecedented size. significanty
increase planning horizons tackledin cases order
magnitude. Given increase horizon one results exponentially larger
search space, constitutes large improvement. Moreover, techniques im496

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

prove scalability respect number agents, leading first ever solutions
general Dec-POMDPs three agents. results demonstrated
optimal techniques yield new insights particular Dec-POMDPs, incremental
clustering revealed properties BroadcastChannel make much easier solve.
addition facilitating optimal solutions, hope advances inspire new principled
approximation methods, incremental clustering already done (Wu et al., 2011),
enable meaningfully benchmarked.

Acknowledgments
thank Raghav Aras Abdeslam Boularias making code available us. Research supported part AFOSR MURI project #FA9550-09-1-0538 part NWO
CATCH project #640.005.003. M.S. funded FP7 Marie Curie Actions Individual
Fellowship #275217 (FP7-PEOPLE-2010-IEF).

Appendix A. Appendix
A.1 Auxiliary algorithms
Algorithm 12 implements BestJointPolicyAndValue function, prunes child
nodes fully specified. Algorithm 13 generates children particular CBG.
Algorithm 12 BestJointPolicyAndValue(QExpand ): Prune fully expanded nodes set
nodes QExpand returning best one value.
Input: QExpand set nodes fully specified joint policies.
Output: best full joint policy input set value.
1: v =
2: q QExpand
3:
QExpand .Remove(q)
4:
h, vi q
5:
v > v
6:
v v
7:

8:
end
9: end
10: return h , v

A.2 Detailed GMAA*-ICE algorithm
complete GMAA*-ICE algorithm shown Algorithm 14.
A.3 Computation V 0...t1 (t )
quantity V 0...t1 (t ) defined recursively via:
V 0...t1 (t ) = V 0...t2 (t1 ) + Est1 ,~t1 [R(st1 , t1 (~ t1 )) | b0 , ].
497

(A.1)

fiOliehoek, Spaan, Amato, & Whiteson

Algorithm 13 GenerateAllChildrenForCBG(B(t )).
Input: CBG B(t ).
Output: QExpand set containing expanded child nodes CBG.
1: QExpand {}
2: jointP
CBG policies B
3:
Vb () Pr()u(,())
4:
t+1 (t , )
{create partial joint policy}

t+1
0...t1

b
b
5:
V ( ) V
( ) + V ( )
{compute heuristic value}
6:
q ht+1 , Vb (t+1 )i
{create child node}
7:
QExpand .Insert(q )
8: end
9: return QExpand

expectation taken respect joint probability distribution states
joint AOHs induced :
X
Pr(st ,~ |b0 ,t ) =
Pr(ot |at1 ,st ) Pr(st |st1 ,at1 ) Pr(at1 |t ,~ t1 ) Pr(st1 ,~ t1 |b0 ,t ).
st1

(A.2)
Here, ~ = (~ t1 ,at1 ,ot ) Pr(at1 |t ,~ t1 ) probability specifies at1
AOH ~ t1 (which 0 1 case deterministic past joint policy ).
A.4 Proofs
Proof Theorem 1
Substituting (2.9) (2.7) yields
X
b ~ , (~ ))
Vb () = Vb ( ) =
Pr(~ |b0 ,t )Q(
~


=

X
~




Pr(~ |b0 ,t ) Est [R(st , (~ )) | ~ ] + E~t+1 [Vb (~ t+1 ) | ~ , (~ )]

= Est ,~t [R(st , (~ ) | b0 , ] + E~t+1 [Vb (~ t+1 ) | b0 , , ]

Est ,~t [R(st , (~ ) | b0 , ] + E~t+1 [Q (~ t+1 , (~ t+1 )) | b0 , t+1 = (t , )]
= Est ,~t [R(st , (~ ) | b0 , ] + H ,t+1...h1 (t+1 ),

H optimal admissible heuristic. Substituting (2.8) obtain
Vb (t+1 = (t , )) = V 0...t1 (t ) + Est ,~t [R(st , (~ ) | b0 , ] + E~t+1 [Vb (~ t+1 ) | b0 , , ]
V 0...t1 (t ) + Est ,~t [R(st , (~ )) | b0 , t+1 ] + H ,t+1...h1 (t+1 )

{via (A.1)} = V 0...t (t+1 ) + H ,t+1...h1 (t+1 ),
demonstrates heuristic value Vb (t ) used GMAA* via CBGs using heuristic
form (2.9) admissible, lower bounded actual value first plus
admissible heuristic. Since performs heuristic search admissible heuristic,
algorithm complete.
498

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

Algorithm 14 GMAA*-ICE
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
32:
33:
34:
35:
36:
37:
38:
39:
40:
41:
42:
43:
44:
45:
46:
47:

vGM AA
0 ()
v +
q 0 h0 , vi
LIE {q 0 }
repeat
q Select(LIE )
{q = ht , vi}
IE
L .pop(q)
IsPlaceholder(q)
B(t ) .CBG
{reuse stored CBG}
else
{Construct extended BG solver:}
B(t1 ) t1 .CBG
{note = (t1 , t1 )}
t1

t1
B( ) ConstructExtendedBG(B( ), )
B(t ) ClusterBG(B(t ))
B(t ).Solver CreateSolver(B(t ))
.CBG B(t )
end
{Expand single child:}
vCBG = vGM AA V 0...(t1) (t )
vCBG = +
last stage = h 1
vCBG = Vb (h1 ) V 0...(h2) (h1 )
end
h , Vb ( )i B(t ).Solver.NextSolution(vCBG ,vCBG )

{fully expanded: solution s.t. V ( h1 ) vCBG }
delete q (and CBG + solver)
continue
{(i.e. goto line 8)}
end
t+1 (t , )
Vb (t+1 ) V 0...t1 (t ) + Vb ( )
last stage = h 1
{Note = t+1 , V () = Vb (t+1 ) }
V () > vGM AA
vGM AA V ()
{found new lower bound}

LIE .prune(vGM AA )
end
delete q (and CBG + solver)
else
q ht+1 , Vb (t+1 )i
LIE .insert(q )
q ht , Vb (t+1 )i
{ Update parent node q, placeholder }
LIE .insert(q)
end
LIE empty

499

fiOliehoek, Spaan, Amato, & Whiteson

Proof Lemma 1

t+1

t+1 ~
Proof. Assume arbitrary ati ,ot+1
6=i ,at6=i ,ot+1
6=i = (~
, 6=i ,s
6=i )).
t+1

~ a,t
Pr(st+1 ,~
6=i ,ot+1
|i ,ai , 6=i )
X


t+1
t+1
6=i |~ia,t )
Pr(ot+1
) Pr(st+1 |st ,ati ,at6=i ) Pr(at6=i |~
6=i , t6=i ) Pr(st ,~
=
,o6=i |ai ,a6=i ,s
st

=

X



t+1
t+1
6=i |~ib,t )
) Pr(st+1 |st ,ati ,at6=i ) Pr(at6=i |~
6=i , t6=i ) Pr(st ,~
Pr(ot+1
,o6=i |ai ,a6=i ,s

st

t+1
~ b,t
= Pr(st+1 ,~
6=i ,ot+1
|i ,ai , 6=i )
t+1

assumed arbitrary st+1 ,~
6=i ,ot+1
,
st+1 ,~t+1 ,ot+1
6=i



t+1
t+1 ~ t+1 t+1 ~ b,t
~ a,t
, 6=i ,oi |i ,ai , 6=i )
Pr(st+1 ,~
6=i ,ot+1
|i ,ai , 6=i ) = Pr(s

(A.3)

general
t+1

Pr(s

t+1

t+1

,~
6=i |~it ,ati ,ot+1
, 6=i ) =

~t
Pr(st+1 ,~
6=i ,ot+1
|i ,ai , 6=i )
Pr(ot+1 |~ ,at , )


=





6=i

t+1
~t
Pr(st+1 ,~
6=i ,ot+1
|i ,ai , 6=i )
P
t+1 t+1
t+1 ,~
~

t+1 Pr(s
6=i ,oi |i ,ai , 6=i )
t+1
~

,
6=i

Now, (A.3), numerator denominator substituting
~ia,t ,~ib,t equation. Consequently, conclude
t+1

t+1 ~ t+1 ~ b,t t+1
, 6=i |i ,ai ,oi , 6=i )
Pr(st+1 ,~
6=i |~ia,t ,ati ,ot+1
, 6=i ) = Pr(s
t+1


t+1 , ~
Finally, ati , ot+1
6=i arbitrarily chosen, conclude
, 6=i ,s
(3.4) holds.

Proof Lemma 2
Proof. Assume arbitrary 6=i ,s 6=i ,
bi (s, 6=i |~ia , 6=i ) , Pr(s, 6=i |~ia , 6=i ,b0 )
X
=
Pr(s, 6=i ,~
6=i |~ia , 6=i ,b0 )
~
6=i

{factoring joint distribution}

=

X

Pr(s,~
6=i |~ia , 6=i ,b0 ) Pr( 6=i |s,~
6=i , ~ia , 6=i ,b0 )

~
6=i

500

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

{ 6=i depends ~
6=i , 6=i } =

X

Pr(s,~
6=i |~ia , 6=i ,b0 ) Pr( 6=i |~
6=i , 6=i )

{ s,~
6=i depend 6=i } =

X

Pr(s,~
6=i |~ia ,6=i ,b0 ) Pr( 6=i |~
6=i , 6=i )

{due PE} =

X

Pr(s,~
6=i |~ib ,6=i ,b0 ) Pr( 6=i |~
6=i , 6=i )

~
6=i

~
6=i

~
6=i

= [...] = Pr(s, 6=i |~ib , 6=i ,b0 ) = bi (s, 6=i |~ib , 6=i )
conclude holds 6=i ,s 6=i .
Proof Theorem 5 (Search Equivalence)
prove search equivalence, explicitly write node tuple q = ht , v, PHi,
past joint policy, v nodes heuristic value, PH boolean indicating whether
placeholder. consider equivalence maintained open lists. open list
L maintained GMAA*-IC contains non-expanded nodes q. contrast, open
list LIE GMAA*-ICE contains non-expanded nodes q placeholders (previously
expanded nodes), q. denote ordered subset LIE containing non-expanded nodes
Q containing placeholders Q. treat open lists ordered sets
heuristic values associated nodes.
Definition 11. L LIE equivalent, L LIE if:
1. Q L.
2. qs ordering: L.remove(L \ Q) = Q.24
3. Nodes q L Q placeholder q parent higher ranked
q:
q=ht ,vq ,falsei(L\Q)

q=ht1 ,vq ,trueiQ s.t. (t = (t1 , ) q < q).

4. placeholders.
Fig. 11 illustrates two equivalent lists past joint policies indexed letters.
Note placeholders LIE ranked higher nodes L represent.
Let us write IT-IC(L) IT-ICE(LIE ) one iteration (i.e., one loop main repeat
Algorithm 1) respective algorithms. Let IT-ICE* denote operation repeats
IT-ICE long placeholder selected (so ends q expanded).
Lemma 4. L LIE , executing IT-IC(L) IT-ICE*(LIE ) leads new open lists
equivalent: L LIE .
Proof. IT-ICE* selects placeholder q, generates child q already present
L (due properties 3 4 Definition 11) inserts it. Insertion occurs
relative location IT-IC algorithms use comparison operator
(Definition 5). Together facts guarantee insertion preserves properties 1 2.
24. A.remove(B) removes elements B without changing ordering.

501

fiOliehoek, Spaan, Amato, & Whiteson

LIE

L
Q
Vb



7
5
4.5

c

e

3
3
2.5
1
0.5

f
g
h

j

Q

Vb



5



3
3

f
g

Vb
8

4




b



placeholder {c,e,j}



nodes: position




placeholder {h,i}
consistent ordering
equal values

Figure 11: Illustration equivalent lists. Past joint policies indexed letters.
example, b expanded earlier (but yet fully expanded ICE-case).
remaining unexpanded children q, IT-ICE* reinserts q updated heuristic
value q.v q .v guaranteed upper bound value unexpanded siblings
q since q .v = Vb (q .) Vb (q .) = q .v (preserving properties 3 4).
IT-ICE* finally selects non-placeholder q, guaranteed q
selected IT-IC (due properties 1 2). Expansion ICE generates one child q (again
inserted relative location IC) inserts placeholder q = hq., q .v, truei
siblings q (again preserving properties 3 4).
Proof Theorem 5. fact GMAA*-ICE GMAA*-IC search-equivalent follows directly Lemma 4. Search equivalence means algorithms select
non-placeholders q expand. Since algorithms begin identical (and therefore trivially equivalent) open lists, maintain equivalent open lists throughout search. such,
property 2 Definition 11 ensures every time IT-ICE* selects non-placeholder, IT-IC
selects too.

References
Allen, M., & Zilberstein, S. (2007). Agent influence predictor difficulty decentralized
problem-solving. Proceedings Twenty-Second AAAI Conference Artificial
Intelligence.
Amato, C., Bernstein, D. S., & Zilberstein, S. (2006). Optimal fixed-size controllers
decentralized POMDPs. Proc. AAMAS Workshop Multi-Agent Sequential
Decision Making Uncertain Domains.
Amato, C., Bernstein, D. S., & Zilberstein, S. (2007). Optimizing memory-bounded controllers
decentralized POMDPs. Proc. Uncertainty Artificial Intelligence.
Amato, C., Bernstein, D. S., & Zilberstein, S. (2010). Optimizing fixed-size stochastic controllers POMDPs decentralized POMDPs. Autonomous Agents Multi-Agent
Systems, 21 (3), 293320.
502

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

Amato, C., Bonet, B., & Zilberstein, S. (2010). Finite-state controllers based Mealy
machines centralized decentralized POMDPs. Proceedings TwentyFourth AAAI Conference Artificial Intelligence.
Amato, C., Carlin, A., & Zilberstein, S. (2007). Bounded dynamic programming decentralized POMDPs. Proc. AAMAS Workshop Multi-Agent Sequential
Decision Making Uncertain Domains.
Amato, C., Dibangoye, J. S., & Zilberstein, S. (2009). Incremental policy generation
finite-horizon DEC-POMDPs. Proc. International Conference Automated
Planning Scheduling.
Aras, R., & Dutech, A. (2010). investigation mathematical programming finite
horizon decentralized POMDPs. Journal Artificial Intelligence Research, 37 , 329
396.
Becker, R., Carlin, A., Lesser, V., & Zilberstein, S. (2009). Analyzing myopic approaches
multi-agent communication. Computational Intelligence, 25 (1), 3150.
Becker, R., Zilberstein, S., & Lesser, V. (2004). Decentralized Markov decision processes
event-driven interactions. Proc. International Conference Autonomous
Agents Multi Agent Systems.
Becker, R., Zilberstein, S., Lesser, V., & Goldman, C. V. (2003). Transition-independent
decentralized Markov decision processes. Proc. International Conference
Autonomous Agents Multi Agent Systems.
Bernstein, D. S., Amato, C., Hansen, E. A., & Zilberstein, S. (2009). Policy iteration
decentralized control Markov decision processes. Journal Artificial Intelligence
Research, 34 , 89132.
Bernstein, D. S., Givan, R., Immerman, N., & Zilberstein, S. (2002). complexity
decentralized control Markov decision processes. Mathematics Operations Research,
27 (4), 819840.
Bertsekas, D. P. (2005). Dynamic Programming Optimal Control (3rd ed., Vol. I). Athena
Scientific.
Beynier, A., & Mouaddib, A.-I. (2011). Solving efficiently decentralized MDPs temporal
resource constraints. Autonomous Agents Multi-Agent Systems, 23 (3), 486
539.
Boularias, A., & Chaib-draa, B. (2008). Exact dynamic programming decentralized
POMDPs lossless policy compression. Proc. International Conference
Automated Planning Scheduling.
Busoniu, L., Babuska, R., & De Schutter, B. (2008). comprehensive survey multi-agent
reinforcement learning. IEEE Transactions Systems, Man, Cybernetics, Part C:
Applications Reviews, 38 (2), 156172.
Carlin, A., & Zilberstein, S. (2008). Value-based observation compression DEC-POMDPs.
Proc. International Conference Autonomous Agents Multi Agent Systems.
503

fiOliehoek, Spaan, Amato, & Whiteson

Carrillo, H., & Lipman, D. (1988). multiple sequence alignment problem biology.
SIAM Journal Applied Mathematics, 48 (5), 10731082.
Cassandra, A., Littman, M. L., & Zhang, N. L. (1997). Incremental pruning: simple, fast,
exact method partially observable Markov decision processes. Proc. Uncertainty
Artificial Intelligence.
Cassandra, A. R. (1998). Exact Approximate Algorithms Partially Observable Markov
Decision Processes. Unpublished doctoral dissertation, Brown University.
Dechter, R., Flerova, N., & Marinescu, R. (2012). Search algorithms best solutions
graphical models. Proceedings Twenty-Sixth AAAI Conference Artificial
Intelligence.
Dekel, E., Fudenberg, D., & Morris, S. (2006). Topologies types. Theoretical Economics,
1 (3), 275309.
Dibangoye, J. S., Amato, C., Doniec, A., & Charpillet, F. (2013). Producing efficient errorbounded solutions transition independent decentralized MDPs. Proc. International Conference Autonomous Agents Multi Agent Systems. (Submitted
publication)
Dibangoye, J. S., Mouaddib, A.-I., & Chai-draa, B. (2009). Point-based incremental pruning heuristic solving finite-horizon DEC-POMDPs. Proc. International
Conference Autonomous Agents Multi Agent Systems.
Doshi, P., & Gmytrasiewicz, P. (2009). Monte Carlo sampling methods approximating
interactive POMDPs. Journal Artificial Intelligence Research, 34 , 297337.
Doshi, P., Zeng, Y., & Chen, Q. (2008). Graphical models interactive POMDPs: representations solutions. Autonomous Agents Multi-Agent Systems, 18 (3), 376416.
Edelkamp, S., & Schrodl, S. (2012). Heuristic search: theory applications. Morgan
Kaufmann.
Eker, B., & Akn, H. L. (2010). Using evolution strategies solve DEC-POMDP problems.
Soft ComputingA Fusion Foundations, Methodologies Applications, 14 (1), 35
47.
Eker, B., & Akn, H. L. (2013). Solving decentralized POMDP problems using genetic
algorithms. Autonomous Agents Multi-Agent Systems, 27 (1), 161196.
Emery-Montemerlo, R. (2005). Game-Theoretic Control Robot Teams. Unpublished
doctoral dissertation, Carnegie Mellon University.
Emery-Montemerlo, R., Gordon, G., Schneider, J., & Thrun, S. (2004). Approximate solutions partially observable stochastic games common payoffs. Proc.
International Conference Autonomous Agents Multi Agent Systems.
Emery-Montemerlo, R., Gordon, G., Schneider, J., & Thrun, S. (2005). Game theoretic
control robot teams. Proc. IEEE International Conference Robotics
Automation.
Givan, R., Dean, T., & Greig, M. (2003). Equivalence notions model minimization
Markov decision processes. Artificial Intelligence, 14 (12), 163223.
504

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

Gmytrasiewicz, P. J., & Doshi, P. (2005). framework sequential planning multi-agent
settings. Journal Artificial Intelligence Research, 24 , 4979.
Goldman, C. V., Allen, M., & Zilberstein, S. (2007). Learning communicate decentralized environment. Autonomous Agents Multi-Agent Systems, 15 (1), 4790.
Goldman, C. V., & Zilberstein, S. (2003). Optimizing information exchange cooperative
multi-agent systems. Proc. International Conference Autonomous Agents
Multi Agent Systems.
Goldman, C. V., & Zilberstein, S. (2004). Decentralized control cooperative systems:
Categorization complexity analysis. Journal Artificial Intelligence Research, 22 ,
143174.
Goldman, C. V., & Zilberstein, S. (2008). Communication-based decomposition mechanisms
decentralized MDPs. Journal Artificial Intelligence Research, 32 , 169202.
Hansen, E. A., Bernstein, D. S., & Zilberstein, S. (2004). Dynamic programming partially observable stochastic games. Proc. National Conference Artificial
Intelligence.
Hauskrecht, M. (2000). Value-function approximations partially observable Markov decision processes. Journal Artificial Intelligence Research, 13 , 3394.
Hsu, K., & Marcus, S. (1982). Decentralized control finite state Markov processes. IEEE
Transactions Automatic Control , 27 (2), 426431.
Huhns, M. N. (Ed.). (1987). Distributed Artificial Intelligence. Pitman Publishing Ltd.
Ikeda, T., & Imai, H. (1999). Enhanced A* algorithms multiple alignments: optimal
alignments several sequences k-opt approximate alignments large cases. Theoretical Computer Science, 210 (2), 341374.
Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1998). Planning acting partially
observable stochastic domains. Artificial Intelligence, 101 (1-2), 99134.
Kumar, A., & Zilberstein, S. (2009). Constraint-based dynamic programming decentralized
POMDPs structured interactions. Proc. International Conference
Autonomous Agents Multi Agent Systems.
Kumar, A., & Zilberstein, S. (2010a). Anytime planning decentralized POMDPs using
expectation maximization. Proc. Uncertainty Artificial Intelligence.
Kumar, A., & Zilberstein, S. (2010b). Point-based backup decentralized POMDPs: Complexity new algorithms. Proc. International Conference Autonomous
Agents Multi Agent Systems.
Littman, M., Cassandra, A., & Kaelbling, L. (1995). Learning policies partially observable environments: Scaling up. Proc. International Conference Machine
Learning.
Marecki, J., Gupta, T., Varakantham, P., Tambe, M., & Yokoo, M. (2008). agents
equal: scaling distributed POMDPs agent networks. Proc. International
Conference Autonomous Agents Multi Agent Systems.
505

fiOliehoek, Spaan, Amato, & Whiteson

Marecki, J., & Tambe, M. (2007). opportunistic techniques solving decentralized
Markov decision processes temporal constraints. Proc. International
Conference Autonomous Agents Multi Agent Systems.
Melo, F. S., & Veloso, M. (2011). Decentralized MDPs sparse interactions. Artificial
Intelligence, 175 (11), 17571789.
Mostafa, H., & Lesser, V. (2011). compact mathematical formulation problems
structured agent interactions. Proc. AAMAS Workshop Multi-Agent Sequential Decision Making Uncertain Domains.
Nair, R., Roth, M., & Yohoo, M. (2004). Communication improving policy computation
distributed POMDPs. Proc. International Conference Autonomous Agents
Multi Agent Systems.
Nair, R., Tambe, M., Yokoo, M., Pynadath, D. V., & Marsella, S. (2003). Taming decentralized POMDPs: Towards efficient policy computation multiagent settings. Proc.
International Joint Conference Artificial Intelligence.
Nair, R., Varakantham, P., Tambe, M., & Yokoo, M. (2005). Networked distributed POMDPs:
synthesis distributed constraint optimization POMDPs. Proc. National Conference Artificial Intelligence.
Oliehoek, F. A. (2010). Value-Based Planning Teams Agents Stochastic Partially Observable Environments. Amsterdam University Press. (Doctoral dissertation, University
Amsterdam)
Oliehoek, F. A. (2012). Decentralized POMDPs. M. Wiering & M. van Otterlo (Eds.),
Reinforcement learning: State art (Vol. 12). Springer Berlin Heidelberg.
Oliehoek, F. A., Kooi, J. F., & Vlassis, N. (2008). cross-entropy method policy search
decentralized POMDPs. Informatica, 32 , 341357.
Oliehoek, F. A., & Spaan, M. T. J. (2012). Tree-based solution methods multiagent
POMDPs delayed communication. Proceedings Twenty-Sixth AAAI Conference Artificial Intelligence.
Oliehoek, F. A., Spaan, M. T. J., Dibangoye, J., & Amato, C. (2010). Heuristic search identical payoff Bayesian games. Proc. International Conference Autonomous
Agents Multi Agent Systems.
Oliehoek, F. A., Spaan, M. T. J., & Vlassis, N. (2007). Dec-POMDPs delayed communication. Proc. AAMAS Workshop Multi-Agent Sequential Decision Making
Uncertain Domains.
Oliehoek, F. A., Spaan, M. T. J., & Vlassis, N. (2008). Optimal approximate Q-value
functions decentralized POMDPs. Journal Artificial Intelligence Research, 32 ,
289353.
Oliehoek, F. A., Spaan, M. T. J., Whiteson, S., & Vlassis, N. (2008). Exploiting locality
interaction factored Dec-POMDPs. Proc. International Conference
Autonomous Agents Multi Agent Systems.
Oliehoek, F. A., & Vlassis, N. (2007). Q-value functions decentralized POMDPs. Proc.
International Conference Autonomous Agents Multi Agent Systems.
506

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

Oliehoek, F. A., Whiteson, S., & Spaan, M. T. J. (2009). Lossless clustering histories
decentralized POMDPs. Proc. International Conference Autonomous
Agents Multi Agent Systems.
Oliehoek, F. A., Whiteson, S., & Spaan, M. T. J. (2013). Approximate solutions factored Dec-POMDPs many agents. Proc. International Conference
Autonomous Agents Multi Agent Systems. (Submitted publication)
Oliehoek, F. A., Witwicki, S., & Kaelbling, L. P. (2012). Influence-based abstraction
multiagent systems. Proceedings Twenty-Sixth AAAI Conference Artificial
Intelligence.
Ooi, J. M., & Wornell, G. W. (1996). Decentralized control multiple access broadcast
channel: Performance bounds. Proc. 35th conference decision control.
Osborne, M. J., & Rubinstein, A. (1994). course game theory. MIT Press.
Pajarinen, J., & Peltonen, J. (2011). Efficient planning factored infinite-horizon DECPOMDPs. Proc. International Joint Conference Artificial Intelligence.
Panait, L., & Luke, S. (2005). Cooperative multi-agent learning: state art.
Autonomous Agents Multi-Agent Systems, 11 (3), 387434.
Puterman, M. L. (1994). Markov Decision ProcessesDiscrete Stochastic Dynamic Programming. John Wiley & Sons, Inc.
Pynadath, D. V., & Marsella, S. C. (2007). Minimal mental models. Proceedings
Twenty-Second AAAI Conference Artificial Intelligence.
Pynadath, D. V., & Tambe, M. (2002). communicative multiagent team decision problem:
Analyzing teamwork theories models. Journal Artificial Intelligence Research,
16 , 389423.
Rabinovich, Z., Goldman, C. V., & Rosenschein, J. S. (2003). complexity multiagent
systems: price silence. Proc. International Conference Autonomous
Agents Multi Agent Systems.
Rabinovich, Z., & Rosenschein, J. S. (2005). Multiagent coordination extended Markov
tracking. Proc. International Conference Autonomous Agents Multi
Agent Systems.
Ross, S., Pineau, J., Paquet, S., & Chaib-draa, B. (2008). Online planning algorithms
POMDPs. Journal Artificial Intelligence Research, 32 , 664704.
Roth, M., Simmons, R., & Veloso, M. (2005). Reasoning joint beliefs executiontime communication decisions. Proc. International Conference Autonomous
Agents Multi Agent Systems.
Roth, M., Simmons, R., & Veloso, M. (2007). Exploiting factored representations decentralized execution multi-agent teams. Proc. International Conference
Autonomous Agents Multi Agent Systems.
Seuken, S., & Zilberstein, S. (2007a). Improved memory-bounded dynamic programming
decentralized POMDPs. Proc. Uncertainty Artificial Intelligence.
507

fiOliehoek, Spaan, Amato, & Whiteson

Seuken, S., & Zilberstein, S. (2007b). Memory-bounded dynamic programming DECPOMDPs. Proc. International Joint Conference Artificial Intelligence.
Seuken, S., & Zilberstein, S. (2008). Formal models algorithms decentralized decision
making uncertainty. Autonomous Agents Multi-Agent Systems, 17 (2), 190
250.
Spaan, M. T. J., Gordon, G. J., & Vlassis, N. (2006). Decentralized planning uncertainty teams communicating agents. Proc. International Conference
Autonomous Agents Multi Agent Systems.
Spaan, M. T. J., & Melo, F. S. (2008). Interaction-driven Markov games decentralized
multiagent planning uncertainty. Proc. International Conference
Autonomous Agents Multi Agent Systems.
Spaan, M. T. J., & Oliehoek, F. A. (2008). MultiAgent Decision Process toolbox:
software decision-theoretic planning multiagent systems. Proc. AAMAS
Workshop Multi-Agent Sequential Decision Making Uncertain Domains.
Spaan, M. T. J., Oliehoek, F. A., & Amato, C. (2011). Scaling optimal heuristic search
Dec-POMDPs via incremental expansion. Proc. International Joint Conference
Artificial Intelligence.
Spaan, M. T. J., Oliehoek, F. A., & Vlassis, N. (2008). Multiagent planning uncertainty
stochastic communication delays. Proc. International Conference
Automated Planning Scheduling.
Sycara, K. P. (1998). Multiagent systems. AI Magazine, 19 (2), 7992.
Szer, D., Charpillet, F., & Zilberstein, S. (2005). MAA*: heuristic search algorithm
solving decentralized POMDPs. Proc. Uncertainty Artificial Intelligence.
Tsitsiklis, J., & Athans, M. (1985). complexity decentralized decision making
detection problems. IEEE Transactions Automatic Control , 30 (5), 440446.
Varaiya, P., & Walrand, J. (1978). delayed sharing patterns. IEEE Transactions
Automatic Control , 23 (3), 443445.
Varakantham, P., Kwak, J. young, Taylor, M. E., Marecki, J., Scerri, P., & Tambe, M. (2009).
Exploiting coordination locales distributed POMDPs via social model shaping.
Proc. International Conference Automated Planning Scheduling.
Varakantham, P., Marecki, J., Yabu, Y., Tambe, M., & Yokoo, M. (2007). Letting loose
SPIDER network POMDPs: Generating quality guaranteed policies. Proc.
International Conference Autonomous Agents Multi Agent Systems.
Varakantham, P., Nair, R., Tambe, M., & Yokoo, M. (2006). Winning back cup distributed POMDPs: planning continuous belief spaces. Proc. International
Conference Autonomous Agents Multi Agent Systems.
Velagapudi, P., Varakantham, P., Scerri, P., & Sycara, K. (2011). Distributed model shaping
scaling decentralized POMDPs hundreds agents. Proc. International Conference Autonomous Agents Multi Agent Systems.
Vlassis, N. (2007). Concise Introduction Multiagent Systems Distributed Artificial
Intelligence. Morgan & Claypool Publishers.
508

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

Williamson, S. A., Gerding, E. H., & Jennings, N. R. (2009). Reward shaping valuing communications multi-agent coordination. Proc. International Conference
Autonomous Agents Multi Agent Systems.
Witwicki, S. J. (2011). Abstracting Influences Efficient Multiagent Coordination
Uncertainty. Unpublished doctoral dissertation, University Michigan, Ann Arbor,
Michigan, USA.
Witwicki, S. J., & Durfee, E. H. (2010). Influence-based policy abstraction weakly-coupled
Dec-POMDPs. Proc. International Conference Automated Planning
Scheduling.
Wu, F., Zilberstein, S., & Chen, X. (2010a). Point-based policy generation decentralized
POMDPs. Proc. International Conference Autonomous Agents Multi
Agent Systems.
Wu, F., Zilberstein, S., & Chen, X. (2010b). Rollout sampling policy iteration decentralized
POMDPs. Proc. Uncertainty Artificial Intelligence.
Wu, F., Zilberstein, S., & Chen, X. (2011). Online planning multi-agent systems
bounded communication. Artificial Intelligence, 175 (2), 487511.
Xuan, P., Lesser, V., & Zilberstein, S. (2001). Communication decisions multi-agent cooperation: Model experiments. Proc. International Conference Autonomous
Agents.
Yoshizumi, T., Miura, T., & Ishida, T. (2000). A* partial expansion large branching
factor problems. Proc. National Conference Artificial Intelligence.
Zeng, Y., & Doshi, P. (2012). Exploiting model equivalences solving interactive dynamic
influence diagrams. Journal Artificial Intelligence Research, 43 , 211255.
Zeng, Y., Doshi, P., Pan, Y., Mao, H., Chandrasekaran, M., & Luo, J. (2011). Utilizing
partial policies identifying equivalence behavioral models. Proceedings
Twenty-Fifth AAAI Conference Artificial Intelligence.

509


