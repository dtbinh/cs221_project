Journal Artificial Intelligence Research 24 (2005) 81-108

Submitted 12/04; published 07/05

Risk-Sensitive Reinforcement Learning Applied Control
Constraints
Peter Geibel

pgeibel@uos.de

Institute Cognitive Science, AI Group
University Osnabruck, Germany

Fritz Wysotzki

wysotzki@cs.tu-berlin.de

Faculty Electrical Engineering Computer Science, AI Group
TU Berlin, Germany

Abstract
paper, consider Markov Decision Processes (MDPs) error states. Error
states states entering undesirable dangerous. define risk
respect policy probability entering state policy
pursued. consider problem finding good policies whose risk smaller
user-specified threshold, formalize constrained MDP two criteria.
first criterion corresponds value function originally given. show
risk formulated second criterion function based cumulative return,
whose definition independent original value function. present model free,
heuristic reinforcement learning algorithm aims finding good deterministic policies.
based weighting original value function risk. weight parameter
adapted order find feasible solution constrained problem good
performance respect value function. algorithm successfully applied
control feed tank stochastic inflows lies upstream distillation
column. control task originally formulated optimal control problem
chance constraints, solved certain assumptions model obtain
optimal solution. power learning algorithm used even
restrictive assumptions relaxed.

1. Introduction
Reinforcement Learning, research area, provides range techniques applicable difficult nonlinear stochastic control problems (see e.g. Sutton & Barto, 1998;
Bertsekas & Tsitsiklis, 1996). reinforcement learning (RL) agent considered
learns control process. agent able perceive state process,
acts order maximize cumulative return based real valued reward
signal. Often, experiences process used improve agents policy instead
previously given analytical model.
notion risk RL related fact, even optimal policy may perform
poorly cases due stochastic nature problem. risk-sensitive RL
approaches concerned variance return, worst outcomes,
(e.g. Coraluppi & Marcus, 1999; Heger, 1994; Neuneier & Mihatsch, 1999), see
discussion section 3. take alternative view risk defined Geibel (2001)
concerned variability return, occurrence errors
c
2005
AI Access Foundation. rights reserved.

fiGeibel & Wysotzki

undesirable states underlying Markov Decision Process (MDP). means
address different class problems compared approaches referring variability
return.
paper, consider constrained MDPs two criteria usual value function risk second value function. value optimized risk
must remain specified threshold. describe heuristic algorithm based
weighted formulation finds feasible policy original constrained problem.
order offer insight behavior algorithm, investigate application algorithm simple grid world problem discounted criterion function.
apply algorithm stochastic optimal control problem continuous states,
set feasible solutions restricted constraint required hold
certain probability, thus demonstrating practical applicability approach.
consider control feed tank lies upstream distillation column respect
two objectives: (1) outflow tank required stay close specified value
order ensure optimal operation distillation column, (2) tank level
substance concentrations required remain within specified intervals, certain
admissible chance constraint violation.
Li, Wendt, Arellano-Garcia, Wozny (2002) formulate problem quadratic
program chance constraints1 (e.g. Kall & Wallace, 1994), relaxed nonlinear
program case Gaussian distributions random input variables systems
whose dynamics given linear equations. nonlinear program solved
sequential quadratic programming.
Note approach Li et al. involves simulation based estimation
gradients chance constraints (Li et al., 2002, p. 1201). Q-learning (Watkins,
1989; Watkins & Dayan, 1992; Sutton & Barto, 1998), learning algorithm based
simulating episodes estimating value risk states, tank control task
correspond measure deviation optimal outflow probability
constraint violation, respectively.
contrast approach Li et al. (2002), RL algorithm applicable systems
continuous state spaces, whose system dynamics governed nonlinear equations
involve randomization noise arbitrary distributions random variables,
makes prior assumptions either aspect. special property
learning algorithm, holds true e.g. Q-learning RL algorithms.
convergence Q-learning combined function approximation techniques necessary
continuous state spaces cannot guaranteed general (e.g. Sutton & Barto, 1998).
holds true algorithm. Nevertheless, RL algorithms successfully applied
many difficult problems continuous state spaces nonlinear dynamics (see e.g.
Sutton & Barto, 1998; Crites & Barto, 1998; Smart & Kaelbling, 2002; Stephan, Debes,
Gross, Wintrich, & Wintrich, 2001).
1. constraint seen relation domains variables restricting possible values.
variables constraint C = C(x1 , . . . , xn ) random, constraint hold certain
probability. Chance constrained programming particular approach stochastic programming
considers constrained optimization problems containing random variables so-called chance
constraints form P(C) p p [0, 1] formulated.

82

fiRisk-Sensitive Reinforcement Learning

article organized follows. section 2, RL framework described. Section 3 reviews related work risk-sensitive approaches. Section 4 describes approach
risk-sensitive RL. section 5, elucidate heuristic learning algorithm solving
constrained problem using weighted formulation. section 6, describe application
grid world problem. tank control task described section 7. section 8,
experiments feed tank control described. Section 9 concludes short
summary outlook.

2. RL Framework
RL one considers agent interacts process controlled.
discrete time-step, agent observes state x takes action u general
depends x. action agent causes environment change state x
according probability px,u (x ). section 7, consider set states, X,
finite set.
action set agent assumed finite, allowed depend
current state. state x, agent uses action set U (x) possible actions.
taking action u U (x), agent receives real valued reinforcement signal rx,u (x )
depends action taken successor state x . case random reward
signal, rx,u (x ) corresponds expected value. Markov property MDP requires
probability distribution successor states one rewards depend
current state action only. distributions change additional
information past states, actions rewards considered, i.e. independent
path leading current state.
aim agent find policy selecting actions maximizes
cumulative reward, called return. return defined
R=


X

rt ,

(1)

t=0

random variable rt denotes reward occurring t-th time step
agent uses policy . Let x0 , x1 , x2 , . . . denote corresponding probabilistic sequence
states, ui sequence actions chosen according policy .
constant [0, 1] discount factor allows control influence future
rewards. expectation return,
h



V (x) = E R | x0 = x ,

(2)

defined value x respect . well-known exist stationary

deterministic policies V (x) optimal (maximal) every state x. stationary deterministic policy function maps states actions particularly defined
independent time Markovian (independent history). work,
use term maximum-value policies instead optimal policies distinguish
minimum-risk policies optimal sense, see section 4.1.
usual, define state/action value function



h



Q (x, u) = E r0 + V (x1 ) x0 = x, u0 = u .
83

(3)

fiGeibel & Wysotzki

Q (x, u) expected return agent first chooses action u, acts according
subsequent time steps. optimal Q-function Q , optimal policies
unique optimal values V derived (x) argmaxu Q (x, u) V (x) = Q (x, (x)).
Q computed using Watkins Q-learning algorithm.
RL one general distinguishes episodic continuing tasks treated
framework (see e.g. Sutton & Barto, 1998). episodic tasks, agent may
reach terminal absorbing state time . reaching absorbing state,
agent stays executes dummy action. reward defined rt = 0
. learning agent restarted according distribution
initial states reached absorbing state.

3. Related Work
P


random variable R =
t=0 rt (return) used define value state possesses
certain variance. risk-averse approaches dynamic programming (DP) reinforcement learning concerned variance R, worst outcomes.
example approach worst case control (e.g. Coraluppi & Marcus, 1999; Heger,
1994), worst possible outcome R optimized. risk-sensitive control based use exponential utility functions (e.g. Liu, Goodwin, & Koenig, 2003a;
Koenig & Simmons, 1994; Liu, Goodwin, & Koenig, 2003b; Borkar, 2002), return R
transformed reflect subjective measure utility. Instead maximizing
expected value R, objective maximize e.g. U = 1 log E(eR ),
parameter R usual return. shown depending parameter
, policies high variance V(R) penalized ( < 0) enforced ( > 0). value-criterion introduced Heger (1994) seen extension worst case control
bad outcomes policy occur probability less neglected.
Neuneier Mihatsch (1999) give model- free RL algorithm based
parameterized transformation temporal difference errors occurring (see Mihatsch
& Neuneier, 2002). parameter transformation allows switch riskaverse risk-seeking policies. influence parameter value function cannot
expressed explicitly.
view risk concerned variance return worst possible
outcomes, instead fact processes generally possess dangerous undesirable states. Think chemical plant temperature pressure exceeding
threshold may cause plant explode. controlling plant, return corresponds plants yield. seems inappropriate let return reflect
cost explosion, e.g. human lives affected.
work, consider processes undesirable terminal states. seemingly straightforward way handle error states system provide high
negative rewards systems enters error state. optimal policy avoid
error states general. drawback approach fact unknown
large risk (probability) entering error state is. Moreover, may want provide
threshold probability entering error state must exceeded
agents policy. general, impossible completely avoid error states, risk
controllable extend. precisely, agent placed state

84

fiRisk-Sensitive Reinforcement Learning

x, follow policy whose risk constrained . parameter [0, 1]
reflects agents risk-averseness. is, goal minimization risk,
maximization V risk kept threshold .
Markowitz (1952) considers combination different criteria equal discount
factors context portfolio selection. risk selected portfolio related
variance combined (weighted) criteria. Markowitz introduces notion
(E, V )-space. notion risk related variance V , depends
occurrence error states MDP. Therefore risk conceptually independent V ,
see e.g. tank control problem described section 7.
idea weighting return risk (Markowitz, 1959; Freund, 1956; Heger, 1994) leads
expected-value-minus-variance-criterion, E(R) kV(R), k parameter.
use idea computing feasible policy problem finding good policy
constrained risk (in regard probability entering error state): value
risk weighted using weight value weight 1 risk. value
increased, giving value weight compared risk, risk state
becomes larger user-specified threshold .
considering ordering relation tuples values, learning algorithm
fixed value related ARTDP approach Gabor, Kalmar, Szepesvari
(1998). article, Gabor et al. additionally propose recursive formulation
MDP constraints may produce suboptimal solutions. applicable
case approach requires nonnegative reward function.
noted aforementioned approaches based variability
return suited problems grid world problem discussed section 6,
tank control task section 7 risk related parameters (variables) state
description. example, grid world problem, policies worst case
outcome. regard approaches based variance, found policy leading
error states fast possible higher variance one reaches
goal states fast possible. policy small variance therefore large risk
(with respect probability entering error state), means address
different class control problems. underpin claim section 8.1.3.
Fulkerson, Littman, Keim (1998) sketch approach framework probabilistic planning similar although based complementary notion
safety. Fulkerson et al. define safety probability reaching goal state (see
BURIDAN system Kushmerick, Hanks, & Weld, 1994). Fulkerson et al. discuss
problem finding plan minimum cost subject constraint safety (see
Blythe, 1999). episodic MDP goal states, safety 1 minus risk.
continuing tasks absorbing states neither goal error states,
safety may correspond smaller value. Fulkerson et al. (1998) manipulate (scale)
(uniform) step reward undiscounted cost model order enforce agent reach
goal quickly (see Koenig & Simmons, 1994). contrast, consider
discounted MDPs, neither require existence goal states. Although
change original reward function, algorithm section 5 seen systematic
approach dealing idea Fulkerson et al. consists modification
relative importance original objective (reaching goal) safety. contrast
aforementioned approaches belonging field probabilistic planning,
85

fiGeibel & Wysotzki

operate previously known finite MDP, designed online learning algorithm
uses simulated actual experiences process. use neural network
techniques algorithm applied continuous-state processes.
Dolgov Durfee (2004) describe approach computes policies
constrained probability violating given resource constraints. notion risk
similar described Geibel (2001). algorithm given Dolgov Durfee
(2004) computes suboptimal policies using linear programming techniques require
previously known model and, contrast approach, cannot easily extended
continuous state spaces. Dolgov Durfee included discussion DP approaches
constrained MDPs (e.g. Altman, 1999) generalize continuous state
spaces (as tank control task) require known model. algorithm described
Feinberg Shwartz (1999) constrained problems two criteria applicable
case, requires discount factors strictly smaller 1,
limited finite MDPs.
Downside risk common notion finance refers likelihood security
investment declining price, amount loss could result
potential decline. scientific literature downside risk (e.g. Bawas, 1975; Fishburn,
1977; Markowitz, 1959; Roy, 1952) investigates risk-measures particularly consider
case return lower mean value, target value encountered.
contrast, notion risk coupled return R, fact state
x error state, example, parameters describing state lie outside
permissible ranges, state lies inside obstacle may occur
robotics applications.

4. Risk
define notion risk precisely, consider set
X

(4)

error states. Error states terminal states. means control agent
ends reaches state . allow additional set non-error terminal states
= .
Now, define risk x respect probability state sequence
(xi )i0 x0 = x, generated executing policy , terminates error state
x .
Definition 4.1 (Risk) Let policy, let x state. risk defined




(x) = P xi | x0 = x .

(5)

definition, (x) = 1 holds x . x , (x) = 0 = .
states 6 , risk depends action choices policy .
following subsection, consider computation minimum-risk policies
analogous computation maximum-value policies.
86

fiRisk-Sensitive Reinforcement Learning

4.1 Risk Minimization
risk considered value function defined cost signal r. see this,
augment state space MDP additional absorbing state
agent transfered reaching state . state introduced technical
reasons.
agent reaches state , reward signals r r become zero.
set r = 0 r = 1, agent reaches error state. states
longer absorbing states. new cost function r defined


rx,u (x ) =

(

1 x x =
0 else.

(6)

construction cost function r, episode states, actions costs
starting initial state x contains exactly cost r = 1 error state
occurs it. process enter error state, sequence r-costs contains
zeros only. Therefore, probability defining risk expressed expectation
cumulative return.
Proposition 4.1 holds


(x) = E

"
X
i=0

discount factor = 1.

#


ri x0 = x


(7)

Proof: r0 , r1 , . . . probabilistic sequence costs related risk. stated
P

above, holds R =def
i=0 ri = 1 trajectory leads error state; otherwise
P
i=0 ri = 0. means return R Bernoulli random variable,
probability q R = 1 corresponds risk x respect . Bernoulli random
variable holds ER = q (see e.g. Ross, 2000). Notice introduction together
fact r = 1 occurs transition error state
iwhen

hP ,



entering respective error state, ensures correct value E
i=0 ri x0 = x
error states x. q.e.d.
Similar Q-function define state/action risk
h

Q (x, u) = E r0 + (x1 ) | x0 = x, u0 = u
=

X





px,u (x ) rx,u (x ) + (x ) .

x



(8)
(9)

Minimum-risk policies obtained variant Q-learning algorithm (Geibel,
2001).
4.2 Maximized Value, Constrained Risk
general, one interested policies minimum risk. Instead, want provide
parameter specifies risk willing accept. Let X X set
states interested in, e.g. X = X ( {}) X = {x0 } distinguished
87

fiGeibel & Wysotzki

starting state x0 . state x X , let px probability selecting starting
state. value
X
V =def
px V (x)
(10)
xX

corresponds performance states X . consider constrained problem
max V

(11)

x X : (x) .

(12)



subject
policy fulfills (12) called feasible. Depending , set feasible policies
may empty. Optimal policies generally depend starting state, nonstationary randomized (Feinberg & Shwartz, 1999; Gabor et al., 1998; Geibel, 2001).
restrict considered policy class stationary deterministic policies, constrained
problem generally well defined X singleton, need
stationary deterministic policy optimal states X . Feinberg Shwartz
(1999) shown case two unequal discount factors smaller 1
exist optimal policies randomized Markovian time step n (i.e.
depend history, may non-stationary randomized), stationary
deterministic (particularly Markovian) time step n onwards. Feinberg Shwartz
(1999) give DP algorithm case (cp. Feinberg & Shwartz, 1994). cannot
applied case = 1, generalize continuous state
spaces. case equal discount factors, shown Feinberg Shwartz (1996)
(for fixed starting state) exist optimal stationary randomized policies
case one constraint consider one action stationary deterministic
policy, i.e. one state policy chooses randomly two
actions.

5. Learning Algorithm
reasons efficiency predictability agents behavior
said end last section, restrict consideration stationary deterministic policies. following present heuristic algorithm aims
computing good policy. assume reader familiar Watkins Q-learning
algorithm (Watkins, 1989; Watkins & Dayan, 1992; Sutton & Barto, 1998).
5.1 Weighting Risk Value
define new (third) value function V state/action value function Q
weighted sum risk value
V (x) = V (x) (x)
Q (x, u)





= Q (x, u) Q (x, u) .

(13)
(14)

parameter 0 determines influence V -values (Q -values) compared
-values (Q -values). = 0, V corresponds negative . means
88

fiRisk-Sensitive Reinforcement Learning

maximization V0 lead minimization . , maximization
V leads lexicographically optimal policy unconstrained, unweighted 2-criteria
problem. one compares performance two policies lexicographically, criteria
ordered. large values , original value function multiplied dominates
weighted criterion.
weight successively adapted starting = 0, see section 5.3. adaptation
, discuss learning fixed proceeds.
5.2 Learning fixed
fixed value , learning algorithm computes optimal policy using
algorithm resembles Q-Learning based ARTDP approach Gabor
et al. (1998).
learning, agent estimates Qt , Qt time 0, thus estimate
Qt performance current greedy policy, policy selects best
action respect current estimate Qt . values updated using example
state transitions: let x current state, u chosen action, x observed
successor state. reward risk signal example state transition given
r r respectively. x , greedy action defined following manner: action
u preferable u Qt (x , u) > Qt (x , u ) holds. equality holds, action
higher Qt -value preferred. write u u , u preferable u .
Let u greedy action x respect ordering . agents
estimates updated according
Qt+1 (x, u) = (1 )Qt (x, u) + (r + Qt (x , u ))

(15)

Qt+1 (x, u) = (1 )Qt (x, u) + (r + Qt (x , u ))

(16)

t+1
Qt+1
(x, u) Qt+1 (x, u)
(x, u) = Q

(17)

Every time new chosen, learning rate set 1. Afterwards decreases
time (cp. Sutton & Barto, 1998).
fixed , algorithm aims computing good stationary deterministic policy
weighted formulation feasible original constrained problem. Existence
optimal stationary deterministic policy weighted problem convergence
learning algorithm guaranteed criteria discount factor, i.e.
= , even < 1. case = , Q forms standard criterion function
rewards r r. consider risk second criterion function, = implies
= = 1. ensure convergence case required either (a)
exists least one proper policy (defined policy reaches absorbing state
probability one), improper policies yield infinite costs (see Tsitsiklis, 1994), (b),
policies proper. case application example. conjecture
case < convergence possibly suboptimal policy guaranteed MDP forms
directed acyclic graph (DAG). cases oscillations non-convergence may occur,
optimal policies weighted problem generally found considered
policy class stationary deterministic policies (as constrained problem).
89

fiGeibel & Wysotzki

5.3 Adaptation
learning starts, agent chooses = 0 performs learning steps lead,
time, approximated minimum-risk policy 0 . policy allows agent
determine constrained problem feasible.
Afterwards value increased step step risk state X becomes
larger . Increasing increases influence Q-values compared
Q-values. may cause agent select actions result higher value,
perhaps higher risk. increasing , agent performs learning
steps greedy policy sufficiently stable. aimed producing optimal
deterministic policy . computed Q- Q-values old (i.e. estimates




.
Q Q ) used initialization computing +
aim increasing give value function V maximum influence possible.
means value maximized, needs chosen user.
adaptation provides means searching space feasible policies.

5.4 Using Discounted Risk
order prevent oscillations algorithm section 5.2 case < , may
advisable set = corresponding using discounted risk defined
(x)

=E

"
X
i=0

#


ri x0 = x .


(18)

values ri positive, holds (x) (x) states x. discounted risk (x) gives weight error states occurring near future, depending
value .
finite MDP fixed , convergence algorithm optimal stationary
policy weighted formulation guaranteed Q (using (x)) forms
standard criterion function rewards r r. terminating adaptation
case risk state X becomes larger , one might still use original
(undiscounted) risk (x) learning done discounted version (x), i.e.
learning algorithm maintain two risk estimates every state, major
problem. Notice case = , effect considering weighted criterion
V corresponds modifying unscaled original reward function r adding
negative reward 1 agent enters error state: set optimal stationary
deterministic policies equal cases (where added absorbing state
single dummy action neglected).
section 6, experiments case < 1 = , X = X ( {}), finite
state space found. sections 7 8 consider application example
infinite state space, X = {x0 }, = = 1.

6. Grid World Experiment
following study behaviour learning algorithm finite MDP
discounted criterion. contrast continuous-state case discussed next
90

fiRisk-Sensitive Reinforcement Learning

E
E
E
a)
E
E G
E E
E
E
E
c)
E
E G
E E

E G
E
E
b)
E
E G
E E E E E E
E G
E
E
d)
E
E G
E E E E E E

G

E





E

E





E

E E
G




E E

Figure 1: a) example grid world, x : horizontal, : vertical. explanation see
text. b) Minimum risk policy ( = 0) 11 unsafe states. c) Maximum value
policy ( = 4.0) 13 unsafe states. d) Result algorithm: policy = 0.64
11 unsafe states.

section, function approximation neural networks needed value
function risk stored table. grid world, chosen <
1 = , X = X , state graph DAG. implies
stationary policy optimal every state X . Although oscillations therefore
expected, found algorithm stabilizes feasible policy
learning rate tends zero. investigated use discounted risk
prevents oscillatory behaviour.
consider 6 6 grid world depicted Figure 1(a). empty field denotes
state, Es denote error states, two Gs denote two goal states. describe
states pairs (x, y) x, {1, 2, 3, 4, 5, 6}. I.e. = {(2, 2), (6, 6)}, = {(1, 1), (1,
2), (1, 3), (1, 4), (1, 5), (1, 6), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)}. additional absorbing
state depicted.
chosen error states lower, i.e. extremal values x
dangerous. One goal states placed next error states, safer
part state space.
agent actions U = {, , , }. action u U takes agent
denoted direction possible. probability 0.21, agent transported
desired direction one three remaining directions.
agent receives reward 1 enters goal state. agent receives reward
0 every case. noted explicit punishment entering
error state, implicit one: agent enters error state, current
episode ends. means agent never receive positive reward
reached error state. Therefore, try reach one goal states,
< 1, try fast possible.
91

fiGeibel & Wysotzki

chosen X = X ( {}), = 0.9, equal probabilities px states.
Although convergence algorithm cannot guaranteed case, experimental results show algorithm yields feasible policy.
selected = 0.13. order illustrate behaviour algorithm
computed minimum-risk maximum-value policy. Figure 1(b) shows
minimum risk policy. Though reward function r defined plays role
minimum risk policy, agent tries reach one two goal states.
goal state probability reaching error state 0. Clearly, respect
value function V , policy Figure 1(b) optimal: e.g. state (3, 3) agent
tries reach distant goal, causes higher discounting goal reward.
minimum risk policy Figure 1(b) 25 safe states, defined states
risk . minimum risk policy estimated mean value V = 0.442.
Figure 1(c) maximum-value policy shown. maximum-value policy
optimizes value without considering risk estimated value V = 0.46.
Thus, performs better minimum-risk policy Figure 1(b), risk (5, 2)
(2, 5) become greater . algorithm starts = 0 computes
minimum-risk policy Figure 1(b). increased step step risk state
changes value lower value > . algorithm stops = 0.64.
policy computed shown Figure 1(d). Obviously, lies minimum risk
policy Figure 1(b) maximum-value policy Figure 1(c).
applied algorithm discounted version risk, , grid
world problem. discounted risk used learning, whereas original risk, ,
used selecting best weight . parameters described above, modified
algorithm produced policy depicted figure 1(d). Seemingly, grid world
example, oscillations present major problem.
tank control task described next section, holds =
= .

7. Stochastic Optimal Control Chance Constraints
section, consider solution stochastic optimal control problem chance
constraints (Li et al., 2002) applying risk-sensitive learning method.
7.1 Description Control Problem
following, consider plant depicted Figure 2. task control
outflow tank lies upstream distillation column order fulfill several
objectives described below. purpose distillation column separation
two substances 1 2. consider finite number time steps 0, . . . , N . outflow
tank, i.e. feedstream distillation column, characterized flowrate
F (t) controlled agent, substance concentrations c1 (t) c2 (t) (for
0 N ).
purpose control designed keep outflow rate F (t) near specified
optimal flow rate Fspec order guarantee optimal operation distillation column.
92

fiRisk-Sensitive Reinforcement Learning

F1
c11
c12

distillation column

F2
c21
c22

ymax
y, h, c1, c2
ymin

F

tank

Fspec
c1min, c1max
c2min, c2max

Figure 2: plant. See text description.
Using quadratic objective function, goal specified
min

F (0),...,F (N 1)

N
1
X

(F (t) Fspec )2 ,

(19)

t=0

values obey
0 N 1 : Fmin F (t) Fmax .

(20)

tank characterized tank level y(t) holdup h(t), = A1 h
constant footprint tank. tank level y(t) concentrations
c1 (t) c2 (t) depend two stochastic inflow streams characterized flowrates
F1 (t) F2 (t), inflow concentrations c1,j (t) c2,j (t) substances j {1, 2}.
linear dynamics tank level given
y(t + 1) = y(t) + A1

X



Fj (t) F (t) .

(21)


A1 X
Fj (t)(cj,i (t) ci (t))
(
y(t) j=1,2

(22)

j=1,2

dynamics concentrations given
= 1, 2 : ci (t + 1) = ci (t) +

initial state system characterized
y(0) = y0 , c1 (0) = c01 , c2 (0) = c02 .

(23)

tank level required fulfill constraint ymin y(t) ymax . concentrations inside tank correspond concentrations outflow. substance
concentrations c1 (t) c2 (t) required remain intervals [c1,min , c1,max ]
93

fiGeibel & Wysotzki

[c2,min , c2,max ], respectively. assume inflows (t) inflow concentrations
ci,j (t) random, governed probability distribution. Li et al. (2002)
assume multivariate Gaussian distribution. randomness variables,
tank level feedstream concentrations may violate given constraints.
therefore formulate stochastic constraint




P ymin y(t) ymax , ci,min ci (t) ci,max , 1 N, = 1, 2 p

(24)

expression (24) called (joint) chance constraint, 1 p corresponds
permissible probability constraint violation. value p given user.
stochastic optimization problem SOP-YC defined quadratic objective function (19) describing sum quadratic differences outflow rates Fspec ,
linear dynamics tank level (21), nonlinear dynamics concentrations
(22), initial state given (23), chance constraint (24).
Li et al. describe simpler problem SOP-Y concentrations considered;
see Figure 3. SOP-Y use cumulative inflow F = F1 + F2 description
tank level dynamics, see (27). SOP-Y describes dynamics linear system.
Li et al. solve SOP-Y relaxing nonlinear program solved sequential
quadratic programming. relaxation possible SOP-Y linear system,
multivariate Gaussian distribution assumed. Solving nonlinear systems SOP-YC
non-Gaussian distributions difficult (e.g. Wendt, Li, & Wozny, 2002),
achieved RL approach.

min

F (0),...,F (N 1)

subject
0 N 1 :
y(t + 1)

N
1
X

(F (t) Fspec )2

(25)

t=0

Fmin F (t) Fmax


= y(t) + A1 F (t) F (t)
y(0) = y0



P ymin y(t) ymax , 1 N p

(26)
(27)
(28)
(29)

Figure 3: problem SOP-Y.
Note control F (t) optimization problems depends time step
t. means solutions SOP-YC SOP-Y yield open loop controls.
dependence initial condition (23), moving horizon approach taken
design closed loop control. discuss issue, goes beyond scope
paper.
94

fiRisk-Sensitive Reinforcement Learning

7.2 Formulation Reinforcement Learning Problem
Using RL instead analytical approach advantage probability distribution doesnt Gaussian unknown. state equations need
known, nonlinear. learning agent must access simulated
empirical data, i.e. samples least random variables.
Independent chosen state representation, immediate reward defined
rx,u (x ) = (u Fspec )2 ,

(30)

u chosen action minus required RL value function
maximized. reward signal depends action chosen, current
successor state.
work consider finite (discretized) action sets, although approach
extended continuous action sets, e.g. using actor-critic method (Sutton &
Barto, 1998). following, assume interval [Fmin , Fmax ] discretized
appropriate manner.
process reaches error state one constraints (24) (or (29), respectively) violated. process artificially terminated transferring agent
additional absorbing state giving risk signal r = 1. V -value error states
set zero, controller could choose action Fspec first constraint
violation, subsequent constraint violations make things worse respect
chance constraints (24) (29), respectively.
7.3 Definition State Space
following consider design appropriate state spaces result either
open loop control (OLC) closed loop control (CLC).
7.3.1 Open Loop Control
note SOP-YC SOP-Y time-dependent finite horizon problems
control F (xt ) = F (t) depends only. means state feedback
resulting controller open-looped. respect state definition xt = (t),
Markov property defined section 2 clearly holds probabilities rewards defining
V . Markov property hold rewards defining . Using xt = (t)
implies agent information state process. including
information history form past action, agent gets idea
current state process. Therefore, inclusion history information changes
probability r = 1, Markov property violated. Including past actions
state description ensures Markov property r. Markov property therefore
recovered considering augmented state definition
xt = (t, ut1 , . . . , u0 ) ,

(31)

past actions (ut1 , . . . , u0 ). first action u0 depends fixed initial tank level y0
fixed initial concentrations only. second action depends first action, i.e.
initial tank level initial concentrations on. Therefore, learning
95

fiGeibel & Wysotzki

states (31) results open loop control, original problems SOP-YC
SOP-Y.
noted MDP, risk depend past actions,
future actions only. choice xt = (t), hidden state information,
MDP Markov property violated. Therefore probability
entering error state conditioned time step, i.e. P (r0 = 1|t), changes
additionally conditioned past actions yielding value P (r0 = 1|t, ut1 , . . . , u0 )
(corresponding agent remembers past actions). example, agent
remembers past time steps current learning episode always used
action F = 0 corresponding zero outflow, conclude increased
probability tank level exceeds ymax , i.e. knowledge increased risk.
If, hand, remember past actions, cannot know increased
risk knows index current time step, carries less information
current state.
well-known Markov property generally recovered including
complete state history state description. xt = (t), state history contains
past time indices, actions r-costs. tank control task, action history
relevant part state history previous r-costs necessarily zero,
indices past time steps already given actual time known
agent. Therefore, past rewards indices past time steps need
included expanded state. Although still complete state information
known agent, knowledge past actions suffices recover Markov property.
respect state choice (31) reward signal (30), expectation
definition value function needed, cp. eq. (2). means
h



V (x) = E R | x0 = x =

N
1
X

(F (t) Fspec )2

t=0

holds, i.e. direct correspondence value function objective
function SOP-YC SOP-Y.
7.3.2 Closed Loop Control
define alternative state space, expectation needed.
decided use state definition
xt = (t, y(t), c1 (t), c2 (t))

(32)

xt = (t, y(t))

(33)

problem SOP-YC
simpler problem SOP-Y. result learning state time-dependent closed
loop controller, achieve better regulation behavior open loop controller,
reacts actual tank level concentrations, whereas open loop control
not. agent access inflow rates concentrations,
included state vector, yielding improved performance controller.
96

fiRisk-Sensitive Reinforcement Learning

Parameter
N
y0
[ymin , ymax ]
A1
Fspec
[Fmin , Fmax ]
RL-YC-CLC:
c01
c02
[c1,min , c1,max ]
[c2,min , c2,max ]

Table 1: Parameter settings
Value
Explanation
16
number time steps
0.4
initial tank level
[0.25, 0.75] admissible interval tank level
0.1
constant, see (22)
0.8
optimal action value
[0.55, 1.05] interval actions, 21 discrete values
0.2
0.8
[0.1, 0.4]
[0.6, 0.9]

initial concentration subst. 1
initial concentration subst. 2
interval concentration 1
interval concentration 2

7.4 RL Problems
definitions, optimization problem defined via (11) (12)
= 1p (see (24) (29)). set X (see (10) (12)) defined contain unique
starting state, i.e X = {x0 }. experiments consider following instantiations
RL problem:
RL-Y-CLC Reduced problem SOP-Y using states xt = (t, y(t)), x0 = (0, y0 ) resulting closed loop controller (CLC).
RL-Y-OLC Open loop controller (OLC) reduced problem SOP-Y. state space
defined action history time, see eq. (31). starting state x0 = (0).
RL-YC-CLC Closed loop controller full problem SOP-YC using states xt =
(t, y(t), c1 (t), c2 (t)) x0 = (0, y0 , c01 , c01 ).
Solving problem RL-Y-OLC yields action vector. problems RL-YC-CLC
RL-Y-CLC result state dependent controllers. present results fourth
natural problem RL-YC-OLC, offer additional insights.
interpolation states used 2 16 multilayer perceptrons (MLPs, e.g.
Bishop, 1995) case RL-Y-OLC extremely large state space (15 dimensions = N 1). used radial basis function (RBF) networks case
RL-YC-CLC RL-Y-CLC, produced faster, stable robust results
compared MLPs.
training respective networks, used direct method corresponds
performing one gradient descent step current state-action pair new

estimate target value (see e.g. Baird, 1995). new estimate Q given

r + Qt (x , u ), Q r + Qt (x , u ) (compare right sides update
equations (15)-(17)).
97

fiGeibel & Wysotzki

(a)
2
1.8
1.6
1.4
1.2
1
0.8
0.6
0.4
0.2
0

outflow rate

Inflow

(b)

0

2

4

6

1
0.95
0.9
0.85
0.8
0.75
0.7
0.65

8 10 12 14 16
time

(c)

omega=0.01
0.8

0

2

4

6

8 10 12 14 16
time

omega=0.05
0.8
outflow rate

outflow rate

(d)
1
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0

2

4

6

8 10 12 14 16
time

1
0.95
0.9
0.85
0.8
0.75
0.7
0.65

omega=0.1
0.8

0

2

4

6

8 10 12 14 16
time

Figure 4: RL-Y-CLC: (a) inflow rates F (t) 10 runs. (b), (c), (d) Example runs
policies = 0.01, 0.05, 0.10 (i.e. p = 0.99, 0.95, 0.90). holds Fspec = 0.8.

8. Experiments
section, examine experimental results obtained tank control task
( = = 1). section 8.1 discuss linear case compare results Li
et al. (2002). linear case, consider closed loop controller obtained solving
RL-Y-CLC (sect. 8.1.1) open loop controller related RL problem RL-Y-OLC
(sect. 8.1.2). closed loop controller, discuss problem non-zero covariances
variables different time steps. nonlinear case discussed section 8.2.
8.1 Problems RL-Y-CLC RL-Y-OLC
start simplified problems, RL-Y-CLC RL-Y-OLC, derived SOP-Y
discussed Li et al. (2002). SOP-Y concentrations considered,
one inflow rate F (t) = F1 (t) + F2 (t). parameter settings Table 1 (first five
lines) taken Li et al. (2002). minimum maximum values actions
determined preliminary experiments.
Li et al. define inflows (F (0), . . . , F (15))T Gaussian distribution
mean vector
(1.8, 1.8, 1.5, 1.5, 0.7, 0.7, 0.5, 0.3, 0.2, 0.2, 0.2, 0.2, 0.2, 0.6, 1.2, 1.2)T .
98

(34)

fiRisk-Sensitive Reinforcement Learning

0.3
0.2
0.1
risk
0
-0.1
value

-0.2

weighted
-0.3
-0.4
-0.5
0

5

10

15

20

xi






Figure 5: RL-Y-CLC: Estimates risk (x0 ), value V (x0 ), V (x0 ) =




V (x0 ) (x0 ) different values .

covariance matrix given





C=

0 1 r01
02
0 1 r01
...


0 N 1 r0(N 1)


0 N 1 r0(N 1)
...
...


2

N
1







(35)

= 0.05. correlation inflows time j defined
rij = rji = 1 0.05(j i)

(36)

0 N 1, < j N 1 (from Li et al., 2002). inflow rates ten example
runs depicted Figure 4(a).
8.1.1 Problem RL-Y-CLC (Constraints Tank Level)
start presentation results problem RL-Y-CLC, control
(i.e. outflow F ) depends time tank level. X = {x0 }
overall performance policy defined (10) corresponds performance x0 ,




V = V (x0 ) .


holds x0 = (0, y0 ). V (x0 ) value respect policy learned
weighted criterion function V , see (13). respective risk


(x0 ) .




Figure 5 estimated2 risk (x0 ) estimated value V (x0 ) depicted


different values . estimate risk (x0 ) value V (x0 )
2. values policies presented following estimated learning algorithm. Note
order enhance readability, denoted learned policy .

99

fiGeibel & Wysotzki

0.5
0.4
0.3
0.2
0.1
0
-0.1
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
xi

Figure 6: RL-Y-CLC: Difference weighted criteria. explanation see text.

increase . Given fixed value p admissible probability constraint violation,

appropriate = (p) obtained value risk (x0 ) lower

= 1p maximum V (x0 ). Due variation performance (see
Fig. 5) found works better selecting maximum . estimate






weighted criterion V (x0 ) = V (x0 ) (x0 ) shown Figure 5.
outflow rate F (control variable) different values found Figure 4(bc). Note rates certain variance since depend probabilistic tank
level. randomly picked one example run value . found control
values F (t) tend approach Fspec increasing values (i.e. decreasing values p).
Correlations definition covariance matrix (35) (36) reveals high
correlation inflow rates neighboring time steps. order better account this,
possible include information past time steps state description time t.
level changes according inflow rate F , investigated inclusion
past values y. inflow rates measured, could included
state vector. Former rewards need included depend past tank
levels, i.e. represent redundant information.
compared performance algorithm augmented state space
defined xt = (t, y(t), y(t 1), y(t 2)) (depth 2 history) normal state space
xt = (t, y(t)) (no history). Fig. 6 shows












V (0, y0 , 0, 0) V (0, y0 ) ,
|

{z
x0

}

| {z }
x0

i.e. difference weighted criteria starting state respect learned
policies (history) (no history). Note starting state x0 , past values
defined 0. curve Figure 6 runs mainly 0. means using
augmented state space results better performance many values . Note
100

fiRisk-Sensitive Reinforcement Learning

(a)

(b)
0.3

1
Risk

0.2

0.95

0.1
outflow rate

0.9

0
-0.1
Value

-0.2

0.85
0.8
0.75

-0.3

0.7

-0.4

0.65
0

2

4

6

8

10

12

14

0

xi


2

4

6

8
time

10

12

14



Figure 7: RL-Y-OLC: (a) Estimates risk (x0 ) value V (x0 ) increasing


values . (b) Learned policy (x0 ) 0.098 V (x0 ) 0.055

larger values original value function overweights risk cases
policy always chooses outflow Fspec approximated. means
difference performance tends zero.
similar, quite pronounced effect observed using history
length 1 only. principle, assume possible achieve even better performance
including full history tank levels state description, tradeoff objective difficulty network training caused number
additional dimensions.
8.1.2 RL-Y-OLC (History Control Actions)
RL problem RL-Y-OLC comprises state descriptions consisting action history
together time, see eq. (31). starting state empty history, i.e. x0 = (0).
result learning time-dependent policy implicit dependence y0 .
learned policy therefore fixed vector actions F (0), . . . , F (15) forms feasible,
general suboptimal solution problem SOP-Y Figure 3.


progression risk estimate, i.e. (x0 ), value, V (x0 ),
different values found Figure 7. results good ones

RL-Y-CLC Figure 5: estimated minimum risk 0.021, risk (x0 ) grows
much faster RL-Y-CLC-risk Figure 5.

policy risk (x0 ) 0.098 depicted Figure 7(b). contrast
policies RL-Y-CLC (see Figure 4(b-c)), control values change different runs.
8.1.3 Comparison
Table 2, compared performance approach Li et al. RL-Y-CLC
RL-Y-OLC p = 0.8 p = 0.9. RL-Y-CLC RL-Y-OLC performed 10
learning runs. respective learned policy , risk (x0 ) value V (x0 )
estimated 1000 test runs. RL-Y-CLC RL-Y-OLC, table shows mean
performance averaged 10 runs together standard deviation parentheses.
101

fiGeibel & Wysotzki

Table 2: Comparison est. squared deviation Fspec (i.e. V (x0 )) results Li et
al. results RL-Y-CLC RL-Y-OLC p = 0.8 ( = 0.2) p = 0.9
( = 0.1). Smaller values better.
approach
Li et al. (2002)
RL-Y-CLC
RL-Y-OLC

p = 0.8
0.0123
0.00758 (0.00190)
0.0104 (0.000302)

p = 0.9
0.0484
0.02 (0.00484)
0.0622 (0.0047)

found that, average, policy determined RL-Y-CLC performs better
obtained approach Li et al. (2002) (with respect estimated squared
deviation desired outflow Fspec , i.e. respect V (x0 ).) policy obtained
RL-Y-OLC performs better p = 0.8 worse p = 0.9. maximal achievable
probability holding constraints 1.0 (sd 0.0) RL-Y-CLC, 0.99 (sd 0.0073)
RL-Y-OLC. Li et al. report p = 0.999 approach.
approach Neuneier Mihatsch (1999) considers worst-case outcomes
policy, i.e. risk related variability return. Neuneier Mihatsch show
learning algorithm interpolates risk-neutral worst-case criterion
limiting behavior exponential utility approach.
0.6

risk
value

0.5
0.4
0.3

risk

0.2
value

0.1
0
-0.1
-1

-0.5

0

0.5

1

kappa

Figure 8: Risk value several values
learning algorithm Neuneier Mihatsch parameter (1.0, 1.0)
allows switch risk-averse behavior ( 1), risk-neutral behavior ( = 0),
risk-seeking behavior ( 1). agent risk-seeking, prefers policies
good best-case outcome. Figure 8 shows risk (probability constraint violation) value
starting state regard policy computed algorithm Neuneier
Mihatsch. Obviously, algorithm able find maximum-value policy yielding zero
deviation Fspec , corresponding choosing F = Fspec = 0.8 states, learning
result sensitive risk parameter . reason worst-case
best-case returns policy always chooses outflow 0.8 correspond
102

fiRisk-Sensitive Reinforcement Learning

(a)

Inflow

(b)
1

1

0.8

0.8
ymax
mu(t)+0.04

0.6



0.6

0.4

0.4
mu(t)-0.04

c1max
c1

ymin
0.2

0.2

c1min
0

0
0

2

4

6

8

10

12

14

0

Time

2

4

6

8

10

12

14

16

Time

Figure 9: RL-YC-CLC: (a) (t) + 0.04 (t) 0.04 (profiles two mode means).
(b) tank level y(t) concentration c1 (t) 10 example runs using
minimum risk policy.

0, best return possible (implying zero variance return). approach
Neuneier Mihatsch variance-based approaches therefore unsuited
problem hand.
8.2 Problem RL-YC-CLC (Constraints Tank Level Concentrations)
following consider full problem RL-YC-CLC. two inflows F1 F2
assumed equal Gaussian distributions distribution cumulative
inflow F (t) = F1 (t) + F2 (t) described covariance matrix (35) mean
vector (34); see Figure 4(a).
order demonstrate applicability approach non-Gaussian distributions,
chosen bimodal distributions inflow concentrations c1 c2 . underlying
assumption upstream plants either increased output,
lower output, e.g. due different hours weekdays.
distribution inflow concentration ci,1 (t) characterized two Gaussian
distributions means
(t) + (1)k 0.04 ,
k = 1, 2 2 = 0.0025. value k {0, 1} chosen beginning
run equal probability outcome. means overall mean value
ci,1 (t) given (t). profiles mean values modes found
Figure 9(a). ci,2 given ci,2 (t) = 1.0 ci,1 (t). minimum maximum values
concentrations ci (t) found Table 1, Figure 9(b). Note
concentrations controlled indirectly choosing appropriate outflow F .
developing risk value starting state shown Figure 10.
resulting curves behave similar problem RL-Y-CLC depicted Figure 5:
value risk increase . seen algorithm covers relatively
broad range policies different value-risk combinations.
103

fiGeibel & Wysotzki

0.4
0.3
0.2
risk
0.1
0
-0.1
-0.2

value

-0.3
-0.4
0

5

10

15

20

25

30

35

40

45

50

xi








Figure 10: RL-YC-CLC: Estimated risk (x0 ), value V (x0 ), V (x0 ) = V (x0 )


(x0 ) different values .

minimum risk policy, curves tank level concentration
c1 found Figure 9(b). bimodal characteristics substance 1 inflow
concentrations reflected c1 (t) (it holds c2 (t) = 1 c1 (t)). attainable minimum
risk 0.062. Increasing weight leads curves similar shown Figures 5
7. assume minimum achievable risk decreased inclusion
additional variables, e.g. inflow rates concentrations, and/or inclusion past
values discussed section 8.1.1. treatment version action history
analogous section 8.1.2. therefore conclude presentation experiments
point.

9. Conclusion
paper, presented approach learning optimal policies constrained risk
MDPs error states. contrast RL DP approaches consider risk
matter variance return worst outcomes, defined risk
probability entering error state.
presented heuristic algorithm aims learning good stationary policies
based weighted formulation problem. weight original value function
increased order maximize return risk required stay
given threshold. fixed weight finite state space, algorithm converges
optimal policy case undiscounted value function. case state
space finite, contains cycles, < 1 holds, conjecture convergence
learning algorithm policy, assume suboptimal weighted
formulation. optimal stationary policy exists weighted formulation,
feasible, generally suboptimal solution constrained problem.
104

fiRisk-Sensitive Reinforcement Learning

weighted approach combined adaptation heuristic searching space feasible stationary policies original constrained problem,
us seems relatively intuitive. conjecture better policies could found allowing
state-dependent weights (x) modified adaptation strategy, extending
considered policy class.
successfully applied algorithm control outflow feed tank
lies upstream distillation column. started formulation stochastic
optimal control problem chance constraints, mapped risk-sensitive learning
problem error states (that correspond constraint violation). latter problem
solved using weighted RL algorithm.
crucial point reformulation RL problem design state space.
found algorithm consistently performed better state information
provided learner. Using time action history resulted large state
spaces, poorer learning performance. RBF networks together sufficient state
information facilitated excellent results.
must mentioned use RL together MLP RBF network based
function approximation suffers usual flaws: non-optimality learned network,
potential divergence learning process, long learning times. contrast
exact method, priori performance guarantee given, course posteriori
estimate performance learned policy made. main advantage
RL method lies broad applicability. tank control task, achieved good
results compared obtained (mostly) analytical approach.
cases |X| > 1 < 1 theoretical investigations convergence
experiments required. Preliminary experiments shown oscillations may
occur algorithm, behavior tends oscillate sensible policies without
getting bad in-between although convergence usefulness policies remains
open issue.
Oscillations prevented using discounted risk leads underestimation
actual risk. existence optimal policy convergence learning
algorithm fixed guaranteed case finite MDP. probabilistic
interpretation discounted risk given considering 1 probability
exiting control MDP (Bertsekas, 1995). investigation discounted
risk may worthwhile right. example, task long episodes,
continuing, i.e. non-episodic, natural give larger weight error
states occurring closer current state.
designed learning algorithm online algorithm. means learning accomplished using empirical data obtained interaction simulated
real process. use neural networks allows apply algorithm processes
continuous state spaces. contrast, algorithm described Dolgov Durfee (2004)
applied case known finite MDP. model obtained
case continuous-state process finding appropriate discretization estimating state transition probabilities together reward function. Although
discretization prevents application Dolgov Durfees algorithm RL-Y-OLC,
15-dimensional state space encountered, probably applied case
RL-Y-OLC. plan investigate point future experiments.
105

fiGeibel & Wysotzki

question arises whether approach applied stochastic optimal
control problems types chance constraints. Consider conjunction chance
constraints
P(C0 ) p1 , . . . , P(CN 1 ) pN 1 ,
(37)
Ct constraint system containing variables time t, pt
respective probability threshold. (37) requires alternative RL formulation risk
state depends next reward, time-step .
solution modified version RL algorithm difficult.
Ct (37) allowed constraint system state variables depending t, things get involved several risk functions needed
state. plan investigating cases future.

Acknowledgments thank Dr. Pu Li providing application example
helpful comments. thank Onder Gencaslan conducting first experiments
masters thesis.

References
Altman, E. (1999). Constrained Markov Decision Processes. Chapman Hall/CRC.
Baird, L. (1995). Residual algorithms: reinforcement learning function approximation. Proc. 12th International Conference Machine Learning, pp. 3037. Morgan
Kaufmann.
Bawas, V. S. (1975). Optimal rules ordering uncertain prospects. Journal Finance,
2 (1), 1975.
Bertsekas, D. P. (1995). Dynamic Programming Optimal Control. Athena Scientific,
Belmont, Massachusetts. Volumes 1 2.
Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientific,
Belmont, MA.
Bishop, C. M. (1995). Neural Networks Pattern Recognition. Oxford University Press,
Oxford.
Blythe, J. (1999). Decision-theoretic planning. AI Magazine, 20 (2), 3754.
Borkar, V. (2002). Q-learning risk-sensitive control. Mathematics Operations Research,
27 (2), 294311.
Coraluppi, S., & Marcus, S. (1999). Risk-sensitive minimax control discrete-time,
finite-state Markov decision processes. Automatica, 35, 301309.
Crites, R. H., & Barto, A. G. (1998). Elevator group control using multiple reinforcement
learning agents. Machine Learning, 33 (2/3), 235262.
Dolgov, D., & Durfee, E. (2004). Approximating optimal policies agents limited
execution resources. Proceedings Eighteenth International Joint Conference
Artificial Intelligence, pp. 11071112. AAAI Press.
106

fiRisk-Sensitive Reinforcement Learning

Feinberg, E., & Shwartz, A. (1994). Markov decision models weighted discounted
criteria. Math. Operations Research, 19, 152168.
Feinberg, E., & Shwartz, A. (1996). Constrained discounted dynamic programming. Math.
Operations Research, 21, 922945.
Feinberg, E., & Shwartz, A. (1999). Constrained dynamic programming two discount
factors: Applications algorithm. IEEE Transactions Automatic Control,
44, 628630.
Fishburn, P. C. (1977). Mean-risk analysis risk associated below-target returns.
American Economics Review, 67 (2), 116126.
Freund, R. (1956). introduction risk programming model. Econometrica, 21,
253263.
Fulkerson, M. S., Littman, M. L., & Keim, G. A. (1998). Speeding safely: Multi-criteria
optimization probabilistic planning. Proceedings Fourteenth National
Conference Artificial Intelligence, p. 831. AAAI Press/MIT Press.
Gabor, Z., Kalmar, Z., & Szepesvari, C. (1998). Multi-criteria reinforcement learning.
Proc. 15th International Conf. Machine Learning, pp. 197205. Morgan Kaufmann,
San Francisco, CA.
Geibel, P. (2001). Reinforcement learning bounded risk. Brodley, E., & Danyluk,
A. P. (Eds.), Machine Learning - Proceedings Eighteenth International Conference (ICML01), pp. 162169. Morgan Kaufmann Publishers.
Heger, M. (1994). Consideration risk reinforcement learning. Proc. 11th International Conference Machine Learning, pp. 105111. Morgan Kaufmann.
Kall, P., & Wallace, S. W. (1994). Stochastic Programming. Wiley, New York.
Koenig, S., & Simmons, R. G. (1994). Risk-sensitive planning probabilistic decision
graphs. Doyle, J., Sandewall, E., & Torasso, P. (Eds.), KR94: Principles Knowledge Representation Reasoning, pp. 363373, San Francisco, California. Morgan
Kaufmann.
Kushmerick, N., Hanks, S., & Weld, D. S. (1994). algorithm probabilistic leastcommitment planning.. AAAI, pp. 10731078.
Li, P., Wendt, M., Arellano-Garcia, & Wozny, G. (2002). Optimal operation distillation
processes uncertain inflows accumulated feed tank. AIChe Journal, 48,
11981211.
Liu, Y., Goodwin, R., & Koenig, S. (2003a). Risk-averse auction agents. Rosenschein, J.,
Sandholm, T., & Wooldridge, M. Yokoo, M. (Eds.), Proceedings Second International Joint Conference Autonomous Agents MultiAgent Systems (AAMAS03), pp. 353360. ACM Press.
Liu, Y., Goodwin, R., & Koenig, S. (2003b). Risk-averse auction agents.. AAMAS, pp.
353360.
Markowitz, H. M. (1952). Portfolio selection. Journal Finance, 7 (1), 7791.
Markowitz, H. M. (1959). Portfolio Selection. John Wiley Sons, New York.
107

fiGeibel & Wysotzki

Mihatsch, O., & Neuneier, R. (2002). Risk-sensitive reinforcement learning. Machine Learning, 49 (2-3), 267290.
Neuneier, R., & Mihatsch, O. (1999). Risk-sensitive reinforcement learning. Michael
S. Kearns, Sara A. Solla, D. A. C. (Ed.), Advances Neural Information Processing
Systems, Vol. 11. MIT Press.
Ross, S. M. (2000). Introduction Probability Models. Academic Press, New York.
Roy, A. D. (1952). Safety first holding assets. Econometrica, 20 (3), 431449.
Smart, W. D., & Kaelbling, L. P. (2002). Effective reinforcement learning mobile robots. Proceedings 2002 IEEE International Conference Robotics
Automation (ICRA 2002).
Stephan, V., Debes, K., Gross, H.-M., Wintrich, F., & Wintrich, H. (2001). new control
scheme combustion processes using reinforcement learning based neural networks. International Journal Computational Intelligence Applications, 1 (2),
121136.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning Introduction. MIT
Press.
Tsitsiklis, J. N. (1994). Asynchronous stochastic approximation Q-learning. Machine
Learning, 16 (3), 185202.
Watkins, C. J. C. H. (1989). Learning Delayed Rewards. Ph.D. thesis, Kings College,
Oxford.
Watkins, C. J. C. H., & Dayan, P. (1992). Q-learning. Machine Learning, 8 (3/4). Special
Issue Reinforcement Learning.
Wendt, M., Li, P., & Wozny, G. (2002). Non-linear chance constrained process optimization
uncertainty. Ind. Eng. Chem. Res., 21, 36213629.

108


