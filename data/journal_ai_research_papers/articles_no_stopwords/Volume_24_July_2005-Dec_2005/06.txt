Journal Artificial Intelligence Research 24 (2005) 933944

Submitted 12/04; published 12/05

Engineering Note
mGPT: Probabilistic Planner Based Heuristic Search
Blai Bonet

bonet@ldc.usb.ve

Departamento de Computacion
Universidad Simon Bolvar, Venezuela

Hector Geffner

hector.geffner@upf.edu

ICREA & Universitat Pompeu Fabra
Paseo de Circunvalacion 8, Barcelona 08003, Spain

Abstract
describe version GPT planner used probabilistic track 4th
International Planning Competition (ipc-4). version, called mGPT, solves Markov
Decision Processes specified ppddl language extracting using different classes
lower bounds along various heuristic-search algorithms. lower bounds
extracted deterministic relaxations alternative probabilistic effects
action mapped different, independent, deterministic actions. heuristic-search
algorithms use lower bounds focusing updates delivering consistent
value function states reachable initial state greedy policy.

1. Introduction
mGPT planner based heuristic search solving Markov Decision Processes (MDPs)
specified high-level planning language ppddl. mGPT captures fragment
functionality GPT system handles non-determinism incomplete information,
qualitative probabilistic forms, including pomdps Conformant planning
(Bonet & Geffner, 2000).
mGPT supports several algorithms admissible heuristic functions (lower bounds)
combined generate wide range solvers. main algorithms lrtdp
hdp. heuristic-search algorithms solving MDPs make use lower bounds
computing consistent value function V : function Bellman residuals bounded
user-provided parameter states reachable given initial state s0
greedy policy based V (Bonet & Geffner, 2003b, 2003a).
lower bounds derived solving relaxations input problem. Since algorithms solving relaxations based heuristic search, implemented
stackable software components created sequence computing complex heuristic functions simpler ones.

2. Algorithms
divide algorithms two groups: deliver consistent value functions
respect user-provided parameter , select actions real time. first

c
2005
AI Access Foundation. rights reserved.

fiBonet & Geffner

class algorithms compute -consistent value function V states reachable
initial state s0 , greedy policy V based V .
following subsection, give definitions admissible consistent value functions, greedy, partial proper policies. Then, present algorithms implemented
mGPT.
2.1 Consistent Value Functions, Greedy, Partial Proper Policies
value function V admissible non-overestimating; i.e. value V (s)
state lower bound optimal expected cost starting s. V -consistent
state Bellman residual s,



X


def
0
0

R(s) = fiV (s) min c(s, a) +
P r(s |s, a)V (s ) ,
(1)
aA(s)

s0

less equal . Here, A(s) denotes actions applicable s, c(s, a)
cost applying action s, P r() probabilistic transition function. V
0-consistent s, say V consistent s.
state reachable initial state s0 policy exists trajectory
s0 , s1 , . . . , sn sn = P (sk+1 |sk , (sk )) > 0 0 k < n. words,
state reached positive probability s0 zero steps using
policy .
known greedy policy V based value function V , defined


X
def
0
0
V (s) = argmin c(s, a) +
P r(s |s, a)V (s ) ,
(2)
aA(s)

s0

optimal V -consistent states sufficiently small . Yet, since goal
find optimal policy respect initial state s0 states reachable
it, sufficient V admissible -consistent states reachable
s0 V .
partial policy policy doesnt need defined states. closed
respect state defined states reachable ,
proper respect goal state reached every state reachable
, finally proper proper respect states.
2.2 Algorithms Compute -Consistent Value Functions
first group algorithms, mGPT implements Value Iteration (vi), Labeled RealTime Dynamic Programming (lrtdp), Heuristic Dynamic Programming (hdp).
Value Iteration (Bertsekas, 1995) applied states reached
given initial state available operators, yields -consistent value function
them.1 mGPTs vi serves bottom-line reference comparison
algorithms.
1. undiscounted problems probabilistic planning, conditions neeeded order
VI finish -consistent value function (Bertsekas, 1995).

934

fimGPT: Probabilistic Planner Based Heuristic Search

Labeled Real-Time Dynamic Programming (Bonet & Geffner, 2003b) heuristicsearch algorithm implements labeling scheme top rtdp algorithm (Barto,
Bradtke, & Singh, 1995) improve convergence. Lrtdp works performing simulated
trials start initial state end solved states, selecting actions according
greedy policy V successor states according corresponding transition
probabilities. Initially, V input heuristic function, solved states
goal states. Then, time action picked state s, value updated
making consistent value successors. end trial, labeling
procedure called checks whether new states labeled solved: state solved
value value descendents -consistent. algorithm ends
initial state labeled solved. point states reachable initial state
s0 greedy policy V -consistent. labeling mechanism guarantees
V proper partial policy respect s0 .
Heuristic Dynamic Programming (Bonet & Geffner, 2003a) second heuristic-search
algorithm supported mGPT solving MDPs. Hdp performs systematic depth-first
searches set states reachable initial state s0 greedy policy
V looking -inconsistent states updating values. top search,
labeling scheme based Tarjans strongly-connected components procedure (Tarjan,
1972), identifies states solved need revisited. initial
value function given heuristic function, algorithm ends initial state
solved. lrtdp, labeling mechanism guarantees V proper respect
s0 .
2.3 Algorithms Real-Time Action Selection
second class algorithms attempt solve given MDP; rather select
actions real-time limited amount processing without offering guarantees
quality resulting policies. Algorithms group include extension
Action Selection Planning algorithm (asp) (Bonet, Loerincs, & Geffner, 1997)
probabilistic domains, basically rtdp algorithm lookahead. Asp, rtdp,
performs value function updates states cannot get trapped loop. Thus,
although policy delivered asp suboptimal, proper policy; i.e. policy
guaranteed reach goal state.

3. Heuristics
algorithms assume initial value function given heuristic function
provides good cost estimates, particular, lrtdp hdp expect heuristic
admissible. described Pearl (1983), informative admissible heuristics
obtained solving suitable relaxations input problem. Two relaxations
supported mGPT: min-min relaxation, Strips relaxation. first defines
(deterministic) shortest-path problem original state space; second used define
(deterministic) shortest-path problems atom space.2 Thus, first solved
2. Atoms refer propositional symbols used representation language, ppddl case,
define problem. number atoms polynomial size input, size
state space is, general, exponential number atoms.

935

fiBonet & Geffner

time polynomial number states, shortest-path problems defined second
solved time polynomial number atoms. methods yield lower bounds
expected cost goal given state, yet bounds produced min-min
relaxation stronger produced Strips relaxation.
3.1 Min-Min State Relaxation
idea behind min-min relaxation transform input probabilistic problem,
described Bellman equations


X
def

0
0
V (s) = min c(s, a) +
P r(s |s, a)V (s ) ,
(3)
aA(s)

s0

deterministic shortest-path problem Bellman equations form,

Vmin
(s) =

def


min c(s, a) + min {Vmin
(s0 ) : P (s0 |s, a) > 0} .

(4)

aA(s)

level representation language, min-min relaxation built transforming probabilistic operator form:
= h , [ p1 : 1 , . . . , pn : n ] ,

(5)

precondition ith probabilistic effect (with probability
pi ), set independent deterministic operators form:
oi = h , ,

1 n.

(6)

Thus, min-min relaxation one actually choose convenient non-deterministic effect operator, hence, cost relaxation lower bound
expected cost original probabilistic problem.
min-min relaxation deterministic problem solved means
standard path-finding algorithms. example, solved Dijkstras algorithm,
a*, ida*, deterministic version lrtdp (i.e. labeled lrta algorithm (Korf, 1990)).
mGPT provides two methods computing min-min heuristic relaxation:
min-min-ida*, uses ida*, min-min-lrtdp, uses lrtdp. versions
lazy sense heuristic values states computed needed
planner requires them.
3.2 Strips Relaxation
Strips relaxation turn converts deterministic problem obtained min-min
relaxation Strips problem, obtains lower bounds original MDP
computing lower bounds resulting Strips problem using methods developed
classical planning (e.g., Bonet & Geffner, 2001; Haslum & Geffner, 2000; Hoffmann & Nebel,
2001; Edelkamp, 2001; Nguyen & Kambhampati, 2000). methods run polynomial
time number atoms yet, unlike min-min relaxation, require casting minmin relaxation Strips format, conversion that, conversion ADL Strips
(Gazen & Knoblock, 1997), may require exponential time space (see below).
936

fimGPT: Probabilistic Planner Based Heuristic Search

mGPT, Strips relaxation obtained directly original problem, first
transforming probabilistic operator form:
= h prec, [ p1 : (add1 , del1 ), . . . , pn : (addn , deln ) ] ,

(7)

prec, addi , deli conjunctions literals represents precondition, ith
add list, ith delete list operator respectively, pi probabilities sum
1. order take operators form (7), disjunctive preconditions, conditional
effects, quantifiers removed described Gazen Knoblock (1997).
operators form (7), Strips relaxation generated splitting
operators n independent Strips operators form:
oi = h prec, addi , deli ,

1 n.

(8)

following heuristics implemented mGPT upon Strips relaxation.
first two lower bounds optimal cost Strips relaxation hence
optimal (expected) cost original MDP, third one necessarily lower bound
either cost.
hm heuristics (h-m) (Haslum & Geffner, 2000) heuristics recursively
approximate cost achieving set atoms C initial state cost
achieving costly subset size C. computed shortestpath algorithm graph nodes standing sets atoms,
result values hm (s) estimate cost reaching goal state s. use
option h-m-k mGPT refer hm heuristic = k.
Pattern database heuristics (patterndb) (Edelkamp, 2001) compute optimal costs
relaxations Strips problem defined multi-valued variables
implicit problem (e.g. location block blocksworld domain
implicit multi-valued variable whose possible values either table top
block). heuristic precomputed once, beginning,
provides lower bound cost arbitrary state goal. pattern
database computed projecting Strips problem respect set atoms
(those define multi-valued variables) solving resulting problem
optimally Dijkstras algorithm. Multiple pattern databases combined
either taking max sum. latter case, pattern database referred
additive.3 use additive pattern databases defined Haslum, Bonet,
Geffner (2005) constraints original problem preserved
projection; something often results stronger heuristics. Patterndb-k refers
pattern database heuristic defined k multi-valued variables.
(ff) heuristic implements heuristic function used planner (Hoffmann & Nebel, 2001). computed building so-called relaxed planning graph
finding plan it. heuristic number operators plan.
3. conditions required adding two pattern databases result remains admissible.
sufficient condition B = sets B used build projections
respectively.

937

fiBonet & Geffner

relaxed planning graph version graph constructed Graphplan
(Blum & Furst, 1997) delete lists ignored. shown computing
heuristic done polynomial time size input problem
(Hoffmann & Nebel, 2001). heuristic however informative non-admissible.
shown below, heuristics plugged directly planning algorithm
used compute informative heuristics. example, patterndb
heuristic used within ida* solve min-min relaxation, gives stronger
heuristic patterndb heuristic. Thus, mGPT implements algorithms heuristics
stackable software components element stack used solve elements
it.

4. Implementation
section gives details implementation mGPT together examples
use. mGPT system implemented C++ upon preliminary parser offered
organizers ipc-4.
4.1 Hash Tables
Perhaps important component modern search-based planners internal
representation states hash tables. Since mGPT uses different search algorithms
hash tables solve given instance (e.g. informative heuristics computed
less informative ones), good internal representations hash table implementation
critical good performance.
grounding atoms operators, state represented ordered list
atoms hold true state. state appear associated different
data multiple hash tables simultaneously. Thus, instead multiples copies
s, mGPT implements system-wide state-hash-table stores representation
states referenced hash tables entries tables simply contain reference
state-hash-table. way, planner saves time space.
Another issue large impact performance average number collisions
hash table. Two points relevant keeping number collisions low:
hashing function size hash table. former, seen
cryptographic hashing functions md4 behave well even though slower
traditional choices. latter, mGPT uses hash tables whose size equal
large prime number (Cormen, Leiserson, & Rivest, 1990).
4.2 Algorithms Heuristics
algorithm mGPT implemented subclass abstract algorithm class
whose members reference problem and, cases, reference hash table
parameter . Similarly, heuristic mGPT implemented subclass
abstract heuristic class whose members reference problem function
maps states non-negative values. Simple heuristics constant-zero function
straightforward, others min-min-lrtdp implemented class whose members are,
addition above, references hash table lrtdp algorithm.
938

fimGPT: Probabilistic Planner Based Heuristic Search

4.3 Examples
main parameters call mGPT -a <algorithm> specifies algorithm
use, -h <heuristic> specifies heuristic function, -e <epsilon>
specifies threshold consistency check. typical call looks like:
mGPT -a lrtdp -h h-m-1 -e .001 <domain> <problem>
instructs mGPT use lrtdp algorithm h-m-1 heuristic = 0.001
domain problem files specified.
h-m-1 heuristic admissible weak. following example shows
compute min-min-lrtdp heuristic using h-m-1 base heuristic:
mGPT -a lrtdp -h "h-m-1|min-min-lrtdp" -e .001 <domain> <problem>
pipe symbol used instruct planner heuristics computed using
heuristics.
Another possibility use mGPT reactive planner decisions taken
on-line respect heuristic function improved time. example,
mGPT -a asp -h <domain> <problem>
uses asp algorithm heuristic,
mGPT -a asp -h "zero|min-min-ida*" <domain> <problem>
uses asp algorithm min-min-ida* heuristic computed constant-zero
heuristic. combinations algorithms heuristics possible. mGPT accepts
parameters control initial hash size, weight heuristic function, values dead-end
states, verbosity level, lookahead settings asp, etc.

5. Competition
competition suite consisted 7 probabilistic domains named blocksworld, explodingblocksworld, boxworld, fileworld, tireworld, towers-of-hanoise, zeno. Blocksworld
exploding-blocksworld variations standard blocksworld domain classical planning. Boxworld logistics-like transportation domain. Fileworld file/folder domain
uncertainty present initial situation destination
file set. Tireworld towers-of-hanoise variations classical tireworld domain
towers-of-hanoi. Zeno traveling domain fuel resource.
domains come two variations: goal-oriented version goal
achieved certainty minimizing expected costs, reward-oriented version
involves rewards. mGPT planner handles first type tasks only.
competition used lrtdp algorithm patterndb-1 heuristic,
parameter = 0.001, weight W = 5 heuristic function. cases,
patterndb-1 heuristic poor, planner switched automatically asp
algorithm heuristic.

939

fiBonet & Geffner

problem name
blocksworld-5
blocksworld-8
blocksworld-11
blocksworld-15
blocksworld-18
blocksworld-21
exploding-bw
boxworld-c5-b10
boxworld-c10-b10
boxworld-c15-b10
fileworld-30-5
towers-of-hanoise
tireworld-g
tireworld-r
zeno

runs
30
30
30
30



30


30

30
30
30

failed
0
0
0
0



0


0

14
0
0

successful
30
30
30
30



30


30

16
30
30

time
43
60
130
7,706



6,370


2,220

48
39
162

reward
494.1
487.7
465.7
397.2



183.6


57.6

266.6
0
500

Table 1: Results mGPT planner competition problems. table shows
problem name, number runs, number failed successful runs (see text),
time reward averages. dash means mGPT able solve
problem. Times milliseconds.

5.1 Results
competition held client/server model. planner evaluated
problem number runs supervision server. planner initiated
session connecting server interacted exchanging messages.
run consisted actions sent planner whose effects transmitted back
server planner. Thus, current state problem maintained
planner server.
Table 1 shows results mGPT competition problems. problem,
30 runs executed. table shows number runs, number failed runs
(i.e. finished without reaching goal state), number successful runs (i.e.
finished goal states), time reward averages per run.4
blocksworld, problem blocksworld-xx means problem xx blocks, boxworld,
problem boxworld-cxx-byy means problem xx cities yy boxes.
seen table, mGPT solve exploding-bw, larger instances
blocksworld boxworld, failed approximately half instances
tireworld-g. difficulties encountered mGPT solving problems often
much probabilities involved, domains, particular,
encodings. basic algorithms used mGPT try solve problems
4. competition format reward-based presentation cost-based. straightforward
go one format other.

940

fimGPT: Probabilistic Planner Based Heuristic Search

computing value function -residuals relevant states (those reachable
initial state optimal policy). this, mGPT computes admissible heuristic
function solving either min-min relaxation, Strips relaxation, both. problem
faced approach many instances neither relaxations could
solved. Here, give detailed explanation problems encountered mGPT
different domains. worth noting many difficulties would surface
Strips planner well, even probabilities ignored.
Blocksworld exploding blocksworld: operator encodings preconditions
containing universally-quantified negative literals, result using clear
predicate. example,
(:action pick-up-block-from
:parameters (?top - block ?bottom)
:precondition (and (not (= ?top ?bottom))
(forall (?b - block) (not (holding ?b)))
(on-top-of ?top ?bottom)
(forall (?b - block) (not (on-top-of ?b ?top))))
:effect (and (decrease (reward) 1)
(probabilistic
0.75 (and (holding ?top) (not (on-top-of ?top ?bottom)))
0.25 (when (not (= ?bottom table))
(and (not (on-top-of ?top ?bottom))
(on-top-of ?top table)))))
)

complex encoding standard planning makes atom-based heuristics almost useless. mGPT could solve instances 5, 8, 11 15 blocks
18 21 blocks. exploding blocksworld, mGPT unable
solve parser incomplete parse complex constructs.
Boxworld: encoding contains drive-truck operator moves truck
intended destination probability 0.8 one three wrong destinations
probability 0.2/3 each. encoding specifies unintended effects means
nested conditional effects form
(:action drive-truck
:parameters (?t - truck ?src - city ?dst - city)
:precondition (and (truck-at-city ?t ?src) (can-drive ?src ?dst))
:effect (and (not (truck-at-city ?t ?src))
(probabilistic
0.2 (forall (?c1 - city)
(when (wrong-drive1 ?src ?c1)
(forall (?c2 - city)
(when (wrong-drive2 ?src ?c2)
(forall (?c3 - city)
(when (wrong-drive3 ?src ?c3)
(probabilistic
1/3 (truck-at-city ?t ?c1)
1/3 (truck-at-city ?t ?c2)
1/3 (truck-at-city ?t ?c3))))))))
0.8 (truck-at-city ?t ?dst)))
)
941

fiBonet & Geffner

Strips relaxation, planner converts ADL-style operators Strips,
suffers exponential blow domain: 10 cities,
thousand operators grounded ADL-operator. set included problems
5, 10 15 cities.
Fileworld: domain, 30 files need filed one 5 different
folders: exact destination determined probabilistically. optimal policy
problem, proper policy, must prescribe action 530 states,
relevant. consequence problem millions relevant states
need stored hash table task compute proper policy.
patterndb-1 heuristic problem informative, revealed analysis
values stored pattern database, thus mGPT switched automatically
asp algorithm heuristic.
Towers-of-hanoise: blocksworld domain, encoding complex operators disjunctions universally-quantified negative literals preconditions, complex conditional effects. Yet problem prevented mGPT
solving problem domain bug code implements conditional
effects surface domains.
Tireworld: two versions: goal-based version called tireworld-g
reward-based version called tireworld-r. domain contains multiple dead ends
locations car gets flat tire spare tire available.
dead ends unavoidable; i.e. proper policy problem. trials
reward-based version end successfully since requirement reach goal
position, rather objective maximize accumulated reward. mGPT treated
versions goal-based problems deal directly reward-based
problems.

6. Conclusions
mGPT planner entered probabilistic planning competition combines heuristicsearch algorithms methods obtaining lower bounds deterministic relaxations.
results obtained competition mixed difficulties
selection domains encodings match capabilities mGPT:
mGPT tries compute proper solutions using heuristics derived Strips relaxations.
described, domains could solved due number relevant
states, others due complexity Strips relaxations themselves.
definition good benchmarks MDP solvers, crucial define
constitutes solution bottom line assessing performance. classical
planning, example, solutions plans bottom line given blind-search
algorithms; progress field measured distance bottom line.
probabilistic setting, difficult always clear means
solve problem. This, however, needs defined way, otherwise performance
comparisons meaningful. Indeed, classical setting, one longer compares
optimal non-optimal planners since types planners different: one
provides guarantees apply solutions, provides guarantees

942

fimGPT: Probabilistic Planner Based Heuristic Search

apply one solution only. probabilistic setting even subtle
different types guarantees. example, restrict class MDPs
constitute simplest generalization classical setting task reaching
goal certainty minimizing expected number steps given initial
state s0 methods yield solutions (policies) ensure goal
reached certainty finite number steps (not necessarily optimal), methods
guarantees. types methods necessary practice, yet crucial
make distinction among identify useful benchmarks class.
methods yield optimal policies, least policies finite expected costs, standard
dynamic programming methods value iteration provide useful bottom-line reference
assessing performance. case, believe useful benchmarks need defined
taking account types tasks various algorithms aim solve,
types guarantees, any, provide solutions.
GPT mGPT available download http://www.ldc.usb.ve/bonet.
Acknowledgements
mGPT built upon parser developed John Asmuth Rutgers University
Hakan Younes Carnegie Mellon University. thank David E. Smith comments helped us improve note.

References
Barto, A., Bradtke, S., & Singh, S. (1995). Learning act using real-time dynamic programming. Artificial Intelligence, 72, 81138.
Bertsekas, D. (1995). Dynamic Programming Optimal Control, (2 Vols). Athena Scientific.
Blum, A., & Furst, M. (1997). Fast planning planning graph analysis. Artificial
Intelligence, 90, 281300.
Bonet, B., & Geffner, H. (2000). Planning incomplete information heuristic search
belief space. Chien, S., Kambhampati, S., & Knoblock, C. (Eds.), Proc. 6th
International Conf. Artificial Intelligence Planning Scheduling, pp. 5261,
Breckenridge, CO. AAAI Press.
Bonet, B., & Geffner, H. (2001). Planning heuristic search. Artificial Intelligence, 129 (1
2), 533.
Bonet, B., & Geffner, H. (2003a). Faster heuristic search algorithms planning
uncertainty full feedback. Gottlob, G. (Ed.), Proc. 18th International Joint
Conf. Artificial Intelligence, pp. 12331238, Acapulco, Mexico. Morgan Kaufmann.
Bonet, B., & Geffner, H. (2003b). Labeled RTDP: Improving convergence real-time
dynamic programming. Giunchiglia, E., Muscettola, N., & Nau, D. (Eds.), Proc.
13th International Conf. Automated Planning Scheduling, pp. 1221, Trento,
Italy. AAAI Press.

943

fiBonet & Geffner

Bonet, B., Loerincs, G., & Geffner, H. (1997). robust fast action selection mechanism
planning. Kuipers, B., & Webber, B. (Eds.), Proc. 14th National Conf.
Artificial Intelligence, pp. 714719, Providence, RI. AAAI Press / MIT Press.
Cormen, T., Leiserson, C., & Rivest, R. (1990). Introduction Algorithms. MIT Press.
Edelkamp, S. (2001). Planning pattern databases. Cesta, A. (Ed.), Proc. 6th
European Conf. Planning, pp. 1324, Toledo, Spain. Springer: LNCS.
Gazen, B., & Knoblock, C. (1997). Combining expressiveness UCPOP
efficiency Graphplan. Steel, S., & Alami, R. (Eds.), Proc. 4th European Conf.
Planning, pp. 221233, Toulouse, France. Springer: LNCS.
Haslum, P., Bonet, B., & Geffner, H. (2005). New admissible heuristics domainindependent planning. Veloso, M., & Kambhampati, S. (Eds.), Proc. 20 National
Conf. Artificial Intelligence, pp. 11631168, Pittsburgh, PA. AAAI Press / MIT
Press.
Haslum, P., & Geffner, H. (2000). Admissible heuristic optimal planning. Chien, S.,
Kambhampati, S., & Knoblock, C. (Eds.), Proc. 6th International Conf. Artificial
Intelligence Planning Scheduling, pp. 140149, Breckenridge, CO. AAAI Press.
Hoffmann, J., & Nebel, B. (2001). planning system: Fast plan generation
heuristic search. Journal Artificial Intelligence Research, 14, 253302.
Korf, R. (1990). Real-time heuristic search. Artificial Intelligence, 42 (23), 189211.
Nguyen, X., & Kambhampati, S. (2000). Extracting effective admissible state-space
heuristics planning graph. Kautz, H., & Porter, B. (Eds.), Proc. 17th
National Conf. Artificial Intelligence, pp. 798805, Austin, TX. AAAI Press /
MIT Press.
Pearl, J. (1983). Heuristics. Morgan Kaufmann.
Tarjan, R. E. (1972). Depth first search linear graph algorithms. SIAM Journal
Computing, 1 (2), 146160.

944


