Journal Artificial Intelligence Research 24 (2005) 341-356

Submitted 11/04; published 09/05

Efficiency versus Convergence Boolean Kernels
On-Line Learning Algorithms
Roni Khardon

roni@cs.tufts.edu

Department Computer Science, Tufts University
Medford, 02155

Dan Roth

danr@cs.uiuc.edu

Department Computer Science, University Illinois
Urbana, IL 61801 USA

Rocco A. Servedio

rocco@cs.columbia.edu

Department Computer Science, Columbia University
New York, NY 10025

Abstract
paper studies machine learning problems example described using
set Boolean features hypotheses represented linear threshold elements.
One method increasing expressiveness learned hypotheses context
expand feature set include conjunctions basic features. done explicitly
possible using kernel function. Focusing well known Perceptron
Winnow algorithms, paper demonstrates tradeoff computational
efficiency algorithm run expanded feature space
generalization ability corresponding learning algorithm.
first describe several kernel functions capture either limited forms conjunctions conjunctions. show kernels used efficiently run
Perceptron algorithm feature space exponentially many conjunctions; however show using kernels, Perceptron algorithm provably make
exponential number mistakes even learning simple functions.
consider question whether kernel functions analogously used
run multiplicative-update Winnow algorithm expanded feature space
exponentially many conjunctions. Known upper bounds imply Winnow algorithm
learn Disjunctive Normal Form (DNF) formulae polynomial mistake bound
setting. However, prove computationally hard simulate Winnows
behavior learning DNF feature set. implies kernel functions
correspond running Winnow problem efficiently computable,
general construction run Winnow kernels.

1. Introduction
problem classifying objects one two classes positive negative
examples concept often studied machine learning. task machine learning
extract classifier given pre-classified examples - problem learning
data. example represented set n numerical features, example
c
2005
AI Access Foundation. rights reserved.

fiKhardon, Roth, & Servedio

seen point Euclidean space <n . common representation classifiers
case hyperplane dimension (n 1) splits domain examples
two areas positive negative examples. representation known linear
threshold function, many learning algorithms output hypothesis represented
manner developed, analyzed, implemented, applied practice.
particular interest paper well known Perceptron (Rosenblatt, 1958; Block,
1962; Novikoff, 1963) Winnow (Littlestone, 1988) algorithms intensively
studied literature.
well known expressiveness linear threshold functions quite limited (Minsky & Papert, 1968). Despite fact, Perceptron Winnow
applied successfully recent years several large scale real world classification problems.
one example, SNoW system (Roth, 1998; Carlson, Cumby, Rosen, & Roth, 1999)
successfully applied variations Perceptron Winnow problems natural language
processing. SNoW system extracts basic Boolean features x1 , . . . , xn labeled pieces
text data order represent examples, thus features numerical values restricted {0, 1}. several ways enhance set basic features x 1 , . . . , xn
Perceptron Winnow. One idea expand set basic features x 1 , . . . , xn using
conjunctions (x1 x3 x4 ) use expanded higher-dimensional examples,
conjunction plays role basic feature, examples Perceptron
Winnow. fact approach SNoW system takes running Perceptron
Winnow space restricted conjunctions basic features. idea closely
related use kernel methods, see e.g. book Cristianini Shawe-Taylor
(2000), feature expansion done implicitly kernel function. approach clearly leads increase expressiveness thus may improve performance.
However, dramatically increases number features (from n 3 n conjunctions used), thus may adversely affect computation time convergence
rate learning. paper provides theoretical study performance Perceptron
Winnow run expanded feature spaces these.
1.1 Background: On-Line Learning Perceptron Winnow
describing results, recall necessary background on-line learning
model (Littlestone, 1988) Perceptron Winnow algorithms.
Given instance space X possible examples, concept mapping instances
one two (or more) classes. concept class C 2X simply set concepts. on-line
learning concept class C fixed advance adversary pick concept c C.
learning modeled repeated game iteration adversary
picks example x X, learner gives guess value c(x) told
correct value. count one mistake iteration value predicted
correctly. learning algorithm learns concept class C mistake bound
choice c C (arbitrarily long) sequence examples, learner guaranteed
make mistakes.
paper consider case examples given Boolean features,
X = {0, 1}n , two class labels denoted 1 1. Thus x {0, 1}n ,
labeled example hx, 1i positive example, labeled example hx, 1i negative
342

fiEfficiency versus Convergence Boolean Kernels

example. concepts consider built using logical combinations n base
features interested mistake bounds polynomial n.
1.1.1 Perceptron
Throughout execution Perceptron maintains weight vector w < N initially
(0, . . . , 0). Upon receiving example x <N algorithm predicts according
linear threshold function w x 0. prediction 1 label 1 (false positive
prediction) vector w set w x, prediction 1 label 1
(false negative) w set w + x. change made w prediction correct.
Many variants basic algorithm proposed studied particular one
add non zero threshold well learning rate controls size update
w. discussed Section 3.
famous Perceptron Convergence Theorem (Rosenblatt, 1958; Block, 1962; Novikoff,
1963) bounds number mistakes Perceptron algorithm make:
Theorem 1 Let hx1 , y1 i, . . . , hxt , yt sequence labeled examples xi <N , kxi k
R yi {1, 1} i. Let u <N , > 0 yi (u xi ) i.
2
2
mistakes example sequence.
Perceptron makes R kuk
2
1.1.2 Winnow
Winnow algorithm (Littlestone, 1988) similar structure. Winnow maintains
hypothesis vector w <N initially w = (1, . . . , 1). Winnow parameterized
promotion factor > 1 threshold > 0; upon receiving example x {0, 1} N
Winnow predicts according threshold function w x . prediction 1
label 1 xi = 1 value wi set wi /; demotion
step. prediction 1 label 1 xi = 1 value wi
set wi ; promotion step. change made w prediction correct.
purposes following mistake bound, implicit Littlestones work (1988),
interest:
Theorem 2 Let target function k-literal monotone disjunction f (x 1 , . . . , xN ) =
xi1 xik . sequence examples {0, 1}N labeled according f number

prediction mistakes made Winnow(, ) 1
N + k( + 1)(1 + log ).
1.2 Results
interested computational efficiency convergence Perceptron
Winnow algorithms run expanded feature spaces conjunctions. Specifically,
study use kernel functions expand feature space thus enhance
learning abilities Perceptron Winnow; refer enhanced algorithms
kernel Perceptron kernel Winnow.
first result (cf. papers Sadohara, 1991; Watkins, 1999; Kowalczyk
et al., 2001) uses kernel functions show possible efficiently run kernel
Perceptron algorithm exponential number conjunctive features.
343

fiKhardon, Roth, & Servedio

Result 1: (see Theorem 3) algorithm simulates Perceptron 3 n dimensional feature space conjunctions n basic features. Given sequence
labeled examples {0, 1}n prediction update example take poly(n, t) time
steps. prove variants result expanded feature space consists
monotone conjunctions conjunctions bounded size.
result closely related one main open problems learning theory:
efficient learnability disjunctions conjunctions, DNF (Disjunctive Normal Form)
expressions.1 Since linear threshold elements represent disjunctions (e.g. x1 x2 x3
true iff x1 + x2 + x3 1), Theorem 1 Result 1 imply kernel Perceptron
used learn DNF. However, framework values N R Theorem 1
exponentially large (note N = 3n R = 2n/2 conjunctions used),
hence mistake bound given Theorem 1 exponential rather polynomial
n. question thus arises whether exponential upper bound implied Theorem
1 essentially tight kernel Perceptron algorithm context DNF learning.
give affirmative answer, thus showing kernel Perceptron cannot efficiently learn
DNF.
Result 2: monotone DNF f x1 , . . . , xn sequence examples labeled
according f causes kernel Perceptron algorithm make 2 (n) mistakes.
result holds generalized versions Perceptron algorithm fixed updated
threshold learning rate used. give variant result showing
kernel Perceptron fails Probably Approximately Correct (PAC) learning model
(Valiant, 1984) well.
Turning Winnow, attractive feature Theorem 2 suitable , bound
logarithmic total number features N (e.g. = 2 = N ). Therefore,
noted several researchers (Maass & Warmuth, 1998), Winnow analogue Theorem 3
could obtained would imply DNF learned computationally efficient
algorithm poly(n)-mistake bound. However, give strong evidence
Winnow analogue Theorem 3 exist.
Result 3: polynomial time algorithm simulates Winnow exponentially many monotone conjunctive features learning monotone DNF unless every problem
complexity class #P solved polynomial time. result holds wide
range parameter settings Winnow algorithm.
observe that, contrast negative result, Maass Warmuth shown
Winnow algorithm simulated efficiently exponentially many conjunctive
features learning simple geometric concept classes (Maass & Warmuth, 1998).
results thus indicate tradeoff computational efficiency convergence
kernel algorithms rich classes Boolean functions DNF formulas; kernel
1. Angluin (1990) proved DNF expressions cannot learned efficiently using equivalence queries
whose hypotheses DNF expressions. Since model exact learning equivalence
queries equivalent mistake bound model consider paper, result implies
online algorithm uses DNF formulas hypotheses efficiently learn DNF. However,
result preclude efficient learnability DNF using different class hypotheses.
kernel Perceptron algorithm generates hypotheses thresholds conjunctions rather DNF
formulas, thus Angluins negative results apply here.

344

fiEfficiency versus Convergence Boolean Kernels

Perceptron algorithm computationally efficient run exponentially slow convergence, whereas kernel Winnow rapid convergence seems require exponential
runtime.

2. Kernel Perceptron Many Features
well known hypothesis w Perceptron algorithm linear combination
previous examples mistakes made (Cristianini & Shaw-Taylor, 2000).
precisely, let L(v) {1, 1} denote label example v,
P
w = vM L(v)v set examples algorithm made mistake.
P
P
Thus prediction Perceptron x 1 iff wx = ( vM L(v)v)x = vM L(v)(v x)
0.
example x {0, 1}n let (x) denote transformation enhanced feature
space space conjunctions. run Perceptron algorithm
enhanced space must predict 1 iff w (x) 0 w weight vector
P
enhanced space; discussion holds iff vM L(v)((v) (x)) 0.
P
Denoting K(v, x) = (v) (x) holds iff vM L(v)K(v, x) 0.
Thus never need construct enhanced feature space explicitly; order run
Perceptron need able compute kernel function K(v, x) efficiently.
idea behind so-called kernel methods, applied algorithm (such
support vector machines) whose prediction function inner products examples.
detailed discussion given book Cristianini Shawe-Taylor (2000).
Thus next theorem simply obtained presenting kernel function capturing
conjunctions.
Theorem 3 algorithm simulates Perceptron feature spaces
(1) conjunctions, (2) monotone conjunctions, (3) conjunctions size k, (4)
monotone conjunctions size k. Given sequence labeled examples {0, 1} n
prediction update example take poly(n, t) time steps.
Proof: case (1) () includes 3n conjunctions (with positive negative literals)
K(x, y) must compute number conjunctions true x y. Clearly,
literal conjunction must satisfy x thus corresponding bit
x, must value. Thus conjunction true x corresponds
subset bits. Counting conjunctions gives K(x, y) = 2same(x,y)
same(x, y) number original features value x y, i.e.
number bit positions xi = yi . kernel obtained independently
Sadohara (2001).
express monotone monomials (2) take K(x, y) = 2|xy| |x y|
number active features common x y, i.e. number bit positions
xi = yi = 1.
Similarly, case (3) number conjunctions satisfy x K(x, y) =
Pk same(x,y)
. kernel reported Watkins (1999). case (4)
l=0
l


P
K(x, y) = kl=0 |xy|
.
2
l
345

fiKhardon, Roth, & Servedio

3. Kernel Perceptron Many Mistakes
section describe simple monotone DNF target function sequence
labeled examples causes monotone monomials kernel Perceptron algorithm
make exponentially many mistakes.
x, {0, 1}n write |x| denote number 1s x and, described above,
|xy| denote number bit positions xi = yi = 1. need following
well-known tail bound sums independent random variables found in,
e.g., Section 9.3 book Kearns Vazirani (1994):
Fact 4 Let X1 , . . . , Xm sequence independent 0/1-valued random variables,
P
E[Xi ] = p. Let X denote
i=1 Xi , E[X] = pm. 0 1,

Pr[X > (1 + )pm] emp

2 /3



Pr[X < (1 )pm] emp

2 /2

.

use following combinatorial property:
Lemma 5 set n-bit strings = {x1 , . . . , xt } {0, 1}n = en/9600
|xi | = n/20 1 |xi xj | n/80 1 < j t.
Proof: use probabilistic method. = 1, . . . , let xi {0, 1}n chosen
independently setting bit 1 probability 1/10. clear
E[|xi |] = n/10. Applying Fact 4, Pr[|xi | < n/20] en/80 , thus
probability xi satisfies |xi | < n/20 ten/80 . Similarly, 6= j
E[|xi xj |] = n/100. Applying Fact 4 Pr[|xi xj | > n/80] en/4800 ,
thus probability xi , xj
6= j satisfies |xi xj | > n/80
n/4800
n/4800
n/9600
+ ten/80 less 1. Thus
. = e
value 2 e
2 e
choice x1 , . . . , xt |xi | n/20 |xi xj | n/80. xi
|xi | > n/20 set |xi | n/20 1s 0s, lemma proved.
2
using previous lemma construct difficult data set kernel Perceptron:
Theorem 6 monotone DNF f x1 , . . . , xn sequence examples labeled
according f causes kernel Perceptron algorithm make 2(n) mistakes.
Proof: target DNF use simple: single conjunction
x1 x2 . . . xn . original Perceptron algorithm n features x1 , . . . , xn easily
seen make poly(n) mistakes target function, show
monotone kernel Perceptron algorithm runs feature space 2 n monotone
monomials make 2 + en/9600 mistakes.
Recall beginning Perceptron algorithms execution 2 n coordinates
w 0. first example negative example 0n . monomial true
example empty monomial true every example. Since w (x) = 0 Perceptron incorrectly predicts 1 example. resulting update causes coefficient
w corresponding empty monomial become 1 2n 1 coordinates
w remain 0. next example positive example 1n . example
w (x) = 1 Perceptron incorrectly predicts 1. Since 2n monotone conjunctions
346

fiEfficiency versus Convergence Boolean Kernels

satisfied example resulting update causes w become 0 2n 1
coordinates w become 1. next en/9600 examples vectors x1 , . . . , xt
described Lemma 5. Since example |xi | = n/20 example negative;
however show Perceptron algorithm predict 1 examples.
Fix value 1 en/9600 consider hypothesis vector w example

x received. Since |xi | = n/20 value w (xi ) sum 2n/20 different
coordinates wT correspond monomials satisfied xi . precisely
P
P
w (xi ) = Ai wT + Bi wT Ai contains monomials satisfied
xi xj j 6= Bi contains monomials satisfied xi
xj j 6= i. lower bound two sums separately.
Let monomial Ai . Lemma 5 Ai contains n/80 variables

Pn/80
monomials Ai . Using well known bound
thus r=0 n/20
r


P` `
(H()+o(1))`
0 < 1/2 H(p) = p log p (1 p) log(1 p)
j=0 j = 2
binary entropy function, found e.g. Theorem 1.4.5 book
Van Lint (1992), 20.8113(n/20)+o(n) < 20.041n terms Ai . Moreover
value wT must least en/9600 since wT decreases 1
P
example, hence Ai wT en/9600 20.041n > 20.042n . hand, Bi
false examples therefore wT demoted wT = 1.
Lemma 5 r > n/80 every r-variable monomial satisfied xi must belong Bi ,


P
Pn/20
hence Bi wT r=n/80+1 n/20
> 20.049n . Combining inequalities
r
w xi 20.042n + 20.049n > 0 hence Perceptron prediction xi 1.
2
Remark 7 first sight might seem result limited simple special case
perceptron algorithm. Several variations exist use: added feature fixed
value enables algorithm update threshold indirectly (via weight w), non
zero fixed (initial) threshold , learning rate , particular three
used simultaneously. generalized algorithm predicts according hypothesis
w x + w updates w w + x w w + promotions similarly
demotions. show exponential lower bounds number mistakes
derived general algorithm well. First, note since kernel
includes feature empty monomial always true, first parameter
already accounted for. two parameters note degree freedom
learning rate fixed threshold since multiplying factor
change hypothesis therefore suffices consider threshold only.
consider several cases value threshold. satisfies 0 2 0.047
use sequence examples. first two examples algorithm makes
promotion 1n (it may may update 0n important).
P
P
examples sequence bounds Ai wT Bi wT still valid
final inequality proof becomes w xi 20.042n + 20.049n > 20.047n true
sufficiently large n. > 20.047n construct following scenario. use
function f = x1 x2 . . . xn , sequence examples includes 2 1 repetitions
example x first bit 1 bits 0. example x satisfies
exactly 2 monomials therefore algorithm make mistakes examples
sequence. < 0 initial hypothesis misclassifies 0n . start example
347

fiKhardon, Roth, & Servedio

sequence repeating example 0n classified correctly, de times.
threshold large absolute value e.g. < 20.042n done. Otherwise
continue example 1n . Since weights except empty monomial zero
stage examples 0n 1n classified way 1n misclassified
therefore algorithm makes promotion. argument rest sequence
(except adding term empty monomial) final inequality becomes
w xi 20.042n 20.042n + 20.049n > 20.042n examples misclassified. Thus
cases kernel Perceptron may make exponential number mistakes.
3.1 Negative Result PAC Model
proof adapted give negative result kernel Perceptron PAC
learning model (Valiant, 1984). model example x independently drawn
fixed probability distribution high probability learner must construct
hypothesis h high accuracy relative target concept c distribution D.
See Kearns-Vazirani text (1994) detailed discussion PAC learning model.
Let probability distribution {0, 1}n assigns weight 1/4 ex1
en/9600 examples
ample 0n , weight 1/4 example 1n , weight 21 en/9600
x1 , . . . , xt .
Theorem 8 kernel Perceptron run using sample polynomial size p(n)
probability least 1/16 error final hypothesis least 0.49.
Proof: probability 1/16, first two examples received 0n
1n . Thus, probability 1/16, two examples (as proof above) Perceptron
algorithm w = 0 coefficients w equal 1.
Consider sequence examples following two examples. First note
trial, occurrence example 1n (i.e. occurrence either xi
P
0n example) decrease [n] wT 2n/20 . Since first two examples
P
w (1n ) = [n] wT = 2n 1, follows least 219n/20 1 examples
must occur 1n example incorrectly classified negative example. Since
consider performance algorithm p(n) < 219n/20 1 steps,
may ignore subsequent occurrences 1n since change algorithms
hypothesis.
observe first example 1n algorithm perform
demotion resulting w = 1 (possibly changing coefficients well). Since
promotions performed rest sample, get w 1 rest
learning process. follows future occurrences example 0 n correctly
classified thus may ignore well.
Considering examples xi sequence constructed above, may ignore example correctly classified since update made it. follows
perceptron algorithm gone examples, hypothesis formed demotions
examples sequence xi s. difference scenario
algorithm may make several demotions example occurs multiple times
sample. However, inspection proof shows x
P
P
seen algorithm, bounds Ai wT Bi wT still valid
348

fiEfficiency versus Convergence Boolean Kernels

therefore xi misclassified. Since sample size p(n) sequence
size en/9600 probability weight examples sample 0.01 sufficiently
large n error hypothesis least 0.49.
2

4. Computational Hardness Kernel Winnow
section, x {0, 1}n let (x) denote (2n 1)-element vector whose coordinates nonempty monomials (monotone conjunctions) x1 , . . . , xn . say
sequence labeled examples hx1 , b1 i, . . . , hxt , bt monotone consistent consistent
monotone function, i.e. xik xjk k = 1, . . . , n implies bi bj .
monotone consistent labeled examples clearly monotone DNF
formula consistent contains conjunctions. consider following
problem:
KERNEL WINNOW PREDICTION(, ) (KWP)
Instance: Monotone consistent sequence = hx1 , b1 i, . . . , hxt , bt labeled examples
xi {0, 1}m bi {1, 1}; unlabeled example z {0, 1}m .
Question: w (z) , w N = (2m 1)-dimensional hypothesis vector
generated running Winnow(, ) example sequence h(x1 ), b1 i, . . . h(xt ), bt i?
order run Winnow 2m 1 nonempty monomials learn monotone DNF,
one must able solve KWP efficiently. main result section proof
KWP computationally hard wide range parameter settings yield
polynomial mistake bound Winnow via Theorem 2.
Recall #P class counting problems associated N P decision problems; well known every function #P computable polynomial time
P = N P. See book Papadimitriou (1994) paper Valiant (1979) details
#P. following problem #P-hard (Valiant, 1979):
MONOTONE 2-SAT (M2SAT)
Instance: Monotone 2-CNF Boolean formula F = c1 c2 . . . cr ci = (yi1 yi2 )
yij {y1 , . . . , yn }; integer K 1 K 2n .
Question: |F 1 (1)| K, i.e. F least K satisfying assignments {0, 1}n ?
Theorem 9 Fix > 0. Let N = 2m 1, let 1 + 1/m1 , let 1

max( 1
N , ( + 1)(1 + log )) = poly(m). polynomial time algorithm
KWP(, ), every function #P computable polynomial time.
Proof: N, described theorem routine calculation shows
1 + 1/m1 poly(m)



2m
2poly(m) .
poly(m)

(1)

proof reduction problem M2SAT. high level idea proof
simple: let (F, K) instance M2SAT F defined variables y1 , . . . , yn .
Winnow algorithm maintains weight wT monomial variables x1 , . . . , xn .
define 1-1 correspondence monomials truth assignments {0, 1}n
349

fiKhardon, Roth, & Servedio

F, give sequence examples Winnow causes wT 0 F (y ) = 0
wT = 1 F (y ) = 1. value w (z) thus related |F 1 (1)|. Note
could control well would sufficient since could use = K
result follow. However parameter algorithm. therefore make
additional updates w (z) + (|F 1 (1)| K) w (z)
|F 1 (1)| K. details somewhat involved since must track resolution
approximations different values final inner product indeed give
correct result respect threshold.
General setup construction. detail, let
U = n + 1 + d(dlog 4e + 1) log e,
n+1
V = log
e + 1,
U +2
W = log
e + 1

let defined
= n + U + 6V n2 + 6U W + 3.

(2)

Since 1 + 1/m1 , using fact log(1 + x) x/2 0 < x < 1
log 1/(2m1 ), easily follows specified polynomial
n. describe polynomial time transformation maps n-variable instance (F, K)
M2SAT m-variable instance (S, z) KWP(, ) = hx 1 , b1 i, . . . , hxt , bt
monotone consistent, xi z belong {0, 1}m , w (z)
|F 1 (1)| K.
Winnow variables x1 , . . . , xm divided three sets A, B C =
{x1 , . . . , xn }, B = {xn+1 , . . . , xn+U } C = {xn+U +1 , . . . , xm }. unlabeled example z
1n+U 0mnU , i.e. variables B set 1 variables C set 0.
P
P
thus w (z) = +MB +MAB = 6=T wT , MB = 6=T B wT
P
MAB = AB,T A6=,T B6= wT . refer monomials 6= type-A monomials,
monomials 6= B type-B monomials, monomials AB, 6= , B 6=
type-AB monomials.
example sequence divided four stages. Stage 1 results |F 1 (1)|;
described n variables correspond n variables CNF formula
F. Stage 2 results q |F 1 (1)| positive integer q specify later.
Stages 3 4 together result MB + MAB q K. Thus final value w (z)
approximately + q (|F 1 (1)| K), w (z) |F 1 (1)| K.
Since variables C 0 z, includes variable C value wT
affect w (z). variables C slack variables (i) make Winnow
perform correct promotions/demotions (ii) ensure monotone consistent.
Stage 1: Setting |F 1(1)|. define following correspondence
truth assignments {0, 1}n monomials : yiT = 0 xi
present T. clause yi1 yi2 F, Stage 1 contains V negative examples
xi1 = xi2 = 0 xi = 1 xi A. show (1) Winnow makes
false positive prediction examples (2) Stage 1 Winnow never
350

fiEfficiency versus Convergence Boolean Kernels

promotion example variable set 1. Consider
F (y ) = 0. Since examples include example monomial
demoted least V times. result Stage 1 , w = 1
F (y ) = 1 0 < wT V F (y ) = 0. Thus = |F 1 (1)| + 1
0 < 1 < 2n V < 21 .
show Stage 1 examples cause Winnow make false positive prediction
negative examples xi1 = xi2 = 0 xi = 1 described
above. negative example Stage 1 six new slack variables x+1 , . . . , x+6 C
used follows: Stage 1 dlog (/3)e repeated instances positive example
x+1 = x+2 = 1 bits 0. examples cause promotions result
wx+1 + wx+2 + wx+1 x+2 < hence wx+1 /3. Two groups
similar examples (the first x+3 = x+4 = 1, second x+5 = x+6 = 1) cause
wx+3 /3 wx+5 /3. next example negative example
xi1 = xi2 = 0, xi = 1 xi A, x+1 = x+3 = x+5 = 1 bits 0.
example w (x) > wx+1 + wx+3 + wx+5 Winnow makes false positive
prediction.
Since F n2 clauses V negative examples per clause,
construction carried using 6V n2 slack variables xn+U +1 , . . . , xn+U +6V n2 .
thus (1) (2) claimed above.
Stage 2: Setting q |F 1(1)|. first Stage 2 example positive example
xi = 1 xi A, xn+U +6V n2 +1 = 1 bits 0. Since 2n
monomials contain xn+U +6V n2 +1 satisfied example wT = 1,
w (x) = 2n + |F 1 (1)| + 1 < 2n+1 . Since > 2m /poly(m) > 2n+1 (recall
equation (2) > 6n2 ), resulting promotion w (x) =
(2n + |F 1 (1)| + 1 ) < 2n+1 . Let
q = dlog (/2n+1 )e 1

q 2n+1 < q+1 2n+1 .

(3)

Stage 2 consists q repeated instances positive example described above.
promotions w (x) = q (2n + |F 1 (1)| + 1 ) < q 2n+1 < . Since 1 <
|F 1 (1)| + 1 < 2n
q < = q (|F 1 (1)| + 1 ) < q 2n < /2.

(4)

Equation (4) gives value throughout rest argument.
Calculations Stages 3 4. start Stage 3 type-B typeAB monomial wT = 1. n variables U variables B
start Stage 3 MB = 2U 1 MAB = (2n 1)(2U 1). Since example
Stages 3 4 satisfies xi A, end Stage 4 still q (|F 1 (1)| + 1 )
MAB still (2n 1)(2U 1). Therefore end Stage 4
w (z) = MB + q (|F 1 (1)| + 1 ) + (2n 1)(2U 1).
351

fiKhardon, Roth, & Servedio

simplify notation let
= (2n 1)(2U 1) q K.
Ideally end Stage 4 value MB would q 1 since would imply
w (z) = + q (|F 1 (1)| K) least |F 1 (1)| K. However
necessary MB assume exact value, since |F 1 (1)| must integer
0 < 1 < 21 . long
1
(5)
MB < + q
2
get
1
+ q (|F 1 (1)| K + 1 ) < w (z) < + q (|F 1 (1)| K + 1 + ).
2
|F 1 (1)| K clearly w (z) . hand |F 1 (1)| < K
since |F 1 (1)| integer value |F 1 (1)| K 1 get w (z) < . Therefore
remains construct examples Stages 3 4 B satisfies
Equation (5).
next calculate appropriate granularity D. Note K 2 n , Equation (3) q K > /2. recall Equations (2) (1) >
2
n + U + 6n2 > 2m /poly(m), /2 2n+U +6n /poly(m) 2n 2U . Consequently
certainly > /4, Equation (3) > /4 > q 2n1 > 14 q .
Let
c = dlog 4e,

1
qc q < D.
4

(6)

unique smallest positive integer p > 1 satisfies pqc < + 41 q .
Stage 3 examples result MB satisfying p < MB < p + 14 . that:
1
qc < pqc < + q
4
3 q

4
q+1 2n+1 3qc
=

qc

(

c+1 n+1

2

3).

(7)
(8)
(9)

(7) holds since K 1, thus (by definition D) + q
equivalent Equation (7). Inequality (8) follows Equations (6) (3).
Hence
1 < p c+1 2n+1 3 2n+1+d(c+1) log e 3 = 2U 3,

(10)

second inequality chain follows Equation (9). use
following lemma:
352

fiEfficiency versus Convergence Boolean Kernels

Lemma 10 ` 1, 1 p 2` 1, monotone CNF F`,p
` Boolean variables ` clauses, exactly p satisfying assignments
{0, 1}` , constructed ` p poly(`) time.
Proof: proof induction `. base case ` = 1 p = 1 F `,p = x1 .
Assuming lemma true ` = 1, . . . , k prove ` = k + 1 :
1 p 2k 1 desired CNF Fk+1,p = xk+1 Fk,p . Since Fk,p k
clauses Fk+1,p k + 1 clauses. 2k + 1 p 2k+1 1 desired CNF
Fk+1,p = xk+1 Fk,p2k . distributing xk clause Fk,p2k write Fk+1,p
CNF k clauses. p = 2k Fk,p = x1 .
2
Stage 3: Setting MB p. Let FU,p r-clause monotone CNF formula
U variables B p satisfying assignments. Similar Stage 1, clause
FU,p , Stage 3 W negative examples corresponding clause, Stage
1 slack variables C used ensure Winnow makes false positive prediction
negative example. Thus examples Stage 3 cause MB = p + 2
0 < 2 < 2U W < 41 . Since six slack variables C used negative example
rW U W negative examples, slack variables xn+U +6V n2 +2 , . . . , xm2
sufficient Stage 3.
Stage 4: Setting MB + MAB q K. remains perform q c
promotions examples xi B set 1. cause MB equal
(p + 2 )qc . inequalities established above, give us
1
1
pqc < (p + 2 )qc = MB < + q + 2 qc < + q
4
2
desired.
order guarantee q c promotions use two sequences examples length
n
U n
q dU
log e log e c respectively. first show positive numbers.
follows directly definitions U = n + 1 + d(dlog 4e + 1) log e c = dlog 4e
n
6n2 (by definition Equation (1)) bounded
U
log c. Since > 2
polynomial m, clearly log(/2n+1 ) > U n + log(). since
n+1 )
U n
n
q = dlog (/2n+1 )e 1 implies q > log(/2
1 > dU
log e, q log e > 0.
log()
n
first q U
log e examples Stage 4 positive example
xi B set 1 xm1 = 1. first time example received,
2
w (x) = 2U + p + 2 < 2U +1 . Since > 26n , inspection U 2U +1 < ,
n
Winnow performs promotion. Similarly, q U
log e occurrences example,

qd U n e
qd U n e
w (x) = log (2U + p + 2 ) < log 2U +1 q 2n+1 <

promotions indeed performed occurrence,
MB =

n
qd U
e
log

(p + 2 ).

n
remaining examples Stage 4 U
log e c repetitions positive example x
xi B set 1 xm = 1. promotions occurred repetition

353

fiKhardon, Roth, & Servedio

example would w (x) =

n
dU
ec
log

(2U +

n
qd U
e
log

(p + 2 )), need

show quantity less . reexpress quantity
qc (p + 2 ).

n
ec U
dU
log
2

1
qc (p + 2 ) < pqc + qc
4
3 q
1
+ q
4
16
1 q
<
2

+

(11)

U n ec

(11) follows (7) definition c. Finally, log 2U
1

22U nc log < 22U n2 < 2
< 21 q , last inequality Equation (3)
2n+1
previous inequality inspection values , U . Combining two
bounds see indeed w (x) < .
Finally, observe construction example sequence monotone consistent.
Since = poly(n) contains poly(n) examples transformation M2SAT
KWP(, ) polynomial-time computable theorem proved.
2(Theorem 9)

5. Conclusion
Linear threshold functions weak representation language interesting learning algorithms. Therefore, linear learning algorithms learn expressive
functions, necessary expand feature space applied.
work explores tradeoff computational efficiency convergence using
expanded feature spaces capture conjunctions base features.
shown iteration kernel Perceptron algorithm
executed efficiently, algorithm provably require exponentially many updates even
learning function simple f (x) = x1 x2 . . . xn . hand, kernel
Winnow algorithm polynomial mistake bound learning polynomial-size monotone
DNF, results show widely accepted computational hardness assumption
impossible efficiently simulate execution kernel Winnow. latter implies
general construction run Winnow using kernel functions.
results indicate additive multiplicative update algorithms lie opposite
extremes tradeoff computational efficiency convergence; believe
fact could significant practical implications. demonstrating provable limitations using kernel functions correspond high-degree feature expansions,
results lend theoretical justification common practice using small degree
similar feature expansions well-known polynomial kernel.2
Since publication initial conference version work (Khardon, Roth, &
Servedio, 2002), several authors explored closely related ideas. One show
construction negative results Perceptron extend (either PAC
2. Boolean kernels different standard polynomial kernels conjunctions
weighted equally, allow negations.

354

fiEfficiency versus Convergence Boolean Kernels

online setting) related algorithms Support Vector Machines work constructing maximum margin hypothesis consistent examples. paper (Khardon
& Servedio, 2003) gives analysis PAC learning performance maximum margin
algorithms monotone monomials kernel, derives several negative results thus
giving negative evidence monomial kernel. paper (Cumby & Roth,
2003) kernel expressions description logic (generalizing monomials kernel)
developed successfully applied natural language molecular problems. Takimoto Warmuth (2003) study use multiplicative update algorithms
Winnow (such weighted majority) obtain positive results restricting
type loss function used additive base features. Chawla et al. (2004)
studied Monte Carlo estimation approaches approximately simulate Winnow algorithms performance run space exponentially many features. use
kernel methods logic learning developing alternative methods feature expansion
multiplicative update algorithms remain interesting challenging problems
investigated.

Acknowledgments
work partly done Khardon University Edinburgh partly
Servedio Harvard University. authors gratefully acknowledge financial
support work EPSRC grant GR/N03167, NSF grant IIS-0099446 Research Semester Fellowship Award Tufts University (Khardon), NSF grants ITR-IIS00-85836, ITR-IIS-0085980 IIS-9984168 (Roth), NSF grant CCR-98-77049 NSF
Mathematical Sciences Postdoctoral Fellowship (Servedio).

References
Angluin, D. (1990). Negative results equivalence queries. Machine Learning, 2, 121150.
Block, H. (1962). perceptron: model brain functioning. Reviews Modern
Physics, 34, 123135.
Carlson, A., Cumby, C., Rosen, J., & Roth, D. (1999). SNoW learning architecture.
Tech. rep. UIUCDCS-R-99-2101, UIUC Computer Science Department.
Chawla, D., Li, L., & Scott., S. (2004). approximating weighted sums exponentially
many terms. Journal Computer System Sciences, 69, 196234.
Cristianini, N., & Shaw-Taylor, J. (2000). Introduction Support Vector Machines.
Cambridge Press.
Cumby, C., & Roth, D. (2003). kernel methods relational learning. Proc.
International Conference Machine Learning.
Kearns, M., & Vazirani, U. (1994). Introduction Computational Learning Theory.
MIT Press, Cambridge, MA.
355

fiKhardon, Roth, & Servedio

Khardon, R., Roth, D., & Servedio, R. (2002). Efficiency versus convergence Boolean
kernels on-line learning algorithms. Dietterich, T. G., Becker, S., & Ghahramani,
Z. (Eds.), Advances Neural Information Processing Systems 14, Cambridge, MA.
MIT Press.
Khardon, R., & Servedio, R. (2003). Maximum margin algorithms Boolean kernels.
Proceedings Sixteenth Annual Conference Computational Learning Theory,
pp. 87101.
Lint, J. V. (1992). Introduction Coding Theory. Springer-Verlag.
Littlestone, N. (1988). Learning quickly irrelevant attributes abound: new linearthreshold algorithm. Machine Learning, 2, 285318.
Maass, W., & Warmuth, M. K. (1998). Efficient learning virtual threshold gates.
Information Computation, 141 (1), 378386.
Minsky, M., & Papert, S. (1968). Perceptrons: introduction computational geometry.
MIT Press, Cambridge, MA.
Novikoff, A. (1963). convergence proofs perceptrons. Proceeding Symposium
Mathematical Theory Automata, Vol. 12, pp. 615622.
Papadimitriou, C. (1994). Computational Complexity. Addison-Wesley.
Rosenblatt, F. (1958). Perceptron: probabilistic model information storage
organization brain. Psychological Review, 65, 386407.
Roth, D. (1998). Learning resolve natural language ambiguities: unified approach.
Proc. American Association Artificial Intelligence, pp. 806813.
Sadohara, K. (2001). Learning Boolean functions using support vector machines. Proc.
Conference Algorithmic Learning Theory, pp. 106118. Springer. LNAI 2225.
Takimoto, E., & Warmuth, M. (2003). Path kernels multiplicative updates. Journal
Machine Learning Research, 4, 773818.
Valiant, L. G. (1979). complexity enumeration reliability problems. SIAM
Journal Computing, 8, 410421.
Valiant, L. G. (1984). theory learnable. Communications ACM, 27 (11),
11341142.
Watkins, C. (1999). Kernels matching operations. Tech. rep. CSD-TR-98-07, Computer
Science Department, Royal Holloway, University London.

356


