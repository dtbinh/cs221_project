Journal Artificial Intelligence Research 24 (2005) 1-48

Submitted 11/04; published 07/05

CIXL2: Crossover Operator Evolutionary Algorithms
Based Population Features
Domingo Ortiz-Boyer
Cesar Hervas-Martnez
Nicolas Garca-Pedrajas

dortiz@uco.es
chervas@uco.es
npedrajas@uco.es

Department Computing Numerical Analysis
University Cordoba, Spain

Abstract
paper propose crossover operator evolutionary algorithms real
values based statistical theory population distributions. operator
based theoretical distribution values genes best individuals
population. proposed operator takes account localization dispersion
features best individuals population objective features
would inherited offspring. aim optimization balance
exploration exploitation search process.
order test efficiency robustness crossover, used set
functions optimized regard different criteria, as, multimodality, separability, regularity epistasis. set functions extract conclusions
function problem hand. analyze results using ANOVA multiple
comparison statistical tests.
example crossover used solve artificial intelligence problems,
applied proposed model problem obtaining weight network
ensemble neural networks. results obtained performance
standard methods.

1. Introduction
Evolutionary algorithms (EAs) general purpose searching methods. selection process crossover mutation operators establish balance exploration
exploitation search space adequate wide variety problems
whose solution presents difficulties insolvable using classical methods.
problems defined continuous domains, evolutionary algorithms applied
use real values, namely, evolution strategies (EPs), real-coded genetic algorithms (RCGAs),
evolutionary programming (EP). paradigms precision solution
depend coding system, binary coded genetic algorithms, precision
computer system algorithms run.
selection process drives searching towards regions best individuals.
mutation operator randomly modifies, given probability, one genes
chromosome, thus increasing structural diversity population. see,
clearly exploration operator, helps recover genetic diversity lost
selection phase explore new solutions avoiding premature convergence. way,
probability reaching given point search space never zero. operator,
c
2005
AI Access Foundation. rights reserved.

fiOrtiz-Boyer, Hervas-Martnez, & Garca-Pedrajas

Exploration
2

2

2

Ex

pl
oi
ta

1

tio
n

H ,

12

Exploration

Exploitation
,

1
1



2
1



ai

(a)

1

2





Exploration

2

1






bi

(b)

Figure 1: (a) Hypercube defined first two genes parents; (b) Representation
segment defined ith genes two chromosomes.

fact, implements random search whose well-studied features useful field
evolutionary computation.
crossover operator combines genes two parents generate better
offspring. based idea exchange information good chromosomes generate even better offspring. effect crossover operator
studied two different points view: chromosome level gene level. effect
crossover operator chromosome level considered geometric way. Given
two parents 1 = {11 , 21 } 2 = {12 , 22 } two genes, denote H 1 2
hypercube defined genes (Figure 1a). gene level representation would
linear, defining case segment interval 1 , 2 pair genes (Figure 1b).


crossover operators generate individuals exploitation zones, 1 , 2 H 1 2 .


way, crossover operator implements depth search exploitation, leaving
breadth search exploration mutation operator.
policy, intuitively natural, makes population converge values within
hypercubes defined parents, producing rapid decrease population
diversity could end premature convergence non-optimal solution. Recent
studies BLX- crossover (Eshelman & Schaffer, 1993), crossover based fuzzy
connectives (Herrera, Herrera-Viedma, Lozano, & Verdegay, 1994), fuzzy recombination
(Voigt, Muhlenbein, & Cvetkovic, 1995), confirmed good performance
crossover operators generate individuals exploration zone. operators
avoid loss diversity premature convergence inner points search
space, generation new individuals exploration zone could slow
search process. reason, crossover operator establish adequate balance
exploration (or interpolation) exploitation (or extrapolation), generate
offspring exploration exploitation zones correct proportion.
Establishing balance exploration exploitation important,
important balance self-adaptive (Kita, 2001; Beyer & Deb, 2001; Deb &
Beyer, 2001), is, must guarantee dispersion offspring depends
2

fiCIXL2: Crossover Operator Evolutionary Algorithms

dispersion parents. So, two close parents must generate close offspring, two
distant parents must generate distant offspring. control dispersion crossover
based fuzzy connectives based generation offspring using fuzzy connectives t-norms, t-conorms, average functions, generalized operator compensation
(Mizumoto, 1989). fuzzy recombination offspring generated using two triangular
distributions whose averages derive genes two parents. BLX-
probability generating offspring parents, area
close parents whose amplitude modulated parameter.
Ono Kobayashi (1997) proposed Unimodal Normally Distributed Crossover
(UNDX), three parents used generate two children. children
obtained using ellipsoidal distribution one axis segment joins two
parents extent orthogonal direction decided perpendicular distance
third parent axis. authors claim operator preserve
statistics population. crossover self-adaptive, differs BLX-
fact probable generate offspring near average first two
parents.
Another self-adaptive crossover Simulated Binary Crossover (SBX) (Deb & Agrawal,
1995). Based search features single-point crossover used binary-coded genetic algorithms, operator respects interval schemata processing, sense
common interval schemata parents preserved offspring. SBX crossover
puts stress generating offspring near parents. So, crossover guarantees
extent children proportional extent parents, favors
near parent individuals monotonically likely chosen children
individuals distant parents.
main goal paper propose crossover operator avoids loss
diversity population individuals, and, time, favors speed
convergence algorithm. two goals are, first, conflicting; adequate
balance controlled two basic features crossover operator: i) balance
exploration exploitation and, ii) self-adaptive component. two
features make evolutionary algorithms avoid premature convergence favor local
fine-tuning. attributes highly appreciated search algorithm.
current crossover operators, features offspring depend features
parents. crossovers take account population features
localization dispersion individuals. use statistical features
population may help convergence population towards global optimum.
crossover operator implements basically depth exploitative search,
methods steepest gradient descent, local search simulated annealing,
three search methods algorithm takes quality solutions account.
So, reasonable think convenient crossover operator consider
performance individuals involved crossover operation. idea already
implemented heuristic crossovers (Wright, 1991).
Nevertheless, following previous line argument, seems rather poor use
two parents, consider promising directions towards would
advisable drive search. is, instead using local heuristic uses two
3

fiOrtiz-Boyer, Hervas-Martnez, & Garca-Pedrajas

individuals, involving whole population adequate subset determination
direction search whose features would specially suitable.
Motivated line argument, paper propose crossover operator,
called Confidence Interval Based Crossover using L2 Norm (CIXL2). one
hand, takes advantage selective component derived extraction
features best n individuals population indicates direction
search, hand, makes self-adaptive sampling around features
whose width depends number best individuals, dispersion best individuals,
confidence coefficient, localization individuals participate crossover.
Now, exploitation region area two parents involved
crossover, area defined confidence interval built n best
individuals population; exploratory region rest search domain.
previous concepts exploration exploitation, merely geometrical, added
probabilistic component depends population features best individuals.
Estimation Distribution Algorithms (EDAs) Probabilistic Model-Building Evolutionary Algorithms (Muhlenbein & Paa, 1998; Muhlenbein, Mahnig, & Rodriguez, 1999)
based a, seemingly, similar idea. algorithms mutation crossover
operators. every generation population distribution selected individuals
estimated new individuals obtained sampling estimated distribution. However, underlying idea behind crossover extraction population features, mean
standard deviation, order detect regions higher probability
getting best individuals. order perform crossover, create three virtual
parents represent localization estimator mean, bounds confidence
interval which, certain confidence degree, localization estimator takes
values. way, children generated three parents inherit features
best individuals population.
rest paper organized follows: Section 2 explains definition CIXL2
features; Section 3 discusses problem selection test sets,
justifies use test set based one proposed Eiben Back (1997a); Section
4 describes experimental setup evolutionary algorithm (RCGA) used tests;
Section 5 studies optimal values parameters CIXL2; Section 6 compares
performance CIXL2 crossovers; Section 7 compares CIXL2 EDAs;
Section 8 describes application RCGAs CIXL2 neural network ensembles;
and, finally, Section 9 states conclusions paper future research lines.

2. CIXL2 Operator
section explain theoretical base supports defined crossover
operator, define crossover. use example explain
dynamics population subject crossover operator.
2.1 Theoretical Foundation
section study distribution i-th gene construction
confidence interval localization parameter associated distribution.
4

fiCIXL2: Crossover Operator Evolutionary Algorithms

Let set N individuals p genes make population
set best n individuals. assume genes individuals
belonging independent random variables continuous distribution H(i )
localization parameter , define model
= + ei ,

= 1, ..., p,

(1)

ei random variable. suppose that, gene i, best n individuals form
, , ..., } distribution , model takes form
random sample {i,1
i,2
i,n


ij
= + eij ,

= 1, ..., p j = 1, ..., n.

(2)

Using model, analyze estimator localization parameter i-th
gene based minimization dispersion function induced L2 norm. L2
norm defined
n
X
(eij )2 ,
(3)
kei k22 =
j=1

hence associated dispersion induced L2 norm model 2
D2 (i ) =

n
X
j=1


(ij
)2 ,

(4)

estimator localization parameter is:




= arg min D2 ( ) = arg min


n
X
j=1


(ij
)2 .

(5)

Using minimization steepest gradient descent method,
S2 (i ) =
obtain
S2 (i ) = 2

D2 (i )
,


n
X
j=1


(ij
),

(6)

(7)

making (7) equal 0 yields
=

Pn


j=1 ij

n

= .

(8)

So, estimator localization parameter i-th gene based minimization dispersion function induced L2 norm mean distribution
(Kendall & Stuart, 1977), is, = .
5

fiOrtiz-Boyer, Hervas-Martnez, & Garca-Pedrajas

sample mean estimator linear estimator1 , properties unbiasedness2 consistency3 , follows normal distribution N (i , 2 /n)

distribution genes H(i ) normal. hypothesis, construct bilateral
confidence interval localization genes best n individuals, using
studentization method, mean localization parameter,and standard deviation
Si dispersion parameter:


Si
Si
CI

(9)
= tn1,/2 ; + tn1,/2
n
n
tn1,/2 value Students distribution n 1 degrees freedom,
1 confidence coefficient, is, probability interval contains true
value population mean.
2.2 CIXL2 Definition
definition confidence interval, define three intervals create three virtual parents, formed lower limits confidence interval gene, CILL 4 ,
upper limits, CIU L5 , means CIM 6 . parents statistical information
localization features dispersion best individuals population, is,
genetic information fittest individuals share. definition is:
CILL = (CILL1 , . . . , CILLi , . . . CILLp )

(10)

CIU L = (CIU L1 , . . . , CIU Li , . . . CIU Lp )
CIM

= (CIM1 , . . . , CIMi , . . . CIMp ),



CILLi = tn1,/2
n


CIU Li = + tn1,/2
n
CIMi = .

(11)

CILL CIU L individuals divide domain gene three subintervals:
Di IiL IiCI IiU , IiL [ai , CILLi ); IiCI [CILLi , CIU Li ]; IiU (CIU Li , bi ];
ai bi bounds domain (see Figure 2).
crossover operator creates one offspring , individual population
f , randomly selected, one individuals CILL, CIU L CIM , depending
localization f , follows:
1. linear combination sample values.
2. estimator unbiased estimator expected value estimator parameter
estimate: E[] = .
3. consistent estimator estimator converges probability quantity estimated
sample size grows.
4. Confidence Interval Lower Limit.
5. Confidence Interval Upper Limit.
6. Confidence Interval Mean.

6

fiCIXL2: Crossover Operator Evolutionary Algorithms

Di


ai

CI

L


Ii
C

C Mi

U






f

Ii


C U Li

bi

Figure 2: example confidence interval based crossover
IiL : fitness f higher CILL, = r(if CILLi ) + , else
= r(CILLi ) + CILLi .
IiCI : fitness f higher CIM, = r(if CIMi ) + , else
= r(CIMi ) + CIMi .
IiU : fitness f higher CIUL, = r(if CIU Li ) + , else
= r(CIU Li ) + CIU Li (this case seen Figure 2).
r random number interval [0, 1].
definition, offspring always takes values direction best
two parents never them. virtual individual one bounds
confidence interval better parent, offspring generated
direction confidence interval likely generate better individuals.
virtual individual worse parent, offspring generated near
parent opposite direction confidence interval. hand,
parent selected population within confidence interval, offspring
outside interval always neighborhood fitness center
confidence interval worse. formulation tries avoid shifting population
towards confidence interval, unless shifting means real improvement fitness
population.
f distant parent, offspring probably undergo marked
change, parents close, change small. first circumstance
likely occur first stages evolutionary process, second one
final stages.
width interval CI depends confidence coefficient, 1 , number
best individuals, n, dispersion best individuals. first stages
evolution, dispersion large, specially multimodal functions, decrease
together convergence genetic algorithm. features allow balance
exploitation exploration adjust dynamically. crossover
exploratory beginning evolution, avoiding premature convergence,
exploitative end, allowing fine tuning. parameters n 1 regulate
dynamics balance favoring higher lower degree exploitation. suggests
CIXL2 establishes self-adaptive equilibrium exploration exploitation based
features share, certain confidence degree 1 , best n individuals
7

fiOrtiz-Boyer, Hervas-Martnez, & Garca-Pedrajas

25

Best individuals distribution
Population distribution
Population distribution crossover

CIP


population
DCI best individuals
population crossover
Individuals
Individuals proyected axis x1x2
Best individuals
Best individuals proyected axis x1x2
CIP



f(x)

20

Individuals number

4000

0

15

10

2
1.5
1
-2

5

0.5
-1.5

-1

-0.5
x1

0
-0.5
0

0.5

1

1.5

-1
-1.5
2

x2

I2L

I2CI

I2U

-2
0

(a)

-2

-1

CILL2

0
x2

CIM2

CIUL2

1

2

(b)

Figure 3: Effect CIXL2 crossover population used minimization
Rosenbrock function two variables

population. preliminary theoretical study aspect carried HervasMartnez Ortiz-Boyer (2005).
2.3 Crossover Dynamics
Figure 3 shows simulation behavior crossover optimization Rosenbrock function (Eiben & Back, 1997b) two variables. Figure 3a, observe
individuals within domain CIP ; best n within confidence domain CI I1CI I2CI . DCI shifted towards minimum function placed
(1, 1), domain CIP new population, generated applying CIXL2,
shifted optimum. displacement higher first stages evolution,
decrease evolution. may modulated parameters n 1 .
Figure 3a shows population, applying crossover operator, distributed
region nearer optimum whose diversity depends parameters operator.
Figure 3b shows whole population n best individuals distributed.
see, distribution best n individuals keeps features distribution
population, shifted optimum. shifting towards optimum
marked value n small. tails distribution best individuals
larger dispersion best individuals large, smaller
concentrated narrow region. size tails depends features
problem, stage evolution, particular gene considered. effect
crossover distribution population shift distribution towards best
n individuals stretch distribution modulately depending amplitude
confidence interval. parameters n 1 responsible displacement
stretching region new individuals generated.
n small, population move promising individuals quickly.
may convenient increasing convergence speed unimodal functions. Nevertheless,
produce premature convergence suboptimal values multimodal functions.
n large, shifting speed convergence smaller. However,
8

fiCIXL2: Crossover Operator Evolutionary Algorithms

evolutionary process robust, feature perfectly adequate
optimization multimodal, non-separable, highly epistatic functions.
parameter n responsible selectiveness crossover, determines
region search directed. selection regulated parameter
1 . parameter bounds error margin crossover operator order obtain
search direction feature shares best individuals population.

3. Benchmark Problems
field evolutionary computation, common compare different algorithms using
large test set, especially test involves function optimization (Gordon & Whitley,
1993). However, effectiveness algorithm another algorithm cannot
measured number problems solves better. free lunch theorem
(Wolpert & Macready, 1995) shows that, compare two searching algorithms
possible functions, performance two algorithms , average,
. result, attempting design perfect test set functions present
order determine whether algorithm better another every function,
fruitless task.
reason why, algorithm evaluated, must look kind
problems performance good, order characterize type problems
algorithm suitable. way, made previous study
functions optimized constructing test set fewer functions better
selection (Whitley, Mathias, Rana, & Dzubera, 1995; Salomon, 1996). allows us
obtain conclusions performance algorithm depending type function.
Taking account reasoning, test set designed Eiben Back (1997b)
adequate. test set several well characterized functions allow us
obtain generalize, far possible, results regarding kind function involved.
Nevertheless, added two functions test set aim balancing
number functions kind. two new functions function Rosenbrock
(Rosenbrock, 1960) extended p dimensions function Schwefel (Schwefel, 1981);
widely used evolutive optimization literature. Table 1 shows
expression function summary features: separability, multimodality,
regularity.
function multimodal two local optima. function p variables
separable rewritten sum p functions one variable (Hadley, 1964).
separability closely related concept epistasis interrelation among
variables function. field evolutionary computation, epistasis measures
much contribution gene fitness individual depends values
genes.
Non separable functions difficult optimize accurate search direction
depends two genes. hand, separable functions optimized
variable turn. problem even difficult function multimodal.
search process must able avoid regions around local minima order
approximate, far possible, global optimum. complex case appears
local optima randomly distributed search space.
9

fiOrtiz-Boyer, Hervas-Martnez, & Garca-Pedrajas

Function
Sphere

Schwefels
double sum
Rosenbrock

Rastrigin

Schwefel

Ackley

Griewangk

Fletcher
Powell

Langerman

Definition
Pp
2
fSph (x) =
i=1 xi
xi [5.12, 5.12]
x = (0, 0, . . . , 0); fSph (x ) = 0
P
2
Pp

fSchDS (x) =
j=1 xj
i=1
xi [65.536, 65.536]
x = (0, 0, . . . , 0); fSchDS (x ) = 0
Pp1
2 2
2
fRos (x) =
i=1 [100(xi+1 xi ) + (xi 1) ]
xi [2.048, 2.048]
x = (1, 1, . . . , 1); fRos (x ) = 0
Pp
fRas (x) = 10p + i=1 (x2
10 cos(2xi ))
xi [5.12, 5.12]

x = (0, 0, . . . , 0); fRas (x ) = 0
p

Pp
fSch (x) = 418.9829 p + i=1 xi sin
|xi |
xi [512.03, 511.97]
x = (420.9687, . . . , 420.9687);
(x )
=0

q fSch
1 Pp
2
fAck (x) = 20 + e 20exp 0.2 p
i=1 xi
P

p
1
exp p
i=1 cos(2xi )
xi [30, 30]

x = (0, 0, . . . , 0); fAck (x ) = 0


Qp
Pp
x2
x


fGri (x) = 1 + i=1 4000
i=1 cos


xi [600, 600]
x (0, 0, . . . , 0); fGri (x ) = 0
Pp
(A Bi )2
le (x) =
i=1
Pp
(aij sinj + bij cosj )
Ai =
j=1
Pp
Bi =
j=1 (aij sinxj + bij cosxj )
xi , [, ]; aij , bij [100, 100]
x = ; le (x ) = 0


P
1 Pp
fLan (x) =
c exp
(x aij )2
Pi=1
j=1 j
p
cos j=1 (xj aij )2
xi [0, 10]; = p
x = random; fLan (x ) = random

Multimodal?


Separable?
yes

Regular?
n/a





n/a





n/a

yes

yes

n/a

yes

yes

n/a

yes



yes

yes



yes

yes





yes





Table 1: Definition function together features

dimensionality search space another important factor complexity
problem. study dimensionality problem features carried
Friedman (1994). order establish degree difficulty problems,
chosen search space dimensionality p = 30 functions.
Sphere function used development theory evolutionary strategies
(Rechenberg, 1973), evaluation genetic algorithms part test set
proposed De Jong (1975). Sphere, De Jongs function F1, simple strongly
convex function. Schwefels double sum function proposed Schwefel (1995). main
difficulty gradient oriented along axis due epistasis among
variables; way, algorithms use gradient converge slowly. Rosenbrock
function (Rosenbrock, 1960), De Jongs function F2, two dimensional function
deep valley shape parabola form x21 = x2 leads global
minimum. Due non-linearity valley, many algorithms converge slowly
change direction search repeatedly. extended version function
proposed Spedicato (1975). versions proposed (Oren, 1974; Dixon,
1974). considered many authors challenge optimization algorithm
(Schlierkamp-Voosen, 1994). difficulty mainly due non-linear interaction among
variables.
Rastrigin function (Rastrigin, 1974) constructed Sphere adding modulator
term cos(2xi ). contour made large number local minima whose value
increases distance global minimum. surface Schwefel function (Schwefel, 1981) composed great number peaks valleys. function second
10

fiCIXL2: Crossover Operator Evolutionary Algorithms

best minimum far global minimum many search algorithms trapped.
Moreover, global minimum near bounds domain.
Ackley, originally proposed Ackley (1987) generalized Back (1993),
exponential term covers surface numerous local minima. complexity
function moderated. algorithm uses gradient steepest descent
trapped local optima, search strategy analyzes wider region
able cross valley among optima achieve better results. order
obtain good results function, search strategy must combine exploratory
exploitative components efficiently. Griewangk function (Back, Fogel, & Michalewicz, 1997)
product term introduces interdependence among variables. aim
failure techniques optimize variable independently. Ackley function,
optima Griewangk function regularly distributed.
functions Fletcher-Powell (Fletcher & Powell, 1963) Langerman (Bersini,
Dorigo, Langerman, Seront, & Gambardella, 1996) highly multimodal, Ackley
Griewangk, non-symmetrical local optima randomly distributed.
way, objective function implicit symmetry advantages might simplify
optimization certain algorithms. Fletcher-Powel function achieves random distribution optima choosing values matrixes b, vector
random. used values provided Back (1996). Langerman function,
used values c referenced Eiben Back (1997b).

4. Evolutionary Algorithm
suitable evolutionary algorithms solve optimization problems continuous
domains evolutionary strategies (Schwefel, 1981; Rechenberg, 1973), genetic algorithms
(Holland, 1975; Goldberg, 1989a) real coding (Goldberg, 1991) evolutionary programming (Fogel, Owens, & Walsh, 1966; Fogel, 1995). evaluating CIXL2
chosen real coded genetic algorithms, search algorithms general purpose crossover operator plays central role. general structure genetic
algorithm shown Figure 4.
Nevertheless, CIXL2 could applied evolutionary algorithms crossover
similar operator. hand, real codification natural one
continuous domains, gene representing variable function. way,
precision solution depends data type used store variables.
objective comparison behavior proposed crossover
crossovers. comparison must made common evolutionary framework
defined features genetic algorithm. definition features,
taken account previous studies matter. following paragraphs
describe depth different components genetic algorithm.
4.1 Structure Individual Population Size
individual made p = 30 genes, dimensionality functions optimize.
size population one critical parameters many applications.
size population small, algorithm could converge quickly towards suboptimal solutions; large, much time resources could wasted.
11

fiOrtiz-Boyer, Hervas-Martnez, & Garca-Pedrajas

Genetic algorithm
begin
t0
initialize (t)
evaluate (t)
(not stop-criterion)
begin
tt+1
select (t) (t 1)
crossover (t)
mutate (t)
evaluate (t)
end
end
Figure 4: Structure genetic algorithm, current generation.
obvious size population, together selective pressure, influences
diversity population.
Several researches studied problems different points view. Grefenstette (1986) used meta-genetic algorithm controlling parameters another genetic
algorithm, population size selection method. Goldberg (1989b) made theoretical analysis optimum population size. study influence parameters
search process carried Schaffer, Caruana, Eshelman Das (1989).
Smith (1993) proposed algorithm adjusts size population respect
error probability selection . Another method consists changing size
population (Arabas, Michalewicz, & Mulawka, 1994) dynamically.
size population usually chosen interval 50 500 individuals,
depending difficulty problem. general practice, function optimization,
size interval [50, 100] unimodal functions, interval [100, 500]
multimodal functions. However, several papers use compromise size 100
functions order homogenize comparison environment. use population
size 100 individuals comparative studies (Zhang & Kim, 2000; Takahashi, Kita,
& Kobayashi, 1999).
4.2 Selection
Zhang Kim (2000) comparative study carried performance four
selection methods: proportional, ranking, tournament Genitor. contrast
studies based asymptotic study less ideal conditions,
paper devoted practical case, problem machine layout. paper analyzes
quality solutions obtained reasonable amount time using mutation
crossover operators. study concludes methods ranking tournament
selection obtain better results methods proportional Genitor selection.
12

fiCIXL2: Crossover Operator Evolutionary Algorithms

chosen binary tournament selection, ranking selection, used
Zhang Kim (2000) two reasons:
complexity tournament selection lower complexity ranking
selection (Back, 1996).
selective pressure higher. feature allows us measure whether
crossover able keep population diversity (Goldberg & Deb, 1991).
Tournament selection runs tournament two individuals selects winner.
order assure best individuals always survive next generation, use
elitism, best individual population generation always included
population generation + 1. proved, theoretically (Rudolph, 1994)
empirically (Back, 1996; Michalewicz, 1992; Zhang & Kim, 2000), convenience
use elitism.
4.3 Population Update Model
different techniques updating population, among important
generational model steady-state model. generational model
generation complete set N new offspring individuals created N parents selected
population. generational models, tournament selection used
choose two parent individuals, crossover pc probability mutation operator
con pm probability applied parents.
contrasts steady-state model, one member population
replaced time. steady-state model selects individual mutated
mutated individual replaces another individual population. crossover two
individuals selected one offspring replaces one individual population.
number different replacement strategies: replace-worst, replace randomly
chosen member, select replacement using negative fitness.
model extrapolates generational steady-state said
generation gap G (De Jong, 1975; Jong & Sarma, 1993). Thus generational model,
G = 1; steady-state model, G = 1/N . One widely used variants
steady-stated genetic algorithm Minimal Generation Gap (MGG) model (Satoh,
Yamamura, & Kobayashi, 1996). model takes two parents randomly population generates children. Two individuals selected parents
offspring: best individual, another individual chosen roulette selection.
two individuals substitute parents population.
generational model frequently used comparative studies use
BLX, SBX, logical crossover fuzzy recombination. reason
model used paper. However, UNDX crossover used MGG model,
UNDX MGG commonly used together generational model
negative influence performance UNDX.
parameters two models used commonly used
literature. generational model, use probability crossover p c = 0.6 (De
Jong, 1975; Herrera, Lozano, & Verdegay, 1998). MGG model used = 200,
13

fiOrtiz-Boyer, Hervas-Martnez, & Garca-Pedrajas

value commonly used papers UNDX (Ono & Kobayashi, 1997; Ono,
Kita, & Kobayashi, 1999; Ono, Kobayashi, & Yoshida, 2000). mutation probability,
values interval pm [0.001, 0.1] usual (De Jong, 1975; Herrera et al., 1998;
Michalewicz, 1992; Back, 1996). chosen value pm = 0.05 models.
4.4 Initialization
search algorithm, initialization method important. many cases
initialization determines success failure search process. opted,
papers (Herrera et al., 1998; De Jong, 1975; Beyer & Deb, 2001; Herrera, Lozano,
& Sanchez, 2003), initializing values genes means uniform random
distribution within domain variable.
4.5 Mutation
mutation operator chosen non-uniform mutation parameter b = 5
(Michalewicz, 1992) dynamical nature makes suitable wide variety
problems (Herrera & Lozano, 2000).
individuals generated mutation obtained follows:
im

=



+ 4(t, bi ) si = 0
4(t, ai ) si = 1

(12)


4(t, y) = y(1 r

(1 g


)b
max

)

(13)

generation, gmax maximum number generations, random value,
{0, 1}, r random number interval [0, 1] b parameter determines
degree dependence mutation regards number iterations. Equation
13 gives values interval [0, y]. probability obtaining value near 0 increases
algorithm progresses. operator performs uniform search initial stages
evolution, localized search final stages.
4.6 Stop Criterion
part genetic algorithm takes time evaluation
fitness function. number evaluations fitness generation depends
operators used population update model. Different operators update models
lead different numbers evaluations per generation. reason
common use number evaluations stop criterion instead number
generations. used limit 300,000 evaluations (Eiben, van der Hauw, & van
Hemert, 1998; De Jong & Kosters, 1998) stop criterion. precision solutions
bounded precision data type used implementation genetic
algorithm. used double precision data type 64 bits following specification
ANSI/IEEE STD 754-1985 (IEEE Standard Binary Floating-Point Arithmetic).
data type precision 15 - 17 digits.
14

fiCIXL2: Crossover Operator Evolutionary Algorithms

5. Analysis CIXL2
section perform analysis crossover, obtain every test
function following information:
1. optimal value confidence coefficient 1 confidence interval.
values used 1 = {0.70, 0.90, 0.95, 0.99}.
2. optimal number best individuals used crossover calculate confidence intervals mean. values used n = {5, 10, 30, 60, 90}.
two factors independent, perform analysis using
possible pairs (1 , n) Cartesian product two sets. pair
perform 30 runs genetic algorithm different random seeds. Table 2 shows
average value standard deviation 30 runs experiment.
study results made means analysis variance ANOVA
II (Dunn & Clark, 1974; Miller, 1981; Snedecor & Cochran, 1980), fitness
best individuals, A, test variable. fitness obtained independently 30 runs
depending two fixed factors interaction. fixed factors are: confidence
coefficient C four levels number best individuals B five levels.
linear model form:

Aij = + Ci + Bj + CBij + eij

(14)

= 1, 2, 3, 4; j = 1, 2, 3, 4, 5
where:
Ci effect i-th level factor C, C1 represents confidence
coefficient 0.70, C2 0.90, C3 0.95 C4 0.99.
Bj effect j-th level factor B, B1 represents value
n = 5, B2 n = 10, B3 n = 30, B4 n = 60 B5 n = 90.
CBij represents effect interaction confidence coefficient C
number best individuals B.
global mean model. variation experimental results
explained effects different levels factors model
interaction.
eij error variables.
hypothesis tests try determine effect term fitness best
individuals, A. carried tests every factor interaction among
factors. subsequent tests performed confidence level 95%.
coefficient R2 linear model tells us percentage variance explained
model.
15

fiOrtiz-Boyer, Hervas-Martnez, & Garca-Pedrajas

Function

n 1

Mean

St. Dev.

1

Mean

Mean

D. Tip.

1

Mean

St. Dev.

Sphere
fSph

5
10
30
60
90

0.70

6.365e-16
5.736e-15
3.728e-12
6.082e-10
3.838e-09

2.456e-16
2.495e-15
1.623e-12
2.499e-10
2.326e-09

0.90

4.885e-16
2.554e-15
1.446e-11
2.867e-08
4.383e-08

St. Dev. 1
1.969e-16
8.934e-16
7.062e-12
1.642e-08
3.068e-08

0.95

3.553e-16
2.642e-15
2.279e-11
1.557e-07
6.840e-08

1.710e-16
1.258e-15
1.256e-11
9.911e-08
5.894e-08

0.99

1.998e-16
1.480e-15
1.248e-10
5.494e-07
1.061e-07

6.775e-17
1.032e-15
5.914e-11
6.029e-07
8.401e-08

Schwefels
5
double sum 10
fSchDS
30
60
90

0.70

1.995e-03
2.232e-02
8.464e-02
1.376e-01
8.048e-01

2.280e-03
2.859e-02
1.168e-01
1.202e-01
5.403e-01

0.90

8.403e-03 7.748e-03
5.407e-02 3.792e-02
3.190e-01 2.798e-01
4.059e-01 2.395e-01
2.257e+00 1.490e+00

0.95

7.662e-03
4.168e-02
2.644e-01
2.223e-01
7.048e-01

9.693e-03
4.383e-02
2.569e-01
1.384e-01
7.689e-01

0.99

1.305e-02
1.462e-02
1.223e-01
2.134e-01
2.799e-01

1.303e-02
1.422e-02
9.018e-02
1.464e-01
2.322e-01

Rosenbrock 5
fRos
10
30
60
90

0.70

2.494e+01
2.579e+01
2.611e+01
2.576e+01
2.562e+01

1.283e+00
2.044e-01
1.471e-01
1.988e-01
2.827e-01

0.90

2.506e+01
2.591e+01
2.632e+01
2.593e+01
2.570e+01

3.050e-01
1.324e-01
1.745e-01
2.292e-01
2.974e-01

0.95

2.497e+01
2.589e+01
2.642e+01
2.600e+01
2.579e+01

4.663e-01
9.426e-02
1.377e-01
4.045e-01
2.629e-01

0.99

Rastrigin
fRas

5
10
30
60
90

0.70

2.919e+00 1.809e+00
6.799e+00 2.480e+00
9.452e+00 2.434e+00
1.413e+01 4.126e+00
1.771e+01 5.063e+00

0.90

6.036e+00
1.068e+01
1.270e+01
1.837e+01
2.438e+01

2.023e+00
3.786e+00
3.522e+00
6.070e+00
7.688e+00

0.95

7.893e+00
1.297e+01
1.327e+01
1.499e+01
1.987e+01

2.450e+00
3.844e+00
4.770e+00
4.434e+00
5.637e+00

0.99

7.164e+00
1.675e+01
1.552e+01
1.691e+01
2.249e+01

2.579e+00
6.554e+00
3.664e+00
4.123e+00
6.058e+00

Schwefel
fSch

5
10
30
60
90

0.70

6.410e+02 2.544e+02
1.793e+03 4.172e+02
2.675e+03 2.592e+02
2.700e+03 1.471e+02
2.738e+03 1.476e+02

0.90

1.145e+03
1.325e+03
2.264e+03
2.513e+03
2.704e+03

5.422e+02
2.340e+02
2.758e+02
1.927e+02
1.516e+02

0.95

1.424e+03
1.486e+03
2.061e+03
2.496e+03
2.672e+03

6.837e+02
2.607e+02
2.369e+02
2.146e+02
1.349e+02

0.99

2.844e+03
2.525e+03
1.986e+03
2.169e+03
2.529e+03

4.168e+02
3.069e+02
2.424e+02
2.434e+02
1.837e+02

Ackley
fAck

5
10
30
60
90

0.70

1.378e-08
2.074e-07
8.328e-06
1.019e-04
2.518e-04

5.677e-09
9.033e-08
1.403e-06
2.396e-05
7.167e-05

0.90

6.320e-09
9.544e-08
1.483e-05
8.292e-04
7.544e-04

2.966e-09
3.422e-08
3.956e-06
2.097e-04
2.668e-04

0.95

4.677e-09
9.396e-08
2.246e-05
1.897e-03
9.571e-02

1.960e-09
3.513e-08
4.957e-06
9.190e-04
3.609e-01

0.99

5.188e-09
5.806e-08
4.976e-05
3.204e-03
1.741e-01

2.883e-09
2.683e-08
1.298e-05
1.373e-03
5.290e-01

Griewangk
fGri

5
10
30
60
90

0.70

1.525e-02
1.647e-02
2.012e-02
7.884e-03
7.391e-03

1.387e-02
1.951e-02
2.372e-02
1.061e-02
7.617e-03

0.90

2.463e-02
2.695e-02
1.819e-02
2.808e-02
5.248e-03

2.570e-02
2.713e-02
1.664e-02
9.686e-02
6.741e-03

0.95

1.574e-02
2.195e-02
2.321e-02
7.410e-03
8.938e-03

1.411e-02
2.248e-02
3.842e-02
1.321e-02
1.196e-02

0.99

1.285e-02
3.194e-02
2.254e-02
1.582e-02
1.230e-02

1.801e-02
3.680e-02
1.877e-02
2.727e-02
2.356e-02

Fletcher
le

5
10
30
60
90

0.70

1.523e+04
1.966e+04
2.145e+04
2.133e+04
2.432e+04

1.506e+04
1.585e+04
1.631e+04
2.110e+04
2.273e+04

0.90

2.293e+04
2.248e+04
2.129e+04
2.124e+04
2.898e+04

1.882e+04
2.300e+04
1.310e+04
1.213e+04
3.131e+04

0.95

1.286e+04 1.317e+04
1.633e+04 1.344e+04
3.049e+04 2.306e+04
2.935e+04 2.155e+04
2.918e+04 2.418e+04

0.99

1.527e+04
1.891e+04
2.492e+04
2.374e+04
3.453e+04

1.362e+04
1.612e+04
1.967e+04
1.479e+04
2.498e+04

Langerman 5
fLan
10
30
60
90

0.70

-2.064e-01
-2.339e-01
-2.124e-01
-1.975e-01
-1.599e-01

9.346e-02
1.280e-01
1.038e-01
1.405e-01
9.057e-02

0.90

-2.544e-01
-2.582e-01
-2.191e-01
-1.752e-01
-1.336e-01

1.401e-01
1.574e-01
1.100e-01
7.145e-02
6.042e-02

0.95

-3.545e-01
-2.663e-01
-1.908e-01
-1.762e-01
-1.656e-01

0.99

-2.803e-01
-2.830e-01
-2.382e-01
-1.949e-01
-1.796e-01

1.350e-01
1.645e-01
1.572e-01
9.500e-02
8.453e-02

1.802e-01
1.247e-01
9.776e-02
8.929e-02
8.336e-02

2.463e+01 1.330e+00
2.579e+01
1.609e-01
2.668e+01
9.999e-02
2.617e+01
4.787e-01
2.585e+01
3.654e-01

Table 2: Average value standard deviation 30 runs experiment

16

fiCIXL2: Crossover Operator Evolutionary Algorithms

determining whether significant differences among various levels
factor, perform multiple comparison test average fitness obtained
different levels factor. First, carry Levene test (Miller, 1996; Levene,
1960) evaluating equality variances. hypothesis variances
equal accepted, perform Bonferroni test (Miller, 1996) ranking means
level factor. aim find level factor whose average fitness
significantly better average fitness rest levels factor.
test Levene results rejecting equality covariance matrixes, perform
Tamhane test (Tamhane & Dunlop, 2000) instead Bonferroni test. Tables 9, 12,
13 Appendix show results obtained following methodology.
Sphere function, significant levels term linear model Table 9
show none factors linear model significant effect model built
explain variance fitness A. effect due fact fSph easy
optimize fitness behaves singular random variable sample variance near
0. see Table 2 best results obtained pair (0.99, 5).
multiple comparison test Table 12 confirms means obtained value n = 5
significatively better means obtained values. way,
average fitness 1 = 0.70 significantly best one. results show that,
value n, best value 1 , general, 1 = 0.70. Due simple form
fSph , best parameters crossover show high exploitative component fast
shifting towards region best individuals.
unimodal non-separable functions fSchDS fRos , factors
interaction significant linear model explains sample variance
determination coefficient around 0.5. Table 2 shows best results obtained
n = 5; Tamhane test shows means obtained value n
significatively better means obtained values. results value
confidence coefficient less conclusive. fact, fRos significant
differences among different values 1 , although best results obtained
1 = 0.7. fSchDS average fitness 0.99 best one, without significant
differences 0.70 . 0.70 together n = 5 one shows best results.
conclude feature non-separability functions imply notable
change parameters crossover respect parameters used f Sph .
fRas fSch , separable multimodal, adequate pair
parameters (0.70, 5). fRas , test shows performance pair significantly better. However, fSch , best mean obtained 5 results
significantly better obtained values, exception 10 .
significant differences among 0.70 , 0.95 90 . three factors linear
model significant quite large determination coefficients 0.617 f Ras 0.805
forfSch . means factors interaction explain high percentage
variance fitness A.
fAck , best results obtained pair (0.95, 5). Tamhane test confirms
n = 5 suitable value, significant differences among 0.70 ,
0.95 0.99 . fGri best results obtained pair (0.90, 90). test
shows large values n suitable optimization function.
significant differences among performance different values 1 .
17

fiOrtiz-Boyer, Hervas-Martnez, & Garca-Pedrajas

functions determination coefficient linear model low, showing
linear model explain variance fitness. lack linear relation
among n, 1 fitness makes difficult determine best value
parameters crossover.
case le fLan similar, linear model hardly gives information
effect parameters fitness. adequate pair optimization two functions (0.95, 5). test shows best values n n = 5
n = 10. hand, significant differences among performance
crossover different values 1 .
overall results show selection best n = 5 individuals population
would suffice obtaining localization estimator good enough guide search process
even multimodal functions small value n could favor convergence
local optima. However, virtual parents worse fitness parent
population, offspring generated near latter, domain explored
multiple directions. way, premature convergence suboptimal virtual parents
avoided.
However, best n individuals concentrated local optimum algorithm
likely converge optimum. reason complex functions
larger value n may reasonable, adding confidence interval individuals located
near different optima. example this, case fGri best results
achieved n = 90 n = 60 noteworthy.
confidence coefficient bounds error determination localization
parameter responsible focussing search. multiple comparison tests show
value 1 = 0.70 best 6 problems, is, least, worse
best one problems. chosen adequate value
parameter.

6. Comparative Study Crossovers
Due large amount different crossovers available, unfeasible make comprehensive comparison crossovers CIXL2. chosen
crossovers obtain interesting results whose features similar crossover,
is, self-adaptive establish balance exploration exploitation search space. way two features balanced regulated
one parameters crossover. parameters chosen following
authors recommendations papers devoted comparison different
operators.
crossovers used comparison are: BLX (Eshelman & Schaffer, 1993)
different degrees exploration determined values = {0.2, 0.5} (Herrera et al.,
2003); fuzzy recombination (Voigt et al., 1995); based fuzzy connectives logical
family (logical crossover) (Herrera et al., 1998) using S2 strategies = 0.5 (Herrera &
Lozano, 2000), SBX (Deb & Agrawal, 1995) using values = {2, 5} (Deb & Beyer, 2001);
(Kita, Ono, & Kobayashi, 1998;
UNDX (Ono & Kobayashi, 1997) = 21 = 0.35
p
Kita, 2001). CIXL2, determined previous study, use n = 5
1 = 0.70.
18

fiCIXL2: Crossover Operator Evolutionary Algorithms

Following setup previous study, performed ANOVA II analysis
multiple comparison test. might expected, keeping mind no-free
lunch theorem diversity functions test set, tests show
crossover whose results significatively better results crossovers.
mean differences could exist certain kinds functions.
So, order determine kind function whether crossover better
others, performed ANOVA analysis factor crossover
operator multiple comparison test. Additionally, graphically study speed
convergence RCGA regard crossover operator. order enforce
clearness graphics crossover, show curve best performing
set parameters BLX SBX crossovers.
Crossover
CIXL2
BLX(0.3)
BLX(0.5)
SBX(2)
SBX(5)
Ext. F.
Logical
UNDX
CIXL2
BLX(0.3)
BLX(0.5)
SBX(2)
SBX(5)
Ext. F.
Logical
UNDX
CIXL2
BLX(0.3)
BLX(0.5)
SBX(2)
SBX(5)
Ext. F.
Logical
UNDX

Mean

St.Dev.
fSph
6.365e-16 2.456e-16
3.257e-16 1.396e-16
4.737e-16 4.737e-16
1.645e-12 8.874e-13
4.873e-12 3.053e-12
2.739e-15 1.880e-15
3.695e-13 1.670e-13
2.910e-05 1.473e-05
fRas
2.919e+00 1.809e+00
2.189e+00 1.417e+00
3.018e+00 1.683e+00
1.844e+01 4.417e+00
1.419e+01 3.704e+00
2.245e+01 4.914e+00
6.325e+01 1.012e+01
1.107e+02 1.242e+01
fGri
1.525e-02 1.387e-02
4.749e-02 4.579e-02
3.760e-02 2.874e-02
2.196e-02 1.874e-02
3.128e-02 2.737e-02
1.315e-03 3.470e-03
6.078e-03 6.457e-03
7.837e-02 4.438e-02

Mean
St.Dev.
fSchDS
1.995e-03 2.280e-03
1.783e-02 1.514e-02
9.332e-03 1.086e-02
2.033e-01 1.966e-01
3.933e-01 2.881e-01
3.968e+01 1.760e+01
1.099e+01 7.335e+00
2.080e+01 7.216e+00
fSch
6.410e+02 2.544e+02
3.695e+02 1.595e+02
4.200e+02 1.916e+02
1.470e+03 3.827e+02
1.104e+03 3.353e+02
3.049e+03 2.876e+02
2.629e+03 9.749e+01
8.050e+03 3.741e+02
fFle
1.523e+04 1.506e+04
1.570e+04 1.515e+04
1.802e+04 1.483e+04
3.263e+04 3.110e+04
3.333e+04 2.973e+04
1.691e+04 1.446e+04
2.718e+04 1.388e+04
3.469e+04 2.136e+04

Mean

St.Dev.
fRos
2.494e+01 1.283e+00
2.923e+01 1.723e+01
3.161e+01 2.094e+01
2.775e+01 9.178e+00
3.111e+01 1.971e+01
2.743e+01 1.394e+01
2.703e+01 8.358e-02
2.840e+01 3.606e-01
fAck
1.378e-08 5.677e-09
4.207e-08 1.713e-08
6.468e-08 1.928e-08
5.335e-06 1.453e-06
9.662e-06 2.377e-06
1.797e-07 5.823e-08
2.531e-06 7.129e-07
3.551e-02 1.224e-02
fLan
-2.064e-01 9.346e-02
-3.003e-01 1.388e-01
-3.457e-01 1.684e-01
-1.939e-01 1.086e-01
-1.866e-01 9.080e-02
-1.064e-01 5.517e-02
-7.396e-08 2.218e-07
-2.130e-01 9.116e-02

Table 3: Average values standard deviation 30 runs every crossover operator.

Table 3 shows average values standard deviations 30 runs performed
crossover operator. Table 10 Appendix shows how, functions, except
fRos , crossover operator significant effect linear model. table
shows results Levene test indicate inequality variances
results functions, excepting le . So, use Bonferroni test le ,
Tamhane test others. results multiple comparison test, ranking
established tests significant level differences among results
crossovers shown Tables 14, 15 16 (Appendix A). Figures 5 - 13, Appendix
B, show, logarithmic scale, convergence curves function.
19

fiOrtiz-Boyer, Hervas-Martnez, & Garca-Pedrajas

fSph high value determination coefficient shows linear model
explains much variance fitness. best values obtained BLX(0.3),
BLX(0.5) CIXL2, order. operators obtain precisions around
1e-16. Figure 5 shows CIXL2 fastest convergence, surpassed BLX
last generations.
fSchDS fRos best results obtained CIXL2. fSchDS difference
performance crossovers statistically significant. f Ros differences
significant, CIXL2 compared Logical UNDX. f SchDS Figure 6
shows CIXL2 achieves quasi-exponential convergence precise final result.
fRos , Figure 7 see speed convergence CIXL2 highest,
although profile crossovers similar fast initial convergence
followed poor evolution due high epistasis function. differences
overall process small. fact explains linear model influence
factor crossover significant determination coefficient small.
fRas , BLX(0.3) obtains best results without significant difference
average values obtained CIXL2 BLX(0.5). three operators obtain
best results fSch ; however, tests show significant differences
CIXL2 BLX(0.5), differences BLX(0.5) BLX(0.3).
latter obtains best results. Figures 8 9 show BLX best terms
convergence speed followed CIXL2. large value R 2 means crossover
significant influence evolutive process.
fAck , CIXL2 obtains significantly better results. Figure 10 see
converges faster. large value R2 means crossover significant
influence evolutive process. fGri , Fuzzy operator obtains significantly better
results. following ones, significant differences them, Logical
CIXL2. Figure 11 shows fast initial convergence CIXL2, end Logical
Fuzzy obtain better results.
le best results obtained CIXL2, difference significant
SBX UNDX. Figure 12 shows CIXL2 fastest convergence,
curve profile similar BLX Fuzzy. fLan , best operator BLX(0.5),
differences significant operators exception BLX(0.3).
UNDX CIXL2 together third place. Figure 13 shows behavior
crossovers similar, except Logical crossover converges value far
operators.

7. Comparison Estimation Distribution Algorithms
EDAs evolutionary algorithms use, CIXL2, best individuals population
direct search. comparison paradigm interesting, although
significant differences EDAs RCGAs.
EDAs remove operators crossover mutation. generation subset
population selected distribution individuals subset estimated.
individuals population next generation obtained sampling estimated
distribution. Although selection method could applied, common one
selection best individuals population.
20

fiCIXL2: Crossover Operator Evolutionary Algorithms

first EDAs developed discrete spaces. Later, adapted continuous domains. distinguish two types EDAs, whether take account
dependencies variables not. One used among EDAs
consider dependencies U DAc (Univariate Marginal Distribution Algorithm continuous domains) (Larranaga, Etxeberria, Lozano, & Pena, 2000). every generation
every variable U DAc carries statistical test order find density
function best fits variable. densities identified, estimation
parameters performed maximum likelihood estimates. distributions
normal, two parameters mean standard deviation. particular
case denoted U DAG
c (Univariate Marginal Distribution Algorithm Gaussian
models).
Among type EDAs, consider EGN ABGe (Estimation Gaussian
Network Algorithm) (Larranaga et al., 2000) whose good results function optimization
reported Bengoetxea Miquelez (2002). generation, EGN BGe learns
Gaussian network structure using Bayesian score gives value
Gaussian networks reflecting conditional dependencies used. Next, calculates
estimations parameters Gaussian network structure.
experiments used parameters reported Bengoetxea T. Miquelez
(2002): population 2000 individuals, initialized using uniform distribution,
subset best 1000 individuals selected estimate density function,
elitist approach chosen (the best individual included next population 1999
individuals simulated). algorithm run 30 times stop criterion
300,000 evaluations fitness function.
results EDAs compared results RCGA CIXL2 parameters
n = 5 1 = 0.70. performed ANOVA analysis three levels
factor different algorithms: RCGA CIXL2, U DAc EGN ABGe .
carried multiple comparison test.
Table 4 shows average values standard deviations 30 runs algorithm.
Table 11 Appendix shows how, functions excepting fAck , type algorithm significant effect linear model exist inequality variances
results (Levene test). So, used Tamhane test functions Bonferroni test fAck . Table 17 (Appendix A) shows results multiple comparison test
ranking established test.
fSph results similar. fitness behaves singular random variable
sample variance near 0 statistical tests feasible.
fSchDS results CIXL2 significantly better results U DAc
EGN ABGe . situation occurs fRos , fRas , fSch fAck , exception
four functions significant differences two EDAs.
fGri , EGN ABGe U DAc achieve best results, significantly better CIXL2.
le , U DAc significantly better EGN ABGe CIXL2,
differences two. fLan , CIXL2 obtains best results,
significant differences among three algorithms.
estimation distribution function best individuals population
performed EDAs advantage fSph , unimodal separable, fGri fAck
whose optima regularly distributed. results EDAs fGri better
21

fiOrtiz-Boyer, Hervas-Martnez, & Garca-Pedrajas

results CIXL2, results fAck worse. results fSph
algorithms similar. non-separable unimodal functions, f SchDS fRos ,
interdependence among variables favor performance EGN BGe
U DAc CIXL2. Nevertheless, CIXL2 achieves best results two functions.
multimodal separable functions, fRas fSch , difficult identify distribution
best individuals performance EDAs performance CIXL2.
extremely complex functions, le fLan , results less conclusive.
le best results obtained U DAc , differences
EGN ABGe CIXL2. fLan , CIXL2 achieves best results, differences
among three algorithms statistically significant.
EA
CIXL2
U DAc
EGN ABGe
CIXL2
U DAc
EGN ABGe
CIXL2
U DAc
EGN ABGe

Mean

St.Dev.
fSph
6.365e-16 2.456e-16
1.196e-16 1.713e-17
1.077e-16 1.001e-17
fRas
2.919e+00 1.809e+00
1.576e+02 7.382e+00
1.563e+02 8.525e+00
fGri
1.525e-02 1.387e-02
9.465e-16 1.207e-16
8.200e-16 1.149e-16

Mean
St.Dev.
fSchDS
1.995e-03 2.280e-03
2.221e+01 3.900e+00
2.096e-01 1.189e-01
fSch
6.410e+02 2.544e+02
1.153e+04 9.167e+01
1.155e+04 8.754e+01
fFle
1.523e+04 1.506e+04
5.423e+03 1.562e+03
9.069e+03 7.592e+03

Mean

St.Dev.
fRos
2.494e+01 1.283e+00
2.787e+01 2.278e-02
2.785e+01 1.629e-01
fAck
1.378e-08 5.677e-09
2.478e-08 1.831e-09
2.297e-08 2.095e-09
fLan
-2.064e-01 9.346e-02
-1.734e-01 4.258e-11
-1.734e-01 1.864e-11

Table 4: Average values standard deviation 30 runs three evolutionary algorithms: RCGA CIXL2 crossover, U DAc EGN ABGe .

8. Application Artificial Intelligence
Genetic algorithms applied almost kind problem, as, object recognition artificial vision (Singh, Chatterjee, & Chaudhury, 1997; Bebis, Louis, Varol, &
Yfantis, 2002), robotics path planing (Davidor, 1991; Sedighi, Ashenayi, Manikas, Wainwright, & Tai, 2004), parameter estimation (Johnson & Husbands, 1990; Ortiz-Boyer,
Hervas-Martnez, & Munoz-Perez, 2003), instance selection (Cano, Herrera, & Lozano,
2003; Kuncheva, 1995), reinforcement learning (Moriarty, Schultz, & Grefenstette, 1999),
neural network (Miller, Todd, & Hedge, 1991; Andersen & Tsoi, 1993; Bebis, Georgiopoulos, & Kasparis, 1997) ensemble design (Zhou, Wu, & Tang, 2002).
Real-coded genetic algorithms using CIXL2 applied problems
provided defined continuous domain. chosen application RCGAs
estimation weight network ensemble. interesting
problem standard methods encounter many difficulties.
8.1 Estimation Weights Networks Ensemble
Neural network ensembles (Perrone & Cooper, 1993) (Garca-Pedrajas, Hervas-Martnez,
& Ortiz-Boyer, 2005) receiving increasing attention recent neural network research,
due interesting features. powerful tool specially facing complex
22

fiCIXL2: Crossover Operator Evolutionary Algorithms

problems. Network ensembles made linear combination several networks
trained using data, although actual sample used network
learn different. network within ensemble potentially different weight
output ensemble. Several papers shown (Perrone & Cooper, 1993)
network ensemble generalization error generally smaller obtained
single network variance ensemble lesser variance
single network. output ensemble, y, input pattern x presented, is:
y(x) =

k
X

yi (x),

(15)

i=1

yi output network i, wi weight associated network.
networks one output, different weight usually assigned output.
ensembles neural networks advantages large networks without
problems long training time risk over-fitting.
Moreover, combination several networks cooperate solving given task
important advantages, (Liu, Yao, & Higuchi, 2000; Sharkey, 1996):
perform complex tasks subcomponents.
make overall system easier understand modify.
robust single network.
Techniques using multiple models usually consist two independent phases: model
generation model combination (Merz, 1999b). network trained
assigned weights (model generation), are, classification environment three basic
methods combining outputs networks (model combination):
1. Majority voting. pattern classified class majority networks places (Merz, 1999b). Majority voting effective, prone fail two
scenarios:
(a) subset redundant less accurate models comprise majority,
(b) dissenting vote recognized area specialization particular model.
2. Sum outputs networks. output ensemble sum
outputs individual networks.
3. Winner takes all. pattern assigned class highest output
outputs networks. is, network largest outputs directly
classify pattern, without taking account networks.
commonly used methods combining networks majority voting
sum outputs networks, weight vector measures
23

fiOrtiz-Boyer, Hervas-Martnez, & Garca-Pedrajas

confidence prediction network. problem obtaining weight vector
easy task. Usually, values weights constrained:
N
X

= 1,

(16)

i=1

order help produce estimators lower prediction error (Leblanc & Tibshirani,
1993), although justification constraint intuitive (Breiman, 1996).
method majority voting applied, vote network weighted
counted:
F (x) = arg maxy

X

.

(17)

i:fi (x)=y

problem finding optimal weight vector complex task. Basic
ensemble method (BEM), called Perrone Cooper (1993), consists weighting
networks equally. So, N networks, output ensembles is:
F (x) =

N
1 X
(x).
N

(18)

i=1

Perrone Cooper (1993) defined Generalized Ensemble Method, equivalent Mean Square Error - Optimal Linear Combination (MSE-OLC) without
constant term Hashem (Hashem, 1997). form output ensemble is:
fGEM (x)

N
X

(x),

(19)

i=1

i0 real satisfy constraint
by:

PN

i=1

1
j Cij
= P P 1 .
k
j Ckj

P

= 1. values given

(20)

Cij symmetric correlation matrix Cij E[mi (x)mj (x)], mk (x) defines
misfit function k, deviation true solution f (x), mk (x) f (x)
fk (x). previous methods commonly used. Nevertheless, many techniques
proposed last years. Among others, methods based
linear regression (Leblanc & Tibshirani, 1993), principal components analysis leastsquare regression (Merz, 1999a), correspondence analysis (Merz, 1999b), use
validation set (Opitz & Shavlik, 1996).
application, use genetic algorithm obtaining weight component. approach similar use gradient descent procedure (Kivinen &
Warmuth, 1997), avoiding problem trapped local minima. use
genetic algorithm additional advantage optimal linear combination,
former affected collinearity problem (Perrone & Cooper, 1993; Hashem, 1997).
24

fiCIXL2: Crossover Operator Evolutionary Algorithms

8.1.1 Experimental Setup
set available data divided two subsets: 75% patterns used
learning, remaining 25% testing generalization networks.
two exceptions, Sonar Vowel problems, patterns two problems
prearranged two specific subsets due particular features. summary
data sets shown Table 5. validation set used experiments.
Data set
Anneal
Autos
Balance
Breast-cancer
Card
German
Glass
Heart
Hepatitis
Horse
Ionosphere
Iris
Labor
Liver
Lymphography
Pima
Promoters
Segment
Sonar
Soybean
TicTacToe
Vehicle
Vote
Vowel
Zoo

Cases
Train Test
674
224
154
51
469
156
215
71
518
172
750
250
161
53
226
76
117
38
273
91
264
87
113
37
43
14
259
86
111
37
576
192
80
26
1733 577
104
104
513
170
719
239
635
211
327
108
528
462
76
25

Classes
5
6
3
2
2
2
6
2
2
3
2
3
2
2
4
2
2
7
2
19
2
4
2
11
7

Features
C B N
6 14 18
15 4
6
4



3
6
6
4
5
6
3 11
9


6
3
4
6 13
13 2
5
33 1

4


8
3
5
6



9
6
8

57
19

60

16 19


9
18

16
10

1 15

Inputs
59
72
4
15
51
61
9
22
19
58
34
4
29
2
38
8
114
19
60
82
9
18
16
10
16

Table 5: Summary data sets. features data set C(continuous),
B(binary) N(nominal). Inputs column shows number inputs
network depends number input variables
type.

data sets cover wide variety problems. problems different
numbers available patterns, 57 2310, different numbers classes, 2
19, different kinds inputs, nominal, binary continuous, different areas
25

fiOrtiz-Boyer, Hervas-Martnez, & Garca-Pedrajas

application, medical diagnosis vowel recognition. Testing model wide
variety problems give us clear idea performance. sets
method applied.
order test efficiency proposed crossover classical artificial intelligence
problem, used RCGA adjust weight network within ensemble.
method considers ensemble chromosome applies RCGA optimize
weight network. weight network ensemble codified
real number. chromosome formed way subject CIXL2 crossover nonuniform mutation. parameters CIXL2 used rest paper,
n = 5 1 = 0.7. combination method used weighted sum outputs
networks. Nevertheless, genetic algorithm could used weighting
network majority voting model used.
exact conditions experiments run algorithms
following:
ensemble formed 30 networks. network trained separately using
standard back-propagation algorithm using learning data.
30 networks trained, different methods obtaining
weights applied. So, methods use ensemble networks
run experiment. genetic algorithm, fitness individual
population classification accuracy learning set.
obtaining vector weights, generalization error method evaluated using testing data.
Tables 6 7 show results terms accurate classification 25 problems.
tables show results using RCGA CIXL2, standard BEM GEM
methods. order compare three methods performed sign test
win/draw/loss record three algorithms (Webb, 2000). tests shown Table
8.
Table 8 shows comparison statistics three models (Webb, 2000).
model show win/draw/loss statistic, first value number data sets
col < row, second number col = row, third
number col > row. second row shows p-value two-tailed sign test
win-loss record. table shows genetic algorithm using CIXL2 able
outperform two standard algorithms BEM GEM 10% confidence.
hand, significant differences BEM GEM. result
especially interesting used comprehensive set problems
different domains, different types inputs, different numbers classes.

9. Conclusions Future Work
paper proposed crossover operator allows offspring inherit
features common best individuals population. extraction common
features carried determination confidence intervals mean
26

fiCIXL2: Crossover Operator Evolutionary Algorithms

Problem
Anneal

Autos

Balance

Breast

Cancer

Card

German

Glass

Heart

Hepa.

Horse

Ionos.

Iris

CIXL2
BEM
GEM
CIXL2
BEM
GEM
CIXL2
BEM
GEM
CIXL2
BEM
GEM
CIXL2
BEM
GEM
CIXL2
BEM
GEM
CIXL2
BEM
GEM
CIXL2
BEM
GEM
CIXL2
BEM
GEM
CIXL2
BEM
GEM
CIXL2
BEM
GEM
CIXL2
BEM
GEM
CIXL2
BEM
GEM

Mean
0.9933
0.9879
0.9915
0.8957
0.8649
0.8740
0.9340
0.9179
0.9148
0.8575
0.8321
0.8274
0.9723
0.9678
0.9673
0.9201
0.9074
0.9049
0.8785
0.8587
0.8642
0.8509
0.8043
0.8246
0.9297
0.9089
0.9182
0.9385
0.9131
0.9179
0.8723
0.8444
0.8485
0.9635
0.9481
0.9554
1.0000
1.0000
1.0000

Learning
St.Dev.
Best
0.0046 0.9985
0.0054 0.9955
0.0054 0.9985
0.0233 0.9416
0.0211 0.9091
0.0262 0.9351
0.0067 0.9446
0.0068 0.9318
0.0101 0.9318
0.0195 0.8930
0.0287 0.8698
0.0314 0.8791
0.0021 0.9771
0.0034 0.9733
0.0034 0.9733
0.0087 0.9363
0.0088 0.9247
0.0093 0.9208
0.0080 0.8973
0.0090 0.8827
0.0099 0.8827
0.0225 0.9006
0.0246 0.8447
0.0293 0.8820
0.0216 0.9653
0.0214 0.9604
0.0239 0.9554
0.0224 0.9744
0.0253 0.9573
0.0289 0.9744
0.0174 0.9084
0.0194 0.8718
0.0207 0.8864
0.0164 0.9886
0.0171 0.9773
0.0205 0.9886
0.0000 1.0000
0.0000 1.0000
0.0000 1.0000

Worst
0.9777
0.9733
0.9777
0.8506
0.8312
0.8182
0.9232
0.9019
0.8785
0.8047
0.7395
0.7488
0.9676
0.9600
0.9581
0.9054
0.8880
0.8822
0.8653
0.8440
0.8427
0.8075
0.7578
0.7640
0.8861
0.8663
0.8663
0.8718
0.8462
0.8376
0.8315
0.7949
0.8095
0.9356
0.9167
0.9167
1.0000
1.0000
1.0000

Mean
0.9778
0.9729
0.9780
0.7261
0.7052
0.7033
0.9201
0.9158
0.9158
0.6892
0.6826
0.6817
0.9799
0.9793
0.9785
0.8574
0.8521
0.8533
0.7333
0.7355
0.7377
0.6962
0.6824
0.6855
0.8358
0.8333
0.8279
0.8702
0.8658
0.8711
0.7044
0.7000
0.7004
0.8950
0.8920
0.8958
1.0000
1.0000
1.0000

Test
St.Dev.
Best
0.0090 0.9911
0.0091 0.9911
0.0103 0.9911
0.0577 0.8235
0.0586 0.8039
0.0707 0.8039
0.0118 0.9487
0.0111 0.9423
0.0110 0.9359
0.0322 0.7465
0.0375 0.7606
0.0354 0.7324
0.0065 0.9885
0.0076 0.9943
0.0084 0.9885
0.0153 0.8895
0.0212 0.8953
0.0203 0.8953
0.0184 0.7640
0.0141 0.7600
0.0149 0.7680
0.0365 0.7736
0.0424 0.7925
0.0479 0.7736
0.0271 0.8971
0.0263 0.8824
0.0312 0.8971
0.0372 0.9211
0.0319 0.9211
0.0399 0.9474
0.0313 0.7692
0.0301 0.7582
0.0300 0.7802
0.0225 0.9195
0.0206 0.9195
0.0198 0.9310
0.0000 1.0000
0.0000 1.0000
0.0000 1.0000

Worst
0.9420
0.9464
0.9420
0.5882
0.5686
0.5294
0.8910
0.8910
0.8910
0.6338
0.6056
0.6056
0.9655
0.9655
0.9598
0.8256
0.7965
0.7965
0.7000
0.7040
0.7160
0.6038
0.6038
0.6038
0.7794
0.7794
0.7794
0.8158
0.8158
0.7895
0.6264
0.6374
0.6484
0.8276
0.8276
0.8621
1.0000
1.0000
1.0000

Table 6: Ensemble results using real-coded genetic algorithm (CIXL2), basic ensemble
method (BEM), generalized ensemble method (GEM). problem
marked whichever CIXL2 better (+), equal, (=), worse (-)
BEM/GEM.
27

+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
=
=

fiOrtiz-Boyer, Hervas-Martnez, & Garca-Pedrajas

Problem
Labor

Liver

Lymph

Pima

Promot.

Segment

Sonar

Soybean

TicTacToe

Vote

Vowel

Zoo

CIXL2
BEM
GEM
CIXL2
BEM
GEM
CIXL2
BEM
GEM
CIXL2
BEM
GEM
CIXL2
BEM
GEM
CIXL2
BEM
GEM
CIXL2
BEM
GEM
CIXL2
BEM
GEM
CIXL2
BEM
GEM
CIXL2
BEM
GEM
CIXL2
BEM
GEM
CIXL2
BEM
GEM

Mean
0.9651
0.9488
0.9527
0.8126
0.7799
0.7744
0.9456
0.9318
0.9306
0.7982
0.7782
0.7752
0.9496
0.9300
0.9263
0.9502
0.9339
0.9423
0.9074
0.8859
0.8907
0.9758
0.9602
0.9691
0.9913
0.9868
0.9876
0.9832
0.9793
0.9801
0.9146
0.8733
0.9157
0.9807
0.9671
0.9750

Learning
St.Dev.
Best
0.0257 1.0000
0.0283 0.9767
0.0270 0.9767
0.0175 0.8494
0.0176 0.8108
0.0198 0.8108
0.0208 0.9730
0.0242 0.9640
0.0254 0.9730
0.0073 0.8194
0.0079 0.7934
0.0089 0.7882
0.0304 1.0000
0.0357 0.9875
0.0319 0.9875
0.0030 0.9544
0.0042 0.9411
0.0044 0.9521
0.0236 0.9519
0.0266 0.9423
0.0277 0.9519
0.0114 0.9903
0.0130 0.9805
0.0157 0.9883
0.0027 0.9972
0.0020 0.9917
0.0024 0.9930
0.0055 0.9939
0.0060 0.9908
0.0062 0.9908
0.0148 0.9432
0.0179 0.9015
0.0129 0.9394
0.0175 1.0000
0.0215 1.0000
0.0203 1.0000

Worst
0.8837
0.8837
0.8837
0.7761
0.7336
0.7336
0.8919
0.8739
0.8559
0.7830
0.7535
0.7431
0.8875
0.8500
0.8625
0.9446
0.9256
0.9319
0.8654
0.8269
0.8365
0.9454
0.9240
0.9376
0.9847
0.9847
0.9847
0.9725
0.9664
0.9664
0.8845
0.8371
0.8845
0.9211
0.9079
0.9211

Mean
0.8857
0.8833
0.8833
0.6992
0.6950
0.6826
0.7847
0.7775
0.7784
0.7811
0.7885
0.7793
0.8244
0.8269
0.8218
0.9259
0.9183
0.9236
0.7849
0.7865
0.7853
0.9057
0.9039
0.9067
0.9794
0.9791
0.9792
0.9278
0.9284
0.9262
0.4925
0.4913
0.4973
0.9360
0.9307
0.9307

Test
St.Dev.
Best
0.0550 1.0000
0.0663 1.0000
0.0689 1.0000
0.0276 0.7442
0.0253 0.7442
0.0337 0.7442
0.0538 0.8649
0.0539 0.8649
0.0504 0.8378
0.0209 0.8177
0.0199 0.8177
0.0222 0.8281
0.0726 1.0000
0.0612 0.9231
0.0711 0.9615
0.0057 0.9376
0.0054 0.9341
0.0061 0.9359
0.0286 0.8462
0.0286 0.8365
0.0266 0.8462
0.0165 0.9353
0.0182 0.9353
0.0187 0.9353
0.0024 0.9874
0.0000 0.9791
0.0008 0.9833
0.0110 0.9537
0.0068 0.9444
0.0107 0.9444
0.0293 0.5606
0.0331 0.5584
0.0342 0.5541
0.0290 0.9600
0.0392 0.9600
0.0347 0.9600

Worst
0.7857
0.7143
0.7143
0.6512
0.6395
0.6047
0.6486
0.6486
0.6486
0.7292
0.7448
0.7292
0.7308
0.7308
0.6923
0.9151
0.9081
0.9116
0.7404
0.7212
0.7404
0.8706
0.8647
0.8706
0.9749
0.9791
0.9791
0.8889
0.9167
0.8981
0.4459
0.4264
0.4221
0.8800
0.8400
0.8400

Table 7: Ensemble results using real-coded genetic algorithm (CIXL2), basic ensemble
method (BEM), generalized ensemble method (GEM). problem
marked whichever CIXL2 better (+), equal, (=), worse (-)
BEM/GEM.

28

+
+
+
+
+
+

+

+
+
+


+

+
+

+
+

+
+

fiCIXL2: Crossover Operator Evolutionary Algorithms

Algorithm
CIXL2
BEM

BEM
19/1/5
0.0066

GEM
17/1/7
0.0639
9/4/12
0.6636

win/draw/loss
p-value
win/draw/loss
p-value

Table 8: Comparison three methods. Win/draw/loss record algorithms
p-value sign test.

best individuals population. confidence intervals, CIXL2 creates three
virtual parents used implement directed search towards region fittest
individuals. amplitude speed search determined number best
individuals selected confidence coefficient.
study carried order obtain best parameters CIXL2 concludes
value n = 5 best individuals suitable obtain localization estimator guide
search problems tested. However, difficult problems, would
advisable larger value n avoid premature convergence evolutionary
process. confident coefficient, 1 , responsible, together dispersion
best individuals, modulation wideness confidence interval centered
localization estimator. study results best value 1 = 0.70. pair
values acceptable performance problems, although optimum
pair values problems.
comparative analysis crossover operators shows CIXL2 good alternative widely used crossovers BLX unimodal function fSph , fSchDS ,
fRos . Noteworthy performance CIXL2 two non-separable functions,
fSchDS fRos , crossovers disparate behavior.
unimodal functions strategy extracting statistical features localization
dispersion best individuals guarantee good performance, case
multimodal functions quite different, performance algorithm assured
priori. Nevertheless, results obtained kind functions show CIXL2
always one best performing operators. instance, functions high complexity
fAck multimodal, non-separable regular le multimodal, nonseparable irregular CIXL2 obtains best results. behavior reveals
determination region best individuals means confidence intervals
provides robust methodology that, applied crossover operator, shows interesting
performance even difficult functions. summary, affirm paper
proves CIXL2 promising alternative bear mind, must choose
crossover use real-coded genetic algorithm.
EDAs shown good performance unimodal separable functions, f Sph ,
functions whose optima regularly distributed, fAck fGri . performance
EDAs decreases multimodal, fRas fSch , epistatic functions, fSchDS fRos .
hand, CIXL2 less sensitive type function. main reason
behavior may found fact CIXL2 uses distribution information obtained
best individuals population differently. CIXL2 creates three virtual parents
29

fiOrtiz-Boyer, Hervas-Martnez, & Garca-Pedrajas

distribution, virtual parents worse fitness individual
mates, offspring generated near virtual parents. way, CIXL2
prevents shifting population confidence interval improvement
performance significant.
applicability proposed crossover problem artificial neural network
ensembles shows model used solving standard artificial intelligence
problems. RCGAs CIXL2 used aspects ensemble design,
as, selection subset networks, sampling training set network.
promising results motivate beginning new line research geared
study distribution best individuals taking account kind problem
hand. aim propose new techniques selection individuals considered
obtaining confidence interval reliable way. multimodal, irregular,
many chaotically scattered optima functions difficulty obtaining distributions
best individuals enormous. kind functions would interesting
perform cluster analysis selected best individuals obtain confidence interval
every cluster. idea would allow implementation multi-directional crossover
towards different promising regions.
hand, likely evolutive process progresses distribution
best individuals changes. case, would advisable perform, regular
intervals, statistical tests determine distribution best reflects features
best individuals population.
Alternatively, considering construction non-parametric confidence intervals.
way, need robust estimators parameters localization dispersion
genes best individuals. performed preliminary studies using
median different measures dispersion results quite encouraging.
Another research line currently open study application CIXL2 problems optimization restrictions, especially presence non-linearity,
generation individuals feasible region big issue. orientation
search based identification region best individuals implemented
CIXL2 could favor generation feasible individuals. feature would
interesting advantage respect crossover operators.

Acknowledgments
authors would acknowledge R. Moya-Sanchez helping final version
paper.
work financed part project TIC2002-04036-C05-02 Spanish
Inter-Ministerial Commission Science Technology (CICYT) FEDER funds.

30

fiCIXL2: Crossover Operator Evolutionary Algorithms

Appendix A. Results Statistical Study
Function
fSph
fSchDS
fRos
fRas
fSch
fAck
fGri
le
fLan

C
1.000
0.000
0.005
0.000
0.000
0.095
0.149
0.410
0.040

B
1.000
0.000
0.000
0.000
0.000
0.000
0.001
0.000
0.000

CB

0.000
0.006
0.000
0.000
0.019


0.024

R2

0.601
0.526
0.617
0.805
0.083
0.040
0.054
0.159

T. Levene
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.003
0.000

Table 9: Significant levels, , term linear model, determination coefficient
R2 , value Levene test statistical analysis CIXL2 parameters.

Function
fSph
fSchDS
fRos
fRas
fSch
fAck
fGri
le
fLan

Crossover
0.000
0.000
0.573
0.000
0.000
0.000
0.000
0.000
0.000

R2
0.779
0.786
0.024
0.971
0.987
0.884
0.421
0.137
0.486

Levene test
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.091
0.000

Table 10: Significance level crossover operator determination coefficient R 2
linear model, value Levene test comparative study crossovers.

Function
fSchDS
fRos
fRas
fSch
fAck
fGri
le
fLan

EA
0.000
0.000
0.000
0.000
1.000
0.000
0.001
0.027

R2
0.955
0.778
0.992
0.999
0.641
0.455
0.150
0.079

Levene test
0.000
0.000
0.000
0.000
1.000
0.000
0.000
0.000

Table 11: Significance level evolutionary algorithms determination coefficient R 2
linear model, value Levene test comparative study betwen
CIXL2 EDAs.

31

fiOrtiz-Boyer, Hervas-Martnez, & Garca-Pedrajas



J

5

10
30
60
90
10
5
30
60
90
30
5
10
60
90
60
5
10
30
90
90
5
10
30
60
Ranking

5

10
30
60
90
10
5
30
60
90
30
5
10
60
90
60
5
10
30
90
90
5
10
30
60
Ranking

5

10
30
60
90
10
5
30
60
90
30
5
10
60
90
60
5
10
30
90
90
5
10
30
60
Ranking



J
fSph
-2.683e-15
-4.144e-11
-1.836e-07
-5.554e-08
2.683e-15
-4.144e-11
-1.836e-07
-5.554e-08
4.144e-11
4.144e-11
-1.835e-07
-5.549e-08
1.836e-07
1.836e-07
1.835e-07
1.281e-07
5.554e-08
5.554e-08
5.549e-08
-1.281e-07
60 > 90 > 30
fRas
-5.79e+00
-6.72e+00
-1.01e+01
-1.51e+01
5.79e+00
-9.31e-01
-4.30e+00
-9.32e+00
6.72e+00
9.31e-01
-3.37e+00
-8.39e+00
1.01e+01
4.30e+00
3.37e+00
-5.02e+00
1.51e+01
9.32e+00
8.39e+00
5.02e+00
90 > 60 >
30 >

0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.003
0.000
0.000
0.000
0.003
> 10 > 5

0.000
0.000
0.000
0.000
0.000
0.807
0.000
0.000
0.000
0.807
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
10 > 5
5

fGri
-7.207E-03
0.174
-3.896E-03
0.864
2.329E-03
1.000
8.649E-03
0.001
7.207E-03
0.174
3.311E-03
0.983
9.535E-03
0.533
1.586E-02
0.000
3.896E-03
0.864
-3.311E-03
0.983
6.225E-03
0.930
1.254E-02
0.000
-2.329E-03
1.000
-9.535E-03
0.533
-6.225E-03
0.930
6.320E-03
0.884
-8.649E-03
0.001
-1.586E-02
0.000
-1.254E-02
0.000
-6.320E-03
0.884
60 90
5 > 90
10 > 90
30 > 90

J



fSchDS
-2.540e-02
0.000
-1.899e-01
0.000
-2.371e-01
0.000
-1.004e+00
0.000
2.540e-02
0.000
-1.645e-01
0.000
-2.117e-01
0.000
-9.785e-01
0.000
1.899e-01
0.000
1.645e-01
0.000
-4.720e-02
0.572
-8.140e-01
0.000
2.371e-01
0.000
2.117e-01
0.000
4.720e-02
0.572
-7.668e-01
0.000
1.004e+00
0.000
9.785e-01
0.000
8.140e-01
0.000
7.668e-01
0.000
90 > 30 > 10 > 5
60 > 5
fSch
-2.691e+02
0.082
-7.338e+02
0.000
-9.559e+02
0.000
-1.148e+03
0.000
2.691e+02
0.082
-4.647e+02
0.000
-6.868e+02
0.000
-8.786e+02
0.000
7.338e+02
0.000
4.647e+02
0.000
-2.221e+02
0.000
-4.139e+02
0.000
9.559e+02
0.000
6.868e+02
0.000
2.221e+02
0.000
-1.918e+02
0.000
1.148e+03
0.000
8.786e+02
0.000
4.139e+02
0.000
1.918e+02
0.000
90 > 60 > 30 > 5
10 5
fFle
-2.776e+03
0.885
-7.968e+03
0.004
-7.342e+03
0.008
-1.268e+04
0.000
2.776e+03
0.885
-5.192e+03
0.234
-4.566e+03
0.378
-9.899e+03
0.006
7.968e+03
0.004
5.192e+03
0.234
6.254e+02
1.000
-4.707e+03
0.678
7.342e+03
0.008
4.566e+03
0.378
-6.254e+02
1.000
-5.333e+03
0.491
1.268e+04
0.000
9.899e+03
0.006
4.707e+03
0.678
5.333e+03
0.491
10 5
30 > 5
60 > 5
90 > 5

J
fRos
-9.433e-01
-1.486e+00
-1.058e+00
-8.375e-01
9.433e-01
-5.425e-01
-1.142e-01
1.058e-01
1.486e+00
5.425e-01
4.283e-01
6.483e-01
1.058e+00
1.142e-01
-4.283e-01
2.200e-01
8.375e-01
-1.058e-01
-6.483e-01
-2.200e-01
30 > 60 > 10


0.000
0.000
0.000
0.000
0.000
0.000
0.025
0.014
0.000
0.000
0.000
0.000
0.000
0.025
0.000
0.000
0.000
0.014
0.000
0.000
> 90 > 5

fAck
-1.063e-07
0.000
-2.384e-05
0.000
-1.508e-03
0.000
-6.769e-02
0.216
1.063e-07
0.000
-2.373e-05
0.000
-1.508e-03
0.000
-6.769e-02
0.216
2.384e-05
0.000
2.373e-05
0.000
-1.484e-03
0.000
-6.767e-02
0.216
1.508e-03
0.000
1.508e-03
0.000
1.484e-03
0.000
-6.619e-02
0.242
6.769e-02
0.216
6.769e-02
0.216
6.767e-02
0.216
6.619e-02
0.242
60 > 30 > 10 > 5
90 5
fLan
-1.354e-02
0.998
-5.881e-02
0.009
-8.794e-02
0.000
-1.142e-01
0.000
1.354e-02
0.998
-4.527e-02
0.082
-7.440e-02
0.000
-1.007e-01
0.000
5.881e-02
0.009
4.527e-02
0.082
-2.913e-02
0.354
-5.540e-02
0.000
8.794e-02
0.000
7.440e-02
0.000
2.913e-02
0.354
-2.627e-02
0.247
1.142e-01
0.000
1.007e-01
0.000
5.540e-02
0.000
2.627e-02
0.247
10 5
30 > 5
60 > 5
90 > 5

Table 12: Results functions multiple comparison test ranking
obtained depending number best individuals n.

32

fiCIXL2: Crossover Operator Evolutionary Algorithms



J

0.70

0.90
0.95
0.99
0.90
0.70
0.95
0.99
0.95
0.70
0.90
0.99
0.99
0.70
0.90
0.95
Ranking

J
fSph
-1.361e-08
-4.394e-08
-1.302e-07
1.361e-08
-3.033e-08
-1.166e-07
4.394e-08
3.033e-08
-8.628e-08
1.302e-07
1.166e-07
8.628e-08


0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.019
0.000
0.000
0.019

0.99 > 0.95 > 0.90 > 0.70

J
fSchDS
-3.985e-01
-3.783e-02
8.165e-02
3.985e-01
3.607e-01
4.802e-01
3.783e-02
-3.607e-01
1.195e-01
-8.165e-02
-4.802e-01
-1.195e-01


0.000
0.967
0.114
0.000
0.001
0.000
0.967
0.001
0.013
0.114
0.000
0.013

0.90 > 0.95 > 0.99

J
fRos
-1.360e-01
-1.693e-01
-1.813e-01
1.360e-01
-3.333e-02
-4.533e-02
1.693e-01
3.333e-02
-1.200e-02
1.813e-01
4.533e-02
1.200e-02


0.281
0.131
0.310
0.281
0.995
0.996
0.131
0.995
1.000
0.310
0.996
1.000

0.99 0.95 0.90 0.70

0.70 0.99

0.70

0.90
0.95
0.99
0.90
0.70
0.95
0.99
0.95
0.70
0.90
0.99
0.99
0.70
0.90
0.95
Ranking

0.70

0.90
0.95
0.99
0.90
0.70
0.95
0.99
0.95
0.70
0.90
0.99
0.99
0.70
0.90
0.95
Ranking

fRas
-4.23e+00
-3.59e+00
-5.56e+00
4.23e+00
6.40e-01
-1.33e+00
3.59e+00
-6.40e-01
-1.97e+00
5.56e+00
1.33e+00
1.97e+00

0.000
0.000
0.000
0.000
0.966
0.551
0.000
0.966
0.044
0.000
0.551
0.044

0.99 > 0.95 > 0.70
0.90 > 0.70

fGri
-7.196E-03
-2.027E-03
-5.667E-03
7.196E-03
5.168E-03
1.529E-03
2.027E-03
-5.168E-03
-3.640E-03
5.667E-03
-1.529E-03
3.640E-03

0.395
0.945
0.155
0.395
0.791
1.000
0.945
0.791
0.747
0.155
1.000
0.747

0.90 0.99 0.95 0.70

fSch
1.198e+02
8.247e+01
-3.008e+02
-1.198e+02
-3.736e+01
-4.206e+02
-8.247e+01
3.736e+01
-3.833e+02
3.008e+02
4.206e+02
3.833e+02

0.714
0.919
0.001
0.714
0.997
0.000
0.919
0.997
0.000
0.001
0.000
0.000

0.70 0.95 0.90
0.99 > 0.90

fFle
-2.986e+03
-3.241e+03
-3.079e+03
2.986e+03
-2.547e+02
-9.255e+01
3.241e+03
2.547e+02
1.622e+02
3.079e+03
9.255e+01
-1.622e+02

0.717
0.635
0.644
0.717
1.000
1.000
0.635
1.000
1.000
0.644
1.000
1.000

0.95 0.99 0.90 0.70

fAck
-2.471e-04
-1.944e-02
-3.541e-02
2.471e-04
-1.919e-02
-3.516e-02
1.944e-02
1.919e-02
-1.597e-02
3.541e-02
3.516e-02
1.597e-02

0.000
0.617
0.382
0.000
0.631
0.390
0.617
0.631
0.985
0.382
0.390
0.985

0.99 0.95 0.70
0.90 > 0.70

fLan
6.105e-03
2.867e-02
3.309e-02
-6.105e-03
2.257e-02
2.698e-02
-2.867e-02
-2.257e-02
4.415e-03
-3.309e-02
-2.698e-02
-4.415e-03

0.998
0.272
0.133
0.998
0.585
0.363
0.272
0.585
1.000
0.133
0.363
1.000

0.70 0.90 0.95 0.99

Table 13: Results functions multiple comparison test ranking
obtained depending confidence coefficient 1 .

33

fiOrtiz-Boyer, Hervas-Martnez, & Garca-Pedrajas



Crossover
J

CIXL2

BLX(0.3)

BLX(0.5)

SBX(2)

SBX(5)

Fuzzy

Logical

UNDX

Function
fSph
fSchDS
fRos

BLX(0.3)
BLX(0.5)
SBX(2)
SBX(5)
Fuzzy
Logical
UNDX
CIXL2
BLX(0.5)
SBX(2)
SBX(5)
Fuzzy
Logical
UNDX
CIXL2
BLX(0.3)
SBX(2)
SBX(5)
Fuzzy
Logical
UNDX
CIXL2
BLX(0.3)
BLX(0.5)
SBX(5)
Fuzzy
Logical
UNDX
CIXL2
BLX(0.3)
BLX(0.5)
SBX(2)
Fuzzy
Logical
UNDX
CIXL2
BLX(0.3)
BLX(0.5)
SBX(2)
SBX(5)
Logical
UNDX
CIXL2
BLX(0.3)
BLX(0.5)
SBX(2)
SBX(5)
Fuzzy
UNDX
CIXL2
BLX(0.3)
BLX(0.5)
SBX(2)
SBX(5)
Fuzzy
Logical

fSph
J
3.109e-16
1.628e-16
-1.644e-12
-4.873e-12
-2.102e-15
-3.689e-13
-2.910e-05
-3.109e-16
-1.480e-16
-1.644e-12
-4.873e-12
-2.413e-15
-3.692e-13
-2.910e-05
-1.628e-16
1.480e-16
-1.644e-12
-4.873e-12
-2.265e-15
-3.690e-13
-2.910e-05
1.644e-12
1.644e-12
1.644e-12
-3.229e-12
1.642e-12
1.275e-12
-2.910e-05
4.873e-12
4.873e-12
4.873e-12
3.229e-12
4.871e-12
4.504e-12
-2.910e-05
2.102e-15
2.413e-15
2.265e-15
-1.642e-12
-4.871e-12
-3.668e-13
-2.910e-05
3.689e-13
3.692e-13
3.690e-13
-1.275e-12
-4.504e-12
3.668e-13
-2.910e-05
2.910e-05
2.910e-05
2.910e-05
2.910e-05
2.910e-05
2.910e-05
2.910e-05

fSchDS
J





fRos
J



0.000
0.212
0.000
0.000
0.000
0.000
0.000
0.000
0.074
0.000
0.000
0.000
0.000
0.000
0.212
0.074
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000

-1.583e-02
0.000
-4.283e+00
0.997
-7.337e-03
0.028
-6.667e+00
0.933
-2.014e-01
0.000
-2.809e+00
0.958
-3.913e-01
0.000
-6.165e+00
0.944
-3.968e+01
0.000
-2.487e+00
1.000
-1.098e+01
0.000
-2.092e+00
0.000
-2.080e+01
0.000
-3.460e+00
0.000
1.583e-02
0.000
4.283e+00
0.997
8.495e-03
0.357
-2.384e+00
1.000
-1.855e-01
0.000
1.473e+00
1.000
-3.755e-01
0.000
-1.882e+00
1.000
-3.966e+01
0.000
1.796e+00
1.000
-1.097e+01
0.000
2.191e+00
1.000
-2.078e+01
0.000
8.225e-01
1.000
7.337e-03
0.028
6.667e+00
0.933
-8.495e-03
0.357
2.384e+00
1.000
-1.940e-01
0.000
3.857e+00
1.000
-3.840e-01
0.000
5.019e-01
1.000
-3.967e+01
0.000
4.179e+00
1.000
-1.098e+01
0.000
4.575e+00
1.000
-2.079e+01
0.000
3.206e+00
1.000
2.014e-01
0.000
2.809e+00
0.958
1.855e-01
0.000
-1.473e+00
1.000
1.940e-01
0.000
-3.857e+00
1.000
-1.900e-01
0.115
-3.355e+00
1.000
-3.948e+01
0.000
3.222e-01
1.000
-1.078e+01
0.000
7.179e-01
1.000
-2.060e+01
0.000
-6.508e-01
1.000
3.913e-01
0.000
6.165e+00
0.944
3.755e-01
0.000
1.882e+00
1.000
3.840e-01
0.000
-5.019e-01
1.000
1.900e-01
0.115
3.355e+00
1.000
-3.929e+01
0.000
3.678e+00
1.000
-1.059e+01
0.000
4.073e+00
1.000
-2.041e+01
0.000
2.705e+00
1.000
3.968e+01
0.000
2.487e+00
1.000
3.966e+01
0.000
-1.796e+00
1.000
3.967e+01
0.000
-4.179e+00
1.000
3.948e+01
0.000
-3.222e-01
1.000
3.929e+01
0.000
-3.678e+00
1.000
2.870e+01
0.000
3.957e-01
1.000
1.888e+01
0.000
-9.730e-01
1.000
1.098e+01
0.000
2.092e+00
0.000
1.097e+01
0.000
-2.191e+00
1.000
1.098e+01
0.000
-4.575e+00
1.000
1.078e+01
0.000
-7.179e-01
1.000
1.059e+01
0.000
-4.073e+00
1.000
-2.870e+01
0.000
-3.957e-01
1.000
-9.812e+00
0.000
-1.369e+00
0.000
2.080e+01
0.000
3.460e+00
0.000
2.078e+01
0.000
-8.225e-01
1.000
2.079e+01
0.000
-3.206e+00
1.000
2.060e+01
0.000
6.508e-01
1.000
2.041e+01
0.000
-2.705e+00
1.000
-1.888e+01
0.000
9.730e-01
1.000
9.812e+00
0.000
1.369e+00
0.000
Ranking
U N DX > SBX(5) > SBX(2) > Logical > Ext.F. > CIXL2 BLX(0.5) BLX(0.3)
Ext.F. > U N DX > Logical > SBX(5) SBX(2) > BLX(0.3) BLX(0.5) > CIXL2
BLX(0.5) SBX(5) BLX(0.3) U N DX SBX(2) Ext.F. Logical > CIXL2

Table 14: Results multiple comparison tests fSph , fSchDS fRos functions
ranking established test regarding crossover operator.

34

fiCIXL2: Crossover Operator Evolutionary Algorithms



Crossover
J

CIXL2

BLX(0.3)

BLX(0.5)

SBX(2)

SBX(5)

Fuzzy

Logical

UNDX

Function
fRas
fSch
fAck

BLX(0.3)
BLX(0.5)
SBX(2)
SBX(5)
Fuzzy
Logical
UNDX
CIXL2
BLX(0.5)
SBX(2)
SBX(5)
Fuzzy
Logical
UNDX
CIXL2
BLX(0.3)
SBX(2)
SBX(5)
Fuzzy
Logical
UNDX
CIXL2
BLX(0.3)
BLX(0.5)
SBX(5)
Fuzzy
Logical
UNDX
CIXL2
BLX(0.3)
BLX(0.5)
SBX(2)
Fuzzy
Logical
UNDX
CIXL2
BLX(0.3)
BLX(0.5)
SBX(2)
SBX(5)
Logical
UNDX
CIXL2
BLX(0.3)
BLX(0.5)
SBX(2)
SBX(5)
Fuzzy
UNDX
CIXL2
BLX(0.3)
BLX(0.5)
SBX(2)
SBX(5)
Fuzzy
Logical

fRas
J
7.296e-01
-9.950e-02
-1.552e+01
-1.128e+01
-1.953e+01
-6.033e+01
-1.078e+02
-7.296e-01
-8.291e-01
-1.625e+01
-1.201e+01
-2.026e+01
-6.106e+01
-1.085e+02
9.950e-02
8.291e-01
-1.542e+01
-1.118e+01
-1.943e+01
-6.023e+01
-1.077e+02
1.552e+01
1.625e+01
1.542e+01
4.245e+00
-4.013e+00
-4.481e+01
-9.227e+01
1.128e+01
1.201e+01
1.118e+01
-4.245e+00
-8.258e+00
-4.905e+01
-9.651e+01
1.953e+01
2.026e+01
1.943e+01
4.013e+00
8.258e+00
-4.079e+01
-8.826e+01
6.033e+01
6.106e+01
6.023e+01
4.481e+01
4.905e+01
4.079e+01
-4.746e+01
1.078e+02
1.085e+02
1.077e+02
9.227e+01
9.651e+01
8.826e+01
4.746e+01

fSch
J





fAck
J



0.923
1.000
0.000
0.000
0.000
0.000
0.000
0.923
0.713
0.000
0.000
0.000
0.000
0.000
1.000
0.713
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.005
0.042
0.000
0.000
0.000
0.000
0.000
0.005
0.000
0.000
0.000
0.000
0.000
0.000
0.042
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000

2.715e+02
0.000
-2.830e-08
0.000
2.210e+02
0.010
-5.090e-08
0.000
-8.287e+02
0.000
-5.322e-06
0.000
-4.631e+02
0.000
-9.649e-06
0.000
-2.408e+03
0.000
-1.659e-07
0.000
-1.988e+03
0.000
-2.517e-06
0.000
-7.409e+03
0.000
-3.550e-02
0.000
-2.715e+02
0.000
2.830e-08
0.000
-5.050e+01
1.000
-2.261e-08
0.000
-1.100e+03
0.000
-5.293e-06
0.000
-7.346e+02
0.000
-9.620e-06
0.000
-2.680e+03
0.000
-1.376e-07
0.000
-2.260e+03
0.000
-2.488e-06
0.000
-7.680e+03
0.000
-3.550e-02
0.000
-2.210e+02
0.010
5.090e-08
0.000
5.050e+01
1.000
2.261e-08
0.000
-1.050e+03
0.000
-5.271e-06
0.000
-6.841e+02
0.000
-9.598e-06
0.000
-2.629e+03
0.000
-1.150e-07
0.000
-2.209e+03
0.000
-2.466e-06
0.000
-7.630e+03
0.000
-3.550e-02
0.000
8.287e+02
0.000
5.322e-06
0.000
1.100e+03
0.000
5.293e-06
0.000
1.050e+03
0.000
5.271e-06
0.000
3.655e+02
0.006
-4.327e-06
0.000
-1.579e+03
0.000
5.156e-06
0.000
-1.159e+03
0.000
2.805e-06
0.000
-6.580e+03
0.000
-3.550e-02
0.000
4.631e+02
0.000
9.649e-06
0.000
7.346e+02
0.000
9.620e-06
0.000
6.841e+02
0.000
9.598e-06
0.000
-3.655e+02
0.006
4.327e-06
0.000
-1.945e+03
0.000
9.483e-06
0.000
-1.525e+03
0.000
7.132e-06
0.000
-6.946e+03
0.000
-3.550e-02
0.000
2.408e+03
0.000
1.659e-07
0.000
2.680e+03
0.000
1.376e-07
0.000
2.629e+03
0.000
1.150e-07
0.000
1.579e+03
0.000
-5.156e-06
0.000
1.945e+03
0.000
-9.483e-06
0.000
4.199e+02
0.000
-2.351e-06
0.000
-5.001e+03
0.000
-3.550e-02
0.000
1.988e+03
0.000
2.517e-06
0.000
2.260e+03
0.000
2.488e-06
0.000
2.209e+03
0.000
2.466e-06
0.000
1.159e+03
0.000
-2.805e-06
0.000
1.525e+03
0.000
-7.132e-06
0.000
-4.199e+02
0.000
2.351e-06
0.000
-5.421e+03
0.000
-3.550e-02
0.000
7.409e+03
0.000
3.550e-02
0.000
7.680e+03
0.000
3.550e-02
0.000
7.630e+03
0.000
3.550e-02
0.000
6.580e+03
0.000
3.550e-02
0.000
6.946e+03
0.000
3.550e-02
0.000
5.001e+03
0.000
3.550e-02
0.000
5.421e+03
0.000
3.550e-02
0.000
Ranking
U N DX > Logical > Ext.F. > SBX(2) > SBX(5) > BLX(0.5) CIXL2 BLX(0.3)
U N DX > Ext.F. > Logical > SBX(2) > SBX(5) > CIXL2 > BLX(0.5) BLX(0.3)
U N DX > SBX(5) > SBX(2) > Logical > Ext.F. > BLX(0.5) > BLX(0.3) > CIXL2

Table 15: Results multiple comparison tests fRas , fSch fAck functions
ranking established test regarding crossover operator.

35

fiOrtiz-Boyer, Hervas-Martnez, & Garca-Pedrajas



Crossover
J

CIXL2

BLX(0.3)

BLX(0.5)

SBX(2)

SBX(5)

Fuzzy

Logical

UNDX

Function
fGri
le
fLan

BLX(0.3)
BLX(0.5)
SBX(2)
SBX(5)
Fuzzy
Logical
UNDX
CIXL2
BLX(0.5)
SBX(2)
SBX(5)
Fuzzy
Logical
UNDX
CIXL2
BLX(0.3)
SBX(2)
SBX(5)
Fuzzy
Logical
UNDX
CIXL2
BLX(0.3)
BLX(0.5)
SBX(5)
Fuzzy
Logical
UNDX
CIXL2
BLX(0.3)
BLX(0.5)
SBX(2)
Fuzzy
Logical
UNDX
CIXL2
BLX(0.3)
BLX(0.5)
SBX(2)
SBX(5)
Logical
UNDX
CIXL2
BLX(0.3)
BLX(0.5)
SBX(2)
SBX(5)
Fuzzy
UNDX
CIXL2
BLX(0.3)
BLX(0.5)
SBX(2)
SBX(5)
Fuzzy
Logical

fGri
J
-3.224e-02
-2.235e-02
-6.710e-03
-1.603e-02
1.394e-02
9.173e-03
-6.312e-02
3.224e-02
9.893e-03
2.553e-02
1.621e-02
4.618e-02
4.142e-02
-3.088e-02
2.235e-02
-9.893e-03
1.564e-02
6.320e-03
3.629e-02
3.152e-02
-4.077e-02
6.710e-03
-2.553e-02
-1.564e-02
-9.320e-03
2.065e-02
1.588e-02
-5.641e-02
1.603e-02
-1.621e-02
-6.320e-03
9.320e-03
2.997e-02
2.520e-02
-4.709e-02
-1.394e-02
-4.618e-02
-3.629e-02
-2.065e-02
-2.997e-02
-4.763e-03
-7.706e-02
-9.173e-03
-4.142e-02
-3.152e-02
-1.588e-02
-2.520e-02
4.763e-03
-7.229e-02
6.312e-02
3.088e-02
4.077e-02
5.641e-02
4.709e-02
7.706e-02
7.229e-02

fFle
J





fLan
J



0.021
0.012
0.973
0.167
0.000
0.057
0.000
0.021
1.000
0.188
0.952
0.000
0.001
0.252
0.012
1.000
0.361
1.000
0.000
0.000
0.003
0.973
0.188
0.361
0.980
0.000
0.003
0.000
0.167
0.952
1.000
0.980
0.000
0.001
0.000
0.000
0.000
0.000
0.000
0.000
0.025
0.000
0.057
0.001
0.000
0.003
0.001
0.025
0.000
0.000
0.252
0.003
0.000
0.000
0.000
0.000

-4.779e+02
1.000
9.384e-02
0.091
-2.789e+03
1.000
1.392e-01
0.007
-1.740e+04
0.034
-1.253e-02
1.000
-1.810e+04
0.022
-1.982e-02
1.000
-1.686e+03
1.000
-1.000e-01
0.000
-1.196e+04
0.709
-2.064e-01
0.000
-1.947e+04
0.009
6.557e-03
1.000
4.779e+02
1.000
-9.384e-02
0.091
-2.311e+03
1.000
4.540e-02
1.000
-1.693e+04
0.046
-1.064e-01
0.046
-1.763e+04
0.029
-1.137e-01
0.013
-1.208e+03
1.000
-1.938e-01
0.000
-1.148e+04
0.888
-3.003e-01
0.000
-1.899e+04
0.012
-8.728e-02
0.151
2.789e+03
1.000
-1.392e-01
0.007
2.311e+03
1.000
-4.540e-02
1.000
-1.461e+04
0.179
-1.518e-01
0.004
-1.531e+04
0.121
-1.591e-01
0.001
1.104e+03
1.000
-2.392e-01
0.000
-9.169e+03
1.000
-3.457e-01
0.000
-1.668e+04
0.054
-1.327e-01
0.012
1.740e+04
0.034
1.253e-02
1.000
1.693e+04
0.046
1.064e-01
0.046
1.461e+04
0.179
1.518e-01
0.004
-7.002e+02
1.000
-7.285e-03
1.000
1.572e+04
0.095
-8.747e-02
0.008
5.446e+03
1.000
-1.939e-01
0.000
-2.061e+03
1.000
1.909e-02
1.000
1.810e+04
0.022
1.982e-02
1.000
1.763e+04
0.029
1.137e-01
0.013
1.531e+04
0.121
1.591e-01
0.001
7.002e+02
1.000
7.285e-03
1.000
1.642e+04
0.063
-8.018e-02
0.004
6.146e+03
1.000
-1.866e-01
0.000
-1.361e+03
1.000
2.637e-02
1.000
1.686e+03
1.000
1.000e-01
0.000
1.208e+03
1.000
1.938e-01
0.000
-1.104e+03
1.000
2.392e-01
0.000
-1.572e+04
0.095
8.747e-02
0.008
-1.642e+04
0.063
8.018e-02
0.004
-1.027e+04
1.000
-1.064e-01
0.000
-1.778e+04
0.027
1.066e-01
0.000
1.196e+04
0.709
2.064e-01
0.000
1.148e+04
0.888
3.003e-01
0.000
9.169e+03
1.000
3.457e-01
0.000
-5.446e+03
1.000
1.939e-01
0.000
-6.146e+03
1.000
1.866e-01
0.000
1.027e+04
1.000
1.064e-01
0.000
-7.507e+03
1.000
2.130e-01
0.000
1.947e+04
0.009
-6.557e-03
1.000
1.899e+04
0.012
8.728e-02
0.151
1.668e+04
0.054
1.327e-01
0.012
2.061e+03
1.000
-1.909e-02
1.000
1.361e+03
1.000
-2.637e-02
1.000
1.778e+04
0.027
-1.066e-01
0.000
7.507e+03
1.000
-2.130e-01
0.000
Ranking
U N DX BLX(0.3) BLX(0.5) SBX(5) SBX(2) CIXL2 Logical > Ext.F.
U N DX SBX(5) SBX(2) Logical BLX(0.5) Ext.F. BLX(0.3) CIXL2
Logical > Ext.F. > SBX(5) SBX(2) CIXL2 U N DX BLX(0.3) BLX(0.5)

Table 16: Results multiple comparison tests fGri , le fLan functions
ranking established test regarding crossover operator.

36

fiCIXL2: Crossover Operator Evolutionary Algorithms



J

CIXL2

U DAc
EGN ABGe
CIXL2
EGN ABGe
CIXL2
U DAc

U DAc
EGN ABGe
Function
fSchDS
fRos
fRas
fSch
CIXL2
U DAc
EGN ABGe
Function
fAck
fGri
le
fLan

J

J

J
fSchDS
fRos
fRas
-2.221e+01 0.000 -2.928e+00 0.000 -1.547e+02
-2.076e-01 0.000 -2.906e+00 0.000 -1.533e+02
2.221e+01 0.000 2.928e+00 0.000 1.547e+02
2.200e+01 0.000 2.207e-02 0.856 1.360e+00
2.076e-01 0.000 2.906e+00 0.000 1.533e+02
-2.200e+01 0.000 -2.207e-02 0.856 -1.360e+00
Ranking
U DAc > EGN

BGe





J
fSch
-1.089e+04
-1.091e+04
1.089e+04
-2.390e+01
1.091e+04
2.390e+01

0.000
0.000
0.000
0.677
0.000
0.677

fLan
0.004
-3.306e-02
0.150
-3.306e-02
0.004
3.306e-02
0.049 1.33781e-11
0.150
3.306e-02
0.049 -1.33781e-11

0.176
0.176
0.176
0.325
0.176
0.325

0.000
0.000
0.000
0.888
0.000
0.888

> CIXL2

U DAc EGN
> CIXL2
BGe
U DAc EGN
> CIXL2
BGe

EGN

U DAc
EGN ABGe
CIXL2
EGN ABGe
CIXL2
U DAc

fAck
-1.101e-08
-9.194e-09
1.101e-08
1.817e-09
9.194e-09
-1.817e-09

0.000
0.000
0.000
0.175
0.000
0.175

BGe

fGri
1.525e-02
1.525e-02
-1.525e-02
1.266e-16
-1.525e-02
-1.266e-16

U DAc > CIXL2

fFle
0.000 9.803e+03
0.000 6.157e+03
0.000 -9.803e+03
0.000 -3.646e+03
0.000 -6.157e+03
0.000 3.646e+03
Ranking

U DAc EGN

BGe

> CIXL2

CIXL2 > U DAc > EGN

BGe

CIXL2 EGN
> U DAc
BGe
U DAc EGN
CIXL2
BGe

Table 17: Results functions multiple comparison test ranking
obtained depending evolutionary algorithm.

37

fiOrtiz-Boyer, Hervas-Martnez, & Garca-Pedrajas

Appendix B. Convergence Graphics

Average fitness best individual 30 runs

100

CIXL2(0.70,5)
BLX(0.3)
SBX(2)
Fuzzy
Logical
UNDX

1
0.01
0.0001
1e-06
1e-08
1e-10
1e-12
1e-14
1e-16

0

50000

100000

150000

200000

250000

300000

Evaluations

Figure 5: Evolution average fitness, logarithmic scale, using different crossover
operators function fSph .

Average fitness best individual 30 runs

100000

CIXL2(0.70,5)
BLX(0.5)
SBX(2)
Fuzzy
Logical
UNDX

10000

1000

100

10

1

0.1

0.01

0.001

0

50000

100000

150000
Evaluations

200000

250000

300000

Figure 6: Evolution average fitness, logarithmic scale, using different crossover
operators function fSchDS .

38

fiCIXL2: Crossover Operator Evolutionary Algorithms

Average fitness best individual 30 runs

1000

CIXL2(0.70,5)
BLX(0.3)
SBX(2)
Fuzzy
Logical
UNDX

100

10

0

50000

100000

150000

200000

250000

300000

Evaluations

Figure 7: Evolution averaged fitness, logarithmic scale, using different crossover
operators function fRos .

Average fitness best individual 30 runs

1000

CIXL2(0.70,5)
BLX(0.3)
SBX(5)
Fuzzy
Logical
UNDX

100

10

1

0

50000

100000

150000
Evaluations

200000

250000

300000

Figure 8: Evolution average fitness, logarithmic scale, using different crossover
operators function fRas .

39

fiOrtiz-Boyer, Hervas-Martnez, & Garca-Pedrajas

Average fitness best individual 30 runs

100000

CIXL2(0.70,5)
BLX(0.3)
SBX(5)
Fuzzy
Logical
UNDX

10000

1000

100

0

50000

100000

150000

200000

250000

300000

Evaluations

Figure 9: Evolution average fitness, logarithmic scale, using different crossover
operators function fSch .

Average fitness best individual 30 runs

100

CIXL2(0.70,5)
BLX(0.3)
SBX(2)
Fuzzy
Logical
UNDX

1

0.01

0.0001

1e-06

1e-08

0

50000

100000

150000
Evaluations

200000

250000

300000

Figure 10: Evolution average fitness, logarithmic scale, using different crossover
operators function fAck .

40

fiCIXL2: Crossover Operator Evolutionary Algorithms

Average fitness best individual 30 runs

100

CIXL2(0.70,5)
BLX(0.5)
SBX(2)
Fuzzy
Logical
UNDX

10

1

0.1

0.01

0.001

0

50000

100000

150000

200000

250000

300000

Evaluations

Figure 11: Evolution average fitness, logarithmic scale, using different crossover
operators function fGri .

Average fitness best individual 30 runs

1e+07

CIXL2(0.70,5)
BLX(0.3)
SBX(2)
Fuzzy
Logical
UNDX

1e+06

100000

10000

0

50000

100000

150000
Evaluations

200000

250000

300000

Figure 12: Evolution average, logarithmic scale, using different crossover operators
function le .

41

fiOrtiz-Boyer, Hervas-Martnez, & Garca-Pedrajas

Average fitness best individual 30 runs

1

0.01

0.0001

1e-06

1e-08

1e-10
CIXL2(0.70,5)
BLX(0.3)
SBX(5)
Fuzzy
Logical
UNDX

1e-12

1e-14

0

50000

100000

150000
Evaluations

200000

250000

300000

Figure 13: Evolution average fitness, logarithmic scale, using different crossover
operators function fLan .

42

fiCIXL2: Crossover Operator Evolutionary Algorithms

References
Ackley, D. (1987). empirical study bit vector function optimizacion. Genetic Algorithms Simulated Annealing, 170215.
Andersen, H. C., & Tsoi, A. C. (1993). constructive algorithm training
multilayer pereptron based genetic algorithm. Complex Systems, 7 (4), 249
268.
Arabas, J., Michalewicz, Z., & Mulawka, J. (1994). Gavaps - genetic algorithm
varying population size. Michalewicz, Z., Krawczyk, J., Kazemi, M., & Janikow,
C. (Eds.), First IEEE International Conference Evolutionary Computation, Vol. 1,
pp. 7378, Orlando. IEEE Service Center, Piscataway, NJ.
Bebis, G., Georgiopoulos, M., & Kasparis, T. (1997). Coupling weight elimination genetic algorithms reduce network size preserve generalization. Neurocomputing,
17, 167194.
Bebis, G., Louis, S., Varol, Y., & Yfantis, A. (2002). Genetic object recognition using
combinations views. IEEE Transactions Evolutionary Computation, 6 (2), 132.
Bengoetxea, E., & Miquelez, T. (2002). Estimation distribution algorithms: new tool
evolutionary computation (D.E. Goldberg edition)., Vol. 2 Genetic algorithms
evolutionary computation, chap. Experimental result function optimization
EDAs continuous Domain. Kluwer.
Bersini, H., Dorigo, M., Langerman, S., Seront, G., & Gambardella, L. M. (1996). Results
first international contest evolutionary optimisation (1st iceo). Proceedings
IEEE International Conference Evolutionary Computation, IEEE-EC 96, pp.
611615, Nagoya, Japan. IEEE Press.
Beyer, H.-G., & Deb, K. (2001). self-adapting features real-parameter evolutionary
algorithms. IEEE Transactions evolutionary computation, 5 (3), 250270.
Breiman, L. (1996). Stacked regressions. Machine Learning, 24 (1), 4964.
Back, J. H. (1996). Evolutionary Algorithms Theory Practice. Oxford University
Press, Oxford.
Back, T., Fogel, D., & Michalewicz, Z. (1997). Handbook Evolutionary Computation.
Institute Physics Publishing Ltd, Bristol Oxford University Press, New York.
Back, T., & Schwefel, H. P. (1993). overview evolutionary algorithms parameter
optimization. Evolutionary Computation, 1 (1), 123.
Cano, J., Herrera, F., & Lozano, M. (2003). Using evolutionary algorithms instance
selection data reduction kdd: experimental study. IEEE Transactions
Evolutionary Computation, 7 (6), 561575.
Davidor, Y. (1991). Genetic Algorithms Robotics: Heuristic Strategy Optimization,
Vol. 1 Robotics Automated Systems. World Scientific.
De Jong, K. D. (1975). analysis behavior class genetic adaptive systems.
Ph.D. thesis, Departament Computer Communication Sciences, University
Michigan, Ann Arbor.
43

fiOrtiz-Boyer, Hervas-Martnez, & Garca-Pedrajas

De Jong, M. B., & Kosters, W. (1998). Solving 3-sat using adaptive sampling. Poutre,
H., & van den Herik, J. (Eds.), Proceedings Tenth Dutch/Belgian Artificial
Intelligence Conference, pp. 221228.
Deb, K., & Agrawal, R. B. (1995). Simulated binary crossover continuous search space.
Complex Systems, 9, 115148.
Deb, K., & Beyer, H. (2001). Self-adaptive genetic algorithms simulated binary
crossover. Evolutionary Computation, 9 (2), 195219.
Dixon, L. C. W. (1974). Nonlinear optimization: survey state art. Software
Numerical Mathematics, 193216. Academic Press.
Dunn, O. J., & Clark, V. (1974). Applied Statistics: Analysis Variance Regression.
Wiley, New York.
Eiben, A., & Back, T. (1997a). Multi-parent recombination operators continuous search
spaces. Tech. rep. TR-97-01, Leiden University.
Eiben, A. E., & Back, T. (1997b). Empirical investigation multi-parent recombination
operators evolution strategies. Evolutionary Computation, 5 (3), 347365.
Eiben, A., van der Hauw, J., & van Hemert, J. (1998). Graph coloring adaptive
evolutionary algorithms. Journal Heuristics, 4 (1), 2546.
Eshelman, L. J., & Schaffer, J. D. (1993). Real-coded genetic algorithms intervalschemata. Whitley, L. D. (Ed.), Foundation Genetic Algorithms 2, pp.
187C3.3.7:1C3.3.7:8.202, San Mateo. Morgan Kaufmann.
Fletcher, R., & Powell, M. J. D. (1963). rapidly convergent descent method minimization. Computer Journal, pp. 163168.
Fogel, D. B. (1995). Evolutionary Computation: Toward New Philosophy Machine
Intelligence. IEEE Press, Piscataway, New Jork.
Fogel, L. J., Owens, A. J., & Walsh, M. J. (1966). Artificial Intelligence Simulated
Evolution. John Wiley & Sons.
Friedman, J. H. (1994). overview predictive learning function approximation.
Cherkassky, V., Friedman, J. H., & Wechsler, H. (Eds.), Statistics Neural
Networks, Theory Pattern Recognition Applications, Vol. 136 NATO ASI Series
F, pp. 161. Springer-Verlag.
Garca-Pedrajas, N., Hervas-Martnez, C., & Ortiz-Boyer, D. (2005). Cooperative coevolution artificial neural network ensembles pattern classification. IEEE Transactions Evolutionary Computation, 9 (3), 271302.
Goldberg, D. E. (1989a). Genetic Algorithms Search, Optimization, Machine Learning. Addison-Wesley, New York.
Goldberg, D. E. (1989b). Sizing populations serial parallel genetic algorithms.
Schaffer, J. (Ed.), 3rd International Conference Genetic Algorithms, pp. 7079,
San Mateo, CA. Morgan Kaufmann.
Goldberg, D. E. (1991). Real-coded genetic algorithms, virtual alphabets, blocking.
Complex Systems, pp. 139167.
44

fiCIXL2: Crossover Operator Evolutionary Algorithms

Goldberg, D. E., & Deb, K. (1991). comparative analysis selection schemes used
genetic algorithms. Rawlins, G. J. E. (Ed.), Foundations Genetic Algorithms,
pp. 6993, San Mateo, CA. Morgan Kaufmann.
Gordon, V. S., & Whitley, D. (1993). Serial parallel genetic algorithms function
optimizers. Forrest, S. (Ed.), Fifth International Conference Genetic Algorithms,
pp. 177183. Morgan Kaufmann.
Grefenstette, J. J. (1986). Optimization control parameters genetic algorithms. IEEE
Transactions Systems, Mans, Cybernetics, 16 (1), 122128.
Hadley, G. (1964). Nonlinear Dynamics Programming. Addison Wesley.
Hashem, S. (1997). Optimal linear combinations neural networks. Neural Networks,
10 (4), 599614.
Herrera, F., Herrera-Viedma, E., Lozano, E., & Verdegay, J. L. (1994). Fuzzy tools
improve genetic algorithms. Second European Congress Intelligent Techniques
Soft Computing, pp. 15321539.
Herrera, F., & Lozano, M. (2000). Gradual distributed real-coded genetic algorithms. IEEE
Transactions Evolutionary Computation, 4 (1), 4363.
Herrera, F., Lozano, M., & Sanchez, A. M. (2003). taxonomy crossover operator
real-coded genetic algorithms: experimental study. International Journal
Intelligent Systems, 18, 309338.
Herrera, F., Lozano, M., & Verdegay, J. L. (1998). Tackling real-coded genetic algorithms:
Operators tools behavioural analysis. Artificial Inteligence Review, pp. 265
319. Kluwer Academic Publisher. Printed Netherlands.
Hervas-Martnez, C., & Ortiz-Boyer, D. (2005). Analizing statistical features cixl2
crossover offspring. Soft Computing, 9 (4), 270279.
Holland, J. H. (1975). Adaptation natural artificial systems. University
Michigan Press, Ann Arbor, MI.
Johnson, T., & Husbands, P. (1990). System identification using genetic algorithms.
Parallel Problem Solving Nature, Vol. 496 Lecture Notes Computer Science,
pp. 8589, Berlin. Springer-Verlag.
Jong, K. A. D., & Sarma, J. (1993). Generation gaps revisited. Whitley, L. D. (Ed.),
Foundations Genetic Algorithms, Vol. 2, pp. 1928. Morgan Kaufmann, San Mateo.
Kendall, M., & Stuart, S. (1977). advanced theory statistics, Vol. 1. Charles GriOEn
& Company.
Kita, H. (2001). comparison study self-adaptation evolution strategies real-code
genetic algorithms. Evolutionary Computation, 9 (2), 223241.
Kita, H., Ono, I., & Kobayashi, S. (1998). Theoretical analysis unimodal normal distribution crossover real-coded genetic algorithms. IEEE International Conference
Evolutionary Computation ICEC98, pp. 529534, Anchorage, Alaska, USA.
Kivinen, J., & Warmuth, M. (1997). Exponential gradient descent versus gradient descent
linear predictors. Information Computation, 132 (1), 163.
45

fiOrtiz-Boyer, Hervas-Martnez, & Garca-Pedrajas

Kuncheva, L. (1995). Editing k-nearest neighbors rule genetic algorithm.
Pattern Recognition Letter, 16, 809814.
Larranaga, P., Etxeberria, R., Lozano, J., & Pena, J. (2000). Optimization continuous
domains learning simulation gaussian networks. Wu, A. (Ed.), Proceeding
2000 Genetic Evolutionary Computation Conference Workshop Program,
pp. 201204.
Leblanc, M., & Tibshirani, R. (1993). Combining estimates regression classification.
Tech. rep., Department Statistics, University Toronto.
Levene, H. (1960). Contributions Probability Statistics, chap. Essays Honor
Harold Hotelling, pp. 278292. Stanford University Press.
Liu, Y., Yao, X., & Higuchi, T. (2000). Evolutionary ensembles negative correlation
learning. IEEE Transactions Evolutionary Computation, 4 (4), 380387.
Merz, C. J. (1999a). principal components approach combining regression estimates.
Machine Learning, 36 (1), 932.
Merz, C. J. (1999b). Using correspondence analysis combine classifiers. Machine Learning, 36 (1), 3358.
Michalewicz, Z. (1992). Genetic Algorithms + Data Structures = Evolution Programs.
Springer-Verlag, New York.
Miller, G. F., Todd, P. M., & Hedge, S. U. (1991). Designing neural networks. Neural
Networks, 4, 5360.
Miller, R. G. (1981). Simultaneous Statistical Inference (2 edition). Wiley, New York.
Miller, R. G. (1996). Beyond ANOVA, Basics Applied Statistics (2 edition). Chapman
& Hall, London.
Mizumoto, M. (1989). Pictorial representations fuzzy connectives. part i: Cases tnorms, t-conorms averaging operators. Fuzzy Sets Systems, 31, 217242.
Moriarty, D., Schultz, A., & Grefenstette, J. (1999). Evolutionary algorithms reinforcement learning. Journal Artificial Intelligence Reserarch, 11.
Muhlenbein, H., Mahnig, T., & Rodriguez, O. (1999). Schemata, distributions graphical
models evolutionary optimazation. Journal Heuristics, pp. 215247.
Muhlenbein, H., & Paa, G. (1998). recombination genes estimation
distributions i. binary parameters.. Eiben, A. E., Back, T., Schoenauer, M., &
Schwefel, H.-P. (Eds.), 5th Conference Parallel Problem Solving Nature,
pp. 178187. Springer.
Ono, I., Kita, H., & Kobayashi, S. (1999). robust real-coded genetic algorithm using
unimodal normal distribution crossover augmented uniform crossover: Effects
self-adaptation crossover probabilities. Banzhaf, W., Daida, J., Eiben, A. E.,
Garzon, M. H., Honavar, V., Jakiela, M., & Smith, R. E. (Eds.), Genetic Evolutionary Computation Conf. (GECCO99), pp. 496503, San Francisco, CA. Morgan
Kaufmann.
46

fiCIXL2: Crossover Operator Evolutionary Algorithms

Ono, I., & Kobayashi, S. (1997). real-coded genetic algorithm function optimization
using unimodal normal distribution crossover. 7th International Conference
Genetic Algorithms, pp. 246253, Michigan, USA. Michigan State University, Morgan
Kaufman.
Ono, I., Kobayashi, S., & Yoshida, K. (2000). Optimal lens design real-coded genetic
algorithms using undx. Computer methods applied mechanics engineering, pp.
483497.
Opitz, D. W., & Shavlik, J. W. (1996). Actively searching effective neural network
ensemble. Connection Science, 8 (3), 337353.
Oren, S. S. (1974). selection parameters self scaling variable metric algorithms.
Mathematical Programming, pp. 351367.
Ortiz-Boyer, D., Hervas-Martnez, C., & Munoz-Perez, J. (2003). Metaheuristics: Computer
Decision-Making, chap. Study genetic algorithms crossover based confidence
intervals alternative classic least squares estimation methods non-linear
models, pp. 127151. Kluwer Academic Publishers.
Perrone, M. P., & Cooper, L. N. (1993). networks disagree: Ensemble methods
hybrid neural networks. Mammone, R. J. (Ed.), Neural Networks Speech
Image Processing, pp. 126142. Chapman Hall.
Rastrigin, L. A. (1974). Extremal control systems. Theoretical Foundations Engineering Cybernetics Series. Moscow: Nauka, Russian.
Rechenberg, I. (1973). Evolutionsstrategie-Optimierum technischer Systeme nach Prinzipien der biologischen Evolution. Ph.D. thesis, Stuttgart-Bad Cannstatt: FrommannHolzboog.
Rosenbrock, H. H. (1960). automatic method finding greatest least value
function. Computer Journal, pp. 175184.
Rudolph, G. (1994). Convergence analysis canonical genetic algorithms. IEEE Transactions Neural Networks, special issue evolutionary computation, 5 (1), 96101.
Salomon, R. (1996). Reevaluating genetic algorithm performance coordinate rotation
benchmark functions. BioSystems, pp. 263278.
Satoh, H., Yamamura, M., & Kobayashi, S. (1996). Minimal generation gap model
gas considering exploration exploitation.. Proceeding IIZUKA:
Methodologies Conception, Design, Application Intelligent Sstems, pp.
494497.
Schaffer, J., Caruana, R., Eshelman, L., & Das, R. (1989). study control parameters affecting online performance genetic algorithms function optimization.
Schaffer, J. (Ed.), 3rd International Conference Genetic Algorithms, pp. 5160,
San Mateo, CA. Morgan Kaufmann.
Schlierkamp-Voosen, D. (1994). Strategy adaptation competition. Second European
Congress Intelligent Techniques Soft Computing, pp. 12701274.
47

fiOrtiz-Boyer, Hervas-Martnez, & Garca-Pedrajas

Schwefel, H. P. (1981). Numerical Optimization Computer Models. John Wiley & Sons.
English translation Numerische Optimierung von Computer-Modellen mittels der
Evolutionsstrategie, 1977.
Schwefel, H. P. (1995). Evolution Optimum Seeking. John Wiley & Sons.
Sedighi, K., Ashenayi, K., Manikas, T., Wainwright, R., & Tai, H. (2004). Autonomous
local path planning mobile robot using genetic algorithm. IEEE Congress
Evolutionary Computation.
Sharkey, A. J. C. (1996). combining artificial neural nets. Connection Science, 8,
299313.
Singh, M., Chatterjee, A., & Chaudhury, S. (1997). Matching structural shape descriptions
using genetic algorithms. Pattern Recognition, 30 (9), 14511462.
Smith, R. E. (1993). Adaptively resizing populations: algorithm analysis. Forrest,
S. (Ed.), 5th International Conference Genetic Algorithms, p. 653, San Mateo, CA.
Morgan Kaufmann.
Snedecor, G. W., & Cochran, W. G. (1980). Statistical Methods (7 edition). Iowa State
University Press, Ames, Iowa.
Spedicato, E. (1975). Computational experience quasi-newton algorithms minimization problems moderately large size. Tech. rep. CISE-N-175, Centro Informazioni
Studi Esperienze, Segrate (Milano), Italy.
Takahashi, O., Kita, H., & Kobayashi, S. (1999). distance dependent alternation model
real-coded genetic algorithms. IEEE International Conference Systems, Man,
Cybernetics, pp. 619624.
Tamhane, A. C., & Dunlop, D. D. (2000). Statistics Data Analysis. Prentice Hall.
Voigt, H. M., Muhlenbein, H., & Cvetkovic, D. (1995). Fuzzy recombination breeder
genetic algorithms. Eshelman, L. (Ed.), 6th International Conference Genetic
Algorithms, pp. 104111, San Mateo, CA. Morgan Kaufmann.
Webb, G. I. (2000). Multiboosting: technique combining boosting wagging.
Machine Learning, 40 (2), 159196.
Whitley, D., Mathias, K., Rana, S., & Dzubera, J. (1995). Building better test functions.
Eshelman, L. (Ed.), Sixth International Conference Genetic Algorithms, pp.
239246. Morgan Kaufmann.
Wolpert, D. H., & Macready, W. G. (1995). free-lunch theorems search. Tech. rep.
95-02-010, Santa Fe Institute.
Wright, A. (1991). Genetic algorithms real parameter optimization. Rawlin, G.
J. E. (Ed.), Foundations Genetic Algorithms 1, pp. 205218, San Mateo. Morgan
Kaufmann.
Zhang, B. T., & Kim, J. J. (2000). Comparison selection methods evolutionary
optimization. Evolutionary Optimization, 2 (1), 5570.
Zhou, Z.-H., Wu, J., & Tang, W. (2002). Ensembling neural networks: Many could better
all. Artificial Intelligence, 137 (12), 239253.

48


