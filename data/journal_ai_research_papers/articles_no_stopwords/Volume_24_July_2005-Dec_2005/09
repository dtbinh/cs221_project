journal artificial intelligence

submitted published

efficiency versus convergence boolean kernels
line learning
roni khardon

roni cs tufts edu

department computer science tufts university
medford

dan roth

danr cs uiuc edu

department computer science university illinois
urbana il usa

rocco servedio

rocco cs columbia edu

department computer science columbia university
york ny

abstract
studies machine learning example described
set boolean features hypotheses represented linear threshold elements
one method increasing expressiveness learned hypotheses context
expand feature set include conjunctions basic features done explicitly
possible kernel function focusing well known perceptron
winnow demonstrates tradeoff computational
efficiency run expanded feature space
generalization ability corresponding learning
first describe several kernel functions capture limited forms conjunctions conjunctions kernels used efficiently run
perceptron feature space exponentially many conjunctions however kernels perceptron provably make
exponential number mistakes even learning simple functions
consider question whether kernel functions analogously used
run multiplicative update winnow expanded feature space
exponentially many conjunctions known upper bounds imply winnow
learn disjunctive normal form dnf formulae polynomial mistake bound
setting however prove computationally hard simulate winnows
behavior learning dnf feature set implies kernel functions
correspond running winnow efficiently computable
general construction run winnow kernels

introduction
classifying objects one two classes positive negative
examples concept often studied machine learning task machine learning
extract classifier given pre classified examples learning
data example represented set n numerical features example
c

ai access foundation rights reserved

fikhardon roth servedio

seen point euclidean space n common representation classifiers
case hyperplane dimension n splits domain examples
two areas positive negative examples representation known linear
threshold function many learning output hypothesis represented
manner developed analyzed implemented applied practice
particular interest well known perceptron rosenblatt block
novikoff winnow littlestone intensively
studied literature
well known expressiveness linear threshold functions quite limited minsky papert despite fact perceptron winnow
applied successfully recent years several large scale real world classification
one example snow system roth carlson cumby rosen roth
successfully applied variations perceptron winnow natural language
processing snow system extracts basic boolean features x xn labeled pieces
text data order represent examples thus features numerical values restricted several ways enhance set basic features x xn
perceptron winnow one idea expand set basic features x xn
conjunctions x x x use expanded higher dimensional examples
conjunction plays role basic feature examples perceptron
winnow fact snow system takes running perceptron
winnow space restricted conjunctions basic features idea closely
related use kernel methods see e g book cristianini shawe taylor
feature expansion done implicitly kernel function clearly leads increase expressiveness thus may improve performance
however dramatically increases number features n n conjunctions used thus may adversely affect computation time convergence
rate learning provides theoretical study performance perceptron
winnow run expanded feature spaces
background line learning perceptron winnow
describing recall necessary background line learning
model littlestone perceptron winnow
given instance space x possible examples concept mapping instances
one two classes concept class c x simply set concepts line
learning concept class c fixed advance adversary pick concept c c
learning modeled repeated game iteration adversary
picks example x x learner gives guess value c x told
correct value count one mistake iteration value predicted
correctly learning learns concept class c mistake bound
choice c c arbitrarily long sequence examples learner guaranteed
make mistakes
consider case examples given boolean features
x n two class labels denoted thus x n
labeled example hx positive example labeled example hx negative


fiefficiency versus convergence boolean kernels

example concepts consider built logical combinations n base
features interested mistake bounds polynomial n
perceptron
throughout execution perceptron maintains weight vector w n initially
upon receiving example x n predicts according
linear threshold function w x prediction label false positive
prediction vector w set w x prediction label
false negative w set w x change made w prediction correct
many variants basic proposed studied particular one
add non zero threshold well learning rate controls size update
w discussed section
famous perceptron convergence theorem rosenblatt block novikoff
bounds number mistakes perceptron make
theorem let hx hxt yt sequence labeled examples xi n kxi k
r yi let u n yi u xi


mistakes example sequence
perceptron makes r kuk

winnow
winnow littlestone similar structure winnow maintains
hypothesis vector w n initially w winnow parameterized
promotion factor threshold upon receiving example x n
winnow predicts according threshold function w x prediction
label xi value wi set wi demotion
step prediction label xi value wi
set wi promotion step change made w prediction correct
purposes following mistake bound implicit littlestones work
interest
theorem let target function k literal monotone disjunction f x xn
xi xik sequence examples n labeled according f number

prediction mistakes made winnow
n k log

interested computational efficiency convergence perceptron
winnow run expanded feature spaces conjunctions specifically
study use kernel functions expand feature space thus enhance
learning abilities perceptron winnow refer enhanced
kernel perceptron kernel winnow
first cf papers sadohara watkins kowalczyk
et al uses kernel functions possible efficiently run kernel
perceptron exponential number conjunctive features


fikhardon roth servedio

see theorem simulates perceptron n dimensional feature space conjunctions n basic features given sequence
labeled examples n prediction update example take poly n time
steps prove variants expanded feature space consists
monotone conjunctions conjunctions bounded size
closely related one main open learning theory
efficient learnability disjunctions conjunctions dnf disjunctive normal form
expressions since linear threshold elements represent disjunctions e g x x x
true iff x x x theorem imply kernel perceptron
used learn dnf however framework values n r theorem
exponentially large note n n r n conjunctions used
hence mistake bound given theorem exponential rather polynomial
n question thus arises whether exponential upper bound implied theorem
essentially tight kernel perceptron context dnf learning
give affirmative answer thus showing kernel perceptron cannot efficiently learn
dnf
monotone dnf f x xn sequence examples labeled
according f causes kernel perceptron make n mistakes
holds generalized versions perceptron fixed updated
threshold learning rate used give variant showing
kernel perceptron fails probably approximately correct pac learning model
valiant well
turning winnow attractive feature theorem suitable bound
logarithmic total number features n e g n therefore
noted several researchers maass warmuth winnow analogue theorem
could obtained would imply dnf learned computationally efficient
poly n mistake bound however give strong evidence
winnow analogue theorem exist
polynomial time simulates winnow exponentially many monotone conjunctive features learning monotone dnf unless every
complexity class p solved polynomial time holds wide
range parameter settings winnow
observe contrast negative maass warmuth shown
winnow simulated efficiently exponentially many conjunctive
features learning simple geometric concept classes maass warmuth
thus indicate tradeoff computational efficiency convergence
kernel rich classes boolean functions dnf formulas kernel
angluin proved dnf expressions cannot learned efficiently equivalence queries
whose hypotheses dnf expressions since model exact learning equivalence
queries equivalent mistake bound model consider implies
online uses dnf formulas hypotheses efficiently learn dnf however
preclude efficient learnability dnf different class hypotheses
kernel perceptron generates hypotheses thresholds conjunctions rather dnf
formulas thus angluins negative apply



fiefficiency versus convergence boolean kernels

perceptron computationally efficient run exponentially slow convergence whereas kernel winnow rapid convergence seems require exponential
runtime

kernel perceptron many features
well known hypothesis w perceptron linear combination
previous examples mistakes made cristianini shaw taylor
precisely let l v denote label example v
p
w vm l v v set examples made mistake
p
p
thus prediction perceptron x iff wx vm l v v x vm l v v x

example x n let x denote transformation enhanced feature
space space conjunctions run perceptron
enhanced space must predict iff w x w weight vector
p
enhanced space discussion holds iff vm l v v x
p
denoting k v x v x holds iff vm l v k v x
thus never need construct enhanced feature space explicitly order run
perceptron need able compute kernel function k v x efficiently
idea behind called kernel methods applied
support vector machines whose prediction function inner products examples
detailed discussion given book cristianini shawe taylor
thus next theorem simply obtained presenting kernel function capturing
conjunctions
theorem simulates perceptron feature spaces
conjunctions monotone conjunctions conjunctions size k
monotone conjunctions size k given sequence labeled examples n
prediction update example take poly n time steps
proof case includes n conjunctions positive negative literals
k x must compute number conjunctions true x clearly
literal conjunction must satisfy x thus corresponding bit
x must value thus conjunction true x corresponds
subset bits counting conjunctions gives k x x
x number original features value x e
number bit positions xi yi kernel obtained independently
sadohara
express monotone monomials take k x xy x
number active features common x e number bit positions
xi yi
similarly case number conjunctions satisfy x k x
pk x
kernel reported watkins case
l
l


p
k x kl xy


l


fikhardon roth servedio

kernel perceptron many mistakes
section describe simple monotone dnf target function sequence
labeled examples causes monotone monomials kernel perceptron
make exponentially many mistakes
x n write x denote number x described
xy denote number bit positions xi yi need following
well known tail bound sums independent random variables found
e g section book kearns vazirani
fact let x xm sequence independent valued random variables
p
e xi p let x denote
xi e x pm

pr x pm emp





pr x pm emp





use following combinatorial property
lemma set n bit strings x xt n en
xi n xi xj n j
proof use probabilistic method let xi n chosen
independently setting bit probability clear
e xi n applying fact pr xi n en thus
probability xi satisfies xi n ten similarly j
e xi xj n applying fact pr xi xj n en
thus probability xi xj
j satisfies xi xj n
n
n
n
ten less thus
e
value e
e
choice x xt xi n xi xj n xi
xi n set xi n lemma proved

previous lemma construct difficult data set kernel perceptron
theorem monotone dnf f x xn sequence examples labeled
according f causes kernel perceptron make n mistakes
proof target dnf use simple single conjunction
x x xn original perceptron n features x xn easily
seen make poly n mistakes target function
monotone kernel perceptron runs feature space n monotone
monomials make en mistakes
recall beginning perceptron execution n coordinates
w first example negative example n monomial true
example empty monomial true every example since w x perceptron incorrectly predicts example resulting update causes coefficient
w corresponding empty monomial become n coordinates
w remain next example positive example n example
w x perceptron incorrectly predicts since n monotone conjunctions


fiefficiency versus convergence boolean kernels

satisfied example resulting update causes w become n
coordinates w become next en examples vectors x xt
described lemma since example xi n example negative
however perceptron predict examples
fix value en consider hypothesis vector w example

x received since xi n value w xi sum n different
coordinates wt correspond monomials satisfied xi precisely
p
p
w xi ai wt bi wt ai contains monomials satisfied
xi xj j bi contains monomials satisfied xi
xj j lower bound two sums separately
let monomial ai lemma ai contains n variables

pn
monomials ai well known bound
thus r n
r


p
h
h p p log p p log p
j j
binary entropy function found e g theorem book
van lint n n n terms ai moreover
value wt must least en since wt decreases
p
example hence ai wt en n n hand bi
false examples therefore wt demoted wt
lemma r n every r variable monomial satisfied xi must belong bi


p
pn
hence bi wt r n n
n combining inequalities
r
w xi n n hence perceptron prediction xi

remark first sight might seem limited simple special case
perceptron several variations exist use added feature fixed
value enables update threshold indirectly via weight w non
zero fixed initial threshold learning rate particular three
used simultaneously generalized predicts according hypothesis
w x w updates w w x w w promotions similarly
demotions exponential lower bounds number mistakes
derived general well first note since kernel
includes feature empty monomial true first parameter
already accounted two parameters note degree freedom
learning rate fixed threshold since multiplying factor
change hypothesis therefore suffices consider threshold
consider several cases value threshold satisfies
use sequence examples first two examples makes
promotion n may may update n important
p
p
examples sequence bounds ai wt bi wt still valid
final inequality proof becomes w xi n n n true
sufficiently large n n construct following scenario use
function f x x xn sequence examples includes repetitions
example x first bit bits example x satisfies
exactly monomials therefore make mistakes examples
sequence initial hypothesis misclassifies n start example


fikhardon roth servedio

sequence repeating example n classified correctly de times
threshold large absolute value e g n done otherwise
continue example n since weights except empty monomial zero
stage examples n n classified way n misclassified
therefore makes promotion argument rest sequence
except adding term empty monomial final inequality becomes
w xi n n n n examples misclassified thus
cases kernel perceptron may make exponential number mistakes
negative pac model
proof adapted give negative kernel perceptron pac
learning model valiant model example x independently drawn
fixed probability distribution high probability learner must construct
hypothesis h high accuracy relative target concept c distribution
see kearns vazirani text detailed discussion pac learning model
let probability distribution n assigns weight ex
en examples
ample n weight example n weight en
x xt
theorem kernel perceptron run sample polynomial size p n
probability least error final hypothesis least
proof probability first two examples received n
n thus probability two examples proof perceptron
w coefficients w equal
consider sequence examples following two examples first note
trial occurrence example n e occurrence xi
p
n example decrease n wt n since first two examples
p
w n n wt n follows least n examples
must occur n example incorrectly classified negative example since
consider performance p n n steps
may ignore subsequent occurrences n since change
hypothesis
observe first example n perform
demotion resulting w possibly changing coefficients well since
promotions performed rest sample get w rest
learning process follows future occurrences example n correctly
classified thus may ignore well
considering examples xi sequence constructed may ignore example correctly classified since update made follows
perceptron gone examples hypothesis formed demotions
examples sequence xi difference scenario
may make several demotions example occurs multiple times
sample however inspection proof shows x
p
p
seen bounds ai wt bi wt still valid


fiefficiency versus convergence boolean kernels

therefore xi misclassified since sample size p n sequence
size en probability weight examples sample sufficiently
large n error hypothesis least


computational hardness kernel winnow
section x n let x denote n element vector whose coordinates nonempty monomials monotone conjunctions x xn say
sequence labeled examples hx b hxt bt monotone consistent consistent
monotone function e xik xjk k n implies bi bj
monotone consistent labeled examples clearly monotone dnf
formula consistent contains conjunctions consider following

kernel winnow prediction kwp
instance monotone consistent sequence hx b hxt bt labeled examples
xi bi unlabeled example z
question w z w n dimensional hypothesis vector
generated running winnow example sequence h x b h xt bt
order run winnow nonempty monomials learn monotone dnf
one must able solve kwp efficiently main section proof
kwp computationally hard wide range parameter settings yield
polynomial mistake bound winnow via theorem
recall p class counting associated n p decision well known every function p computable polynomial time
p n p see book papadimitriou valiant details
p following p hard valiant
monotone sat sat
instance monotone cnf boolean formula f c c cr ci yi yi
yij yn integer k k n
question f k e f least k satisfying assignments n
theorem fix let n let let

max
n log poly polynomial time
kwp every function p computable polynomial time
proof n described theorem routine calculation shows
poly




poly
poly



proof reduction sat high level idea proof
simple let f k instance sat f defined variables yn
winnow maintains weight wt monomial variables x xn
define correspondence monomials truth assignments n


fikhardon roth servedio

f give sequence examples winnow causes wt f
wt f value w z thus related f note
could control well would sufficient since could use k
follow however parameter therefore make
additional updates w z f k w z
f k details somewhat involved since must track resolution
approximations different values final inner product indeed give
correct respect threshold
general setup construction detail let
u n dlog e log e
n
v log
e
u
w log
e

let defined
n u v n u w



since fact log x x x
log easily follows specified polynomial
n describe polynomial time transformation maps n variable instance f k
sat variable instance z kwp hx b hxt bt
monotone consistent xi z belong w z
f k
winnow variables x xm divided three sets b c
x xn b xn xn u c xn u xm unlabeled example z
n u mnu e variables b set variables c set
p
p
thus w z mb mab wt mb b wt
p
mab ab b wt refer monomials type monomials
monomials b type b monomials monomials ab b
type ab monomials
example sequence divided four stages stage f
described n variables correspond n variables cnf formula
f stage q f positive integer q specify later
stages together mb mab q k thus final value w z
approximately q f k w z f k
since variables c z includes variable c value wt
affect w z variables c slack variables make winnow
perform correct promotions demotions ii ensure monotone consistent
stage setting f define following correspondence
truth assignments n monomials yit xi
present clause yi yi f stage contains v negative examples
xi xi xi xi winnow makes
false positive prediction examples stage winnow never


fiefficiency versus convergence boolean kernels

promotion example variable set consider
f since examples include example monomial
demoted least v times stage w
f wt v f thus f
n v
stage examples cause winnow make false positive prediction
negative examples xi xi xi described
negative example stage six slack variables x x c
used follows stage dlog e repeated instances positive example
x x bits examples cause promotions
wx wx wx x hence wx two groups
similar examples first x x second x x cause
wx wx next example negative example
xi xi xi xi x x x bits
example w x wx wx wx winnow makes false positive
prediction
since f n clauses v negative examples per clause
construction carried v n slack variables xn u xn u v n
thus claimed
stage setting q f first stage example positive example
xi xi xn u v n bits since n
monomials contain xn u v n satisfied example wt
w x n f n since poly n recall
equation n resulting promotion w x
n f n let
q dlog n e

q n q n



stage consists q repeated instances positive example described
promotions w x q n f q n since
f n
q q f q n



equation gives value throughout rest argument
calculations stages start stage type b typeab monomial wt n variables u variables b
start stage mb u mab n u since example
stages satisfies xi end stage still q f
mab still n u therefore end stage
w z mb q f n u


fikhardon roth servedio

simplify notation let
n u q k
ideally end stage value mb would q since would imply
w z q f k least f k however
necessary mb assume exact value since f must integer
long


mb q

get

q f k w z q f k

f k clearly w z hand f k
since f integer value f k get w z therefore
remains construct examples stages b satisfies
equation
next calculate appropriate granularity note k n equation q k recall equations

n u n poly n u n poly n u consequently
certainly equation q n q
let
c dlog e


qc q




unique smallest positive integer p satisfies pqc q
stage examples mb satisfying p mb p

qc pqc q

q


q n qc


qc



c n









holds since k thus definition q
equivalent equation inequality follows equations
hence
p c n n c log e u



second inequality chain follows equation use
following lemma


fiefficiency versus convergence boolean kernels

lemma p monotone cnf f p
boolean variables clauses exactly p satisfying assignments
constructed p poly time
proof proof induction base case p f p x
assuming lemma true k prove k
p k desired cnf fk p xk fk p since fk p k
clauses fk p k clauses k p k desired cnf
fk p xk fk p k distributing xk clause fk p k write fk p
cnf k clauses p k fk p x

stage setting mb p let fu p r clause monotone cnf formula
u variables b p satisfying assignments similar stage clause
fu p stage w negative examples corresponding clause stage
slack variables c used ensure winnow makes false positive prediction
negative example thus examples stage cause mb p
u w since six slack variables c used negative example
rw u w negative examples slack variables xn u v n xm
sufficient stage
stage setting mb mab q k remains perform q c
promotions examples xi b set cause mb equal
p qc inequalities established give us


pqc p qc mb q qc q


desired
order guarantee q c promotions use two sequences examples length
n
u n
q du
log e log e c respectively first positive numbers
follows directly definitions u n dlog e log e c dlog e
n
n definition equation bounded
u
log c since
polynomial clearly log n u n log since
n
u n
n
q dlog n e implies q log
du
log e q log e
log
n
first q u
log e examples stage positive example
xi b set xm first time example received

w x u p u since n inspection u u
n
winnow performs promotion similarly q u
log e occurrences example

qd u n e
qd u n e
w x log u p log u q n

promotions indeed performed occurrence
mb

n
qd u
e
log

p

n
remaining examples stage u
log e c repetitions positive example x
xi b set xm promotions occurred repetition



fikhardon roth servedio

example would w x

n
du
ec
log

u

n
qd u
e
log

p need

quantity less reexpress quantity
qc p

n
ec u
du
log



qc p pqc qc

q

q


q







u n ec

follows definition c finally log u


u nc log u n
q last inequality equation
n
previous inequality inspection values u combining two
bounds see indeed w x
finally observe construction example sequence monotone consistent
since poly n contains poly n examples transformation sat
kwp polynomial time computable theorem proved
theorem

conclusion
linear threshold functions weak representation language interesting learning therefore linear learning learn expressive
functions necessary expand feature space applied
work explores tradeoff computational efficiency convergence
expanded feature spaces capture conjunctions base features
shown iteration kernel perceptron
executed efficiently provably require exponentially many updates even
learning function simple f x x x xn hand kernel
winnow polynomial mistake bound learning polynomial size monotone
dnf widely accepted computational hardness assumption
impossible efficiently simulate execution kernel winnow latter implies
general construction run winnow kernel functions
indicate additive multiplicative update lie opposite
extremes tradeoff computational efficiency convergence believe
fact could significant practical implications demonstrating provable limitations kernel functions correspond high degree feature expansions
lend theoretical justification common practice small degree
similar feature expansions well known polynomial kernel
since publication initial conference version work khardon roth
servedio several authors explored closely related ideas one
construction negative perceptron extend pac
boolean kernels different standard polynomial kernels conjunctions
weighted equally allow negations



fiefficiency versus convergence boolean kernels

online setting related support vector machines work constructing maximum margin hypothesis consistent examples khardon
servedio gives analysis pac learning performance maximum margin
monotone monomials kernel derives several negative thus
giving negative evidence monomial kernel cumby roth
kernel expressions description logic generalizing monomials kernel
developed successfully applied natural language molecular takimoto warmuth study use multiplicative update
winnow weighted majority obtain positive restricting
type loss function used additive base features chawla et al
studied monte carlo estimation approaches approximately simulate winnow performance run space exponentially many features use
kernel methods logic learning developing alternative methods feature expansion
multiplicative update remain interesting challenging
investigated

acknowledgments
work partly done khardon university edinburgh partly
servedio harvard university authors gratefully acknowledge financial
support work epsrc grant gr n nsf grant iis semester fellowship award tufts university khardon nsf grants itr iis itr iis iis roth nsf grant ccr nsf
mathematical sciences postdoctoral fellowship servedio

references
angluin negative equivalence queries machine learning
block h perceptron model brain functioning reviews modern
physics
carlson cumby c rosen j roth snow learning architecture
tech rep uiucdcs r uiuc computer science department
chawla li l scott approximating weighted sums exponentially
many terms journal computer system sciences
cristianini n shaw taylor j introduction support vector machines
cambridge press
cumby c roth kernel methods relational learning proc
international conference machine learning
kearns vazirani u introduction computational learning theory
mit press cambridge


fikhardon roth servedio

khardon r roth servedio r efficiency versus convergence boolean
kernels line learning dietterich g becker ghahramani
z eds advances neural information processing systems cambridge
mit press
khardon r servedio r maximum margin boolean kernels
proceedings sixteenth annual conference computational learning theory
pp
lint j v introduction coding theory springer verlag
littlestone n learning quickly irrelevant attributes abound linearthreshold machine learning
maass w warmuth k efficient learning virtual threshold gates
information computation
minsky papert perceptrons introduction computational geometry
mit press cambridge
novikoff convergence proofs perceptrons proceeding symposium
mathematical theory automata vol pp
papadimitriou c computational complexity addison wesley
rosenblatt f perceptron probabilistic model information storage
organization brain psychological review
roth learning resolve natural language ambiguities unified
proc american association artificial intelligence pp
sadohara k learning boolean functions support vector machines proc
conference algorithmic learning theory pp springer lnai
takimoto e warmuth path kernels multiplicative updates journal
machine learning
valiant l g complexity enumeration reliability siam
journal computing
valiant l g theory learnable communications acm

watkins c kernels matching operations tech rep csd tr computer
science department royal holloway university london




