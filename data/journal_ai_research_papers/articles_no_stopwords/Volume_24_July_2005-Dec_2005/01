Journal Artificial Intelligence Research 24 (2005) 49-79

Submitted 09/04; published 07/05

Framework Sequential Planning Multi-Agent Settings
Piotr J. Gmytrasiewicz
Prashant Doshi

PIOTR @ CS . UIC . EDU
PDOSHI @ CS . UIC . EDU

Department Computer Science
University Illinois Chicago
851 S. Morgan St
Chicago, IL 60607

Abstract
paper extends framework partially observable Markov decision processes (POMDPs)
multi-agent settings incorporating notion agent models state space. Agents
maintain beliefs physical states environment models agents,
use Bayesian updates maintain beliefs time. solutions map belief states actions.
Models agents may include belief states related agent types considered
games incomplete information. express agents autonomy postulating models directly manipulable observable agents. show important properties
POMDPs, convergence value iteration, rate convergence, piece-wise linearity convexity value functions carry framework. approach complements
traditional approach interactive settings uses Nash equilibria solution paradigm.
seek avoid drawbacks equilibria may non-unique capture
off-equilibrium behaviors. cost represent, process continuously
revise models agents. Since agents beliefs may arbitrarily nested, optimal solutions decision making problems asymptotically computable. However, approximate
belief updates approximately optimal plans computable. illustrate framework using
simple application domain, show examples belief updates value functions.

1. Introduction
develop framework sequential rationality autonomous agents interacting
agents within common, possibly uncertain, environment. use normative paradigm
decision-theoretic planning uncertainty formalized partially observable Markov decision
processes (POMDPs) (Boutilier, Dean, & Hanks, 1999; Kaelbling, Littman, & Cassandra, 1998;
Russell & Norvig, 2003) point departure. Solutions POMDPs mappings
agents beliefs actions. drawback POMDPs comes environments populated
agents agents actions represented implicitly environmental noise
within the, usually static, transition model. Thus, agents beliefs another agent part
solutions POMDPs.
main idea behind formalism, called interactive POMDPs (I-POMDPs), allow
agents use sophisticated constructs model predict behavior agents. Thus,
replace flat beliefs state space used POMDPs beliefs physical
environment agent(s), possibly terms preferences, capabilities,
beliefs. beliefs could include others beliefs others, thus nested arbitrary
levels. called interactive beliefs. space interactive beliefs rich
updating beliefs complex updating flat counterparts, use value
c
2005
AI Access Foundation. rights reserved.

fiG MYTRASIEWICZ & OSHI

function plots show solutions I-POMDPs least good as, usual cases superior
to, comparable solutions POMDPs. reason intuitive maintaining sophisticated models
agents allows refined analysis behavior better predictions actions.
I-POMDPs applicable autonomous self-interested agents locally compute actions execute optimize preferences given believe interacting
others possibly conflicting objectives. approach using decision-theoretic framework solution concept complements equilibrium approach analyzing interactions used
classical game theory (Fudenberg & Tirole, 1991). drawback equilibria could
many (non-uniqueness), describe agents optimal actions if, when,
equilibrium reached (incompleteness). approach, instead, centered optimality
best response anticipated action agent(s), rather stability (Binmore, 1990;
Kadane & Larkey, 1982). question whether, circumstances, kind
equilibria could arise solutions I-POMDPs currently open.
approach avoids difficulties non-uniqueness incompleteness traditional equilibrium approach, offers solutions likely better solutions traditional
POMDPs applied multi-agent settings. advantages come cost processing
maintaining possibly infinitely nested interactive beliefs. Consequently, approximate belief
updates approximately optimal solutions planning problems computable general.
define class finitely nested I-POMDPs form basis computable approximations infinitely nested ones. show number properties facilitate solutions POMDPs carry
finitely nested I-POMDPs. particular, interactive beliefs sufficient statistics
histories agents observations, belief update generalization update POMDPs,
value function piece-wise linear convex, value iteration algorithm converges
rate.
remainder paper structured follows. start brief review related
work Section 2, followed overview partially observable Markov decision processes
Section 3. There, include simple example tiger game. introduce concept
agent types Section 4. Section 5 introduces interactive POMDPs defines solutions.
finitely nested I-POMDPs, properties introduced Section 6. continue
example application finitely nested I-POMDPs multi-agent version tiger game
Section 7. There, show examples belief updates value functions. conclude
brief summary current research issues Section 8. Details proofs
Appendix.

2. Related Work
work draws prior research partially observable Markov decision processes,
recently gained lot attention within AI community (Smallwood & Sondik, 1973; Monahan,
1982; Lovejoy, 1991; Hausktecht, 1997; Kaelbling et al., 1998; Boutilier et al., 1999; Hauskrecht,
2000).
formalism Markov decision processes extended multiple agents giving rise
stochastic games Markov games (Fudenberg & Tirole, 1991). Traditionally, solution concept
used stochastic games Nash equilibria. recent work AI follows tradition
(Littman, 1994; Hu & Wellman, 1998; Boutilier, 1999; Koller & Milch, 2001). However,
mentioned before, pointed game theorists (Binmore, 1990; Kadane &
50

fiA F RAMEWORK EQUENTIAL P LANNING ULTI -AGENT ETTINGS

Larkey, 1982), Nash equilibria useful describing multi-agent system when, if,
reached stable state, solution concept sufficient general control paradigm.
main reasons may multiple equilibria clear way choose among
(non-uniqueness), fact equilibria specify actions cases agents believe
agents may act according equilibrium strategies (incompleteness).
extensions POMDPs multiple agents appeared AI literature recently (Bernstein,
Givan, Immerman, & Zilberstein, 2002; Nair, Pynadath, Yokoo, Tambe, & Marsella, 2003).
called decentralized POMDPs (DEC-POMDPs), related decentralized control
problems (Ooi & Wornell, 1996). DEC-POMDP framework assumes agents fully cooperative, i.e., common reward function form team. Furthermore, assumed
optimal joint solution computed centrally distributed among agents execution.
game-theoretic side, motivated subjective approach probability
games (Kadane & Larkey, 1982), Bayesian games incomplete information (see Fudenberg &
Tirole, 1991; Harsanyi, 1967, references therein), work interactive belief systems (Harsanyi,
1967; Mertens & Zamir, 1985; Brandenburger & Dekel, 1993; Fagin, Halpern, Moses, & Vardi,
1995; Aumann, 1999; Fagin, Geanakoplos, Halpern, & Vardi, 1999), insights research
learning game theory (Fudenberg & Levine, 1998). approach, closely related decisiontheoretic (Myerson, 1991), epistemic (Ambruster & Boge, 1979; Battigalli & Siniscalchi, 1999;
Brandenburger, 2002) approach game theory, consists predicting actions agents given
available information, choosing agents action (Kadane & Larkey, 1982).
Thus, descriptive aspect decision theory used predict others actions, prescriptive
aspect used select agents optimal action.
work presented extends previous work Recursive Modeling Method (RMM)
(Gmytrasiewicz & Durfee, 2000), adds elements belief update sequential planning.

3. Background: Partially Observable Markov Decision Processes
partially observable Markov decision process (POMDP) (Monahan, 1982; Hausktecht, 1997;
Kaelbling et al., 1998; Boutilier et al., 1999; Hauskrecht, 2000) agent defined
POMDPi = hS, Ai , Ti , , Oi , Ri

(1)

where: set possible states environment. Ai set actions agent execute. Ti
transition function Ti : Ai [0, 1] describes results agent actions.
set observations agent make. Oi agents observation function Oi : Ai
[0, 1] specifies probabilities observations given agents actions resulting states. Finally,
Ri reward function representing agent preferences R : Ai <.
POMDPs, agents belief state represented probability distribution S.
Initially, observations actions take place, agent (prior) belief, b 0i .
time steps, t, assume agent + 1 observations performed actions 1 .

assembled agent observation history: h ti = {o0i , o1i , .., ot1
, oi } time t. Let
Hi denote set observation histories agent i. agents current belief, b ti S,
continuously revised based new observations expected results performed actions. turns
1. assume action taken every time step; without loss generality since actions maybe
No-op.

51

fiG MYTRASIEWICZ & OSHI

agents belief state sufficient summarize past observation history
initial belief; hence called sufficient statistic.2
t1
belief update takes account changes initial belief, b t1
, due action, ai , executed


time 1, new observation, oi . new belief, bi , current state st , is:
bti (st ) = Oi (oti , st , at1
)

X

bit1 (st1 )Ti (st , ati , st1 )

(2)

st1

normalizing constant.
convenient summarize update performed states

bti = SE(bit1 , at1
, oi ) (Kaelbling et al., 1998).
3.1 Optimality Criteria Solutions
agents optimality criterion, OCi , needed specify rewards acquired time
handled. Commonly used criteria include:
finite horizon criterion,P
agent maximizes expected value sum
following rewards: E( Tt=0 rt ). Here, rt reward obtained time length
horizon. denote criterion fhT .
AnP
infinite horizon criterion discounting, according agent maximizes


E(
t=0 rt ), 0 < < 1 discount factor. denote criterion ih .

infinite horizon criterion averaging, according agent maximizes
average reward per time step. denote ihAV .

follows, concentrate infinite horizon criterion discounting, approach easily adapted criteria.
utility associated belief state, bi composed best immediate rewards
obtained bi , together discounted expected sum utilities associated
belief states following bi :

U (bi ) = max

ai Ai

X

bi (s)Ri (s, ai ) +

X

P r(oi |ai , bi )U (SEi (bi , ai , oi ))

oi

sS



(3)

Value iteration uses Equation 3 iteratively obtain values belief states longer time
horizons. step value iteration error current value estimate reduced
factor least (see example Russell & Norvig, 2003, Section 17.2.) optimal action, ,
element set optimal actions, OP (bi ), belief state, defined as:

OP (bi ) = argmax
ai Ai

X

bi (s)Ri (s, ai ) +

X

oi

sS

2. See (Smallwood & Sondik, 1973) proof.

52

P r(oi |ai , bi )U (SE(bi , ai , oi ))



(4)

fiA F RAMEWORK EQUENTIAL P LANNING ULTI -AGENT ETTINGS

L


OL

10

Value Function(U)

8

6

4

2

0

0

0.2

0.4

0.6

0.8

1

pp_i(TL)
(TL)
POMDP noise

POMDP

Figure 1: value function single agent tiger game time horizon length 1, OC = fh1 .
Actions are: open right door - OR, open left door - OL, listen - L. value
time horizon value function POMDP noise factor identical single
agent POMDP.

3.2 Example: Tiger Game
briefly review POMDP solutions tiger game (Kaelbling et al., 1998). purpose
build insights POMDP solutions provide simple case illustrate solutions
interactive versions game later.
traditional tiger game resembles game-show situation decision maker
choose open one two doors behind lies either valuable prize dangerous tiger.
Apart actions open doors, subject option listening tigers growl
coming left, right, door. However, subjects hearing imperfect, given
percentages (say, 15%) false positive false negative occurrences. Following (Kaelbling et al.,
1998), assume value prize 10, pain associated encountering
tiger quantified -100, cost listening -1.
value function, Figure 1, shows values various belief states agents time
horizon equal 1. Values beliefs based best action available belief state,
specified Eq. 3. state certainty valuable agent knows location
tiger open opposite door claim prize certainly awaits. Thus,
probability tiger location 0 1, value 10. agent sufficiently uncertain,
best option play safe listen; value -1. agent indifferent opening
doors listening assigns probabilities 0.9 0.1 location tiger.
Note that, time horizon equal 1, listening provide useful information
since game continue allow use information. longer time horizons
benefits results listening results policies better ranges initial belief.
Since value function composed values corresponding actions, linear prob53

fiG MYTRASIEWICZ & OSHI

L\();L\(GL),OL\(GR)

L\();L\(*)

L\();OR\(GL),L\(GR)
L\();OR\(*)
OR\();L\(*)

L\();OL\(*)
OL\();L\(*)

8

Value Function(U)

6

4

2

0

-2
0

0.2

0.4

0.6

0.8

1

pp_i(TL)
(TL)
POMDP noise

POMDP

Figure 2: value function single agent tiger game compared agent facing noise factor, horizon length 2. Policies corresponding value lines conditional plans.
Actions, L, OL, conditioned observational sequences parenthesis.
example L\();L\(GL),OL\(GR) denotes plan perform listening action, L,
beginning (list observations empty), another L observation growl
left (GL), open left door, OL, observation GR. wildcard
usual interpretation.

ability tiger location, value function property piece-wise linear convex
(PWLC) horizons. simplifies computations substantially.
Figure 2 present comparison value functions horizon length 2 single
agent, agent facing noisy environment. presence noise could
due another agent opening doors listening probabilities. 3 Since POMDPs
include explicit models agents, noise actions included transition
model, .
Consequences folding noise two-fold. First, effectiveness agents optimal
policies declines since value hearing growls diminishes many time steps. Figure 3 depicts
comparison value functions horizon length 3. Here, example, two consecutive growls
noisy environment valuable agent knows acting alone since noise
may perturbed state system growls. time horizon length 1
noise matter value vectors overlap, Figure 1.
Second, since presence another agent implicit static transition model, agent
cannot update model agents actions repeated interactions. effect becomes important time horizon increases. approach addresses issue allowing
explicit modeling agent(s). results policies superior quality, show
Section 7. Figure 4 shows policy agent facing noisy environment time horizon 3.
compare corresponding I-POMDP policy Section 7. Note slightly different
3. assumed that, due noise, either door opens probabilities 0.1 turn, nothing happens
probability 0.8. explain origin assumption Section 7.

54

fiA F RAMEWORK EQUENTIAL P LANNING ULTI -AGENT ETTINGS

L\();L\(*);OL\(GR;GR),L\(?)
L\();L\(GL),OL\(GR);OL\(GL;GR),L\(?)

L\();L\(*);OR\(GL;GL),L\(?)
L\();OR\(GL),L\(GR);OR\(GR;GL),L\(?)
L\();L\(*);OR\(GL;GL),OL\(GR;GR),L\(?)
OR\();L\(*);L\(*)
L\();L\(*);OR\(*)

OL\();L\(*);L\(*)
L\();L\(*);OL\(*)
8

Value Function(U)

7
6
5
4
3
2
1
0

0.2

0.4

0.6

0.8

1

p (TL)
p_i(TL)


POMDP noise

POMDP

Figure 3: value function single agent tiger game compared agent facing noise factor,
horizon length 3. ? description policy stands
perceptual sequences yet listed description policy.

[00.045)
OL
*

[0.0450.135) [0.1350.175) [0.1750.825)
L

L

GR

GL

GR

L
GR

GL

*

OL

L
GR

L
GL

GR


GL

*

GL

L
GL

[0.8650.955) [0.9551]

L

GR

OL

[0.8250.865)

L
*

L

GR


GL

*



Figure 4: policy graph corresponding value function POMDP noise depicted
Fig. 3.

55

fiG MYTRASIEWICZ & OSHI

policy without noise example Kaelbling, Littman Cassandra (1998) due
differences value functions.

4. Agent Types Frames
POMDP definition includes parameters permit us compute agents optimal behavior, 4
conditioned beliefs. Let us collect implementation independent factors construct
call agent type.
Definition 1 (Type). type agent is, = hbi , Ai , , Ti , Oi , Ri , OCi i, bi agent
state belief (an element (S)), OCi optimality criterion, rest elements
defined before. Let set agent types.
Given type, , assumption agent Bayesian-rational, set agents optimal
actions denoted OP (i ). next section, generalize notion type situations include interactions agents; coincides notion type used
Bayesian games (Fudenberg & Tirole, 1991; Harsanyi, 1967).
convenient define notion frame, bi , agent i:

b set
Definition 2 (Frame). frame agent is, bi = hAi , , Ti , Oi , Ri , OCi i. Let
agent frames.

brevity one write type consisting agents belief together frame: =
hbi , bi i.
context tiger game described previous section, agent type describes
agents actions results, quality agents hearing, payoffs, belief
tiger location.
Realistically, apart implementation-independent factors grouped type, agents behavior may depend implementation-specific parameters, processor speed, memory
available, etc. included (implementation dependent, complete) type, increasing accuracy predicted behavior, cost additional complexity. Definition use
complete types topic ongoing work.

5. Interactive POMDPs
mentioned, intention generalize POMDPs handle presence agents.
including descriptions agents (their types example) state space.
simplicity presentation, consider agent i, interacting one agent, j.
formalism easily generalizes larger number agents.
Definition 3 (I-POMDP). interactive POMDP agent i, I-POMDPi , is:
I-POMDPi = hISi , A, Ti , , Oi , Ri

(5)

4. issue computability solutions POMDPs subject much research (Papadimitriou & Tsitsiklis,
1987; Madani, Hanks, & Condon, 2003). obvious importance one uses POMDPs model agents;
return issue later.

56

fiA F RAMEWORK EQUENTIAL P LANNING ULTI -AGENT ETTINGS

where:
ISi set interactive states defined ISi = Mj ,5 interacting agent i,
set states physical environment, Mj set possible models agent
j. model, mj Mj , defined triple mj = hhj , fj , Oj i, fj : Hj (Aj )
agent js function, assumed computable, maps possible histories js observations
distributions actions. hj element Hj , Oj function specifying way
environment supplying agent input. Sometimes write model j mj = hhj ,
b j i,

b j consists fj Oj . convenient subdivide set models two classes.
subintentional models, SMj , relatively simple, intentional models, IMj , use
notion rationality model agent. Thus, Mj = IMj SMj .
Simple examples subintentional models include no-information model fictitious play
model, history independent. no-information model (Gmytrasiewicz & Durfee,
2000) assumes agents actions executed equal probability. Fictitious
play (Fudenberg & Levine, 1998) assumes agent chooses actions according fixed
unknown distribution, original agents prior belief distribution takes form
Dirichlet distribution.6 example powerful subintentional model finite state
controller.
intentional models sophisticated ascribe agent beliefs,
preferences rationality action selection.7 Intentional models thus js types, j = hbj , bj i,
assumption agent j Bayesian-rational.8 Agent js belief probability distribution
states environment models agent i; b j (S Mi ). notion type
use coincides notion type game theory, defined consisting
agent private information relevant decision making (Harsanyi, 1967; Fudenberg
& Tirole, 1991). particular, agents beliefs private information, types involve
possibly infinitely nested beliefs others types beliefs others (Mertens & Zamir,
1985; Brandenburger & Dekel, 1993; Aumann, 1999; Aumann & Heifetz, 2002). 9 related
recursive model structures prior work (Gmytrasiewicz & Durfee, 2000). definition
interactive state space consistent notion completely specified state space put forward
Aumann (1999). Similar state spaces proposed others (Mertens & Zamir, 1985;
Brandenburger & Dekel, 1993).
= Ai Aj set joint moves agents.
Ti transition model. usual way define transition probabilities POMDPs
assume agents actions change aspect state description. case IPOMDPs, would mean actions modifying aspect interactive states, including
agents observation histories functions, or, modeled intentionally, beliefs
reward functions. Allowing agents directly manipulate agents ways, however,
violates notion agents autonomy. Thus, make following simplifying assumption:
1
5. agents, say N > 2, ISi = N
j=1 Mj
6. Technically, according notation, fictitious play actually ensemble models.
7. Dennet (1986) advocates ascribing rationality agent(s), calls assuming intentional stance towards
them.
8. Note space types far richer computable models. particular, since set computable
models countable set types uncountable, many types computable models.
9. Implicit definition interactive beliefs assumption coherency (Brandenburger & Dekel, 1993).

57

fiG MYTRASIEWICZ & OSHI

Model Non-manipulability Assumption (MNM): Agents actions change
agents models directly.
Given simplification, transition model defined : [0, 1]
Autonomy, formalized MNM assumption, precludes, example, direct mind control,
implies agents belief states changed indirectly, typically changing
environment way observable them. words, agents beliefs change, POMDPs,
result belief update observation, direct result agents
actions.10
defined POMDP model.
Oi observation function. defining function make following assumption:
Model Non-observability (MNO): Agents cannot observe others models directly.
Given assumption observation function defined : [0, 1].
MNO assumption formalizes another aspect autonomy agents autonomous
observations functions, beliefs properties, say preferences, intentional
models, private agents cannot observe directly. 11
Ri defined Ri : ISi <. allow agent preferences physical
states models agents, usually physical state matter.
mentioned, see interactive POMDPs subjective counterpart objective external view stochastic games (Fudenberg & Tirole, 1991), followed work
AI (Boutilier, 1999) (Koller & Milch, 2001) decentralized POMDPs (Bernstein et al.,
2002; Nair et al., 2003). Interactive POMDPs represent individual agents point view
environment agents, facilitate planning problem solving agents
individual level.
5.1 Belief Update I-POMDPs
show that, POMDPs, agents beliefs interactive states sufficient
statistics, i.e., fully summarize agents observation histories. Further, need show
beliefs updated agents action observation, solutions defined.
t1
new belief state, bti , function previous belief state, bt1
, last action, ai ,

new observation, oi , POMDPs. two differences complicate belief
update compared POMDPs. First, since state physical environment depends
actions performed agents prediction physical state changes
made based probabilities various actions agent. probabilities others
actions obtained based models. Thus, unlike Bayesian stochastic games,
assume actions fully observable agents. Rather, agents attempt infer
actions agents performed sensing results environment. Second, changes
models agents included update. reflect others observations
and, modeled intentionally, update agents beliefs. case, agent
update beliefs agent based anticipates agent observes
10. possibility agents influence observational capabilities agents accommodated
including factors change sensing capabilities set S.
11. Again, possibility agents observe factors may influence observational capabilities agents
allowed including factors S.

58

fiA F RAMEWORK EQUENTIAL P LANNING ULTI -AGENT ETTINGS

updates. could expected, update possibly infinitely nested belief
others types is, general, asymptotically computable.
Proposition 1. (Sufficiency) interactive POMDP agent i, current belief, i.e., probability distribution set Mj , sufficient statistic past history observations.
t1
next proposition defines agent belief update function, b ti (ist ) = P r(ist |oti , at1
, bi ),
ist ISi interactive state. use belief state estimation function, SE , abt1
breviation belief updates individual states bti = SEi (bt1
, ai , oi ).
t1 t1
t1
t1
(bi , ai , oi , bi ) stand P r(bti |bi , ai , oti ). define set
type-dependent optimal actions agent, OP (i ).

Proposition 2. (Belief Update) MNM MNO assumptions, belief update function
interactive POMDP hISi , A, Ti , , Oi , Ri i, mj ist intentional, is:
bti (ist ) =
Ti

P

t1 )
bt1
(is

ist1 :m
b t1
=bjt
j
P
t1
t1
(s , , st )
otj

P

t1
t1 , ot )
P r(at1

j |j )Oi (s ,

at1
j
t1 t1
t1 , ot )

(b
j j , aj , oj , bj )Oj (s ,
j

(6)

=m
b tj ,
mj ist subintentional first summation extends ist1 :
b t1
j
t1
t1
t1
t1 t1
P r(at1
j |j ) replaced P r(aj |mj ), jt (bj , aj , oj , bj ) replaced


Kronecker delta function K (APPEND(ht1
j , oj ), hj ).

Above, bt1
btj belief elements jt1 jt , respectively, normalizing constant,
j
t1
t1
P r(at1
Bayesian rational agent described type
j |j ) probability aj
t1
t1
1
j . probability equal |OP (j )| aj OP (j ), equal zero otherwise.
define OP Section 5.2.12 case js subintentional model, = (s, mj ), ht1

j
respectively, observation
htj observation histories part mt1
,


j
j
j
t1
t1
t1
function mtj , P r(at1
|m
)


probability
assigned




j
j
j
j . APPEND returns
string second argument appended first. proofs propositions
Appendix.
Proposition 2 Eq. 6 lot common belief update POMDPs,
expected. depend agent observation transition functions. However, since agent
observations depend agent js actions, probabilities various actions j
included (in first line Eq. 6.) Further, since update agent js model depends
j observes, probabilities various observations j included (in second line
Eq. 6.) update js beliefs represented j term. belief update easily
generalized setting one agents co-exist agent i.
P
12. agents prior belief ISi given probability density function
ist1 replaced
:
, otj , btj ) takes form Dirac delta function argument bt1
, at1
integral. case jt (bt1
j
j
j
, otj ) btj ).
, at1
(SEjt (bt1
j
j

59

fiG MYTRASIEWICZ & OSHI

5.2 Value Function Solutions I-POMDPs
Analogously POMDPs, belief state I-POMDP associated value reflecting maximum payoff agent expect belief state:


P
P
b
ERi (is, ai )bi (is) +
U (i ) = max
P r(oi |ai , bi )U (hSEi (bi , ai , oi ), i)
(7)
ai Ai

oi



P
where, ERi (is, ai ) =
aj Ri (is, ai , aj )P r(aj |mj ). Eq. 7 basis value iteration IPOMDPs.
Agent optimal action, ai , case infinite horizon criterion discounting,
element set optimal actions belief state, OP (i ), defined as:


P
P
OP (i ) = argmax
ERi (is, ai )bi (is) +
P r(oi |ai , bi )U (hSEi (bi , ai , oi ), bi i)
ai Ai

oi



(8)
case belief update, due possibly infinitely nested beliefs, step value iteration
optimal actions asymptotically computable.

6. Finitely Nested I-POMDPs
Possible infinite nesting agents beliefs intentional models presents obvious obstacle
computing belief updates optimal solutions. Since models agents infinitely
nested beliefs correspond agent functions computable natural consider
finite nestings. follow approaches game theory (Aumann, 1999; Brandenburger & Dekel,
1993; Fagin et al., 1999), extend previous work (Gmytrasiewicz & Durfee, 2000), construct
finitely nested I-POMDPs bottom-up. Assume set physical states world S, two
agents j. Agent 0-th level beliefs, bi,0 , probability distributions S. 0-th level
types, i,0 , contain 0-th level beliefs, frames, analogously agent j. 0-level types
are, therefore, POMDPs.13 0-level models include 0-level types (i.e., intentional models)
subintentional models, elements SM . agents first level beliefs probability distributions
physical states 0-level models agent. agents first level types consist
first level beliefs frames. first level models consist types upto level 1
subintentional models. Second level beliefs defined terms first level models on.
Formally, define spaces:
ISi,0 = S,
j,0 = {hbj,0 , bj : bj,0 (ISj,0 )}, Mj,0 = j,0 SMj
ISi,1 = Mj,0 ,
j,1 = {hbj,1 , bj : bj,1 (ISj,1 )}, Mj,1 = j,1 Mj,0
.
.
.
.
.
.
ISi,l = Mj,l1 , j,l = {hbj,l , bj : bj,l (ISj,l )}, Mj,l = j,l Mj,l1
Definition 4. (Finitely Nested I-POMDP) finitely nested I-POMDP agent i, I-POMDP i,l , is:
I-POMDPi,l = hISi,l , A, Ti , , Oi , Ri
13. 0-level types agents actions folded , R functions noise.

60

(9)

fiA F RAMEWORK EQUENTIAL P LANNING ULTI -AGENT ETTINGS

parameter l called strategy level finitely nested I-POMDP. belief update,
value function, optimal actions finitely nested I-POMDPs computed using Equation 6
Equation 8, recursion guaranteed terminate 0-th level subintentional models.
Agents strategic capable modeling others deeper levels (i.e., levels
strategy level l), always boundedly optimal. such, agents
could fail predict strategy sophisticated opponent. fact computability
agent function implies agent may suboptimal interactions pointed
Binmore (1990), proved recently Nachbar Zame (1996). Intuitively,
difficulty agents unbounded optimality would include capability model
agents modeling original agent. leads impossibility result due self-reference,
similar Godels incompleteness theorem halting problem (Brandenburger,
2002). positive note, convergence results (Kalai & Lehrer, 1993) strongly suggest
approximate optimality achievable, although applicability work remains open.
mentioned, 0-th level types POMDPs. provide probability distributions
actions agent modeled level models strategy level 1. Given probability
distributions agents actions level-1 models solved POMDPs,
provide probability distributions yet higher level models. Assume number models
considered level bound number, . Solving I-POMDP i,l equivalent
solving O(M l ) POMDPs. Hence, complexity solving I-POMDPi,l PSPACE-hard
finite time horizons,14 undecidable infinite horizons, POMDPs.
6.1 Properties I-POMDPs
section establish two important properties, namely convergence value iteration
piece-wise linearity convexity value function, finitely nested I-POMDPs.
6.1.1 C ONVERGENCE



VALUE TERATION

agent I-POMDPi,l , show sequence value functions, {U n },
n horizon, obtained value iteration defined Eq. 7, converges unique fixed-point, U .
Let us define backup operator H : B B U n = HU n1 , B set
bounded value functions. order prove convergence result, first establish
properties H.
Lemma 1 (Isotonicity). finitely nested I-POMDP value functions V U , V U ,
HV HU .
proof lemma analogous one due Hauskrecht (1997), POMDPs.
sketched Appendix. Another important property exhibited backup operator
property contraction.
Lemma 2 (Contraction). finitely nested I-POMDP value functions V , U discount
factor (0, 1), ||HV HU || ||V U ||.
proof lemma similar corresponding one POMDPs (Hausktecht,
1997). proof makes use Lemma 1. || || supremum norm.
14. Usually PSPACE-complete since number states I-POMDPs likely larger time horizon
(Papadimitriou & Tsitsiklis, 1987).

61

fiG MYTRASIEWICZ & OSHI

contraction property H, noting space value functions along
supremum norm forms complete normed space (Banach space), apply Contraction
Mapping Theorem (Stokey & Lucas, 1989) show value iteration I-POMDPs converges
unique fixed point (optimal solution). following theorem captures result.
Theorem 1 (Convergence). finitely nested I-POMDP, value iteration algorithm starting arbitrary well-defined value function converges unique fixed-point.
detailed proof theorem included Appendix.
case POMDPs (Russell & Norvig, 2003), error iterative estimates, U n ,
finitely nested I-POMDPs, i.e., ||U n U ||, reduced factor least iteration.
Hence, number iterations, N , needed reach error is:
N = dlog(Rmax /(1 ))/ log(1/)e

(10)

Rmax upper bound reward function.
6.1.2 P IECEWISE L INEARITY



C ONVEXITY

Another property carries POMDPs finitely nested I-POMDPs piecewise
linearity convexity (PWLC) value function. Establishing property allows us decompose I-POMDP value function set alpha vectors, represents policy
tree. PWLC property enables us work sets alpha vectors rather perform value
iteration continuum agents beliefs. Theorem 2 states PWLC property
I-POMDP value function.
Theorem 2 (PWLC). finitely nested I-POMDP, U piecewise linear convex.
complete proof Theorem 2 included Appendix. proof similar one
due Smallwood Sondik (1973) POMDPs proceeds induction. basis case
established considering horizon 1 value function. Showing PWLC inductive step
requires substituting belief update (Eq. 6) Eq. 7, followed factoring belief
terms equation.

7. Example: Multi-agent Tiger Game
illustrate optimal sequential behavior agents multi-agent settings apply I-POMDP
framework multi-agent tiger game, traditional version described before.
7.1 Definition
Let us denote actions opening doors listening OR, OL L, before. TL
TR denote states corresponding tiger located behind left right door, respectively.
transition, reward observation functions depend actions agents. Again,
assume tiger location chosen randomly next time step agents opened
doors current step. assume agent hears tigers growls, GR GL,
accuracy 85%. make interaction interesting added observation
door creaks, depend action executed agent. Creak right, CR, likely due
62

fiA F RAMEWORK EQUENTIAL P LANNING ULTI -AGENT ETTINGS

agent opened right door, similarly creak left, CL. Silence, S, good
indication agent open doors listened instead. assume accuracy
creaks 90%. assume agents payoffs analogous single agent versions
described Section 3.2 make cases comparable. Note result assumption
agents actions impact original agents payoffs directly, rather indirectly
resulting states matter original agent. Table 1 quantifies factors.

hai , aj
hOL,
hOR,
h, OLi
h, ORi
hL, Li
hL, Li

State
*
*
*
*
TL
TR

TL
0.5
0.5
0.5
0.5
1.0
0

TR
0.5
0.5
0.5
0.5
0
1.0

hai , aj
hOR, ORi
hOL, OLi
hOR, OLi
hOL, ORi
hL, Li
hL, ORi
hOR, Li
hL, OLi
hOL, Li

Transition function: Ti = Tj

TL
10
-100
10
-100
-1
-1
10
-1
-100

TR
-100
10
-100
10
-1
-1
-100
-1
10

hai , aj
hOR, ORi
hOL, OLi
hOR, OLi
hOL, ORi
hL, Li
hL, ORi
hOR, Li
hL, OLi
hOL, Li

TL
10
-100
-100
10
-1
10
-1
-100
-1

TR
-100
10
10
-100
-1
-100
-1
10
-1

Reward functions agents j

hai , aj
hL, Li
hL, Li
hL, OLi
hL, OLi
hL, ORi
hL, ORi
hOL,
hOR,

State
TL
TR
TL
TR
TL
TR



h GL, CL
0.85*0.05
0.15*0.05
0.85*0.9
0.15*0.9
0.85*0.05
0.15*0.05
1/6
1/6

h GL, CR
0.85*0.05
0.15*0.05
0.85*0.05
0.15*0.05
0.85*0.9
0.15*0.9
1/6
1/6

h GL,
0.85*0.9
0.15*0.9
0.85*0.05
0.15*0.05
0.85*0.05
0.15*0.05
1/6
1/6

h GR, CL
0.15*0.05
0.85*0.05
0.15*0.9
0.85*0.9
0.15*0.05
0.85*0.05
1/6
1/6

h GR, CR
0.15*0.05
0.85*0.05
0.15*0.05
0.85*0.05
0.15*0.9
0.85*0.9
1/6
1/6

h GR,
0.15*0.9
0.85*0.9
0.15*0.05
0.85*0.05
0.15*0.05
0.85*0.05
1/6
1/6

hai , aj
hL, Li
hL, Li
hOL, Li
hOL, Li
hOR, Li
hOR, Li
h, OLi
h, ORi

State
TL
TR
TL
TR
TL
TR



h GL, CL
0.85*0.05
0.15*0.05
0.85*0.9
0.15*0.9
0.85*0.05
0.15*0.05
1/6
1/6

h GL, CR
0.85*0.05
0.15*0.05
0.85*0.05
0.15*0.05
0.85*0.9
0.15*0.9
1/6
1/6

h GL,
0.85*0.9
0.15*0.9
0.85*0.05
0.15*0.05
0.85*0.05
0.15*0.05
1/6
1/6

h GR, CL
0.15*0.05
0.85*0.05
0.15*0.9
0.85*0.9
0.15*0.05
0.85*0.05
1/6
1/6

h GR, CR
0.15*0.05
0.85*0.05
0.15*0.05
0.85*0.05
0.15*0.9
0.85*0.9
1/6
1/6

h GR,
0.15*0.9
0.85*0.9
0.15*0.05
0.85*0.05
0.15*0.05
0.85*0.05
1/6
1/6

Observation functions agents j.
Table 1: Transition, reward, observation functions multi-agent Tiger game.
agent makes choice multi-agent tiger game, considers believes
location tiger, well whether agent listen open door,
turn depends agents beliefs, reward function, optimality criterion, etc. 15 particular,
agent open doors tiger location next time step would
chosen randomly. Thus, information obtained hearing previous growls would
discarded. simplify situation considering I-POMDP single level nesting,
assuming agent js properties, except beliefs, known i, js time
horizon equal is. words, uncertainty pertains js beliefs
frame. Agent interactive state space is, ISi,1 = j,0 , physical state, S={TL,
15. assume intentional model agent here.

63

fiG MYTRASIEWICZ & OSHI

TR}, j,0 set intentional models agent js, differs js beliefs
location tiger.
7.2 Examples Belief Update
Section 5, presented belief update equation I-POMDPs (Eq. 6). consider
examples beliefs, bi,1 , agent i, probability distributions j,0 . 0-th
level type agent j, j,0 j,0 , contains flat belief location tiger,
represented single probability assignment bj,0 = pj (T L).
0.506
0.504

0.504

Pr(TL,p
Pr(TL,b_j) )
j

Pr(TL,p
Pr(TL,b_j)

j

)

0.506
0.502
0.5
0.498

0.502
0.5
0.498

0.496
0.496

0.494
0

0.2

0.4

0.6

0.8

1

0.494
0

0.2

0.4

0.6

0.8

1

0.8

1

pb_j
j (TL)

0.506

0.506

0.504

0.504

j

)

0.502

Pr(TR,p
Pr(TR,b_j)

Pr(TR,p
Pr(TR,b_j)

j

)

pjb_j
(TL)
j(TL)

0.5
0.498

0.502
0.5
0.498

0.496
0.494

0.496

0

0.2

0.4 0.6 0.8
ppb_j
(TL)
(TR)
(TL)

1
0.494

jj

0

0.2

0.4

0.6

p j (TL)
b_j

(i)

(ii)

Figure 5: Two examples singly nested belief states agent i. case information
tigers location. (i) agent knows j know location
tiger; single point (star) denotes Dirac delta function integrates height
point, 0.5 . (ii) agent uninformed js beliefs tigers location.

Fig. 5 show examples level 1 beliefs agent i. case know
location tiger marginals top bottom sections figure sum
0.5 probabilities TL TR each. Fig. 5(i), knows j assigns 0.5 probability tiger
behind left door. represented using Dirac delta function. Fig. 5(ii), agent
uninformed js beliefs. represented uniform probability density values
probability j could assign state TL.
make presentation belief update transparent decompose formula
Eq. 6 two steps:
64

fiA F RAMEWORK EQUENTIAL P LANNING ULTI -AGENT ETTINGS

t1
Prediction: agent performs action at1
, given agent j performs aj ,
predicted belief state is:

bbt (ist ) = P r(ist |at1 , at1 , bt1 ) = P t1 bt1 bt bt1 (ist1 )P r(at1 |t1 )

j
j

j


|j =j
P
(st1 , at1 , st ) Oj (st , at1 , otj )

(11)

otj

t1
jt (bt1
j , j , j , bj )

Correction: agent perceives observation, ti , predicted belief states,
t1 t1
P r(|at1
, aj , bi ), combined according to:
bti (ist ) = P r(ist |oti , ait1 , bt1
)=

X

t1 t1
Oi (st , at1 , oti )P r(ist |at1
, j , bi )

(12)

at1
j

normalizing constant.

t1



0.496
0.494
0

0.2

0.4 0.6 0.8
pb_j(TL)

1

j

0.496
0.494
0.4 0.6 0.8
pb_j(TL)

1

0.8

1

0.7
0.6
0.5
0.4
0.3
L,<GL,S>
0.2
0.1
L,<GL,S> 0
0
0.8
1

L,<GL,S>

0

0.2

0.4

0.6

pjb_j(TL)

<GL,S>

<GL,S>
0.2

0.4

0.6

0.8

L,<GL,S>

0.1

L,<GL,S>

0.06

0.8

1

0.4 0.6 0.8
pb_j(TL)

1

0.02

0.01

0.005

0.04

L,<GL,S>
0.2

0.4

pjb_j
(TL)

0.6

pjb_j
(TL)

(b)

0.6

pjb_j(TL)

0.015

0.08

0

1

0.4

0.025

0.12

0.02
0

0.2

L,<GL,S>

0.14

0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05

j

(a)

0.6

pjb_j
(TL)

0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1

j)

j)
0.4

Pr(TR,pj )

0.498

Pr(TR,b_j)

j)

Pr(TR,p
Pr(TR,b_j)

L,(L,GL)

0.5

0.2

0.2

L,(L,GR)

0.504

0

<GL,S>

0

0.506
0.502

L,<GL,S>

0.45
0.4
0.35
0.3
0.25
0.2
0.15
0.1
0.05

Pr(TL,p
Pr(TL,b_j)

L,(L,GR)

0.5
0.498

bi

Pr(TR,b_j)
Pr(TR,p
)
j

0.502

t+1

bi

<GL,S>

Pr(TL,p
Pr(TL,b_j)

0.504

Pr(TL,p
Pr(TL,b_j) )
j

Pr(TL,p
Pr(TL,b_j)

j)

0.506



bi

L,(L,GL)

Pr(TR,p
Pr(TR,b_j) )
j

bi

(c)

0.8

1

0
0

0.2

j

(d)

Figure 6: trace belief update agent i. (a) depicts prior. (b) result prediction
given listening action, L, pair denoting js action observation. knows
j listen could hear tigers growl right left, probabilities
j would assign TL 0.15 0.85, respectively. (c) result correction
observes tigers growl left creaks, hGL,Si. probability assigns
TL greater TR. (d) depicts results another update (both prediction
correction) another listen action observation, hGL,Si.
discrete point denotes, again, Dirac delta function integrates height
point.
Fig. 6, display example trace update singly nested belief. first
column Fig. 6, labeled (a), example agent prior belief introduced before, according
65

fiG MYTRASIEWICZ & OSHI

knows j uninformed location tiger. 16 Let us assume listens
hears growl left creaks. second column Fig. 6, (b), displays predicted
belief performs listen action (Eq. 11). part prediction step, agent must solve
js model obtain js optimal action belief 0.5 (term P r(a t1
j |j ) Eq. 11). Given
value function Fig. 3, evaluates probability 1 listen action, zero opening
doors. updates js belief given j listens hears tiger growling either
t1
left, GL, right, GR, (term jt (bt1
j , aj , oj , bj ) Eq. 11). Agent js updated probabilities
tiger left 0.85 0.15, js hearing GL GR, respectively. tiger
left (top Fig. 6 (b)) js observation GL likely, consequently js assigning
probability 0.85 state TL likely (i assigns probability 0.425 state.)
tiger right j likely hear GR assigns lower probability, 0.075,
js assigning probability 0.85 tiger left. third column, (c), Fig. 6 shows
posterior belief correction step. belief column (b) updated account
hearing growl left creaks, hGL,Si. resulting marginalised probability
tiger left higher (0.85) tiger right. assume
next time step listens hears tiger growling left creaks, belief
state depicted fourth column Fig. 6 results.
Fig. 7 show belief update starting prior Fig. 5 (ii), according
agent initially information j believes tigers location.
traces belief updates Fig. 6 Fig. 7 illustrate changing state information agent
agents beliefs. benefit representing updates explicitly that,
stage, optimal behavior depends estimate probabilities js actions.
informative estimates value agent expect interaction. Below,
show increase value function I-POMDPs compared POMDPs noise factor.
7.3 Examples Value Functions
section compares value functions obtained solving POMDP static noise factor,
accounting presence another agent,17 value functions level-1 I-POMDP. advantage refined modeling update I-POMDPs due two factors. First ability
keep track agents state beliefs better predict future actions. second
ability adjust agents time horizon number steps go interaction
decreases. Neither possible within classical POMDP formalism.
continue simple example I-POMDPi,1 agent i. Fig. 8 display
value function time horizon 1, assuming initial belief value j assigns
TL, pj (T L), depicted Fig. 5 (ii), i.e. information j believes
tigers location. value function identical value function obtained agent using
traditional POMDP framework noise, well single agent POMDP described
Section 3.2. value functions overlap since agents update beliefs
advantage refined modeling agent j I-POMDP become apparent. Put
another way, agent models j using intentional model, concludes agent j open
door probability 0.1 listen probability 0.8. coincides noise factor
described Section 3.2.
16. points Fig. 7 denote Dirac delta functions integrate value equal points height.
17. POMDP noise level-0 I-POMDP.

66

fiA F RAMEWORK EQUENTIAL P LANNING ULTI -AGENT ETTINGS

t1


bi

bi

L(L,GR)
L(L,GL)
0.5

0.5

L(L,GL)
0.4

0.4

L(L,GR)

0.2

0.5

0.3

0.2

0.498
0.1

0.1

L(L,GR)

0.496
0.494
0

0.2

0.4

0.6

0.8

0

L(OL/OR,*)

1

pjb_j(TL)

0

0.504
0.502

0.494
0

0.2

0.4

0.6

0.8

1

0.8

0

1

0

Pr(TL,b_j)

L(L,GR)

pjb_j(TL)

0.4

0.6

0.8

1

pjb_j
(TL)
0.02275
0.0227
0.02265

0.0226

0.0226

0.02255
0.0225

0.02245

0.02255
0.0225
0.02245

0.0224

0.0224

0.02235

0.02235

0.0223

L(OL/OR,*)

0.2

0.02265

Pr(TL, pj )

L(OL/OR,*)

0.496

0.6

0.0227

L(OL/OR,*)

0.498

0.4

0.02275

L(L,GL)

0.5

0.2

pjb_j
(TL)

L(L,GL)

0.506

Pr(TR,
pj )
Pr(TR,b_j)

0.3

Pr(TR,b_j)
Pr(TR,p
j)

Pr(TL,p
)
Pr(TL,b_j)
j

0.502

Pr(TR,b_j)
Pr(TR,p
j)

0.504

Pr(TL,b_j)
Pr(TL,
pj )

0.506

0.0223

0.02225

0.02225
0

0.2

0.4

0.6

0.8

1

0

pjb_j
(TL)

(a)

0.2

0.4

0.6

0.8

1

pjb_j
(TL)

(b)
<GL,S>



bi

0.3

1.8

1.6
0.25

Pr(TR,pj )

1.2

Pr(TR,b_j)

Pr(TL,p
)
Pr(TL,b_j)
j

1.4

1

0.8

0.2

0.15

0.1

0.6

0.4
0.05
0.2
0

0
0

0.2

0.4

0.6

pjb_j
(TL)

0.8

0

1

0.2

0.4

0.6

0.8

1

pj b_j
(TL)

(c)

Figure 7: trace belief update agent i. (a) depicts prior according
uninformed js beliefs. (b) result prediction step listening
action (L). top half (b) shows belief listened given j
listened. two observations j make, GL GR, probability dependent
tigers location, give rise flat portions representing knows js belief
case. increased probability assigns js belief 0.472 0.528
due js updates hears GL hears GR resulting values
interval. bottom half (b) shows belief listened j opened
left right door (plots identical action one shown).
knows j information tigers location case. (c) result
correction observes tigers growl left creaks hGL,Si. plots (c)
obtained performing weighted summation plots (b). probability
assigns TL greater TR, information js beliefs allows refine
prediction js action next time step.

67

fiG MYTRASIEWICZ & OSHI

L


OL

10

Value Function (U)

8

6

4

2

0

0

0.2

0.4

0.6

0.8

1

pp_i(TL)
(TL)
Level 1 I-POMDP

POMDP noise

Figure 8: time horizon 1 value functions obtained solving singly nested I-POMDP
POMDP noise factor overlap.
L\();OL\(<GR,S>),L\(?)

L\();OR\(<GL,S>),L\(?)

L\();L\(<GL,*>),OL\(<GR,*>)

OL\();L\(*)

L\();L\(*)

L\();OR\(<GL,*>),L\(<GR,*>)

L\();L\(GL),OL\(GR)

L\();OR\(GL),L\(GR)

OR\();L\(*)

8

Value Function (U)

6

4

2

0

-2
0

0.2

0.4

0.6

0.8

1

pp_i(TL)
(TL)
Level 1 I-POMDP

POMDP noise

Figure 9: Comparison value functions obtained solving I-POMDP POMDP
noise time horizon 2. I-POMDP value function dominates due agent adjusting
behavior agent j remaining steps go interaction.

68

fiA F RAMEWORK EQUENTIAL P LANNING ULTI -AGENT ETTINGS

8

Value Function (U)

7
6
5
4
3
2
1
0

0.2

0.4

0.6

0.8

1

p (TL)
p_i(TL)
Level 1 I-POMDP

POMDP noise

Figure 10: Comparison value functions obtained solving I-POMDP POMDP
noise time horizon 3. I-POMDP value function dominates due agent adjusting js remaining steps go, due modeling js belief update. factors
allow better predictions js actions interaction. descriptions individual policies omitted clarity; read Fig. 11.

Fig. 9 display value functions time horizon 2. value function
I-POMDPi,1 higher value function POMDP noise factor. reason
related advantages modeling agent js beliefs effect becomes apparent time
horizon 3 longer. Rather, I-POMDP solution dominates due agent modeling js time
horizon interaction: knows last time step j behave according optimal
policy time horizon 1, two steps go j optimize according 2 steps go
policy. mentioned, effect cannot modeled using POMDP static noise factor
included transition function.
Fig. 10 shows comparison I-POMDP noisy POMDP value functions
horizon 3. advantage refined agent modeling within I-POMDP framework
increased.18 factors, adjusting js steps go modeling js belief update
interaction responsible superiority values achieved using I-POMDP. particular,
recall second time step information js beliefs tigers location
depicted Fig. 7 (c). enables make high quality prediction that, two steps left
go, j perform actions OL, L, probabilities 0.009076, 0.96591 0.02501,
respectively (recall POMDP noise probabilities remained unchanged 0.1, 0,8,
0.1, respectively.)
Fig. 11 shows agent policy graph time horizon 3. usual, prescribes optimal
first action depending initial belief tigers location. subsequent actions depend
observations received. observations include creaks indicative agents
18. Note I-POMDP solution good solution POMDP agent operating alone environment shown Fig. 3.

69

fiG MYTRASIEWICZ & OSHI

[0 0.029)
OL

[0.029 0.089)

[0.089 0.211)

L
*

L

<GR,S>

<GL,CL/CR>
<GR,*>

[0.211 0.789)

[0.789 0.911)

L

*

OL


*

<GR,S>
<GL,CL\CR>

L

L
<GR,*>

[0.971 1]

L

<GR,CL\CR>
<GR,CL\CR>
<GL,CL\CR>
<GL,S>
<GR,*> <GL,*>
<GL,*>
<GR,S>
<GL,S>

<GL,S>
<GR,CL\CR>

OL

[0.911 0.971)

L

<GL,*>

L
*

<GR,*>


<GL,*>

*



L

Figure 11: policy graph corresponding I-POMDP value function Fig. 10.
opened door. creaks contain valuable information allow agent make
refined choices, compared ones noisy POMDP Fig. 4. Consider case agent
starts fairly strong belief tigers location, decides listen (according four
off-center top row L nodes Fig. 11) hears door creak. agent position
open either left right door, even counter initial belief. reason
creak indication tigers position likely reset agent j j
open doors following two time steps. Now, two growls coming
door lead enough confidence open door. agent hearing
tigers growls indicative tigers position state following agents actions,
Note value functions policy depict special case agent
information probability j assigns tigers location (Fig. 5 (ii)). Accounting
visualizing possible beliefs js beliefs difficult due complexity
space interactive beliefs. ongoing work indicates, drastic reduction complexity
possible without loss information, consequently representation solutions manageable
number dimensions indeed possible. report results separately.

8. Conclusions
proposed framework optimal sequential decision-making suitable controlling autonomous
agents interacting agents within uncertain environment. used normative
paradigm decision-theoretic planning uncertainty formalized partially observable Markov
decision processes (POMDPs) point departure. extended POMDPs cases agents
interacting agents allowing beliefs physical environment, agents. could include beliefs others abilities, sensing
capabilities, beliefs, preferences, intended actions. framework shares numerous properties
POMDPs, analogously defined solutions, reduces POMDPs agents alone
environment.
contrast recent work DEC-POMDPs (Bernstein et al., 2002; Nair et al., 2003),
work motivated game-theoretic equilibria (Boutilier, 1999; Hu & Wellman, 1998; Koller
70

fiA F RAMEWORK EQUENTIAL P LANNING ULTI -AGENT ETTINGS

& Milch, 2001; Littman, 1994), approach subjective amenable agents independently
computing optimal solutions.
line work presented opens area future research integrating frameworks
sequential planning elements game theory Bayesian learning interactive settings.
particular, one avenues future research centers proving formal properties
I-POMDPs, establishing clearer relations solutions I-POMDPs various flavors
equilibria. Another concentrates developing efficient approximation techniques solving
I-POMDPs. POMDPs, development approximate approaches I-POMDPs crucial
moving beyond toy problems. One promising approximation technique working particle
filtering. devising methods representing I-POMDP solutions without assumptions
whats believed agents beliefs. mentioned, spite complexity
interactive state space, seem intuitive representations belief partitions corresponding
optimal policies, analogous POMDPs. research issues include suitable
choice priors models,19 ways fulfill absolute continuity condition needed
convergence probabilities assigned alternative models interactions (Kalai & Lehrer,
1993).

Acknowledgments
research supported National Science Foundation CAREER award IRI-9702132,
NSF award IRI-0119270.

Appendix A. Proofs
Proof Propositions 1 2. start Proposition 2, applying Bayes Theorem:

t1
bti (ist ) = P r(ist |oti , at1
, bi ) =

)
,bt1
P r(ist ,oti |at1


t1 t1

P r(oi |ai ,bi )

P
t1 )
= ist1 bit1 (ist1 )P r(ist , oti |at1
,
P
P
t1
t1 )P r(at1 |at1 , ist1 )
= ist1 bit1 (ist1 ) at1 P r(ist , oti |at1
, aj ,
j

j
P
P
(13)
t1
t1 )P r(at1 |ist1 )
,

,

= ist1 bit1 (ist1 ) at1 P r(ist , oti |at1
j
j

j
P
P
t1

t1 , ist1 )P r(ist |at1 , ist1 )
= ist1 bit1 (ist1 ) at1 P r(at1
j |mj )P r(ot |is ,
j
P
P
t1

t1 )P r(ist |at1 , ist1 )
= ist1 bit1 (ist1 ) at1 P r(at1
j |mj )P r(ot |is ,
j
P
P
t1
t1 , ot )P r(ist |at1 , ist1 )
= ist1 bit1 (ist1 ) at1 P r(at1

j |mj )Oi (s ,
j

19. looking Kolmogorov complexity (Li & Vitanyi, 1997) possible way assign priors.

71

fiG MYTRASIEWICZ & OSHI

simplify term P r(ist |at1 , ist1 ) let us substitute interactive state ist components. mj interactive states intentional: ist = (st , jt ) = (st , btj , bjt ).

P r(ist |at1 , ist1 ) = P r(st , btj , bjt |at1 , ist1 )
= P r(btj |st , bjt , at1 , ist1 )P r(st , bjt |at1 , ist1 )
= P r(btj |st , bjt , at1 , ist1 )P r(bjt |st , at1 , ist1 )P r(st |at1 , ist1 )
= P r(btj |st , bjt , at1 , ist1 )I(bjt1 , bjt )Ti (st1 , at1 , st )
(14)
b tj ).
mj subintentional: ist = (st , mtj ) = (st , htj ,

P r(ist |at1 , ist1 ) = P r(st , htj ,
b tj |at1 , ist1 )
b tj |at1 , ist1 )
b tj , at1 , ist1 )P r(st ,
= P r(htj |st ,
b tj , at1 , ist1 )P r(bjt |st , at1 , ist1 )P r(st |at1 , ist1 )
= P r(htj |st ,


= P r(hj |s ,
b tj , at1 , ist1 )I(m
b tj )Ti (st1 , at1 , st )
b t1
(14)
j ,m

joint action pair, at1 , may change physical state. third term right-hand
side Eqs. 14 140 captures transition. utilized MNM assumption replace
second terms equations boolean identity functions, I( bjt1 , bjt ) I(m
b t1
b tj )
j ,m
respectively, equal 1 two frames identical, 0 otherwise. Let us turn attention
first terms. mj ist ist1 intentional:
P
P r(btj |st , bjt , at1 , ist1 ) = ot P r(btj |st , bjt , at1 , ist1 , otj )P r(otj |st , bjt , at1 , ist1 )
Pj
= ot P r(btj |st , bjt , at1 , ist1 , otj )P r(otj |st , bjt , at1 )
Pj
t1
t1 , ot )
= ot jt (bt1
j
j , aj , oj , bj )Oj (st ,

(15)

j

Else subintentional:

P r(htj |st ,
b tj , at1 , ist1 ) =

=

=

P



Po j


Po j
otj

b tj , at1 , ist1 )
b tj , at1 , ist1 , otj )P r(otj |st ,
P r(htj |st ,

b tj , at1 )
b tj , at1 , ist1 , otj )P r(otj |st ,
P r(htj |st ,



t1 , ot )
K (APPEND(ht1
j
j , oj ), hj )Oj (st ,

(15)

t1
Eq. 15, first term right-hand side 1 agent js belief update, SE j (bt1
j , j , oj )
generates belief state equal btj . Similarly, Eq. 150 , first term 1 appending otj
ht1
results htj . K Kronecker delta function. second terms right-hand
j
side equations, MNO assumption makes possible replace P r(o |st , bt , at1 )
j

j

Oj (st , at1 , otj ), P r(otj |st ,
b tj , at1 ) Oj (st , at1 , otj ) respectively.
Let us substitute Eq. 15 Eq. 14.
P
t1
t1 , ot )I(
bt1 , bt )Ti (st1 , at1 , st )
P r(ist |at1 , ist1 ) = ot jt (bt1
j
j
j , aj , oj , bj )Oj (s ,
j
j
(16)
0
0
Substituting Eq. 15 Eq. 14 get,
P


t1 , ot )I(m
P r(ist |at1 , ist1 ) = ot K (APPEND(ht1
b t1
b tj )
j
j , oj ), hj )Oj (s ,
j ,m
j

Ti (st1 , at1 , st )

(16)

72

fiA F RAMEWORK EQUENTIAL P LANNING ULTI -AGENT ETTINGS

Replacing Eq. 16 Eq. 13 get:
P
P
t1
t1
t1 )
P r(at1
|jt1 )Oi (st , at1 , oti ) ot jt (bt1
ist1 bi (is
j , j , j , bj )
j
at1
j
j
Oj (st , at1 , otj )I(bjt1 , bjt )Ti (st1 , at1 , st )

bti (ist ) =

P

(17)

Similarly, replacing Eq. 160 Eq. 13 get:

P
P
t1
t1 , ot )
t1 )
P r(at1
bti (ist ) = ist1 bt1

j |mj )Oi (s ,
(is
at1
j
P
t1
t1


t1

bj ,m
b tj )Ti (st1 , at1 , st )
ot K (APPEND(hj , oj ), hj )Oj (s , , oj )I(m
j

arrive final expressions belief update removing terms
I(m
b t1
b tj ) changing scope first summations.
j ,m
mj interactive states intentional:

I( bjt1 , bjt )

P
P
t1
t1 , ot )
bt1 (ist1 ) at1 P r(at1
bti (ist ) = ist1 :m

j |j )Oi (s ,
b t1
=bjt
j
j
P
t1 t1
t1
t1


t1

ot jt (bj , aj , oj , bj )Oj (s , , oj )Ti (s , , )

(170 )


(18)

j

Else, subintentional:
P
P
t1
t1
t1 , ot )
(ist1 ) at1 P r(at1
bti (ist ) = ist1 :m
bi

j |mj )Oi (s ,
=

b
b t1
j
j
j
P
), ht )O (st , at1 , ot )T (st1 , at1 , st )
,

ot K (APPEND(ht1
j
j
j
j
j

(19)

j

Since proposition 2 expresses belief bti (ist ) terms parameters previous time step
only, Proposition 1 holds well.
present proof Theorem 1 note Equation 7, defines value
iteration I-POMDPs, rewritten following form, U n = HU n1 . Here, H : B B
backup operator, defined as,
HU n1 (i ) = max h(i , ai , U n1 )
ai Ai

h : Ai B R is,
h(i , ai , U ) =

P


bi (is)ERi (is, ai ) +

P

oi

P r(oi |ai , bi )U (hSEi (bi , ai , oi ), i)

B set bounded value functions U . Lemmas 1 2 establish important
properties backup operator. Proof Lemma 1 given below, proof Lemma 2 follows
thereafter.
Proof Lemma 1. Select arbitrary value functions V U V ( i,l ) U (i,l ) i,l
i,l . Let i,l arbitrary type agent i.
73

fiG MYTRASIEWICZ & OSHI





P

P

HV (i,l ) = max
oi P r(oi |ai , bi )V (hSEi,l (bi , ai , oi ), i)
bi (is)ERi (is, ai ) +
ai Ai
P
P
= bi (is)ERi (is, ai ) + oi P r(oi |ai , bi )V (hSEi,l (bi , ai , oi ), i)
P
P



b
(is)ERi (is, ai ) +
oi P r(oi |ai , bi )U (hSEi,l (bi , ai , oi ), i)

P
P
max
oi P r(oi |ai , bi )U (hSEi,l (bi , ai , oi ), i)
bi (is)ERi (is, ai ) +
ai Ai

= HU (i,l )

Since i,l arbitrary, HV HU .
Proof Lemma 2. Assume two arbitrary well defined value functions V U V U .
Lemma 1 follows HV HU . Let i,l arbitrary type agent i. Also, let ai
action optimizes HU (i,l ).
0 HU (i,l ) HV (i,l )



P

= max sumis bi (is)ERi (is, ai ) + oi P r(oi |ai , bi )U (SEi,l (bi , ai , oi ), hi i)
ai Ai

P
P
max
bi (is)ERi (is, ai ) +
oi P r(oi |ai , bi )V (SEi,l (bi , ai , oi ), hi i)
ai Ai
P
P
bi (is)ERi (is, ai ) + oi P r(oi |ai , bi )U (SEi,l (bi , ai , oi ), hi i)
P
P



oi P r(oi |ai , bi )V (SEi,l (bi , ai , oi ), hi i)
bi (is)ERi (is, ai )
P


= oi P r(oi |ai , bi )U (SEi,l (bi , ai , oi ), hi i)
P

oi P r(oi |ai , bi )V (SE

i,l (bi , ai , oi ), hi i)
P



= oi P r(oi |ai , bi ) U (SEi,l (bi , ai , oi ), hi i) V (SEi,l (bi , ai , oi ), hi i)
P
oi P r(oi |ai , bi )||U V ||
= ||U V ||

supremum norm symmetrical, similar result derived HV ( i,l ) HU (i,l ).
Since i,l arbitrary, Contraction property follows, i.e. ||HV HU || ||V U ||.
Lemmas 1 2 provide stepping stones proving Theorem 1. Proof Theorem 1 follows
straightforward application Contraction Mapping Theorem. state Contraction
Mapping Theorem (Stokey & Lucas, 1989) below:
Theorem 3 (Contraction Mapping Theorem). (S, ) complete metric space :
contraction mapping modulus ,
1. exactly one fixed point U S,
2. sequence {U n } converges U .
Proof Theorem 1 follows.
74

fiA F RAMEWORK EQUENTIAL P LANNING ULTI -AGENT ETTINGS

Proof Theorem 1. normed space (B, || ||) complete w.r.t metric induced supremum norm. Lemma 2 establishes contraction property backup operator, H. Using Theorem 3, substituting H, convergence value iteration I-POMDPs unique fixed
point established.
go piecewise linearity convexity (PWLC) property value function.
follow outlines analogous proof POMDPs (Hausktecht, 1997; Smallwood &
Sondik, 1973).
Let : R real-valued bounded function. Let space real-valued
bounded functions B(IS). define inner product.
Definition 5 (Inner product). Define inner product, h, : B(IS) (IS) R,
X
h, bi =
bi (is)(is)


next lemma establishes bilinearity inner product defined above.
Lemma 3 (Bilinearity). s, R, f, g B(IS), b, (IS) following equalities
hold:
hsf + tg, bi = shf, bi + thg, bi
hf, sb + ti = shf, bi + thf,
ready give proof Theorem 2. Theorem 4 restates Theorem 2 mathematically, proof follows thereafter.
Theorem 4 (PWLC). value function, U n , finitely nested I-POMDP piece-wise linear
convex (PWLC). Mathematically,
U n (i,l ) = max
n


X

bi (is)n (is)

n = 1, 2, ...



Proof Theorem 4. Basis Step: n = 1
Bellmans Dynamic Programming equation,
U 1 (i ) = max
ai

X

bi (is)ER(is, ai )

(20)



P
ERi (is, ai ) = aj R(is, ai , aj )P r(aj |mj ). Here, ERi () represents expectation
R w.r.t. agent js actions. Eq. 20 represents inner product using Lemma 3, inner product
linear bi . selecting maximum set linear vectors (hyperplanes), obtain PWLC
horizon 1 value function.
Inductive Hypothesis: Suppose U n1 (i,l ) PWLC. Formally have,
U n1 (i,l ) = max
n1

=

P

max

n1 ,

bi (is)

n1



P

n1 (is)

is:mj IMj bi

(is)n1 (is)
75

+

P

is:mj SMj bi

(is)n1 (is)



(21)

fiG MYTRASIEWICZ & OSHI

Inductive Proof: show U n (i,l ) PWLC.

U n (i,l ) = max
at1


(

X

t1
bt1
)ERi (ist1 , at1
(is
)+

X

t1
n1
P r(oti |at1
(i,l )
, bi )U

oti

ist1

inductive hypothesis:
(
P
t1
t1 )ER (ist1 , at1 )
U n (i,l ) = max

ist1 bi (is

at1


+

P

oti

t1
P r(oti |at1
, bi )

max

n1 n1

P


n1 (ist )
ist bi (is )

)

)

t1
t1 t1

Let l(bt1
, ai , oi ) index alpha vector maximizes value b = SE(bi , ai , oi ).
Then,
(
P
t1
t1 )ER (ist1 , at1 )
U n (i,l ) = max

ist1 bi (is

t1
ai
)
P
P
t1

n1
+ ot P r(oti |at1
ist bi (is )l(bt1 ,at1 ,ot )
, bi )








second equation inductive hypothesis:
(
P
P
t1
t1 )ER (ist1 , at1 ) +
n
t1 t1
U (i,l ) = max

ot P r(oi |ai , bi )
ist1 bi (is

at1








P


n1
ist :mtj IMj bi (is )l(bt1 ,at1 ,ot )




+

P


n1
ist :mtj SMj bi (is )l(bt1 ,at1 ,ot )




Substituting bti appropriate belief updates Eqs. 17 17 0 get:
(
P
P
t1
t1 t1
t1 )ER (ist1 , at1 ) +
U n (i,l ) = max

oti P r(oi |ai , bi )
ist1 bi (is

t1
ai
"


P
P
P
t1
t1 t1
t1

)
P r(aj |j ) Oi (st , at1 , oti )
ist :mtj IMj
ist1 bi (is
at1
j


P
t1 t1
t1


t1


t1
t1

ot Oj (s , , oj ) jt (bj , aj , oj , bj )I(bj , bj )Ti (s , , )

)

j

n1

l(b
t1 t1 (is )
,ai ,oi )



P
P
P
t1
t1
t1
t1
+ ist :mt SMj ist1 bi (is )
P r(aj |mj ) Oi (st , at1 , oti )
at1
j
j


P
t1
t1


t1



t1
t1

ot Oj (s , , oj ) K (APPEND(hj , oj ) hj )I(m
bj ,m
b j )Ti (s , , )
j
#)
n1

l(b
t1 t1 (is )
,a
,o )






76

fiA F RAMEWORK EQUENTIAL P LANNING ULTI -AGENT ETTINGS


U n (i,l ) = max
at1





P

P

(

P

t1
t1 )ER (ist1 , at1 )

ist1 bi (is


t1
t1 )
ist1 bi (is



t1 , ot )
j
otj Oj (s ,

P



at1
j

+

P

oti

"

P

ist :mtj IMj


t1
P r(at1
|
)
Oi (st , at1 , oti )
j
j

t1
t1 , at1 , st )
bt1 bt
jt (bt1
j , aj , oj , bj )I(j , j )Ti (s



n1

l(b
t1 t1 (is )
,ai ,oi )



P
P
P
t1
t1
t1
t1
P r(aj |mj ) Oi (st , at1 , oti )
+ ist :mt SMj ist1 bi (is )
at1
j
j


P
t1
) ht )I(m
)T (st1 , at1 , st )
ot Ojt (st , at1 , otj ) K (APPEND(ht1
,

b
,

b
j
j
j
j
j
j
#)
n1

l(b
t1 t1 (is )
,a
,o )






Rearranging terms equation:
U n (

(


P
P P
t1
t1 ) ER (ist1 , at1 ) +
)
=
max
b
(is
t1
t1 :m

i,l
oti
ist :mtj IMj



IM
j
j
at1



P
P
t1 t1

P r(aj |j ) Oi (st , at1 , oti ) ot Ojt (st , at1 , otj )
at1
j
j



n1
t1 t1
t1

t1
t1


jt (bj , aj , oj , bj )I(bj , bj )Ti (s , , )
l(bt1 ,at1 ,ot ) (is )




P
P
P
P
+ ist1 :mt1 SMj bit1 (ist1 ) ERi (ist1 , at1
oti
ist :mtj SMj
oti
)+
j


P
P
P r(ajt1 |mt1
) Oi (st , at1 , oti ) ot Ojt (st , at1 , otj )

j
at1
j
j

)

n1



l(b
b t1
b tj )Ti (st1 , at1 , st )
K (APPEND(ht1
t1 t1 (is )
j ,m
j , oj ) hj )I(m
,ai ,oi )


P
t1
t1 )n (ist1 )
= max
ai
IMj bi (is
ist1 :mt1
j
at1


P
t1
t1
t1
n
+ ist1 :mt1 SMj bi (is )ai (is )
j

Therefore,
U n (

i,l )

= max
n
n
,

+
=

P



P

t1
t1 )n (ist1 )
ist1 :mt1
IMj bi (is
j



t1
t1 )n (ist1 )
SMj bi (is
ist1 :mt1
j
P
t1
t1 )n (ist1 ) = maxhbt1 , n
max
ist1 bi (is

n

n

77

(22)

fiG MYTRASIEWICZ & OSHI

where, mjt1 ist1 intentional n = n :
n (ist1 )

ERi (ist1 , at1
)



P P

P

t1
P r(at1
j |j )



Oi (ist , at1 , oti )
+ ot ist :mt IMj
at1
j
j


P
t1 t1
t1


t1


t1
t1

ot Oj (isj , , oj ) jt (bj , aj , oj , bj )I(bj , bj )Ti (s , , )

=

j

n1

l(b
t1 t1 (is )
,o )
,a






and, mjt1 subintentional n = n :
n (ist1 )

ERi (ist1 , at1
)



P P

P

t1
P r(at1
j |j )



Oi (ist , at1 , oti )
+ ot ist :mt SMj
at1

j
j


P
t1
t1
t1
t1



t1



bj ,m
b j )Ti (s , , )
ot Oj (isj , , oj ) K (APPEND(hj , oj ) hj )I(m

=

j

n1

l(b
t1 t1 (is )
,ai ,oi )


Eq. 22 inner product using Lemma 3, value function linear b t1
. Furthermore,
maximizing set linear vectors (hyperplanes) produces piecewise linear convex value
function.

References
Ambruster, W., & Boge, W. (1979). Bayesian game theory. Moeschlin, O., & Pallaschke, D. (Eds.), Game
Theory Related Topics. North Holland.
Aumann, R. J. (1999). Interactive epistemology i: Knowledge. International Journal Game Theory, pp.
263300.
Aumann, R. J., & Heifetz, A. (2002). Incomplete information. Aumann, R., & Hart, S. (Eds.), Handbook
Game Theory Economic Applications, Volume III, Chapter 43. Elsevier.
Battigalli, P., & Siniscalchi, M. (1999). Hierarchies conditional beliefs interactive epistemology
dynamic games. Journal Economic Theory, pp. 188230.
Bernstein, D. S., Givan, R., Immerman, N., & Zilberstein, S. (2002). complexity decentralized control
markov decision processes. Mathematics Operations Research, 27(4), 819840.
Binmore, K. (1990). Essays Foundations Game Theory. Blackwell.
Boutilier, C. (1999). Sequential optimality coordination multiagent systems. Proceedings
Sixteenth International Joint Conference Artificial Intelligence, pp. 478485.
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions computational leverage. Journal Artificial intelligence Research, 11, 194.
Brandenburger, A. (2002). power paradox: recent developments interactive epistemology.
Tech. rep., Stern School Business, New York University, http://pages.stern.nyu.edu/ abranden/.
Brandenburger, A., & Dekel, E. (1993). Hierarchies beliefs common knowledge. Journal Economic
Theory, 59, 189198.
Dennett, D. (1986). Intentional systems. Dennett, D. (Ed.), Brainstorms. MIT Press.
Fagin, R. R., Geanakoplos, J., Halpern, J. Y., & Vardi, M. Y. (1999). hierarchical approach modeling
knowledge common knowledge. International Journal Game Theory, pp. 331365.
Fagin, R. R., Halpern, J. Y., Moses, Y., & Vardi, M. Y. (1995). Reasoning Knowledge. MIT Press.
Fudenberg, D., & Levine, D. K. (1998). Theory Learning Games. MIT Press.
78

fiA F RAMEWORK EQUENTIAL P LANNING ULTI -AGENT ETTINGS

Fudenberg, D., & Tirole, J. (1991). Game Theory. MIT Press.
Gmytrasiewicz, P. J., & Durfee, E. H. (2000). Rational coordination multi-agent environments. Autonomous Agents Multiagent Systems Journal, 3(4), 319350.
Harsanyi, J. C. (1967). Games incomplete information played Bayesian players. Management
Science, 14(3), 159182.
Hauskrecht, M. (2000). Value-function approximations partially observable markov decision processes.
Journal Artificial Intelligence Research, pp. 3394.
Hausktecht, M. (1997). Planning control stochastic domains imperfect information. Ph.D. thesis,
MIT.
Hu, J., & Wellman, M. P. (1998). Multiagent reinforcement learning: Theoretical framework algorithm. Fifteenth International Conference Machine Learning, pp. 242250.
Kadane, J. B., & Larkey, P. D. (1982). Subjective probability theory games. Management Science,
28(2), 113120.
Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1998). Planning acting partially observable
stochastic domains. Artificial Intelligence, 101(2), 99134.
Kalai, E., & Lehrer, E. (1993). Rational learning leads nash equilibrium. Econometrica, pp. 12311240.
Koller, D., & Milch, B. (2001). Multi-agent influence diagrams representing solving games. Seventeenth International Joint Conference Artificial Intelligence, pp. 10271034, Seattle, Washington.
Li, M., & Vitanyi, P. (1997). Introduction Kolmogorov Complexity Applications. Springer.
Littman, M. L. (1994). Markov games framework multi-agent reinforcement learning. Proceedings
International Conference Machine Learning.
Lovejoy, W. S. (1991). survey algorithmic methods partially observed markov decision processes.
Annals Operations Research, 28(1-4), 4766.
Madani, O., Hanks, S., & Condon, A. (2003). undecidability probabilistic planning related
stochastic optimization problems. Artificial Intelligence, 147, 534.
Mertens, J.-F., & Zamir, S. (1985). Formulation Bayesian analysis games incomplete information.
International Journal Game Theory, 14, 129.
Monahan, G. E. (1982). survey partially observable markov decision processes: Theory, models,
algorithms. Management Science, 116.
Myerson, R. B. (1991). Game Theory: Analysis Conflict. Harvard University Press.
Nachbar, J. H., & Zame, W. R. (1996). Non-computable strategies discounted repeated games. Economic
Theory, 8, 103122.
Nair, R., Pynadath, D., Yokoo, M., Tambe, M., & Marsella, S. (2003). Taming decentralized pomdps: Towards
efficient policy computation multiagent settings. Proceedings Eighteenth International
Joint Conference Artificial Intelligence (IJCAI-03).
Ooi, J. M., & Wornell, G. W. (1996). Decentralized control multiple access broadcast channel.
Proceedings 35th Conference Decision Control.
Papadimitriou, C. H., & Tsitsiklis, J. N. (1987). complexity markov decision processes. Mathematics
Operations Research, 12(3), 441450.
Russell, S., & Norvig, P. (2003). Artificial Intelligence: Modern Approach (Second Edition). Prentice Hall.
Smallwood, R. D., & Sondik, E. J. (1973). optimal control partially observable markov decision
processes finite horizon. Operations Research, pp. 10711088.
Stokey, N. L., & Lucas, R. E. (1989). Recursive Methods Economic Dynamics. Harvard Univ. Press.

79


