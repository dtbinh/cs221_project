journal artificial intelligence

submitted published

framework sequential multi agent settings
piotr j gmytrasiewicz
prashant doshi

piotr cs uic edu
pdoshi cs uic edu

department computer science
university illinois chicago
morgan st
chicago il

abstract
extends framework partially observable markov decision processes pomdps
multi agent settings incorporating notion agent state space agents
maintain beliefs physical states environment agents
use bayesian updates maintain beliefs time solutions map belief states actions
agents may include belief states related agent types considered
games incomplete information express agents autonomy postulating directly manipulable observable agents important properties
pomdps convergence value iteration rate convergence piece wise linearity convexity value functions carry framework complements
traditional interactive settings uses nash equilibria solution paradigm
seek avoid drawbacks equilibria may non unique capture
equilibrium behaviors cost represent process continuously
revise agents since agents beliefs may arbitrarily nested optimal solutions decision making asymptotically computable however approximate
belief updates approximately optimal plans computable illustrate framework
simple application domain examples belief updates value functions

introduction
develop framework sequential rationality autonomous agents interacting
agents within common possibly uncertain environment use normative paradigm
decision theoretic uncertainty formalized partially observable markov decision
processes pomdps boutilier dean hanks kaelbling littman cassandra
russell norvig point departure solutions pomdps mappings
agents beliefs actions drawback pomdps comes environments populated
agents agents actions represented implicitly environmental noise
within usually static transition model thus agents beliefs another agent part
solutions pomdps
main idea behind formalism called interactive pomdps pomdps allow
agents use sophisticated constructs model predict behavior agents thus
replace flat beliefs state space used pomdps beliefs physical
environment agent possibly terms preferences capabilities
beliefs beliefs could include others beliefs others thus nested arbitrary
levels called interactive beliefs space interactive beliefs rich
updating beliefs complex updating flat counterparts use value
c

ai access foundation rights reserved

fig mytrasiewicz oshi

function plots solutions pomdps least good usual cases superior
comparable solutions pomdps reason intuitive maintaining sophisticated
agents allows refined analysis behavior better predictions actions
pomdps applicable autonomous self interested agents locally compute actions execute optimize preferences given believe interacting
others possibly conflicting objectives decision theoretic framework solution concept complements equilibrium analyzing interactions used
classical game theory fudenberg tirole drawback equilibria could
many non uniqueness describe agents optimal actions
equilibrium reached incompleteness instead centered optimality
best response anticipated action agent rather stability binmore
kadane larkey question whether circumstances kind
equilibria could arise solutions pomdps currently open
avoids difficulties non uniqueness incompleteness traditional equilibrium offers solutions likely better solutions traditional
pomdps applied multi agent settings advantages come cost processing
maintaining possibly infinitely nested interactive beliefs consequently approximate belief
updates approximately optimal solutions computable general
define class finitely nested pomdps form basis computable approximations infinitely nested ones number properties facilitate solutions pomdps carry
finitely nested pomdps particular interactive beliefs sufficient statistics
histories agents observations belief update generalization update pomdps
value function piece wise linear convex value iteration converges
rate
remainder structured follows start brief review related
work section followed overview partially observable markov decision processes
section include simple example tiger game introduce concept
agent types section section introduces interactive pomdps defines solutions
finitely nested pomdps properties introduced section continue
example application finitely nested pomdps multi agent version tiger game
section examples belief updates value functions conclude
brief summary current issues section details proofs
appendix

related work
work draws prior partially observable markov decision processes
recently gained lot attention within ai community smallwood sondik monahan
lovejoy hausktecht kaelbling et al boutilier et al hauskrecht

formalism markov decision processes extended multiple agents giving rise
stochastic games markov games fudenberg tirole traditionally solution concept
used stochastic games nash equilibria recent work ai follows tradition
littman hu wellman boutilier koller milch however
mentioned pointed game theorists binmore kadane


fia f ramework equential p lanning ulti agent ettings

larkey nash equilibria useful describing multi agent system
reached stable state solution concept sufficient general control paradigm
main reasons may multiple equilibria clear way choose among
non uniqueness fact equilibria specify actions cases agents believe
agents may act according equilibrium strategies incompleteness
extensions pomdps multiple agents appeared ai literature recently bernstein
givan immerman zilberstein nair pynadath yokoo tambe marsella
called decentralized pomdps dec pomdps related decentralized control
ooi wornell dec pomdp framework assumes agents fully cooperative e common reward function form team furthermore assumed
optimal joint solution computed centrally distributed among agents execution
game theoretic side motivated subjective probability
games kadane larkey bayesian games incomplete information see fudenberg
tirole harsanyi references therein work interactive belief systems harsanyi
mertens zamir brandenburger dekel fagin halpern moses vardi
aumann fagin geanakoplos halpern vardi insights
learning game theory fudenberg levine closely related decisiontheoretic myerson epistemic ambruster boge battigalli siniscalchi
brandenburger game theory consists predicting actions agents given
available information choosing agents action kadane larkey
thus descriptive aspect decision theory used predict others actions prescriptive
aspect used select agents optimal action
work presented extends previous work recursive modeling method rmm
gmytrasiewicz durfee adds elements belief update sequential

background partially observable markov decision processes
partially observable markov decision process pomdp monahan hausktecht
kaelbling et al boutilier et al hauskrecht agent defined
pomdpi hs ai ti oi ri



set possible states environment ai set actions agent execute ti
transition function ti ai describes agent actions
set observations agent make oi agents observation function oi ai
specifies probabilities observations given agents actions resulting states finally
ri reward function representing agent preferences r ai
pomdps agents belief state represented probability distribution
initially observations actions take place agent prior belief b
time steps assume agent observations performed actions

assembled agent observation history h ti ot
oi time let
hi denote set observation histories agent agents current belief b ti
continuously revised observations expected performed actions turns
assume action taken every time step without loss generality since actions maybe
op



fig mytrasiewicz oshi

agents belief state sufficient summarize past observation history
initial belief hence called sufficient statistic

belief update takes account changes initial belief b
due action ai executed


time observation oi belief bi current state st
bti st oi oti st


x

bit st ti st ati st



st

normalizing constant
convenient summarize update performed states

bti se bit
oi kaelbling et al
optimality criteria solutions
agents optimality criterion oci needed specify rewards acquired time
handled commonly used criteria include
finite horizon criterion p
agent maximizes expected value sum
following rewards e tt rt rt reward obtained time length
horizon denote criterion fht
anp
infinite horizon criterion discounting according agent maximizes


e
rt discount factor denote criterion ih

infinite horizon criterion averaging according agent maximizes
average reward per time step denote ihav

follows concentrate infinite horizon criterion discounting easily adapted criteria
utility associated belief state bi composed best immediate rewards
obtained bi together discounted expected sum utilities associated
belief states following bi

u bi max

ai ai

x

bi ri ai

x

p r oi ai bi u sei bi ai oi

oi

ss





value iteration uses equation iteratively obtain values belief states longer time
horizons step value iteration error current value estimate reduced
factor least see example russell norvig section optimal action
element set optimal actions op bi belief state defined

op bi argmax
ai ai

x

bi ri ai

x

oi

ss

see smallwood sondik proof



p r oi ai bi u se bi ai oi





fia f ramework equential p lanning ulti agent ettings

l


ol



value function u























pp tl
tl
pomdp noise

pomdp

figure value function single agent tiger game time horizon length oc fh
actions open right door open left door ol listen l value
time horizon value function pomdp noise factor identical single
agent pomdp

example tiger game
briefly review pomdp solutions tiger game kaelbling et al purpose
build insights pomdp solutions provide simple case illustrate solutions
interactive versions game later
traditional tiger game resembles game situation decision maker
choose open one two doors behind lies valuable prize dangerous tiger
apart actions open doors subject option listening tigers growl
coming left right door however subjects hearing imperfect given
percentages say false positive false negative occurrences following kaelbling et al
assume value prize pain associated encountering
tiger quantified cost listening
value function figure shows values belief states agents time
horizon equal values beliefs best action available belief state
specified eq state certainty valuable agent knows location
tiger open opposite door claim prize certainly awaits thus
probability tiger location value agent sufficiently uncertain
best option play safe listen value agent indifferent opening
doors listening assigns probabilities location tiger
note time horizon equal listening provide useful information
since game continue allow use information longer time horizons
benefits listening policies better ranges initial belief
since value function composed values corresponding actions linear prob

fig mytrasiewicz oshi

l l gl ol gr

l l

l gl l gr
l
l

l ol
ol l



value function u






















pp tl
tl
pomdp noise

pomdp

figure value function single agent tiger game compared agent facing noise factor horizon length policies corresponding value lines conditional plans
actions l ol conditioned observational sequences parenthesis
example l l gl ol gr denotes plan perform listening action l
beginning list observations empty another l observation growl
left gl open left door ol observation gr wildcard
usual interpretation

ability tiger location value function property piece wise linear convex
pwlc horizons simplifies computations substantially
figure present comparison value functions horizon length single
agent agent facing noisy environment presence noise could
due another agent opening doors listening probabilities since pomdps
include explicit agents noise actions included transition
model
consequences folding noise two fold first effectiveness agents optimal
policies declines since value hearing growls diminishes many time steps figure depicts
comparison value functions horizon length example two consecutive growls
noisy environment valuable agent knows acting alone since noise
may perturbed state system growls time horizon length
noise matter value vectors overlap figure
second since presence another agent implicit static transition model agent
cannot update model agents actions repeated interactions effect becomes important time horizon increases addresses issue allowing
explicit modeling agent policies superior quality
section figure shows policy agent facing noisy environment time horizon
compare corresponding pomdp policy section note slightly different
assumed due noise door opens probabilities turn nothing happens
probability explain origin assumption section



fia f ramework equential p lanning ulti agent ettings

l l ol gr gr l
l l gl ol gr ol gl gr l

l l gl gl l
l gl l gr gr gl l
l l gl gl ol gr gr l
l l
l l

ol l l
l l ol


value function u




















p tl
p tl


pomdp noise

pomdp

figure value function single agent tiger game compared agent facing noise factor
horizon length description policy stands
perceptual sequences yet listed description policy


ol



l

l

gr

gl

gr

l
gr

gl



ol

l
gr

l
gl

gr


gl



gl

l
gl



l

gr

ol



l


l

gr


gl





figure policy graph corresponding value function pomdp noise depicted
fig



fig mytrasiewicz oshi

policy without noise example kaelbling littman cassandra due
differences value functions

agent types frames
pomdp definition includes parameters permit us compute agents optimal behavior
conditioned beliefs let us collect implementation independent factors construct
call agent type
definition type type agent hbi ai ti oi ri oci bi agent
state belief element oci optimality criterion rest elements
defined let set agent types
given type assumption agent bayesian rational set agents optimal
actions denoted op next section generalize notion type situations include interactions agents coincides notion type used
bayesian games fudenberg tirole harsanyi
convenient define notion frame bi agent

b set
definition frame frame agent bi hai ti oi ri oci let
agent frames

brevity one write type consisting agents belief together frame
hbi bi
context tiger game described previous section agent type describes
agents actions quality agents hearing payoffs belief
tiger location
realistically apart implementation independent factors grouped type agents behavior may depend implementation specific parameters processor speed memory
available etc included implementation dependent complete type increasing accuracy predicted behavior cost additional complexity definition use
complete types topic ongoing work

interactive pomdps
mentioned intention generalize pomdps handle presence agents
including descriptions agents types example state space
simplicity presentation consider agent interacting one agent j
formalism easily generalizes larger number agents
definition pomdp interactive pomdp agent pomdpi
pomdpi hisi ti oi ri



issue computability solutions pomdps subject much papadimitriou tsitsiklis
madani hanks condon obvious importance one uses pomdps model agents
return issue later



fia f ramework equential p lanning ulti agent ettings


isi set interactive states defined isi mj interacting agent
set states physical environment mj set possible agent
j model mj mj defined triple mj hhj fj oj fj hj aj
agent js function assumed computable maps possible histories js observations
distributions actions hj element hj oj function specifying way
environment supplying agent input sometimes write model j mj hhj
b j

b j consists fj oj convenient subdivide set two classes
subintentional smj relatively simple intentional imj use
notion rationality model agent thus mj imj smj
simple examples subintentional include information model fictitious play
model history independent information model gmytrasiewicz durfee
assumes agents actions executed equal probability fictitious
play fudenberg levine assumes agent chooses actions according fixed
unknown distribution original agents prior belief distribution takes form
dirichlet distribution example powerful subintentional model finite state
controller
intentional sophisticated ascribe agent beliefs
preferences rationality action selection intentional thus js types j hbj bj
assumption agent j bayesian rational agent js belief probability distribution
states environment agent b j mi notion type
use coincides notion type game theory defined consisting
agent private information relevant decision making harsanyi fudenberg
tirole particular agents beliefs private information types involve
possibly infinitely nested beliefs others types beliefs others mertens zamir
brandenburger dekel aumann aumann heifetz related
recursive model structures prior work gmytrasiewicz durfee definition
interactive state space consistent notion completely specified state space put forward
aumann similar state spaces proposed others mertens zamir
brandenburger dekel
ai aj set joint moves agents
ti transition model usual way define transition probabilities pomdps
assume agents actions change aspect state description case ipomdps would mean actions modifying aspect interactive states including
agents observation histories functions modeled intentionally beliefs
reward functions allowing agents directly manipulate agents ways however
violates notion agents autonomy thus make following simplifying assumption

agents say n isi n
j mj
technically according notation fictitious play actually ensemble
dennet advocates ascribing rationality agent calls assuming intentional stance towards

note space types far richer computable particular since set computable
countable set types uncountable many types computable
implicit definition interactive beliefs assumption coherency brandenburger dekel



fig mytrasiewicz oshi

model non manipulability assumption mnm agents actions change
agents directly
given simplification transition model defined
autonomy formalized mnm assumption precludes example direct mind control
implies agents belief states changed indirectly typically changing
environment way observable words agents beliefs change pomdps
belief update observation direct agents
actions
defined pomdp model
oi observation function defining function make following assumption
model non observability mno agents cannot observe others directly
given assumption observation function defined
mno assumption formalizes another aspect autonomy agents autonomous
observations functions beliefs properties say preferences intentional
private agents cannot observe directly
ri defined ri isi allow agent preferences physical
states agents usually physical state matter
mentioned see interactive pomdps subjective counterpart objective external view stochastic games fudenberg tirole followed work
ai boutilier koller milch decentralized pomdps bernstein et al
nair et al interactive pomdps represent individual agents point view
environment agents facilitate solving agents
individual level
belief update pomdps
pomdps agents beliefs interactive states sufficient
statistics e fully summarize agents observation histories need
beliefs updated agents action observation solutions defined

belief state bti function previous belief state bt
last action ai

observation oi pomdps two differences complicate belief
update compared pomdps first since state physical environment depends
actions performed agents prediction physical state changes
made probabilities actions agent probabilities others
actions obtained thus unlike bayesian stochastic games
assume actions fully observable agents rather agents attempt infer
actions agents performed sensing environment second changes
agents included update reflect others observations
modeled intentionally update agents beliefs case agent
update beliefs agent anticipates agent observes
possibility agents influence observational capabilities agents accommodated
including factors change sensing capabilities set
possibility agents observe factors may influence observational capabilities agents
allowed including factors



fia f ramework equential p lanning ulti agent ettings

updates could expected update possibly infinitely nested belief
others types general asymptotically computable
proposition sufficiency interactive pomdp agent current belief e probability distribution set mj sufficient statistic past history observations

next proposition defines agent belief update function b ti ist p r ist oti
bi
ist isi interactive state use belief state estimation function se abt
breviation belief updates individual states bti sei bt
ai oi



bi ai oi bi stand p r bti bi ai oti define set
type dependent optimal actions agent op

proposition belief update mnm mno assumptions belief update function
interactive pomdp hisi ti oi ri mj ist intentional
bti ist
ti

p


bt


ist
b
bjt
j
p


st
otj

p


ot
p r

j j oi


j

ot

b
j j aj oj bj oj
j




b tj
mj ist subintentional first summation extends ist
b
j




p r
j j replaced p r aj mj jt bj aj oj bj replaced


kronecker delta function k append ht
j oj hj

bt
btj belief elements jt jt respectively normalizing constant
j


p r
bayesian rational agent described type
j j probability aj



j probability equal op j aj op j equal zero otherwise
define op section case js subintentional model mj ht

j
respectively observation
htj observation histories part mt



j
j
j



function mtj p r




probability
assigned




j
j
j
j append returns
string second argument appended first proofs propositions
appendix
proposition eq lot common belief update pomdps
expected depend agent observation transition functions however since agent
observations depend agent js actions probabilities actions j
included first line eq since update agent js model depends
j observes probabilities observations j included second line
eq update js beliefs represented j term belief update easily
generalized setting one agents co exist agent
p
agents prior belief isi given probability density function
ist replaced

otj btj takes form dirac delta function argument bt

integral case jt bt
j
j
j
otj btj

sejt bt
j
j



fig mytrasiewicz oshi

value function solutions pomdps
analogously pomdps belief state pomdp associated value reflecting maximum payoff agent expect belief state


p
p
b
eri ai bi
u max
p r oi ai bi u hsei bi ai oi

ai ai

oi



p
eri ai
aj ri ai aj p r aj mj eq basis value iteration ipomdps
agent optimal action ai case infinite horizon criterion discounting
element set optimal actions belief state op defined


p
p
op argmax
eri ai bi
p r oi ai bi u hsei bi ai oi bi
ai ai

oi




case belief update due possibly infinitely nested beliefs step value iteration
optimal actions asymptotically computable

finitely nested pomdps
possible infinite nesting agents beliefs intentional presents obvious obstacle
computing belief updates optimal solutions since agents infinitely
nested beliefs correspond agent functions computable natural consider
finite nestings follow approaches game theory aumann brandenburger dekel
fagin et al extend previous work gmytrasiewicz durfee construct
finitely nested pomdps bottom assume set physical states world two
agents j agent th level beliefs bi probability distributions th level
types contain th level beliefs frames analogously agent j level types
therefore pomdps level include level types e intentional
subintentional elements sm agents first level beliefs probability distributions
physical states level agent agents first level types consist
first level beliefs frames first level consist types upto level
subintentional second level beliefs defined terms first level
formally define spaces
isi
j hbj bj bj isj mj j smj
isi mj
j hbj bj bj isj mj j mj






isi l mj l j l hbj l bj bj l isj l mj l j l mj l
definition finitely nested pomdp finitely nested pomdp agent pomdp l
pomdpi l hisi l ti oi ri
level types agents actions folded r functions noise





fia f ramework equential p lanning ulti agent ettings

parameter l called strategy level finitely nested pomdp belief update
value function optimal actions finitely nested pomdps computed equation
equation recursion guaranteed terminate th level subintentional
agents strategic capable modeling others deeper levels e levels
strategy level l boundedly optimal agents
could fail predict strategy sophisticated opponent fact computability
agent function implies agent may suboptimal interactions pointed
binmore proved recently nachbar zame intuitively
difficulty agents unbounded optimality would include capability model
agents modeling original agent leads impossibility due self reference
similar godels incompleteness theorem halting brandenburger
positive note convergence kalai lehrer strongly suggest
approximate optimality achievable although applicability work remains open
mentioned th level types pomdps provide probability distributions
actions agent modeled level strategy level given probability
distributions agents actions level solved pomdps
provide probability distributions yet higher level assume number
considered level bound number solving pomdp l equivalent
solving l pomdps hence complexity solving pomdpi l pspace hard
finite time horizons undecidable infinite horizons pomdps
properties pomdps
section establish two important properties namely convergence value iteration
piece wise linearity convexity value function finitely nested pomdps
c onvergence



value teration

agent pomdpi l sequence value functions u n
n horizon obtained value iteration defined eq converges unique fixed point u
let us define backup operator h b b u n hu n b set
bounded value functions order prove convergence first establish
properties h
lemma isotonicity finitely nested pomdp value functions v u v u
hv hu
proof lemma analogous one due hauskrecht pomdps
sketched appendix another important property exhibited backup operator
property contraction
lemma contraction finitely nested pomdp value functions v u discount
factor hv hu v u
proof lemma similar corresponding one pomdps hausktecht
proof makes use lemma supremum norm
usually pspace complete since number states pomdps likely larger time horizon
papadimitriou tsitsiklis



fig mytrasiewicz oshi

contraction property h noting space value functions along
supremum norm forms complete normed space banach space apply contraction
mapping theorem stokey lucas value iteration pomdps converges
unique fixed point optimal solution following theorem captures
theorem convergence finitely nested pomdp value iteration starting arbitrary well defined value function converges unique fixed point
detailed proof theorem included appendix
case pomdps russell norvig error iterative estimates u n
finitely nested pomdps e u n u reduced factor least iteration
hence number iterations n needed reach error
n dlog rmax log e



rmax upper bound reward function
p iecewise l inearity



c onvexity

another property carries pomdps finitely nested pomdps piecewise
linearity convexity pwlc value function establishing property allows us decompose pomdp value function set alpha vectors represents policy
tree pwlc property enables us work sets alpha vectors rather perform value
iteration continuum agents beliefs theorem states pwlc property
pomdp value function
theorem pwlc finitely nested pomdp u piecewise linear convex
complete proof theorem included appendix proof similar one
due smallwood sondik pomdps proceeds induction basis case
established considering horizon value function showing pwlc inductive step
requires substituting belief update eq eq followed factoring belief
terms equation

example multi agent tiger game
illustrate optimal sequential behavior agents multi agent settings apply pomdp
framework multi agent tiger game traditional version described
definition
let us denote actions opening doors listening ol l tl
tr denote states corresponding tiger located behind left right door respectively
transition reward observation functions depend actions agents
assume tiger location chosen randomly next time step agents opened
doors current step assume agent hears tigers growls gr gl
accuracy make interaction interesting added observation
door creaks depend action executed agent creak right cr likely due


fia f ramework equential p lanning ulti agent ettings

agent opened right door similarly creak left cl silence good
indication agent open doors listened instead assume accuracy
creaks assume agents payoffs analogous single agent versions
described section make cases comparable note assumption
agents actions impact original agents payoffs directly rather indirectly
resulting states matter original agent table quantifies factors

hai aj
hol
hor
h oli
h ori
hl li
hl li

state




tl
tr

tl







tr







hai aj
hor ori
hol oli
hor oli
hol ori
hl li
hl ori
hor li
hl oli
hol li

transition function ti tj

tl










tr










hai aj
hor ori
hol oli
hor oli
hol ori
hl li
hl ori
hor li
hl oli
hol li

tl










tr










reward functions agents j

hai aj
hl li
hl li
hl oli
hl oli
hl ori
hl ori
hol
hor

state
tl
tr
tl
tr
tl
tr



h gl cl









h gl cr









h gl









h gr cl









h gr cr









h gr









hai aj
hl li
hl li
hol li
hol li
hor li
hor li
h oli
h ori

state
tl
tr
tl
tr
tl
tr



h gl cl









h gl cr









h gl









h gr cl









h gr cr









h gr









observation functions agents j
table transition reward observation functions multi agent tiger game
agent makes choice multi agent tiger game considers believes
location tiger well whether agent listen open door
turn depends agents beliefs reward function optimality criterion etc particular
agent open doors tiger location next time step would
chosen randomly thus information obtained hearing previous growls would
discarded simplify situation considering pomdp single level nesting
assuming agent js properties except beliefs known js time
horizon equal words uncertainty pertains js beliefs
frame agent interactive state space isi j physical state tl
assume intentional model agent



fig mytrasiewicz oshi

tr j set intentional agent js differs js beliefs
location tiger
examples belief update
section presented belief update equation pomdps eq consider
examples beliefs bi agent probability distributions j th
level type agent j j j contains flat belief location tiger
represented single probability assignment bj pj l





pr tl p
pr tl b j
j

pr tl p
pr tl b j

j













































pb j
j tl









j





pr tr p
pr tr b j

pr tr p
pr tr b j

j



pjb j
tl
j tl


















ppb j
tl
tr
tl




jj









p j tl
b j



ii

figure two examples singly nested belief states agent case information
tigers location agent knows j know location
tiger single point star denotes dirac delta function integrates height
point ii agent uninformed js beliefs tigers location

fig examples level beliefs agent case know
location tiger marginals top bottom sections figure sum
probabilities tl tr fig knows j assigns probability tiger
behind left door represented dirac delta function fig ii agent
uninformed js beliefs represented uniform probability density values
probability j could assign state tl
make presentation belief update transparent decompose formula
eq two steps


fia f ramework equential p lanning ulti agent ettings


prediction agent performs action
given agent j performs aj
predicted belief state

bbt ist p r ist bt p bt bt bt ist p r

j
j

j


j j
p
st st oj st otj



otj


jt bt
j j j bj

correction agent perceives observation ti predicted belief states

p r
aj bi combined according
bti ist p r ist oti ait bt


x


oi st oti p r ist
j bi




j

normalizing constant












pb j tl



j




pb j tl












l gl


l gl




l gl









pjb j tl

gl

gl








l gl



l gl








pb j tl











l gl




pjb j
tl



pjb j
tl

b



pjb j tl




















l gl













j





pjb j
tl










j

j


pr tr pj



pr tr b j

j

pr tr p
pr tr b j

l l gl







l l gr





gl






l gl











pr tl p
pr tl b j

l l gr




bi

pr tr b j
pr tr p

j





bi

gl

pr tl p
pr tl b j



pr tl p
pr tl b j
j

pr tl p
pr tl b j

j





bi

l l gl

pr tr p
pr tr b j
j

bi

c










j



figure trace belief update agent depicts prior b prediction
given listening action l pair denoting js action observation knows
j listen could hear tigers growl right left probabilities
j would assign tl respectively c correction
observes tigers growl left creaks hgl si probability assigns
tl greater tr depicts another update prediction
correction another listen action observation hgl si
discrete point denotes dirac delta function integrates height
point
fig display example trace update singly nested belief first
column fig labeled example agent prior belief introduced according


fig mytrasiewicz oshi

knows j uninformed location tiger let us assume listens
hears growl left creaks second column fig b displays predicted
belief performs listen action eq part prediction step agent must solve
js model obtain js optimal action belief term p r
j j eq given
value function fig evaluates probability listen action zero opening
doors updates js belief given j listens hears tiger growling

left gl right gr term jt bt
j aj oj bj eq agent js updated probabilities
tiger left js hearing gl gr respectively tiger
left top fig b js observation gl likely consequently js assigning
probability state tl likely assigns probability state
tiger right j likely hear gr assigns lower probability
js assigning probability tiger left third column c fig shows
posterior belief correction step belief column b updated account
hearing growl left creaks hgl si resulting marginalised probability
tiger left higher tiger right assume
next time step listens hears tiger growling left creaks belief
state depicted fourth column fig
fig belief update starting prior fig ii according
agent initially information j believes tigers location
traces belief updates fig fig illustrate changing state information agent
agents beliefs benefit representing updates explicitly
stage optimal behavior depends estimate probabilities js actions
informative estimates value agent expect interaction
increase value function pomdps compared pomdps noise factor
examples value functions
section compares value functions obtained solving pomdp static noise factor
accounting presence another agent value functions level pomdp advantage refined modeling update pomdps due two factors first ability
keep track agents state beliefs better predict future actions second
ability adjust agents time horizon number steps go interaction
decreases neither possible within classical pomdp formalism
continue simple example pomdpi agent fig display
value function time horizon assuming initial belief value j assigns
tl pj l depicted fig ii e information j believes
tigers location value function identical value function obtained agent
traditional pomdp framework noise well single agent pomdp described
section value functions overlap since agents update beliefs
advantage refined modeling agent j pomdp become apparent put
another way agent j intentional model concludes agent j open
door probability listen probability coincides noise factor
described section
points fig denote dirac delta functions integrate value equal points height
pomdp noise level pomdp



fia f ramework equential p lanning ulti agent ettings




bi

bi

l l gr
l l gl




l l gl




l l gr














l l gr















l ol



pjb j tl



























pr tl b j

l l gr

pjb j tl









pjb j
tl



























l ol





pr tl pj

l ol







l ol







l l gl





pjb j
tl

l l gl



pr tr
pj
pr tr b j



pr tr b j
pr tr p
j

pr tl p

pr tl b j
j



pr tr b j
pr tr p
j



pr tl b j
pr tl
pj






















pjb j
tl













pjb j
tl

b
gl



bi








pr tr pj



pr tr b j

pr tl p

pr tl b j
j





























pjb j
tl

















pj b j
tl

c

figure trace belief update agent depicts prior according
uninformed js beliefs b prediction step listening
action l top half b shows belief listened given j
listened two observations j make gl gr probability dependent
tigers location give rise flat portions representing knows js belief
case increased probability assigns js belief
due js updates hears gl hears gr resulting values
interval bottom half b shows belief listened j opened
left right door plots identical action one shown
knows j information tigers location case c
correction observes tigers growl left creaks hgl si plots c
obtained performing weighted summation plots b probability
assigns tl greater tr information js beliefs allows refine
prediction js action next time step



fig mytrasiewicz oshi

l


ol



value function u























pp tl
tl
level pomdp

pomdp noise

figure time horizon value functions obtained solving singly nested pomdp
pomdp noise factor overlap
l ol gr l

l gl l

l l gl ol gr

ol l

l l

l gl l gr

l l gl ol gr

l gl l gr

l



value function u






















pp tl
tl
level pomdp

pomdp noise

figure comparison value functions obtained solving pomdp pomdp
noise time horizon pomdp value function dominates due agent adjusting
behavior agent j remaining steps go interaction



fia f ramework equential p lanning ulti agent ettings



value function u




















p tl
p tl
level pomdp

pomdp noise

figure comparison value functions obtained solving pomdp pomdp
noise time horizon pomdp value function dominates due agent adjusting js remaining steps go due modeling js belief update factors
allow better predictions js actions interaction descriptions individual policies omitted clarity read fig

fig display value functions time horizon value function
pomdpi higher value function pomdp noise factor reason
related advantages modeling agent js beliefs effect becomes apparent time
horizon longer rather pomdp solution dominates due agent modeling js time
horizon interaction knows last time step j behave according optimal
policy time horizon two steps go j optimize according steps go
policy mentioned effect cannot modeled pomdp static noise factor
included transition function
fig shows comparison pomdp noisy pomdp value functions
horizon advantage refined agent modeling within pomdp framework
increased factors adjusting js steps go modeling js belief update
interaction responsible superiority values achieved pomdp particular
recall second time step information js beliefs tigers location
depicted fig c enables make high quality prediction two steps left
go j perform actions ol l probabilities
respectively recall pomdp noise probabilities remained unchanged
respectively
fig shows agent policy graph time horizon usual prescribes optimal
first action depending initial belief tigers location subsequent actions depend
observations received observations include creaks indicative agents
note pomdp solution good solution pomdp agent operating alone environment shown fig



fig mytrasiewicz oshi


ol





l


l

gr

gl cl cr
gr





l



ol




gr
gl cl cr

l

l
gr



l

gr cl cr
gr cl cr
gl cl cr
gl
gr gl
gl
gr
gl

gl
gr cl cr

ol



l

gl

l


gr


gl





l

figure policy graph corresponding pomdp value function fig
opened door creaks contain valuable information allow agent make
refined choices compared ones noisy pomdp fig consider case agent
starts fairly strong belief tigers location decides listen according four
center top row l nodes fig hears door creak agent position
open left right door even counter initial belief reason
creak indication tigers position likely reset agent j j
open doors following two time steps two growls coming
door lead enough confidence open door agent hearing
tigers growls indicative tigers position state following agents actions
note value functions policy depict special case agent
information probability j assigns tigers location fig ii accounting
visualizing possible beliefs js beliefs difficult due complexity
space interactive beliefs ongoing work indicates drastic reduction complexity
possible without loss information consequently representation solutions manageable
number dimensions indeed possible report separately

conclusions
proposed framework optimal sequential decision making suitable controlling autonomous
agents interacting agents within uncertain environment used normative
paradigm decision theoretic uncertainty formalized partially observable markov
decision processes pomdps point departure extended pomdps cases agents
interacting agents allowing beliefs physical environment agents could include beliefs others abilities sensing
capabilities beliefs preferences intended actions framework shares numerous properties
pomdps analogously defined solutions reduces pomdps agents alone
environment
contrast recent work dec pomdps bernstein et al nair et al
work motivated game theoretic equilibria boutilier hu wellman koller


fia f ramework equential p lanning ulti agent ettings

milch littman subjective amenable agents independently
computing optimal solutions
line work presented opens area future integrating frameworks
sequential elements game theory bayesian learning interactive settings
particular one avenues future centers proving formal properties
pomdps establishing clearer relations solutions pomdps flavors
equilibria another concentrates developing efficient approximation techniques solving
pomdps pomdps development approximate approaches pomdps crucial
moving beyond toy one promising approximation technique working particle
filtering devising methods representing pomdp solutions without assumptions
whats believed agents beliefs mentioned spite complexity
interactive state space seem intuitive representations belief partitions corresponding
optimal policies analogous pomdps issues include suitable
choice priors ways fulfill absolute continuity condition needed
convergence probabilities assigned alternative interactions kalai lehrer


acknowledgments
supported national science foundation career award iri
nsf award iri

appendix proofs
proof propositions start proposition applying bayes theorem


bti ist p r ist oti
bi


bt
p r ist oti




p r oi ai bi

p

ist bit ist p r ist oti

p
p

p r ist
ist bit ist p r ist oti
aj
j

j
p
p


p r ist




ist bit ist p r ist oti
j
j

j
p
p


ist p r ist ist
ist bit ist p r
j mj p r ot
j
p
p


p r ist ist
ist bit ist p r
j mj p r ot
j
p
p

ot p r ist ist
ist bit ist p r

j mj oi
j

looking kolmogorov complexity li vitanyi possible way assign priors



fig mytrasiewicz oshi

simplify term p r ist ist let us substitute interactive state ist components mj interactive states intentional ist st jt st btj bjt

p r ist ist p r st btj bjt ist
p r btj st bjt ist p r st bjt ist
p r btj st bjt ist p r bjt st ist p r st ist
p r btj st bjt ist bjt bjt ti st st

b tj
mj subintentional ist st mtj st htj

p r ist ist p r st htj
b tj ist
b tj ist
b tj ist p r st
p r htj st
b tj ist p r bjt st ist p r st ist
p r htj st


p r hj
b tj ist
b tj ti st st
b

j

joint action pair may change physical state third term right hand
side eqs captures transition utilized mnm assumption replace
second terms equations boolean identity functions bjt bjt
b
b tj
j
respectively equal two frames identical otherwise let us turn attention
first terms mj ist ist intentional
p
p r btj st bjt ist ot p r btj st bjt ist otj p r otj st bjt ist
pj
ot p r btj st bjt ist otj p r otj st bjt
pj

ot
ot jt bt
j
j aj oj bj oj st



j

else subintentional

p r htj st
b tj ist





p



po j


po j
otj

b tj ist
b tj ist otj p r otj st
p r htj st

b tj
b tj ist otj p r otj st
p r htj st



ot
k append ht
j
j oj hj oj st




eq first term right hand side agent js belief update se j bt
j j oj
generates belief state equal btj similarly eq first term appending otj
ht
htj k kronecker delta function second terms right hand
j
side equations mno assumption makes possible replace p r st bt
j

j

oj st otj p r otj st
b tj oj st otj respectively
let us substitute eq eq
p

ot
bt bt ti st st
p r ist ist ot jt bt
j
j
j aj oj bj oj
j
j



substituting eq eq get
p


ot
p r ist ist ot k append ht
b
b tj
j
j oj hj oj
j
j

ti st st





fia f ramework equential p lanning ulti agent ettings

replacing eq eq get
p
p



p r
jt oi st oti ot jt bt
ist bi
j j j bj
j

j
j
oj st otj bjt bjt ti st st

bti ist

p



similarly replacing eq eq get

p
p

ot

p r
bti ist ist bt

j mj oi


j
p






bj
b tj ti st st
ot k append hj oj hj oj oj
j

arrive final expressions belief update removing terms

b
b tj changing scope first summations
j
mj interactive states intentional

bjt bjt

p
p

ot
bt ist p r
bti ist ist

j j oi
b
bjt
j
j
p







ot jt bj aj oj bj oj oj ti






j

else subintentional
p
p


ot
ist p r
bti ist ist
bi

j mj oi


b
b
j
j
j
p
ht st ot st st


ot k append ht
j
j
j
j
j



j

since proposition expresses belief bti ist terms parameters previous time step
proposition holds well
present proof theorem note equation defines value
iteration pomdps rewritten following form u n hu n h b b
backup operator defined
hu n max h ai u n
ai ai

h ai b r
h ai u

p


bi eri ai

p

oi

p r oi ai bi u hsei bi ai oi

b set bounded value functions u lemmas establish important
properties backup operator proof lemma given proof lemma follows
thereafter
proof lemma select arbitrary value functions v u v l u l l
l let l arbitrary type agent


fig mytrasiewicz oshi





p

p

hv l max
oi p r oi ai bi v hsei l bi ai oi
bi eri ai
ai ai
p
p
bi eri ai oi p r oi ai bi v hsei l bi ai oi
p
p



b
eri ai
oi p r oi ai bi u hsei l bi ai oi

p
p
max
oi p r oi ai bi u hsei l bi ai oi
bi eri ai
ai ai

hu l

since l arbitrary hv hu
proof lemma assume two arbitrary well defined value functions v u v u
lemma follows hv hu let l arbitrary type agent let ai
action optimizes hu l
hu l hv l



p

max sumis bi eri ai oi p r oi ai bi u sei l bi ai oi hi
ai ai

p
p
max
bi eri ai
oi p r oi ai bi v sei l bi ai oi hi
ai ai
p
p
bi eri ai oi p r oi ai bi u sei l bi ai oi hi
p
p



oi p r oi ai bi v sei l bi ai oi hi
bi eri ai
p


oi p r oi ai bi u sei l bi ai oi hi
p

oi p r oi ai bi v se

l bi ai oi hi
p



oi p r oi ai bi u sei l bi ai oi hi v sei l bi ai oi hi
p
oi p r oi ai bi u v
u v

supremum norm symmetrical similar derived hv l hu l
since l arbitrary contraction property follows e hv hu v u
lemmas provide stepping stones proving theorem proof theorem follows
straightforward application contraction mapping theorem state contraction
mapping theorem stokey lucas
theorem contraction mapping theorem complete metric space
contraction mapping modulus
exactly one fixed point u
sequence u n converges u
proof theorem follows


fia f ramework equential p lanning ulti agent ettings

proof theorem normed space b complete w r metric induced supremum norm lemma establishes contraction property backup operator h theorem substituting h convergence value iteration pomdps unique fixed
point established
go piecewise linearity convexity pwlc property value function
follow outlines analogous proof pomdps hausktecht smallwood
sondik
let r real valued bounded function let space real valued
bounded functions b define inner product
definition inner product define inner product h b r
x
h bi
bi


next lemma establishes bilinearity inner product defined
lemma bilinearity r f g b b following equalities
hold
hsf tg bi shf bi thg bi
hf sb ti shf bi thf
ready give proof theorem theorem restates theorem mathematically proof follows thereafter
theorem pwlc value function u n finitely nested pomdp piece wise linear
convex pwlc mathematically
u n l max
n


x

bi n

n



proof theorem basis step n
bellmans dynamic programming equation
u max
ai

x

bi er ai





p
eri ai aj r ai aj p r aj mj eri represents expectation
r w r agent js actions eq represents inner product lemma inner product
linear bi selecting maximum set linear vectors hyperplanes obtain pwlc
horizon value function
inductive hypothesis suppose u n l pwlc formally
u n l max
n



p

max

n

bi

n



p

n

mj imj bi

n




p

mj smj bi

n





fig mytrasiewicz oshi

inductive proof u n l pwlc

u n l max





x


bt
eri ist



x


n
p r oti
l
bi u

oti

ist

inductive hypothesis

p

er ist
u n l max

ist bi






p

oti


p r oti
bi

max

n n

p


n ist
ist bi








let l bt
ai oi index alpha vector maximizes value b se bi ai oi


p

er ist
u n l max

ist bi


ai

p
p


n
ot p r oti
ist bi l bt ot
bi








second equation inductive hypothesis

p
p

er ist
n

u l max

ot p r oi ai bi
ist bi










p


n
ist mtj imj bi l bt ot






p


n
ist mtj smj bi l bt ot




substituting bti appropriate belief updates eqs get

p
p


er ist
u n l max

oti p r oi ai bi
ist bi


ai



p
p
p





p r aj j oi st oti
ist mtj imj
ist bi

j


p










ot oj oj jt bj aj oj bj bj bj ti



j

n

l b

ai oi



p
p
p




ist mt smj ist bi
p r aj mj oi st oti

j
j


p











ot oj oj k append hj oj hj
bj
b j ti
j

n

l b











fia f ramework equential p lanning ulti agent ettings


u n l max






p

p



p


er ist

ist bi




ist bi



ot
j
otj oj

p




j



p

oti



p

ist mtj imj



p r


oi st oti
j
j


st
bt bt
jt bt
j aj oj bj j j ti



n

l b

ai oi



p
p
p




p r aj mj oi st oti
ist mt smj ist bi

j
j


p

ht
st st
ot ojt st otj k append ht


b


b
j
j
j
j
j
j

n

l b









rearranging terms equation
u n




p
p p

er ist


max
b




l
oti
ist mtj imj



im
j
j




p
p


p r aj j oi st oti ot ojt st otj

j
j



n







jt bj aj oj bj bj bj ti
l bt ot




p
p
p
p
ist mt smj bit ist eri ist
oti
ist mtj smj
oti

j


p
p
p r ajt mt
oi st oti ot ojt st otj

j

j
j



n



l b
b
b tj ti st st
k append ht

j
j oj hj
ai oi


p

n ist
max
ai
imj bi
ist mt
j



p



n
ist mt smj bi ai
j

therefore
u n

l

max
n
n





p



p


n ist
ist mt
imj bi
j




n ist
smj bi
ist mt
j
p

n ist maxhbt n
max
ist bi

n

n





fig mytrasiewicz oshi

mjt ist intentional n n
n ist

eri ist




p p

p


p r
j j



oi ist oti
ot ist mt imj

j
j


p










ot oj isj oj jt bj aj oj bj bj bj ti



j

n

l b









mjt subintentional n n
n ist

eri ist




p p

p


p r
j j



oi ist oti
ot ist mt smj


j
j


p











bj
b j ti
ot oj isj oj k append hj oj hj



j

n

l b

ai oi


eq inner product lemma value function linear b
furthermore
maximizing set linear vectors hyperplanes produces piecewise linear convex value
function

references
ambruster w boge w bayesian game theory moeschlin pallaschke eds game
theory related topics north holland
aumann r j interactive epistemology knowledge international journal game theory pp

aumann r j heifetz incomplete information aumann r hart eds handbook
game theory economic applications iii chapter elsevier
battigalli p siniscalchi hierarchies conditional beliefs interactive epistemology
dynamic games journal economic theory pp
bernstein givan r immerman n zilberstein complexity decentralized control
markov decision processes mathematics operations
binmore k essays foundations game theory blackwell
boutilier c sequential optimality coordination multiagent systems proceedings
sixteenth international joint conference artificial intelligence pp
boutilier c dean hanks decision theoretic structural assumptions computational leverage journal artificial intelligence
brandenburger power paradox recent developments interactive epistemology
tech rep stern school business york university http stern nyu edu abranden
brandenburger dekel e hierarchies beliefs common knowledge journal economic
theory
dennett intentional systems dennett ed brainstorms mit press
fagin r r geanakoplos j halpern j vardi hierarchical modeling
knowledge common knowledge international journal game theory pp
fagin r r halpern j moses vardi reasoning knowledge mit press
fudenberg levine k theory learning games mit press


fia f ramework equential p lanning ulti agent ettings

fudenberg tirole j game theory mit press
gmytrasiewicz p j durfee e h rational coordination multi agent environments autonomous agents multiagent systems journal
harsanyi j c games incomplete information played bayesian players management
science
hauskrecht value function approximations partially observable markov decision processes
journal artificial intelligence pp
hausktecht control stochastic domains imperfect information ph thesis
mit
hu j wellman p multiagent reinforcement learning theoretical framework fifteenth international conference machine learning pp
kadane j b larkey p subjective probability theory games management science

kaelbling l p littman l cassandra r acting partially observable
stochastic domains artificial intelligence
kalai e lehrer e rational learning leads nash equilibrium econometrica pp
koller milch b multi agent influence diagrams representing solving games seventeenth international joint conference artificial intelligence pp seattle washington
li vitanyi p introduction kolmogorov complexity applications springer
littman l markov games framework multi agent reinforcement learning proceedings
international conference machine learning
lovejoy w survey algorithmic methods partially observed markov decision processes
annals operations
madani hanks condon undecidability probabilistic related
stochastic optimization artificial intelligence
mertens j f zamir formulation bayesian analysis games incomplete information
international journal game theory
monahan g e survey partially observable markov decision processes theory
management science
myerson r b game theory analysis conflict harvard university press
nachbar j h zame w r non computable strategies discounted repeated games economic
theory
nair r pynadath yokoo tambe marsella taming decentralized pomdps towards
efficient policy computation multiagent settings proceedings eighteenth international
joint conference artificial intelligence ijcai
ooi j wornell g w decentralized control multiple access broadcast channel
proceedings th conference decision control
papadimitriou c h tsitsiklis j n complexity markov decision processes mathematics
operations
russell norvig p artificial intelligence modern second edition prentice hall
smallwood r sondik e j optimal control partially observable markov decision
processes finite horizon operations pp
stokey n l lucas r e recursive methods economic dynamics harvard univ press




