journal artificial intelligence

submitted published

risk sensitive reinforcement learning applied control
constraints
peter geibel

pgeibel uos de

institute cognitive science ai group
university osnabruck germany

fritz wysotzki

wysotzki cs tu berlin de

faculty electrical engineering computer science ai group
tu berlin germany

abstract
consider markov decision processes mdps error states error
states states entering undesirable dangerous define risk
respect policy probability entering state policy
pursued consider finding good policies whose risk smaller
user specified threshold formalize constrained mdp two criteria
first criterion corresponds value function originally given
risk formulated second criterion function cumulative return
whose definition independent original value function present model free
heuristic reinforcement learning aims finding good deterministic policies
weighting original value function risk weight parameter
adapted order feasible solution constrained good
performance respect value function successfully applied
control feed tank stochastic inflows lies upstream distillation
column control task originally formulated optimal control
chance constraints solved certain assumptions model obtain
optimal solution power learning used even
restrictive assumptions relaxed

introduction
reinforcement learning area provides range techniques applicable difficult nonlinear stochastic control see e g sutton barto
bertsekas tsitsiklis reinforcement learning rl agent considered
learns control process agent able perceive state process
acts order maximize cumulative return real valued reward
signal often experiences process used improve agents policy instead
previously given analytical model
notion risk rl related fact even optimal policy may perform
poorly cases due stochastic nature risk sensitive rl
approaches concerned variance return worst outcomes
e g coraluppi marcus heger neuneier mihatsch see
discussion section take alternative view risk defined geibel
concerned variability return occurrence errors
c

ai access foundation rights reserved

figeibel wysotzki

undesirable states underlying markov decision process mdp means
address different class compared approaches referring variability
return
consider constrained mdps two criteria usual value function risk second value function value optimized risk
must remain specified threshold describe heuristic
weighted formulation finds feasible policy original constrained
order offer insight behavior investigate application simple grid world discounted criterion function
apply stochastic optimal control continuous states
set feasible solutions restricted constraint required hold
certain probability thus demonstrating practical applicability
consider control feed tank lies upstream distillation column respect
two objectives outflow tank required stay close specified value
order ensure optimal operation distillation column tank level
substance concentrations required remain within specified intervals certain
admissible chance constraint violation
li wendt arellano garcia wozny formulate quadratic
program chance constraints e g kall wallace relaxed nonlinear
program case gaussian distributions random input variables systems
whose dynamics given linear equations nonlinear program solved
sequential quadratic programming
note li et al involves simulation estimation
gradients chance constraints li et al p q learning watkins
watkins dayan sutton barto learning
simulating episodes estimating value risk states tank control task
correspond measure deviation optimal outflow probability
constraint violation respectively
contrast li et al rl applicable systems
continuous state spaces whose system dynamics governed nonlinear equations
involve randomization noise arbitrary distributions random variables
makes prior assumptions aspect special property
learning holds true e g q learning rl
convergence q learning combined function approximation techniques necessary
continuous state spaces cannot guaranteed general e g sutton barto
holds true nevertheless rl successfully applied
many difficult continuous state spaces nonlinear dynamics see e g
sutton barto crites barto smart kaelbling stephan debes
gross wintrich wintrich
constraint seen relation domains variables restricting possible values
variables constraint c c x xn random constraint hold certain
probability chance constrained programming particular stochastic programming
considers constrained optimization containing random variables called chance
constraints form p c p p formulated



firisk sensitive reinforcement learning

article organized follows section rl framework described section reviews related work risk sensitive approaches section describes
risk sensitive rl section elucidate heuristic learning solving
constrained weighted formulation section describe application
grid world tank control task described section section
experiments feed tank control described section concludes short
summary outlook

rl framework
rl one considers agent interacts process controlled
discrete time step agent observes state x takes action u general
depends x action agent causes environment change state x
according probability px u x section consider set states x
finite set
action set agent assumed finite allowed depend
current state state x agent uses action set u x possible actions
taking action u u x agent receives real valued reinforcement signal rx u x
depends action taken successor state x case random reward
signal rx u x corresponds expected value markov property mdp requires
probability distribution successor states one rewards depend
current state action distributions change additional
information past states actions rewards considered e independent
path leading current state
aim agent policy selecting actions maximizes
cumulative reward called return return defined
r


x

rt





random variable rt denotes reward occurring th time step
agent uses policy let x x x denote corresponding probabilistic sequence
states ui sequence actions chosen according policy
constant discount factor allows control influence future
rewards expectation return
h



v x e r x x



defined value x respect well known exist stationary

deterministic policies v x optimal maximal every state x stationary deterministic policy function maps states actions particularly defined
independent time markovian independent history work
use term maximum value policies instead optimal policies distinguish
minimum risk policies optimal sense see section
usual define state action value function



h



q x u e r v x x x u u




figeibel wysotzki

q x u expected return agent first chooses action u acts according
subsequent time steps optimal q function q optimal policies
unique optimal values v derived x argmaxu q x u v x q x x
q computed watkins q learning
rl one general distinguishes episodic continuing tasks treated
framework see e g sutton barto episodic tasks agent may
reach terminal absorbing state time reaching absorbing state
agent stays executes dummy action reward defined rt
learning agent restarted according distribution
initial states reached absorbing state

related work
p


random variable r
rt return used define value state possesses
certain variance risk averse approaches dynamic programming dp reinforcement learning concerned variance r worst outcomes
example worst case control e g coraluppi marcus heger
worst possible outcome r optimized risk sensitive control use exponential utility functions e g liu goodwin koenig
koenig simmons liu goodwin koenig b borkar return r
transformed reflect subjective measure utility instead maximizing
expected value r objective maximize e g u log e er
parameter r usual return shown depending parameter
policies high variance v r penalized enforced value criterion introduced heger seen extension worst case control
bad outcomes policy occur probability less neglected
neuneier mihatsch give model free rl
parameterized transformation temporal difference errors occurring see mihatsch
neuneier parameter transformation allows switch riskaverse risk seeking policies influence parameter value function cannot
expressed explicitly
view risk concerned variance return worst possible
outcomes instead fact processes generally possess dangerous undesirable states think chemical plant temperature pressure exceeding
threshold may cause plant explode controlling plant return corresponds plants yield seems inappropriate let return reflect
cost explosion e g human lives affected
work consider processes undesirable terminal states seemingly straightforward way handle error states system provide high
negative rewards systems enters error state optimal policy avoid
error states general drawback fact unknown
large risk probability entering error state moreover may want provide
threshold probability entering error state must exceeded
agents policy general impossible completely avoid error states risk
controllable extend precisely agent placed state



firisk sensitive reinforcement learning

x follow policy whose risk constrained parameter
reflects agents risk averseness goal minimization risk
maximization v risk kept threshold
markowitz considers combination different criteria equal discount
factors context portfolio selection risk selected portfolio related
variance combined weighted criteria markowitz introduces notion
e v space notion risk related variance v depends
occurrence error states mdp therefore risk conceptually independent v
see e g tank control described section
idea weighting return risk markowitz freund heger leads
expected value minus variance criterion e r kv r k parameter
use idea computing feasible policy finding good policy
constrained risk regard probability entering error state value
risk weighted weight value weight risk value
increased giving value weight compared risk risk state
becomes larger user specified threshold
considering ordering relation tuples values learning
fixed value related artdp gabor kalmar szepesvari
article gabor et al additionally propose recursive formulation
mdp constraints may produce suboptimal solutions applicable
case requires nonnegative reward function
noted aforementioned approaches variability
return suited grid world discussed section
tank control task section risk related parameters variables state
description example grid world policies worst case
outcome regard approaches variance found policy leading
error states fast possible higher variance one reaches
goal states fast possible policy small variance therefore large risk
respect probability entering error state means address
different class control underpin claim section
fulkerson littman keim sketch framework probabilistic similar although complementary notion
safety fulkerson et al define safety probability reaching goal state see
buridan system kushmerick hanks weld fulkerson et al discuss
finding plan minimum cost subject constraint safety see
blythe episodic mdp goal states safety minus risk
continuing tasks absorbing states neither goal error states
safety may correspond smaller value fulkerson et al manipulate scale
uniform step reward undiscounted cost model order enforce agent reach
goal quickly see koenig simmons contrast consider
discounted mdps neither require existence goal states although
change original reward function section seen systematic
dealing idea fulkerson et al consists modification
relative importance original objective reaching goal safety contrast
aforementioned approaches belonging field probabilistic


figeibel wysotzki

operate previously known finite mdp designed online learning
uses simulated actual experiences process use neural network
techniques applied continuous state processes
dolgov durfee describe computes policies
constrained probability violating given resource constraints notion risk
similar described geibel given dolgov durfee
computes suboptimal policies linear programming techniques require
previously known model contrast cannot easily extended
continuous state spaces dolgov durfee included discussion dp approaches
constrained mdps e g altman generalize continuous state
spaces tank control task require known model described
feinberg shwartz constrained two criteria applicable
case requires discount factors strictly smaller
limited finite mdps
downside risk common notion finance refers likelihood security
investment declining price amount loss could
potential decline scientific literature downside risk e g bawas fishburn
markowitz roy investigates risk measures particularly consider
case return lower mean value target value encountered
contrast notion risk coupled return r fact state
x error state example parameters describing state lie outside
permissible ranges state lies inside obstacle may occur
robotics applications

risk
define notion risk precisely consider set
x



error states error states terminal states means control agent
ends reaches state allow additional set non error terminal states

define risk x respect probability state sequence
xi x x generated executing policy terminates error state
x
definition risk let policy let x state risk defined




x p xi x x



definition x holds x x x
states risk depends action choices policy
following subsection consider computation minimum risk policies
analogous computation maximum value policies


firisk sensitive reinforcement learning

risk minimization
risk considered value function defined cost signal r see
augment state space mdp additional absorbing state
agent transfered reaching state state introduced technical
reasons
agent reaches state reward signals r r become zero
set r r agent reaches error state states
longer absorbing states cost function r defined


rx u x



x x
else



construction cost function r episode states actions costs
starting initial state x contains exactly cost r error state
occurs process enter error state sequence r costs contains
zeros therefore probability defining risk expressed expectation
cumulative return
proposition holds


x e


x


discount factor




ri x x




proof r r probabilistic sequence costs related risk stated
p

holds r def
ri trajectory leads error state otherwise
p
ri means return r bernoulli random variable
probability q r corresponds risk x respect bernoulli random
variable holds er q see e g ross notice introduction together
fact r occurs transition error state
iwhen

hp



entering respective error state ensures correct value e
ri x x
error states x q e
similar q function define state action risk
h

q x u e r x x x u u


x





px u x rx u x x

x






minimum risk policies obtained variant q learning geibel

maximized value constrained risk
general one interested policies minimum risk instead want provide
parameter specifies risk willing accept let x x set
states interested e g x x x x distinguished


figeibel wysotzki

starting state x state x x let px probability selecting starting
state value
x
v def
px v x

xx

corresponds performance states x consider constrained
max v



x x x





subject
policy fulfills called feasible depending set feasible policies
may empty optimal policies generally depend starting state nonstationary randomized feinberg shwartz gabor et al geibel
restrict considered policy class stationary deterministic policies constrained
generally well defined x singleton need
stationary deterministic policy optimal states x feinberg shwartz
shown case two unequal discount factors smaller
exist optimal policies randomized markovian time step n e
depend history may non stationary randomized stationary
deterministic particularly markovian time step n onwards feinberg shwartz
give dp case cp feinberg shwartz cannot
applied case generalize continuous state
spaces case equal discount factors shown feinberg shwartz
fixed starting state exist optimal stationary randomized policies
case one constraint consider one action stationary deterministic
policy e one state policy chooses randomly two
actions

learning
reasons efficiency predictability agents behavior
said end last section restrict consideration stationary deterministic policies following present heuristic aims
computing good policy assume reader familiar watkins q learning
watkins watkins dayan sutton barto
weighting risk value
define third value function v state action value function q
weighted sum risk value
v x v x x
q x u





q x u q x u




parameter determines influence v values q values compared
values q values v corresponds negative means


firisk sensitive reinforcement learning

maximization v lead minimization maximization
v leads lexicographically optimal policy unconstrained unweighted criteria
one compares performance two policies lexicographically criteria
ordered large values original value function multiplied dominates
weighted criterion
weight successively adapted starting see section adaptation
discuss learning fixed proceeds
learning fixed
fixed value learning computes optimal policy
resembles q learning artdp gabor
et al
learning agent estimates qt qt time thus estimate
qt performance current greedy policy policy selects best
action respect current estimate qt values updated example
state transitions let x current state u chosen action x observed
successor state reward risk signal example state transition given
r r respectively x greedy action defined following manner action
u preferable u qt x u qt x u holds equality holds action
higher qt value preferred write u u u preferable u
let u greedy action x respect ordering agents
estimates updated according
qt x u qt x u r qt x u



qt x u qt x u r qt x u




qt
x u qt x u
x u q



every time chosen learning rate set afterwards decreases
time cp sutton barto
fixed aims computing good stationary deterministic policy
weighted formulation feasible original constrained existence
optimal stationary deterministic policy weighted convergence
learning guaranteed criteria discount factor e
even case q forms standard criterion function
rewards r r consider risk second criterion function implies
ensure convergence case required
exists least one proper policy defined policy reaches absorbing state
probability one improper policies yield infinite costs see tsitsiklis b
policies proper case application example conjecture
case convergence possibly suboptimal policy guaranteed mdp forms
directed acyclic graph dag cases oscillations non convergence may occur
optimal policies weighted generally found considered
policy class stationary deterministic policies constrained


figeibel wysotzki

adaptation
learning starts agent chooses performs learning steps lead
time approximated minimum risk policy policy allows agent
determine constrained feasible
afterwards value increased step step risk state x becomes
larger increasing increases influence q values compared
q values may cause agent select actions higher value
perhaps higher risk increasing agent performs learning
steps greedy policy sufficiently stable aimed producing optimal
deterministic policy computed q q values old e estimates





q q used initialization computing
aim increasing give value function v maximum influence possible
means value maximized needs chosen user
adaptation provides means searching space feasible policies

discounted risk
order prevent oscillations section case may
advisable set corresponding discounted risk defined
x

e


x





ri x x




values ri positive holds x x states x discounted risk x gives weight error states occurring near future depending
value
finite mdp fixed convergence optimal stationary
policy weighted formulation guaranteed q x forms
standard criterion function rewards r r terminating adaptation
case risk state x becomes larger one might still use original
undiscounted risk x learning done discounted version x e
learning maintain two risk estimates every state major
notice case effect considering weighted criterion
v corresponds modifying unscaled original reward function r adding
negative reward agent enters error state set optimal stationary
deterministic policies equal cases added absorbing state
single dummy action neglected
section experiments case x x finite
state space found sections consider application example
infinite state space x x

grid world experiment
following study behaviour learning finite mdp
discounted criterion contrast continuous state case discussed next


firisk sensitive reinforcement learning

e
e
e

e
e g
e e
e
e
e
c
e
e g
e e

e g
e
e
b
e
e g
e e e e e e
e g
e
e

e
e g
e e e e e e

g

e





e

e





e

e e
g




e e

figure example grid world x horizontal vertical explanation see
text b minimum risk policy unsafe states c maximum value
policy unsafe states policy
unsafe states

section function approximation neural networks needed value
function risk stored table grid world chosen
x x state graph dag implies
stationary policy optimal every state x although oscillations therefore
expected found stabilizes feasible policy
learning rate tends zero investigated use discounted risk
prevents oscillatory behaviour
consider grid world depicted figure empty field denotes
state es denote error states two gs denote two goal states describe
states pairs x x e
additional absorbing
state depicted
chosen error states lower e extremal values x
dangerous one goal states placed next error states safer
part state space
agent actions u action u u takes agent
denoted direction possible probability agent transported
desired direction one three remaining directions
agent receives reward enters goal state agent receives reward
every case noted explicit punishment entering
error state implicit one agent enters error state current
episode ends means agent never receive positive reward
reached error state therefore try reach one goal states
try fast possible


figeibel wysotzki

chosen x x equal probabilities px states
although convergence cannot guaranteed case experimental yields feasible policy
selected order illustrate behaviour
computed minimum risk maximum value policy figure b shows
minimum risk policy though reward function r defined plays role
minimum risk policy agent tries reach one two goal states
goal state probability reaching error state clearly respect
value function v policy figure b optimal e g state agent
tries reach distant goal causes higher discounting goal reward
minimum risk policy figure b safe states defined states
risk minimum risk policy estimated mean value v
figure c maximum value policy shown maximum value policy
optimizes value without considering risk estimated value v
thus performs better minimum risk policy figure b risk
become greater starts computes
minimum risk policy figure b increased step step risk state
changes value lower value stops
policy computed shown figure obviously lies minimum risk
policy figure b maximum value policy figure c
applied discounted version risk grid
world discounted risk used learning whereas original risk
used selecting best weight parameters described modified
produced policy depicted figure seemingly grid world
example oscillations present major
tank control task described next section holds


stochastic optimal control chance constraints
section consider solution stochastic optimal control chance
constraints li et al applying risk sensitive learning method
description control
following consider plant depicted figure task control
outflow tank lies upstream distillation column order fulfill several
objectives described purpose distillation column separation
two substances consider finite number time steps n outflow
tank e feedstream distillation column characterized flowrate
f controlled agent substance concentrations c c
n
purpose control designed keep outflow rate f near specified
optimal flow rate fspec order guarantee optimal operation distillation column


firisk sensitive reinforcement learning

f
c
c

distillation column

f
c
c

ymax
h c c
ymin

f

tank

fspec
c min c max
c min c max

figure plant see text description
quadratic objective function goal specified
min

f f n

n

x

f fspec





values obey
n fmin f fmax



tank characterized tank level holdup h h
constant footprint tank tank level concentrations
c c depend two stochastic inflow streams characterized flowrates
f f inflow concentrations c j c j substances j
linear dynamics tank level given


x



fj f




x
fj cj ci

j



j

dynamics concentrations given
ci ci

initial state system characterized
c c c c



tank level required fulfill constraint ymin ymax concentrations inside tank correspond concentrations outflow substance
concentrations c c required remain intervals c min c max


figeibel wysotzki

c min c max respectively assume inflows inflow concentrations
ci j random governed probability distribution li et al
assume multivariate gaussian distribution randomness variables
tank level feedstream concentrations may violate given constraints
therefore formulate stochastic constraint




p ymin ymax ci min ci ci max n p



expression called joint chance constraint p corresponds
permissible probability constraint violation value p given user
stochastic optimization sop yc defined quadratic objective function describing sum quadratic differences outflow rates fspec
linear dynamics tank level nonlinear dynamics concentrations
initial state given chance constraint
li et al describe simpler sop concentrations considered
see figure sop use cumulative inflow f f f description
tank level dynamics see sop describes dynamics linear system
li et al solve sop relaxing nonlinear program solved sequential
quadratic programming relaxation possible sop linear system
multivariate gaussian distribution assumed solving nonlinear systems sop yc
non gaussian distributions difficult e g wendt li wozny
achieved rl

min

f f n

subject
n


n

x

f fspec





fmin f fmax


f f




p ymin ymax n p






figure sop
note control f optimization depends time step
means solutions sop yc sop yield open loop controls
dependence initial condition moving horizon taken
design closed loop control discuss issue goes beyond scope



firisk sensitive reinforcement learning

formulation reinforcement learning
rl instead analytical advantage probability distribution doesnt gaussian unknown state equations need
known nonlinear learning agent must access simulated
empirical data e samples least random variables
independent chosen state representation immediate reward defined
rx u x u fspec



u chosen action minus required rl value function
maximized reward signal depends action chosen current
successor state
work consider finite discretized action sets although
extended continuous action sets e g actor critic method sutton
barto following assume interval fmin fmax discretized
appropriate manner
process reaches error state one constraints respectively violated process artificially terminated transferring agent
additional absorbing state giving risk signal r v value error states
set zero controller could choose action fspec first constraint
violation subsequent constraint violations make things worse respect
chance constraints respectively
definition state space
following consider design appropriate state spaces
open loop control olc closed loop control clc
open loop control
note sop yc sop time dependent finite horizon
control f xt f depends means state feedback
resulting controller open looped respect state definition xt
markov property defined section clearly holds probabilities rewards defining
v markov property hold rewards defining xt
implies agent information state process including
information history form past action agent gets idea
current state process therefore inclusion history information changes
probability r markov property violated including past actions
state description ensures markov property r markov property therefore
recovered considering augmented state definition
xt ut u



past actions ut u first action u depends fixed initial tank level
fixed initial concentrations second action depends first action e
initial tank level initial concentrations therefore learning


figeibel wysotzki

states open loop control original sop yc
sop
noted mdp risk depend past actions
future actions choice xt hidden state information
mdp markov property violated therefore probability
entering error state conditioned time step e p r changes
additionally conditioned past actions yielding value p r ut u
corresponding agent remembers past actions example agent
remembers past time steps current learning episode used
action f corresponding zero outflow conclude increased
probability tank level exceeds ymax e knowledge increased risk
hand remember past actions cannot know increased
risk knows index current time step carries less information
current state
well known markov property generally recovered including
complete state history state description xt state history contains
past time indices actions r costs tank control task action history
relevant part state history previous r costs necessarily zero
indices past time steps already given actual time known
agent therefore past rewards indices past time steps need
included expanded state although still complete state information
known agent knowledge past actions suffices recover markov property
respect state choice reward signal expectation
definition value function needed cp eq means
h



v x e r x x

n

x

f fspec



holds e direct correspondence value function objective
function sop yc sop
closed loop control
define alternative state space expectation needed
decided use state definition
xt c c



xt



sop yc
simpler sop learning state time dependent closed
loop controller achieve better regulation behavior open loop controller
reacts actual tank level concentrations whereas open loop control
agent access inflow rates concentrations
included state vector yielding improved performance controller


firisk sensitive reinforcement learning

parameter
n

ymin ymax

fspec
fmin fmax
rl yc clc
c
c
c min c max
c min c max

table parameter settings
value
explanation

number time steps

initial tank level
admissible interval tank level

constant see

optimal action value
interval actions discrete values





initial concentration subst
initial concentration subst
interval concentration
interval concentration

rl
definitions optimization defined via
p see set x see defined contain unique
starting state e x x experiments consider following instantiations
rl
rl clc reduced sop states xt x resulting closed loop controller clc
rl olc open loop controller olc reduced sop state space
defined action history time see eq starting state x
rl yc clc closed loop controller full sop yc states xt
c c x c c
solving rl olc yields action vector rl yc clc
rl clc state dependent controllers present fourth
natural rl yc olc offer additional insights
interpolation states used multilayer perceptrons mlps e g
bishop case rl olc extremely large state space dimensions n used radial basis function rbf networks case
rl yc clc rl clc produced faster stable robust
compared mlps
training respective networks used direct method corresponds
performing one gradient descent step current state action pair

estimate target value see e g baird estimate q given

r qt x u q r qt x u compare right sides update
equations


figeibel wysotzki














outflow rate

inflow

b



















time

c

omega











time

omega

outflow rate

outflow rate



















time










omega











time

figure rl clc inflow rates f runs b c example runs
policies e p holds fspec

experiments
section examine experimental obtained tank control task
section discuss linear case compare li
et al linear case consider closed loop controller obtained solving
rl clc sect open loop controller related rl rl olc
sect closed loop controller discuss non zero covariances
variables different time steps nonlinear case discussed section
rl clc rl olc
start simplified rl clc rl olc derived sop
discussed li et al sop concentrations considered
one inflow rate f f f parameter settings table first five
lines taken li et al minimum maximum values actions
determined preliminary experiments
li et al define inflows f f gaussian distribution
mean vector





firisk sensitive reinforcement learning




risk


value



weighted













xi






figure rl clc estimates risk x value v x v x




v x x different values

covariance matrix given





c

r

r



n r n


n r n






n










correlation inflows time j defined
rij rji j



n j n li et al inflow rates ten example
runs depicted figure
rl clc constraints tank level
start presentation rl clc control
e outflow f depends time tank level x x
overall performance policy defined corresponds performance x




v v x


holds x v x value respect policy learned
weighted criterion function v see respective risk


x




figure estimated risk x estimated value v x depicted


different values estimate risk x value v x
values policies presented following estimated learning note
order enhance readability denoted learned policy



figeibel wysotzki









xi

figure rl clc difference weighted criteria explanation see text

increase given fixed value p admissible probability constraint violation

appropriate p obtained value risk x lower

p maximum v x due variation performance see
fig found works better selecting maximum estimate






weighted criterion v x v x x shown figure
outflow rate f control variable different values found figure bc note rates certain variance since depend probabilistic tank
level randomly picked one example run value found control
values f tend fspec increasing values e decreasing values p
correlations definition covariance matrix reveals high
correlation inflow rates neighboring time steps order better account
possible include information past time steps state description time
level changes according inflow rate f investigated inclusion
past values inflow rates measured could included
state vector former rewards need included depend past tank
levels e represent redundant information
compared performance augmented state space
defined xt depth history normal state space
xt history fig shows












v v


z
x



z
x

e difference weighted criteria starting state respect learned
policies history history note starting state x past values
defined curve figure runs mainly means
augmented state space better performance many values note


firisk sensitive reinforcement learning



b



risk






outflow rate





value
































xi









time









figure rl olc estimates risk x value v x increasing


values b learned policy x v x

larger values original value function overweights risk cases
policy chooses outflow fspec approximated means
difference performance tends zero
similar quite pronounced effect observed history
length principle assume possible achieve even better performance
including full history tank levels state description tradeoff objective difficulty network training caused number
additional dimensions
rl olc history control actions
rl rl olc comprises state descriptions consisting action history
together time see eq starting state empty history e x
learning time dependent policy implicit dependence
learned policy therefore fixed vector actions f f forms feasible
general suboptimal solution sop figure


progression risk estimate e x value v x
different values found figure good ones

rl clc figure estimated minimum risk risk x grows
much faster rl clc risk figure

policy risk x depicted figure b contrast
policies rl clc see figure b c control values change different runs
comparison
table compared performance li et al rl clc
rl olc p p rl clc rl olc performed
learning runs respective learned policy risk x value v x
estimated test runs rl clc rl olc table shows mean
performance averaged runs together standard deviation parentheses


figeibel wysotzki

table comparison est squared deviation fspec e v x li et
al rl clc rl olc p p
smaller values better

li et al
rl clc
rl olc

p




p




found average policy determined rl clc performs better
obtained li et al respect estimated squared
deviation desired outflow fspec e respect v x policy obtained
rl olc performs better p worse p maximal achievable
probability holding constraints sd rl clc sd
rl olc li et al report p
neuneier mihatsch considers worst case outcomes
policy e risk related variability return neuneier mihatsch
learning interpolates risk neutral worst case criterion
limiting behavior exponential utility


risk
value





risk


value














kappa

figure risk value several values
learning neuneier mihatsch parameter
allows switch risk averse behavior risk neutral behavior
risk seeking behavior agent risk seeking prefers policies
good best case outcome figure shows risk probability constraint violation value
starting state regard policy computed neuneier
mihatsch obviously able maximum value policy yielding zero
deviation fspec corresponding choosing f fspec states learning
sensitive risk parameter reason worst case
best case returns policy chooses outflow correspond


firisk sensitive reinforcement learning



inflow

b







ymax
mu










mu

c max
c

ymin




c min





















time

















time

figure rl yc clc profiles two mode means
b tank level concentration c example runs
minimum risk policy

best return possible implying zero variance return
neuneier mihatsch variance approaches therefore unsuited
hand
rl yc clc constraints tank level concentrations
following consider full rl yc clc two inflows f f
assumed equal gaussian distributions distribution cumulative
inflow f f f described covariance matrix mean
vector see figure
order demonstrate applicability non gaussian distributions
chosen bimodal distributions inflow concentrations c c underlying
assumption upstream plants increased output
lower output e g due different hours weekdays
distribution inflow concentration ci characterized two gaussian
distributions means
k
k value k chosen beginning
run equal probability outcome means overall mean value
ci given profiles mean values modes found
figure ci given ci ci minimum maximum values
concentrations ci found table figure b note
concentrations controlled indirectly choosing appropriate outflow f
developing risk value starting state shown figure
resulting curves behave similar rl clc depicted figure
value risk increase seen covers relatively
broad range policies different value risk combinations


figeibel wysotzki




risk





value

























xi








figure rl yc clc estimated risk x value v x v x v x


x different values

minimum risk policy curves tank level concentration
c found figure b bimodal characteristics substance inflow
concentrations reflected c holds c c attainable minimum
risk increasing weight leads curves similar shown figures
assume minimum achievable risk decreased inclusion
additional variables e g inflow rates concentrations inclusion past
values discussed section treatment version action history
analogous section therefore conclude presentation experiments
point

conclusion
presented learning optimal policies constrained risk
mdps error states contrast rl dp approaches consider risk
matter variance return worst outcomes defined risk
probability entering error state
presented heuristic aims learning good stationary policies
weighted formulation weight original value function
increased order maximize return risk required stay
given threshold fixed weight finite state space converges
optimal policy case undiscounted value function case state
space finite contains cycles holds conjecture convergence
learning policy assume suboptimal weighted
formulation optimal stationary policy exists weighted formulation
feasible generally suboptimal solution constrained


firisk sensitive reinforcement learning

weighted combined adaptation heuristic searching space feasible stationary policies original constrained
us seems relatively intuitive conjecture better policies could found allowing
state dependent weights x modified adaptation strategy extending
considered policy class
successfully applied control outflow feed tank
lies upstream distillation column started formulation stochastic
optimal control chance constraints mapped risk sensitive learning
error states correspond constraint violation latter
solved weighted rl
crucial point reformulation rl design state space
found consistently performed better state information
provided learner time action history resulted large state
spaces poorer learning performance rbf networks together sufficient state
information facilitated excellent
must mentioned use rl together mlp rbf network
function approximation suffers usual flaws non optimality learned network
potential divergence learning process long learning times contrast
exact method priori performance guarantee given course posteriori
estimate performance learned policy made main advantage
rl method lies broad applicability tank control task achieved good
compared obtained mostly analytical
cases x theoretical investigations convergence
experiments required preliminary experiments shown oscillations may
occur behavior tends oscillate sensible policies without
getting bad although convergence usefulness policies remains
open issue
oscillations prevented discounted risk leads underestimation
actual risk existence optimal policy convergence learning
fixed guaranteed case finite mdp probabilistic
interpretation discounted risk given considering probability
exiting control mdp bertsekas investigation discounted
risk may worthwhile right example task long episodes
continuing e non episodic natural give larger weight error
states occurring closer current state
designed learning online means learning accomplished empirical data obtained interaction simulated
real process use neural networks allows apply processes
continuous state spaces contrast described dolgov durfee
applied case known finite mdp model obtained
case continuous state process finding appropriate discretization estimating state transition probabilities together reward function although
discretization prevents application dolgov durfees rl olc
dimensional state space encountered probably applied case
rl olc plan investigate point future experiments


figeibel wysotzki

question arises whether applied stochastic optimal
control types chance constraints consider conjunction chance
constraints
p c p p cn pn

ct constraint system containing variables time pt
respective probability threshold requires alternative rl formulation risk
state depends next reward time step
solution modified version rl difficult
ct allowed constraint system state variables depending things get involved several risk functions needed
state plan investigating cases future

acknowledgments thank dr pu li providing application example
helpful comments thank onder gencaslan conducting first experiments
masters thesis

references
altman e constrained markov decision processes chapman hall crc
baird l residual reinforcement learning function approximation proc th international conference machine learning pp morgan
kaufmann
bawas v optimal rules ordering uncertain prospects journal finance

bertsekas p dynamic programming optimal control athena scientific
belmont massachusetts volumes
bertsekas p tsitsiklis j n neuro dynamic programming athena scientific
belmont
bishop c neural networks pattern recognition oxford university press
oxford
blythe j decision theoretic ai magazine
borkar v q learning risk sensitive control mathematics operations

coraluppi marcus risk sensitive minimax control discrete time
finite state markov decision processes automatica
crites r h barto g elevator group control multiple reinforcement
learning agents machine learning
dolgov durfee e approximating optimal policies agents limited
execution resources proceedings eighteenth international joint conference
artificial intelligence pp aaai press


firisk sensitive reinforcement learning

feinberg e shwartz markov decision weighted discounted
criteria math operations
feinberg e shwartz constrained discounted dynamic programming math
operations
feinberg e shwartz constrained dynamic programming two discount
factors applications ieee transactions automatic control

fishburn p c mean risk analysis risk associated target returns
american economics review
freund r introduction risk programming model econometrica

fulkerson littman l keim g speeding safely multi criteria
optimization probabilistic proceedings fourteenth national
conference artificial intelligence p aaai press mit press
gabor z kalmar z szepesvari c multi criteria reinforcement learning
proc th international conf machine learning pp morgan kaufmann
san francisco ca
geibel p reinforcement learning bounded risk brodley e danyluk
p eds machine learning proceedings eighteenth international conference icml pp morgan kaufmann publishers
heger consideration risk reinforcement learning proc th international conference machine learning pp morgan kaufmann
kall p wallace w stochastic programming wiley york
koenig simmons r g risk sensitive probabilistic decision
graphs doyle j sandewall e torasso p eds kr principles knowledge representation reasoning pp san francisco california morgan
kaufmann
kushmerick n hanks weld probabilistic leastcommitment aaai pp
li p wendt arellano garcia wozny g optimal operation distillation
processes uncertain inflows accumulated feed tank aiche journal

liu goodwin r koenig risk averse auction agents rosenschein j
sandholm wooldridge yokoo eds proceedings second international joint conference autonomous agents multiagent systems aamas pp acm press
liu goodwin r koenig b risk averse auction agents aamas pp

markowitz h portfolio selection journal finance
markowitz h portfolio selection john wiley sons york


figeibel wysotzki

mihatsch neuneier r risk sensitive reinforcement learning machine learning
neuneier r mihatsch risk sensitive reinforcement learning michael
kearns sara solla c ed advances neural information processing
systems vol mit press
ross introduction probability academic press york
roy safety first holding assets econometrica
smart w kaelbling l p effective reinforcement learning mobile robots proceedings ieee international conference robotics
automation icra
stephan v debes k gross h wintrich f wintrich h control
scheme combustion processes reinforcement learning neural networks international journal computational intelligence applications

sutton r barto g reinforcement learning introduction mit
press
tsitsiklis j n asynchronous stochastic approximation q learning machine
learning
watkins c j c h learning delayed rewards ph thesis kings college
oxford
watkins c j c h dayan p q learning machine learning special
issue reinforcement learning
wendt li p wozny g non linear chance constrained process optimization
uncertainty ind eng chem res




