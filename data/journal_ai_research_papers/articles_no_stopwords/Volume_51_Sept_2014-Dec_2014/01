Journal Artificial Intelligence Research 51 (2014) 71-131

Submitted 5/14; published 9/14

Testability BDI Agent Systems
Michael Winikoff
Stephen Cranefield

michael.winikoff@otago.ac.nz
stephen.cranefield@otago.ac.nz

Department Information Science
University Otago
New Zealand

Abstract
deploying software system need assure (and stakeholders)
system behave correctly. assurance usually done testing system.
However, intuitively obvious adaptive systems, including agent-based systems,
exhibit complex behaviour, thus harder test. paper examine
obvious intuition case Belief-Desire-Intention (BDI) agents. analyse
size behaviour space BDI agents show although intuition correct,
factors influence size expected be. Specifically,
found introduction failure handling much larger effect size
behaviour space expected. discuss implications findings
testability BDI agents.

1. Introduction
Increasingly called upon develop software systems operate dynamic environments, robust face failure, required exhibit flexible behaviour, operate open environments. One approach developing systems
demonstrated effectiveness range domains use metaphor
software agents (Wooldridge, 2002). Agent-based systems increasingly finding
deployment wide range applications (e.g. Munroe, Miller, Belecheanu, Pechoucek,
McBurney, & Luck, 2006; Benfield, Hendrickson, & Galanti, 2006).
agent-based systems increasingly deployed, issue assurance rears head.
deploying system, need convince rely system (or
responsible fails) system will, fact, work. Traditionally,
assurance done testing1 . However, generally accepted adaptive systems
exhibit wide complex range behaviours, making testing hard. example:
Validation extensive tests mandatory . . . . However, task proved
challenging . . . . Agent-based systems explore realms behaviour outside peoples expectations often yield surprises. (Munroe et al., 2006, Section 3.7.2)
is, intuition agent systems exhibit complex behaviour, makes
hard test. paper explore intuition, focusing well known BeliefDesire-Intention (BDI) approach realising adaptive flexible agents (Rao & Georgeff,
1. Although considerable research formal methods context agent systems (Dastani,
Hindriks, & Meyer, 2010), yet ready real world application (see Section 7),
concerns scope work applicability (Winikoff, 2010).
c
2014
AI Access Foundation. rights reserved.

fiWinikoff & Cranefield

1991; Bratman, 1987), demonstrated practically applicable, resulting
reduced development cost increased flexibility (Benfield et al., 2006).
explore intuition agent systems hard test analysing
space possible behaviours BDI agents, is, number paths BDI
program, probability failure. focus BDI agents provide welldefined execution mechanism analysed, seek understand
complexities (and testability implications) adaptive intelligent behaviour
absence parallelism (since implications parallelism already well known).
derive number paths BDI program function various parameters (e.g. number applicable plans per goal failure fate). naturally
leads us consider number paths affected various parameters.
might expected, show intuition agent systems hard test
correct, i.e. agent systems large number paths. show BDI
agents harder test procedural programs, showing number paths
BDI program much larger number paths similarly-sized
procedural program.
contribution paper threefold. Firstly, confirms intuition BDI
programs hard test. Secondly, quantifying number paths,
function parameters BDI program. Thirdly, find surprising results
parameters influence number paths.
Although recently increasing interest testing agent systems (Zhang,
Thangarajah, & Padgham, 2009; Ekinci, Tiryaki, Cetin, & Dikenelli, 2009; Gomez-Sanz,
Bota, Serrano, & Pavon, 2009; Nguyen, Perini, & Tonella, 2009b), surprisingly little work determining feasibility testing agent systems first place.
Padgham Winikoff (2004, pp. 1719) analyse number successful executions
BDI agents goal-plan tree (defined Section 3), consider failure failure
handling analysis, consider testability implications. Shaw, Farwer
Bordini (2008) analysed goal-plan trees shown checking whether goal-plan
tree execution schedule respect resource requirements NP-complete.
different problem one tackle: concerned allocation
resources amongst goals, rather behaviour space.
briefly address number possible criticisms work, considering
existing work.
1. number paths useful metric assessing testability?
consider related area software testing (Section 1.1) argue
metric well established one, appropriate use assess testability.
2. Isnt obvious corollary complexity HTN planning?
consider detail HTN planning problem (Section 1.2) argue although
BDI execution cycle certain similarities HTN planning, differences
significant, and, particular, mean problem HTN planning
simply different problem testing BDI programs.
3. use combinatorial analysis, rather complexity analysis?
combinatorial analysis precise: yields formulae exact number
72

fiOn Testability BDI Agent Systems

paths, exact probabilities failure. latter (see Section 4.5)
informative order magnitude complexity. Additionally allows
us consider issues complexity analysis would address, effect
number failures number paths.
1.1 Software Testing
trying assess hard agent systems test. concretely, given BDI
agent program, want know hard program test. reduced
directly question test set adequacy. agent program P easy test precisely
extent exists test set adequate testing P ,
infeasibly large. Conversely, agent program P hard test extent
adequate test set would infeasibly large. words, hardness
testing program directly assessed size required test set adequate
respect suitable adequacy criteria.
many criteria used assess whether given set tests adequate (for recent overview, see Mathur, 2008). Given interested assessing
difficulty testing given program, clearly looking white box testing. Furthermore, working abstract goal-plan trees rather detailed programs
(see Section 2). means need consider control-flow based metrics, rather
data-flow, since abstract goal-plan tree contain data-flow information.
Focussing white box testing criteria control-flow based, basic
long-standing criterion assessing test set adequacy paths program
covered (Miller & Maloney, 1963). example, consider program following form.
3. ...
1. Input x

2. condition ...

5. endif

6. C

4. ... else B

two paths program: (1, 2, 3, 5, 6) (1, 2, 4, 5, 6),
adequate test set must least two tests adequate: one exercise first path,
another exercise second. case test set single test
inadequate, result part program executed testing.
obvious complication covering paths program loop result
infinite number paths, since loop potentially executed number
times. standard technique dealing bound length paths,
number executions loop (Zhu, Hall, & May, 1997, p. 375). Bounding execution
loops done either calculating upper bound number iterations based
data (Mathur, 2008, p. 53), considering paths loops executed
zero times one time (Mathur, 2008, p. 408).
One question might asked consider paths, rather weaker
criterion. Agent applications typically involve environments non-episodic. is,
environments history matters. means behaviour given plan goal
is, general, sensitive agents history, hence need consider different
possible histories. Achieving goal may different done first thing
73

fiWinikoff & Cranefield

agent does, failed plan already performed number actions.
means makes sense consider path-based criterion testing.
Furthermore, although paths adequacy criterion often considered impractical, reason appears primarily existence infinite number paths
presence loops. instance, Zhu et al. (1997, p. 375) say plan coverage criterion
strong practically useful programs, infinite
number different paths program loops. setting,
loops, existence infinite number paths issue, considering number
paths possible.
therefore use number paths proxy measure testing difficulty:
paths program, adequate test set (according
paths criterion) need large. hand, number paths
large, adequate test set need large.
one issue need consider: since paths strong criterion,
possible that, even absence (or bounding) loops, criterion always results
infeasibly large numbers paths. order address issue analysis
number paths procedural programs (of equivalent size), compare
number paths BDI programs (see Section 6).
Finally, bears noting paths criterion considers parts
program traversed testing, ignores values variables. So, example,
trivial program consisting single statement x := x x single one-step path,
trivially covered, many traces (x = 0, 1, 2 . . .).
1.2 HTN Planning
similarities Hierarchical Task Network (HTN) planning (Erol, Hendler,
& Nau, 1994) BDI execution (de Silva & Padgham, 2004): use hierarchical
representation goals (non-primitive tasks HTN terminology), plans (decomposition methods) goal-plan trees (task networks). complexity HTN planning
explored. Given similarities, simply exploit known complexity
results?
turns cannot so, simple reason complexity HTN
planning concerns plan finding problem, different BDI plan execution,
Sardina Padgham explain:
BDI agent systems HTN planners come different communities
differ many important ways. former focus execution plans,
whereas latter concerned actual generation plans.
former generally designed respond goals information; latter
designed bring goals. addition, BDI systems meant
embedded real world therefore take decisions based particular
(current) state. Planners, hand, perform hypothetical reasoning
actions interactions multiple potential states. Thus, failure
different meaning two types systems. context
planning, failure means plan potential plan suitable; within
BDI agent systems failure typically means active (sub)plan ought
74

fiOn Testability BDI Agent Systems

aborted. Whereas backtracking upon failure option planning systems,
generally BDI systems, actions taken real world. (Sardina
& Padgham, 2011, p. 45, bold emphasis added)
words, HTN systems plan ahead execution, whereas BDI systems interleave
execution planning2 .
HTN plan existence problem answers question plan exist?
complexity studied. settings correspond BDI execution (many goals,
total ordering within plans, variables) known EXPSPACE-hard
DEXPTIME (Erol, Hendler, & Nau, 1994, 1996). However, work address
question BDI execution. considering complexity plan existence HTN
planning asking computational complexity search process
result plan. hand, asking number paths
goal-plan tree asking possibilities arise executing plan.
illustrate point, consider following example. Suppose single goal
G decomposed two alternative plans, P1 P2 . Plan P1 consists
sequential execution actions a, b, c; plan P2 consists sequential execution
actions e. plan existence problem boils considering options P1
P2 , since case search space simple, offering two options.
hand, question many paths exist BDI execution considers different
ways goal-plan tree executed. Whereas HTN planning considers P1
single atomic decomposition, BDI execution needs consider sequence actions a, b, c
distinct steps. possible three actions succeed (giving trace a, b, c),
possible action b fail, followed P2 (successfully) used (giving trace
a, b8, d, e), action c fail, followed P2 (successfully) used (giving trace
a, b, c8, d, e).
Overall, means complexity analysis Erol et al. (1994, 1996)
different problem, HTN complexity results relevant. Finally, note
that, fact, setting, plan existence problem actually trivially true: since
BDI programs constraints always expansion program
sequence actions.
remainder paper structured follows. begin briefly presenting
BDI execution model (Section 2) discussing BDI execution viewed
process transforming goal-plan trees (Section 3). Section 4 core paper
analyse number paths BDI-style goal-plan tree. consider
analysis assumptions hold real system real platform (Section 5),
analysis BDI programs compares analysis (number paths)
conventional procedural programs (Section 6). Finally, conclude discussion
implications testing future work (Section 7).
2. approaches blur difference adding look-ahead planning BDI online execution
HTNs, example planner RETSINA multi-agent system (Paolucci, Shehory, Sycara, Kalp,
& Pannu, 2000) ability interleave planning execution. However, theoretical analysis
extension reported, analysis Erol, Hendler, Nau (1994, 1996) applies
classical HTN planning.

75

fiWinikoff & Cranefield

2. BDI Execution Model
describe Belief-Desire-Intention (BDI) model explain chose
model agent execution. addition well known widely used, BDI model
well defined generic. well defined allows us analyse behaviour spaces
result using it. generic implies analysis applies wide range
platforms.
BDI model viewed philosophical (Bratman, 1987) logical (Rao
& Georgeff, 1991) perspectives, interested implementation perspective, exhibited range architectures platforms, JACK (Busetta,
Ronnquist, Hodgson, & Lucas, 1999), JAM (Huber, 1999), dMARS (dInverno, Kinny, Luck,
& Wooldridge, 1998), PRS (Georgeff & Lansky, 1986; Ingrand, Georgeff, & Rao, 1992),
UM-PRS (Lee, Huber, Kenny, & Durfee, 1994), Jason (Bordini, Hubner, & Wooldridge,
2007), SPARK (Morley & Myers, 2004), Jadex (Pokahr, Braubach, & Lamersdorf, 2005)
IRMA (Bratman, Israel, & Pollack, 1988). purposes analysis here,
formal detailed presentation unnecessary. interested formal semantics
BDI languages referred work Rao (1996), Winikoff, Padgham, Harland,
Thangarajah (2002) Bordini et al. (2007), example.
implementation BDI agent key concepts beliefs (or, generally,
data), events plans. reader may find surprising goals key concepts
BDI systems. reason goals modelled events: acquisition new goal
viewed new goal event, agent responds selecting executing plan
handle event3 . remainder section, keeping established
practice, describe BDI plans handling events (not goals).
BDI plan consists three parts: event pattern specifying event(s) relevant
for, context condition (a Boolean condition) indicates situations plan
used, plan body executed. plans event pattern context condition
may terms containing variables, matching unification process (depending
particular BDI system) used BDI interpreters find plan instances respond
given event. general plan body contain arbitrary code programming
language4 , however purposes assume5 plan body sequence steps,
step either action6 (which succeed fail) event posted.
example, consider simple plans shown Figure 1. first plan, Plan A,
relevant handling event achieve goal go-home, applicable situations
agent believes train imminent. plan body consists sequence
four steps (in case assume actions, could modelled
events handled plans).
key feature BDI approach plan encapsulates conditions
applicable defining event pattern context condition. allows
additional plans given event added modular fashion, since invoking
3. types event typically include addition removal beliefs agents belief set.
4. example, JACK plan body written language superset Java.
5. follows abstract notations AgentSpeak(L) (Rao, 1996) (Winikoff et al., 2002)
aim capture essence range (more complex) BDI languages.
6. includes traditional actions affect agents environment, internal actions
invoke code, check whether certain condition follows agents beliefs.

76

fiOn Testability BDI Agent Systems

Plan A: handles event:
achieve goal go-home
context condition:
train imminent
plan body:
(1) walk train station
(2) check train running time
(3) catch train
(4) walk home
Plan B: handles event:
achieve goal go-home
context condition:
raining bicycle
plan body:
(1) cycle home
Plan C: handles event:
achieve goal go-home
context condition:
true (i.e. always applicable)
plan body:
(1) walk bus stop
(2) check buses running
(3) catch bus
(4) walk home
Figure 1: Three Simple Plans
context (i.e. triggering event posted) contain code selects amongst
available plans, key reason flexibility BDI programming.
typical BDI execution cycle elaboration following event-driven process
(summarised Figure 2)7 :
1. event occurs (either received outside source, triggered within
agent).
2. agent determines set instances plans plan library event patterns
match triggering event. set relevant plan instances.
3. agent evaluates context conditions relevant plan instances generate
set applicable plan instances. relevant plan instance applicable context
condition true. applicable plan instances event deemed
failed, posted plan, plan fails. Note
single relevant plan may lead applicable plan instances (if context condition
false), one applicable plan instance (if context condition,
may contain free variables, multiple solutions).
4. One applicable plan instances selected executed. selection mechanism varies platforms. generality, analysis make as7. BDI engines are, fact, complicated interleave execution multiple
active plan instances (or intentions) triggered different events.

77

fiWinikoff & Cranefield

Boolean function execute(an-event)
let relevant-plans = set plan instances resulting
matching plans event patterns an-event
let tried-plans =
true
let applicable-plans = set plan instances resulting
solving context conditions relevant-plans
applicable-plans := applicable-plans \ tried-plans
applicable-plans empty return false
select plan p applicable-plans
tried-plans := tried-plans {p}
execute(p.body) = true return true
endwhile
Boolean function execute(plan-body)
plan-body empty return true
elseif execute(first(plan-body)) = false return false
else return execute(rest(plan-body))
endif
Boolean function execute(action)
attempt perform action
action executed successfully return true else return false endif

Figure 2: BDI Execution Cycle
sumptions plan selection. plans body may create additional events
handled using process.
5. plan body fails, failure handling triggered.
brevity, remainder paper use term plan loosely mean
either plan plan instance intention clear context.
Regarding final step, approaches dealing failure. Perhaps
common approach, used many existing BDI platforms,
select alternative applicable plan, consider event failed
remaining applicable plans. determining alternative applicable plans one may
either consider existing set applicable plans, re-calculate set applicable
plans (ignoring already tried), done Figure 2. makes
sense situation may changed since applicable plans determined.
Many (but all) BDI platforms use failure-handling mechanism retrying
plans upon failure, analysis applies platforms.
One alternative failure-handling approach, used Jason (Bordini et al., 2007),
post failure event handled user-provided plan. Although
flexible, since user specify upon failure, place burden
78

fiOn Testability BDI Agent Systems

specifying failure handling user. Note Jason provides pattern allows
traditional BDI failure-handling mechanism specified succinctly (Bordini et al., 2007,
pp. 171172). Another alternative failure-handling approach used 2APL (Dastani,
2008) predecessor, 3APL: permit programmer write plan repair rules
conditionally rewrite (failed) plan another plan. approach, Jasons,
quite flexible, possible analyse general way plan rules
quite arbitrary. Another well known BDI architecture IRMA, described
high-level prescribe specific failure-handling mechanism:
full development architecture would give account
ways resource-bounded agent would monitor prior plans
light changes belief. However developed, course
times agent give prior plan light new belief
plan longer executable. happens, new process
deliberation may triggered (Bratman et al., 1988).
Given BDI execution cycle discussed above, three example plans given earlier
(Figure 1) give rise range behaviours, including following:
Suppose event achieve goal go-home posted agent believes
train imminent. walks train station, finds train running
time, catches train, walks home.
Suppose upon arrival train station agent finds trains
delayed. Step (2) Plan fails, agent considers alternative plans.
raining present time, Plan B applicable, Plan C adopted
(to catch bus).
Suppose agent decided catch bus (because train believed
imminent, raining), attempting execute Plan C fails (e.g.
bus strike). agent reconsider plans rain stopped (and
bicycle) may use Plan B.
Note correct (respectively incorrect) behaviour distinct successful
(respectively failed) execution plan. Software testing essence process running system checking whether observed behaviour trace correct (i.e. conforms
specification, model). hand, BDI agents behaviour traces
classified successful failed. However, correctness given execution
trace independent whether trace successful failed execution. successful
execution may, fact, exhibit behaviour correct, instance, traffic controller
agent may successfully execute actions set traffic signals intersection green
achieve goal so. successful execution, incorrect behaviour.
possible failed execution correct. instance, traffic controller agent
attempting route cars point point B, traffic accident blocked key
bridge two points, rational (and correct) behaviour agent
fail achieve goal.
79

fiWinikoff & Cranefield

3. BDI Execution Goal-Plan Tree Expansion
BDI execution, summarised Figure 2, dynamic process progressively executes
actions goals posted. order easily analyse process, present
alternative view declarative. Instead viewing BDI execution process,
view data transformation (finite) goal-plan tree sequence action
executions.
events plans visualised tree goal8 children
plan instances applicable it, plan instance children sub-goals
posts. goal-plan tree and-or tree: goal realised one plan
instances (or) plan instance needs sub-goals achieved (and).
Viewing BDI execution terms goal-plan tree action sequences makes
analysis behaviour space size9 easier. consider BDI execution process taking
goal-plan tree transforming sequence recording (failed successful)
executions actions, progressively making decisions plans use
goal executing plans.
process non-deterministic: need choose plan goal tree.
Furthermore, consider failure, need consider action whether fails
not, fail, failure recovery done.
define transformation process detail. Prolog code implementing
process found Figure 3. defines non-deterministic predicate exec first
argument (input) goal-plan tree, second argument (output) sequence
actions. goal-plan tree represented Prolog term conforming following
simple grammar (where GPT abbreviates Goal-Plan Tree, AoGL abbreviates Action
Goal List, symbol):
hGPT ::= goal([]) | goal([hPlanListi])
hPlanListi ::= hPlani | hPlani,hPlanListi
hPlani ::= plan([]) | plan([hAoGLi])
hAoGLi ::= act(A) | hGPT | act(A),hAoGLi | hGPT i,hAoGLi
example, simple goal-plan tree shown Figure 4 modelled Prolog term
goal([plan([act(a)]), plan([act(b)])]).
analysis make simplifying assumption. Instead modelling instantiation
plans plan instances, assume goal-plan tree contains applicable plan
instances. Thus, order transform goal node sequence actions (nondeterministically) select one applicable plan instances. selected plan
transformed turn, resulting action sequence (line 2 Figure 3). selecting
plan, consider possibility applicable plans could chosen,
first plan. done different points time different plan instances may
applicable. saw example earlier, Plan chosen failed,
8. order consistent existing practice shall use term goal rather event
remainder paper.
9. remainder paper use term behaviour space size, rather
cumbersome term number paths BDI program.

80

fiOn Testability BDI Agent Systems

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

exec ( goal ([]) ,[]).
exec ( goal ( Plans ) , Trace ) : - remove ( Plans , Plan , Rest ) , exec ( Plan , Trace1 ) ,
( failed ( Trace1 ) -> recover ( Rest , Trace1 , Trace ) ; Trace = Trace1 ).
exec ( plan ([]) , []).
exec ( plan ([ Step | Steps ]) , Trace ) : - exec ( Step , Trace1 ) ,
( failed ( Trace1 ) -> Trace = Trace1 ; continue ( Steps , Trace1 , Trace )).
exec ( act ( Action ) , [ Action ]).
exec ( act ( Action ) , [ Action , fail ]).
failed ( Trace ) : - append (X ,[ fail ] , Trace ).
recover ( Plans , Trace1 , Traces ) : exec ( goal ( Plans ) , Trace2 ) , append ( Trace1 , Trace2 , Traces ).
continue ( Steps , Trace1 , Trace ) : - exec ( plan ( Steps ) , Trace2 ) ,
append ( Trace1 , Trace2 , Trace ).
remove ([ X | Xs ] ,X , Xs ).
remove ([ X | Xs ] ,Y ,[ X | Z ]) : - remove ( Xs ,Y , Z ).

Figure 3: Prolog Code Expand Goal-Plan Trees
goal
plan

plan



b

Figure 4: Simple Goal-Plan Tree

Plan C selected (and failed), finally Plan B (which applicable
Plan failed) selected.
selected plan executes successfully (i.e. action trace doesnt end fail
marker; line 9), resulting trace trace goals execution (line 3). Otherwise, perform failure recovery (line 10), done taking remaining plans
transforming goal plans options. resulting action sequence
appended action sequence failed plan obtain complete action sequence
goal.
process easily seen match described Figure 2 (with exception,
discussed above, begin applicable plans, relevant plans). Specifically,
applicable plan selected executed, successful execution stops.
successful, alternative plan selected execution continues (i.e. action
sequences appended).
order transform plan node first transform first step plan,
either sub-goal action (line 5). successful, continue transform
rest plan, append two resulting traces together (lines 6 12).
first step plan successful, trace simply trace first step
(line 6); words stop transforming plan step fails. Again, process
easily seen correspond plan body execution Figure 2.
81

fiWinikoff & Cranefield

Finally, order transform action action sequence simply take
action singleton sequence (line 7). However, need take account
possibility action may fail, thus second possibility action followed
failure indicator (line 8). Again, process easily seen correspond action
execution Figure 2. Note model dont concern
action fails: could lack resources, environmental issues.
example applying process two example goal-plan trees found
Appendix A.

4. Behaviour Space Size BDI Agents
consider many paths goal-plan tree used
BDI agent realise goal10 using tree. use analysis previous section
basis; is, view BDI execution transforming goal-plan tree action
traces. Thus, question large behaviour space BDI agents, answered
deriving formulae allow one compute number behaviours, successful
unsuccessful (i.e. failed), given goal-plan tree.
make following uniformity assumptions allow us perform analysis.
simplifying assumptions concern form goal-plan tree.
1. assume subtrees goal plan node structure. is,
leaves goal-plan tree distance (number edges) away
root tree. therefore define depth goal-plan tree
number layers goal nodes contains. goal-plan tree depth 0 plan
sub-goals, goal-plan tree depth > 0 either plan node
children goal nodes depth goal node children plan
nodes depth 1. Note definition depth reverse usual
definition (where depth trees root defined 0). use definition
simplifies presentation derivations later section.
2. assume plan instances depth > 0 k sub-goals.
3. assume goals j applicable plan instances. case
goal j relevant plans, results exactly one applicable plan
instance, case ways. instance, goal may 2j
relevant plans, half applicable current situation, goal may
single relevant plan j applicable instances. Note assumption rules
possibility infinite number applicable plan instances,
would case plans context condition infinite number solutions.
cannot occur context condition defined terms conjunctions
propositions refer finite belief base. However, occur agents
context conditions make use Prolog-like knowledge base (as case
agent-oriented programming languages, Jason Goal). Nevertheless,
since deal applicable plans, dont model context conditions.
10. focus single goal analysis: multiple goals treated concurrent interleaving
individual goals. Multiple agents treated concurrent interleaving, care
needs taken details agent waiting another agent respond.

82

fiOn Testability BDI Agent Systems

Figure 5 shows uniform goal-plan tree depth 2.
g2

d=2

@
R pj1
p11 . . . @
..
.
@

R
@
g11 . . . gk1
..
.
@

R
@
p10 . . . pj0

d=1
d=1
d=0

Figure 5: uniform goal-plan tree
assumptions made clearly unrealistic. means consider
possibility real agent programs behave quite differently, since meet
assumptions. address issue number ways. Firstly, Section 4.4
consider relaxation assumptions defining semi-uniform trees,
number available plan instances (j) vary across different levels tree. Secondly,
Section 5.2 consider example (non-uniform) goal-plan tree industrial
application. derive number paths real goal-plan tree compare
analysis similarly-sized uniform goal-plan trees see whether real (non-uniform)
tree significantly lower number paths uniform tree. Finally, Section 4.7,
consider issue infinite trees allowing trees recursive, defining
number paths (up bound path length) recursive tree.
analysis uses following terminology:
uniformity assumptions mean structure subtree rooted goal
plan node determined solely depth, therefore denote goal
plan node depth gd pd (respectively).
use n4(xd ) denote number successful execution paths goal-plan tree
depth rooted x (where x either goal g plan p). specifying
important sometimes elide it, writing n4(x).
Similarly, use n8(xd ) denote number unsuccessful execution paths
goal-plan tree depth root x (either g p).
extend notation plan body segments, i.e. sequences x1 ; . . . ; xn xi
goal action ; denotes sequential composition. abbreviate sequence
n occurrences x xn (for example, g13 = g1 ; g1 ; g1 ).
4.1 Base Case: Successful Executions
begin calculating number successful paths goal-plan tree
absence failure (and failure handling). analysis follows Padgham
Winikoff (2004, pp. 1719).
Roughly speaking, number ways goal achieved sum number
ways children achieved (since children represent alternatives,
83

fiWinikoff & Cranefield

i.e. goal represented node). hand, number ways plan
achieved product number ways children achieved
(since children must achieved, i.e. plan represented node).
precisely, n4(x1 ; x2 ) = n4(x1 ) n4(x2 ); is, sequence successful x1
x2 successful.
Given tree root g (a goal), assume j children achieved
n different ways11 ; then, select one children, number ways
g achieved jn. Similarly, tree root p (a plan), assume
k children achieved n different ways, then, execute children,
number ways p executed n n, nk . plan children
(i.e. depth 0) executed (successfully) exactly one way. yields following
definition:
n4(gd ) = j n4(pd1 )
n4(p0 ) = 1
n4(pd ) = n4(gd k ) = n4(gd )k
Expanding definition obtain
n4(g1 ) = j n4(p0 ) = j 1 = j
k

n4(g2 ) = j n4(p1 ) = j (n4(g1 ) ) = j (j k ) = j k+1
n4(g3 ) = j n4(p2 ) = j (j k+1 )k = j k
n4(g4 ) = j n4(p3 ) = j (j k

2 +k+1

2 +k+1

)k = j k

3 +k 2 +k+1

generalised to:
n4(gd ) = j

Pd1
i=0

ki

k > 1 simplified using equivalence k i1 + . . . + k 2 + k + 1 = (k 1)/(k 1)
give following closed form definition: (and k = 1 n4(gd ) = n4(pd ) = j )
n4(gd ) = j (k
4

n (pd ) = j

1)/(k1)

k (kd 1)/(k1)

(1)
(2)

Note equation n4(pd ) assumes sub-goals achieved sequentially.
executed parallel number options higher, since need consider
possible interleavings sub-goals execution. example, suppose plan pd
two sub-goals, g1d g2d , sub-goals n4(gd ) successful executions,
execution l steps (we assume ease analysis execution paths
length). number ways interleaving two parallel executions,
length l, calculated follows (Naish, 2007, Section 3):


(2 l)!
2l
=
l
(l!) (l!)
11. tree assumed uniform, children achieved number
ways, thus interchangeable analysis, allowing us write j n rather n1 + . . . + nj .

84

fiOn Testability BDI Agent Systems

hence number ways executing pd parallel execution subgoals is:
4

4

2



n (pd ) = n (gd )

2l
l



= n4(gd )2

(2 l)!
(l!) (l!)

remainder paper assume sub-goals plan achieved
sequentially, since common case, since yields lower figure which,
shall see, still large enough allow conclusions drawn.
4.2 Adding Failure
extend analysis include failure, determine number unsuccessful
executions, i.e. executions result failure top-level goal. moment
assume failure handing (we add failure handling Section 4.3).
order determine number failed executions know failure
occur. BDI systems two places failure occurs: goal
applicable plan instances, action (within applicable plan instance) fails.
However, uniformity assumption means address former caseit
assumed goal always j instances applicable plans. Note
conservative assumption: relaxing results number unsuccessful executions
even larger.
order model latter case need extend model plans encompass
actions. example, suppose plan body form a1; ga; a2; gb; a3 ai
actions, ga gb sub-goals, ; denotes sequential execution. plan
following five cases unsuccessful (i.e. failed) executions:
1. a1 fails
2. a1 succeeds, ga fails
3. a1 ga succeed, a2 fails
4. a1, ga, a2 succeed, gb fails
5. a1, ga, a2 gb succeed, a3 fails
Suppose ga executed successfully n4(ga) different ways. third
case corresponds n4(ga) different failed executions: successful execution ga,
extend adding failed execution a2 (actions executed one way,
i.e. n4(a) = 1 n8(a) = 1). Similarly, gb n4(gb) successful executions fifth
case corresponds n4(ga) n4(gb) different failed executions. ga unsuccessfully
executed n8(ga) different ways second case corresponds n8(ga) different executions. Similarly, fourth case corresponds n4(ga) n8(gb) different executions. Putting
together, total number unsuccessful executions plan p
body a1; ga; a2; gb; a3 sum five cases:
1 + n8(ga) + n4(ga) + n4(ga) n8(gb) + n4(ga) n4(gb)
85

fiWinikoff & Cranefield

formally, n8(x1 ; x2 ) = n8(x1 ) + n4(x1 ) n8(x2 ); is, sequence fail either
x1 fails, x1 succeeds x2 fails. follows n4(xk ) = n4(x)k n8(xk ) =
n8(x) (1 + + n4(x)k1 ), easily proven induction.
generally, assume ` actions before, after, sub-goals
plan, i.e. example plan corresponds ` = 1, following plan body
corresponds ` = 2: a1; a2; g3; a4; a5; g6; a7; a8. plan sub-goals (i.e. depth
0) considered consist ` actions (which quite conservative: particular,
use ` = 1 assume plans depth 0 consist single action).
number unsuccessful execution traces goal-plan tree defined,
based analysis above, follows. First calculate numbers successes
failures following repeated section plan body: gd ; a` :
n4(gd ; a` ) = n4(gd ) n4(a` )
= n4(gd ) n4(a)`
= n4(gd ) 1`
= n4(gd )
n8(gd ; a` ) = n8(gd ) + n4(gd ) n8(a` )
= n8(gd ) + n4(gd ) n8(a) (1 + + n4(a)`1 )
= n8(gd ) + n4(gd ) `
> 0:
n8(pd ) = n8(a` ; (gd ; a` )k )
= n8(a` ) + n4(a` ) n8((gd ; a` )k )
= n8(a) (1 + + n4(a)`1 )) + n4(a)` n8((gd ; a` )k )
= ` + 1 n8(gd ; a` ) (1 + + n4(gd ; a` )k1 )
= ` + (n8(gd ) + n4(gd ) `) (1 + + n4(gd )k1 ))
n4(gd )k 1
= ` + (n8(gd ) + ` n4(gd )) 4
(assuming n4(gd ) > 1)
n (gd ) 1
yields following definitions number unsuccessful executions goalplan tree, without failure handling. equation n8(gd ) derived using
reasoning previous section: single plan selected executed, j
plans.
n8(gd ) = j n8(pd1 )
n8(p0 ) = `
n4(gd )k 1
n4(gd ) 1
4
(for > 0 n (gd ) > 1)

n8(pd ) = ` + (n8(gd ) + ` n4(gd ))

Finally, note analysis number successful executions goal-plan
tree absence failure handling presented Section 4.1 unaffected addition
actions plan bodies. one way sequence actions
succeed, Equations 1 2 remain correct.
86

fiOn Testability BDI Agent Systems

4.3 Adding Failure Handling
consider introduction failure-handling mechanism affects analysis.
common means dealing failure BDI systems respond failure
plan trying alternative applicable plan event triggered plan.
example, suppose goal g (e.g. achieve goal go-home) three applicable plans pa,
pb pc, pa selected, fails. failure-handling mechanism
respond selecting pb pc executing it. Assume pc selected. pc fails,
last remaining plan (pb) used, fails, goal deemed
failed.
result that, might hope, harder fail: way goal
execution fail applicable plans tried fails12 .
number executions computed follows: goal gd j applicable plan instances, n8(pd1 ) unsuccessful executions, n8(pd1 )j
unsuccessful executions plans sequence. Since plans selected
order multiply j! yielding n8(gd ) = j! n8(pd1 )j .
number ways plan fail still defined equation
failure handling happens level goalsbut n8(g) refers new
definition:
j

n8(gd ) = j! n8(pd1 )

(3)

8

n (p0 ) = `

(4)
4

)k

n (gd 1
n4(gd ) 1
4
(for > 0 n (gd ) > 1)

n8(pd ) = ` + (n8(gd ) + ` n4(gd ))

(5)

Turning number successful executions (i.e. n4(x)) observe
effect adding failure handling convert failures successes, i.e. execution
would otherwise unsuccessful extended longer execution may succeed.
Consider simple case: depth 1 tree consisting goal g (e.g. achieve goal go-home)
three children: pa, pbandpc. Previously successful executions corresponded
pi (i.e. select pi execute it). However, failure handling,
following additional successful executions (as well additional cases corresponding
different orderings plans, e.g. pb failing pa successfully executed):
pa fails, pb executed successfully
pa fails, pb executed fails, pc executed succeeds
leads definition form
n4(g) = n4(pa) + n8(pa) n4(pb) + n8(pa) n8(pb) n4(pc)
12. fact, actually underestimate: possible goal fail none untried
relevant plans applicable resulting situation. noted earlier, assume analysis
goals cannot fail result applicable plan instances. conservative assumption:
relaxing results number behaviours even larger.

87

fiWinikoff & Cranefield

However, need account different orderings plans. instance, case
first selected plan succeeds (corresponding first term, n4(pa)) fact applies
j plans, first term, including different orderings, j n4(p).
Similarly, second term (n8(pa) n4(pb)), corresponding case initially
selected plan fails next plan selected succeeds, fact applies j initial plans,
j 1 next plans, yielding j (j 1) n8(p) n4(p).
Continuing process (for j = 3) yields following formulae:
2

n4(g) = 3 n4(p) + 32 n8(p) n4(p) + 3! n8(p) n4(p)
generalises
j1

n4(g) = j n4(p) + j (j 1) n8(p) n4(p) + + j! n8(p)

n4(p)

resulting following equations (again, since failure handling done goal level,
equation plans Section 4.1):
4

n (gd ) =

j
X

i1

n8(pd1 )

n4(pd1 )

i=1

n4(p0 ) = 1
4

j!
(j i)!

(6)
(7)

4

n (pd ) = n (gd )

k

(for > 0 )

(8)

used standard BDI failure-handling mechanism trying alternative
applicable plans. let us briefly consider alternative failure-handling mechanism
simply re-posts event, without tracking plans already attempted.
fairly easy see this, fact, creates infinite number behaviours: suppose
goal g achieved pa pb, pa could selected, executed resulting
failure, pa could selected again, fail again, etc. suggests
standard BDI failure-handling mechanism is, fact, appropriate, avoids
infinite behaviour space, possibility infinite loop. discussed earlier (in
Section 2), failure recovery mechanism used 3APL 2APL (Dastani, 2008) cannot
analysed general way, since depends details specific agent program;
IRMA (Bratman et al., 1988) provide sufficient details allow analysis.
Tables 1 2 make various equations developed far concrete showing illustrative values n8 n4 range reasonable (and fairly low) values j, k
using ` = 1. Number columns show number goals, plans actions tree. number actions brackets many actions executed
single (successful) execution failure handling. number goals calculated
follows. depth 1 single goal (see Figure 5). depth n + 1
1 + (j k G(n)) goals, G(n) denotes number goals depth n tree.
gives G(n) = 1 + (j k) + (j k)2 + + (j k)n1 . example, j = k = 2,
G(3) = 1 + 4 + 16 = 21. Since goal exactly j plans, number plans tree
depth n j G(n). consider number actions. non-leaf plan
` (k + 1) actions (since k goals, k + 1 places ` actions).
leaf plan ` actions. tree depth n j (j k)n1 leaf plans. Let P (n)
number plans depth n tree, comprised Pn (n) non-leaf plans
88

fiOn Testability BDI Agent Systems

Parameters
j k

2 2
3
3 3
3
2 3
4
3 4
3

goals
21
91
259
157

Number
plans
42
273
518
471


actions
62 (13)
363 (25)
776 (79)
627 (41)

n4(g)
128
1,594,323

n8(g)
614
6,337,425

1,099,511,627,776

6,523,509,472,174

10,460,353,203

41,754,963,603

Table 1: Illustrative values n4(g) n8(g) without failure handling. first number actions (e.g. 62) number actions tree, second
(e.g. 13) number actions single execution failures occur.

Parameters
j k

2 2
3
3 3
3
2 3
4
3 4
3

goals
21
91
259
157

Number
plans
42
273
518
471


actions
62 (13)
363 (25)
776 (79)
627 (41)

n4(g)
6.33 1012
1.02 10107
1.82 10157
3.13 10184

n8(g)
1.82 1013
2.56 10107
7.23 10157
7.82 10184

Table 2: Illustrative values n4(g) n8(g) failure handling
Pl (n) leaf plans, i.e. P (n) = Pn (n) + Pl (n). number actions depth n tree
(` (k + 1)) Pn (n) + ` Pl (n). example, j = k = 2 ` = 1,
P (3) = 2 G(3) = 42, comprised 32 leaf plans 10 non-leaf plans.
therefore (1 3 10) + (1 32) = 62 actions.
4.4 Recurrence Relations
equations previous sections define functions n4 n8 mutual recurrence
depth goal-plan tree uniform branching structure. effect
increasing parameters k ` evident level recursion,
clear effect increasing number applicable plan instances j
given goal. aim section explore effects changing j.
relaxing uniformity assumption. Specifically, allow number plans available
vary goal nodes different depths tree, still assuming nodes
given depth structure. refer semi-uniform goal-plan trees.
derive set recurrence relations n4 n8 presence failure handling
explicitly show effect adding new plan goal root particular
sub-tree.
begin defining generalised notation n8(gj ) n4(gj ) j list13
(jd , jd1 , . . . , j0 ) element ji represents number plans available goals
depth goal-plan tree. denote empty list hi write j j represent
list head j tail j.
13. order corresponds definition depth, decreases tree.

89

fiWinikoff & Cranefield

generalise Equations 3 6 apply semi-uniform goal-plan trees,
derivation equations depended sub-nodes goal plan node
structure. assumption preserved generalised setting.
therefore rewrite equations using new notation, express right
hand sides functions f 8 f 4 n8(pj ) (for f 4 ) n4(pj ). aim find recursive
definition f 8 f 4 recurrence j.
n8(gjj ) = f 8 (j, n8(pj ))
n4(gjj ) = f 4 (j, n8(pj ), n4(pj ))

f 8 (j, a) = j! aj
f 4 (j, a, b) =

j
X

b ai1

i=1

j!
(j i)!

(change bounds 0 . . . n, hence replace + 1)
=

j1
X

b a(i+1)1

i=0

j!
(j (i + 1))!

(simplify using (j (i + 1))! = (j i)!/(j i) )
=

j1
X
i=0

b ai

j!(j i)
(j i)!

(multiple i!/i! reorder)
=

j1
X
i=0

j!
i! ai (j i) b
i!(j i)!


j
(use definition binomial:
= j!/i!(j i)!)

j1
X
j
i! ai (j i) b
=


(9)

i=0

expression right last line corresponds following combinatorial analysis f 4 . goal gjj , successful execution involve sequence
plan executions thatfail (for i, 0 j 1) followed one plan execution
succeeds. ji ways choosing failed plans, ordered i! ways,
plan = n8(pj ) ways fail. j ways choosing final
successful plan, b = n4(pj ) ways succeed.
goal find explicit characterisation incremental effect adding
extra plan n8(gjj ) n4(gjj ) finding definitions f 8 f 4 recurrence relations
terms parameter j. Deriving recurrence relation f 8 straightforward:
f 8 (j, a) = j! aj = (j (j 1) . . . 21) (a
. . a}) = (j a) ((j 1) a) . . . (2 a) (1 a)
| .{z
j times
90

fiOn Testability BDI Agent Systems

n4(gjj ) = f 4 (j, n8(pj ), n4(pj ))
n8(gjj ) = f 8 (j, n8(pj ))
f 4 (0, a, b) = 0
f 4 (j +1, a, b) = (j +1) (b + f 4 (j, a, b))

(10)

8

f (0, a) = 1
8

f (j +1, a) = (j +1) f 8 (j, a)
n4(phi ) = 1
n8(phi ) = `
n4(pj ) = n4(gj )k , j 6= hi
n4(gj )k 1
, j 6= hi
n8(pj ) = ` + n8(gj ) + ` n4(gj )
n4(gj ) 1
Figure 6: Recurrence relations numbers failures successes goal plan tree
presence failure handling

shows f 8 (0, a) = 1 f 8 (j +1, a) = (j +1) f 8 (j, a)
However, derivation recurrence relation f 4 simple. use
technique first finding exponential generating function (e.g.f.) (Wilf, 1994)
sequence {f 4 (j, a, b)}
j=0 , using derive recurrence relation. details
given Appendix B, yield equation 10 Figure 6.
Equation 10 (copied Equation 25 Appendix B) gives us recurrence relation
14
sequence {f 4 (j, a, b)}
j=0 seeking . Figure 6 brings together
equations far failure-handling case (including previous
section defining n4(pd ) n8(pd ), generalised semi-uniform trees).
formulation gives us different way looking recurrence, allows us
easily see behaviour space grows number applicable plans, j,
goal grows. Considering meaning parameters b numbers failures
successes (respectively) plan level current goal node, equation
f 4 (j +1, a, b) seen following combinatorial interpretation. One plan
must selected try initially (there j +1 choices) either succeed (in one
b different ways), meaning plans need tried, fail (in one different
ways). fails, goal must succeed using remaining j plans,
occur f 4 (j, a, b) ways.
see growth number successful executions goal grows
rate greater j!aj , presence b term. relaxed uniformity
14. simple case = b = 1 listed sequence A007526 On-Line Encyclopedia
Integer Sequences (Sloane, 2007): number permutations nonempty subsets {1, , n}.

91

fiWinikoff & Cranefield

constraint used recurrence relations gives us way investigate numbers
traces goal-plan trees different semi-uniform shapes. However, remainder
paper focus uniform trees using original parameter j (with exception
Section 4.7).
4.5 Probability Failing
Section 4.3 said introducing failure handling makes harder fail. However,
Tables 1 2 appear first glance contradict this, many ways
failing failure handling without failure handling.
key understanding apparent discrepancy consider probability
failing: Tables 1 2 merely count number possible execution paths, without
considering likelihood particular path taken. Working probability
failing (as below) shows although many ways failing (and
succeeding), probability failing is, indeed, much lower.
Let us denote probability execution goal-plan tree root x depth
failing p8(xd ), probability succeeding p4(xd ) = 1 p8(xd ).
assume probability action failing 15 . probability
given plans actions succeeding simply (1 )x x number actions.
Hence probability plan failing failure (one of) actions simply
1 (1 )x , i.e. plan depth 0 probability failure is:
0 = 1 (1 )`
plan depth greater 0 probability failure due actions is:
= 1 (1 )` (k+1)
(recall plan ` actions before, after, between, k sub-goals).
Considering actions sub-goals g1 , . . . , gk plan p,
plan succeed, sub-goals must succeed, additionally, plans
actions must succeed giving p4(pd ) = (1 ) p4(gd )k . easily derive
equation p8(pd ) (given below). Note reasoning applies plan regardless
whether failure handling, failure handling done goal level.
absence failure handling, goal g possible plans p1 , . . . , pj succeed
must select one plan execute it, probability success probability
plan succeeding, i.e. p4(gd ) = p4(pd1 ). ignore moment possibility
goal failing applicable plans. assumption relaxed later on.
Formally, then, case without failure handling:
p8(gd ) = p8(pd1 )
p8(p0 ) = 0
k

p8(pd ) = 1 [(1 ) (1 p8(gd )) ]
15. simplicity, assume failure action plan independent failure
actions plan.

92

fiOn Testability BDI Agent Systems


0.05

0.01


2
3
4
2
3
4

failure handling
30%
72%
98%
07%
22%
55%

failure handling
0.64%
0.81%
0.86%
0.006%
0.006%
0.006%

Table 3: Goal failure probabilities without failure handling
consider happens failure handling added. case, order
goal fail, plans must fail, i.e. p8(gd ) = p8(pd1 )j . Since failure handling
goal level, equation plans unchanged, giving:
p8(gd ) = p8(pd1 )j
p8(p0 ) = 0
k

p8(pd ) = 1 [(1 ) (1 p8(gd )) ]
easy see equations patterns probabilities actually are,
so, illustration purposes, Table 3 shows probability failure is,
without failure handling, two scenarios. values computed using j = k = 3
(i.e. relatively small branching factor) ` = 1. consider two cases:
= 0.05 hence 0.185 (which rather high), = 0.01 hence
0.04.
seen, without failure handling, failure magnified : larger goalplan tree is, actions involved, hence greater chance action
somewhere failing, leading failure top-level goal (since failure
handling). hand, failure handling, probability failure low,
doesnt appear grow significantly goal-plan tree grows.
relax assumption goal cannot fail applicable
plans, i.e. goal fail plans tried. Unfortunately, relaxing
assumption complicates analysis need consider possibility none
remaining plans applicable point failure handling attemped.
Let us begin reconsidering case failure handling. use g
denote probability goal failing none remaining plans applicable.
case failure handling non-zero g indicates situations
goal applicable plans, may indicate error part
programmer, certain situations goal may possible achieve.
assume, analysis purposes, probability constant, particular,
depend plans already tried number relevant
plans remaining.
probability goal failing p8(gd ) = g + (1 g ) p8(pd1 ), i.e. goal fails
either plans applicable applicable plans selected
plan fails. before, equation plans unchanged, since failure handling done
93

fiWinikoff & Cranefield

goal level. following equations case without failure handling:
p8(gd ) = g + (1 g ) p8(pd1 )
p8(p0 ) = 0
k

p8(pd ) = 1 [(1 ) (1 p8(gd )) ]
Observe setting g = 0 yields equations derived earlier, assumed
goal cannot fail due inapplicable plans.
consider probability failure failure handling. goal two
plans following cases:
goal fail plans applicable (g )
applicable plans ((1 g ) . . .) goal fail first selected
plan fails (p8(pd1 ) . . .) failure handling successful, occur
either applicable plans (g ) applicable plans ((1 g ) . . .)
selected plan fails (p8(pd1 )).
Putting together, goal two plans have:
p8(gd ) = g + (1 g ) p8(pd1 ) (g + (1 g ) p8(pd1 ))
general case j available plans, goal fail if:
A. applicable plans outset, probability g ,
B. applicable plans (1 g ), selected plan fails (p8(pd1 ))
either applicable plans (g ),
C. applicable plans (1 g ), selected plan fails (p8(pd1 ))
either applicable plans (g ),
D. on: reasoning B repeated j times.
gives definition following form:
g + (1 g ) p8(pd1 ) (g + (1 g ) p8(pd1 ) (g + . . .g ))
|{z} |
{z
}|
{z
} |{z}
B
C


defined terms auxiliary function p8(gd , i) defines probability
failure goal g depth remaining relevant plan instances may
(or may not) yield applicable plan instances:
p8(gd ) = p8(gd , j)
p8(gd , 1) = g + (1 g ) p8(pd1 )
p8(gd , + 1) = g + (1 g ) p8(pd1 ) p8(gd , i)
p8(p0 ) = 0
k

p8(pd ) = 1 [(1 ) (1 p8(gd )) ]
94

fiOn Testability BDI Agent Systems


0.05


0.01


2
3
4

2
3
4

failure handling
g = 0
g = 0.01 g = 0.05
30%
33%
43%
72%
76%
86%
98%
99%
100%
g = 0 g = 0.005 g = 0.01
7%
9%
10%
22%
27%
32%
55%
63%
70%

failure handling
g = 0
g = 0.01 g = 0.05
0.64%
2.2%
9.4%
0.81%
2.6%
12.8%
0.86%
2.8%
16.5%
g = 0 g = 0.005 g = 0.01
0.006%
0.5%
1.1%
0.006%
0.6%
1.1%
0.006%
0.6%
1.1%

Table 4: Goal failure probabilities without failure handling goals
applicable plans

Observe setting g = 0 reduces definition derived earlier, since g +(1g ) X
simplifies X, hence p8(gd , i) = p8(pd1 )i .
before, immediately clear formulae actual patterns
probability are. Considering illustrative examples, Table 4 shows (a) overall behaviour before, (b) g assumed relatively low compared
probability action failure ( 0 ), doesnt significantly affect probabilities.
4.6 Analysis Rate Failures
section briefly examine number traces goal-plan tree affected
placing bound rate action failures occur within trace. simplicity,
work uniform goal-plan trees, construction extends trivially semiuniform goal-plan trees.
Figure 6 presented equations calculating total number behaviours
goal-plan tree (with failure handling). many behaviours involve possibly
unrealistic number action failures? make assumption upper
limit rate action failures16 , i.e. number failures divided length
trace, affect number possible behaviours? large numbers
seen reduce significantly?
instance, considering j = k = 2, ` = 1 = 2, 1,922 possible executions
result failure. many involve high rate action failure many
involve small percentage failures? Figure 7 contains (cumulative) counts
generated looking possible executions (small) case, plotted
number action failures. x axis shows given value N {0, . . . , 6} many
traces N fewer action failures. instance, N = 2, 426
traces 2 fewer action failures. 426 traces, 328 successful 98
unsuccessful. Figure 8 shows equivalent graph rate action failure: trace
failure rate computed (the number failures divided length trace),
16. Bounding rate action failures allows us model assumption environment limited
unpredictability, perhaps programmer limited incompetence!

95

fiWinikoff & Cranefield

ok"

failed"

both"

3500"

Number'of'traces'(cumula0ve)'

3000"

2500"

2000"

1500"

1000"

500"

0"

0"

1"

2"

3"

4"

5"

6"

ok"

8"

80"

328"

704"

960"

1024"

1024"

failed"

0"

0"

98"

546"

1282"

1794"

1922"

both"

8"

80"

426"

1250"

2242"

2818"

2946"

Number'of'failures'

Figure 7: Number traces (cumulative) vs. number failures j = k = 2, ` = 1, = 2
3500

3000

Number traces (cumulative)

2500

2000

1500

1000

500

0
0.1

0.2

0.3

0.4

0.5

0.6

0.7

0.8

0.9

1

Failure Rate

Figure 8: Number traces (cumulative) vs. failure rate j = k = 2, ` = 1, = 2

96

fiOn Testability BDI Agent Systems

number traces counted range failure rate. instance, first
data point graph shows 40 traces failure rate 0.1.
question generalise analysis larger execution spaces. Clearly,
counting possible executions feasible. Instead, turn generating functions.
given plan body segment17 (and particularly = gd ), interested
computing numbers successful failed traces failure rate bounded
given ratio r number failed actions total number actions,
i.e. proportion actions execution trace fail. denote n4r(s)
n8r(s). compute values, first determine integers > 0 n 0
numbers successful failed traces length contain exactly n action
failures, denoted n4r(s, m, n) n8r(s, m, n), respectively. define length trace
number actions (both successful unsuccessful) contains. Note
finite goal-plan tree, uniform semi-uniform one, maximum
possible trace length n4r(s, m, n) n8r(s, m, n) non-zero finite
number integer pairs (m, n) positive quadrant plane positive
axis (in case n = 0). values, calculate n4r(s)
n
r, similarly n8rs using n8r(s, m, n).
sum n4r(s, m, n)
begin considering ordinary 18 bivariate generating functions (Wilf, 1994)
values n4r(s, m, n) n8r(s, m, n):
4

Fr (s, x, y) =
Fr8(s, x, y) =

X

X

n4r(s, m, n) xm n

m=0 n=0
X

X

n8r(s, m, n) xm n

m=0 n=0

action one successful execution, length 1 contains action
failures, Fr4(a, x, y) = x (a power series coefficient x1 0 1
coefficients 0). Similarly, Fr8(a, x, y) = x1 1 = xy, one failed execution,
length 1 one action failure.
consider Fr4(s1 ; s2 ):
Fr4(s1 ; s2 , x, y) =
=

X

X

n4r(s1 ; s2 , m, n) xm n

m=0 n=0
X

X
m=0 n=0

X

X


nr(s1 , p, t) nr(s2 , q, u) xm n
4

4

p+q=m t+u=n

double sum parentheses considers, trace, possible ways allocating
number actions number action failures n (necessarily) successful
executions s1 s2 , sums non-negative integer values p, q, u.
17. Recall that, defined towards start Section 4 (page 83), plan body segment sequence
x1 ; . . . ; xn xi either goal action.
18. Ordinary generating functions differ exponential generating functions including denominators
factorials powers variable(s).

97

fiWinikoff & Cranefield

have:
Fr4(s1 ; s2 , x, y) =
=


X
X
X X

n4r(s1 , p, t) xp n4r(s2 , q, u) xq u

m=0 p+q=m n=0 t+u=n
X
X
X

X

n4r(s1 , p, t) xp n4r(s2 , q, u) xq u

p=0 q=0 t=0 u=0

X
X

X

X

4
p
4
q u
=
nr(s1 , p, t) x
nr(s2 , q, u) x
p=0 t=0

q=0 u=0

4

4

= Fr (s1 , x, y) Fr (s2 , x, y)
P
P P
P
second line derived using identity
q
p
m=0
p+q=m f (p, q) =
f (p, q). expressions sum non-negative integers p q, first expression
first summing non-negative values horizontal axis,
summing pairs (p, q) non-negative integers lying line slope 1
intersects horizontal axis m.
Considering Fr8(s1 ; s2 , x, y), have:
Fr8(s1 ; s2 , x, y) =
=

X

X

n8r(s1 ; s2 , m, n) xm n

m=0 n=0
X

X

n8r(s1 , m, n)

m=0 n=0

+

=

X

X



nr(s1 , p, t) nr(s2 , q, u) xm n
4

8

p+q=m t+u=n


XX

n8r(s1 , m, n) xm n

m=0 n=0
X

X

+

m=0 n=0

X

X


nr(s1 , p, t) nr(s2 , q, u) xm n
4

8

p+q=m t+u=n

8

= Fr (s1 , x, y)
X
X

X

X

+
n4r(s1 , p, t) xp
n8r(s2 , q, u) xq u
p=0 t=0

q=0 u=0

= Fr8(s1 , x, y) + Fr4(s1 , x, y) Fr8(s2 , x, y)
second line based observation failed execution s1 ; s2
length n action failures either failed execution s1 length n
action failures occurring execution, successful execution s1 length p
failures followed failed execution s2 length q u failures,
p + q = + u = n.
Now, assuming know Fr4(gd , x, y) Fr8(gd , x, y) depth d,
construct functions Fr4(pd , x, y) Fr8(pd , x, y) applying results expand
right hand sides following equations (which simply replace pd plan body):
98

fiOn Testability BDI Agent Systems

Fr4(pd , x, y) = Fr4(a` ; (gd ; a` )k , x, y)
Fr8(pd , x, y) = Fr8(a` ; (gd ; a` )k , x, y)

remains define Fr4(gd , x, y) Fr8(gd , x, y) terms Fr4(pd1 , x, y) Fr8(pd1 ,
x, y). count successful executions gd length n action failures,
must first choose one j applicable plans one ultimately succeeds.
must choose 0 j1 remaining applicable plans tried
failed, consider possible orderings plans. actions trace
n action failures must distributed across failed successful plans. leads
us following derivation procedure construct Fr4(gd , x, y):
Fr4(gd , x, y)
X

X
=
n4r(gd , m, n) xm n
m=0 n=0

=

X

X


j

m=0 n=0

= j

p=0


j1
X
j 1
p

p=0

= j


j1
X
j 1

p!

p

X

p!

X

n4r(pd1 , `0 , f0 )

p



X

X


m=0 n=0

n8r(pd1 , `i , ) xm n

i=1

`0 ++`p =m f0 ++fp =n

X

X



n4r(pd1 , `0 , f0 )

p



n8r(pd1 , `i , ) xm n

i=1

`0 ++`p =m f0 ++fp =n

j1
X
p=0

X
X
p
X

X

j 1
4
` f
8
` f
p!
nr(pd1 , `, f ) x
nr(pd1 , `, f ) x
p
`=0 f =0

`=0 f =0


j1
X
j 1
= j
p! Fr4(pd1 , x, y) Fr8(pd1 , x, y)p
p
p=0

Constructing Fr8(gd , x, y) simpler. failed execution goal involves failed attempts
execute j applicable plans. j! orderings plans must considered.
gives us following construction Fr8(gd , x, y):
Fr8(gd , x, y) =
=

X

X

n8r(gd , m, n) xm n

m=0 n=0
X

X

X

j!

m=0 n=0

= j!

X
X


X

`1 ++`j =m f1 ++fj =n
8

` f

nr(pd1 , `, f ) x

`=0 f =0

= j! Fr8(pd1 , x, y)j
99

j



nr(pd1 , `1 , f1 ) nr(pd1 , `j , fj ) xm n
8

8

fiWinikoff & Cranefield

equations define recursive procedure computing Fr4(gd , x, y) Fr8(gd ,
x, y) given values d, j, k `. discussed earlier section, given way
n
calculating n4r(s, m, n), calculate n4r(s) sum n4r(s, m, n)
r,
8
similarly nrs. used Python rmpoly GMPY2 libraries generate
polynomial representations functions Fr4(gd , x, y) Fr8(gd , x, y) specified
values d, j, k l, calculate n4r(s) n8r(s) various ratios r19 . Figure 9
shows results = j = k = 3 ` = 1.
Examining Figure 9 conclude two things. one hand, number traces
really explodes larger rates action failures. example, Figure 9 traces
failure rate greater 0.4. hand, although making assumptions
failure rate reduce number possible traces, number traces still quite
large (note scale y-axis). instance, failure rate 0.1 around
4.8 1044 failed executions 8.7 1047 successful executions. failure rate 0.2
respective numbers 1.0 1077 6.7 1077 , failure rate 0.3
1.2 1096 2.7 1096 .
shape Figure 9 explained follows. Firstly, occurrence action
failure triggers activity (alternative plans), failures result longer traces.
Secondly, longer traces shorter traces, simply
longer trace, possibilities variations (e.g. different orders trying
plans). explains increase Figure 9 starts slowly accelerates:
get failures, longer traces, longer traces
them. word, plot non-cumulative number paths
ratio action failures would see initial increase: ratio grows,
paths. doesnt explain beyond certain point get fewer traces,
cumulative graph levels out. explanation quite simple: beyond certain
ratio (which appears around 0.4) successful traces, number
failed traces declines.
4.7 Recursive Trees
Section 4.4 developed recurrence relations allowed us relax assumption
goal-plan trees uniform, considered semi-uniform trees. section
relax assumption goal-plan trees finite, allow trees
shape. considering arbitrary trees allowed contain labels
refer parts tree, i.e. allow trees recursive. derive generating
functions, seen extension derived previous section,
number paths (both successful unsuccessful) executing recursive goalplan trees. Obviously, infinite tree infinite number paths, define
generating functions take parameter bound lengths paths counted.

19. finite number actions attempted execution goal-plan tree,
bounds length possible traces number action failures occur within them.
Thus Fr4(gd , x, y) Fr8(gd , x, y) polynomials finite orderonly finite number coefficients
non-zero infinite sums define them.

100

fiOn Testability BDI Agent Systems

Failed executions (cumulative)

Successful executions (cumulative)

(cumulative)

4E+107

3.5E+107

Number traces (cumulative)

3E+107

2.5E+107

2E+107

1.5E+107

1E+107

5E+106

0.005
0.025
0.045
0.065
0.085
0.105
0.125
0.145
0.165
0.185
0.205
0.225
0.245
0.265
0.285
0.305
0.325
0.345
0.365
0.385
0.405
0.425
0.445
0.465
0.485
0.505
0.525
0.545
0.565
0.585
0.605
0.625
0.645
0.665
0.685
0.705
0.725
0.745
0.765
0.785
0.805
0.825
0.845
0.865
0.885
0.905
0.925
0.945
0.965
0.985

0

Failure rate

Figure 9: Number traces (cumulative) vs. failure rate j = k = = 3 ` = 1
given upper bound path length equations specify number paths
many actions20 .
begin defining notation representing recursive trees: goals, plan-body
multisets, plans, variables bindings. goal represented term form
goal(plan-body-multiset) plan-body-multiset multiset representing different
applicable plan instances used satisfy goal. multiset
combinatorial analysis, structure plans significant. Therefore
use single abstract action represent actions21 , goal may achievable
using multiple plan instances structure, must treat
distinct. need represent bodies plan instances, element
multiset (i.e. plan) sequence terms separated right-associative sequential
composition operator ;. term sequence either abstract action term a,
goal term defined (representing sub-goal), label (see below). Formally,
plan-body multiset P multiset plans, written {p1 :c1 , . . . , pj :cj }
ci number times associated plan pi appears multiset.
define following multiset operations: set(P ) set pi multiset P ,
P (pi ) characteristic function denoting number times plan pi appears
multiset (i.e. ci ); P M1 P M2 multiset subtraction, defined P M1 P M2 (x) =
max(P M1 (x) P M2 (x), 0). Finally |P | size multiset, i.e. sum ci .

20. use equations derived section non-recursive trees, case allow
= , define 1 = F power(x) = F .
21. However, avoid confusion, use numeric subscripts (a1 , a2 , . . .) distinguish different occurrences actions.

101

fiWinikoff & Cranefield

order allow recursive trees represented, possible step plan
label (denoted , 0 ) referring term provided binding, simply
mapping labels terms (either goal plan terms). b binding write
b[] denote item mapped b, entry b.
example, consider simple tree below, consisting goal two plans, together
binding maps variable root tree. first plan (on left)
two steps: action (a1 ), recursive reference root tree ().
second plan (on right) single action (a2 ).
: goal
plan

plan
a1



a2

recursive tree represented follows. define binding b = { 7
goal({(a1 ; ):1, a2 :1})} maps whole tree, tree .
proceed defining generating functions, introduce auxiliary notation. P power series use standard notation [xp11 xpnn ]P denote
coefficient term xp11 xpnn series. define P cond denote power
xn

series containing terms P satisfy condition cond. define f g
(f g) power(x)n , i.e. f g terms power x greater n removed.
n
define f mx (f )m power(x)n , i.e. (f )m terms power x greater
n removed.22
position derive generating functions specify number
paths arbitrary, possible recursive, goal-plan tree, given bound
path length. define BDI program represented term (i.e. goal, plan,
plan multiset, action, label), b binding mapping labels terms (as defined
above). define n4(s, m, n, b) number successful paths, respect
binding b, actions, n failed actions. Similarly define
n8(s, m, n, b) number failed paths, respect b, actions,
n failed actions. want derive recurrence relations generating
functions23 :
4
F
(s, x, y, b, ) =

8
F
(s, x, y, b, ) =

X

X
m=0 n=0
X

X

n4(s, m, n, b)xm n
n8(s, m, n, b)xm n

m=0 n=0

upper bound number actions path.
22. This, previous operation, directly supported rmpoly Python library multivariate
polynomials series, used compute generating functions.
23. subscript used distinguish generating functions, allow recursive tree,
generating functions defined elsewhere paper.

102

fiOn Testability BDI Agent Systems

order simplify presentation, details complex derivations
given Appendix C. resulting equations shown Figure 10. first two
equations (Equations 11 12 Figure 10), applicable term t, capture
assumption > 0 (and remaining equations apply > 0). next
two equations simply specify labels looked provided binding. Equation 15
indicates single successful path action a, single
action unsuccessful actions (i.e. generating function 1x1 0 ). Equation 16
similarly indicates single unsuccessful path single action a, which,
unsurprisingly, single unsuccessful action (so generating function 1x1 1 ).
Equations 17 18 deal sequences: sequence s1 ; s2 succeed s1
s2 must succeed, count paths concatenating sub-paths, corresponds
multiplying power series. sequence s1 ; s2 fail either s1 fails, s1 succeeds
s2 fails (alternatives correspond addition power series). equations
special case: s1 action, divide overall path-length limit
precisely: s1 must trace length 1 (since action) s2 must therefore
maximum length 1.
dealt labels (), single actions, sequences, next turn goals (equations 19 20). cases derivation complex, covered Appendix C.
4
F
(Equation 19 Appendix C.1), intuition successful path
goals execution involves single successful plan p, number failed executions
plan selected remaining multiset plans (P {p:1}). case plan
appears multiset, select occurrences, hence
multiplication P (p). number failed paths goal (Equation 20
Appendix C.2) introduce auxiliary generating function G8(P M, x, y, z, b, ),
P multiset plans, z variable whose power z indicates exact number plans P used. words, given power series denoted
G8(P M, x, y, z, b, ), term cmno xm n z /o! indicates cmno paths involve actions, n failed, exactly plans P . generating
8
function G8 technical device allows us derive definition F
need.
8
Given power series, definition F simply selects terms |P |
power z (since plans must fail goal fail) using removes
z |P | terms dividing. G8 exponential generating function z,
means includes division factorial, need multiply factorial |P |!
remove it.
8
Equation 21 defines F
(P M, x, y, b, ), used Equation 19, terms
8
auxiliary function G. derivation given Appendix C.3. intuition
possible number plans could used (o) limit power series G8
value o, remove z dividing. o! due G8 exponential
generating function z (see Appendix C).

Finally, Equations 22 23 give definition G8(P M, x, y, z, b, ) (see Appendix C.4
derivation). Intuitively, Equation 22 creates power series plan type,
x

combines (using ). Equation 23 little complex: single way
failing (when plans used, corresponding term x0 0 z 0 = 1). Otherwise
select c plans, plans must fail (corresponding term
103

fiWinikoff & Cranefield

4
F
(t, x, y, b, ) = 0 0

(11)

8

F(t, x, y, b, ) = 0 0

(12)

4

4

(13)

8

8

(14)

F(, x, y, b, ) = F(b[], x, y, b, )
F(, x, y, b, ) = F(b[], x, y, b, )
4

(15)

8

(16)

F(a, x, y, b, ) = x
F(a, x, y, b, ) = xy
4

F(s1 ; s2 , x, y, b, )
(
4
4
F
(s1 , x, y, b, 1) F
(s2 , x, y, b, 1) s1 action
=
x
4
4
F(s1 , x, y, b, ) F
(s2 , x, y, b, ) otherwise
8
F
(s1 ; s2 , x, y, b, )
(
4
8
8
(s1 , x, y, b, 1)+F
(s1 , x, y, b, 1) F
(s2 , x, y, b, 1) s1 action
F
=
x
8
4
8
F
(s1 , x, y, b, )+F
(s1 , x, y, b, ) F
(s2 , x, y, b, ) otherwise
4
F
(goal(P ), x, y, b, )
X
x
4
8
=
P (p)F
(p, x, y, b, ) F
(P {p:1}, x, y, b, )

(17)

(18)

(19)

pset(P )
8
F
(goal(P ), x, y, b, ) = |P |!

|P |
8
F
(P M, x, y, b, ) =

X
o=0

o!

G8(P M, x, y, z, b, ) power(z)=|P |
z |P |
G8(P M, x, y, z, b, ) power(z)=o
zo

(20)
(21)

G8({p1 :c1 , . . . , pj :cj }, x, y, z, b, )
x

x

G8({p1 :c1 }, x, y, z, b, ) G8({pj :cj }, x, y, z, b, )

=

c
X
c
G({p:c}, x, y, z, b, ) = 1 +
F 8 (p, x, y, b, )ox z

8

(22)

(23)

o=1

Figure 10: Equations Recursive Goal-Plan Trees
8
F
(p, x, y, b, )), giving number failed traces across plans as:
x

x

8
8
8
F
(p, x, y, b, )ox = F
(p, x, y, b, ) F
(p, x, y, b, )
|
{z
}

times

used Python rmpoly GMPY2 libraries generate polynomial repre4
8
sentations functions F
(t, x, y, b, ) F
(t, x, y, b, ) (as defined Figure 10)
specified values t, b, . defined simple tree (the one given earlier
section example) computed number paths different values .
104

fiOn Testability BDI Agent Systems

values chosen correspond values Table 2 (which
values n4(g) n8(g) come from24 ). Table 2, values 62 363 correspond
longest path, argue comparing recursive tree uniform tree,
consider path length limit. results shown Table 5.

62
363

n4(g)
6.33 1012
1.02 10107

n8(g)
1.82 1013
2.56 10107

n4(s)
3.8 1013
1.9 1076

n8(s)
4.3 109
6.1 1054

Table 5: Comparing n4 n4 (respectively n8 n8).
Looking numbers Table 5, worth noting recursive tree
used extremely simple: two plans, single action. low number
actions (and sparseness tree) account relatively low number unsuccessful
paths. instance, modify tree adding extra actions (giving tree
binding below) = 62 around 3.9 1013 successful paths, 1.5 1011
unsuccessful paths. Unfortunately, Python unable calculate n4 n8 tree
= 363, manage = 362, 1.26 1064 successful paths,
3.281063 unsuccessful paths. shows, expected, number unsuccessful
paths higher complex tree. fewer successful paths
complex tree explained observing that, tree, traces longer (more
actions need done), traces excluded bound trace
length .
: goal
plan

plan
a1

a3



a2

a4

Overall, analysis section, application = 62 363 confirms
number paths recursive tree depends trees structure (which
unsurprising), indicates even simple recursive tree, number
paths given upper bound path length quickly becomes extremely large.

5. Reality Check
previous section analysed abstract model BDI execution order determine
size behaviour space. analysis yielded information size
behaviour space affected various factors, probability goal
failing.
section consider two issues whether analysis faithful, whether
applicable real systems. analysis made number simplifying assumptions,
24. correspond first two rows table, respectively involve 62 363 actions.

105

fiWinikoff & Cranefield

mean results may faithful semantics real BDI platform,
may apply real systems. thus conduct two reality checks assess
whether analysis faithful (Section 5.1) whether applicable (Section 5.2).
firstly assess whether analysis faithful real BDI platforms, i.e.
omit significant features, contain errors. comparing abstract
BDI execution model results real BDI platform, namely JACK (Busetta et al.,
1999). comparison allows us assess extent analysis abstract BDI
execution model matches execution takes place real (industrial strength) BDI
platform. comparison is, essence, basic reality check: simply checking
analysis previous section indeed match execution semantics typical
BDI platform. modelling artificial goal-plan tree BDI platform.
Next, order assess extent analysis results apply real systems,
analyse goal-plan tree real industrial application. analysis allows us
determine extent conclusions analysis uniform (and semi-uniform)
goal-plan trees applies real applications, goal-plan trees likely
uniform. words, extent large numbers Tables 1 2 apply
real applications?
5.1 Real Platform
order compare real BDI platforms execution results abstract BDI
execution model implemented two goal-plan trees Appendix JACK agent
programming language25 . structure plans events26 precisely mirrors
structure tree. goal-plan tree, event two relevant plans,
always applicable, selectable either order. Actions implemented
using code printed action name, then, depending condition (described
below), either continued execution triggered failure (and printed failure indicator):
System.out.print("a"); // Action "a"
((N.i & 1)==0) {
System.out.print("x");
false; // trigger failure
}
conditions determined whether action failed succeeded, plan
selected first, controlled input (N.i, Java class variable). test harness
systematically generated inputs, thus forcing decision options explored.
results matched computed Prolog code Figure 3, giving precisely
six traces smaller tree, 162 traces larger tree.
indicates abstract BDI execution model indeed accurate description
takes place real BDI platform (specifically JACK).
Note selected JACK two reasons. One modern, well known,
industry-strength BDI platform. other, important, reason, JACK descendent line BDI platforms going back PRS, thus good representative
25. code available upon request authors.
26. JACK models goal BDIGoalEvent.

106

fiOn Testability BDI Agent Systems

Parameters
Number
j k

goals
actions
2 2
3
21
62 (13)
3 3
3
91 363 (25)
Workflow 57 goals(*)
(*) paper says 60 goals,
Figure 11 57 goals.

failure handling
(secs 4.1 4.2)
n4(g)
n8(g)
128
614
1,594,323 6,337,425
294,912 3,250,604
294,912 1,625,302
294,912
812,651

failure handling
(Section 4.3)
n4(g)
n8(g)
6.33 1012
1.82 1013
107
1.02 10
2.56 10107
2.98 1020
9.69 1020
15
6.28 10
8.96 1015
9.66 1011
6.27 1011

Table 6: Illustrative values n4(g) n8(g) (bottom part ` = 4 first row, ` = 2
second, ` = 1 last row)

larger family BDI platforms. words, showing BDI execution model
analysed matches JACKs model, able argue matches execution
JACKs predecessors (including PRS dMARS), close relatives (e.g. UM-PRS
JAM).
5.2 Real Application
consider extent real systems deep branching goal-plan trees,
extent large numbers shown Tables 1 2 apply real applications,
rather uniform goal-plan trees. example real application consider
industrial application Daimler used BDI agents realise agile business processes
(Burmeister, Arnold, Copaciu, & Rimassa, 2008). Note finding suitable application
somewhat challenging: need application real (not toy system). However,
order able analyse it, application BDI-based, furthermore,
details applications goal-plan tree need available. Unfortunately, many
reported BDI-based industrial applications provide sufficient details
internals allow analysis carried out.
Figure 11 shows27 goal-plan tree work Burmeister et al. (2008)
60 achieve goals 7 levels. 10 maintain goals, 85 plans 100 context
variables (Burmeister et al., 2008, p. 41). Unlike typical goal-plan trees used BDI
platforms, tree Figure 11 consists layers and-refined goals,
refinements leaves (where plans are). terms analysis presented
paper treat link goal g set goals, say, g1 , g2 , g3
equivalent goal g single plan p performs g1 , g2 , g3 (and actions,
i.e. ` = 0 non-leaf plans).
last row Table 6 gives various n values goal-plan tree, ` = 4 (top
row), ` = 2 (middle row) ` = 1 (bottom row). Note figures actually
lower bounds assumed plans depth 0 simple linear combinations
` actions, whereas clear Burmeister et al. (2008) plans fact
27. details meant legible: structure matters.

107

fiWinikoff & Cranefield

model
LS/AB
differe
model
keep th

figuretree6:from
goal
ACM
prototype
Figure 11: Goal-plan
thehierarchy
work Burmeister
et al. (2008,
Figure 6) (reproduced
permission IFAAMAS)

advantage modeling approach implicitly offers
support parallel execution process parts
depend other. reduce overall time needed
complicated, contain nested decision making (e.g., see Burmeister et al., 2008,
process
execution. Moreover maintain goals good means
Figure 4).
provide
process


agent
monitors
roughthe
indication
size
aadditional
goal-plan tree isagility:
number
goals.

57 goals,
tree Figure
sizeto


firstthroughout
two rows Table
Comparing(e.g.

conditions
that11
fulfilled
the6. process
number possible behaviours uniform goal-plan trees real (and nontime
constraints)

pro-actively
activities
avoid
uniform)
goal-plan tree,
see
behaviour initiates
space somewhat
smaller

real
tree, thatbefore
stillthey
quite appear.
large, especially case failure handling. However,
problems
note following points:

development prototype support rapid
1. tree Figure 11 plans leaves, reduces complexity.
prototyping
execution process models provided
words goal-plan tree typical plans alternating
goals would

larger number
possible
behaviours.
LS/ABPM

proven
ofvery
helpful.
developed models
represent
living process models, directly executed
2. figures tree conservative estimate, since assume leaf plans
visualized.

part ofIn
interface
thatcalculated
coupled
simple
behaviour.
otherweb
words,user
number
paths

theis
actual
number directly
paths thefrom
real application.
under-estimate
workflow
generated
process model.

interface computed directly parameters
6. Comparison Procedural Programs
corresponding task: context variables, types possible
order argue BDI programs harder test non-agent programs, need
values.

approach
ofprocess
programs,
quickly
comparison.
Specifically,
need changes
analyze number
paths non-agent
compareand
tested.
agent
programs.
us address
concern
modeled
Thus
errorsThis

theallow
models
bethediscovered
paths criterion test suite adequacy always requires infeasibly large number

corrected
briefly
short
tests.
section
doestime.
this, analyzing number paths procedural
program.

stated starting point building ACM-prototype
model ACM-reference 108
process model developed
software demonstrator. underlying agent engine
demonstrator (JadeX) partially different modeling
execution semantics compared LS/ABPM tool.


model
prototy

compl
depend
challen

execut
depend
proces

Based
con

contex
manip
model
plans,

compl
possib
variab
variab
one
proces
variab
startin
goals.
create
h
model

fiOn Testability BDI Agent Systems

Number actions / statements
62
363
776
627

n(m)
6,973,568,802
5.39 1057
2.5 10123
5.23 1099

n4(g)
6.33 1012
1.02 10107
1.82 10157
3.13 10184

n8(g)
1.82 1013
2.56 10107
7.23 10157
7.82 10184

Table 7: Comparison values n(m), n4(g) n8(g).
define program composed primitive statements s, sequences statements P1 ; P2 , conditionals select two sub-programs. Since capture
conditions statements, elide condition, write conditional P1 + P2 indicating one Pi selected. Note that, BDI analysis, exclude loops.
define number paths program P n(P ). straightforward28 see
definition n(P ) is:
n(s) = 1
n(P1 ; P2 ) = n(P1 ) n(P2 )
n(P1 + P2 ) = n(P1 ) + n(P2 )
order compare BDI programs, consider size program,
compare programs size. key question is: procedural program
nodes significantly fewer paths BDI program size? define
size program P number primitive statements contains, denote
|P |. Note means count internal nodes syntax tree
(i.e. + ;). Therefore, comparing BDI programs, consider size
BDI program number actions29 .
work number paths varies size program P .
size program (and therefore natural number), define n(m) max{n(P ) :
|P | = m}. is, n(m) largest number paths possible program size m.
Appendix contains derivation n(m), resulting following definition (where
6 multiple 3):
n(1)
n(3)
n(5)
n(m + 1)

=
=
=
=

1
3
6
4
3

3m/3

n(2)
n(4)
n(m)
n(m + 2)

=
=
=
=

2
4
3m/3
2 3m/3

Table 7 shows comparison values n(m) n4(g) n8(g), same-sized
programs, based Table 2. worth emphasising n(m) highest possible value:
defined maximum possible programs. However, maximal program
highly atypical. example, considering programs seven statements,
28. path P1 ; P2 simply concatenates path P1 path P2 , hence product; path
P1 + P2 either path P1 path P2 , hence addition.
29. Using total number nodes tree yields almost identical results.

109

fiWinikoff & Cranefield

total 8,448 possible programs. 8448 programs, 32 12 paths (the
maximum). Figure 12 shows number paths (112) many programs
many paths. maximum 12 clearly typical: indeed, mean number paths
seven statement program 4.379, median 4. consider programs
9 statements, 366,080 programs, 16 maximal
number paths (which 27). average number paths across programs
5.95.
Overall, looking Table 7, conclude number paths BDI programs
much larger even (atypical) maximal number paths procedural program
size. supports conclusion BDI programs harder test
procedural programs.
2500"

Number'of'programs'

2000"

1500"

1000"

500"

0"
1"

2"

3"

4"
5"
6"
7"
8"
9"
Number'of'paths'in'a'procedural'program'

10"

11"

12"

Figure 12: Profile number paths 7-statement programs

7. Conclusion
summarise, analysis found space possible behaviours BDI agents is,
indeed, large, absolute sense, relative sense (compared procedural
programs size).
expected, number possible behaviours grows trees depth (d) breadth
(j k) grow. However, somewhat surprisingly, introduction failure handling makes
significant difference number behaviours. instance, uniform goalplan tree depth 3 j = k = 2, adding failure handling took number successful
behaviours 128 6,332,669,231,104.
consider negative consequences analysis, worth highlighting
one positive consequence: analysis provides quantitative support long-held belief
110

fiOn Testability BDI Agent Systems

BDI agents allow definition highly flexible robust agents. Flexibility
defined number possible behaviours agent, shown large.
Robustness defined ability agent recover failure. analysis
Section 4.6 showed BDI failure recovery mechanism effective achieving low
rate actual failure (< 1%), even action reasonable chance failing (5%).
analysis paper tell us testability BDI agent systems?
answer question, need consider tested. Testing
typically carried levels individual components (unit testing), collections
components (integration testing), system whole.
Consider testing whole system. behaviour space sizes depicted Tables 1, 2
6 suggest quite strongly attempting obtain assurance systems correctness
testing system whole feasible. reason (as discussed
Section 1.1), adequate test suite (using paths criterion adequacy) requires
least many tests paths program tested. program has, say,
1013 paths, even test suite tens thousands tests inadequate,
hugely inadequate, since covers tiny fraction percent number
paths.
fact, situation even worse consider number possible
executions probability failing: space unsuccessful executions particularly hard test, since many unsuccessful executions (more successful ones),
probability unsuccessful execution low, making part behaviour
space hard reach. Furthermore, shown Section 4.6, although making assumptions
possible numbers action failures occur given execution reduces
number possible behaviours, still many many behaviours, even relatively
small trees (e.g. j = k = = 3).
system testing BDI agents seems impractical. unit testing
integration testing? Unfortunately, always clear apply usefully
agent systems interesting behaviour complex possibly emergent.
example, given ant colony optimisation system (Dorigo & Stutzle, 2004), testing single
ant doesnt provide much useful information correct functioning whole
system. Similarly, BDI agents, testing sub-goal difficult ensure
testing covers situations goal may attempted. Consequently,
difficult draw conclusions correctness goal results testing
sub-goals.
need acknowledge analysis somewhat pessimistic: real BDI systems
necessarily deep heavily branching goal-plan trees. Indeed, tree
real application described Section 5 smaller behaviour space abstract
goal-plan trees analysed Section 4. However, even though smaller, still quite large,
cause problems validation:

One big challenges test phase keep model consistent
define right context conditions result correct execution
scenarios. Therefore support dependency analysis, automated
111

fiWinikoff & Cranefield

simulation testing process models needed (Burmeister et al., 2008,
p. 42)30 .
leave us respect testing agent systems? conclusion
seems testing whole BDI system feasible. number possible
approaches dealing issue testability could recommended:
Keep BDI goal-plan trees shallow sparse. keeps number behaviours small. issue approach lose benefits BDI
approach: reasonably large number behaviours desirable provides
flexibility robustness.
Avoid failure handling. Since failure handling large contributor behaviour space, could modify agent languages disable failure handling. Again,
useful approach disabling failure handling removes benefits
approach, specifically ability recover failures.
Make testing sophisticated. Could testing coverage perhaps improved
incorporating additional information domain knowledge, detailed
model environment (which indicates possible failure modes probabilities)? answer known, potentially interesting area
work. However, large number paths encourage much optimism
approach.
Another, related, direction see whether patterns exist behaviour space.
Since failure recovery mechanism certain structure, may
results behaviour space large, but, sense, structured.
structure exists, may useful making agents testable. However,
point time, research direction may may turn fruitful;
viable testing strategy.
Finally, related direction try intelligent selection
test cases, order gain coverage given number test cases. One
approach this, recently described, evolutionary testing
(Nguyen, Miles, Perini, Tonella, Harman, & Luck, 2009a), genetic evolution
used find good (i.e. challenging) test cases.
Supplement testing alternative means assurance. Since testing
able cover large behaviour space, consider forms assurance.
promising candidate form formal method31 . Unfortunately, formal
methods techniques yet applicable industry-sized agent systems (we return
below, Section 7.1).
30. Burmeister et al. made following observation: approach changes process
quickly modeled tested. Thus errors models discovered corrected short time.
discussing advantages executable models, arguing able execute
model allowed testing, useful detecting errors model. able execute
model undoubtedly useful, evidence given (nor specific claim made) testing
sufficient assuring correctness agent system.
31. See volume edited Dastani et al. (2010) recent overview current state-of-the-art,
including chapter role formal methods assurance agents (Winikoff, 2010).

112

fiOn Testability BDI Agent Systems

Proceed caution. Accept BDI agent systems general robust (due
failure-handling mechanisms), is, present, practical way
assuring behave appropriately possible situations. worth
noting humans similar respect. Whilst train, examine
certify human certain role (e.g. pilot surgeon), way assuring
he/she behave appropriately situations. Consequently, situations
incorrect behaviour may dire consequences, surrounding system needs
safety precautions built (e.g. process double-checks information,
backup system co-pilot).
7.1 Future Work
room extending analysis Section 4. Firstly, analysis single
goal within single agent. Multiple agents collaborating achieve single highlevel goal viewed shared goal-plan tree certain goals and/or plans
allocated certain agents. course, distributed goal plan tree
concurrency. concurrency introduced, would useful consider whether
certain interleavings concurrent goals fact equivalent. Furthermore,
considered achievement goals. would interesting consider types goals (van
Riemsdijk, Dastani, & Winikoff, 2008). Secondly, analysis focused BDI agents,
one particular type agent. would interesting consider sorts
agent systems, and, broadly, sorts adaptive systems.
Another extension analysis consider criteria test suite adequacy.
paper used paths criterion, arguing appropriate.
recognize paths actually quite strong criterionit subsumes many
criteria (Zhu et al., 1997, Figure 7). alternative criterion could consider
edges, known branch coverage decision coverage. requires
choice program, statement, tests test suite
exercise options, i.e. edges program graph covered. edges
criterion weaker paths regarded generally accepted minimum
(Jorgensen, 2002).
Another area refinement analysis make less abstract. Two specific areas
could made detailed resources environment. analysis
consider resources environment directly, instead, considers actions may
fail range reasons might include resource issues, environmental issues.
analysis could extended explicitly consider resources interaction goals
(Thangarajah, Winikoff, Padgham, & Fischer, 2002). could extended
explicit model environment.
Whilst analysis consider real application, would desirable consider
range applications. could provide additional evidence analysis unduly
pessimistic, would lead understanding variance goal-plan trees
characteristics across applications. key challenge finding suitable applications
BDI-based, sufficiently complex (ideally real applications), detailed design
information available (and preferably source code). Another challenge methodology:
analysed shape goal-plan tree Daimler workflow application,
113

fiWinikoff & Cranefield

access run system. alternative methodology, requires access
implemented system probably source code, run it, force generate
traces sub-goals32 (which would require modification either source code
underlying agent platform). collected data shape real-world
industrial applications, able analyse whether uniform semi-uniform goalplan trees good models types system, whether seek ways
relax uniformity assumption.
importantly, highlighted difficulties assuring BDI agent systems
testing, need find ways assuring systems.
approach promise automatic generation test cases agent
systems (Nguyen, Perini, & Tonella, 2007; Zhang, Thangarajah, & Padgham, 2009). However, size behaviour space suggests number test cases needed may
large, testing failed plan execution difficult. One interesting,
potentially promising, avenue use formal techniques help guide test generation
process (e.g. symbolic execution specification-guided testing) (Dwyer, Hatcliff, Pasareanu, Robby, & Visser, 2007).
Another approach33 attracted interest model checking agent systems
(Wooldridge, Fisher, Huget, & Parsons, 2002; Bordini, Fisher, Pardavila, & Wooldridge,
2003; Raimondi & Lomuscio, 2007). work promising model checking techniques use range abstractions cover large search space without deal
individual cases one-at-a-time (Burch, Clarke, McMillan, Dill, & Hwang, 1992; Fix,
Grumberg, Heyman, Heyman, & Schuster, 2005). Furthermore, verifying subgoal considers possibilities, possible combine verification different sub-goals.
However, work needed: Raimondi Lomuscio (2007) verify systems agents
defined abstractly, i.e. terms plans goals. MABLE agent programming
language (Wooldridge et al., 2002) actually imperative language augmented certain agent features, BDI language; work Bordini et al. (2003)
include failure handling. general, state art model checking agent system
implementations still limited quite small systems (Dennis, Fisher, Webster, & Bordini,
2012).
Acknowledgements
would thank members Department Information Science University
Otago discussions relating paper. would thank Lin Padgham
comments draft paper. Finally, would thank anonymous
reviewers insightful comments helped improve paper.
work paper done Winikoff sabbatical RMIT, visiting
University Otago.

32. Generating traces top level goal likely feasible.
33. work deductive verification, (based research verification
concurrent systems) appears less likely result verification tools (relatively)
easy use applicable real systems.

114

fiOn Testability BDI Agent Systems

Appendix A. Example Goal-Plan Trees Expansions
Suppose following two trees: sample (left) sample2 (right). trees
correspond j = 2, k = ` = 1, = 1 sample = 2 sample2.
goal
plan


goal

plan

goal

b c

goal



plan

plan

plan

plan

plan

plan



b

e

f

g

h

trees expanded respectively following sequences actions,
letter indicates execution action, 8 indicates failure34 . predicted
formulae, four successful executions two unsuccessful executions
first tree:

b

a8b
b8a

a8b8
b8a8

second tree, expansions following 162 possibilities (consisting 64
successful 98 unsuccessful traces).





















e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e
e

b
b8cgd
b8cgd8
b8cg8hd
b8cg8hd8
b8cg8h8
b8chd
b8chd8
b8ch8gd
b8ch8gd8
b8ch8g8
b8c8
8fb
8fb8cgd
8fb8cgd8
8fb8cg8hd
8fb8cg8hd8
8fb8cg8h8
8fb8chd
8fb8chd8






















f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f
f

b8chd
b8chd8
b8ch8gd
b8ch8gd8
b8ch8g8
b8c8
8eb
8eb8cgd
8eb8cgd8
8eb8cg8hd
8eb8cg8hd8
8eb8cg8h8
8eb8chd
8eb8chd8
8eb8ch8gd
8eb8ch8gd8
8eb8ch8g8
8eb8c8
8e8cgd
8e8cgd8

c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c

g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g

d8
d8
d8
d8
d8
d8
d8
d8
d8
d8
d8
8h
8h
8h
8h
8h
8h
8h
8h
8h

aeb
aeb8
ae8fb
ae8fb8
ae8f8
afb
afb8
af8eb
af8eb8
af8e8
a8

d8aeb
d8aeb8
d8ae8fb
d8ae8fb8
d8ae8f8
d8afb
d8afb8
d8af8eb

c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c

h
h
h
h
h
h
h
h
h
h
h
h
h
h
h
h
h
h
h
h

d8afb8
d8af8eb
d8af8eb8
d8af8e8
d8a8
8gd
8gd8aeb
8gd8aeb8
8gd8ae8fb
8gd8ae8fb8
8gd8ae8f8
8gd8afb
8gd8afb8
8gd8af8eb
8gd8af8eb8
8gd8af8e8
8gd8a8
8g8aeb
8g8aeb8
8g8ae8fb

34. Note failure marker isnt counted considering length trace Section 4.6.

115

fiWinikoff & Cranefield























e8fb8ch8gd
e8fb8ch8gd8
e8fb8ch8g8
e8fb8c8
e8f8cgd
e8f8cgd8
e8f8cg8hd
e8f8cg8hd8
e8f8cg8h8
e8f8chd
e8f8chd8
e8f8ch8gd
e8f8ch8gd8
e8f8ch8g8
e8f8c8
fb
fb8cgd
fb8cgd8
fb8cg8hd
fb8cg8hd8
fb8cg8h8

af8e8cg8hd
af8e8cg8hd8
af8e8cg8h8
af8e8chd
af8e8chd8
af8e8ch8gd
af8e8ch8gd8
af8e8ch8g8
af8e8c8
a8cgd
a8cgd8
a8cg8hd
a8cg8hd8
a8cg8h8
a8chd
a8chd8
a8ch8gd
a8ch8gd8
a8ch8g8
a8c8
cgd

c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c

g8hd8af8eb8
g8hd8af8e8
g8hd8a8
g8h8aeb
g8h8aeb8
g8h8ae8fb
g8h8ae8fb8
g8h8ae8f8
g8h8afb
g8h8afb8
g8h8af8eb
g8h8af8eb8
g8h8af8e8
g8h8a8
hd
hd8aeb
hd8aeb8
hd8ae8fb
hd8ae8fb8
hd8ae8f8
hd8afb

c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c
c

h8g8ae8fb8
h8g8ae8f8
h8g8afb
h8g8afb8
h8g8af8eb
h8g8af8eb8
h8g8af8e8
h8g8a8
8aeb
8aeb8
8ae8fb
8ae8fb8
8ae8f8
8afb
8afb8
8af8eb
8af8eb8
8af8e8
8a8

Appendix B. Analysis Recurrence Relations
appendix contains details derivation Section 4.4.
exponential generating function F (x) sequence {f 4 (j, a, b)}
j=0 function
defined following power series:
F (x) =


X

f 4 (j, a, b)

j=0

xj
j!

(24)

(by definition f 4 )
!
!
j1



j
X
X
X
X
x
xj
j
j
=
i!ai (j i)b
=
i!ai (j i)b

j!

j!
j=0

i=0

j=0

i=0

right hand side
changed upper limit inner sum based
j
generalised definition j(j 1)(j 2) . . . (j + 1)/i!, valid

complex numbers j non-zero integers (Wilf, 1994) gives ji = 0 > j.
right hand side form product exponential generating functions (Wilf,
1994, Rule 30 , Section 2.3):



!




j
j
X
X
X
X
x
x
j
xj

(j)
(j) =
(i)(j i)
j!
j!

j!
j=0

j=0

j=0

i=0

where, case, (j) = j! aj (j) = j b. Therefore, write:





j
j
X
X
(ax)
x
F (x) =
j!
jb
j!
j!
j=0

j=0

116

fiOn Testability BDI Agent Systems

P
1
left hand sum G(ax) G(y) = n n = 1y
(Wilf, 1994, Equation 2.5.1)35 .

P
n
x
x

0
right hand sum equal bx dx
n! (Wilf, 1994, Rule 2 , Section 2.3) = bx dx e
x
(Wilf, 1994, Equation 2.5.3) = bxe . Thus have:
F (x) =

1
bxex
bxex =
1 ax
1 ax

P
xj
4
Therefore, f 4 (0, a, b) constant term power series
j=0 f (j, a, b) j! ,
F (0) = 0. find recurrence relation defining f 4 (j + 1, a, b) equate original
definition F (x) Equation 24 closed form function, differentiate
side (to give us power series f 4 (j, a, b) values shifted one position left),
multiply denominator closed form, giving us following derivation.





X 4

bxex
xj
(1 ax)
= (1 ax)
f (j, a, b)
dx
j!
dx 1 ax
j=0



X
xj1
b(x+1)ex
abxex
4
= (1 ax)
f (j, a, b)j
= (1 ax)
+
j!
1 ax
(1 ax)2
j=0

=


X
j=0



bxex
xj1 X 4
xj

= b(x+1)ex +
f (j, a, b)j
af (j, a, b)j
j!
j!
1 ax
4

j=0

4

(recall f (0, a, b) = 0)


X
X
xj
xj
4
=
f (j +1, a, b)(j +1)

ajf 4 (j, a, b)
(j +1)!
j!
j=0

j=0

= bxex + bex +


X

f 4 (j, a, b)

j=0

(recall

bxex

=

j=0

Equating coefficients

xj
j!

xj
j=0 jb j! ,

j
X

ex =

j=0

j=0

P


X
xj
=b
j +b
j!

xj
j!
xj
j=0 j! )

P



X
xj
x
+a
f 4 (j, a, b)
j!
j!

get:

f 4 (j +1, a, b) ajf 4 (j, a, b) = bj + b + af 4 (j, a, b)
= f 4 (j +1, a, b) = b(j +1) + af 4 (j, a, b) + ajf 4 (j, a, b)
= (j +1)(b + af 4 (j, a, b))

(25)

35. Note many operations performed generating functions (and used paper)
valid without concern convergence series. combinatorics, generating functions often
treated analytic functions evaluated specific variable values, rather formal (possibly
infinite) algebraic objects, well defined operations addition multiplication.
set formal power series finite set variables structure ring abstract algebra,
ring notion function convergence evaluation (Wilf, 1994, ch. 2).

117

fiWinikoff & Cranefield

Appendix C. Analysis Recursive Goal-Plan Trees
appendix contains detailed derivations relating Section 4.7.
4
C.1 Derivation F
(goal(P ), x, y, b, )
4
define F
(goal(P ), x, y, b, ) terms n4 usual way, noting upper
bound realise length bound:

4
F
(goal(P ), x, y, b, ) =

X

X

n4(goal(P ), m, n, b)xm n

m=0 n=0
4

n(goal(P ), m, n, b) defined Section 4.7. make use nonbounded version (which four arguments):
4
F
(goal(P ), x, y, b) =

X

X

n4(goal(P ), m, n, b)xm n

m=0 n=0
4

define n counting successful traces:
n4(goal(P ), m, n, b)
X
=
P (p)

X

n4(p, m1 , n1 , b) n8(P {p:1}, m2 , n2 , b)

m1 +m2 =m
n1 +n2 =n

pset(P )

n8(P M, m, n, b) number unsuccessful paths using zero plans
plan multiset P (with respect binding b) actions, n
failed actions.
inner sum considers ways partition numbers actions (m) action
failures (n) caused single plan shape p P
P
caused


plans.


Section
4.6
(page
98)

use

identity
m=0
p+q=m f (p, q) =
P P
p=0
q=0 f (p, q) rewrite:
4
F
(goal(P ), x, y, b)
X

X
X
X
=
P (p)
n4(p, m1 , n1 , b) n8(P {p:1}, m2 , n2 , b)xm n

m1 +m2 =m
n1 +n2 =n

m=0 n=0 pset(P )

X

=

P (p)

pset(P )

X

X

X

n4(p, m1 , n1 , b) n8(P {p:1}, m2 , n2 , b)xm n

m=0 n=0 m1 +m2 =m
n1 +n2 =n

give us:
4
F
(goal(P ), x, y, b)
X
=
P (p)

pset(P )


X
X

X

X

n4(p, m1 , n1 , b)xm1 n1 n8(P {p:1}, m2 , n2 , b)xm2 n2

m1 =0 m2 =0 n1 =0 n2 =0

118

fiOn Testability BDI Agent Systems

=

X

P (p)

pset(P )
X

X

4

n(p, m1 , n1 , b)x



=

n8(P {p:1}, m2 , n2 , b)xm2 n2

m2 =0 n2 =0

m1 =0 n1 =0

X

X

X

m1 n1

4

8

P (p) F(p, x, y, b) F(P {p:1}, x, y, b)

pset(P )
8
F
(P M, x, y, b) generating function n8(P M, m, n, b). Section C.3
8
provide definition F
(P M, x, y, b) terms auxiliary function G8 (see Section C.4).
introduce bound length paths giving:
4
F
(goal(P ), x, y, b, )
X
x
4
8
=
P (p) (F
(p, x, y, b) F
(P {p : 1}, x, y, b))

pset(P )

=

X

x

4
8
P (p) (F
(p, x, y, b, ) F
(P {p : 1}, x, y, b, ))

pset(P )
8
C.2 Derivation F
(goal(P ), x, y, b, )

Similarly previous derivation, define:
8
F
(goal(P ), x, y, b, ) =

X

X

n8(goal(P ), m, n, b)xm n

m=0 n=0
8
(goal(P ), . . . ) terms plans P ,
derive recursive definition F
8
first define new function n(P M, m, n, o, b), denotes number unsuccessful
paths use plans multiset P . have:

n8(goal(P ), m, n, b) = n8(P M, m, n, |P |, b)
states goal fail, |P | plans multiset must tried.
define generating function G8(P M, x, y, z, b, ) n8(P M, m, n, o, b) ordinary x exponential z, i.e. coefficients xm n z /o! values
n8(P M, m, n, o, b).
have:
8
F
(goal(P ), x, y, b, ) =

X

X

n8(P M, m, n, |P |, b)xm n

m=0 n=0

wish rewrite terms G8 . generalising right hand side
sum possible values number plans used (o), followed restriction
select values = |P |:

119

fiWinikoff & Cranefield

8
F
(goal(P ), x, y, b, )

= |P |!

X

X
n8 (P M, m, n, |P |, b) z |P |


|P |! z |P |

xm n

m=0 n=0

X
P
8

X
o=0 n(P M, m, n, o, b)z /o! power(z)=|P |
|P |!
xm n
|P |
z
m=0 n=0

=

P
= |P |!

P P 8

n
m=0
n=0
o=0 n(P M, m, n, o, b)x z /o!



power(z)=|P |

z |P |

Since nested sum definition G8 (see Section C.4), simplify to:
8

F(goal(P ), x, y, b, ) = |P |!

G8(P M, x, y, z, b, ) power(z)=|P |
z |P |

8
Section C.4 derive definition G8(P M, . . .) terms F
(p, . . .) p
set(P ).

8
C.3 Definition F
(P M, x, y, b, )

Recall n8(P M, m, n, b) number unsuccessful paths using zero
plans plan multiset P (with respect binding b) actions, n
8
failed actions, F
(P M, x, y, b, ) ordinary generating function.
First consider case P empty. case, precisely one way
8
fail, generates trace length zero. Therefore, F
({}, x, y, b, ) = 1x0 0 = 1.
case P non-empty sum number plans used
execution, yields following definition:
|P |
8

n(P M, m, n, b) =

X

n8(P M, m, n, o, b)

o=0

n8(P M, m, n, o, b) is, before, number unsuccessful paths plan
8
multiset
, using plans. Therefore, using definition F
(P M, x, y, b, ) =
P PP

8

n
n
(P
M,
m,
n,
o,
b)x

,

have:
m=0
n=0
8
F
(P M, x, y, b, )

=

M|
X
|P
X
X

n8(P M, m, n, o, b)xm n

m=0 n=0 o=0

(replace n8 looking coefficient corresponding term G8 ,
o! accounts division o! G8 ; reorder summations)
|P |

=

X

X X

o![xm n z ]G8(P M, x, y, z, b, )xm n

o=0 m=0 n=0

120

fiOn Testability BDI Agent Systems

(we shift o! outwards, multiply z /z )
P
|P | P
n
8
n
X
n=0 [x z ]G(P M, x, y, z, b, )x z
m=0
=
o!
zo
o=0

|P |

=

X
o=0

o!

G8(P M, x, y, z, b, ) power(z)=o
zo

C.4 Definition G8(P M, x, y, z, b, )
define G8(P M, x, y, z, b, ) generating function n8(P M, m, n, o, b) ordinary x exponential z (hence division o! below), ( 0)
maximum allowed trace length:
G8(P M, x, y, z, b, ) =

X
X

X

n8(P M, m, n, o, b)

m=0 n=0 o=0

xm n z
o!

Recall n8(P M, m, n, o, b) denotes number unsuccessful paths use
plans multiset P . empty multiset plans successful execution,
single unsuccessful execution 0 actions, uses 0 plans, hence:
(
1 = n = = 0
8
n({}, m, n, o, b) =
0 otherwise
Therefore, G8({}, x, y, z, b, ) = 1. non-empty multisets must partition actions
trace, action failures, numbers plans used, across different plan bodies
multiset, consider ways plans various plan shapes
interleaved give overall order attempting plans:
n8({p1 :c1 , . . . , pj :cj }, m, n, o, b)
X
=
n8({p1 :c1 }, m1 , n1 , o1 , b) n8({pj :cj }, mj , nj , oj , b)
m1 ++mj =m
n1 ++nj =n
o1 ++oj =o

multinomial coefficient
Thus:


o1 ...oj



=

o!
o1 !...oj !

G8({p1 :c1 , . . . , pj :cj }, x, y, z, b, )
(by definition G8, using restriction, rather bounded sum m,
expanding n8 above)



X
X

=
n8({p1 :c1 }, m1 , n1 , o1 , b)

.
.
.

1
j
++m =m
m,n,o=0

1

j

n1 ++nj =n
o1 ++oj =o

xm n z
n({pj :cj }, mj , nj , oj , b)
o!
8

121

!
power(x)

fiWinikoff & Cranefield

=


X

X

o!
n8({p1 :c1 }, m1 , n1 , o1 , b)

!
.
.
.

!
1
j
=m

m,n,o=0 m1 ++mj
n1 ++nj =n
o1 ++oj =o

xm n z
n8({pj :cj }, mj , nj , oj , b)
o!

!
power(x)

(cancelling o! distributing oi ! xmi , ni z oi )
=


X

X

n8({p1 :c1 }, m1 , n1 , o1 , b)

m,n,o=0 m1 ++mj =m
n1 ++nj =n
o1 ++oj =o

xm1 n1 z o1
o1 !

xmj nj z oj
n({pj :cj }, mj , nj , oj , b)
oj !

!

8

(replacing


X

X



m=0 m1 +m2 =m


=


X



X
X

redistributing sums)

m1 =0 m2 =0

n8({p1 :c1 }, m1 , n1 , o1 , b)

xm1 n1 z o1

m1 ,n1 ,o1 =0




X



power(x)

o1 !




n8({pj :cj }, mj , nj , oj , b)

xmj nj z oj

mj ,nj ,oj =0

oj !


power(x)

x

(replacing restriction )




n

X
1
1
1
x z x
=
n8({p1 :c1 }, m1 , n1 , o1 , b)

o1 !
m1 ,n1 ,o1 =0




n

X
j
j
j
x
x z

n8({pj :cj }, mj , nj , oj , b)
oj !
mj ,nj ,oj =0

8

(by definition G . . . )
x

x

= G8({p1 :c1 }, x, y, z, b, ) G8({pj :cj }, x, y, z, b, )

need define G8({pi :ci }, x, y, z, b, ).
Consider n8({p:c}, m, n, o, b). simple cases = 0: use
plans, single unsuccessful
path, actions (m = n = 0).

c!
hand, > 0 oc = o!(co)!
ways selecting c available
copies plan p. selected plans executed o! different orders.
execution sum possible distributions actions (successful unsuccessful)
122

fiOn Testability BDI Agent Systems

amongst plans. gives:

X
c


o!
n8(p, m1 , n1 , b) n8(p, mo , , b)




m1 ++mo =m
n1 ++no =n
n8({p:c}, m, n, o, b) =


1
= n = = 0



0
otherwise

> 0

therefore following definition G8({p:c}, x, y, z, b, ), initial 1 abbreviates 1x0 0 z 0 /0!, i.e. base case = n = = 1, rest
definition G8 , expanding n8 using definition.
G8({p:c}, x, y, z, b, )
= 1+



X
c
o!




X

X

n8(p, m1 , n1 , b) n8(p, mo , , b)

1 ++mo =m
n1 ++no =n

m=0 n=0,o=1

xm n z
o!

(cancel o!/o!, rearrange sums replace upper bound
restriction)





X
X
X
X

c
8
8

n


= 1+
n(p, m1 , n1 , b) n(p, mo , , b)x

power(x) z


o=1
m=0 n=0
m1 ++mo =m
n1 ++no =n


X

(replacing

X

m=0 m1 +m2 =m

= 1+





X
X

redistributing sums)

m1 =0 m2 =0


X
c
o=1


X

X

!
n8(p, m1 , n1 , b)xm1 n1



m1 =0 n1 =0

X

X

!!
n8(p, mo , , b) xmo

mo =0 =0


power(x) z
!o
X
X

X
c
= 1+
n8(p, m, n, b) xm n
power(x) z

m=0 n=0
o=1
XX
8
(Replace
n8(p, m, n, b)xm n F
(p, x, y, b, ) per definition)


n

c
X
c
= 1+
F 8 (p, x, y, b, )o power(x) z

o=1
c
X
c
F 8 (p, x, y, b, )ox z
= 1+

o=1

123

fiWinikoff & Cranefield

Appendix D. Analysis Procedural Code Structures
seek derive expression largest possible number paths program
given size have, i.e. definition n(m) = max{n(P ) : |P | = m}. Recall
program either (atomic) statement single path (i.e. n(s) = 1), sequence
two programs P1 ; P2 n(P1 ; P1 ) = n(P1 ) n(P2 ), conditional P1 + P2
n(P1 + P2 ) = n(P1 ) + n(P2 ).
relatively easy see examining possible programs 3
n(m) = m. instance, largest number paths = 3 obtained program
+ + s. easy show = 4 largest number paths possible 4.
larger values m? observe > 4 program36
largest number paths follows particular form. = 5 program
largest path written P5 = (s + + s); (s + s), n(P5 ) = 3 2.
generally, define S2 + s, S3 + + s, following
result, shows programs maximal number paths size,
considered particular form.
Theorem D.1 program size (for > 4) largest possible number
paths written Pi = Pi1 ; Pi2 ; . . . ; Pik Pij (1 j k) either S2
S3 .
Proof: establish result induction. assume holds n
4 < n m, show must hold + 1. So, let us assume
program Pm+1 maximal number paths, form
j
1
2
k
Pm+1
; Pm+1
; . . . ; Pm+1
Pm+1
either S2 S3 . two cases, depending
structure Pm+1 . consider case turn show fact either (a)
Pm+1 rewritten desired form, preserving number paths
program size; (b) Pm+1 cannot maximal, since construct program size
+ 1 larger number paths Pm+1 .
j
1
2
k
Case 1: Pm+1 form Pm+1
; Pm+1
; . . . ; Pm+1
least one Pm+1
neither

S2 S3 . Let Pm+1 one sub-programs neither S2 S3 . convenience

define P shorthand Pm+1
. Now, since P size less + 1, induction
hypothesis applies37 , written form Pi1 ; Pi2 ; . . . Pil Pij
either S2 S3 . easy see one rewrite Pm+1 desired form
exploiting associativity ;, rewriting follows:
i1
i+1
i1
i+1
. . . Pm+1
; (Pi1 ; Pi2 ; . . . Pij ); Pm+1
; . . . = . . . Pm+1
; Pi1 ; Pi2 ; . . . Pij ; Pm+1
;...

Applying rewriting Pm+1
S2 S3 yields program size
+ 1, number paths original program, desired form:
sequence sub-programs, either S2 S3 . shows result holds
+ 1, i.e. maximal-path program written desired form.
1
2
k
Case 2: Pm+1 form Pm+1
; Pm+1
; . . . ; Pm+1
k, means
1
k
Pm+1 must consist single conditional, i.e. Pm+1 = Pm+1 + . . . + Pm+1
k > 1.

36. fact one maximal-path program, structure, modulo
swapping order arguments + ;.
37. Or, size 4, written S2 ; S2 maximal number paths program
size 4 meets desired form.

124

fiOn Testability BDI Agent Systems

1
2
Without loss generality view Pm+1 form Pm+1
+ Pm+1
(by viewing
1
k
1
k
Pm+1 + . . . + Pm+1 (Pm+1 + . . .) + Pm+1 k > 2). consider following
1
2
sub-cases, depending values n(Pm+1
) n(Pm+1
).
1
2
Case 2a: n(Pm+1
) n(Pm+1
) greater 2. show Pm+1
0
1
2
maximal. Consider program Pm+1
= Pm+1
; Pm+1
(i.e. + replaced
1
2
1
2
;). know n(Pm+1 ; Pm+1 ) = n(Pm+1 ) n(Pm+1
). Without loss gen1
2
erality, lets assume n(Pm+1 ) n(Pm+1 ). show original Pm+1
0
1
fewer paths Pm+1
. number paths Pm+1 n(Pm+1 ) = n(Pm+1
)+
2
1
2
1
2
n(Pm+1 ). Since n(Pm+1 ) n(Pm+1 ), n(Pm+1 ) = n(Pm+1 ) + n(Pm+1 )
2
2
2
2
1
n(Pm+1
) + n(Pm+1
) = 2 n(Pm+1
). Since n(Pm+1
) n(Pm+1
) greater
2
1
2
0
2, 2 n(Pm+1 ) < n(Pm+1 ) n(Pm+1 ) = n(Pm+1
), i.e.
0
Pm+1 paths Pm+1 , hence Pm+1 maximal + 1.
1
2
Case 2b: least one n(Pm+1
) n(Pm+1
) greater 2. Without loss
1
2
1
generality, assume n(Pm+1 ) n(Pm+1
). two cases: n(Pm+1
)
either 2 1.
1
) = 1.
Sub-case 2b(i): Let us consider first case n(Pm+1
program one path statement s, sequence statements s; s; . . . ; s.
Clearly latter maximal since replacing + + . . . + would
result program size paths. So, therefore Pm+1
1
2
2
maximal, Pm+1
must s, Pm+1 = + Pm+1
. Therefore Pm+1

size m. two sub-cases: either still greater 4, = 4.
second sub-case simple: 4 show, inspecting possible
2
programs size 4, n(4) = 4, therefore n(s + Pm+1
)
1 + 4 = 5. However, know (s + + s); (s + s) size 5 6 paths,
hence sub-case Pm+1 cannot maximal number paths.
first sub-case, still greater 4, induction hypothesis applies
2
2
written desired form. abbreviate Pm+1

therefore Pm+1
j

1
2
P2 , Pm+1 = + (P2 ; P2 ; . . . ; P2 ) P2 either S2
00
S3 . Consider variant program Pm+1
= ((s + P21 ); P22 ; . . . P2j ),
00
clearly size Pm+1 . show Pm+1
paths
j
00
1
2
2
Pm+1 : n(Pm+1 ) = ((1 + n(P2 )) n(P2 ; . . . ; P2 )) = n(P2 ; . . . ; P2j ) + (n(P21 )
n(P22 ; . . . ; P2j )). Now, n(Pm+1 ) = 1 + (n(P21 ) n(P22 ; . . . ; P2j )). order show
00
n(Pm+1 ) < n(Pm+1
) need show 1 < n(P22 ; . . . ; P2j )
follows fact must least one P2i , that, since P2i
either S2 S3 , size least 2.
1
2
1
Sub-case 2b(ii): know n(Pm+1
) = 2 2 n(Pm+1
). Since n(Pm+1
)
2
2
2
n(Pm+1 ) n(Pm+1 ) 2 hence n(Pm+1 ) = 2 + n(Pm+1 )
2
0
2
2 n(Pm+1
) = n(Pm+1
). Now, n(Pm+1
) strictly greater 2
0
n(Pm+1 ) strictly less n(Pm+1 ) shown Pm+1 actually
2
maximal number paths. hand, n(Pm+1
)=2
2
n(Pm+1 ) = 2 + n(Pm+1 ) = 2 + 2 = 4. However, values
theorem applies, know n(m) > 4, therefore
shown sub-case Pm+1 maximal + 1.

125

fiWinikoff & Cranefield

shown assume Pm+1 maximal structure
specified, fact one derive another program, size + 1, either
satisfy desired structure, larger number paths Pm+1 , contradicts assumption Pm+1 maximal. establishes desired property Pm+1 .
induction result applies > 4, desired.
previous result shows considering programs given size
largest possible number paths (denoted Pm ), limit considering
programs form P1m ; P2m ; . . . ; Pkm Pim either + + + s.
derive definition n(m). Firstly, observe that, inspecting cases:
n(m) = m, 4
n(5) = 6
n(6) = 9
two first cases discussed above. last case, two programs
appropriate structure size 6: S2 ; S2 ; S2 (with 8 paths) S3 ; S3
(with 9 paths).
consider > 6. Adding statement program (i.e. going m+1)
effect modifies Pm adding one Pim , increments n(Pim ) one.
Since multiplication commutative associative, without loss generality, assume
) n(P )
increment n(Pkm ). therefore n(Pm ) = n(P1m ; . . . ; Pk1
k



n(Pm+1 ) = n(P1 ; . . . ; Pk1 ) (n(Pk ) + 1). two cases:
)3 therefore
Case 1: Pim S3 , n(Pm ) = n(P1m ; . . . ; Pk1
) 4 = n(P ) 4 . Note case P
n(Pm+1 ) = n(P1m ; . . . ; Pk1

m+1
3


written P1 ; . . . ; Pk1 ; S2 ; S2 .

Case 2: Pim S2 S3 observe replacing 2 3 gives
greater increase number paths replacing 3 4, hence (after
)2
possibly reordering Pim Pkm = S2 ) n(Pm ) = n(P1m ; . . . ; Pk1
3


n(Pm+1 ) = n(P1 ; . . . ; Pk1 ) 3 = n(Pm ) 2 .
therefore recursive definition n(m) depending form Pm . next
observe fact form Pm follows simple cycle. know = 6, case 1
holds (as above, P6 = S3 ; S3 ). therefore P7 written S3 ; S2 ; S2 , hence
P8 written S3 ; S3 ; S2 S3 ; S2 ; S3 , hence P9 written S3 ; S3 ; S3 .
generally, prove induction Pm written P1m ; . . . ; Pkm
following holds: (a) multiple 3, Pim S3 ; (b)
one multiple 3, exactly two Pim S2 rest S3 ;
(c) two multiple 3, exactly one Pim S2 rest
S3 . gives us following recursive definition, 6 multiple 3:
n(m + 1) = n(m)

4
3

n(m + 2) = n(m + 1)
126

3
2

fiOn Testability BDI Agent Systems

n(m + 3) = n(m + 2)

3
2

simplified to:
4
3
34
n(m + 2) = n(m)
= 2 n(m)
23
334
= 3 n(m)
n(m + 3) = n(m)
223
n(m + 1) = n(m)

easily derive non-recursive definition focusing last case observing
n(6) = 9 = 32 n(m + 3) = 3 n(m) (for 6 multiple 3),
n(m) = 3m/3 . substitute definition obtain
following complete definition n(m), 6 multiple 3:
n(1) = 1
n(2) = 2
n(3) = 3
n(4) = 4
n(5) = 6
n(m) = 3m/3
4
n(m + 1) =
3m/3
3
n(m + 2) = 2 3m/3

127

fiWinikoff & Cranefield

References
Benfield, S. S., Hendrickson, J., & Galanti, D. (2006). Making strong business case
multiagent technology. Stone, P., & Weiss, G. (Eds.), Proceedings Fifth International Joint Conference Autonomous Agents Multiagent Systems (AAMAS),
pp. 1015. ACM Press.
Bordini, R. H., Fisher, M., Pardavila, C., & Wooldridge, M. (2003). Model checking AgentSpeak. Proceedings Second International Joint Conference Autonomous
Agents Multiagent Systems (AAMAS), pp. 409416. ACM Press.
Bordini, R. H., Hubner, J. F., & Wooldridge, M. (2007). Programming multi-agent systems
AgentSpeak using Jason. Wiley.
Bratman, M. E., Israel, D. J., & Pollack, M. E. (1988). Plans resource-bounded practical
reasoning. Computational Intelligence, 4, 349355.
Bratman, M. E. (1987). Intentions, Plans, Practical Reason. Harvard University Press,
Cambridge, MA.
Burch, J., Clarke, E., McMillan, K., Dill, D., & Hwang, J. (1992). Symbolic model checking:
1020 states beyond. Information Computation, 98 (2), 142170.
Burmeister, B., Arnold, M., Copaciu, F., & Rimassa, G. (2008). BDI-agents agile goaloriented business processes. Proceedings Seventh International Conference
Autonomous Agents Multiagent Systems (AAMAS) [Industry Track], pp. 3744.
IFAAMAS.
Busetta, P., Ronnquist, R., Hodgson, A., & Lucas, A. (1999). JACK Intelligent Agents Components Intelligent Agents Java. AgentLink News (2).
Dastani, M. (2008). 2APL: practical agent programming language. Autonomous Agents
Multi-Agent Systems, 16 (3), 214248.
Dastani, M., Hindriks, K. V., & Meyer, J.-J. C. (Eds.). (2010). Specification Verification
Multi-agent systems. Springer, Berlin/Heidelberg.
de Silva, L., & Padgham, L. (2004). comparison BDI based real-time reasoning
HTN based planning. Webb, G., & Yu, X. (Eds.), AI 2004: Advances Artificial
Intelligence, Vol. 3339 Lecture Notes Computer Science, pp. 11671173. Springer,
Berlin/Heidelberg.
Dennis, L. A., Fisher, M., Webster, M. P., & Bordini, R. H. (2012). Model checking agent
programming languages. Automated Software Engineering, 19 (1), 363.
dInverno, M., Kinny, D., Luck, M., & Wooldridge, M. (1998). formal specification
dMARS. Singh, M., Rao, A., & Wooldridge, M. (Eds.), Intelligent Agents IV:
Proceedings Fourth International Workshop Agent Theories, Architectures,
Languages, Vol. 1365 Lecture Notes Artificial Intelligence, pp. 155176,
Berlin/Heidelberg. Springer.
Dorigo, M., & Stutzle, T. (2004). Ant Colony Optimization. MIT Press.
Dwyer, M. B., Hatcliff, J., Pasareanu, C., Robby, & Visser, W. (2007). Formal software analysis: Emerging trends software model checking. Future Software Engineering
2007, pp. 120136, Los Alamitos, CA. IEEE Computer Society.
128

fiOn Testability BDI Agent Systems

Ekinci, E. E., Tiryaki, A. M., Cetin, O., & Dikenelli, O. (2009). Goal-oriented agent testing
revisited. Luck, M., & Gomez-Sanz, J. J. (Eds.), Agent-Oriented Software Engineering IX, Vol. 5386 Lecture Notes Computer Science, pp. 173186, Berlin/Heidelberg. Springer.
Erol, K., Hendler, J., & Nau, D. (1996). Complexity results HTN planning. Annals
Mathematics Artificial Intelligence, 18 (1), 6993.
Erol, K., Hendler, J. A., & Nau, D. S. (1994). HTN planning: Complexity expressivity.
Proceedings 12th National Conference Artificial Intelligence (AAAI), pp.
11231128. AAAI Press.
Fix, L., Grumberg, O., Heyman, A., Heyman, T., & Schuster, A. (2005). Verifying
large industrial circuits using 100 processes beyond. Peled, D., & Tsay, Y.K. (Eds.), Automated Technology Verification Analysis, Vol. 3707 Lecture
Notes Computer Science, pp. 1125, Berlin/Heidelberg. Springer.
Georgeff, M. P., & Lansky, A. L. (1986). Procedural knowledge. Proceedings IEEE,
Special Issue Knowledge Representation, 74 (10), 13831398.
Gomez-Sanz, J. J., Bota, J., Serrano, E., & Pavon, J. (2009). Testing debugging
MAS interactions INGENIAS. Luck, M., & Gomez-Sanz, J. J. (Eds.), AgentOriented Software Engineering IX, Vol. 5386 Lecture Notes Computer Science,
pp. 199212, Berlin/Heidelberg. Springer.
Huber, M. J. (1999). JAM: BDI-theoretic mobile agent architecture. Proceedings
Third International Conference Autonomous Agents (Agents99), pp. 236243.
ACM Press.
Ingrand, F. F., Georgeff, M. P., & Rao, A. S. (1992). architecture real-time reasoning
system control. IEEE Expert, 7 (6), 3344.
Jorgensen, P. (2002). Software Testing: Craftsmans Approach (Second edition). CRC
Press.
Lee, J., Huber, M. J., Kenny, P. G., & Durfee, E. H. (1994). UM-PRS: implementation procedural reasoning system multirobot applications. Proceedings Conference Intelligent Robotics Field, Factory, Service, Space
(CIRFFSS94), pp. 842849. American Institute Aeronautics Astronautics.
Mathur, A. P. (2008). Foundations Software Testing. Pearson.
Miller, J. C., & Maloney, C. J. (1963). Systematic mistake analysis digital computer
programs. Communications ACM, 6 (2), 5863.
Morley, D., & Myers, K. (2004). SPARK agent framework. Proceedings
Third International Joint Conference Autonomous Agents Multiagent Systems
(AAMAS), pp. 714721, New York. ACM.
Munroe, S., Miller, T., Belecheanu, R., Pechoucek, M., McBurney, P., & Luck, M. (2006).
Crossing agent technology chasm: Experiences challenges commercial applications agents. Knowledge Engineering Review, 21 (4), 345392.
129

fiWinikoff & Cranefield

Naish, L. (2007). Resource-oriented deadlock analysis. Dahl, V., & Niemela, I. (Eds.),
Proceedings 23rd International Conference Logic Programming, Vol. 4670
Lecture Notes Computer Science, pp. 302316. Springer, Berlin/Heidelberg.
Nguyen, C., Miles, S., Perini, A., Tonella, P., Harman, M., & Luck, M. (2009a). Evolutionary testing autonomous software agents. Proceedings 8th International
Conference Autonomous Agents Multiagent Systems (AAMAS), pp. 521528.
IFAAMAS.
Nguyen, C. D., Perini, A., & Tonella, P. (2009b). Experimental evaluation ontology-based
test generation multi-agent systems. Luck, M., & Gomez-Sanz, J. J. (Eds.),
Agent-Oriented Software Engineering IX, Vol. 5386 Lecture Notes Computer
Science, pp. 187198, Berlin/Heidelberg. Springer.
Nguyen, C. D., Perini, A., & Tonella, P. (2007). Automated continuous testing multiagent systems. Proceedings Fifth European Workshop Multi-Agent Systems
(EUMAS).
Padgham, L., & Winikoff, M. (2004). Developing Intelligent Agent Systems: Practical
Guide. John Wiley Sons.
Paolucci, M., Shehory, O., Sycara, K. P., Kalp, D., & Pannu, A. (2000). planning component RETSINA agents. Jennings, N. R., & Lesperance, Y. (Eds.), Proceedings
6th International Workshop Agent Theories, Architectures, Languages
(ATAL), Vol. 1757 Lecture Notes Computer Science, pp. 147161, Berlin/Heidelberg. Springer.
Pokahr, A., Braubach, L., & Lamersdorf, W. (2005). Jadex: BDI reasoning engine.
Bordini, R. H., Dastani, M., Dix, J., & El Fallah Seghrouchni, A. (Eds.), Multi-Agent
Programming: Languages, Platforms Applications, chap. 6, pp. 149174. Springer.
Raimondi, F., & Lomuscio, A. (2007). Automatic verification multi-agent systems
model checking via ordered binary decision diagrams. J. Applied Logic, 5 (2), 235
251.
Rao, A. S. (1996). AgentSpeak(L): BDI agents speak logical computable language.
de Velde, W. V., & Perrame, J. (Eds.), Agents Breaking Away: Proceedings
Seventh European Workshop Modelling Autonomous Agents Multi-Agent
World (MAAMAW96), Vol. 1038 Lecture Notes Artificial Intelligence, pp. 4255,
Berlin/Heidelberg. Springer.
Rao, A. S., & Georgeff, M. P. (1991). Modeling rational agents within BDI-architecture.
Allen, J., Fikes, R., & Sandewall, E. (Eds.), Proceedings Second International
Conference Principles Knowledge Representation Reasoning, pp. 473484.
Morgan Kaufmann.
Sardina, S., & Padgham, L. (2011). BDI agent programming language failure handling, declarative goals, planning. Autonomous Agents Multi-Agent Systems,
23 (1), 1870.
Shaw, P., Farwer, B., & Bordini, R. (2008). Theoretical experimental results
goal-plan tree problem. Proceedings Seventh International Conference
Autonomous Agents Multiagent Systems (AAMAS), pp. 13791382. IFAAMAS.
130

fiOn Testability BDI Agent Systems

Sloane, N. J. A. (2007). on-line encyclopedia integer sequences. http://www.research.
att.com/njas/sequences/.
Thangarajah, J., Winikoff, M., Padgham, L., & Fischer, K. (2002). Avoiding resource
conflicts intelligent agents. van Harmelen, F. (Ed.), Proceedings 15th
European Conference Artificial Intelligence (ECAI), pp. 1822. IOS Press.
van Riemsdijk, M. B., Dastani, M., & Winikoff, M. (2008). Goals agent systems:
unifying framework. Proceedings Seventh Conference Autonomous Agents
Multiagent Systems (AAMAS), pp. 713720. IFAAMAS.
Wilf, H. S. (1994). generatingfunctionology (Second edition). Academic Press Inc., Boston,
MA. http://www.math.upenn.edu/wilf/gfology2.pdf.
Winikoff, M. (2010). Assurance Agent Systems: Role Formal Verification
play?. Dastani, M., Hindriks, K. V., & Meyer, J.-J. C. (Eds.), Specification
Verification Multi-agent systems, chap. 12, pp. 353383. Springer, Berlin/Heidelberg.
Winikoff, M., Padgham, L., Harland, J., & Thangarajah, J. (2002). Declarative & procedural goals intelligent agent systems. Proceedings Eighth International
Conference Principles Knowledge Representation Reasoning (KR2002), pp.
470481, Toulouse, France. Morgan Kaufmann.
Wooldridge, M. (2002). Introduction MultiAgent Systems. John Wiley & Sons,
Chichester, England.
Wooldridge, M., Fisher, M., Huget, M.-P., & Parsons, S. (2002). Model checking multi-agent
systems MABLE. Proceedings First International Joint Conference
Autonomous Agents Multi-Agent Systems (AAMAS), pp. 952959. ACM Press.
Zhang, Z., Thangarajah, J., & Padgham, L. (2009). Model based testing agent systems.
Filipe, J., Shishkov, B., Helfert, M., & Maciaszek, L. (Eds.), Software Data
Technologies, Vol. 22 Communications Computer Information Science, pp.
399413, Berlin/Heidelberg. Springer.
Zhu, H., Hall, P. A. V., & May, J. H. R. (1997). Software unit test coverage adequacy.
ACM Computing Surveys, 29 (4), 366427.

131


