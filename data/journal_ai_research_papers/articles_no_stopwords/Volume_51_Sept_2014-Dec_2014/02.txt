Journal Artificial Intelligence Research 51 (2014) 133164

Submitted 05/14; published 09/14

Text Rewriting Improves Semantic Role Labeling
Kristian Woodsend
Mirella Lapata

k.woodsend@ed.ac.uk
mlap@inf.ed.ac.uk

Institute Language, Cognition Computation
School Informatics, University Edinburgh
10 Crichton Street, Edinburgh EH8 9AB

Abstract
Large-scale annotated corpora prerequisite developing high-performance NLP
systems. corpora expensive produce, limited size, often demanding linguistic
expertise. paper use text rewriting means increasing amount labeled
data available model training. method uses automatically extracted rewrite rules
comparable corpora bitexts generate multiple versions sentences annotated
gold standard labels. apply idea semantic role labeling show
model trained rewritten data outperforms state art CoNLL-2009
benchmark dataset.

1. Introduction
Recent years witnessed increased interest automatic identification labeling
semantic roles conveyed sentential constituents (Gildea & Jurafsky, 2002).
goal semantic role labeling task discover relations hold
predicate arguments given input sentence (e.g., whom,
when, where, how).
(1)

[Mrs. Yeargin]A0 [gave]V [the questions answers]A1 [two days
examination]TMP [two low-ability geography classes]ARG2 .

sentence (1), A0 represents Agent giver, A1 represents theme thing given,
A2 represents Recipient, TMP temporal modifier indicating action took
place, V determines boundaries predicate. semantic roles example
labeled style PropBank (Palmer, Gildea, & Kingsbury, 2005), broad-coverage
human-annotated corpus semantic roles syntactic realizations. PropBank annotation framework predicate associated set core roles (named A0,
A1, A2, on) whose interpretations specific predicate1 set adjunct
roles location time whose interpretation common across predicates (e.g., two
days examination sentence (1) above).
type semantic information shallow relatively straightforward infer automatically useful development broad coverage, domain-independent language
understanding systems. Indeed, analysis produced existing semantic role labelers
shown benefit wide spectrum applications ranging information extraction
(Surdeanu, Harabagiu, Williams, & Aarseth, 2003) question answering (Shen & Lapata,
1. precisely, A0 A1 common interpretation across predicates proto-agent protopatient sense described Dowty (1991).
c
2014
AI Access Foundation. rights reserved.

fiWoodsend & Lapata

Source
1. retreating guerrillas soon pursued government forces.
2. survey conducted Gallup Poll
last summer indicated one four
Americans takes cues stars believes ghosts.
3. examiner kind let student finish lunch.
4. didnt know rules,
died.
5. Mexico City, biggest city world,
many interesting archaeological sites.
6. arrival train unexpected.

Target
Government forces soon pursued retreating guerrillas.
survey conducted Gallup
Poll last summer. indicated one
four Americans takes cues stars
believes ghosts.
kind examiner let student finish
lunch.
died, didnt know
rules.
Mexico City many interesting archaeological sites.
trains arrival unexpected.

Table 1: Examples syntactic rewriting.

2007), machine translation (Wu & Fung, 2009) summarization (Melli, Wang, Liu,
Kashani, Shi, Gu, Sarkar, & Popowich, 2005).
SRL systems date conceptualize semantic role labeling task supervised
learning problem rely role-annotated data model training. Supervised methods
deliver reasonably good performance, F1-scores low eighties standard test
collections English. rely primarily syntactic features (such path features)
order identify classify roles. mixed blessing path
argument predicate informative quite complicated. Many paths
parse tree likely occur relatively small number times (or all)
resulting sparse information classifier learn from. Even training
data includes examples specific predicate set arguments, unless test sentence
contains syntactic structure, far classifier concerned,
labeling items within two sentences unrelated.
idea use use rewrite rules order create several syntactic variants
sentence, thus alleviating training requirements semantic role labeling. Rewrite
rules typically synchronous grammar rules defining sequence source terminals
nonterminals rewrites sequence target terminals nonterminals. rules
often extracted monolingual corpora containing parallel translations
source text (Barzilay & McKeown, 2001; Pang, Knight, & Marcu, 2003), bilingual
corpora consisting documents translations (Bannard & Callison-Burch, 2005a;
Callison-Burch, 2007), comparable corpora Wikipedia revision histories (Coster
& Kauchak, 2011; Woodsend & Lapata, 2011). Examples rewrites given Table 1.
include transforming passive active sentences (see sentence pair (1) Table 1),
splitting long complicated sentence several shorter ones (see (2) Table 1),
removing redundant parts sentence (see (3) Table 1), reordering parts sentence
(see (4) Table 1), deleting appositives (see (5) Table 1), transforming prepositional
phrase genitive (see (6) Table 1), on.
134

fiText Rewriting Improves Semantic Role Labeling

automatically extract syntactic rewrite rules corpora use generate
multiple versions role annotated sentences whilst preserving original semantic roles.
therefore expand training data wide range syntactic variations
predicate-argument combination learn semantic role labeler expanded
dataset. approach describe essentially increases size training data
creating many different syntactic variations different predicates roles.
Rewrite rules previously deployed variety text-to-text generation applications ranging summarisation (Galley & McKeown, 2007; Yamangil & Nelken, 2008;
Cohn & Lapata, 2009; Ganitkevitch, Callison-Burch, Napoles, & Van Durme, 2011),
question answering (Wang, Smith, & Mitamura, 2007), information retrieval (Park, Croft,
& Smith, 2011), simplification (Zhu, Bernhard, & Gurevych, 2010; Woodsend & Lapata,
2011; Feblowitz & Kauchak, 2013), machine translation (Callison-Burch, 2008; Marton, Callison-Burch, & Resnik, 2009; Ganitkevitch, Cao, Weese, Post, & Callison-Burch,
2012). However, application text rewriting means increasing amount
labeled data available model training novel knowledge. show experimentally, syntactic transformations improve SRL performance beyond state art
using CoNLL-2009 benchmark dataset best scoring system (Bjorkelund,
Hafdell, & Nugues, 2009). Importantly, approach used combination
SRL learner role-annotated data. Moreover, specifically tied SRL task
employed learning model dataset. Rewrite rules could used expand
training data tasks make use syntactic features semantic parsing
(Kwiatkowski, 2012) textual entailment (Mehdad, Negri, & Federico, 2010; Wang &
Manning, 2010).
following, present overview related work (Section 2) describe
rewrite rules automatically extracted filtered correctness (Section 3). Section 4 details experiments Section 5 reports results.

2. Related Work
idea transforming sentences make amenable NLP technology dates
back Chandrasekar, Doran, Srinivas (1996) argue simpler sentences would
decrease likelihood incorrect parse. end, employ mostly hand-crafted
syntactic rules aimed splitting long complicated sentences simpler ones. Klebanov, Knight, Marcu (2004) preprocess texts Easy Access Sentences, i.e., sentences
consisting one finite verb dependents order facilitate information seeking applications summarization information retrieval accessing factual information.
similar vein, Vickrey Koller (2008) devise large number hand-written rules
order simplify sentences semantic role labeling. present log-linear model
jointly learns select best simplification (out possibly exponential space
candidates) role labeling. Kundu Roth (2011) use textual transformations
domain adaptation. Rather training new model out-of-domain data, propose
rewrite out-of-domain text similar training domain.
pilot idea semantic role labeling using hand-written rewrite rules show
compares favorably approaches retrain model target domain.
135

fiWoodsend & Lapata

work focused idea automatically expanding data available
given task without, however, applying transformations. Furstenau Lapata (2012)
combine labeled unlabeled data projecting semantic role annotations labeled
source sentence onto unlabeled target sentence. find novel instances classifier
training based lexical structural similarity manually labeled seed instances.
Zanzotto Pennacchiotti (2010) increase datasets textual entailment mining
Wikipedia revision histories.
Contrary previous work, automatically extract general rewrite rules various data sources including Wikipedia revision histories, comparable articles, bilingual
corpora. Given sentence (semantic role) annotated data, create several syntactic transformations, many may erroneous. maintain model
training transformations whose role labels preserved syntactic rewrite.
identify transformations label-preserving automatically, without requiring
SRL-specific knowledge. approach differs Vickrey Koller (2008)
three important aspects: (a) employ automatic rules simplification specific, (b) attempt select best rewrite, transformations preserve
gold standard role labels used training (c) model jointly
rewrites sentences labels semantic roles; rewrite training data
available model SRL task. work shares Kundu
Roth (2011) idea transforming sentences preserving gold standard
role labels. However, transform test data make look training
data. unavoidably requires specialized knowledge differences two
domains, general model have.
mentioned earlier, use synchronous grammars extract set possible syntactic rewrites. Synchronous context-free grammars (SCFGs; Aho & Ullman, 1969)
generalization context-free grammar (CFG) formalism simultaneously produce
strings two languages. used extensively syntax-based statistical MT
(Wu, 1997; Yamada & Knight, 2001; Chiang, 2007; Graehl & Knight, 2004) related
generation tasks sentence compression (Galley & McKeown, 2007; Cohn & Lapata,
2009, 2013; Ganitkevitch et al., 2011), sentence simplification (Zhu et al., 2010; Feblowitz
& Kauchak, 2013; Woodsend & Lapata, 2011), summarization (Woodsend & Lapata,
2012). Rather focusing one type transformation (e.g., simplification compression), learn full spectrum rewrite operations select rules appropriate
task hand. Furthermore, results show rewrite rules improve semantic role
labeling performance across board, irrespectively specific variant synchronous
grammar corpus used. experiment conventional (weighted) SCFGs (Aho &
Ullman, 1969) tree substitution grammars (Eisner, 2003) employ transformation
rules extracted Wikipedia revision histories (Zhu et al., 2010; Woodsend & Lapata,
2011) bitexts (Ganitkevitch, Van Durme, & Callison-Burch, 2013).

3. Method
section describe general idea behind algorithm move
present specific implementation. define transformation g function
maps example sentence modified sentence s0 . Let G set known
136

fiText Rewriting Improves Semantic Role Labeling

transformation functions, G = {g1 , g2 , . . . , gn }. Suppose labels associated
example s. context paper, semantic role labels. Labels could
defined spans tokens, use CoNLL 20089 formalism
head word span labelled. transformation function therefore mapping
tokens sentence tokens t0 s0 . require mapping
involves tokens s0 , require mappings one-to-one.
label-preserving transformation transformation gi mapping (some the)
tokens example tokens t0 s0 , (correct) labels t0 identical
labels source tokens token mappings defined gi . words,
labels could preserved, preserved, others introduced.
Let G set label-preserving transformation functions, G G. problem
address paper therefore two-fold: Firstly, find automatically set
possible transformation functions G due automated nature unavoidably
error-prone process. Secondly, identify (again automatically) transformations G
actually label-preserving specifically, transformations rewrite
training instance s0 varying syntactic structure, yet preserve
semantic roles arguments appear new version s0 .
Algorithm 1 describes approach boils three steps: (a) extracting
transformations, (b) refining transformations, (c) generating labeling extended
corpus. standard gold annotated corpus used train initial semantic role labeling
model (see lines 12 Algorithm 1). Meanwhile, set candidate transformation functions G extracted suitable comparable parallel corpus (line 3). full
set transformation functions used rewrite gold corpus, creating much extended
corpus inevitably contain grammatically semantically incorrect sentences.
extended corpus next automatically labeled using original SRL model preprocessing normal SRL pipeline (whose details discuss Section 4.2), without
knowledge transformation functions involved.
could theory use extended corpus basis training SRL
model. However, contain many errors, unlikely yield useful information
guide model. One approach could manually correct rewrites
generated automatically, would time resource-intensive. Instead,
corrections automatically, create extended corpus rewrites
impair quality training data. therefore learn rules yield accurate
rewrites, i.e., rewrites preserve labels gold-standard. intuition
that, given large number possible rewrites, SRL model general label
accurate rewrites correctly mis-label erroneous sentences, due finding
confusing. thus compare semantic role labels produced model
labels corresponding predicate-argument pairs gold corpus, provide
samples train binary classifier (here SVM) learns predict rewrites
likely successful problematic (lines 1119 Algorithm 1).
rewritten sentence classed positive sample SRL model able label
transformation standard better able label original
sentence, i.e. labels SRL model predicts transformed sentence match
predicted original, corrected respect mapped
gold labels. If, however, semantic role longer predicted correctly, missed,
137

fiWoodsend & Lapata

Algorithm 1 Learn SRL model Mextended extending gold training corpus Cgold
transformation functions G.
1: Mgold SRL model trained Cgold
2: Cmodel label Cgold using Mgold
Extract transformations:
3: G transformation functions extracted pairs aligned sentences
comparable corpora
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:

19:
20:
21:
22:
23:
24:
25:
26:
27:

Refine transformations:
initialize SVM training data DSVM
sentences Cgold
g applicable transformations G
s0 rewrite using g
label s0 using Mgold
SRL labels s0 match labels Cgold equivalent Cmodel
true
else
false
end
add (s0 , y) DSVM
end
end
train SVM using DSVM
G {g G : g positive SVM weight}
Generate extended corpus:
initialize refined rewrite corpus Crefined
sentences Cgold
g applicable transformations G
s0 rewrite using g
project labels s0 using g
add s0 Crefined
end
end
Cextended Cgold Crefined

Retrain SRL model:
Mextended SRL model trained Cextended
29: return Mextended

28:

138

fiText Rewriting Improves Semantic Role Labeling

erroneous role introduced, classified negative sample, sample likely
harm training new SRL model.
SVM identified refined set transformation functions G (line 20),
transformations used create extended training corpus. time, knowledge
transformation function involved project labels correspond original
gold corpus (lines 2128). case SRL, labels describe predicate
arguments. extended corpus supplements original gold standard corpus (line 29),
combination used create SRL model (line 30).
worth noting method impinge actual process learning
SRL model, concerned preparation training data. therefore believe
applied range SRL modeling approaches, gains performance
achieve largely orthogonal could made improving aspects
learning process (see Section 5.3 empirical evidence).
3.1 Learning Transformations
Conceptually wide range text-rewriting transformation functions could included
set G, paraphrasing, simplification translation another language. Here,
focus transformation functions expressed synchronous context-free
grammars (Aho & Ullman, 1969). Synchronous rules operate parse tree constituents
context-free manner, typically modify syntax. transformations consider
sub-categorized into:
1. Statement extraction. Constituents sub-tree parse tree identified, extracted context rewritten complete sentence, typically shorter
simpler, although necessarily so.
2. Compression. original sentence rewritten compressing constituents
parse tree, typically deleting nodes.
3. Insertion. New elements added parse tree. significant chunks new
text would semantic role information own, practice insertions
often additional punctuation clarify scope phrases, simple structure
. . . . aid statement extraction.
4. Substitution. using lexicalized synchronous grammar, text replaced
new text, paraphrases represented.
obtain set possible transformations G monolingual comparable corpora
drawn Wikipedia bitexts (see Section 4 details). following describe
grammar formalisms resources consider.
3.1.1 Transformations Monolingual Corpora
extract transformation rules corpora broadly comparable, using
unsupervised process. corpora constructed Wikipedia revision histories,
comparable Wikipedia articles. result, cannot guaranteed aligned source
target sentences truly related, expected source sentence
139

fiWoodsend & Lapata

fully generate target sentence. practice means addition requiring
strictly synchronous structure source target sentences, cannot assume
alignment source target root nodes, require surjective alignment
target nodes nodes source parse tree. able describe structural mismatches
non-isomorphic tree pairs (the grammar rules comprise trees arbitrary depth,
fragments mapped) represent transformation functions using synchronous
tree substitution grammar formalism (Eisner, 2003).
synchronous tree-substitution grammar (STSG) defines space valid pairs
source target parse trees. Rules specify map tree fragments source parse
tree fragments target tree, recursively free context. Following Cohn
Lapata (2009), STSG 7-tuple, G = (NS , NT , , , P, RS , RT ) N
non-terminals terminals, subscripts indicating source
target respectively. P productions RS NS RT NT distinguished
root symbols.
Typically, production rewrite rule two aligned non-terminals X NS
NT source target:
hX, h, , i,
elementary trees rooted symbols X respectively.
synchronous context free grammar would limited one level elementary
trees, STSG imposes limits elementary trees arbitrarily deep.
one-to-one alignment frontier nodes (non-terminal leaves elementary
trees) specified .
experiments, investigate two STSG variants, strictly synchronous tree
substitution grammar T3 (Cohn & Lapata, 2009), originally developed
task text compression, support full range transformation operations
quasi-synchronous tree substitution grammar (QTSG) Woodsend Lapata (2011),
used text simplification summarization (Woodsend & Lapata, 2012).
T3 tokens first aligned using probabilistic aligner initially provided
identity mappings entire vocabulary. experiments used Berkeley
aligner (Liang, Taskar, & Klein, 2006), however aligner broadly similar output
could used instead. Synchronous rules comprising trees arbitrary depth
extracted pair input CFG parse trees, consistent alignment. Across
complete corpus aligned trees, T3 filters extracted rules provide maximally
general rule set, consisting rules smallest depth, still capable
synchronously deriving original aligned tree pairs. removing identity rules,
resulting grammar forms transformation functions G.
Unlike T3, QTSG works partial alignment tokens, based identity.
Non-terminal nodes parse trees aligned consistent token
alignment. result sections source target parse trees
remain unaligned. Then, T3, synchronous rules comprising trees minimum necessary
depth extracted pair input trees, consistent alignment,
before, identity rules removed form G.
example, Figure 1 shows two comparable parse trees aligned token level.
synchronous rules extracted alignment T3 QTSG shown Table 2.
140

fiText Rewriting Improves Semantic Role Labeling

ROOT


NNP

.

VP

NP
NNS

VP

VBP



VBN

VP
VP



NP

VB

PP

NP

NP



NN

DT

ADVP

Modern

scholars

come





scholars



question

question





existence

existence

NNS
NP



least

JJ

CD

NNS



first

nine

emperors



first

nine

emperors

DT

JJ

CD

NNS

.

.

NP
PP

NP
NNP

JJS



NN

DT







DT

NP

VBP
VP

.

ROOT

Figure 1: Example sentence alignment showing source (above) target (below) trees.

possible extract rules nodes child level, rules T3
QTSG extract parent level identical. cases sub-tree
compressed, (in example, come question compressed question), QTSG
extracts full sub-tree frontier nodes align, T3 extract several rules
smallest depth.
3.1.2 Transformations Bitexts
obtain transformation rules ParaPhrase DataBase (PPDB, Ganitkevitch
et al., 2013), collection English (and Spanish) paraphrases derived large bilingual
parallel corpora. variety paraphrases (lexical, phrasal, syntactic) obtained
following Bannard Callison-Burchs (2005b) bilingual pivoting method.
141

fiWoodsend & Lapata

Rules extracted T3
], [S NP
VP
. ]i

hS, Si



h[S NP

hNP, NPi



h[NP [NNP Modern] NNS

hVP, VPi



h[VP VBP

hVP, VPi



h[VP VBN

hVP, VPi



h[VP

hVP, VPi



h[VP [VB question] NP

hNP, NPi



h[NP NP

hNP, NPi



h[NP DT

hPP, PPi



h[PP

hNP, NPi



h[NP ADVP

hS, Si



h[S NP

hNP, NPi



h[NP NNP

hVP, VPi



h[VP VBP

hNP, NPi



h[NP NP

hNP, NPi



h[NP DT

hPP, PPi



h[PP

hNP, NPi



h[NP ADVP

VP

1

2

VP



VP

PP

1

NP



1

.

JJ

NP


2

2

3

]i

1

]i

PP

1

NP
CD

2

2

NN

1

1

]i

]i

2

]i

2

NNS

3

1

]i

4

], [NP DT

1

1

1

JJ

2

CD

3

NNS

4

]i

3

[S [VP



], [PP
1

2

], [NP [DT Some] NNS

JJ

2

1

PP

1

], [NP DT

DT

]i

1

1

]i

1

], [VP [VBP question] NP

], [NP NP

2

3

Rules extracted QTSG
], [S NP
VP
. ]i

[VP VBN

NN

1

1

], [NP DT

1

2

]], [VP VP

], [PP

2

PP

1

2

2

1

], [NP NP

2

NNS



1

], [VP VP

DT



VP

1

1

], [NP [DT Some] NNS

], [VP VP

1

1

NN

1

3

[S VP





1

.

2

NN

1

NP
CD

3

2



1

]i

[VP VB

1

NP

2

]]]]], [VP VBP

1

NP

2

]i

]i

2

]i

]i
NNS

4

], [NP DT

1

JJ

2

CD

3

NNS

4

]i

Table 2: Synchronous tree grammar rules extracted T3 QTSG aligned
sentences Figure 1. Boxed indices short-hand notation alignment, .

intuition two English strings e1 e2 translate foreign
string f assumed meaning. method pivots f
extract he1 , e2 pair paraphrases. example shown Figure 2 (taken
Ganitkevitch et al., 2013). method extracts wide range possible paraphrases
unavoidably noisy due inaccurate word alignments. Paraphrases ranked
computing p(e1 |e2 ) shown below:
X
p(e2 |e1 )
p(e2 |f )p(f |e1 )
(2)
f

p(ei |f ) p(f |ei ) translation probabilities estimated bitext (Koehn,
Och, & Marcu, 2003).
Rewrite rules PPDB obtained using generalization method sketched
extract syntactic paraphrases (Ganitkevitch et al., 2011). Using techniques
syntactic machine translation (Koehn, 2010), SCFG rules first extracted Englishforeign sentence pairs. foreign phrase corresponding English phrase found via
142

fiText Rewriting Improves Semantic Role Labeling

... 5 farmers thrown jail

Ireland ...

... fnf Landwirte

festgenommen

, weil ...

... oder wurden

festgenommen

, gefoltert ...

...

imprisoned

, tortured ...

Figure 2: Paraphrases extracted via bilingual pivoting.
word alignments. phrase pair turned SCFG rule assigning left-hand side
nonterminal symbol, corresponding syntactic constituent dominates English
phrase. introduce nonterminals right-hand sides rule, corresponding subphrases English foreign phrases replaced nonterminal symbols.
sentence pairs bilingual parallel corpus results translation grammar
serves basis syntactic machine translation. translation grammar
converted paraphrase grammar follows. Let r1 r2 denote translation rules
left-hand side nonterminals hX, foreign language strings match:
r1 = hX, h1 , ,

(3)

r2 = hX, h2 , ,
paraphrase rule rp created pivoting f :
rp = hX, h1 , 2 ,

(4)

Although shown equations (3) (4), rules SCFG associated
set features combined log-linear model estimate derivation
probabilities.
3.1.3 Manual Transformations
experiments primarily make use automatically learned transformations since
adapted different tasks, domains languages. However, proposed approach necessary transformation functions acquired automatically
functions could crafted hand. thus investigated effectiveness
rewrites generated system Heilman Smith (2010) (henceforth H&S),
uses sophisticated hand-crafted rule-based algorithm extract simplified declarative sentences English syntactically complex ones. rules similar engineered Vickrey Koller (2008) deterministic generate
unique rewrite given sentence. algorithm operates standard phrase
143

fiWoodsend & Lapata

structure tree input sentence. extracts new sentence trees input tree
following: non-restrictive appositives relative clauses; subordinate clauses
subject finite verb; participial phrases modify noun phrases, verb phrases,
clauses. addition, algorithm splits conjoined S, SBAR, VP nodes, extracts new
sentence trees conjunct. output tree processed move leading
prepositional phrases quotations last children main verb phrase,
following removed: noun modifiers offset commas (non-restrictive appositives,
non-restrictive relative clauses, parenthetical phrases, participial phrases), verb modifiers
offset commas (subordinate clauses, participial phrases, prepositional phrases), leading
modifiers main clause (nodes precede subject).
Table 3 shows examples rules extracted using T3, QTSG PPDB grammar
formalisms applied sentence CoNLL dataset. final column Table 3 indicates whether transformation could classed statement extraction, compression,
insertion, substitution. reflected table, T3 captures compression transformations deleting nodes parse tree; QTSG rules range mainly syntactic
transformations; PPDB transformations substitutions words short phrases.
3.2 Refining Transformations
mentioned earlier, transformation rules obtained synchronous grammars
could used rewrite gold standard sentences. Unfortunately, due nature
corpora rules obtained automatic extraction process, many
rules contain errors impair rather improve quality
training data. idea extrapolate rules trust observing SRL
labeler handles rewritten sentences. mis-labeled them, possible
rewrite correct original labels preserved.
rewritten sentence classed positive sample SRL model predicts
labels transformed sentence predicted original, labels
corrected respect gold labels. If, however, semantic role
longer predicted correctly, missed, erroneous role introduced, classified
negative sample, sample likely harm training new SRL model.
capture full impact candidate transformation function, sentence provided
positive sample classifier labels (i.e., predicates arguments)
source sentence successfully projected onto rewrite. Table 4 shows
examples positive negative samples T3, QTSG, PPDB rewrites. Note
refining used H&S outputs.
decide transformation function include refined set, used linear
kernel SVM (Vapnik, 1995) binary classifier, classifiers indeed suitable
statistical tests contingency could used. input SVM learner set
l training samples (x1 , y1 ), . . . , (xl , yl ), xi Rn , {+1, 1}. xi n dimensional
feature vector representing ith sample, yi label sample. learning
process involves solving convex optimization problem find large-margin separation
hyperplane positive negative samples. order cope inseparable data,
misclassification allowed, amount determined parameter C,
thought penalty misclassified training sample. one
144

fiText Rewriting Improves Semantic Role Labeling

Grammar
Original

T3

Examples
Bell, based Los Angeles, makes distributes electronic, computer
building products.
Bell, based, makes distributes electronic, computer building products.
hPP, PPi h[PP NP ], [PP ]i

Comp

Bell, based Los Angeles, makes distributes.
hNP, NPi h[NP ADJP NNS ], [NP ]i

Comp

Based Los Angeles, makes distributes electronic, computer building
products.
hNP, NPi h[NP NNP ,], [NP ]i

Comp

Bell, based Angeless, makes distributes electronic, computer building products.
hNP, NPi h[NP NNP NNP ], [NP NNP
[POS s]]i

Comp

Bell makes distributes electronic, computer building products.
hNP, NPi h[NP NP
, VP ,], [NP NP ]i

Comp

makes distributes electronic, computer building products.
hS, Si
h[S NP VP
. ], [S [NP It] VP
. ]i

Ins

1

QTSG

1

1

1

1

Bell based Los Angeles.
hNP, Si
h[NP NP
, VP
1

2

1

,], [S NP

2

1

2

[VP [VBD was] VP

2

] .]i

Ext

Bell, based Los, makes distributes electronic, computer building
products.
hNP, NPi h[NP NNP
NNP ], [NP NNP ]i

Comp

Los Angeles makes distributes electronic, computer building products.
hNP, NPi h[NP NP , [VP VBN [PP NP ]] ,], [NP NP ]i

Comp

Bell, founded Los Angeles, makes distributes electronic, computer
building products.
hVP, VPi h[VP [X based] PP ], [VP [X founded] PP ]i

Sub

Bell, building Los Angeles, makes distributes electronic, computer
building products.
hVP, VPi h[VP [X based]
NP ], [VP [X building]
NP ]i

Sub

Bell, based Los Angeles, makes distributes electronic, computer
building products.
hVP, VPi h[VP VBN
NP ], [VP VBN
[X during] NP
]i

Sub

1

1

1

PPDB

Type

1

1

1

1

1

2

2

1

1

2

2

Table 3: Examples transformation rules extracted using T3, QTSG PPDB grammar
formalisms, applied sentence marked Original. final column indicates
whether rule statement extraction (Ext), compression (Comp), insertion
(Ins) substitution (Sub). before, boxed indices short-hand notation
alignment, .

view (the dual problem), result set Support Vectors, associated weights ,
constant b. another view (the primal problem), result vector w
defines separation hyperplane, dimension depends particular kernel
145

fiWoodsend & Lapata

Original Bell, based Los Angeles, makes distributes electronic, computer building products.
T3

QTSG

Bell, based, makes distributes electronic, computer building products.
Bell, based Los Angeles, makes distributes.
Based Los Angeles, makes distributes electronic, computer building products.
Bell, based Angeless, makes distributes electronic, computer building products.

+
+
+


Bell makes distributes electronic, computer building products.
makes distributes electronic, computer building products.
Bell based Los Angeles.
Bell, based Los, makes distributes electronic, computer building products.
Bell, based Angeles, makes distributes electronic, computer building products.
Los Angeles makes distributes electronic, computer building products.

+
+
+

+


PPDB

Bell, founded Los Angeles, makes distributes electronic, computer building products.
Bell, building Los Angeles, makes distributes electronic, computer building products.
Bell, based Los Angeles, makes distributes electronic, computer building products.

H&S

Bell makes. Bell distributes. Bell based Los Angeles.

Original employees sign options, college must approve plan.
T3

it, college must approve plan.
college must approve plan.
employees sign options, must approve the.
employees sign for, college must approve plan.


+

+

QTSG

employees sign options, college must approve.
employees sign options, college must approve plan.
employees sign options, college must approve plan.


+


PPDB






+

+


H&S

college must approve plan employees sign options.






employees
employees
employees
employees






sign
sign
sign
sign
















options,
options,
options,
options,






college
college
college
college






must adopt plan.
must agree plan.
must endorse plan.
needs approve plan.

Original went permissible line warm fuzzy feelings.
T3






went permissible line feelings.
went warm fuzzy feelings.
went it.
went.

+


+

QTSG

went line warm fuzzy feelings.
went permissible line feelings.
went permissible warm fuzzy feelings.

+



PPDB

went permissible line hot fuzzy feelings.
went permissible line warm fuzzy feelings.


+

H&S

went permissible line warm fuzzy feelings.

Table 4: Examples rewrites generated T3, QTSG, PPDB source sentence
(Original) CoNLL-2009 training set. Symbols +/ indicate whether
sample classified positive (i.e., argument label preserving) forms part
extended training corpus, not.

146

fiText Rewriting Improves Semantic Role Labeling

function used SVM. case linear kernel function, wPis ndimensional,
feature vectors, straight-forward relationship w = lj=1 yj j xj
primal dual variables, effectively assigning weights explicitly specified features.
kernel functions allow interaction variables. instance using
binary valued features, degree2 polynomial kernel function implies classifier
considers available pairs features well.
used identity transformation functions involved features
sample, size feature space n = kGk, features binary-valued.
features could easily incorporated setting, perhaps capturing information
structure source sentence transformation function, might achieve good
results conjunction polynomial kernel, pursue avenue further.
Instead used linear kernel, due simple structure features, SVM
assigned weight transformation function independent source sentence.
chose transformation functions form refined set based whether
corresponding weight global threshold value, set threshold value
maximizing performance resulting SRL model development set.
3.3 Labeling Extended Corpus
SVM identified refined set transformation functions G, transformations used create extended training corpus. Using alignment information
transformation functions trace position tokens original sentence
rewrite, semantic role labels gold corpus projected onto corresponding predicate-argument pairs rewritten corpus. Assuming SVM correctly
identified transformation function involved indeed label-preserving,
transformation functions applied current context, semantic role labeling
rewrite quality standard source. conditions
however unlikely true, resulting degradation quality rewrite corpus.
corpus rewrites appended original gold standard corpus create new
larger training corpus, used create SRL model.

4. Experimental Setup
section present experimental setup assessing performance approach. give details corpora grammars used create transformations,
model parameters used identify preserve labels. explain existing SRL system modified approach, evaluated effects
increasing training data transformations.
4.1 Grammar Extraction
extracted synchronous grammars two monolingual comparable corpora drawn
Wikipedia. corpus 137,362 aligned sentences created pairing Simple English
Wikipedia English Wikipedia (Kauchak, 2013). corpus 14,831 paired sentences comparing consecutive revisions articles Simple English Wikipedia (Woodsend & Lapata, 2011). corpora provide large repository monolingual, comparable
147

fiWoodsend & Lapata

Grammar
T3
QTSG

Aligned
13,562
3,875

Revisions
5,386
669

Table 5: Non-identical rules extracted Wikipedia corpus, rules appearing
one two times removed.

sentences, taken real-world writing. Advantageously, Simple English Wikipedia encourages contributors employ simpler grammar ordinary English Wikipedia;
corpora therefore naturally contain many examples syntactic variation reordering sentence splitting, well paraphrasing changes content. Table 5 lists
number non-identical rules grammar formalism extracted Wikipedia
corpora, rules instance count one two removed.
addition grammars extracted Simple English Wikipedia, worked
monolingual synchronous grammar included Paraphrase Database (Ganitkevitch et al., 2013), paraphrases extracted bilingual parallel corpora.
English portion PPDB contains 220 million paraphrase pairs, including 140
million paraphrase patterns capturing syntactic transformations varying confidence.
form synchronous grammar, used highest scoring 585,000 paraphrases
subset constituent syntactic paraphrases (where nonterminals labeled Penn
Treebank constituents).
4.2 Semantic Role Labeler
method presented paper crucially relies semantic role labeler refining
transformations performing semantic analysis general. used publicly available system Bjorkelund et al. (2009). competed
CoNLL-2009 SRL-only challenge, ranked first English language, second overall. best knowledge, system represents state-of-the-art English
SRL parsing. system architecture consists four-stage pipeline classifiers:
predicate identification (although module required evaluation), predicate
sense disambiguation, binary classifier argument identification, finally argument
classification using multiclass classifier. Beam search used identify arguments
predicate label them, according local classifiers using features relate
mainly dependency parse information linking predicates potential arguments
siblings. addition, global reranker used select best combination candidates (see Section 5 details). SRL system requires tokenized input lemma,
POS-tag dependency parse information. information already provided
gold-standard training corpus (see immediately below). create equivalent information
transformed text evaluation files, used mate-tools pipeline (Bjorkelund
et al., 2009), retrained (like SRL model itself) training partition data.
used English language benchmark datasets CoNLL-2009 shared task
train evaluate SRL models. identified labeled semantic arguments nouns
148

fiText Rewriting Improves Semantic Role Labeling

Corpus
Training
+ H&S
+ PPDB
+ T3
+ QTSG
+ T3 + QTSG
+ PPDB + T3
+ PPDB + QTSG
+ PPDB + T3 + QTSG
Development
Test in-domain
Test out-of-domain

Sentences
39,272
55,474
238,732
203,941
500,627
704,561
442,666
739,352
943,286
1,334
2,399
425

Tokens
958,174
909,358
7,071,550
4,701,688
9,623,471
14,325,166
11,773,245
16,695,028
21,396,723
33,368
57,676
7,207

Table 6: Statistics corpora used train evaluate SRL models.
verbs (Hajic, Ciaramita, Johansson, Kawahara, Mart, Marquez, Meyers, Nivre, Pado,
Stepanek, Stranak, Surdeanu, Xue, & Zhang, 2009). used training, development,
test out-of-domain test partitions provided, statistics
data sets shown Table 6. Specifically, show increase training data
effected method using transformations obtained T3, QTSG, PPDB,
combinations. comparison use manual transformations available
Heilman Smith (2010). train SRL model (and previous stages NLP
pipeline), used data training partition only, development partition
used identify best subset G transformations2 .
used LibLinear (Fan, Chang, Hsieh, Wang, & Lin, 2008) train SVM,
hyper-parameters SVM tuned cross-validation training set
maximise area ROC curve, using automatic grid-search utility python
package scikit-learn (Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel,
Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, & Duchesnay, 2011). assessment cross-validation accuracy (in terms F1 score area
ROC curve) SVM grammar shown Table 7. results show
PPDB rewrites accurate employ, perhaps rules
heavily lexicalized grammars. T3 grammar unpredictable use,
although SVM scores considerably higher chance.
Test sets used solely evaluation, making use indicators data
files words argument-bearing predicates. Results generated using
CoNLL-2009 evaluation script unmodified. report results semantic roles
(i.e., combination syntactic dependencies tends yield higher scores)
using in-domain out-of-domain evaluation data. evaluation script,
semantic propositions evaluated converting semantic dependencies
2. result re-training performance reported worse models
available mate website, trained partitions CoNLL-2009 data
(training, development test).

149

fiWoodsend & Lapata

Grammar
PPDB
T3
QTSG

F1
0.85
0.67
0.78

Area ROC
0.82
0.61
0.72

Table 7: Statistics SVMs performance grammar, obtained crossvalidation training set.

predicate arguments, labeling dependency labels
corresponding argument. Additionally, dependency created virtual root node
predicate labeled predicate sense. comparable published
results, general report scores combine predicate sense argument role label
predictions. Tables 12, 13 14, however, focus arguments only, remove
predicate sense scores.

5. Results
section provide empirical evidence performance approach.
experiments primarily designed answer following questions. text rewriting
generally improve SRL performance? matter transformation rules use,
i.e., rules better others? transformation rules useful out-ofdomain data? SRL labels mostly affected rewriting? performance vary
depending size original training data? results sensitive learner
employed? first examine effect different (transformation) grammars
SRL task in-domain out-of-domain test data, move assess
labels mostly affected method. Finally, present results effect
combining approach global reranker training different-sized datasets.
5.1 Transformation Rules Improve F1 Across Board
Table 8 (left half) shows SRL performance (measured terms precision, recall, F1)
in-domain CoNLL-2009 test set. training corpora rewritten H&S
system, T3, QTSG, PPDB grammars, resulting SRL models significantly
(p < 0.01) improve model trained original corpus. used stratified shuffling (Noreen, 1989) examine whether differences F1 significant (Pado, 2006).
Recall shows largest increase, particularly acquired synchronous grammars,
indicating increased training data resulting better coverage. Generally
expense precision cases apart PPDB increased well.
Significant gains seen acquired grammars compared H&S system,
exception T3 greater variation performance.
combined rewrites produced different grammars (see T3+QTSG,
PPDB+T3, PPDB+QTSG PPDB+T3+QTSG Table 8) significantly
improve performance individual grammars (although still significantly better
original model H&S system), suggesting grammars capturing
150

fiText Rewriting Improves Semantic Role Labeling

Original
H&S
PPDB
T3
QTSG
PPDB+T3
PPDB+QTSG
T3+QTSG
PPDB+QTSG+T3
label projection

P
86.79
87.08
86.42
86.84
87.04
86.61
86.70
86.78
86.76
80.95

In-domain
R
F1
83.58 85.15
83.73 85.37
84.64 85.52
84.25 85.52
84.34 85.67
84.45 85.51
84.81 85.75
84.62 85.69
84.69 85.71
78.75 79.83

Out-of-domain
P
R
F1
76.04 71.73 73.82
76.33 70.86 73.49
75.37 72.66 73.99
76.04 72.29 74.12
76.88 72.83 74.89
75.65 72.49 74.03
76.64 73.22 74.89
76.56 72.88 74.67
76.54 73.19 74.83
66.94 66.93 66.93

Table 8: Semantic evaluation results CoNLL-2009 in-domain out-of-domain test
sets (combining predicate word sense argument role labels). Results
models trained Original training set, baseline extension training set,
extensions due grammar combinations. label projection: results
training PPDB+QTSG+T3 training corpus, without rewriting
labels using gold corpus information. Difference Original significant
p < 0.01. Difference H&S significant p < 0.01.

Proportion sentences
produced grammar

H&S
QTSG

produced
PPDB
0.4
0.0

grammar
T3
QTSG
0.2
28.1
3.2

Table 9: Sentence rewrite overlap (%) refined rewrite corpora produced H&S,
PPDB, T3 QTSG.

similar information. instance, T3 QTSG extracted corpora
aligned sentence pairs. degree overlap rewrite corpora produced
grammars shown Table 9. Although degree overlap exact sentences low,
relative performance resulting models closer (discussed below). Overall,
best performing system uses transformations obtained QTSG PPDB,
surprising rules extracted grammars present minimal overlap.
Benefits transfer out-of-domain text acquired grammars, improving
overall performance even in-domain data (see right half Table 8).
F1-score QTSG model 1% higher original model, Recall
model combining acquired grammars increased 1.5%. Meanwhile,
rewrites H&S system seem improve coverage, resulting drop Recall
F1-score.
151

fiWoodsend & Lapata

Original
PPDB
T3
QTSG

P
86.79
87.34
87.36
87.48

In-domain
R
F1
83.58 85.15
83.10 85.17
83.26 85.26
83.31 85.34

Out-of-domain
P
R
F1
76.04 71.73 73.82
76.41 71.42 73.83
76.49 71.76 74.05
76.75 72.00 74.30

Table 10: Results CoNLL-2009 in-domain out-of-domain test sets, training SRL
model rewrites labeled positive.

SVM
Thresholds Count
None

+0.001
10
-0.001
10
-0.2
10

10
+0.001
5
+0.001
3

P
80.26
80.74
80.82
80.65
80.55
80.46
80.00

Quality
R
76.28
76.99
76.87
76.86
77.08
76.54
76.13

F1
78.22
78.82
78.80
78.71
78.78
78.45
78.02

Table 11: Effect selecting transforms SVM quality resulting model
(precision, recall F1 measures labeling development set).

addition, examined whether filtering set acquired transformation functions
indeed beneficial. approach proposed, transformations applied
training corpus twice: first time input SVM identify reliable rewrite
rules, second pass reduced set rules applied whole training corpus.
alternative approach would apply transforms once, train SRL
model. thus took rewrites labeled positive steps 914 Algorithm 1
corrected labels gold-standard (step 23). SRL models subsequently trained using
extended training corpus, created concatenating original training dataset
rewrites. Table 10 shows SRL performance different grammars (PPDB, T3,
QTSG) test set. Although precision F1 increased original model,
gains much reduced compared results obtained using SVM (Table 8).
appears extra rewrites obtained applying generally-reliable transforms
whole training set increases coverage, improves performance models.
Table 11 shows altering quality threshold (and removing indicator features
number times transformation function extracted) affects performance.
Results shown QTSG grammar (in-domain) development set (we observed
similar patterns grammars grammar combinations). SVM quality
threshold varied positive (no transformations accepted) negative (all
152

fiText Rewriting Improves Semantic Role Labeling

Original
H&S
PPDB
T3
QTSG
PPDB+T3
PPDB+QTSG
T3+QTSG
PPDB+QTSG+T3

P
82.69
83.08
82.34
82.88
83.00
82.61
82.62
82.75
82.83

In-domain
R
F1
78.25 80.41
78.45 80.70
79.89 81.10
79.30 81.05
79.27 81.09
79.62 81.09
80.01 81.29
79.77 81.23
79.95 81.37

Out-of-domain
P
R
F1
71.44 65.62 68.40
71.65 64.25 67.75
70.68 67.02 68.80
71.54 66.46 68.90
72.48 66.98 69.62
71.16 66.88 68.95
72.11 67.47 69.71
72.08 67.09 69.49
72.08 67.54 69.74

Table 12: Performance labeling semantic arguments (predicate word sense information removed). Difference Original significant p < 0.01. Difference
H&S significant p < 0.01.

Original
H&S
PPDB
T3
QTSG
PPDB+T3
PPDB+QTSG
T3+QTSG
PPDB+QTSG+T3

In-domain
P
R
F1
89.56 84.75 87.09
89.71 84.71 87.14
88.99 86.34 87.65
89.57 85.70 87.59
89.53 85.50 87.47
89.10 86.28 87.66
89.28 86.05 87.63
89.33 86.10 87.68
89.33 86.23 87.75

Out-of-domain
P
R
F1
87.20 80.10 83.50
87.40 78.38 82.65
86.24 81.78 83.95
87.09 80.90 83.88
87.28 80.66 83.84
86.75 81.53 84.06
86.77 81.18 83.88
87.34 81.29 84.20
86.90 81.43 84.07

Table 13: Accuracy identification classification (labeling) semantic arguments.

transformations). findings indicate constructing G transformations
positive SVM weight (threshold +0.001) gives better results transformations,
permissive threshold.
5.2 Transformation Rules Improve Semantic Role Assignment Verbal
Nominal Predicates
results Table 8 combine accuracy predicting sense predicates accuracy
labeling arguments. Generally, models better assigning correct predicate
sense. interesting result much gain performance seen rewriting
training corpus comes improving semantic role assignment. appears
153

fiWoodsend & Lapata



RAMMNR



RAMLOC





RAMCAU





RA2

















RA1
RA0
CA1





















































































































CA0
AMTMP









AMPRD
AMPNC

Argument



RAMTMP







AMNEG





AMMOD





AMMNR







AMLOC



















































AMDIR















AMCAU



















































AMDIS

AMADV







A5



A4

























A2





















A1



























A0







JJ

NN

VBP

VBZ

A3

NNP NNS

VB

+10%

+1%






AMEXT

Change F1



VBD VBG VBN

0%
1%

10%
Occurrences




1+
10+

100+



1000+

Predicate

Figure 3: Changes F1-score PPDB+T3+QTSG model Original, measured
pairs predicate POS-tag argument.

introducing syntactic variation training data provides model wider coverage
syntactic dependency paths predicate arguments.
Table 12 shows results models data sets above, focusing
argument labels only. acquired grammars show biggest improvements,
1% improvement Recall case, gains F1-score 0.5% 1.2%.
models data sets used Table 13, results argument
identification only, classification (unlabelled arguments). improvements
154

fiText Rewriting Improves Semantic Role Labeling



RAMMNR



RAMLOC





RAMCAU





RA2

















RA1
RA0
CA1











































































+1%







0%







































CA0
AMTMP









AMPRD
AMPNC

Argument



RAMTMP







AMNEG





AMMOD





AMMNR







AMLOC






















































AMDIR















AMCAU



















































AMEXT
AMDIS

AMADV







A5



A4

























A2





















A1



























A0







JJ

NN

VBP

VBZ

A3

NNP NNS

VB



VBD VBG VBN

Change F1
+10%

1%

10%
Occurrences




1+
10+

100+



1000+

Predicate

Figure 4: Relative performance terms F1-score QTSG (red) PPDB (blue)
models, pairs predicate POS-tag argument.

Original Recall F1. large before, showing overall
gains result improvements argument identification classification.
breakdown gains F1-score predicate POS-tag argument shown
Figure 3, illustrating relative improvements model trained acquired grammars (PPDB+T3+QTSG) model trained original CoNLL training data.
analysis reveals gain came increased precision recall
predicting core arguments. additional gains modifiers nominal pred155

fiWoodsend & Lapata

Dependency path distance
Proportion test set
SRL model:
Original
PPDB
T3
QTSG
PPDB+QTSG+T3

01
75.75

2
13.67

3
5.54

4
2.62

5
1.13

6
0.56

7+
0.73

88.83
+0.49
+0.63
+0.53
+0.74

74.27
+1.43
+1.15
+0.66
+1.60

61.73
+2.65
+1.65
+1.82
+2.49

54.76
+3.26
+1.67
+2.98
+2.02

43.08
+4.78
+1.42
+3.01
+6.20

23.91
+5.53
+1.22
0.96
+1.35

12.27
0.06
+0.69
+0.43
+1.61

Table 14: F1-scores labeled arguments distance predicate argument
measured number arcs dependency graph. Results
CoNLL in-domain test set. Lower rows show change F1-score
Original SRL model.

icates. improvement losses common core arguments
(A0 A1) verbal predicates, striking gains seen
core argument labels. seems consistent models learning wider syntactic
coverage. Figure 4 shows similar breakdown gains F1-score predicate POStag argument, time comparing improvements seen QTSG corpus
resulting PPDB. differences less pronounced, PPDB improving
core arguments more, QTSG improving performance labeling modifiers.
investigated effect label projection mechanism itself. used
rewrites produced grammars (PPDB+T3+QTSG) extend training set. However, instead using projected labels, used original model Mgold (trained
training partition CoNLL-2009) label refined corpus. retrained
extended corpus used retrained model label test corpus. words,
removed step 25 Algorithm 1. considered form self-training. Results
test out-of-domain sets show using automatically generated labels
instead projected ones seriously impairs resulting model, F1-scores decreasing
almost 6% test set 8% out-of-domain set (see last row Table 8).
5.3 Transformation Rules Improve Performance Relations Involving Long
Dependency Paths
dependency path (the sequence arcs syntactic dependency tree)
predicate argument typically short. Table 14 shows gold-labeled
test set, three-quarters arguments direct dependency heads children
predicate, case nominal predicates, argument predicate itself. Existing
SRL models highly accurate shorter pathsthe original SRL model
F1-score almost 89%but prediction accuracy drops considerably dependency path
grows. seen Table 14, adding rewrites training set improves prediction
accuracy almost combinations transformation grammar dependency path
distance, largest gains seen number arcs dependency path
156

fiText Rewriting Improves Semantic Role Labeling

Original
H&S
PPDB
T3
QTSG
T3+QTSG
PPDB+T3
PPDB+QTSG
PPDB+QTSG+T3

P
88.44
88.68
86.42
88.04
88.41
88.24
86.61
86.70
87.94

In-domain
R
F1
84.42 86.38
84.34 86.46
84.64 85.52
84.78 86.38
85.05 86.70
85.21 86.70
84.45 85.51
84.81 85.75
85.25 86.57

Out-of-domain
P
R
F1
77.89 72.73 75.22
78.11 71.76 74.80
76.73 73.36 75.01
77.07 72.97 74.97
78.34 73.70 75.95
78.00 73.53 75.70
77.30 73.51 75.35
77.41 73.82 75.57
77.67 73.73 75.64

Table 15: Results CoNLL test sets models combining extended training data
global reranker. Difference Original significant p < 0.01. Difference
H&S significant p < 0.01.

three six. Improvements F1-score observed individual grammars
combination (PPDB+QTSG+T3).
5.4 Transformation Rules Improve Performance Even Global
Reranker Used
SRL system used (Bjorkelund et al., 2009) optionally incorporate global
reranker (Toutanova, Haghighi, & Manning, 2005). reranker re-scores complete
predicate-argument structure, using features stages local pipeline additional features representing sequence core argument labels current predicate.
Table 15 presents evaluation results global reranker trained extended corpora
produced method. Compared model trained original corpus, adding
reranker provide significant improvement.3 Training extended data gives
increases performance; smaller, though still significant,
case Table 8. indicates global reranker compensating some,
all, new information contained extended training data.
5.5 Transformation Rules Improve Performance Across (Small Large)
Datasets
investigated accuracy labeler function size original
training data. size, subsets original training data created (with replacement) used train SRL model, performance resulting model
measured using development set. training subset, applied Algorithm 1:
original SRL model trained subset; created extended corpus
3. scores reported higher official CoNLL-2009 ones (in domain P:87.46, R:83.87,
F1:85.63; domain P:76.04, R:70.76, F1:73.31) using mate-tools NLP pipeline
dependency parse, rather dependency information provided test set.

157

fiWoodsend & Lapata




80

80











75




75












70

70

Recall

Precision











Rewrites



Source


65



65



60

60






55

55
160

625

2500

10000

40000



160

Source sentences

625

2500

10000

40000

Source sentences

Figure 5: SRL model performance function size training data,
without additional rewrites. Error bars show standard error 10 experiments.

subset using grammar; SVM trained time refine transformations
preserved labels; SRL model retrained original plus refined
rewritten version corpus subset.
particular, wanted investigate rewritten text provided performance
benefit small amount training data, benefit would
subsumed labeled training data provided. learning curves Figure 5 show
contrary: increasing quantity source training data undoubtedly improves
quality SRL model, found including rewritten training data addition
consistently improves precision recall measures. learning curves Figure 5
use QTSG grammar set transformation functions; obtained similar results
PPDB T3 (and grammar combinations), however omit sake
brevity.

6. Conclusions
paper investigated potential text rewriting means increasing
amount training data available supervised NLP tasks. method automatically
extracts rewrite rules comparable corpora uses generate multiple syntactic variants sentences annotated gold standard labels. Application method
semantic role labeling reveals syntactic transformations improve SRL performance
158

fiText Rewriting Improves Semantic Role Labeling

QTSG

PPDB

hNP, NPi



h[NP DT

hNP, NPi



h[NP NP

hNP, Si



h[NP NP

hS, Si



h[S even VBZ

hADJP, ADJPi



h[ADJP JJ

hPP, PPi



h[PP past month], [PP last month]i

1
1
1

JJ



, NP
PP

NNS

], [NP DT

CC NP



2

2



1

NP

2

NNS

], [NP NP

], [S NP
1

1

1

PP

2

1

2

]i

]i

.]i

], [S even though VBZ

], [ADJP equally JJ

1

1

NP

2

]i

]i

Table 16: Examples QTSG PPDB synchronous grammar rules given high importance
refinement. Boxed indices indicate alignment, .

beyond sate art CoNLL 2009 benchmark dataset. Specifically, experimentally show (a) rewrite rules, whether automatic hand-written, consistently
improve SRL performance, although automatic variants tend perform best; (b) syntactic transformations improve SRL performance within- out-of-domain; (c)
improvements observed across learners, even using global reranker.
future would explore better ways identifying best (i.e., performance enhancing) rewrite rules may task grammar specific. Table 16
illustrates rules deemed important (i.e., given high weight) SVM classifier
SRL task. instance, could undertake detailed feature engineering, including tree-based ngram features capture grammaticality rewritten sentences.
Throughout paper argued transformation rules used enhance
performance SRL task. Conversely, work described might
relevance NLP tasks employing rewriting. example, idea identifying
label preserving transformations, could used learn rules meaning preserving
consequently safe use tasks simplification sentence compression. Machine
translation, textual entailment, semantic parsing additional application areas
stand benefit accurate rewrite rules. Much methodology reported
could adapted machine translation either training larger datasets (CallisonBurch, Koehn, & Osborne, 2006), domain-adaptation (Irvine, Quirk, & Daume III,
2013), evaluation (Kauchak & Barzilay, 2006)
Finally, beyond supervised SRL, would adapt method unsupervised
semantic role induction (Lang & Lapata, 2011; Titov & Klementiev, 2012), investigate alternative synchronous grammar extraction methods (e.g., based dependency information),
obtain rewrite rules larger comparable corpora.

Acknowledgments
grateful anonymous referees whose feedback helped substantially improve
present paper. acknowledge financial support EPSRC (EP/K017845/1)
framework CHIST-ERA READERS project.
159

fiWoodsend & Lapata

References
Aho, A. V., & Ullman, J. D. (1969). Syntax directed translations pushdown assembler. Journal Computer System Sciences, 3 (1), 3756.
Bannard, C., & Callison-Burch, C. (2005a). Paraphrasing Bilingual Parallel Corpora.
Proceedings 43rd Annual Meeting Association Computational Linguistics, pp. 597604, Ann Arbor.
Bannard, C., & Callison-Burch, C. (2005b). Paraphrasing Bilingual Parallel Corpora.
Proceedings 43rd ACL, pp. 255262, Ann Arbor, MI.
Barzilay, R., & McKeown, K. (2001). Extracting Paraphrases Parallel Corpus.
Proceedings ACL/EACL, pp. 5057, Toulouse, France.
Bjorkelund, A., Hafdell, L., & Nugues, P. (2009). Multilingual semantic role labeling.
Proceedings Thirteenth Conference Computational Natural Language Learning (CoNLL 2009): Shared Task, pp. 4348, Boulder, Colorado. Software retrieved
https://code.google.com/p/mate-tools/.
Callison-Burch, C. (2007). Paraphrasing Translation. Ph.D. thesis, University Edinburgh.
Callison-Burch, C. (2008). Syntactic Constraints Paraphrases Extracted Parallel
Corpora. Proceedings 2008 Conference Empirical Methods Natural
Language Processing, pp. 196205, Honolulu, Hawaii.
Callison-Burch, C., Koehn, P., & Osborne, M. (2006). Improved statistical machine translation using paraphrases. Proceedings Human Language Technology Conference
NAACL, Main Conference, pp. 1724, New York City, USA.
Chandrasekar, R., Doran, C., & Srinivas, B. (1996). Motivations Methods Text
Simplification. Proceedings 16th International Conference Computational
Linguistics, pp. 10411044, Copenhagen, Denmark.
Chiang, D. (2007). Hierarchical Phrase-Based Translation. Computational Linguistics,
33 (2), 201228.
Cohn, T., & Lapata, M. (2009). Sentence Compression Tree Transduction. Journal
Artificial Intelligence Research, 34, 637674.
Cohn, T., & Lapata, M. (2013). abstractive approach sentence compression. ACM
Trans. Intell. Syst. Technol., 4 (3), 41:141:35.
Coster, W., & Kauchak, D. (2011). Simple English Wikipedia: New Text Simplification
Task. Proceedings 49th Annual Meeting Association Computational
Linguistics: Human Language Technologies, pp. 665669, Portland, Oregon, USA.
Dowty, D. (1991). Thematic Proto Roles Argument Selection. Language, 67 (3), 547
619.
Eisner, J. (2003). Learning Non-Isomorphic Tree Mappings Machine Translation.
Proceedings ACL Interactive Poster/Demonstration Sessions, pp. 205208, Sapporo, Japan.
160

fiText Rewriting Improves Semantic Role Labeling

Fan, R. E., Chang, K. W., Hsieh, C. J., Wang, X. R., & Lin, C. J. (2008). LIBLINEAR:
Library Large Linear Classification. Journal Machine Learning Research, 9,
18711874.
Feblowitz, D., & Kauchak, D. (2013). Sentence simplification tree transduction.
Proceedings Second Workshop Predicting Improving Text Readability
Target Reader Populations, pp. 110, Sofia, Bulgaria.
Furstenau, H., & Lapata, M. (2012). Semi-supervised semantic role labeling via structural
alignment. Computational Linguistics, 38 (1), 135171.
Galley, M., & McKeown, K. (2007). Lexicalized Markov Grammars Sentence Compression. Proceedings NAACL/HLT, pp. 180187, Rochester, NY.
Ganitkevitch, J., Callison-Burch, C., Napoles, C., & Van Durme, B. (2011). Learning
Sentential Paraphrases Bilingual Parallel Corpora Text-to-Text Generation.
Proceedings 2011 Conference Empirical Methods Natural Language
Processing, pp. 11681179, Edinburgh, Scotland, UK.
Ganitkevitch, J., Cao, Y., Weese, J., Post, M., & Callison-Burch, C. (2012). Joshua 4.0:
Packing, pro, paraphrases. Proceedings Seventh Workshop Statistical
Machine Translation, pp. 283291, Montreal, Canada.
Ganitkevitch, J., Van Durme, B., & Callison-Burch, C. (2013). PPDB: Paraphrase
Database. Proceedings 2013 Conference North American Chapter
Association Computational Linguistics: Human Language Technologies, pp.
758764, Atlanta, Georgia. used prepackaged small constituent syntactic
subset PPDB, retrieved http://paraphrase.org.
Gildea, D., & Jurafsky, D. (2002). Automatic Labeling Semantic Roles. Computational
Linguistics, 28 (3), 245288.
Graehl, J., & Knight, K. (2004). Training Tree Transducers. HLT-NAACL 2004: Main
Proceedings, pp. 105112, Boston, MA.
Hajic, J., Ciaramita, M., Johansson, R., Kawahara, D., Mart, M. A., Marquez, L., Meyers,
A., Nivre, J., Pado, S., Stepanek, J., Stranak, P., Surdeanu, M., Xue, N., & Zhang, Y.
(2009). conll-2009 shared task: Syntactic semantic dependencies multiple
languages. Proceedings Thirteenth Conference Computational Natural
Language Learning (CoNLL 2009): Shared Task, pp. 118, Boulder, Colorado.
Heilman, M., & Smith, N. (2010). Extracting Simplified Statements Factual Question
Generation. Proceedings 3rd Workshop Question Generation, pp. 1120,
Carnegie Mellon University, PA. Software available http://www.ark.cs.cmu.edu/
mheilman/questions/.
Irvine, A., Quirk, C., & Daume III, H. (2013). Monolingual marginal matching translation model adaptation. Proceedings 2013 Conference Empirical Methods
Natural Language Processing, pp. 10771088, Seattle, Washington, USA.
Kauchak, D. (2013). Improving text simplification language modeling using unsimplified
text data. Proceedings 51st Annual Meeting Association Computational Linguistics (Volume 1: Long Papers), pp. 15371546, Sofia, Bulgaria. used
161

fiWoodsend & Lapata

Version 2.0 sentence-aligned corpus, retrieved http://www.cs.middlebury.
edu/~dkauchak/simplification/.
Kauchak, D., & Barzilay, R. (2006). Paraphrasing automatic evaluation. Proceedings
Human Language Technology Conference NAACL, Main Conference, pp.
455462, New York City, USA.
Klebanov, B. B., Knight, K., & Marcu, D. (2004). Text Simplification InformationSeeking Applications. Meersman, R., & Tari, Z. (Eds.), Move Meaningful
Internet Systems 2004: CoopIS, DOA, ODBASE, pp. 735747. Springer Berlin
Heidelberg.
Koehn, P. (2010). Statistical Machine Translation. Cambridge University Press.
Koehn, P., Och, F. J., & Marcu, D. (2003). Statistical phrase-based translation. Proceedings HLT/NAACL, pp. 4854, Edmonton, Canada.
Kundu, G., & Roth, D. (2011). Adapting Text instead Model: Open Domain
Approach. Proceedings Fifteenth Conference Computational Natural Language Learning, pp. 229237, Portland, Oregon, USA.
Kwiatkowski, T. (2012). Probabilistic Grammar Induction Sentences Structured
Meanings. Ph.D. thesis, University Edinburgh.
Lang, J., & Lapata, M. (2011). Unsupervised semantic role induction via split-merge clustering. Proceedings 49th Annual Meeting Association Computational
Linguistics: Human Language Technologies, pp. 11171126, Portland, Oregon, USA.
Liang, P., Taskar, B., & Klein, D. (2006). Alignment Agreement. Proceedings
HLT/NAACL, pp. 104111, New York, NY.
Marton, Y., Callison-Burch, C., & Resnik, P. (2009). Improved statistical machine translation using monolingually-derived paraphrases. Proceedings 2009 Conference
Empirical Methods Natural Language Processing, pp. 381390, Singapore.
Mehdad, Y., Negri, M., & Federico, M. (2010). Towards cross-lingual textual entailment.
Human Language Technologies: 2010 Annual Conference North American
Chapter Association Computational Linguistics, pp. 321324, Los Angeles,
California.
Melli, G., Wang, Y., Liu, Y., Kashani, M. M., Shi, Z., Gu, B., Sarkar, A., & Popowich, F.
(2005). Description SQUASH, SFU Question Answering Summary Handler
DUC-2005 Summarization Task. Proceedings Human Language Technology Conference Conference Empirical Methods Natural Language
Processing Document Understanding Workshop, Vancouver, Canada.
Noreen, E. (1989). Computer-intensive methods testing hypotheses: introduction.
Wiley.
Pado, S. (2006). Users guide sigf: Significance testing approximate randomisation.
Retrieved http://www.nlpado.de/~sebastian/software/sigf.shtml.
Palmer, M., Gildea, D., & Kingsbury, P. (2005). Proposition Bank: Annotated
Corpus Semantic Roles. Computational Linguistics, 31 (1), 71106.
162

fiText Rewriting Improves Semantic Role Labeling

Pang, B., Knight, K., & Marcu, D. (2003). Syntax-based Alignment Multiple Translations:
Extracting Paraphrases Generating New Sentences. Proceedings NAACL,
pp. 181188, Edmonton, Canada.
Park, J. H., Croft, B., & Smith, D. A. (2011). Quasi-synchronous Dependence Model
Information Retrieval. Proceedings 20th ACM International Conference
Information Knowledge Management, pp. 1726, Glasgow, United Kingdom.
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel,
M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau,
D., Brucher, M., Perrot, M., & Duchesnay, E. (2011). Scikit-learn: Machine learning
Python. Journal Machine Learning Research, 12, 28252830.
Shen, D., & Lapata, M. (2007). Using Semantic Roles Improve Question Answering.
Proceedings 2007 Joint Conference Empirical Methods Natural Language
Processing Computational Natural Language Learning (EMNLP-CoNLL), pp. 12
21, Prague, Czech Republic.
Surdeanu, M., Harabagiu, S., Williams, J., & Aarseth, P. (2003). Using Predicate-Argument
Structures Information Extraction. Proceedings Annual Meeting
Association Computational Linguistics, pp. 815, Sapporo, Japan.
Titov, I., & Klementiev, A. (2012). bayesian approach unsupervised semantic role
induction. Proceedings 13th Conference European Chapter
Association Computational Linguistics, pp. 1222, Avignon, France.
Toutanova, K., Haghighi, A., & Manning, C. (2005). Joint learning improves semantic role
labeling. Proceedings 43rd Annual Meeting Association Computational Linguistics (ACL05), pp. 589596, Ann Arbor, Michigan.
Vapnik, V. (1995). Nature Statistical Learning Theory. Springer-Verlag New York,
Inc.
Vickrey, D., & Koller, D. (2008). Sentence simplification semantic role labeling.
Proceedings ACL-08: HLT, pp. 344352, Columbus, Ohio.
Wang, M., & Manning, C. (2010). Probabilistic tree-edit models structured latent
variables textual entailment question answering. Proceedings 23rd
International Conference Computational Linguistics (Coling 2010), pp. 11641172,
Beijing, China.
Wang, M., Smith, N. A., & Mitamura, T. (2007). Jeopardy model? quasisynchronous grammar QA. Proceedings 2007 Joint Conference Empirical Methods Natural Language Processing Computational Natural Language
Learning (EMNLP-CoNLL), pp. 2232, Prague, Czech Republic.
Woodsend, K., & Lapata, M. (2011). Learning simplify sentences quasi-synchronous
grammar integer programming. Proceedings 2011 Conference Empirical Methods Natural Language Processing, pp. 409420, Edinburgh, Scotland, UK.
used Wikipedia revisions corpus, retrieved http://homepages.inf.ed.
ac.uk/kwoodsen/wiki.html.
Woodsend, K., & Lapata, M. (2012). Multiple aspect summarization using integer linear
programming. Proceedings 2012 Joint Conference Empirical Methods
163

fiWoodsend & Lapata

Natural Language Processing Computational Natural Language Learning, pp.
233243, Jeju Island, Korea.
Wu, D. (1997). Stochastic Inversion Transduction Grammars Bilingual Parsing
Parallel Corpora. Computational Linguistics, 23 (3), 377404.
Wu, D., & Fung, P. (2009). Semantic Roles SMT: Hybrid Two-Pass Model. Proceedings Human Language Technologies: Annual Conference North American
Chapter Association Computational Linguistics, Companion Volume: Short
Papers, pp. 1316, Boulder, Colorado.
Yamada, K., & Knight, K. (2001). Syntax-based Statistical Translation Model. Proceedings 39th Annual Meeting Association Computational Linguistics,
pp. 523530, Toulouse, France.
Yamangil, E., & Nelken, R. (2008). Mining Wikipedia revision histories improving
sentence compression. Proceedings ACL-08: HLT, Short Papers, pp. 137140,
Columbus, Ohio.
Zanzotto, F. M., & Pennacchiotti, M. (2010). Expanding textual entailment corpora
fromwikipedia using co-training. Proceedings 2nd Workshop Peoples Web Meets NLP: Collaboratively Constructed Semantic Resources, pp. 2836,
Beijing, China. Coling 2010 Organizing Committee.
Zhu, Z., Bernhard, D., & Gurevych, I. (2010). Monolingual Tree-based Translation Model
Sentence Simplification. Proceedings 23rd International Conference
Computational Linguistics, pp. 13531361, Beijing, China.

164


