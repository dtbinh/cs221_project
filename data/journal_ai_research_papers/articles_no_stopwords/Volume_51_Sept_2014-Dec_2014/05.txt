Journal Artificial Intelligence Research 51 (2014)

Submitted 02/14; published 11/14

Using Meta-mining Support Data Mining Workflow
Planning Optimization
Phong Nguyen
Melanie Hilario

Phong.Nguyen@unige.ch
Melanie.Hilario@unige.ch

Department Computer Science
University Geneva
Switzerland

Alexandros Kalousis

Alexandros.Kalousis@hesge.ch

Department Business Informatics
University Applied Sciences
Western Switzerland,
Department Computer Science
University Geneva
Switzerland

Abstract
Knowledge Discovery Databases complex process involves many different
data processing learning operators. Todays Knowledge Discovery Support Systems
contain several hundred operators. major challenge assist user designing
workflows valid ideally optimize performance measure
associated user goal. paper present system. system relies
meta-mining module analyses past data mining experiments extracts metamining models associate dataset characteristics workflow descriptors view
workflow performance optimization. meta-mining model used within data mining
workflow planner, guide planner workflow planning. learn metamining models using similarity learning approach, extract workflow descriptors
mining workflows generalized relational patterns accounting domain
knowledge provided data mining ontology. evaluate quality data mining
workflows system produces collection real world datasets coming
biology show produces workflows significantly better alternative
methods workflow selection planning.

1. Introduction
Learning models extracting knowledge data using data mining extremely
complex process requires combining number Data Mining (DM) operators, selected large pools available operators, combined data mining workflow. DM
workflow assembly individual data transformations analysis steps, implemented
DM operators, composing DM process data analyst chooses address
his/her DM task. Workflows recently emerged new paradigm representing
managing complex computations accelerating pace scientific progress. (meta-)
analysis becoming increasingly challenging growing number complexity
available operators (Gil et al., 2007).
c
2014
AI Access Foundation. rights reserved.

fiNguyen, Hilario & Kalousis

Todays second generation knowledge discovery support systems (KDSS) allow complex
modeling workflows contain several hundreds operators; RapidMiner platform (Klinkenberg, Mierswa, & Fischer, 2007), extended version Weka (Hall
et al., 2009) R (R Core Team, 2013), proposes actually 500 operators,
complex data control flows, e.g. bagging boosting operators,
several sub-workflows interleaved. consequence, possible number
workflows modeled within systems order several millions,
ranging simple elaborated workflows several hundred operators. Therefore data analyst carefully select among operators ones
meaningfully combined address his/her knowledge discovery problem. However, even
sophisticated data miner overwhelmed complexity modeling,
rely his/her experience biases well thorough experimentation
hope finding best operator combination. advance new generation
KDSS provide even advanced functionalities, becomes important provide
automated support user workflow modeling process, issue
identified one top-ten challenges data mining (Yang & Wu, 2006).

2. State Art DM Workflow Design Support
last decade, rather limited number systems proposed provide
automated user support design DM workflows. Bernstein, Provost, Hill (2005)
propose ontology-based Intelligent Discovery Assistant (ida) plans valid DM workflows valid sense executed without failure according basic
descriptions input dataset attribute types, presence missing values, number
classes, etc. describing DM ontology input conditions output effects
DM operators, according three main steps KD process, pre-processing, modeling post-processing (Fayyad, Piatetsky-Shapiro, & Smyth, 1996), ida systematically
enumerates workflow planner possible valid operator combinations, workflows,
fulfill data input request. ranking workflows computed according
user defined criteria speed memory consumption measured
past experiments.
Zakova, Kremen, Zelezny, Lavrac (2011) propose kd ontology support automatic design DM workflows relational DM. ontology, DM relational algorithms
datasets modeled semantic web language OWL-DL, providing semantic
reasoning inference querying DM workflow repository. Similar ida,
kd ontology describes DM algorithms data input/output specifications.
authors developed translator ontology representation Planning Domain Definition Language (PDDL) (McDermott et al., 1998), produce
abstract directed-acyclic graph workflows using FF-style planning algorithm (Hoffmann,
2001). demonstrate approach genomic product engineering (CAD) usecases complex workflows produced make use relational data structure
background knowledge.
recently, e-LICO project1 featured another ida built upon planner
constructs DM plans following hierarchical task networks (HTN) planning approach.
1. http://www.e-lico.eu

606

fiUsing Meta-mining Support DM Workflow Planning Optimization

specification HTN given Data Mining Workflow (dmwf) ontology (Kietz,
Serban, Bernstein, & Fischer, 2009). predecessors, e-LICO ida designed
identify operators whose preconditions met given planning step order plan
valid DM workflows exhaustive search space possible DM plans.
None three DM support systems discussed consider eventual
performance workflows plan respect DM task supposed
address. example, goal plan/design workflows solve classification
problem, would consider measure classification performance, accuracy,
deliver workflows optimize it. discussed DM support systems deliver
extremely large number plans, DM workflows, typically ranked simple
heuristics, workflow complexity expected execution time, leaving user
loss best workflow terms expected performance DM task
he/she needs address. Even worse, planning search space large
systems even fail complete planning process, see example discussion
Kietz, Serban, Bernstein, Fischer (2012).
considerable work tries support user, view performance
maximization, specific part DM process, modeling learning.
number approaches proposed, collectively identified meta-learning (Brazdil,
Giraud-Carrier, Soares, & Vilalta, 2008; Kalousis, 2002; Kalousis & Theoharis, 1999; Hilario, 2002; Soares & Brazdil, 2000). main idea meta-learning given new
dataset system able rank pool learning algorithms respect
expected performance dataset. so, one builds meta-learning model
analysis past learning experiments, searching associations algorithms performances dataset characteristics. However, already mentioned, meta-learning
approaches address learning/modeling part applicable complete
process level.
work Hilario, Nguyen, Do, Woznica, Kalousis (2011), first effort
lift meta-learning ideas level complete DM workflows. proposed novel metalearning framework called meta-mining process-oriented meta-learning applied
complete DM process. associate workflow descriptors dataset descriptors,
applying decision tree algorithms past experiments, order learn couplings
workflows datasets lead high predictive performance. workflow descriptors
extracted using frequent pattern mining accommodating background knowledge,
given Data Mining Optimization (dmop) ontology, DM tasks, operators, workflows, performance measures relationships. However predictive performance
system rather low, due limited capacity decision trees capturing
relations dataset workflow characteristics essential
performance prediction.
address limitation, presented work Nguyen, Wang, Hilario,
Kalousis (2012b) approach learns called heterogeneous similarity measures,
associating dataset workflow characteristics. learn similarity measures
dataset space, workflow space, dataset-workflow space. similarity
measures reflect respectively: similarity datasets given similarity
relative workflow performance vectors workflows applied them;
similarity workflows given performance based similarity different datasets;
607

fiNguyen, Hilario & Kalousis

dataset-workflow similarity based expected performance latter applied
former. However two meta-mining methods described limited select
from, rank, set given workflows according expected performance, i.e.
cannot plan new workflows given input dataset.
Retrospectively, presented work Nguyen, Kalousis, Hilario (2011)
initial blueprint approach DM workflow planning view workflow performance optimization. suggested planner guided metamining model ranks partial candidate workflows planning step. gave
preliminary evaluation proposed approach interesting results (Nguyen, Kalousis,
& Hilario, 2012a). However, meta-mining module rather trivial, uses dataset
pattern-based workflow descriptors nearest-neighbor search dataset descriptors identify similar datasets dataset want plan
workflows. Within neighborhood, ranks partial workflows using support
workflow patterns workflows perform best datasets neighborhood. pattern-based ranking workflows cumbersome heuristic;
system learning associations dataset workflow characteristics explicitly optimize expected workflow performance, must guide workflow
planning. approach deployed Kietz et al. (2012).
paper, follow line work first sketched work Nguyen et al.
(2011). couple tightly together workflow planning meta-mining module
develop DM workflow planning system given input dataset designs workflows
expected optimize performance given dataset. meta-mining
module applies heterogeneous similarity learning method presented Nguyen et al.
(2012b) learn associations dataset workflow descriptors lead optimal
performance. exploits learned associations guide planner workflow
construction planning. best knowledge, first system
kind, i.e. able design DM workflows specifically tailored
characteristics input dataset view optimizing DM task performance measure.
evaluate system number real world datasets show workflows
plans significantly better workflows delivered number baseline methods.
rest paper structured follows. section 3, present global architecture system, together brief description planner. section 4
describe detail meta-mining module, including dataset workflow characteristics uses, learning model, learned model used workflow planning.
section 5, provide detailed experimental results evaluate approach different
settings. Finally, conclude section 6.

3. System Description
section, provide general description system. start defining
notations use throughout paper give brief overview
different components system. two important components, planner
meta-miner described subsequent sections (3.4 4 respectively).
close section providing formal representation DM workflows
use planner meta-miner.
608

fiUsing Meta-mining Support DM Workflow Planning Optimization

Symbol

e
wl = [o1 , . . . , ol ]
wcl = (I(p1 wl ), . . . , I(p|P | wl ))T
xu = (x1 , . . . , xd )T
r(x, w)
g


= {o1 , . . . , }
Cl
Sl

Meaning
workflow operator.
workflow data type.
ground DM workflow sequence l operators.
fixed length |P |-dimensional vector description
workflow wl
d-dimensional vector description dataset.
relative performance rank w workflow x
dataset
DM goal.
HTN task.
HTN method.
HTN abstract operator n possible operators.
set candidate workflows abstract operator O.
set candidate workflows selected Cl .

Table 1: Summary notations used.
3.1 Notations
provide basic notations subsequently introduce additional notations needed. use term DM experiment designate execution
DM workflow w W dataset x X . describe dataset x d-dimensional
column vector xu = (x1 , . . . xd )T ; describe detail section 4.1.1 dataset descriptions use. experiment characterized performance measure;
example mining problem addressing classification problem one
performance measure accuracy. performance measures workflows
applied given dataset x extract relative performance rank r(x, w) R+
workflow w x dataset. statistically comparing performance
differences different workflows given dataset (more section 4.1.3).
matrix X : n contains descriptions n datasets used training system. given workflow w two representations. wl denote
l-length operator sequence constitutes workflow. Note different workflows
different lengths, fixed length representation. wcl denote
fixed-length |P |-dimensional binary vector representation w workflow; feature
wcl indicates presence absence relational feature/pattern workflow.
Essentially wcl propositional representation workflow; describe
detail section 4.1.2 extract propositional representation. Finally W denotes
collection workflows, used training system, W
corresponding |P | matrix contains propositional representations. Depending context different workflow notations used interchangeably. Table 1
summarizes important notations.
3.2 System Architecture Operational Pipeline
provide Figure 1 high level architectural description system. three blue
shaded boxes are: (i) Data Mining Experiment Repository (dmer) stores
609

fiNguyen, Hilario & Kalousis

input data

3. User Interface
goal
input MD

1.

input MD

DMER

optimal plans 5.

training MD

MetaMiner

Intelligent Discovery Assistant (IDA)

(partial) candidate workflows
AI
Planner

DM Workflow
Ontology (DMWF)

workflow ranking
4.

online
mode

metamined
model

offline
mode

2.

DM Optimization
Ontology (DMOP)

software
data flow

Figure 1: meta-mining systems components pipeline.
base-level resources, i.e. training datasets, workflows, well performance results
application latter former; essentially dmer contains training data
used derive models necessary workflow planning
design; (ii) user interface user interacts system, specifying
data mining tasks input datasets (iii) Intelligent Discovery Assistant (ida)
component actually plans data mining workflows optimize
performance measure given dataset data mining task user provided
input. ida constitutes core system contains planning component
meta-mining component interact closely order deliver optimal workflows
given input problem; describe two components detail sections 3.4 4
respectively. system operates two modes, offline online mode. offline
mode meta-mining component analyses past base-level data mining experiments,
stored dmer, learn meta-mining model associates dataset workflow
characteristics view performance optimization. online mode meta-miner
interacts planner guide planning workflows using meta-mining
model.
go briefly different steps systems life cycle. first
need collect dmer sufficient number base-level data mining experiments,
i.e. applications different data mining workflows different datasets (step 1).
experiments used step 2 meta-miner generate meta-mining model.
precisely extract base level experiments number characteristics
describe datasets workflows performance latter achieved
applied former. meta-data meta-miner learns associations dataset
workflow characteristics lead high performance; learning heterogeneous similarity measure outputs high similarity workflow dataset
former expected achieve high performance applied latter
(more details section 4.2).
model learned, offline phase completed online phase
start. online phase system directly interacts user. given user
select data mining task g G, classification, regression, clustering, etc, well
610

fiUsing Meta-mining Support DM Workflow Planning Optimization

input dataset task applied; might specify
number top-k optimal workflows planned (step 3). Given new task
input dataset action transfered IDA ai-planner starts
planning process. step planning process ai-planner generates list
valid actions, partial candidate workflows, passes ranking meta-miner
according expected performance given dataset, step 4. meta-miner
ranks using learned meta-mining model planning continues topranked partial candidate workflows data mining task resolved. end
planning process, top-k workflows presented user order given
expected performance input dataset. greedy planning approach
step select top-k current solutions. principle let aiplanner first generate possible workflows meta-miner rank them.
resulting plans would ranked according local greedy approach,
would ranked globally thus optimally respect meta-mining model.
However general feasible due complexity planning process
combinatorial explosion number plans.
3.3 DM Workflow Representation
give formal definition DM workflow represent it. DM
workflows directed acyclic typed graphs (DAGs), nodes correspond operators
edges nodes data input/output objects. fact hierarchical DAGs
since dominating nodes/operators contain sub-workflows. typical
example cross-validation operator whose control flow given parallel execution
training sub-workflows, complex operator boosting. formally, let:
set available operators appear DM workflow, e.g. classification operators, J48, SVMs, etc. includes dominating operators
defined one sub-workflows dominate. operator
defined name labelling function (o), data types e E
inputs outputs, direct sub-workflows dominating operator.
E set available data types appear DM workflow, namely
data types various I/O objects appear DM workflow
models, datasets, attributes, etc.
graph structure DM workflow pair (O , E ), contains subworkflows any. set vertices correspond operators used
DM workflow sub-workflow(s), E E set pairs nodes, (oi , oj ),
directed edges, correspond data types output/input objects,
passed operator oi operator oj . Following graph structure, topological sort
DM workflow permutation vertices graph edge (oi , oj )
implies oi appears oj , i.e. complete ordering nodes directed
acyclic graph given node sequence:
wl = [o1 , .., ol ]
611

(1)

fiNguyen, Hilario & Kalousis

Legend
input / output edges
sub input / output edges
X
X

End

Retrieve

basic nodes
output

composite nodes

result
X-Validation
Split

training set

Join

test set

Weight Information Gain

weights
example set
Apply Model
performance
Select Weights
labelled data
training set
Performance
Naive Bayes
model

Figure 2: Example DM workflow performance estimation combination
feature selection classification algorithms.

subscript wl denotes length l (i.e. number operators) topological
sort. topological sort DM workflow structurally represented rooted,
labelled ordered tree (Bringmann, 2004; Zaki, 2005), depth-first search
graph structure maximum depth given expanding recursively subworkflows dominating operators. Thus topological sort workflow tree
representation reduction original directed acyclic graph nodes
edges fully ordered.
example hierarchical DAG representing RapidMiner DM workflow given
Figure 2. graph corresponds DM workflow cross-validates feature selection
method followed classification model building step NaiveBayes classifier. XValidation typical example dominating operator workflow
two basic blocks, training block arbitrary workflow receives
input dataset outputs model, testing block receives input model
dataset outputs performance measure. particular, training subworkflow, feature weights computed InformationGain operator,
number features selected SelectByWeights operator, followed
final model building NaiveBayes operator. testing block, typical
sub-workflow consists application learned model testing set
ApplyModel operator, followed performance estimation given Performance
operator. topological sort graph given ordered tree given Figure 3.
612

fiUsing Meta-mining Support DM Workflow Planning Optimization

Retrieve
1

Weight
Information Gain
3

Select
Weights
4

X-Validation
2/8

Naive
Bayes
5

End
9

Apply
Model
6

Performance
7

Figure 3: topological order DM workflow given Figure 2.

3.4 Workflow Planning
workflow planner use based work Kietz et al. (2009, 2012), designs
DM workflows using hierarchical task network (HTN) decomposition crisp-dm process model (Chapman et al., 2000). However planner workflow enumeration
generating possible plans, i.e. consider expected workflow performance
planning process scale large set operators explode
search space. order address limitations, presented preliminary results
planner coupled frequent pattern meta-mining algorithm
nearest-neighbor algorithm rank partial workflows step workflow planning (Nguyen et al., 2011, 2012a). system deployed Kietz et al. (2012).
approach followed prioritize partial workflows according support
frequent patterns contained achieved set workflows performed
well set datasets similar input dataset want plan
workflow. However learning associations dataset workflow
characteristics, approach follow here.
section 3.4.1 briefly describe HTN planner Kietz et al. (2009, 2012);
section 3.4.2 describe use prediction expected performance
partial-workflow applied given dataset guide HTN planner. give
complete description meta-mining module learns associations dataset
workflow characteristics expected achieve high performance section 4.
3.4.1 HTN Planning
Given goal g G, AI-planner decompose top-down manner goal
elements two sets, tasks methods . task achieve g,
subset associated methods share data input/output
(I/O) object specification address it. turn, method
defines sequence operators, and/or abstract operators (see below), and/or sub-tasks,
executed order achieve m. recursively expanding tasks, methods
operators given goal g, AI-planner sequentially construct HTN plan
terminal nodes correspond operators, non-terminal nodes HTN
task method decompositions, dominating operators (X-Validation instance
dominate training testing sub-workflows). example HTN plan given
613

fiNguyen, Hilario & Kalousis

EvaluateAttributeSelectionClassification
X-Validation
In: Dataset
Out: Performance

AttributeSelection
ClassificationTraining

Classification
Training

AttributeSelection
Training

Attribute
Weighting
Operator
In: Dataset
Out: AttributeWeights

Select
Weights
In: Dataset
In: AttributeWeights
Out: Dataset

Predictive
Supervised
Learner
In: Dataset
Out: Predictive
Model

AttributeSelection
ClassificationTesting

Model
Application
Apply
Model
In: Dataset
In: Predictive
Model
Out: Labelled
Dataset

Model
Evaluation
In: Labelled
Dataset
Out: Performance

Figure 4: HTN plan DM workflow given Figure 2. Non-terminal nodes
HTN tasks/methods, except dominating operator X-Validation. Abstract
operators bold simple operators italic, annotated
I/O specification.

Figure 4. plan corresponds feature selection classification workflow
Figure 2 exactly topological sort (of operators) given Figure 3.
sets goals G, tasks , methods , operators O, relations,
described dmwf ontology (Kietz et al., 2009). There, methods operators
annotated pre- post-conditions used AI-planner.
Additionally, set operators enriched shallow taxonomic view
operators share I/O object specification grouped common
ancestor:
= {o1 , . . . , }

(2)

defines abstract operator, i.e. operator choice point alternative among set
n syntactically similar operators. example, abstract AttributeWeighting operator
given Figure 4 include feature weighting algorithms InformationGain
ReliefF, similarly abstract Predictive Supervised Learner operator contain
classification algorithms NaiveBayes linear SVM.
HTN planner plan operators applied attribute, e.g.
continuous attribute normalization operator, discretization operator. uses cyclic
planning structures apply subsets attributes. case use attributegrouping functionality require use cyclic planning structures.
precisely operator selected application applied appropriate
attributes. Overall HTN grammar contains descriptions 16 different tasks
614

fiUsing Meta-mining Support DM Workflow Planning Optimization

100 operators, planner plan. numbers limited
modeling effort required describe tasks operators, inherent
limitation HTN grammar/planner. Kietz et al. (2012) developed module
open-source ontology editor Protege called E-ProPlan2 facilitate modelling
new operators describe task-method decomposition grammars DM problems
dmwf ontology.
3.4.2 Workflow Selection Task
AI-planner designs construction HTN plan several partial workflows,
derived substituting abstract operator one n
operators. number planned workflows order several thousands
leave user loss workflow choose her/his problem;
even worse, possible planning process never succeeds find valid plan
terminate due complexity search space.
support user selection DM workflows, use post-planning approach designed workflows evaluated according evaluation
measure order find k ones globally best given mining problem. However, global approach computational expensive planning
phase mentioned above. Instead, follow planning approach
locally guide AI-planner towards design DM workflows optimized
given problem, avoiding thus need explore whole planning search space.
Clearly, following local approach cost one pay potential reduction
performance since workflows explored evaluated, potentially missing good
ones due greedy nature plan construction.
adopt heuristic hill climbing approach guide planner. abstract
operator need determine n candidate operators expected
achieve best performance given dataset. formally, define by:
Cl := {wlo = [wl1 Sl1 |o O]}kn

(3)

set k n partial candidate workflows length l generated
expansion abstract operator adding one n candidate operators
one k candidate workflows length l 1 constitute set Sl1 workflows
selected previous planning step (S0 empty set see below). let xu vector
description input dataset want plan workflows address
data mining goal g optimize performance measure. Moreover let wclo binary
vector provides propositional representation wlo workflow respect
set generalized relational frequent workflow patterns contains3 . construct
set Sl selected workflows current planning step according to:
Sl := {arg max r(xu , wclo |g)}k

(4)

{wlo Cl }

2. Available http://www.e-lico.eu/eproplan.html
3. provide detailed description extract wclo descriptors section 4.1.2,
moment note propositional representations fixed length representations,
depend l.

615

fiNguyen, Hilario & Kalousis

r(xu , wclo |g) estimated performance workflow propositional description wclo applied dataset description xu , i.e. planning step
select best current partial workflows according estimated expected performance. meta-mining model, learn past experiments, delivers
estimations expected performance. section 4.2 describe derive
meta-mining models use get estimates expected performance.
stress producing performance estimate r(xu , wclo |g)
meta-mining model uses workflow description wclo , candidate operator descriptions. pattern-based descriptions capture dependencies interactions
different operators workflows (again wclo representation
section 4.1.2). rather crucial point since well known fact construct data mining workflows need consider relations biases different
algorithms use within them, bias combinations better others.
pattern based descriptions use provide precisely type information, i.e.
information operator combinations appearing within workflows.
next section provide complete description meta-miner module,
including characterize datasets, workflows performance latter applied
former, course learn meta-mining models use planning.

4. Meta-Miner
meta-miner component operates two modes. offline mode learns
past experimental data meta-mining model provides expected performance
estimates r(xu , wclo |g). on-line mode interacts AI-planner step
planning process, delivering r(xu , wclo |g) estimates used planner
select workflows planning step according Eq.(4).
rest section organised follows. subsection 4.1.1 explain
describe datasets, i.e. derive xu dataset descriptors; subsection 4.1.2
derive propositional representation wclo data mining workflows subsection 4.1.3 rank workflows according performance given dataset, i.e.
r(xu , wclo |g). Finally subsection 4.2 explain build models past mining
experiments provide expected performance estimations r(xu , wclo |g)
use models within planner.
4.1 Meta-Data Performance Measures
section describe meta-data, namely dataset workflow descriptions,
performance measures used meta-miner learn meta-mining models
associate dataset workflow characteristics view planning DM workflows
optimize performance measure.
4.1.1 Dataset Characteristics
idea characterize datasets full-fledged research problem early
inception meta-learning (Michie, Spiegelhalter, Taylor, & Campbell, 1994;
Kopf, Taylor, & Keller, 2000; Pfahringer, Bensusan, & Giraud-Carrier., 2000; Soares &
616

fiUsing Meta-mining Support DM Workflow Planning Optimization

Brazdil, 2000; Hilario & Kalousis, 2001; Peng, Flach, Soares, & Brazdil, 2002; Kalousis,
Gama, & Hilario, 2004). Following state-of-art dataset characteristics, characterize
dataset x X three following groups characteristics.
Statistical information-theoretic measures: group refers data characteristics
defined STATLOG (Michie et al., 1994; King, Feng, & Sutherland, 1995)
METAL projects4 (Soares & Brazdil, 2000), includes number instances,
number classes, proportion missing values, proportion continuous / categorical
features, noise signal ratio, class entropy, mutual information. mainly describe
attribute statistics class distributions given dataset sample.
Geometrical topological measures: group concerns new measures try
capture geometrical topological complexity class boundaries (Ho & Basu,
2002, 2006), includes non-linearity, volume overlap region, maximum Fishers
discriminant ratio, fraction instance class boundary, ratio average intra/inter
class nearest neighbour distance.
Landmarking model-based measures: group related measures asserted
fast machine learning algorithms, called landmarkers (Pfahringer et al., 2000),
derivative based learned models (Peng et al., 2002), includes error
rates pairwise 1p values obtained landmarkers 1NN DecisionStump,
histogram weights learned Relief SVM. extended last group
new landmarking methods based weight distribution feature weighting
algorithms Relief SVM build twenty different histogram
representations discretized feature weights.
Overall, system makes use total = 150 numeric characteristics describe
dataset. denote vectorial representation dataset x X xu .
far exhaustive dataset characteristics used, including
characteristics subsampling landmarks (Leite & Brazdil, 2010). main goal
work produce comprehensive set dataset descriptors design DM
workflow planning system given set dataset characteristics, coupled workflow
descriptors, plan DM workflows optimize performance measure.
4.1.2 Workflow Characteristics
seen section 3.3 workflows graph structures quite complex
containing several nested sub-structures. often difficult analyze
spaghetti-like structure information
subtask addressed workflow component (Van der Aalst & Giinther,
2007). Process mining addresses problem mining generalized patterns
workflow structures (Bose & der Aalst, 2009).
characterize DM workflows follow process mining approach;
extract generalized, relational, frequent patterns tree representations,
use derive propositional representations them. possible generalizations
4. http://www.metal-kdd.org/

617

fiNguyen, Hilario & Kalousis

DM-Algorithm
DataProcessing
Algorithm

PredictiveModelling
Algorithm

FeatureWeighting
Algorithm

ClassificationModelling
Algorithm

LearnerFreeFW
Algorithm

UnivariateFW
Algorithm

MultivariateFW
Algorithm

MissingValues
Tolerant
Algorithm

Irrelevant
Tolerant
Algorithm

ExactCOS
Based
Algorithm

C4.5

Naive
Bayes

SVM

EntropyBasedFW
Algorithm

IG

ReliefF
is-followed-by

is-implemented-by

Figure 5: part dmops algorithm taxonomies. Short dashed arrows represent
is-followed-by relation DM algorithms, long dashed arrows represent is-implemented-by relation DM operators DM algorithms.

(a)

(b)

X-Validation

(c)

X-Validation

Feature
Weighting
Algorithm

Classification
Modeling
Algorithm

Feature
Weighting
Algorithm

Classification
Modeling
Algorithm

UnivariateFW
Algorithm

Irrelevant
Tolerant
Algorithm

LearnerFreeFW
Algorithm

MissingValues
Tolerant
Algorithm

EntropyBasedFW
Algorithm

X-Validation
Feature
Weighting
Algorithm

Classification
w
Algorithm
ExactCOS
Based
Algorithm

Figure 6: Three workflow patterns cross-level concepts. Thin edges depict workflow
decomposition; double lines depict dmops concept subsumption.

described domain knowledge which, among knowledge, given data
mining ontology. use Data Mining Optimization (dmop) ontology (Hilario
et al., 2009, 2011). Briefly, ontology provides formal conceptualization DM
domain describing DM algorithms defining relations terms DM tasks,
models workflows. describes learning algorithms C4.5, NaiveBayes SVM,
according learning behavior bias/variance profile, sensitivity
type attributes, etc. instance, algorithms cited tolerant irrelevant attributes, C4.5 NaiveBayes algorithms tolerant missing values,
whereas SVM NaiveBayes algorithms exact cost function. Algorithm characteristics families classified taxonomies dmop primitive
618

fiUsing Meta-mining Support DM Workflow Planning Optimization

concept DM-Algorithm. Moreover, dmop specifies workflow relations, algorithm order,
is-followed-by relation relates workflow operators DM algorithms
is-implemented-by relation. Figure 5 shows snapshot dmops algorithm taxonomies ground operators bottom related DM algorithms implement.
mine generalized relational patterns DM workflows, follow method
presented Hilario et al. (2011). First, use dmop ontology annotate set W
workflows. Then, extract set generalized patterns using frequent pattern
mining algorithm. Concretely, operator contained parse tree training DM
workflow wl W , insert tree branch operator taxonomic concepts,
ordered top bottom, implemented operator, given
dmop. result new parse tree additional nodes dmops
concepts. call parse tree augmented parse tree. reorder nodes
augmented parse tree satisfy dmops algorithm taxonomies relations.
example feature selection algorithm typically composed feature weighting algorithm
followed decision rule selects features according heuristics. result
set augmented reordered workflow parse trees. representation,
apply tree mining algorithm (Zaki, 2005) extracts set P frequent patterns.
pattern corresponds tree appears frequently within augmented parse trees;
mine patterns support higher equal five. principle go
low support one, exploding dimensionality description workflows,
probably features poor discriminatory power. Nevertheless since metamining models rely metric learning, able learn importance different
meta-features, would able cope scenario. Note extract
workflow characteristics could used different techniques graph
mining directly graph structures defined workflows ontology,
main reason computational cost latter approaches, well
fact frequent pattern mining propositionalization known work well.
Figure 6 give examples mined patterns. Note extracted patterns
generalized, sense contain entities defined different abstraction levels,
provided dmop ontology. relational describe
relations, order relations, structures appear within workflow,
contain properties entities described dmop ontology.
example pattern (c) Figure 6 states feature weighting algorithm (abstract
concept) followed (relation) classification algorithm exact cost function
(property), within cross-validation.
use set P frequent workflow patterns describe DM workflow wl W
patterns p P wl workflow contains. propositional description
workflow wl given |P |-length binary vector:
wcl = (I(p1 wl ), . . . , I(p|P | wl ))T {0, 1}|P |

(5)

denotes induced tree relation (Zaki, 2005) I(pi wl ) returns one
frequent pattern, pi , appears within workflow zero otherwise.
use propositional workflow representation together tabular representation datasets characteristics learn meta-mining models describe
next section. Although could used tree even graph properties represent
619

fiNguyen, Hilario & Kalousis

workflows, propositionalization standard approach used extensively successfully
learning problems learning instances complex structures (Kramer, Lavrac,
& Flach, 2000).
Also, propositional workflow representation easily deal parameter values
different operators appear within workflows. so, discretize
range values continuous parameter ranges low, medium, high,
ranges depending nature parameter, treat discretized values
simply property operators. resulting patterns parameter-aware;
include information parameter range mined operators
used support parameter setting planning DM workflows.
However within paper explore possibility.
4.1.3 Performance-Based Ranking DM Workflow
characterize performance number workflows applied given dataset
use relative performance rank schema derive using statistical significance
tests. Given estimations performance measure different workflows
given dataset use statistical significance test compare estimated performances
every pair workflows. within given pair one workflows significantly
better gets one point gets zero points.
significance difference workflows get half point. final performance rank
workflow given dataset simply sum points pairwise
performance comparisons, higher better. denote relative performance
rank workflow wc applied dataset xu r(xu , wc ). Note workflow
applicable, executed, dataset x, set rank score default value zero
means workflow appropriate (if yet executed) given dataset.
planning goal g classification task, use evaluation measure
experiments classification accuracy, estimated ten-fold cross-validation,
significance testing using McNemars test, significance level 0.05.
next section describe build meta-mining models
past data-mining experiments using meta-data performance measures
described far use models support DM workflow planning.
4.2 Learning Meta-mining Models Workflow Planning
starting describe detail build meta-mining models let us take
step back give abstract picture type meta-mining setting
address. previous sections, described two types learning instances: datasets
x X workflows w W. Given set datasets set workflows stored
dmer, meta-miner build these, two training matrices X W.
X : n dataset matrix, ith row description xui ith dataset.
W : |P | workflow matrix, j th row description wcj jth workflow.
preference matrix R : n m, Rij = r(xui , wcj ), i.e. gives
relative performance rank workflow wj applied dataset xi respect
workflows. see Rij measure appropriateness match
wj workflow xi dataset. ith line R matrix contains vector
620

fiUsing Meta-mining Support DM Workflow Planning Optimization

relative performance ranks workflows applied xui dataset.
meta-miner take input X, W R matrices output model
predicts expected performance, r(xu , wc ), workflow w applied dataset x.
construct meta-mining model using similarity learning, exploiting two basic
strategies initially presented context DM workflow selection (Nguyen et al., 2012b).
give high level presentation them, details interested user
refer original paper. first strategy learn homogeneous similarity
measures, measuring similarity datasets similarity workflows, use
derive r(xu , wc |g) estimates. second learn heterogeneous similarity measures
directly estimate appropriateness workflow dataset, i.e. produce
direct estimates r(xu , wc |g).
4.2.1 Learning Homogeneous Similarity Measures
goal provide meta-mining models good predictors performance
workflow applied dataset. simplest approach want learn good
similarity measure dataset space deem two datasets similar set
workflows applied result similar relative performance, i.e.
order workflows according performance achieve dataset
two datasets similar workflow orders similar. Thus learned similarity
measure dataset space good predictor similarity datasets
determined relative performance order workflows. completely
symmetrical manner consider two workflows similar achieve similar
relative performance scores set datasets. Thus case workflows learn
similarity measure workflow space good predictor similarity
relative performance scores set datasets.
Briefly, learn two Mahalanobis metric matrices, MX , MW , datasets
workflows respectively, optimizing two following convex metric learning optimization
problems:
min F1 = ||RRT XMX XT ||2F + tr(MX )
MX

s.t.

(6)

MX 0


min F2 = ||RT R WMW WT ||2F + tr(MW )
MW

s.t.

(7)

MW 0

||.||F Frobenius matrix norm, tr() matrix trace, 0 parameter
controlling trade-off empirical error metric complexity control overfitting. RRT : n n matrix reflects similarity relative workflow performance
vectors different dataset pairs learned dataset similarity metric
reflect. RT R : matrix gives respective similarities workflows.
details learning problem solve it, see work Nguyen et al.
(2012b).
621

fiNguyen, Hilario & Kalousis

Note far model computes expected relative performance
r(xu , wclo ). case homogeneous metric learning compute on-line
mode planning phase; describe right away following
paragraph.
Planning homogeneous similarity metrics (P1) use two
learned Mahalanobis matrices, MX , MW , compute dataset similarity workflow similarity finally compute estimates r(xu , wclo ) planning
step.
Concretely, prior planning determine similarity input dataset xu (for
want plan optimal DM workflows) training datasets xui X using
MX dataset metric measure dataset similarities. Mahalanobis similarity
two datasets, xu , xui , given
sX (xu , xui ) = xTu MX xui

(8)

Then, planning planning step determine similarity candidate
workflow wlo Cl training workflows wcj W,
sW (wclo , wcj ) = wcTlo MW wcj .

(9)

Finally derive r(xu , wclo ) estimate weighted average elements
R matrix. weights given similarity input dataset xu
training datasets, similarities candidate workflow wclo training
workflows. formally expected rank given by:
P
P
wcj W xui wcj r(xui , wcj |g)
xui X
P
P
(10)
r(xu , wclo |g) =
wc W xui wcj
xu X
j



xui wcj Gaussian weights given xui = exp(sX (xu , xui )/x ) wcj =
exp(sW (wclo , wcj )/w ); x w kernel widths control size neighbors
data workflow spaces respectively (Smart & Kaelbling, 2000; Forbes & Andre,
2000).
Using rank performance estimates delivered Eq.(10), select planning step best candidate workflows set, Sl , according Eq.(4). call resulting
planning strategy P1. P1 expected performance set selected candidate
workflows Sl greedily increases deliver k DM complete workflows
expected achieve best performance given dataset.
4.2.2 Learning Heterogeneous Similarity Measure
P1 planning strategy makes use two similarity measures learned independently other, one defined feature space. simplistic
assumption model interactions workflows datasets,
know certain types DM workflows appropriate datasets certain
types characteristics. order address limitation, define heterogeneous
metric learning problem directly estimate similarity/appropriateness
622

fiUsing Meta-mining Support DM Workflow Planning Optimization

workflow given dataset given r(xu , wc ) relative performance
measure.
Since learning Mahalanobis metric equivalent learning linear transformation
rewrite two Mahalanobis metric matrices described previously MX = UUT
MW = VVT . U : V : |P | respective linear transformation matrices
dimensionality = min(rank(X), rank(W)).
learn heterogeneous similarity
measure datasets workflows using two linear transformations solve
following optimization problem:
min F4 = ||R XUVT WT ||2F + ||RRT XUUT XT ||2F
U,V

+ ||RT R WVVT WT ||2F +

(11)


(||U||2F + ||V||2F )
2

using alternating gradient descent algorithm, first optimize U keeping
V fixed vice versa. optimization problem non-convex algorithm
converge local minimum. first term similar low-rank matrix factorization
Srebro, Rennie, Jaakkola (2005). However factorization learn
function dataset workflow feature spaces result address
samples training instances, known cold start problem
recommendation systems. case DM workflow planning problem strong
requirement need able plan workflows datasets never
seen training, able qualify workflows seen
training. second third terms define metrics reflect performancebased similarities datasets workflows respectively (along lines homogeneous
metrics given previously), together give directly similarity/appropriateness
DM workflow dataset estimating expected relative predictive performance
as:
r(xu , wclo |g) = xu UVT wcTlo

(12)

see heterogeneous similarity metric performing projection dataset
workflow spaces common latent space compute standard similarity
projections. details, see work Nguyen et al. (2012b).
Planning heterogeneous similarity measure (P2) Planning heterogeneous similarity measure, strategy denote P2, much simpler planning homogeneous similarity measures. Given input dataset described xu
step planning make use relative performance estimate r(xu , wclo |g)
delivered Eq.(12) select set best workflows Sl set partial workflows Cl using selection process described Eq.(4). Unlike planning strategy P1
computes r(xu , wclo |g) weighted average help two independently learned similarity metrics, P2 relies heterogeneous metric directly computes
r(xu , wclo |g), modeling thus explicitly interactions dataset workflow characteristics.
note P1 P2 planning strategies able construct
workflows even pools ground operators include operators
never experimented baseline experiments, provided operators well
623

fiNguyen, Hilario & Kalousis

described within dmop. meta-mining models planner uses
prioritize workflows rely wclo descriptions workflow generalized
descriptions workflows operators.
next section evaluate ability two planning strategies
introduced plan DM workflows optimize predictive performance compare
number baseline strategies different scenarios.

5. Experimental Evaluation
evaluate approach data mining task classification. reasons
rather practical. Classification supervised task means
ground truth compare results produced classification
algorithm, using different evaluation measures accuracy, error, precision etc;
mining tasks clustering, performance evaluation comparison bit
problematic due lack ground truth. extensively studied,
extensively used many application fields, resulting plethora benchmark datasets,
easily reuse construct base-level experiments well evaluate
system. Moreover, extensively addressed context meta-learning,
providing baseline approaches compare approach. Finally approach
requires different algorithms operators use well described
dmop ontology. Due historical predominance classification task algorithms
well extensive use real world problems, started developing dmop them;
result task classification corresponding algorithms well described.
said this, emphasize approach limited task
classification. applied mining task define evaluation
measure, collect set benchmark datasets perform base-level experiments, provide descriptions task respective algorithms dmop.
train evaluate approach, collected set benchmark classification
datasets. applied set classification data mining workflows.
base-level experiments learn meta-mining models used
planner plan data mining workflows. challenge system new datasets
used training meta-mining models, datasets
plan new classification workflows achieve high level predictive performance.
explore two distinct evaluation scenarios. first one, constrain
system plans DM workflows selecting operators restricted operator
pool, namely operators experimented base-level experiments.
Thus operators characterized dmop ontology tested
base-level experiments; call tested operators. second scenario
allow system choose operators never experimented
characterized dmop ontology; call operators untested
operators. goal second scenario evaluate extend system
effectively use untested operators workflows designs.
624

fiUsing Meta-mining Support DM Workflow Planning Optimization

Type
FS/tested
FS/tested
FS/tested

Abbr.
IG
CHI
RF

Parameters
-#features selected k = 10
-#features selected k = 10
-#features selected k = 10

SVMRFE

-#features selected k = 10

FS/untested
CL/tested

Operator
Information Gain
Chi-Square
ReliefF
Recursive feature
elimination SVM
Information Gain Ratio
One-nearest-neighbor

IGR
1NN

-#features selected k = 10

CL/tested

C4.5

C4.5

CL/tested

CART

CART

FS/tested

CL/tested
CL/tested

NaiveBayes normal
density estimation
Logistic regression
Linear kernel SVM

CL/tested

Gaussian kernel SVM

SVMr

CL/untested
CL/untested
CL/untested
CL/untested

Linear discriminant analysis
Rule induction
Random decision tree
Perceptron neural network

LDA
Ripper
RDT
NNet

CL/tested

-pruning confidence C
-min. inst. per leaf
-pruning confidence C
-min. inst. per leaf

= 0.25
=2
= 0.25
=2

NB
LR
SVMl

-complexity C = 1
-complexity C = 1
-gamma = 0.1

Table 2: Table operators used design DM workflows 65 datasets. type
corresponds feature selection (FS) classification (CL) operators. operators experimented marked tested, otherwise untested.

5.1 Base-Level Datasets DM Workflows
construct base-level experiments, collected 65 real world datasets genomic
microarray proteomic data related cancer diagnosis prognosis, mostly
National Center Biotechnology Information5 . typical datasets,
datasets use characterized high-dimensionality small sample size,
relatively low number classes, often two. average 79.26 instances,
15268.57 attributes, 2.33 classes.
build base-level experiments, applied datasets workflows consisted either single classification algorithm, combination feature selection
classification algorithm. Although HTN planner use (Kietz et al., 2009, 2012)
able generate much complex workflows, 16 different tasks, 100
operators, limit planning classification, feature selection
classification, workflows simply respective tasks, algorithms operators
well annotated dmop. annotation important characterization
workflows construction good meta-mining models used guide
planning. Nevertheless, system directly usable planning scenarios com5. http://www.ncbi.nlm.nih.gov/

625

fiNguyen, Hilario & Kalousis

plexity, describe HTN grammar, provided appropriate tasks,
algorithms operators annotated dmop ontology.
used four feature selection algorithms together seven classification algorithms
build set base-level training experiments. given Table 2, noted
tested. mentioned previously, plan operators parameters
discretizing range values parameters treating properties
operators. Another alternative use inner cross-validation automatically select
set parameter values; strictly speaking, case, would selecting
standard operator cross-validated variant. Nevertheless, would incur significant
computational cost.
Overall, seven workflows contained classification algorithm,
28 workflows combination feature selection classification algorithm,
resulting total 35 workflows applied 65 datasets corresponds 2275 baselevel DM experiments. performance measure use accuracy estimate
using ten-fold cross-validation. algorithms, used implementations provided
RapidMiner DM suite (Klinkenberg et al., 2007).
already said, two evaluation settings. first, Scenario 1, constrain
system plan workflows using tested operators. second, Scenario 2,
allow system select untested operators. additional operators
given Table 2, denoted untested. total number possible workflows
setting 62.
5.2 Meta-learning & Default Methods
compare performance system two baseline methods default
strategy. two baseline methods simple approaches fall classic metalearning stream instead selecting individual algorithms select
workflows. Thus cannot plan DM workflows used setting
workflows choose seen model construction phase.
first meta-learning method use, call Eucl, standard
approach meta-learning (Kalousis & Theoharis, 1999; Soares & Brazdil, 2000),
makes use Euclidean based similarity dataset characteristics select N
similar datasets input dataset xu want select workflows
averages workflow rank vectors produce average rank vector:
N
1 X
rxui , xui {arg max xTu xui }N
N
xui X

(13)



uses order different workflows. Thus method simply ranks workflows
according average performance achieve neighborhood input
dataset. second meta-learning method call Metric makes use learned
dataset similarity measure given Eq.(8) select N similar datasets
input dataset averages well respective workflow rank vectors:
N
1 X
rxui , xui {arg max xTu MX xui }N
N
xui X


626

(14)

fiUsing Meta-mining Support DM Workflow Planning Optimization

0.8

0.6

0.4

0.2

CHI+C45
RF+NBN
SVMRFE+C45
CHI+CART
RF+C45
SVMr
1NN
IG+C45
RF+1NN
SVMRFE+CART
C45
CHI+SVMr
RF+CART
CHI+SVMl
IG+CART
RF+SVMr
RF+SVMl
SVMRFE+1NN
CHI+1NN
IG+1NN
SVMRFE+SVMr
CART
CHI+NBN
IG+SVMr
SVMRFE+LR
CHI+LR
RF+LR
SVMRFE+NBN
IG+SVMl
SVMl
IG+LR
SVMRFE+SVMl
NBN
IG+NBN
LR

0.0

Figure 7: Percentage times workflow among top-5 workflows different datasets.

default recommendation strategy, simply use average rxui workflow
rank vectors collection training datasets:
1X
rxui , xui X
n
n

(15)



rank select workflows. note rather difficult baseline
beat. see case plot Figure 7 percentage times
35 DM workflows appears among top-5 worfklows 65 datasets. top
workflow, consisting LR algorithm, among top-5 80%
datasets. next two workflows, NBN IG NBN, among top-5
almost 60% datasets. words select top-5 workflows using default
strategy roughly 80% datasets LR correctly them,
NBN IG NBN percentage around 60%. Thus set dataset
quite similar respect workflows perform better them, making
default strategy rather difficult one beat.
5.3 Evaluation Methodology
estimate performance planned workflows evaluation scenarios
use leave-one-dataset-out, using time 64 datasets build meta-mining
models one dataset plan.
evaluate method measuring well list, L, top-k ranked workflows, delivers given dataset, correlates true list, , top-k ranked
627

fiNguyen, Hilario & Kalousis

workflows dataset using rank correlation measure. place true quotes
general case, i.e. restrict choice operators specific
set, cannot know true best workflows unless exhaustively examine
exponential number them, however since select restricted list operators
set best. precisely, measure rank correlation
two lists L , use Kendall distance p penalty, denote
K (p) (L, ) (Fagin, Kumar, & Sivakumar, 2003). Kendall distance gives number
exchanges needed bubble sort convert one list other. assigns penalty
p pair workflows one workflow one list other;
set p = 1/2. K (1/2) (L, ) normalized, propose define normalized
Kendall similarity Ks(L,T) as:
1

K ( 2 ) (L, )
Ks(L, ) = 1
u

(16)

1

(2)

Pktakes values [0, 1]. u upper bound K (L, ) given u = 0.5k(5k + 1)
2 i=1 i, derived direct application lemma 3.2 work Fagin et al. (2003),
assume two lists share element. qualify method,
m, including two baselines, Kendall similarity gain, Kg(m), i.e. gain (or loss)
achieves respect default strategy given datasets, compute as:

Kg(m)(L, ) =

Ks(m)(L, )
1
Ks(def )(L, )

(17)

method, report average Kendall similarity gain overall datasets,
Kg(m). Note that, Scenario 1, default strategy based average ranks
35 workflows. Scenario 1, default strategy based average ranks 62
workflows, experiment order set baseline.
addition see well top-k ranked list workflows, given method
suggests given dataset, correlates true list, compute average accuracy
top-k workflows suggests achieve given dataset, report average
overall datasets.
5.4 Meta-mining Model Selection
iteration leave-one-dataset-out evaluation planning performance,
rebuild meta-mining model tune parameter Mahalanobis metric
learning using inner ten-fold cross-validation; select value maximizes
Spearman rank correlation coefficient predicted workflow rank vectors
real rank vectors. heterogenous metric, used parameter setting defined
Nguyen et al. (2012b). two meta-learning methods, fixed number N
nearest neighbors five, reflecting prior belief appropriate neighborhood size.
planning, set manually dataset kernel width parameter kx = 0.04
workflow kernel width parameter kw = 0.08 result small dataset workflow
neighborhoods respectively. Again, two parameters tuned simply set
prior belief respective neighborhood size.
628

fiUsing Meta-mining Support DM Workflow Planning Optimization

(b) Scenario 2, tested untested operators.

0.10

10

15

20

25

30

Kg

0.00

0.05

5

0.10
0.15

0.15

0

P2
P1
def62

0.05

0.00
0.05

P2
P1
Metric
Eucl
def32

0.10

Kg

0.05

0.10

(a) Scenario 1, tested operators.

35

0

k

5

10

15

20

25

30

35

k

Figure 8: Average correlation gain Kg different methods baseline
65 bio-datasets. x-axis, k = 2 . . . 35, number top-k workflows
suggested user. P1 P2 two planning strategies. Metric
Eucl baseline methods defX default strategy computed set
X workflows.

5.5 Experimental Results
following sections give results experimental evaluation different
methods presented far two evaluation scenarios described above.
5.5.1 Scenario 1, Building DM workflows Pool Tested
Operators
scenario, evaluate quality DM workflows constructed two
planning strategies P1 P2 compare two baseline methods well
default strategy. leave-one-dataset-out evaluate workflow
recommendations given method. Figure 8(a) give average Kendall gain
Kg method default strategy compute top-k lists
k = 2, . . . , 35. Clearly P2 strategy one gives largest improvements
respect default strategy, 5% 10% gain, compared method.
establish statistical significance results k, counting number
datasets method better/worse default, using McNemars
test. summarize Figure 9 statistical significance results given p-values
different ks give detailed results Table 4 appendix methods
k = 2 . . . 35. see P 2 far best method significantly better
default 16 34 values k, close significant (0.05 < p-value 0.1)
ten 34 times never significantly worse. methods, P2
managed beat default 2 34 cases k.
629

fiNguyen, Hilario & Kalousis

0.5
5

10

15

20

25

30

35

5

10

15

20

25

30

Metric (0 Wins/0 Losses)

Eucl (0 Wins/0 Losses)

35

0.5
0.0
1.0

1.0

0.5

0.0

pvalue

0.5

1.0

k

1.0

k

0.5

pvalue

0.0

pvalue

1.0

0.5

0.0
1.0

0.5

pvalue

0.5

1.0

P1 (2 Wins/0 Losses)

1.0

P2 (16 Wins/0 Losses)

5

10

15

20

25

30

35

5

k

10

15

20

25

30

35

k

Figure 9: P-values McNemars test number times Kendal similarity
method better default given k, Scenario 1. positive pvalue means wins losses, negative opposite. solid lines
p = +/ 0.05, dash-dotted p = +/ 0.1. X Wins/ Losses
header indicates number times k = 3..35 method
significantly better/worse default.

examine average accuracy top-k workflows suggested
method achieve, advantage P2 even striking. average accuracy 1.25%
1.43% higher default strategy, k = 3 k = 5 respectively, see
Table 3(a). k = 3, P2 achieves higher average accuracy default 39
65 datasets, under-performs compared default 20. Using
McNemars test statistical significance 0.02, i.e. P2 significantly better
default strategy comes average accuracy top k = 3 workflows
plans; results similar k = 5. fact eight top-k lists, k = 3 . . . 10, P2
significantly better default five values k, close significantly better once,
never significantly worse. higher values k, k = 11 . . . 35, significantly
better 11 times, close significantly better three times, never significantly worse.
stops significantly better k > 30. large k values, average taken
almost workflows, thus expect important differences lists
produced different methods. Figure 10, visualize statistical significance
results different values k = 3 . . . 35 give detailed results Table 5
Appendix.
630

fiUsing Meta-mining Support DM Workflow Planning Optimization

0.5
5

10

15

20

25

30

35

5

10

15

20

25

30

Metric (4 Wins/0 Losses)

Eucl (0 Wins/0 Losses)

35

0.5
0.0
1.0

1.0

0.5

0.0

pvalue

0.5

1.0

k

1.0

k

0.5

pvalue

0.0

pvalue

1.0

0.5

0.0
1.0

0.5

pvalue

0.5

1.0

P1 (6 Wins/0 Losses)

1.0

P2 (16 Wins/0 Losses)

5

10

15

20

25

30

35

5

k

10

15

20

25

30

35

k

Figure 10: P-values McNemars test number times Average Accuracy method better default given k, Scenario 1.
figure interpretation Figure 9

P1 never significantly better default k = 3 . . . 10, k = 11 . . . 35,
significantly better nine values k, close significantly better three times,
close significantly worse once. Metric baseline never significantly better
default k = 3 . . . 10, k = 11 . . . 35, significantly better four values
k,and close significantly better four times. results EC quite poor. terms
average accuracy, similar default, terms number times
performs better default, cases, less number
times performs worse default. Figure 10 give results
statistical significance results different values k detailed results
Table 5 appendix. P2 method directly learns exploits
workflow planning associations dataset workflow characteristics
experimental results clearly demonstrate strategy best pays off.
5.5.2 Scenario 2: Building DM workflows Pool Tested
Non-Tested Operators
second scenario, evaluate performance two planning strategies, P1
P2, pool available operators planning limited
operators already experimented base-level experimented with,
extended include additional operators described dmop ontology.
631

fiNguyen, Hilario & Kalousis

(a) Scenario 1, tested operators

P2
P1
Metric
Eucl
def35

Avg. Acc
0.7988
0.7886
0.7861
0.7829
0.7863

k=3
W/L
39/20
26/38
25/38
30/32

p value
+0.02
0.17
0.13
0.90

Avg. Acc
0.7925
0.7855
0.7830
0.7782
0.7787

k=5
W/L
41/21
35/28
32/33
32/33

p value
+0.02
0.45
1.00
1.00

(b) Scenario 2, tested untested operators

P2
P1
def62

Avg. Acc
0.7974
0.7890
0.7867

k=3
W/L
39/24
29/34

p value
0.08
0.61

Avg. Acc
0.7907
0.7853
0.7842

k=5
W/L
34/29
31/34

p value
0.61
0.80

Table 3: Average accuracy top-k workflows suggested method. W indicates
number datasets method achieved top-k average accuracy larger
respective default, L number datasets smaller
default. p value result McNemars statistical significance
test; + indicates method statistically better default.

already described exact setting section 5.1; reminder number
possible workflows 62. before, estimate performances using leave-onedataset-out. Note two baseline methods, Metric Eucl, applicable
setting, since deployed workflows already
experimented baseline experiments. Here, already explained section 5.3,
default strategy correspond average rank 62 possible workflows
denote def62. Note highly optimistically-biased default method
since relies execution 62 possible workflows base-level datasets, unlike
P1 P2 get see 35 workflows model building, operators
therein, plan larger pool.
Figure 8(b), give average Kendall gain Kg P1 P2 def62
baseline. Similarly first evaluation scenario, P2 advantage P1 since
demonstrates higher gains default. Note though performance gains
smaller previously. terms number k values P2
(close be) significantly better default, six eight,
different k = 2 . . . 35. def62 significantly better P2 close
significantly better. concerns P1, significant difference
performance def62, value k. values k > 30, P2 systematically
under-performs compared def62, due advantage latter comes
seeing performance 62 workflows base-level dataset. visualize
statistical significance results top row Figure 11, give detailed results
Table 6 Appendix values k = 2 . . . 35.
632

fiUsing Meta-mining Support DM Workflow Planning Optimization

0.5
10

15

20

25

30

35

5

10

15

20

25

30

k

P2 (1 Wins/13 Losses)

P2 (0 Wins/3 Losses)

35

0.5
0.0
0.5
1.0

1.0

0.5

0.0

pvalue

0.5

1.0

k

1.0

5

pvalue

0.0

pvalue

1.0

0.5

0.0
1.0

0.5

pvalue

0.5

1.0

P1 (0 Wins/0 Losses)

1.0

P2 (4 Wins/1 Loss)

5

10

15

20

25

30

35

5

k

10

15

20

25

30

35

k

Figure 11: Top row p-values McNemars test number times Kendall
similarity method better default given k, Scenario 2. Bottom
row Average Accuracy. figure interpretation Figure 9.

concerns performance P2 respect average accuracy
top-k workflows suggests, slight advantage def62 small
values k, four. significantly better compared def62 once, k = 4.
k = 5 17, two methods significant difference, k = 18 . . . 35 P2
worse, 13 times significant manner. P1 picture slightly different, average
accuracy significantly different def62, exception three k values
significantly worse. visualize statistical significance results bottom
row Figure 11, give detailed results Table 7. seems fact P2
learns directly associations datasets workflow characteristics puts
disadvantage want plan operators tested training
phase. scenario, P1 strategy weights preferences dataset similarity
workflow similarity seems cope better untested operators. Nevertheless
still possible outperform default strategy significant manner, keeping
however mind def62 optimistic default strategy based
experimentation possible workflows training dataset.
5.6 Discussion
previous sections, evaluated two workflow planning strategies two settings:
planning tested operators, planning tested untested operators.
633

fiNguyen, Hilario & Kalousis

first scenario, P2 planning strategy makes use heterogeneous metric
learning model, directly connects dataset workflow characteristics, clearly stands
out. outperforms default strategy terms Kendall Similarity gain,
statistically significant, close statistically significant, manner 24 values k
[2, . . . , 35]; terms average accuracy top-k workflows, outperforms 20
values k statistically significant, close statistically significant, manner.
methods, including P1, follow large performance difference P2.
allow planners include workflows operators
used baseline experiments, annotated dmop ontology, P2s
performance advantage smaller. terms Kendall similarity gain, statistically
significant, close to, k [10, . . . , 20]. respect average accuracy top-k
lists, better default small lists, k = 3, 4; k > 23,
fact significantly worse. P1 fairs better second scenario, however performance
different default method. Keep mind though baseline used
second scenario quite optimistic one.
fact, see able generalize plan well datasets,
evidenced good performance P2 first setting. However, comes
generalizing datasets operators case second scenario
performance planned workflows good, exception top
workflows. take look new operators added second scenario,
feature selection algorithm, Information Gain Ratio, four classification algorithms,
namely Linear Discriminant Algorithm, Ripper rule induction algorithm, Neural Net
algorithm, Random Tree. them, Information Gain Ratio seen
base level set experiments algorithm, Information Gain, rather
similar learning bias it. Ripper rule induction sequential covering algorithm,
closest operators set training operators two decision tree
algorithms recursive partitioning algorithms. respect dmop ontology,
Ripper shares certain number common characteristics decision trees, however
meta-mining model contains information set-covering learning bias performs
different datasets. might lead selected given dataset based
common features decision trees, learning bias fact appropriate
dataset. Similar observations hold algorithms, example LDA
shares number properties SVMl LR, however learning bias, maximizing
between-to-within class distances ratio, different learning biases
two, meta-mining model contains information bias associates
dataset characteristics.
Overall, extent system able plan, tested untested
operators, workflows achieve good performance, depends extend
properties latter seen training meta-mining models
within operators experimented with, well whether
unseen properties affect critically final performance. case operators
well characterized experimentally, Scenario 1, performance
workflows designed P2 strategy good. Note necessary
operators workflows applied datasets, enough sufficient set
experiments operator. heterogeneous metric learning algorithm handle
634

fiUsing Meta-mining Support DM Workflow Planning Optimization

(b)

(a)

X-Validation

X-Validation

DataProcessing
Algorithm
FWAlgorithm

ClassificationModeling DataProcessing
Algorithm
Algorithm
FWAlgorithm

HighBiasCMA

MultivariateFW
Algorithm

ClassificationModeling
Algorithm
HighVarianceCMA

UnivariateFW
Algorithm

Figure 12: Top-ranked workflow patterns according average absolute weights given
matrix V.

incomplete preference matrices, using available information. course clear
available information, whether form complete preference matrices
form extensive base-level experiments large number datasets, better
quality learned meta-mining model be. interesting explore
sensitivity heterogeneous metric learning method different levels completeness
preference matrix; however outside scope present paper.
quantify importance different workflow patterns
operators properties analyzing linear transformation workflow patterns
contained heterogeneous metric. precisely, establish learned importance
workflow pattern averaging absolute values weights assigned
different factors (rows) V linear transformation matrix Eq.(11). Note
approach, establish importance patterns, whether
associated good bad predictive performance. Figure 12, give two
important patterns determined basis averaged absolute
weights. describe relations workflow operators, first one
indicates multivariate feature weighting algorithm followed high bias
classification algorithm, second describes univariate feature weighting algorithm
followed high bias classification algorithm. systematic analysis learned model
could provide hints one focus ontology building effort, looking
important patterns well patterns used. addition,
reveal parts ontology might need refinement order distinguish
different workflows respect expected performance.

6. Conclusions Future Work
paper, presented is, best knowledge, first system
able plan data mining workflows, given task given input dataset,
expected optimize given performance measure. system relies tight
interaction hierarchical task network planner learned meta-mining model
plan workflows. meta-mining model, heterogeneous learned metric, associates
datasets characteristics workflow characteristics expected lead good
635

fiNguyen, Hilario & Kalousis

performance. workflow characteristics describe relations different components workflows, capturing global interactions various operators appear
within them, incorporate domain knowledge latter given data mining ontology (dmop). learn meta-mining model collection past base-level mining
experiments, data mining workflows applied different datasets. carefully evaluated
system task classification showed outperforms significant
manner number baselines default strategy plan operators
experimented base-level experiments. performance
advantage less pronounced consider planning operators
experimented base-level experiments, especially
properties operators present within operators
experimented base-level experiments.
system directly applicable mining tasks e.g. regression, clustering.
reasons focused classification mainly practical: extensive
annotation classification task related concepts data mining ontology,
large availability classification datasets, extensive relevant work meta-learning
dataset characterization classification. main hurdle experimenting
different mining task annotation necessary operators dmop ontology
set base-level collection mining experiments specific task. Although
annotation new algorithms operators quite labor intensive task, many
concepts currently available dmop directly usable mining tasks, e.g. cost
functions, optimization problems, feature properties etc. addition, small active
community, DMO-foundry6 , maintaining augmenting collaboratively ontology
new tasks operators, significantly reducing deployment barrier new task.
DMO-foundry web site, one find number tools templates facilitate
addition new concepts operators ontology well annotate
existing ones. said note use dmop sine-qua-non
system function. well perform workflow characterization task
mining ground operators, without using ontology. downside
would extracted patterns generalized contain operator
properties. Instead defined ground operators. Everything else remains
is.
number issues still need explore finer detail. would
gain deeper understanding better characterization reduced performance planning untested operators; example, conditions
relatively confident suitability untested operator within workflow. want
experiment strategy suggested parameter tuning, treat
parameters yet another property operators, order see whether gives better
results; expect will. want study detail level missing information
preference matrix affects performance system, well whether using
ranking based loss functions metric learning problem instead sum squares would
lead even better performance.

6. http://www.dmo-foundry.org/

636

fiUsing Meta-mining Support DM Workflow Planning Optimization

ambitious level want bring ideas reinforcement learning (Sutton
& Barto, 1998); let system design workflows systematic way
applied collection available datasets order derive even better characterizations
workflow space relate dataset space, exploring example areas
meta-mining model less confident.

Acknowledgments
work partially supported European Community 7th framework program ICT-2007.4.4 grant number 231519 e- Lico: e-Laboratory Interdisciplinary Collaborative Research Data Mining Data-Intensive Science. Alexandros
Kalousis partially supported RSCO ISNET NFT project. basic HTN planner result collaborative work within e-LICO project Jorg-Uwe Kietz,
Floarea Serban, Simon Fischer. would thank Jun Wang important contribution developing metric learning part paper. addition would
thank members AI lab, Adam Woznica, Huyen Do, Jun Wang,
significant effort placed providing content DMOP. Finally, would
thank reviewers suggestions helped improve paper.

637

fiNguyen, Hilario & Kalousis

Appendix A. Detailed Results
k
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
Total

P2
W L p-value
8
7
1
17 11 0.344
24 18 0.440
35 21 0.082
39 23 0.056
41 19 0.006
43 20 0.005
43 22 0.013
47 18 0.000
40 24 0.060
42 21 0.011
40 25 0.082
40 25 0.082
43 21 0.008
40 25 0.082
40 25 0.082
40 25 0.082
40 25 0.082
38 27 0.214
38 27 0.214
39 25 0.104
38 27 0.214
41 24 0.047
40 24 0.060
39 26 0.136
41 23 0.033
42 22 0.017
41 24 0.047
41 24 0.047
44 21 0.006
44 21 0.006
43 21 0.008
42 21 0.011
42 21 0.011
Wins 16/Losses 0

P1
W L p-value
13 13 1
17 20 0.742
18 27 0.233
24 29 0.582
29 31 0.897
33 31 0.900
36 27 0.313
33 31 0.900
35 30 0.619
30 35 0.619
34 29 0.614
33 28 0.608
38 26 0.169
38 26 0.169
37 27 0.260
35 29 0.531
35 29 0.531
34 30 0.707
37 26 0.207
35 30 0.619
37 27 0.260
35 29 0.531
36 28 0.381
36 28 0.381
36 29 0.456
38 27 0.214
38 26 0.169
39 26 0.136
39 25 0.104
40 24 0.060
42 23 0.025
41 24 0.047
39 25 0.104
40 24 0.060
Wins 2/Losses 0

Metric
W L p-value
4
11 0.121
12 16 0.570
19 27 0.302
23 27 0.671
26 35 0.305
27 37 0.260
30 34 0.707
30 33 0.801
29 35 0.531
26 38 0.169
33 31 0.900
32 33 1.000
34 31 0.804
34 30 0.707
34 31 0.804
32 32 1.000
34 31 0.804
32 33 1.000
31 32 1.000
30 35 0.619
31 34 0.804
32 33 1.000
33 31 0.900
33 32 1.000
31 34 0.804
32 32 1.000
35 29 0.531
35 30 0.619
37 28 0.321
39 26 0.136
39 26 0.136
38 27 0.214
37 27 0.260
36 28 0.381
Wins 0/Losses 0

Eucl
W L p-value
9
11 0.823
16 15 1.000
25 19 0.450
29 25 0.683
32 27 0.602
35 25 0.245
31 30 1.000
32 32 1.000
33 31 0.900
30 35 0.619
27 37 0.260
28 36 0.381
33 31 0.900
32 33 1.000
30 35 0.619
31 34 0.804
33 32 1.000
32 33 1.000
32 33 1.000
28 37 0.321
30 35 0.619
30 35 0.619
32 33 1.000
31 34 0.804
30 35 0.619
29 35 0.531
29 35 0.531
29 36 0.456
30 35 0.619
31 34 0.804
32 33 1.000
32 33 1.000
32 33 1.000
31 34 0.804
Wins 0/Losses 0

Table 4: Wins/Losses respective P-values McNemars test number
times Kendal similarity method better Kendal similarity
default, Scenario 1. bold, winning p-value lower 0.05.

638

fiUsing Meta-mining Support DM Workflow Planning Optimization

k
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
Total

P2
Avg.Acc W L p-value
0.798
39 20 0.019
0.793
41 21 0.015
0.792
41 21 0.015
0.789
43 21 0.008
0.786
39 24 0.077
0.784
38 25 0.130
0.782
41 24 0.047
0.780
36 26 0.253
0.778
41 20 0.010
0.777
36 27 0.313
0.777
44 20 0.004
0.774
38 23 0.073
0.773
38 25 0.130
0.772
40 22 0.030
0.771
43 18 0.002
0.770
40 22 0.030
0.769
40 22 0.030
0.767
36 26 0.253
0.766
42 21 0.011
0.765
34 29 0.614
0.763
39 24 0.077
0.762
38 26 0.169
0.761
37 25 0.162
0.760
37 24 0.124
0.759
39 19 0.012
0.757
33 20 0.099
0.755
32 14 0.012
0.753
33 15 0.014
0.751
32 10 0.001
0.749
21 15 0.404
0.747
8
5
0.579
0.742
18 10 0.185
0.737
2
3
1
Wins 16/Losses 0

P1
Avg.Acc W L p-value
0.788
26 38 0.169
0.786
28 35 0.449
0.785
35 28 0.449
0.785
38 25 0.130
0.786
33 30 0.801
0.783
33 29 0.703
0.782
40 25 0.082
0.781
38 27 0.214
0.778
38 25 0.130
0.777
30 33 0.801
0.777
40 22 0.030
0.775
40 23 0.043
0.774
37 26 0.207
0.772
40 24 0.060
0.771
41 21 0.015
0.770
40 22 0.030
0.769
39 23 0.056
0.768
35 27 0.374
0.767
41 18 0.004
0.765
33 24 0.289
0.763
45 19 0.001
0.762
33 27 0.518
0.761
34 30 0.707
0.760
43 18 0.002
0.758
43 20 0.005
0.757
39 23 0.056
0.754
34 17 0.025
0.751
32 27 0.602
0.748
28 30 0.895
0.746
22 31 0.271
0.744
16 30 0.055
0.741
26 36 0.253
0.737
2
2
1
Wins 6/0 Losses

Metric
Avg.Acc W L p-value
0.786
25 38 0.130
0.784
26 37 0.207
0.783
32 33 1
0.782
33 32 1
0.780
32 31 1
0.779
28 33 0.608
0.779
35 27 0.374
0.778
34 30 0.707
0.776
34 28 0.525
0.775
29 33 0.703
0.774
40 24 0.060
0.773
39 25 0.104
0.771
32 33 1
0.770
33 29 0.703
0.770
40 25 0.082
0.769
38 27 0.214
0.767
36 26 0.253
0.767
29 35 0.531
0.766
38 25 0.130
0.765
36 25 0.200
0.764
42 22 0.017
0.763
35 27 0.374
0.762
36 29 0.456
0.761
45 11 0.001
0.759
41 20 0.010
0.758
38 24 0.098
0.755
35 10 0.000
0.752
33 19 0.071
0.750
34 17 0.025
0.748
22 18 0.635
0.745
13 17 0.583
0.742
19 21 0.874
0.737
2
5
0.449
Wins 4/Losses 0

EC
Avg.Acc W L p-value
0.782
30 32 0.898
0.780
32 32 1
0.778
32 33 1
0.777
34 30 0.707
0.776
30 33 0.801
0.774
30 35 0.619
0.773
30 34 0.707
0.773
31 33 0.900
0.773
31 34 0.804
0.772
26 37 0.207
0.772
31 33 0.900
0.772
33 30 0.801
0.771
31 34 0.804
0.770
30 33 0.801
0.769
34 30 0.707
0.767
33 31 0.900
0.767
32 31 1
0.766
30 35 0.619
0.765
37 28 0.321
0.764
33 30 0.801
0.762
32 30 0.898
0.761
30 32 0.898
0.760
30 32 0.898
0.758
37 26 0.207
0.757
37 22 0.068
0.756
33 26 0.434
0.754
31 19 0.119
0.751
32 24 0.349
0.749
25 28 0.783
0.747
20 28 0.312
0.745
12 25 0.048
0.742
13 18 0.472
0.737
2
3
1
Wins 0/ Losses 0

Def
Avg.Acc
0.786
0.785
0.778
0.777
0.780
0.778
0.775
0.775
0.774
0.774
0.771
0.771
0.771
0.769
0.766
0.765
0.765
0.766
0.763
0.763
0.761
0.761
0.760
0.756
0.756
0.756
0.753
0.751
0.749
0.748
0.747
0.742
0.737

Table 5: Average Accuracy, Wins/Losses, respective P-values McNemars test
number times Average Accuracy method better Average
Accuracy default, Scenario 1. bold, winning p-value lower
0.05.

639

fiNguyen, Hilario & Kalousis

k
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
Total

P2
W L p-value
9
24 0.014
15 28 0.067
25 32 0.426
28 34 0.525
33 30 0.801
36 29 0.456
38 27 0.214
40 25 0.082
43 22 0.013
43 22 0.013
40 25 0.082
41 24 0.047
43 22 0.013
43 22 0.013
40 25 0.082
42 23 0.025
40 25 0.082
40 25 0.082
40 25 0.082
39 26 0.136
39 26 0.136
38 27 0.214
38 27 0.214
40 25 0.082
40 25 0.082
38 27 0.214
38 27 0.214
38 27 0.214
37 28 0.321
35 30 0.619
35 30 0.619
30 35 0.619
29 36 0.456
29 36 0.456
Wins 4/Losses 0

P1
W L p-value
8
11 0.646
13 13 1.000
17 18 1.000
21 25 0.658
24 29 0.582
32 28 0.698
34 26 0.366
34 29 0.614
35 30 0.619
33 32 1.000
34 31 0.804
32 32 1.000
34 31 0.804
36 29 0.456
35 30 0.619
36 29 0.456
35 30 0.619
35 30 0.619
36 29 0.456
36 28 0.381
37 28 0.321
35 30 0.619
35 30 0.619
34 31 0.804
34 31 0.804
35 30 0.619
34 30 0.707
34 31 0.804
33 32 1.000
33 32 1.000
33 32 1.000
33 32 1.000
34 31 0.804
32 33 1.000
Wins 0/Losses 0

Table 6: Wins/Losses P-values McNemars test number times Kendal
similarity method better Kendal similarity default, Scenario
2. bold, winning p-value lower 0.05.

640

fiUsing Meta-mining Support DM Workflow Planning Optimization

k
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
Total

P2
Avg.Acc W L
0.797
39 24
0.792
44 20
0.790
34 29
0.787
37 27
0.785
37 27
0.783
36 28
0.782
35 28
0.781
38 26
0.779
38 25
0.777
34 30
0.775
34 30
0.774
35 28
0.773
35 29
0.773
33 32
0.772
32 33
0.770
19 44
0.769
26 39
0.768
24 37
0.768
25 39
0.767
24 39
0.767
26 38
0.766
23 40
0.765
23 42
0.763
20 45
0.762
14 51
0.762
15 50
0.761
16 48
0.760
18 47
0.759
18 47
0.757
18 47
0.756
17 48
0.756
16 49
0.755
13 51
Wins 1/Losses

p-value
0.077
0.004
0.614
0.260
0.260
0.381
0.449
0.169
0.130
0.707
0.707
0.449
0.531
1.000
1.000
0.002
0.136
0.124
0.104
0.077
0.169
0.043
0.025
0.002
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
13

P1
Avg.Acc W L p-value
0.789
29 34 0.614
0.784
33 31 0.900
0.785
31 34 0.804
0.782
32 32 1.000
0.783
33 32 1.000
0.783
32 33 1.000
0.783
30 34 0.707
0.784
31 33 0.900
0.782
35 29 0.531
0.780
36 29 0.456
0.779
38 27 0.214
0.779
38 26 0.169
0.777
37 27 0.260
0.776
34 31 0.804
0.774
31 33 0.900
0.773
26 38 0.169
0.772
27 37 0.260
0.772
28 37 0.321
0.770
23 42 0.025
0.770
24 40 0.060
0.769
29 36 0.456
0.768
26 38 0.169
0.768
29 36 0.456
0.767
24 40 0.060
0.768
22 42 0.017
0.767
24 41 0.047
0.767
32 32 1.000
0.766
33 31 0.900
0.766
39 26 0.136
0.765
35 29 0.531
0.765
33 33 1.000
0.764
31 34 0.804
0.763
27 38 0.214
Wins 0/Losses 3

Def
Avg.Acc
0.786
0.780
0.784
0.778
0.778
0.779
0.778
0.776
0.773
0.772
0.770
0.770
0.770
0.770
0.771
0.773
0.772
0.771
0.770
0.771
0.769
0.768
0.767
0.768
0.769
0.768
0.766
0.764
0.763
0.764
0.764
0.764
0.764

Table 7: Avg.Acc., Wins/Losses, respective P-values McNemars test
number times Average Accuracy method better Average
Accuracy default, Scenario 2. bold, winning p-value lower
0.05.

641

fiNguyen, Hilario & Kalousis

References
Bernstein, A., Provost, F., & Hill, S. (2005). Toward intelligent assistance data mining
process: ontology-based approach cost-sensitive classification. Knowledge
Data Engineering, IEEE Transactions on, 17 (4), 503518.
Bose, R. J. C., & der Aalst, W. M. V. (2009). Abstractions process mining: taxonomy
patterns. Proceedings 7th International Conference Bussiness Process
Management.
Brazdil, P., Giraud-Carrier, C., Soares, C., & Vilalta, R. (2008). Metalearning: Applications
Data Mining (1 edition). Springer Publishing Company, Incorporated.
Bringmann, B. (2004). Matching frequent tree discovery. Proceedings Fourth
IEEE International Conference Data Mining (ICDM04, pp. 335338.
Chapman, P., Clinton, J., Kerber, R., Khabaza, T., Reinartz, T., Shearer, C., & Wirth, R.
(2000). Crisp-dm 1.0 step-by-step data mining guide. Tech. rep., CRISP-DM
consortium.
Fagin, R., Kumar, R., & Sivakumar, D. (2003). Comparing top k lists. Proceedings
fourteenth annual ACM-SIAM symposium Discrete algorithms, SODA 03, pp.
2836, Philadelphia, PA, USA. Society Industrial Applied Mathematics.
Fayyad, U., Piatetsky-Shapiro, G., & Smyth, P. (1996). data mining knowledge
discovery databases. AI magazine, 17 (3), 37.
Forbes, J., & Andre, D. (2000). Practical reinforcement learning continuous domains.
Tech. rep., Berkeley, CA, USA.
Gil, Y., Deelman, E., Ellisman, M., Fahringer, T., Fox, G., Gannon, D., Goble, C., Livny,
M., Moreau, L., & Myers, J. (2007). Examining challenges scientific workflows.
Computer, 40 (12), 2432.
Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, P., & Witten, I. H. (2009).
weka data mining software: update. SIGKDD Explor. Newsl., 11 (1), 1018.
Hilario, M. (2002). Model complexity algorithm selection classification. Proceedings 5th International Conference Discovery Science, DS 02, pp. 113126,
London, UK, UK. Springer-Verlag.
Hilario, M., & Kalousis, A. (2001). Fusion meta-knowledge meta-data casebased model selection. Proceedings 5th European Conference Principles
Data Mining Knowledge Discovery, PKDD 01, pp. 180191, London, UK, UK.
Springer-Verlag.
Hilario, M., Kalousis, A., Nguyen, P., & Woznica, A. (2009). data mining ontology
algorithm selection meta-learning. Proc ECML/PKDD09 Workshop
Third Generation Data Mining: Towards Service-oriented Knowledge Discovery.
Hilario, M., Nguyen, P., Do, H., Woznica, A., & Kalousis, A. (2011). Ontology-based metamining knowledge discovery workflows. Jankowski, N., Duch, W., & Grabczewski,
K. (Eds.), Meta-Learning Computational Intelligence. Springer.
642

fiUsing Meta-mining Support DM Workflow Planning Optimization

Ho, T. K., & Basu, M. (2002). Complexity measures supervised classification problems.
IEEE Trans. Pattern Anal. Mach. Intell., 24 (3), 289300.
Ho, T. K., & Basu, M. (2006). Data complexity pattern recognition. Springer.
Hoffmann, J. (2001). Ff: fast-forward planning system. AI magazine, 22 (3), 57.
Kalousis, A. (2002). Algorithm Selection via Metalearning. Ph.D. thesis, University
Geneva.
Kalousis, A., Gama, J., & Hilario, M. (2004). data algorithms: Understanding
inductive performance. Machine Learning, 54 (3), 275312.
Kalousis, A., & Theoharis, T. (1999). Noemon: Design, implementation performance
results intelligent assistant classifier selection. Intell. Data Anal., 3 (5), 319
337.
Kietz, J.-U., Serban, F., Bernstein, A., & Fischer, S. (2009). Towards Cooperative Planning Data Mining Workflows. Proc ECML/PKDD09 Workshop Third
Generation Data Mining: Towards Service-oriented Knowledge Discovery (SoKD-09).
Kietz, J.-U., Serban, F., Bernstein, A., & Fischer, S. (2012). Designing kdd-workflows via
htn-planning intelligent discovery assistance. 5th PLANNING LEARN
WORKSHOP WS28 ECAI 2012, p. 10.
King, R. D., Feng, C., & Sutherland, A. (1995). Statlog: Comparison classification
algorithms large real-world problems. Applied Artificial Intelligence, 9 (3), 289
333.
Klinkenberg, R., Mierswa, I., & Fischer, S. (2007). Free data mining software: Rapidminer
4.0 (formerly yale). http://www.rapid-i.com/.
Kopf, C., Taylor, C., & Keller, J. (2000). Meta-analysis: data characterisation
meta-learning meta-regression. Proceedings PKDD-00 Workshop Data
Mining, Decision Support,Meta-Learning ILP.
Kramer, S., Lavrac, N., & Flach, P. (2000). Relational data mining.. chap. Propositionalization Approaches Relational Data Mining, pp. 262286. Springer-Verlag New
York, Inc., New York, NY, USA.
Leite, R., & Brazdil, P. (2010). Active testing strategy predict best classification algorithm via sampling metalearning. Proceedings 2010 conference ECAI
2010: 19th European Conference Artificial Intelligence, pp. 309314, Amsterdam,
Netherlands, Netherlands. IOS Press.
McDermott, D., Ghallab, M., Howe, A., Knoblock, C., Ram, A., Veloso, M., Weld, D., &
Wilkins, D. (1998). Pddl-the planning domain definition language..
Michie, D., Spiegelhalter, D. J., Taylor, C. C., & Campbell, J. (1994). Machine learning,
neural statistical classification..
Nguyen, P., Kalousis, A., & Hilario, M. (2011). meta-mining infrastructure support kd
workflow optimization. Proc ECML/PKDD11 Workshop Planning Learn
Service-Oriented Knowledge Discovery, 1.
643

fiNguyen, Hilario & Kalousis

Nguyen, P., Kalousis, A., & Hilario, M. (2012a). Experimental evaluation e-lico
meta-miner. 5th PLANNING LEARN WORKSHOP WS28 ECAI 2012,
p. 18.
Nguyen, P., Wang, J., Hilario, M., & Kalousis, A. (2012b). Learning heterogeneous similarity
measures hybrid-recommendations meta-mining. IEEE 12th International
Conference Data Mining (ICDM), pp. 1026 1031.
Peng, Y., Flach, P. A., Soares, C., & Brazdil, P. (2002). Improved dataset characterisation
meta-learning. Discovery Science, pp. 141152. Springer.
Pfahringer, B., Bensusan, H., & Giraud-Carrier., C. (2000). Meta-learning landmarking various learning algorithms.. Proc. 17th International Conference Machine
Learning, 743750.
R Core Team (2013). R: language environment statistical computing. http:
//www.R-project.org/.
Smart, W. D., & Kaelbling, L. P. (2000). Practical reinforcement learning continuous
spaces. Proceedings Seventeenth International Conference Machine Learning, ICML 00, pp. 903910, San Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
Soares, C., & Brazdil, P. (2000). Zoomed ranking: Selection classification algorithms based
relevant performance information. Proceedings 4th European Conference
Principles Data Mining Knowledge Discovery, PKDD 00, pp. 126135,
London, UK. Springer-Verlag.
Srebro, N., Rennie, J. D. M., & Jaakkola, T. S. (2005). Maximum-margin matrix factorization. Saul, L. K., Weiss, Y., & Bottou, L. (Eds.), Advances Neural Information
Processing Systems 17, pp. 13291336. MIT Press, Cambridge, MA.
Sutton, R., & Barto, A. (1998). Reinforcement learning: introduction. Neural Networks,
IEEE Transactions on, 9 (5), 1054.
Van der Aalst, W. M., & Giinther, C. (2007). Finding structure unstructured processes:
case process mining. Application Concurrency System Design, 2007.
ACSD 2007. Seventh International Conference on, pp. 312. IEEE.
Yang, Q., & Wu, X. (2006). 10 challenging problems data mining research. International
Journal Information Technology & Decision Making, 5 (04), 597604.
Zaki, M. J. (2005). Efficiently mining frequent trees forest: Algorithms applications.
IEEE Transactions Knowledge Data Engineering, 17 (8), 10211035. special
issue Mining Biological Data.
Zakova, M., Kremen, P., Zelezny, F., & Lavrac, N. (2011). Automating knowledge discovery workflow composition ontology-based planning. Automation Science
Engineering, IEEE Transactions on, 8 (2), 253264.

644


