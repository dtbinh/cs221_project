Journal Artificial Intelligence Research 56 (2016) 657-691

Submitted 01/16; published 08/16

Engineering Note
IBaCoP Planning System: Instance-Based Configured Portfolios
Isabel Cenamor
Tomas de la Rosa
Fernando Fernandez

ICENAMOR @ INF. UC 3 . ES
TROSA @ INF. UC 3 . ES
FFERNAND @ INF. UC 3 . ES

Departamento de Informatica, Universidad Carlos III de Madrid
Avda. de la Universidad, 30. Leganes (Madrid). Spain

Abstract
Sequential planning portfolios powerful exploiting complementary strength
different automated planners. main challenge portfolio planner define
base planners run, assign running time planner decide order
carried optimize planning metric. Portfolio configurations usually derived
empirically training benchmarks remain fixed evaluation phase. work,
create per-instance configurable portfolio, able adapt every planning task.
proposed system pre-selects group candidate planners using Pareto-dominance filtering
approach decides planners include time assigned according predictive
models. models estimate whether base planner able solve given problem and,
so, long take. define different portfolio strategies combine knowledge
generated models. experimental evaluation shows resulting portfolios provide
improvement compared non-informed strategies. One proposed portfolios
winner Sequential Satisficing Track International Planning Competition held
2014.

1. Introduction
Planning process chooses organizes actions anticipating outcomes
aim achieving pre-stated objectives. Artificial Intelligence, Automated Planning (AP)
computational study deliberation process (Ghallab, Nau, & Traverso, 2004). Automated
planners systems that, regardless application domain, able receive declarative
representation environment, initial state set goals input. output synthesized plan achieve goals initial situation. context, International
Planning Competition (IPC) excellent initiative foster studying development automated planning systems. IPC created 1998 set common framework comparing
automated planners.
Different planning systems awards previous IPCs. However, one main invariants
competition single planner always best planner (or least equal)
every domain every problem. means that, although planner which, following
quality metrics competition, considered best, always find problems
different domains planners outperform overall winner. Therefore, assume
AP community generated set single planners better others specific
situations. reason, discarding priori solvers seems meaningless.
c
2016
AI Access Foundation. rights reserved.

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

fact, idea reusing set individual base systems generate accurate solutions obtained separately new Artificial Intelligence. instance, Machine
Learning, meta-classifiers use different base classifier increase coverage representation
bias resulting classifier (Dietterich, 2000). problem solving, portfolios search algorithms
demonstrated outperform results single search strategy (Xu, Hutter,
Hoos, & Leyton-Brown, 2008; Xu, Hoos, & Leyton-Brown, 2010; Malitsky, Sabharwal, Samulowitz, & Sellmann, 2013). example, SAT competition 2013 included special track
portfolios. automated planning community, planner portfolios subject great
deal interest. IPCs 2006 2014, portfolio approaches close winning
tracks took part.
However, although use portfolios become usual community, still
agreement planning portfolio (Vallati, Chrpa, & Kitchin, 2015). work,
assume portfolio planners set base planners selection strategy. selection
strategy generates specific portfolio configuration, whose goal maximize performance metrics. Therefore, configuration define three main elements: (1) sub-set
planners run, (2) long run planner? (3) order. many
techniques configure planning portfolio (Vallati, 2012), depending accurate
are, chances selecting best planner given situation increase. Note that,
definition, planner different configuration parameters modify behavior, parameterization considered different base planner, base planners considered black
boxes.
number planners state art huge, first filtering select minimum
number ensures best performance achieved, evaluated planning domain (or even
problem domain). Obviously, good results current domains ensure good
results new domains but, shown, good estimator. sense, Pareto efficiencybased approach (Censor, 1977) reduce number planners consider eligible
planning portfolio presented. However, show mechanism, first
aforementioned questions answered partially since number candidate planners
might still large.
best solution portfolio configuration problem oracle predicts,
given domain problem, planner obtain best performance long
take. Given oracle, work propose use predictive models, automatically generated Machine Learning Data Mining techniques. models summarize results candidate planners past: whether able solve planning
problems, well time required generate good solution (Cenamor, de la Rosa,
& Fernandez, 2012, 2013). Given knowledge past, inductive hypothesis gives us
estimation behave future planning domains different problems,
order planners implemented given accuracy predictions.
Therefore, predictive models, able configure portfolio planning problem, previous works use portfolios search (Gomes & Selman, 2001).
renewed idea automated planning since recent works focused static (Helmert, 2006)
domain-specific portfolios (Gerevini, Saetti, & Vallati, 2009, 2014), configuration
portfolio fixed domains chosen one respectively.
IBAC P (Instance-based Configured Portfolio) family planning portfolios built
competing IPC-2014. article first present IBAC P general framework
658

fiT IBAC P P LANNING YSTEM

ultimate goal building per-instance configurable portfolios. technique reproduced
whenever new automated planners new planning benchmarks arise. Then, describe
build different version IBAC P following defined processes. One versions
winner Sequential Satisficing Track IPC-2014. include results empirical
study confirms good performance IBAC P planners compared different base
planners different portfolio configuration strategies. Then, summarize related work,
finally, last section sets conclusions future lines research.

2. System Architecture
section, present general idea building planning portfolio configured
particular planning task using predictive models. process seen general
technique given inputs (planners benchmarks) might change future due progress
planning community, new portfolio configurations generated use
new inputs.
2.1 Portfolio Construction
consider construction instance-based planning portfolio comprises three main
parts. (1) Planner filtering, making pre-selection good candidate planners set
known available planners. proposed pre-selection technique based multi-criteria approximation. previously unexplored technique selecting set planners provides
enough diversity planner portfolio. (2) Performance modeling, providing predictors
planners behavior function planning task features. research, include set
well-known features (Cenamor et al., 2012), built preprocessing step
FAST OWNWARD (Helmert, 2006). take advantage output information
translation process (Fawcett, Vallati, Hutter, Hoffmann, Hoos, & Leyton-Brown, 2014)
heuristic values computed first step search process FAST OWNWARD. addition,
use several totally new features characteristics relaxed plan initial state
proposed. Finally, (3) strategy selection: establish procedure combines performance
predictions output portfolio configuration. propose novel strategy selection
exploit effectiveness predictive models. Next, explain details
construction steps.
2.1.1 P LANNER F ILTERING
planner filtering process consists pre-selection good candidate base planners
larger amount available planners. Even though sufficient evidence
overall best planner across variety benchmarks, verified empirically
dominance planners others. Therefore make sense include, base
planners, always worse terms performance metrics. want filtering process
select diverse, small, subset planners elements among divide
available execution time.
work, propose multi-criteria pre-selection mechanism focuses two IPC metrics (quality time) alternative extended ones planner filtering. example,
FDSS (Helmert, Roger, Seipp, Karpas, Hoffmann, Keyder, Nissim, Richter, & Westphal, 2011)
659

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

uses selection planners maximizes coverage; MIP LAN (Nunez, Borrajo, & Linares
Lopez, 2015) uses portfolio configuration obtains best achievable performance terms
score.
filtering propose run candidate planners representative set benchmarks
evaluate terms time quality. consider metrics propose
approach based Pareto-efficiency (Censor, 1977) allows us determine dominance
planners multi-criteria fashion. particular, select planner candidate
portfolio best planner least one domain terms IPC-2011 multi-criteria QT
score (Linares Lopez, Celorrio, & Olaya, 2015). Briefly, single problem, metric computes
tuple hQ, planner, Q quality planners best solution
time used find solution. Then, given planner, p, dominance relations p
rest planners computed.
tuple hQ, Pareto-dominates tuple hQ , Q Q < . Planner
p gets NN points, N number tuples p Pareto-dominates another planner,
N number different tuples planner p appears. Finally, QT-Pareto score
domain sum points achieved problems domain. idea selection
mechanism follows: planner shows good dominance property given domain,
included portfolio good candidate solving problems
domain even planning tasks similar characteristics. Therefore, simple strategy
filter first pool planners given procedure selects planners
maximum QT-Pareto score least one domain. refer procedure QT-Pareto Score
Filtering.
2.1.2 P ERFORMANCE ODELING
Given planning task, want predict selected base planners perform order
decide whether include make good assignment time ordering
configuring portfolio. Thus, modeling planner behavior function planning
task features becomes key process building instance-based portfolios. learn predictive
models follow Data Mining approach, shown Figure 1. case, start set
candidate planners set planning benchmarks. output process set models
predict performance candidate planners. defined data mining goal
creation two predictive models. First, whether planner able solve problem
(i.e. classification task) and, so, time required compute best plan (i.e.,
regression task).
first step mining process comprises generation training test datasets.
one hand, planners run set benchmarks obtain performance data.
data includes outcome execution (success failure) and, positive cases, time
elapsed finding best solution. hand, planning tasks processed extract set
features characterize them. features extended set previously proposed
set (Cenamor et al., 2013). According mechanism generating features, classify
following categories:
PDDL features: Basic features extracted PDDL representation domain
problem files, instance, number actions, objects goals.
660

fiT IBAC P P LANNING YSTEM

Figure 1: General Diagram Learning Planning Performance Predictive Models
FD Instantiation features: Fast-Downward pre-processor instantiates translates
planning tasks finite domain representation (Helmert, 2009). output take
general information number instantiated actions number relevant
facts, data specific FD-translator, number auxiliary atoms.
SAS+ features: finite domain representation SAS+ associated Causal Graph
(CG) set Domain Transition Graphs (DTGs). CG extract basic properties
(e.g., number variables edges), ratios properties. regards
DTGs, number graphs problem corresponds number edges CG,
makes difficult encode general attributes DTG. Therefore, summarize DTGs characteristics aggregating relevant properties graphs. Thus,
features DTGs statistics maximum, average standard
deviation graph properties.
Heuristic features: initial state, compute heuristic values using set widely-used
unit cost heuristic functions (e.g., hmax , hFF ,. . . ). compute heuristics
initial state, obtained reasonable cost. use unit cost heuristics
obtain domain-independent estimation helps characterization problem size
and/or difficulty.
Fact Balance Features: Using relaxed plan (RP ) initial state, extracted computing hFF heuristic, compute set features represent fact balance
RP . define fact balance fact p, number times p appears added
effect action belonging RP , minus number times p deleted effect
action RP , considering original actions deletes ignored. intuition
behind fact balances high positive values would characterize easier (relaxed) problems
given domain, since achieved facts need deleted many times. Given
number relevant facts planning task variable, compute statistics (i.e., min, max,
average variance) fact balance relevant facts. Additionally, compute
statistics considering facts goals, following procedure.
661

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

complete set 89 features listed organized category Appendix A.
Data Integration process Figure 1 receives features performance datasets inputs
produce final dataset according modeling goal. dataset classification task,
training/test instance includes planning task features plus planner name Boolean
feature indicating whether planner solved planning task. dataset regression task
includes cases planning tasks solved. make exclusion
make sense model estimate planning time beyond given time limit
cases time unknown. training/test instance regression dataset includes
planning task features, planner name time planner used find best solution.
Feature Selection optional process reducing number features used
modeling. procedure applied might irrelevant redundant features
could degrade modeling capabilities learning techniques (Blum & Langley, 1997).
outcome process dependent original data. Thus, decision whether apply
taken based results model evaluation.
Modeling process, use off-the-shelf data-mining tool provides set learning
algorithms classification regression. generated models evaluated
Evaluation process determine best model classification regression tasks.
many different ways carrying model evaluation comparison (Han, Kamber, & Pei,
2011; Witten & Frank, 2005), reflect generalization ability different models
making predictions unseen data.
2.1.3 TRATEGY ELECTION
strategy selection final step construction IBAC P planner. Selecting strategy implies decide transform predictions best models actual
portfolio configuration. several alternatives range ignoring model predictions trusting completely. classification model, candidate planner get
yes/no prediction given new planning task. direct use Boolean variable makes difficult
decide planners include portfolio. Consider, instance. two extreme cases:
(1) planners get positive prediction, include them? (2) planners get
negative prediction, planner include portfolio? Instead using Boolean
prediction propose rank predictions confidence positive class,
make selection planners according ranking. Then, planner assigned
slide total time, assignment carried uniformly dependently, again,
predictive models learned. Therefore, depending use make predictive
models, propose three basic strategies:
1. Equal Time (ET): strategy use predictive models all. assign
equal time planner (uniform strategy). idea behind strategy
planners less time one. strategy obtained good results
portfolios (Seipp, Braun, Garimort, & Helmert, 2012).
2. Best N confidence (BN): strategy include subset N planners best
prediction confidence positive class portfolio. Then, get equal time
solving planning task. case, idea select subset promising planners
spend time solving planning task.
662

fiT IBAC P P LANNING YSTEM

3. Best N Estimated Time (BNE): subset planners selected mentioned before,
time assigned proportionally estimated time provided regression model.
2.2 Portfolio Configuration
instance-based configuration portfolio implies subset base planners time
assigned one varies function planning task features. set candidate planners,
predictive models configuration strategy previously fixed construction phase.
Algorithm 1 shows use components configure portfolio given planning
task.
Algorithm 1: Algorithm configuring portfolio particular planning task.
Data: Problem (), Domain (d), Set base planners (Pini ), Classification model (C),
Regression model (R), Available time (T ), Strategy (SN )
Result: Portfolio Configuration: sequence planners assigned runtime,
Portfolio = [hp1 , t1 i, . . . , hpc , tc i]
Portfolio=[];
SN == ET
/*(No classification regression models available)*/
n = size(Pini );
p Pini
append(hp, Tn i, Portfolio);
else
hF, tF = extractFeatures(d, );
pk Pini
predictionhpk , confk predict (C, hF, pk i);

sorted candidates sort(prediction, key = conf );
p sorted candidates[i . . . N ];
SN == BN
/*Classification model available, applying Best N confidence strategy*/
= 1 N
F
append(hpi ,
N i, Portfolio);
else
/*Regression model available, applying Best N Estimated Time*/
= 1 N
ti = predict time(R, hF, pi i);
= scaleTime(t, tF );
= 1 N
append(hpi , ti i, Portfolio) ;

method receives problem (), domain (d), set base planners (Pini ), classification model (C), regression model (R), time available (T ) portfolio configuration
strategy (SN {ET, BN, BN E}). procedure calls several functions described below:
663

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

extractFeatures: feature extraction procedure used portfolio construction phase. pair (domain, problem) function outputs set features F .
function computes time (tF ) time spent extracting features.
predict: function query classification model C. receives new instance
represented tuple hF, pi, F previously computed features, p
planner name. result function ignore class, keep prediction
confidence positive class, forming tuple hp, conf i. output represents
confidence planner p solve problem.
predict time: function uses model R estimate execution time subset
planners PN Pini established best N candidates terms classification confidence. classification model, function receives input tuple
hF, pi.
scaleTime: function transforms vector estimated times another proportional
vector sum fits available time, original time bound minus
time used compute features tF . Thus, time assigned planner computed
F )t
formula = (TPt
N
i=1 ti

output algorithm sequence planners assigned time. execution
particular configuration portfolio comprises sequential execution base planners
ensuring CPU process exceed assigned time.

3. IBaCoP Planning System
section describe follow approach presented Section 2 build different
portfolios.
3.1 Candidate Planners
initial set planners includes 27 planners Sequential Satisficing Track IPC-2011
plus LPG- TD (Gerevini, Saetti, & Serina, 2006). Although LGP- Td compete IPC-2011
considered worthwhile include still considered state-of-the-art planner due
great performance previous competitions.
first step apply QT-Pareto Score Filtering described subsection 2.1.1 reduce
initial set candidate planners. benchmarks computing QT-Pareto Score set
domains problems Sequential Satisficing Track IPC-2011.
Table 1 shows best planner terms QT-Pareto score domain. Additionally,
include number problems solved best planner highlight correlation among
values. QT-Pareto score values closer 20 reflect planner able beat
planners problems. P ROBE best planner 4 domains. However
planners stood one domain. reinforces motivation find diverse subset
planners. Finally, 28 initial planners, QT-Pareto score filtering pre-selected candidate
planners subset 11 planners, made of: LAMA -2011, PROBE , ARVAND , FDSS 2, FD - AUTOTUNE -1, FD - AUTOTUNE -2, LAMAR , LAMA -2008, MADAGASCAR , YAHSP 2- MT
LPG- TD. brief description planners found Appendix D.
664

fiT IBAC P P LANNING YSTEM

Planner
PROBE
PROBE
PROBE
PROBE
ARVAND
MADAGASCAR
LAMA -2008
LAMA -2011
FD - AUTOTUNE -1
FD - AUTOTUNE -2
FDSS -2
LAMAR
YAHSP 2- MT
LPG- TD

Domain
scanalyzer
woodworking
tidybot
barman
pegsol
parcprinter
transport
openstacks
sokoban
nomystery
elevators
parking
visitall
floortile

total

QT
16.59
18.55
16.77
19.42
18.88
17.63
17.84
17.30
17.56
16.73
17.84
18.12
18.74
11.96
243.77

Coverage
20
20
18
20
20
20
19
20
19
19
20
20
20
12
267

Table 1: List best planners ordered QT-Pareto score domain IPC-2011.
Table 2 shows ranking planners IPC results (i.e., planner ordering established
quality score) (Linares Lopez et al., 2015) selected QT-Pareto Score
Filtering. worth noting attention 10 11 best planners IPC built top
FD, reduces diversity planners. However, QT-Pareto Score Filtering
includes 8 them. addition, pointed last three selections QT-Pareto
Score Filtering planners lower positions table which, demonstrated
later, increases diversity portfolio performance.
Ranking
1
2
3
4
5
6
7
8
9
10
11
17
22
24

planner
LAMA -2011
FDSS -1
FDSS -2
FD - AUTOTUNE -1
ROAMER
FORKUNIFORM
FD - AUTOTUNE -2
PROBE
ARVAND
LAMA -2008
LAMAR
YAHSP 2- MT
MADAGASCAR
LPG- TD

Eligible












FD











Table 2: List 11 best planners ordered score IPC-2011. third column shows whether
selected QT-Pareto Score Filtering. forth column shows planners
built top FD.

665

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

3.2 Performance Models
inputs performance modeling phase candidate planners (i.e., 11 candidates
selected previous section) benchmark planning tasks selected purpose. Next,
describe generated training data, inputs produce specific instances
IBAC P planners.
3.2.1 RAINING DATA
training data learning process requires set domains problems used gather
input features. need wide range domains problems generalize future unknown
planning tasks properly. included planning problems available IPC-2006 onwards. mention test set explicitly, always refer satisficing tracks
competitions. included domain problems are:
IPC-2006: openstacks, pathways, rovers, storage, tpp trucks.
IPC-2008: cybersec, elevators, openstacks, pegsol, pipesworld, scanalyzer, sokoban, transport woodworking.
IPC-2011: barman, elevators, floortile, nomystery, visitall, tidybot, openstacks, parcprinter,
parking, pegsol, sokoban, scanalyzer, transport woodworking.
Learning track IPC-2008: gold-miner, matching-bw, n-puzzle, parking, thoughful sokoban.
Learning track IPC-2011: barman, blockworld, depots, gripper, parking, rovers satellite,
spanner tpp.
list obtained 45 different domain descriptions. Although represent
alternative encodings domain, included. Candidate planners run
benchmarks obtain features related performance planners. Thus, used
total 1, 251 planning tasks. performance data comprises 13, 761 instances (i.e., 1, 251 problems 11 planners) 8, 697 successful 5, 394 failed. proportion instances
solved candidate planner different. Table 16 Appendix C shows per-planner summary
performance data.
89 features representing planning task automatically generated domain
problem definitions. PDDL features, FD instantiation SAS+ features computed
using FAST-D OWNWARD pre-processor. computation time needed extract features
negligible compared SAS+ translation, given compute sums statistics
data provided SAS+ representation. heuristic features computed using FASTD OWNWARD search engine, fact balance features generated using relaxed planning
graph structures (of initial state) provided planner (Hoffmann, 2003). FASTD OWNWARD pre-processor could fail instantiating planning task. case, regarding
features computed missing values assumed.
Table 3 shows success rate extracting features type training problems, average maximum time seconds extract them. PDDL, FD SAS+
features extracted FD pre-processor success rate.
time required compute heuristic features time calculating heuristic value
initial state, calculated FD pre-process finished successfully.
666

fiT IBAC P P LANNING YSTEM

Class
PDDL
FD
SAS+
Heuristic
Fact Balance
Total

Success
97%
97%
97%
87.54%
93%
-

Average (s.)
6.97
52.73
22.60
20.20
5.20
107.7

Max (s.)
46.00
141.40
60.60
30.50
21.20
299.7

# features
8
16
50
8
7
89

Table 3: Summary extracted features average maximum time seconds (s.)
extract them. processes top two first step planners based
FD.

3.2.2 F EATURE ELECTION
carried feature selection process two main reasons. one hand, features
might irrelevant whilst others might redundant modeling purpose. Therefore want
analyze whether possible obtain better models using subset available features.
hand, study allow us recognize relevant features characterizing
planning task.
feature selection carried using J48 algorithm, top-down induction algorithm
build decision trees (Quinlan, 1993), selecting features appear top nodes
tree (Grabczewski & Jankowski, 2005). Decision trees make implicit feature selection
model includes queries features considered relevant. applying feature selection
process feature dataset, total number features decreased 89 34. leads
dataset size reduction around 62%. Table 4 contains list features resulting
feature selection process. selection chooses features categories. modeling
evaluation process kept datasets separate, one available features (f-all)
one selected features (f-34).
3.2.3 C LASSIFICATION ODELS
trained classifiers using 31 classification algorithms provided Weka (Witten & Frank,
2005), includes different model types decision trees, rules, support vector machines
instance based learning. recall training instances include planning task features
described Section 2.1.2 plus planner name Boolean feature indicating whether
planner solved planning task not. performance predictive models evaluated
10-fold cross-validation uniform random permutation training data. best
model datasets f-all f-34 generated Rotation Forest (Rodriguez, Kuncheva,
& Alonso, 2006), achieving 93.39 92.35% accuracy respectively. results quite
better result default model (ZeroR), obtained 61.72% accuracy. See
results classification models Table 14 Appendix B.
Even though good accuracy classification model guarantee good performance
portfolio, result great starting point selecting promising planners. accuracy
results feature selection showed small differences compared results obtained
667

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

Type

PDDL
(4)

CG & DTG
(11)

Features
types
goal
objects
functions

numberVariablesCG
inputEdgeCGStd
outputEdgeCGAvg
outputWeightCGMax
outputWeightCGAvg
outputEdgeHVStd
outputWeightHVMax
numberVariablesDTG
totalEdgesDTG
inputWeightDTGMax
hvRatio

Type

FD
(6)

Heuristics
(7)

Balance
(6)

Features
auxiliary atoms
implied effects removed
translator facts
translator total mutex groups size
num relevant facts
num instance actions
Additive
Context-enhanced additive

Goal count
Landmark count
Landmark-cut
Max
rp fact balance avg
rp fact balance var
rp goal balance min
rp goal balance avg
rp goal balance var
h ratio

Table 4: List features feature selection. complete set features listed Appendix A.

features. 3 algorithms statistically better accuracy f-34 dataset nine
similar accuracy, cases best achieved accuracy

3.2.4 R EGRESSION ODELS
trained regression models positive instances classification training
phase. classification phase, planners proportion instances,
case, planners number instances given solved different
number problems. Nevertheless consider relevant bias models
include planner name, somehow encodes single models planner, grouped
model. trained models 20 regression algorithms, provided Weka.
best algorithm f-all Decision Table (Kohavi, 1995) Relative Absolute Error
(RAE) 49.87 best one f-34 Bagging (Breiman, 1996) RAE 50.62.
Nevertheless, simplicity selected Decision Table model regression task
datasets (f-all f-34). decision justified results show significant
difference t-test result. following sections, regression model always refer
trained Decision Table algorithm. See results regression models
Table 15 Appendix B.
668

fiT IBAC P P LANNING YSTEM

3.3 IBaCoP Strategies
considered various strategies configuration IBAC P portfolios. list
strategies ordered depending use make knowledge provided predictive
models. experiments, configuration run 1800 seconds. named
portfolios according names given IPC-2014.
IBAC P: portfolio uses equal time strategy (ET) set 11 candidate planners previously filtered QT-Pareto Score Filtering procedure. Therefore, single planners
run 163 seconds. strategy use predictive models. planner using
strategy awarded runner-up sequential satisficing track IPC-2014.
IBAC P2: portfolio uses Best N confidence strategy (BN), N = 5. means
5 planners best prediction confidence solving problem included
configuration. run time assigned uniformly planner (360 seconds).
strategy, using f-34 model winner sequential satisficing track IPC-2014.1
IBAC P2-B5E: portfolio uses Best estimated time strategy (BNE) N = 5. follows procedure IBAC P2 select 5 planners, time assigned
scaling time prediction provided regression model (Decision Table). strategy
participated learning track IPC-2014 name LIBAC P2. case
training data models generated domain separately, since learning track
provides training problem set domain priori.
addition, built portfolio configurations serve baseline comparison.
Overall Equal Time (OET): strategy non-informed strategy carry
planner filtering use predictive models. assigns equal time available planner.
Given 28 planners (all participants IPC-2011 plus LPG-td), planner
run 64 seconds. planner see need planner filtering since,
although already obtains results close current state art base planners, results
improved selecting reduced set planners.
Best 11 Planners (B11): strategy selects top 11 planners IPC-2011 ordered score
competition, shown Table 2. Although selecting best 11 planners good
choice intuitively, show table selection reduces planner diversity
portfolio, since top planners competition based FD, exception Probe. strategy comparable implemented BUS portfolio (Howe,
Dahlman, Hansen, Scheetz, & von Mayrhauser, 1999), control strategy ordering planners allocating time derived performance study data.
Random 5 Planners (Rand): strategy one baselines compare best 5 confidence strategy (IBAC P2). Given planning task, strategy takes random sample
5 planners population 11 candidate planners selected QT-Pareto filtering,
1. Predictive models submitted IBAC P2 IPC-2014 trained different benchmark set. case
best accuracy achieved Random Forest (Breiman, 2001).

669

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

assigns equal time them. expect wise selection 5 planners (IBAC P2)
average better random selection.
Default 5 Planners (Def): case, strategy always includes 5 best planners terms
quality score training data. 5 planners subset 11 candidate
planners selected QT-Pareto filtering (i.e., LAMA -2011, PROBE , FD -AUTOTUNE -1,
LAMA -2008 FD - AUTOTUNE -2). Then, time assigned equitably. want see
whether using best 5 planners better making per-instance selection 5 planners.
3.4 Implementation Details
section describe engineering details incorporated IBAC P
planners. instance, competition rules proposed include domains conditional effects.
this, included parser translates tasks conditional effects
equivalent planning task without property. translator based previous translator ADL2STRIPS (Hoffmann, Edelkamp, Thiebaux, Englert, dos Santos Liporace, & Trug, 2006).
Specifically, implemented compilation creates artificial actions effect evaluations (Nebel, 2000).
Furthermore, many 11 candidate planners built FAST-D OWNWARD framework, among things, separate planning process sub-process translation,
pre-processing search. Indeed, translation pre-process steps already executed
feature generation given task performed. take advantage fact avoid
first two steps repeatedly planners included configuration
portfolio regarding task. version compatibility reasons procedure divided
two groups. output FD pre-process, used feature extraction, used
search input LAMA -2011, FDSS -2 FD - AUTOTUNE (1 & 2). previous FD pre-processor
2 used common LAMA -2008, ARVAND LAMAR . optimization used
strategies evaluated. remaining planners totally independent FD pre-processing.
Moreover, bugs arose execution IPC-2014, issues domain
models required updates (Vallati, Chrpa, & McMcluskey, 2014a), planners updated
Mercury (Vallati, Chrpa, & McMcluskey, 2014b). issues fixed prior
running experimental evaluation presented article.

4. Experimental Evaluation
section, describe settings experimental evaluation present results
planners benchmarks used IPC-2014, specifically, Sequential Satisficing
track. addition, provide analysis diversity planner selection achieved
configurations.
4.1 Experimental Settings
evaluated different portfolio strategies described Section 3.3, permits different
portfolio configurations created. IBAC P2 IBAC P2-B5E run two predictive
model versions, one trained features (f-all) one trained selected fea2. version corresponds version used submit planners IPC-2011

670

fiT IBAC P P LANNING YSTEM

tures (f-34). Random strategy run 5 times average reported. addition,
included JASPER ERCURY planners comparison. planners competed IPC-2014. ERCURY (Domshlak, Hoffmann, & Katz, 2015) second best planner
terms IPC score JASPER (Xie, Muller, & Holte, 2014) second best planner terms
problems solved (coverage). test set used benchmarks IPC-2014,
updates described Section 3.4. test set comprises 14 domains 20 problems
domain.
Experiments run cluster Intel XEON 2.93 Ghz nodes, 8 GB RAM,
using Linux Ubuntu 12.04 LTS. planners cutoff 1, 800 seconds 4 GB RAM.
IBAC P configurations requiring feature extraction, process limited 4 GB RAM
(following IPC competition rules) 300 seconds (which maximum time used training
set obtain features, described Table 3). time extract features included
execution portfolio where, worse case, feature extraction process took 300 seconds
and, therefore, candidate planners 1, 500 run. system extract
features time, input features treated missing values.
4.2 Results
Table 5 shows results evaluated planners using IPC quality score. recall

score gives ratio Q
Qi planner problem, Qi quality best solution
found planner i, Q best solution found planner. planner solve
problem score 0.

Hiking
Openstacks
Thoughtful
GED
Barman
Parking
Visitall
Maintenance
Tetris
Childsnack
Transport
Floortile
CityCar
CaveDiving
Total

IBaCoP2
f-all
f-34

IBaCoP2-B5S
f-all
f-34

Mercury

Jasper

OET

B11

Def

Rand

IBaCoP

18,9
19.6
12.7
19.4
14.6
18.0
20.0
5.1
16.3
0.0
19.9
2.0
4.1
7.0

18.1
18.8
16.4
17.9
19.0
17.0
15.4
10.0
16.2
0.0
12.0
2.0
11.5
8.0

18.2
15.4
14.5
18.3
16.7
17.6
13.3
15.0
5.0
12.0
8.9
4.8
6.0
0.0

19.2
17.2
19.4
17.1
16.7
13.8
8.1
15.9
11.5
3.4
3.8
3.4
8.8
0.0

18.7
19.2
19.2
16.3
17.2
18.0
13.7
11.6
9.3
2.6
6.9
4.1
5.0
7.0

18.4
16.3
17.4
13.0
13.8
11.6
15.0
14.5
11.9
8.9
8.2
12.3
9.4
7.0

19.0
17.8
19.2
17.5
16.9
16.3
15.2
15.6
13.3
19.2
10.3
16.2
12.5
6.3

18.9
18.6
19.2
17.6
17.1
18.1
18.0
15.5
12.5
18.4
11.5
15.3
9.0
7.0

18.6
18.5
17.4
17.5
17.1
18.1
18.0
15.4
11.9
18.9
11.6
17.2
6.2
7.0

18.8
18.2
17.6
17.6
17.2
18.5
18.0
15.5
15.7
15.0
11.1
17.5
9.9
7.0

18.6
18.3
19.2
17.5
17.2
18.1
18.0
15.4
13.6
18.9
12.1
12.0
7.78
7.0

177.6

182.1

165.7

158.3

168.8

177.6

215.3

216.5

213.4

217.4

213.7

Table 5: Results IBAC P configurations. table includes results Jasper, Mercury
four baseline configurations, OET, Best 11, Default Random.

overall best planner IBAC P2-B5E (f-all), closely followed IBAC P2 (f-all).
difference two configurations negligible. configurations using predictive
models much better OET, Default, Best 11 Random. IBAC P good performance, comparable best performance. Moreover, big difference
671

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

configurations planners (Jasper Mercury). IBAC P based configurations 32
points higher cases.
Figure 2 details evolution number problems solved function run-time
elapsed. far right-hand point figure represents final coverage. best planner
terms coverage IBAC P, 249 problems, second IBAC P2 (f-all) 246.
Figure 2, planners show two different behaviors. one hand, asymptotic growing
number problems solved demonstrates giving time planners permit
number problems solved increased. JASPER extreme case, 300 seconds
almost unable improve. ERCURY problem, well portfolio configurations
take care diversity. However, IBAC P, IBAC P2 IBAC P2-B5E,
selected diverse set planners, show growing behavior throughout time.
250

200

Problems

150

100

IBaCoP
IBaCoP2
IBaCoP2-B5E
Random
Jasper
Default
Mercury
Best11
0ET

50

0
0

200

400

600

800

1000

1200

1400

1600

1800

Time

Figure 2: Comparison IBAC P configurations, baseline configurations, Mercury
Jasper planners.
results derive insights regarding different configurations. score
difference OET IBAC P reveals importance making pre-selection candidate
planner accurate filtering procedure. Pareto-dominance approach allows us
smaller set planners, means time per planner. trade-off
time per planner loosing diversity solvers, results suggest
important maintain diversity increasing running time per planner. instance,
11 best IPC-2011 planners (B11) obtain worse results using original 28 (OET), even
though B11 base planners longer running time. However, QT-Pareto filtering approach
able reduce number planners sacrificing diversity, produces good
results.
Reducing number planners portfolio configuration 11 5 puts risk
diversity solvers, shown results Def approach (the best 5 planners terms
672

fiT IBAC P P LANNING YSTEM

performance) Rand (the random selection 5 planners). Nevertheless, IBAC P2 (f-all)
(f-34) perform quite better Def Rand, demonstrates classification models
select average good subset planners solving particular task. results quite
promising exploiting empirical performance models planning portfolios. However,
current setting, results IBAC P2 quite similar IBAC P. Thus, classification models
manage reduce set planners without deteriorating performance fixed portfolio,
hardly contribute better overall performance.
Table 6 presents number problems solved 11 candidate planners. final
column maximum number problems solved complete set candidate
planners (i.e., problem solved least one candidate planners solved
problem). optimal selection 5 planners planning task would lead 253 problems
solved. IBAC P2 close optimum, confirming ability selecting good candidates
portfolio. default configuration solved 193 problems, average number problems
solved random configuration 207 problems. far best possible
value.
Hiking
Thoughtful
Openstacks
Tetris
GED
Transport
Parking
Barman
Maintenance
CityCar
Visitall
Childsnack
Floortile
CaveDiving
total

lama11
18
15
20
9
20
15
20
20
7
1
20
0
2
0

probe
20
12
4
14
20
12
9
19
8
0
10
0
2
0

FDA1
18
16
19
15
20
7
14
15
10
5
0
2
2
0

lama08
20
17
20
8
0
12
13
13
1
4
2
2
2
0

FDA2
20
12
20
1
0
6
2
2
8
5
0
0
5
0

lamar
20
14
20
13
0
7
18
15
1
8
0
2
2
0

arvand
20
20
20
18
0
5
0
0
17
19
2
8
1
0

fdss2
20
17
12
17
20
10
16
8
16
5
0
3
2
0

ya2-mt
4
13
0
0
0
20
0
0
3
2
20
0
0
0

LPG
20
8
1
0
14
0
0
0
8
0
1
7
19
0


3
5
0
0
0
0
0
0
6
14
0
20
0
7

Max
20
20
20
18
20
20
20
20
17
19
20
20
19
7

166

130

143

114

81

120

128

146

62

74

55

260

Table 6: Results candidate planners defined Table 1 maximum number problems
solved complete set planners.

set 5 planners selected per-instance configuration, regression
models contribute better performance. task estimating run time needed
solve problem difficult classification task (Schwefel, Wegener, & Weinert, 2013).
Additionally, given aggregated time predictions could exceed time limit, proposal rescales estimations alters real predictions. One alternative proposal keep
real prediction run planners order established confidence classification
prediction, one reaches time limit. However, preliminary experiments
development planner showed us approach compensate risk losing
diversity due fewer planner executions.
Another aspect analyzed performance planners new domains. IPC2014 incorporated seven new domains, means QT-Pareto Filtering predictive
models trained them. domains Cave Diving, Child-Snack, CityCar,
673

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

GED, Hiking, Maintenance Tetris. results conclude behavior
IBAC P configurations new domains average similar performance previously seen
domains.
4.3 Per-Instance Selection Planners
previous section showed benefit configuring portfolio per problem
set selected planners better adjusted problem, using fewer planners, providing
execution time planner. section want analyze diversity planner
selections made IBAC P2 see predictive models classifying planners good
solving specific domains identifying properties specific problems
different domains. Note test problems given domain usually range easy hard.
increase difficulty mainly due larger size problems. Nevertheless, increase
affects learning features different scale intensity.

lama-2011
probe
fd-autotune-1
lama-2008
fd-autotune-2
lamar
arvand
fdss-2
yahsp2-mt
LPG-td
madagascar

ita
Vis
rt
po
ns
l
Tra
tfu
gh
ou
Th
tris
Te
ng
rki
Pa tacks

en
e
Op nanc
e
int

ing

Hik


GE e
il
ort
Flo r

yC
k
Cit
ac
sn
ild
Ch
ing
Div

Ca n

rm
Ba

Figure 3: Proportion number times planner selected domain. red
dots, proportion IBAC P2 (f-all), blue dots, proportion IBAC P2
(f-34).

Figure 3 shows diversity planners according selection made IBAC P2 (blue
dots f-34 red dots f-all). x axis shows IPC-2014 domains axis lists
11 candidate planners portfolio use. size dots proportional
number times planner selected particular domain, i.e. number problems
planner selected. domain five dots one column (one domain), means
selected portfolio configuration problems domain. However, every
674

fiT IBAC P P LANNING YSTEM

column five dots reveals use different 5-planner sets different problems
domain. highlight analysis 11 planners selected
least one domain, 13 14 domains selections involve 5 planners.
Note, instance, LAMA -2011 best priori confidence solving problems,
sometimes used (i.e., selected 6 times Floortile 11 times Openstacks).
Furthermore, planners low priori probability selected, frequently
used domains (like LPG- TD Floortile).
Table 7 shows sum number times planner selected. maximum number times planner could selected 14 20 = 280. last column reports
average standard deviation number times planner selected
per domain approximations (all reduced set features).

LAMA -2011
PROBE
FD - AUTOTUNE -1
LAMA -2008
FD - AUTOTUNE -2
LAMAR
ARVAND
FDSS -2
YAHSP 2- MT
LPG- TD
MADAGASCAR

f-34
248
200
173
173
93
152
65
122
95
29
35

f-all
256
206
151
157
88
133
111
149
71
31
45

Average
18,00
14,50
11,57
14,50
6,46
10,18
6,29
9,68
5,93
2,14
2,86














STD
4,02
6,71
6,29
6,71
7,13
6,98
5,90
7,94
7,26
4,99
5,73

Table 7: Number times candidate planner selected two different classification
models (f-34 f-all).

addition previous analysis, wanted delve underlying mechanism
achieve per-instance selection planners. recall planners selected based
confidence success prediction. Therefore, order achieve different 5-planner sets
domain, ranking prediction confidence vary throughout problem.
visualize confirm fact, selected Tetris domain, one new
domains IPC-2014 shows good diversity selection shown Figure 3. domain
simplified version well-known Tetris game.
heatmap success prediction confidences appears Figure 4. glance realized general, planner higher success rate training time obtains higher confidence,
confidence ranking varies throughout different problems domain. Another way
read picture 5 darkest squares per column form set selected planners. instance, lama-2011 selected problems probe selected 18 times. hand
Madagascar selected, LPG-td selected 3 times.
675

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

lama-2011
probe
fd-autotune-1
lama-2008

Score

fd-autotune-2
lamar
arvand
fdss-2
yahsp2-mt
LPG-td
madagascar
0

1

2

3

4

5

6

7

8

9

10 11 12 13 14 15 16 17 18 19

Figure 4: Success prediction confidence provided classification model (f-all) planner
problem tetris Domain. Scale goes 0.0 (white) confidence
1.0 (dark blue) complete confidence.

5. Related Work
section, summarize relevant research portfolio configuration relates
work. addition, summarize different approaches characterization planning
tasks, cornerstone work predict behavior planners.
idea exploiting synergy different solvers improve performance individual ones applied propositional satisfiability problems (SAT), constraint satisfaction problems
(CSP), answer set programming (ASP) scope paper, Automated Planning.
SAT area carried extensive research importance selecting components
portfolio (Xu, Hutter, Hoos, & Leyton-Brown, 2012) select component (Lindauer, Hoos, Hutter, & Schaub, 2015b) automatically. study strategy selection area
includes per-instance selections (Lindauer, Hoos, & Hutter, 2015a). addition, intensive
study solver runtime prediction (Hutter, Xu, Hoos, & Leyton-Brown, 2015), including good
characterization satisfiability task. fields Artificial Intelligence, CSP portfolio configurations based machine learning techniques SUNNY (Amadini, Gabbrielli, &
Mauro, 2014b) empirical research (Amadini, Gabbrielli, & Mauro, 2014a). example
ASP, ASP-based Solver Scheduling (Hoos, Kaminski, Schaub, & Schneider, 2012) multicriteria optimization problem provides corresponding ASP encodings. paper
report main systems related Automated Planning detail.
5.1 Portfolios Automated Planning
Howe et al. (1999) describes one first planner portfolios. implement system called
BUS runs 6 planners whose goal find solution shortest period time.
achieve it, run planners portions time circular order one finds
solution. portfolio, planners sorted following estimation provided linear
676

fiT IBAC P P LANNING YSTEM

regression model success run-time so, case, use predictive models
behavior planners decide order execution. However, use 5 features
extracted PDDL description. domain, count number actions
number predicates. problem, count number objects, number predicates
initial conditions number goals. BUS minimizes expected cost implementing
sequence algorithms one works, contrast IBAC P IBAC P2, stop
assigned time over.
Fast Downward Stone Soup (FDSS, Helmert et al., 2011) based Fast Downward (FD)
planning system (Helmert, 2006), several versions different tracks. FDSS approach
select combine heuristics search algorithms. configuration combination search
algorithm group heuristics. training, evaluate possible configurations time
limit, select set configurations maximizes coverage. portfolio presented
IPC-2011 Sequential Satisficing Track, sort configurations decreasing order
coverage, hence beginning algorithms likely succeed quickly. time limit
component lowest value would still lead portfolio score training phase.
However, order important, since setting communicates quality best solution
found far following one, value used improve performance next
setting. Therefore, FDSS include configurations within FD framework. Conversely,
IBAC P IBAC P2 build portfolio using mixture generic planners different styles
techniques. Indeed FDSS one IBAC P candidate planners.
PbP (Gerevini et al., 2009) configures domain-specific portfolio. portfolio incorporates
macro-actions specific knowledge domains. incorporation knowledge establishes order subset planners contain macro-actions. running time assigned
round-robin strategy. portfolio incorporates seven planners (the latest version, PbP2,
adds lama-2008, see Gerevini et al., 2014). automatic portfolio configuration PbP IBA C P aims build different types planning systems: domain-optimized portfolio planner
given domain PbP IBAC P efficient domain-independent planner portfolio.
IBAC P PbP configuration processes significantly different. PbP uses several planners
focus macro-actions whilst IBAC P uses generic planners. execution scheduling
strategy PbP runs selected planners round-robin rather sequentially case
IBAC P.
Fast Downward Cedalion (Seipp, Sievers, Helmert, & Hutter, 2015) algorithm automatically configuring sequential planning portfolios parametric planner. Given parametric
planner set training instances, selects pair planner time iteratively. end
iteration instances current portfolio finds best solution removed
training set. algorithm stops total run time added configurations
reaches portfolio time limit training set becomes empty. Configurations generated
using SMAC (Hutter, Hoos, & Leyton-Brown, 2011) model-based algorithm configurator
remaining training instances. Cedalion configuration problems
different configuration per version IBAC P different configuration per problem. diversity candidate planner limited IBAC P may completely include independent base
planners. configuration processes resulting configured portfolios Cedalion
FDSS.
Fast Downward Uniform (Seipp et al., 2012) portfolio runs 21 automatically configured Fast
Downward instantiations sequentially amount time. Uniform portfolio approaches
677

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

configured using automatic parameter tuning framework ParamILS (Hutter, Hoos, LeytonBrown, & Stutzle, 2009) find fast configurations Fast Downward planning system 21
planning domains separately. runtime, configurations found run sequentially
amount time 85 seconds.
MiPlan (Nunez et al., 2015) sequential portfolio using Mixed-Integer Programming,
computes portfolio obtains best achievable performance respect selection
training planning tasks. case created sequential portfolio subset
sequential planners fixed times whilst IBAC P2 different configurations per problem.
approximation, planner consider portfolios, components.
contrast, IBAC P IBAC P2 includes planners appear competitions, i.e.
black boxes.
5.2 Features Planning Problems
construction models predict performance planners novel idea. Roberts et
al. (2008, 2009) showed models learned planners performance known benchmarks
2008 obtain high accuracy predicting whether planner succeed not.
use 19-32 features extracted domain problem definition. main difference
approach include features based SAS+ , heuristics initial state
fact balance relaxed plan. features come ground instantiation
problem, key differentiate tasks share feature values PDDL
level.
Torchlight (Hoffmann, 2011) toolkit allows search space topology analyzed
without actually running search. analysis based relation topology
delete relaxation heuristics causal graph well DTGs. feature extraction
process built top planner (Hoffmann & Nebel, 2001).
Recently, Fawcett et al. (2014) generated models accurately predicting planner run
time. models exploit large set instance features, including many features depicted
Section 2.1.2. features derived PDDL SAS+ representations problem, SAT encoding planning problem short runs planners. features
extracted Torchlight (Hoffmann, 2011). experimental results work indicate
performance models generated able produce accurate run time predictions. study
empirical performance models applied portfolio configurations.

6. Conclusion Future Work
work introduced framework creation configurable planning portfolios,
IBAC P. first step portfolio creation find small number planners maintains
diversity initial planner set based QT-Pareto score filtering. train predictive
models select promising sub-set planners solving particular planning task.
experimental evaluation confirmed great performance IBAC P IBAC P2
IPC-2014. summarize lessons learned development current IBAC P
portfolios following:
really matters generation good portfolio selection diverse set
planners. shown QT-Pareto score filtering reduces set candidate plan678

fiT IBAC P P LANNING YSTEM

ners preserving diversity. filtering produces better results rankings
based coverage quality score.
selection smaller sets planners portfolio configuration (e.g., sub-set 5
planners experiments) dangerous given portfolio might lose planner diversity.
observed situation Def Random configurations, select 5 11
planners.
portfolio configurations using classification models able select good subset
5 planners, uniformly distributed time outperformed selection provided
random default selection number planners.
Estimating runtime solving problem still difficult reason regression models providing additional useful information portfolio construction.
current form, predictive models hardly contribute overall performance
portfolio. Per-instance configurations using classification models achieve similar performance fixed portfolio, running fewer planners.
Even though current architecture benefits using predictive models limited,
results promising good performance IBAC P2 compared baseline
configurations. think room research direction. argument
static portfolio configurations (including IBAC P) limited components fixed
time bound base planner. performance upper-limit, computed MiPlan,
smaller achievable performance dynamic configuration.
per-instance configuration portfolio strategy could assign different times base planners.
future work want study additional features better characterization planning
tasks. computation could carried pre-process step, even information
first evaluated search nodes, could help making predictive models accurate.
models could incorporate information, instance, landmark graph time elapsed
computing initial state heuristics. future work study importance created
features, including comparison different groups accordance semantics
features.

7. Acknowledgments
thank authors base planners work based largely previous effort.
work partially supported Spanish projects TIN2011-27652-C03-02, TIN201238079-C03-02 TIN2014-55637-C2-1-R.

679

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

A. Appendix: Complete Feature Description
Appendix present list features used characterize planning task.
feature include brief description computed. Features grouped
category separate tables.
A.1 PDDL Features
N.
1
2
3
4
5
6
7
8

Name
Objects
Goals
Init
Types
Actions
Predicates
Axioms
Functions

Description
number objects problem.
number goals problem.
number facts initial state.
number types domain.
number actions domain.
number predicates domain.
number axioms domain.
number functions domain.

Table 8: PDDL Features.

A.2 FD Instantiation Features
N.
9
10
11
12
13

Name
Relevant facts
Cost metric
Generated rules
Relevant atoms
Auxiliary atoms

14

Final queue length

15

18

Total queue pushes
Implied
effects
removed
Effect preconditions
added
Translator variables

19

Derived variables

20
21
22
23
24

Translator facts
Mutex groups
Total mutex size
Translator operators
Total task size

16
17

Description
number facts marked relevant instantiation.
Whether action costs used not.
number created rules translation process create SAS+ task.
number relevant atoms found translator process.
number auxiliary atoms found translator process.
length queue end translation. queue auxiliary
list used translation process compute model.
number times element pushed queue.
number implied effects removed. implied effects
translator knows already included.
number implied effects added.
number created variables SAS+ formulation.
number state variables correspond derived predicates
artificial variables directly affected operator applications.
number facts pre-process takes account.
number mutex groups.
sum mutex group sizes.
number instantiated operators SAS+ formulation.
allowed memory translation process.

Table 9: Features extracted console output FD system.

680

fiT IBAC P P LANNING YSTEM

A.3 SAS+ Feature Description
recall CG, high-level variables variables defined value
goal. Although common definition CG consider edges weighted,
FD system computes edge weights CG number instantiated actions induced
edge. consider weights computing features.
N.

Name

25
26
27
28

Number Variables
High-Level Variables
TotalEdges
TotalWeight

29

VERatio

30

WERatio

31

WVRatio

32

HVRatio

33-35

InputEdge

36-38

InputWeight

39-41

OutputEdge

42-44

OutputWeight

45-47

InputEdgeHV

48-50

InputWeightHV

51-53

OutputEdgeHV

54-57

OutputWeightHV

Description
General Features
number variables CG.
number high-level variables.
number edges.
sum edge weights.
CG Ratios
ratio total number variables total number
edges. ratio shows level connection CG.
ratio sum weights number edges.
ratio shows average weight edges.
ratio sum weights number variables.
ratio number high-level variables total number variables. ratio shows percentage variables involved
problem goals.
Statistics CG
Maximum, average standard deviation number incoming
edges variable.
Maximum, average standard deviation sum weights
incoming edges variable.
Maximum, average standard deviation number outgoing
edges variable.
Maximum, average standard deviation sum weights
incoming edges variable.
Statistics high-level Variables
number incoming edges high level variables.
value produces three new features following computation
InputEdgeCG (features 33-35).
edge weight sum incoming edges high level
variables. value produces three new features following
computation InputWeightCG.
number outgoing edges high level variables.
sum weights incoming edges high level variables.

Table 10: Features Causal Graph.

681

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

N.

Name

58
59

Number Vertices
Total Edges

60

Total Weight

61

edVa Ratio

62

weEdRatio

63

weVaRatio

64-66

Input Edge

67-69

Input Weight

70-72

Output Edge

73-75

Output Weight

Description
General Aggregated Features DTG
sum number nodes DTGs.
sum number edges DTGs.
sum edge weights DTGs. edge weight DTG
corresponds cost applying action induced edge.
DTG Ratios
ratio total number edges total numbers
variables. ratio shows level connection DTG.
ratio sum weights number edges.
ratio shows number restrictions need make transition.
ratio sum weights number variables.
Statistics DTGs
Maximum, average standard deviation number incoming
edges vertex DTG.
Maximum, average standard deviation sum weights
incoming edges nodes.
Maximum, average standard deviation number outgoing
edges vertex DTG.
Maximum, average standard deviation sum weights
outgoing edges nodes.

Table 11: Features aggregate information DTGs.

682

fiT IBAC P P LANNING YSTEM

A.4 Heuristic Features
N.

Name

76

Max

77

Landmark cut

78

Landmark
count
Goal count

79



80

Additive

81

Causal Graph

82

Contextenhanced
additive

Description
(Bonet, Loerincs, & Geffner, 1997; Bonet & Geffner, 2000) maximum
accumulated costs paths goal propositions relaxed
problem.
(Helmert & Domshlak, 2009) sum costs disjunctive action
landmark represents cut justification graph towards goal propositions.
(Richter, Helmert, & Westphal, 2008) sum costs minimum
cost achiever unsatisfied required landmark.
number unsatisfied goals.
(Hoffmann & Nebel, 2001) cost plan reaches goals
relaxed problem ignores negative interactions.
(Bonet et al., 1997; Bonet & Geffner, 2000) sum accumulated costs
paths goal propositions relaxed problem.
(Helmert, 2004) cost reaching goal given search state
solving number sub problems planning task derived
causal graph.
(Helmert & Geffner, 2008) causal graph heuristic modified use pivots
define contexts relevant heuristic computation.

Table 12: Unit cost heuristics included features.

A.5 Fact Balance
N.

Name

83-85

RP init

86-88

RP goal

89

Ratio

Description
Minimum, average variance number times fact
initial state deleted computation relaxed plan.
Minimum, average variance number times goal
deleted computation relaxed plan.
ratio value max heuristic. proportion
shows idea parallelization plan.

Table 13: Fact balance features.

683

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

B. Appendix: Learning Results
appendix shows detailed results machine learning algorithms used train predictive models.
B.1 Classification
Algorithm
rules.ZeroR
rules.Ridor
rules.PART
rules.JRip
rules.DecisionTable
rules.ConjunctiveRule
trees.REPTree
trees.RandomTree
trees.RandomForest
trees.LMT
trees.J48
trees.ADTree
trees.NBTree
trees.DecisionStump
lazy.LWL
lazy.IBk -K 1
lazy.IBk -K 3
lazy.IBk -K 5
meta.RotationForest
meta.AttributeSelectedClassifier
meta.ClassificationViaClustering
meta.ClassificationViaRegression
meta.Bagging
meta.MultiClassClassifier
functions.SimpleLogistic
functions.MultilayerPerceptron
functions.RBFNetwork
functions.SMO
bayes.NaiveBayes
bayes.NaiveBayesUpdateable
bayes.BayesNet

f-all dataset
61.72 0.03
82.52 2.48
90.81 0.89
87.21 1.38
85.78 0.98
69.33 1.20
89.08 0.85
86.39 1.81
90.96 0.78
91.11 0.72
90.84 1.01
75.46 1.24
90.38 0.88
67.96 0.96
67.96 0.96
85.93 0.84
86.04 0.90
85.36 0.91
93.39 0.70
89.69 0.89
52.32 1.98
90.82 0.84
90.99 0.74
77.15 1.09
76.37 1.12
87.27 1.65
67.71 1.03
75.39 1.16
69.00 0.98
69.00 0.98
75.43 1.29

f-34 dataset
61.72 0.03
81.76 2.11
89.62 0.89
86.26 1.20
84.94 1.37
69.64 1.61
88.06 0.89
87.91 0.95
90.27 0.85
90.03 0.94
89.24 0.87
74.39 1.30
89.47 0.92
64.10 1.30
63.48 1.86
82.97 1.03
84.13 1.03
84.17 1.01
92.35 0.73
88.64 1.00
57.99 2.66
89.80 0.75
89.83 0.85
75.02 1.14
74.48 1.23
88.65 1.01
68.10 1.17
73.94 1.10
68.87 0.97
68.87 0.97
75.05 1.21

























Table 14: Accuracy standard deviation training algorithm using 10-fold crossvalidation. Also, results t-test (OMahony, 1986) two training sets shown.
Symbols , means statistically significant improvement degradation respectively.
significance level t-test 0.05 baseline left column.

684

fiT IBAC P P LANNING YSTEM

B.2 Regression

trees.DecisionStump
trees.REPTree
trees.RandomTree
trees.RandomForest
functions.M5P
rules.ConjunctiveRule
rules.DecisionTable
rules.M5Rules
meta.Bagging
meta.AdditiveRegression
lazy.IBk 1
lazy.IBk 3
lazy.IBk 5
lazy.KStar
lazy.LWL
functions.LinearRegression
functions.MultilayerPerceptron
functions.LeastMedSq
functions.RBFNetwork
functions.SMOreg

f-all dataset
RAE

82.09 2.36 0.42 0.05
57.70 3.40 0.66 0.05
59.28 6.06 0.55 0.07
52.54 2.66 0.71 0.04
60.44 13.26 0.59 0.18
87.31 2.79 0.38 0.06
49.87 3.03 0.69 0.04
90.60 138.25 0.58 0.18
50.95 2.71 0.74 0.04
80.91 3.21 0.51 0.04
92.96 11.09 0.36 0.06
74.31 6.31 0.47 0.06
73.03 5.91 0.47 0.06
69.26 3.35 0.44 0.05
81.82 2.30 0.43 0.05
77.71 2.55 0.55 0.04
86.01 72.86 0.66 0.05
66.36 2.94 0.33 0.08
94.201.60 0.23 0.05
57.01 2.88 0.48 0.05

f-34 dataset
RAE

82.09 2.36 0.42 0.05
56.69 3.36 0.67 0.05
53.71 4.54 0.61 0.06
45.62 2.68 0.76 0.03
56.38 4.22 0.65 0.09
87.25 2.80 0.39 0.06
51.19 2.78 0.68 0.05
65.84 12.74 0.61 0.14
50.62 2.58 0.74 0.04
79.93 3.29 0.51 0.04
66.73 5.17 0.54 0.06
63.57 4.25 0.60 0.05
64.38 3.81 0.60 0.05
67.75 3.36
0.470.05
81.82 2.33 0.43 0.05
78.58 2.52 0.51 0.04
81.59 45.93 0.66 0.05
66.29 3.01 0.31 0.07
94.251.54
0.210.04
58.75 2.62 0.45 0.05








Table 15: Results 10-fold cross-validation regression models. RAE Relative
Absolute Error correlation coefficient. small RAE values better.
Symbols , means statistically significant improvement degradation respectively.
significance level t-test 0.05 baseline left column.

685

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

C. Appendix: Training Results

openstacks
pathways
rovers
storage
tpp
trucks
pipesworld
cybersec
Openstacks-adl
openstacks
pegsol
scanalyzer
sokoban
transport
woodworking
elevators
barman
elevators
floortile
nomystery
openstacks
parcprinter
parking
pegsol
scanalyzer
sokoban
tidybot
transport
visitall
woodworking
Gold-miner
Matching-bw
N-puzzle
parking
sokoban
thoughtful
barman
blocksworld
depots
gripper
parking
rovers
satellite
spanner
tpp
Total

L-11

Probe

FDA1

L-08

FDA2

Lamar

Arvand

FDSS2

ya2-mt

LPG

30
30
40
40
30
19
42
28
31
30
30
30
29
18
23
30
20
20
6
10
20
20
20
20
20
19
16
19
20
20
30
25
29
28
23
0
9
29
1
0
18
30
16
0
30

30
30
40
40
30
8
44
24
31
30
30
30
27
10
30
29
20
20
5
6
14
14
19
20
20
17
18
20
20
20
30
15
20
24
23
18
5
30
30
0
9
30
10
0
20

30
26
40
40
30
18
40
28
31
30
30
30
29
17
25
30
20
20
7
10
20
20
19
20
20
19
15
11
2
20
30
24
30
25
30
0
0
22
0
0
6
30
3
0
30

30
29
40
40
30
16
38
28
31
30
30
30
25
17
26
25
4
6
3
12
20
1
20
20
20
15
14
19
20
14
30
23
29
28
18
0
0
21
0
0
13
29
3
0
30

30
29
40
40
30
22
33
26
31
30
30
27
27
18
24
30
6
17
9
19
20
14
9
20
17
16
17
10
5
14
26
23
9
16
30
0
0
15
0
30
1
24
29
0
6

30
30
40
40
30
15
43
27
31
30
30
30
25
17
25
27
6
11
3
12
20
0
20
20
20
14
19
3
11
9
29
17
27
30
17
0
0
0
6
0
19
30
1
0
21

27
30
40
40
30
15
46
28
31
30
30
30
8
19
30
30
0
20
3
19
20
20
4
20
20
2
17
15
10
20
30
16
6
17
30
0
0
0
0
4
4
30
2
0
30

30
0
40
40
30
20
42
28
31
30
30
30
29
15
30
30
17
20
7
12
19
20
20
20
20
19
18
15
6
20
0
0
0
0
30
0
13
20
0
0
9
30
22
0
25

0
0
40
40
30
0
41
0
0
1
22
27
0
11
23
2
12
0
8
10
0
13
3
15
17
0
0
20
20
19
30
25
20
13
28
22
0
16
29
0
0
30
13
0
30

27
30
40
40
24
11
33
7
1
0
1
0
0
0
0
0
0
0
12
0
2
0
0
0
0
0
15
0
8
0
30
22
30
13
15
7
0
29
6
30
0
11
30
30
1


11
30
40
40
30
21
14
0
16
15
27
21
2
9
2
0
0
0
0
17
0
20
0
17
11
0
1
0
0
1
30
1
0
0
22
0
0
0
0
0
0
14
0
15
9

#
30
30
40
40
30
30
50
30
31
30
30
30
30
30
30
30
20
20
20
20
20
20
20
20
20
20
20
20
20
20
30
30
30
30
30
30
30
30
30
30
30
30
30
30
30

998

960

927

877

869

835

823

837

630

505

436

1251

Table 16: Solved problems training phase. first part table results IPC2005, second part IPC-2008 IPC-2011 satisficing tracks. two last rows
(from Gold-miner tpp) IPC-2008-2011 learning track. last column
number problems included training.

686

fiT IBAC P P LANNING YSTEM

D. Appendix: Planners
following list set planners pre-selected candidates Pareto-dominance filtering
described Section 3.1
Arvand (Nakhost, Muller, Valenzano, & Xie, 2011): stochastic planner uses Monte
Carlo random walks balance exploration exploitation heuristic search. version
uses online learning algorithm find best configuration parameters given
problem.
Fast Downward Autotune-1 Fast Downward Autotune-2 (Fawcett, Helmert, Hoos, Karpas,
Roger, & Seipp, 2011): two instantiations FD planning system automatically configured performance wide range planning domains, using well-known ParamILS
configurator. planners use three main types search combination several heuristics.
Fast Downward Stone Soup-2 (Helmert et al., 2011) (FDSS-2): sequential portfolio
several search algorithms heuristics. Given results training benchmarks,
best combination algorithms heuristics found hill-climbing search. Here,
information communicated component solvers quality best
solution found far.
LAMA-2008 LAMA-2011 (Richter & Westphal, 2010; Richter, Westphal, & Helmert,
2011) propositional planner based combination landmark count heuristic
heuristic. search performs set weighted iteratively decreasing weights.
planner developed within FD Planning System (Helmert, 2006).
Lamar (Olsen & Bryce, 2011) modification LAMA planner includes randomized construction landmark count heuristic.
Madagascar (Rintanen, 2011): implements several innovations SAT planning, including
compact parallelized/interleaved search strategies SAT-based heuristics.
Probe (Lipovetzky & Geffner, 2011): exploits idea wisely constructed lookaheads
probes, action sequences computed without searching given state
quickly go deep state space, terminating either goal failure. technique
integrated within standard greedy best first search.
YAHSP2-MT (Vidal, 2011) extracts information relaxed plan order generate
lookahead states. strategy implemented complete best-first search algorithm,
modified take helpful actions account.
LPG-td (Gerevini et al., 2006) based stochastic local search space particular
action graphs derived planning problem specification.

References
Amadini, R., Gabbrielli, M., & Mauro, J. (2014a). Portfolio approaches constraint optimization
problems. TPLP, 8426, 2135.
687

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

Amadini, R., Gabbrielli, M., & Mauro, J. (2014b). SUNNY: lazy portfolio approach constraint
solving. TPLP, 14(4-5), 509524.
Blum, A. L., & Langley, P. (1997). Selection relevant features examples machine learning.
Artificial intelligence, 97(1), 245271.
Bonet, B., & Geffner, H. (2000). Planning heuristic search: New results. Recent Advances
AI Planning, pp. 360372. Springer.
Bonet, B., Loerincs, G., & Geffner, H. (1997). robust fast action selection mechanism
planning. AAAI/IAAI, pp. 714719.
Breiman, L. (1996). Bagging predictors. Machine Learning, 24(2), 123140.
Breiman, L. (2001). Random forests. Machine learning, 45(1), 532.
Cenamor, I., de la Rosa, T., & Fernandez, F. (2012). Mining IPC-2011 results. Proceedings
Third Workshop International Planning Competition - ICAPS.
Cenamor, I., de la Rosa, T., & Fernandez, F. (2013). Learning predictive models configure planning portfolios. Proceedings Workshop Planning Learning - ICAPS.
Censor, Y. (1977). Pareto optimality multiobjective problems. Applied Mathematics Optimization, 4(1), 4159.
Dietterich, T. G. (2000). Ensemble methods machine learning. Kittler, J., & Roli, F.
(Eds.), Multiple Classifier Systems, First International Workshop, MCS 2000, Cagliari, Italy,
June 21-23, 2000, Proceedings, Vol. 1857 Lecture Notes Computer Science, pp. 115.
Springer.
Domshlak, C., Hoffmann, J., & Katz, M. (2015). Red-black planning: new systematic approach
partial delete relaxation. Artificial Intelligence, 221, 73114.
Fawcett, C., Helmert, M., Hoos, H., Karpas, E., Roger, G., & Seipp, J. (2011). FD-Autotune:
Domain-specific configuration using fast-downward. Proceedings Workshop
Planning Learning - ICAPS, 2011(8).
Fawcett, C., Vallati, M., Hutter, F., Hoffmann, J., Hoos, H. H., & Leyton-Brown, K. (2014). Improved features runtime prediction domain-independent planners. Proceedings
24th International Conference Automated Planning Scheduling (ICAPS-14).
Gerevini, A., Saetti, A., & Vallati, M. (2009). automatically configurable portfolio-based planner
macro-actions: PbP. Proceedings 19th International Conference Automated
Planning Scheduling (ICAPS-09).
Gerevini, A., Saetti, A., & Serina, I. (2006). approach temporal planning scheduling
domains predictable exogenous events. Journal Artificial Intelligence Research, 25,
187231.
Gerevini, A., Saetti, A., & Vallati, M. (2014). Planning automatic portfolio configuration:
PbP approach. Journal Artificial Intelligence Research, 50, 639696.
Ghallab, M., Nau, D., & Traverso, P. (2004). Automated planning: theory & practice. Access Online
via Elsevier.
Gomes, C. P., & Selman, B. (2001). Algorithm portfolios. Artificial Intelligence, 126(1), 4362.
688

fiT IBAC P P LANNING YSTEM

Grabczewski, K., & Jankowski, N. (2005). Feature selection decision tree criterion. Proceedings Fifth International Conference Hybrid Intelligent Systems (HIS05), pp.
212217. IEEE.
Han, J., Kamber, M., & Pei, J. (2011). Data mining: concepts techniques. Elsevier.
Helmert, M. (2004). planning heuristic based causal graph analysis. Proceedings
14th International Conference Automated Planning Scheduling (ICAPS-04), Vol. 16,
pp. 161170.
Helmert, M. (2006). Fast Downward Planning System. Journal Artificial Intelligence Research, 26, 191246.
Helmert, M. (2009). Concise finite-domain representations PDDL planning tasks. Artificial
Intelligence, 173, 503535.
Helmert, M., & Domshlak, C. (2009). Landmarks, critical paths abstractions: Whats difference anyway?. Proceedings 19th International Conference Automated
Planning Scheduling (ICAPS-09).
Helmert, M., & Geffner, H. (2008). Unifying causal graph additive heuristics. Proceedings 18th International Conference Automated Planning Scheduling (ICAPS08), pp. 140147.
Helmert, M., Roger, G., Seipp, J., Karpas, E., Hoffmann, J., Keyder, E., Nissim, R., Richter, S.,
& Westphal, M. (2011). Fast downward stone soup. Seventh International Planning
Competition, IPC-7 planner abstracts, 38.
Hoffmann, J. (2003). metric-FF planning system: Translating ignoring delete lists numeric
state variables. Journal Artificial Intelligence Research, 20, 291341.
Hoffmann, J. (2011). Analyzing search topology without running search: connection
causal graphs h+. Journal Artificial Intelligence Research, 41, 155229.
Hoffmann, J., Edelkamp, S., Thiebaux, S., Englert, R., dos Santos Liporace, F., & Trug, S. (2006).
Engineering benchmarks planning: domains used deterministic part IPC-4.
Journal Artificial Intelligence Research, 26, 453541.
Hoffmann, J., & Nebel, B. (2001). planning system: Fast plan generation heuristic
search. Journal Artificial Intelligence Research, 14, 253302.
Hoos, H., Kaminski, R., Schaub, T., & Schneider, M. T. (2012). aspeed: ASP-based solver scheduling. ICLP (Technical Communications), 17, 176187.
Howe, A. E., Dahlman, E., Hansen, C., Scheetz, M., & von Mayrhauser, A. (1999). Exploiting
competitive planner performance. Biundo, S., & Fox, M. (Eds.), Recent Advances AI
Planning, 5th European Conference Planning, ECP99, Durham, UK, September 8-10,
1999, Proceedings, Vol. 1809 Lecture Notes Computer Science, pp. 6272. Springer.
Hutter, F., Hoos, H. H., & Leyton-Brown, K. (2011). Sequential model-based optimization
general algorithm configuration. Learning Intelligent Optimization, pp. 507523.
Springer.
Hutter, F., Hoos, H. H., Leyton-Brown, K., & Stutzle, T. (2009). ParamILS: automatic algorithm
configuration framework. Journal Artificial Intelligence Research, 36, 267306.
689

fiC ENAMOR , DE LA ROSA & F ERN ANDEZ

Hutter, F., Xu, L., Hoos, H., & Leyton-Brown, K. (2015). Algorithm runtime prediction: Methods
evaluation (extended abstract). Yang, Q., & Wooldridge, M. (Eds.), Proceedings
Twenty-Fourth International Joint Conference Artificial Intelligence, IJCAI 2015, Buenos
Aires, Argentina, July 25-31, 2015, pp. 41974201. AAAI Press.
Kohavi, R. (1995). power decision tables. Machine Learning: ECML-95, pp. 174189.
Springer.
Linares Lopez, C., Celorrio, S. J., & Olaya, A. G. (2015). deterministic part seventh
international planning competition. Artificial Intelligence, 223, 82119.
Lindauer, M. T., Hoos, H. H., & Hutter, F. (2015a). sequential algorithm selection parallel
portfolio selection. Dhaenens, C., Jourdan, L., & Marmion, M. (Eds.), Learning Intelligent Optimization - 9th International Conference, LION 9, Lille, France, January 12-15,
2015. Revised Selected Papers, Vol. 8994 Lecture Notes Computer Science, pp. 116.
Springer.
Lindauer, M. T., Hoos, H. H., Hutter, F., & Schaub, T. (2015b). Autofolio: automatically configured algorithm selector. Journal Artificial Intelligence Research, 53, 745778.
Lipovetzky, N., & Geffner, H. (2011). Searching plans carefully designed probes.
Proceedings 21st International Conference Automated Planning Scheduling
(ICAPS-11), pp. 154161.
Malitsky, Y., Sabharwal, A., Samulowitz, H., & Sellmann, M. (2013). Algorithm portfolios based
cost-sensitive hierarchical clustering. Proceedings Twenty-Third international
joint conference Artificial Intelligence, pp. 608614. AAAI Press.
Nakhost, H., Muller, M., Valenzano, R., & Xie, F. (2011). Arvand: art random walks.
Seventh International Planning Competition, IPC-7 planner abstracts, 1516.
Nebel, B. (2000). compilability expressive power propositional planning formalisms.
Journal Artificial Intelligence Research, 12, 271315.
Nunez, S., Borrajo, D., & Linares Lopez, C. (2015). Automatic construction optimal static
sequential portfolios AI planning beyond. Artificial Intelligence, 226, 75101.
Olsen, A., & Bryce, D. (2011). Randward Lamar: Randomizing heuristic. Seventh
International Planning Competition, IPC-7 planner abstracts, 55.
OMahony, M. (1986). Sensory evaluation food: statistical methods procedures, Vol. 16.
CRC Press.
Quinlan, J. R. (1993). C4. 5: programs machine learning, Vol. 1. Morgan kaufmann.
Richter, S., Helmert, M., & Westphal, M. (2008). Landmarks revisited. AAAI, Vol. 8, pp. 975
982.
Richter, S., & Westphal, M. (2010). LAMA planner: Guiding cost-based anytime planning
landmarks. Journal Artificial Intelligence Research, 39(1), 127177.
Richter, S., Westphal, M., & Helmert, M. (2011). Lama 2008 2011. Seventh International
Planning Competition, IPC-7 planner abstracts, 50.
Rintanen, J. (2011). Madagascar: Efficient planning SAT. Seventh International Planning
Competition, IPC-7 planner abstracts, 61.
690

fiT IBAC P P LANNING YSTEM

Roberts, M., & Howe, A. (2009). Learning planner performance. Artificial Intelligence, 173,
536561.
Roberts, M., Howe, A. E., Wilson, B., & desJardins, M. (2008). makes planners predictable?.
Proceedings 18th International Conference Automated Planning Scheduling (ICAPS-08), pp. 288295.
Rodriguez, J. J., Kuncheva, L. I., & Alonso, C. J. (2006). Rotation forest: new classifier ensemble
method. IEEE Transactions Pattern Analysis Machine Intelligence, 28(10), 1619
1630.
Schwefel, H.-P., Wegener, I., & Weinert, K. (2013). Advances computational intelligence: Theory
practice. Springer Science & Business Media.
Seipp, J., Braun, M., Garimort, J., & Helmert, M. (2012). Learning portfolios automatically tuned
planners. McCluskey, L., Williams, B., Silva, J. R., & Bonet, B. (Eds.), Proceedings
Twenty-Second International Conference Automated Planning Scheduling, ICAPS
2012, Atibaia, Sao Paulo, Brazil, June 25-19, 2012. AAAI.
Seipp, J., Sievers, S., Helmert, M., & Hutter, F. (2015). Automatic configuration sequential
planning portfolios. Bonet, B., & Koenig, S. (Eds.), Proceedings Twenty-Ninth
AAAI Conference Artificial Intelligence, January 25-30, 2015, Austin, Texas, USA., pp.
33643370. AAAI Press.
Vallati, M. (2012). guide portfolio-based planning. Multi-disciplinary Trends Artificial
Intelligence, pp. 5768. Springer.
Vallati, M., Chrpa, L., & Kitchin, D. E. (2015). Portfolio-based planning: State art, common
practice open challenges. AI Communications, 29, 117.
Vallati, M., Chrpa, L., & McMcluskey, L. (2014a).
https://helios.hud.ac.uk/scommv/IPC-14/benchmark.html.

Competition

Domains.

Vallati, M., Chrpa, L., & McMcluskey, L. (2014b). Source code Erratum Deterministic part.
https://helios.hud.ac.uk/scommv/IPC-14/errPlan.html.
Vidal, V. (2011). YAHSP2: Keep simple, stupid. Seventh International Planning Competition,
IPC-7 planner abstracts, 83.
Witten, I. H., & Frank, E. (2005). Data Mining: Practical Machine Learning Tools Techniques.
2nd Edition, Morgan Kaufmann.
Xie, F., Muller, M., & Holte, R. (2014). Jasper: art exploration greedy best first search.
Planner abstracts. IPC-2014.
Xu, L., Hoos, H., & Leyton-Brown, K. (2010). Hydra: Automatically configuring algorithms
portfolio-based selection. Proceedings Twenty-Fourth AAAI Conference Artificial
Intelligence (AAAI 2010), pp. 210216.
Xu, L., Hutter, F., Hoos, H., & Leyton-Brown, K. (2012). Evaluating component solver contributions portfolio-based algorithm selectors. Theory Applications Satisfiability
TestingSAT 2012, pp. 228241. Springer.
Xu, L., Hutter, F., Hoos, H. H., & Leyton-Brown, K. (2008). SATzilla: Portfolio-based algorithm
selection SAT. Journal Artificial Intelligence Research, 32, 565606.

691


