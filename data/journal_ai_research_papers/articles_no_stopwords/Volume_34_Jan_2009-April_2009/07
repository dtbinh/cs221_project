journal artificial intelligence

submitted published

wikipedia semantic interpretation
natural language processing
evgeniy gabrilovich
shaul markovitch

gabr yahoo inc com
shaulm cs technion ac il

department computer science
technionisrael institute technology
technion city haifa israel

abstract
adequate representation natural language semantics requires access vast amounts
common sense domain specific world knowledge prior work field
purely statistical techniques make use background knowledge limited
lexicographic knowledge bases wordnet huge manual efforts
cyc project propose novel method called explicit semantic analysis esa
fine grained semantic interpretation unrestricted natural language texts method
represents meaning high dimensional space concepts derived wikipedia
largest encyclopedia existence explicitly represent meaning text terms
wikipedia concepts evaluate effectiveness method text categorization computing degree semantic relatedness fragments natural
language text esa significant improvements previous state
art tasks importantly due use natural concepts esa model
easy explain human users

introduction
recent proliferation world wide web common availability inexpensive storage
media accumulate time enormous amounts digital data contributed
importance intelligent access data sheer amount data available
emphasizes intelligent aspect accessno one willing capable browsing
small subset data collection carefully selected satisfy ones
precise information need
artificial intelligence long aimed endowing machines ability
understand natural language one core issues challenge represent language semantics way manipulated computers prior work
semantics representation purely statistical techniques lexicographic knowledge elaborate endeavors manually encode large amounts knowledge simplest
represent text semantics treat text unordered bag words
words possibly stemmed become features textual object
sheer ease makes reasonable candidate many information retrieval
tasks search text categorization baeza yates ribeiro neto sebastiani
however simple model reasonably used texts fairly long
performs sub optimally short texts furthermore little address two
main natural language processing nlp polysemy synonymy
c

ai access foundation rights reserved

figabrilovich markovitch

latent semantic analysis lsa deerwester dumais furnas landauer harshman
another purely statistical technique leverages word co occurrence information large unlabeled corpus text lsa use explicit human organized
knowledge rather learns representation applying singular value decomposition
svd words documents co occurrence matrix lsa essentially dimensionality reduction technique identifies number prominent dimensions data
assumed correspond latent concepts meanings words documents
represented space defined concepts
lexical databases wordnet fellbaum rogets thesaurus roget
encode important relations words synonymy hypernymy meronymy
approaches resources budanitsky hirst jarmasz map text
words word senses use latter concepts however lexical resources offer
little information different word senses thus making word sense disambiguation
nearly impossible achieve another drawback approaches creation
lexical resources requires lexicographic expertise well lot time effort consequently resources cover small fragment language lexicon specifically
resources contain proper names neologisms slang domain specific technical
terms furthermore resources strong lexical orientation predominantly contain information individual words little world knowledge general
inherently limited individual words approaches require extra level
sophistication handle longer texts mihalcea corley strapparava example
computing similarity pair texts amounts comparing word one text
word text
studies artificial intelligence long recognized importance knowledge
solving general natural language processing particular back
early years ai buchanan feigenbaum formulated knowledge
power hypothesis postulated power intelligent program perform
task well depends primarily quantity quality knowledge
task
computer programs face tasks require human level intelligence natural language processing natural use encyclopedia endow machine
breadth knowledge available humans however several obstacles
way encyclopedic knowledge first knowledge available textual
form may require natural language understanding major
right furthermore even language understanding may enough texts written
humans normally assume reader possesses large amount common sense knowledge
omitted even detailed encyclopedia articles lenat thus
circular dependencyunderstanding encyclopedia articles requires natural language
understanding capabilities latter turn require encyclopedic knowledge
address situation lenat colleagues launched cyc project aims
explicitly catalog common sense knowledge humankind
developed methodology makes possible use encyclopedia directly
without need manually encoded common sense knowledge observe encyclopedia consists large collection articles provides comprehensive
exposition focused single topic thus view encyclopedia collection con

fiwikipedia semantic interpretation

cepts corresponding articles accompanied large body text article
contents propose use high dimensional space defined concepts order
represent meaning natural language texts compared bag words lsa
approaches concepts allows computer benefit huge amounts world
knowledge normally accessible humans compared electronic dictionaries
thesauri method uses knowledge resources order magnitude larger
uniformly treats texts arbitrarily longer single word even
importantly method uses body text accompanies concepts order
perform word sense disambiguation later knowledge rich concepts
addresses polysemy synonymy longer manipulate mere words call
method explicit semantic analysis esa uses knowledge concepts explicitly
defined manipulated humans
applicable many nlp tasks whose input document shorter
natural language utterance output decision document contents
examples tasks information retrieval whether document relevant text
categorization whether document belongs certain category comparing pairs
documents assess similarity
observe documents manipulated tasks given form
encyclopedic knowledge intend useplain text key observation allows us
circumvent obstacles enumerated use encyclopedia directly without
need deep language understanding pre cataloged common sense knowledge
quantify degree relevance wikipedia concept input text comparing
text article associated concept
let us illustrate importance external knowledge couple examples without external knowledge specifically knowledge financial markets one infer
little information brief news title bernanke takes charge however
developed consulting wikipedia following concepts
highly relevant input ben bernanke federal reserve chairman
federal reserve alan greenspan bernankes predecessor monetarism economic theory money supply central banking inflation deflation another
example consider title apple patents tablet mac without deep knowledge hitech industry gadgets one finds hard predict contents news item
wikipedia identify following related concepts apple computer mac os
macintosh operating system laptop general name portable computers
tablet mac specific example aqua gui mac os x ipod another prominent product apple apple newton name apples early personal digital
assistant
ease presentation examples showed concepts identified
esa relevant input however essence method representing meaning text weighted combination wikipedia concepts
thus consider tasks machine translation natural language generation whose output
includes piece text input
note correctly identify concept representing computer company apple computer
rather fruit apple



figabrilovich markovitch

depending nature task hand use entire vectors concepts
use relevant concepts enrich bag words representation
contributions twofold first propose methodology
use wikipedia enriching representation natural language texts
named explicit semantic analysis effectively capitalizes human knowledge encoded
wikipedia leveraging information cannot deduced solely input texts
processed second evaluate esa two commonly occurring nlp tasks namely text
categorization computing semantic relatedness texts tasks esa
resulted significant improvements existing state art performance
recently esa used researchers variety tasks consistently proved
superior approaches explicitly used large scale repositories human
knowledge gurevych mueller zesch implemented esa
german language wikipedia found superior judging semantic relatedness
words compared system german version wordnet germanet chang
ratinov roth srikumar used esa text classification task without explicit
training set learning knowledge encoded wikipedia milne witten
found esa compare favorably approaches solely hyperlinks
thus confirming wealth textual descriptions wikipedia exlicitly superior
structural information alone

explicit semantic analysis
meaning word cat one way interpret word cat via
explicit definition cat mammal four legs belongs feline species
etc another way interpret meaning cat strength association
concepts know cat relates strongly concepts feline pet somewhat
less strongly concepts mouse tom jerry etc
use latter association method assign semantic interpretation words
text fragments assume availability vector basic concepts c cn
represent text fragment vector weights w wn wi represents
strength association ci thus set basic concepts viewed
canonical n dimensional semantic space semantics text segment corresponds
point space call weighted vector semantic interpretation vector

canonical representation powerful effectively allows us estimate
semantic relatedness text fragments distance space following
section describe two main components scheme set basic concepts
maps text fragments interpretation vectors
wikipedia repository basic concepts
build general semantic interpreter represent text meaning variety
tasks set basic concepts needs satisfy following requirements
comprehensive enough include concepts large variety topics


fiwikipedia semantic interpretation

constantly maintained concepts promptly added
needed
since ultimate goal interpret natural language would concepts
natural concepts recognized used human beings
concept ci associated text di determine strength
affinity term language
creating maintaining set natural concepts requires enormous effort many
people luckily collection already exists form wikipedia one
largest knowledge repositories web wikipedia available dozens languages
english version largest contains million words nearly
one million articles contributed volunteer editors even though wikipedia
editors required established researchers practitioners open editing yields remarkable quality recent study giles found wikipedia accuracy
rival encyclopaedia britannica however britannica order magnitude
smaller million words articles http store britannica com visited
february
appropriate encyclopedia article comprises comprehensive exposition
single topic consequently view wikipedia article defining concept
corresponds topic example article artificial intelligence defines
concept artificial intelligence article parasitic extraction circuit
design defines concept layout extraction body articles critical
allows us compute affinity concepts words
input texts
important advantage thus use vast amounts highly organized human knowledge compared lexical resources wordnet methodology
leverages knowledge bases orders magnitude larger comprehensive
importantly web knowledge repositories use work undergo constant
development breadth depth steadily increase time compared latent
semantic analysis methodology explicitly uses knowledge collected organized
humans semantic analysis explicit sense manipulate manifest concepts grounded human cognition rather latent concepts used lsa therefore
call explicit semantic analysis esa
building semantic interpreter
given set concepts c cn set associated documents dn build
sparse table n columns corresponds concept rows

corresponds word occurs n di entry j table corresponds
tfidf value term ti document dj
j tf ti dj log

n

dfi

use titles articles convenient way refer articles treats
articles atomic concepts



figabrilovich markovitch

term frequency defined


tf ti dj

log count ti dj count ti dj


otherwise

dfi dk ti dk number documents collection contain
term ti document frequency
finally cosine normalization applied row disregard differences document
length
j
j ppr


l j
r number terms
semantic interpretation word ti obtained row table
meaning word given vector concepts paired tfidf scores
reflect relevance concept word
semantic interpretation text fragment ht tk centroid vectors
representing individual words definition allows us partially perform word sense
disambiguation consider example interpretation vector term mouse
two sets strong components correspond two possible meanings mouse rodent mouse computing similarly interpretation vector word screen
strong components associated window screen computer screen text
fragment purchased mouse screen summing two interpretation vectors boost computer related components effectively disambiguating words
table viewed inverted index maps word list
concepts appears inverted index provides efficient computation
distance interpretation vectors
given amount information encoded wikipedia essential control
amount noise present text discarding insufficiently developed articles
eliminating spurious association articles words done setting
zero weights concepts whose weights given term low see
section
link structure
natural electronic encyclopedia provide cross references form
hyperlinks typical wikipedia article many links entries
articles conventional printed encyclopedias
link structure used number ways observe link associated
anchor text clickable highlighted phrase anchor text identical
canonical name target article different anchor texts used refer
article different contexts example anchor texts pointing federal
reserve include fed u federal reserve board u federal reserve system
board governors federal reserve federal reserve bank foreign reserves
free banking era thus anchor texts provide alternative names variant spellings
related phrases target concept use enrich article text
target concept


fiwikipedia semantic interpretation

furthermore inter article links often reflect important relations concepts
correspond linked articles explore use relations feature generation
next section
second order interpretation
knowledge concepts subject many relations including generalization meronymy
part holonymy synonymy well specific relations capital
birthplace birthdate etc wikipedia notable example knowledge repository
features relations represented hypertext links wikipedia
articles
links encode large amount knowledge found article texts
consequently leveraging knowledge likely lead better interpretation
therefore distinguish first order use knowledge encoded
wikipedia articles second order incorporate knowledge encoded
inter article links similarly refer information obtained inter article
links second order information
rule presence link implies relation concepts connects
example article united states links washington c country
capital north america continent country situated links
multitude concepts definitely related source concept albeit
difficult define relations links include united states declaration
independence president united states elvis presley
however observations reveal existence link imply
two articles strongly related fact many words phrases typical wikipedia
article link articles entries corresponding concepts
example education subsection article united states gratuitous
links concepts high school college literacy rate therefore order
use wikipedia links semantic interpretation essential filter linked concepts
according relevance text fragment interpreted
intuitive way incorporate concept relations examine number top scoring
concepts
eto boost scores concepts linked let esa



w wn interpretation vector term define second level interpretation term




esa w wn



wi



wi

x



e



wj

j link cj ci

ensures linked concepts taken reduced weights
experiments used
opposite truethe absence link may simply due oversight adafre de rijke
studied discovering missing links wikipedia



figabrilovich markovitch

concept generality filter
concepts identified links equally useful relevance newly
added concepts certainly important criterion suppose
given input text google search additional concept likely
useful characterize input nigritude ultramarine specially crafted meaningless
phrase used search engine optimization contest website suppose input
artificial intelligence concept likely contribute representation
input john mccarthy computer scientist logic believe
examples second concept would useful overly specific
consequently conjecture add linked concepts sparingly taking
general concepts triggered judge
generality concepts may tricky achieve general case pun
intended propose following task oriented criterion given two concepts ca cb
compare numbers links pointing say ca general
cb number incoming links least order magnitude larger
log inlinks ca log inlinks cb
examples additional concepts identified inter article links section section evaluate effect inter article links additional
knowledge source section specifically examine effect
general linked concepts e adding concepts general concepts
triggered

explicit semantic analysis computing semantic
relatedness texts
section discuss application semantic interpretation methodology
automatic assessment semantic relatedness words texts
automatic computation semantic relatedness
related cat mouse preparing manuscript writing article ability quantify semantic relatedness texts underlies many fundamental tasks computational linguistics including word sense disambiguation information
retrieval word text clustering error correction budanitsky hirst reasoning semantic relatedness natural language utterances routinely performed
humans remains unsurmountable obstacle computers humans judge text
relatedness merely level text words words trigger reasoning much deeper
level manipulates conceptsthe basic units meaning serve humans organize
share knowledge thus humans interpret specific wording document
much larger context background knowledge experience lacking
elaborate resources computers need alternative ways represent texts reason

explicit semantic analysis represents text interpretation vectors high dimensional space concepts representation computing semantic relatedness texts
preliminary reported gabrilovich markovitch



fiwikipedia semantic interpretation

building semantic interpreter

word
wordi

building weighted
inverted index
wikipedia

wordn

weighted list
concepts
wikipedia
articles

weighted
inverted index

semantic interpreter

text

semantic
interpreter

vector
comparison

relatedness
estimation

text
weighted
vector
wikipedia
concepts

figure knowledge semantic interpreter

simply amounts comparing vectors vectors could compared variety
metrics zobel moffat use cosine metric throughout experiments
reported figure illustrates process
implementation details
used wikipedia snapshot november parsing wikipedia xml
dump obtained gb text articles although wikipedia almost
million articles equally useful feature generation articles correspond overly specific concepts e g metnal ninth level mayan underworld
otherwise unlikely useful subsequent text categorization e g specific dates
list events happened particular year articles short
cannot reliably classify texts onto corresponding concepts developed set
simple heuristics pruning set concepts discarding articles fewer
non stop words fewer incoming outgoing links discard articles describe specific dates well wikipedia disambiguation category
pruning articles left defined concepts used
feature generation processed text articles first tokenizing removing
stop words rare words occurring fewer articles stemmed remaining
words yielded distinct terms


figabrilovich markovitch

preprocessing wikipedia xml dump
wikipedia data publicly available online http download wikimedia org
data distributed xml format several packaged versions available article texts
edit history list page titles interlanguage links etc project use article
texts ignore information article authors page modification history
building semantic interpreter perform number operations distributed
xml dump
simplify original xml removing fields used feature
generation author ids last modification times
wikipedia syntax defines proprietary format inter article links whereas name
article referred enclosed brackets e g united states map
articles numeric ids article build list ids articles refers
count number incoming outgoing links article
wikipedia defines redirection mechanism maps frequently used variant names
entities canonical names examples united states america
mapped united states resolve redirections initial preprocessing
another frequently used mechanism templates allows articles include
frequently reused fragments text without duplication including pre defined
optionally parameterized templates fly speed subsequent processing
resolve template inclusions beginning
collect anchor texts point article
preprocessing stage yields xml file used building feature
generator
effect knowledge breadth
wikipedia constantly expanded material volunteer editors contribute
articles extend existing ones consequently conjectured addition
information beneficial esa would rely larger knowledge base
test assumption acquired newer wikipedia snapshot march
table presents comparison amount information two wikipedia
snapshots used number articles shown table reflects total number
articles date snapshot next table line number concepts
used reflects number concepts remained pruning explained
beginning section
following sections confirm larger knowledge base beneficial
esa juxtaposing obtained two wikipedia snapshots therefore
dimensionality reduction performed input text fragment represented space features features case later
wikipedia snapshot course many features zero values feature
vectors sparse


fiwikipedia semantic interpretation

combined article text
number articles
concepts used
distinct terms

wikipedia snapshot
november
gb




wikipedia snapshot
march
gb




table comparison two wikipedia snapshots
inverted index pruning
eliminate spurious association articles words setting zero weights
concepts whose weights given term low
pruning inverted index operates follows first sort
concepts given word according tfidf weights decreasing order
scan resulting sequence concepts sliding window length truncate
sequence difference scores first last concepts window
drops highest scoring concept word positioned first
sequence technique looks fast drops concept scores would signify
concepts tail sequence loosely associated word e
even though word occurred articles corresponding concepts
truly characteristic article contents evaluated principled approaches
observing values first second derivatives data seemed
noisy reliable estimation derivatives researchers studied use derivatives
similar contexts e g begelman keller smadja found
derivative alone sufficient hence found necessary estimate magnitude
peaks means consequently opted use simple efficient metric
purpose pruning eliminate spurious associations concepts
terms mainly beneficial pruning inverted index entries common
words occur many wikipedia articles criteria analyzed
inverted index wikipedia version dated november see section
majority terms fewer concepts non zero weight
concept term weights decreased gracefully qualify pruning pruned
entries terms total terms among terms whose concept
vector pruned term link largest number concepts non zero
weight retained concepts another example
concept vector term number pruned entries
average concepts retained pruning rates second
wikipedia version dated march similar
processing time
world knowledge requires additional computation extra computation includes
one time preprocessing step semantic interpreter built well
actual mapping input texts interpretation vectors performed online standard workstation parsing wikipedia xml dump takes hours building


figabrilovich markovitch

semantic interpreter takes less hour semantic interpreter built
throughput e generation interpretation vectors textual input several hundred words per second light improvements computing semantic relatedness
text categorization accuracy report sections believe
extra processing time well compensated
empirical evaluation explicit semantic analysis
humans innate ability judge semantic relatedness texts human judgements
reference set text pairs thus considered correct definition kind gold
standard computer evaluated several studies measured
inter judge correlations found consistently high budanitsky hirst
jarmasz finkelstein gabrilovich matias rivlin solan wolfman ruppin
r findings expectedafter consensus allows
people understand consequently evaluation amounts computing
correlation esa relatedness scores human judgments
better evaluate wikipedia semantic interpretation implemented semantic interpreter another large scale knowledge repositorythe open directory
project odp http www dmoz org largest web directory date case
odp concepts ci correspond categories directory e g top computers artificial intelligence text di associated concept obtained pooling
together titles descriptions urls catalogued corresponding category interpretation text fragment amounts computing weighted vector odp
concepts ordered affinity input text built odp semantic
interpreter odp snapshot april implementation details
found previous work gabrilovich markovitch b
test collections
work use two datasets best knowledge largest publicly
available collections kind test collections use correlation
computer assigned scores human scores assess performance
assess word relatedness use wordsimilarity collection finkelstein et al
finkelstein gabrilovich matias rivlin solan wolfman ruppin b
contains noun pairs representing degrees similarity pair
human judgements made individuals university degrees mothertongue level otherwise fluent command english language word pairs
assigned relatedness scores scale totally unrelated words much
related identical words judgements collected word pair averaged
recently zesch gurevych discussed automatic creation datasets assessing semantic
similarity however focus work automatical generation set sufficiently
diverse word pairs thus relieving humans need construct word lists manually obviously
establishing gold standard semantic relatedness word pair still performed manually
human judges
previous studies jarmasz szpakowicz suggested word pairs comprising
collection might culturally biased



fiwikipedia semantic interpretation

produce single relatedness score spearmans rank order correlation coefficient used
compare computed relatedness scores human judgements non parametric
spearmans correlation coefficient considered much robust pearsons
linear correlation comparing studies computed
spearmans correlation coefficient human judgments raw data
document similarity used collection documents australian
broadcasting corporations news mail service lee pincombe welsh pincombe
documents words long covered variety topics
judges students university adelaide australia paid
small fee work documents paired possible ways
pairs human judgements averaged pair neutralize effects
ordering document pairs presented random order order documents
within pair randomized well human judgements averaged
pair collection relatedness scores distinct values spearmans
correlation appropriate case therefore used pearsons linear correlation
coefficient
importantly instructions human judges test collections specifically directed
participants assess degree relatedness words texts involved example
case antonyms judges instructed consider similar rather
dissimilar
prior work
number prior studies proposed variety approaches computing word similarity
wordnet rogets thesaurus lsa table presents applying
approaches wordsimilarity test collection
jarmasz replicated several wordnet methods compared
rogets thesaurus hirst st onge viewed
wordnet graph considered length directionality graph path connecting two nodes leacock chodorow used length shortest graph
path normalized maximum taxonomy depth jiang conrath
later resnik used notion information content lowest node subsuming two given words lin b proposed computation word similarity
information theory see budanitsky hirst comprehensive discussion
wordnet approaches computing word similarity
according jarmasz rogets thesaurus number advantages compared
wordnet including links different parts speech topical groupings variety relations word senses consequently method developed authors
rogets source knowledge achieved much better wordnet
methods finkelstein et al reported computing word similarity
finkelstein et al report inter judge agreement wordsimilarity collection
performed assessment inter judge agreement dataset following snow
oconnor jurafsky ng divided human judges two sets averaged numeric
judgements word pair among judges set thus yielding element long vector
average judgments set spearmans correlation coefficient vectors two sets




figabrilovich markovitch

lsa model deerwester et al trained grolier academic american encyclopedia recently hughes ramage proposed method computing semantic relatedness random graph walks wordsimilarity dataset competitive reported jarmasz finkelstein et al

strube ponzetto proposed alternative computing word similarity wikipedia comparing articles whose titles words occur discuss
greater detail section
prior work assessing similarity textual documents comparing
documents bags words well lsa lee et al compared number
approaches bag words representation used binary tfidf
representation word weights variety similarity measures correlation jaccard
cosine overlap authors implemented lsa model trained set
news documents australian broadcasting corporation test documents whose
similarity computed came distribution experiments
reported table

better understand explicit semantic analysis works let us consider similarity computation pairs actual phrases example given two phrases scientific article
journal publication esa determines following wikipedia concepts found
among top concepts phrase scientific journal nature journal
academic publication science journal peer review compute
similarity rna dna following concepts found shared among
top lists transcription genetics gene rna cell biology
presence identical concepts among top concepts characterizing phrase
allows esa establish semantic similarity
table shows applying methodology estimating relatedness
individual words statistically significant improvements shown bold values
shown table represent spearmans correlation human judgments
relatedness scores produced different methods jarmasz compared
performance wordnet metrics namely proposed hirst st onge
jiang conrath leacock chodorow lin b resnik
table report performance best metrics namely
lin b resnik wikirelate strube ponzetto
authors report many different method variations report
performance best one metric proposed leacock chodorow

see esa techniques yield substantial improvements previous state
art notably esa achieves much better another recently
introduce method wikipedia strube ponzetto provide detailed
comparison latter work section table shows
computing relatedness entire documents tables statistical
significance difference performance esa wikipedia march


fiwikipedia semantic interpretation



wordnet techniques jarmasz
rogets thesaurus technique jarmasz
lsa finkelstein et al
wikirelate strube ponzetto
markovlink hughes ramage
esa wikipedia march version
esa wikipedia november version
esa odp

spearmans
correlation
human judgements









stat
significance
p value









table spearmans rank correlation word relatedness scores human judgements
wordsimilarity collection


bag words lee et al
lsa lee et al
esa wikipedia march version
esa wikipedia november version
esa odp

pearsons
correlation
human judgements






stat
significance
p value






table pearsons correlation text relatedness scores human judgements lee et
al document collection

version fishers z transformation press teukolsky
vetterling flannery section
test collections wikipedia semantic interpretation superior
odp one word relatedness task superiority statistically significant
p believe two factors contribute phenomenon first axes
multi dimensional interpretation space ideally independent possible
hierarchical organization open directory reflects generalization relation
concepts obviously violates independence requirement second increase
amount training data building odp semantic interpreter crawled
urls listed odp allowed us increase amount textual data several
orders magnitude brought non negligible amount noise
common web hand wikipedia articles virtually noise free
whenever range values available compared esa wikipedia best performing method
range



figabrilovich markovitch

mostly qualify standard written english thus textual descriptions wikipedia
concepts arguably focused odp concepts
essential note experiments newer wikipedia snapshot
leads better although difference performance two versions
admittedly small
evaluated effect second order interpretation computing semantic
relatedness texts yielded negligible improvements hypothesize
reason finding computing semantic relatedness essentially uses available
wikipedia concepts second order interpretation slightly modify weights
existing concepts next section describes application esa text
categorization trim interpretation vectors sake efficiency consider
highest scoring concepts input text fragment scenario secondorder interpretation positive effect actually improves accuracy text
categorization section happens selected wikipedia concepts
used augment text representation second order selectively adds
highly related concepts identified analyzing wikipedia links

explicit semantic analysis text categorization
section evaluate benefits external knowledge text categorization
background text categorization
text categorization tc deals assigning category labels natural language documents categories come fixed set labels possibly organized hierarchy
document may assigned one categories text categorization systems
useful wide variety tasks routing news e mail appropriate corporate
desks identifying junk email correctly handling intelligence reports
majority existing text classification systems represent text bag words
use variant vector space model weighting schemes salton mcgill
thus features commonly used text classification weighted occurrence
frequencies individual words state art systems text categorization use variety
induction techniques support vector machines k nearest neighbor
neural networks bag words bow method effective easy medium
difficulty categorization tasks category document identified several
easily distinguishable keywords however performance becomes quite limited
demanding tasks dealing small categories short documents
attempts extend basic bow several studies
augmented bag words n grams caropreso matwin sebastiani peng
shuurmans mladenic raskutti ferra kowalczyk statistical
language peng schuurmans wang others used linguistically motivated
features syntactic information available part speech tagging
shallow parsing sable mckeown church basili moschitti pazienza
additional studies researched use word clustering baker mccallum bekker preliminary reported gabrilovich markovitch



fiwikipedia semantic interpretation

man dhillon mallela kumar neural networks jo jo japkowicz
jo well dimensionality reduction techniques lsa deerwester
et al hull zelikovitz hirsh cai hofmann however
attempts mostly limited success
believe bag words inherently limited use
pieces information explicitly mentioned documents
vocabulary consistently used throughout bow cannot generalize
words consequently words testing document never appeared training
set necessarily ignored synonymous words appear infrequently training
documents used infer general principle covers cases furthermore
considering words unordered bag makes difficult correctly resolve sense
polysemous words longer processed native context
shortcomings stem fact bag words method access wealth
world knowledge possessed humans therefore easily puzzled facts terms
cannot easily deduced training set
esa feature generation
propose solution augments bag words knowledge features
given document classified would use esa represent document
text space wikipedia concepts however text categorization crucially different
computing semantic relatedness cf section two important respects
first computing semantic relatedness essentially one task given
particular pair text fragments need quantify relatedness prior
examples specific task cases words text fragments likely
marginal usefulness especially two fragments one word long
happens data available us limited two input fragments
cases share words
hand supervised text categorization one usually given collection
labeled text documents one induce text categorizer consequently
words occur training examples serve valuable featuresthis
bag words born observed earlier work gabrilovich
markovitch b ill advised completely replace bag words
generated concepts instead advantageous enrich bag words rather
opt augment bag words carefully selected knowledge concepts become
features document refer process feature generation
actually construct document features beyond bag words
second enriching document representation text categorization possible
wikipedia concepts extremely expensive computationally machine learning
classifier learned augmented feature space representation obviously
takes lot storage space cannot processed efficiently multitude
concepts involved whose number easily reach hundreds thousands therefore
text categorization task prune interpretation vectors retain number
highest scoring concepts input text fragment



figabrilovich markovitch

multi resolution feature generation believe considering document single unit often misleading text might diverse
readily mapped right set concepts notions mentioned briefly may
overlooked instead partition document series non overlapping segments
called contexts generate features finer level context mapped
number wikipedia concepts knowledge base pooling concepts
together describe entire document multi faceted classification way
resulting set concepts represents aspects sub topics covered
document
potential candidates contexts simple sequences words linguistically motivated chunks sentences paragraphs optimal resolution document segmentation determined automatically validation set earlier work gabrilovich markovitch b proposed principled multiresolution simultaneously partitions document several levels linguistic abstraction windows words sentences paragraphs taking entire document
one big chunk performs feature generation levels rely
subsequent feature selection step eliminate extraneous features preserving
genuinely characterize document
essential emphasize multi resolution makes sense
interpretation vectors pruned retain number highest scoring concepts context explained exactly case text categorization
without pruning producing interpretation vectors context summing
would equivalent simply multiplying weight concept constant
factor order explain situation different presence pruning let us
consider example suppose long document mentions particular
topic last paragraph since topic central document n topscoring concepts documents interpretation vector unlikely cover topic
although likely covered concepts concepts lower weight
going pruned however produce interpretation vectors
paragraph document retain n highest scoring concepts
concepts generated last paragraph cover consequently representation joined set concepts generated document many text categorization
tasks documents labeled particular topic even mention topic briefly
hence generating features describing topics important
feature generation feature generation performed prior text categorization
document transformed series local contexts represented
interpretation vectors esa top ten concepts vectors pooled together
give rise generated features document added bag words
since concepts correspond wikipedia articles constructed features
correspond articles thus set features generated document viewed
representing set wikipedia articles relevant document contents
constructed features used conjunction original bag words
resulting set optionally undergoes feature selection discriminative features
retained document representation


fiwikipedia semantic interpretation

basic
features

feature
selection

selected
features

labeled
documents

feature
valuation

induction


classifier

classifier

classified
documents

labeled
feature
vectors

training

testing
testing
documents

feature
valuation

figure standard text categorization

feature generation
feature
construction

feature
selection

generated
features

wikipedia
labeled
documents

feature
valuation

induction


classifier

labeled
feature
vectors

figure induction text classifiers proposed framework feature generation

figure depicts standard text categorization figure outlines
proposed feature generation framework observe feature generation box replaces
feature selection box framed bold figure
essential note use encyclopedia simply increase amount
training data text categorization neither use text corpus collect
word co occurrence statistics rather use knowledge distilled encyclopedia
enrich representation documents text categorizer induced
augmented knowledge rich feature space


figabrilovich markovitch

test collections
section gives brief description test collections used evaluate methodology provide much detailed description test collections appendix b
reuters reuters historically often used dataset text categorization following common practice used modapte split training
testing documents two category sets largest categories categories
least one training testing example
newsgroups ng lang well balanced dataset categories
containing documents
movie reviews movies pang lee vaithyanathan defines sentiment
classification task reviews express positive negative opinion
movies dataset documents two categories positive negative
reuters corpus rcv lewis yang rose li
documents speed experiments used subset rcv training documents dated testing ones following
brank grobelnik milic frayling mladenic used topic industry
categories constitute representative samples full groups categories
respectively randomly sampled topic industry categories sets
categories
ohsumed hersh buckley leone hickam subset medline
contains medical documents document contains title two thirds
contain abstract document labeled average mesh
categories total following joachims used subset documents
abstracts taking first documents training next
testing limit number categories experiments randomly
generated sets categories
datasets allows us comprehensively evaluate performance
specifically comparing newsgroups two reuters datasets reuters reuters corpus observe former substantially
noisy since data obtained usenet newsgroups reuters datasets
significantly cleaner movie reviews collection presents example sentiment
classification different standard topical text categorization finally
ohsumed dataset presents example comprehensive taxonomy
categories explain next section used dataset create collection
labeled short texts allowed us quantify performance method
texts
short documents derived several datasets short documents test
collections described recall one third ohsumed documents
titles abstract therefore considered short documents used
range documents defined considered without abstracts
yielded training testing documents datasets created
full definition category sets used available table see section b
http www nlm nih gov mesh
full definition category sets used available table see section b



fiwikipedia semantic interpretation

short document original document taking title latter
exception movie reviews documents titles
noted however substituting title full document poor
mans way obtain collection classified short documents documents first
labeled categories human labeller saw document entirety particular
category might assigned document basis facts mentioned
body even though information may well missing short title thus taking
categories original documents genuine categories title often
misleading however know publicly available test collections short
documents decided construct datasets explained importantly ohsumed
documents without abstracts classified humans working
ohsumed derived dataset thus considered pure experiment
experimentation procedure
used support vector machines learning build text categorizers since
prior studies found svms best performance text categorization sebastiani
dumais platt heckerman sahami yang liu following established
practice use precision recall break even point bep measure text categorization
performance bep defined terms standard measures precision recall
precision proportion true document category assignments among assignments predicted classifier recall proportion true document category
assignments predicted classifier obtained tuning
classifier precision equal recall sampling several precision recall points
bracket expected bep value interpolating extrapolating event
sampled points lie side
two reuters datasets ohsumed report micro macro averaged
bep since categories differ size significantly micro averaged bep operates
document level primarily affected categorization performance larger categories
hand macro averaged bep averages individual categories thus
small categories training examples large impact overall performance
reuters datasets reuters rcv ohsumed used fixed
train test split defined section consequently used macro sign test test
yang liu assess statistical significance differences classifier performance ng movies performed fold cross validation used paired test
assess significance used non parametric wilcoxon signed ranks test demsar compare baseline fg classifiers multiple data sets
latter case individual measurements taken micro macro averaged bep
values observed dataset
used svm light implementation joachims default parameters earlier
work feature selection gabrilovich markovitch conducted thorough experimentation
wide range values c parameter found major importance
datasets consequently leave parameter default setting well



figabrilovich markovitch

text categorization infrastructure
conducted experiments text categorization platform design
development named hogwarts davidov gabrilovich markovitch opted
build comprehensive infrastructure text categorization surprisingly software tools publicly available researchers available allow
limited control operation hogwarts facilitates full cycle text categorization
including text preprocessing feature extraction construction selection weighting followed actual classification cross validation experiments system currently
provides xml parsing part speech tagging brill sentence boundary detection
stemming porter wordnet fellbaum lookup variety feature selection
tfidf feature weighting schemes hogwarts configurable
parameters control modus operandi minute detail hogwarts interfaces
svm knn c text categorization computes standard measures
categorization performance hogwarts designed particular emphasis
processing efficiency portably implemented ansi c programming language
c standard template library system built loaders reuters
reuters rcv lewis et al newsgroups lang movie reviews
pang et al ohsumed hersh et al additional datasets
easily integrated modular way
document undergoes following processing steps document text first tokenized title words replicated twice emphasize importance stop
words numbers mixed alphanumeric strings removed remaining words
stemmed bag words next merged set features generated
document analyzing contexts explained section rare features occurring
fewer documents removed
since earlier studies found bow features indeed useful svm text
categorization joachims rogati yang brank et al bekkerman
leopold kindermann lewis et al take bag words
entirety exception rare features removed previous step generated
features however undergo feature selection information gain criterion finally
feature weighting performed ltc tf idf function logarithmic term frequency
inverse document frequency followed cosine normalization salton buckley
debole sebastiani
baseline performance hogwarts
demonstrate performance basic text categorization implementation column baseline table consistent state art reflected
published studies svm reuters dumais et al achieved
hogwarts school witchcraft wizardry educational institution attended harry potter
rowling
gabrilovich markovitch described class feature selection bag
words actually improves svm performance
course feature selection performed training set documents



fiwikipedia semantic interpretation

micro bep categories categories ng bekkerman
obtained bep pang et al obtained accuracy movies
minor variations performance due differences data preprocessing
different systems example movies dataset worked raw html files
rather official tokenized version order recover sentence paragraph
structure contextual analysis rcv ohsumed direct comparison published difficult limited category sets date span
documents speed experimentation
feature generator
core engine explicit semantic analysis implemented explained section
used multi resolution feature generation classifying document contexts level individual words complete sentences paragraphs finally entire
document context features generated best matching concepts
produced feature generator
wikipedia feature generation
section report experimental evaluation methodology
qualitative analysis feature generation
study process feature generation number actual examples
feature generation per se illustrate features generated
several text fragments whenever applicable provide short explanations generated
concepts cases explanations taken wikipedia wikipedia
text wal mart supply chain goes real time
top generated features wal mart sam walton sears holdings
corporation target corporation albertsons asda rfid
hypermarket united food commercial workers chain store
selected explanations wal mart founder prominent competitors walmart wal mart subsidiary uk radio frequency identification
technology wal mart uses extensively manage stock superstore
general concept wal mart specific example labor union
comparison reported bekkerman administered single test run e
without cross validation taking first postings newsgroup training rest
testing
comparison reported pang et al administered single test run e
without cross validation taking first data opinion type training rest
testing
ng dataset exception owing high level intrinsic noise renders identification
sentence boundaries extremely unreliable causes word level feature generation produce
many spurious classifications consequently dataset restrict multi resolution
individual paragraphs entire document



figabrilovich markovitch

trying organize wal marts workers general concept
wal mart specific example
particularly interesting juxtapose features generated fragments
contain ambiguous words end features generated two phrases
contain word bank two different senses bank america financial
institution bank amazon river bank readily seen feature generation methodology capable performing word sense disambiguation
considering ambiguous words context neighbors
text bank america
top generated features bank bank america bank
america plaza atlanta bank america plaza dallas mbna
bank holding company acquired bank america visa credit
card bank america tower york city nasdaq mastercard bank america corporate center
text bank amazon
top generated features amazon river amazon basin amazon rainforest amazon com rainforest atlantic ocean
brazil loreto region region peru located amazon rainforest
river economy brazil
method however accurate cases generates features
somewhat relevant even irrelevant input text example outcome feature generation title earlier article
gabrilovich markovitch concept list input words
triggered words stemmed sorted decreasing order
contribution
text overcoming brittleness bottleneck wikipedia enhancing text categorization encyclopedic knowledge
top generated features
encyclopedia encyclopedia knowledge wikipedia text
wikipedia wikipedia enhance encyclopedia text
enterprise content management category knowledge text overcome enhance
performance bottleneck category enhance
immanuel kant category knowledge overcome
tooth enamel brittleness text enhance
lucid dreaming enhance text knowledge category
bottleneck bottleneck
java programming language category bottleneck enhance


fiwikipedia semantic interpretation

transmission control protocol category enhance overcome
generated features clearly relevant input encyclopedia
wikipedia enterprise content management others however spurious
tooth enamel transmission control protocol since process
feature generation relies bag words matching concepts input text
suffers bow shortcomings mentioned section consequently
features generated corresponding wikipedia articles happen
share words input text even though words characteristic
article whole explained method successfully operate
presence extraneous features due use feature selection
way generated features informative predicting document categories
filtered informative features actually retained learning
classification model
inter article links generating additional features section
presented generates additional features inter article links relations concepts follows series text fragments
fragment features generated regular fg b features
generated wikipedia links c general features generated links
see examples features constructed links often relevant
input text
text google search
regular feature generation search engine google video google
google search google maps google desktop google verb
google news search engine optimization spamdexing search engine
spamming
features generated links pagerank adwords adsense
gmail google platform website sergey brin google bomb
msn search nigritude ultramarine meaningless phrase used search
engine optimization contest
general features website mozilla firefox portable
document format world wide web
text programming tools
regular feature generation tool programming tool computer
software integrated development environment computer aided software engineering macromedia flash borland game programmer
c programming language performance analysis
features generated links compiler debugger source code
software engineering microsoft revision control scripting
language gnu make linux
general features microsoft software engineering
linux compiler gnu


figabrilovich markovitch

effect feature generation
table shows wikipedia feature generation significant
improvements p shown bold different rows table correspond
performance different datasets subsets defined section
consistently observed larger improvements macro averaged bep dominated
categorization effectiveness small categories goes line expectations
contribution encyclopedic knowledge especially prominent categories training examples categorization performance improved virtually
datasets notable improvements rcv ohsumed
wilcoxon test found wikipedia classifier significantly superior baseline p micro macro averaged cases
clearly demonstrate advantage knowledge feature generation
prior work gabrilovich markovitch b performed
feature generation text categorization alternative source knowledge namely
open directory project odp wikipedia competitive
odp slight advantage wikipedia observe wikipedia
constantly updated numerous volunteers around globe odp virtually
frozen nowadays hence future expect obtain improvements
newer versions wikipedia
effect knowledge breadth examined effect performing feature
generation newer wikipedia snapshot explained section appendix
reports experiment small consistent improvement due
larger knowledge base
classifying short documents
conjectured wikipedia feature generation particularly useful
classifying short documents
table presents evaluation datasets defined section
majority cases feature generation yielded greater improvement short documents regular documents notably improvements particularly high
ohsumed pure experimentation short documents possible see section
according wilcoxon test wikipedia classifier significantly superior
baseline p findings confirm hypothesis encyclopedic knowledge particularly useful categorizing short documents
inadequately represented standard bag words
inter article links concept relations
inter article links generating additional features observed improvements text categorization performance short documents see table
absolute majority cases links generate general features
superior strategy explain section inter article links viewed relations
concepts represented articles consequently links allows us


fiwikipedia semantic interpretation

dataset

baseline
micro macro
bep bep
reuters cat
reuters cat
rcv industry

rcv industry

rcv industry b

rcv industry c

rcv industry

rcv industry e

rcv topic

rcv topic

rcv topic b

rcv topic c

rcv topic

rcv topic e

ohsumed

ohsumed b

ohsumed c

ohsumed

ohsumed e

ng

movies


wikipedia
micro macro
bep bep






















improvement
micro macro
bep
bep






















table effect feature generation long documents



figabrilovich markovitch

dataset

baseline
micro macro
bep bep
reuters cat
reuters cat
rcv industry

rcv industry

rcv industry b

rcv industry c

rcv industry

rcv industry e

rcv topic

rcv topic

rcv topic b

rcv topic c

rcv topic

rcv topic e

ohsumed

ohsumed b

ohsumed c

ohsumed

ohsumed e

ng


wikipedia
micro macro
bep bep





















improvement
micro
macro
bep
bep
























table feature generation short documents



fiwikipedia semantic interpretation

dataset

baseline

micro
bep
reuters cat
reuters cat
rcv industry

rcv topic

ng

dataset
reuters cat
reuters cat
rcv industry
rcv topic
ng







macro
bep










wikipedia

wikipedia
links

micro macro
bep bep





improvement
baseline






micro macro
bep bep





improvement
baseline






wikipedia
links
general
features
micro macro
bep
bep





improvement
baseline






table feature generation short documents inter article links
identify additional concepts related context analyzed leads better
representation context additional relevant generated features

related work
section puts methodology context related prior work
past number attempts represent meaning natural
language texts early computational linguistics focused deep natural language
understanding strived represent text semantics logical formulae montague
however task proved difficult little progress made
develop comprehensive grammars non trivial fragments language consequently
mainstream effectively switched statistically methods manning
schuetze
although studies tried explicitly define semantic representation
modus operandi frequently induces particular representation system distributional similarity methods lee compute similarity pair words w w comparing
distributions words given two e g comparing vectors probabilities p v w p v w large vocabulary v words v v therefore
techniques seen representing meaning word w vector conditional
probabilities words given w dagan marcus markovitch refined
technique considering co occurrence probabilities word left right contextual neighbors example word water would represented vector
left neighbors drink pour clean vector right neighbors
molecule level surface lin represented word meaning considering syntactic roles words co occur sentence example


figabrilovich markovitch

semantics word water would represented vector triples water
obj drink water adj mod clean qiu frei proposed method
concept query expansion however expanded queries additional words
rather features corresponding semantic concepts
latent semantic analysis probably similar method prior
explicitly represents meaning text fragment lsa manipulating
vector called latent concepts obtained svd decomposition
word document matrix training corpus cyc lenat lenat guha pittman
pratt shepherd represents semantics words elaborate network
interconnected richly annotated concepts
contrast method represents meaning piece text weighted vector
knowledge concepts importantly entries vector correspond unambiguous
human defined concepts rather plain words often ambiguous compared
lsa benefits large amounts manually encoded human knowledge
opposed defining concepts statistical analysis training corpus compared
cyc streamlines process semantic interpretation depend
manual encoding inference rules exception lsa prior approaches
semantic interpretation explicitly represent semantics individual words require
extra level sophistication represent longer texts conversely represents
meaning texts uniform way regardless length
semantic similarity semantic relatedness
study deal semantic relatedness rather semantic similarity
semantic distance often used literature extensive survey
relatedness measures budanitsky hirst argued notion relatedness
general similarity former subsumes many different kind specific
relations including meronymy antonymy functional association others
maintained computational linguistics applications often require measures relatedness
rather narrowly defined measures similarity example word sense
disambiguation use related words context merely similar words
budanitsky hirst argued notion semantic distance might
confusing due different ways used literature
estimating semantic relatedness words somewhat reminiscent
distributional co occurrence similarity lee dagan lee pereira indeed compare meanings words comparing occurrence patterns across
large collection natural language documents however compilation documents arbitrary rather documents aligned encyclopedia articles
focused single topic furthermore distributional similarity methods
inherently suitable comparing individual words method compute
similarity arbitrarily long texts
prior work field mostly focused semantic similarity words r g
rubenstein goodenough list word pairs c miller charles
list word pairs similarity relation considered lexical resources
often successful enough reaching pearsons correlation human


fiwikipedia semantic interpretation

judgements budanitsky hirst jarmasz case lexical techniques
even slight edge esa wikipedia whose correlation human scores
c r g however entire language wealth considered
attempt capture general semantic relatedness lexical techniques yield substantially inferior see table wordnet technique consider
generalization relation words achieve correlation
human judgements budanitsky hirst jarmasz jarmasz szpakowiczs
elkb system jarmasz rogets thesaurus roget achieves higher
correlation due use richer set relations
studying semantic similarity relatedness words related assessing similarity
relations example task establish word pairs carpenter wood
mason stone relationally similar words pairs stand relation
profession material state art relational similarity latent
relational analysis turney
sahami heilman proposed use web source additional knowledge
measuring similarity short text snippets end defined kernel function
sends two snippets queries search engine compares bags words
two sets returned documents major limitation technique
applicable short texts sending long text query search engine likely
return even hand applicable
text fragments arbitrary length additional studies explored web gather
information computing word similarity include turney metzler dumais
meek main difference works method latter
uses structured representation human knowledge defined wikipedia concepts
mentioned techniques inherently limited individual words
adaptation comparing longer texts requires extra level complexity mihalcea
et al contrast method treats words texts essentially
way
strube ponzetto used wikipedia computing semantic relatedness
however method called wikirelate radically different given pair
words w w wikirelate searches wikipedia articles p p respectively
contain w w titles semantic relatedness computed
distance measures p p measures rely texts
path distances within category hierarchy wikipedia
hand represents word weighted vector wikipedia concepts semantic
relatedness computed comparing two concept vectors
thus differences two approaches
wikirelate process words actually occur titles wikipedia articles
esa requires word appears within text wikipedia articles
wikirelate limited single words esa compare texts length
wikirelate strube ponzetto achieved relatively low scores domains



figabrilovich markovitch

wikirelate represents semantics word text article
associated node category hierarchy esa much
structured semantic representation consisting vector wikipedia concepts
indeed shown section richer representation esa yields much better

feature generation text categorization
date quite attempts made deviate orthodox bag words
paradigm usually limited success particular representations phrases
lewis dumais et al fuernkranz mitchell riloff named entities
kumaran allan term clustering lewis croft bekkerman
explored however none techniques could possibly overcome
underlying examples reviewed paperlack world knowledge
feature generation techniques found useful variety machine learning tasks
markovitch rosenstein fawcett matheus techniques search
features describe target concept better ones supplied
training instances number proposed feature generation pagallo haussler matheus rendell hu kibler murphy pazzani hirsh
japkowicz led significant improvements performance range classification tasks however even though feature generation established area
machine learning works applied text processing kudenko hirsh
mikheev cohen scott scott matwin contrast
techniques use exogenous knowledge
prior work gabrilovich markovitch b assumed external
knowledge available form generalization hierarchy used open directory
project example method however number drawbacks
corrected wikipedia
first requiring knowledge repository define hierarchy limits choice
appropriate repositories moreover hierarchical organization embodies one particular
relation nodes generalization numerous relations relatedness meronymy holonymy chronology ignored second large scale hierarchies
tend extremely unbalanced relative size branches disproportionately large small due peculiar views editors phenomena indeed
common odp example top society branch heavily dominated one
childrenreligion spirituality top science branch dominated
biology child considerable fraction mass top recreation concentrated
pets finally learn scope every odp concept short textual descriptions
concepts augmented crawling web sites cataloged odp procedure
allowed us accumulate many gigabytes worth textual data price texts
obtained web often quite far formal writing plagued noise
crawling typical web site often brings auxiliary material little
site theme legal disclaimers privacy statements help
proposed use world knowledge encoded wikipedia arguably largest knowledge repository web compared odp wikipedia


fiwikipedia semantic interpretation

possesses several advantageous properties first articles much cleaner typical
web mostly qualify standard written english although wikipedia offers
several orthogonal browsing interfaces structure fairly shallow propose
treat wikipedia essentially hierarchy way mapping tex fragments onto
relevant wikipedia concepts yields truly multi faceted classification text avoids
unbalanced hierarchy branches moreover requiring knowledge
repository hierarchically organized suitable domains
ontology available finally wikipedia articles heavily cross linked way
reminiscent linking web conjectured links encode many interesting relations concepts constitute important source information
addition article texts explored inter article links section
feature generation electronic dictionaries
several studies performed feature construction wordnet electronic dictionary
fellbaum domain specific dictionaries scott scott matwin
urena lopez buenaga gomez wang mckay abbass barlow
bloehdorn hotho
scott matwin attempted augment conventional bag words representation additional features symbolic classification system ripper cohen
study evaluated features syntactically statistically motivated
phrases well wordnet synsets latter case system performed generalizations hypernym hierarchy wordnet completely replaced bag words
bag synsets hypernyms allowed ripper produce general
comprehensible rules achieved performance gains small classification tasks performance benefits could obtained larger tasks even suffered
degradation classification accuracy consistent published findings
lewis dumais et al fuernkranz et al phrase representation
yield significant performance benefits bag words
urena lopez et al used wordnet conjunction rocchio rocchio
widrow hoff lewis schapire callan papka widrow stearns chapter linear classifiers fine tune category vectors wang et al used medical
subject headings mesh replace bag words canonical medical terms
bloehdorn hotho used similar augment reuters documents
wordnet synsets ohsumed medical documents mesh terms
noted however wordnet originally designed powerful
knowledge base rather lexical database suitable peculiar lexicographers
needs specifically wordnet following drawbacks used knowledge base
text categorization
identification syntactic phrases performed noun phrase extractor built top part
speech tagger brill
synset wordnet notion sense shared group synonymous words
sebastiani casts use bag words versus phrases utilizing lexical semantics rather
compositional semantics interestingly bag words approaches notably knn may considered
context sensitive assume independence features terms categories yang
pedersen



figabrilovich markovitch

wordnet fairly small coveragefor test collections used
unique words missing wordnet particular many proper
names slang domain specific technical terms included wordnet
designed general purpose dictionary
additional information synsets beyond identity limited
wordnet implements differential rather constructive lexical semantics
theory glosses accompany synsets mainly designed distinguish
synsets rather provide definition sense concept usage examples
occasionally constitute part gloss serve purpose without
auxiliary information reliable word sense disambiguation almost impossible
wordnet designed professional linguists trained recognize minute
differences word senses common words far many distinct
senses useful information retrieval mihalcea example word
make many senses verb alone fine grained distinctions
synsets present additional difficulty word sense disambiguation
techniques use wordnet manipulate collection
concepts however number crucial differences previous studies
performed feature generation individual words handle arbitrarily long short text fragments alike considering words context allows
perform word sense disambiguation approaches wordnet cannot achieve disambiguation information synsets limited merely words
wikipedia concepts associated huge amounts text even individual words
provides much sophisticated mapping words concepts
analysis large bodies texts associated concepts allows us represent
meaning words texts weighted combination concepts mapping word
wordnet amounts simple lookup without weights furthermore wordnet
senses word mutually exclusive concepts reflect different
aspects input thus yielding weighted multi faceted representation text
appendix illustrate limitations wordnet specific example
juxtapose wordnet wikipedia representation
unlabeled examples
best knowledge exception studies used wordnet
attempts date automatically use large scale repositories structured background knowledge feature generation interesting nonstructured background knowledge proposed zelikovitz hirsh work
uses collection unlabeled examples intermediaries comparing testing examples
training ones specifically unknown test instance appear
resemble labeled training instances unlabeled examples similar may
used bridges possible handle situation
training test document words common unlabeled documents
utilized define cosine similarity metric used knn
actual text categorization however suffers efficiency


fiwikipedia semantic interpretation

looking intermediaries compare every two documents makes necessary explore
combinatorial search space
subsequent zelikovitz hirsh proposed alternative way use
unlabeled documents background knowledge work unlabeled texts pooled
together training documents compute latent semantic analysis lsa deerwester et al model lsa analyzes large corpus unlabeled text automatically
identifies called latent concepts singular value decomposition resulting
lsa metric facilitates comparison test documents training documents addition unlabeled documents significantly increases amount data word
co occurrence statistics estimated thus providing solution text categorization training data particularly scarce however subsequent studies found
lsa rarely improve strong baseline established svm often even
performance degradation wu gunopulos liu chen zhang wu
contrast lsa manipulates virtual concepts methodology relies
concepts identified described humans

conclusions
proposed explicit semantic analysisa semantic interpretation methodology natural language processing order render computers knowledge
world use wikipedia build semantic interpreter represents meaning
texts high dimensional space knowledge concepts concepts correspond wikipedia articles methodology provides fully automatic way tap
collective knowledge tens hundreds thousands people conceptbased representation text contains information cannot deduced input
text alone consequently supersedes conventional bag words representation
believe important aspects proposed ability
address synonymy polysemy arguably two important
nlp thus two texts discuss topic different words
conventional bag words able identify commonality
hand mere fact two texts contain word necessarily
imply discuss topic since word could used two texts two
different meanings believe concept representation allows generalizations
refinements partially address synonymy polysemy
consider example following text fragment taken appendix c group
european led astronomers made photograph appears planet orbiting
another star would first confirmed picture world beyond solar
system fifth concept generated fragment extrasolar planet
exactly topic text even though words mentioned input
generated concepts e g astronomy planetary orbit highly
characteristic astronomy related texts additions enrich text representation
increase chances finding common features texts essential note
course generated concepts need match features documents
even concepts match gain valuable insights document contents


figabrilovich markovitch

succeeded make automatic use encyclopedia without deep language understanding specially crafted inference rules relying additional common sense knowledge
bases made possible applying standard text classification techniques match
document texts relevant wikipedia articles
empirical evaluation confirmed value explicit semantic analysis two common tasks natural language processing compared previous state art
esa significant improvements automatically assessing semantic relatedness words texts specifically correlation computed relatedness scores
human judgements increased r spearman individual words
r pearson texts contrast existing methods esa offers
uniform way computing relatedness individual words arbitrarily long text
fragments esa perform feature generation text categorization yielded consistent improvements across diverse range datasets recently performance
best text categorization systems became similar previous work mostly achieved small
improvements wikipedia source external knowledge allowed us improve
performance text categorization across diverse collection datasets
noted although recent study giles found wikipedia accuracy rival encyclopaedia britannica arguably wikipedia articles
equally high quality one hand wikipedia notion featured articles
http en wikipedia org wiki featured article considered
best articles wikipedia determined wikipedias editors currently fewer
articles achieve status hand many articles incomplete socalled stubs might even contain information incorrect represent
consensus among editors yet cases wikipedia content might prone
spamming despite editorial process attempts review recent changes believe
method overly susceptible cases long majority content
correct arguably except outright vandalism spamming would likely modify
articles contain information related topic article important
essential majority readers long newly added content remains
relevant gist article method likely able correctly determine
input texts article relevant however proper evaluation robustness
method presence imperfect content beyond scope article
believe constitutes step towards enriching natural language
processing humans knowledge world hope explicit semantic
analysis useful nlp tasks beyond computing semantic relatedness
text categorization intend investigate future work recently
used esa improve performance conventional information retrieval egozi
gabrilovich markovitch work augmented queries documents
generated features documents indexed augmented space words
concepts potthast stein anderka sorg cimiano adapted
esa multi lingual cross lingual information retrieval
another recent study gurevych et al applied methodology computing
word similarity german information retrieval task searched job
descriptions given users description career interests found method superior
wordnet importantly study confirms method


fiwikipedia semantic interpretation

easily adapted languages english version wikipedia
corresponding desired target language
future work intend apply esa word sense disambiguation current
approaches word sense disambiguation represent contexts contain ambiguous words
bag words augmented part speech information believe representation contexts greatly improved use feature generation map
contexts relevant knowledge concepts anecdotal evidence examples presented section implies method promise improving state art
word sense disambiguation work capitalized inter article links wikipedia
several ways future work intend investigate elaborate techniques
leveraging high degree cross linking wikipedia articles
wiki technology underlying wikipedia project often used nowadays variety open editing initiatives include corporate intranets use wiki primary
documentation tool well numerous domain specific encyclopedias topics ranging
mathematics orthodox christianity therefore believe methodology
used augmenting document representation many specialized domains
essential note wikipedia available numerous languages different
language versions cross linked level concepts believe information
leveraged use wikipedia semantic interpretation improving machine
translation
work proposes methodology explicit semantic analysis wikipedia
however esa implemented repositories human knowledge
satisfy requirements listed section section reported
building esa semantic interpreter open directory project gabrilovich
markovitch b zesch mueller gurevych proposed use wiktionary computing semantic relatedness future work intend implement
esa additional knowledge repositories
finally readers interested wikipedia work main software
deliverable described work wikipedia preprocessor wikiprep available online
part sourceforge open source project http wikiprep sourceforge net

acknowledgments
thank michael lee brandon pincombe making available document similarity data thank deepak agarwal advice assessing statistical significance
computing semantic relatedness work partially supported funding
ec sponsored muscle network excellence
first authors current address yahoo mission college blvd santa
clara ca usa

see http en wikipedia org wiki category online encyclopedias longer list examples



figabrilovich markovitch

appendix effect knowledge breadth text categorization
appendix examine effect performing feature generation newer
wikipedia snapshot defined section see table
larger amount knowledge leads average greater improvements text categorization performance although difference performance two versions
admittedly small consistent across datasets similar situation happens assessing
role external knowledge computing semantic relatedness see section

dataset

baseline

micro
bep
reuters cat
reuters cat
rcv industry

rcv industry
rcv industry b
rcv industry c
rcv industry
rcv industry e
rcv topic

rcv topic

rcv topic b

rcv topic c

rcv topic

rcv topic e

ohsumed

ohsumed b

ohsumed c

ohsumed

ohsumed e

ng

movies

average

macro
bep




















wikipedia

micro macro
bep bep






















improvement

micro macro
bep
bep























improvement

micro macro
bep
bep























table effect feature generation newer wikipedia snapshot dated
march



fiwikipedia semantic interpretation

appendix b test collections text categorization
appendix provides detailed description test collections used evaluate
knowledge feature generation text categorization
b reuters
data set contains one year worth english language stories distributed
reuters newswire arguably often used test collection
text categorization reuters cleaned version earlier release named
reuters contained errors duplicate documents
collection contains documents hence name sgml format
documents categorized e assigned category label marked belonging
category documents explicit classification
reasonably belong categories judged content marked several train test splits collection defined modapte modified apte
commonly used one modapte split divides collection chronologically
allocates first documents training rest documents testing
documents labeled categories labels per document
average category distribution extremely skewed largest category
earn positive examples categories one positive example
several category sets defined collection
largest categories earn acq money fx grain crude trade interest ship wheat corn
categories least one document training set one testing set
yang
galavotti sebastiani simi used set categories least one
training example three categories cottonseed f cattle sfr training
examples modapte split
full set categories least one positive example training
testing set
following common practice used modapte split two category sets largest
categories categories least one training testing example
b newsgroups ng
newsgroups collection lang comprised postings usenet
newsgroups documents single label defined name newsgroup
sent documents cross posted hence several
labels newsgroup contains exactly positive examples exception
soc religion christian contains
categories quite close scope example comp sys ibm pc hardware
comp sys mac hardware talk religion misc soc religion christian document


figabrilovich markovitch

posted single newsgroup may reasonably considered appropriate groups
author may simply known similar groups thus cross posted
message naturally poses additional difficulty classification
noted internet news postings informal therefore documents frequently contain non standard abbreviated words foreign words proper
names well large amount markup characters used attribution authorship
message separation
b movie reviews
movie reviews collection pang et al presents example sentiment classification different standard topical text categorization collection
contains reviews movies half express positive sentiment opinion
movie half negative reviews collected rec arts movies reviews
newsgroup archived internet movie database imdb http www imdb com
classification case determine semantic orientation document rather relate content one predefined topics
arguably difficult topical text categorization since notion semantic orientation quite general saw collection opportunity apply feature generation
techniques task
recent works semantic orientation include turney littman turney
pang et al two former studies used unsupervised learning techniques
latent semantic indexing estimating semantic distance given document
two reference words represent polar opinions namely excellent poor
latter work used classical tc techniques
b reuters corpus version rcv
rcv newest corpus released reuters lewis et al rose stevenson
whitehead considerably larger predecessor contains
news items dated august august stories labeled
category sets topics industries regions
topics close nature category set old reuters collection
reuters topic codes categories per document
average topics organized hierarchy hierarchy policy required category assigned document ancestors hierarchy
assigned well many topic assignments
field genre classification attempts establish genre document somewhat related
sentiment classification examples possible genres radio news transcripts classified advertisements work dewdney vaness dykema macmillan cast text
categorization presentation features addition words presentation features included
part speech tags verb tenses well mean variance statistics sentence word length
punctuation usage amount whitespace characters support vector machines actual
classification authors found performance due presentation features alone least
good achieved plain words combined feature set usually resulted
improvement several percentage points



fiwikipedia semantic interpretation

due four general categories ccat ecat gcat mcat consequently micro averaged performance scores dominated categories
lewis et al macro averaging becomes interest minimum code
policy required document assigned least one topic one region
code
industries fine grained topics therefore harder classification categories organized hierarchy although hierarchy
policy partially enforced documents labeled
industry codes
region codes correspond geographical places subdivided countries regional groupings economic groupings lewis et al argue
region codes might suitable named entity recognition text categorization
experiments used topic industry categories due sheer size
collection processing categories set would unreasonably long allowing
conduct experiments speed experimentation used subset corpus
training documents dated august testing documents
dated august following scheme introduced brank et al
used topic industry categories constitute representative sample
full groups categories respectively randomly sampled topic
industry categories sets categories table gives full definition
category sets used
noted lewis et al original rcv distribution contains number
errors particular documents conform minimum code
hierarchy policy labeled erratic codes lewis et al proposed procedure
correct errors defined version collection named rcv v
opposed original distribution referred rcv v experiments
rcv v
b ohsumed
ohsumed hersh et al subset medline database contains
references documents published medical journals period
reference contains publication title two thirds contain
abstract document labeled several mesh categories mesh
distinct categories collection average categories per
document ohsumed frequently used information retrieval text categorization

following joachims used subset documents abstracts
taking first documents training next testing limit
number categories experiments randomly generated sets categories
table gives full definition category sets used
micro averaged scores topic codes much higher macro averaged ones see
section



figabrilovich markovitch

set name
topic
topic
topic b
topic c
topic
topic e
industry

industry
industry b
industry c
industry
industry e

categories comprising set
e gobit e c e godd ghea e c
gspo c e gpol c
e c c c c ecat c c c
c g gwea grel c e c e c
c c gtour c g gdef e genv e c
c c e gdis c c gpro c g c
c e e ghea c gdip gcrim e gvio














table definition rcv category sets used experiments

appendix c additional examples feature generation text
categorization
appendix list number additional feature generation examples
text development cell leukaemia following otherwise successful treatment three patients x linked severe combined immune deficiency x scid
gene therapy trials haematopoietic stem cells led evaluation
mouse model gene therapy x scid
corrective therapeutic gene il rg act contributor genesis
cell lymphomas one third animals affected gene therapy trials
x scid assumption il rg minimally oncogenic
may therefore pose risk patients
top generated features leukemia severe combined immunodeficiency cancer non hodgkin lymphoma aids icd chapter
ii neoplasms chapter iii diseases blood blood forming organs
certain disorders involving immune mechanism bone marrow transplant immunosuppressive drug acute lymphoblastic leukemia multiple sclerosis

selected explanations particular cancer type disease code icd
international statistical classification diseases related health
text scientific methods biology


fiwikipedia semantic interpretation

set name
ohsumed

ohsumed b

ohsumed c

ohsumed

ohsumed e

categories comprising set
parentheses contain mesh identifiers
b lymphocytes
metabolism inborn errors
creatinine hypersensitivity
bone diseases metabolic fungi
england biliary tract
forecasting radiation
thymus gland insurance
historical geographic locations
leukocytes hemodynamics
depression clinical competence
anti inflammatory agents non steroidal
cytophotometry hydroxy acids
endothelium vascular
contraceptives oral hormonal
acquired immunodeficiency syndrome
gram positive bacteria diarrhea
embolism thrombosis
health behavior molecular probes
bone diseases developmental
referral consultation
antineoplastic immunosuppressive agents
receptors antigen cell
government arthritis rheumatoid
animal structures bandages
italy investigative techniques
physical sciences anthropology
htlv blv infections
hemoglobinopathies vulvar diseases
polycyclic hydrocarbons aromatic
age factors philosophy medical
antigens cd
computing methodologies
islets langerhans regeneration

table definition ohsumed category sets used experiments



figabrilovich markovitch

top generated features biology scientific classification science chemical biology binomial nomenclature nature journal
social sciences philosophy biology scientist history
biology
selected explanations formal method naming species biology
text quavering voices parents grandparents killed world
trade center read names victims solemn recitation today marking
third anniversary terror attacks ceremony one many planned
united states around world honor memory nearly victims

top generated features september attack memorials services united airlines flight aftermath september
attacks world trade center september attacks oklahoma city bombing world trade center bombing arlington national
cemetery world trade center site jewish bereavement
selected explanations one four flights hijacked september
terrorist attack oklahoma city american military cemetery
text u intelligence cannot say conclusively saddam hussein weapons
mass destruction information gap complicating white house efforts
build support attack saddams iraqi regime cia advised top
administration officials assume iraq weapons mass destruction
agency given president bush smoking gun according u
intelligence administration officials
top generated features iraq disarmament crisis yellowcake forgery senate report pre war intelligence iraq iraq weapons
mass destruction iraq survey group september dossier iraq
war scott ritter iraq war rationale operation desert fox
selected explanations falsified intelligence documents iraqs alleged
attempt purchase yellowcake uranium iraqs weapons mass
destruction published uk government un weapons inspector
iraq us uk joint military campaign iraq
another example consider pair contexts contain word jaguar
first one contains ambiguous word sense car model second
onein sense animal
text jaguar car
top generated features jaguar car jaguar type
jaguar x type jaguar e type jaguar xj daimler motor company british leyland motor corporation luxury vehicles v
engine jaguar racing
top generated features particular jaguar car
car manufacturing company became part jaguar


fiwikipedia semantic interpretation

another vehicle manufacturing company merged jaguar internal
combustion engine used jaguar car formula one team
used jaguar promote brand name
text jaguar panthera onca
top generated features jaguar felidae black panther
leopard puma tiger panthera hybrid cave lion
american lion kinkajou
top generated features family include lions tigers jaguars
related feline species another carnivore mammal
number examples generating features inter article links
text artificial intelligence
regular feature generation artificial intelligence film
mit computer science artificial intelligence laboratory artificial
life strong ai swarm intelligence computer science frame
cognitive science carl hewitt
features generated links robot john mccarthy computer scientist artificial consciousness marvin minsky planner programming language actor model model concurrent computation formulated
carl hewitt colleagues logic scientific community metaphor
natural language processing lisp programming language
general features robot massachusetts institute technology psychology consciousness lisp programming language
text group european led astronomers made photograph appears
planet orbiting another star would first confirmed picture
world beyond solar system
regular feature generation planet solar system astronomy
planetary orbit extrasolar planet pluto jupiter neptune
minor planet mars
features generated links asteroid earth oort cloud
postulated cloud comets comet sun saturn moon mercury
planet asteroid belt orbital period
general features earth moon asteroid sun
national aeronautics space administration
text nearly percent americans say careful eat
even say diet essential good health according nationwide health
poll obesity ranked second among biggest health concerns
regular feature generation veganism vegetarianism obesity
atkins nutritional binge eating disorder dick gregory
nutrition super size health insurance eating disorder


figabrilovich markovitch

selected explanations philosophy avoiding animal derived food
american nutritionist documentary film individual eats
mcdonalds fast food one full month
features generated links raw food diet diabetes mellitus
healthy eating body mass index omega fatty acid important nutritional component dieting milk united states hypertension
egg food
general features united states diabetes mellitus cancer food mcdonalds

appendix comparing knowledge sources feature generation
wikipedia versus wordnet
demonstrate shortcomings wordnet source knowledge feature generation juxtapose wordnet wikipedia feature generation sample
sentence examined section repeat wikipedia context classifications
readers convenience
used wordnet version follows wordnet synsets denoted curly
braces noun verb synsets followed immediate hypernym general
synset applicable
text wal mart supply chain goes real time
relevant wikipedia classifications
wal mart
sam walton wal mart founder
sears holdings corporation target corporation albertsons prominent competitors wal mart
rfid radio frequency identification technology wal mart uses
extensively manage stock
hypermarket superstore general concept wal mart specific
example
united food commercial workers labor union trying
organize wal marts workers
wordnet
wal mart word found wordnet
supply indefinite quantity supply economic process provision
providing supply supplying activity
classification quite interesting issue discussed input text fragment characteristic american life style



fiwikipedia semantic interpretation

supply provide render furnish give issue supply distribute
provide supply ply cater give add append supply state say
tell
chain concatenation series chain chemical chain unit building
block chain ligament chain business concern business concern
business organization chain restraint constraint chain linear
unit range mountain range range mountains chain mountain chain
chain maountains geological formation geology formation iron irons
chain chains shackle bond hamper trammel trammels chain string
strand necklace
chain fasten fix secure
go spell tour turn shift work shift duty period crack fling go pass
whirl offer attempt effort endeavor endeavour try go go game
board game
travel move go locomote go proceed move act move
verb senses omitted brevity
real number real real spanish coin
real existent real less stated real true real
taken lightly real tangible actual genuine literal real real economics substantial real material real property veridical real
real founded practical matters
really real rattling
time clip case instance example time time period period
period time amount time time moment minute second instant time abstraction clock time time reading meter reading fourth dimension time dimension time experience meter
time rhythmicity prison term sentence time term
clock time quantify measure time schedule time
determine shape influence regulate time adjust set
evidently wordnet classifications overly general diverse context words
cannot properly disambiguated furthermore owing lack proper names wordnet
cannot possibly provide wealth information encoded wikipedia easily overcomes drawbacks wordnet methodology proposed suffer
shortcomings



figabrilovich markovitch

references
adafre f de rijke discovering missing links wikipedia proceedings
workshop link discovery issues approaches applications linkkdd pp
baeza yates r ribeiro neto b modern information retrieval addison wesley
york ny
baker mccallum k distributional clustering words text classification croft b moffat van rijsbergen c j wilkinson r zobel j eds
proceedings st acm international conference development
information retrieval pp melbourne au acm press york us
basili r moschitti pazienza language sensitive text classification
proceedings riao th international conference recherche dinformation
assistee par ordinateur pp paris france
begelman g keller p smadja f automated tag clustering improving search
exploration tag space proceedings collaborative web tagging
workshop conjunction th international world wide web conference
edinburgh scotland
bekkerman r distributional clustering words text categorization masters
thesis technion
bloehdorn hotho boosting text classification semantic features
proceedings msw workshop th acm sigkdd conference
knowledge discovery data mining pp
brank j grobelnik milic frayling n mladenic interaction feature
selection methods linear classification workshop text learning
held icml
brill e transformation error driven learning natural language processing case study part speech tagging computational linguistics

buchanan b g feigenbaum e forward davis r lenat eds
knowledge systems artificial intelligence mcgraw hill
budanitsky hirst g evaluating wordnet measures lexical semantic
relatedness computational linguistics
cai l hofmann text categorization boosting automatically extracted
concepts proceedings th international conference development information retrieval pp
caropreso f matwin sebastiani f learner independent evaluation
usefulness statistical phrases automated text categorization chin g
ed text databases document management theory practice pp
idea group publishing hershey us


fiwikipedia semantic interpretation

chang w ratinov l roth srikumar v importance semantic
representation dataless classification proceedings rd aaai conference
artificial intelligence pp
cohen w w fast effective rule induction proceedings th international
conference machine learning icml pp
cohen w w automatically extracting features concept learning web
proceedings th international conference machine learning
dagan lee l pereira f c n similarity word cooccurrence
probabilities machine learning
dagan marcus markovitch contextual word similarity estimation
sparse data computer speech language
davidov gabrilovich e markovitch parameterized generation labeled
datasets text categorization hierarchical directory proceedings
th acm international conference development information
retrieval pp
debole f sebastiani f supervised term weighting automated text categorization proceedings sac th acm symposium applied computing
pp
deerwester dumais furnas g landauer harshman r indexing
latent semantic analysis journal american society information science

demsar j statistical comparison classifiers multiple data sets journal
machine learning
dewdney n vaness dykema c macmillan r form substance
classification genres text workshop hlt km held acl
dhillon mallela kumar r divisive information theoretic feature clustering text classification journal machine learning

dumais platt j heckerman sahami inductive learning
representations text categorization proceedings th acm international conference information knowledge management pp
egozi gabrilovich e markovitch concept feature generation
selection information retrieval aaai
fawcett feature discovery solving systems ph thesis umass
fellbaum c ed wordnet electronic lexical database mit press cambridge
finkelstein l gabrilovich e matias rivlin e solan z wolfman g ruppin
e placing search context concept revisited acm transactions
information systems


figabrilovich markovitch

finkelstein l gabrilovich e matias rivlin e solan z wolfman g ruppin
e b wordsimilarity test collection
fuernkranz j mitchell riloff e case study linguistic phrases
text categorization www sahami ed learning text categorization proceedings aaai icml workshop pp aaai press
madison wisconsin
gabrilovich e markovitch text categorization many redundant features
aggressive feature selection make svms competitive c proceedings
st international conference machine learning pp
gabrilovich e markovitch feature generation text categorization world knowledge proceedings th international joint conference
artificial intelligence pp edinburgh scotand
gabrilovich e markovitch overcoming brittleness bottleneck
wikipedia enhancing text categorization encyclopedic knowledge proceedings st national conference artificial intelligence pp
gabrilovich e markovitch computing semantic relatedness wikipediabased explicit semantic analysis proceedings th international joint conference artificial intelligence pp
gabrilovich e markovitch b harnessing expertise human editors knowledge feature generation text categorization journal machine
learning
galavotti l sebastiani f simi experiments use feature selection
negative evidence automated text categorization borbinha j baker
eds proceedings ecdl th european conference advanced
technology digital libraries pp lisbon portugal
giles j internet encyclopaedias go head head nature
gurevych mueller c zesch electronic career guidance
semantic relatedness proceedings th annual meeting
association computational linguistics
hersh w buckley c leone hickam ohsumed interactive
retrieval evaluation large test collection proceedings
th acm international conference development information
retrieval pp
hirsh h japkowicz n bootstrapping training data representations inductive
learning case study molecular biology proceedings twelfth national
conference artificial intelligence pp
hirst g st onge lexical chains representations context detection
correction malapropisms wordnet electronic lexical database pp
mit press cambridge
hu j kibler wrapper constructive induction
thirteenth national conference artificial intelligence pp


fiwikipedia semantic interpretation

hughes ramage lexical semantic relatedness random graph walks
proceedings conference empirical methods natural language processing
emnlp
hull improving text retrieval routing latent semantic
indexing croft w b van rijsbergen c j eds proceedings th acm
international conference development information retrieval pp
dublin ireland springer verlag heidelberg germany
jarmasz rogets thesaurus lexical resource natural language processing
masters thesis university ottawa
jarmasz szpakowicz rogets thesaurus semantic similarity
proceedings international conference recent advances natural language
processing pp
jiang j j conrath w semantic similarity corpus statistics
lexical taxonomy proceedings th international conference
computational linguistics pp
jo neurotextcategorizer model neural network text categorization
proceedings international conference neural information processing pp
taejon south korea
jo dynamic document organization text categorization text clustering ph thesis university ottawa
jo japkowicz n text clustering ntso proceedings international joint conference neural networks pp
joachims text categorization support vector machines learning many
relevant features proceedings european conference machine learning
pp
joachims making large scale svm learning practical schoelkopf b burges
c smola eds advances kernel methods support vector learning pp
mit press
kudenko hirsh h feature generation sequence categorization proceedings th conference american association artificial intelligence
pp
kumaran g allan j text classification named entities event
detection proceedings th acm international conference
development information retrieval pp
lang k newsweeder learning filter netnews proceedings th international conference machine learning pp
leacock c chodorow combining local context wordnet similarity
word sense identification wordnet electronic lexical database pp
mit press cambridge
lee l measures distributional similarity proceedings th annual
meeting acl pp


figabrilovich markovitch

lee pincombe b welsh comparison machine measures text
document similarity human judgments th annual meeting cognitive
science society cogsci pp
lenat b cyc large scale investment knowledge infrastructure communications acm
lenat b common sense mind hal hals
legacy pp mit press
lenat b guha r v pittman k pratt shepherd cyc towards
programs common sense communications acm
leopold e kindermann j text categorization support vector machines
represent texts input space machine learning
lewis evaluation phrasal clustered representations text
categorization task proceedings th acm international conference
development information retrieval pp
lewis croft w b term clustering syntactic phrases proceedings
th acm international conference development information
retrieval pp
lewis schapire r e callan j p papka r training
linear text classifiers proceedings th acm international conference
development information retrieval pp
lewis yang rose li f rcv benchmark collection
text categorization journal machine learning
lin automatic retrieval clustering similar words proceedings
th international conference computational linguistics th annual meeting
association computational linguistics pp
lin b information theoretic definition word similarity proceedings
th international conference machine learning pp
liu chen z zhang b w wu g improving text classification
local latent semantic indexing icdm pp
manning c schuetze h foundations statistical natural language processing mit press
markovitch rosenstein feature generation general constructor
functions machine learning
matheus c j need constructive induction birnbaum l collins g
eds proceedings eighth international workshop machine learning pp

matheus c j rendell l constructive induction decision trees
proceedings th international conference artificial intelligence pp



fiwikipedia semantic interpretation

mesh medical subject headings mesh
http www nlm nih gov mesh

national library medicine

metzler dumais meek c similarity measures short segments text
proceedings th european conference information retrieval pp
mihalcea r turning wordnet information retrieval resource systematic
polysemy conversion hierarchical codes international journal pattern recognition artificial intelligence ijprai
mihalcea r corley c strapparava c corpus knowledge
measures text semantic similarity aaai pp
mikheev feature lattices maximum entropy proceedings
th international conference computational linguistics pp
miller g charles w g contextual correlates semantic similarity language cognitive processes
milne witten effective low cost measure semantic relatedness
obtained wikipedia links proceedings aaai workshop wikipedia
artificial intelligence conjunction rd aaai conference artificial
intelligence
mladenic turning yahoo automatic web page classifier proceedings
th european conference artificial intelligence pp
montague r proper treatment quantification ordinary english
hintikka j moravcsik j suppes p eds approaches natural language pp
reidel dordrecht
murphy p pazzani j id constructive induction n concepts discriminators decision trees proceedings th international
conference machine learning pp morgan kaufmann
pagallo g haussler boolean feature discovery empirical learning machine
learning
pang b lee l vaithyanathan thumbs sentiment classification
machine learning techniques proceedings conference empirical methods
natural language processing pp
peng f schuurmans wang augmenting naive bayes classifiers
statistical language information retrieval
peng f shuurmans combining naive bayes n gram language
text classification proceedings th european conference information
retrieval ecir pp
pincombe b comparison human latent semantic analysis lsa judgements
pairwise document similarities news corpus tech rep dsto rr information sciences laboratory defence science technology organization department defense australian government
porter suffix stripping program


figabrilovich markovitch

potthast stein b anderka wikipedia multilingual retrieval
model european conference information retrieval
press w h teukolsky vetterling w flannery b p numerical
recipes c art scientific computing cambridge university press
qiu frei h concept query expansion proceedings acm
international conference development information retrieval
raskutti b ferra h kowalczyk second order features maximizing
text classification performance de raedt l flach p eds proceedings
european conference machine learning ecml lecture notes artificial
intelligence lnai pp springer verlag
resnik p semantic similarity taxonomy information measure
application ambiguity natural language journal artificial
intelligence
reuters reuters text categorization test collection distribution reuters
daviddlewis com resources testcollections reuters
rocchio j j relevance feedback information retrieval smart retrieval
system experiments automatic document processing pp prentice hall
rogati yang high performing feature selection text classification
proceedings international conference information knowledge management cikm pp
roget p rogets thesaurus english words phrases longman group ltd
rose stevenson whitehead reuters corpus
yesterdays news tomorrows language resources proceedings third international conference language resources evaluation pp
rowling j harry potter philosophers stone bloomsbury
rubenstein h goodenough j b contextual correlates synonymy communications acm
sable c mckeown k church k w nlp found helpful least one
text categorization task conference empirical methods natural language
processing pp
sahami heilman web kernel function measuring similarity
short text snippets www pp acm press
salton g buckley c term weighting approaches automatic text retrieval
information processing management
salton g mcgill
mcgraw hill

introduction modern information retrieval

scott feature engineering symbolic text classification masters
thesis u ottawa
scott matwin feature engineering text classification proceedings
th international conference machine learning pp


fiwikipedia semantic interpretation

sebastiani f machine learning automated text categorization acm computing
surveys
snow r oconnor b jurafsky ng cheap fast good
evaluating non expert annotations natural language tasks proceedings
conference empirical methods natural language processing
sorg p cimiano p cross lingual information retrieval explicit semantic
analysis working notes clef workshop
strube ponzetto p wikirelate computing semantic relatedness
wikipedia aaai pp boston
turney p thumbs thumbs semantic orientation applied unsupervised classification reviews proceedings th annual meeting
association computational linguistics pp
turney p measuring semantic similarity latent relational analysis proceedings
nineteenth international joint conference artificial intelligence ijcai
pp edinburgh scotland
turney p similarity semantic relations computational linguistics

turney p littman l unsupervised learning semantic orientation
hundred billion word corpus tech rep erb national council
canada
turney p mining web synonyms pmi ir versus lsa toefl
proceedings twelfth european conference machine learning pp
urena lopez buenaga gomez j integrating linguistic resources
tc wsd computers humanities
wang b b mckay r abbass h barlow comparative study
domain ontology guided feature extraction proceedings th australian
computer science conference ascs pp
widrow b stearns adaptive signal processing prentice hall
wikipedia wikipedia free encyclopedia http en wikipedia org
wu h gunopulos evaluating utility statistical phrases latent
semantic indexing text classification ieee international conference data
mining pp
yang study thresholding strategies text categorization proceedings
th international conference development information
retrieval pp
yang liu x examination text categorization methods proceedings
nd international conference development information
retrieval pp
yang pedersen j comparative study feature selection text categorization proceedings th international conference machine learning
pp


figabrilovich markovitch

zelikovitz hirsh h improving short text classification unlabeled background knowledge assess document similarity proceedings th international conference machine learning pp
zelikovitz hirsh h lsi text classification presence
background text proceedings conference information knowledge
management pp
zesch gurevych automatically creating datasets measures semantic
relatedness proceedings acl workshop linguistic distances pp
sydney australia
zesch mueller c gurevych wiktionary computing semantic
relatedness proceedings rd aaai conference artificial intelligence
pp
zobel j moffat exploring similarity space acm sigir forum





