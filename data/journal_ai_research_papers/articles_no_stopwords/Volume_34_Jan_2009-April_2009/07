Journal Artificial Intelligence Research 34 (2009) 443-498

Submitted 08/08; published 03/09

Wikipedia-based Semantic Interpretation
Natural Language Processing
Evgeniy Gabrilovich
Shaul Markovitch

gabr@yahoo-inc.com
shaulm@cs.technion.ac.il

Department Computer Science
TechnionIsrael Institute Technology
Technion City, 32000 Haifa, Israel

Abstract
Adequate representation natural language semantics requires access vast amounts
common sense domain-specific world knowledge. Prior work field based
purely statistical techniques make use background knowledge, limited
lexicographic knowledge bases WordNet, huge manual efforts
CYC project. propose novel method, called Explicit Semantic Analysis (ESA),
fine-grained semantic interpretation unrestricted natural language texts. method
represents meaning high-dimensional space concepts derived Wikipedia,
largest encyclopedia existence. explicitly represent meaning text terms
Wikipedia-based concepts. evaluate effectiveness method text categorization computing degree semantic relatedness fragments natural
language text. Using ESA results significant improvements previous state
art tasks. Importantly, due use natural concepts, ESA model
easy explain human users.

1. Introduction
Recent proliferation World Wide Web, common availability inexpensive storage
media accumulate time enormous amounts digital data, contributed
importance intelligent access data. sheer amount data available
emphasizes intelligent aspect accessno one willing capable browsing
small subset data collection, carefully selected satisfy ones
precise information need.
Research artificial intelligence long aimed endowing machines ability
understand natural language. One core issues challenge represent language semantics way manipulated computers. Prior work
semantics representation based purely statistical techniques, lexicographic knowledge, elaborate endeavors manually encode large amounts knowledge. simplest
approach represent text semantics treat text unordered bag words,
words (possibly stemmed) become features textual object.
sheer ease approach makes reasonable candidate many information retrieval
tasks search text categorization (Baeza-Yates & Ribeiro-Neto, 1999; Sebastiani,
2002). However, simple model reasonably used texts fairly long,
performs sub-optimally short texts. Furthermore, little address two
main problems natural language processing (NLP), polysemy synonymy.
c
2009
AI Access Foundation. rights reserved.

fiGabrilovich & Markovitch

Latent Semantic Analysis (LSA) (Deerwester, Dumais, Furnas, Landauer, & Harshman,
1990) another purely statistical technique, leverages word co-occurrence information large unlabeled corpus text. LSA use explicit human-organized
knowledge; rather, learns representation applying Singular Value Decomposition
(SVD) words-by-documents co-occurrence matrix. LSA essentially dimensionality reduction technique identifies number prominent dimensions data,
assumed correspond latent concepts. Meanings words documents
represented space defined concepts.
Lexical databases WordNet (Fellbaum, 1998) Rogets Thesaurus (Roget, 1852)
encode important relations words synonymy, hypernymy, meronymy.
Approaches based resources (Budanitsky & Hirst, 2006; Jarmasz, 2003) map text
words word senses, use latter concepts. However, lexical resources offer
little information different word senses, thus making word sense disambiguation
nearly impossible achieve. Another drawback approaches creation
lexical resources requires lexicographic expertise well lot time effort, consequently resources cover small fragment language lexicon. Specifically,
resources contain proper names, neologisms, slang, domain-specific technical
terms. Furthermore, resources strong lexical orientation predominantly contain information individual words, little world knowledge general.
inherently limited individual words, approaches require extra level
sophistication handle longer texts (Mihalcea, Corley, & Strapparava, 2006); example,
computing similarity pair texts amounts comparing word one text
word text.
Studies artificial intelligence long recognized importance knowledge
problem solving general, natural language processing particular. Back
early years AI research, Buchanan Feigenbaum (1982) formulated knowledge
power hypothesis, postulated power intelligent program perform
task well depends primarily quantity quality knowledge
task.
computer programs face tasks require human-level intelligence, natural language processing, natural use encyclopedia endow machine
breadth knowledge available humans. are, however, several obstacles
way using encyclopedic knowledge. First, knowledge available textual
form, using may require natural language understanding, major problem
right. Furthermore, even language understanding may enough, texts written
humans normally assume reader possesses large amount common-sense knowledge,
omitted even detailed encyclopedia articles (Lenat, 1997). Thus,
circular dependencyunderstanding encyclopedia articles requires natural language
understanding capabilities, latter turn require encyclopedic knowledge.
address situation, Lenat colleagues launched CYC project, aims
explicitly catalog common sense knowledge humankind.
developed new methodology makes possible use encyclopedia directly,
without need manually encoded common-sense knowledge. Observe encyclopedia consists large collection articles, provides comprehensive
exposition focused single topic. Thus, view encyclopedia collection con444

fiWikipedia-based Semantic Interpretation

cepts (corresponding articles), accompanied large body text (the article
contents). propose use high-dimensional space defined concepts order
represent meaning natural language texts. Compared bag words LSA
approaches, using concepts allows computer benefit huge amounts world
knowledge, normally accessible humans. Compared electronic dictionaries
thesauri, method uses knowledge resources order magnitude larger,
uniformly treats texts arbitrarily longer single word. Even
importantly, method uses body text accompanies concepts order
perform word sense disambiguation. show later, using knowledge-rich concepts
addresses polysemy synonymy, longer manipulate mere words. call
method Explicit Semantic Analysis (ESA), uses knowledge concepts explicitly
defined manipulated humans.
approach applicable many NLP tasks whose input document (or shorter
natural language utterance), output decision based document contents.
Examples tasks information retrieval (whether document relevant), text
categorization (whether document belongs certain category), comparing pairs
documents assess similarity.1
Observe documents manipulated tasks given form
encyclopedic knowledge intend useplain text. key observation allows us
circumvent obstacles enumerated above, use encyclopedia directly, without
need deep language understanding pre-cataloged common-sense knowledge.
quantify degree relevance Wikipedia concept input text comparing
text article associated concept.
Let us illustrate importance external knowledge couple examples. Without using external knowledge (specifically, knowledge financial markets), one infer
little information brief news title Bernanke takes charge. However, using
algorithm developed consulting Wikipedia, find following concepts
highly relevant input: Ben Bernanke, Federal Reserve, Chairman
Federal Reserve, Alan Greenspan (Bernankes predecessor), Monetarism (an economic theory money supply central banking), inflation deflation. another
example, consider title Apple patents Tablet Mac. Without deep knowledge hitech industry gadgets, one finds hard predict contents news item. Using
Wikipedia, identify following related concepts: Apple Computer2 , Mac OS (the
Macintosh operating system) Laptop (the general name portable computers,
Tablet Mac specific example), Aqua (the GUI Mac OS X), iPod (another prominent product Apple), Apple Newton (the name Apples early personal digital
assistant).
ease presentation, examples showed concepts identified
ESA relevant input. However, essence method representing meaning text weighted combination Wikipedia concepts. Then,
1. Thus, consider tasks machine translation natural language generation, whose output
includes new piece text based input.
2. Note correctly identify concept representing computer company (Apple Computer)
rather fruit (Apple).

445

fiGabrilovich & Markovitch

depending nature task hand either use entire vectors concepts,
use relevant concepts enrich bag words representation.
contributions paper twofold. First, propose new methodology
use Wikipedia enriching representation natural language texts. approach,
named Explicit Semantic Analysis, effectively capitalizes human knowledge encoded
Wikipedia, leveraging information cannot deduced solely input texts
processed. Second, evaluate ESA two commonly occurring NLP tasks, namely, text
categorization computing semantic relatedness texts. tasks, using ESA
resulted significant improvements existing state art performance.
Recently, ESA used researchers variety tasks, consistently proved
superior approaches explicitly used large-scale repositories human
knowledge. Gurevych, Mueller, Zesch (2007) re-implemented ESA approach
German-language Wikipedia, found superior judging semantic relatedness
words compared system based German version WordNet (GermaNet). Chang,
Ratinov, Roth, Srikumar (2008) used ESA text classification task without explicit
training set, learning knowledge encoded Wikipedia. Milne Witten
(2008) found ESA compare favorably approaches solely based hyperlinks,
thus confirming wealth textual descriptions Wikipedia exlicitly superior
using structural information alone.

2. Explicit Semantic Analysis
meaning word cat? One way interpret word cat via
explicit definition: cat mammal four legs, belongs feline species,
etc. Another way interpret meaning cat strength association
concepts know: cat relates strongly concepts feline pet, somewhat
less strongly concepts mouse Tom & Jerry, etc.
use latter association-based method assign semantic interpretation words
text fragments. assume availability vector basic concepts, C1 , . . . , Cn ,
represent text fragment vector weights, w1 , . . . , wn , wi represents
strength association Ci . Thus, set basic concepts viewed
canonical n-dimensional semantic space, semantics text segment corresponds
point space. call weighted vector semantic interpretation vector
t.
canonical representation powerful, effectively allows us estimate
semantic relatedness text fragments distance space. following
section describe two main components scheme: set basic concepts,
algorithm maps text fragments interpretation vectors.
2.1 Using Wikipedia Repository Basic Concepts
build general semantic interpreter represent text meaning variety
tasks, set basic concepts needs satisfy following requirements:
1. comprehensive enough include concepts large variety topics.
446

fiWikipedia-based Semantic Interpretation

2. constantly maintained new concepts promptly added
needed.
3. Since ultimate goal interpret natural language, would concepts
natural, is, concepts recognized used human beings.
4. concept Ci associated text di , determine strength
affinity term language.
Creating maintaining set natural concepts requires enormous effort many
people. Luckily, collection already exists form Wikipedia, one
largest knowledge repositories Web. Wikipedia available dozens languages,
English version largest all, contains 300+ million words nearly
one million articles, contributed 160,000 volunteer editors. Even though Wikipedia
editors required established researchers practitioners, open editing approach yields remarkable quality. recent study (Giles, 2005) found Wikipedia accuracy
rival Encyclopaedia Britannica. However, Britannica order magnitude
smaller, 44 million words 65,000 articles (http://store.britannica.com, visited
February 10, 2006).
appropriate encyclopedia, article comprises comprehensive exposition
single topic. Consequently, view Wikipedia article defining concept
corresponds topic. example, article artificial intelligence defines
concept Artificial Intelligence, article parasitic extraction circuit
design defines concept Layout extraction.3 body articles critical
approach, allows us compute affinity concepts words
input texts.
important advantage approach thus use vast amounts highly organized human knowledge. Compared lexical resources WordNet, methodology
leverages knowledge bases orders magnitude larger comprehensive.
Importantly, Web-based knowledge repositories use work undergo constant
development breadth depth steadily increase time. Compared Latent
Semantic Analysis, methodology explicitly uses knowledge collected organized
humans. semantic analysis explicit sense manipulate manifest concepts grounded human cognition, rather latent concepts used LSA. Therefore,
call approach Explicit Semantic Analysis (ESA).
2.2 Building Semantic Interpreter
Given set concepts, C1 , . . . , Cn , set associated documents, d1 , . . . , dn , build
sparse table n columns corresponds concept, rows

corresponds word occurs i=1...n di . entry [i, j] table corresponds
TFIDF value term ti document dj
[i, j] = tf (ti , dj ) log

n
,
dfi

3. use titles articles convenient way refer articles, algorithm treats
articles atomic concepts.

447

fiGabrilovich & Markovitch

term frequency defined
(

tf (ti , dj ) =

1 + log count(ti , dj ), count(ti , dj ) > 0
,
0,
otherwise

dfi = |{dk : ti dk }| number documents collection contain
term ti (document frequency).
Finally, cosine normalization applied row disregard differences document
length:
[i, j]
[i, j] pPr
,
2
l=1 [i, j]
r number terms.
semantic interpretation word ti obtained row table . is,
meaning word given vector concepts paired TFIDF scores,
reflect relevance concept word.
semantic interpretation text fragment, ht1 , . . . , tk i, centroid vectors
representing individual words. definition allows us partially perform word sense
disambiguation. Consider, example, interpretation vector term mouse.
two sets strong components, correspond two possible meanings: mouse (rodent) mouse (computing). Similarly, interpretation vector word screen
strong components associated window screen computer screen. text
fragment purchased mouse screen, summing two interpretation vectors boost computer-related components, effectively disambiguating words.
Table viewed inverted index, maps word list
concepts appears. Inverted index provides efficient computation
distance interpretation vectors.
Given amount information encoded Wikipedia, essential control
amount noise present text. discarding insufficiently developed articles,
eliminating spurious association articles words. done setting
zero weights concepts whose weights given term low (see
Section 3.2.3).
2.3 Using Link Structure
natural electronic encyclopedia provide cross-references form
hyperlinks. result, typical Wikipedia article many links entries
articles conventional printed encyclopedias.
link structure used number ways. Observe link associated
anchor text (clickable highlighted phrase). anchor text always identical
canonical name target article, different anchor texts used refer
article different contexts. example, anchor texts pointing Federal
Reserve include Fed, U.S. Federal Reserve Board, U.S. Federal Reserve System,
Board Governors Federal Reserve, Federal Reserve Bank, foreign reserves
Free Banking Era. Thus, anchor texts provide alternative names, variant spellings,
related phrases target concept, use enrich article text
target concept.
448

fiWikipedia-based Semantic Interpretation

Furthermore, inter-article links often reflect important relations concepts
correspond linked articles. explore use relations feature generation
next section.
2.3.1 Second-order Interpretation
Knowledge concepts subject many relations, including generalization, meronymy
(part of), holonymy synonymy, well specific relations capital of,
birthplace/birthdate etc. Wikipedia notable example knowledge repository
features relations, represented hypertext links Wikipedia
articles.
links encode large amount knowledge, found article texts.
Consequently, leveraging knowledge likely lead better interpretation models.
therefore distinguish first-order models, use knowledge encoded
Wikipedia articles, second-order models, incorporate knowledge encoded
inter-article links. Similarly, refer information obtained inter-article
links second-order information.
rule, presence link implies relation concepts connects.
example, article United States links Washington, D.C. (country
capital) North America (the continent country situated). links
multitude concepts, definitely related source concept, albeit
difficult define relations; links include United States Declaration
Independence, President United States, Elvis Presley.
However, observations reveal existence link always imply
two articles strongly related.4 fact, many words phrases typical Wikipedia
article link articles entries corresponding concepts.
example, Education subsection article United States gratuitous
links concepts High school, College, Literacy rate. Therefore, order
use Wikipedia links semantic interpretation, essential filter linked concepts
according relevance text fragment interpreted.
intuitive way incorporate concept relations examine number top-scoring
concepts,
Eto boost scores concepts linked them. Let ESA(1) (t) =

(1)
(1)
w1 , . . . , wn interpretation vector term t. define second-level interpretation term


(2)

ESA(2) (t) = w1 , . . . , wn(2)

(2)

wi

(1)

= wi

X

+

E

(1)

wj

{j|link(cj ,ci )}

Using < 1 ensures linked concepts taken reduced weights.
experiments used = 0.5.
4. opposite truethe absence link may simply due oversight. Adafre de Rijke
(2005) studied problem discovering missing links Wikipedia.

449

fiGabrilovich & Markovitch

2.3.2 Concept Generality Filter
new concepts identified links equally useful. Relevance newly
added concepts certainly important, criterion. Suppose
given input text Google search. additional concept likely
useful characterize input: Nigritude ultramarine (a specially crafted meaningless
phrase used search engine optimization contest) Website? suppose input
artificial intelligence concept likely contribute representation
input, John McCarthy (computer scientist) Logic? believe
examples, second concept would useful overly specific.
Consequently, conjecture add linked concepts sparingly, taking
general concepts triggered them. judge
generality concepts? may tricky achieve general case (no pun
intended), propose following task-oriented criterion. Given two concepts ca cb ,
compare numbers links pointing them. Then, say ca general
cb number incoming links least order magnitude larger, is,
log10 (#inlinks(ca )) log10 (#inlinks(cb )) > 1.
show examples additional concepts identified using inter-article links Section 4.5.1. Section 4.5.4 evaluate effect using inter-article links additional
knowledge source. section specifically examine effect using
general linked concepts (i.e., adding concepts general concepts
triggered them).

3. Using Explicit Semantic Analysis Computing Semantic
Relatedness Texts
section discuss application semantic interpretation methodology
automatic assessment semantic relatedness words texts.5
3.1 Automatic Computation Semantic Relatedness
related cat mouse? preparing manuscript writing article? ability quantify semantic relatedness texts underlies many fundamental tasks computational linguistics, including word sense disambiguation, information
retrieval, word text clustering, error correction (Budanitsky & Hirst, 2006). Reasoning semantic relatedness natural language utterances routinely performed
humans remains unsurmountable obstacle computers. Humans judge text
relatedness merely level text words. Words trigger reasoning much deeper
level manipulates conceptsthe basic units meaning serve humans organize
share knowledge. Thus, humans interpret specific wording document
much larger context background knowledge experience. Lacking
elaborate resources, computers need alternative ways represent texts reason
them.
Explicit Semantic Analysis represents text interpretation vectors high-dimensional space concepts. representation, computing semantic relatedness texts
5. Preliminary results research reported Gabrilovich Markovitch (2007a).

450

fiWikipedia-based Semantic Interpretation

Building Semantic Interpreter

word1
wordi

Building weighted
inverted index
Wikipedia

wordn

Weighted list
concepts
(= Wikipedia
articles)

Weighted
inverted index

Using Semantic Interpreter

Text1

Semantic
interpreter

Vector
comparison

Relatedness
estimation

Text2
Weighted
vector
Wikipedia
concepts

Figure 1: Knowledge-based semantic interpreter

simply amounts comparing vectors. Vectors could compared using variety
metrics (Zobel & Moffat, 1998); use cosine metric throughout experiments
reported paper. Figure 1 illustrates process.
3.2 Implementation Details
used Wikipedia snapshot November 11, 2005. parsing Wikipedia XML
dump, obtained 1.8 Gb text 910,989 articles. Although Wikipedia almost
million articles, equally useful feature generation. articles correspond overly specific concepts (e.g., Metnal, ninth level Mayan underworld),
otherwise unlikely useful subsequent text categorization (e.g., specific dates
list events happened particular year). articles short,
cannot reliably classify texts onto corresponding concepts. developed set
simple heuristics pruning set concepts, discarding articles fewer
100 non stop words fewer 5 incoming outgoing links. discard articles describe specific dates, well Wikipedia disambiguation pages, category pages
like. pruning, 171,332 articles left defined concepts used
feature generation. processed text articles first tokenizing it, removing
stop words rare words (occurring fewer 3 articles), stemmed remaining
words; yielded 296,157 distinct terms.
451

fiGabrilovich & Markovitch

3.2.1 Preprocessing Wikipedia XML Dump
Wikipedia data publicly available online http://download.wikimedia.org.
data distributed XML format, several packaged versions available: article texts,
edit history, list page titles, interlanguage links etc. project, use article
texts, ignore information article authors page modification history.
building semantic interpreter, perform number operations distributed
XML dump:
simplify original XML removing fields used feature
generation, author ids last modification times.
Wikipedia syntax defines proprietary format inter-article links, whereas name
article referred enclosed brackets (e.g., [United States]). map
articles numeric ids, article build list ids articles refers
to. count number incoming outgoing links article.
Wikipedia defines redirection mechanism, maps frequently used variant names
entities canonical names. examples, United States America
mapped United States. resolve redirections initial preprocessing.
Another frequently used mechanism templates, allows articles include
frequently reused fragments text without duplication, including pre-defined
optionally parameterized templates fly. speed subsequent processing,
resolve template inclusions beginning.
collect anchor texts point article.
preprocessing stage yields new XML file, used building feature
generator.
3.2.2 Effect Knowledge Breadth
Wikipedia constantly expanded new material volunteer editors contribute
new articles extend existing ones. Consequently, conjectured addition
information beneficial ESA, would rely larger knowledge base.
test assumption, acquired newer Wikipedia snapshot March 26,
2006. Table 1 presents comparison amount information two Wikipedia
snapshots used. number articles shown table reflects total number
articles date snapshot. next table line (the number concepts
used) reflects number concepts remained pruning explained
beginning Section 3.2.
following sections confirm using larger knowledge base beneficial
ESA, juxtaposing results obtained two Wikipedia snapshots. Therefore,
dimensionality reduction performed, input text fragment represented space 171,332 features (or 241,393 features case later
Wikipedia snapshot); course, many features zero values, feature
vectors sparse.
452

fiWikipedia-based Semantic Interpretation

Combined article text
Number articles
Concepts used
Distinct terms

Wikipedia snapshot
November 11, 2005
1.8 Gb
910,989
171,332
296,157

Wikipedia snapshot
March 23, 2006
2.9 Gb
1,187,839
241,393
389,202

Table 1: Comparison two Wikipedia snapshots
3.2.3 Inverted Index Pruning
eliminate spurious association articles words setting zero weights
concepts whose weights given term low.
algorithm pruning inverted index operates follows. first sort
concepts given word according TFIDF weights decreasing order.
scan resulting sequence concepts sliding window length 100, truncate
sequence difference scores first last concepts window
drops 5% highest-scoring concept word (which positioned first
sequence). technique looks fast drops concept scores, would signify
concepts tail sequence loosely associated word (i.e.,
even though word occurred articles corresponding concepts,
truly characteristic article contents). evaluated principled approaches
observing values first second derivatives, data seemed
noisy reliable estimation derivatives. researchers studied use derivatives
similar contexts (e.g., Begelman, Keller, & Smadja, 2006), found
derivative alone sufficient, hence found necessary estimate magnitude
peaks means. Consequently, opted use simple efficient metric.
purpose pruning eliminate spurious associations concepts
terms, mainly beneficial pruning inverted index entries common
words occur many Wikipedia articles. Using criteria, analyzed
inverted index Wikipedia version dated November 11, 2005 (see Section 3.2.2).
majority terms, either fewer 100 concepts non-zero weight,
concept-term weights decreased gracefully qualify pruning. pruned
entries 4866 terms total 296,157 terms. Among terms whose concept
vector pruned, term link largest number concepts non-zero
weight106,988of retained 838 concepts (0.8%); another example,
concept vector term number pruned 52,244 entries 1360 (2.5%).
average, 24% concepts retained. pruning rates second
Wikipedia version (dated March 23, 2006) similar these.
3.2.4 Processing Time
Using world knowledge requires additional computation. extra computation includes
(one-time) preprocessing step semantic interpreter built, well
actual mapping input texts interpretation vectors, performed online. standard workstation, parsing Wikipedia XML dump takes 7 hours, building
453

fiGabrilovich & Markovitch

semantic interpreter takes less hour. semantic interpreter built,
throughput (i.e., generation interpretation vectors textual input) several hundred words per second. light improvements computing semantic relatedness
text categorization accuracy report Sections 3 4, believe
extra processing time well compensated for.
3.3 Empirical Evaluation Explicit Semantic Analysis
Humans innate ability judge semantic relatedness texts. Human judgements
reference set text pairs thus considered correct definition, kind gold
standard computer algorithms evaluated. Several studies measured
inter-judge correlations found consistently high (Budanitsky & Hirst, 2006;
Jarmasz, 2003; Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, & Ruppin, 2002a),
r = 0.88 0.95. findings expectedafter all, consensus allows
people understand other. Consequently, evaluation amounts computing
correlation ESA relatedness scores human judgments.
better evaluate Wikipedia-based semantic interpretation, implemented semantic interpreter based another large-scale knowledge repositorythe Open Directory
Project (ODP, http://www.dmoz.org), largest Web directory date. case
ODP, concepts Ci correspond categories directory (e.g., Top/Computers/Artificial Intelligence), text di associated concept obtained pooling
together titles descriptions URLs catalogued corresponding category. Interpretation text fragment amounts computing weighted vector ODP
concepts, ordered affinity input text. built ODP-based semantic
interpreter using ODP snapshot April 2004. implementation details
found previous work (Gabrilovich & Markovitch, 2005, 2007b).
3.3.1 Test Collections
work, use two datasets best knowledge largest publicly
available collections kind.6 test collections, use correlation
computer-assigned scores human scores assess algorithm performance.
assess word relatedness, use WordSimilarity-353 collection (Finkelstein et al.,
2002a; Finkelstein, Gabrilovich, Matias, Rivlin, Solan, Wolfman, & Ruppin, 2002b),
contains 353 noun pairs representing various degrees similarity.7 pair 13
16 human judgements made individuals university degrees either mothertongue-level otherwise fluent command English language. Word pairs
assigned relatedness scores scale 0 (totally unrelated words) 10 (very much
related identical words). Judgements collected word pair averaged
6. Recently, Zesch Gurevych (2006) discussed automatic creation datasets assessing semantic
similarity. However, focus work automatical generation set sufficiently
diverse word pairs, thus relieving humans need construct word lists manually. Obviously,
establishing gold standard semantic relatedness word pair still performed manually
human judges.
7. previous studies (Jarmasz & Szpakowicz, 2003) suggested word pairs comprising
collection might culturally biased.

454

fiWikipedia-based Semantic Interpretation

produce single relatedness score.8 Spearmans rank-order correlation coefficient used
compare computed relatedness scores human judgements; non-parametric,
Spearmans correlation coefficient considered much robust Pearsons
linear correlation. comparing results studies, computed
Spearmans correlation coefficient human judgments based raw data.
document similarity, used collection 50 documents Australian
Broadcasting Corporations news mail service (Lee, Pincombe, & Welsh, 2005; Pincombe,
2004). documents 51 126 words long, covered variety topics.
judges 83 students University Adelaide, Australia, paid
small fee work. documents paired possible ways,
1,225 pairs 812 human judgements (averaged pair). neutralize effects
ordering, document pairs presented random order, order documents
within pair randomized well. human judgements averaged
pair, collection 1,225 relatedness scores 67 distinct values. Spearmans
correlation appropriate case, therefore used Pearsons linear correlation
coefficient.
Importantly, instructions human judges test collections specifically directed
participants assess degree relatedness words texts involved. example,
case antonyms, judges instructed consider similar rather
dissimilar.
3.3.2 Prior Work
number prior studies proposed variety approaches computing word similarity
using WordNet, Rogets thesaurus, LSA. Table 2 presents results applying
approaches WordSimilarity-353 test collection.
Jarmasz (2003) replicated results several WordNet-based methods, compared
new approach based Rogets Thesaurus. Hirst St-Onge (1998) viewed
WordNet graph, considered length directionality graph path connecting two nodes. Leacock Chodorow (1998) used length shortest graph
path, normalized maximum taxonomy depth. Jiang Conrath (1997),
later Resnik (1999), used notion information content lowest node subsuming two given words. Lin (1998b) proposed computation word similarity based
information theory. See (Budanitsky & Hirst, 2006) comprehensive discussion
WordNet-based approaches computing word similarity.
According Jarmasz (2003), Rogets Thesaurus number advantages compared
WordNet, including links different parts speech, topical groupings, variety relations word senses. Consequently, method developed authors
using Rogets source knowledge achieved much better results WordNet-based
methods. Finkelstein et al. (2002a) reported results computing word similarity using
8. Finkelstein et al. (2002a) report inter-judge agreement 0.95 WordSimilarity-353 collection.
performed assessment inter-judge agreement dataset. Following Snow,
OConnor, Jurafsky, Ng (2008), divided human judges two sets averaged numeric
judgements word pair among judges set, thus yielding (353 element long) vector
average judgments set. Spearmans correlation coefficient vectors two sets
0.903.

455

fiGabrilovich & Markovitch

LSA-based model (Deerwester et al., 1990) trained Grolier Academic American Encyclopedia. Recently, Hughes Ramage (2007) proposed method computing semantic relatedness using random graph walks; results WordSimilarity353 dataset competitive reported Jarmasz (2003) Finkelstein et al.
(2002a).
Strube Ponzetto (2006) proposed alternative approach computing word similarity based Wikipedia, comparing articles whose titles words occur. discuss
approach greater detail Section 5.1.
Prior work assessing similarity textual documents based comparing
documents bags words, well LSA. Lee et al. (2005) compared number
approaches based bag words representation, used binary tfidf
representation word weights variety similarity measures (correlation, Jaccard,
cosine, overlap). authors implemented LSA-based model trained set
news documents Australian Broadcasting Corporation (test documents whose
similarity computed came distribution). results experiments
reported Table 3.
3.3.3 Results
better understand Explicit Semantic Analysis works, let us consider similarity computation pairs actual phrases. example, given two phrases scientific article
journal publication, ESA determines following Wikipedia concepts found
among top 20 concepts phrase: Scientific journal, Nature (journal),
Academic publication, Science (journal), Peer review. compute
similarity RNA DNA, following concepts found shared among
top 20 lists: Transcription (genetics), Gene, RNA, Cell (biology).
presence identical concepts among top concepts characterizing phrase
allows ESA establish semantic similarity.
Table 2 shows results applying methodology estimating relatedness
individual words, statistically significant improvements shown bold. values
shown table represent Spearmans correlation human judgments
relatedness scores produced different methods. Jarmasz (2003) compared
performance 5 WordNet-based metrics, namely, proposed Hirst St-Onge
(1998), Jiang Conrath (1997), Leacock Chodorow (1998), Lin (1998b), Resnik
(1999). Table 2 report performance best metrics, namely,
Lin (1998b) Resnik (1999). WikiRelate! paper (Strube & Ponzetto, 2006),
authors report results many 6 different method variations, report
performance best one (based metric proposed Leacock Chodorow,
1998).
see, ESA techniques yield substantial improvements previous state
art results. Notably, ESA achieves much better results another recently
introduce method based Wikipedia (Strube & Ponzetto, 2006). provide detailed
comparison approach latter work Section 5.1. Table 3 shows results
computing relatedness entire documents. tables, show statistical
significance difference performance ESA-Wikipedia (March 26, 2006
456

fiWikipedia-based Semantic Interpretation

Algorithm

WordNet-based techniques (Jarmasz, 2003)
Rogets Thesaurus-based technique (Jarmasz, 2003)
LSA (Finkelstein et al., 2002a)
WikiRelate! (Strube & Ponzetto, 2006)
MarkovLink (Hughes & Ramage, 2007)
ESA-Wikipedia (March 26, 2006 version)
ESA-Wikipedia (November 11, 2005 version)
ESA-ODP

Spearmans
correlation
human judgements
0.35
0.55
0.56
0.50
0.55
0.75
0.74
0.65

Stat.
significance
(p-value)
4 1016
1.3 106
3.4 106
8 109
1.6 106


0.0044

Table 2: Spearmans rank correlation word relatedness scores human judgements
WordSimilarity-353 collection
Algorithm

Bag words (Lee et al., 2005)
LSA (Lee et al., 2005)
ESA-Wikipedia (March 26, 2006 version)
ESA-Wikipedia (November 11, 2005 version)
ESA-ODP

Pearsons
correlation
human judgements
0.10.5
0.60
0.72
0.71
0.69

Stat.
significance
(p-value)
4 1019
5 108


0.07

Table 3: Pearsons correlation text relatedness scores human judgements Lee et
al.s document collection

version) algorithms9 using Fishers z-transformation (Press, Teukolsky,
Vetterling, & Flannery, 1997, Section 14.5).
test collections, Wikipedia-based semantic interpretation superior
ODP-based one; word relatedness task, superiority statistically significant
p < 0.005. believe two factors contribute phenomenon. First, axes
multi-dimensional interpretation space ideally independent possible.
hierarchical organization Open Directory reflects generalization relation
concepts obviously violates independence requirement. Second, increase
amount training data building ODP-based semantic interpreter, crawled
URLs listed ODP. allowed us increase amount textual data several
orders magnitude, brought non-negligible amount noise,
common Web pages. hand, Wikipedia articles virtually noise-free,
9. Whenever range values available, compared ESA-Wikipedia best-performing method
range.

457

fiGabrilovich & Markovitch

mostly qualify Standard Written English. Thus, textual descriptions Wikipedia
concepts arguably focused ODP concepts.
essential note experiments, using newer Wikipedia snapshot
leads better results (although difference performance two versions
admittedly small).
evaluated effect using second-order interpretation computing semantic
relatedness texts, yielded negligible improvements. hypothesize
reason finding computing semantic relatedness essentially uses available
Wikipedia concepts, second-order interpretation slightly modify weights
existing concepts. next section, describes application ESA text
categorization, trim interpretation vectors sake efficiency, consider
highest-scoring concepts input text fragment. scenario, secondorder interpretation positive effect actually improves accuracy text
categorization (Section 4.5.4). happens selected Wikipedia concepts
used augment text representation, second-order approach selectively adds
highly related concepts identified analyzing Wikipedia links.

4. Using Explicit Semantic Analysis Text Categorization
section evaluate benefits using external knowledge text categorization.10
4.1 Background Text Categorization
Text categorization (TC) deals assigning category labels natural language documents. Categories come fixed set labels (possibly organized hierarchy)
document may assigned one categories. Text categorization systems
useful wide variety tasks, routing news e-mail appropriate corporate
desks, identifying junk email, correctly handling intelligence reports.
majority existing text classification systems represent text bag words,
use variant vector space model various weighting schemes (Salton & McGill,
1983). Thus, features commonly used text classification weighted occurrence
frequencies individual words. State-of-the-art systems text categorization use variety
induction techniques, support vector machines, k-nearest neighbor algorithm,
neural networks. bag words (BOW) method effective easy medium
difficulty categorization tasks category document identified several
easily distinguishable keywords. However, performance becomes quite limited
demanding tasks, dealing small categories short documents.
various attempts extend basic BOW approach. Several studies
augmented bag words n-grams (Caropreso, Matwin, & Sebastiani, 2001; Peng
& Shuurmans, 2003; Mladenic, 1998; Raskutti, Ferra, & Kowalczyk, 2001) statistical
language models (Peng, Schuurmans, & Wang, 2004). Others used linguistically motivated
features based syntactic information, available part-of-speech tagging
shallow parsing (Sable, McKeown, & Church, 2002; Basili, Moschitti, & Pazienza, 2000).
Additional studies researched use word clustering (Baker & McCallum, 1998; Bekker10. Preliminary results research reported Gabrilovich Markovitch (2006).

458

fiWikipedia-based Semantic Interpretation

man, 2003; Dhillon, Mallela, & Kumar, 2003), neural networks (Jo, 2000; Jo & Japkowicz,
2005; Jo, 2006), well dimensionality reduction techniques LSA (Deerwester
et al., 1990; Hull, 1994; Zelikovitz & Hirsh, 2001; Cai & Hofmann, 2003). However,
attempts mostly limited success.
believe bag words approach inherently limited, use
pieces information explicitly mentioned documents,
vocabulary consistently used throughout. BOW approach cannot generalize
words, consequently words testing document never appeared training
set necessarily ignored. synonymous words appear infrequently training
documents used infer general principle covers cases. Furthermore,
considering words unordered bag makes difficult correctly resolve sense
polysemous words, longer processed native context.
shortcomings stem fact bag words method access wealth
world knowledge possessed humans, therefore easily puzzled facts terms
cannot easily deduced training set.
4.2 Using ESA Feature Generation
propose solution augments bag words knowledge-based features.
Given document classified, would use ESA represent document
text space Wikipedia concepts. However, text categorization crucially different
computing semantic relatedness (cf. Section 3) two important respects.
First, computing semantic relatedness essentially one-off task, is, given
particular pair text fragments, need quantify relatedness prior
examples specific task. cases, words text fragments likely
marginal usefulness, especially two fragments one word long.
happens data available us limited two input fragments,
cases share words, all.
hand, supervised text categorization, one usually given collection
labeled text documents, one induce text categorizer. Consequently,
words occur training examples serve valuable featuresthis
bag words approach born. observed earlier work (Gabrilovich
& Markovitch, 2005, 2007b), ill-advised completely replace bag words
generated concepts, instead advantageous enrich bag words. Rather,
opt augment bag words carefully selected knowledge concepts, become
new features document. refer process feature generation,
actually construct new document features beyond bag words.
Second, enriching document representation text categorization possible
Wikipedia concepts extremely expensive computationally, machine learning
classifier learned augmented feature space. representation obviously
takes lot storage space, cannot processed efficiently multitude
concepts involved (whose number easily reach hundreds thousands). Therefore,
text categorization task, prune interpretation vectors retain number
highest-scoring concepts input text fragment.

459

fiGabrilovich & Markovitch

Using multi-resolution approach feature generation believe considering document single unit often misleading: text might diverse
readily mapped right set concepts, notions mentioned briefly may
overlooked. Instead, partition document series non-overlapping segments
(called contexts), generate features finer level. context mapped
number Wikipedia concepts knowledge base, pooling concepts
together describe entire document results multi-faceted classification. way,
resulting set concepts represents various aspects sub-topics covered
document.
Potential candidates contexts simple sequences words, linguistically motivated chunks sentences paragraphs. optimal resolution document segmentation determined automatically using validation set. earlier work (Gabrilovich & Markovitch, 2005, 2007b), proposed principled multiresolution approach simultaneously partitions document several levels linguistic abstraction (windows words, sentences, paragraphs, taking entire document
one big chunk), performs feature generation levels. rely
subsequent feature selection step eliminate extraneous features, preserving
genuinely characterize document.
essential emphasize using multi-resolution approach makes sense
interpretation vectors pruned retain number highest-scoring concepts context. explained above, exactly case text categorization.
Without pruning, producing interpretation vectors context summing
would equivalent simply multiplying weight concept constant
factor. order explain situation different presence pruning, let us
consider example. Suppose long document mentions particular
topic last paragraph. Since topic central document, N topscoring concepts documents interpretation vector unlikely cover topic.
Although likely covered concepts I, concepts lower weight
going pruned. However, produce interpretation vectors
paragraph document, retain N highest-scoring concepts each,
concepts generated last paragraph cover . Consequently, representation joined set concepts generated document. many text categorization
tasks, documents labeled particular topic even mention topic briefly,
hence generating features describing topics important.
Feature generation Feature generation performed prior text categorization.
document transformed series local contexts, represented
interpretation vectors using ESA. top ten concepts vectors pooled together,
give rise generated features document, added bag words.
Since concepts approach correspond Wikipedia articles, constructed features
correspond articles. Thus, set features generated document viewed
representing set Wikipedia articles relevant document contents.
constructed features used conjunction original bag words.
resulting set optionally undergoes feature selection, discriminative features
retained document representation.
460

fiWikipedia-based Semantic Interpretation

Basic
features

Feature
selection

Selected
features

Labeled
documents

Feature
valuation

Induction
algorithm

Classifier

Classifier

Classified
documents

Labeled
feature
vectors

Training

Testing
Testing
documents

Feature
valuation

Figure 2: Standard approach text categorization.

Feature generation
Feature
construction

Feature
selection

Generated
features

Wikipedia
Labeled
documents

Feature
valuation

Induction
algorithm

Classifier

Labeled
feature
vectors

Figure 3: Induction text classifiers using proposed framework feature generation.

Figure 2 depicts standard approach text categorization. Figure 3 outlines
proposed feature generation framework; observe Feature generation box replaces
Feature selection box framed bold Figure 2.
essential note use encyclopedia simply increase amount
training data text categorization; neither use text corpus collect
word co-occurrence statistics. Rather, use knowledge distilled encyclopedia
enrich representation documents, text categorizer induced
augmented, knowledge-rich feature space.
461

fiGabrilovich & Markovitch

4.3 Test Collections
section gives brief description test collections used evaluate methodology. provide much detailed description test collections Appendix B.
1. Reuters-21578 (Reuters, 1997) historically often used dataset text categorization research. Following common practice, used ModApte split (9603 training,
3299 testing documents) two category sets, 10 largest categories 90 categories
least one training testing example.
2. 20 Newsgroups (20NG) (Lang, 1995) well-balanced dataset 20 categories
containing 1000 documents each.
3. Movie Reviews (Movies) (Pang, Lee, & Vaithyanathan, 2002) defines sentiment
classification task, reviews express either positive negative opinion
movies. dataset 1400 documents two categories (positive/negative)
4. Reuters Corpus Volume (RCV1) (Lewis, Yang, Rose, & Li, 2004)
800,000 documents. speed experiments, used subset RCV1 17,808 training documents (dated 2027/08/96) 5,341 testing ones (2831/08/96). Following
Brank, Grobelnik, Milic-Frayling, Mladenic (2002), used 16 Topic 16 Industry
categories constitute representative samples full groups 103 354 categories,
respectively. randomly sampled Topic Industry categories 5 sets
10 categories each.11
5. OHSUMED (Hersh, Buckley, Leone, & Hickam, 1994) subset MEDLINE,
contains 348,566 medical documents. document contains title, two-thirds
(233,445) contain abstract. document labeled average 13 MeSH12
categories (out total 14,000). Following Joachims (1998), used subset documents
1991 abstracts, taking first 10,000 documents training next
10,000 testing. limit number categories experiments, randomly
generated 5 sets 10 categories each.13
Using 5 datasets allows us comprehensively evaluate performance
approach. Specifically, comparing 20 Newsgroups two Reuters datasets (Reuters21578 Reuters Corpus Volume 1), observe former substantially
noisy since data obtained Usenet newsgroups, Reuters datasets
significantly cleaner. Movie Reviews collection presents example sentiment
classification, different standard (topical) text categorization. Finally,
OHSUMED dataset presents example comprehensive taxonomy 14,000
categories. explain next section, used dataset create collection
labeled short texts, allowed us quantify performance method
texts.
Short Documents derived several datasets short documents test
collections described above. Recall one-third OHSUMED documents
titles abstract, therefore considered short documents as-is. used
range documents defined above, considered without abstracts;
yielded 4,714 training 5,404 testing documents. datasets, created
11. full definition category sets used available Table 8 (see Section B.4).
12. http://www.nlm.nih.gov/mesh
13. full definition category sets used available Table 9 (see Section B.5).

462

fiWikipedia-based Semantic Interpretation

short document original document taking title latter (with
exception Movie Reviews, documents titles).
noted, however, substituting title full document poor
mans way obtain collection classified short documents. documents first
labeled categories, human labeller saw document entirety. particular,
category might assigned document basis facts mentioned
body, even though information may well missing (short) title. Thus, taking
categories original documents genuine categories title often
misleading. However, know publicly available test collections short
documents, decided construct datasets explained above. Importantly, OHSUMED
documents without abstracts classified humans; working
OHSUMED-derived dataset thus considered pure experiment.
4.4 Experimentation Procedure
used support vector machines14 learning algorithm build text categorizers, since
prior studies found SVMs best performance text categorization (Sebastiani,
2002; Dumais, Platt, Heckerman, & Sahami, 1998; Yang & Liu, 1999). Following established
practice, use precision-recall break-even point (BEP) measure text categorization
performance. BEP defined terms standard measures precision recall,
precision proportion true document-category assignments among assignments predicted classifier, recall proportion true document-category
assignments predicted classifier. obtained either tuning
classifier precision equal recall, sampling several (precision, recall) points
bracket expected BEP value interpolating (or extrapolating, event
sampled points lie side).
two Reuters datasets OHSUMED report micro- macro-averaged
BEP, since categories differ size significantly. Micro-averaged BEP operates
document level primarily affected categorization performance larger categories.
hand, macro-averaged BEP averages results individual categories, thus
small categories training examples large impact overall performance.
Reuters datasets (Reuters-21578 RCV1) OHSUMED used fixed
train/test split defined Section 4.3, consequently used macro sign test (S-test)
(Yang & Liu, 1999) assess statistical significance differences classifier performance. 20NG Movies performed 4-fold cross-validation, used paired t-test
assess significance. used non-parametric Wilcoxon signed-ranks test (Demsar, 2006) compare baseline FG-based classifiers multiple data sets.
latter case, individual measurements taken (micro- macro-averaged) BEP
values observed dataset.
14. used SVM light implementation (Joachims, 1999) default parameters. earlier
work feature selection (Gabrilovich & Markovitch, 2004), conducted thorough experimentation
wide range values C parameter, found major importance
datasets; consequently, leave parameter default setting well.

463

fiGabrilovich & Markovitch

4.4.1 Text Categorization Infrastructure
conducted experiments using text categorization platform design
development named Hogwarts 15 (Davidov, Gabrilovich, & Markovitch, 2004). opted
build comprehensive new infrastructure text categorization, surprisingly software tools publicly available researchers, available allow
limited control operation. Hogwarts facilitates full-cycle text categorization
including text preprocessing, feature extraction, construction, selection weighting, followed actual classification cross-validation experiments. system currently
provides XML parsing, part-of-speech tagging (Brill, 1995), sentence boundary detection,
stemming (Porter, 1980), WordNet (Fellbaum, 1998) lookup, variety feature selection
algorithms, TFIDF feature weighting schemes. Hogwarts 250 configurable
parameters control modus operandi minute detail. Hogwarts interfaces
SVM, KNN C4.5 text categorization algorithms, computes standard measures
categorization performance. Hogwarts designed particular emphasis
processing efficiency, portably implemented ANSI C++ programming language
C++ Standard Template Library. system built-in loaders Reuters-21578
(Reuters, 1997), RCV1 (Lewis et al., 2004), 20 Newsgroups (Lang, 1995), Movie Reviews
(Pang et al., 2002), OHSUMED (Hersh et al., 1994), additional datasets
easily integrated modular way.
document undergoes following processing steps. Document text first tokenized, title words replicated twice emphasize importance. Then, stop
words, numbers mixed alphanumeric strings removed, remaining words
stemmed. bag words next merged set features generated
document analyzing contexts explained Section 4.2, rare features occurring
fewer 3 documents removed.
Since earlier studies found BOW features indeed useful SVM text
categorization16 (Joachims, 1998; Rogati & Yang, 2002; Brank et al., 2002; Bekkerman,
2003; Leopold & Kindermann, 2002; Lewis et al., 2004), take bag words
entirety (with exception rare features removed previous step). generated
features, however, undergo feature selection using information gain criterion.17 Finally,
feature weighting performed using ltc TF.IDF function (logarithmic term frequency
inverse document frequency, followed cosine normalization) (Salton & Buckley, 1988;
Debole & Sebastiani, 2003).
4.4.2 Baseline Performance Hogwarts
demonstrate performance basic text categorization implementation (column Baseline Table 4) consistent state art reflected
published studies (all using SVM). Reuters-21578, Dumais et al. (1998) achieved
15. Hogwarts School Witchcraft Wizardry educational institution attended Harry Potter
(Rowling, 1997).
16. Gabrilovich Markovitch (2004) described class problems feature selection bag
words actually improves SVM performance.
17. course, feature selection performed using training set documents.

464

fiWikipedia-based Semantic Interpretation

micro-BEP 0.920 10 categories 0.870 categories. 20NG18 , Bekkerman
(2003) obtained BEP 0.856. Pang et al. (2002) obtained accuracy 0.829 Movies19 .
minor variations performance due differences data preprocessing
different systems; example, Movies dataset worked raw HTML files
rather official tokenized version, order recover sentence paragraph
structure contextual analysis. RCV1 OHSUMED, direct comparison published results difficult limited category sets date span
documents speed experimentation.
4.4.3 Using Feature Generator
core engine Explicit Semantic Analysis implemented explained Section 3.2.
used multi-resolution approach feature generation, classifying document contexts level individual words, complete sentences, paragraphs, finally entire
document.20 context, features generated 10 best-matching concepts
produced feature generator.
4.5 Wikipedia-based Feature Generation
section, report results experimental evaluation methodology.
4.5.1 Qualitative Analysis Feature Generation
study process feature generation number actual examples.
Feature Generation per se illustrate approach, show features generated
several text fragments. Whenever applicable, provide short explanations generated
concepts; cases, explanations taken Wikipedia (Wikipedia, 2006).
Text: Wal-Mart supply chain goes real time
Top 10 generated features: (1) Wal-Mart; (2) Sam Walton; (3) Sears Holdings
Corporation; (4) Target Corporation; (5) Albertsons; (6) ASDA; (7) RFID; (8)
Hypermarket; (9) United Food Commercial Workers; (10) Chain store
Selected explanations: (2) Wal-Mart founder; (5) prominent competitors WalMart; (6) Wal-Mart subsidiary UK; (7) Radio Frequency Identification,
technology Wal-Mart uses extensively manage stock; (8) superstore
(a general concept, Wal-Mart specific example); (9) labor union
18. comparison results reported Bekkerman (2003) administered single test run (i.e.,
without cross-validation), taking first 3/4 postings newsgroup training, rest
testing.
19. comparison results reported Pang et al. (2002) administered single test run (i.e.,
without cross-validation), taking first 2/3 data opinion type training, rest
testing.
20. 20NG dataset exception, owing high level intrinsic noise renders identification
sentence boundaries extremely unreliable, causes word-level feature generation produce
many spurious classifications. Consequently, dataset restrict multi-resolution approach
individual paragraphs entire document only.

465

fiGabrilovich & Markovitch

trying organize Wal-Marts workers; (10) general concept,
Wal-Mart specific example
particularly interesting juxtapose features generated fragments
contain ambiguous words. end, show features generated two phrases
contain word bank two different senses, Bank America (financial
institution) Bank Amazon (river bank). readily seen, feature generation methodology capable performing word sense disambiguation
considering ambiguous words context neighbors.
Text: Bank America
Top 10 generated features: (1) Bank; (2) Bank America; (3) Bank
America Plaza (Atlanta); (4) Bank America Plaza (Dallas); (5) MBNA
(a bank holding company acquired Bank America); (6) VISA (credit
card); (7) Bank America Tower, New York City; (8) NASDAQ; (9) MasterCard; (10) Bank America corporate Center
Text: Bank Amazon
Top 10 generated features: (1) Amazon River; (2) Amazon Basin; (3) Amazon Rainforest; (4) Amazon.com; (5) Rainforest; (6) Atlantic Ocean; (7)
Brazil; (8) Loreto Region (a region Peru, located Amazon Rainforest);
(9) River; (10) Economy Brazil
method, however, 100% accurate, cases generates features
somewhat relevant even irrelevant input text. example, show outcome feature generation title earlier article
(Gabrilovich & Markovitch, 2006). concept, show list input words
triggered (the words stemmed sorted decreasing order
contribution).
Text: Overcoming Brittleness Bottleneck using Wikipedia: Enhancing Text Categorization Encyclopedic Knowledge
Top 10 generated features:
1. Encyclopedia (encyclopedia, knowledge, Wikipedia, text)
2. Wikipedia (Wikipedia, enhance, encyclopedia, text)
3. Enterprise content management (category, knowledge, text, overcome, enhance)
4. Performance problem (bottleneck, category, enhance)
5. Immanuel Kant (category, knowledge, overcome)
6. Tooth enamel (brittleness, text, enhance)
7. Lucid dreaming (enhance, text, knowledge, category)
8. Bottleneck (bottleneck)
9. Java programming language (category, bottleneck, enhance)
466

fiWikipedia-based Semantic Interpretation

10. Transmission Control Protocol (category, enhance, overcome)
generated features clearly relevant input, Encyclopedia,
Wikipedia, Enterprise content management. Others, however, spurious,
Tooth enamel Transmission Control Protocol. Since process
feature generation relies bag words matching concepts input text,
suffers BOW shortcomings mentioned (Section 4.1). Consequently,
features generated corresponding Wikipedia articles happen
share words input text, even though words characteristic
article whole. explained above, method successfully operate
presence extraneous features due use feature selection.
way, generated features informative predicting document categories
filtered out, informative features actually retained learning
classification model.
Using Inter-article Links Generating Additional Features Section 1,
presented algorithm generates additional features using inter-article links relations concepts. follows, show series text fragments,
fragment show (a) features generated regular FG algorithm, (b) features
generated using Wikipedia links, (c) general features generated using links.
see examples, features constructed using links often relevant
input text.
Text: Google search
Regular feature generation: (1) Search engine; (2) Google Video; (3) Google;
(4) Google (search); (5) Google Maps; (6) Google Desktop; (7) Google (verb);
(8) Google News; (9) Search engine optimization; (10) Spamdexing (search engine
spamming)
Features generated using links: (1) PageRank; (2) AdWords; (3) AdSense; (4)
Gmail; (5) Google Platform; (6) Website; (7) Sergey Brin; (8) Google bomb; (9)
MSN Search; (10) Nigritude ultramarine (a meaningless phrase used search
engine optimization contest 2004)
general features only: (1) Website; (2) Mozilla Firefox; (3) Portable
Document Format; (4) Algorithm; (5) World Wide Web
Text: programming tools
Regular feature generation: (1) Tool; (2) Programming tool; (3) Computer
software; (4) Integrated development environment; (5) Computer-aided software engineering; (6) Macromedia Flash; (7) Borland; (8) Game programmer;
(9) C programming language; (10) Performance analysis
Features generated using links: (1) Compiler; (2) Debugger; (3) Source code;
(4) Software engineering; (5) Microsoft; (6) Revision control; (7) Scripting
language; (8) GNU; (9) Make; (10) Linux
general features only: (1) Microsoft; (2) Software engineering; (3)
Linux; (4) Compiler; (5) GNU
467

fiGabrilovich & Markovitch

4.5.2 Effect Feature Generation
Table 4 shows results using Wikipedia-based feature generation, significant
improvements (p < 0.05) shown bold. different rows table correspond
performance different datasets subsets, defined Section 4.3.
consistently observed larger improvements macro-averaged BEP, dominated
categorization effectiveness small categories. goes line expectations
contribution encyclopedic knowledge especially prominent categories training examples. Categorization performance improved virtually
datasets, notable improvements 30.4% RCV1 18% OHSUMED.
Using Wilcoxon test, found Wikipedia-based classifier significantly superior baseline p < 105 micro- macro-averaged cases. results
clearly demonstrate advantage knowledge-based feature generation.
prior work (Gabrilovich & Markovitch, 2005, 2007b), performed
feature generation text categorization using alternative source knowledge, namely,
Open Directory Project (ODP). results using Wikipedia competitive
using ODP, slight advantage Wikipedia. Observe Wikipedia
constantly updated numerous volunteers around globe, ODP virtually
frozen nowadays. Hence, future expect obtain improvements
using newer versions Wikipedia.
Effect Knowledge Breadth examined effect performing feature
generation using newer Wikipedia snapshot, explained Section 3.2.2. Appendix
reports results experiment, show small consistent improvement due
using larger knowledge base.
4.5.3 Classifying Short Documents
conjectured Wikipedia-based feature generation particularly useful
classifying short documents.
Table 5 presents results evaluation datasets defined Section 4.3.
majority cases, feature generation yielded greater improvement short documents regular documents. Notably, improvements particularly high
OHSUMED, pure experimentation short documents possible (see Section 4.3).
According Wilcoxon test, Wikipedia-based classifier significantly superior
baseline p < 2 106 . findings confirm hypothesis encyclopedic knowledge particularly useful categorizing short documents,
inadequately represented standard bag words.
4.5.4 Using Inter-article links Concept Relations
Using inter-article links generating additional features, observed improvements text categorization performance short documents. see Table 6,
absolute majority cases using links generate general features
superior strategy. explain Section 2.3, inter-article links viewed relations
concepts represented articles. Consequently, using links allows us
468

fiWikipedia-based Semantic Interpretation

Dataset

Baseline
micro macro
BEP BEP
Reuters-21578 (10 cat.) 0.925 0.874
Reuters-21578 (90 cat.) 0.877 0.602
RCV1 Industry-16
0.642 0.595
RCV1 Industry-10A
0.421 0.335
RCV1 Industry-10B
0.489 0.528
RCV1 Industry-10C
0.443 0.414
RCV1 Industry-10D
0.587 0.466
RCV1 Industry-10E
0.648 0.605
RCV1 Topic-16
0.836 0.591
RCV1 Topic-10A
0.796 0.587
RCV1 Topic-10B
0.716 0.618
RCV1 Topic-10C
0.687 0.604
RCV1 Topic-10D
0.829 0.673
RCV1 Topic-10E
0.758 0.742
OHSUMED-10A
0.518 0.417
OHSUMED-10B
0.656 0.500
OHSUMED-10C
0.539 0.505
OHSUMED-10D
0.683 0.515
OHSUMED-10E
0.442 0.542
20NG
0.854
Movies
0.813

Wikipedia
micro macro
BEP BEP
0.932 0.887
0.883 0.603
0.645 0.617
0.448 0.437
0.523 0.566
0.468 0.431
0.595 0.459
0.641 0.612
0.843 0.661
0.798 0.682
0.723 0.656
0.699 0.618
0.839 0.688
0.765 0.755
0.538 0.492
0.667 0.534
0.545 0.522
0.692 0.546
0.462 0.575
0.862
0.842

Improvement
micro macro
BEP
BEP
+0.8% +1.5%
+0.7% +0.2%
+0.5% +3.7%
+6.4% +30.4%
+7.0% +7.2%
+5.6% +4.1%
+1.4% -1.5%
-1.1% +1.2%
+0.8% +11.8%
+0.3% +16.2%
+1.0% +6.1%
+1.7% +2.3%
+1.2% +2.2%
+0.9% +1.8%
+3.9% +18.0%
+1.7% +6.8%
+1.1% +3.4%
+1.3% +6.0%
+4.5% +6.1%
+1.0%
+3.6%

Table 4: effect feature generation long documents

469

fiGabrilovich & Markovitch

Dataset

Baseline
micro macro
BEP BEP
Reuters-21578 (10 cat.) 0.868 0.774
Reuters-21578 (90 cat.) 0.793 0.479
RCV1 Industry-16
0.454 0.400
RCV1 Industry-10A
0.249 0.199
RCV1 Industry-10B
0.273 0.292
RCV1 Industry-10C
0.209 0.199
RCV1 Industry-10D
0.408 0.361
RCV1 Industry-10E
0.450 0.410
RCV1 Topic-16
0.763 0.529
RCV1 Topic-10A
0.718 0.507
RCV1 Topic-10B
0.647 0.560
RCV1 Topic-10C
0.551 0.471
RCV1 Topic-10D
0.729 0.535
RCV1 Topic-10E
0.643 0.636
OHSUMED-10A
0.302 0.221
OHSUMED-10B
0.306 0.187
OHSUMED-10C
0.441 0.296
OHSUMED-10D
0.441 0.356
OHSUMED-10E
0.164 0.206
20NG
0.699

Wikipedia
micro macro
BEP BEP
0.877 0.793
0.803 0.506
0.481 0.437
0.293 0.256
0.337 0.363
0.294 0.327
0.452 0.379
0.474 0.434
0.769 0.542
0.725 0.544
0.643 0.564
0.573 0.507
0.735 0.563
0.670 0.653
0.405 0.299
0.383 0.256
0.528 0.413
0.460 0.402
0.219 0.280
0.749

Improvement
micro
macro
BEP
BEP
+1.0%
+2.5%
+1.3% +5.6%
+5.9% +9.2%
+17.7% +28.6%
+23.4% +24.3%
+40.7% +64.3%
+10.8% +5.0%
+5.3% +5.9%
+0.8%
+2.5%
+1.0% +7.3%
-0.6%
+0.7%
+4.0% +7.6%
+0.8% +5.2%
+4.2% +2.7%
+34.1% +35.3%
+25.2% +36.9%
+19.7% +39.5%
+4.3% +12.9%
+33.5% +35.9%
+7.1%

Table 5: Feature generation short documents

470

fiWikipedia-based Semantic Interpretation

Dataset

Baseline

micro
BEP
Reuters-21578 (10 cat.) 0.868
Reuters-21578 (90 cat.) 0.793
RCV1 Industry-16
0.454
RCV1 Topic-16
0.763
20NG
0.699
Dataset
Reuters-21578 (10 cat.)
Reuters-21578 (90 cat.)
RCV1 Industry-16
RCV1 Topic-16
20NG







macro
BEP
0.774
0.479
0.400
0.529






Wikipedia

Wikipedia
+ links

micro macro
BEP BEP
0.877 0.793
0.803 0.506
0.481 0.437
0.769 0.542
0.749
Improvement
baseline
+1.0% +2.5%
+1.3% +5.6%
+5.9% +9.2%
+0.8% +2.5%
+7.1%

micro macro
BEP BEP
0.878 0.796
0.804 0.506
0.486 0.445
0.769 0.539
0.753
Improvement
baseline
+1.2% +2.8%
+1.4% +5.6%
+7.1% +11.3%
+0.8% +1.9%
+7.7%

Wikipedia
+ links
(more general
features only)
micro macro
BEP
BEP
0.880 0.801
0.809 0.507
0.488 0.444
0.775 0.545
0.756
Improvement
baseline
+1.4% +3.5%
+2.0% +5.8%
+7.5% +11.0%
+1.6% +3.0%
+8.1%

Table 6: Feature generation short documents using inter-article links
identify additional concepts related context analyzed, leads better
representation context additional relevant generated features.

5. Related Work
section puts methodology context related prior work.
past, number attempts represent meaning natural
language texts. Early research computational linguistics focused deep natural language
understanding, strived represent text semantics using logical formulae (Montague,
1973). However, task proved difficult little progress made
develop comprehensive grammars non-trivial fragments language. Consequently,
mainstream research effectively switched statistically-based methods (Manning
& Schuetze, 2000).
Although studies tried explicitly define semantic representation,
modus operandi frequently induces particular representation system. Distributional similarity methods (Lee, 1999) compute similarity pair words w1 w2 comparing
distributions words given two, e.g., comparing vectors probabilities P (v|w1 ) P (v|w2 ) large vocabulary V words (v V ). Therefore,
techniques seen representing meaning word w vector conditional
probabilities words given w. Dagan, Marcus, Markovitch (1995) refined
technique considering co-occurrence probabilities word left right contextual neighbors. example, word water would represented vector
left neighbors drink, pour, clean, vector right neighbors
molecule, level, surface. Lin (1998a) represented word meaning considering syntactic roles words co-occur sentence. example,
471

fiGabrilovich & Markovitch

semantics word water would represented vector triples (water,
obj-of, drink) (water, adj-mod, clean). Qiu Frei (1993) proposed method
concept-based query expansion; however, expanded queries additional words
rather features corresponding semantic concepts.
Latent Semantic Analysis probably similar method prior research,
explicitly represents meaning text fragment. LSA manipulating
vector so-called latent concepts, obtained SVD decomposition
word-by-document matrix training corpus. CYC (Lenat, 1995; Lenat, Guha, Pittman,
Pratt, & Shepherd, 1990) represents semantics words elaborate network
interconnected richly-annotated concepts.
contrast, method represents meaning piece text weighted vector
knowledge concepts. Importantly, entries vector correspond unambiguous
human-defined concepts rather plain words, often ambiguous. Compared
LSA, approach benefits large amounts manually encoded human knowledge,
opposed defining concepts using statistical analysis training corpus. Compared
CYC, approach streamlines process semantic interpretation depend
manual encoding inference rules. exception LSA, prior approaches
semantic interpretation explicitly represent semantics individual words, require
extra level sophistication represent longer texts. Conversely, approach represents
meaning texts uniform way regardless length.
5.1 Semantic Similarity Semantic Relatedness
study deal semantic relatedness rather semantic similarity
semantic distance, often used literature. extensive survey
relatedness measures, Budanitsky Hirst (2006) argued notion relatedness
general similarity, former subsumes many different kind specific
relations, including meronymy, antonymy, functional association, others.
maintained computational linguistics applications often require measures relatedness
rather narrowly defined measures similarity. example, word sense
disambiguation use related words context, merely similar words.
Budanitsky Hirst (2006) argued notion semantic distance might
confusing due different ways used literature.
approach estimating semantic relatedness words somewhat reminiscent
distributional (or co-occurrence) similarity (Lee, 1999; Dagan, Lee, & Pereira, 1999). Indeed, compare meanings words comparing occurrence patterns across
large collection natural language documents. However, compilation documents arbitrary, rather, documents aligned encyclopedia articles,
focused single topic. Furthermore, distributional similarity methods
inherently suitable comparing individual words, method compute
similarity arbitrarily long texts.
Prior work field mostly focused semantic similarity words, using R&G
(Rubenstein & Goodenough, 1965) list 65 word pairs M&C (Miller & Charles, 1991)
list 30 word pairs. similarity relation considered, using lexical resources
often successful enough, reaching Pearsons correlation 0.700.85 human
472

fiWikipedia-based Semantic Interpretation

judgements (Budanitsky & Hirst, 2006; Jarmasz, 2003). case, lexical techniques
even slight edge ESA-Wikipedia, whose correlation human scores 0.723
M&C 0.816 R&G. 21 However, entire language wealth considered
attempt capture general semantic relatedness, lexical techniques yield substantially inferior results (see Table 2). WordNet-based technique, consider
generalization (is-a) relation words, achieve correlation 0.330.35
human judgements (Budanitsky & Hirst, 2006; Jarmasz, 2003). Jarmasz & Szpakowiczs
ELKB system (Jarmasz, 2003) based Rogets Thesaurus (Roget, 1852) achieves higher
correlation 0.55 due use richer set relations.
Studying semantic similarity relatedness words related assessing similarity
relations. example task establish word pairs carpenter:wood
mason:stone relationally similar, words pairs stand relation
(profession:material). State art results relational similarity based Latent
Relational Analysis (Turney, 2006, 2005).
Sahami Heilman (2006) proposed use Web source additional knowledge
measuring similarity short text snippets. end, defined kernel function
sends two snippets queries search engine, compares bags words
two sets returned documents. major limitation technique
applicable short texts, sending long text query search engine likely
return even results all. hand, approach applicable
text fragments arbitrary length. Additional studies explored Web gather
information computing word similarity include (Turney, 2001) (Metzler, Dumais, &
Meek, 2007). main difference works method latter
uses structured representation human knowledge defined Wikipedia concepts.
above-mentioned based techniques inherently limited individual words,
adaptation comparing longer texts requires extra level complexity (Mihalcea
et al., 2006). contrast, method treats words texts essentially
way.
Strube Ponzetto (2006) used Wikipedia computing semantic relatedness.
However, method, called WikiRelate!, radically different ours. Given pair
words w1 w2 , WikiRelate! searches Wikipedia articles, p1 p2 , respectively
contain w1 w2 titles. Semantic relatedness computed based various
distance measures p1 p2 . measures either rely texts
pages, path distances within category hierarchy Wikipedia. approach,
hand, represents word weighted vector Wikipedia concepts. Semantic
relatedness computed comparing two concept vectors.
Thus, differences two approaches are:
1. WikiRelate! process words actually occur titles Wikipedia articles.
ESA requires word appears within text Wikipedia articles.
2. WikiRelate! limited single words ESA compare texts length.
21. WikiRelate! (Strube & Ponzetto, 2006) achieved relatively low scores 0.310.54 domains.

473

fiGabrilovich & Markovitch

3. WikiRelate! represents semantics word either text article
associated it, node category hierarchy. ESA much
structured semantic representation consisting vector Wikipedia concepts.
Indeed, shown Section 3.3, richer representation ESA yields much better
results.
5.2 Feature Generation Text Categorization
date, quite attempts made deviate orthodox bag words
paradigm, usually limited success. particular, representations based phrases
(Lewis, 1992; Dumais et al., 1998; Fuernkranz, Mitchell, & Riloff, 1998), named entities
(Kumaran & Allan, 2004), term clustering (Lewis & Croft, 1990; Bekkerman, 2003)
explored. However, none techniques could possibly overcome problem
underlying various examples reviewed paperlack world knowledge.
Feature generation techniques found useful variety machine learning tasks
(Markovitch & Rosenstein, 2002; Fawcett, 1993; Matheus, 1991). techniques search
new features describe target concept better ones supplied
training instances. number proposed feature generation algorithms (Pagallo & Haussler, 1990; Matheus & Rendell, 1989; Hu & Kibler, 1996; Murphy & Pazzani, 1991; Hirsh
& Japkowicz, 1994) led significant improvements performance range classification tasks. However, even though feature generation established research area
machine learning, works applied text processing (Kudenko & Hirsh,
1998; Mikheev, 1998; Cohen, 2000; Scott, 1998; Scott & Matwin, 1999). contrast
approach, techniques use exogenous knowledge.
prior work (Gabrilovich & Markovitch, 2005, 2007b), assumed external
knowledge available form generalization hierarchy, used Open Directory
Project example. method, however, number drawbacks,
corrected using Wikipedia.
First, requiring knowledge repository define is-a hierarchy limits choice
appropriate repositories. Moreover, hierarchical organization embodies one particular
relation nodes (generalization), numerous relations, relatedness, meronymy/holonymy chronology, ignored. Second, large-scale hierarchies
tend extremely unbalanced, relative size branches disproportionately large small due peculiar views editors. phenomena indeed
common ODP. example, Top/Society branch heavily dominated one
childrenReligion Spirituality; Top/Science branch dominated
Biology child; considerable fraction mass Top/Recreation concentrated
Pets. Finally, learn scope every ODP concept, short textual descriptions
concepts augmented crawling Web sites cataloged ODP. procedure
allowed us accumulate many gigabytes worth textual data, price, texts
obtained Web often quite far formal writing plagued noise.
Crawling typical Web site often brings auxiliary material little
site theme, legal disclaimers, privacy statements, help pages.
paper proposed use world knowledge encoded Wikipedia, arguably largest knowledge repository Web. Compared ODP, Wikipedia
474

fiWikipedia-based Semantic Interpretation

possesses several advantageous properties. First, articles much cleaner typical
Web pages, mostly qualify standard written English. Although Wikipedia offers
several orthogonal browsing interfaces, structure fairly shallow, propose
treat Wikipedia essentially hierarchy. way, mapping tex fragments onto
relevant Wikipedia concepts yields truly multi-faceted classification text, avoids
problem unbalanced hierarchy branches. Moreover, requiring knowledge
repository hierarchically organized, approach suitable new domains,
ontology available. Finally, Wikipedia articles heavily cross-linked, way
reminiscent linking Web. conjectured links encode many interesting relations concepts, constitute important source information
addition article texts. explored using inter-article links Section 4.5.4.
5.2.1 Feature Generation Using Electronic Dictionaries
Several studies performed feature construction using WordNet electronic dictionary
(Fellbaum, 1998) domain-specific dictionaries (Scott, 1998; Scott & Matwin,
1999; Urena-Lopez, Buenaga, & Gomez, 2001; Wang, McKay, Abbass, & Barlow, 2003;
Bloehdorn & Hotho, 2004).
Scott Matwin (1999) attempted augment conventional bag-of-words representation additional features, using symbolic classification system Ripper (Cohen,
1995). study evaluated features based syntactically22 statistically motivated
phrases, well WordNet synsets 23 . latter case, system performed generalizations using hypernym hierarchy WordNet, completely replaced bag words
bag synsets. using hypernyms allowed Ripper produce general
comprehensible rules achieved performance gains small classification tasks, performance benefits could obtained larger tasks, even suffered
degradation classification accuracy. Consistent published findings
(Lewis, 1992; Dumais et al., 1998; Fuernkranz et al., 1998), phrase-based representation
yield significant performance benefits bag-of-words approach.24
Urena-Lopez et al. (2001) used WordNet conjunction Rocchio (Rocchio, 1971)
Widrow-Hoff (Lewis, Schapire, Callan, & Papka, 1996; Widrow & Stearns, 1985, Chapter 6) linear classifiers fine-tune category vectors. Wang et al. (2003) used Medical
Subject Headings (MeSH, 2003) replace bag words canonical medical terms;
Bloehdorn Hotho (2004) used similar approach augment Reuters-21578 documents
WordNet synsets OHSUMED medical documents MeSH terms.
noted, however, WordNet originally designed powerful
knowledge base, rather lexical database suitable peculiar lexicographers
needs. Specifically, WordNet following drawbacks used knowledge base
text categorization:
22. Identification syntactic phrases performed using noun phrase extractor built top part
speech tagger (Brill, 1995).
23. synset WordNet notion sense shared group synonymous words.
24. Sebastiani (2002) casts use bag words versus phrases utilizing lexical semantics rather
compositional semantics. Interestingly, bag-of-words approaches (notably, KNN) may considered
context-sensitive assume independence either features (terms) categories (Yang
& Pedersen, 1997).

475

fiGabrilovich & Markovitch

WordNet fairly small coveragefor test collections used paper,
50% unique words missing WordNet. particular, many proper
names, slang domain-specific technical terms included WordNet,
designed general-purpose dictionary.
Additional information synsets (beyond identity) limited.
WordNet implements differential rather constructive lexical semantics
theory, glosses accompany synsets mainly designed distinguish
synsets rather provide definition sense concept. Usage examples
occasionally constitute part gloss serve purpose. Without
auxiliary information, reliable word sense disambiguation almost impossible.
WordNet designed professional linguists trained recognize minute
differences word senses. result, common words far many distinct
senses useful information retrieval (Mihalcea, 2003); example, word
make many 48 senses verb alone. fine-grained distinctions
synsets present additional difficulty word sense disambiguation.
approach techniques use WordNet manipulate collection
concepts. However, number crucial differences. previous studies
performed feature generation individual words only. approach handle arbitrarily long short text fragments alike. Considering words context allows approach
perform word sense disambiguation. Approaches using WordNet cannot achieve disambiguation information synsets limited merely words,
Wikipedia concepts associated huge amounts text. Even individual words,
approach provides much sophisticated mapping words concepts,
analysis large bodies texts associated concepts. allows us represent
meaning words (or texts) weighted combination concepts, mapping word
WordNet amounts simple lookup, without weights. Furthermore, WordNet
senses word mutually exclusive. approach, concepts reflect different
aspects input, thus yielding weighted multi-faceted representation text.
Appendix illustrate limitations WordNet specific example,
juxtapose WordNet-based Wikipedia-based representation.
5.2.2 Using Unlabeled Examples
best knowledge, exception studies used WordNet,
attempts date automatically use large-scale repositories structured background knowledge feature generation. interesting approach using nonstructured background knowledge proposed Zelikovitz Hirsh (2000). work
uses collection unlabeled examples intermediaries comparing testing examples
training ones. Specifically, unknown test instance appear
resemble labeled training instances, unlabeled examples similar may
used bridges. Using approach, possible handle situation
training test document words common. unlabeled documents
utilized define cosine similarity metric, used KNN algorithm
actual text categorization. approach, however, suffers efficiency problems,
476

fiWikipedia-based Semantic Interpretation

looking intermediaries compare every two documents makes necessary explore
combinatorial search space.
subsequent paper, Zelikovitz Hirsh (2001) proposed alternative way use
unlabeled documents background knowledge. work, unlabeled texts pooled
together training documents compute Latent Semantic Analysis (LSA) (Deerwester et al., 1990) model. LSA analyzes large corpus unlabeled text, automatically
identifies so-called latent concepts using Singular Value Decomposition. resulting
LSA metric facilitates comparison test documents training documents. addition unlabeled documents significantly increases amount data word
co-occurrence statistics estimated, thus providing solution text categorization problems training data particularly scarce. However, subsequent studies found
LSA rarely improve strong baseline established SVM, often even results
performance degradation (Wu & Gunopulos, 2002; Liu, Chen, Zhang, Ma, & Wu, 2004).
contrast LSA, manipulates virtual concepts, methodology relies using
concepts identified described humans.

6. Conclusions
paper proposed Explicit Semantic Analysisa semantic interpretation methodology natural language processing. order render computers knowledge
world, use Wikipedia build semantic interpreter, represents meaning
texts high-dimensional space knowledge-based concepts. concepts correspond Wikipedia articles, methodology provides fully automatic way tap
collective knowledge tens hundreds thousands people. conceptbased representation text contains information cannot deduced input
text alone, consequently supersedes conventional bag words representation.
believe important aspects proposed approach ability
address synonymy polysemy, arguably two important problems
NLP. Thus, two texts discuss topic using different words,
conventional bag words approach able identify commonality.
hand, mere fact two texts contain word necessarily
imply discuss topic, since word could used two texts two
different meanings. believe concept-based representation allows generalizations
refinements partially address synonymy polysemy.
Consider, example, following text fragment (taken Appendix C): group
European-led astronomers made photograph appears planet orbiting
another star. so, would first confirmed picture world beyond solar
system. fifth concept generated fragment Extrasolar planet,
exactly topic text, even though words mentioned input.
generated concepts (e.g., Astronomy Planetary orbit) highly
characteristic astronomy-related texts. additions enrich text representation,
increase chances finding common features texts. essential note
that, course, generated concepts need match features documents.
Even concepts match, gain valuable insights document contents.
477

fiGabrilovich & Markovitch

succeeded make automatic use encyclopedia without deep language understanding, specially crafted inference rules relying additional common-sense knowledge
bases. made possible applying standard text classification techniques match
document texts relevant Wikipedia articles.
Empirical evaluation confirmed value Explicit Semantic Analysis two common tasks natural language processing. Compared previous state art,
using ESA results significant improvements automatically assessing semantic relatedness words texts. Specifically, correlation computed relatedness scores
human judgements increased r = 0.56 0.75 (Spearman) individual words
r = 0.60 0.72 (Pearson) texts. contrast existing methods, ESA offers
uniform way computing relatedness individual words arbitrarily long text
fragments. Using ESA perform feature generation text categorization yielded consistent improvements across diverse range datasets. Recently, performance
best text categorization systems became similar, previous work mostly achieved small
improvements. Using Wikipedia source external knowledge allowed us improve
performance text categorization across diverse collection datasets.
noted although recent study (Giles, 2005) found Wikipedia accuracy rival Encyclopaedia Britannica, arguably Wikipedia articles
equally high quality. one hand, Wikipedia notion featured articles
(http://en.wikipedia.org/wiki/Featured Article), considered
best articles Wikipedia, determined Wikipedias editors. Currently, fewer
0.1% articles achieve status. hand, many articles incomplete (socalled stubs), might even contain information incorrect represent
consensus among editors. Yet cases, Wikipedia content might prone
spamming, despite editorial process attempts review recent changes. believe
method overly susceptible cases, long majority content
correct. Arguably, except outright vandalism, spamming would likely modify
articles contain information related topic article, important
essential majority readers. long newly added content remains
relevant gist article, method likely able correctly determine
input texts article relevant for. However, proper evaluation robustness
method presence imperfect content beyond scope article.
believe research constitutes step towards enriching natural language
processing humans knowledge world. hope Explicit Semantic
Analysis useful NLP tasks beyond computing semantic relatedness
text categorization, intend investigate future work. Recently,
used ESA improve performance conventional information retrieval (Egozi,
Gabrilovich, & Markovitch, 2008). work, augmented queries documents
generated features, documents indexed augmented space words
concepts. Potthast, Stein, Anderka (2008) Sorg Cimiano (2008) adapted
ESA multi-lingual cross-lingual information retrieval.
another recent study, Gurevych et al. (2007) applied methodology computing
word similarity German, information retrieval task searched job
descriptions given users description career interests, found method superior
WordNet-based approach. Importantly, study confirms method
478

fiWikipedia-based Semantic Interpretation

easily adapted languages English, using version Wikipedia
corresponding desired target language.
future work, intend apply ESA word sense disambiguation. Current
approaches word sense disambiguation represent contexts contain ambiguous words
using bag words augmented part-of-speech information. believe representation contexts greatly improved use feature generation map
contexts relevant knowledge concepts. Anecdotal evidence (such examples presented Section 4.5.1) implies method promise improving state art
word sense disambiguation. work capitalized inter-article links Wikipedia
several ways, future work intend investigate elaborate techniques
leveraging high degree cross-linking Wikipedia articles.
Wiki technology underlying Wikipedia project often used nowadays variety open-editing initiatives. include corporate intranets use Wiki primary
documentation tool, well numerous domain-specific encyclopedias topics ranging
mathematics Orthodox Christianity.25 Therefore, believe methodology
used augmenting document representation many specialized domains.
essential note Wikipedia available numerous languages, different
language versions cross-linked level concepts. believe information
leveraged use Wikipedia-based semantic interpretation improving machine
translation.
work proposes methodology Explicit Semantic Analysis using Wikipedia.
However, ESA implemented using repositories human knowledge
satisfy requirements listed Section 2.1. Section 3.3 reported results
building ESA-based semantic interpreter using Open Directory Project (Gabrilovich
& Markovitch, 2005, 2007b). Zesch, Mueller, Gurevych (2008) proposed use Wiktionary computing semantic relatedness. future work, intend implement
ESA using additional knowledge repositories.
Finally, readers interested using Wikipedia work, main software
deliverable described work Wikipedia preprocessor (WikiPrep), available online
part SourceForge open-source project http://wikiprep.sourceforge.net.

Acknowledgments
thank Michael D. Lee Brandon Pincombe making available document similarity data. thank Deepak Agarwal advice assessing statistical significance
results computing semantic relatedness. work partially supported funding
EC-sponsored MUSCLE Network Excellence.
first authors current address Yahoo! Research, 2821 Mission College Blvd, Santa
Clara, CA 95054, USA.

25. See http://en.wikipedia.org/wiki/Category:Online encyclopedias longer list examples.

479

fiGabrilovich & Markovitch

Appendix A. effect knowledge breadth text categorization
appendix, examine effect performing feature generation using newer
Wikipedia snapshot, defined Section 3.2.2. see Table 7, using
larger amount knowledge leads average greater improvements text categorization performance. Although difference performance two versions
admittedly small, consistent across datasets (a similar situation happens assessing
role external knowledge computing semantic relatedness, see Section 3.3.3).

Dataset

Baseline

micro
BEP
Reuters-21578 (10 cat.) 0.925
Reuters-21578 (90 cat.) 0.877
RCV1 Industry-16
0.642
RCV1 Industry-10A 0.421
RCV1 Industry-10B 0.489
RCV1 Industry-10C 0.443
RCV1 Industry-10D 0.587
RCV1 Industry-10E 0.648
RCV1 Topic-16
0.836
RCV1 Topic-10A
0.796
RCV1 Topic-10B
0.716
RCV1 Topic-10C
0.687
RCV1 Topic-10D
0.829
RCV1 Topic-10E
0.758
OHSUMED-10A
0.518
OHSUMED-10B
0.656
OHSUMED-10C
0.539
OHSUMED-10D
0.683
OHSUMED-10E
0.442
20NG
0.854
Movies
0.813
Average

macro
BEP
0.874
0.602
0.595
0.335
0.528
0.414
0.466
0.605
0.591
0.587
0.618
0.604
0.673
0.742
0.417
0.500
0.505
0.515
0.542

Wikipedia
(26/03/06)
micro macro
BEP BEP
0.935 0.891
0.883 0.600
0.648 0.616
0.457 0.450
0.527 0.559
0.458 0.424
0.607 0.448
0.649 0.607
0.842 0.659
0.802 0.689
0.725 0.660
0.697 0.627
0.838 0.687
0.762 0.752
0.545 0.490
0.667 0.529
0.553 0.527
0.694 0.550
0.461 0.588
0.859
0.850

Improvement
(26/03/06)
micro macro
BEP
BEP
+1.1% +1.9%
+0.7% -0.3%
+0.9% +3.5%
+8.6% +34.3%
+7.8% +5.9%
+3.4% +2.4%
+3.4% -3.9%
+0.2% +0.3%
+0.7% +11.5%
+0.8% +17.4%
+1.3% +6.8%
+1.5% +3.8%
+1.1% +2.1%
+0.5% +1.3%
+5.2% +17.5%
+1.7% +5.8%
+2.6% +4.4%
+1.6% +6.8%
+4.3% +8.5%
+0.6%
+4.5%
+2.50% +6.84%

Improvement
(05/11/05)
micro macro
BEP
BEP
+0.8% +1.5%
+0.7% +0.2%
+0.5% +3.7%
+6.4% +30.4%
+7.0% +7.2%
+5.6% +4.1%
+1.4% -1.5%
-1.1% +1.2%
+0.8% +11.8%
+0.3% +16.2%
+1.0% +6.1%
+1.7% +2.3%
+1.2% +2.2%
+0.9% +1.8%
+3.9% +18.0%
+1.7% +6.8%
+1.1% +3.4%
+1.3% +6.0%
+4.5% +6.1%
+1.0%
+3.6%
+2.11% +6.71%

Table 7: effect feature generation using newer Wikipedia snapshot (dated
March 26, 2006)

480

fiWikipedia-based Semantic Interpretation

Appendix B. Test Collections Text Categorization
Appendix provides detailed description test collections used evaluate
knowledge-based feature generation text categorization.
B.1 Reuters-21578
data set contains one year worth English-language stories distributed
Reuters newswire 19861987, arguably often used test collection
text categorization research. Reuters-21578 cleaned version earlier release named
Reuters-22173, contained errors duplicate documents.
collection contains 21578 documents (hence name) SGML format. those,
12902 documents categorized, i.e., assigned category label marked belonging
category. documents explicit classification; is,
reasonably belong categories (judged content), marked so. Several train/test splits collection defined, ModApte (Modified Apte)
commonly used one. ModApte split divides collection chronologically,
allocates first 9603 documents training, rest 3299 documents testing.
documents labeled 118 categories; 016 labels per document,
average 1.04. category distribution extremely skewed: largest category
(earn) 3964 positive examples, 16 categories one positive example.
Several category sets defined collection:
10 largest categories (earn, acq, money-fx, grain, crude, trade, interest, ship, wheat, corn).
90 categories least one document training set one testing set
(Yang, 2001).
Galavotti, Sebastiani, Simi (2000) used set 115 categories least one
training example (three categories, cottonseed, f-cattle sfr training
examples ModApte split).
full set 118 categories least one positive example either training
testing set.
Following common practice, used ModApte split two category sets, 10 largest
categories 90 categories least one training testing example.
B.2 20 Newsgroups (20NG)
20 Newsgroups collection (Lang, 1995) comprised 19997 postings 20 Usenet
newsgroups. documents single label, defined name newsgroup
sent to; 4% documents cross-posted, hence several
labels. newsgroup contains exactly 1000 positive examples, exception
soc.religion.christian contains 997.
categories quite close scope, example, comp.sys.ibm.pc.hardware
comp.sys.mac.hardware, talk.religion.misc soc.religion.christian. document
481

fiGabrilovich & Markovitch

posted single newsgroup may reasonably considered appropriate groups
(the author may simply known similar groups, thus cross-posted
message); naturally poses additional difficulty classification.
noted Internet news postings informal, therefore documents frequently contain non-standard abbreviated words, foreign words, proper
names, well large amount markup characters (used attribution authorship
message separation).
B.3 Movie Reviews
Movie Reviews collection (Pang et al., 2002) presents example sentiment classification, different standard (topical) text categorization. collection
contains 1400 reviews movies, half express positive sentiment (opinion)
movie, half negative. reviews collected rec.arts.movies.reviews
newsgroup, archived Internet Movie Database (IMDB, http://www.imdb.com).
classification problem case determine semantic orientation document, rather relate content one predefined topics. problem
arguably difficult topical text categorization, since notion semantic orientation quite general. saw collection opportunity apply feature generation
techniques new task.
Recent works semantic orientation include (Turney & Littman, 2002; Turney, 2002;
Pang et al., 2002).26 two former studies used unsupervised learning techniques based
latent semantic indexing, estimating semantic distance given document
two reference words represent polar opinions, namely, excellent poor.
latter work used classical TC techniques.
B.4 Reuters Corpus Version 1 (RCV1)
RCV1 newest corpus released Reuters (Lewis et al., 2004; Rose, Stevenson, &
Whitehead, 2002). considerably larger predecessor, contains 800,000
news items, dated August 20, 1996 August 19, 1997. stories labeled
3 category sets, Topics, Industries Regions.
Topics close nature category set old Reuters collection
(Reuters-21578). 103 topic codes, 3.24 categories per document
average. topics organized hierarchy, Hierarchy Policy required category assigned document, ancestors hierarchy
assigned well. result, many 36% Topic assignments
26. field genre classification, attempts establish genre document, somewhat related
sentiment classification. Examples possible genres radio news transcripts classified advertisements. work Dewdney, VanEss-Dykema, MacMillan (2001) cast problem text
categorization, using presentation features addition words. presentation features included
part speech tags verb tenses, well mean variance statistics sentence word length,
punctuation usage, amount whitespace characters. Using support vector machines actual
classification, authors found performance due presentation features alone least
good achieved plain words, combined feature set usually resulted
improvement several percentage points.

482

fiWikipedia-based Semantic Interpretation

due four general categories, CCAT, ECAT, GCAT, MCAT. Consequently, micro-averaged performance scores dominated categories
(Lewis et al., 2004), macro-averaging becomes interest.27 Minimum Code
Policy required document assigned least one Topic one Region
code.
Industries fine-grained Topics, therefore harder classification. categories organized hierarchy, although Hierarchy
Policy partially enforced them. 351,761 documents labeled
Industry codes.
Region codes correspond geographical places, subdivided countries, regional groupings economic groupings. Lewis et al. (2004) argue
Region codes might suitable named entity recognition text categorization.
experiments used Topic Industry categories. Due sheer size
collection, processing categories set would unreasonably long, allowing
conduct experiments. speed experimentation, used subset corpus
17,808 training documents (dated August 2027, 1996) 5341 testing documents
(dated August 2831, 1996). Following scheme introduced Brank et al. (2002),
used 16 Topic 16 Industry categories, constitute representative sample
full groups 103 354 categories, respectively. randomly sampled Topic
Industry categories 5 sets 10 categories each. Table 8 gives full definition
category sets used.
noted Lewis et al. (2004), original RCV1 distribution contains number
errors; particular, documents conform either Minimum Code
Hierarchy Policy, labeled erratic codes. Lewis et al. (2004) proposed procedure
correct errors, defined new version collection, named RCV1-v2 (as
opposed original distribution, referred RCV1-v1 ). experiments
based RCV1-v2.
B.5 OHSUMED
OHSUMED (Hersh et al., 1994) subset MEDLINE database, contains
348,566 references documents published medical journals period 19871991.
reference contains publication title, two-thirds (233,445) contain
abstract. document labeled several MeSH categories (MeSH, 2003).
14,000 distinct categories collection, average 13 categories per
document. OHSUMED frequently used information retrieval text categorization
research.
Following Joachims (1998), used subset documents 1991 abstracts,
taking first 10,000 documents training next 10,000 testing. limit
number categories experiments, randomly generated 5 sets 10 categories
each. Table 9 gives full definition category sets used.
27. micro-averaged scores Topic codes much higher macro-averaged ones, see
Section 4.4.2.

483

fiGabrilovich & Markovitch

Set name
Topic-16
Topic-10A
Topic-10B
Topic-10C
Topic-10D
Topic-10E
Industry-16

Industry-10A
Industry-10B
Industry-10C
Industry-10D
Industry-10E

Categories comprising set
e142, gobit, e132, c313, e121, godd, ghea, e13, c183, m143,
gspo, c13, e21, gpol, m14, c15
e31, c41, c151, c313, c31, m13, ecat, c14, c331, c33
m132, c173, g157, gwea, grel, c152, e311, c21, e211, c16
c34, c13, gtour, c311, g155, gdef, e21, genv, e131, c17
c23, c411, e13, gdis, c12, c181, gpro, c15, g15, c22
c172, e513, e12, ghea, c183, gdip, m143, gcrim, e11, gvio
i81402, i79020, i75000, i25700, i83100, i16100, i1300003, i14000,
i3302021, i8150206, i0100132, i65600, i3302003, i8150103, i3640010,
i9741102
i47500, i5010022, i3302021, i46000, i42400, i45100, i32000, i81401,
i24200, i77002
i25670, i61000, i81403, i34350, i1610109, i65600, i3302020, i25700,
i47510, i9741110
i25800, i41100, i42800, i16000, i24800, i02000, i34430, i36101,
i24300, i83100
i1610107, i97400, i64800, i0100223, i48300, i81502, i34400, i82000,
i42700, i81402
i33020, i82003, i34100, i66500, i1300014, i34531, i16100, i22450,
i22100, i42900

Table 8: Definition RCV1 category sets used experiments

Appendix C. Additional Examples Feature Generation Text
Categorization
Appendix, list number additional feature generation examples.
Text: development T-cell leukaemia following otherwise successful treatment three patients X-linked severe combined immune deficiency (X-SCID)
gene-therapy trials using haematopoietic stem cells led re-evaluation
approach. Using mouse model gene therapy X-SCID, find
corrective therapeutic gene IL2RG act contributor genesis
T-cell lymphomas, one-third animals affected. Gene-therapy trials
X-SCID, based assumption IL2RG minimally oncogenic,
may therefore pose risk patients.
Top 10 generated features: (1) Leukemia; (2) Severe combined immunodeficiency; (3) Cancer; (4) Non-Hodgkin lymphoma; (5) AIDS; (6) ICD-10 Chapter
II: Neoplasms; Chapter III: Diseases blood blood-forming organs,
certain disorders involving immune mechanism; (7) Bone marrow transplant; (8) Immunosuppressive drug; (9) Acute lymphoblastic leukemia; (10) Multiple sclerosis

Selected explanations: (4) particular cancer type; (6) disease code ICD
International Statistical Classification Diseases Related Health Problems
Text: Scientific methods biology
484

fiWikipedia-based Semantic Interpretation

Set name
OHSUMED-10A

OHSUMED-10B

OHSUMED-10C

OHSUMED-10D

OHSUMED-10E

Categories comprising set
(parentheses contain MeSH identifiers)
B-Lymphocytes (D001402);
Metabolism, Inborn Errors (D008661);
Creatinine (D003404); Hypersensitivity (D006967);
Bone Diseases, Metabolic (D001851); Fungi (D005658);
New England (D009511); Biliary Tract (D001659);
Forecasting (D005544); Radiation (D011827)
Thymus Gland (D013950); Insurance (D007341);
Historical Geographic Locations (D017516);
Leukocytes (D007962); Hemodynamics (D006439);
Depression (D003863); Clinical Competence (D002983);
Anti-Inflammatory Agents, Non-Steroidal (D000894);
Cytophotometry (D003592); Hydroxy Acids (D006880)
Endothelium, Vascular (D004730);
Contraceptives, Oral, Hormonal (D003278);
Acquired Immunodeficiency Syndrome (D000163);
Gram-Positive Bacteria (D006094); Diarrhea (D003967);
Embolism Thrombosis (D016769);
Health Behavior (D015438); Molecular Probes (D015335);
Bone Diseases, Developmental (D001848);
Referral Consultation (D012017)
Antineoplastic Immunosuppressive Agents (D000973);
Receptors, Antigen, T-Cell (D011948);
Government (D006076); Arthritis, Rheumatoid (D001172);
Animal Structures (D000825); Bandages (D001458);
Italy (D007558); Investigative Techniques (D008919);
Physical Sciences (D010811); Anthropology (D000883)
HTLV-BLV Infections (D006800);
Hemoglobinopathies (D006453); Vulvar Diseases (D014845);
Polycyclic Hydrocarbons, Aromatic (D011084);
Age Factors (D000367); Philosophy, Medical (D010686);
Antigens, CD4 (D015704);
Computing Methodologies (D003205);
Islets Langerhans (D007515); Regeneration (D012038)

Table 9: Definition OHSUMED category sets used experiments

485

fiGabrilovich & Markovitch

Top 10 generated features: (1) Biology; (2) Scientific classification; (3) Science; (4) Chemical biology; (5) Binomial nomenclature; (6) Nature (journal);
(7) Social sciences; (8) Philosophy biology; (9) Scientist; (10) History
biology
Selected explanations: (5) formal method naming species biology
Text: quavering voices, parents grandparents killed World
Trade Center read names victims solemn recitation today, marking
third anniversary terror attacks. ceremony one many planned
United States around world honor memory nearly 3,000 victims
9/11.
Top 10 generated features: (1) September 11, 2001 attack memorials services; (2) United Airlines Flight 93; (3) Aftermath September 11, 2001
attacks; (4) World Trade Center; (5) September 11, 2001 attacks; (6) Oklahoma City bombing; (7) World Trade Center bombing; (8) Arlington National
Cemetery; (9) World Trade Center site; (10) Jewish bereavement
Selected explanations: (2) one four flights hijacked September 11, 2001;
(6) terrorist attack Oklahoma City 1995; (8) American military cemetery
Text: U.S. intelligence cannot say conclusively Saddam Hussein weapons
mass destruction, information gap complicating White House efforts
build support attack Saddams Iraqi regime. CIA advised top
administration officials assume Iraq weapons mass destruction.
agency given President Bush smoking gun, according U.S.
intelligence administration officials.
Top 10 generated features: (1) Iraq disarmament crisis; (2) Yellowcake forgery; (3) Senate Report Pre-War Intelligence Iraq; (4) Iraq weapons
mass destruction; (5) Iraq Survey Group; (6) September Dossier; (7) Iraq
war; (8) Scott Ritter; (9) Iraq War Rationale; (10) Operation Desert Fox
Selected explanations: (2) falsified intelligence documents Iraqs alleged
attempt purchase yellowcake uranium; (6) paper Iraqs weapons mass
destruction published UK government 2002; (8) UN weapons inspector
Iraq; (10) US UK joint military campaign Iraq 1998
another example, consider pair contexts contain word jaguar,
first one contains ambiguous word sense car model, second
onein sense animal.
Text: Jaguar car models
Top 10 generated features: (1) Jaguar (car); (2) Jaguar (S-Type); (3)
Jaguar X-type; (4) Jaguar E-Type; (5) Jaguar XJ; (6) Daimler Motor Company; (7) British Leyland Motor Corporation; (8) Luxury vehicles; (9) V8
engine; (10) Jaguar Racing
Top 10 generated features: (2), (3), (4), (5) particular Jaguar car models;
(6) car manufacturing company became part Jaguar 1960; (7)
486

fiWikipedia-based Semantic Interpretation

another vehicle manufacturing company merged Jaguar; (9) internal
combustion engine used Jaguar car models; (10) Formula One team
used Jaguar promote brand name
Text: Jaguar (Panthera onca)
Top 10 generated features: (1) Jaguar; (2) Felidae; (3) Black panther;
(4) Leopard; (5) Puma; (6) Tiger; (7) Panthera hybrid; (8) Cave lion; (9)
American lion; (10) Kinkajou
Top 10 generated features: (2) family include lions, tigers, jaguars,
related feline species; (10) another carnivore mammal
show number examples generating features using inter-article links.
Text: artificial intelligence
Regular feature generation: (1) Artificial intelligence; (2) A.I. (film); (3)
MIT Computer Science Artificial Intelligence Laboratory; (4) Artificial
life; (5) Strong AI; (6) Swarm intelligence; (7) Computer Science; (8) Frame
problem; (9) Cognitive science; (10) Carl Hewitt
Features generated using links: (1) Robot; (2) John McCarthy (computer scientist); (3) Artificial consciousness; (4) Marvin Minsky; (5) Planner programming language; (6) Actor model (a model concurrent computation formulated
Carl Hewitt colleagues); (7) Logic; (8) Scientific Community Metaphor;
(9) Natural language processing; (10) Lisp programming language
general features only: (1) Robot; (2) Massachusetts Institute Technology; (3) Psychology; (4) Consciousness; (5) Lisp programming language
Text: group European-led astronomers made photograph appears
planet orbiting another star. so, would first confirmed picture
world beyond solar system.
Regular feature generation: (1) Planet; (2) Solar system; (3) Astronomy; (4)
Planetary orbit; (5) Extrasolar planet; (6) Pluto; (7) Jupiter; (8) Neptune; (9)
Minor planet; (10) Mars
Features generated using links: (1) Asteroid; (2) Earth; (3) Oort cloud (a
postulated cloud comets); (4) Comet; (5) Sun; (6) Saturn; (7) Moon; (8) Mercury
(planet); (9) Asteroid belt; (10) Orbital period
general features only: (1) Earth; (2) Moon; (3) Asteroid; (4) Sun; (5)
National Aeronautics Space Administration
Text: Nearly 70 percent Americans say careful eat,
even say diet essential good health, according new nationwide health
poll obesity ranked second among biggest health concerns.
Regular feature generation: (1) Veganism; (2) Vegetarianism; (3) Obesity; (4)
Atkins Nutritional Approach; (5) Binge eating disorder; (6) Dick Gregory; (7)
Nutrition; (8) Super Size Me; (9) Health insurance; (10) Eating disorder
487

fiGabrilovich & Markovitch

Selected explanations: (1) philosophy avoiding animal-derived food; (6)
American nutritionist; (7) documentary film individual eats
McDonalds fast food one full month.
Features generated using links: (1) Raw food diet; (2) Diabetes mellitus; (3)
Healthy eating; (4) Body mass index; (5) Omega-3 fatty acid (an important nutritional component); (6) Dieting; (7) Milk; (8) United States28 ; (9) Hypertension;
(10) Egg (food)
general features only: (1) United States; (2) Diabetes mellitus; (3) Cancer; (4) Food; (5) McDonalds

Appendix D. Comparing Knowledge Sources Feature Generation:
Wikipedia versus WordNet
demonstrate shortcomings WordNet source knowledge-based feature generation, juxtapose WordNet-based Wikipedia-based feature generation sample
sentence examined Section 4.5.1 (we repeat Wikipedia context classifications
readers convenience).
used WordNet version 1.6. follows, WordNet synsets denoted curly
braces, noun verb synsets followed immediate hypernym (more general
synset), applicable.
Text: Wal-Mart supply chain goes real time
relevant Wikipedia classifications:
Wal-Mart
Sam Walton Wal-Mart founder
Sears Holdings Corporation, Target Corporation, Albertsons prominent competitors Wal-Mart
RFID Radio Frequency Identification, technology Wal-Mart uses
extensively manage stock
Hypermarket superstore, general concept Wal-Mart specific
example
United Food Commercial Workers labor union trying
organize Wal-Marts workers
WordNet :
{Wal-Mart} { }; (word found WordNet)
{supply} {indefinite quantity}; {supply} {economic process}; {provision,
providing, supply supplying} {activity};
28. classification quite interesting, issue discussed input text fragment characteristic American life style.

488

fiWikipedia-based Semantic Interpretation

{supply provide, render, furnish} {give}; {issue, supply} {distribute};
{provide, supply ply, cater} {give}; {add, append, supply} {state, say,
tell};
{chain, concatenation} {series}; {chain, chemical chain} {unit, building
block}; {chain} {ligament}; {chain} {business, concern, business concern,
business organization}; {chain} {restraint, constraint}; {chain} {linear
unit}; {range, mountain range, range mountains, chain, mountain chain,
chain maountains} {geological formation, geology, formation}; {iron, irons,
chain, chains} {shackle, bond, hamper, trammel, trammels}; {chain, string,
strand} {necklace};
{chain} {fasten, fix, secure};
{go, spell, tour, turn} {shift, work shift, duty period}; {crack, fling, go, pass,
whirl, offer} {attempt, effort, endeavor, endeavour, try}; {go, go game}
{board game};
{travel, move, go, locomote}; {go, proceed, move} {act, move}; (28
verb senses omitted brevity)
{real number, real}; {real (Spanish coin)};
{real, existent}; {real (no less stated)}; {real, true}; {real (not
taken lightly)}; {real, tangible}; {actual, genuine, literal, real}; {real (economics)}; {substantial, real, material}; {real (of property)}; {veridical, real};
{real (founded practical matters)};
{very, really, real, rattling};
{time, clip} {case, instance, example}; {time} {time period, period,
period time, amount time}; {time} {moment, minute, second, instant}; {time} {abstraction}; {clock time, time} {reading, meter reading}; {fourth dimension, time} {dimension}; {time} {experience}; {meter,
time} {rhythmicity}; {prison term, sentence, time} {term};
{clock, time} {quantify, measure}; {time} {schedule}; {time}
{determine, shape, influence, regulate}; {time} {adjust, set};
Evidently, WordNet classifications overly general diverse context words
cannot properly disambiguated. Furthermore, owing lack proper names, WordNet
cannot possibly provide wealth information encoded Wikipedia, easily overcomes drawbacks WordNet. methodology proposed suffer
shortcomings.

489

fiGabrilovich & Markovitch

References
Adafre, S. F., & de Rijke, M. (2005). Discovering missing links Wikipedia. Proceedings
Workshop Link Discovery: Issues, Approaches Applications (LinkKDD2005), pp. 9097.
Baeza-Yates, R., & Ribeiro-Neto, B. (1999). Modern Information Retrieval. Addison Wesley,
New York, NY.
Baker, D., & McCallum, A. K. (1998). Distributional clustering words text classification. Croft, B., Moffat, A., Van Rijsbergen, C. J., Wilkinson, R., & Zobel, J. (Eds.),
Proceedings 21st ACM International Conference Research Development
Information Retrieval, pp. 96103, Melbourne, AU. ACM Press, New York, US.
Basili, R., Moschitti, A., & Pazienza, M. T. (2000). Language-sensitive text classification.
Proceedings RIAO-00, 6th International Conference Recherche dInformation
Assistee par Ordinateur, pp. 331343, Paris, France.
Begelman, G., Keller, P., & Smadja, F. (2006). Automated tag clustering: Improving search
exploration tag space. Proceedings Collaborative Web Tagging
Workshop, conjunction 15th International World Wide Web Conference,
Edinburgh, Scotland.
Bekkerman, R. (2003). Distributional clustering words text categorization. Masters
thesis, Technion.
Bloehdorn, S., & Hotho, A. (2004). Boosting text classification semantic features.
Proceedings MSW 2004 Workshop 10th ACM SIGKDD Conference
Knowledge Discovery Data Mining, pp. 7087.
Brank, J., Grobelnik, M., Milic-Frayling, N., & Mladenic, D. (2002). Interaction feature
selection methods linear classification models. Workshop Text Learning
held ICML-2002.
Brill, E. (1995). Transformation-based error-driven learning natural language processing: case study part speech tagging. Computational Linguistics, 21 (4),
543565.
Buchanan, B. G., & Feigenbaum, E. (1982). Forward. Davis, R., & Lenat, D. (Eds.),
Knowledge-Based Systems Artificial Intelligence. McGraw-Hill.
Budanitsky, A., & Hirst, G. (2006). Evaluating wordnet-based measures lexical semantic
relatedness. Computational Linguistics, 32 (1), 1347.
Cai, L., & Hofmann, T. (2003). Text categorization boosting automatically extracted
concepts. Proceedings 26th International Conference Research Development Information Retrieval, pp. 182189.
Caropreso, M. F., Matwin, S., & Sebastiani, F. (2001). learner-independent evaluation
usefulness statistical phrases automated text categorization. Chin, A. G.
(Ed.), Text Databases Document Management: Theory Practice, pp. 78102.
Idea Group Publishing, Hershey, US.
490

fiWikipedia-based Semantic Interpretation

Chang, M.-W., Ratinov, L., Roth, D., & Srikumar, V. (2008). Importance semantic
representation: Dataless classification. Proceedings 23rd AAAI Conference
Artificial Intelligence, pp. 830835.
Cohen, W. W. (1995). Fast effective rule induction. Proceedings 12th International
Conference Machine Learning (ICML-95), pp. 115123.
Cohen, W. W. (2000). Automatically extracting features concept learning web.
Proceedings 17th International Conference Machine Learning.
Dagan, I., Lee, L., & Pereira, F. C. N. (1999). Similarity-based models word cooccurrence
probabilities. Machine Learning, 34 (13), 4369.
Dagan, I., Marcus, S., & Markovitch, S. (1995). Contextual word similarity estimation
sparse data. Computer Speech Language, 9 (2), 123152.
Davidov, D., Gabrilovich, E., & Markovitch, S. (2004). Parameterized generation labeled
datasets text categorization based hierarchical directory. Proceedings
27th ACM International Conference Research Development Information
Retrieval, pp. 250257.
Debole, F., & Sebastiani, F. (2003). Supervised term weighting automated text categorization. Proceedings SAC-03, 18th ACM Symposium Applied Computing,
pp. 784788.
Deerwester, S., Dumais, S., Furnas, G., Landauer, T., & Harshman, R. (1990). Indexing
latent semantic analysis. Journal American Society Information Science,
41 (6), 391407.
Demsar, J. (2006). Statistical comparison classifiers multiple data sets. Journal
Machine Learning Research, 7, 130.
Dewdney, N., VanEss-Dykema, C., & MacMillan, R. (2001). form substance:
Classification genres text. Workshop HLT KM held ACL-2001.
Dhillon, I., Mallela, S., & Kumar, R. (2003). divisive information-theoretic feature clustering algorithm text classification. Journal Machine Learning Research, 3,
12651287.
Dumais, S., Platt, J., Heckerman, D., & Sahami, M. (1998). Inductive learning algorithms
representations text categorization. Proceedings 7th ACM International Conference Information Knowledge Management, pp. 148155.
Egozi, O., Gabrilovich, E., & Markovitch, S. (2008). Concept-based feature generation
selection information retrieval. AAAI08.
Fawcett, T. (1993). Feature Discovery Problem Solving Systems. Ph.D. thesis, UMass.
Fellbaum, C. (Ed.). (1998). WordNet: Electronic Lexical Database. MIT Press, Cambridge, MA.
Finkelstein, L., Gabrilovich, E., Matias, Y., Rivlin, E., Solan, Z., Wolfman, G., & Ruppin,
E. (2002a). Placing search context: concept revisited. ACM Transactions
Information Systems, 20 (1), 116131.
491

fiGabrilovich & Markovitch

Finkelstein, L., Gabrilovich, E., Matias, Y., Rivlin, E., Solan, Z., Wolfman, G., & Ruppin,
E. (2002b). WordSimilarity-353 test collection..
Fuernkranz, J., Mitchell, T., & Riloff, E. (1998). case study using linguistic phrases
text categorization WWW. Sahami, M. (Ed.), Learning Text Categorization: Proceedings 1998 AAAI/ICML Workshop, pp. 512. AAAI Press,
Madison, Wisconsin.
Gabrilovich, E., & Markovitch, S. (2004). Text categorization many redundant features:
Using aggressive feature selection make SVMs competitive C4.5. Proceedings
21st International Conference Machine Learning, pp. 321328.
Gabrilovich, E., & Markovitch, S. (2005). Feature generation text categorization using world knowledge. Proceedings 19th International Joint Conference
Artificial Intelligence, pp. 10481053, Edinburgh, Scotand.
Gabrilovich, E., & Markovitch, S. (2006). Overcoming brittleness bottleneck using
Wikipedia: Enhancing text categorization encyclopedic knowledge. Proceedings 21st National Conference Artificial Intelligence, pp. 13011306.
Gabrilovich, E., & Markovitch, S. (2007a). Computing semantic relatedness using wikipediabased explicit semantic analysis. Proceedings 20th International Joint Conference Artificial Intelligence, pp. 16061611.
Gabrilovich, E., & Markovitch, S. (2007b). Harnessing expertise 70,000 human editors: Knowledge-based feature generation text categorization. Journal Machine
Learning Research, 8, 22972345.
Galavotti, L., Sebastiani, F., & Simi, M. (2000). Experiments use feature selection
negative evidence automated text categorization. Borbinha, J., & Baker, T.
(Eds.), Proceedings ECDL-00, 4th European Conference Research Advanced
Technology Digital Libraries, pp. 5968, Lisbon, Portugal.
Giles, J. (2005). Internet encyclopaedias go head head. Nature, 438, 900901.
Gurevych, I., Mueller, C., & Zesch, T. (2007). be? electronic career guidance
based semantic relatedness. Proceedings 45th Annual Meeting
Association Computational Linguistics.
Hersh, W., Buckley, C., Leone, T., & Hickam, D. (1994). OHSUMED: interactive
retrieval evaluation new large test collection research. Proceedings
17th ACM International Conference Research Development Information
Retrieval, pp. 192201.
Hirsh, H., & Japkowicz, N. (1994). Bootstrapping training-data representations inductive
learning: case study molecular biology. Proceedings Twelfth National
Conference Artificial Intelligence, pp. 639644.
Hirst, G., & St-Onge, D. (1998). Lexical chains representations context detection
correction malapropisms. WordNet: Electronic Lexical Database, pp.
305332. MIT Press, Cambridge, MA.
Hu, Y.-J., & Kibler, D. (1996). wrapper approach constructive induction.
Thirteenth National Conference Artificial Intelligence, pp. 4752.
492

fiWikipedia-based Semantic Interpretation

Hughes, T., & Ramage, D. (2007). Lexical semantic relatedness random graph walks.
Proceedings Conference Empirical Methods Natural Language Processing
(EMNLP).
Hull, D. A. (1994). Improving text retrieval routing problem using latent semantic
indexing. Croft, W. B., & Van Rijsbergen, C. J. (Eds.), Proceedings 17th ACM
International Conference Research Development Information Retrieval, pp.
282289, Dublin, Ireland. Springer Verlag, Heidelberg, Germany.
Jarmasz, M. (2003). Rogets thesaurus lexical resource natural language processing.
Masters thesis, University Ottawa.
Jarmasz, M., & Szpakowicz, S. (2003). Rogets thesaurus semantic similarity.
Proceedings International Conference Recent Advances Natural Language
Processing, pp. 111120.
Jiang, J. J., & Conrath, D. W. (1997). Semantic similarity based corpus statistics
lexical taxonomy. Proceedings 10th International Conference Research
Computational Linguistics, pp. 5763.
Jo, T. (2000). Neurotextcategorizer: new model neural network text categorization.
Proceedings International Conference Neural Information Processing, pp.
280285, Taejon, South Korea.
Jo, T. (2006). Dynamic Document Organization using Text Categorization Text Clustering. Ph.D. thesis, University Ottawa.
Jo, T., & Japkowicz, N. (2005). Text clustering using NTSO. Proceedings International Joint Conference Neural Networks, pp. 558563.
Joachims, T. (1998). Text categorization support vector machines: Learning many
relevant features. Proceedings European Conference Machine Learning,
pp. 137142.
Joachims, T. (1999). Making large-scale SVM learning practical. Schoelkopf, B., Burges,
C., & Smola, A. (Eds.), Advances Kernel Methods Support Vector Learning, pp.
169184. MIT Press.
Kudenko, D., & Hirsh, H. (1998). Feature generation sequence categorization. Proceedings 15th Conference American Association Artificial Intelligence,
pp. 733738.
Kumaran, G., & Allan, J. (2004). Text classification named entities new event
detection. Proceedings 27th ACM International Conference Research
Development Information Retrieval, pp. 297304.
Lang, K. (1995). Newsweeder: Learning filter netnews. Proceedings 12th International Conference Machine Learning, pp. 331339.
Leacock, C., & Chodorow, M. (1998). Combining local context WordNet similarity
word sense identification. WordNet: Electronic Lexical Database, pp. 265283.
MIT Press, Cambridge, MA.
Lee, L. (1999). Measures distributional similarity. Proceedings 37th Annual
Meeting ACL, pp. 2532.
493

fiGabrilovich & Markovitch

Lee, M. D., Pincombe, B., & Welsh, M. (2005). comparison machine measures text
document similarity human judgments. 27th Annual Meeting Cognitive
Science Society (CogSci2005), pp. 12541259.
Lenat, D. B. (1995). CYC: large-scale investment knowledge infrastructure. Communications ACM, 38 (11).
Lenat, D. B. (1997). 2001 2001: Common sense mind HAL. HALs
Legacy, pp. 194209. MIT Press.
Lenat, D. B., Guha, R. V., Pittman, K., Pratt, D., & Shepherd, M. (1990). CYC: Towards
programs common sense. Communications ACM, 33 (8).
Leopold, E., & Kindermann, J. (2002). Text categorization support vector machines:
represent texts input space. Machine Learning, 46, 423444.
Lewis, D. D. (1992). evaluation phrasal clustered representations text
categorization task. Proceedings 15th ACM International Conference
Research Development Information Retrieval, pp. 3750.
Lewis, D. D., & Croft, W. B. (1990). Term clustering syntactic phrases. Proceedings
13th ACM International Conference Research Development Information
Retrieval, pp. 385404.
Lewis, D. D., Schapire, R. E., Callan, J. P., & Papka, R. (1996). Training algorithms
linear text classifiers. Proceedings 19th ACM International Conference
Research Development Information Retrieval, pp. 298306.
Lewis, D. D., Yang, Y., Rose, T., & Li, F. (2004). RCV1: new benchmark collection
text categorization research. Journal Machine Learning Research, 5, 361397.
Lin, D. (1998a). Automatic retrieval clustering similar words. Proceedings
17th International Conference Computational Linguistics 36th Annual Meeting
Association Computational Linguistics, pp. 768774.
Lin, D. (1998b). information-theoretic definition word similarity. Proceedings
15th International Conference Machine Learning, pp. 296304.
Liu, T., Chen, Z., Zhang, B., Ma, W.-y., & Wu, G. (2004). Improving text classification
using local latent semantic indexing. ICDM04, pp. 162169.
Manning, C. D., & Schuetze, H. (2000). Foundations Statistical Natural Language Processing. MIT Press.
Markovitch, S., & Rosenstein, D. (2002). Feature generation using general constructor
functions. Machine Learning, 49 (1), 5998.
Matheus, C. J. (1991). need constructive induction. Birnbaum, L., & Collins, G.
(Eds.), Proceedings Eighth International Workshop Machine Learning, pp.
173177.
Matheus, C. J., & Rendell, L. A. (1989). Constructive induction decision trees.
Proceedings 11th International Conference Artificial Intelligence, pp. 645
650.
494

fiWikipedia-based Semantic Interpretation

MeSH (2003). Medical subject headings (MeSH).
http://www.nlm.nih.gov/mesh.

National Library Medicine.

Metzler, D., Dumais, S., & Meek, C. (2007). Similarity measures short segments text.
Proceedings 29th European Conference Information Retrieval, pp. 1627.
Mihalcea, R. (2003). Turning wordnet information retrieval resource: Systematic
polysemy conversion hierarchical codes. International Journal Pattern Recognition Artificial Intelligence (IJPRAI), 17 (1), 689704.
Mihalcea, R., Corley, C., & Strapparava, C. (2006). Corpus-based knowledge-based
measures text semantic similarity. AAAI06, pp. 775780.
Mikheev, A. (1998). Feature lattices maximum entropy models. Proceedings
17th International Conference Computational Linguistics, pp. 848854.
Miller, G. A., & Charles, W. G. (1991). Contextual correlates semantic similarity. Language Cognitive Processes, 6 (1), 128.
Milne, D., & Witten, I. (2008). effective, low-cost measure semantic relatedness
obtained wikipedia links. Proceedings AAAI-08 Workshop Wikipedia
Artificial Intelligence, conjunction 23rd AAAI Conference Artificial
Intelligence.
Mladenic, D. (1998). Turning Yahoo automatic web-page classifier. Proceedings
13th European Conference Artificial Intelligence, pp. 473474.
Montague, R. (1973). proper treatment quantification ordinary English.
Hintikka, J., Moravcsik, J., & Suppes, P. (Eds.), Approaches Natural Language, pp.
373398. Reidel, Dordrecht.
Murphy, P. M., & Pazzani, M. J. (1991). ID2-of-3: Constructive induction M-of-N concepts discriminators decision trees. Proceedings 8th International
Conference Machine Learning, pp. 183188. Morgan Kaufmann.
Pagallo, G., & Haussler, D. (1990). Boolean feature discovery empirical learning. Machine
Learning, 5 (1), 7199.
Pang, B., Lee, L., & Vaithyanathan, S. (2002). Thumbs up? Sentiment classification using
machine learning techniques. Proceedings Conference Empirical Methods
Natural Language Processing, pp. 7986.
Peng, F., Schuurmans, D., & Wang, S. (2004). Augmenting naive Bayes classifiers
statistical language models. Information Retrieval, 7 (3-4), 317345.
Peng, F., & Shuurmans, D. (2003). Combining naive Bayes n-gram language models
text classification. Proceedings 25th European Conference Information
Retrieval Research (ECIR-03), pp. 335350.
Pincombe, B. (2004). Comparison human latent semantic analysis (LSA) judgements
pairwise document similarities news corpus. Tech. rep. DSTO-RR-0278, Information Sciences Laboratory, Defence Science Technology Organization, Department Defense, Australian Government.
Porter, M. (1980). algorithm suffix stripping. Program, 14 (3), 130137.
495

fiGabrilovich & Markovitch

Potthast, M., Stein, B., & Anderka, M. (2008). wikipedia-based multilingual retrieval
model. European Conference Information Retrieval.
Press, W. H., Teukolsky, S. A., Vetterling, W. T., & Flannery, B. P. (1997). Numerical
Recipes C: Art Scientific Computing. Cambridge University Press.
Qiu, Y., & Frei, H. (1993). Concept based query expansion. Proceedings ACM
International Conference Research Development Information Retrieval.
Raskutti, B., Ferra, H., & Kowalczyk, A. (2001). Second order features maximizing
text classification performance. De Raedt, L., & Flach, P. (Eds.), Proceedings
European Conference Machine Learning (ECML), Lecture notes Artificial
Intelligence (LNAI) 2167, pp. 419430. Springer-Verlag.
Resnik, P. (1999). Semantic similarity taxonomy: information-based measure
application problems ambiguity natural language. Journal Artificial
Intelligence Research, 11, 95130.
Reuters (1997). Reuters-21578 text categorization test collection, Distribution 1.0. Reuters.
daviddlewis.com/resources/testcollections/reuters21578.
Rocchio, J. J. (1971). Relevance feedback information retrieval. SMART Retrieval
System: Experiments Automatic Document Processing, pp. 313323. Prentice Hall.
Rogati, M., & Yang, Y. (2002). High-performing feature selection text classification.
Proceedings International Conference Information Knowledge Management (CIKM02), pp. 659661.
Roget, P. (1852). Rogets Thesaurus English Words Phrases. Longman Group Ltd.
Rose, T., Stevenson, M., & Whitehead, M. (2002). Reuters Corpus Volume 1from
yesterdays news tomorrows language resources. Proceedings Third International Conference Language Resources Evaluation, pp. 713.
Rowling, J. (1997). Harry Potter Philosophers Stone. Bloomsbury.
Rubenstein, H., & Goodenough, J. B. (1965). Contextual correlates synonymy. Communications ACM, 8 (10), 627633.
Sable, C., McKeown, K., & Church, K. W. (2002). NLP found helpful (at least one
text categorization task). Conference Empirical Methods Natural Language
Processing, pp. 172179.
Sahami, M., & Heilman, T. (2006). web-based kernel function measuring similarity
short text snippets. WWW06, pp. 377386. ACM Press.
Salton, G., & Buckley, C. (1988). Term weighting approaches automatic text retrieval.
Information Processing Management, 24 (5), 513523.
Salton, G., & McGill, M. (1983).
McGraw-Hill.

Introduction Modern Information Retrieval.

Scott, S. (1998). Feature engineering symbolic approach text classification. Masters
thesis, U. Ottawa.
Scott, S., & Matwin, S. (1999). Feature engineering text classification. Proceedings
16th International Conference Machine Learning, pp. 379388.
496

fiWikipedia-based Semantic Interpretation

Sebastiani, F. (2002). Machine learning automated text categorization. ACM Computing
Surveys, 34 (1), 147.
Snow, R., OConnor, B., Jurafsky, D., & Ng, A. Y. (2008). Cheap fast - good?
evaluating non-expert annotations natural language tasks. Proceedings
Conference Empirical Methods Natural Language Processing.
Sorg, P., & Cimiano, P. (2008). Cross-lingual information retrieval explicit semantic
analysis. Working Notes CLEF Workshop.
Strube, M., & Ponzetto, S. P. (2006). WikiRelate! Computing semantic relatedness using
Wikipedia. AAAI06, pp. 14191424, Boston, MA.
Turney, P. (2002). Thumbs thumbs down? Semantic orientation applied unsupervised classification reviews. Proceedings 40th Annual Meeting
Association Computational Linguistics, pp. 417424.
Turney, P. (2005). Measuring semantic similarity latent relational analysis. Proceedings
Nineteenth International Joint Conference Artificial Intelligence (IJCAI-05),
pp. 11361141, Edinburgh, Scotland.
Turney, P. (2006). Similarity semantic relations. Computational Linguistics, 32 (3), 379
416.
Turney, P., & Littman, M. L. (2002). Unsupervised learning semantic orientation
hundred-billion-word corpus. Tech. rep. ERB-1094, National Research Council
Canada.
Turney, P. D. (2001). Mining web synonyms: PMI-IR versus LSA TOEFL.
Proceedings Twelfth European Conference Machine Learning, pp. 491502.
Urena-Lopez, A., Buenaga, M., & Gomez, J. M. (2001). Integrating linguistic resources
TC WSD. Computers Humanities, 35, 215230.
Wang, B. B., McKay, R., Abbass, H. A., & Barlow, M. (2003). comparative study
domain ontology guided feature extraction. Proceedings 26th Australian
Computer Science Conference (ASCS-2003), pp. 6978.
Widrow, B., & Stearns, S. (1985). Adaptive Signal Processing. Prentice Hall.
Wikipedia (2006). Wikipedia, free encyclopedia.. http://en.wikipedia.org.
Wu, H., & Gunopulos, D. (2002). Evaluating utility statistical phrases latent
semantic indexing text classification. IEEE International Conference Data
Mining, pp. 713716.
Yang, Y. (2001). study thresholding strategies text categorization. Proceedings
24th International Conference Research Development Information
Retrieval, pp. 137145.
Yang, Y., & Liu, X. (1999). re-examination text categorization methods. Proceedings
22nd International Conference Research Development Information
Retrieval, pp. 4249.
Yang, Y., & Pedersen, J. (1997). comparative study feature selection text categorization. Proceedings 14th International Conference Machine Learning,
pp. 412420.
497

fiGabrilovich & Markovitch

Zelikovitz, S., & Hirsh, H. (2000). Improving short-text classification using unlabeled background knowledge assess document similarity. Proceedings 17th International Conference Machine Learning, pp. 11831190.
Zelikovitz, S., & Hirsh, H. (2001). Using LSI text classification presence
background text. Proceedings Conference Information Knowledge
Management, pp. 113118.
Zesch, T., & Gurevych, I. (2006). Automatically creating datasets measures semantic
relatedness. Proceedings ACL Workshop Linguistic Distances, pp. 1624,
Sydney, Australia.
Zesch, T., Mueller, C., & Gurevych, I. (2008). Using wiktionary computing semantic
relatedness. Proceedings 23rd AAAI Conference Artificial Intelligence,
pp. 861866.
Zobel, J., & Moffat, A. (1998). Exploring similarity space. ACM SIGIR Forum, 32 (1),
1834.

498


