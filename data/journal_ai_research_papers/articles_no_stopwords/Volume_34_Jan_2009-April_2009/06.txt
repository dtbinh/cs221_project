Journal Artificial Intelligence Research 34 (2009) 569-603

Submitted 07/08; published 04/09

Learning Document-Level Semantic Properties
Free-Text Annotations
S.R.K. Branavan
Harr Chen
Jacob Eisenstein
Regina Barzilay

BRANAVAN @ CSAIL . MIT. EDU
HARR @ CSAIL . MIT. EDU
JACOBE @ CSAIL . MIT. EDU
REGINA @ CSAIL . MIT. EDU

Computer Science Artificial Intelligence Laboratory
Massachusetts Institute Technology
77 Massachusetts Avenue, Cambridge 02139

Abstract
paper presents new method inferring semantic properties documents leveraging free-text keyphrase annotations. annotations becoming increasingly abundant due
recent dramatic growth semi-structured, user-generated online content. One especially
relevant domain product reviews, often annotated authors pros/cons
keyphrases real bargain good value. annotations representative
underlying semantic properties; however, unlike expert annotations, noisy: lay authors
may use different labels denote property, labels may missing. learn
using noisy annotations, find hidden paraphrase structure clusters keyphrases.
paraphrase structure linked latent topic model review texts, enabling system predict properties unannotated documents effectively aggregate semantic
properties multiple reviews. approach implemented hierarchical Bayesian model
joint inference. find joint inference increases robustness keyphrase clustering
encourages latent topics correlate semantically meaningful properties. Multiple evaluations demonstrate model substantially outperforms alternative approaches summarizing
single multiple documents set semantically salient keyphrases.

1. Introduction
Identifying document-level semantic properties implied text core problem natural
language understanding. example, given text restaurant review, would useful
extract semantic-level characterization authors reaction specific aspects restaurant, food service quality (see Figure 1). Learning-based approaches dramatically
increased scope robustness semantic processing, typically dependent
large expert-annotated datasets, costly produce (Zaenen, 2006).
propose use alternative source annotations learning: free-text keyphrases produced novice users. example, consider lists pros cons often accompany
reviews products services. end-user annotations increasingly prevalent online,
grow organically keep pace subjects interest socio-cultural trends. Beyond
pragmatic considerations, free-text annotations appealing linguistic standpoint
capture intuitive semantic judgments non-specialist language users. many real-world
datasets, annotations created documents original author, providing direct window
semantic judgments motivated document text.

c
2009
AI Access Foundation. rights reserved.

fiB RANAVAN , C HEN , E ISENSTEIN , & BARZILAY

pros/cons: great nutritional value
... combines all: amazing product, quick friendly service, cleanliness, great nutrition ...
pros/cons: bit pricey, healthy
... awesome place go health conscious. really great low calorie dishes
publish calories fat grams per serving.

Figure 1: Excerpts online restaurant reviews pros/cons phrase lists. reviews assert
restaurant serves healthy food, use different keyphrases. Additionally,
first review discusses restaurants good service, annotated
keyphrases.

major obstacle computational use free-text annotations inherently noisy fixed vocabulary, explicit relationship annotation keyphrases,
guarantee relevant semantic properties document annotated. example,
pros/cons annotations accompanying restaurant reviews Figure 1, underlying
semantic idea expressed different ways keyphrases great nutritional value
healthy. Additionally, first review discusses quality service, annotated such.
contrast, expert annotations would replace synonymous keyphrases single canonical label, would fully label semantic properties described text. expert annotations
typically used supervised learning methods. demonstrate paper, traditional
supervised approaches perform poorly free-text annotations used instead clean, expert
annotations.
paper demonstrates new approach handling free-text annotation context
hidden-topic analysis document text. show regularities text clarify noise
annotations example, although great nutritional value healthy different
surface forms, text documents annotated two keyphrases likely
similar. modeling relationship document text annotations large dataset,
possible induce clustering annotation keyphrases help overcome
problem inconsistency. model addresses problem incompleteness novice
annotators fail label relevant semantic topics estimating topics predicted
document text alone.
Central approach idea document text associated annotations reflect
single underlying set semantic properties. text, semantic properties correspond
induced hidden topics similar growing body work latent topic models,
latent Dirichlet allocation (LDA; Blei, Ng, & Jordan, 2003). However, unlike existing work topic
modeling, tie hidden topics text clusters observed keyphrases. connection
motivated idea text associated annotations grounded shared set
semantic properties. modeling properties directly, ensure inferred hidden
topics semantically meaningful, clustering free-text annotations robust
noise.
approach takes form hierarchical Bayesian framework, includes LDA-style
component word text generated mixture multinomials. addition, incorporate similarity matrix across universe annotation keyphrases,

570

fiL EARNING OCUMENT-L EVEL EMANTIC P ROPERTIES F REE -T EXT NNOTATIONS

constructed based orthographic distributional features keyphrases. model
matrix generated underlying clustering keyphrases, keyphrases
clustered together likely produce high similarity scores. generate words
document, model two distributions semantic properties one governed annotation
keyphrases clusters, background distribution cover properties mentioned
annotations. latent topic word drawn mixture two distributions.
learning model parameters noisily-labeled training set, apply model unlabeled
data.
build system extracts semantic properties reviews products services.
system uses training corpus includes user-created free-text annotations pros cons
review. Training yields two outputs: clustering keyphrases semantic properties,
topic model capable inducing semantic properties unlabeled text. clustering
annotation keyphrases relevant applications content-based information retrieval,
allowing users retrieve documents semantically relevant annotations even surface
forms differ query term. topic model used infer semantic properties
unlabeled text.
topic model used perform multi-document summarization, capturing key
semantic properties multiple reviews. Unlike traditional extraction-based approaches multidocument summarization, induced topic model abstracts text review representation capturing relevant semantic properties. enables comparison reviews even
use superficially different terminology describe set semantic properties.
idea implemented review aggregation system extracts majority sentiment
multiple reviewers product service. example output produced system
shown Figure 6. system applied reviews 480 product categories, allowing users
navigate semantic properties 49,490 products based total 522,879 reviews.
effectiveness approach confirmed several evaluations.
summarization single multiple documents, compare properties inferred model expert annotations. approach yields substantially better results
alternatives research literature; particular, find learning clustering free-text
annotation keyphrases essential extracting meaningful semantic properties dataset.
addition, compare induced clustering gold standard clustering produced expert
annotators. comparison shows tying clustering hidden topic model substantially
improves quality, clustering induced system coheres well clustering
produced expert annotators.
remainder paper structured follows. Section 2 compares approach previous work topic modeling, semantic property extraction, multi-document summarization.
Section 3 describes properties free-text annotations motivate approach. model
described Section 4, method parameter estimation presented Section 5.
Section 6 describes implementation evaluation single-document multi-document
summarization systems using techniques. summarize contributions consider directions future work Section 7. code, datasets expert annotations used paper
available online http://groups.csail.mit.edu/rbg/code/precis/.

571

fiB RANAVAN , C HEN , E ISENSTEIN , & BARZILAY

2. Related Work
material presented section covers three lines related work. First, discuss work
Bayesian topic modeling related technique learning free-text annotations.
Next, discuss state-of-the-art methods identifying analyzing product properties
review text. Finally, situate summarization work landscape prior research
multi-document summarization.
2.1 Bayesian Topic Modeling
Recent work topic modeling literature demonstrated semantically salient topics
inferred unsupervised fashion constructing generative Bayesian model document text. One notable example line research Latent Dirichlet Allocation (LDA; Blei
et al., 2003). LDA framework, semantic topics equated latent distributions words
text; thus, document modeled mixture topics. class models
used variety language processing tasks including topic segmentation (Purver, Kording,
Griffiths, & Tenenbaum, 2006), named-entity resolution (Bhattacharya & Getoor, 2006), sentiment
ranking (Titov & McDonald, 2008b), word sense disambiguation (Boyd-Graber, Blei, & Zhu,
2007).
method similar LDA assigns latent topic indicators word
dataset, models documents mixtures topics. However, LDA model unsupervised,
provide method linking latent topics external observed representations
properties interest. contrast, model exploits free-text annotations dataset
ensure induced topics correspond semantically meaningful properties.
Combining topics induced LDA external supervision first considered Blei
McAuliffe (2008) supervised Latent Dirichlet Allocation (sLDA) model. induction
hidden topics driven annotated examples provided training stage. perspective supervised learning, approach succeeds hidden topics mediate
document annotations lexical features. Blei McAuliffe describe variational expectationmaximization procedure approximate maximum-likelihood estimation models parameters. tested two polarity assessment tasks, sLDA shows improvement model
topics induced unsupervised model added features supervised
model.
key difference model sLDA assume access clean
supervision data training. Since annotations provided algorithm free-text
nature, incomplete fraught inconsistency. substantial difference input
structure motivates need model simultaneously induces hidden structure freetext annotations learns predict properties text.
2.2 Property Assessment Review Analysis
model applied task review analysis. Traditionally, task identifying properties product review texts cast extraction problem (Hu & Liu, 2004; Liu,
Hu, & Cheng, 2005; Popescu, Nguyen, & Etzioni, 2005). example, Hu Liu (2004) employ
association mining identify noun phrases express key portions product reviews. polarity extracted phrases determined using seed set adjectives expanded via WordNet

572

fiL EARNING OCUMENT-L EVEL EMANTIC P ROPERTIES F REE -T EXT NNOTATIONS

relations. summary review produced extracting property phrases present verbatim
document.
Property extraction refined PINE (Popescu et al., 2005), another system
review analysis. PINE employs novel information extraction method identify noun phrases
could potentially express salient properties reviewed products; candidates
pruned using WordNet morphological cues. Opinion phrases identified using set handcrafted rules applied syntactic dependencies extracted input document. semantic
orientation properties computed using relaxation labeling method finds optimal assignment polarity labels given set local constraints. Empirical results demonstrate PINE
outperforms Hu Lius system opinion extraction identifying polarity opinion words.
two feature extraction methods informed human knowledge way opinions
typically expressed reviews: Hu Liu (2004), human knowledge encoded using
WordNet seed adjectives; Popescu et al. (2005), opinion phrases extracted via handcrafted rules. alternative approach learn rules feature extraction annotated
data. end, property identification modeled classification framework (Kim &
Hovy, 2006). classifier trained using corpus free-text pro con keyphrases
specified review authors. keyphrases compared sentences review
text; sentences exhibit high word overlap previously identified phrases marked pros
cons according phrase polarity. rest sentences marked negative examples.
Clearly, accuracy resulting classifier depends quality automatically induced annotations. analysis free-text annotations several domains shows automatically mapping even manually-extracted annotation keyphrases document text difficult
task, due variability keyphrase surface realizations (see Section 3). argue rest
paper, beneficial explicitly address difficulties inherent free-text annotations.
end, work distinguished two significant ways property extraction methods described above. First, able predict properties beyond appear verbatim text.
Second, approach learns semantic relationships different keyphrases, allowing
us draw direct comparisons reviews even semantic ideas expressed using
different surface forms.
Working related domain web opinion mining, Lu Zhai (2008) describe system
generates integrated opinion summaries, incorporate expert-written articles (e.g., review online magazine) user-generated ordinary opinion snippets (e.g., mentions
blogs). Specifically, expert article assumed structured segments, collection
representative ordinary opinions aligned segment. Probabilistic Latent Semantic Analysis
(PLSA) used induce clustering opinion snippets, cluster attached one
expert article segments. clusters may unaligned segment, indicating
opinions entirely unexpressed expert article. Ultimately, integrated opinion summary combination single expert article multiple user-generated opinion snippets
confirm supplement specific segments review.
works final goal different aim provide highly compact summary multitude user opinions identifying underlying semantic properties, rather supplementing
single expert article user opinions. specifically leverage annotations users already
provide reviews, thus obviating need expert article template opinion inte-

573

fiB RANAVAN , C HEN , E ISENSTEIN , & BARZILAY

gration. Consequently, approach suitable goal producing concise keyphrase
summarizations user reviews, particularly review taken authoritative.
work closest methodology approach review summarizer developed Titov
McDonald (2008a). method summarizes review selecting list phrases
express writers opinions set predefined properties (e.g.,, food ambiance restaurant
reviews). system access numerical ratings set properties,
training set providing examples appropriate keyphrases extract. Similar sLDA, method
uses numerical ratings bias hidden topics towards desired semantic properties. Phrases
strongly associated properties via hidden topics extracted part summary.
several important differences work summarization method
Titov McDonald. method assumes predefined set properties thus cannot capture
properties outside set. Moreover, consistent numerical annotations required training,
method emphasizes use free-text annotations. Finally, since Titov McDonalds
algorithm extractive, facilitate property comparison across multiple reviews.
2.3 Multidocument Summarization
paper relates large body work multi-document summarization. Researchers
long noted central challenge multi-document summarization identifying redundant
information input documents (Radev & McKeown, 1998; Carbonell & Goldstein, 1998; Mani
& Bloedorn, 1997; Barzilay, McKeown, & Elhadad, 1999). task crucial significance
multi-document summarizers operate related documents describe facts
multiple times. fact, common assume repetition information among related sources
indicator importance (Barzilay et al., 1999; Radev, Jing, & Budzikowska, 2000; Nenkova,
Vanderwende, & McKeown, 2006). Many algorithms first cluster sentences together,
extract generate sentence representatives clusters.
Identification repeated information equally central approach multi-document
summarization method selects properties stated plurality users, thereby eliminating rare and/or erroneous opinions. key difference algorithm existing summarization systems method identifying repeated expressions single semantic property.
Since existing work multi-document summarization focuses topic-independent
newspaper articles, redundancy identified via sentence comparison. instance, Radev et al.
(2000) compare sentences using cosine similarity corresponding word vectors. Alternatively, methods compare sentences via alignment syntactic trees (Barzilay et al., 1999;
Marsi & Krahmer, 2005). string- tree-based comparison algorithms augmented
lexico-semantic knowledge using resources WordNet.
approach described paper perform comparisons sentence level. Instead, first abstract reviews set properties compare property overlap across
different documents. approach relates domain-dependent approaches text summarization (Radev & McKeown, 1998; White, Korelsky, Cardie, Ng, Pierce, & Wagstaff, 2001; Elhadad
& McKeown, 2001). methods identify relations documents comparing
abstract representations. cases, abstract representation constructed using off-the-shelf
information extraction tools. template specifying types information select crafted
manually domain interest. Moreover, training information extraction systems requires
corpus manually annotated relations interest. contrast, method require

574

fiL EARNING OCUMENT-L EVEL EMANTIC P ROPERTIES F REE -T EXT NNOTATIONS

Incompleteness
Property
Good food
Good service
Good price
Bad food
Bad service
Bad price
Average

Recall

Precision

F-score

0.736
0.329
0.500
0.516
0.475
0.690
0.578

0.968
0.821
0.707
0.762
0.633
0.645
0.849

0.836
0.469
0.586
0.615
0.543
0.667
0.688

Inconsistency
Keyphrase Top Keyphrase
Count
Coverage %
23
38.3
27
28.9
20
41.8
16
23.7
20
22.0
15
30.6
22.6
33.6

Table 1: Incompleteness inconsistency restaurant domain, six major properties prevalent reviews. incompleteness figures recall, precision, F-score
author annotations (manually clustered properties) gold standard property
annotations. Inconsistency measured number different keyphrase realizations
least five occurrences associated property, percentage frequency
commonly occurring keyphrases used annotate property.
averages bottom row weighted according frequency property occurrence.

manual template specification corpora annotated experts. abstract representations
induce linguistically rich extraction templates, nevertheless enable us
perform in-depth comparisons across different reviews.

3. Analysis Free-Text Keyphrase Annotations
section, explore characteristics free-text annotations, aiming quantify degree
noise observed data. results analysis motivate development learning
algorithm described Section 4.
perform investigation domain online restaurant reviews using documents downloaded popular Epinions1 website. Users website evaluate products providing
textual description opinion, well concise lists keyphrases (pros cons)
summarizing review. Pros/cons keyphrases appealing source annotations online
review texts. However, contributed independently multiple users thus unlikely
clean expert annotations. analysis, focus two features free-text annotations: incompleteness inconsistency. measure incompleteness quantifies degree
label omission free-text annotations, inconsistency reflects variance keyphrase
vocabulary used various annotators.
test quality user-generated annotations, compare expert annotations produced systematic fashion. annotation effort focused six properties
commonly mentioned review authors, specifically shown Table 1. Given
review property, task assess whether reviews text supports property.
annotations produced two judges guided standardized set instructions. contrast
author annotations website, judges conferred training session ensure consistency completeness. two judges collectively annotated 170 reviews, 30 annotated
1. http://www.epinions.com/

575

fiB RANAVAN , C HEN , E ISENSTEIN , & BARZILAY

Property: good price
relatively inexpensive, dirt cheap, relatively cheap, great price, fairly priced, well priced, reasonable
prices, cheap prices, affordable prices, reasonable cost

Figure 2: Examples many different paraphrases related property good price appear
pros/cons keyphrases reviews used inconsistency analysis.

both. Cohens Kappa, measure inter-annotator agreement ranges zero one,
0.78 joint set, indicating high agreement (Cohen, 1960). average, review text
annotated 2.56 properties.
Separately, one judges standardized free-text pros/cons annotations
170 reviews. reviews keyphrases matched six properties. standardization allows direct comparison properties judged supported reviews
text properties described reviews free-text annotations. find many semantic properties judged present text user annotated average,
keyphrases expressed 1.66 relevant semantic properties per document, text expressed
2.56 properties. gap demonstrates frequency authors omitted relevant semantic
properties review annotations.
3.1 Incompleteness
measure incompleteness, compare properties stated review authors form
pros cons stated review text, judged expert annotators.
comparison performed using precision, recall F-score. setting, recall proportion
semantic properties text review author provided least one annotation
keyphrase; precision proportion keyphrases conveyed properties judged supported
text; F-score harmonic mean. results comparison summarized
left half Table 1.
incompleteness results demonstrate significant discrepancy user expert
annotations. expected, recall quite low; 40% property occurrences stated
review text without explicitly mentioned annotations. precision scores indicate
converse true, though lesser extent keyphrases express properties
mentioned text.
Interestingly, precision recall vary greatly depending specific property.
highest good food, matching intuitive notion high food quality would key salient
property restaurant, thus likely mentioned text annotations. Conversely, recall good service lower users, high quality service apparently
key point summarizing review keyphrases.
3.2 Inconsistency
lack unified annotation scheme restaurant review dataset apparent across
reviewers, annotations feature 26,801 unique keyphrase surface forms set 49,310 total
keyphrase occurrences. Clearly, many unique keyphrases express semantic property
Figure 2, good price expressed ten different ways. quantify phenomenon, judges
576

fiL EARNING OCUMENT-L EVEL EMANTIC P ROPERTIES F REE -T EXT NNOTATIONS

Figure 3: Cumulative occurrence counts top ten keyphrases associated good service
property. percentages total 1,210 separate keyphrase occurrences
property.

manually clustered subset keyphrases associated six previously mentioned properties. Specifically, 121 keyphrases associated six major properties chosen, accounting
10.8% keyphrase occurrences.
use manually clustered annotations examine distributional pattern keyphrases
describe underlying property, using two different statistics. First, number
different keyphrases property gives lower bound number possible paraphrases.
Second, measure often common keyphrase used annotate property,
i.e., coverage keyphrase. metric gives sense diffuse keyphrases within
property are, specifically whether one single keyphrase dominates occurrences property.
Note value overestimate true coverage, since considering tenth
keyphrase occurrences.
right half Table 1 summarizes variability property paraphrases. Observe
property associated numerous paraphrases, found multiple times
actual keyphrase set. importantly, frequent keyphrase accounted third
property occurrences, strongly suggesting targeting labels learning
limited approach. illustrate last point, consider property good service, whose
keyphrase realizations distributional histogram appears Figure 3. cumulative percentage
frequencies frequent keyphrases associated property plotted. top four
keyphrases account three quarters property occurrences, even within limited
set keyphrases consider analysis, motivating need aggregate consideration
keyphrases.
next section, introduce model induces clustering among keyphrases
relating keyphrase clusters text, directly addressing characteristics data.

577

fiB RANAVAN , C HEN , E ISENSTEIN , & BARZILAY


x

h


c

z

w













keyphrase cluster model
keyphrase cluster assignment
keyphrase similarity values
document keyphrases
document keyphrase topics
probability selecting instead
selects word topics
background word topic model
word topic assignment
language models topic
document words

Dirichlet(0 )
x` Multinomial()
(
Beta(= ) x` = x`0
s`,`0
Beta(6= ) otherwise


= [d,1 . . . d,K ]

(
d,k

1


x` = k l hd
otherwise

Beta(0 )
cd,n Bernoulli(d )
Dirichlet(0 )
(
Multinomial(d ) cd,n = 1
zd,n
Multinomial(d ) otherwise
k Dirichlet(0 )
wd,n Multinomial(zd,n )

Figure 4: plate diagram model. Shaded circles denote observed variables, squares
denote hyperparameters. dotted arrows indicate constructed deterministically x h. use refer small constant probability mass.

578

fiL EARNING OCUMENT-L EVEL EMANTIC P ROPERTIES F REE -T EXT NNOTATIONS

4. Model Description
present generative Bayesian model documents annotated free-text keyphrases.
model assumes annotated document generated set underlying semantic topics.
Semantic topics generate document text indexing language model; approach,
associated clusters keyphrases. way, model viewed extension
Latent Dirichlet Allocation (Blei et al., 2003), latent topics additionally biased
toward keyphrases appear training data. However, coupling flexible,
words permitted drawn topics represented keyphrase annotations.
permits model learn effectively presence incomplete annotations, still
encouraging keyphrase clustering cohere topics supported document text.
Another critical aspect model desire ability use arbitrary comparisons
keyphrases, addition information surface forms. accommodate
goal, treat keyphrase surface forms generated model. Rather, acquire
real-valued similarity matrix across universe possible keyphrases, treat matrix
generated keyphrase clustering. representation permits use surface
distributional features keyphrase similarity, described Section 4.1.
advantage hierarchical Bayesian models easy change parts
model observed parts hidden. training, keyphrase annotations
observed, hidden semantic topics coupled clusters keyphrases. account
words related semantic topics, topics may associated keyphrases. test
time, model presented documents keyphrase annotations hidden.
model evaluated ability determine keyphrases applicable, based hidden
topics present document text.
judgment whether topic applies given unannotated document based probability mass assigned topic documents background topic distribution.
annotations, background topic distribution capture entirety documents
topics. task involving reviews products services, multiple topics may accompany
document. case, topic whose probability threshold (tuned development
set) predicted supported.
4.1 Keyphrase Clustering
handle hidden paraphrase structure keyphrases, one component model estimates
clustering keyphrases. goal obtain clusters cluster correspond welldefined semantic topic e.g., healthy good nutrition grouped single
cluster. overall joint model generative, generative model clustering could easily
integrated larger framework. approach would treat keyphrases
cluster generated parametric distribution. However, representation would
permit many powerful features assessing similarity pairs keyphrases, string
overlap keyphrase co-occurrence corpus (McCallum, Bellare, & Pereira, 2005).
reason, represent keyphrase real-valued vector rather surface
form. vector given keyphrase includes similarity scores respect every observed keyphrase (the similarity scores represented Figure 4). model similarity
scores generated cluster memberships (represented x Figure 4). two keyphrases

579

fiB RANAVAN , C HEN , E ISENSTEIN , & BARZILAY

Lexical

cosine similarity surface forms two keyphrases, represented word frequency vectors.

Co-occurrence

keyphrase represented vector co-occurrence values.
vector counts many times keyphrases appear documents
annotated keyphrase. example, similarity vector
good food may include entry tasty food, value
would number documents annotated good food
contain tasty food text. similarity two
keyphrases cosine similarity co-occurrence vectors.

Table 2: two sources information used compute similarity matrix experiments.
final similarity scores linear combinations two values. Note cooccurrence similarity contains second-order co-occurrence information.

Figure 5: surface plot keyphrase similarity matrix set restaurant reviews, computed according Table 2. Red indicates high similarity, whereas blue indicates low
similarity. diagram, keyphrases grouped according expertcreated clustering, keyphrases similar meaning close together. strong series
similarity blocks along diagonal hint information could induce
reasonable clustering.

580

fiL EARNING OCUMENT-L EVEL EMANTIC P ROPERTIES F REE -T EXT NNOTATIONS

clustered together, similarity score generated distribution encouraging high similarity; otherwise, distribution encouraging low similarity used.2
features used producing similarity matrix given Table 2, encompassing lexical
distributional similarity measures. implemented system takes linear combination
two data sources, weighting sources equally. resulting similarity matrix keyphrases
restaurant domain shown Figure 5.
described next section, clustering keyphrases, model takes advantage
topic structure documents annotated keyphrases, addition information
individual keyphrases themselves. sense, differs traditional approaches paraphrase
identification (Barzilay & McKeown, 2001; Lin & Pantel, 2001).
4.2 Document Topic Modeling
analysis document text based probabilistic topic models LDA (Blei et al.,
2003). LDA framework, word generated language model indexed
words topic assignment. Thus, rather identifying single topic document, LDA identifies
distribution topics. High probability topic assignments identify compact, low-entropy
language models, probability mass language model topic divided among
relatively small vocabulary.
model operates similar manner, identifying topic word, denoted z
Figure 4. However, LDA learns distribution topics document, deterministically construct document-specific topic distribution clusters represented
documents keyphrases figure. assigns equal probability topics
represented keyphrase annotations, small probability topics. Generating
word topics way ties together clustering language models.
noted above, sometimes keyphrase annotation represent semantic
topics expressed text. reason, construct another background distribution topics. auxiliary variable c indicates whether given words topic drawn
distribution derived annotations, background model. Representing c
hidden variable allows us stochastically interpolate two language models
. addition, given document likely discuss topics covered
keyphrase. account this, model allowed leave clusters empty, thus
leaving topics independent keyphrases.
4.3 Generative Process
model assumes observed data generated stochastic process involving hidden
parameters. section, formally specify generative process. specification guides
inference hidden parameters based observed data, following:
L keyphrases, vector s` length L denoting pairwise similarity score
interval [0, 1] every keyphrase.
document d, bag words wd length Nd . nth word wd,n .
2. Note model similarity score independent draw; clearly assumption strong, due
symmetry transitivity. Models making similar assumptions independence related hidden variables
previously shown successful (for example, Toutanova & Johnson, 2008).

581

fiB RANAVAN , C HEN , E ISENSTEIN , & BARZILAY

document d, set keyphrase annotations hd , includes index ` document annotated keyphrase `.
number clusters K, large enough encompass topics actual
clusters keyphrases, well word-only topics.
observed variables generated according following process:
1. Draw multinomial distribution K keyphrase clusters symmetric Dirichlet
prior parameter 0 .3
2. ` = 1 . . . L:
(a) Draw `th keyphrases cluster assignment x` Multinomial().
3. (`, `0 ) = (1 . . . L, 1 . . . L):
(a) x` = x`0 , draw s`,`0 Beta(= ) Beta(2, 1), encouraging scores biased
toward values close one.
(b) x` 6= x`0 , draw s`,`0 Beta(6= ) Beta(1, 2), encouraging scores biased
toward values close zero.
4. k = 1 . . . K:
(a) Draw language model k symmetric Dirichlet prior parameter 0 .
5. = 1 . . . D:
(a) Draw background topic model symmetric Dirichlet prior parameter 0 .
(b) Deterministically construct annotation topic model , based keyphrase cluster
assignments x observed document annotations hd . Specifically, let H set
topics represented phrases hd . Distribution assigns equal probability
element H, small probability mass topics.4
(c) Draw weighted coin Beta(0 ), determine balance
annotation background topic models .
(d) n = 1 . . . Nd :
i. Draw binary auxiliary variable cd,n Bernoulli(d ), determines whether
topic word wd,n drawn annotation topic model background model .
ii. Draw topic assignment zd,n appropriate multinomial indicated
cd,n .
iii. Draw word wd,n Multinomial(zd,n ), is, language model indexed
words topic.
3. Variables subscripted zero fixed hyperparameters.
4. Making hard assignment zero probability topics creates problems parameter estimation.
probability 104 assigned topics represented keyphrase cluster memberships.

582

fiL EARNING OCUMENT-L EVEL EMANTIC P ROPERTIES F REE -T EXT NNOTATIONS

5. Parameter Estimation
make predictions unseen data, need estimate parameters model. Bayesian
inference, estimate distribution parameter, conditioned observed data
hyperparameters. inference intractable general case, sampling approaches allow
us approximately construct distributions parameter interest.
Gibbs sampling perhaps generic straightforward sampling technique. Conditional distributions computed hidden variable, given variables model.
repeatedly sampling distributions turn, possible construct Markov chain
whose stationary distribution posterior model parameters (Gelman, Carlin, Stern, &
Rubin, 2004). use sampling techniques natural language processing previously
investigated many researchers, including Finkel, Grenager, Manning (2005) Goldwater,
Griffiths, Johnson (2006).
present sampling equations hidden variables Figure 4. prior
keyphrase clusters sampled based hyperprior 0 keyphrase cluster assignments
x. write p( | . . .) mean probability conditioned variables.
p( | . . .) p( | 0 )p(x | ),

= p( | 0 )
p(x` | )
`

= Dirichlet(; 0 )



Multinomial(x` ; )

`

= Dirichlet(; 0 ),
i0 0 + count(x` = i). conditional distribution derived based conjugacy
multinomial Dirichlet distribution. first line follows Bayes rule, second
line conditional independence cluster assignments x given keyphrase distribution .
Resampling equations k derived similar manner:
p(d | . . .) Dirichlet(d ; 0d ),
p(k | . . .) Dirichlet(k ; k0 ),
P
0 = +
0d,i = 0 + count(zn,d = cn,d = 0) k,i
0
count(wn,d = zn,d = k).
0
building counts , consider cases cn,d = 0, indicating topic zn,d
indeed drawn background topic model . Similarly, building counts k0 ,
consider cases word wd,n drawn topic k.
resample , employ conjugacy Beta prior Bernoulli observation likelihoods, adding counts c prior 0 .
p(d | . . .) Beta(d ; 0d ),

P
count(c
=
1)
d,n
0
n
.
= 0 + P
n count(cd,n = 0)

583

fiB RANAVAN , C HEN , E ISENSTEIN , & BARZILAY

keyphrase cluster assignments represented x, whose sampling distribution depends
, s, z, via :
p(x` | . . .) p(x` | )p(s | x` , x` , )p(z | , , c)






p(x` | )
p(s`,`0 | x` , x`0 , )
p(zd,n | )
`0 6=`

cd,n =1


= Multinomial(x` ; )







Beta(s`,`0 ; x` ,x`0 )
Multinomial(zd,n ; ) .

`0 6=`

cd,n =1

leftmost term equation prior x` . next term encodes dependence
similarity matrix cluster assignments; slight abuse notation, write x` ,x`0
denote = x` = x`0 , 6= otherwise. third term dependence word topics
zd,n topic distribution . compute final result probability expression
possible setting x` , sample normalized multinomial.
word topics z sampled according topic distribution , background distribution
, observed words w, auxiliary variable c:
p(zd,n | . . .) p(zd,n | , , cd,n )p(wd,n | zd,n , )
(
Multinomial(zd,n ; )Multinomial(wd,n ; zd,n )
=
Multinomial(zd,n ; )Multinomial(wd,n ; zd,n )

cd,n = 1,
otherwise.

x, zd,n sampled computing conditional likelihood possible setting
within constant proportionality, sampling normalized multinomial.
Finally, sample auxiliary variable cd,n , indicates whether hidden topic zd,n
drawn . c depends prior hidden topic assignments z:
p(cd,n | . . .) p(cd,n | )p(zd,n | , , cd,n )
(
Bernoulli(cd,n ; )Multinomial(zd,n ; )
=
Bernoulli(cd,n ; )Multinomial(zd,n ; )

cd,n = 1,
otherwise.

Again, compute likelihood cd,n = 0 cd,n = 1 within constant proportionality,
sample normalized Bernoulli distribution.
Finally, model requires values fixed hyperparameters 0 , 0 , 0 , 0 , tuned
standard way based development set performance. Appendix C lists hyperparameters
values used domain experiments.
One main applications model predict properties supported documents
annotated keyphrases. test time, would compute posterior estimate
unannotated test document d. Since annotations present, property prediction
based text component model. estimate, use Gibbs sampling
procedure, restricted zd,n , stipulation cd,n fixed zero zd,n
always drawn . particular, treat language models known; accurately
integrate possible language models, use final 1000 samples language models
training opposed using point estimate. topic, probability exceeds
certain threshold, topic predicted. threshold tuned independently topic
development set. empirical results present Section 6 obtained manner.
584

fiL EARNING OCUMENT-L EVEL EMANTIC P ROPERTIES F REE -T EXT NNOTATIONS

Figure 6: Summary reviews movie Pirates Caribbean: Worlds End P R ECIS.
summary based 27 documents. list pros cons generated automatically using system described paper. generation numerical ratings
based algorithm described Snyder Barzilay (2007).

6. Evaluation Summarization Quality
model document analysis implemented P R ECIS,5 system performs single-
multi-document review summarization. goal P R ECIS provide users effective access
review data via mobile devices. P R ECIS contains information 49,490 products services
ranging childcare products restaurants movies. products, system
contains collection reviews downloaded consumer websites Epinions, CNET,
Amazon. P R ECIS compresses data product short list pros cons
supported majority reviews. example summary 27 reviews movie
Pirates Caribbean: Worlds End shown Figure 6. contrast traditional multidocument summarizers, output system sequence sentences, rather list
phrases indicative product properties. summarization format follows format pros/cons
summaries individual reviewers provide multiple consumer websites. Moreover, brevity
summary particularly suitable presenting small screens mobile
devices.
automatically generate combined pros/cons list product service, first apply
model review. model trained independently product domain (e.g., movies)
using corresponding subset reviews free-text annotations. annotations provide
set keyphrases contribute clusters associated product properties.
5. P R ECIS accessible http://groups.csail.mit.edu/rbg/projects/precis/.

585

fiB RANAVAN , C HEN , E ISENSTEIN , & BARZILAY

model trained, labels review set properties. Since set possible properties
reviews product, comparison among reviews straightforward
property, count number reviews support it, select property part
summary supported majority reviews. set semantic properties converted
pros/cons list presenting common keyphrase property.
aggregation technology applicable two scenarios. system applied unannotated reviews, inducing semantic properties document text; conforms traditional way learning-based systems applied unlabeled data. However, model
valuable even individual reviews include pros/cons keyphrase annotations. Due
high degree paraphrasing, direct comparison keyphrases challenging (see Section 3).
inferring clustering keyphrases, model permits comparison keyphrase annotations
semantic level.
remainder section provides set evaluations models ability capture
semantic content document text keyphrase annotations. Section 6.1 describes evaluation
systems ability extract meaningful semantic summaries individual documents,
assesses quality paraphrase structure induced model. Section 6.2 extends
evaluation systems ability summarize multiple review documents.
6.1 Single-Document Evaluation
First, evaluate model respect ability reproduce annotations present individual documents, based document text. compare wide variety baselines
variations model, demonstrating appropriateness approach task. addition,
explicitly evaluate quality paraphrase structure induced model comparing
gold standard clustering keyphrases provided expert annotators.
6.1.1 E XPERIMENTAL ETUP
section, describe datasets evaluation techniques used experiments
system automatic methods. comment hyperparameters tuned
model, sampling initialized.
Statistic
# reviews
avg. review length
avg. keyphrases / review

Restaurants
5735
786.3
3.42

Cell Phones
1112
1056.9
4.91

Digital Cameras
3971
1014.2
4.84

Table 3: Statistics datasets used evaluations
Data Sets evaluate system reviews three domains: restaurants, cell phones,
digital cameras. reviews downloaded Epinions website; used user-authored
pros cons associated reviews keyphrases (see Section 3). Statistics datasets
provided Table 3. domains, selected 50% documents training.
consider two strategies constructing test data. First, consider evaluating semantic
properties inferred system expert annotations semantic properties present
document. end, use expert annotations originally described Section 3 test

586

fiL EARNING OCUMENT-L EVEL EMANTIC P ROPERTIES F REE -T EXT NNOTATIONS

set;6 reiterate, annotations 170 reviews restaurant domain,
hold 50 development set. review texts annotated six properties according
standardized guidelines. strategy enforces consistency completeness ground truth
annotations, differentiating free-text annotations.
Unfortunately, ability evaluate expert annotations limited cost producing annotations. expand evaluation domains, use author-written keyphrase
annotations present original reviews. annotations noisy presence
property annotation document strong evidence document supports property,
inverse necessarily true. is, lack annotation necessarily imply
respective property hold e.g., review good service-related keyphrase may
still praise service body document.
experiments using free-text annotations, overcome pitfall restricting evaluation predictions individual properties documents annotated
property antonym. instance, evaluating prediction good service property,
select documents either annotated good service bad service-related
keyphrases.7 reason, semantic property evaluated unique subset documents. details development test sets presented Appendix A.
ensure free-text annotations reliably used evaluation, compare
results produced expert annotations whenever possible. shown Section 6.1.2, free-text
evaluations produce results cohere well obtained expert annotations, suggesting
labels used reasonable proxy expert annotation evaluations.
Evaluation Methods first evaluation leverages expert annotations described Section 3.
One complication expert annotations marked level semantic properties,
model makes predictions appropriateness individual keyphrases. address
representing expert annotation commonly-observed keyphrase
manually-annotated cluster keyphrases associated semantic property. example,
annotation semantic property good food represented common keyphrase realization, great food. evaluation checks whether keyphrase within clusters
keyphrases predicted model.
evaluation author free-text annotations similar evaluation expert
annotations. case, annotation takes form individual keyphrases rather semantic
properties. noted, author-generated keyphrases suffer inconsistency. obtain consistent
evaluation mapping author-generated keyphrase cluster keyphrases determined
expert annotator, selecting common keyphrase realization
cluster. example, author may use keyphrase tasty, maps semantic cluster
good food; select common keyphrase realization, great food. expert
evaluation, check whether keyphrase within clusters predicted model.
Model performance quantified using recall, precision, F-score. computed
standard manner, based models representative keyphrase predictions compared
corresponding references. Approximate randomization (Yeh, 2000; Noreen, 1989) used
statistical significance testing. test repeatedly performs random swaps individual results
6. expert annotations available http://groups.csail.mit.edu/rbg/code/precis/.
7. determination made mapping author keyphrases properties using expert-generated gold standard
clustering keyphrases. much cheaper produce expert clustering keyphrases obtain expert
annotations semantic properties every document.

587

fiB RANAVAN , C HEN , E ISENSTEIN , & BARZILAY

candidate system, checks whether resulting performance gap remains least
large. use test valid comparing nonlinear functions random variables, F-scores, unlike common methods sign test. Previous work
used test include evaluations Message Understanding Conference (Chinchor, Lewis, &
Hirschman, 1993; Chinchor, 1995); recently, Riezler Maxwell (2005) advocated
use evaluating machine translation systems.
Parameter Tuning Initialization improve models convergence rate, perform two
initialization steps Gibbs sampler. First, sampling done keyphrase clustering
component model, ignoring document text. Second, fix clustering sample
remaining model parameters. two steps run 5,000 iterations each. full joint model
sampled 100,000 iterations. Inspection parameter estimates confirms model convergence. 2GHz dual-core desktop machine, multithreaded C++ implementation model
training takes two hours dataset.
model needs provided number clusters K.8 set K large enough
model learn effectively development set. restaurant data set K 20. cell
phones digital cameras, K set 30 40, respectively. values tuned using
development set. However, found long K large enough accommodate significant number keyphrase clusters, additional account topics keyphrases,
specific value K affect models performance. hyperparameters
adjusted based development set performance, though tuning extensive.
previously mentioned, obtain document properties examining probability mass
topic distribution assigned property. probability threshold set property via
development set, optimizing maximum F-score.
6.1.2 R ESULTS
section, report performance model, comparing array increasingly
sophisticated baselines model variations. first demonstrate learning clustering annotation keyphrases crucial accurate semantic prediction. Next, investigate impact
paraphrasing quality model accuracy considering expert-generated gold standard clustering keyphrases another comparison point; consider alternative automatically computed
sources paraphrase information.
ease comparison, results experiments shown Table 5 Table 6,
summary baselines model variations Table 4.
Comparison Simple Baselines first evaluation compares model four nave
baselines. four treat keyphrases independent, ignoring latent paraphrase structure.
Random: keyphrase supported document probability one half.
results baseline computed expectation, rather actually run. baseline
expected recall 0.5, expectation select half correct
keyphrases. precision average proportion annotations test set
number possible annotations. is, test set size n properties, property
8. requirement could conceivably removed modeling cluster indices drawn Dirichlet
process prior.

588

fiL EARNING OCUMENT-L EVEL EMANTIC P ROPERTIES F REE -T EXT NNOTATIONS

Random

keyphrase supported document probability one half.

Keyphrase text

keyphrase supported document appears verbatim text.

Keyphrase classifier

separate support vector machine classifier trained keyphrase.
Positive examples documents labeled author
keyphrase; documents considered negative examples.
keyphrase supported document keyphrases classifier returns
positive prediction.

Heuristic keyphrase
classifier

Similar keyphrase classifier, except heuristic methods used attempt reduce noise training documents. Specifically wish
remove sentences discuss keyphrases positive examples.
heuristic removes positive examples sentences
word overlap given keyphrase.

Model cluster text

keyphrase supported document paraphrases appear
text. Paraphrasing based models keyphrase clusters.

Model cluster classifier

separate classifier trained cluster keyphrases. Positive examples documents labeled author keyphrase
cluster; documents negative examples. keyphrases
cluster supported document clusters classifier returns
positive prediction. Keyphrase clustering based model.

Heuristic model cluster
classifier

Similar model cluster classifier, except heuristic methods used reduce noise training documents. Specifically wish remove
positive examples sentences discuss keyphrases
clusters. heuristic removes positive examples sentences
word overlap keyphrases given cluster.
Keyphrase clustering based model.

Gold cluster model

variation model clustering keyphrases fixed
expert-created gold standard. text modeling parameters learned.

Gold cluster text

Similar model cluster text, except clustering keyphrases according expert-produced gold standard.

Gold cluster classifier

Similar model cluster classifier, except clustering keyphrases
according expert-produced gold standard.

Heuristic gold cluster
classifier

Similar heuristic model cluster classifier, except clustering
keyphrases according expert-produced gold standard.

Independent cluster model

variation model clustering keyphrases first learned
keyphrase similarity information only, separately text.
resulting independent clustering fixed text modeling parameters learned. variations key distinction full model
lack joint learning keyphrase clustering text topics.

Independent cluster text

Similar model cluster text, except clustering keyphrases
according independent clustering.

Independent cluster
classifier

Similar model cluster classifier, except clustering keyphrases
according independent clustering.

Heuristic independent
cluster classifier

Similar heuristic model cluster classifier, except clustering
keyphrases according independent clustering.

Table 4: summary baselines variations model compared.
589

fiB RANAVAN , C HEN , E ISENSTEIN , & BARZILAY

Method
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

model
Random
Keyphrase text
Keyphrase classifier
Heuristic keyphrase classifier
Model cluster text
Model cluster classifier
Heuristic model cluster classifier
Gold cluster model
Gold cluster text
Gold cluster classifier
Heuristic gold cluster classifier
Independent cluster model
Independent cluster text
Independent cluster classifier
Heuristic independent cluster classifier

Recall
0.920
0.500
0.048
0.769
0.839
0.227
0.721
0.731
0.936
0.339
0.693
1.000
0.745
0.220
0.586
0.592

Restaurants
Prec. F-score
0.353 0.510
0.346 0.409
0.500 0.087
0.353 0.484
0.340 0.484
0.385 0.286
0.402 0.516
0.366 0.488
0.344 0.502
0.360 0.349
0.366 0.479
0.326 0.492
0.363 0.488
0.340 0.266
0.384 0.464
0.386 0.468

Table 5: Comparison property predictions made model series baselines
model variations restaurant domain, evaluated expert semantic annotations.
results divided according experiment. methods model
significantly better results using approximate randomization indicated
p 0.05, p 0.1.

590

fiL EARNING OCUMENT-L EVEL EMANTIC P ROPERTIES F REE -T EXT NNOTATIONS

Method
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

model
Random
Keyphrase text
Keyphrase classif.
Heur. keyphr. classif.
Model cluster text
Model cluster classif.
Heur. model classif.
Gold cluster model
Gold cluster text
Gold cluster classif.
Heur. gold classif.
Indep. cluster model
Indep. cluster text
Indep. cluster classif.
Heur. indep. classif.

Recall
0.923
0.500
0.077
0.905
0.997
0.416
0.859
0.910
0.992
0.541
0.865
0.997
0.984
0.382
0.753
0.881

Restaurants
Prec. F-score
0.623 0.744
0.500 0.500
0.906 0.142
0.527 0.666
0.497 0.664
0.613 0.496
0.711 0.778
0.567 0.698
0.500 0.665
0.604 0.571
0.720 0.786
0.499 0.665
0.528 0.687
0.569 0.457
0.696 0.724
0.478 0.619

Recall
0.971
0.500
0.171
1.000
0.845
0.829
0.876
1.000
0.924
0.914
0.810
0.969
0.838
0.724
0.638
1.000

Cell Phones
Prec. F-score
0.537 0.692
0.489 0.494
0.529 0.259
0.500 0.667
0.474 0.607
0.547 0.659
0.561 0.684
0.464 0.634
0.561 0.698
0.497 0.644
0.559 0.661
0.468 0.631
0.564 0.674
0.481 0.578
0.472 0.543
0.464 0.634

Digital Cameras
Recall Prec. F-score
0.905 0.586 0.711
0.500 0.501 0.500
0.715 0.642 0.676
0.942 0.540 0.687
0.845 0.531 0.652
0.812 0.596 0.687
0.927 0.568 0.704
0.942 0.568 0.709
0.962 0.510 0.667
0.903 0.522 0.661
0.874 0.674 0.761
0.971 0.508 0.667
0.945 0.519 0.670
0.469 0.476 0.473
0.496 0.588 0.538
0.969 0.501 0.660

Table 6: Comparison property predictions made model series baselines
model variations three product domains, evaluated author free-text annotations. results divided according experiment. methods
model significantly better results using approximate randomization indicated
p 0.05, p 0.1. Methods perform significantly better
model p 0.05 indicated .

591

fiB RANAVAN , C HEN , E ISENSTEIN , & BARZILAY

P
ni
appears ni times, expected precision
i=1 mn . instance, restaurants
gold standard evaluation, six tested properties appeared total 249 times 120
documents, yielding expected precision 0.346.
Keyphrase text: keyphrase supported document appears verbatim
text. Precision high recall low, model unable detect
paraphrases keyphrase text. instance, first review Figure 1,
cleanliness would supported appears text; however, healthy would
supported, even though synonymous great nutrition appear.
Keyphrase classifier:9 separate discriminative classifier trained keyphrase. Positive examples documents labeled author keyphrase; documents considered negative examples. Consequently, particular keyphrase,
documents labeled synonymous keyphrases would among negative examples.
keyphrase supported document keyphrases classifier returns positive prediction.
use support vector machines, built using SVMlight (Joachims, 1999) features
model, i.e.,word counts.10 partially circumvent imbalanced positive/negative
data problem, tuned prediction thresholds development set maximize F-score,
manner tuned thresholds model.
Heuristic keyphrase classifier: baseline similar keyphrase classifier above, attempts mitigate noise inherent training data. Specifically, given
positive example document may contain text unrelated given keyphrase. attempt
reduce noise removing positive examples sentences word
overlap given keyphrase. keyphrase supported document keyphrases
classifier returns positive prediction.11
Lines 2-5 Tables 5 6 present results, using gold annotations original
authors annotations testing. model outperforms three baselines evaluations
strong statistical significance.
keyphrase text baseline fares poorly: F-score random baseline three
four evaluations. expected, recall baseline usually low requires
keyphrases appear verbatim text. precision somewhat better, presence
significant number false positives indicates presence keyphrase text
necessarily reliable indicator associated semantic property.
Interestingly, one domain keyphrase text perform well digital cameras.
believe prevalence specific technical terms keyphrases used
domain, zoom battery life. technical terms frequently used
review text, making recall keyphrase text substantially higher domain
evaluations.
9. Note classifier results reported initial publication (Branavan, Chen, Eisenstein, & Barzilay, 2008)
obtained using default parameters maximum entropy classifier. Tuning classifiers parameters allowed us
significantly improve performance classifier baselines.
10. general, SVMs additional advantage able incorporate arbitrary features, sake
comparison restrict using features across methods.
11. thank reviewer suggesting baseline.

592

fiL EARNING OCUMENT-L EVEL EMANTIC P ROPERTIES F REE -T EXT NNOTATIONS

keyphrase classifier baseline outperforms random keyphrase text baselines,
still achieves consistently lower performance model four evaluations. Notably,
performance heuristic keyphrase classifier worse keyphrase classifier except one case.
alludes difficulty removing noise inherent document text.
Overall, results indicate methods learn predict keyphrases without accounting intrinsic hidden structure insufficient optimal property prediction. leads us
toward extending present baselines clustering information.
important assess consistency evaluation based free-text annotations (Table 6) evaluation uses expert annotations (Table 5). absolute scores
expert annotations dataset lower scores free-text annotations, ordering performance various automatic methods across two evaluation scenarios.
consistency maintained rest experiments well, indicating purpose
relative comparison different automatic methods, method evaluating
free-text annotations reasonable proxy evaluation expert-generated annotations.
Comparison Clustering-based Approaches previous section demonstrates
model outperforms baselines account paraphrase structure keyphrases.
ask whether possible enhance baselines performance augmenting
keyphrase clustering induced model. Specifically, introduce three systems, none
true baselines, since use information inferred model.
Model cluster text: keyphrase supported document paraphrases
appears text. Paraphrasing based models clustering keyphrases.
use paraphrasing information enhances recall potential cost precision, depending
quality clustering. example, assuming healthy great nutrition
clustered together, presence healthy text would indicate support great
nutrition, vice versa.
Model cluster classifier: separate discriminative classifier trained cluster
keyphrases. Positive examples documents labeled author keyphrase
cluster; documents negative examples. keyphrases cluster
supported document clusters classifier returns positive prediction. Keyphrase
clustering based model. keyphrase classifier, use support vector machines trained word count features, tune prediction thresholds individual cluster development set.
Another perspective model cluster classifier augments simplistic text modeling
portion model discriminative classifier. Discriminative training often considered powerful equivalent generative approaches (McCallum et al., 2005),
leading us expect high level performance system.
Heuristic model cluster classifier: method similar model cluster classifier above,
additional heuristics used reduce noise inherent training data. Positive
example documents may contain text unrelated given cluster. reduce noise,
sentences word overlap clusters keyphrases removed.
keyphrases cluster supported document clusters classifier returns positive prediction. Keyphrase clustering based model.
593

fiB RANAVAN , C HEN , E ISENSTEIN , & BARZILAY

Lines 6-8 Tables 5 6 present results methods. expected, using clustering
keyphrases baseline methods substantially improves recall, low impact
precision. Model cluster text invariably outperforms keyphrase text recall keyphrase
text improved addition clustering information, though precision worse cases.
phenomenon holds even cameras domain, keyphrase text already performs well.
However, model still significantly outperforms model cluster text evaluations.
Adding clustering information classifier baseline results performance sometimes
better models. result surprising, model cluster classifier gains
benefit models robust clustering learning sophisticated classifier assigning
properties texts. resulting combined system complex model itself,
potential yield better performance. hand, using simple heuristic reduce
noise present training data consistently hurts performance classifier, possibly
due reduction amount training data.
Overall, enhanced performance methods, contrast keyphrase baselines,
aligned previous observations entailment research (Dagan, Glickman, & Magnini, 2006),
confirming paraphrasing information contributes greatly improved performance semantic
inference tasks.
Impact Paraphrasing Quality previous section demonstrates one central
claims paper: accounting paraphrase structure yields substantial improvements semantic inference using noisy keyphrase annotations. second key aspect research
idea clustering quality benefits tying clusters hidden topics document
text. evaluate claim comparing models clustering independent clustering
baseline. compare gold standard clustering produced expert human annotators. test impact clustering methods, substitute models inferred clustering
alternative examine resulting semantic inferences change. comparison
performed semantic inference mechanism model, well model cluster
text, model cluster classifier heuristic model cluster classifier baselines.
add gold standard clustering model, replace hidden variables correspond keyphrase clusters observed values set according gold standard clustering.12 parameters trained modeling text. model variation, gold
cluster model, predicts properties using inference mechanism original model.
baseline variations gold cluster text, gold cluster classifier heuristic gold cluster classifier
likewise derived substituting automatically computed clustering gold standard clusters.
additional clustering obtained using keyphrase similarity information. Specifically, modify original model learns keyphrase clustering isolation
text, learns property language models. framework, keyphrase clustering
entirely independent review text, text modeling learned keyphrase
clustering fixed. refer modification model independent cluster model.
model treats document text mixture latent topics, reminiscent models
supervised latent Dirichlet allocation (sLDA; Blei & McAuliffe, 2008), labels acquired
performing clustering across keyphrases preprocessing step. previous experiment, introduce three new baseline variations independent cluster text, independent cluster
classifier heuristic independent cluster classifier.
12. gold standard clustering created part evaluation procedure described Section 6.1.1.

594

fiL EARNING OCUMENT-L EVEL EMANTIC P ROPERTIES F REE -T EXT NNOTATIONS

Lines 9-16 Tables 5 6 present results experiments. gold cluster model
produces F-scores comparable original model, providing strong evidence clustering
induced model sufficient quality semantic inference. application expertgenerated clustering baselines (lines 10, 11 12) yields less consistent results, overall
evaluation provides little reason believe performance would substantially improved
obtaining clustering closer gold standard.
independent cluster model consistently reduces performance respect full joint
model, supporting hypothesis joint learning gives rise better prediction. independent
clustering baselines, independent cluster text, independent cluster classifier heuristic independent cluster classifier (lines 14 16), worse counterparts use model
clustering (lines 6 8). observation leads us conclude expert-annotated
clustering always improve results, independent clustering always degrades them.
supports view joint learning clustering text models important prerequisite
better property prediction.
Clustering
Model clusters
Independent clusters

Restaurants
0.914
0.892

Cell Phones
0.876
0.759

Digital Cameras
0.945
0.921

Table 7: Rand Index scores models clusters, learned keyphrases text jointly, compared clusters learned keyphrase similarity. Evaluation cluster quality
based gold standard clustering.

Another way assessing quality automatically-obtained keyphrase clustering
quantify similarity clustering produced expert annotators. purpose
use Rand Index (Rand, 1971), measure cluster similarity. measure varies zero
one, higher scores indicating greater similarity. Table 7 shows Rand Index scores
models full joint clustering, well clustering obtained independent cluster model.
every domain, joint inference produces overall clustering improves upon keyphrasesimilarity-only approach. scores confirm joint inference across keyphrases
document text produces better clustering considering features keyphrases alone.
6.2 Summarizing Multiple Reviews
last experiment examines multi-document summarization capability system.
study models ability aggregate properties across set reviews, compared baselines
aggregate directly using free-text annotations.
6.2.1 DATA E VALUATION
selected 50 restaurants, five user-written reviews restaurant. Ten annotators
asked annotate reviews five restaurants each, comprising 25 reviews per annotator.
used six salient properties annotation guidelines previous restaurant
annotation experiment (see Section 3). constructing ground truth, label properties
supported least three five reviews.

595

fiB RANAVAN , C HEN , E ISENSTEIN , & BARZILAY

Method
model
Keyphrase aggregation
Model cluster aggregation
Gold cluster aggregation
Indep. cluster aggregation

Recall
0.905
0.036
0.238
0.226
0.214

Prec.
0.325
0.750
0.870
0.826
0.720

F-score
0.478
0.068
0.374
0.355
0.330

Table 8: Comparison aggregated property predictions made model series
baselines use free-text annotations. methods model significantly better results using approximate randomization indicated p 0.05.

make property predictions set reviews model baselines
presented below. automatic methods, register prediction system judges
property supported least two five reviews.13 recall, precision, F-score
computed aggregate predictions, six salient properties marked annotators.
6.2.2 AGGREGATION PPROACHES
evaluation, run trained version model described Section 6.1.1. Note
keyphrases provided model, though provided baselines.
obvious baseline summarizing multiple reviews would directly aggregate
free-text keyphrases. annotations presumably representative reviews semantic
properties, unlike review text, keyphrases matched directly other. first
baseline applies notion directly:
Keyphrase aggregation: keyphrase supported restaurant least two five
reviews annotated verbatim keyphrase.
simple aggregation approach obvious downside requiring strict matching independently authored reviews. reason, consider extensions aggregation
approach allow annotation paraphrasing:
Model cluster aggregation: keyphrase supported restaurant least two
five reviews annotated keyphrase one paraphrases. Paraphrasing
according models inferred clustering.
Gold cluster aggregation: model cluster aggregation, using expert-generated
clustering paraphrasing.
Independent cluster aggregation: model cluster aggregation, using clustering
learned keyphrase similarity paraphrasing.
13. three corroborating reviews required, baseline systems produce positive predictions, leading
poor recall. Results setting presented Appendix B.

596

fiL EARNING OCUMENT-L EVEL EMANTIC P ROPERTIES F REE -T EXT NNOTATIONS

6.2.3 R ESULTS
Table 8 compares baselines model. model outperforms annotationbased baselines, despite access keyphrase annotations. Notably, keyphrase aggregation performs poorly, makes predictions, result requirement
exact keyphrase string match. before, inclusion keyphrase clusters improves performance baseline models. However, incompleteness keyphrase annotations (see
Section 3) explains recall scores still low compared model. incorporating
document text, model obtains dramatically improved recall, cost reduced precision,
ultimately yielding significantly improved F-score.
results demonstrate review summarization benefits greatly joint model
review text keyphrases. Nave approaches consider keyphrases yield inferior results,
even augmented paraphrase information.

7. Conclusions Future Work
paper, shown free-text keyphrase annotations provided novice users
leveraged training set document-level semantic inference. Free-text annotations
potential vastly expand set training data available developers semantic inference
systems; however, shown, suffer lack consistency completeness.
overcome problems inducing hidden structure semantic properties, correspond
clusters keyphrases hidden topics text. approach takes form
hierarchical Bayesian model, addresses text keyphrases jointly.
model implemented system successfully extracts semantic properties unannotated restaurant, cell phone, camera reviews, empirically validating approach. experiments demonstrate necessity handling paraphrase structure free-text keyphrase
annotations; moreover, show better paraphrase structure learned joint framework
models document text. approach outperforms competitive baselines semantic
property extraction single multiple documents. permits aggregation across
multiple keyphrases different surface forms multi-document summarization.
work extends actively growing literature document topic modeling. topic modeling paraphrasing posit hidden layer captures relationship disparate surface
forms: topic modeling, set latent distributions lexical items, paraphrasing
represented latent clustering phrases. show two latent structures linked,
resulting increased robustness semantic coherence.
see several avenues future work. First, model draws substantial power features measure keyphrase similarity. ability use arbitrary similarity metrics desirable;
however, representing individual similarity scores random variables compromise,
clearly independent. believe problem could avoided modeling generation
entire similarity matrix jointly.
related approach would treat similarity matrix across keyphrases indicator
covariance structure. model, would learn separate language models keyphrase,
keyphrases rated highly similar would constrained induce similar language
models. approach might possible Gaussian process framework (Rasmussen &
Williams, 2006).

597

fiB RANAVAN , C HEN , E ISENSTEIN , & BARZILAY

Currently focus model identify semantic properties expressed given
document, allows us produce summary properties. However, mentioned
Section 3, human authors give equal importance properties producing summary
pros cons. One possible extension work would explicitly model likelihood
topic annotated document. might avoid current post-processing step
uses property-specific thresholds compute final predictions model output.
Finally, assumed semantic properties unstructured. reality,
properties related interesting ways. Trivially, domain reviews would desirable
model antonyms explicitly, e.g., restaurant review simultaneously labeled
good bad food. relationships properties, hierarchical structures, could
considered. suggests possible connections correlated topic model Blei
Lafferty (2006).

Bibliographic Note
Portions work previously presented conference publication (Branavan et al., 2008).
current article extends work several ways, notably: development evaluation
multi-document review summarization system uses semantic properties induced
method (Section 6.2); detailed analysis distributional properties free-text annotations
(Section 3); expansion evaluation include additional domain sets baselines
considered original paper (Section 6.1.1).

Acknowledgments
authors acknowledge support National Science Foundation (NSF) CAREER grant IIS0448168, Microsoft Research New Faculty Fellowship, U.S. Office Naval Research
(ONR), Quanta Computer, Nokia Corporation. Harr Chen supported National Defense Science Engineering NSF Graduate Fellowships. Thanks Michael Collins, Zoran
Dzunic, Amir Globerson, Aria Haghighi, Dina Katabi, Kristian Kersting, Terry Koo, Yoong Keok
Lee, Brian Milch, Tahira Naseem, Dan Roy, Christina Sauper, Benjamin Snyder, Luke Zettlemoyer,
journal reviewers helpful comments suggestions. thank Marcia Davidson
members NLP group MIT help expert annotations. opinions, findings,
conclusions recommendations expressed article authors, necessarily reflect views NSF, Microsoft, ONR, Quanta, Nokia.

598

fiL EARNING OCUMENT-L EVEL EMANTIC P ROPERTIES F REE -T EXT NNOTATIONS

Appendix A. Development Test Set Statistics
Table 9 lists semantic properties domain number documents used
evaluating properties. noted Section 6.1.1, gold standard evaluation
complete, testing every property document. Conversely, free-text evaluations
property use documents annotated property antonym
number documents differs semantic property.
Domain
Restaurants (gold)
Restaurants

Cell Phones

Cameras

Property
properties
Good food
Bad food
Good price
Bad price
Good service
Bad service
Good reception
Bad reception
Good battery life
Poor battery life
Good price
Bad price
Small
Large
Good price
Bad price
Good battery life
Poor battery life
Great zoom
Limited zoom

Development documents
50

Test Documents
120

88

179

31

66

69

140

33

67

59

120

28

57

84

168

56

113

51

102

34

69

Table 9: Breakdown property development test sets used evaluations section 6.1.2.

599

fiB RANAVAN , C HEN , E ISENSTEIN , & BARZILAY

Appendix B. Additional Multiple Review Summarization Results
Table 10 lists results multi-document experiment, variation aggregation
require automatic method predict property three five reviews predict
property product, rather two presented Section 6.2. baseline systems,
change causes precipitous drop recall, leading F-score results substantially worse
presented Section 6.2.3. contrast, F-score model consistent across
evaluations.
Method
model
Keyphrase aggregation
Model cluster aggregation
Gold cluster aggregation
Indep. cluster aggregation

Recall
0.726
0.000
0.024
0.036
0.036

Prec.
0.365
0.000
1.000
1.000
1.000

F-score
0.486
0.000
0.047
0.068
0.068

Table 10: Comparison aggregated property predictions made model series
baselines use free-text annotations. Aggregation requires three five reviews
predict property, rather two Section 6.2. methods
model significantly better results using approximate randomization indicated
p 0.05.

Appendix C. Hyperparameter Settings
Table 11 lists values hyperparameters 0 , 0 , 0 used experiments domain.
values arrived tuning development set. cases, 0 set
(1, 1), making Beta(0 ) uniform distribution.
Hyperparameters
0
0
0

Restaurants
0.0001
0.001
0.001

Cell Phones
0.0001
0.0001
0.0001

Cameras
0.0001
0.1
0.001

Table 11: Values hyperparameters used domain across experiments.

600

fiL EARNING OCUMENT-L EVEL EMANTIC P ROPERTIES F REE -T EXT NNOTATIONS

References
Barzilay, R., McKeown, K., & Elhadad, M. (1999). Information fusion context multidocument summarization. Proceedings ACL, pp. 550557.
Barzilay, R., & McKeown, K. R. (2001). Extracting paraphrases parallel corpus. Proceedings ACL, pp. 5057.
Bhattacharya, I., & Getoor, L. (2006). latent Dirichlet model unsupervised entity resolution.
Proceedings SIAM International Conference Data Mining.
Blei, D. M., & Lafferty, J. D. (2006). Correlated Topic Models. Advances NIPS, pp. 147154.
Blei, D. M., & McAuliffe, J. (2008). Supervised topic models. Advances NIPS, pp. 121128.
Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent Dirichlet allocation. Journal Machine
Learning Research, 3, 9931022.
Boyd-Graber, J., Blei, D., & Zhu, X. (2007). topic model word sense disambiguation.
Proceedings EMNLP, pp. 10241033.
Branavan, S. R. K., Chen, H., Eisenstein, J., & Barzilay, R. (2008). Learning document-level semantic properties free-text annotations. Proceedings ACL, pp. 263271.
Carbonell, J., & Goldstein, J. (1998). use MMR, diversity-based reranking reordering
documents producing summaries. Proceedings ACM SIGIR, pp. 335336.
Chinchor, N. (1995). Statistical significance MUC-6 results. Proceedings 6th Conference
Message Understanding, pp. 3943.
Chinchor, N., Lewis, D. D., & Hirschman, L. (1993). Evaluating message understanding systems:
analysis third message understanding conference (MUC-3). Computational Linguistics, 19(3), 409449.
Cohen, J. (1960). coefficient agreement nominal scales. Educational Psychological
Measurement, 20(1), 3746.
Dagan, I., Glickman, O., & Magnini, B. (2006). PASCAL recognising textual entailment challenge. Lecture Notes Computer Science, 3944, 177190.
Elhadad, N., & McKeown, K. R. (2001). Towards generating patient specific summaries medical
articles. Proceedings NAACL Workshop Automatic Summarization, pp. 3240.
Finkel, J. R., Grenager, T., & Manning, C. (2005). Incorporating non-local information information extraction systems Gibbs sampling. Proceedings ACL, pp. 363370.
Gelman, A., Carlin, J. B., Stern, H. S., & Rubin, D. B. (2004). Bayesian Data Analysis (2nd edition).
Texts Statistical Science. Chapman & Hall/CRC.
Goldwater, S., Griffiths, T. L., & Johnson, M. (2006). Contextual dependencies unsupervised
word segmentation. Proceedings ACL, pp. 673680.
Hu, M., & Liu, B. (2004). Mining summarizing customer reviews. Proceedings SIGKDD,
pp. 168177.
Joachims, T. (1999). Making Large-Scale Support Vector Machine Learning Practical, pp. 169184.
MIT Press.

601

fiB RANAVAN , C HEN , E ISENSTEIN , & BARZILAY

Kim, S.-M., & Hovy, E. (2006). Automatic identification pro con reasons online reviews.
Proceedings COLING/ACL, pp. 483490.
Lin, D., & Pantel, P. (2001). Discovery inference rules question-answering. Natural Language
Engineering, 7(4), 343360.
Liu, B., Hu, M., & Cheng, J. (2005). Opinion observer: Analyzing comparing opinions
web. Proceedings WWW, pp. 342351.
Lu, Y., & Zhai, C. (2008). Opinion integration semi-supervised topic modeling. Proceedings WWW, pp. 121130.
Mani, I., & Bloedorn, E. (1997). Multi-document summarization graph search matching.
Proceedings AAAI, pp. 622628.
Marsi, E., & Krahmer, E. (2005). Explorations sentence fusion. Proceedings European
Workshop Natural Language Generation, pp. 109117.
McCallum, A., Bellare, K., & Pereira, F. (2005). conditional random field discriminativelytrained finite-state string edit distance. Proceedings UAI, pp. 388395.
Nenkova, A., Vanderwende, L., & McKeown, K. (2006). compositional context sensitive multidocument summarizer: exploring factors influence summarization. Proceedings
SIGIR, pp. 573580.
Noreen, E. (1989). Computer-Intensive Methods Testing Hypotheses: Introduction. John
Wiley Sons.
Popescu, A.-M., Nguyen, B., & Etzioni, O. (2005). OPINE: Extracting product features opinions reviews. Proceedings HLT/EMNLP, pp. 339346.
Purver, M., Kording, K. P., Griffiths, T. L., & Tenenbaum, J. B. (2006). Unsupervised topic modelling multi-party spoken discourse. Proceedings COLING/ACL, pp. 1724.
Radev, D., Jing, H., & Budzikowska, M. (2000). Centroid-based summarization multiple documents: Sentence extraction, utility-based evaluation user studies. Proceedings
ANLP/NAACL Summarization Workshop.
Radev, D., & McKeown, K. (1998). Generating natural language summaries multiple on-line
sources. Computational Linguistics, 24(3), 469500.
Rand, W. M. (1971). Objective criteria evaluation clustering methods. Journal
American Statistical Association, 66(336), 846850.
Rasmussen, C. E., & Williams, C. K. I. (2006). Gaussian Processes Machine Learning. MIT
Press.
Riezler, S., & Maxwell, J. T. (2005). pitfalls automatic evaluation significance
testing MT. Proceedings ACL Workshop Intrinsic Extrinsic Evaluation
Measures Machine Translation and/or Summarization, pp. 5764.
Snyder, B., & Barzilay, R. (2007). Multiple aspect ranking using good grief algorithm.
Proceedings NAACL/HLT, pp. 300307.
Titov, I., & McDonald, R. (2008a). joint model text aspect ratings sentiment summarization. Proceedings ACL, pp. 308316.

602

fiL EARNING OCUMENT-L EVEL EMANTIC P ROPERTIES F REE -T EXT NNOTATIONS

Titov, I., & McDonald, R. (2008b). Modeling online reviews multi-grain topic models.
Proceedings WWW, pp. 111120.
Toutanova, K., & Johnson, M. (2008). Bayesian LDA-based model semi-supervised part-ofspeech tagging. Advances NIPS, pp. 15211528.
White, M., Korelsky, T., Cardie, C., Ng, V., Pierce, D., & Wagstaff, K. (2001). Multi-document
summarization via information extraction. Proceedings HLT, pp. 17.
Yeh, A. (2000). accurate tests statistical significance result differences. Proceedings COLING, pp. 947953.
Zaenen, A. (2006). Mark-up barking wrong tree. Computational Linguistics, 32(4), 577580.

603


