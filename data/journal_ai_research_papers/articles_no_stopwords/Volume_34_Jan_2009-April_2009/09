Journal Artificial Intelligence Research 34 (2009) 339-389

Submitted 06/08; published 03/09

Identification Pleonastic Using Web
Yifan Li
Petr Musilek
Marek Reformat
Loren Wyard-Scott

yifan@ece.ualberta.ca
musilek@ece.ualberta.ca
reform@ece.ualberta.ca
wyard@ece.ualberta.ca

Department Electrical Computer Engineering
University Alberta
Edmonton, AB T6G 2V4 Canada

Abstract
significant minority cases, certain pronouns, especially pronoun it,
used without referring specific entity. phenomenon pleonastic pronoun
usage poses serious problems systems aiming even shallow understanding natural
language texts. paper, novel approach proposed identify uses it:
extrapositional cases identified using series queries web, cleft
cases identified using simple set syntactic rules. system evaluated four
sets news articles containing 679 extrapositional cases well 78 cleft constructs.
identification results comparable obtained human efforts.

1. Introduction
Anaphora resolution, associates word phrase (the anaphor) previously
mentioned entity (the antecedent), active field Natural Language Processing (NLP)
research. important role many applications non-trivial level understanding natural language texts desired, notably information extraction
machine translation. illustrate, information extraction system trying keep track
corporate activities may find dealing news Microsoft today announced
adopting XML default file format next major version Microsoft
Office software . . . would impossible provide insight Microsofts
intention without associating pronominal anaphors antecedent,
Microsoft.
Adding already complex problem finding correct antecedent, pronouns
always used fashion shown earlier example. well-known
pronouns, especially it, occur without referring nominal antecedent,
antecedent all. Pronouns used without antecedent, often referred
pleonastic structural, pose serious problem anaphora resolution systems. Many
anaphora resolution systems underestimate issue choose implement specific
module handle pleonastic pronouns instead input sanitized manually
exclude cases. However, high frequency pronoun usage general pleonastic
cases particular warrants phenomenon deserves serious treatment.
pronoun it, accounts pleonastic pronoun usages, far
frequently used pronouns British National Corpus (BNC). Wall Street
Journal Corpus (WSJ; Marcus, Marcinkiewicz, & Santorini, 1993), upon study
2009 AI Access Foundation. rights reserved.

fiLi, Musilek, Reformat, & Wyard-Scott

based, accounts 30% personal pronoun usage. percentage
cases lacks nominal antecedent significant: previous studies reported
figures 16% 50% (Gundel, Hedberg, & Zacharski, 2005) analysis
based upon WSJ corpus results value around 25%, half
pleonastic cases.
Applying criteria similar established Gundel et al. (2005), usage
generally categorized follows. instances analyzed shown
italics; corresponding antecedents, extraposed clauses, clefted constituents
marked underlining.
1. Referential nominal antecedent
[0006:002]1 thrift holding company said expects obtain regulatory approval
complete transaction year-end.
refers thrift holding company.
2. Referential clause antecedent
[0041:029] board insurance company financial problems,
insists made secret it.
refers fact person board insurance company.
[0102:002-003] Everyone agrees nations old bridges need
repaired replaced. theres disagreement it.
it, together do, refers action repairing replacing bridge.
3. antecedent Pleonastic
(a) Extraposition
[0034:020] doesnt take much get burned.
infinitive clause get burned extraposed original position
filled expletive it. equivalent non-extraposed sentence get
burned doesnt take much.
[0037:034] shame meeting never took place.
equivalent non-extraposed sentence meeting never took place
shame.
(b) Cleft2
[0044:026] disturbing, educators, students, blamed
much wrongdoing.
equivalent non-cleft version disturbing, educators, students,
blamed much wrongdoing.
1

example sentences selected WSJ corpus, locations encoded format [article:sentence].
2
claim cleft pronouns classified expletive (Gundel, 1977; Hedberg, 2000).
Nevertheless, change fact pronouns nominal antecedents; hence clefts
included analysis.

340

fiIdentification Pleonastic Using Web

[0591:021] partly reason exchange last week began trading
stock basket product . . .
equivalent non-cleft version exchange last week began trading
stock basket product partly reason.
(c) Local Situation
[0207:037] unpleasant evening . . .
category consists instances related weather, time, distance,
information local situation. Since texts reviewed study lack
instances subtypes, weather time cases discussed.
4. Idiomatic
[0010:010] governor couldnt make it, lieutenant governor welcomed
special guests.
paper focuses pleonastic cases (the third category), subclass carries
unique syntactic and/or semantic signatures. idiomatic category, consisting
non-anaphoric cases well, less coherent identification much subjective
nature, making less attractive target.
paper organized follows: Section 2 provides brief survey related work
toward classification identification pleonastic it; Section 3 proposes
web-based approach identification pleonastic it; Section 4 demonstrates proposed
method case study; Section 5 follows evaluation; finally, Section 6 discusses
findings presents ideas future work.

2. Previous Work
Evans (2001) pointed out, usage covered serious surveys English
grammar, (e.g. Sinclair, 1995) provide classifications based semantic
categories. recent study, Gundel et al. (2005) classify third-person personal pronouns
following comprehensive hierarchy:
Noun phrase (NP) antecedent
Inferrable
Non-NP antecedent
Fact
Situation

Proposition
Reason

Activity

Event

Full cleft
Atmospheric

Truncated cleft
pleonastic

Pleonastic
Full extraposition
Truncated extraposition
Idiom
Exophoric
Indeterminate
341

fiLi, Musilek, Reformat, & Wyard-Scott

Without going details category, apparent length
list phenomenon pleonastic it, generally pronouns without explicit
nominal antecedents, painstakingly studied linguists. However, despite
identified one open issues anaphora resolution (Mitkov, 2001), work automatic
identification pleonastic relatively scarce. date, existing studies area fall
one two categories: one wherein rule-based approach used, using
machine-learning approach.

2.1 Rule-based Approaches
Paice Husk (1987) together Lappin Leass (1994) provide examples rulebased systems make use predefined syntactic patterns word lists. Paice
Husk approach employs bracketing patterns . . . . . . meet
syntactic restrictions extraposition cleft. matched portions sentences
evaluated rules represented word lists. example, . . . rule
prescribes one task status words, good bad, must present amid
construct. order reduce false positives, general restrictions applied sentence
features construct length intervening punctuation.
Lappin Leasss (1994) approach employs set detailed rules
Modaladj Cogv-ed S, Modaladj Cogv predefined
lists modal adjectives (e.g. good useful ) cognitive verbs (e.g. think believe),
respectively. Compared Paice Husks (1987) approach, method much
restrictive, especially rigidly-specified grammatical constraints. example,
clear original Lappin Leass paper whether system would able
recognize sentences [0146:014] isnt clear, however, whether . . . despite claim
system takes syntactic variants consideration.
Lappin Leasss (1994) approach part larger system, evaluation
provided. Paice Husk (1987) approach, hand, evaluates impressively.
accuracy 93.9% determining pleonastic constructs data used
rule development, without using part-of-speech tagging parsing.
rule-based systems rely patterns represent syntactic constraints word
lists represent semantic constraints. makes relatively easy implement
maintain. However, features make less scalable challenged
large unfamiliar corpora, accuracies deteriorate. example, Paice Husk
(1987) noticed nearly 10% decrease accuracy rules developed using one subset
corpus applied another subset without modifications. Boyd, Gegg-Harrison,
Byron (2005) observed significant performance penalty approach
applied different corpus. words, rule-based systems good
designed be. Denber (1998) suggested using WordNet (Fellbaum, 1998) extend
word lists, doubtful helpful would considering enormous number
possible words included existing lists number inapplicable words
identified approach.
342

fiIdentification Pleonastic Using Web

2.2 Machine-learning Approaches
Recent years seen shift toward machine-learning approaches, shed new light
issue. Studies Evans (2001, 2000) Boyd et al. (2005) examples class.
systems employ memory-based learning grammatical feature vectors; Boyd et al.s
approach includes decision tree algorithm produces less ideal results.
attempt place uses seven categories, including pleonastic nominal anaphoric
among others, Evans uses 35 features encode information position/proximity,
lemmas, part-of-speech, related pronoun components interest,
words noun phrases, sentence. Evans reported 73.38% precision
69.25% recall binary classification pleonastic cases, overall binary classification
accuracy 71.48%. later study featuring MARS3 , fully automatic pronoun resolution
system employs approach, Mitkov, Evans, Orasan (2002) reported
significantly higher binary classification accuracy 85.54% approach applied
technical manuals.
Boyd et al.s (2005) approach targets pleonastic alone. uses 25 features,
concern lengths specific syntactic structures; included part-of-speech
information lemmas verbs. study reports overall precision 82% recall
71%, and, specifically, recalls extrapositional cleft constructs 81%
45%, respectively.
addition, Clemente, Torisawa, Satou (2004) used support vector machines
feature-set similar proposed Evans (2001) analyze biological medical texts,
reported overall accuracy 92.7% higher memory-based
learning implementation. Ng Cardie (2002) built decision tree binary anaphoricity
classification types noun phrases (including pronouns) using C4.5 induction
algorithm. Ng Cardie reported overall accuracies 86.1% 84.0% MUC-6
MUC-7 data sets. Categorical results, however, reported possible
determine systems performance pronouns. Using automatically induced rules,
Muller (2006) reported overall accuracy 79.6% detecting non-referential
spoken dialogs. inter-annotator agreement study conducted paper indicates
difficult even humans classify instances spoken dialogs. finding
supported experiences.
Machine-learning approaches able partly circumvent restrictions imposed
fixed word lists rigid grammatical patterns learning. However, advantage
comes price training required initial development phase
different corpora re-training preferable since lemmas part feature sets. Since
existing approaches fall within area supervised learning (i.e. training data need
manually classified), limited number lemmas gather training may
lead degraded performance unfamiliar circumstances. Moreover, features used
learning unable reliably capture subtleties original sentences, especially considering non-technical documents. example, quantitative features
frequently used machine-learning approaches, position distance, become less
reliable sentences contain large number adjuncts. Additionally, meanings
lemmas often domain-dependent vary local structural lexical
3

Available online http://clg.wlv.ac.uk/demos/MARS/

343

fiLi, Musilek, Reformat, & Wyard-Scott

environment nuances cannot captured lemma features alone. short,
machine-learning approaches generally deliver better performance classifying
rule-based counterparts do, inherent problems.

3. Web Based Approach
syntactic patterns semantics various clause constituents play important roles
determining third-person personal pronoun pleonastic. role grammar quite
obvious since extrapositions clefts must follow grammatical patterns
defined. example, commonly seen type it-extraposition follows
pattern:
+ copula + status + subordinate clause
[0089:017]

easy
see ancient art ropes.
contrast, role semantics plays little obscure one sits starts
dream exceptions (Paice & Husk, 1987) analogous [0074:005] . . . taken
measures continue shipments work stoppage. vis-a-vis [0367:044] . . . didnt
take rocket scientist change road bike mountain bike . . . , referential
pleonastic cases share syntactic structure. Despite less overt role, failure
process semantic information result severe degradation performance.
observation supported word-list-based systems dramatic decay accuracy
confronted text obtained word lists from.
every classification system, proposed system strives cover many cases
possible time perform classification accurately possible. achieve
this, attempts make good use syntactic semantic information embedded
sentences. set relaxed yet highly relevant syntactic patterns first applied
input text filter syntactically inviable cases. Unlike matching routines
previous approaches, process avoids detailed specification syntactic patterns. Instead,
tries include every piece text containing construct possible interest. Different
levels semantic examinations performed subtype pleonastic constructs.
reasons discussed later Section 3.2.2, semantic analysis performed clefts.
WordNet-based analysis used identify weather/time cases among
samples examined systems development stage, cases pertaining class
relatively uniform manner expression. complex populous class,
extrapositions, candidates subjected series tests performed queries
web. Results queries provide direct evidence specific configuration
clause constituents generally used.
reason corpus-based approach chosen versus applying manually constructed knowledge sources, word list WordNet, fourfold:
1. Manually constructed knowledge sources, regardless comprehensive are,
contain small portion general world knowledge. particular settings
study, general world knowledge used making judgements
words allowed serve matrix verb extraposition, even
subtle, specific sense word permitted.
344

fiIdentification Pleonastic Using Web

2. Manually compiled knowledge sources subject specific manners organization
may satisfy systems needs. Taking WordNet example, identifies
large number various relationships among entities, information mainly
organized along axes synonyms, hypernyms (kind-of relationship), holonyms
(part-of relationship) etc., surroundings particular word
interest study.
3. Natural languages evolving quickly. Taking English example, year new
words incorporated language4 rules grammar
immune changes either. Using large frequently-updated corpus
web allows system automatically adapt changes language.
4. importantly, corpora collect empirical evidence language usage.
sample size large enough, case web, statistics specific
construct generally used corpora employed indicator speakers
intention.
proposed approach inspired Hearsts (1992) work mining semantic
relationships using text patterns, many quests followed direction (Berland & Charniak, 1999; Poesio, Ishikawa, im Walde, & Vieira, 2002; Markert,
Nissim, & Modjeska, 2003; Cimiano, Schmidt-Thieme, Pivk, & Staab, 2005). Unlike
investigations focus semantic relationship among noun phrases, pleonastic
pronoun identification problem mandates complex queries built according
original sentences. However, binary nature problem makes simpler
apply comparative analysis results multiple queries, which, turn, leads better
immunity noise.
Figure 1 illustrates general work flow proposed system. sentence first
preprocessed obtain dependency tree part-of-speech tags, passed
syntactic filtering component determine whether minimum grammatical requirements pleonastic constructs met. syntactic filtering
process clefts weather/time expressions identified using syntactic cues
WordNet respectively. candidate extrapositions thereafter used instantiate
various queries search engines; results returned queries serve parameters
final decision-making mechanism.
3.1 Preprocessing
preprocessing component transforms syntactic information embedded natural
language texts machine-understandable structures. preprocessing stage,
word assigned part-of-speech tag, whole sentence parsed using dependency grammar (DG) parser. simplicitys sake, current system designed use
WSJ corpus, already tagged parsed context-free grammar (CFG). head
percolation table similar proposed Collins (1999) used obtain head component phrase. rest phrase constituents rearranged
4
Metcalf Barnhart (1999) compiled chronicle many important additions vocabulary
American English.

345

fiLi, Musilek, Reformat, & Wyard-Scott

Figure 1: Illustration system work flow broken three processing stages preprocessing, syntactic filtering, web-based analysis.

head component form dependency tree using procedure detailed Xia Palmer
(2001). Figure 2 illustrates syntactic structure sentence WSJ corpus.
original CFG parse tree derived dependency structure shown side-by-side.
Head entities underlined CFG diagram circled DG diagram.

Figure 2: Illustration sentences syntactic structure, annotated WSJ
corpus (left) head percolation (right).

346

fiIdentification Pleonastic Using Web

shown Figure 2, function tags (e.g. SBJ, TMP, CLR) tracing information
present context-free parse tree ported dependency tree.
real-world parsers usually produce tags. Except deliberate omission,
parse trees contain essentially information, presented different manners.
study, dependency structure preferred popular phrase structure
mainly explicit marking head components complementing/modifying relationships among various components. feature helpful
instantiating search-engine queries.
3.2 Syntactic Filtering
syntactic filtering process determines whether clause meets grammatical requirements extraposition cleft construct matching clause respective
syntactic patterns.
3.2.1 Extrapositions
It-extrapositions occur clause dislocated ordinary position replaced
it. it-extraposition usually follows pattern:
matrix clause
z

}|



noun phrase




adjective phrase

+



prepositional phrase
itsubject +







verb phrase

|

{z

matrix verb phrase

{









+ extraposed clause






}

(1)

pattern summarizes general characteristics subject it-extrapositions,
pronoun assumes subject position. matrix verb (the verb following it)
main copula be, serves equate associate subject ensuing logical
predicate, must followed either noun phrase, adjective phrase, prepositional
phrase.5 special requirement matrix verb phrase otherwise. Similarly,
almost restriction placed upon extraposed clause except full clause
either introduced without complementizer (e.g. [0037:034] shame
meeting never took place.) led that, whether, if, one wh-adverbs (e.g.
how, why, when, etc.). constraints developed generalizing small portion
WSJ corpus largely accordance patterns identified Kaltenbock
(2005). Compared patterns proposed Paice Husk (1987), cover
cases . . . , . . . . . . whether , allow broader range
candidates considering sentences explicitly marked (such [0037:034]).
configuration covers sentences as:
5
copula verbs receive treatment. arrangement made accommodate
cases verbs seem appear immediately followed extraposed clause.

347

fiLi, Musilek, Reformat, & Wyard-Scott

[0529:009] Since cost transporting gas important producers ability sell it, helps input access transportation
companies.
[0037:034] shame meeting never took place.
[0360:036] insulting demeaning say scientists needed new crises
generate new grants contracts . . .6
[0336:019] wont clear months whether price increase stick.
Except case last sentence, constructs generally overlooked
previous rule-based approaches identified Section 2.1. last sample sentence
illustrates, plus sign (+) pattern serves indicate forthcoming component
rather suggest two immediately adjacent components.
common grammatical variants pattern recognized system,
including questions (both direct indirect), inverted sentences, parenthetical expressions (Paice & Husk, 1987). expands patterns coverage sentences
as:
[0772:006] remembered hard outsider become accepted . . .
[0562:015] sooner vans hit road morning, easier
us fulfill obligation.
[0239:009] Americans seems followed Malcolm Forbess hot-air lead
taken ballooning heady way.
Aside subject matrix clause, extrapositional appear
object position. system described captures three flavors object extraposition.
first type consists instances followed object complement:
[0044:014] Mrs. Yeargin fired prosecuted unusual South
Carolina law makes crime breach test security.
case system inserts virtual copula object object
complement (a crime), making construct applicable pattern subject extraposition. example, underlined part prior example translates crime
breach test security.
two kinds object extraposition relatively rare:
Object verb (without object complement)
[0114:007] Speculation company asking $100 million operation said losing $20 million year . . .
Object preposition
[1286:054] see kids dont play truant . . .
cases cannot analyzed within framework subject extraposition thus
must approached different pattern:
verb + [preposition] object + full clause

(2)

6
Neither insulting demeaning Paice Husks (1987) list task status words therefore
cannot activate . . . pattern.

348

fiIdentification Pleonastic Using Web

current system requires full clauses start complementizer that.
restriction, however, included simplify implementation. Although object expositions common clauses led that, full clauses without leading
complementizer acceptable.
According Kaltenbocks (2005) analysis special cases noun phrases
appear extraposed component, amazing number theologians
sided Hitler. noted noun phrases semantically close subordinate
interrogative clauses therefore considered marginal case extraposition. However, cases found corpus annotation process
consequently excluded study.
3.2.2 Cleft
It-clefts governed slightly restricted grammatical pattern. Following Hedberg
(1990), it-clefts expressed follows:
subject + copula + clefted constituent + cleft clause

(3)

cleft clause must finite (i.e. full clause relative clause); clefted constituents restricted either noun phrases, clauses, prepositional phrases.7 Examples
sentences meeting constraints include:
[0296:029] total relationship important.
[0267:030] law school Mr. OKicki first wife
first seven daughters.
[0121:048] market goes down, figure paper profits Im losing.
addition, another non-canonical probably even marginal case identified
cleft:
[0296:037] really understand Filipinos feel passionately
involved father figure want dispose yet
need.
Text following structure sample, wh-adverb immediately precedes it,
captured using syntactic pattern appending virtual prepositional phrase
matrix copula (e.g. reason), missing information already
given.
examples represents possible syntactic construct it-clefts.
difficult tell second third cases apart respective extrapositional
counterparts, even difficult differentiate first case ordinary copula
sentence restrictive relative clause (RRC). example, following sentence,
[0062:012] precisely kind product thats created municipal landfill
monster, editors wrote.
slightly modified version,
[0062:012] kind product thats created municipal landfill monster, editors wrote.
7
Adjective adverb phrases possible relatively less frequent excluded
analysis.

349

fiLi, Musilek, Reformat, & Wyard-Scott

similar construction. However, latter considered cleft construct
first RRC construct. make things worse, pointed many (e.g. Boyd
et al., 2005, p.3, example 5), sometimes impossible make distinction without
resorting context sentence.
Fortunately, majority cases syntactic features, especially clefted
constituent, provide useful cues. it-cleft construct, cleft clause constitute head-modifier relationship clefted constituent, instead forms existential exhaustive presupposition8 (Davidse, 2000; Hedberg, 2000; Lambrecht, 2001).
example, figure paper profits Im losing. implies context something (and one thing) speaker going lose, associates paper
profits it. significant difference semantics often leaves visible traces syntactic layer, which, applicability proper nouns clefted constituents,
obvious. Others less obvious. system utilizes following grammatical cues
deciding construct it-cleft9 :
clefted constituent:
Proper nouns10 pronouns, cannot modified RRC;
Common nouns without determiner, generally refer kinds11 ;
Plurals, violate number agreement;
Noun phrases grounded demonstratives possessives,
modified RRCs, unambiguously identify instances, making unnecessary cases employ RRC;
Noun phrases grounded definite determiner the, modified
-preposition whose object noun phrase grounded
plural. constructs usually sufficient introducing uniquely identifiable entities (through association), thus precluding need additional RRC
modifiers. words kind, sort, likes considered exceptions
rule;
Adverbial constructs usually appear complements. example,
phrases denoting location (here, etc.) specific time (today, yesterday
etc.), clause led when;
Full clauses, gerunds, infinitives.
subordinate clause:
constructs appear awkward used RRC. example, one would
generally avoid using sentences place dirty,
8

applies canonical clefts, include class represented [0267:030].
construct considered it-cleft conditions met.
10
exceptional cases proper names used additional determiners RRC modifiers, John TV last night, c.f. Sloats (1969) account.
11
validity assertion debate (Krifka, 2003). Nevertheless, considering particular
syntactic setting discussion, highly unlikely bare noun phrases used denote specific
instances.
9

350

fiIdentification Pleonastic Using Web

better alternatives. current implementation two patterns considered
inappropriate RRCs, especially syntactic settings described Equation 3: A) subordinate verb phrase consists copula verb
adjective; B) subordinate verb phrase consists element
verb itself.
Combined:
clefted constituent prepositional phrase subordinate clause
full clause, case [0267:030], construct classified
cleft12 .
rules based heuristics may exceptions, making less
ideal guidelines. Moreover, mentioned earlier, cleft cases cannot told
apart RRCs grammatical means. However, experiments show rules
relatively accurate provide appropriate coverage, least WSJ corpus.
3.2.3 Additional Filters
Aside patterns described earlier sections, additional filters installed
eliminate semantically unfit constructs therefore reducing number trips
search engines. filtering rules follows:
clause identified subordinate clause subsequently processed
extraposition cleft, number commas, dashes colons clause
either zero one, rule adopted Paice Husks
(1987) proposal.
Except copula be, sentences matrix verbs appearing perfect tense
considered either extraposition cleft.
subject multiple verb phrases, sentence considered
either extraposition cleft.
Sentences noun phrase matrix logical predicate together subordinate
relative clause considered extraposition.
Sentences matrix verb preceded modal auxiliaries could would
subordinate clause led wh-adverb considered extraposition.
example, [0013:017] . . . could complete purchase next summer bid
one approved . . . considered extraposition.
Except first, rules optional deactivated case introduce
false-negatives.
3.3 Using Web Corpus
first question regarding using web corpus whether regarded
corpus all. Kilgarriff Grefenstette (2003) pointed out, following definition
12
case cleft, chances extraposition. assumption, therefore,
affect overall binary classification.

351

fiLi, Musilek, Reformat, & Wyard-Scott

corpus-hood corpus collection texts considered object language
literary study, answer yes. fundamental problem resolved, remains
find whether web effective tool NLP tasks.
corpus, web far well-balanced error-free. However,
one feature corpus even remotely comparable size. one
knows exactly big is, major search engines already indexes billions
pages. Indeed, web large sometimes misspelled word yield tens
thousands results (try word neglectible). sends mixed signal using
web corpus: good side, even relatively infrequent terms yield sizable results;
bad side, web introduces much noise manually-compiled corpora do.
Markert Nissims (2005) recent study evaluating different knowledge sources
anaphora resolution, web-based method achieves far higher recall ratio
BNC- WordNet-based, time yielding slightly lower precision.
Similar things said webs diverse unbalanced composition,
means used universal knowledge source one manage
get overwhelmed non-domain-specific information.
said, still hard overstate benefits web offers.
largest collection electronic texts natural language, hosts good portion
general world knowledge, stores information using syntax
defines language. addition, devoid systematic noise introduced
manually-constructed knowledge sources compilation process (e.g. failure
include less frequent items inflexible ways information organization). Overall, web
statistically reliable instrument analyzing various semantic relationships stored
natural languages means examples.
suggested Kilgarriff (2007) many others, technically difficult
exploit web use local corpus often dangerous rely solely
statistics provided commercial search engines. mainly due fact
commercial search engines designed corpus research. Worse, design
goals even impede uses. example, search engines skew order results using
number different factors order provide users best results. Combined
fact return results certain thresholds, making essentially
impossible get unbiased results. annoyances include unreliable result counts, lack
advanced search features13 , unwillingness provide unrestricted access
APIs. new search engine specifically designed corpus research available,
seems work around restrictions live rest.
3.4 Design Search Engine Queries
discussed previous sections, it-extrapositions cannot reliably identified using syntactic signatures alone combination synthetic knowledge bases. overcome
artificial limitations imposed knowledge sources, proposed system resorts web
necessary semantic information.
13
example, wildcard () feature Google, could immensely useful query construction, longer restricts results single words since 2003; Yahoos ability support alternate words
within quoted texts limited, MSN offer feature all.

352

fiIdentification Pleonastic Using Web

system employs three sets query patterns: what-cleft, comparative expletive test, missing-object construction. set provides unique perspective
sentence question. what-cleft pattern designed find sentence investigation valid what-cleft counterpart. Since it-extrapositions what-clefts
syntactically compatible (as shown Section 3.4.1) valid readings usually
obtained transformations one construct other, validity what-cleft
indicative whether original sentence extrapositional. comparative
expletive test patterns straightforward directly check whether instance
replaced entities cannot used expletively context
extrapositional it. alternate construct invalid, original sentence
determined expletive. third set patterns supplemental. intended
identifying relatively rare phenomenon missing-object construction,
may reliably handled previous pattern sets.
Designing appropriate query patterns important step efforts exploit
large corpora knowledge sources. complex queries web, especially
important suppress unwanted uses certain components, could result different word senses, different sentence configuration, speakers imperfect command
language. example, query shame could return valid extrapositional construct RRC shame perpetuated life;
query right could return valid what-clefts sentences
ought right . . . study employs three different approaches
curb unwanted results:
first important measure comparative analysis pairs similarlyconstructed queries sent search engine ratios result counts
used decision. method effective problems caused different
sentence configuration bad language usage, since generally neither contribute
fraction results large enough significantly affect ratio. method
provides normalized view web interest study
exactly frequently specific construct used, whether likely
carry specific semantic meaning used.
second measure use stubs query patterns, detailed following
sections. Stubs help ensure outcomes queries syntactically semantically similar original sentences partly resolve problems caused
word sense difference.
Finally, infeasible use comparative analysis, part query results
validated obtain estimated number valid results.
3.4.1 Query Pattern I: What-cleft
first query pattern,
+ verb phrase + copula + stub

(4)

what-(pseudo-)cleft construct encompasses matrix-level information found
it-extraposition. pattern obtained using three-step transformation illustrated
353

fiLi, Musilek, Reformat, & Wyard-Scott

below:

1)

2)

3)

+ verb phrase + clause

easy
see ancient art ropes. [0089:017]

clause
+
verb phrase
see ancient art ropes easy.

+ verb phrase + copula + clause
easy

see ancient art ropes.

+ verb phrase + copula + stub


easy

(5)

Step 1 transforms original sentence (or clause) corresponding non-extraposition
form removing pronoun restoring information canonical subjectverb-complement order. example, clause see . . . considered real
subject moved back canonical position. non-extraposition form subsequently converted step 2 what-cleft highlights verb phrase. Finally,
step 3, subordinate clause reduced stub enhance patterns coverage.
choice stub depends structure original subordinate clause: used
original subordinate clause infinitive, gerund, . . . infinitive construct14 .
rest cases, original complementizer, that, case
complementizer, used stub. use stub pattern imposes syntactic
constraint, addition ones prescribed pronoun copula is,
demands subordinate clause present query results. choice stubs reflects,
certain degree, semantics original texts therefore seen weak
semantic constraint.
examples what-cleft transformation:
[0059:014] remains unclear whether bond issue rolled over.
remains unclear whether
[0037:034] shame meeting never took place.
shame
what-cleft pattern identifies whether matrix verb phrase capable
functioning constituent it-extraposition. Information subordinate clauses
discarded construct used relatively infrequently adding extra restrictions
query prohibit yielding results many cases.
it-extraposition constructs appears . . . said . . .
valid non-extraposition counterpart, what-cleft versions often bear
certain degrees validity queries instantiated pattern often yield results
(albeit many) reputable sources. worth noting although input
output constructs transformation syntactically compatible,
necessarily equivalent terms givenness (whether information one sentence
14
According Hamawand (2003), . . . infinitive construct carries distinct semantics; reducing
infinitive alone changes function. However, exceptional cases, find reduction
generally acceptable. i.e. lost semantics affect judgment expletiveness.

354

fiIdentification Pleonastic Using Web

entailed previous discourse). Kaltenbock (2005) noted percentage
extrapositional constructs carrying new information varies greatly depending
category text. contrast, what-cleft generally expresses new information
subordinate clause. presupposed contents two constructs different, too.
What-clefts, according Gundel (1977), it-clefts derived,
existential exhaustive presuppositions carried it-cleft counterparts.
hand, it-extrapositions, semantically identical corresponding
non-extrapositions, lack presuppositions or, most, imply weaker strength
(Geurts & van der Sandt, 2004). discrepancies hint derived what-cleft
stronger expression original extraposition, may queries
instantiated pattern tend yield considerably less results.
Another potential problem pattern omission subordinate verb,
occasionally leads false positives. example, differentiate
helps input access transportation companies helps expand
horizon. deficiency accommodated additional query patterns.
3.4.2 Query Pattern II: Comparative Expletiveness Test
second group patterns provides simplified account original text
different flavors. execution, results individual queries compared assess
expletiveness subject pronoun. set patterns takes following general
form:
pronoun + verb phrase + simplified extraposed clause
(6)
difference among individual patterns lies choice matrix clause subject
pronoun: it, which, who, this, he. patterns instantiated submitted
search engine, number hits obtained version far outnumber
versions combined original text it-extraposition; otherwise
number hits least comparable. behavior reflects expletive nature
pronoun it-extraposition, renders sentence invalid replaced
pronouns pleonastic use.
simplified extraposed clause take different forms depending original
structure:
Original Structure
infinitive (to meet you)
. . . infinitive15 (for see document)
gerund (meeting you)
full clause led complementizer
(it shame meeting never took place)
full clause without complementizer
(it shame meeting never took place)

Simplified
infinitive + stub
infinitive + stub
gerund + stub
complementizer + stub
+ stub

Table 1: Simplification extraposed clause
15
. . . passive-infinitive transformed active voice (e.g. products sold sell
products).

355

fiLi, Musilek, Reformat, & Wyard-Scott

Similar case Pattern I, stub used syntactic constraint
semantic cue. Depending type search engine, stub either the,
widely used determiner, combination various determiners, personal
pronouns possessive pronouns, indicate subsequent noun phrase.
case infinitive construct involves subordinate clause led wh-adverb that,
complementizer used stub. arrangement guarantees results returned
query conform original text syntactically semantically. null value
used stubs object position original text lacks nominal object.
illustrate rules transformation, consider following sentence:
[0044:010] teacher said OK use notes test,
said.
relevant part sentence is:
+ verb phrase + clause

OK
use notes test
Applying clause simplification rules, first query obtained:
+ verb phrase + simplified clause

OK
use
second query generated simply replacing pronoun alternative
pronoun:
alternative pronoun + verb phrase + simplified clause

OK
use
Google reports 94,200 hits query, one page found using alternative
query. Since pronoun used much broader context, replacing
alone hardly makes balanced comparison. Instead, combination which, who, this,
used, illustrated following examples:
[0044:010] teacher said OK use notes test,
said.

)
(

ok use
which/who/this/he
[0089:017]
( easy see
) ancient art ropes.

easy see
which/who/this/he
special set patterns used object extrapositions16 accommodate unique
syntactic construct:
verb + [preposition] pronoun + + stub

(7)

Stubs chosen according rules main pattern set, however one
alternative pronoun used.
16
Instances containing object complements treated framework subject extraposition
included here.

356

fiIdentification Pleonastic Using Web

[0114:007] Speculation company asking $100 million
operation
said)to losing $20 million year . . .
(




3.4.3 Query Pattern III: Missing-object Construction
One search engine annoyance ignore punctuation marks. means one
search text matches specific pattern string, sentences
end pattern string. stubs used Pattern II generally helpful excluding sentences semantically incompatible original search
results. However, circumstances stub attached queries (where
query results ideally consist sentences end query string),
search engine may produce results needed. Sentences conforming pattern
+ copula + missing-object construction, (referring book) easy
read, present one situation. unique construction special
treatment needed missing-object construction usually it-extraposition
counterpart object present, example easy read book . Since
missing-object constructions virtually (only shorter) extrapositional counterparts, good chance identified extrapositions.
following additional examples missing-object construction:
[0290:025] non-violent civil disobedience centerpiece, rather
lawful demonstration may attract crime, difficult
justify.
[0018:024-025] price new shares set. Instead, companies
leave marketplace decide.
[0111:005] declined elaborate, say, seemed right
thing minute.
Two sets patterns proposed17 identify likes foregoing examples.
first pattern, compound adjective test, inspired Nannis (1980) study considering
easy-type adjective followed infinitive (also commonly termed tough construction)
single complex adjective. pattern takes form
stub + adjectivebase -to -verb

(8)

stub, serving limit outcome query noun phrases, takes combination determiners a/an alone; original adjective converted base form
adjectivebase comparative superlative form. Expanding Nannis original
claims, pattern used evaluate adjectives18 well constructs furnished
. . . infinitive complements. following example demonstrates patterns
usage:
17

Preliminary experiments confirmed effectiveness patterns. However, due sparseness
samples belonging class, included reported evaluation.
18
based observation compounds ready-to-fly (referring model aircrafts)
exist, hard obtain complete enumeration easy-type adjectives.

357

fiLi, Musilek, Reformat, & Wyard-Scott

[0258:024] machine uses single processor, makes easier program
competing machines using several processors.
easy-to-program
second set consists two patterns used comparative analysis
general profile:
+ verbgerund + stub
(9)
verbgerund gerund form original infinitive. complementizer
used sole purpose ensuring verbgerund appears subject subordinate
clause sentences returned queries. words, phrases computer
programming pattern matching excluded. first pattern, stub
combination prepositions (currently chosen); second one,
combination determiners alone used. example:
[0258:024] machine uses single processor, makes easier program
competing machines
using)several processors.
(
in|from
programming

set patterns tests transitivity verb semantic environment similar
original sentence. verb used transitively often, pattern
determiners yield results, vice versa. supported preceding
sample sentences, usually-transitive verb used without object19 good indicator
missing-object construction sentence diagnosed referential.
3.4.4 Query Instantiation
Patterns must instantiated information found original sentences
submitted search engine. Considering general design principles system,
advisable instantiate patterns original texts significantly reduces
queries coverage. Instead, object matrix verb phrase truncated
matrix verb expanded order obtain desired level coverage.
truncation process provides different renditions based structure original
object:
Adjective phrases:
head word used. head word modified too,
modifier retained order better support . . . construct
maintain compatibility semantics original text.
Common noun phrases:
possessive ending/pronoun, -preposition:
phrase replaced $PRPS$ plus head word. $PRPS$ either list
possessive pronouns one widely used, depending caliber
search engine used. example, location expanded |
| | | | | location.
19
omitted object preposition (e.g. difficult account for.) effect,
identifiable syntactic means alone.

358

fiIdentification Pleonastic Using Web

determiners:
phrase replaced choice $DTA$, $DTTS$, $DTTP$, combination
$DTA$ $DTTS$, plus head word. $DTA$ list (or one the)
general determiners (i.e. a, an, etc.). $DTTS$ refers combination
definite article singular demonstratives that. $DTTP$
plural counterpart $DTTS$. choice based configuration
original text maintain semantic compatibility.
without determiner:
head word used.
Proper nouns pronouns:
phrase replaced $PRP$, list (or one the) personal pronouns.
Prepositional phrases:
object preposition truncated recursive operation.
Numeric values:
phrase lot used instead.
Matrix verbs expanded include simple past tense third person singular present form aid WordNet generic patterns.
applicable, particles remain attached verb.
Generally speaking, truncation expansion good ways boosting patterns
coverage. However, current procedures truncation still crude, especially
handling complex phrases. example, phrase reckless course action
([0198:011]) yields $PRPS$ course, results total loss original semantics.
enhancements truncation process may improve performance
improvement likely limited due endless possibilities language usage
constraints imposed search engines.
Aside truncating expanding original texts, stepped-down version
Pattern II, denoted Pattern II0 , provided enhance systems coverage.
current scheme simply replace extraposed clause new stub
original extraposed clause infinitive, . . . infinitive, gerund construct.
example,
[0089:017]
( easy see
) ancient art ropes.

easy
which/who/this/he
situations, downgraded version applied.
3.5 Binary Classification It-extraposition
Five factors taken consideration determining whether sentence question
it-extraposition:
Estimated popularity what-cleft construct (query Pattern I)
denoted
W = nw vw
359

fiLi, Musilek, Reformat, & Wyard-Scott

nw number results reported search engine, vw
percentage valid instances within first batch snippets (usually 10, depending
search engine service) returned query. Validation performed
case-sensitive regular expression derived original query. Since whatcleft pattern capitalized beginning, regular expression looks
instances appearing beginning sentence. particularly important
validate results what-cleft queries search engines produce
results based interpretation original query. example, Google
returns pages containing Whats found query found that,
might helpful counterproductive purpose
study.
Result comparative expletiveness test (query Pattern II)
denoted
nX
r=
nit
nit number results obtained original version query,
nX total number results produced replacing pronouns
who. smaller ratio r is, likely sentence
investigated extraposition. Extrapositional sentences usually produce
r value 0.1 less. versions query yield insufficient results
(max(nit , nX ) < Nmin ), r takes value Rscarce = 1000. Since it-extrapositions
relatively rare, better assume sentence extrapositional
insufficient data judge otherwise. case nX sufficient
version query produces result (nX >= Nmin nit = 0), r takes
value Rzero = 100. Values Rzero Rscarce large numbers chosen arbitrarily,
mainly visualization purposes. words Rzero Rscarce hint
sentence probably extrapositional, however neither indicates degree
likelihood.
Result stepped-down
comparative expletiveness test
n0
0
0
denoted r0 = nX
0 , nit nX number results returned

version alternate version stepped-down queries (c.f. Section 3.4.4,
Page 359). stepped-down queries simplified versions queries used
calculate r. Due simplification, r0 usually sensitive extrapositions.
However queries stepped-down versions, case original queries
reused, causing r0 = r. Similar way r defined, r0 takes values
Rscarce Rzero special situations.
Synthesized expletiveness
new variable R defined based values r, nit , nX , r0 :
(

R=

r, max(nit , nX ) Nmin ,
r0 , max(nit , nX ) < Nmin .

original queries yield enough results, R takes value r since original
queries better preserve sentence context generally accurate. However,
360

fiIdentification Pleonastic Using Web

original queries fail, system resorts back-up method using
stepped-down queries bases judgement results instead. Overall, R
seen synthesized indicator subject pronoun generally used
similar syntactic semantic setting original sentence.
Syntactic structure sentence
denoted S, binary variable indicating sentence investigation belongs
syntactic construct prone generating false-positives. average
what-cleft queries yield fewer results less reliable since cannot
used provide comparative ratios. However, still useful last line
defence curb impacts certain syntactic constructs repeatedly cause
comparative expletive tests produce false-positives. Currently one construct
identified verb infinitive construct, helps input
everyone expects post results tomorrow . Therefore,
(

S=

TRUE, sentence matches verb infinitive,
FALSE, otherwise.

final binary classification it-extraposition, E, defined follows:
(

E=

((R < Rexp ) (W > Nmin )), = TRUE,
(R < Rexp ),
= FALSE.

(10)

Nmin Rexp , set 10 0.15 respectively study, threshold constants
chosen based upon empirical observations. words, system recognizes instance extrapositional unlikely (by comparing R Rexp ) alternative
pronoun used place syntactic semantic settings. verb
infinitive constructs, required sentence viable what-cleft variant
(by comparing W Nmin ).
worth noting todays major commercial search engines return exact
number results query rather estimates. negative effect
somewhat mitigated basing final decision ratios instead absolute numbers.

4. Case Study
better illustrate system work flow, two sample sentences selected WSJ
corpus taken whole process. first sample, [0231:015], classified
it-extraposition; other, [0331:033] (with preceding sentence providing context),
referential case nominal antecedent. particulars implementation
discussed here.
[0231:015] fund manager life-insurance company said three factors make
difficult read market direction.
[0331:032-033] recent report classifies stock hold. appears
sort hold one makes heading door.
361

fiLi, Musilek, Reformat, & Wyard-Scott

4.1 Syntactic Filtering
First, syntactic structures sentence identified dependencies among
constituents established, shown Figures 3 4.

Figure 3: Syntactic structure [0231:015] (fragment)

Figure 4: Syntactic structure [0331:033] (fragment). Readings B, indicated
DG parse tree, discussed text.

362

fiIdentification Pleonastic Using Web

sample sentence [0231:015], expletive appears object verb makes
followed object complement difficult, therefore virtual copula (tagged VBX)
created dependency tree order treat framework subject
it-extrapositions. [0331:033], two different readings produced one assuming
appears matrix verb (reading A, c.f. Figure 4), taking (reading
B). accomplished drilling chain verbs beginning parent
verb node. top chain, system starts recursive process
find verbs infinitives directly attached current node moves
newly found node. process interrupted current verb node furnished
elements verbal adverbial complements/modifiers.
filtering process, various components sentences identified, listed
Table 2.

Sentence
0231:015
0331:033
0331:033

Reading

B

Matrix
Verb
Object

difficult
appears

sort

Conjunction



Subordinate
Subject Verb
Object
read direction

sort
One

Table 2: Component breakdown case study samples

4.2 Pattern Instantiation
Using components identified Table 2, five queries generated reading,
listed Tables 3-5. Patterns II0 -it II0 -others refer stepped-down versions
(c.f. Section 3.4.4, Page 359) II-it II-others respectively. queries shown
generated specifically Google take advantage features available Google.
use alternative search engine Yahoo, component expansions determiner
lists turned off, separate queries need prepared individual pronouns.
order get accurate results, queries must enclosed double quotes
sent search engines.

Pattern

II-it
II-others
II0 -it
II0 -others

Query
is|was|s difficult is|was
is|was|s difficult read the|a|an|no|this|these|their|his|our
which|this|who|he is|was|s difficult read the|a|an|no|this|these|
their|his|our
is|was|s difficult
which|this|who|he is|was|s difficult
Table 3: Queries [0231:015]
363

Results
1060
3960
153
6.3 106
1.5 105

fiLi, Musilek, Reformat, & Wyard-Scott

Pattern

II-it
II-others
II0 -it
II0 -others

Query
appears|appeared is|was
appears|appeared the|a|an|no|this|these|their|his|our
which|this|who|he appears|appeared the|a|an|no|this|these|
their|his|our
appears|appeared
which|this|who|he appears|appeared

Results
44
7.5 104
3.2 105
2.2 106
2.6 106

Table 4: Queries [0331:033], Reading

Pattern

II-it
II-others
II0 -it
II0 -others

Query
is|was|s its|my|our|his|her|their|your sort is|was
is|was|s its|my|our|his|her|their|your sort the|a|an|no|
this|these|they|we|he|their|his|our
which|this|who|he is|was|s its|my|our|his|her|their|your sort
the|a|an|no|this|these|they|we|he|their|his|our
II-it
II-others

Results
0
0
0
0
0

Table 5: Queries [0331:033], Reading B
4.3 Query Results Classification
every reading, number results five queries (nw Pattern I;
nit II-it; nX II-others; n0it II0 -it; n0X II0 -others) obtained
search engine; first 10 results what-cleft query validated obtain
estimated percentage (vw ) valid constructs. W (= nw vw ), r(= nX /nit ), r0 (= n0X /n0it ),
R (choosing either r r0 depending whether max(nit , nX ) 10)
calculated accordingly, recorded Table 6.
Query
[0231:015]
[0331:033].A
[0331:033].B

nw
1060
44
0

vw
70%
0%
-

nit
3960
7.5E4
0

nX
153
3.2E5
0

n0it
6.3E6
2.2E6
0

n0X
1.5E5
2.6E6
0

W
742
0
0

r
0.04
4.3
1000

r0
0.02
1.2
1000

R
0.04
4.3
1000

Table 6: Query results case study sample sentences
appears suspicious vw set 0 reading [0331:033].A, means
valid instances found. quick look returned snippets reveals that, indeed, none
10 snippets queried contents beginning sentence. note
reading [0331:033].B, r r0 , consequently R set Rscarce = 1000
since query produced enough results.
decided Table 2 readings [0231:015] [0331:033].B bear
verb infinitive construct, hence = FALSE; [0331:033].A = TRUE.
Applying Equation 10 Section 3.5, [0231:015] [0331:033].B, final classification
364

fiIdentification Pleonastic Using Web

E based whether R sufficiently small (R < 0.15). [0331:033].A, system
needs check whether what-cleft query returned sufficient valid results (W > 10).
final classifications listed Table 7.
Sentence
[0231:015]
[0331:033]
[0331:033]

Reading

B

W
742
0
0


FALSE
TRUE
FALSE

R
0.04
4.3
1000

Ereading
YES



E
YES


Table 7: Final binary classification case study sample sentences
Since neither readings [0331:033] classified such, sentence it-extraposition
construct.

5. Evaluation
order provide comprehensive picture systems performance, twofold assessment used. first evaluation, system exposed sentence collection
assisted development. Accordingly, results obtained evaluation reflect,
certain degree, systems optimal performance. second evaluation aims revealing systems performance unfamiliar texts running developed system
random dataset drawn rest corpus. Two additional experiments
conducted provide estimation systems performance whole corpus.
Three performance measures used throughout section: precision, recall,
balanced F-measure (van Rijsbergen, 1979). Precision defined ratio correctly
classified instances specific category (or collection categories) number
instances identified system belonging category (categories). words,
P
precision calculated P = PT+F
P , P F P number true positives
false positives respectively. Recall defined ratio correctly classified instances
specific category (or collection categories) total number instances
P
category (categories), R = PT+F
N , F N denotes number false negatives.
Finally, F-measure weighted harmonic mean precision recall used indicate
systems overall performance. precision recall weighted equally, used
R
study, balanced F-measure defined F = P2P+R
.
Following Efron Tibshiranis (1993) Bootstrap method, 95% confidence intervals
obtained using 2.5th 97.5th percentiles bootstrap replicates
provided alongside system performance figures indicate reliability. number
replicates arbitrarily set B = 9999, much greater commonly
suggested value 1000 (e.g., see Davison & Hinkley, 1997; Efron & Tibshirani, 1993)
pleonastic instances sparse. case precision recall value 100%,
bootstrap percentile method reports interval 100%-100%, makes little sense.
Therefore, situation adjusted Wald interval (Agresti & Coull, 1998) presented
instead. two systems compared, approximate randomization test (Noreen,
1989) similar used Chinchor (1992) performed determine difference
statistical significance. significance level = 0.05 number shuffles R = 9999,
chosen arbitrarily, used significance tests performed.
365

fiLi, Musilek, Reformat, & Wyard-Scott

5.1 Development Dataset
purpose study, first 1000 occurrences WSJ corpus
manually annotated authors20 . part set inspected order
determine values constants specified Section 3.5, develop surface
structure processor. annotation process facilitated custom-designed utility
displays sentence within context represented nine-sentence window containing
six immediately preceding sentences, original, two sentences follow.
Post-annotation review indicates presentation corpus sentences worked well.
Except (less 0.5%) cases, authors found need resort broader
contexts understand sentence; circumstances valid antecedents
located outside context window antecedent found within it.
Category
Nominal Antecedent
Clause Antecedent
Extraposition
Cleft
Weather/Time
Idiom

Grand Total

Instances
756
60
118
13
9
18
26
1000

Percentage
75.60%
6.00%
11.80%
1.30%
0.90%
1.80%
2.60%
100.00%

Table 8: Profile development dataset according authors annotation
Table 8 summarizes distribution instances dataset according authors
consensus. category labeled consists mostly instances fit well
categories, e.g. identified nominal antecedent plural
antecedent inferred, well certain confusing instances. twenty-six instances,
two might remotely recognized one types interests study:
[0101:007] though size loan guarantees approved yesterday significant, recent experience similar program Central America
indicates could take several years new Polish government fully use aid effectively.
[0296:048] comic try pretend theyre still master race.
Neither instance identified anaphoric. However, first construct neither
valid non-extraposition version valid what-cleft version, making difficult justify
extraposition, second case considered refer atmosphere
aroused action detailed when-clause.
order assess whether pleonastic categories well-defined ability
ordinary language users identify pleonastic instances, two volunteers, native English
speakers, invited classify instances development dataset. help
concentrate pleonastic categories, volunteers required assign
instance one following categories: referential, extraposition, cleft, weather/time,
20

Annotations published online appendix http://www.ece.ualberta.ca/~musilek/pleo.

zip.

366

fiIdentification Pleonastic Using Web

idiom. referential category covers instances nominal antecedents
clause antecedents, well instances inferrable antecedents. Table 9 outlines
annotators performance reference authors consensus. degree agreement
annotators, measured kappa coefficient (; Cohen, 1960), given
table.
Category

Precision

Referential
99.38%
Extraposition
82.54%
Cleft
38.46%
Weather/Time 66.67%
Idiom
39.39%
Overall Accuracy/

Volunteer 1
Recall F-measure

95.49%
88.14%
76.92%
44.44%
72.22%
93.50%

97.40%
85.25%
51.28%
53.33%
50.98%

Precision

96.38%
88.68%
72.73%
75.00%
50.00%

Volunteer 2
Recall F-measure

98.10%
79.66%
61.54%
33.33%
61.11%
94.20%

97.23%
83.93%
66.67%
46.15%
55.00%


.749
.795
.369
-.005
.458
.702



Except Weather/Time category (p = 0.5619), values statistically significant p <
0.0001.

Table 9: Performance volunteer annotators development dataset (evaluated
using authors annotation reference) degree inter-annotator agreement measured Cohens kappa (). authors annotations refitted
simplified annotation scheme used volunteers.
many factors contributing apparently low values Table 9,
notably skewed distribution categories inappropriate communication
classification rules. Di Eugenio Glass (2004) others pointed out, skewed distribution categories negative effect value. Since distribution
instances dataset fairly unbalanced, commonly-accepted guideline
interpreting values ( > 0.67 > 0.8 thresholds tentative definite conclusions respectively; Krippendorff, 1980) may directly applicable case.
addition, classification rules communicated annotators orally examples not-so-common cases, object it-extrapositions, might
well understood annotators. Another interesting note results
strong tendency annotators (albeit different cases) classify
it-clefts it-extrapositions. Rather taking sign cleft category
well-defined, believe reflects inherent difficulties identifying instances pertaining
category.
5.2 Baselines
Two baselines available comparison WSJ annotation, done manually
provided corpus; results replication Paice Husks (1987)
algorithm (PHA). cautioned that, given subjectivity issues discussed
paper lack consensus certain topics field linguistics, recall ratios
presented baseline results forthcoming results proposed system
compared quantitatively. example, original Paice Husk algorithm
recognize certain types object extrapositions always distinguish
367

fiLi, Musilek, Reformat, & Wyard-Scott

individual types pleonastic it; WSJ corpus neither special annotation
parenthetical (c.f. Section 3.2.1, Page 348, [0239:009]) established annotation
policy certain types object extrapositions (Bies, Ferguson, Katz, & MacIntyre, 1995).
attempts made correct issues.
Table 10 summarizes performance baselines development dataset.
expected, Paice Husks (1987) algorithm perform well since WSJ
articles different from, tend sophisticated than, technical
essays algorithm designed for. Compared originally reported precision
93% recall 96%, replicated PHA yields 54% 75% respectively
development dataset. performance replica largely line Boyd et al.
(2005) obtained implementation algorithm different dataset.

Measurement
Reference
Identified Baseline
Baseline True Positives
Precision
Recall
F-measure

WSJ Annotation
Extraposition
Cleft
118
13
88
12
87b
12
98.86%
100%
73.73% 92.31%
84.47% 96.00%

Replicated PHA
Overalla
140
194
105
54.12%
75.00%
62.87%



Includes clefts, extrapositions, time/weather cases.
Based manual inspection, two cases originally annotated extrapositional WSJ
determined inappropriate. See discussions below.
b

Table 10: Performance baselines development dataset, evaluated
authors annotation.
31 (118 87) extrapositional cases annotated WSJ broken
following categories followed respective number instances:
Category
Unrecognized
Object without complement
Parenthetical
Inappropriate non-extraposition
Agentless passive
seems/appears . . .
worth . . .
Others
Valid non-extraposition
. . .
Others
Total

Items
3
1
2
18
9
4
2
3
10
2
8
31

Table 11: Profile false negatives WSJ annotation reference authors
annotation
368

fiIdentification Pleonastic Using Web

stating Characteristic extraposition final clause replace
it, Bies et al. (1995) define class narrowest sense. Since interpretation
definition entirely subjective matter, way determining real coverage
annotations. However, portions corpus reviewed,
practice annotation entirely consistent.
Two sentences marked extraposition corpus annotators consensus
indicates otherwise. Considering golden standard status WSJ corpus,
listed here:
[0277:040] Moreover, member Mitsubishi group, headed
one Japans largest banks, sure win favorable loan.
[0303:006] compromises convince Washingtons liberals
simply stay course, administration stray
course issues.
first sentence considered dubious likely referring company
member Mitsubishi group. second one considered cleft actually
marked cleft corpus. Since case corpus annotations,
extraposition marking considered mistake manually removed.
Paice Husk (1987) algorithm suffers false-positive . . . . . .
construct detection, may fixed incorporating part-of-speech phrase structure information together additional rules. However, fixes greatly complicate
original system.
5.3 Results
development dataset, results produced proposed system follows:
Extraposition
118
116
113

Cleft
13
13
13

Weather/Time
9
10
9

Overalla
140
139
136

Precision
95% C.I.b

97.41%
94.07-100.00%

100.00%
79.74-100.00%

90.00%
66.67-100.00%

97.84%
95.21-100.00%

Recall
95% C.I.b

95.76%
91.79-99.12%

100.00%
79.74-100.00%

100.00%
73.07-100.00%

97.14%
93.98-99.34%

F-measure
95% C.I.

96.58%
93.98-98.72%

100.00%
-

94.74%
80.00-100.00%

97.49%
95.45-99.21%

Measurement
Reference
Identified
True Positives


b

Combining extraposition, cleft, weather/time one category.
Adjusted Wald intervals reported extreme measurements.

Table 12: Performance system development dataset, evaluated using authors annotation reference.

369

fiLi, Musilek, Reformat, & Wyard-Scott

statistical significance tests reveal information regarding systems performance comparison two volunteers baselines:
Compared volunteer annotators, systems better performance three
pleonastic categories statistically significant.
extraposition category, difference WSJ annotations (higher)
precision system statistically significant.
Compared Paice Husks (1987) algorithm, systems higher precision
statistically significant.

Target System
Volunteer 1
Volunteer 2
WSJ Annotation
Replicated PHA

Extraposition

Cleft

Weather/Time

F-measure+ /p < .001 F-measure+ /p < .001 F-measure+ /p = .033
F-measure+ /p < .001 F-measure+ /p = .007 F-measure+ /p = .025
Precision /p = .630 F-measure+ /p = 1.00
(All Categories) Precision+ /p < .001

Table 13: Results statistical significance tests presented format
Test Statisticsign /p-value. plus sign (+ ) indicates system performs
better reported measurement; otherwise minus sign ( ) used. fair
comparisons made precision recall, F-measure used
test statistic; otherwise applicable measurement reported.

Using authors annotation reference, system outperforms human volunteers. higher performance usually desirable, particular case, could
indicate possible problems design experiment. Since English language
used speakers shaped group people, impractical system speaks better English human counterparts do. One
plausible clue paradox analytic approach needed gain insight
issue pronoun classification, casual English speakers see
perspective. Green Hecht (1992) many others indicated, capable users
language necessarily ability formulate linguistic rules. However,
kinds analytic skills prerequisite order explicitly classify pronoun one
many categories. Thus, true performance casual speakers measured
ability comprehend produce various pleonastic constructs. addition,
factors, time constraints imperfections category definitions
conveyed, may play role limiting volunteers performance. authors
annotation, hand, much less influenced issues therefore considered expert opinion experiment. shown Section 5.2, WSJ annotation
extrapositions clefts, considered expert opinion, highly compatible
authors. differences two annotations mostly attributed
narrower definition extraposition adopted WSJ annotators. Therefore,
WSJ annotations precision 98.86% extrapositions (when verified authors
370

fiIdentification Pleonastic Using Web

annotation) probably appropriate hint upper-limit practically important
system performance.
extraposition category, 279 individual cases passed syntactic filters
evaluated search engine queries. Results queries obtained Google
web service, Google SOAP21 Search API. three (116 113) cases false-positives
caused missing-object constructions corrected using patterns detailed
Section 3.4.3.
five (118 113) false-negative cases listed below:
[0283:013] newspaper said past time Soviet Union create
unemployment insurance retraining programs
West.
[0209:040] one thing say sterilize, another successfully pollinate plant, said.
[0198:011] Sen. Kennedy said . . . would reckless course
action President Bush claim authority without congressional approval.
[0290:049] Worse, remained well-meaning naive president
United States administer final infamy upon fought
died Vietnam.
[0085:047] easy roll something comprehensive, make
pay, Mr. Jacob says.
Sentence [0283:013] misplaced weather/time. Sentence [0209:040] properly handled syntactic processing subcomponent. Sentences [0198:011] [0290:049] involve
complex noun phrases (underlined) object position matrix verbs
difficult reduce something generic, head noun pronoun, still remain confident original semantics maintained. last case,
sentence [0085:047], fails full queries (containing part subordinate clause)
failed yield enough results stepped-down versions overwhelmed noise.
last four false-negatives annotated correctly WSJ corpus. systems
recall ratio 87 verified WSJ extraposition annotations therefore 95.40%, comparable
overall recall.
5.4 System Performance Parser Output
Thus far, system evaluated based assumption underlying
sentences tagged parsed (almost) perfect accuracy. Much effort
made reduce dependency. example, tracing information function tags
original phrase structures deliberately discarded; system tries search
possible extraposed cleft clauses marked complements matrix object.
However, deficiencies tagging parsing may still impact systems performance.
Occasionally, even golden standard manual markups appear problematic happen
get way task.
21

Simple Object Access Protocol XML-based message protocol web services.

371

fiLi, Musilek, Reformat, & Wyard-Scott

therefore necessary evaluate system sentences automatically
tagged parsed order answer question well would perform
real world. Two state-of-the-art parsers employed study: reranking parser
Charniak Johnson (2005), Berkeley parser Petrov, Barrett, Thibaux,
Klein (2006). systems performance respective interpretations
development dataset sentences reported Tables 14 15. Table 16 compares
systems real-world performance various baselines.
Measurement
Reference
Identified
True Positives
Precision
95% C.I.b
Recall
95% C.I.b
F-measure
95% C.I.

b

Extraposition
118
114
110
96.49%
92.68-99.20%
93.22%
88.43-97.41%
94.83%
91.60-97.49%

Cleft
13
12
12
100.00%
78.40-100.00%
92.31%
73.33-100.00%
96.00%
84.62-100.00%

Weather/Time
9
10
9
90.00%
66.67-100.00%
100.00%
73.07-100.00%
94.74%
80.00-100.00%

Overalla
140
136
132
97.06%
93.92-99.32%
94.29%
90.18-97.81%
95.65%
93.08-97.90%

Combining extraposition, cleft, weather/time one category.
Adjusted Wald intervals reported extreme measurements.

Table 14: Performance system development dataset parsed Charniak
parser, using authors annotation reference.

Extraposition
118
114
111

Cleft
13
11
10

Weather/Time
9
9
8

Overalla
140
134
130

Precision
95% C.I.

97.37%
94.07-100.00%

90.91%
70.00-100.00%

88.89%
62.50-100.00%

97.01%
93.81-99.32%

Recall
95% C.I.

94.07%
89.47-98.18%

76.92%
50.00-100.00%

88.89%
62.50-100.00%

92.86%
88.44-96.91%

F-measure
95% C.I.

95.69%
92.75-98.17%

83.33%
62.50-96.55%

88.89%
66.67-100.00%

94.89%
92.02-97.35%

Measurement
Reference
Identified
True Positives



Combining extraposition, cleft, weather/time one category.

Table 15: Performance system development dataset parsed Berkeley
parser, using authors annotation reference.

372

fiIdentification Pleonastic Using Web

Comparing System Performance Charniak Parser Output to:
Target System
Extraposition
Cleft
Weather/Time


System w/o Parser F-measure /p = .131 F-measure /p = 1.00 F-measure= /p = 1.00
Volunteer 1
F-measure+ /p = .001 F-measure+ /p < .001 F-measure+ /p = .030
Volunteer 2
F-measure+ /p < .001 F-measure+ /p = .041 F-measure+ /p = .021
WSJ Annotation
Precision /p = .368 F-measure= /p = 1.00
Replicated PHA
(All Categories) Precision+ /p < .001
Comparing System Performance Berkeley Parser Output to:
Target System
Extraposition
Cleft
Weather/Time


System w/o Parser F-measure /p = .380 F-measure /p = .128 F-measure /p = 1.00
Volunteer 1
F-measure+ /p < .001 F-measure+ /p = .014 F-measure+ /p = .061
Volunteer 2
F-measure+ /p < .001 F-measure+ /p = .314 F-measure+ /p = .046
WSJ Annotation
Precision /p = .627 F-measure /p = .374
Replicated PHA
(All Categories) Precision+ /p < .001
Table 16: Results statistical significance tests comparing systems performance
parser output various systems, presented format
Test Statisticsign /p-value. plus sign (+ ) indicates proposed system
performs better target system reported measurement; equal
sign (= ) indicates tie; otherwise minus sign ( ) used. fair comparisons
made precision recall, F-measure used test
statistic; otherwise applicable measurement reported.

significance tests reveal that:
using parser statistically significant influence systems performance;
system outperforms volunteer annotators identifying it-extrapositions;
regardless parser used, difference systems performance
WSJ annotation statistically significant;
regardless parser used, system outperforms Paice Husk (1987)
algorithm.
5.5 Correlation Analysis Extrapositions
Figures 5 8 illustrate correlation decision factors true
expletiveness pronoun question. 279 items passed initial syntactic
filtering process included dataset first 116 extrapositional
rest separated break X-axis. arrangement made order better
visualize contrast positive group negative group. Figures 6
8, different grey levels used indicate number results returned
queries darker shade, popular construct question web.
constant Rexp = 0.15 indicated break Y-axis.
373

fiLi, Musilek, Reformat, & Wyard-Scott

illustrated, factors identified Section 3.5 good indicators expletiveness. W
(Figure 5) weakest four factors due number false positives produced
incorrect language usage. clear evidence web noisier ordinary corpora
results counts web may appropriate sole decision-making
factor. comparison, r (Figure 6) almost perfect correlation expletiveness
instances. However, full versions queries usually return fewer results many
cases yield results expletive cases (unfilled items plotted top graph
indicate cases enough results, c.f. Section 3.5). stepped-down versions
queries (Figure 7), less accurate themselves, serve well used
back up, illustrated R plot (Figure 8). Part false-positive outliers
R plot produced full queries expressions habitually associated it,
[0135:002] . . . said expects post sales current fiscal year . . . .
used pronoun, expressions usually describe information quoted person
organization already named earlier sentence, making natural
choice subject pronoun. Normally problematic expressions take form verb
infinitive-complement, i.e. S=TRUE. According decision process described
Section 3.5, W considered situation, effectively eliminates noise.

374

fiIdentification Pleonastic Using Web

Figure 5: scatter plot illustrating correlation W (the estimated number
valid results returned what-cleft queries) expletiveness
instance. extrapositional instances arranged left side plot
rest cases right. query returns valid results,
corresponding item shown hollow circle bottom plot.

375

fiLi, Musilek, Reformat, & Wyard-Scott

Number

Results

Figure 6: scatter plot illustrating correlation r (the ratio hit count
produced expression substitute pronouns original expression) expletiveness instance. extrapositional instances
arranged left side plot rest cases right.
items shaded according hit counts produced corresponding
original expressions. query returns insufficient results, corresponding item
shown hollow unshaded circle top plot.

376

fiIdentification Pleonastic Using Web

Number

Results

Figure 7: scatter plot illustrating correlation r0 (similar r
stepped-down queries) expletiveness instance. extrapositional instances arranged left side plot rest cases
right. items shaded according hit counts produced
corresponding original expressions. query returns insufficient results,
corresponding item shown hollow unshaded circle top plot.

377

fiLi, Musilek, Reformat, & Wyard-Scott

Number

Results

Figure 8: scatter plot illustrating correlation R (synthesized expletiveness;
takes value r complex queries produce enough results,
takes value r0 fail so) expletiveness
instance. extrapositional instances arranged left side plot
rest cases right. items shaded according
hit counts produced corresponding original expressions. query returns
insufficient results, corresponding item shown hollow unshaded circle
top plot.

5.6 Generalization Study
order evaluate well system generalizes, 500 additional sample sentences
randomly selected rest WSJ corpus test dataset. distribution
instances comparable development dataset, shown Table 17.
378

fiIdentification Pleonastic Using Web

Category
Nominal Antecedent
Clause Antecedent
Extraposition
Cleft
Weather/Time
Idiom

Grand Total

Instances
375
24
63
8
6
11
13
500

Percentage
75.00%
4.80%
12.60%
1.60%
1.20%
2.20%
2.60%
100.00%

Table 17: Profile test dataset according authors annotation
shown Table 18, overall level inter-annotator agreement slightly higher
development dataset. Except idiom category, categorical values
higher counterparts development dataset. discrepancy
likely due chance, since two volunteers worked independently started
different datasets (Volunteer 1 started development dataset Volunteer 2 started
test dataset).
Category

Precision

Referential
98.48%
Extraposition
87.10%
Cleft
29.41%
Weather/Time 100.00%
Idiom
31.82%
Overall Accuracy/


Volunteer 1
Recall F-measure

95.12%
85.71%
62.50%
50.00%
53.85%
91.80%

96.77%
86.40%
40.00%
66.67%
40.00%

Precision

97.30%
80.00%
57.14%
100.00%
47.06%

Volunteer 2
Recall F-measure

96.83%
82.54%
50.00%
50.00%
61.54%
92.80%

97.07%
81.25%
53.33%
66.67%
53.33%


.797
.811
.490
.665
.280
.720

values statistically significant p < 0.0001.

Table 18: Performance volunteer annotators test dataset (evaluated using
authors annotation reference) degree inter-annotator agreement
measured Cohens kappa (). authors annotations refitted
simplified annotation scheme used volunteers.

Measurement
Reference
Identified Baseline
Baseline True Positives
Precision
Recall
F-measure


WSJ Annotation
Extraposition
Cleft
63
8
54
6
52
6
96.30% 100.00%
82.54%
75.00%
88.89%
85.71%

Replicated PHA
Overalla
77
97
55
56.70%
71.43%
63.22%

Includes clefts, extrapositions, time/weather cases.

Table 19: Performance baselines test dataset, evaluated authors
annotation.
379

fiLi, Musilek, Reformat, & Wyard-Scott

Table 19 summarizes performance baselines test dataset. two
(54 52) false-positive extrapositions WSJ annotation listed together
respective context:
[1450:054-055] Another solution cities might consider giving special priority
police patrols small-business areas. cities losing business
suburban shopping centers, may wise business investment
help keep jobs sales taxes within city limits.
[1996:061-062] think go turn things around. tough
thing cant.
first case considered referential, second case believed refer
hypothetical situation introduced when-clause.
5.6.1 Performance Analysis
test dataset, system able maintain precision; exhibits slight deterioration recall overall performance still within expectations. findings
summarized Table 20.
Extraposition
63
60
58

Cleft
8
6
6

Weather/Time
6
7
6

Overalla
77
73
70

Precision
95% C.I.b

96.67%
91.38-100.00%

100.00%
64.26-100%

85.71%
50.00-100.00%

95.89%
90.77-100.00%

Recall
95% C.I.b

92.06%
84.85-98.25%

75.00%
40.00-100.00%

100.00%
64.26-100.00%

90.91%
84.15-97.01%

F-measure
95% C.I.

94.31%
89.60-98.11%

85.71%
57.14-100.00%

92.31%
66.67-100.00%

93.33%
88.75-97.10%

Measurement
Reference
Identified
True Positives


b

Combining extraposition, cleft, weather/time one category.
Adjusted Wald intervals reported extreme measurements.

Table 20: Performance system test dataset, evaluated using authors
annotation reference.

149 instances evaluated extraposition using queries, covering 62 63 extrapositions. excluded case introduced form direct question, whose particulars
syntactic processing subsystem prepared for. four false negatives,
three involve noun phrases matrix object position. One two clefts
recognized arises imperfect processing corpus. addition, false positive
weather/time category caused verb hail , treated noun
system.
five (63 58) false-negative extraposition cases annotated corpus
WSJ annotation agrees six clefts identified proposed system. Thus
380

fiIdentification Pleonastic Using Web

systems recall ratio verified WSJ annotations 90.38% extraposition 100%
cleft.
Target System
Volunteer 1
Volunteer 2
WSJ Annotation
Replicated PHA

Extraposition

Cleft

Weather/Time

F-measure+ /p = .041 F-measure+ /p = .005 F-measure+ /p = .248
F-measure+ /p = .002 F-measure+ /p = .119 F-measure+ /p = .254
Precision /p = .697 F-measure= /p = 1.00
(All Categories) Precision+ /p < .001

Table 21: Results statistical significance tests, presented format
Test Statisticsign /p-value. plus sign (+ ) indicates system performs
better reported measurement; equal sign (= ) indicates tie; otherwise
minus sign ( ) used. fair comparisons made precision
recall, F-measure used test statistic; otherwise applicable
measurement reported.

Measurement
Reference
Identified
True Positives

Performance Charniak Parser Output
Extraposition
Cleft Weather/Time
63
8
6
58
7
7
55
6
6

Overalla
77
72
67

Precision
95% C.I.

94.83%
88.24-100.00%

85.71%
50.00-100.00%

85.71%
50.00-100.00%

93.06%
86.36-98.51%

Recall
95% C.I.b

87.30%
78.26-95.08%

75.00%
37.50-100.00%

100.00%
64.26-100.00%

87.01%
78.95-94.12%

F-measure
95% C.I.

90.91%
84.75-95.77%

80.00%
50.00-100.00%

92.31%
66.67-100.00%

89.93%
84.30-94.57%

Performance Berkeley Parser Output
Extraposition
Cleft Weather/Time
63
8
6
58
5
7
56
5
6

Overalla
77
70
67

Measurement
Reference
Identified
True Positives
Precision
95% C.I.b

96.55%
91.11-100.00%

100.00%
59.90-100.00%

85.71%
50.00-100.00%

95.71%
90.28-100.00%

Recall
95% C.I.b

88.89%
80.60-96.23%

62.50%
25.00-100.00%

100.00%
64.26-100.00%

87.01%
79.22-93.90%

F-measure
95% C.I.

92.56%
87.14-96.97%

76.92%
40.00-100.00%

92.31%
66.67-100.00%

91.16%
85.94-95.52%


b

Combining extraposition, cleft, weather/time one category.
Adjusted Wald intervals reported extreme measurements.

Table 22: Performance system test dataset using parser-generated output,
evaluated using authors annotation reference.
381

fiLi, Musilek, Reformat, & Wyard-Scott

Results significance tests, summarized Table 21, reveal following additional
information systems performance test dataset:
systems higher performance recognizing it-extrapositions volunteers
statistically significant;
extraposition category, difference WSJ annotations (higher) precision system statistically significant;
system outperforms Paice Husk (1987) algorithm, difference
statistically significant.
Tables 22 23 outline systems performance test dataset parsers
used. Again, parsers cause slight deteriorations system performance. However,
changes statistically significant. either parser used, system able
perform well WSJ annotations.
Comparing System Performance Charniak Parser Output to:
Target System
Extraposition
Cleft
Weather/Time


System w/o Parser F-measure /p = .125 F-measure /p = 1.00 F-measure= /p = 1.00
Volunteer 1
F-measure+ /p = .298 F-measure+ /p = .013 F-measure+ /p = .247
Volunteer 2
F-measure+ /p = .022 F-measure+ /p = .269 F-measure+ /p = .246
WSJ Annotation
Precision /p = .886 F-measure /p = 1.00
Replicated PHA
(All Categories) Precision+ /p < .001
Comparing System Performance Berkeley Parser Output to:
Target System
Extraposition
Cleft
Weather/Time
System w/o Parser F-measure /p = .501 F-measure /p = 1.00 F-measure= /p = 1.00
Volunteer 1
F-measure+ /p = .131 F-measure+ /p = .035 F-measure+ /p = .256
Volunteer 2
F-measure+ /p = .009 F-measure+ /p = .308
F-measure+ /p = .27


WSJ Annotation
Precision /p = .809 F-measure /p = 1.00
Replicated PHA
(All Categories) Precision+ /p < .001
Table 23: Results statistical significance tests comparing systems performance
parser output various systems, presented format
Test Statisticsign /p-value. plus sign (+ ) indicates source system performs better reported measurement; equal sign (= ) indicates tie;
otherwise minus sign ( ) used. fair comparisons made
precision recall, F-measure used test statistic; otherwise
applicable measurement reported.

5.6.2 Estimated System Performance Whole Corpus
relative sparseness clefts makes hard assess real effectiveness proposed
approach. compensate this, approximate study conducted. First, instances
whole corpus processed automatically using proposed approach. identified
382

fiIdentification Pleonastic Using Web

cleft instances merged already annotated corpus
form evaluation dataset 84 sentences, subsequently verified manually. 76
instances 84 considered valid cleft constructs authors. Respective
performances proposed approach WSJ annotation reported Table 24;
differences statistically significant.
System
WSJ

Proposed
Approach

Total
76

Identified
66

Common
Precision
63
95.45%
95% C.I.: 89.55-100.00%

Recalla
82.94%
74.32-90.79%

F-measurea
88.73%
82.86-93.79%

76

75

70
93.33%
95% C.I.: 87.50-98.65%

92.11%
85.53-97.40%

92.72%
87.84-96.65%



reported recall ratios F-measures synthetic dataset cannot extended
whole corpus.

Table 24: Estimated system performance it-cleft identification entire corpus
Three false positives produced proposed approach actually extrapositions22 , expected (c.f. Footnote 12, Page 351). Thus, binary classification
pleonastic it, items cleft category higher contributions overall
precision category. whole corpus annotated,
impossible obtain precise recall figures either WSJ annotations proposed
approach. However, since rest corpus (other synthetic dataset)
contain true positives either system contains number false-negatives
systems, proposed system maintain higher recall ratio
WSJ annotations whole corpus.
similar experiment conducted extrapositions using sentences already
annotated corpus. 656 annotated extrapositional instances manually verified
637 (97.10%) turn valid cases. system produced queries 623
instances consequently recognized 575 them, translating 90.27% (95% C.I. 89.0193.56%) recall ratio verified annotations. Given fact development
dataset test dataset proposed system yields slightly higher recall whole
dataset subsets identified WSJ annotations, performance
extrapositions whole WSJ corpus likely remain 90% recall.
Similar situation test based random cases, large portion falsepositives contributed imperfect handling surface structures noun phrases
matrix object position, particularly form takes/took . . . . . .
additional experiments, seems particular construct addressed
different pattern, what/whatever takes verb, eliminates noun phrase.
Alternatively, construct could possibly assumed extrapositional without issuing
queries all.
22
kind cleft separated extrapositions using additional pattern attaches
prepositional phrase subordinate verb. However, number samples justify
inclusion study.

383

fiLi, Musilek, Reformat, & Wyard-Scott

6. Discussion
paper novel pleonastic-it identification system proposed. Unlike precursors,
system classifies extrapositions submitting queries web analyzing returned
results. set rules proposed classification clefts, whose particular manner
composition makes difficult apply web-based approach. Components
proposed system simple effectiveness independent type text
processed. shown generalization tests, system maintains precision
recall degrades small margin confronted unfamiliar texts.
indication general principles behind system over-fitted text
derived. Overall, evaluated WSJ news articles
considered difficult type nonfiction system capable producing results
par slightly inferior casually trained humans.
systems success important implications beyond particular problem
pleonastic-it identification. First, shows web used answer linguistic questions based upon simplistic semantic relationships. Second,
comparative study effective means get highly accurate results web despite fact noisier manually compiled corpora. addition, success
simple guidelines used identifying clefts may serve evidence speakers
intention heavily reflected surface structures utterance, bid
make distinguishable similarly constructed sentences.
problems left unaddressed current study, notably handling
complex noun phrases prepositional phrases. Generally speaking, approach
query instantiation somewhat crude. solve noun-phrase issue, finer-grained query
downgrading proposed, viz. first supply query original noun phrase,
head noun, finally adjective modifies head noun, one.
effectiveness approach determined. discussed Section 5.6.2, special
rule used verb take. This, however, may open door exception-based
processing, contradicts principle system provide unified approach
pleonastic pronoun identification. Overall, much data experiments
needed query instantiation procedures finalized.
Aside two sets patterns currently use, information
used assess validity possible extraposition. example, extrapositions
matrix verbs much likely remain present tense past tense,
noun phrases (if any) matrix object position likely indefinite,
extraposed clauses generally longer matrix verb phrases. fuzzy-based
decision system multiple input variables could possibly provide significant performance
gains.
Although system able yield reasonable performances output either
parser tested, introduce additional errors final results. combined
dataset development test items, parsers cause statistically significant deteriorations performance significance level 0.1 (Charniak parser: p=0.008 F-measure
extrapositions; p=0.071 F-measure clefts). possible incorporating
pattern-based method compensate problems caused imperfect parsing
improve recall ratios; however, data needed confirm this.
384

fiIdentification Pleonastic Using Web

Another concern syntactic processing component used system limited.
limitation, caused designers lack exposure large variety different
constructs, essentially different problem imposed limited number
patterns previous systems. Eventually, proposed system, limitation
eliminated. illustrate, current design able correctly process sentences
difference make buy; however, takes minor effort correct
upgrading subsystem recognizes pre-posed objects. upgrade,
may performed manually even automatically machine-learning
approaches, solves one syntactic problems moves system closer able
recognize grammatically valid constructs. contrast, take considerably
effort patch rigidly defined rules upgrade word lists rule-based
systems achieve comparable performances.
writing article, Google deprecated SOAP-based search API.
move makes technically difficult precisely replicate results reported study
since search engines lack ability process alternate expressions (i.e. WordA
WordB ) embedded within quoted query. use different search engine, matrix verbs
expanded instead converted respective third-person
singular present form only. Stubs simplest form only, described
earlier sections. preliminary experiments seems possible replace
combination which/who/this/he alone, plus necessary changes maintain
number agreement among constituents queries. changes may
negative effects final outcome system, unlikely severe.
NLP tasks, classifying usage inherently difficult, even
human annotators already knowledge problem one thing
speak language, another clearly explain rationale behind specific
construct. Although widely accepted extrapositional expletive, line
extrapositional cases referential ones sometimes thin.
clearly manifested existence truncated extrapositions (Gundel et al., 2005),
obviously valid referential readings. Similar things said relationship
among three pleonastic categories well idioms. example, Paice Husk classify
remains . . . idiom construct classified extraposition
evaluations. Aside applying syntactic guidelines proposed study,
assumed annotation process extraposition either valid
non-extraposed reading valid what-cleft reading. assumed cleft
generate valid non-clefted reading joining clefted constituent directly cleft
clause without leading relative pronoun adverb. light subjective nature
problem, annotations published web online appendix better
serve readers.

References
Agresti, A., & Coull, B. A. (1998). Approximate better exact interval estimation binomial proportions. American Statistician, 52 (2), 119126.
Berland, M., & Charniak, E. (1999). Finding parts large corpora. Proceedings 37th annual meeting Association Computational Linguistics
385

fiLi, Musilek, Reformat, & Wyard-Scott

Computational Linguistics, pp. 5764.
Bies, A., Ferguson, M., Katz, K., & MacIntyre, R. (1995). Bracketing guidelines Treebank II style. Tech. rep. MS-CIS-95-06, Department Computer Information
Science, University Pennsylvania.
Boyd, A., Gegg-Harrison, W., & Byron, D. (2005). Identifying non-referential it: machine
learning approach incorporating linguistically motivated patterns. Proceedings
ACL Workshop Feature Engineering Machine Learning Natural Language
Processing, pp. 4047. Association Computational Linguistics.
Charniak, E., & Johnson, M. (2005). Coarse-to-fine n-best parsing maxent discriminative reranking. Proceedings 43rd Annual Meeting Association
Computational Linguistics (ACL05), pp. 173180, Morristown, NJ, USA. Association
Computational Linguistics.
Chinchor, N. (1992). statistical significance MUC-4 results. Proceedings
4th conference Message understanding (MUC4), pp. 3050, San Mateo, CA.
Morgan Kaufmann.
Cimiano, P., Schmidt-Thieme, L., Pivk, A., & Staab, S. (2005). Learning taxonomic relations heterogeneous evidence. Buitelaar, P., Cimiano, P., & Magnini, B.
(Eds.), Ontology Learning Text: Methods, Applications Evaluation, Frontiers Artificial Intelligence Applications, pp. 5973. IOS Press, Amsterdam.
Clemente, J. C., Torisawa, K., & Satou, K. (2004). Improving identification nonanaphoric using support vector machines. Proceedings International
Joint Workshop Natural Language Processing Biomedicine Applications
(NLPBA/BioNLP04).
Cohen, J. (1960). coefficient agreement nominal scales. Educational Psychological Measurement, 20 (1), 3746.
Collins, M. (1999). Head-Driven Statistical Models Natural Language Parsing. Ph.D.
thesis, University Pennsylvania.
Davidse, K. (2000). constructional approach clefts. Linguistics, 38 (6), 11011131.
Davison, A. C., & Hinkley, D. V. (1997). Bootstrap Methods Application. Cambridge series statistical probabilistic mathematics. Cambridge University Press,
Cambridge, UK.
Denber, M. (1998). Automatic resolution anaphora English. Tech. rep., Eastman
Kodak Co.
Di Eugenio, B., & Glass, M. (2004). kappa statistic: second look. Computational
Linguistics, 30 (1), 95101.
Efron, B., & Tibshirani, R. (1993). Introduction Bootstrap. Chapman Hall,
New York, USA.
Evans, R. (2000). comparison rule-based machine learning methods identifying
non-nominal it. Christodoulakis, D. (Ed.), Proceedings 2nd International
Conference Natural Language Processing (NLP00), Vol. 1835 Lecture Notes
Computer Science, pp. 233241, Berlin. Springer.
386

fiIdentification Pleonastic Using Web

Evans, R. (2001). Applying machine learning toward automatic classification it.
Literary Linguistic Computing, 16 (1), 4557.
Fellbaum, C. (Ed.). (1998). WordNet: Electronic Lexical Database. MIT Press,
Cambridge, Mass., USA.
Geurts, B., & van der Sandt, R. (2004). Interpreting focus. Theoretical Linguistics, 30 (1),
144.
Green, P. S., & Hecht, K. (1992). Implicit explicit grammar: empirical study. Applied
Linguistics, 13 (2), 168184.
Gundel, J., Hedberg, N., & Zacharski, R. (2005). Pronouns without NP antecedents:
know pronoun referential?. Branco, A., McEnery, T., & Mitkov,
R. (Eds.), Anaphora Processing: Linguistic, Cognitive Computational Modelling,
pp. 351364. John Benjamins, Amsterdam, Netherlands.
Gundel, J. K. (1977). cleft sentences come from?. Language, 53 (3), 543559.
Hamawand, Z. (2003). For-to complement clauses English: cognitive grammar analysis.
Studia Linguistica, 57 (3), 171192.
Hearst, M. A. (1992). Automatic acquisition hyponyms large text corpora.
Proceedings 14th international conference Computational Linguistics, pp.
539545.
Hedberg, N. (1990). Discourse Function Cleft Sentences English. Ph.D. thesis,
University Minnesota.
Hedberg, N. (2000). referential status clefts. Language, 76 (4), 891920.
Kaltenbock, G. (2005). It-extraposition English: functional view. International Journal
Corpus Linguistics, 10 (2), 119159.
Kilgarriff, A. (2007). Googleology bad science. Computational Linguistics, 33 (1), 147151.
Kilgarriff, A., & Grefenstette, G. (2003). Introduction special issue Web
corpus. Computational Linguistics, 29 (3), 333347.
Krifka, M. (2003). Bare NPs: Kind-referring, indefinites, both, neither?. Proceedings
Semantics Linguistic Theory (SALT) XIII, New York, USA. CLC Publications.
Krippendorff, K. (1980). Content Analysis: Introduction Methodology. Sage Publications, Inc., Beverly Hills, USA.
Lambrecht, K. (2001). framework analysis cleft constructions. Linguistics,
39 (3), 463516.
Lappin, S., & Leass, H. J. (1994). algorithm pronominal anaphora resolution. Computational Linguistics, 20 (4), 535561.
Marcus, M. P., Marcinkiewicz, M. A., & Santorini, B. (1993). Building large annotated
corpus English: Penn Treebank. Computational Linguistics, 19 (2), 313330.
Markert, K., & Nissim, M. (2005). Comparing knowledge sources nominal anaphora
resolution. Computational Linguistics, 31 (3), 367402.
387

fiLi, Musilek, Reformat, & Wyard-Scott

Markert, K., Nissim, M., & Modjeska, N. N. (2003). Using web nominal anaphora
resolution. Dale, R., van Deemter, K., & Mitkov, R. (Eds.), Proceedings
EACL Workshop Computational Treatment Anaphora, pp. 3946.
Metcalf, A., & Barnhart, D. K. (1999). America Many Words: Words
Shaped America. Houghton Mifflin, Boston, USA.
Mitkov, R. (2001). Outstanding issues anaphora resolution. Gelbukh, A. (Ed.),
Proceedings 2nd International Conference Computational Linguistics
Intelligent Text Processing (CICLing01), Vol. 2004 Lecture Notes Computer
Science, pp. 110125, Berlin. Springer.
Mitkov, R., Evans, R., & Orasan, C. (2002). new, fully automatic version Mitkovs
knowledge-poor pronoun resolution method. Gelbukh, A. F. (Ed.), Proceedings
3rd International Conference Computational Linguistics Intelligent Text
Processing (CICLing02), Vol. 2276 Lecture Notes Computer Science, pp. 168186,
London, UK. Springer-Verlag.
Muller, C. (2006). Automatic detection nonreferential spoken multi-party dialog.
Proceedings 11th Conference European Chapter Association
Computational Linguistics (EACL06), pp. 4956.
Nanni, D. L. (1980). surface syntax constructions easy-type adjectives.
Language, 56 (3), 568581.
Ng, V., & Cardie, C. (2002). Identifying anaphoric non-anaphoric noun phrases
improve coreference resolution. Proceedings 19th international conference
Computational linguistics (COLING02), pp. 17, Morristown, NJ, USA. Association
Computational Linguistics.
Noreen, E. W. (1989). Computer-Intensive Methods Testing Hypotheses : Introduction. Wiley-Interscience, New York, USA.
Paice, C. D., & Husk, G. D. (1987). Towards automatic recognition anaphoric
features english text: impersonal pronoun it. Computer Speech & Language,
2 (2), 109132.
Petrov, S., Barrett, L., Thibaux, R., & Klein, D. (2006). Learning accurate, compact,
interpretable tree annotation. Proceedings 21st International Conference
Computational Linguistics 44th annual meeting ACL (ACL06), pp.
433440, Morristown, NJ, USA. Association Computational Linguistics.
Poesio, M., Ishikawa, T., im Walde, S. S., & Vieira, R. (2002). Acquiring lexical knowledge
anaphora resolution. Proceedings Third International Conference
Language Resources Evaluation, pp. 12201224.
Sinclair, J. (Ed.). (1995). Collins COBUILD English Grammar. Harper Collins, London,
U.K.
Sloat, C. (1969). Proper nouns English. Language, 45 (1), 2630.
van Rijsbergen, C. J. (1979). Information Retrieval (2nd edition). Butterworth-Heinemann,
Newton, MA, USA.
388

fiIdentification Pleonastic Using Web

Xia, F., & Palmer, M. (2001). Converting dependency structures phrase structures.
Proceedings first international conference Human language technology
research (HLT01), pp. 15, Morristown, NJ, USA. Association Computational
Linguistics.

389


