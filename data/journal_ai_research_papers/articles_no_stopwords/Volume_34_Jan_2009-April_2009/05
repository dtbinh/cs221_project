Journal Artificial Intelligence Research 34 (2009) 637-674

Submitted 07/08; published 04/09

Sentence Compression Tree Transduction
Trevor Cohn

tcohn@inf.ed.ac.uk

Mirella Lapata

mlap@inf.ed.ac.uk

School Informatics
University Edinburgh
10 Crichton Street Edinburgh EH8 10AB, UK

Abstract
paper presents tree-to-tree transduction method sentence compression.
model based synchronous tree substitution grammar, formalism allows local
distortion tree topology thus naturally capture structural mismatches.
describe algorithm decoding framework show model
trained discriminatively within large margin framework. Experimental results sentence
compression bring significant improvements state-of-the-art model.

1. Introduction
Recent years witnessed increasing interest text-to-text generation methods many
natural language processing applications, ranging text summarisation question answering machine translation. heart methods lies ability perform
rewriting operations. instance, text simplification identifies phrases sentences
document pose reading difficulty given user substitutes simpler alternatives (Carroll, Minnen, Pearce, Canning, Devlin, & Tait, 1999; Chandrasekar &
Srinivas, 1996). question answering, questions often paraphrased order achieve
flexible matching potential answers (Lin & Pantel, 2001; Hermjakob, Echihabi,
& Marcu, 2002). Another example concerns reformulating written language render
natural sounding speech synthesis applications (Kaji, Okamoto, & Kurohashi,
2004).
Sentence compression perhaps one popular text-to-text rewriting methods.
aim produce summary single sentence retains important
information remaining grammatical (Jing, 2000). appeal sentence compression
lies potential summarization generally document compression, e.g.,
displaying text small screens mobile phones PDAs (Vandeghinste & Pan,
2004). Much current work literature focuses simplified formulation
compression task allow rewriting operations word deletion.
Given input source sentence words x = x1 , x2 , . . . , xn , target compression formed
removing subset words (Knight & Marcu, 2002).
Despite restricted word deletion, compression task remains challenging
modeling perspective. Figure 1 illustrates source sentence target compression
taken one compression corpora used experiments (see Section 5 details).
case, hypothetical compression system must apply series rewrite rules order
c
2009
AI Access Foundation. rights reserved.

fiCohn & Lapata














VP
WHNP
RB

WP

exactly

NP

VP
NP

NNS

WHNP

VBD PRP CC

records

WP

NP

WHNP

NP

VBN

WP

NNS

VBP

VBN

involved



records



involved

NNS VBP

made ones

VP
VP

(a) Source

VP

(b) Target

Figure 1: Example sentence compression showing source target trees. bold
source nodes show terminals need removed produce target
string.

WHNP
RB



WHNP

WP

WP

NP



NP
NP

VP



(1)



(2)

(3)




VP

WHNP

VP
VP


WHNP

CC




WHNP




NP

VP


(4)

(5)

Figure 2: Example transduction rules, displayed pair tree fragments. left
(source) fragment matched node source tree, matching
part replaced right (target) fragment. Dotted lines denote variable
correspondences, denotes node deletion.

obtain target, e.g., delete leaf nodes exactly and, delete subtrees made
ones, merge subtrees corresponding records involved.
concretely, system must access rules shown Figure 2. rules
displayed pair tree fragments left fragment corresponds source
right target. instance, rule (1) states wh-noun phrase (WHNP)
consisting adverb (RB) wh-pronoun (WP) (e.g., exactly what) rewritten
wh-pronoun (without adverb). two things note here. First,
syntactic information plays important role, since deletion decisions limited
individual words often span larger constituents. Secondly, large number
compression rules varying granularity complexity (see rule (5) Figure 2).
Previous solutions compression problem cast mostly supervised
learning setting (for unsupervised methods see Clarke & Lapata, 2008; Hori & Furui, 2004;
Turner & Charniak, 2005). Sentence compression often modeled generative framework
638

fiSentence Compression Tree Transduction

aim estimate joint probability P (x, y) source sentence x
target compression (Knight & Marcu, 2002; Turner & Charniak, 2005; Galley &
McKeown, 2007). approaches essentially learn rewrite rules similar shown
Figure 4 parsed parallel corpus subsequently use find best
compression set possible compressions given sentence. approaches
model compression discriminatively subtree deletion (Riezler, King, Crouch, & Zaenen,
2003; Nguyen, Horiguchi, Shimazu, & Ho, 2004; McDonald, 2006).
Despite differences formulation, existing models specifically designed sentence compression mind generally applicable tasks requiring
complex rewrite operations substitutions, insertions, reordering. common
assumption underlying previous work tree structures representing source
sentences target compressions isomorphic, i.e., exists edge-preserving
bijection nodes two trees. assumption valid sentence compression hold rewriting tasks. Consequently, sentence compression
models restrictive; cannot readily adapted generation problems
since able handle structural lexical divergences. related issue concerns deletion operations often take place without considering
structure target compression (the goal generate compressed string rather
tree representing it). Without syntax-based language model (Turner & Charniak,
2005) explicit generation mechanism licenses tree transformations
guarantee compressions well-formed syntactic structures.
straightforward process subsequent generation analysis tasks.
paper present sentence compression model deletion-specific
account ample rewrite operations scales rewriting tasks. formulate
compression problem tree-to-tree rewriting using synchronous grammar (with rules
shown Figure 2). Specifically, adopt synchronous tree substitution
grammar (STSG) formalism (Eisner, 2003) model non-isomorphic tree structures
efficient inference algorithms. show grammar induced
parallel corpus propose discriminative model rewriting task
viewed weighted tree-to-tree transducer. learning framework makes use
large margin algorithm put forward Tsochantaridis, Joachims, Hofmann, Altun
(2005) efficiently learns prediction function minimize given loss function.
develop appropriate algorithm used training (i.e., learning
model weights) decoding (i.e., finding plausible compression model).
Beyond sentence compression, hope work described might
relevance tasks involving structural matching (see discussion Section 8).
remainder paper structured follows. Section 2 provides overview
related work. Section 3 presents STSG framework compression model
employ experiments. Section 5 discusses experimental set-up Section 6
presents results. Discussion future work concludes paper.

2. Related Work
Synchronous context-free grammars (SCFGs, Aho & Ullman, 1969) generalization
context-free grammar (CFG) formalism simultaneously produce strings two
639

fiCohn & Lapata

languages. used extensively syntax-based statistical MT. Examples
include inversion transduction grammar (Wu, 1997), head transducers (Alshawi, Bangalore,
& Douglas, 2000), hierarchical phrase-based translation (Chiang, 2007), several variants
tree transducers (Yamada & Knight, 2001; Grael & Knight, 2004).
Sentence compression bears resemblance machine translation. Instead translating one language another, translating long sentences shorter ones
within language. therefore surprising previous work adopted
SCFGs compression task. Specifically, Knight Marcu (2002) proposed noisychannel formulation sentence compression. model consists two components:
language model P (y) whose role guarantee compression output grammatical channel model P (x|y) capturing probability source sentence x
expansion target compression y. decoding algorithm searches compression maximizes P (y)P (x|y). channel model stochastic SCFG,
rules extracted parsed parallel corpus weights estimated using
maximum likelihood. Galley McKeown (2007) show obtain improved SCFG
probability estimates Markovization. Turner Charniak (2005) note SCFG
rules expressive enough model structurally complicated compressions
restricted trees depth 1. remedy supplying synchronous grammar set general special rules. example, allow rules form
hNP,NPi h[NP NP 1 CC NP 2 ], NP 1 (boxed subscripts added distinguish
two NPs).
work formulates sentence compression framework synchronous treesubstitution grammar (STSG, Eisner, 2003). STSG allows describe non-isomorphic tree
pairs (the grammar rules comprise trees arbitrary depth) thus suited textrewriting tasks typically involve number local modifications input text.
Especially modification described succinctly terms syntactic transformations, dropping adjectival phrase converting passive verb phrase active
form. STSG restricted version synchronous tree adjoining grammar (STAG, Shieber
& Schabes, 1990) without adjunction operation. STAG affords mild context sensitivity,
however increased cost inference. SCFG STSG weakly equivalent, is,
string languages identical produce equivalent tree pairs. example,
Figure 2, rules (1)(4) expressed SCFG rules, rule (5) cannot
source target fragments two level trees. fact would impossible
describe trees Figure 1 using SCFG. grammar rules therefore general
obtained Knight Marcu (2002) account elaborate tree
divergences. Moreover, adopting expressive grammar formalism, naturally model syntactically complex compressions without specify additional rules
(as Turner & Charniak, 2005).
synchronous grammar license large number compressions given source
tree. grammar rule typically score overall score compression sentence x derived. Previous work estimates scores generatively
discussed above. opt discriminative training procedure allows incorporation manner powerful features. use large margin technique proposed
Tsochantaridis et al. (2005). framework attractive supports configurable loss function, describes extent predicted target tree differs
640

fiSentence Compression Tree Transduction

reference tree. devising suitable loss functions model straightforwardly
adapted text rewriting tasks besides sentence compression.
McDonald (2006) presents sentence compression model uses discriminative
large margin algorithm. model rich feature set defined compression bigrams
including parts speech, parse trees, dependency information, without however making explicit use synchronous grammar. Decoding model amounts finding
combination bigrams maximize scoring function defined adjacent words
compression intervening words dropped. model differs
McDonalds two important respects. First, capture complex tree transformations go beyond bigram deletion. tree-based, decoding algorithm
better able preserve grammaticality compressed output. Second, treebased representation allows greater modeling flexibility, e.g., defining wide range
loss functions tree string yield. contrast, McDonald define loss
functions final compression.
Although bulk research sentence compression relies parallel corpora
modeling purposes, approaches use training data small amount.
example work Hori Furui (2004), propose model automatically
transcribed spoken text. method scores candidate compressions using language
model combined significance score (indicating whether word topical not),
score representing speech recognizers confidence transcribing given word
correctly. Despite conceptually simple knowledge lean, model operates
word level. Since take syntax account, means deleting
constituents spanning several subtrees (e.g., relative clauses). Clarke Lapata (2008)
show unsupervised models greatly improved linguistically motivated
constraints used decoding.

3. Problem Formulation
mentioned earlier, formulate sentence compression tree-to-tree rewriting problem
using weighted synchronous grammar coupled large margin training process.
model learns parallel corpus input (uncompressed) output (compressed) pairs
(x1 , y1 ), . . . , (xn , yn ) predict target labeled tree source labeled tree x.
capture dependency x weighted STSG define
following section. Section 3.2 discusses extract grammar parallel
corpus. rule score, ngram output tree,
overall score compression sentence x derived. introduce scoring
function Section 3.3 explain training algorithm Section 3.5. framework
decoding amounts finding best target tree licensed grammar given source
tree. present chart-based decoding algorithm Section 3.4.
3.1 Synchronous Grammar
synchronous grammar defines space valid source target tree pairs, much
regular grammar defines space valid trees. Synchronous grammars treated tree
transducers reasoning space possible sister trees given tree, is,
trees produced alongside given tree. essentially transducer
641

fiCohn & Lapata

Algorithm 1 Generative process creating pair trees.
initialize source tree, x = RS
initialize target tree, = RT
initialize stack frontier nodes, F = [(RS , RT )]
node pairs, (vS , vT ) F
choose rule hvS , vT h, ,
rewrite node vS x
rewrite node vT
variables, u
find aligned child nodes, (cS , cT ), vS vT corresponding u
push (cS , cT ) F
end
end
x complete

takes tree input produces tree output. grammar rules specify
steps taken transducer recursively mapping tree fragments input tree
fragments target tree. many families synchronous grammars (see
Section 2), elect use synchronous tree-substitution grammar (STSG). one
simpler formalisms, consequently efficient inference algorithms, still
complex enough model rich suite tree edit operations.
STSG 7-tuple, G = (NS , NT , , , P, RS , RT ) N non-terminals
terminals, subscripts indicating source target respectively, P productions RS NS RT NT distinguished root symbols.
production rewrite rule two aligned non-terminals X NS NT
source target:
hX, h, ,
(1)
elementary trees rooted symbols X respectively. Note
synchronous context free grammar (SCFG) limits one level elementary
trees, otherwise identical STSG, imposes limits. Non-terminal
leaves elementary trees referred frontier nodes variables.
points recursion transductive process. one-to-one alignment frontier
nodes specified . alignment represent deletion (or insertion)
aligning node special symbol, indicates node present
tree. nodes aligned , allows subtrees deleted
transduction. disallow converse, -aligned nodes , would license
unlimited insertion target tree, independently source tree. capability
would limited use sentence compression, increasing complexity
inference.
grammar productions used generative setting produce pairs trees,
transductive setting produce target tree given source tree. Algorithms 1
2 present pseudo-code processes. generative process (Algorithm 1) starts
two root symbols applies production rewrites symbols
productions elementary trees. elementary trees might contain frontier nodes,
642

fiSentence Compression Tree Transduction

Algorithm 2 transduction source tree target tree.
Require: complete source tree, x, root node labeled RS
initialize target tree, = RT
initialize stack frontier nodes, F = [(root(x), RT )]
node pairs, (vS , vT ) F
choose rule hvS , vT h, , matches sub-tree rooted vS x
rewrite vT
variables, u
find aligned child nodes, (cS , cT ), vS vT corresponding u
push (cS , cT ) F
end
end
complete

case aligned pairs frontier nodes pushed stack, later rewritten
using another production. process continues recursive fashion stack
empty frontier nodes remaining , point two trees complete.
sequence rewrite rules referred derivation, source
target tree recovered deterministically.
model uses STSG transductive setting, source tree given
target tree generated. necessitates different rewriting process,
shown Algorithm 2. start source tree, RT , target root symbol,
aligned root node source, denoted root(x). choose production
rewrite pair aligned non-terminals productions source side, , matches
source tree. target symbol rewritten using . variable
matching node source corresponding leaf node target tree pushed
stack later processing.1 process repeats stack empty,
therefore source tree covered. complete target tree.
use term derivation refer sequence production applications. target
string yield target tree, given reading non-terminals tree
left right manner.
Let us consider compression example Figure 1. tree editing rules
Figure 2 encoded STSG productions Figure 3 (see rules (1)(5)). Production (1),
reproduces tree pair (1) Figure 2, production (2) tree pair (2), on. notation
Figure 3 (primarily space reasons) uses brackets ([]) indicate constituent boundaries.
Brackets surround constituents non-terminal child nodes,
terminals, non-terminals bracketed subtrees. boxed indices short-hand notation
alignment, . example, rule (1) specify two WP non-terminals
aligned RB node occurs source tree (i.e., heads deleted subtree). grammar rules allow differences non-terminal category source
target, seen rules (2)(4). allow arbitrarily deep elementary trees,
1. Special care must taken aligned variables. Nodes -aligned signify source
sub-tree point deleted without affecting target tree. reason safely
ignore source nodes deleted manner.

643

fiCohn & Lapata

(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
(9)
(10)
(11)
(12)

hWHNP, WHNPi
hS, NPi
hS, VPi
hS, VPi
hS, Si
hWP, WPi
hNP, NPi
hNNS, NNSi
hVP, VPi
hVBP, VBPi
hVP, VPi
hVBN, VBNi

Rules perform major tree edits
h[WHNP RB WP 1 ], [WHNP WP 1 ]i
h[S NP 1 VP ], NP 1
h[S NP VP 1 ], VP 1
h[S WHNP 1 ], VP 1
h[S [S WHNP 1 2 ] [CC and] 3 ], [S WHNP 1 [S NP 2 VP 3 ]]i
Rules preserve tree structure
h[WP what], [WP what]i
h[NP NNS 1 ], [NP NNS 1 ]i
h[NNS records], [NNS records]i
h[VP VBP 1 VP 2 ], [VP VBP 1 VP 2 ]i
h[VBP are], [VBP are]i
h[VP VBN 1 ], [VP VBN 1 ]i
h[VBN involved], [VBN involved]i

Figure 3: rules Synchronous Tree Substitution Grammar (STSG) capable generating sentence pair Figure 1. Equivalently, grammar defines
transducer convert source tree (Figure 1(a)) target tree
(Figure 1(b)). rule rewrites pair non-terminals pair subtrees,
shown bracketed notation.

evidenced rule (5) trees depth two. Rules (6)(12) complete
toy grammar describes tree pair Figure 1. rules copy parts
source tree target, terminals (e.g., rule (6)) internal nodes children
(e.g., rule (9)).
Figure 4 shows grammar used transduce source tree
target tree Figure 1. first steps derivation shown graphically Figure 5. start source tree, seek transduce root symbol
target root symbol, denoted S/S. first rule applied rule (5) Figure 3; source side, = [S [S WHNP S] [CC and] S], matches root source tree
requisite target category, = S. matching part source tree
rewritten using rules target elementary tree, = [S WHNP [S NP VP]]. three
three variables annotated reflect category transformations required
node, WHNP/WHNP, S/NP S/VP. process continues leftmost
nodes, labeled WHNP/WHNP. Rule (1) (from Figure 3) applied, deletes
nodes left child, shown RB/, retains right child. subsequent rule completes
transduction WHNP node matching string exactly. algorithm continues visit variable node finishes variable nodes remaining,
resulting desired target tree.
3.2 Grammar
previous section outlined STSG formalism employ sentence compression
model, save one important detail: grammar itself. example, could obtain
644

fiSentence Compression Tree Transduction

[S/S [S [WHNP exactly what] [S [NP records] [VP made it]]]
[CC and] [S [WHNP which] [S [NP ones] [VP involved]]]]
5
[S [WHNP/WHNP [RB exactly] [WP what]] [S [S/NP [NP records] [VP made it]]
[S/VP [WHNP which] [S [NP ones] [VP involved]]]]]
1
[S [WHNP [WP/WP what]] [S [S/NP [NP records] [VP made it]]
[S/VP [WHNP which] [S [NP ones] [VP involved]]]]]
6
[S [WHNP [WP what]] [S [S/NP [NP records] [VP [VBD made] [NP [PRP it]]]]]
[S/VP [WHNP which] [S [NP ones] [VP [VBP are] [VP [VBN involved]]]]]]]
2
[S [WHNP [WP what]] [S [NP [NNS/NNS records]]
[S/VP [WHNP which] [S [NP ones] [VP involved]]]]]
8
[S [WHNP [WP what]] [S [NP [NNS records]]]
[S/VP [WHNP which] [S [NP ones] [VP involved]]]]
4
[S [WHNP what] [S [NP records] [S/VP [NP ones] [VP involved]]]]
3
[S [WHNP what] [S [NP records] [VP/VP [VP [VBP are] [VP [VBN involved]]]]]]
9
[S [WHNP what] [S [NP records] [VP [VBP/VBP are] [VP/VP [VBN involved]]]]]
10 [S [WHNP what] [S [NP records] [VP [VBP are] [VP/VP [VBN involved]]]]]
11 [S [WHNP what] [S [NP records] [VP [VBP are] [VP [VBN/VBN involved]]]]]
12 [S [WHNP [WP what]] [S [NP [NNS records]] [VP [VBP are] [VP [VBN involved]]]]]

Figure 4: Derivation example sentence pair Figure 1. line shows rewrite step,
denoted subscript identifies rule used. frontier
nodes shown bold X/Y indicating symbol X must transduced
subsequent steps. sake clarity, internal nodes
omitted.

synchronous grammar hand, automatically corpus, combination.
requirement grammar allows source trees training set
transduced corresponding target trees. maximum generality, devised
automatic method extract grammar parsed, word-aligned parallel compression
corpus. method maps word alignment constituent level alignment
nodes source target trees. Pairs aligned subtrees next generalized create
tree fragments (elementary trees) form rules grammar.
first step algorithm find constituent alignment, define
set source target constituent pairs whose yields aligned one another
word alignment. base approach alignment template method (Och & Ney,
2004), uses word alignments define alignments ngrams (called phrases
SMT literature). method finds pairs ngrams least one word one
ngrams aligned word other, word either ngram aligned
word outside ngram. addition, require ngrams syntactic
constituents. formally, define constituent alignment as:
C = {(vS , vT ), ((s, t) (vS ) (vT ))

(2)

(@(s, t) (s (vS ) (vT )))}
vS vT source target tree nodes (subtrees), = {(s, t)} set word
alignments (pairs word-indices), () returns yield span subtree (the minimum
maximum word index yield) exclusive-or operator. Figure 6 shows
645

fiCohn & Lapata











VP

WHNP

NP

RB

WP

NNS

exactly



records

VP
NP

WHNP

VBD PRP CC
made





WP


NP

VP

NNS VBP
ones



VBN
involved









VP

WHNP

NP

RB

WP

NNS

exactly



records

WHNP


NP

made

NP

VP
WHNP

VBD PRP CC




WP


NP


VBN
involved










NP

RB

WP

NNS

exactly



records

WHNP


VP

WHNP

WHNP

VBD PRP CC
made





WP


NP

VP

NNS VBP
ones



WP NP VP

VP
NP

VP

VP

NNS VBP
ones





VBN
involved

Figure 5: Graphical depiction first two steps derivation Figure 4. source
tree shown left partial target tree right. Variable nodes
shown bold face dotted lines show alignment.

word alignment constituent alignments licensed sentence pair
Figure 1.
next step generalize aligned subtree pairs replacing aligned child subtrees
variable nodes. example, Figure 6 consider pair aligned subtrees
[S ones involved] [VP involved], could extract rule:
hS,VPi h[S [WHNP [WP which]] [S [NP [NNS ones] [VP [VBP are] [VP [VBN involved]]]]]],
[VP [VBP are] [VP [VBN involved]]]i

(3)

However, rule specific consequently useful transduction
model. order applied, must see full subtree, highly unlikely
occur another sentence. Ideally, generalize rule match many
source trees, thereby allow transduction previously unseen structures.
example, node pairs labeled (VP1 , VP1 ), (VBP, VBP), (VP2 , VP2 ) (VBN, VBN)
generalized nodes aligned constituents (subscripts added distinguish
646

fiSentence Compression Tree Transduction








VP

WHNP

NP

NP

VP
WHNP NP

VP

RB
WP NNS VBD PRP CC WP NNS VBP VBN
exactly records made
ones involved


VP
WHNP NP
VP
WP NNS VBP VBN
records involved

Figure 6: Tree pair word alignments shown binary matrix. dark square indicates
alignment words row column. overlaid rectangles
show constituent alignments inferred word alignment.

two VP nodes). addition, nodes WHNP, WP, NP NNS source
unaligned, therefore generalized using -alignment signify deletion.
perform possible generalizations example,2 would produce
rule:
hS,VPi h[S WHNP 1 ], VP 1
(4)
many possible rules extracted applying different legal
combinations generalizations (there 45 total example).
Algorithm 3 shows minimial (most general) rules extracted.3 results
minimal set synchronous rules describe tree pair.4 rules
minimal sense cannot made smaller (e.g., replacing subtree
variable) still honoring word-alignment. Figure 7 shows resulting minimal
set synchronous rules example Figure 6. seen example,
many rules extracted overly general. Ideally, would extract every rule
every legal combination generalizations, however leads massive number rules
exponential size source tree. address problem allowing limited
number generalizations skipped extraction process. equivalent
altering lines 4 7 Algorithm 3 first make non-deterministic decision whether
match ignore match continue descending source tree. recursion depth
limits number matches ignored way. example, allow one
2. generalizations mutually exclusive, take highest match trees.
3. non-deterministic matching step line 8 allows matching options individually.
implemented mutually recursive function replicates algorithm state process
different match.
4. Algorithm 3 extension Galley, Hopkins, Knight, Marcus (2004) technique extracting
SCFG word-aligned corpus consisting (tree, string) pairs.

647

fiCohn & Lapata

Algorithm 3 extract(x, y, A): extracts minimal rules constituent-aligned trees
Require: source tree, x, target tree, y, constituent-alignment,
1: initialize source target sides rule, = x, =
2: initialize frontier alignment, =
3: nodes vS , top-down
4:
vS null-aligned
5:
(vS , )
6:
delete children
7:
else vS aligned target node(s)
8:
choose target node, vT
{non-deterministic choice}
9:
call extract(vS , vT , A)
10:
(vS , vT )
11:
delete children vS
12:
delete children vT
13:
end
14: end
15: emit rule hroot(), root()i h, ,
level recursion extracting rules (S, VP) pair Figure 6, get
additional rules:
hS,VPi h[S [WHNP WP ] 1 ], VP 1
hS,VPi h[S WHNP [S NP VP 1 ]], VP 1
two levels recursion, get:
hS,VPi h[S [WHNP [WP which]] 1 ], VP 1
hS,VPi h[S [WHNP [WP which]] [S NP VP 1 ]], VP 1
hS,VPi h[S WHNP [S [NP NNS ] VP 1 ]], VP 1
hS,VPi h[S WHNP [S NP [VP VBD 1 VP 2 ]]], [VBD 1 VBD 2 ]i
Compared rule (4) see specialized rules add useful structure
lexicalisation, still sufficiently abstract generalize new sentences, unlike
rule (3). number rules exponential recursion depth, fixed depth
polynomial size source tree fragment. set recursion depth
small number (one two) experiments.
guarantee induced rules good coverage unseen trees.
Tree fragments containing previously unseen terminals non-terminals, even unseen
sequence children parent non-terminal, cannot matched grammar productions. case transduction algorithm (Algorithm 2) fail way
covering source tree. However, problem easily remedied adding new
rules grammar allow source tree fully covered.5 node
5. alternative, equally valid, techniques improving coverage simplify syntax trees.
example, done explicitly binarizing large productions (e.g., Petrov, Barrett, Thibaux,
& Klein, 2006) implicitly Markov grammar grammar productions (e.g., Collins, 1999).

648

fiSentence Compression Tree Transduction

hS,Si
hWHNP,WHNPi
hWP,WPi
hS,NPi
hNP,NPi
hNNS,NNSi
hS,VPi
hS,VPi
hVP,VPi
hVBP,VBPi
hVP,VPi
hVBN,VBNi














h[S [S WHNP 1 2 ] CC 3 ], [S WHNP 1 [S NP 2 VP 3 ]]i
h[WHNP RB WP 1 ], [WHNP WP 1 ]i
h[WP what], [WP what]i
h[S NP 1 VP ], NP 1
h[NP NNS 1 ], [NP NNS 1 ]i
h[NNS records], [NNS records]i
h[S WHNP 1 ], VP 1
h[S NP VP 1 ], VP 1
h[VP VBP 1 VP 2 ], [VP VBP 1 VP 2 ]i
h[VBP are], [VBP are]i
h[VP VBN 1 ], [VP VBN 1 ]i
h[VBN involved], [VBN involved]i

Figure 7: minimal set STSG rules extracted aligned trees Figure 6.

source tree, rule created copy node child nodes target tree.
example, see fragment [NP DT JJ NN] source tree, add rule:

hNP,NPi h[NP DT 1 JJ 2 NN 3 ], [NP DT 1 JJ 2 NN 3 ]i

rules, source node copied target tree, therefore transduction algorithm trivially recreate original tree. course, grammar
rules work conjunction copying rules produce target trees.
copy rules solve coverage problem unseen data, solve
related problem under-compression. occurs unseen CFG productions
source tree therefore applicable grammar rules copy rules,
copy child nodes target. None child subtrees deleted unless
parent node deleted higher-level rule, case children
deleted. Clearly, would add considerable modelling flexibility able delete some,
all, children. reason, add explicit deletion rules source
CFG production allow subsets child nodes deleted linguistically
plausible manner.
deletion rules attempt preserve important child nodes. measure
importance using head-finding heuristic Collins parser (Appendix A, Collins,
1999). Collins method finds single head child CFG production using hand-coded
tables non-terminal type. desire set child nodes, run algorithm
find matches rather stopping first match. order match
found used ranking importance child. ordered list child nodes
used create synchronous rules retain head 1, heads 12, . . . , heads.
649

fiCohn & Lapata

fragment [NP DT JJ NN], heads found following order (NN, DT,
JJ). Therefore create rules retain children (NN); (DT, NN) (DT, JJ, NN):
hNP,NPi h[NP DT JJ NN 1 ], [NP NN 1 ]i
hNP,NNi h[NP DT JJ NN 1 ], NN 1
hNP,NPi h[NP DT 1 JJ NN 2 ], [NP DT 1 NN 2 ]i
hNP,NPi h[NP DT 1 JJ 2 NN 3 ], [NP DT 1 JJ 2 NN 3 ]i
Note one child remains, rule produced without parent node,
seen second rule above.
3.3 Linear Model
STSG defines transducer capable mapping source tree many possible
target trees, little use without kind weighting towards grammatical trees
constructed using sensible STSG productions yield fluent compressed target sentences. Ideally model would define scoring function target
trees strings, however instead operate derivations. general, may many
derivations produce target tree, situation referred spurious ambiguity. fully account spurious ambiguity would require aggregating derivations
produce target tree. would break polynomial-time dynamic program used inference, rendering inference problem NP-complete (Knight, 1999).
end, define scoring function derivations:
score(d; w) = h(d), wi

(5)

derivation6 consisting sequence rules, w model parameters,
vector-valued feature function operator h, inner product.
parameters, w, learned training, described Section 3.5.
feature function, , defined as:
X
X
(d) =
(r, source(d)) +
(m, source(d))
(6)
rd

mngrams(d)

r rules derivation, ngrams(d) ngrams yield target
tree feature function returning vector feature values rule. Note
feature function access rule, r, source tree, source(d),
conditional model therefore overhead terms modeling
assumptions complexity inference.
second summand (6), ngrams yield target tree
feature function ngrams. Traditional (weighted) synchronous grammars
allow features decompose derivation (i.e., expressed using first
summand (6)). However, limiting requirement, ngram features
allow modeling local coherence commonly used sentence compression
literature (Knight & Marcu, 2002; Turner & Charniak, 2005; Galley & McKeown, 2007;
6. derivation, d, fully specifies source, x = source(d), target tree, = target(d).

650

fiSentence Compression Tree Transduction

Clarke & Lapata, 2008; Hori & Furui, 2004; McDonald, 2006). instance, deleting
sub-tree left right siblings, critical know new siblings
grammatical configuration, yield still forms coherent string.
reason, allow ngram features, specifically conditional log-probability
ngram language model. Unfortunately, comes price ngram features
significantly increase complexity inference used training decoding.
3.4 Decoding
Decoding aims find best target tree licensed grammar given source tree.
mentioned above, deal derivations place target trees. Decoding finds
maximizing derivation, , of:
=

argmax

score(d; w)

(7)

d:source(d)=x

x (given) source tree, source(d) extracts source tree derivation
score defined (5). maximization performed space derivations
given source tree, defined transduction process shown Algorithm 2.
maximization problem (7) solved using chart-based dynamic program
shown Algorithm 4. extends earlier inference algorithms weighted STSGs (Eisner, 2003) assume scoring function must decompose derivation,
i.e., features apply rules terminal ngrams. Relaxing assumption leads
additional complications increased time space complexity. equivalent using grammar intersection original grammar ngram language
model, explained Chiang (2007) context string transduction SCFG.
algorithm defines chart, C, record best scoring (partial) target tree
source node vS root non-terminal t. back-pointers, B, record maximizing
rule store pointers child chart cells filling variable rule. chart
indexed n 1 terminals left right edges target trees yield
allow scoring ngram features.7 terminal ngrams provide sufficient context evaluate
ngram features overlapping cells boundary chart cell combined another
rule application (this operation performed boundary-ngrams function line
15). best illustrated example. Using trigram features, n = 3, node
rewritten [NP fast car] must store ngram context (the fast, fast car)
chart entry. Similarly [VP skidded halt] would ngram context (skidded to,
halt). applying parent rule [S NP VP] rewrites two trees adjacent
siblings need find ngrams boundary NP VP.
easily retrieved two chart cells contexts. combine right edge NP
context, fast car, left edge VP context, skidded to, get two trigrams
fast car skidded car skidded to. trigrams fast car, skidded
halt already evaluated child chart cells. new
combined chart cell given context (the fast, halt) taking left right
7. Strictly speaking, terminals right edge required compression model would
create target string left-to-right manner. However, algorithm general
allows reordering rules hPP,PPi h[PP 1 NP 2 ], [PP NP 2 1 ]i. rules required
text-rewriting tasks besides sentence compression.

651

fiCohn & Lapata

Algorithm 4 Exact chart based decoding algorithm.
Require: complete source tree, x, root node labeled RS
1: let C[v, t, l] R chart representing score best derivation transducing
tree rooted v tree root category ngram context l
2: let B[v, t, l] (P, x NT L) corresponding back-pointers, consisting
production source node, target category ngram context
productions variables
3: initialize chart, C[, , ] =
4: initialize back-pointers, B[, , ] = none
5: source nodes, vS x, bottom-up
6:
rules, r = hvS , h, , matches sub-tree rooted vS
7:
let target ngrams wholly contained
8:
let features vector, (r, x) + (m, x)
9:
let l empty ngram context
10:
let score, q 0
11:
variables, u
12:
find source child node, cu , vS corresponding u
13:
let tu non-terminal target child node corresponding u
14:
choose child chart entry, qu = C[cu , tu , lu ]
{non-deterministic choice lu }
15:
let boundary-ngrams(r, lu )
16:
update features, + (m, x)
17:
update ngram context, l merge-ngram-context(l, lu )
18:
update score, q q + qu
19:
end
20:
update score, q q + h, wi
21:
q > C[vS , Y, l]
22:
update chart, C[vS , Y, l] q
23:
update back-pointers, B[vS , Y, l] (r, {(cu , tu , lu )u})
24:
end
25:
end
26: end
27: find best root chart entry, l argmaxl C[root(x), RT , l]
28: create derivation, d, traversing back-pointers B[root(x), RT , l ]

edges two child cells. merging process performed merge-ngram-context
function line 17. Finally add artificial root node target tree n 1 artificial
start terminals one end terminal. allows ngram features applied
boundary ngrams beginning end target string.
decoding algorithm processes source tree post-order traversal, finding
set possible trees ngram contexts source node inserting
chart. rules match node processed lines 624. feature vector,
, calculated rule ngrams therein (line 8), ngrams bordering child
cells filling rules variables (line 16). Note feature vector includes
features specific rule boundary ngrams, wholly contained
652

fiSentence Compression Tree Transduction

child cell. reason score sum scores child cell (line
18) feature vector model weights (line 20). new ngram context, l,
calculated combining rules frontier ngram contexts child cells (line
17). Finally chart entry node updated score betters previous value
(lines 2124).
choosing child chart cell entry line 14, many different entries
different ngram context, lu . affects ngram features, , consequently
ngram context, l, score, q, rule. non-determinism means every
combination child chart entries chosen variable, combinations
evaluated inserted chart. number combinations product
number child chart entries variable. bounded O(|TT |2(n1)V )
|TT | size target lexicon V number variables. Therefore
asymptotic time complexity decoding O(SR|TT |2(n1)V ) number
source nodes R number matching rules node. high complexity
clearly makes exact decoding infeasible, especially either n V large.
adopt popular approach syntax-inspired machine translation address
problem (Chiang, 2007). Firstly, use beam-search, limits number different
ngram contexts stored chart cell constant, W . changes base
complexity term, leading improved O(SRW V ) still exponential
number variables. addition, use Chiangs cube-pruning heuristic
limit number combinations. Cube-pruning uses heuristic scoring function
approximates conditional log-probability ngram language model logprobability unigram model.8 allows us visit combinations best-first
order heuristic scoring function beam filled.The beam rescored
using correct scoring function. done cheaply O(W V ) time, leading
overall time complexity decoding O(SRW V ). refer interested reader
work Chiang (2007) details.
3.5 Training
turn problem derivations scored model. given source
tree, space sister target trees implied synchronous grammar often large,
majority trees ungrammatical poor compressions. job
training algorithm find weights reference target trees high scores
many target trees licensed grammar given lower scores.
explained Section 3.3 define scoring function derivations. function
given (5) (7), reproduced below:
f (d; w) =

argmax hw, (d)i

(8)

d:source(d)=x

Equation (8) finds best scoring derivation, d, given source, x, linear model.
Recall derivation generates source tree x target tree. goal
8. use conditional log-probability ngram language model ngram feature. order
use ngram features, binary identity features specific ngrams, would first advisable
construct approximation decomposes derivation use cube-pruning heuristic.

653

fiCohn & Lapata

training procedure find parameter vector w satisfies condition:
i, : source(d) = xi 6= di : hw, (di ) (d)i 0

(9)

xi , di ith training source tree reference derivation. condition states
training instances reference derivation least high scoring
derivations. Ideally, would know extent predicted target
tree differs reference tree. example, compression differs gold
standard respect one two words treated differently compression
bears resemblance it. Another important factor length compression.
Compressions whose length similar gold standard preferable longer
shorter output. loss function (yi , y) quantifies accuracy prediction
respect true output value yi .
plethora different discriminative training frameworks optimize
linear model. Possibilities include perceptron training (Collins, 2002), log-linear optimisation conditional log-likelihood (Berger, Pietra, & Pietra, 1996) large margin
methods. base training Tsochantaridis et al.s (2005) framework learning
Support Vector Machines (SVMs) structured output spaces, using SVMstruct implementation.9 framework supports configurable loss function particularly
appealing context sentence compression generally text-to-text generation. efficient training algorithm powerful regularization. latter
critical discriminative models large numbers features, would otherwise
over-fit training sample expense generalization accuracy. briefly summarize
approach below; detailed description refer interested reader
work Tsochantaridis et al. (2005).
Traditionally SVMs learn linear classifier separates two classes
largest possible margin. Analogously, structured SVMs attempt separate correct
structure structures large margin. learning objective
structured SVM uses soft-margin formulation allows errors training set
via slack variables, :
n

1
CX
min ||w||2 +
, 0
w, 2
n

(10)

i=1

i, : source(d) = xi 6= di : hw, (di ) (d)i (di , d)
slack variables, , introduced training example, xi C constant
controls trade-off training error minimization margin maximization.
Note slack variables combined loss incurred linear constraints. means high loss output must separated larger margin
low loss output, much larger slack variable satisfy constraint. Alternatively, loss function used rescale slack parameters, case
constraints (10) replaced hw, (di ) (d)i 1 (dii ,d) . Margin rescaling
theoretically less desirable scale invariant, therefore requires tuning
additional hyperparameter compared slack rescaling. However, empirical results show
9. http://svmlight.joachims.org/svm_struct.html

654

fiSentence Compression Tree Transduction

little difference two rescaling methods (Tsochantaridis et al., 2005). use
margin rescaling practical reason approximated accurately
slack rescaling chart based inference method.
optimization problem (10) approximated using algorithm proposed
Tsochantaridis et al. (2005). algorithm finds small set constraints fullsized optimization problem ensures sufficiently accurate solution. Specifically,
constructs nested sequence successively tighter relaxation original problem using
(polynomial time) cutting plane algorithm. training instance, algorithm
keeps track selected constraints defining current relaxation. Iterating
training examples, proceeds finding output radically violates
constraint. case, optimization crucially relies finding derivation
high scoring high loss compared gold standard. requires finding
maximizer of:
H(d) = (d , d) hw, (di ) (d)i

(11)

search maximizer H(d) (11) performed decoding algorithm presented Section 3.4 extensions. Firstly, expanding (11)
H(d) = (d , d) h(di ), wi + h(d), wi see second term constant
respect d, thus influence search. decoding algorithm maximizes
last term, remains include loss function search process.
Loss functions
decompose
rules target ngrams derivation,
P
P

, r) +

(d
(d , d) =
nngrams(d) N (d , n), easily integrated
rd R
decoding algorithm. done adding partial loss, R (d , r) + N (d , n)
rules score line 20 Algorithm 4 (the ngrams recovered ngram contexts
manner used evaluate ngram features).
However, many loss functions decompose rules ngrams.
order calculate losses chart must stratified loss functions arguments
(Joachims, 2005). example, unigram precision measures ratio correctly predicted
tokens total predicted tokens therefore loss arguments pair counts,
(T P, F P ), true false positives. initialized (0, 0) updated
rule used derivation. equates checking whether target terminal
reference string incrementing relevant value. chart extended (stratified)
store loss arguments way ngram contexts stored decoding.
means rule accessing child chart cell get multiple entries,
different loss argument values well multiple ngram contexts (line 14 Algorithm
4). loss argument rule application calculated rule loss
arguments children. stored chart back-pointer list (lines
2223 Algorithm 4). Although loss evaluated correctly complete
derivations, evaluate loss partial derivations part cube-pruning
heuristic. Losses large space argument values coarsely approximated
beam search, prunes number chart entries constant size.
reason, focused mainly simple loss functions relatively small space
argument values, use wide beam search (200 unique items 500
items, whichever comes first).
655

fiCohn & Lapata

Algorithm 5 Find gold standard derivation pair trees (i.e., alignment).
Require: source tree, x, target tree,
1: let C[vS , vT ] R chart representing maximum number rules used align
nodes vS x vT
2: let B[vS , vT ] (P, x y) corresponding back-pointers, consisting production
pair aligned nodes productions variables
3: initialize chart, C[, ] =
4: initialize back-pointers, B[, ] = none
5: source nodes, vS x, bottom-up
6:
rules, r = hvS , h, , matches sub-tree rooted vS
7:
target nodes, vT y, matching
8:
let rule count, j 1
9:
variables, u
10:
find aligned child nodes, (cS , cT ), vS vT corresponding u
11:
update rule count, j j + C[cS , cT ]
12:
end
13:
n greater previous value chart
14:
update chart, C[vS , vT ] j
15:
update back-pointers, B[vS , vT ] (r, {(cS , cT )u})
16:
end
17:
end
18:
end
19: end
20: C[root(x), root(y)] 6=
21:
success; create derivation traversing back-pointers B[root(x), root(y)]
22: end
discussion far assumed given gold standard derivation, yi
glossing issue find it. Spurious ambiguity grammar means
often many derivations linking source target, none clearly
correct. select derivation using maximum number rules,
small, therefore provide maximum generality.10 found using Algorithm 5,
chart-based dynamic program similar alignment algorithm inverse transduction
grammars (Wu, 1997). algorithm time complexity O(S 2 R) size
larger two trees R number rules match node.
3.6 Loss Functions
training algorithm described highly modular theory support wide
range loss functions. widely accepted evaluation metric text compression. zero-one loss would straightforward define inappropriate problem,
10. experimented heuristics, including choosing derivation random selecting
derivation maximum minimum score model (all using search algorithm
different objective). these, maximum scoring derivation competitive
maximum rules heuristic.

656

fiSentence Compression Tree Transduction

would always penalize target derivations differ even slightly reference
derivation. Ideally, would loss wider scoring range discriminate
derivations differ reference. may good compressions whereas others may entirely ungrammatical. reason developed
range loss functions draw inspiration various metrics used evaluating
text-to-text rewriting tasks summarization machine translation.
Loss functions defined derivations look item accessible including
tokens, ngrams CFG rules. first class loss functions calculates Hamming
distance unordered bags items. measures number predicted items
appear reference, along penalty short output:
hamming (d , d) = F P + max (l (T P + F P ), 0)

(12)

P F P number true false positives, respectively, comparing
predicted target, dT , reference, dT , l length reference.
include second term penalize overly short output otherwise predicting little
nothing would incur penalty.
created three instantiations loss function (12) over: 1) tokens,
2) ngrams (n 3), 3) CFG productions. case, loss argument space
quadratic size source tree. Hamming ngram loss attempt defining
loss function similar BLEU (Papineni, Roukos, Ward, & Zhu, 2002). latter
defined documents rather individual sentences, thus directly applicable
problem. Now, since losses operate unordered bags may reward
erroneous predictions, example, permutation reference tokens zero
token-loss. less problem CFG ngram losses whose items overlap,
thereby encoding partial order. Another problem loss functions described
penalize multiply predicting item occurred reference. could problem function words common sentences.
Therefore developed two additional loss functions take multiple predictions
account. first measures edit distance number insertions deletions
predicted reference compressions, bags-of-tokens. contrast
previous loss functions, requires true positive counts clipped
number occurrences type reference. edit distance given by:
X
edit (d , d) = p + r 2
min(pi , qi )
(13)


p q denote number target tokens predicted tree, target(d),
reference, = target(d ), respectively, pi qi counts type i. loss
arguments edit distance consist vector counts item type
reference, {pi , i}. space possible values exponential size source tree,
compared quadratic Hamming losses. Consequently, expect beam search
result many search errors using edit distance loss.
last loss function F1 measure, harmonic mean precision recall,
measured bags-of-tokens. edit distance, calculation requires counts
clipped number occurrences terminal type reference.
657

fiCohn & Lapata

Ref:
Pred:

[S [WHNP [WP what]] [S [NP [NNS records]] [VP [VBP are] [VP [VBN involved]]]]]
[S [WHNP [WP what]] [S [NP [NNS ones]] [VP [VBP are] [VBN involved]]]]
Loss
Token Hamming
3-gram Hamming
CFG Hamming
Edit distance
F1

Arguments
P = 3, F P = 1
P = 8, F P = 5
P = 8, F P = 1
p = (1, 0, 1, 1, 1)
p = (1, 0, 1, 1, 1)

Value
1/4
5/14
1/9
2
1/4

Table 1: Loss arguments values example predicted reference compressions.
Note loss values compared different loss functions;
values purely illustrative.

therefore use loss arguments calculation. F1 loss given by:
F1 (d , d) = 1
P

min(p ,q )

2 precision recall
precision + recall
P

(14)

min(p ,q )

precision = p recall = q . F1 shares arguments
edit distance loss, exponential space loss argument values
consequently subject severe pruning beam search used training.
illustrate loss functions, present example Table 1. Here,
prediction (Pred) reference (Ref) length (4 tokens), identical syntactic
structure, differ one word (ones versus records). Correspondingly, three
correct tokens one incorrect, forms arguments token Hamming loss,
resulting loss 1/4. ngram loss measured n 3 start end
string padded special symbols allow evaluation boundary ngrams.
CFG loss records one incorrect CFG production (the preterminal [NNS ones])
total nine productions. last two losses use arguments: vector values
counts reference type. first four cells correspond what, records,
involved, last cell records types. example, edit distance two
(one deletion one insertion) F1 loss 1/4 (precision recall 3/4).

4. Features
feature space defined source trees, x, target derivations, d. devised two
broad classes features, applying grammar rules ngrams target terminals.
defined single ngram feature, conditional log-probability trigram language
model. trained BNC (100 million words) using SRI Language Modeling
toolkit (Stolcke, 2002), modified Kneser-Ney smoothing.
rule hX,Y h, , i, extract features according templates detailed
below. templates give rise binary indicator features, except explicitly stated.
features perform boolean test, returning value 1 test succeeds 0
otherwise. example rule corresponding features shown Table 2.
658

fiSentence Compression Tree Transduction

Type: Whether rule extracted training set, created copy rule and/or
created delete rule. allows model learn preference
three sources grammar rules (see row Type Table 2)
Root: root categories source, X, target, , conjunction, X
(see rows Root Table 2).
Identity: source side, , target side, , full rule, (, , ). allows
model learn weights individual rules sharing elementary tree. Another feature checks rules source target elementary trees identical, =
(see rows Identity Table 2).
Unlexicalised Identity: identity feature templates replicated unlexicalised elementary trees, i.e., terminals removed frontiers (see
rows UnlexId Table 2).
Rule count: feature always 1, allowing model count number rules
used derivation (see row Rule count Table 2).
Word count: Counts number terminals , allowing global preference
shorter longer output. Additionally, record number terminals
source tree, used target terminal count find number
deleted terminals (see rows Word count Table 2).
Yield: features compare terminal yield source, (), target, ().
first feature checks identity two sequences, () (). use identity
features terminal yields, terminal source (see
rows Yield Table 2). replicate feature templates sequence
non-terminals frontier (pre-terminals variable non-terminals).
Length: Records difference lengths frontiers , whether
targets frontier shorter source (see rows Length Table 2).
features listed defined rules grammar. includes
copy delete rules, described Section 3.2, added address
problem unseen words productions source trees test time. Many
rules applied training set, receive weight share
features rules used training. However, training model learns
disprefer coverage rules unnecessary model training set,
described perfectly using extracted transduction rules. dual use training
set grammar extraction parameter estimation results bias coverage
rules. bias could addressed extracting grammar separate corpus,
case coverage rules would useful modeling training set
testing sets. However, solution problems, namely many target
trees training may longer reachable. bias possible solutions
interesting research problem deserves work.
659

fiCohn & Lapata

Rule: hNP,NNSi h[NP CD ADJP [NNS activists]], [NNS activists]i
Type
type = training set
1
Root
X = NP
1
Root
= NNS
1
Root
X = NP = NNS
1
Identity
= [NP CD ADJP [NNS activists]]
1
Identity
= [NNS activists]
1
Identity = [NP CD ADJP [NNS activists]] = [NNS activists]
1
UnlexId.
unlex. = [NP CD ADJP NNS]
1
UnlexId.
unlex. = NNS
1
UnlexId.
unlex. = [NP CD ADJP NNS] = NNS
1
Rule count

1
Word count
target terminals
1
Word count
source terminals 1
Yield
source = [activists] target = [activists]
1
Yield
terminal activists source target
1
Yield
non-terms. source = [CD, ADJP, NNS] target = [NNS]
1
Yield
non-terminal CD source target
1
Yield
non-terminal ADJP source target
1
Yield
non-terminal NNS source target
1
Length
difference length
2
Length
target shorter
1
Table 2: Features instantiated synchronous rule shown above. features
non-zero values displayed. number source terminals calculated using
source tree time rule applied.

5. Experimental Set-up
section present experimental set-up assessing performance
sentence compression model described above. give details corpora used, briefly
introduce McDonalds (2006) model used comparison approach, explain
system output evaluated.
5.1 Corpora
evaluated system three publicly available corpora. first Ziff-Davis
corpus, popular choice sentence compression literature. corpus originates
collection news articles computer products. created automatically
matching sentences occur article sentences occur abstract (Knight
& Marcu, 2002). two corpora11 created manually; annotators asked
produce target compressions deleting extraneous words source without changing
word order (Clarke & Lapata, 2008). One corpus sampled written sources,
11. Available http://homepages.inf.ed.ac.uk/s0460084/data/.

660

fiSentence Compression Tree Transduction

Corpus
CLspoken
CLwritten
Ziff-Davis

Articles
50
82


Sentences
1370
1433
1084

Training
882
908
1020

Development
78
63
32

Testing
410
462
32

Table 3: Sizes various corpora, measured articles sentence pairs. data split
training, development testing sets measured sentence pairs.

British National Corpus (BNC) American News Text corpus, whereas
created manually transcribed broadcast news stories. henceforth refer
two corpora CLwritten CLspoken, respectively. sizes three
corpora shown Table 3.
three corpora pose different challenges hypothetical sentence compression
system. Firstly, representative different domains text genres. Secondly,
different compression requirements. Ziff-Davis corpus aggressively
compressed comparison CLspoken CLwritten (Clarke & Lapata, 2008). CLspoken speech corpus, often contains incomplete ungrammatical utterances
speech artefacts disfluencies, false starts hesitations. utterances varying lengths, wordy whereas others cannot reduced further. means
compression system leave sentences uncompressed. Finally,
note CLwritten average longer sentences Ziff-Davis CLspoken. Parsers
likely make mistakes long sentences could potentially problematic
syntax-based systems one presented here.
Although model capable performing editing operation, reordering
substitution, learn training corpora. corpora contain
deletions, therefore model learn transduction rules encoding, e.g.,
reordering. Instead rules encode deleting inserting terminals restructuring internal nodes syntax tree. However, model capable general text
rewriting, given appropriate training set learn perform additional
edits. demonstrated recent results adapting model abstractive
compression (Cohn & Lapata, 2008), edit permitted, deletion.
experiments CLspoken CLwritten followed Clarke Lapatas (2008) partition training, test, development sets. partition sizes shown Table 3.
case Ziff-Davis corpus, Knight Marcu (2002) defined development
set. Therefore randomly selected (and held-out) 32 sentence pairs training
set form development set.
5.2 Comparison State-of-the-Art
evaluated results McDonalds (2006) discriminative model. approach,
sentence compression formalized classification task: pairs words source
sentence classified adjacent target compression. Let x = x1 , . . . , xN
denote source sentence target compression = y1 , . . . , yM yi occurs
x. function L(yi ) {1 . . . N } maps word yi target index word
661

fiCohn & Lapata

source (subject constraint L(yi ) < L(yi+1 )). McDonald defines score
compression sentence x dot product high dimensional feature
representation, f , bigrams corresponding weight vector, w,
score(x, y; w) =


X

hw, f (x, L(yj1 ), L(yj ))i

(15)

i=2

Decoding framework amounts finding combination bigrams maximize
scoring function (15). maximization solved using semi-Markov Viterbi
algorithm (McDonald, 2006).
model parameters estimated using Margin Infused Relaxed Algorithm
(MIRA Crammer & Singer, 2003), discriminative large-margin online learning technique.
McDonald (2006) uses similar loss function Hamming loss (see (12)) without
explicit length penalty. loss function counts number words falsely retained
dropped predicted target relative reference. McDonald employs rich feature
set defined words, parts speech, phrase structure trees, dependencies.
gathered adjacent words compression words dropped.
Clarke Lapata (2008) reformulate McDonalds (2006) model context integer
linear programming (ILP) augment constraints ensuring compressed
output grammatically semantically well formed. example, target sentence
negation, must included compression; source verb subject,
must retained compression. generate solve ILP every
source sentence using branch-and-bound algorithm. Since obtain performance
improvements McDonalds model several corpora, use comparison
model.
summarize, believe McDonalds (2006) model good basis comparison
several reasons. First, good performance, treated state-of-theart model. Secondly, similar model many respects training algorithm
feature space differs one important respect: compression performed
strings trees. McDonalds system make use syntax trees,
peripherally via feature set. contrast, syntax tree integral part
model.
5.3 Evaluation
line previous work assessed models output eliciting human judgments.
Following Knight Marcu (2002), conducted two separate experiments. first
experiment participants presented source sentence target compression
asked rate well compression preserved important information
source sentence. second experiment, asked rate grammaticality
compressed outputs. cases used five point rating scale high
number indicates better performance. randomly selected 20 sentences test
portion corpus. sentences compressed automatically system
McDonalds (2006) system. included gold standard compressions. materials
thus consisted 180 (20 3 3) source-target sentences. Latin square design ensured
subjects see two different compressions sentence. collected
662

fiSentence Compression Tree Transduction

ratings 30 unpaid volunteers, self reported native English speakers. studies
conducted Internet using WebExp,12 software package running Internetbased experiments.
report results using F1 computed grammatical relations (Riezler et al.,
2003). Although F1 conflates grammaticality importance single score, nevertheless shown correlate reliably human judgments (Clarke & Lapata,
2006). Furthermore, usefully employed development feature engineering parameter optimization experiments. measured F1 directed labeled
dependency relations. models compressed output parsed using RASP
dependency parser (Briscoe & Carroll, 2002). Note could extract dependencies directly output model since generates trees addition strings. However,
refrained order compare models equal footing.

6. Results
framework presented Section 3 quite flexible. Depending grammar extraction strategy, choice features, loss function, different classes models derived.
presenting results test set discuss specific model employed
experiments explain parameters instantiated.
6.1 Model Selection
parameter tuning model selection experiments conducted development set CLspoken corpus. obtained syntactic analyses source target
sentences Bikels (2002) parser. corpus automatically aligned using algorithm finds set deletions transform source target.
equivalent minimum edit distance script deletion operations permitted.
expected, predicted parse trees contained number errors, although
gold standard trees quantify error effect prediction
output. notice, however, errors source trees test set always
negatively affect performance model. many instances model able
recover errors still produce good output compressions. recoveries,
cases involved either deleting erroneous structure entirely preserving it.
often resulted poor output tree, string yield acceptable cases. Less
commonly, model corrected errors source using tree transformation rules.
rules acquired training set errors source tree
test tree. example, one transformation allows prepositional phrase
moved high VP attachment object NP attachment.
obtained synchronous tree substitution grammar CLspoken corpus using
method described Section 3.2. extracted maximally general synchronous rules.
complemented specified rules allowing recursion one ancestor
given node.13 Grammar rules represented features described Section 4.
important parameter modeling framework choice loss function.
12. See http://www.webexp.info/.
13. Rules pruned 5 variables 15 nodes.

663

fiCohn & Lapata

Losses
Hamming (tokens)
Hamming (ngram)
Hamming (CFG)
Edit Distance
F1
Reference

Rating
3.38
3.28
3.22
3.30
3.15
4.28

Std. dev
1.05
1.13
0.91
1.20
1.13
0.70

Table 4: Mean ratings system output (CLspoken development set) using different
loss functions.

evaluated loss functions presented Section 3.6 follows. performed grid search
hyper-parameters (a regularization parameter feature scaling parameter,
balances magnitude feature vectors scale loss function)14
minimized relevant loss development set, used corresponding system
output. gold standard derivation selected using maximum number rules
heuristic, described Section 3.5. beam limited 100 unique items 200 items
total. grammar filtered allow 50 target elementary trees
every source elementary tree.
next asked two human judges rate scale 1 5 systems compressions
optimized different loss functions. get idea quality output
included human-authored reference compressions. Sentences given high numbers
grammatical preserved important information. mean ratings
shown Table 4. seen differences among losses large,
standard deviation high. Hamming loss tokens performed best
mean rating 3.38, closely followed edit distance (3.30). chose former
latter less coarsely approximated search. subsequent experiments
report results using token-based Hamming loss.
wanted investigate synchronous grammar influences performance.
default system described used general rules together specialized rules
recursion depth limited one. experimented grammar uses
specialised rules maximum recursion depth two grammar uses solely
maximally general rules. Table 5 report average compression rate, relations-based
F1 Hamming loss tokens different grammars. see adding
specified rules allows better F1 (and loss) despite fact search space
remains same. observe slight degradation performance moving depth 2
rules. probably due increase spurious ambiguity affecting search quality,
allowing greater overfitting training data. number transduction rules
grammar grows substantially increased depth 20,764
maximally general extraction technique 33,430 62,116 specified rules depth
14. found setting regularization parameter C = 0.01 scaling parameter 1 generally
yields good performance across loss functions.

664

fiSentence Compression Tree Transduction

Model
max general rules
depth 1-specified rules
depth 2-specified rules
max rules
max scoring
unigram LM
bigram LM
trigram LM
features
rule features
token features

Compression rate
80.79
79.72
79.71
79.72
81.03
76.83
83.12
79.72
79.72
83.06
85.10

Relations F1
65.04
68.56
66.44
68.56
65.54
59.05
67.71
68.56
68.56
67.51
68.31

Loss
341
315
328
315
344
336
317
315
315
346
341

Table 5: Parameter exploration feature ablation studies (CLspoken development set).
default system shown asterisk.

1 2, respectively. growth grammar size exponential specification
depth therefore small values used.
inspected rules obtained maximally general extraction technique
better assess rules differ obtained vanilla SCFG (see Knight &
Marcu, 2002). Many rules (12%) deeper structure therefore would
licensed SCFG. due structural divergences source target
syntax trees training set. 13% rules describe change syntactic
category (X 6= ), therefore remaining 76% rules would allowable
Knight Marcus transducer. proportion SCFG rules decreases substantially
rule specification depth increased.
Recall Section 3.3 scoring function defined derivations rather
target trees strings, treat derivation using maximum number rules
gold standard derivation. sanity check, experimented selecting
derivation maximum score model. results Table 5 indicate
latter strategy effective selecting derivation maximum number
rules. conjecture due overfitting. training data used
extract grammar, derivations maximum score may consist rules
rare features model data well generalize unseen instances.
Finally, conducted feature ablation study assess features useful
task. particularly interested see ngram features would bring
benefit, especially since increase computational complexity decoding
training. experimented unigram, bigram, trigram language model. Note
unigram language model computationally expensive two models
need record ngram contexts chart. shown Table 5,
unigram language model substantially worse bigram trigram deliver
similar performances. examined impact features grouping
two broad classes, defined rules defined tokens. aim
see whether underlying grammar (represented rule-based features) contributes
665

fiCohn & Lapata

better compression output. results Table 5 reveal two feature groups
perform comparably. However, model using token-based features tends compress
less. features highly lexicalized, model able generalize well
unseen data. conclusion, full feature set better counts two
ablation sets, better compression rate.
results reported measured string output. done first
stripping tree structure compression output, reparsing, extracting dependency
relations finally comparing dependency relations reference. However,
may wish measure quality trees themselves, string yield.
simple way measure this15 would extract dependency relations directly
phrase-structure tree output.16 Compared dependencies extracted predicted
parses using Bikels (2002) parser output string, observe relation F1
score increases uniformly tasks, 2.50% 4.15% absolute. Therefore
systems tree output better encodes syntactic dependencies tree resulting
re-parsing string output. system part NLP pipeline, output
destined down-stream processing, accurate syntax tree extremely
important. true related tasks desired output tree, e.g.,
semantic parsing.

7. Model Comparison
section present results test set using best performing model
previous section. model uses grammar unlexicalized lexicalized rules
(recursion depth 1), Hamming loss based tokens, features Section 4.
model trained separately corpus (training portion). first discuss
results using relations F1 move human study.
Table 6 illustrates performance model (Transducer1) CLspoken, CLwritten, Ziff Davis. report results corpora using McDonalds (2006)
model (McDonald) improved version (Clarke ILP) put forward Clarke
Lapata (2008). present compression rate system reference gold
standard. cases tree transducer model outperforms McDonalds original model
improved ILP-based version.
Nevertheless, may argued model unfair advantage since
tends compress less models, therefore less likely make many
mistakes. ensure case, created version model
compression rate similar McDonald. done relatively straightforwardly
manipulating length penalty Hamming loss. smaller penalty
words model tend drop. Therefore, varied length penalty (and
hyper-parameters) development set order obtain compression rate similar
15. could alternatively measure tree metrics, tree edit distance. However, standard
measures used parser evaluation (e.g., EVALB) would suitable, assume parse
yield fixed. case reference target string often different systems output.
16. extract dependency relations conversion tool CoNLL 2007 shared task, available
http://nlp.cs.lth.se/pennconverter/.

666

fiSentence Compression Tree Transduction

Model
Transducer1
Transducer2
McDonald
Clarke ILP
Reference

CLspoken
Compression rate
82.30
69.89
68.56
77.70
76.11

Relations F1
66.63
59.58
47.48
54.12


Model
Transducer1
Transducer2
McDonald
Clarke ILP
Reference

CLwritten
Compression rate
76.52
61.09
60.12
71.99
70.24

Relations F1
58.02
49.48
48.39
54.84


Model
Transducer1
McDonald
Clarke ILP
Reference

Ziff Davis
Compression rate
67.45
66.26
48.67
56.61

Relations F1
56.55
54.12
46.77


Table 6: Results CLspoken, CLwritten, Ziff Davis corpus (testing set); compression
rate relations-based F1.

McDonald.17 model applied test set performance shown
Table 6 Transducer2. refrained Ziff-Davis, since original
transducer obtained compression rate comparable McDonald (67.45 vs. 66.26).
seen, Transducer2 yields better F1 CLspoken CLwritten. differences
F1 statistically significant using Wilcoxon test (p < 0.01). Transducer1
numerically outperforms McDonald Ziff-Davis, however difference significant
(the Ziff-Davis test set consists solely 32 sentences).
next consider results judgment elicitation study assesses
detail quality generated compressions. Recall participants judge compressed output two dimensions, grammaticality importance. compared
output system (Transducer2 CLspoken CLwritten Transducer1
Ziff-Davis) output McDonald (2006) reference gold standard. Table 7
illustrates examples compressions participants saw.
17. matched compression rate McDonald scaling length penalty 0.50 0.25
CLwritten CLspoken corpora, respectively. Another way control compression rate would
modify chart-based decoder fashion similar McDonald (2006). However, leave
future work.

667

fiCohn & Lapata

S: wish parents teachers could teacher,
could communicate.
M: wish teachers could teacher.
T: wish teachers could this, could communicate.
R: wish parents teachers could this, could
communicate.
S: Treasury refusing fund phase city technology
colleges.
M: Treasury refusing fund colleges.
T: Treasury refusing fund city technology colleges.
R: Treasury refusing fund city technology colleges.
S: Apparel makers use design clothes quickly produce
deliver best-selling garments.
M: Apparel makers use design clothes produce deliver
best-selling garments.
T: Apparel makers use design clothes.
R: Apparel makers use design clothes.
S: Earlier week, conference call analysts, bank said boosted
credit card reserves $350 million.
M: Earlier said credit card reserves $350 million.
T: conference call analysts, bank boosted card reserves $350
million.
R: conference call analysts bank said boosted credit card
reserves $350 million.
Table 7: Compression examples CLspoken, CLwritten, Ziff-Davis (S: source sentence, M: McDonald, 2006, T: transducer, R: reference gold standard)

Table 8 shows mean ratings18 system (and reference) CLspoken,
CLwritten, Ziff-Davis. carried Analysis Variance (Anova) examine
effect system type (McDonald, Transducer, Reference) compression ratings. Anova revealed reliable effect three corpora. used post-hoc Tukey
tests examine whether mean ratings system differed significantly (p < 0.01).
CLspoken corpus Transducer perceived significantly better McDonald, terms grammaticality importance. obtain result
CLwritten corpus. two systems achieve similar performances Ziff-Davis (the
grammaticality importance score differ significantly). Ziff-Davis seems
less challenging corpus CLspoken CLwritten less likely highlight differences among systems. example, Turner Charniak (2005) present several variants
noisy-channel model, achieve compressions similar quality Ziff-Davis
(grammaticality ratings varied 0.13 informativeness ratings 0.31
human evaluation). cases Transducer McDonald yield significantly
18. statistical tests reported subsequently done using mean ratings.

668

fiSentence Compression Tree Transduction

Model
Transducer
McDonald
Reference

CLspoken
Grammaticality
4.18
2.74
4.58

Importance
3.98
2.51
4.22

Model
Transducer
McDonald
Reference

CLwritten
Grammaticality
4.06
3.05
4.52

Importance
3.21
2.82
3.70

Model
Transducer
McDonald
Reference

Ziff-Davis
Grammaticality
4.07
3.98
4.65

Importance
3.23
3.22
4.12

Table 8: Mean ratings compression output elicited humans ( : sig. diff. McDonald ( < 0.01); : sig. diff. Reference ( < 0.01); using post-hoc Tukey
tests)

worse performance Reference, save one exception. CLspoken corpus,
significant difference Transducer gold standard.
results indicate highly expressive framework good model sentence compression. several experimental conditions, across different domains,
obtain better performance previous work. Importantly, model described
compression-specific, could easily adapted tasks, corpora languages (for
syntactic analysis tools available). supervised, model learns fit
compression rate training data. sense, somewhat inflexible cannot
easily adapt specific rate given user imposed application (e.g.,
displaying text small screens). Nevertheless, compression rate indirectly manipulated adopting loss functions encourage discourage compression directly
decoding stratifying chart length (McDonald, 2006).

8. Conclusions
paper formulated sentence compression tree-to-tree rewriting task.19
developed system licenses space possible rewrites using tree substitution grammar. grammar rule assigned weight learned discriminatively
within large margin model (Tsochantaridis et al., 2005). specialized algorithm used
learn model weights find best scoring compression model. argue
19. source code freely available http://homepages.inf.ed.ac.uk/tcohn/t3.

669

fiCohn & Lapata

proposed framework appealing several reasons. synchronous grammar
provides expressive power capture rewrite operations go beyond word deletion
reordering, changes non-terminal categories lexical substitution. Since
deletion-specific, model could ported rewriting tasks (see Cohn & Lapata,
2008, example) without overhead devising new algorithms decoding
training. Moreover, discriminative nature learning algorithm allows incorporation manner powerful features. rich feature space conjunction
choice appropriate loss function afford greater flexibility fitting empirical
data different domains tasks.
evaluated model three compression corpora (CLspoken, CLwritten, ZiffDavis) showed cases yields results superior state-of-the-art (McDonald, 2006). experiments designed assess several aspects proposed
framework complexity synchronous grammar, choice loss function,
effect various features, quality generated tree output. observed
performance improvements allowing maximally general grammar rules specified
once, producing larger lexicalized rules. concurs Galley McKeown
(2007) find lexicalization yields better compression output. choice
loss function appears less effect. devised three classes loss functions
based Hamming distance, Edit distance F1 score. Overall, simple token-based
Hamming loss achieved best results. conjecture due simplicity
evaluated precisely many loss functions isnt affected
poor parser output. feature ablation study revealed ngram features beneficial,
mirroring similar finding machine translation literature (Chiang, 2007). Finally,
found trees created generation algorithm accurate compared
output parser applied string output. augurs well use cascaded NLP
pipeline, systems use compression output input processing,
potentially make better use system output.
Future extensions many varied. obvious extension concerns porting
framework rewriting applications document summarization (Daume III &
Marcu, 2002) machine translation (Chiang, 2007). Initial work (Cohn & Lapata, 2008)
shows tree-to-tree transduction model presented easily adapted
sentence abstraction task compression takes place using rewrite operations
restricted word deletion. Examples include substitution, reordering, insertion.
future directions involve detailed feature engineering, including source conditioned features ngram features besides language model. research needed
establish suitable loss functions compression rewriting tasks. particular
interesting experiment loss functions incorporate wider range
linguistic features beyond parts speech. Examples include losses based parse trees
semantic similarity. Finally, experiments presented work use grammar
acquired training corpus. However, nothing inherent formalization
restricts us particular grammar. therefore plan investigate potential method unsupervised semi-supervised grammar induction techniques
rewriting tasks including paraphrase generation machine translation.
670

fiSentence Compression Tree Transduction

Acknowledgments
grateful Philip Blunsom insightful comments suggestions
anonymous referees whose feedback helped substantially improve present paper.
Special thanks James Clarke sharing implementations Clarke Lapatas
(2008) McDonalds (2006) models us. acknowledge support EPSRC
(grants GR/T04540/01 GR/T04557/01). work made use resources
provided Edinburgh Compute Data Facility (ECDF). ECDF partially
supported eDIKT initiative. preliminary version work published
proceedings EMNLP/CoNLL 2007.

References
Aho, A. V., & Ullman, J. D. (1969). Syntax directed translations pushdown assembler. Journal Computer System Sciences, 3, 3756.
Alshawi, H., Bangalore, S., & Douglas, S. (2000). Learning dependency translation models
collections finite state head transducers. Computational Linguistics, 26 (1), 45
60.
Berger, A. L., Pietra, S. A. D., & Pietra, V. J. D. (1996). maximum entropy approach
natural language processing. Computational Linguistics, 22 (1), 3971.
Bikel, D. (2002). Design multi-lingual, parallel-processing statistical parsing engine.
Proceedings 2nd International Conference Human Language Technology
Research, pp. 2427, San Diego, CA.
Briscoe, E. J., & Carroll, J. (2002). Robust accurate statistical annotation general text.
Proceedings Third International Conference Language Resources Evaluation, pp. 14991504, Las Palmas, Gran Canaria.
Carroll, J., Minnen, G., Pearce, D., Canning, Y., Devlin, S., & Tait, J. (1999). Simplifying
text language impaired readers. Proceedings 9th Conference European Chapter Association Computational Linguistics, pp. 269270, Bergen,
Norway.
Chandrasekar, R., & Srinivas, C. D. B. (1996). Motivations methods text simplification. Proceedings 16th International Conference Computational
Linguistics, pp. 10411044, Copenhagen, Danemark.
Chiang, D. (2007). Hierarchical phrase-based translation. Computational Linguistics, 33 (2),
201228.
Clarke, J., & Lapata, M. (2006). Models sentence compression: comparison across
domains, training requirements evaluation measures. Proceedings 21st
International Conference Computational Linguistics 44th Annual Meeting
Association Computational Linguistics, pp. 377384, Sydney, Australia.
Clarke, J., & Lapata, M. (2008). Global inference sentence compression: integer linear
programming approach. Journal Artificial Intelligence Research, 31, 399429.
671

fiCohn & Lapata

Cohn, T., & Lapata, M. (2008). Sentence compression beyond word deletion. Proceedings 22nd International Conference Computational Linguistics, pp. 137144,
Manchester, UK.
Collins, M. (2002). Discriminative training methods hidden Markov models: theory
experiments perceptron algorithms. Proceedings 2002 Conference
Empirical Methods Natural Language Processing, pp. 18, Morristown, NJ.
Collins, M. J. (1999). Head-driven statistical models natural language parsing. Ph.D.
thesis, University Pennsylvania, Philadelphia, PA.
Crammer, K., & Singer, Y. (2003). Ultraconservative online algorithms multiclass problems. Machine Learning, 3, 951999.
Daume III, H., & Marcu, D. (2002). noisy-channel model document compression.
Proceedings 40th Annual Meeting thev Association Computational
Linguistics, pp. 449456, Philadelphia, PA.
Eisner, J. (2003). Learning non-isomorphic tree mappings machine translation.
Companion Volume Proceedings 41st Annual Meeting Association
Computational Linguistics, pp. 205208, Sapporo, Japan.
Galley, M., Hopkins, M., Knight, K., & Marcu, D. (2004). Whats translation rule?.
Proceedings 2004 Human Language Technology Conference North
American Chapter Association Computational Linguistics, pp. 273280,
Boston, MA.
Galley, M., & McKeown, K. (2007). Lexicalized Markov grammars sentence compression.
Proceedings Human Language Technologies 2007: Conference North
American Chapter Association Computational Linguistics, pp. 180187,
Rochester, NY.
Grael, J., & Knight, K. (2004). Training tree transducers. Proceedings 2004 Human
Language Technology Conference North American Chapter Association
Computational Linguistics, pp. 105112, Boston, MA.
Hermjakob, U., Echihabi, A., & Marcu, D. (2002). Natural language based reformulation
resource wide exploitation question answering. Proceedings 11th Text
Retrieval Conference, Gaithersburg, MD.
Hori, C., & Furui, S. (2004). Speech summarization: approach word extraction
method evaluation. IEICE Transactions Information Systems, E87D(1), 1525.
Jing, H. (2000). Sentence reduction automatic text summarization. Proceedings
6th Applied Natural Language Processing Conference, pp. 310315, Seattle, WA.
Joachims, T. (2005). support vector method multivariate performance measures.
Proceedings 22nd International Conference Machine Learning, pp. 377384,
Bonn, Germany.
Kaji, N., Okamoto, M., & Kurohashi, S. (2004). Paraphrasing predicates written
language spoken language using web. Proceedings 2004 Human Language Technology Conference North American Chapter Association
Computational Linguistics, pp. 241248, Boston, MA.
672

fiSentence Compression Tree Transduction

Knight, K. (1999). Decoding complexity word-replacement translation models. Computational Linguistics, 25 (4), 607615.
Knight, K., & Marcu, D. (2002). Summarization beyond sentence extraction: probabilistic
approach sentence compression. Artificial Intelligence, 139 (1), 91107.
Lin, D., & Pantel, P. (2001). Discovery inference rules question answering. Natural
Language Engineering, 7 (4), 342360.
McDonald, R. (2006). Discriminative sentence compression soft syntactic constraints.
Proceedings 11th Conference European Chapter Association
Computational Linguistics, pp. 297304, Trento, Italy.
Nguyen, M. L., Horiguchi, S., Shimazu, A., & Ho, B. T. (2004). Example-based sentence
reduction using hidden markov model. ACM Transactions Asian Language
Information Processing, 3 (2), 146158.
Och, F. J., & Ney, H. (2004). alignment template approach statistical machine
translation. Computational Linguistics, 30 (4), 417449.
Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2002). BLEU: method automatic
evaluation machine translation. Proceedings 40th Annual Meeting thev
Association Computational Linguistics, pp. 311318, Philadelphia, PA.
Petrov, S., Barrett, L., Thibaux, R., & Klein, D. (2006). Learning accurate, compact,
interpretable tree annotation. Proceedings 21st International Conference
Computational Linguistics 44th Annual Meeting Association Computational Linguistics, pp. 433440, Sydney, Australia.
Riezler, S., King, T. H., Crouch, R., & Zaenen, A. (2003). Statistical sentence condensation
using ambiguity packing stochastic disambiguation methods lexical-functional
grammar. Proceedings 2003 Human Language Technology Conference
North American Chapter Association Computational Linguistics, pp.
118125, Edmonton, Canada.
Shieber, S., & Schabes, Y. (1990). Synchronous tree-adjoining grammars. Proceedings 13th International Conference Computational Linguistics, pp. 253258,
Helsinki, Finland.
Stolcke, A. (2002). SRILM extensible language modeling toolkit. Proceedings
International Conference Spoken Language Processing, Denver, CO.
Tsochantaridis, I., Joachims, T., Hofmann, T., & Altun, Y. (2005). Large margin methods
structured interdependent output variables. Journal Machine Learning
Research, 6, 14531484.
Turner, J., & Charniak, E. (2005). Supervised unsupervised learning sentence
compression. Proceedings 43rd Annual Meeting Association Computational Linguistics, pp. 290297, Ann Arbor, MI.
Vandeghinste, V., & Pan, Y. (2004). Sentence compression automated subtitling:
hybrid approach. Text Summarization Branches Out: Proceedings ACL-04
Workshop, pp. 8995, Barcelona, Spain.
673

fiCohn & Lapata

Wu, D. (1997). Stochastic inversion transduction grammars bilingual parsing parallel
corpora. Computational Linguistics, 23 (3), 377404.
Yamada, K., & Knight, K. (2001). syntax-based statistical translation model. Proceedings 39th Annual Meeting Association Computational Linguistics,
pp. 523530, Toulouse, France.

674


