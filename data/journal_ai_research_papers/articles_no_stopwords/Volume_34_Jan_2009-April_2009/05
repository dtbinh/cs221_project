journal artificial intelligence

submitted published

sentence compression tree transduction
trevor cohn

tcohn inf ed ac uk

mirella lapata

mlap inf ed ac uk

school informatics
university edinburgh
crichton street edinburgh eh ab uk

abstract
presents tree tree transduction method sentence compression
model synchronous tree substitution grammar formalism allows local
distortion tree topology thus naturally capture structural mismatches
describe decoding framework model
trained discriminatively within large margin framework experimental sentence
compression bring significant improvements state art model

introduction
recent years witnessed increasing interest text text generation methods many
natural language processing applications ranging text summarisation question answering machine translation heart methods lies ability perform
rewriting operations instance text simplification identifies phrases sentences
document pose reading difficulty given user substitutes simpler alternatives carroll minnen pearce canning devlin tait chandrasekar
srinivas question answering questions often paraphrased order achieve
flexible matching potential answers lin pantel hermjakob echihabi
marcu another example concerns reformulating written language render
natural sounding speech synthesis applications kaji okamoto kurohashi

sentence compression perhaps one popular text text rewriting methods
aim produce summary single sentence retains important
information remaining grammatical jing appeal sentence compression
lies potential summarization generally document compression e g
displaying text small screens mobile phones pdas vandeghinste pan
much current work literature focuses simplified formulation
compression task allow rewriting operations word deletion
given input source sentence words x x x xn target compression formed
removing subset words knight marcu
despite restricted word deletion compression task remains challenging
modeling perspective figure illustrates source sentence target compression
taken one compression corpora used experiments see section details
case hypothetical compression system must apply series rewrite rules order
c

ai access foundation rights reserved

ficohn lapata














vp
whnp
rb

wp

exactly

np

vp
np

nns

whnp

vbd prp cc

records

wp

np

whnp

np

vbn

wp

nns

vbp

vbn

involved



records



involved

nns vbp

made ones

vp
vp

source

vp

b target

figure example sentence compression showing source target trees bold
source nodes terminals need removed produce target
string

whnp
rb



whnp

wp

wp

np



np
np

vp














vp

whnp

vp
vp


whnp

cc




whnp




np

vp






figure example transduction rules displayed pair tree fragments left
source fragment matched node source tree matching
part replaced right target fragment dotted lines denote variable
correspondences denotes node deletion

obtain target e g delete leaf nodes exactly delete subtrees made
ones merge subtrees corresponding records involved
concretely system must access rules shown figure rules
displayed pair tree fragments left fragment corresponds source
right target instance rule states wh noun phrase whnp
consisting adverb rb wh pronoun wp e g exactly rewritten
wh pronoun without adverb two things note first
syntactic information plays important role since deletion decisions limited
individual words often span larger constituents secondly large number
compression rules varying granularity complexity see rule figure
previous solutions compression cast mostly supervised
learning setting unsupervised methods see clarke lapata hori furui
turner charniak sentence compression often modeled generative framework


fisentence compression tree transduction

aim estimate joint probability p x source sentence x
target compression knight marcu turner charniak galley
mckeown approaches essentially learn rewrite rules similar shown
figure parsed parallel corpus subsequently use best
compression set possible compressions given sentence approaches
model compression discriminatively subtree deletion riezler king crouch zaenen
nguyen horiguchi shimazu ho mcdonald
despite differences formulation existing specifically designed sentence compression mind generally applicable tasks requiring
complex rewrite operations substitutions insertions reordering common
assumption underlying previous work tree structures representing source
sentences target compressions isomorphic e exists edge preserving
bijection nodes two trees assumption valid sentence compression hold rewriting tasks consequently sentence compression
restrictive cannot readily adapted generation
since able handle structural lexical divergences related issue concerns deletion operations often take place without considering
structure target compression goal generate compressed string rather
tree representing without syntax language model turner charniak
explicit generation mechanism licenses tree transformations
guarantee compressions well formed syntactic structures
straightforward process subsequent generation analysis tasks
present sentence compression model deletion specific
account ample rewrite operations scales rewriting tasks formulate
compression tree tree rewriting synchronous grammar rules
shown figure specifically adopt synchronous tree substitution
grammar stsg formalism eisner model non isomorphic tree structures
efficient inference grammar induced
parallel corpus propose discriminative model rewriting task
viewed weighted tree tree transducer learning framework makes use
large margin put forward tsochantaridis joachims hofmann altun
efficiently learns prediction function minimize given loss function
develop appropriate used training e learning
model weights decoding e finding plausible compression model
beyond sentence compression hope work described might
relevance tasks involving structural matching see discussion section
remainder structured follows section provides overview
related work section presents stsg framework compression model
employ experiments section discusses experimental set section
presents discussion future work concludes

related work
synchronous context free grammars scfgs aho ullman generalization
context free grammar cfg formalism simultaneously produce strings two


ficohn lapata

languages used extensively syntax statistical mt examples
include inversion transduction grammar wu head transducers alshawi bangalore
douglas hierarchical phrase translation chiang several variants
tree transducers yamada knight grael knight
sentence compression bears resemblance machine translation instead translating one language another translating long sentences shorter ones
within language therefore surprising previous work adopted
scfgs compression task specifically knight marcu proposed noisychannel formulation sentence compression model consists two components
language model p whose role guarantee compression output grammatical channel model p x capturing probability source sentence x
expansion target compression decoding searches compression maximizes p p x channel model stochastic scfg
rules extracted parsed parallel corpus weights estimated
maximum likelihood galley mckeown obtain improved scfg
probability estimates markovization turner charniak note scfg
rules expressive enough model structurally complicated compressions
restricted trees depth remedy supplying synchronous grammar set general special rules example allow rules form
hnp npi h np np cc np np boxed subscripts added distinguish
two nps
work formulates sentence compression framework synchronous treesubstitution grammar stsg eisner stsg allows describe non isomorphic tree
pairs grammar rules comprise trees arbitrary depth thus suited textrewriting tasks typically involve number local modifications input text
especially modification described succinctly terms syntactic transformations dropping adjectival phrase converting passive verb phrase active
form stsg restricted version synchronous tree adjoining grammar stag shieber
schabes without adjunction operation stag affords mild context sensitivity
however increased cost inference scfg stsg weakly equivalent
string languages identical produce equivalent tree pairs example
figure rules expressed scfg rules rule cannot
source target fragments two level trees fact would impossible
describe trees figure scfg grammar rules therefore general
obtained knight marcu account elaborate tree
divergences moreover adopting expressive grammar formalism naturally model syntactically complex compressions without specify additional rules
turner charniak
synchronous grammar license large number compressions given source
tree grammar rule typically score overall score compression sentence x derived previous work estimates scores generatively
discussed opt discriminative training procedure allows incorporation manner powerful features use large margin technique proposed
tsochantaridis et al framework attractive supports configurable loss function describes extent predicted target tree differs


fisentence compression tree transduction

reference tree devising suitable loss functions model straightforwardly
adapted text rewriting tasks besides sentence compression
mcdonald presents sentence compression model uses discriminative
large margin model rich feature set defined compression bigrams
including parts speech parse trees dependency information without however making explicit use synchronous grammar decoding model amounts finding
combination bigrams maximize scoring function defined adjacent words
compression intervening words dropped model differs
mcdonalds two important respects first capture complex tree transformations go beyond bigram deletion tree decoding
better able preserve grammaticality compressed output second treebased representation allows greater modeling flexibility e g defining wide range
loss functions tree string yield contrast mcdonald define loss
functions final compression
although bulk sentence compression relies parallel corpora
modeling purposes approaches use training data small amount
example work hori furui propose model automatically
transcribed spoken text method scores candidate compressions language
model combined significance score indicating whether word topical
score representing speech recognizers confidence transcribing given word
correctly despite conceptually simple knowledge lean model operates
word level since take syntax account means deleting
constituents spanning several subtrees e g relative clauses clarke lapata
unsupervised greatly improved linguistically motivated
constraints used decoding

formulation
mentioned earlier formulate sentence compression tree tree rewriting
weighted synchronous grammar coupled large margin training process
model learns parallel corpus input uncompressed output compressed pairs
x xn yn predict target labeled tree source labeled tree x
capture dependency x weighted stsg define
following section section discusses extract grammar parallel
corpus rule score ngram output tree
overall score compression sentence x derived introduce scoring
function section explain training section framework
decoding amounts finding best target tree licensed grammar given source
tree present chart decoding section
synchronous grammar
synchronous grammar defines space valid source target tree pairs much
regular grammar defines space valid trees synchronous grammars treated tree
transducers reasoning space possible sister trees given tree
trees produced alongside given tree essentially transducer


ficohn lapata

generative process creating pair trees
initialize source tree x rs
initialize target tree rt
initialize stack frontier nodes f rs rt
node pairs vs vt f
choose rule hvs vt h
rewrite node vs x
rewrite node vt
variables u
aligned child nodes cs ct vs vt corresponding u
push cs ct f
end
end
x complete

takes tree input produces tree output grammar rules specify
steps taken transducer recursively mapping tree fragments input tree
fragments target tree many families synchronous grammars see
section elect use synchronous tree substitution grammar stsg one
simpler formalisms consequently efficient inference still
complex enough model rich suite tree edit operations
stsg tuple g ns nt p rs rt n non terminals
terminals subscripts indicating source target respectively p productions rs ns rt nt distinguished root symbols
production rewrite rule two aligned non terminals x ns nt
source target
hx h

elementary trees rooted symbols x respectively note
synchronous context free grammar scfg limits one level elementary
trees otherwise identical stsg imposes limits non terminal
leaves elementary trees referred frontier nodes variables
points recursion transductive process one one alignment frontier
nodes specified alignment represent deletion insertion
aligning node special symbol indicates node present
tree nodes aligned allows subtrees deleted
transduction disallow converse aligned nodes would license
unlimited insertion target tree independently source tree capability
would limited use sentence compression increasing complexity
inference
grammar productions used generative setting produce pairs trees
transductive setting produce target tree given source tree
present pseudo code processes generative process starts
two root symbols applies production rewrites symbols
productions elementary trees elementary trees might contain frontier nodes


fisentence compression tree transduction

transduction source tree target tree
require complete source tree x root node labeled rs
initialize target tree rt
initialize stack frontier nodes f root x rt
node pairs vs vt f
choose rule hvs vt h matches sub tree rooted vs x
rewrite vt
variables u
aligned child nodes cs ct vs vt corresponding u
push cs ct f
end
end
complete

case aligned pairs frontier nodes pushed stack later rewritten
another production process continues recursive fashion stack
empty frontier nodes remaining point two trees complete
sequence rewrite rules referred derivation source
target tree recovered deterministically
model uses stsg transductive setting source tree given
target tree generated necessitates different rewriting process
shown start source tree rt target root symbol
aligned root node source denoted root x choose production
rewrite pair aligned non terminals productions source side matches
source tree target symbol rewritten variable
matching node source corresponding leaf node target tree pushed
stack later processing process repeats stack empty
therefore source tree covered complete target tree
use term derivation refer sequence production applications target
string yield target tree given reading non terminals tree
left right manner
let us consider compression example figure tree editing rules
figure encoded stsg productions figure see rules production
reproduces tree pair figure production tree pair notation
figure primarily space reasons uses brackets indicate constituent boundaries
brackets surround constituents non terminal child nodes
terminals non terminals bracketed subtrees boxed indices short hand notation
alignment example rule specify two wp non terminals
aligned rb node occurs source tree e heads deleted subtree grammar rules allow differences non terminal category source
target seen rules allow arbitrarily deep elementary trees
special care must taken aligned variables nodes aligned signify source
sub tree point deleted without affecting target tree reason safely
ignore source nodes deleted manner



ficohn lapata














hwhnp whnpi
hs npi
hs vpi
hs vpi
hs si
hwp wpi
hnp npi
hnns nnsi
hvp vpi
hvbp vbpi
hvp vpi
hvbn vbni

rules perform major tree edits
h whnp rb wp whnp wp
h np vp np
h np vp vp
h whnp vp
h whnp cc whnp np vp
rules preserve tree structure
h wp wp
h np nns np nns
h nns records nns records
h vp vbp vp vp vbp vp
h vbp vbp
h vp vbn vp vbn
h vbn involved vbn involved

figure rules synchronous tree substitution grammar stsg capable generating sentence pair figure equivalently grammar defines
transducer convert source tree figure target tree
figure b rule rewrites pair non terminals pair subtrees
shown bracketed notation

evidenced rule trees depth two rules complete
toy grammar describes tree pair figure rules copy parts
source tree target terminals e g rule internal nodes children
e g rule
figure shows grammar used transduce source tree
target tree figure first steps derivation shown graphically figure start source tree seek transduce root symbol
target root symbol denoted first rule applied rule figure source side whnp cc matches root source tree
requisite target category matching part source tree
rewritten rules target elementary tree whnp np vp three
three variables annotated reflect category transformations required
node whnp whnp np vp process continues leftmost
nodes labeled whnp whnp rule figure applied deletes
nodes left child shown rb retains right child subsequent rule completes
transduction whnp node matching string exactly continues visit variable node finishes variable nodes remaining
resulting desired target tree
grammar
previous section outlined stsg formalism employ sentence compression
model save one important detail grammar example could obtain


fisentence compression tree transduction

whnp exactly np records vp made
cc whnp np ones vp involved

whnp whnp rb exactly wp np np records vp made
vp whnp np ones vp involved

whnp wp wp np np records vp made
vp whnp np ones vp involved

whnp wp np np records vp vbd made np prp
vp whnp np ones vp vbp vp vbn involved

whnp wp np nns nns records
vp whnp np ones vp involved

whnp wp np nns records
vp whnp np ones vp involved

whnp np records vp np ones vp involved

whnp np records vp vp vp vbp vp vbn involved

whnp np records vp vbp vbp vp vp vbn involved
whnp np records vp vbp vp vp vbn involved
whnp np records vp vbp vp vbn vbn involved
whnp wp np nns records vp vbp vp vbn involved

figure derivation example sentence pair figure line shows rewrite step
denoted subscript identifies rule used frontier
nodes shown bold x indicating symbol x must transduced
subsequent steps sake clarity internal nodes
omitted

synchronous grammar hand automatically corpus combination
requirement grammar allows source trees training set
transduced corresponding target trees maximum generality devised
automatic method extract grammar parsed word aligned parallel compression
corpus method maps word alignment constituent level alignment
nodes source target trees pairs aligned subtrees next generalized create
tree fragments elementary trees form rules grammar
first step constituent alignment define
set source target constituent pairs whose yields aligned one another
word alignment base alignment template method och ney
uses word alignments define alignments ngrams called phrases
smt literature method finds pairs ngrams least one word one
ngrams aligned word word ngram aligned
word outside ngram addition require ngrams syntactic
constituents formally define constituent alignment
c vs vt vs vt



vs vt
vs vt source target tree nodes subtrees set word
alignments pairs word indices returns yield span subtree minimum
maximum word index yield exclusive operator figure shows


ficohn lapata











vp

whnp

np

rb

wp

nns

exactly



records

vp
np

whnp

vbd prp cc
made





wp


np

vp

nns vbp
ones



vbn
involved









vp

whnp

np

rb

wp

nns

exactly



records

whnp


np

made

np

vp
whnp

vbd prp cc




wp


np


vbn
involved










np

rb

wp

nns

exactly



records

whnp


vp

whnp

whnp

vbd prp cc
made





wp


np

vp

nns vbp
ones



wp np vp

vp
np

vp

vp

nns vbp
ones





vbn
involved

figure graphical depiction first two steps derivation figure source
tree shown left partial target tree right variable nodes
shown bold face dotted lines alignment

word alignment constituent alignments licensed sentence pair
figure
next step generalize aligned subtree pairs replacing aligned child subtrees
variable nodes example figure consider pair aligned subtrees
ones involved vp involved could extract rule
hs vpi h whnp wp np nns ones vp vbp vp vbn involved
vp vbp vp vbn involved



however rule specific consequently useful transduction
model order applied must see full subtree highly unlikely
occur another sentence ideally generalize rule match many
source trees thereby allow transduction previously unseen structures
example node pairs labeled vp vp vbp vbp vp vp vbn vbn
generalized nodes aligned constituents subscripts added distinguish


fisentence compression tree transduction








vp

whnp

np

np

vp
whnp np

vp

rb
wp nns vbd prp cc wp nns vbp vbn
exactly records made
ones involved


vp
whnp np
vp
wp nns vbp vbn
records involved

figure tree pair word alignments shown binary matrix dark square indicates
alignment words row column overlaid rectangles
constituent alignments inferred word alignment

two vp nodes addition nodes whnp wp np nns source
unaligned therefore generalized alignment signify deletion
perform possible generalizations example would produce
rule
hs vpi h whnp vp

many possible rules extracted applying different legal
combinations generalizations total example
shows minimial general rules extracted
minimal set synchronous rules describe tree pair rules
minimal sense cannot made smaller e g replacing subtree
variable still honoring word alignment figure shows resulting minimal
set synchronous rules example figure seen example
many rules extracted overly general ideally would extract every rule
every legal combination generalizations however leads massive number rules
exponential size source tree address allowing limited
number generalizations skipped extraction process equivalent
altering lines first make non deterministic decision whether
match ignore match continue descending source tree recursion depth
limits number matches ignored way example allow one
generalizations mutually exclusive take highest match trees
non deterministic matching step line allows matching options individually
implemented mutually recursive function replicates state process
different match
extension galley hopkins knight marcus technique extracting
scfg word aligned corpus consisting tree string pairs



ficohn lapata

extract x extracts minimal rules constituent aligned trees
require source tree x target tree constituent alignment
initialize source target sides rule x
initialize frontier alignment
nodes vs top

vs null aligned

vs

delete children

else vs aligned target node

choose target node vt
non deterministic choice

call extract vs vt

vs vt

delete children vs

delete children vt

end
end
emit rule hroot root h
level recursion extracting rules vp pair figure get
additional rules
hs vpi h whnp wp vp
hs vpi h whnp np vp vp
two levels recursion get
hs vpi h whnp wp vp
hs vpi h whnp wp np vp vp
hs vpi h whnp np nns vp vp
hs vpi h whnp np vp vbd vp vbd vbd
compared rule see specialized rules add useful structure
lexicalisation still sufficiently abstract generalize sentences unlike
rule number rules exponential recursion depth fixed depth
polynomial size source tree fragment set recursion depth
small number one two experiments
guarantee induced rules good coverage unseen trees
tree fragments containing previously unseen terminals non terminals even unseen
sequence children parent non terminal cannot matched grammar productions case transduction fail way
covering source tree however easily remedied adding
rules grammar allow source tree fully covered node
alternative equally valid techniques improving coverage simplify syntax trees
example done explicitly binarizing large productions e g petrov barrett thibaux
klein implicitly markov grammar grammar productions e g collins



fisentence compression tree transduction

hs si
hwhnp whnpi
hwp wpi
hs npi
hnp npi
hnns nnsi
hs vpi
hs vpi
hvp vpi
hvbp vbpi
hvp vpi
hvbn vbni














h whnp cc whnp np vp
h whnp rb wp whnp wp
h wp wp
h np vp np
h np nns np nns
h nns records nns records
h whnp vp
h np vp vp
h vp vbp vp vp vbp vp
h vbp vbp
h vp vbn vp vbn
h vbn involved vbn involved

figure minimal set stsg rules extracted aligned trees figure

source tree rule created copy node child nodes target tree
example see fragment np dt jj nn source tree add rule

hnp npi h np dt jj nn np dt jj nn

rules source node copied target tree therefore transduction trivially recreate original tree course grammar
rules work conjunction copying rules produce target trees
copy rules solve coverage unseen data solve
related compression occurs unseen cfg productions
source tree therefore applicable grammar rules copy rules
copy child nodes target none child subtrees deleted unless
parent node deleted higher level rule case children
deleted clearly would add considerable modelling flexibility able delete
children reason add explicit deletion rules source
cfg production allow subsets child nodes deleted linguistically
plausible manner
deletion rules attempt preserve important child nodes measure
importance head finding heuristic collins parser appendix collins
collins method finds single head child cfg production hand coded
tables non terminal type desire set child nodes run
matches rather stopping first match order match
found used ranking importance child ordered list child nodes
used create synchronous rules retain head heads heads


ficohn lapata

fragment np dt jj nn heads found following order nn dt
jj therefore create rules retain children nn dt nn dt jj nn
hnp npi h np dt jj nn np nn
hnp nni h np dt jj nn nn
hnp npi h np dt jj nn np dt nn
hnp npi h np dt jj nn np dt jj nn
note one child remains rule produced without parent node
seen second rule
linear model
stsg defines transducer capable mapping source tree many possible
target trees little use without kind weighting towards grammatical trees
constructed sensible stsg productions yield fluent compressed target sentences ideally model would define scoring function target
trees strings however instead operate derivations general may many
derivations produce target tree situation referred spurious ambiguity fully account spurious ambiguity would require aggregating derivations
produce target tree would break polynomial time dynamic program used inference rendering inference np complete knight
end define scoring function derivations
score w h wi



derivation consisting sequence rules w model parameters
vector valued feature function operator h inner product
parameters w learned training described section
feature function defined
x
x

r source
source

rd

mngrams

r rules derivation ngrams ngrams yield target
tree feature function returning vector feature values rule note
feature function access rule r source tree source
conditional model therefore overhead terms modeling
assumptions complexity inference
second summand ngrams yield target tree
feature function ngrams traditional weighted synchronous grammars
allow features decompose derivation e expressed first
summand however limiting requirement ngram features
allow modeling local coherence commonly used sentence compression
literature knight marcu turner charniak galley mckeown
derivation fully specifies source x source target tree target



fisentence compression tree transduction

clarke lapata hori furui mcdonald instance deleting
sub tree left right siblings critical know siblings
grammatical configuration yield still forms coherent string
reason allow ngram features specifically conditional log probability
ngram language model unfortunately comes price ngram features
significantly increase complexity inference used training decoding
decoding
decoding aims best target tree licensed grammar given source tree
mentioned deal derivations place target trees decoding finds
maximizing derivation


argmax

score w



source x

x given source tree source extracts source tree derivation
score defined maximization performed space derivations
given source tree defined transduction process shown
maximization solved chart dynamic program
shown extends earlier inference weighted stsgs eisner assume scoring function must decompose derivation
e features apply rules terminal ngrams relaxing assumption leads
additional complications increased time space complexity equivalent grammar intersection original grammar ngram language
model explained chiang context string transduction scfg
defines chart c record best scoring partial target tree
source node vs root non terminal back pointers b record maximizing
rule store pointers child chart cells filling variable rule chart
indexed n terminals left right edges target trees yield
allow scoring ngram features terminal ngrams provide sufficient context evaluate
ngram features overlapping cells boundary chart cell combined another
rule application operation performed boundary ngrams function line
best illustrated example trigram features n node
rewritten np fast car must store ngram context fast fast car
chart entry similarly vp skidded halt would ngram context skidded
halt applying parent rule np vp rewrites two trees adjacent
siblings need ngrams boundary np vp
easily retrieved two chart cells contexts combine right edge np
context fast car left edge vp context skidded get two trigrams
fast car skidded car skidded trigrams fast car skidded
halt already evaluated child chart cells
combined chart cell given context fast halt taking left right
strictly speaking terminals right edge required compression model would
create target string left right manner however general
allows reordering rules hpp ppi h pp np pp np rules required
text rewriting tasks besides sentence compression



ficohn lapata

exact chart decoding
require complete source tree x root node labeled rs
let c v l r chart representing score best derivation transducing
tree rooted v tree root category ngram context l
let b v l p x nt l corresponding back pointers consisting
production source node target category ngram context
productions variables
initialize chart c
initialize back pointers b none
source nodes vs x bottom

rules r hvs h matches sub tree rooted vs

let target ngrams wholly contained

let features vector r x x

let l empty ngram context

let score q

variables u

source child node cu vs corresponding u

let tu non terminal target child node corresponding u

choose child chart entry qu c cu tu lu
non deterministic choice lu

let boundary ngrams r lu

update features x

update ngram context l merge ngram context l lu

update score q q qu

end

update score q q h wi

q c vs l

update chart c vs l q

update back pointers b vs l r cu tu lu u

end

end
end
best root chart entry l argmaxl c root x rt l
create derivation traversing back pointers b root x rt l

edges two child cells merging process performed merge ngram context
function line finally add artificial root node target tree n artificial
start terminals one end terminal allows ngram features applied
boundary ngrams beginning end target string
decoding processes source tree post order traversal finding
set possible trees ngram contexts source node inserting
chart rules match node processed lines feature vector
calculated rule ngrams therein line ngrams bordering child
cells filling rules variables line note feature vector includes
features specific rule boundary ngrams wholly contained


fisentence compression tree transduction

child cell reason score sum scores child cell line
feature vector model weights line ngram context l
calculated combining rules frontier ngram contexts child cells line
finally chart entry node updated score betters previous value
lines
choosing child chart cell entry line many different entries
different ngram context lu affects ngram features consequently
ngram context l score q rule non determinism means every
combination child chart entries chosen variable combinations
evaluated inserted chart number combinations product
number child chart entries variable bounded tt n v
tt size target lexicon v number variables therefore
asymptotic time complexity decoding sr tt n v number
source nodes r number matching rules node high complexity
clearly makes exact decoding infeasible especially n v large
adopt popular syntax inspired machine translation address
chiang firstly use beam search limits number different
ngram contexts stored chart cell constant w changes base
complexity term leading improved srw v still exponential
number variables addition use chiangs cube pruning heuristic
limit number combinations cube pruning uses heuristic scoring function
approximates conditional log probability ngram language model logprobability unigram model allows us visit combinations best first
order heuristic scoring function beam filled beam rescored
correct scoring function done cheaply w v time leading
overall time complexity decoding srw v refer interested reader
work chiang details
training
turn derivations scored model given source
tree space sister target trees implied synchronous grammar often large
majority trees ungrammatical poor compressions job
training weights reference target trees high scores
many target trees licensed grammar given lower scores
explained section define scoring function derivations function
given reproduced
f w

argmax hw



source x

equation finds best scoring derivation given source x linear model
recall derivation generates source tree x target tree goal
use conditional log probability ngram language model ngram feature order
use ngram features binary identity features specific ngrams would first advisable
construct approximation decomposes derivation use cube pruning heuristic



ficohn lapata

training procedure parameter vector w satisfies condition
source xi di hw di



xi di ith training source tree reference derivation condition states
training instances reference derivation least high scoring
derivations ideally would know extent predicted target
tree differs reference tree example compression differs gold
standard respect one two words treated differently compression
bears resemblance another important factor length compression
compressions whose length similar gold standard preferable longer
shorter output loss function yi quantifies accuracy prediction
respect true output value yi
plethora different discriminative training frameworks optimize
linear model possibilities include perceptron training collins log linear optimisation conditional log likelihood berger pietra pietra large margin
methods base training tsochantaridis et al framework learning
support vector machines svms structured output spaces svmstruct implementation framework supports configurable loss function particularly
appealing context sentence compression generally text text generation efficient training powerful regularization latter
critical discriminative large numbers features would otherwise
fit training sample expense generalization accuracy briefly summarize
detailed description refer interested reader
work tsochantaridis et al
traditionally svms learn linear classifier separates two classes
largest possible margin analogously structured svms attempt separate correct
structure structures large margin learning objective
structured svm uses soft margin formulation allows errors training set
via slack variables
n


cx
min w

w
n





source xi di hw di di
slack variables introduced training example xi c constant
controls trade training error minimization margin maximization
note slack variables combined loss incurred linear constraints means high loss output must separated larger margin
low loss output much larger slack variable satisfy constraint alternatively loss function used rescale slack parameters case
constraints replaced hw di dii margin rescaling
theoretically less desirable scale invariant therefore requires tuning
additional hyperparameter compared slack rescaling however empirical
http svmlight joachims org svm struct html



fisentence compression tree transduction

little difference two rescaling methods tsochantaridis et al use
margin rescaling practical reason approximated accurately
slack rescaling chart inference method
optimization approximated proposed
tsochantaridis et al finds small set constraints fullsized optimization ensures sufficiently accurate solution specifically
constructs nested sequence successively tighter relaxation original
polynomial time cutting plane training instance
keeps track selected constraints defining current relaxation iterating
training examples proceeds finding output radically violates
constraint case optimization crucially relies finding derivation
high scoring high loss compared gold standard requires finding
maximizer
h hw di



search maximizer h performed decoding presented section extensions firstly expanding
h h di wi h wi see second term constant
respect thus influence search decoding maximizes
last term remains include loss function search process
loss functions
decompose
rules target ngrams derivation
p
p

r



nngrams n n easily integrated
rd r
decoding done adding partial loss r r n n
rules score line ngrams recovered ngram contexts
manner used evaluate ngram features
however many loss functions decompose rules ngrams
order calculate losses chart must stratified loss functions arguments
joachims example unigram precision measures ratio correctly predicted
tokens total predicted tokens therefore loss arguments pair counts
p f p true false positives initialized updated
rule used derivation equates checking whether target terminal
reference string incrementing relevant value chart extended stratified
store loss arguments way ngram contexts stored decoding
means rule accessing child chart cell get multiple entries
different loss argument values well multiple ngram contexts line
loss argument rule application calculated rule loss
arguments children stored chart back pointer list lines
although loss evaluated correctly complete
derivations evaluate loss partial derivations part cube pruning
heuristic losses large space argument values coarsely approximated
beam search prunes number chart entries constant size
reason focused mainly simple loss functions relatively small space
argument values use wide beam search unique items
items whichever comes first


ficohn lapata

gold standard derivation pair trees e alignment
require source tree x target tree
let c vs vt r chart representing maximum number rules used align
nodes vs x vt
let b vs vt p x corresponding back pointers consisting production
pair aligned nodes productions variables
initialize chart c
initialize back pointers b none
source nodes vs x bottom

rules r hvs h matches sub tree rooted vs

target nodes vt matching

let rule count j

variables u

aligned child nodes cs ct vs vt corresponding u

update rule count j j c cs ct

end

n greater previous value chart

update chart c vs vt j

update back pointers b vs vt r cs ct u

end

end

end
end
c root x root

success create derivation traversing back pointers b root x root
end
discussion far assumed given gold standard derivation yi
glossing issue spurious ambiguity grammar means
often many derivations linking source target none clearly
correct select derivation maximum number rules
small therefore provide maximum generality found
chart dynamic program similar alignment inverse transduction
grammars wu time complexity r size
larger two trees r number rules match node
loss functions
training described highly modular theory support wide
range loss functions widely accepted evaluation metric text compression zero one loss would straightforward define inappropriate
experimented heuristics including choosing derivation random selecting
derivation maximum minimum score model search
different objective maximum scoring derivation competitive
maximum rules heuristic



fisentence compression tree transduction

would penalize target derivations differ even slightly reference
derivation ideally would loss wider scoring range discriminate
derivations differ reference may good compressions whereas others may entirely ungrammatical reason developed
range loss functions draw inspiration metrics used evaluating
text text rewriting tasks summarization machine translation
loss functions defined derivations look item accessible including
tokens ngrams cfg rules first class loss functions calculates hamming
distance unordered bags items measures number predicted items
appear reference along penalty short output
hamming f p max l p f p



p f p number true false positives respectively comparing
predicted target dt reference dt l length reference
include second term penalize overly short output otherwise predicting little
nothing would incur penalty
created three instantiations loss function tokens
ngrams n cfg productions case loss argument space
quadratic size source tree hamming ngram loss attempt defining
loss function similar bleu papineni roukos ward zhu latter
defined documents rather individual sentences thus directly applicable
since losses operate unordered bags may reward
erroneous predictions example permutation reference tokens zero
token loss less cfg ngram losses whose items overlap
thereby encoding partial order another loss functions described
penalize multiply predicting item occurred reference could function words common sentences
therefore developed two additional loss functions take multiple predictions
account first measures edit distance number insertions deletions
predicted reference compressions bags tokens contrast
previous loss functions requires true positive counts clipped
number occurrences type reference edit distance given
x
edit p r
min pi qi



p q denote number target tokens predicted tree target
reference target respectively pi qi counts type loss
arguments edit distance consist vector counts item type
reference pi space possible values exponential size source tree
compared quadratic hamming losses consequently expect beam search
many search errors edit distance loss
last loss function f measure harmonic mean precision recall
measured bags tokens edit distance calculation requires counts
clipped number occurrences terminal type reference


ficohn lapata

ref
pred

whnp wp np nns records vp vbp vp vbn involved
whnp wp np nns ones vp vbp vbn involved
loss
token hamming
gram hamming
cfg hamming
edit distance
f

arguments
p f p
p f p
p f p
p
p

value






table loss arguments values example predicted reference compressions
note loss values compared different loss functions
values purely illustrative

therefore use loss arguments calculation f loss given
f
p

min p q

precision recall
precision recall
p



min p q

precision p recall q f shares arguments
edit distance loss exponential space loss argument values
consequently subject severe pruning beam search used training
illustrate loss functions present example table
prediction pred reference ref length tokens identical syntactic
structure differ one word ones versus records correspondingly three
correct tokens one incorrect forms arguments token hamming loss
resulting loss ngram loss measured n start end
string padded special symbols allow evaluation boundary ngrams
cfg loss records one incorrect cfg production preterminal nns ones
total nine productions last two losses use arguments vector values
counts reference type first four cells correspond records
involved last cell records types example edit distance two
one deletion one insertion f loss precision recall

features
feature space defined source trees x target derivations devised two
broad classes features applying grammar rules ngrams target terminals
defined single ngram feature conditional log probability trigram language
model trained bnc million words sri language modeling
toolkit stolcke modified kneser ney smoothing
rule hx h extract features according templates detailed
templates give rise binary indicator features except explicitly stated
features perform boolean test returning value test succeeds
otherwise example rule corresponding features shown table


fisentence compression tree transduction

type whether rule extracted training set created copy rule
created delete rule allows model learn preference
three sources grammar rules see row type table
root root categories source x target conjunction x
see rows root table
identity source side target side full rule allows
model learn weights individual rules sharing elementary tree another feature checks rules source target elementary trees identical
see rows identity table
unlexicalised identity identity feature templates replicated unlexicalised elementary trees e terminals removed frontiers see
rows unlexid table
rule count feature allowing model count number rules
used derivation see row rule count table
word count counts number terminals allowing global preference
shorter longer output additionally record number terminals
source tree used target terminal count number
deleted terminals see rows word count table
yield features compare terminal yield source target
first feature checks identity two sequences use identity
features terminal yields terminal source see
rows yield table replicate feature templates sequence
non terminals frontier pre terminals variable non terminals
length records difference lengths frontiers whether
targets frontier shorter source see rows length table
features listed defined rules grammar includes
copy delete rules described section added address
unseen words productions source trees test time many
rules applied training set receive weight share
features rules used training however training model learns
disprefer coverage rules unnecessary model training set
described perfectly extracted transduction rules dual use training
set grammar extraction parameter estimation bias coverage
rules bias could addressed extracting grammar separate corpus
case coverage rules would useful modeling training set
testing sets however solution namely many target
trees training may longer reachable bias possible solutions
interesting deserves work


ficohn lapata

rule hnp nnsi h np cd adjp nns activists nns activists
type
type training set

root
x np

root
nns

root
x np nns

identity
np cd adjp nns activists

identity
nns activists

identity np cd adjp nns activists nns activists

unlexid
unlex np cd adjp nns

unlexid
unlex nns

unlexid
unlex np cd adjp nns nns

rule count


word count
target terminals

word count
source terminals
yield
source activists target activists

yield
terminal activists source target

yield
non terms source cd adjp nns target nns

yield
non terminal cd source target

yield
non terminal adjp source target

yield
non terminal nns source target

length
difference length

length
target shorter

table features instantiated synchronous rule shown features
non zero values displayed number source terminals calculated
source tree time rule applied

experimental set
section present experimental set assessing performance
sentence compression model described give details corpora used briefly
introduce mcdonalds model used comparison explain
system output evaluated
corpora
evaluated system three publicly available corpora first ziff davis
corpus popular choice sentence compression literature corpus originates
collection news articles computer products created automatically
matching sentences occur article sentences occur abstract knight
marcu two corpora created manually annotators asked
produce target compressions deleting extraneous words source without changing
word order clarke lapata one corpus sampled written sources
available http homepages inf ed ac uk data



fisentence compression tree transduction

corpus
clspoken
clwritten
ziff davis

articles




sentences




training




development




testing




table sizes corpora measured articles sentence pairs data split
training development testing sets measured sentence pairs

british national corpus bnc american news text corpus whereas
created manually transcribed broadcast news stories henceforth refer
two corpora clwritten clspoken respectively sizes three
corpora shown table
three corpora pose different challenges hypothetical sentence compression
system firstly representative different domains text genres secondly
different compression requirements ziff davis corpus aggressively
compressed comparison clspoken clwritten clarke lapata clspoken speech corpus often contains incomplete ungrammatical utterances
speech artefacts disfluencies false starts hesitations utterances varying lengths wordy whereas others cannot reduced means
compression system leave sentences uncompressed finally
note clwritten average longer sentences ziff davis clspoken parsers
likely make mistakes long sentences could potentially problematic
syntax systems one presented
although model capable performing editing operation reordering
substitution learn training corpora corpora contain
deletions therefore model learn transduction rules encoding e g
reordering instead rules encode deleting inserting terminals restructuring internal nodes syntax tree however model capable general text
rewriting given appropriate training set learn perform additional
edits demonstrated recent adapting model abstractive
compression cohn lapata edit permitted deletion
experiments clspoken clwritten followed clarke lapatas partition training test development sets partition sizes shown table
case ziff davis corpus knight marcu defined development
set therefore randomly selected held sentence pairs training
set form development set
comparison state art
evaluated mcdonalds discriminative model
sentence compression formalized classification task pairs words source
sentence classified adjacent target compression let x x xn
denote source sentence target compression ym yi occurs
x function l yi n maps word yi target index word


ficohn lapata

source subject constraint l yi l yi mcdonald defines score
compression sentence x dot product high dimensional feature
representation f bigrams corresponding weight vector w
score x w


x

hw f x l yj l yj





decoding framework amounts finding combination bigrams maximize
scoring function maximization solved semi markov viterbi
mcdonald
model parameters estimated margin infused relaxed
mira crammer singer discriminative large margin online learning technique
mcdonald uses similar loss function hamming loss see without
explicit length penalty loss function counts number words falsely retained
dropped predicted target relative reference mcdonald employs rich feature
set defined words parts speech phrase structure trees dependencies
gathered adjacent words compression words dropped
clarke lapata reformulate mcdonalds model context integer
linear programming ilp augment constraints ensuring compressed
output grammatically semantically well formed example target sentence
negation must included compression source verb subject
must retained compression generate solve ilp every
source sentence branch bound since obtain performance
improvements mcdonalds model several corpora use comparison
model
summarize believe mcdonalds model good basis comparison
several reasons first good performance treated state theart model secondly similar model many respects training
feature space differs one important respect compression performed
strings trees mcdonalds system make use syntax trees
peripherally via feature set contrast syntax tree integral part
model
evaluation
line previous work assessed output eliciting human judgments
following knight marcu conducted two separate experiments first
experiment participants presented source sentence target compression
asked rate well compression preserved important information
source sentence second experiment asked rate grammaticality
compressed outputs cases used five point rating scale high
number indicates better performance randomly selected sentences test
portion corpus sentences compressed automatically system
mcdonalds system included gold standard compressions materials
thus consisted source target sentences latin square design ensured
subjects see two different compressions sentence collected


fisentence compression tree transduction

ratings unpaid volunteers self reported native english speakers studies
conducted internet webexp software package running internetbased experiments
report f computed grammatical relations riezler et al
although f conflates grammaticality importance single score nevertheless shown correlate reliably human judgments clarke lapata
furthermore usefully employed development feature engineering parameter optimization experiments measured f directed labeled
dependency relations compressed output parsed rasp
dependency parser briscoe carroll note could extract dependencies directly output model since generates trees addition strings however
refrained order compare equal footing


framework presented section quite flexible depending grammar extraction strategy choice features loss function different classes derived
presenting test set discuss specific model employed
experiments explain parameters instantiated
model selection
parameter tuning model selection experiments conducted development set clspoken corpus obtained syntactic analyses source target
sentences bikels parser corpus automatically aligned finds set deletions transform source target
equivalent minimum edit distance script deletion operations permitted
expected predicted parse trees contained number errors although
gold standard trees quantify error effect prediction
output notice however errors source trees test set
negatively affect performance model many instances model able
recover errors still produce good output compressions recoveries
cases involved deleting erroneous structure entirely preserving
often resulted poor output tree string yield acceptable cases less
commonly model corrected errors source tree transformation rules
rules acquired training set errors source tree
test tree example one transformation allows prepositional phrase
moved high vp attachment object np attachment
obtained synchronous tree substitution grammar clspoken corpus
method described section extracted maximally general synchronous rules
complemented specified rules allowing recursion one ancestor
given node grammar rules represented features described section
important parameter modeling framework choice loss function
see http www webexp info
rules pruned variables nodes



ficohn lapata

losses
hamming tokens
hamming ngram
hamming cfg
edit distance
f
reference

rating







std dev







table mean ratings system output clspoken development set different
loss functions

evaluated loss functions presented section follows performed grid search
hyper parameters regularization parameter feature scaling parameter
balances magnitude feature vectors scale loss function
minimized relevant loss development set used corresponding system
output gold standard derivation selected maximum number rules
heuristic described section beam limited unique items items
total grammar filtered allow target elementary trees
every source elementary tree
next asked two human judges rate scale systems compressions
optimized different loss functions get idea quality output
included human authored reference compressions sentences given high numbers
grammatical preserved important information mean ratings
shown table seen differences among losses large
standard deviation high hamming loss tokens performed best
mean rating closely followed edit distance chose former
latter less coarsely approximated search subsequent experiments
report token hamming loss
wanted investigate synchronous grammar influences performance
default system described used general rules together specialized rules
recursion depth limited one experimented grammar uses
specialised rules maximum recursion depth two grammar uses solely
maximally general rules table report average compression rate relations
f hamming loss tokens different grammars see adding
specified rules allows better f loss despite fact search space
remains observe slight degradation performance moving depth
rules probably due increase spurious ambiguity affecting search quality
allowing greater overfitting training data number transduction rules
grammar grows substantially increased depth
maximally general extraction technique specified rules depth
found setting regularization parameter c scaling parameter generally
yields good performance across loss functions



fisentence compression tree transduction

model
max general rules
depth specified rules
depth specified rules
max rules
max scoring
unigram lm
bigram lm
trigram lm
features
rule features
token features

compression rate












relations f












loss












table parameter exploration feature ablation studies clspoken development set
default system shown asterisk

respectively growth grammar size exponential specification
depth therefore small values used
inspected rules obtained maximally general extraction technique
better assess rules differ obtained vanilla scfg see knight
marcu many rules deeper structure therefore would
licensed scfg due structural divergences source target
syntax trees training set rules describe change syntactic
category x therefore remaining rules would allowable
knight marcus transducer proportion scfg rules decreases substantially
rule specification depth increased
recall section scoring function defined derivations rather
target trees strings treat derivation maximum number rules
gold standard derivation sanity check experimented selecting
derivation maximum score model table indicate
latter strategy effective selecting derivation maximum number
rules conjecture due overfitting training data used
extract grammar derivations maximum score may consist rules
rare features model data well generalize unseen instances
finally conducted feature ablation study assess features useful
task particularly interested see ngram features would bring
benefit especially since increase computational complexity decoding
training experimented unigram bigram trigram language model note
unigram language model computationally expensive two
need record ngram contexts chart shown table
unigram language model substantially worse bigram trigram deliver
similar performances examined impact features grouping
two broad classes defined rules defined tokens aim
see whether underlying grammar represented rule features contributes


ficohn lapata

better compression output table reveal two feature groups
perform comparably however model token features tends compress
less features highly lexicalized model able generalize well
unseen data conclusion full feature set better counts two
ablation sets better compression rate
reported measured string output done first
stripping tree structure compression output reparsing extracting dependency
relations finally comparing dependency relations reference however
may wish measure quality trees string yield
simple way measure would extract dependency relations directly
phrase structure tree output compared dependencies extracted predicted
parses bikels parser output string observe relation f
score increases uniformly tasks absolute therefore
systems tree output better encodes syntactic dependencies tree resulting
parsing string output system part nlp pipeline output
destined stream processing accurate syntax tree extremely
important true related tasks desired output tree e g
semantic parsing

model comparison
section present test set best performing model
previous section model uses grammar unlexicalized lexicalized rules
recursion depth hamming loss tokens features section
model trained separately corpus training portion first discuss
relations f move human study
table illustrates performance model transducer clspoken clwritten ziff davis report corpora mcdonalds
model mcdonald improved version clarke ilp put forward clarke
lapata present compression rate system reference gold
standard cases tree transducer model outperforms mcdonalds original model
improved ilp version
nevertheless may argued model unfair advantage since
tends compress less therefore less likely make many
mistakes ensure case created version model
compression rate similar mcdonald done relatively straightforwardly
manipulating length penalty hamming loss smaller penalty
words model tend drop therefore varied length penalty
hyper parameters development set order obtain compression rate similar
could alternatively measure tree metrics tree edit distance however standard
measures used parser evaluation e g evalb would suitable assume parse
yield fixed case reference target string often different systems output
extract dependency relations conversion tool conll shared task available
http nlp cs lth se pennconverter



fisentence compression tree transduction

model
transducer
transducer
mcdonald
clarke ilp
reference

clspoken
compression rate






relations f






model
transducer
transducer
mcdonald
clarke ilp
reference

clwritten
compression rate






relations f






model
transducer
mcdonald
clarke ilp
reference

ziff davis
compression rate





relations f





table clspoken clwritten ziff davis corpus testing set compression
rate relations f

mcdonald model applied test set performance shown
table transducer refrained ziff davis since original
transducer obtained compression rate comparable mcdonald vs
seen transducer yields better f clspoken clwritten differences
f statistically significant wilcoxon test p transducer
numerically outperforms mcdonald ziff davis however difference significant
ziff davis test set consists solely sentences
next consider judgment elicitation study assesses
detail quality generated compressions recall participants judge compressed output two dimensions grammaticality importance compared
output system transducer clspoken clwritten transducer
ziff davis output mcdonald reference gold standard table
illustrates examples compressions participants saw
matched compression rate mcdonald scaling length penalty
clwritten clspoken corpora respectively another way control compression rate would
modify chart decoder fashion similar mcdonald however leave
future work



ficohn lapata

wish parents teachers could teacher
could communicate
wish teachers could teacher
wish teachers could could communicate
r wish parents teachers could could
communicate
treasury refusing fund phase city technology
colleges
treasury refusing fund colleges
treasury refusing fund city technology colleges
r treasury refusing fund city technology colleges
apparel makers use design clothes quickly produce
deliver best selling garments
apparel makers use design clothes produce deliver
best selling garments
apparel makers use design clothes
r apparel makers use design clothes
earlier week conference call analysts bank said boosted
credit card reserves million
earlier said credit card reserves million
conference call analysts bank boosted card reserves
million
r conference call analysts bank said boosted credit card
reserves million
table compression examples clspoken clwritten ziff davis source sentence mcdonald transducer r reference gold standard

table shows mean ratings system reference clspoken
clwritten ziff davis carried analysis variance anova examine
effect system type mcdonald transducer reference compression ratings anova revealed reliable effect three corpora used post hoc tukey
tests examine whether mean ratings system differed significantly p
clspoken corpus transducer perceived significantly better mcdonald terms grammaticality importance obtain
clwritten corpus two systems achieve similar performances ziff davis
grammaticality importance score differ significantly ziff davis seems
less challenging corpus clspoken clwritten less likely highlight differences among systems example turner charniak present several variants
noisy channel model achieve compressions similar quality ziff davis
grammaticality ratings varied informativeness ratings
human evaluation cases transducer mcdonald yield significantly
statistical tests reported subsequently done mean ratings



fisentence compression tree transduction

model
transducer
mcdonald
reference

clspoken
grammaticality




importance




model
transducer
mcdonald
reference

clwritten
grammaticality




importance




model
transducer
mcdonald
reference

ziff davis
grammaticality




importance




table mean ratings compression output elicited humans sig diff mcdonald sig diff reference post hoc tukey
tests

worse performance reference save one exception clspoken corpus
significant difference transducer gold standard
indicate highly expressive framework good model sentence compression several experimental conditions across different domains
obtain better performance previous work importantly model described
compression specific could easily adapted tasks corpora languages
syntactic analysis tools available supervised model learns fit
compression rate training data sense somewhat inflexible cannot
easily adapt specific rate given user imposed application e g
displaying text small screens nevertheless compression rate indirectly manipulated adopting loss functions encourage discourage compression directly
decoding stratifying chart length mcdonald

conclusions
formulated sentence compression tree tree rewriting task
developed system licenses space possible rewrites tree substitution grammar grammar rule assigned weight learned discriminatively
within large margin model tsochantaridis et al specialized used
learn model weights best scoring compression model argue
source code freely available http homepages inf ed ac uk tcohn



ficohn lapata

proposed framework appealing several reasons synchronous grammar
provides expressive power capture rewrite operations go beyond word deletion
reordering changes non terminal categories lexical substitution since
deletion specific model could ported rewriting tasks see cohn lapata
example without overhead devising decoding
training moreover discriminative nature learning allows incorporation manner powerful features rich feature space conjunction
choice appropriate loss function afford greater flexibility fitting empirical
data different domains tasks
evaluated model three compression corpora clspoken clwritten ziffdavis showed cases yields superior state art mcdonald experiments designed assess several aspects proposed
framework complexity synchronous grammar choice loss function
effect features quality generated tree output observed
performance improvements allowing maximally general grammar rules specified
producing larger lexicalized rules concurs galley mckeown
lexicalization yields better compression output choice
loss function appears less effect devised three classes loss functions
hamming distance edit distance f score overall simple token
hamming loss achieved best conjecture due simplicity
evaluated precisely many loss functions isnt affected
poor parser output feature ablation study revealed ngram features beneficial
mirroring similar finding machine translation literature chiang finally
found trees created generation accurate compared
output parser applied string output augurs well use cascaded nlp
pipeline systems use compression output input processing
potentially make better use system output
future extensions many varied obvious extension concerns porting
framework rewriting applications document summarization daume iii
marcu machine translation chiang initial work cohn lapata
shows tree tree transduction model presented easily adapted
sentence abstraction task compression takes place rewrite operations
restricted word deletion examples include substitution reordering insertion
future directions involve detailed feature engineering including source conditioned features ngram features besides language model needed
establish suitable loss functions compression rewriting tasks particular
interesting experiment loss functions incorporate wider range
linguistic features beyond parts speech examples include losses parse trees
semantic similarity finally experiments presented work use grammar
acquired training corpus however nothing inherent formalization
restricts us particular grammar therefore plan investigate potential method unsupervised semi supervised grammar induction techniques
rewriting tasks including paraphrase generation machine translation


fisentence compression tree transduction

acknowledgments
grateful philip blunsom insightful comments suggestions
anonymous referees whose feedback helped substantially improve present
special thanks james clarke sharing implementations clarke lapatas
mcdonalds us acknowledge support epsrc
grants gr gr work made use resources
provided edinburgh compute data facility ecdf ecdf partially
supported edikt initiative preliminary version work published
proceedings emnlp conll

references
aho v ullman j syntax directed translations pushdown assembler journal computer system sciences
alshawi h bangalore douglas learning dependency translation
collections finite state head transducers computational linguistics

berger l pietra pietra v j maximum entropy
natural language processing computational linguistics
bikel design multi lingual parallel processing statistical parsing engine
proceedings nd international conference human language technology
pp san diego ca
briscoe e j carroll j robust accurate statistical annotation general text
proceedings third international conference language resources evaluation pp las palmas gran canaria
carroll j minnen g pearce canning devlin tait j simplifying
text language impaired readers proceedings th conference european chapter association computational linguistics pp bergen
norway
chandrasekar r srinivas c b motivations methods text simplification proceedings th international conference computational
linguistics pp copenhagen danemark
chiang hierarchical phrase translation computational linguistics

clarke j lapata sentence compression comparison across
domains training requirements evaluation measures proceedings st
international conference computational linguistics th annual meeting
association computational linguistics pp sydney australia
clarke j lapata global inference sentence compression integer linear
programming journal artificial intelligence


ficohn lapata

cohn lapata sentence compression beyond word deletion proceedings nd international conference computational linguistics pp
manchester uk
collins discriminative training methods hidden markov theory
experiments perceptron proceedings conference
empirical methods natural language processing pp morristown nj
collins j head driven statistical natural language parsing ph
thesis university pennsylvania philadelphia pa
crammer k singer ultraconservative online multiclass machine learning
daume iii h marcu noisy channel model document compression
proceedings th annual meeting thev association computational
linguistics pp philadelphia pa
eisner j learning non isomorphic tree mappings machine translation
companion proceedings st annual meeting association
computational linguistics pp sapporo japan
galley hopkins knight k marcu whats translation rule
proceedings human language technology conference north
american chapter association computational linguistics pp
boston
galley mckeown k lexicalized markov grammars sentence compression
proceedings human language technologies conference north
american chapter association computational linguistics pp
rochester ny
grael j knight k training tree transducers proceedings human
language technology conference north american chapter association
computational linguistics pp boston
hermjakob u echihabi marcu natural language reformulation
resource wide exploitation question answering proceedings th text
retrieval conference gaithersburg md
hori c furui speech summarization word extraction
method evaluation ieice transactions information systems e
jing h sentence reduction automatic text summarization proceedings
th applied natural language processing conference pp seattle wa
joachims support vector method multivariate performance measures
proceedings nd international conference machine learning pp
bonn germany
kaji n okamoto kurohashi paraphrasing predicates written
language spoken language web proceedings human language technology conference north american chapter association
computational linguistics pp boston


fisentence compression tree transduction

knight k decoding complexity word replacement translation computational linguistics
knight k marcu summarization beyond sentence extraction probabilistic
sentence compression artificial intelligence
lin pantel p discovery inference rules question answering natural
language engineering
mcdonald r discriminative sentence compression soft syntactic constraints
proceedings th conference european chapter association
computational linguistics pp trento italy
nguyen l horiguchi shimazu ho b example sentence
reduction hidden markov model acm transactions asian language
information processing
och f j ney h alignment template statistical machine
translation computational linguistics
papineni k roukos ward zhu w j bleu method automatic
evaluation machine translation proceedings th annual meeting thev
association computational linguistics pp philadelphia pa
petrov barrett l thibaux r klein learning accurate compact
interpretable tree annotation proceedings st international conference
computational linguistics th annual meeting association computational linguistics pp sydney australia
riezler king h crouch r zaenen statistical sentence condensation
ambiguity packing stochastic disambiguation methods lexical functional
grammar proceedings human language technology conference
north american chapter association computational linguistics pp
edmonton canada
shieber schabes synchronous tree adjoining grammars proceedings th international conference computational linguistics pp
helsinki finland
stolcke srilm extensible language modeling toolkit proceedings
international conference spoken language processing denver co
tsochantaridis joachims hofmann altun large margin methods
structured interdependent output variables journal machine learning

turner j charniak e supervised unsupervised learning sentence
compression proceedings rd annual meeting association computational linguistics pp ann arbor mi
vandeghinste v pan sentence compression automated subtitling
hybrid text summarization branches proceedings acl
workshop pp barcelona spain


ficohn lapata

wu stochastic inversion transduction grammars bilingual parsing parallel
corpora computational linguistics
yamada k knight k syntax statistical translation model proceedings th annual meeting association computational linguistics
pp toulouse france




