journal artificial intelligence

submitted published

interactive policy learning
confidence autonomy
sonia chernova
manuela veloso

soniac cs cmu edu
veloso cs cmu edu

computer science dept
carnegie mellon university
pittsburgh pa usa

abstract
present confidence autonomy cba interactive policy
learning demonstration cba consists two components take
advantage complimentary abilities humans computer agents first component confident execution enables agent identify states demonstration
required request demonstration human teacher learn policy
acquired data selects demonstrations measure action
selection confidence confident execution agent requires fewer demonstrations learn policy demonstrations selected
human teacher second algorithmic component corrective demonstration enables
teacher correct mistakes made agent additional demonstrations
order improve policy future task performance cba individual components compared evaluated complex simulated driving domain complete
cba best overall learning performance successfully reproducing
behavior teacher balancing tradeoff number demonstrations
number incorrect actions learning

introduction
learning demonstration growing area artificial intelligence explores
techniques programming autonomous agents demonstrating desired behavior
task demonstration approaches teacher typically human shows agent
perform task agent records demonstrations sequences stateaction pairs learns policy reproduces observed behavior
many learning demonstration approaches inspired way humans animals
teach aiming provide intuitive method transfer human task knowledge
autonomous systems compared exploration methods demonstration learning
often reduces learning time eliminates frequently difficult task defining
detailed reward function smart schaal
article present interactive demonstration learning confidencebased autonomy cba enables agent learn policy interaction
human teacher learning agent begins initial knowledge
learns policy incrementally demonstrations acquired practices task
demonstration consists training point representing correct action performed
particular state agents state represented n dimensional feature vector
c

ai access foundation rights reserved

fichernova veloso

composed continuous discrete values agents actions bound
finite set action primitives basic actions combined together perform
overall task given sequence demonstrations si ai state si teacherselected action ai goal agent learn imitate teachers behavior
generalizing demonstrations learning policy mapping possible
states actions
method gathering demonstrations heart demonstration learning
cba performs function two algorithmic components confident
execution enables agent select demonstrations real time interacts
environment automatically calculated confidence thresholds corrective demonstration enables teacher improve learned policy correct
mistakes additional demonstrations complete confidence autonomy
provides fast intuitive method policy learning incorporating shared
decision making learner teacher experimental evaluation
highlight strengths learning components compare learning performance
five different demonstration selection techniques indicate complex
domain confident execution reduces number demonstrations required
learn task compared demonstration selection performed human teacher
additionally teachers ability correct mistakes performed agent
critical optimizing policy performance
section discuss related work learning demonstration present
overview complete confidence autonomy learning section
followed detailed descriptions confident execution corrective demonstration
components sections respectively section present experimental
evaluation complete components complex simulated driving
domain section presents summary discussion possible extensions work

related work
wide variety policy learning demonstration proposed
within machine learning robotics communities within context reinforcement
learning sutton barto demonstration viewed source reliable
information used accelerate learning process number approaches
taking advantage information developed deriving modifying
reward function demonstrations thomaz breazeal abbeel ng
papudesi atkeson schaal demonstration experiences
prime agents value function model takahashi hikita asada price
boutilier smart schaal
demonstration coupled supervised learning policy
learning including locally weighted regression low level skill acquisition grollman
jenkins browning xu veloso smart bayesian networks high level
behaviors lockerd breazeal inamura inaba inoue k nearest
neighbors fast paced games robot navigation tasks saunders nehaniv
dautenhahn bentivegna ude atkeson cheng recent survey covers


fiinteractive policy learning confidence autonomy

demonstration learning detail argall chernova browning
veloso
addition policy learning demonstration several areas
explored demonstration selection within machine learning active
learning blum langley cohn atlas ladner enables learner query
expert obtain labels unlabeled training examples aimed domains
large quantity data available labeling expensive active learning directs
expert label informative examples goal minimizing number
queries context reinforcement learning ask help framework enables
agent request advice agents confused action take
event characterized relatively equal quality estimates possible actions given
state clouse similarly motivated techniques used robotics identify
situations robot request demonstration teacher grollman
jenkins lockerd breazeal nicolescu inamura et al
closely related work dogged learning grollman jenkins
confidence learning teaching low level robotic skills
robot indicates teacher certainty performing elements task
teacher may choose provide additional demonstrations feedback
similarly motivated work differs dogged learning number
ways important use classification instead regression policy
learning ability adjust confidence threshold data instead
fixed value

confidence autonomy overview
confence autonomy enables human user train task policy
demonstration consists two components
confident execution ce enables agent learn policy
demonstrations obtained regulating autonomy requesting help
teacher demonstrations selected automatically calculated classification
confidence thresholds
corrective demonstration cd enables teacher improve
learned policy correcting mistakes made agent supplementary
demonstrations
figure shows interaction components confident execution agent selects states demonstration real time interacts
environment targeting states unfamiliar current policy action
uncertain timestep evaluates agents current state actively
decides autonomously executing action selected policy requesting
additional demonstration human teacher
assume underlying model agents task mdp agents policy
represented learned supervised learning training data acquired
demonstrations confidence autonomy combined supervised


fichernova veloso

figure confidence autonomy learning process
learning provides measure confidence classification policy
represented classifier c c db trained state vectors si inputs
actions ai labels classification query model returns model selected
action action selection confidence c decision boundary db highest
confidence query e g gaussian component gmms
effectively select demonstrations learner must able autonomously identify
situations demonstration provide useful information improve policy
confident execution selects agent autonomy request demonstration
measure action selection confidence c returned classifier given current
state learner queries policy obtain confidence selecting
action state regulates autonomy confidence learner
executes returned action ap confidence c threshold determined
decision boundary classifier db confidence threshold indicates
agent uncertain action take seeks help teacher
form demonstration receiving additional demonstration ad low confidence
situation improves policy leading increased confidence therefore autonomy
future similar states training data becomes available quality policy
improves autonomy agent increases entire task performed
without help teacher section compare two methods classification
confidence select states demonstration
confident execution agent incrementally acquires demonstrations explores environment practices task agent uses policy
learned point make decisions demonstration autonomous execution however relying policy learning complete likely


fiinteractive policy learning confidence autonomy

make mistakes due factors overgeneralization classifier incomplete
data area state space address article introduces
second algorithmic component corrective demonstration allows teacher provide corrections agents mistakes method incorrect action
observed teacher provides additional demonstration agent indicating
action executed place addition indicating wrong action selected method provides correct action perform
place ac correction therefore informative negative reinforcement
punishment techniques common leading agent learn quickly
mistakes
together confident execution corrective demonstration form interactive learning learner human teacher play complimentary roles learner
able identify states demonstration required fact
able better human teacher due differences perception
representation abilities teacher hand possesses expert knowledge
overall task applied performing demonstrations spotting execution
mistakes function agent cannot perform yet learned
desired behavior way confidence autonomy takes advantage
complimentary abilities human agent sections present confident
execution corrective demonstration components detail

confident execution
confident execution policy learning agent must select demonstration examples real time interacts environment timestep
uses thresholds determine whether demonstration correct action
agents current state provide useful information improve agents policy
demonstration required agent requests help teacher updates policy resulting action label otherwise agent continues perform task
autonomously policy
two distinct situations agent requires help teacher
unfamiliar states ambiguous states unfamiliar state occurs agent encounters situation significantly different previously demonstrated state
represented outlying points figure want demonstrate
every possible state therefore need model generalize would prevent
generalization truly different states
ambiguous states occur agent unable select multiple actions
certainty situation demonstrations different actions similar
states make accurate classification impossible region overlapping data classes
figure cases additional demonstrations may help disambiguate situation
goal confident execution divide state space regions
high confidence autonomous execution low confidence demonstration
unfamiliar ambiguous regions fall low confidence areas given world state
two evaluation criteria used select demonstration autonomy


fichernova veloso

nearest neighbor distance given n earestn eighbor distance
current state nearest similar training datapoint agent may act
autonomously distance threshold dist
classification confidence given c classification confidence current state
agent may act autonomously value c confidence threshold
conf
methods calculating thresholds dist conf presented sections
section continue discussion confident execution assuming
values given
presents details confident execution assume
preexisting knowledge task initialize empty set
training points since classifier initially available threshold conf initialized
infinity ensure agent controlled demonstration initial
learning stage distance threshold dist initialized
main learning consists loop lines iteration
represents single timestep behavior determined whether
agent currently executing action action progress performs
additional computation timestep line action complete
evaluates state determine next action perform lines
evaluation begins obtaining agents current state environment line
information used calculate nearest neighbor distance query
learned classifier c obtain policy action ap confidence c values
compared confidence distance thresholds decide demonstration
autonomy line similar states previously observed learned model
confident selection finishes timestep initiating autonomous

figure outlying points regions overlapping data classes represent unfamiliar
ambiguous state regions respectively



fiinteractive policy learning confidence autonomy

confident execution

conf inf
dist
true

actioncomplete

getsensordata

nearestneighbor

ap c db c

c conf dist

executeaction ap

else

requestdemonstration

ad getteacheraction

ad n u

ad

c updateclassifier

conf dist updatethresholds

executeaction ad

else

nothing

execution policy selected action ap line otherwise initiates request
teacher demonstration lines
agent requests demonstration pausing indicating teacher
demonstration required note assume domain allows agent pause
execution following demonstration request checks whether demonstration performed lines teachers response available training
datapoint consisting current state corresponding demonstrated action ad
added training set line model classifier retrained threshold
values updated executing teacher selected action lines
teachers response immediately available timestep terminates
whole process repeated next iteration agent senses state performs
threshold comparison checks demonstration non blocking mechanism
enables agent wait demonstration teacher without losing awareness
surroundings cases agents environment dynamic maintaining
date information important state may change time initial
request demonstration associating action label agents recent
state one teacher likely responding therefore critical learning
accurate model additionally changes environment agent attaining
high confidence state without actions cases autonomous execution
task automatically resumed summary demonstration request made
actions taken agent demonstration received
teacher changes environment high confidence state


fichernova veloso

confident execution enables agent incrementally acquire
demonstrations representing desired behavior datapoints acquired fewer
states distant training data encountered performance classification
confidence improve autonomy agent increases task learning complete
agent able repeatedly perform desired behavior without requesting demonstrations following sections present methods calculating distance
confidence thresholds
distance threshold
purpose distance threshold evaluate similarity agents
current state previous demonstrations evaluation metric uses nearest neighbor
distance defined euclidian distance query closest point
dataset agent state query obtain nearest neighbor distance representing
similar previously demonstrated state value compared distance
threshold dist
value distance threshold dist calculated function average nearest
neighbor distance across dataset demonstrations evaluating average similarity
states provides domain independent method detecting
outliers points unusually far previously encountered states trials article
value dist set three times average nearest neighbor distance across
dataset
alternate method detecting outliers would use classification confidence
request demonstrations low confidence states however situations arise
confidence directly correlated state similarity example many classifiers
set datapoints encircling empty region similar shape donut would
highest classification confidence associated empty center region far
previous demonstrations distance provides reliable prediction similarity even
cases
confidence threshold
confidence threshold used select regions uncertainty points
multiple classes overlap agents perspective points regions represent
demonstrations two distinct actions states appear similar difficult
distinguish sensor data frequently arises demonstration
learning number reasons teachers inability demonstrate task
consistently noise sensor readings inconsistency agents
teachers sensing abilities would set confidence threshold value
prevents model classifying overlapping region high confidence
following section discuss use limitations single fixed threshold value
present multiple adjustable thresholds section
see section discussion data regions



fiinteractive policy learning confidence autonomy



b

c

figure examples fixed threshold failure cases fully separable data classes
overly conservative threshold value b overlapping data classes overly
general threshold value c data classes different distributions common
threshold value

single fixed threshold
single fixed confidence threshold value provides simple mechanism approximate
high confidence regions state space previous utilizing classification confidence threshold behavior arbitration used manually selected single threshold
value inamura et al lockerd breazeal grollman jenkins however choosing appropriate value difficult constantly changing dataset
model figure presents examples three frequently encountered
figure presents case two action classes distinct fully separable
model trained dataset able classify points complete accuracy without
misclassifications however current threshold value classifies points
high confidence marking remaining points uncertain case
lower threshold value would preferred would allow model generalize
freely resulting larger high confidence region would reduce number redundant
demonstrations without increasing classification error rate data class
figure b presents example opposite case stricter threshold value
would preferred example data classes overlap resulting middle region
points cannot classified high accuracy higher threshold value would
prevent classification points region data class initiating instead
request demonstration would allow teacher disambiguate situation
figure c presents case datapoints two data classes
different distributions fixed threshold value appropriate left class
points right class labeled low confidence
classification complex multi class data depends upon multiple decision boundaries
value decision boundaries exacerbate highlighted
single value often cannot found constrains model classification
areas allowing generalization others resulting effect agent requests
many demonstrations things already knows demonstrations
unlearned behavior address present calculating
unique threshold value decision boundary


fichernova veloso



b

c

figure autonomy threshold calculation example dataset highlighted overlapping region b learned decision boundary misclassified points marked
confidence values c learned threshold values data class low confidence region containing overlapping points remains center

multiple adjustable thresholds
section contribute calculating confidence threshold
decision boundary customized unique distribution points analysis
assume able query classifier obtain confidence score representing
likelihood particular input belongs within specified decision boundary
begins dividing dataset training test set training
classifier c resulting learned model used classify withheld test set
correct action labels known calculates unique confidence
threshold decision boundary confidence scores misclassified points
given confidence scores set points mistakenly classified decision boundary
assume future classifications confidences values likely
misclassifications well threshold therefore calculated function
confidence scores
specifically define classified point tuple c original
observation demonstrated action label model selected action c
model action confidence let mi ai c ai set points
mistakenly classified decision boundary confidence threshold
pvalue set
mi

c

average classification confidence misclassified points conf mi take
average avoid overfitting noisy data values maximum standard
deviation used conservative estimate required threshold value
indicates misclassifications occurred model able generalize freely
figure presents example threshold calculation process figure presents
small sample dataset rectangular box figure highlights region state
space points classes overlap figure b shows learned decision
boundary case svm separating two data classes six misclassified points
marked mis classification confidences returned model misclassified points
side decision boundary used calculate respective confidence
thresholds figure c shows confidence threshold lines values


fiinteractive policy learning confidence autonomy



b

c

figure multiple adjustable thresholds applied failure cases shown figure

calculations resulting low confidence region middle image captures
noisy datapoints
given multi threshold classification points performed first
selecting action class highest confidence query comparison
line performed threshold decision boundary
highest confidence query method threshold value
likely decision boundary represent point used decide demonstration
autonomy
figure shows example failure cases discussed section addressed
multi thresholded customizing threshold value unique data
distribution enables correctly classify points figures
c since misclassifications model generalizes freely examples
dataset figure b perfect classification possible confidence
thresholds set overlapping region falls low confidence area
example uses gaussian mixture model elliptical confidence gradient around
mean large low confidence area even far overlapping region
classification methods support vector machines drawback
presented multi threshold independent figure presents
classification four different classification methods gaussian mixture random forests rf support vector machine quadratic kernel svm radial
basis function rbf kernel table summarizes classification performance
lists threshold values

gmm
rf
svm quad
svm rbf

correct misclas unclass





thresholds





table classifier comparison



fichernova veloso

gaussian mixture model

b random forest

c svm quadratic

svm rbf

figure classification dataset high low confidence regions different classification methods

corrective teacher demonstration
presented confident execution enables agent identify unfamiliar
ambiguous states prevents autonomous execution situations however states
incorrect action selected high confidence autonomous execution
still occur typically due generalization classifier article present
corrective demonstration coupled confident execution enables
teacher correct mistakes made agent combines corrective
demonstration lines denoted confident execution presents complete
confidence autonomy
corrective demonstration technique comes play time agent executes
autonomous action action selected autonomous execution
records agents state led decision saves value within variable sc
line execution autonomously selected action checks
teacher demonstration every timestep lines corrective demonstration
made training datapoint consisting recorded demonstration state sc
corrective action ac added training set line classifier thresholds
retrained information


fiinteractive policy learning confidence autonomy

confidence autonomy confident execution corrective
demonstration

conf inf
dist
true

getsensordata

actioncomplete

ap c db c

nearestneighbor

c conf dist

executeaction ap

sc


else

requestdemonstration

ad getteacheraction

ad n u

ad

c updateclassifier

conf dist updatethresholds

executeaction ad

else

autonomousaction


ac getteacheraction


ac n u


sc ac


c updateclassifier


conf dist updatethresholds


teacher observes autonomous execution agent
corrects incorrect actions unlike previous demonstration technique
agent given next action perform correction performed relation
agents previous state mistake made example observing
driving agent approaching close behind another car teacher able indicate
instead continuing drive forward agent merging
passing lane way addition indicating wrong action performed
corrective demonstration provides action
performed place technique effective negative reinforcement
punishment techniques common leading agent learn quickly
mistakes


fichernova veloso

figure screenshot driving simulator agent black car currently
center lane drives fixed speed must navigate around cars avoid
collisions road consists five lanes three traffic lanes two shoulder
lanes

evaluation comparison
section present evaluation comparison complete confidence
autonomy components simulated car driving domain abbeel ng
shown figure
domain description
driving domain agent represents car driving busy highway
learners car travels fixed speed mph cars move lanes
predetermined speeds mph road three normal lanes
shoulder lane sides agent allowed drive shoulder pass
cars cannot go road since learner cannot change speed must
navigate cars use shoulder lanes avoid collision agent
limited three actions remaining current lane shifting one lane left
right current position forward left right teacher demonstrates task
keyboard interface simulator framerate fps paused
demonstration requests
agents state represented l dl dc dr state feature l discrete value
symbolizing agents current lane number remaining three features denoted
letter represent distance nearest car three driving lanes
left center right distance features continuously valued range
note nearest car lane behind agent distance measurements
corrupted noise create complex testing environment agents policy
relearned time demonstrations acquired
driving domain presents varied challenging environment car distances
discretized rounding nearest integer value domain would contain
possible states due complexity domain agent requires large


fiinteractive policy learning confidence autonomy

number demonstrations initialize classifier resulting nearly constant demonstration requests early training process simplify task teacher add
short datapoint approximately second non interactive driving demonstration
session initialize learning process learning stage required simplifies task teacher continuous demonstration preferred frequent
pauses demonstration requests
performance learning evaluated time demonstrations acquired evaluation agent drove timesteps road
segment fixed consistent traffic pattern road segment used
training instead trained randomly generated car traffic pattern
since aims imitate behavior expert true reward function
exists evaluate performance given policy present two domain specific evaluation metrics capture key characteristics driving task first evaluation
metric agents lane preference proportion time agent spends
lane course trial metric provides estimate similarity driving
styles since demonstrated behavior attempts navigate domain without collisions
second evaluation metric number collisions caused agent collisions
measured percentage total timesteps agent spends contact
another car driving straight colliding every car middle lane
collision rate
experimental
present performance evaluation comparison following demonstration
selection techniques
g teacher guided demonstrations selected teacher without confidence feedback without ability perform retroactive
corrections
ces confident execution demonstrations selected agent single
fixed confidence threshold
cem confident execution demonstrations selected agent multiple
adjustable confidence thresholds
cd corrective demonstration demonstrations selected teacher performed corrections response mistakes made agent
cba complete confidence autonomy combining confident
execution multiple adjustable confidence thresholds corrective demonstration
demonstration selection method underlying policy agent learned
multiple gaussian mixture one action class chernova veloso
videos driving task available www cs cmu edu soniac
figure presents performance five respect
defined lane preference collision metrics describe discuss elements


fichernova veloso

figure evaluation agents driving performance demonstration intervals
five demonstration selection methods bar graphs indicate
percentage time agent spent road lane values bar
indicate percentage collision timesteps accrued evaluation trial
teacher performance bar right figure shows teachers driving
lane preference collision rate evaluation road segment goal
achieve performance similar teacher



fiinteractive policy learning confidence autonomy

figure detail following sections evaluation figure presents bar
representing composite graph showing percentage time spent agent
lane value bar indicates number demonstrations upon
evaluated policy value bar indicates percentage incurred
collisions evaluation
bar right figure shows performance teacher evaluation road segment evaluation indicates teacher prefers drive center
left lanes followed preference left shoulder right shoulder right lane
teacher successfully avoids collisions resulting collision rate goal
learning achieve driving lane pattern similar teacher
without collisions note described previous section policy learning
initialized demonstration dataset initialization
identical performance across initial learning segment
g demonstration selection
top row figure summarizes performance teacher guided demonstration
selection teacher performed training alternating
observing performance agent selecting demonstrations opinion
would improve driving performance teacher selected training examples without
receiving feedback action selection confidence without ability provide
corrective demonstrations incorrect actions already executed agent
instead teacher required anticipate data would improve policy
training process terminated teacher saw improvement agent
performance
figure shows agents performance evaluations demonstration
intervals throughout learning process similarity driving lane preference
agent improves slowly course learning significant fluctuations
example demonstrations agents preference drive empty left
shoulder thereby incurring collisions one hundred demonstrations later policy
shifted prefer center lane however agent yet learned avoid
cars resulting collision rate policy stabilizes approximately
demonstrations representing driving style similar teacher small
number collisions without confidence feedback agent difficult
teacher select exact termination point learning training continued
demonstrations learners policy showed little improvement final policy
resulted lane preference similar expert collision
rate
ces demonstration selection
second row figure presents confident execution
single autonomy threshold demonstration selection demonstrations
selected agent learning terminated agent stopped requesting
demonstrations performed actions autonomously autonomy threshold value


fichernova veloso

selected hand evaluated multiple performance trials best fixed
threshold presented
compared teacher guided policy learned ces
stabilizes quickly achieving performance similar teachers demonstrations number collisions low persistent even agent gains full
confidence stops requesting demonstrations demonstrations final lane
preference similar expert collision rate
cem demonstration selection
third row figure presents confident execution
multiple autonomy thresholds calculated presented section demonstration selection methods cem required fewest number
demonstrations learn task completing learning demonstrations
indicates use multiple adjustable thresholds successfully focuses demonstration selection informative areas state space greatly reducing number
redundant demonstrations throughout learning process number gaussian
components within model varied large variation highlights
importance automating threshold calculation process since hand selecting individual
thresholds component would impractical lane preference final policy
similar expert however agent still maintained small collision
rate
cd demonstration selection
evaluation first three highlights difficulty driving
approaches able select demonstrations resulted policy
mimics overall driving style teacher however policies resulted
small number collisions typically occurred agent merged close
another vehicle touched bumper mistakes difficult correct
techniques evaluated far even within teacher guided demonstration selection
method human teacher full control demonstration training data
time collision observed incorrect decision already made
instead retroactive demonstration required correct already made
mistakes corrective demonstration
fourth row figure present evaluation demonstration selection
corrective demonstration demonstrations
selected teacher corrections response mistakes made agent
behavior corrected teacher included collisions well incorrect lane preference
e g driving shoulder rapid oscillations lanes enable
teacher accurately perform corrections simulation slowed frames
per second learning terminated agent required corrections
shown figure complete training process corrective demonstration took
demonstrations achieving final policy correctly imitates teachers driving style
collision rate following section discuss performance compares
complete cba


fiinteractive policy learning confidence autonomy

cba demonstration selection
final row figure presents evaluation complete confidence autonomy combines cem cd learning complete
agent longer requests demonstrations able perform driving task
without collisions cba agent required total demonstrations learn
task successfully learning navigate highway without collisions
analyze impact two cba learning components comparing number
distribution demonstrations acquired learning process
section refer learning components cba cba ce cba cd
differentiate evaluations presented previous sections note
behavior confident execution component dependent upon method used
set autonomy thresholds evaluation use multiple adjustable thresholds
calculated average value misclassified points
figure datapoint along x axis represents number demonstrations
requested cba ce top initiated teacher cba cd bottom
timestep interval approximately seconds simulator runtime excluding pauses
demonstration requests since first three demonstration timesteps consist entirely non interactive demonstration values timesteps due
scaling exceed bounds graph figure b shows cumulative number
demonstrations component total grows respect training time
complete training process lasts approximately hour half
analysis graphs shows demonstrations occur early training
process importantly confident execution accounts total number demon



b

figure timeline showing number demonstrations initiated agent
confident execution top initiated teacher corrective demonstrations bottom changes course training b
cumulative number demonstrations acquired component total
time



fichernova veloso

strations indicating agent guides learning demonstration requests occur first minutes training agent encounters
many novel states classification confidence remains low agent requires
corrections stage many mistakes prevented requesting demonstration instead performing low confidence action corrective demonstration plays
greatest role towards end training process accounts final
demonstrations stage learning agents action selection confidence
high enough rarely asks demonstrations policy already closely imitates
teachers driving style small number collisions remain corrective demonstration
enables teacher fine tune policy eliminate collisions highlights
importance corrective demonstration whether alone conjunction another
selection technique optimizing policy performance
cba achieves similar final performance compared cd evaluated
previous section requires approximately additional demonstrations learn
policy additional demonstrations attributed confident execution demonstration requests served increase classification confidence change
outcome agents action viewed another way datapoints correspond states
agent would performed correct action even asked
demonstration appears allowing agent make mistakes
correcting fact done cd evaluation may best demonstration
selection respect performance metrics defined overall
number demonstrations
however eliminating ability request demonstrations utilizing retroactive correction several drawbacks namely requiring constant full attention
teacher importantly requiring agent make many mistakes learns
correct policy comparison cba enables agent request demonstrations low confidence states thereby avoiding many incorrect actions original
lane preference collision metrics take difference account focus
final policy performance agent
evaluate difference additionally examine number
collisions agent incurs course learning cd
agent incurs collisions vs training cba
therefore allowing agent request demonstrations low confidence states
cba requires slightly greater number demonstrations greatly reducing
number incorrect actions performed learning reduction number
action errors significant due importance many learning domains especially
robotic applications errors may pose dangers system
summary evaluation shown ability retroactively correct mistakes
crucial optimizing policy eliminating collisions best performance
achieved corrective demonstration confidence autonomy methods
cd requiring fewer demonstrations incurring greater number collisions
training choice cd cba therefore viewed tradeoff
number demonstrations frequency undesired actions training
fact cd special case cba autonomy threshold set classify
points high confidence adjusting selectiveness cba autonomy thresholds


fiinteractive policy learning confidence autonomy

could therefore provide user sliding control mechanism effects agents
tendency perform autonomous actions versus demonstration requests importantly
note overall number demonstrations required less
teacher guided method tiny fraction overall state space

discussion
section discuss several promising directions future work well number
existing extensions presented learning methods
evaluation non technical users
presented demonstration learning provides fast intuitive method
programming adapting behavior autonomous agents believe general
representation classifier independent makes cba usable wide range
applications one particular application interest use demonstration learning
enable non technical users program autonomous agents believe cba would
highly suitable application assume teacher technical
knowledge policy learning requiring teacher expert task
presented article obtained single teacher one
authors additional studies could evaluate usability performance wider
user base non programmers particular
representation action choices
demonstration learning provides natural intuitive interface transferring human task knowledge autonomous agents however operating rich environments
agents inevitably face situations multiple actions equivalently applicable
example agent encounters obstacle directly path option moving
left right avoid surrounding space empty directions equally valid
performing desired task human demonstrators faced choice equivalent
actions typically perform demonstrations consistently instead selecting among
applicable actions arbitrarily time choice encountered training
data obtained agent lacks consistency identical nearly identical states
associated different actions presented cba inconsistent
demonstrations would persistent region low confidence leading agent
repeatedly request demonstrations within inconsistent domain region successfully extended cba identify regions state space conflicting demonstrations
represent choice multiple actions explicitly within agents policy chernova veloso
improvement beyond teacher performance
policy learned confidence autonomy inherently limited
quality demonstrations provided human teacher assuming
teacher expert task aims imitate behavior teacher
however many domains teacher demonstrations may suboptimal limited


fichernova veloso

human ability several demonstration learning approaches developed enable
agent learn experiences addition demonstrations thereby improving
performance beyond abilities teacher stolle atkeson smart
extending cba include similar capability remains promising direction
future work possible approaches include incorporating high level feedback argall
browning veloso reward signal thomaz breazeal teacher
well filtering noisy inaccurate demonstrations
policy use learning
cba considers learning complete agent able perform
required behavior repeatedly correctly without requesting demonstrations
requiring corrections policy learning complete standard procedure
vast majority policy learning turn learning process freeze
policy used propose
continuing use confident execution component may long term benefits
beyond policy learning particular ability identify anomalous states
may enable agent detect notify user system errors unexpected input
studies needed evaluate use believe
mechanism would provide useful safety feature long term autonomous operation
negligible cost performing threshold comparison timestep
richer interaction
presented demonstration learning relies limited form interaction agent teacher agent requests demonstrations teacher
teacher responds single recommended action level interaction
typical traditional active learning approaches fails take full advantage
vast task knowledge teacher possesses believe extending
include richer interaction abilities could provide faster intuitive training
method many promising directions future exist area example
developing domain independent dialog exchange agent teacher incorporates clarification questions high level advice could speed learning enable
agent represent high level goals task ability play back rewind
demonstration sequences would additionally enable teacher agent reexamine
reevaluate past learning experiences
application single robot multi robot systems
learning demonstration techniques extensively studied within robotics
community due interactive nature fast learning times work
shown cba highly effective learning variety single robot tasks
chernova veloso
furthermore many complex tasks require collaboration multiple robots
one greatest challenges preventing demonstration learning
generalizing multi robot domains limited human attention


fiinteractive policy learning confidence autonomy

fact teacher able pay attention interact robots
time cba developed first multi robot
demonstration learning system addresses limited human attention
taking advantage fact confident execution component cba prevents
autonomous execution actions low confidence states chernova veloso b
flexmlfd system utilizes individual instances cba robot learner
acquires unique set demonstrations learns individual task policy preventing
autonomous execution low confidence states cba makes learner robust periods
teacher neglect allowing multiple robots taught time

conclusion
article presented confidence autonomy interactive policy
learning demonstration agent incrementally learns
action policy demonstrations acquired practices task cba
contains two methods obtaining demonstrations confident execution component
enables agent select demonstrations real time interacts environment
confidence distance thresholds target states unfamiliar
current policy action uncertain corrective demonstration component allows
teacher additionally perform corrective demonstrations incorrect action
selected agent teacher retroactively provides demonstrations specific error
cases instead attempting anticipate errors ahead time combined techniques
provide fast intuitive policy learning incorporating shared decision
making learner teacher
experimentally used complex simulated driving domain compare five methods
selecting demonstration training data manual data selection teacher confidencebased selection single fixed threshold confidence selection multiple
automatically calculated thresholds corrective demonstration confidence selection combined corrective demonstration evaluation conclude
confidence methods able select informative demonstrations
human teacher single multiple threshold approaches multiple adjustable
threshold technique required significantly fewer demonstrations focusing onto regions
uncertainty reducing number redundant datapoints best final policy performance however achieved corrective demonstration complete confidencebased autonomy achieved lane preference similar
teacher without collisions together demonstration selection represent
tradeoff number demonstrations frequency undesired actions
training corrective demonstration required slightly fewer demonstrations
learn final policy compared cba resulted significant increase number
errors made agent course learning process cba
therefore provides best demonstration selection method domains incorrect
actions desirable training process


fichernova veloso

acknowledgments
partially sponsored department interior national business
center contract nbchd sri international subcontract
bbnt solutions subcontract via prime air force
contract sa c views conclusions contained document
authors interpreted representing official policies
expressed implied sponsoring institution u government
entity additional thanks paul rybski making simulation package available

references
abbeel p ng apprenticeship learning via inverse reinforcement learning
proceedings international conference machine learning york ny
usa acm press
argall b chernova browning b veloso survey robot learning
demonstration robotics autonomous systems appear
argall b browning b veloso learning demonstration critique human teacher second annual conference human robot interactions
hri arlington virginia
atkeson c g schaal robot learning demonstration proceedings
international conference machine learning pp san francisco ca
usa morgan kaufmann publishers inc
bentivegna c ude atkeson c g cheng g learning act
observation practice international journal humanoid robotics
blum l langley p selection relevant features examples machine
learning artificial intelligence
browning b xu l veloso skill acquisition use dynamicallybalancing soccer robot proceedings nineteenth national conference artificial
intelligence pp
chernova veloso confidence policy learning demonstration
gaussian mixture proceedings international conference
autonomous agents multiagent systems pp
chernova veloso learning equivalent action choices demonstration
proceedings international conference intelligent robots systems
pp
chernova veloso b teaching collaborative multi robot tasks
demonstration proceedings ieee ras international conference humanoid robots
clouse j integrating apprentice learning reinforcement learning ph
thesis university massachisetts department computer science
cohn atlas l ladner r improving generalization active learning
machine learning


fiinteractive policy learning confidence autonomy

grollman jenkins dogged learning robots ieee international
conference robotics automation pp
inamura inaba inoue h acquisition probabilistic behavior decision model interactive teaching method proceedings ninth
international conference advanced robotics pp
lockerd breazeal c tutelage socially guided robot learning proceedings ieee rsj international conference intelligent robots systems
pp
nicolescu n framework learning demonstration generalization
practice human robot domains ph thesis university southern california
papudesi v integrating advice reinforcement learning masters thesis university texas arlington
price b boutilier c accelerating reinforcement learning implicit
imitation journal artificial intelligence
saunders j nehaniv c l dautenhahn k teaching robots moulding behavior scaffolding environment proceeding st acm sigchi sigart
conference human robot interaction pp york ny usa acm
press
schaal learning demonstration advances neural information processing systems pp mit press
smart w making reinforcement learning work real robots ph thesis
department computer science brown university providence ri
stolle atkeson c g knowledge transfer local features proceedings ieee international symposium approximate dynamic programming
reinforcement learning pp
sutton r barto reinforcement learning introduction mit press
cambridge
takahashi hikita k asada hierarchical multi module learning system
self interpretation instructions coach proceedings robocup
robot soccer world cup vii pp
thomaz l breazeal c reinforcement learning human teachers evidence feedback guidance implications learning performance proceedings twenty first conference artificial intelligence pp




