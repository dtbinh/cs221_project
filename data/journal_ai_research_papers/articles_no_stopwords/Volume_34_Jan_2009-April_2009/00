Journal Artificial Intelligence Research 34 (2009) 1-25

Submitted 04/08; published 1/09

Interactive Policy Learning
Confidence-Based Autonomy
Sonia Chernova
Manuela Veloso

soniac@cs.cmu.edu
veloso@cs.cmu.edu

Computer Science Dept.
Carnegie Mellon University
Pittsburgh, PA USA

Abstract
present Confidence-Based Autonomy (CBA), interactive algorithm policy
learning demonstration. CBA algorithm consists two components take
advantage complimentary abilities humans computer agents. first component, Confident Execution, enables agent identify states demonstration
required, request demonstration human teacher learn policy based
acquired data. algorithm selects demonstrations based measure action
selection confidence, results show using Confident Execution agent requires fewer demonstrations learn policy demonstrations selected
human teacher. second algorithmic component, Corrective Demonstration, enables
teacher correct mistakes made agent additional demonstrations
order improve policy future task performance. CBA individual components compared evaluated complex simulated driving domain. complete
CBA algorithm results best overall learning performance, successfully reproducing
behavior teacher balancing tradeoff number demonstrations
number incorrect actions learning.

1. Introduction
Learning demonstration growing area artificial intelligence research explores
techniques programming autonomous agents demonstrating desired behavior
task. demonstration-based approaches, teacher, typically human, shows agent
perform task. agent records demonstrations sequences stateaction pairs, learns policy reproduces observed behavior.
Many learning demonstration approaches inspired way humans animals
teach other, aiming provide intuitive method transfer human task knowledge
autonomous systems. Compared exploration-based methods, demonstration learning
often reduces learning time eliminates frequently difficult task defining
detailed reward function (Smart, 2002; Schaal, 1997).
article, present interactive demonstration learning algorithm, ConfidenceBased Autonomy (CBA), enables agent learn policy interaction
human teacher. learning approach, agent begins initial knowledge
learns policy incrementally demonstrations acquired practices task.
demonstration consists training point representing correct action performed
particular state. agents state represented using n-dimensional feature vector
c
2009
AI Access Foundation. rights reserved.

fiChernova & Veloso

composed continuous discrete values. agents actions bound
finite set action primitives, basic actions combined together perform
overall task. Given sequence demonstrations (si , ai ), state si teacherselected action ai A, goal agent learn imitate teachers behavior
generalizing demonstrations learning policy mapping possible
states actions A.
method gathering demonstrations heart demonstration learning
algorithms. CBA performs function two algorithmic components: Confident
Execution, enables agent select demonstrations real time interacts
environment using automatically calculated confidence thresholds, Corrective Demonstration, enables teacher improve learned policy correct
mistakes additional demonstrations. complete Confidence-Based Autonomy
algorithm provides fast intuitive method policy learning, incorporating shared
decision making learner teacher. experimental evaluation,
highlight strengths learning components compare learning performance
five different demonstration selection techniques. results indicate complex
domain, Confident Execution algorithm reduces number demonstrations required
learn task compared demonstration selection performed human teacher.
Additionally, find teachers ability correct mistakes performed agent
critical optimizing policy performance.
Section 2, discuss related work learning demonstration. present
overview complete Confidence-Based Autonomy learning algorithm Section 3,
followed detailed descriptions Confident Execution Corrective Demonstration
components Sections 4 5, respectively. Section 6, present experimental
evaluation complete algorithm components complex simulated driving
domain. Section 7 presents summary discussion possible extensions work.

2. Related Work
wide variety algorithms policy learning demonstration proposed
within machine learning robotics communities. Within context reinforcement
learning (Sutton & Barto, 1998), demonstration viewed source reliable
information used accelerate learning process. number approaches
taking advantage information developed, deriving modifying
reward function based demonstrations (Thomaz & Breazeal, 2006; Abbeel & Ng,
2004; Papudesi, 2002; Atkeson & Schaal, 1997), using demonstration experiences
prime agents value function model (Takahashi, Hikita, & Asada, 2004; Price &
Boutilier, 2003; Smart, 2002; Schaal, 1997).
Demonstration coupled supervised learning algorithms policy
learning, including Locally Weighted Regression low level skill acquisition (Grollman &
Jenkins, 2007; Browning, Xu, & Veloso, 2004; Smart, 2002), Bayesian networks high level
behaviors (Lockerd & Breazeal, 2004; Inamura, Inaba, & Inoue, 1999), k-nearest
neighbors algorithm fast-paced games robot navigation tasks (Saunders, Nehaniv,
& Dautenhahn, 2006; Bentivegna, Ude, Atkeson, & Cheng, 2004). recent survey covers
2

fiInteractive Policy Learning Confidence-Based Autonomy

demonstration learning algorithms detail (Argall, Chernova, Browning,
& Veloso, 2009).
addition policy learning demonstration, several areas research
explored algorithms demonstration selection. Within machine learning research, active
learning (Blum & Langley, 1997; Cohn, Atlas, & Ladner, 1994) enables learner query
expert obtain labels unlabeled training examples. Aimed domains
large quantity data available labeling expensive, active learning directs
expert label informative examples goal minimizing number
queries. context reinforcement learning, Ask Help framework enables
agent request advice agents confused action take,
event characterized relatively equal quality estimates possible actions given
state (Clouse, 1996). Similarly motivated techniques used robotics identify
situations robot request demonstration teacher (Grollman &
Jenkins, 2007; Lockerd & Breazeal, 2004; Nicolescu, 2003; Inamura et al., 1999).
closely related work Dogged Learning algorithm (Grollman & Jenkins, 2007),
confidence-based learning approach teaching low-level robotic skills. algorithm,
robot indicates teacher certainty performing various elements task.
teacher may choose provide additional demonstrations based feedback.
similarly motivated, work differs Dogged Learning algorithm number
ways, important use classification instead regression policy
learning, algorithms ability adjust confidence threshold data instead
using fixed value.

3. Confidence-Based Autonomy Overview
Confence-Based Autonomy algorithm enables human user train task policy
demonstration. algorithm consists two components:
Confident Execution (CE): algorithm enables agent learn policy based
demonstrations obtained regulating autonomy requesting help
teacher. Demonstrations selected based automatically calculated classification
confidence thresholds.
Corrective Demonstration (CD): algorithm enables teacher improve
learned policy correcting mistakes made agent supplementary
demonstrations.
Figure 1 shows interaction components. Using Confident Execution algorithm, agent selects states demonstration real time interacts
environment, targeting states unfamiliar current policy action
uncertain. timestep, algorithm evaluates agents current state actively
decides autonomously executing action selected policy requesting
additional demonstration human teacher.
assume underlying model agents task MDP. agents policy
represented learned using supervised learning based training data acquired
demonstrations. Confidence-Based Autonomy combined supervised
3

fiChernova & Veloso

Figure 1: Confidence-Based Autonomy learning process.
learning algorithm provides measure confidence classification. policy
represented classifier C : (a, c, db), trained using state vectors si inputs,
actions ai labels. classification query, model returns model-selected
action A, action selection confidence c, decision boundary db highest
confidence query (e.g. Gaussian component GMMs).
effectively select demonstrations, learner must able autonomously identify
situations demonstration provide useful information improve policy.
Confident Execution selects agent autonomy request demonstration based
measure action-selection confidence c returned classifier. Given current
state learner, algorithm queries policy obtain confidence selecting
action state, regulates autonomy based confidence. learner
executes returned action ap confidence c threshold , determined
decision boundary classifier, db. Confidence threshold indicates
agent uncertain action take, seeks help teacher
form demonstration. Receiving additional demonstration, ad , low confidence
situation improves policy, leading increased confidence, therefore autonomy,
future similar states. training data becomes available, quality policy
improves autonomy agent increases entire task performed
without help teacher. Section 4 compare two methods using classification
confidence select states demonstration.
Using Confident Execution algorithm, agent incrementally acquires demonstrations explores environment. practices task, agent uses policy
learned point make decisions demonstration autonomous execution. However, relying policy learning complete, algorithm likely
4

fiInteractive Policy Learning Confidence-Based Autonomy

make mistakes due factors overgeneralization classifier incomplete
data area state space. address problem article introduces
second algorithmic component, Corrective Demonstration, allows teacher provide corrections agents mistakes. Using method, incorrect action
observed, teacher provides additional demonstration agent indicating
action executed place. addition indicating wrong action selected, method provides algorithm correct action perform
place, ac . correction therefore informative negative reinforcement
punishment techniques common algorithms, leading agent learn quickly
mistakes.
Together, Confident Execution Corrective Demonstration form interactive learning algorithm learner human teacher play complimentary roles. learner
able identify states demonstration required; fact, results show
algorithm able better human teacher due differences perception
representation abilities. teacher, hand, possesses expert knowledge
overall task, applied performing demonstrations spotting execution
mistakes. function agent cannot perform yet learned
desired behavior. way, Confidence-Based Autonomy takes advantage
complimentary abilities human agent. Sections 4 5 present Confident
Execution Corrective Demonstration components detail.

4. Confident Execution Algorithm
Confident Execution policy learning algorithm agent must select demonstration examples, real time, interacts environment. timestep,
algorithm uses thresholds determine whether demonstration correct action
agents current state provide useful information improve agents policy.
demonstration required, agent requests help teacher, updates policy based resulting action label. Otherwise agent continues perform task
autonomously based policy.
two distinct situations agent requires help teacher,
unfamiliar states ambiguous states. unfamiliar state occurs agent encounters situation significantly different previously demonstrated state,
represented outlying points Figure 2. want demonstrate
every possible state, therefore need model generalize, would prevent
over-generalization truly different states.
Ambiguous states occur agent unable select multiple actions
certainty. situation result demonstrations different actions similar
states make accurate classification impossible, region overlapping data classes
Figure 2. cases, additional demonstrations may help disambiguate situation.
goal Confident Execution algorithm divide state space regions
high confidence (autonomous execution) low confidence (demonstration)
unfamiliar ambiguous regions fall low confidence areas. Given world state,
two evaluation criteria used select demonstration autonomy:
5

fiChernova & Veloso

Nearest Neighbor distance: Given = N earestN eighbor(s), distance
current state nearest (most similar) training datapoint, agent may act
autonomously distance threshold dist .
Classification confidence: Given c, classification confidence current state,
agent may act autonomously value c confidence threshold
conf .
methods calculating thresholds dist conf presented Sections 4.1 4.2.
section, continue discussion Confident Execution algorithm assuming
values given.
Algorithm 1 presents details Confident Execution algorithm. assume
preexisting knowledge task, initialize algorithm empty set
training points . Since classifier initially available, threshold conf initialized
infinity ensure agent controlled demonstration initial
learning stage. Distance threshold dist initialized 0.
main learning algorithm consists loop (lines 4-20), iteration
represents single timestep. behavior algorithm determined whether
agent currently executing action. action progress, algorithm performs
additional computation timestep (line 20). action complete,
algorithm evaluates state determine next action perform (lines 6-18).
Evaluation begins obtaining agents current state environment (line 6).
information used calculate nearest neighbor distance query
learned classifier C obtain policy action ap confidence c. values
compared confidence distance thresholds decide demonstration
autonomy (line 9). similar states previously observed, learned model
confident selection, algorithm finishes timestep initiating autonomous

Figure 2: Outlying points regions overlapping data classes represent unfamiliar
ambiguous state regions, respectively.

6

fiInteractive Policy Learning Confidence-Based Autonomy

Algorithm 1 Confident Execution Algorithm
1: {}
2: conf inf
3: dist 0
4: true
5:
actionComplete
6:
GetSensorData()
7:
= NearestNeighbor(s)
8:
(ap , c, db) C(s)
9:
c > conf < dist
10:
ExecuteAction(ap )
11:
else
12:
RequestDemonstration()
13:
ad GetTeacherAction()
14:
ad 6= N U
15:
{(s, ad )}
16:
C UpdateClassifier(T )
17:
(conf , dist ) UpdateThresholds()
18:
ExecuteAction(ad )
19:
else
20:
//do nothing

execution policy selected action ap (line 10). Otherwise initiates request
teacher demonstration (lines 12-18).
agent requests demonstration pausing indicating teacher
demonstration required. Note assume domain allows agent pause
execution. Following demonstration request, algorithm checks whether demonstration performed (lines 13-14). teachers response available, new training
datapoint consisting current state corresponding demonstrated action ad
added training set (line 15). model classifier retrained, threshold
values updated, executing teacher selected action (lines 16-18).
teachers response immediately available, timestep terminates
whole process repeated next iteration. agent senses state, performs
threshold comparison checks demonstration. non-blocking mechanism
enables agent wait demonstration teacher without losing awareness
surroundings. cases agents environment dynamic, maintaining
date information important state may change time initial
request demonstration. Associating action label agents recent
state, one teacher likely responding to, therefore critical learning
accurate model. Additionally, changes environment result agent attaining
high confidence state without actions own. cases, autonomous execution
task automatically resumed. summary, demonstration request made,
actions taken agent either demonstration received
teacher, changes environment result high confidence state.
7

fiChernova & Veloso

Using approach, Confident Execution enables agent incrementally acquire
demonstrations representing desired behavior. datapoints acquired, fewer
states distant training data encountered, performance classification
confidence improve, autonomy agent increases. Task learning complete
agent able repeatedly perform desired behavior without requesting demonstrations. following sections present methods calculating distance
confidence thresholds.
4.1 Distance Threshold
purpose distance threshold evaluate similarity agents
current state previous demonstrations. evaluation metric uses nearest neighbor
distance, defined Euclidian distance query closest point
dataset. agent state query, obtain nearest neighbor distance representing
similar previously demonstrated state. value compared distance
threshold dist .
value distance threshold dist calculated function average nearest
neighbor distance across dataset demonstrations. Evaluating average similarity
states provides algorithm domain-independent method detecting
outliers, points unusually far previously encountered states. trials article,
value dist set three times average nearest neighbor distance across
dataset.
alternate method detecting outliers would use classification confidence
request demonstrations low confidence states. However, situations arise
confidence directly correlated state similarity. example, many classifiers
set datapoints encircling empty region, similar shape donut, would result
highest classification confidence associated empty center region far
previous demonstrations. Distance provides reliable prediction similarity, even
cases.
4.2 Confidence Threshold
confidence threshold used select regions uncertainty points
multiple classes overlap. agents perspective, points regions represent
demonstrations two distinct actions states appear similar, difficult
distinguish based sensor data. problem frequently arises demonstration
learning number reasons, teachers inability demonstrate task
consistently, noise sensor readings, inconsistency agents
teachers sensing abilities. would set confidence threshold value
prevents either model classifying overlapping region high confidence1 .
following section discuss use limitations single fixed threshold value.
present algorithm using multiple adjustable thresholds Section 4.2.2.
1. See Section 7.2 discussion data regions.

8

fiInteractive Policy Learning Confidence-Based Autonomy

(a)

(b)

(c)

Figure 3: Examples fixed threshold failure cases: (a) Fully separable data classes
overly conservative threshold value (b) Overlapping data classes overly
general threshold value (c) Data classes different distributions common
threshold value

4.2.1 Single Fixed Threshold
single, fixed confidence threshold value provides simple mechanism approximate
high confidence regions state space. Previous algorithms utilizing classification confidence threshold behavior arbitration used manually-selected single threshold
value (Inamura et al., 1999; Lockerd & Breazeal, 2004; Grollman & Jenkins, 2007). However, choosing appropriate value difficult constantly changing dataset
model. Figure 3 presents examples three frequently encountered problems.
Figure 3(a) presents case two action classes distinct fully separable.
model trained dataset able classify points complete accuracy, without
misclassifications. However, current threshold value classifies 72% points
high confidence, marking remaining 28% points uncertain. case,
lower threshold value would preferred would allow model generalize
freely. resulting larger high confidence region would reduce number redundant
demonstrations without increasing classification error rate either data class.
Figure 3(b) presents example opposite case, stricter threshold value
would preferred. example data classes overlap, resulting middle region
points cannot classified high accuracy. higher threshold value would
prevent classification points region either data class, initiating instead
request demonstration would allow teacher disambiguate situation.
Figure 3(c) presents case datapoints two data classes
different distributions. fixed threshold value appropriate left class, 42%
points right class labeled low confidence.
Classification complex multi-class data depends upon multiple decision boundaries.
Using value decision boundaries exacerbate problems highlighted
above, single value often cannot found constrains model classification
areas allowing generalization others. resulting effect agent requests
many demonstrations things already knows, demonstrations
unlearned behavior. address problem, present algorithm calculating
unique threshold value decision boundary.
9

fiChernova & Veloso

(a)

(b)

(c)

Figure 4: Autonomy threshold calculation: (a) Example dataset, highlighted overlapping region (b) Learned decision boundary, misclassified points marked
confidence values (c) Learned threshold values data class, low confidence region containing overlapping points remains center.

4.2.2 Multiple Adjustable Thresholds
section, contribute algorithm calculating confidence threshold
decision boundary, customized unique distribution points. analysis,
assume able query classifier obtain confidence score representing
likelihood particular input belongs within specified decision boundary.
algorithm begins dividing dataset training test set training
classifier C. resulting learned model used classify withheld test set,
correct action labels known. algorithm calculates unique confidence
threshold decision boundary based confidence scores misclassified points.
Given confidence scores set points mistakenly classified decision boundary,
assume future classifications confidences values likely
misclassifications well. threshold therefore calculated function
confidence scores.
Specifically, define classified point tuple (o, a, , c), original
observation, demonstrated action label, model-selected action, c
model action confidence. Let Mi = {(o, ai , , c)|am 6= ai } set points
mistakenly classified decision boundary i. confidence threshold
Pvalue set
Mi

c

average classification confidence misclassified points: conf = |Mi | . take
average avoid overfitting noisy data. values, based maximum standard
deviation, used conservative estimate required. threshold value 0
indicates misclassifications occurred model able generalize freely.
Figure 4 presents example threshold calculation process. Figure 4(a) presents
small sample dataset, rectangular box figure highlights region state
space points classes overlap. Figure 4(b) shows learned decision
boundary (in case SVM) separating two data classes. Six misclassified points
marked (mis-)classification confidences returned model. Misclassified points
side decision boundary used calculate respective confidence
thresholds. Figure 4(c) shows confidence threshold lines values based
10

fiInteractive Policy Learning Confidence-Based Autonomy

(a)

(b)

(c)

Figure 5: Multiple adjustable thresholds applied failure cases shown Figure 3.

calculations. resulting low confidence region middle image captures
noisy datapoints.
Given multi-threshold approach, classification new points performed first
selecting action class highest confidence query. comparison
line 9 Algorithm 1 performed using threshold decision boundary
highest confidence query. Using method, threshold value
likely decision boundary represent point used decide demonstration
autonomy.
Figure 5 shows example failure cases discussed Section 4.2.1 addressed
multi-thresholded approach. Customizing threshold value unique data
distribution enables algorithm correctly classify 100% points Figures 5(a)
(c). Since misclassifications, model generalizes freely examples.
dataset Figure 5(b), perfect classification possible, confidence
thresholds set overlapping region falls low confidence area.
example uses Gaussian mixture model, elliptical confidence gradient around
mean results large low confidence area even far overlapping region.
classification methods, Support Vector Machines, drawback.
presented multi-threshold approach algorithm independent, Figure 6 presents
classification results four different classification methods: Gaussian mixture models, random forests (RF), Support Vector Machine quadratic kernel, SVM radial
basis function (RBF) kernel. table summarizes classification performance
algorithm lists threshold values models.
Algorithm
GMM
RF
SVM quad.
SVM RBF

Correct-Misclas.-Unclass.
98.6% 0.4% 1.0%
99.1% 0.1% 0.8%
98.5% 0.1% 1.4%
98.9% 0.1% 1.0%

Thresholds
(0, 0, 0.012)
(0.14, -0.355)
(335.33, -68.77)
(0.825, -0.268)

Table 1: Classifier comparison.

11

fiChernova & Veloso

(a) Gaussian mixture model

(b) Random Forest

(c) SVM (quadratic)

(d) SVM (RBF)

Figure 6: Classification dataset high low confidence regions using different classification methods.

5. Corrective Teacher Demonstration
presented Confident Execution algorithm enables agent identify unfamiliar
ambiguous states prevents autonomous execution situations. However, states
incorrect action selected high confidence autonomous execution
still occur, typically due over-generalization classifier. article present
Corrective Demonstration algorithm which, coupled Confident Execution, enables
teacher correct mistakes made agent. Algorithm 2 combines Corrective
Demonstration (lines denoted ) Confident Execution presents complete
Confidence-Based Autonomy algorithm.
Corrective Demonstration technique comes play time agent executes
autonomous action. action selected autonomous execution, algorithm
records agents state led decision saves value within variable sc
(line 11). execution autonomously selected action, algorithm checks
teacher demonstration every timestep (lines 22-23). corrective demonstration
made, new training datapoint consisting recorded demonstration state sc
corrective action ac added training set (line 24). classifier thresholds
retrained using new information.
12

fiInteractive Policy Learning Confidence-Based Autonomy

Algorithm 2 Confidence-Based Autonomy algorithm: Confident Execution Corrective
Demonstration
1: {}
2: conf inf
3: dist 0
4: true
5:
GetSensorData()
6:
actionComplete
7:
(ap , c, db) C(s)
8:
= NearestNeighbor(s)
9:
c > conf < dist
10:
ExecuteAction(ap )
11:
sc

12:
else
13:
RequestDemonstration()
14:
ad GetTeacherAction()
15:
ad 6= N U
16:
{(s, ad )}
17:
C UpdateClassifier(T )
18:
(conf , dist ) UpdateThresholds()
19:
ExecuteAction(ad )
20:
else
21:
autonomousAction

22:
ac GetTeacherAction()

23:
ac 6= N U

24:
{(sc , ac )}

25:
C UpdateClassifier(T )

26:
(conf , dist ) UpdateThresholds()


Using algorithm, teacher observes autonomous execution agent
corrects incorrect actions. Unlike previous demonstration technique
agent given next action perform, correction performed relation
agents previous state mistake made. example, observing
driving agent approaching close behind another car, teacher able indicate
instead continuing drive forward, agent merging
passing lane. way, addition indicating wrong action performed,
Corrective Demonstration provides algorithm action
performed place. technique effective negative reinforcement,
punishment, techniques common algorithms, leading agent learn quickly
mistakes.
13

fiChernova & Veloso

Figure 7: Screenshot driving simulator. agent, black car currently
center lane, drives fixed speed must navigate around cars avoid
collisions. road consists five lanes: three traffic lanes two shoulder
lanes.

6. Evaluation Comparison
section present evaluation comparison complete Confidence-Based
Autonomy algorithm components simulated car driving domain (Abbeel & Ng,
2004), shown Figure 7.
6.1 Domain Description
driving domain, agent represents car driving busy highway.
learners car travels fixed speed 60 mph, cars move lanes
predetermined speeds 20 40 mph. road three normal lanes
shoulder lane sides; agent allowed drive shoulder pass
cars, cannot go off-road. Since learner cannot change speed, must
navigate cars use shoulder lanes avoid collision. agent
limited three actions: remaining current lane, shifting one lane left
right current position (A = {forward,left,right}). teacher demonstrates task
keyboard interface. simulator framerate 5 fps paused
demonstration requests.
agents state represented by: = {l, dl , dc , dr }. State feature l discrete value
symbolizing agents current lane number. remaining three features, denoted
letter d, represent distance nearest car three driving lanes
(left, center right). distance features continuously valued [-25,25] range;
note nearest car lane behind agent. Distance measurements
corrupted noise create complex testing environment. agents policy
relearned time 10 new demonstrations acquired.
driving domain presents varied challenging environment; car distances
discretized rounding nearest integer value, domain would contain
600,000 possible states. Due complexity domain, agent requires large
14

fiInteractive Policy Learning Confidence-Based Autonomy

number demonstrations initialize classifier, resulting nearly constant demonstration requests early training process. simplify task teacher, add
short 300 datapoint, approximately 60 second, non-interactive driving demonstration
session initialize learning process. learning stage required, simplifies task teacher continuous demonstration preferred frequent
pauses demonstration requests.
performance learning algorithm evaluated time 100 new demonstrations acquired. evaluation, agent drove 1000 timesteps road
segment fixed consistent traffic pattern. road segment used
training, instead algorithm trained using randomly generated car traffic pattern.
Since algorithm aims imitate behavior expert, true reward function
exists evaluate performance given policy. present two domain-specific evaluation metrics capture key characteristics driving task. first evaluation
metric agents lane preference, proportion time agent spends
lane course trial. metric provides estimate similarity driving
styles. Since demonstrated behavior attempts navigate domain without collisions,
second evaluation metric number collisions caused agent. Collisions
measured percentage total timesteps agent spends contact
another car. Always driving straight colliding every car middle lane results
30% collision rate.
6.2 Experimental Results
present performance evaluation comparison following demonstration
selection techniques:
G Teacher-guided, demonstrations selected teacher without confidence feedback algorithm without ability perform retroactive
corrections
CES Confident Execution, demonstrations selected agent using single
fixed confidence threshold
CEM Confident Execution, demonstrations selected agent using multiple
adjustable confidence thresholds
CD Corrective Demonstration, demonstrations selected teacher performed corrections response mistakes made agent
CBA complete Confidence-Based Autonomy algorithm combining Confident
Execution using multiple adjustable confidence thresholds Corrective Demonstration
demonstration selection method, underlying policy agent learned
using multiple Gaussian mixture models, one action class (Chernova & Veloso,
2007). Videos driving task available www.cs.cmu.edu/soniac.
Figure 8 presents performance results five algorithms respect
defined lane preference collision metrics. describe discuss elements
15

fiChernova & Veloso

Figure 8: Evaluation agents driving performance 100-demonstration intervals
five demonstration selection methods. bar graphs indicate
percentage time agent spent road lane. Values bar
indicate percentage collision timesteps accrued evaluation trial.
teacher performance bar right figure shows teachers driving
lane preference collision rate evaluation road segment. goal
algorithm achieve performance similar teacher.

16

fiInteractive Policy Learning Confidence-Based Autonomy

figure detail following sections. evaluation, figure presents bar
representing composite graph showing percentage time spent agent
lane. value bar indicates number demonstrations upon
evaluated policy based. value bar indicates percentage incurred
collisions evaluation.
bar right figure shows performance teacher evaluation road segment. evaluation indicates teacher prefers drive center
left lanes, followed preference left shoulder, right shoulder right lane.
teacher successfully avoids collisions, resulting collision rate 0%. goal
learning algorithm achieve driving lane pattern similar teacher
without collisions. Note that, described previous section, policy learning
initialized 300-demonstration dataset algorithms. initialization
results identical performance across algorithms initial learning segment.
6.2.1 G Demonstration Selection
top row Figure 8 summarizes performance teacher-guided demonstration
selection approach. approach, teacher performed training alternating
observing performance agent selecting demonstrations that, opinion,
would improve driving performance. teacher selected training examples without
receiving feedback action selection confidence, without ability provide
corrective demonstrations incorrect actions already executed agent.
Instead, teacher required anticipate data would improve policy.
training process terminated teacher saw improvement agent
performance.
Figure 8 shows results agents performance evaluations 100-demonstration
intervals throughout learning process. similarity driving lane preference
agent improves slowly course learning, significant fluctuations.
example, 500 demonstrations, agents preference drive empty left
shoulder, thereby incurring collisions. One hundred demonstrations later, policy
shifted prefer center lane. However, agent yet learned avoid
cars, resulting 38.8% collision rate. policy stabilizes approximately 1100
demonstrations, representing driving style similar teacher, small
number collisions. Without confidence feedback agent, difficult
teacher select exact termination point learning. Training continued until,
1300 demonstrations, learners policy showed little improvement. final policy
resulted lane preference similar expert, 2.7% collision
rate.
6.2.2 CES Demonstration Selection
second row Figure 8 presents results Confident Execution algorithm
single autonomy threshold. demonstration selection approach, demonstrations
selected agent learning terminated agent stopped requesting
demonstrations performed actions autonomously. autonomy threshold value
17

fiChernova & Veloso

selected hand evaluated multiple performance trials. Results best fixed
threshold presented.
Compared teacher-guided approach, policy learned using CES algorithm
stabilizes quickly, achieving performance similar teachers 700 demonstrations. number collisions low persistent, even agent gains full
confidence stops requesting demonstrations 1008 demonstrations. final lane
preference similar expert, collision rate 3.8%.
6.2.3 CEM Demonstration Selection
third row Figure 8 presents results Confident Execution algorithm
multiple autonomy thresholds, calculated using algorithm presented Section 4.2.2. demonstration selection methods, CEM required fewest number
demonstrations learn task, completing learning 504 demonstrations.
result indicates use multiple adjustable thresholds successfully focuses demonstration selection informative areas state space greatly reducing number
redundant demonstrations. Throughout learning process, number Gaussian
components within model varied 9 41. large variation highlights
importance automating threshold calculation process, since hand-selecting individual
thresholds component would impractical. lane preference final policy
similar expert. However, agent still maintained small collision
rate 1.9%.
6.2.4 CD Demonstration Selection
evaluation first three algorithms highlights difficulty driving problem.
approaches able select demonstrations resulted policy
mimics overall driving style teacher. However, policies resulted
small number collisions, typically occurred agent merged close
another vehicle touched bumper. mistakes difficult correct using
techniques evaluated far. Even within teacher guided demonstration selection
method, human teacher full control demonstration training data,
time collision observed incorrect decision already made
algorithm. Instead, retroactive demonstration required correct already made
mistakes, Corrective Demonstration algorithm.
fourth row Figure 8 present evaluation demonstration selection
using Corrective Demonstration algorithm. approach, demonstrations
selected teacher corrections response mistakes made agent.
Behavior corrected teacher included collisions, well incorrect lane preference
(e.g. always driving shoulder) rapid oscillations lanes. enable
teacher accurately perform corrections, simulation slowed 5 2 frames
per second. Learning terminated agent required corrections.
shown Figure 8, complete training process using Corrective Demonstration took 547
demonstrations, achieving final policy correctly imitates teachers driving style
0% collision rate. following section, discuss performance compares
complete CBA algorithm.
18

fiInteractive Policy Learning Confidence-Based Autonomy

6.2.5 CBA Demonstration Selection
final row Figure 8 presents evaluation complete Confidence-Based Autonomy algorithm, combines CEM CD. Using approach, learning complete
agent longer requests demonstrations able perform driving task
without collisions. Using CBA agent required total 703 demonstrations learn
task, successfully learning navigate highway without collisions.
analyze impact two CBA learning components comparing number
distribution demonstrations acquired algorithm learning process.
section refer learning components CBA CBA-CE CBA-CD
differentiate algorithm evaluations presented previous sections. Note
behavior Confident Execution component dependent upon method used
set autonomy thresholds. evaluation use multiple adjustable thresholds
calculated average value misclassified points.
Figure 9(a), datapoint along x-axis represents number demonstrations
requested using CBA-CE (top) initiated teacher using CBA-CD (bottom)
100-timestep interval, approximately 40 seconds simulator runtime (excluding pauses
demonstration requests). Since first three 100-demonstration timesteps consist entirely non-interactive demonstration, values timesteps 100 and, due
scaling, exceed bounds graph. Figure 9(b) shows cumulative number
demonstrations component, total, grows respect training time.
complete training process lasts approximately hour half.
Analysis graphs shows demonstrations occur early training
process. Importantly, Confident Execution accounts 83% total number demon-

(a)

(b)

Figure 9: (a) Timeline showing number demonstrations initiated agent
Confident Execution (top) initiated teacher Corrective Demonstrations (bottom) changes course training. (b)
cumulative number demonstrations acquired component, total,
time.

19

fiChernova & Veloso

strations, indicating agent guides learning. demonstration requests occur first minutes training agent encounters
many novel states classification confidence remains low. agent requires
corrections stage many mistakes prevented requesting demonstration instead performing low confidence action. Corrective Demonstration plays
greatest role towards end training process, accounts 73% final
100 demonstrations. stage learning agents action selection confidence
high enough rarely asks demonstrations. policy already closely imitates
teachers driving style small number collisions remain. Corrective Demonstration
enables teacher fine-tune policy eliminate collisions. result highlights
importance Corrective Demonstration, whether alone conjunction another
selection technique, optimizing policy performance.
CBA achieves similar final performance compared CD algorithm evaluated
previous section, requires approximately 150 additional demonstrations learn
policy. additional demonstrations attributed Confident Execution demonstration requests served increase classification confidence change
outcome agents action. Viewed another way, datapoints correspond states
agent would performed correct action even asked
demonstration. result appears allowing agent make mistakes
correcting fact, done CD evaluation, may best demonstration
selection approach respect performance metrics defined overall
number demonstrations.
However, eliminating ability request demonstrations utilizing retroactive correction several drawbacks, namely requiring constant full attention
teacher, and, importantly, requiring agent make many mistakes learns
correct policy. comparison, CBA algorithm enables agent request demonstrations low confidence states, thereby avoiding many incorrect actions. original
lane preference collision metrics take difference account focus
final policy performance agent.
evaluate difference algorithms, additionally examine number
collisions agent incurs course learning. Using CD algorithm,
agent incurs 48% collisions (278 vs. 188) training using CBA.
Therefore, allowing agent request demonstrations low-confidence states,
CBA algorithm requires slightly greater number demonstrations greatly reducing
number incorrect actions performed learning. reduction number
action errors significant due importance many learning domains, especially
robotic applications errors may pose dangers system.
summary, evaluation shown ability retroactively correct mistakes
crucial optimizing policy eliminating collisions. best performance
achieved Corrective Demonstration Confidence-Based Autonomy methods,
CD requiring fewer demonstrations incurring greater number collisions
training. choice CD CBA therefore viewed tradeoff
number demonstrations frequency undesired actions training.
fact, CD special case CBA autonomy threshold set classify
points high confidence. Adjusting selectiveness CBA autonomy thresholds
20

fiInteractive Policy Learning Confidence-Based Autonomy

could, therefore, provide user sliding control mechanism effects agents
tendency perform autonomous actions versus demonstration requests. Importantly,
note overall number demonstrations required either approach less
teacher-guided method tiny fraction overall state space.

7. Discussion
section, discuss several promising directions future work, well number
existing extensions presented learning methods.
7.1 Evaluation Non-Technical Users
presented demonstration learning algorithm provides fast intuitive method
programming adapting behavior autonomous agents. believe general
representation classifier-independent approach makes CBA usable wide range
applications. One particular application interest use demonstration learning
enable non-technical users program autonomous agents. believe CBA would
highly suitable application assume teacher technical
knowledge policy learning, requiring teacher expert task.
results presented article obtained using single teacher, one
authors. Additional studies could evaluate algorithm usability performance wider
user base, non-programmers particular.
7.2 Representation Action Choices
Demonstration-based learning provides natural intuitive interface transferring human task knowledge autonomous agents. However, operating rich environments,
agents inevitably face situations multiple actions equivalently applicable.
example, agent encounters obstacle directly path option moving
left right avoid it. surrounding space empty, directions equally valid
performing desired task. Human demonstrators faced choice equivalent
actions typically perform demonstrations consistently, instead selecting among
applicable actions arbitrarily time choice encountered. result, training
data obtained agent lacks consistency, identical, nearly identical, states
associated different actions. presented CBA algorithm, inconsistent
demonstrations would result persistent region low confidence, leading agent
repeatedly request demonstrations within inconsistent domain region. successfully extended CBA identify regions state space conflicting demonstrations
represent choice multiple actions explicitly within agents policy (Chernova & Veloso, 2008a).
7.3 Improvement Beyond Teacher Performance
policy learned Confidence-Based Autonomy algorithm inherently limited
quality demonstrations provided human teacher. Assuming
teacher expert task, approach aims imitate behavior teacher.
However, many domains teacher demonstrations may suboptimal limited
21

fiChernova & Veloso

human ability. Several demonstration learning approaches developed enable
agent learn experiences addition demonstrations, thereby improving
performance beyond abilities teacher (Stolle & Atkeson, 2007; Smart, 2002).
Extending CBA algorithm include similar capability remains promising direction
future work. Possible approaches include incorporating high-level feedback (Argall,
Browning, & Veloso, 2007) reward signal (Thomaz & Breazeal, 2006) teacher,
well filtering noisy inaccurate demonstrations.
7.4 Policy Use Learning
CBA algorithm considers learning complete agent able perform
required behavior, repeatedly correctly, without requesting demonstrations
requiring corrections. policy learning complete, standard procedure
vast majority policy learning algorithms turn learning process freeze
policy. approach used algorithm, propose
continuing use Confident Execution component may long-term benefits
beyond policy learning. particular, algorithms ability identify anomalous states
may enable agent detect notify user system errors unexpected input.
studies needed evaluate use algorithm, believe
mechanism would provide useful safety feature long-term autonomous operation
negligible cost performing threshold comparison timestep.
7.5 Richer Interaction
presented demonstration learning approach relies limited form interaction agent teacher. agent requests demonstrations teacher,
teacher responds single recommended action. level interaction
typical traditional active learning approaches, fails take full advantage
vast task knowledge teacher possesses. believe extending algorithm
include richer interaction abilities could provide faster intuitive training
method. Many promising directions future research exist area. example,
developing domain-independent dialog exchange agent teacher incorporates clarification questions high level advice could speed learning enable
agent represent high level goals task. ability play back rewind
demonstration sequences would additionally enable teacher agent reexamine
reevaluate past learning experiences.
7.6 Application Single-Robot Multi-Robot Systems
Learning demonstration techniques extensively studied within robotics
community due interactive nature fast learning times. work,
shown CBA algorithm highly effective learning variety single-robot tasks
(Chernova & Veloso, 2007, 2008a).
Furthermore, many complex tasks require collaboration multiple robots.
now, one greatest challenges preventing demonstration learning algorithms
generalizing multi-robot domains problem limited human attention,
22

fiInteractive Policy Learning Confidence-Based Autonomy

fact teacher able pay attention to, interact with, robots
time. Based CBA algorithm, developed first multi-robot
demonstration learning system addresses limited human attention problem
taking advantage fact Confident Execution component CBA prevents
autonomous execution actions low-confidence states (Chernova & Veloso, 2008b).
flexMLfD system utilizes individual instances CBA robot, learner
acquires unique set demonstrations learns individual task policy. preventing
autonomous execution low-confidence states, CBA makes learner robust periods
teacher neglect, allowing multiple robots taught time.

8. Conclusion
article presented Confidence-Based Autonomy, interactive algorithm policy
learning demonstration. Using algorithm, agent incrementally learns
action policy demonstrations acquired practices task. CBA algorithm
contains two methods obtaining demonstrations. Confident Execution component
enables agent select demonstrations real time interacts environment,
using confidence distance thresholds target states unfamiliar
current policy action uncertain. Corrective Demonstration component allows
teacher additionally perform corrective demonstrations incorrect action
selected agent. teacher retroactively provides demonstrations specific error
cases instead attempting anticipate errors ahead time. Combined, techniques
provide fast intuitive approach policy learning, incorporating shared decision
making learner teacher.
Experimentally, used complex simulated driving domain compare five methods
selecting demonstration training data: manual data selection teacher, confidencebased selection using single fixed threshold, confidence-based selection using multiple
automatically calculated thresholds, corrective demonstration, confidence-based selection combined corrective demonstration. Based evaluation, conclude
confidence-based methods able select informative demonstrations
human teacher. single multiple threshold approaches, multiple adjustable
threshold technique required significantly fewer demonstrations focusing onto regions
uncertainty reducing number redundant datapoints. best final policy performance, however, achieved Corrective Demonstration complete ConfidenceBased Autonomy algorithms, achieved lane preference similar
teacher without collisions. Together, demonstration selection algorithms represent
tradeoff number demonstrations frequency undesired actions
training. Corrective Demonstration required slightly fewer demonstrations
learn final policy, compared CBA resulted significant increase number
errors made agent course learning process. CBA algorithm,
therefore, provides best demonstration selection method domains incorrect
actions desirable training process.
23

fiChernova & Veloso

Acknowledgments
research partially sponsored Department Interior, National Business
Center contract no. NBCHD030010 SRI International subcontract no.
03-000211, BBNT Solutions subcontract no. 950008572, via prime Air Force
contract no. SA-8650-06-C-7606. views conclusions contained document
authors interpreted representing official policies,
either expressed implied, sponsoring institution, U.S. government
entity. Additional thanks Paul Rybski making simulation package available.

References
Abbeel, P., & Ng, A. (2004). Apprenticeship learning via inverse reinforcement learning.
Proceedings International Conference Machine Learning, New York, NY,
USA. ACM Press.
Argall, B., Chernova, S., Browning, B., & Veloso, M. (2009). survey robot learning
demonstration. Robotics Autonomous Systems, appear.
Argall, B., Browning, B., & Veloso, M. (2007). Learning demonstration critique human teacher. Second Annual Conference Human-Robot Interactions
(HRI 07), Arlington, Virginia.
Atkeson, C. G., & Schaal, S. (1997). Robot learning demonstration. Proceedings
International Conference Machine Learning, pp. 1220, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
Bentivegna, D. C., Ude, A., Atkeson, C. G., & Cheng, G. (2004). Learning act
observation practice. International Journal Humanoid Robotics, 1 (4).
Blum, A. L., & Langley, P. (1997). Selection relevant features examples machine
learning. Artificial Intelligence, 97 (1-2), 245271.
Browning, B., Xu, L., & Veloso, M. (2004). Skill acquisition use dynamicallybalancing soccer robot. Proceedings Nineteenth National Conference Artificial
Intelligence, pp. 599604.
Chernova, S., & Veloso, M. (2007). Confidence-based policy learning demonstration
using gaussian mixture models. Proceedings International Conference
Autonomous Agents Multiagent Systems, pp. 18.
Chernova, S., & Veloso, M. (2008a). Learning equivalent action choices demonstration.
Proceedings International Conference Intelligent Robots Systems,
pp. 12161221.
Chernova, S., & Veloso, M. (2008b). Teaching collaborative multi-robot tasks
demonstration. Proceedings IEEE-RAS International Conference Humanoid Robots.
Clouse, J. A. (1996). integrating apprentice learning reinforcement learning. Ph.D.
thesis, University Massachisetts, Department Computer Science.
Cohn, D., Atlas, L., & Ladner, R. (1994). Improving generalization active learning.
Machine Learning, 15 (2), 201221.
24

fiInteractive Policy Learning Confidence-Based Autonomy

Grollman, D., & Jenkins, O. (2007). Dogged learning robots. IEEE International
Conference Robotics Automation, pp. 24832488.
Inamura, T., Inaba, M., & Inoue, H. (1999). Acquisition probabilistic behavior decision model based interactive teaching method. Proceedings Ninth
International Conference Advanced Robotics, pp. 523528.
Lockerd, A., & Breazeal, C. (2004). Tutelage socially guided robot learning. Proceedings IEEE/RSJ International Conference Intelligent Robots Systems,
pp. 34753480.
Nicolescu, M. N. (2003). framework learning demonstration, generalization
practice human-robot domains. Ph.D. thesis, University Southern California.
Papudesi, V. (2002). Integrating advice reinforcement learning. Masters thesis, University Texas Arlington.
Price, B., & Boutilier, C. (2003). Accelerating reinforcement learning implicit
imitation.. Journal Artificial Intelligence Research, 19, 569629.
Saunders, J., Nehaniv, C. L., & Dautenhahn, K. (2006). Teaching robots moulding behavior scaffolding environment. Proceeding 1st ACM SIGCHI/SIGART
conference Human-robot interaction, pp. 118125, New York, NY, USA. ACM
Press.
Schaal, S. (1997). Learning demonstration. Advances Neural Information Processing Systems, pp. 10401046. MIT press.
Smart, W. D. (2002). Making Reinforcement Learning Work Real Robots. Ph.D. thesis,
Department Computer Science, Brown University, Providence, RI.
Stolle, M., & Atkeson, C. G. (2007). Knowledge transfer using local features. Proceedings IEEE International Symposium Approximate Dynamic Programming
Reinforcement Learning, pp. 2631.
Sutton, R., & Barto, A. (1998). Reinforcement Learning: Introduction. MIT Press,
Cambridge, MA.
Takahashi, Y., Hikita, K., & Asada, M. (2004). hierarchical multi-module learning system
based self-interpretation instructions coach. Proceedings RoboCup 2003:
Robot Soccer World Cup VII, pp. 576 583.
Thomaz, A. L., & Breazeal, C. (2006). Reinforcement learning human teachers: Evidence feedback guidance implications learning performance. Proceedings Twenty-First Conference Artificial Intelligence, pp. 10001005.

25


