journal artificial intelligence

submitted published

heuristic search
continuous resources stochastic domains
nicolas meuleau

nicolas f meuleau nasa gov

nasa ames center
mail stop
moffet field ca usa

emmanuel benazera

ebenazer laas fr

laas cnrs universite de toulouse
av du colonel roche
toulouse cedex france

ronen brafman

brafman cs bgu ac il

department computer science
ben gurion university
beer sheva israel

eric hansen

hansen cse msstate edu

department computer science engineering
mississippi state university
mississippi state ms usa

mausam

mausam cs washington edu

department computer science engineering
university washington
seattle wa usa

abstract
consider optimal stochastic domains resource constraints
resources continuous choice action step depends resource availability introduce hao generalization ao performs
search hybrid state space modeled discrete continuous state variables continuous variables represent monotonic resources heuristic search
hao leverages knowledge start state admissible heuristic focus
computational effort parts state space could reached start state
following optimal policy especially effective resource
constraints limit much state space reachable experimental demonstrate
effectiveness domain motivates automated planetary
exploration rovers

introduction
many nasa planetary exploration missions rely rovers mobile robots carry suite
scientific instruments use characterizing planetary surfaces transmitting information back
earth difficulties communicating devices distant planets direct human
control rovers tele operation infeasible rovers must able act autonomously
substantial periods time example mars exploration rovers mer aka spirit
opportunity designed communicate ground twice per martian day
autonomous control planetary exploration rovers presents many challenges
automated progress made meeting challenges example
software developed mars sojourner mer rovers contributed significantly

c

ai access foundation rights reserved

fimeuleau benazera brafman hansen mausam

success missions bresina jonsson morris rajan many important
challenges must still addressed achieve ambitious goals future missions bresina
dearden meuleau ramakrishnan smith washington
among challenges plan execution uncertain environments planetary
surfaces mars uncertainty terrain meteorological conditions state
rover position battery charge solar panels component wear etc turn leads
uncertainty outcome rovers actions much uncertainty resource
consumption example factors slope terrain affect speed movement rate
power consumption making difficult predict certainty long take rover
travel two points much power consume limits
critical resources time battery power rover plans currently conservative
worst case estimates time resource usage addition instructions sent
planetary rovers form sequential plan attaining single goal e g photographing
interesting rock action unintended outcome causes plan fail rover
stops waits instructions makes attempt recover achieve alternative
goal utilized resources missed science opportunities
past decade great deal generate conditional
plans domains uncertain action outcomes much work formalized framework
markov decision processes puterman boutilier dean hanks however
bresina et al point important aspects rover adequately
handled traditional including markov decision processes
particular traditional planners assume discrete state space small discrete number
action outcomes automated planetary exploration rovers critical resources
time battery power continuous uncertainty domain
effect actions variables requires conditional planner branch
discrete action outcomes availability continuous resources planner
must able reason continuous well discrete state variables
closely related challenges uncertain plan execution continuous resources
challenge subscription rovers future missions much improved
capabilities whereas current mer rovers require average three days visit single rock
progress areas automatic instrument placement allow rovers visit multiple rocks
perform large number scientific observations single communication cycle pedersen
smith deans sargent kunz lees rajagopalan moreover communication cycles
lengthen substantially distant missions moons jupiter saturn requiring longer
periods autonomous behavior space scientists future missions expected
specify large number science goals often present known oversubscription refers infeasible achieve goals
objective achieve best subset goals within resource constraints smith
case rover multiple locations rover could reach many experiments
rover could conduct combinations infeasible due resource constraints
planner must select feasible subset maximizes expected science return action
outcomes including resource consumption stochastic plan maximizes expected science
return conditional plan prescribes different courses action
previous actions including resource availability
present implemented handles
together uncertain action outcomes limited continuous resources subscription
formalize rover hybrid state markov decision process markov
decision process mdp discrete continuous state variables use continuous
variables represent resources introduce heuristic search
called hao hybrid state ao generalization classic ao heuristic search nilsson pearl whereas ao searches discrete state spaces hao solves



fihao

hybrid domains discrete continuous state variables handle
hybrid domains hao builds earlier work dynamic programming continuous
hybrid state mdps particular work feng et al
generalizing graph search hybrid state spaces poses complex challenge
consider special case particular continuous variables used represent
monotonic resources search best conditional plan allows branching
values discrete variables availability resources violate
resource constraint
well known heuristic search efficient dynamic programming
uses reachability analysis guided heuristic focus computation relevant parts state
space resource constraints including subscription
heuristic search especially effective resource constraints significantly limit
reachability unlike dynamic programming systematic forward search ao keeps
track trajectory start state reachable state thus check whether
trajectory feasible violates resource constraint pruning infeasible trajectories heuristic
search dramatically reduce number states must considered
optimal policy particularly important domain discrete state space huge
exponential number goals yet portion reachable initial state relatively
small due resource constraints

formulation background
start formal definition tackling special case
hybrid state markov decision process first define model discuss
include resource constraints formalize subscription model finally
review class dynamic programming solving hybrid state mdps since
algorithmic techniques incorporated heuristic search develop
section
hybrid state markov decision process
hybrid state markov decision process hybrid state mdp factored markov decision process
discrete continuous state variables define tuple n x p r
n discrete state variable x x x xd set continuous state variables set
actions p stochastic state transition model r reward function describe
elements detail hybrid state mdp sometimes referred simply hybrid
mdp term hybrid refer dynamics model discrete another
term hybrid state mdp originates markov chain literature general state
mdp
although hybrid state mdp multiple discrete variables plays role described notational convenience model discrete component
state space single variable n focus continuous component assume
n
domain continuous variable xi x closed interval real line x xi
hypercube continuous variables defined state set hybrid state
mdp set possible assignments values state variables particular hybrid
state pair n x n n value discrete variable x xi vector
values continuous variables
state transitions occur actions process evolves according markovian
state transition probabilities pr n x denotes state action
n x denotes state action called arrival state probabilities
decomposed



fimeuleau benazera brafman hansen mausam

discrete marginals pr n n x n x

pr n n x
r
continuous conditionals pr x n x n n x n x x pr x n x n dx

p

n n

assume reward associated transition function arrival state let
rn x denote reward associated transition state n x complex dependencies
possible sufficient goal domain consider
resource constraints subscription
model rover consider special type mdp objective
optimize expected cumulative reward subject resource constraints make following
assumptions
initial allocation one non replenishable resources
action minimum positive consumption least one resource
resources exhausted action taken
one way model mdp resource constraints formulate constrained mdp
model widely studied operations community altman
model action incurs transition dependent resource cost cai resource
given initial allocation resources initial state linear programming used
best feasible policy may randomized policy although constrained mdp
resource consumption include resources state space policy cannot
conditioned upon resource availability resource consumption
deterministic unobservable good fit rover domain resource
consumption stochastic observable rover take different actions depending
current resource availability
adopt different modeling resource constraints resources included
state description although increases size state space allows decisions
made resource availability allows stochastic model resource consumption since
resources rover domain continuous use continuous variables hybrid state mdp
represent resources note duration actions one biggest sources uncertainty
rover model time one continuous resources resource constraints
represented form executability constraints actions x denotes set
actions executable state n x action cannot executed state satisfy
minimum resource requirements
discussed incorporate resource consumption resource constraints hybridstate mdp next discuss formalize subscription rover
scientists provide planner set goals would rover achieve
goal corresponds scientific task taking picture rock performing
analysis soil sample scientists specify utility reward goal usually
subset goals feasible resource constraints feasible
plan maximizes expected utility subscription planetary exploration rovers
considered smith van den briel et al deterministic domains
consider subscription stochastic domains especially domains stochastic
resource consumption requires construction conditional plans selection goals
achieve change depending resource availability
subscription utility associated goal achieved
additional utility achieved repeating task therefore discrete state must include set
boolean variables keep track set goals achieved far rover one boolean


fihao

variable goal keeping track already achieved goals ensures markovian reward structure
since achievement goal rewarded achieved past however
significantly increases size discrete state space maintaining history information ensure
markovian reward structure simple example non markovian rewards thiebaux
gretton slaney price kabanza
optimality equation
rover consider special case finite horizon hybrid state mdp
termination occurs indefinite number steps bellman optimality equation
takes following form
vn x



vn x



n x terminal state otherwise


z
x
max
pr n n x
pr x n x n rn x vn x dx

aan x

n n



x

define terminal state state actions eligible execute x
use terminal states model conditions plan termination includes situation
goals achieved situation resources exhausted
situation action error condition requires executing safe sequence
rover terminating plan execution addition terminal states assume explicit
initial state denoted n x
assuming resources limited non replenishable every action consumes
resource amount consumed greater equal positive quantity c plan
execution terminate finite number steps maximum number steps bounded
initial resource allocation divided c minimal resource consumption per step actual
number steps usually much less indefinite resource consumption stochastic
choice action influences resource consumption number steps takes
plan terminate bounded indefinite call bounded horizon mdp contrast
finite horizon mdp however note bounded horizon mdp converted
finite horizon mdp specifying horizon equal maximum number plan steps
introducing op action taken terminal state
note usually difference number plan steps time plan takes
execute since model time one continuous resources time takes execute
plan step state action dependent stochastic
given hybrid state mdp set terminal states initial state n x objective
policy n x maximizes expected cumulative reward specifically
optimal policy value function satisfies optimality equation given equation
rover domain cumulative reward equal sum rewards goals achieved
reaching terminal state direct incentive save resources optimal solution saves
resources allows achieving goals however framework general enough
allow reasoning cost availability resources example incentive
conserving resources could modeled specifying reward proportional amount
resources left unused upon entering terminal state note framework allows reasoning
cost availability resources without needing formulate
multi objective optimization stay standard decision theoretic framework
dynamic programming continuous state hybrid state mdps
consider finite horizon hybrid state mdp solved
solving finite horizon hybrid state mdps solving hybridstate continuous state mdps rely form approximation widely used


fimeuleau benazera brafman hansen mausam

figure value function initial state simple rover optimal expected return
function two continuous variables time energy remaining

discretize continuous state space finite number grid points solve resulting
finite state mdp dynamic programming interpolation rust munos moore
another parametric function approximation function associated
dynamic programming value function policy function approximated
smooth function k unknown parameters general parametric function approximation
faster grid approximation drawback may fail converge may
converge incorrect solution parametric function approximation used
solving continuous state mdps besides dynamic programming reinforcement learning
use artificial neural networks function approximators bertsekas tsitsiklis
solving mdps called approximate linear programming extended allow continuous
well discrete state variables kveton hauskrecht guestrin
review another solving hybrid state continuous state mdps assumes
special structure exploited dynamic
programming
r
structure assumed ensures convolution x pr x n x n rn x
vn x dx equation computed exactly finite time value function computed
dynamic programming piecewise constant piecewise linear initial idea
due work boyan littman describe class mdps called time dependent
mdps transitions take place along single irreversible continuous dimension
describe dynamic programming computing exact piecewise linear value function
transition probabilities discrete rewards piecewise linear feng et al
extend continuous state spaces one dimension consider mdps
discrete transition probabilities two types reward piecewise constant piecewise
linear li littman extend allow transition probabilities
piecewise constant instead discrete although extension requires approximation
dynamic programming
structure exploited characteristic mars rover domain
subscription figure shows optimal value functions
initial state typical mars rover function two continuous variables
time energy remaining bresina et al value functions feature set humps
plateaus representing region state space similar goals pursued
optimal policy sharpness hump plateau reflects uncertainty achieving
goal constraints impose minimal resource levels attempting actions introduce



fihao

sharp cuts regions plateau regions expected reward nearly constant represent
regions state space optimal policy probability distribution
future histories induced optimal policy nearly constant
structure value function exploited partitioning continuous state
space finite number hyper rectangular regions region hyper rectangle
cartesian product intervals dimension hyper rectangle value function
constant piecewise constant function linear piecewise linear function
resolution hyper rectangular partitioning adjusted fit value function large hyperrectangles used represent large plateaus small hyper rectangles used represent regions
state space finer discretization value function useful edges
plateaus curved hump time energy available natural choice
data structures rectangular partitioning continuous space kd trees friedman bentley
finkel although choices possible figures section value
functions initial state simple rover created piecewise constant
partitioning continuous state space
continuous state domains transition reward functions similarly partitioned
hyper rectangles reward function action piecewise constant piecewiselinear representation value function transition function partitions state space
regions set outcomes action probability distribution set
outcomes identical following boyan littman relative absolute transitions
supported relative outcome viewed shifting region constant
two states x region transition probabilitiesp r x x p r
defined term probability x x absolute outcome
maps states region single state two states x region
p r x x p r x view relative outcome pair p p probability
outcome view absolute outcome pair x p assumes
finite number non zero probabilities e probability distribution discretized
means state action finite set states reached non zero probability
representation guarantees dynamic programming update piecewise constant value
function another piecewise constant value function feng et al
transition functions finite horizon exists partition continuous space
hyper rectangles optimal value function piecewise constant linear
restriction discrete transition functions strong one often means transition
function must approximated example rover power consumption normally distributed
thus must discretized since amount power available must non negative
implementation truncates negative part normal distribution renormalizes continuous transition function approximated appropriately fine discretization feng et
al argue provides attractive alternative function approximation approaches
approximates model solves approximate model exactly rather finding
approximate value function original model reason sometimes refer
finding optimal policies value functions even model approximated
avoid discretizing transition function li littman describe allows
piecewise constant transition functions exchange approximation dynamic programming marecki et al describe different class
probability distributions resource consumptions represented phase type distributions dynamic programming exploits representation although use
work feng et al implementation heuristic search develop
next section could use representing computing value
functions policies hybrid state mdp



fimeuleau benazera brafman hansen mausam

heuristic search hybrid state space
section present primary contribution solving special
class hybrid state mdps novel generalization heuristic search ao
particular describe generalization solving hybrid state mdps
continuous variables represent monotonic constrained resources acyclic plan found
search allows branching availability resources
motivation heuristic search potentially huge size state space
makes dynamic programming infeasible one reason size existence continuous
variables even consider discrete component state space size
state space exponential number discrete variables well known ao
effective solving large state space considers states
reachable initial state uses informative heuristic function focus
states reachable course executing good plan ao often
optimal plan exploring small fraction entire state space
begin section review standard ao consider
generalize ao search hybrid state space discuss properties generalized
well efficient implementations
ao
recall ao graph search nilsson pearl
graphs arise choices components choice
multiple consequences component case uncertainty
hansen zilberstein graph search techniques used solving
mdps
following nilsson hansen zilberstein define graph
hypergraph instead arcs connect pairs nodes ordinary graph hypergraph
hyperarcs k connectors connect node set k successor nodes mdp
represented hypergraph node corresponds state root node corresponds start
state leaf nodes correspond terminal states thus often use word state refer
corresponding node hypergraph representing mdp k connector corresponds
action transforms state one k possible successor states probability attached
successor probabilities sum one assume graph
acyclic consistent assumption underlying mdp bounded horizon
graph search solution takes form acyclic subgraph called solution
graph defined follows
start node belongs solution graph
every non terminal node solution graph exactly one outgoing k connector corresponding action part solution graph successor nodes belongs
solution graph
every directed path solution graph terminates terminal node
solution graph maximizes expected cumulative reward found solving following
system equations

terminal
state otherwise
p

v




maxaa
p

r r v
v denotes expected value optimal solution state v called
optimal evaluation function value function mdp terminology note identical


fihao

optimality equation hybrid state mdps defined equation latter restricted
discrete state space keeping convention literature mdps treat
value maximization even though ao usually formalized solving cost minimization

state space search formalized graphs optimal solution
graph found heuristic search ao nilsson pearl
heuristic search advantage ao dynamic programming
optimal solution particular starting state without evaluating states therefore
graph usually supplied explicitly search implicit graph g specified
implicitly start node start state successor function generates successors
states state action pair search constructs explicit graph g initially
consists start state tip leaf state explicit graph said terminal
goal state state action taken otherwise said
nonterminal nonterminal tip state expanded adding explicit graph outgoing
k connectors one action successor states already explicit graph
ao solves state space search gradually building solution graph beginning
start state partial solution graph defined similarly solution graph difference
tip states partial solution graph may nonterminal states implicit graph
partial solution graph defined follows
start state belongs partial solution graph
every non tip state partial solution graph exactly one outgoing k connector corresponding action part partial solution graph successor states
belongs partial solution graph
every directed path partial solution graph terminates tip state explicit graph
value partial solution graph defined similarly value solution graph
difference tip state partial solution graph nonterminal value
propagated backwards instead assume admissible heuristic estimate
h maximal value solution graph state heuristic evaluation function h said
admissible h v every state recursively calculate admissible heuristic
estimate v optimal value state explicit graph follows

terminal state
v
nonterminal tip state

h isp



maxaa
p r r v otherwise



best partial solution graph determined time propagating heuristic estimates
tip states explicit graph start state mark action maximizes
value state best partial solution graph determined starting root
graph selecting best e marked action reachable state
table outlines ao finding optimal solution graph acyclic
graph interleaves forward expansion best partial solution value update step
updates estimated state values best partial solution simplest version ao
values expanded state ancestor states explicit graph updated
fact ancestor states need evaluated expanded state
reached taking marked actions e choosing best action state thus
parenthetical remark step b table indicates parent state added
z unless estimated value state changed state reached state
choosing best action state ao terminates policy expansion step



fimeuleau benazera brafman hansen mausam

explicit graph g initially consists start state
best solution graph nonterminal tip state
expand best partial solution expand nonterminal tip state best partial
solution graph add successor states g state added
g expanding terminal state v else v h
b update state values mark best actions
create set z contains expanded state ancestors explicit
graph along marked action arcs e include ancestor states
expanded state reached following current best solution
ii repeat following steps z empty
remove z state descendant g occurs z
p
b set v maxaa p r r v mark best action
determining best action resolve ties arbitrarily give preference currently marked action
c identify best solution graph nonterminal states fringe
return optimal solution graph
table ao
nonterminal states fringe best solution graph point best solution
graph optimal solution
following literature graph search far referred solution found
ao solution graph following ao used solve mdp sometimes
follow literature mdps referring solution policy sometimes refer
policy graph indicate policy represented form graph
hybrid state ao
consider generalize ao solve bounded horizon hybrid state mdp challenge
face applying ao challenge performing state space search hybrid
state space
solution adopt search aggregate state space represented
graph node distinct value discrete component state
words node graph represents region continuous state space
discrete value given partition continuous state space use
graph search techniques solve mdp parts state space reachable
start state best policy
however graph search techniques must modified important ways allow search
hybrid state space represented way particular longer correspondence nodes graph individual states node corresponds
continuous region state space different actions may optimal different hybrid states associated search node case rover example
best action likely depend much energy time remaining energy time
continuous state variables
address still optimal solution attach search node set
functions continuous variables make possible associate different values heuristics
actions different hybrid states map search node explicit



fihao

search graph consists nodes edges graph generated far
describes states considered far search difference
use complex state representation set continuous functions allows
representation reasoning continuous part state space associated search
node
begin describing complex node data structure describe hao

data structures
node n explicit graph g consists following
value discrete state variable
pointers parents children explicit graph policy graph
openn open list x x openn x indicates whether n x
frontier explicit graph e generated yet expanded
closedn closed list x x closedn x indicates whether n x
interior explicit graph e already expanded
note n x openn x closedn x state cannot open
closed parts continuous state space associated node
neither open closed explicit graph contains trajectory start state
particular hybrid state hybrid state considered generated even search
node corresponds generated states neither open closed
addition non terminal states open closed note refer open
closed nodes instead refer hybrid states associated nodes open
closed
hn heuristic function x x hn x heuristic estimate optimal
expected cumulative reward state n x
vn value function open state n x vn x hn x closed state
n x vn x obtained backing values successor states equation
n policy note defined closed states
reachablen x x reachablen x indicates whether n x reachable
executing current best policy beginning start state n x
assume continuous functions represent information hybrid states associated search node partition state space associated node
discrete number regions associate distinct value action region given
partitioning hao expands evaluates regions hybrid state space
instead individual hybrid states finiteness partition important order ensure
search frontier extended finite number expansions ensure hao
terminate finite number steps implementation hao described section use piecewise constant partitioning continuous state space proposed feng et
al however method discrete partitioning could used provided condition
holds example li littman describe alternative method partitioning
note two forms state space partitioning used first hybrid state
space partitioned finite number regions one discrete state



fimeuleau benazera brafman hansen mausam

regions corresponds node graph second continuous state space associated particular node partitioned smaller regions piecewise constant
representation continuous function one used feng et al
addition complex representation nodes graph
requires complex definition best partial solution standard ao oneto one correspondence nodes individual states means solution policy
represented entirely graph called partial solution graph single action
associated node hao continuum states associated
node different actions may optimal different regions state space associated
particular node hao partial solution graph sub graph explicit
graph defined follows
start node belongs solution graph
every non tip node solution graph one outgoing k connectors part
solution graph one action optimal hybrid state associated
node successor nodes belongs solution graph
every directed path solution graph terminates tip node explicit graph
key difference definition may one optimal action associated
node since different actions may optimal different hybrid states associated
node policy represented solution graph continuous functions n
reachablen particular partial policy specifies action reachable region
continuous state space best partial policy one satisfies following optimality
equation
vn x



vn x

hn x n x nonterminal open state


z
x

max
pr n n x
pr x n x n rn x vn x dx

vn x

n x terminal state

aan x

n n



x

note optimality equation satisfied regions state space reachable
start state n x following optimal policy

table gives high level summary hao outline ao
consists iteration three steps solution policy expansion use
dynamic programming update current value function policy analysis reachability
identify frontier solution eligible expansion detail modified several
important ways allow search hybrid state space following discuss modifications
three steps
policy expansion nodes current solution graph identified one open
regions associated nodes selected expansion one regions
hybrid state space intersection open reachable chosen expansion actions
applicable states open regions simulated actions added
explicit graph cases means adding node graph
cases simply involves marking one regions continuous state space associated
existing node open specifically action leads node node added
explicit graph states corresponding node reachable expanded
region action consideration marked open action leads


fihao

explicit graph g initially consists start node corresponding start state n x
marked open reachable
reachablen x openn x non empty n x
expand best partial solution expand one region open states frontier
explicit state space reachable following best partial policy add
successor states g cases requires adding node
graph cases simply involves marking one regions continuous
state space associated existing node open states expanded region
marked closed
b update state values mark best actions
create set z contains node associated expanded regions
states ancestor nodes explicit graph along marked action arcs
ii decompose part explicit graph consists nodes z
strongly connected components
iii repeat following steps z empty
remove z set nodes belong connected
component descendant nodes occurs z
b every node n connected component states n x
expanded region node n set
vn x

max
aan x

x

pr n n x

z


pr x n x n rn x vn x dx

x

n n

mark best action determining best action resolve ties arbitrarily give preference currently marked action repeat
longer change value nodes
c identify best solution graph nonterminal states frontier step
updates reachablen x
return optimal policy
table hao
existing node region markov states node reachable expanded
region marked closed marked open expanded regions state space marked
closed thus different regions associated node opened expanded
different times process illustrated figure figure nodes corresponding
distinct value discrete state represented rectangles circular connectors represent
actions node see many distinct continuous regions exist region
see whether closed c open whether reachable initial state r
executing current best policy opt instance figure node start
single region marked closed reachable node lost two regions smallest open
reachable largest closed unreachable
dynamic programming standard ao value newly expanded node n must
updated computing bellman backup value functions children n



fimeuleau benazera brafman hansen mausam

start

loc


start
c

c

r

r

navigate
start loc

loc

opt

c

r

navigate
start loc

opt

r

navigate
loc loc

lost


c

lost


r



c

c

r

loc

panoramic
camera



expansion

panoramic
camera

b expansion

figure expanding region state space expansion nodes start
loc lost previously created unique region loc
next region expanded b expansion action navigate loc loc
applied expanded region added graph action lead
preexisting node lost node loc expanded region
loc well continuous regions reachable lost loc
highlighted dotted framed following expansion expanded region closed
discrete state loc added graph reachable regions
open additionally open regions added node lost

explicit graph expanded region state space associated node n
action evaluated best action selected corresponding continuous value function
associated region continuous state value function computed evaluating
continuous integral equation use method computing integral
implementation use dynamic programming feng et al reviewed
section continuous integral x computed exactly long
transition reward functions satisfy certain conditions note hybrid state
dynamic programming techniques feng et al dynamic programming backups may
increase number pieces value function attached updated regions figure
expanded regions continuous state space associated node n reevaluated values must propagated backward explicit graph backward
propagation stops nodes value function modified root node
standard ao summarized figure assumes graph
searches acyclic extensions ao searching graphs contain
cycles one line concerned acyclic solutions graphs
contain cycles jimenez torras another generalization ao called lao allows
solutions contain cycles loops order specify policies infinite horizon mdps hansen
zilberstein



fihao

start

loc
c c c

start
c

c

r

r

navigate
start loc

loc

opt

c c c

r r r

navigate
loc loc

lost




c

opt

c

r

loc


opt

r r r

navigate
loc loc

opt

navigate
start loc

loc

panoramic
camera


r

dynamic programming

lost




c

r

r

r

c

panoramic
camera

b reachability analysis

figure dynamic programming reachability analysis figure continued dynamic programming optimal policy reevaluated navigate loc loc appears
optimal continuous states loc node loc represented finer
partition continuous state space illustrate fact backup increased
number pieces value function associated expanded region b reachability analysis newly created region loc becomes reachable well
regions lost reached navigate loc loc

given assumption every action positive resource consumption
loops state space resources available decrease step
surprisingly loops graph possible
graph represents projection state space onto smaller space consists
discrete component state example possible rover return
site visited rover actually state since fewer resources
available graph represents projection state space include
continuous aspects state resources means rover visit state
projects node graph state visited earlier shown figure
loops graph even loops part
graph corresponds solution sense phantom loops
appear projected state space real state space
nevertheless must modify dynamic programming dp deal loops
loops real state space know exact value function
updated finite number backups performed correct order one backup performed
state visited along path start state expanded node
multiple states map graph node continuous region
state space associated particular node may need evaluated identify
graph nodes need evaluated use following two step




fimeuleau benazera brafman hansen mausam

start

location
energy

location
energy

location

start
energy
location
energy

location
energy

location

figure phantom loops hao solid boxes represent markov states dashed boxes represent
search nodes projection markov states discrete components arrows
represent possible state transition bold arrows instance phantom loop
search space

first consider part graph consists ancestor nodes
expanded node set z nodes identified beginning dp step
decompose part graph strongly connected components graph strongly
connected components acyclic used prescribe order backups almost
way standard ao particular nodes particular component
backed nodes descendant components backed note
case acyclic graph every strongly connected component single node possible
connected component one node loops graph
loops graph primary change dp step
occurs time perform backups nodes connected component one
node case nodes connected component evaluated repeatedly
evaluated value functions nodes converge change
values nodes loops real state space convergence
guaranteed occur finite number steps typically occurs small number
steps advantage decomposing graph connected components
identifies loops localizes effect small number nodes experiments test
domain nodes graph need evaluated dp step
small number nodes often none need evaluated
note decomposition nodes z connected components method improving
efficiency dynamic programming step required correctness alternative repeatedly updating nodes z values converge correct although
likely many useless updates already converged nodes
analysis reachability change value function lead change optimal policy
thus change states visited best policy turn affect
open regions state space eligible expanded final step hao identifies
best partial policy recomputes reachablen nodes states explicit graph
follows see figure b node n best partial solution graph consider
parents n solution graph actions lead one parents n
reachablen x support pn x
x z
pn x
reachablen x pr n n x pr x n x n dx

n n

x



fihao

reachablen x x x pn x equation n set pairs n
best action n reachable resource level
n n n x x pn x n x pr n n x
clear restrict attention state action pairs n
performing reachability analysis hao identifies frontier state space
eligible expansion hao terminates frontier empty
hybrid states intersection reachable open
convergence error bounds
next consider theoretical properties hao first reasonable assumptions
prove hao converges optimal policy finite number steps discuss
use hao sub optimal policies error bounds
proof convergence finite number steps depends among things
assumption hybrid state mdp finite branching factor implementation
means region state space represented hyper rectangle set
successor regions action represented finite set hyper rectangles
assumption assumption number actions finite follows every
assignment n discrete variables set
x n x reachable initial state fixed sequence actions
union finite number open closed hyper rectangles assumption viewed
generalization assumption finite branching factor discrete graph upon
finite convergence proof ao depends
theorem heuristic functions hn admissible optimistic actions positive resource consumptions continuous backups action application computable exactly finite
time branching factor finite
step hao vn x upper bound optimal expected return n x
n x expanded hao
hao terminates finite number steps
termination vn x equal optimal expected return n x n x reachable
optimal policy e reachablen x
proof proof induction every state n x assigned initial heuristic estimate
vn x hn x vn x admissibility heuristic evaluation function make
inductive hypothesis point vn x vn x every state n x
backup performed state n x


z
x
vn x
max
pr n n x
pr x n x n rn x vn x dx
aan x

x

n n




max
aan x

x
n n



z

pr n n x







pr x n x n rn x
x

vn x
last equality restates bellman optimality equation



vn x dx



fimeuleau benazera brafman hansen mausam

action positive bounded resource consumption resources
finite non replenishable complete implicit graph must finite
reason graph turned finite graph without loops along directed loop
graph amount maximal available resources must decrease positive
lower bound amount resources consumed action node graph may
expanded number times bounded number ancestor time
ancestor discovered may lead update set reachable regions node
moreover finite branching factor implies number regions considered within node
bounded finite ways reaching node contributes finite
number hyper rectangles thus overall number regions considered finite
processing required region expansion finite action application backups
computed finite time leads desired conclusion
search terminates policy start state n x complete
lead unexpanded states every state n x reachable
following policy contradictory suppose vn x vn x since implies complete
policy better optimal bellman optimality equation equation know
vn x vn x every state complete policy therefore vn x vn x
hao converges optimal solution stopping early allows flexible
trade solution quality computation time assume state
done action terminates execution zero reward rover would
start safe sequence evaluate current policy step
assuming execution ends time reach leaf policy graph assumption
error current policy step bounded
decomposition value function described chakrabarti et al hansen
zilberstein note point value function decomposed
two parts gn x hn x
gn x



gn x



n x open state fringe greedy policy otherwise
z
x


pr n n x
pr x n x n rn x gn x dx



x

n n


hn x

hn x n x open state fringe greedy policy otherwise
z
x
hn x
pr n n x
pr x n x n hn x dx

n n

x

action maximizes right hand side equation note vn x
gn x hn x use decomposition value function bound error best policy
found far follows
theorem step hao error current best policy bounded
hn x
proof state n x explicit search space lower bound optimal value given
gn x value achieved current policy done action
executed fringe states upper bound given vn x gn x hn x established
theorem follows hn x bounds difference optimal value
current admissible value state n x including initial state n x
note error bound initial state hn x hn x start
decreases progress hn x hao converges optimal
solution


fihao

heuristic function
heuristic function hn focuses search reachable states likely useful
informative heuristic scalable search implementation
hao rover described detail next section used
simple admissible heuristic function assigns node sum rewards associated
goals achieved far note heuristic function depends
discrete component state continuous variables function hn x
constant values x obvious heuristic admissible since represents
maximum additional reward could achieved continuing plan execution although
obvious heuristic simple could useful experimental present
section considered additional informed heuristic function solved
relaxed suitably discretized version however taking account
time required compute heuristic estimate simpler heuristic performed better
expansion policy
hao works correctly converges optimal solution matter continuous region
node expanded iteration step quality solution may
improve quickly heuristics choose region fringe expand
next
one simple strategy select node expand continuous regions node
open reachable preliminary implementation expanded open regions
node likely reached current policy changes value
states greatest effect value earlier nodes implementing strategy requires
performing additional work involved maintaining probability associated state
probabilities available one could focus expanding promising node
node integral hn x times probability values x highest
described mausam benazera brafman meuleau hansen
hansen zilberstein observed case lao efficient
expand several nodes fringe performing dynamic programming explicit
graph cost performing update node largely dominates cost
expanding node expand one node fringe iteration might
perform dp backups expand several nodes common ancestors proceeding
dp limit might want expand nodes fringe iteration
indeed variant lao proved efficient hansen zilberstein
case lao updates expensive loops implicit graph hao
update region induces call hybrid dynamic programming module open
region node therefore technique likely produce benefit
pursuing idea allowed expand nodes fringe
descendants fixed depth iteration defined parameter called expansion
horizon denoted k represent loosely speaking number times whole fringe
expanded iteration k hao expands open reachable regions
nodes fringe recomputing optimal policy k expands regions
fringe children updating policy k consider grandchildren regions fringe k tends infinity essentially
performs exhaustive search first expands graph reachable nodes performs one
pass hybrid dynamic programming graph determine optimal policy balancing
node expansion update expansion horizon allows tuning behavior
exhaustive search traditional heuristic search experiments showed value k
optimal solve hardest benchmark see section



fimeuleau benazera brafman hansen mausam

start

obspt

unsafe

c
obs
pt

featureless
c

w
w

w

obs
pt

obspt

audience

demo

label

waypoint
name

rock
ip champ

obspt
far

science cam

figure k rover top left developed jet propulsion laboratory nasa ames
center prototype mer rovers used test advanced rover
software including automated planners rovers activities right topological map
demo arrows labeled ip champ represent opportunity
deploy arm rock instrument placement take picture
champ camera arrows labeled science cam represent opportunity take
remote picture rock science camera

updating multiple regions
expansion policies described expanding open regions one several
nodes simultaneously allow leveraging hybrid state dynamic programming techniques
feng et al li littman techniques may compute single
iteration piecewise constant linear value functions cover large range continuous states
possibly whole space possible values particular back one iteration
continuous states included given bounds
therefore several open regions node expanded iteration
hao update simultaneously backing subset continuous states
includes regions instance one may record lower bounds upper bounds
continuous variable expanded regions compute value function covers
hyper rectangle bounds
modification impact convergence long value
expanded regions computed convergence proof holds however execution time may adversely affected expanded regions proper subset region continuous states



fihao

value function vn initial node
first plateau corresponds analyzing r second plateau analyzing r third plateau
analyzing r r

b policy n starting
node shows partitions resource space different actions
optimal dark action grey
navigation r light analysis
r

figure optimal value function initial state simple rover possible
values continuous resources time energy remaining value function
partitioned pieces b optimal policy set states

backed case values states open reachable uselessly computed
deviates pure heuristic search
however modification may beneficial avoids redundant computation
hybrid state dynamic programming techniques manipulate pieces value functions thus several
expanded regions included piece value function value computed
practice benefit may outweigh cost evaluating useless regions moreover cost
reduced storing value functions associated node graph
computed values irrelevant regions saved case regions become eligible expansion
e open reachable later thus variant hao fully exploits hybrid state dynamic
programming techniques

experimental evaluation
section describe performance hao solving simulated
planetary exploration rover two monotonic continuous valued resources time battery
power section uses simple toy example illustrate basic steps
hao section tests performance realistic real size
nasa simulation rover analyzes experiments simulation uses
model k rover see figure developed intelligent systems demo nasa
ames center october pedersen et al complex real size model
k rover uses command names understandable rovers execution language
plans produced directly executed rover experiments
reported section simplify nasa simulation model way



fimeuleau benazera brafman hansen mausam

figure first iteration hao toy explicit graph marked dim edges
solution graph marked thick edges tip nodes shown
constant heuristic functions expanded nodes shown backed
value functions

consider autonomous rover must navigate planar graph
representing surroundings authorized navigation paths schedule observations
performed different rocks situated different locations subset observational
goals achieved single run due limited resources therefore oversubscribed
uncertainty since action uncertain
positive resource consumptions probability failing
significant amount uncertainty domain comes tracking mechanism used
rover tracking process rover recognizes rock certain features
camera image associated rock mission operations instance
containing fixed set locations paths rocks built last panoramic camera image
sent rover logical rock instance corresponds real rock
rover must associate two basis features detected instruments
including camera rover moves camera image changes rover must keep track
features image evolve process uncertain subject faults
losing track rock practice tracking modeled following way
order perform measurement rock rover must tracking rock
navigate along path must tracking one rocks enables following path
set rocks enable path part definition given planner
decision start tracking rock must made rover begins move
rover starts moving may keep track rock already tracked voluntarily stop
tracking cannot acquire rock tracked initially



fihao

figure second iteration hao toy
rover may randomly lose track rocks navigating along path probability losing track rock depends rock path followed part
definition given planner
way reacquire rock whose track lost intentionally accident
number rocks tracked strongly influences duration resource consumption
navigate actions higher number rocks tracked costly navigate
along path rover stop regularly check record aspect
rock tracked creates incentive limit number rocks tracked
rover given set goals chosen path intends follow
rover initially selects set rocks track tries keep set small possible
given goals starts moving may lose track rocks may cause
reconsider set goals pursue route get corresponding rocks
purposely stop tracking rock longer necessary given goals left
achieve
implementation hao uses dynamic programming developed feng et
al summarized section order perform backups hybrid state space
partitions continuous state space associated node piecewise constant regions uses
multiple region updates described section upper bound resource
expanded regions computed states included bounds minimal
possible resource levels updated
experiments use variant hao described section
parameter k sets number times whole fringe expanded iteration hao
allows behavior tuned exhaustive search heuristic search
used expansion horizon k simple example section default expansion
horizon k larger examples section section describes experiments
different expansion horizons



fimeuleau benazera brafman hansen mausam

figure third iteration hao toy
implementation hao uses simple heuristic described section augmented
small amount domain knowledge value hn x state n x essentially equal
sum utilities goals yet achieved n however rover already moved
certain rock tracked state n goals requiring rock tracked
included sum reflects fact rover moved cannot start tracking
rock thus goals require rock tracked unreachable resulting
heuristic admissible e never underestimates value state straightforward
compute note depend current resource levels functions hn x
constant values x
example
begin simple example rover order illustrate steps
solve example implementation hao use
solve realistic examples considered section
example targets two rocks r r positioned locations l l
respectively rovers initial location l direct path l l
analyzing rock r yields reward analyzing rock r yields reward rovers
action set simplified notably features single action pic rx represents steps
analyzing rock rx stop tracking actions removed
figure shows optimal value function optimal policy found hao starting
discrete state resources ranging whole space possible values figures
step step process hao solves expansion horizon
k hao solves three iterations follows
iteration shown figure hao expands nodes computes heuristic
function tip nodes backup step yields value function estimates
nodes hao identifies best solution graph fringe node



fihao

pieces

b pieces

c pieces

figure optimal value functions initial state simple rover increasing initial resource levels left right optimal return appears three
dimensional function carved reachable space heuristic function

name
rover
rover
rover
rover

rover
locations





paths

goals

fluents

actions





















discrete
states
approx





reachable
discrete
states





explicit
graph

optimal
policy

longest
branch
















table size benchmark rover
iteration shown figure hao expands nodes starting
previous fringe node computes heuristic functions tip nodes
heuristic value node zero state rover lost track r
already analyzed r backup step improves accuracy value function
several nodes node fringe node since terminal node
iteration shown figure hao expands node node search ends
iteration open node optimal solution graph
comparison figure shows value function found hao varies different initial
resource levels figures unreachable states assigned large constant heuristic value
value function reachable states appears carved plateau heuristic
performance
describe hao performance solving four much larger rover
nasa simulation model characteristics displayed tables columns
two six size terms rover locations paths goals
total number fluents boolean state variables actions columns seven
ten report size discrete state space total number discrete states two raised
power number fluents although huge state space limited number
states reached start state depending initial resource levels eighth
column table shows number reachable discrete states initial time energy levels
set maximum value maximum initial resource levels scenario
demo represent several hours rover activity shows simple reachability


fimeuleau benazera brafman hansen mausam










reachable
created
expanded
optimal policy


number discrete states


number discrete states



reachable
created
expanded
optimal policy













initial energy













initial time

















rover


reachable
created
expanded
optimal policy



number discrete states

number discrete states















initial energy











reachable
created
expanded
optimal policy









initial time

b rover


reachable
created
expanded
optimal policy



number discrete states

number discrete states













initial energy











reachable
created
expanded
optimal policy







initial time

c rover


reachable
created
expanded
optimal policy



number discrete states

number discrete states













initial energy











reachable
created
expanded
optimal policy







initial time

rover
figure number nodes created expanded hao vs number reachable discrete states
graphs left column obtained fixing initial time maximum value
varying initial energy graphs right column obtained fixing
initial energy maximum value varying initial time obtained
k


fihao

analysis resource availability makes huge difference partly due fact
domain close k execution language allow many fluents
true simultaneously columns nine ten number discrete states explicit
graph optimal policy precisely former number nodes created hao
subset reachable discrete states number reachable discrete states thus
size graph explore may seem small compared discrete combinatorial
solved ai techniques iteration continuous approximation two dimensional
backup necessary evaluate hybrid state space associated graph finally last
column table shows length longest branch optimal policy initial
resource levels set maximum value
largest four instances rover exactly october
demo considered large rover example much larger
faced mer rovers never visit one rock single cycle
efficiency pruning
first set simulations try evaluate efficiency heuristic pruning hao
portion discrete search space spared exploration use admissible
heuristics purpose compare number discrete states reachable given
resource level number nodes created expanded hao consider
number nodes optimal policy found
four benchmark presented figure curves obtained
fixing one resource maximum possible value varying maximum
therefore represent mostly one resource constraining
notably single resource enough constrain reachability state space significantly
surprisingly become larger initial resources increase discrete
states become reachable despite simplicity heuristic used hao able pass
significant part search space moreover bigger leverage
take simple heuristic
quite encouraging number nodes created expanded
reflect search time therefore examine time takes hao produce solutions
search time
figure shows hao search time set experiments curves exhibit
monotonicity instead appear significant amount noise surprising
search time increase increase initial levels resource although
search space bigger shows search complexity depend size search
space alone factors must explain complexity peaks observed figure
number nodes created expanded contain noise
reason peaks computation time must time spent dynamic programming
backups moreover search time appears closely related complexity optimal policy
figure shows number nodes branches policy found well
number goals pursued policy shows cases increasing initial
resource level eliminates need branching reduces size optimal solution ii
size optimal policy secondarily number branches explains peaks
search time curves therefore question large solution graph induce long time
spent backups two possible answers question backups take longer
backups performed first explanation pretty intuitive
policy graph contains many branches leading different combinations goals value functions
contain many humps plateaus therefore many pieces impacts complexity
dynamic programming backups however time empirical evidence


















search time

search time

meuleau benazera brafman hansen mausam

























initial energy













initial time







initial time



















search time

search time

rover


























initial energy
















search time

search time

b rover
















initial energy













initial time











initial time












search time

search time

c rover
















initial energy







rover
figure hao search time graphs left column obtained fixing initial time
maximum value graphs right column obtained fixing
initial energy maximum obtained k



fihao

confirm hypothesis conversely observe peak figure comes increase
number backups work required explain
expansion horizon
section hao leverage even simple admissible heuristic prune
large portion search space necessarily follow hao outperform
exhaustive search creates graph reachable states executes one pass
dynamic programming graph optimal policy although hao expands smaller
graph exhaustive search must evaluate graph often section
introduced parameter k expansion horizon order allow adjustment trade
time spent expanding nodes time spent evaluating nodes study influence
parameter
figure shows number nodes created expanded hao function
expansion horizon four benchmark instances surprisingly creates
expands nodes expansion horizon increases essentially behaves
exhaustive search k increased two smallest instances large enough
values k number visited states levels total number reachable states
reached two largest instances interrupt experiments k reached
search time became long
figure shows effect expansion horizon search time hao smallest
instance rover hao clear advantage exhaustive search
k even though explores fewer nodes three larger instances hao
clear advantage rover instance search time hao levels
k indicating limit reachable states reached however duration
exhaustive search several times longer hao smaller settings k benefits
hao clearer two largest instances k increased
quickly overwhelmed combinatorial explosion size search space simulations
eventually need interrupted search time becomes long
instances smaller settings k hao able efficiently optimal solutions
overall clear benefit admissible heuristics prune
search space although expansion horizon must adjusted appropriately order hao
achieve favorable trade node expansion time node evaluation time

conclusion
introduced heuristic search finding optimal conditional plans domains characterized continuous state variables represent limited consumable resources hao
variant ao best knowledge first deal following limited continuous resources uncertain action outcomes
subscription tested hao realistic nasa simulation planetary rover
complex domain practical importance demonstrate effectiveness solving
large solved straightforward application dynamic programming effective heuristic search exploit resource constraints well admissible
heuristic order limit reachable state space
implementation hao integrated dynamic programming feng et al however hao integrated dynamic programming
solving hybrid state mdps feng et al finds optimal policies
limiting assumptions transition probabilities discrete rewards piecewiseconstant piecewise linear recently developed dynamic programming hybridstate mdps make less restrictive assumptions potential improve computational



fimeuleau benazera brafman hansen mausam




















initial energy










nodes
branches
goals























initial time



number branches goals





number nodes


number nodes



nodes
branches
goals

number branches goals






rover




















initial energy










nodes
branches
goals























initial time



number branches goals





number nodes

nodes
branches
goals


number nodes


number branches goals






b rover




















initial energy










nodes
branches
goals























initial time



number branches goals





number nodes

nodes
branches
goals


number nodes


number branches goals






c rover




















initial energy










nodes
branches
goals























initial time



number branches goals





number nodes

nodes
branches
goals


number nodes


number branches goals






rover
figure complexity optimal policy number nodes branches goals optimal
policy setting figure



fihao



number discrete states


number discrete states



created
expanded















expansion horizon









created
expanded









rover


number discrete states

number discrete states



created
expanded












b rover


created
expanded





expansion horizon














expansion horizon





c rover







expansion horizon



rover

figure influence expansion horizon number nodes visited
efficiency li littman marecki et al integrating hao one
could improve performance
several interesting directions work could extended developing hao made assumptions every action consumes resource resources
non replenishable without assumptions state could revisited optimal
plan could loops well branches generalizing allow plans loops
seems necessary handle replenishable resources requires generalizing heuristic search
lao solve hybrid mdps hansen zilberstein another possible extension
allow continuous action variables addition continuous state variables finally heuristic
search could combined approaches improving scalability hierarchical decomposition meuleau brafman would allow handle even larger state
spaces number goals subscription increased
acknowledgments
work funded nasa intelligent systems program grant nra eric hansen
supported part nasa summer faculty fellowship funding mississippi
space grant consortium work performed emmanuel benazera working
nasa ames center ronen brafman visiting nasa ames center
consultants institute advanced computer science ronen brafman
supported part lynn william frankel center computer science paul ivanier
center robotics production management isf grant nicolas meuleau
consultant carnegie mellon university nasa ames center



fimeuleau benazera brafman hansen mausam









search time

search time
























expansion horizon











rover





search time


search time



b rover










expansion horizon












expansion horizon





c rover







expansion horizon



rover

figure influence expansion horizon overall search time

references
altman e constrained markov decision processes chapman hall crc
bertsekas tsitsiklis j neural dynamic programming athena scientific belmont

boutilier c dean hanks decision theoretic structural assumptions
computational leverage journal artificial intelligence
boyan j littman exact solutions time dependent mdps advances neural
information processing systems pp mit press cambridge
bresina j dearden r meuleau n ramakrishnan smith washington r
continuous time resource uncertainty challenge ai proceedings
eighteenth conference uncertainty artificial intelligence pp
bresina j jonsson morris p rajan k activity mars exploration
rovers proceedings fifteenth international conference automated
scheduling pp
chakrabarti p ghose desarkar admissibility ao heuristics overestimate aritificial intelligence
feng z dearden r meuleau n washington r dynamic programming structured continuous markov decision proceedings twentieth conference
uncertainty artificial intelligence pp


fihao

friedman j bentley j finkel r finding best matches logarithmic
expected time acm trans mathematical software
hansen e zilberstein lao heuristic search finds solutions
loops artificial intelligence
jimenez p torras c efficient searching implicit graphs
cycles artificial intelligence
kveton b hauskrecht guestrin c solving factored mdps hybrid state
action variables journal artificial intelligence
li l littman lazy approximation solving continuous finite horizon mdps
proceedings twentieth national conference artificial intelligence pp
marecki j koenig tambe fast analytical solving markov decision
processes real valued resources proceedings th international joint conference
artificial intelligence ijcai pp
mausam benazera e brafman r meuleau n hansen e continuous resources stochastic domains proceedings nineteenth international joint
conference artificial intelligence pp professional book center denver co
meuleau n brafman r hierarchical heuristic forward search stochastic domains
proceedings th international joint conference artificial intelligence ijcai
pp
munos r moore variable resolution discretization optimal control machine
learning
nilsson n principles artificial intelligence tioga publishing company palo alto ca
pearl j heuristics intelligent search strategies computer solving addisonwesley
pedersen l smith deans sargent r kunz c lees rajagopalan
mission target tracking autonomous instrument placement proceedings
ieee aerospace conference big sky montana
puterman markov decision processes discrete stochastic dynamic programming
wiley york ny
rust j randomization break curse dimensionality econimetrica

smith choosing objectives subscription proceedings fourteenth
international conference automated scheduling pp
thiebaux gretton c slaney j price kabanza f decision theoretic
non markovian rewards journal artificial intelligence
van den briel sanchez r kambhampati effective approaches partial
satisfation subscription proceedings nineteenth national conference
artificial intelligence pp




