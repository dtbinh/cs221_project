Journal Artificial Intelligence Research 34 (2009) 2759

Submitted 12/07; published 01/09

Heuristic Search Approach Planning
Continuous Resources Stochastic Domains
Nicolas Meuleau

nicolas.f.meuleau@nasa.gov

NASA Ames Research Center
Mail Stop 269-3
Moffet Field, CA 94035-1000, USA

Emmanuel Benazera

ebenazer@laas.fr

LAAS-CNRS, Universite de Toulouse
7, av. du Colonel Roche
31077 Toulouse Cedex 4, France

Ronen I. Brafman

brafman@cs.bgu.ac.il

Department Computer Science
Ben-Gurion University
Beer-Sheva 84105, Israel

Eric A. Hansen

hansen@cse.msstate.edu

Department Computer Science Engineering
Mississippi State University
Mississippi State, MS 39762, USA

Mausam

mausam@cs.washington.edu

Department Computer Science Engineering
University Washington
Seattle, WA 981952350, USA

Abstract
consider problem optimal planning stochastic domains resource constraints,
resources continuous choice action step depends resource availability. introduce HAO* algorithm, generalization AO* algorithm performs
search hybrid state space modeled using discrete continuous state variables, continuous variables represent monotonic resources. heuristic search
algorithms, HAO* leverages knowledge start state admissible heuristic focus
computational effort parts state space could reached start state
following optimal policy. show approach especially effective resource
constraints limit much state space reachable. Experimental results demonstrate
effectiveness domain motivates research: automated planning planetary
exploration rovers.

1. Introduction
Many NASA planetary exploration missions rely rovers mobile robots carry suite
scientific instruments use characterizing planetary surfaces transmitting information back
Earth. difficulties communicating devices distant planets, direct human
control rovers tele-operation infeasible, rovers must able act autonomously
substantial periods time. example, Mars Exploration Rovers (MER), aka, Spirit
Opportunity, designed communicate ground twice per Martian day.
Autonomous control planetary exploration rovers presents many challenges research
automated planning. Progress made meeting challenges. example,
planning software developed Mars Sojourner MER rovers contributed significantly

c
2009
AI Access Foundation. rights reserved.

fiMeuleau, Benazera, Brafman, Hansen & Mausam

success missions (Bresina, Jonsson, Morris, & Rajan, 2005). many important
challenges must still addressed achieve ambitious goals future missions (Bresina,
Dearden, Meuleau, Ramakrishnan, Smith, & Washington, 2002).
Among challenges problem plan execution uncertain environments. planetary
surfaces Mars, uncertainty terrain, meteorological conditions, state
rover (position, battery charge, solar panels, component wear, etc.) turn, leads
uncertainty outcome rovers actions. Much uncertainty resource
consumption. example, factors slope terrain affect speed movement rate
power consumption, making difficult predict certainty long take rover
travel two points, much power consume so. limits
critical resources time battery power, rover plans currently conservative
based worst-case estimates time resource usage. addition, instructions sent
planetary rovers form sequential plan attaining single goal (e.g., photographing
interesting rock). action unintended outcome causes plan fail, rover
stops waits instructions; makes attempt recover achieve alternative
goal. result under-utilized resources missed science opportunities.
past decade, great deal research generate conditional
plans domains uncertain action outcomes. Much work formalized framework
Markov decision processes (Puterman, 1994; Boutilier, Dean, & Hanks, 1999). However,
Bresina et al. (2002) point out, important aspects rover planning problem adequately
handled traditional planning algorithms, including algorithms Markov decision processes.
particular, traditional planners assume discrete state space small discrete number
action outcomes. automated planning planetary exploration rovers, critical resources
time battery power continuous, uncertainty domain results
effect actions variables. requires conditional planner branch
discrete action outcomes, availability continuous resources, planner
must able reason continuous well discrete state variables.
Closely related challenges uncertain plan execution continuous resources
challenge over-subscription planning. rovers future missions much improved
capabilities. Whereas current MER rovers require average three days visit single rock,
progress areas automatic instrument placement allow rovers visit multiple rocks
perform large number scientific observations single communication cycle (Pedersen,
Smith, Deans, Sargent, Kunz, Lees, & Rajagopalan, 2005). Moreover, communication cycles
lengthen substantially distant missions moons Jupiter Saturn, requiring longer
periods autonomous behavior. result, space scientists future missions expected
specify large number science goals once, often present known oversubscription planning problem. refers problem infeasible achieve goals,
objective achieve best subset goals within resource constraints (Smith, 2004).
case rover, multiple locations rover could reach, many experiments
rover could conduct, combinations infeasible due resource constraints.
planner must select feasible subset maximizes expected science return. action
outcomes (including resource consumption) stochastic, plan maximizes expected science
return conditional plan prescribes different courses action based results
previous actions, including resource availability.
paper, present implemented planning algorithm handles problems
together: uncertain action outcomes, limited continuous resources, over-subscription planning.
formalize rover planning problem hybrid-state Markov decision process, is, Markov
decision process (MDP) discrete continuous state variables, use continuous
variables represent resources. planning algorithm introduce heuristic search algorithm
called HAO*, Hybrid-state AO*. generalization classic AO* heuristic search algorithm (Nilsson, 1980; Pearl, 1984). Whereas AO* searches discrete state spaces, HAO* solves

28

fiHAO*

planning problems hybrid domains discrete continuous state variables. handle
hybrid domains, HAO* builds earlier work dynamic programming algorithms continuous
hybrid-state MDPs, particular, work Feng et al. (2004).
Generalizing AND/OR graph search hybrid state spaces poses complex challenge,
consider special case problem. particular, continuous variables used represent
monotonic resources. search best conditional plan allows branching
values discrete variables, availability resources, violate
resource constraint.
well-known heuristic search efficient dynamic programming
uses reachability analysis guided heuristic focus computation relevant parts state
space. show problems resource constraints, including over-subscription planning
problems, heuristic search especially effective resource constraints significantly limit
reachability. Unlike dynamic programming, systematic forward search algorithm AO* keeps
track trajectory start state reachable state, thus check whether
trajectory feasible violates resource constraint. pruning infeasible trajectories, heuristic
search algorithm dramatically reduce number states must considered find
optimal policy. particularly important domain discrete state space huge
(exponential number goals), yet portion reachable initial state relatively
small, due resource constraints.

2. Problem Formulation Background
start formal definition planning problem tackling. special case
hybrid-state Markov decision process, first define model. discuss
include resource constraints formalize over-subscription planning model. Finally
review class dynamic programming algorithms solving hybrid-state MDPs, since
algorithmic techniques incorporated heuristic search algorithm develop
Section 3.
2.1 Hybrid-State Markov Decision Process
hybrid-state Markov decision process, hybrid-state MDP, factored Markov decision process
discrete continuous state variables. define tuple (N, X, A, P, R),
N discrete state variable, X = {X1 , X2 , ..., Xd } set continuous state variables, set
actions, P stochastic state transition model, R reward function. describe
elements detail below. hybrid-state MDP sometimes referred simply hybrid
MDP. term hybrid refer dynamics model, discrete. Another
term hybrid-state MDP, originates Markov chain literature, general-state
MDP.
Although hybrid-state MDP multiple discrete variables, plays role algorithms described paper, so, notational convenience, model discrete component
state space single variable N . focus continuous component. assume
N
domain continuous variable Xi X closed interval real line, X = Xi
hypercube continuous variables defined. state set hybrid-state
MDP set possible assignments values state variables. particular, hybrid
state pair (n, x) n N value discrete variable, x = (xi ) vector
values continuous variables.
State transitions occur result actions, process evolves according Markovian
state transition probabilities Pr(s0 | s, a), = (n, x) denotes state action
s0 = (n0 , x0 ) denotes state action a, called arrival state. probabilities
decomposed into:

29

fiMeuleau, Benazera, Brafman, Hansen & Mausam

discrete marginals Pr(n0 |n, x, a). (n, x, a),

Pr(n0 |n, x, a) = 1;
R
continuous conditionals Pr(x0 |n, x, a, n0 ). (n, x, a, n0 ), x0 X Pr(x0 |n, x, a, n0 )dx0 =
1.
P

n0 N

assume reward associated transition function arrival state only, let
Rn (x) denote reward associated transition state (n, x). complex dependencies
possible, sufficient goal-based domain models consider paper.
2.2 Resource Constraints Over-Subscription Planning
model rover planning problem, consider special type MDP objective
optimize expected cumulative reward subject resource constraints. make following
assumptions:
initial allocation one non-replenishable resources,
action minimum positive consumption least one resource,
resources exhausted, action taken.
One way model MDP resource constraints formulate constrained MDP,
model widely studied operations research community (Altman, 1999).
model, action incurs transition-dependent resource cost, Cai (s, s0 ), resource
i. Given initial allocation resources initial state, linear programming used find
best feasible policy, may randomized policy. Although constrained MDP models
resource consumption, include resources state space. result, policy cannot
conditioned upon resource availability. problem resource consumption either
deterministic unobservable. good fit rover domain, resource
consumption stochastic observable, rover take different actions depending
current resource availability.
adopt different approach modeling resource constraints resources included
state description. Although increases size state space, allows decisions
made based resource availability, allows stochastic model resource consumption. Since
resources rover domain continuous, use continuous variables hybrid-state MDP
represent resources. Note duration actions one biggest sources uncertainty
rover problems, model time one continuous resources. Resource constraints
represented form executability constraints actions, (x) denotes set
actions executable state (n, x). action cannot executed state satisfy
minimum resource requirements.
discussed incorporate resource consumption resource constraints hybridstate MDP, next discuss formalize over-subscription planning. rover planning
problem, scientists provide planner set goals would rover achieve,
goal corresponds scientific task taking picture rock performing
analysis soil sample. scientists specify utility reward goal. Usually
subset goals feasible resource constraints, problem find feasible
plan maximizes expected utility. Over-subscription planning planetary exploration rovers
considered Smith (2004) van den Briel et al. (2004) deterministic domains.
consider over-subscription planning stochastic domains, especially domains stochastic
resource consumption. requires construction conditional plans selection goals
achieve change depending resource availability.
over-subscription planning, utility associated goal achieved once;
additional utility achieved repeating task. Therefore, discrete state must include set
Boolean variables keep track set goals achieved far rover, one Boolean
30

fiHAO*

variable goal. Keeping track already-achieved goals ensures Markovian reward structure,
since achievement goal rewarded achieved past. However,
significantly increases size discrete state space. Maintaining history information ensure
Markovian reward structure simple example planning non-Markovian rewards (Thiebaux,
Gretton, Slaney, Price, & Kabanza, 2006).
2.3 Optimality Equation
rover planning problem consider special case finite-horizon hybrid-state MDP
termination occurs indefinite number steps. Bellman optimality equation
problem takes following form:
Vn (x)

=

Vn (x)

=

0 (n, x) terminal state; otherwise,
"

Z
X
max
Pr(n0 | n, x, a)
Pr(x0 | n, x, a, n0 ) (Rn0 (x0 ) + Vn0 (x0 )) dx0 .

aAn (x)

n0 N

(1)

x0

define terminal state state actions eligible execute, is, (x) = .
use terminal states model various conditions plan termination. includes situation
goals achieved; situation resources exhausted;
situation action results error condition requires executing safe sequence
rover terminating plan execution. addition terminal states, assume explicit
initial state denoted (n0 , x0 ).
Assuming resources limited non-replenishable, every action consumes
resource (and amount consumed greater equal positive quantity c), plan
execution terminate finite number steps. maximum number steps bounded
initial resource allocation divided c, minimal resource consumption per step. actual
number steps usually much less indefinite, resource consumption stochastic
choice action influences resource consumption. number steps takes
plan terminate bounded indefinite, call bounded-horizon MDP contrast
finite-horizon MDP. However, note bounded-horizon MDP converted
finite-horizon MDP specifying horizon equal maximum number plan steps,
introducing no-op action taken terminal state.
Note usually difference number plan steps time plan takes
execute. Since model time one continuous resources, time takes execute
plan step state action dependent, stochastic.
Given hybrid-state MDP set terminal states initial state (n0 , x0 ), objective
find policy, : (N X) A, maximizes expected cumulative reward; specifically,
optimal policy value function satisfies optimality equation given Equation (1).
rover domain, cumulative reward equal sum rewards goals achieved
reaching terminal state direct incentive save resources; optimal solution saves
resources allows achieving goals. However, framework general enough
allow reasoning cost availability resources. example, incentive
conserving resources could modeled specifying reward proportional amount
resources left unused upon entering terminal state. Note framework allows reasoning
cost availability resources without needing formulate problem
multi-objective optimization, stay standard decision-theoretic framework.
2.4 Dynamic Programming Continuous-State Hybrid-State MDPs
planning problem consider finite-horizon hybrid-state MDP, solved
algorithm solving finite-horizon hybrid-state MDPs. algorithms solving hybridstate (and continuous-state) MDPs rely form approximation. widely-used approach
31

fiMeuleau, Benazera, Brafman, Hansen & Mausam

Figure 1: Value function initial state simple rover problem: optimal expected return
function two continuous variables (time energy remaining).

discretize continuous state space finite number grid points solve resulting
finite-state MDP using dynamic programming interpolation (Rust, 1997; Munos & Moore,
2002). Another approach parametric function approximation; function associated
dynamic programming problem value function policy function approximated
smooth function k unknown parameters. general, parametric function approximation
faster grid-based approximation, drawback may fail converge, may
converge incorrect solution. Parametric function approximation used algorithms
solving continuous-state MDPs besides dynamic programming. Reinforcement learning algorithms
use artificial neural networks function approximators (Bertsekas & Tsitsiklis, 1996). approach
solving MDPs called approximate linear programming extended allow continuous
well discrete state variables (Kveton, Hauskrecht, & Guestrin, 2006).
review another approach solving hybrid-state (or continuous-state) MDPs assumes
problem special structure exploited dynamic
programming algorithm.
R
structure assumed approach ensures convolution x0 Pr(x0 | n, x, a, n0 )(Rn0 (x0 )+
Vn0 (x0 ))dx0 Equation (1) computed exactly finite time, value function computed
dynamic programming piecewise-constant piecewise-linear. initial idea approach
due work Boyan Littman (2000), describe class MDPs called time-dependent
MDPs, transitions take place along single, irreversible continuous dimension.
describe dynamic programming algorithm computing exact piecewise-linear value function
transition probabilities discrete rewards piecewise linear. Feng et al. (2004)
extend approach continuous state spaces one dimension, consider MDPs
discrete transition probabilities two types reward models: piecewise constant piecewise
linear. Li Littman (2005) extend approach allow transition probabilities
piecewise-constant, instead discrete, although extension requires approximation
dynamic programming algorithm.
problem structure exploited algorithms characteristic Mars rover domain
over-subscription planning problems. Figure 1 shows optimal value functions
initial state typical Mars rover problem function two continuous variables:
time energy remaining (Bresina et al., 2002). value functions feature set humps
plateaus, representing region state space similar goals pursued
optimal policy. sharpness hump plateau reflects uncertainty achieving
goal(s). Constraints impose minimal resource levels attempting actions introduce

32

fiHAO*

sharp cuts regions. Plateau regions expected reward nearly constant represent
regions state space optimal policy same, probability distribution
future histories induced optimal policy nearly constant.
structure value function exploited partitioning continuous state
space finite number hyper-rectangular regions. (A region hyper-rectangle
Cartesian product intervals dimension.) hyper-rectangle, value function
either constant (for piecewise-constant function) linear (for piecewise-linear function).
resolution hyper-rectangular partitioning adjusted fit value function. Large hyperrectangles used represent large plateaus. Small hyper-rectangles used represent regions
state space finer discretization value function useful, edges
plateaus curved hump time energy available. natural choice
data structures rectangular partitioning continuous space kd-trees (Friedman, Bentley,
& Finkel, 1977), although choices possible. Figures 6 10 Section 4.1 show value
functions initial state simple rover planning problem, created piecewise-constant
partitioning continuous state space.
continuous-state domains transition reward functions similarly partitioned
hyper-rectangles. reward function action piecewise-constant (or piecewiselinear) representation value function. transition function partitions state space
regions set outcomes action probability distribution set
outcomes identical. Following Boyan Littman (2000), relative absolute transitions
supported. relative outcome viewed shifting region constant . is,
two states x region, transition probabilitiesP r(x0 |x, a) P r(y 0 |y, a)
defined term probability , = (x0 x) = (y 0 y). absolute outcome
maps states region single state. is, two states x region,
P r(x0 |x, a) = P r(x0 |y, a). view relative outcome pair (, p), p probability
outcome, view absolute outcome pair (x0 , p). assumes
finite number non-zero probabilities, i.e., probability distribution discretized,
means state action, finite set states reached non-zero probability.
representation guarantees dynamic programming update piecewise-constant value
function results another piecewise-constant value function. Feng et al. (2004) show
transition functions finite horizon, exists partition continuous space
hyper-rectangles optimal value function piecewise constant linear.
restriction discrete transition functions strong one, often means transition
function must approximated. example, rover power consumption normally distributed,
thus must discretized. (Since amount power available must non-negative,
implementation truncates negative part normal distribution renormalizes.) continuous transition function approximated appropriately fine discretization, Feng et
al. (2004) argue provides attractive alternative function approximation approaches
approximates model solves approximate model exactly, rather finding
approximate value function original model. (For reason, sometimes refer
finding optimal policies value functions, even model approximated.)
avoid discretizing transition function, Li Littman (2005) describe algorithm allows
piecewise-constant transition functions, exchange approximation dynamic programming algorithm. Marecki et al.(2007) describe different approach class problems
probability distributions resource consumptions represented phase-type distributions dynamic programming algorithm exploits representation. Although use
work Feng et al. (2004) implementation, heuristic search algorithm develop
next section could use approach representing computing value
functions policies hybrid-state MDP.

33

fiMeuleau, Benazera, Brafman, Hansen & Mausam

3. Heuristic Search Hybrid State Space
section, present primary contribution paper: approach solving special
class hybrid-state MDPs using novel generalization heuristic search algorithm AO*.
particular, describe generalization algorithm solving hybrid-state MDPs
continuous variables represent monotonic constrained resources acyclic plan found
search algorithm allows branching availability resources.
motivation using heuristic search potentially huge size state space,
makes dynamic programming infeasible. One reason size existence continuous
variables. even consider discrete component state space, size
state space exponential number discrete variables. well-known, AO*
effective solving planning problems large state space considers states
reachable initial state, uses informative heuristic function focus
states reachable course executing good plan. result, AO* often find
optimal plan exploring small fraction entire state space.
begin section review standard AO* algorithm. consider
generalize AO* search hybrid state space discuss properties generalized
algorithm, well efficient implementations.
3.1 AO*
Recall AO* algorithm AND/OR graph search problems (Nilsson, 1980; Pearl, 1984).
graphs arise problems choices (the components), choice
multiple consequences (the component), case planning uncertainty.
Hansen Zilberstein (2001) show AND/OR graph search techniques used solving
MDPs.
Following Nilsson (1980) Hansen Zilberstein (2001), define AND/OR graph
hypergraph. Instead arcs connect pairs nodes ordinary graph, hypergraph
hyperarcs, k-connectors, connect node set k successor nodes. MDP
represented hypergraph, node corresponds state; root node corresponds start
state, leaf nodes correspond terminal states. Thus often use word state refer
corresponding node hypergraph representing MDP. k-connector corresponds
action transforms state one k possible successor states, probability attached
successor probabilities sum one. paper, assume AND/OR graph
acyclic, consistent assumption underlying MDP bounded-horizon.
AND/OR graph search, solution takes form acyclic subgraph called solution
graph, defined follows:
start node belongs solution graph;
every non-terminal node solution graph, exactly one outgoing k-connector (corresponding action) part solution graph successor nodes belongs
solution graph;
every directed path solution graph terminates terminal node.
solution graph maximizes expected cumulative reward found solving following
system equations,

0 terminal
state; otherwise,
P

V (s) =
(2)
0
0
0
maxaA(s)
P
0
r(s |s, a) (R(s ) + V (s )) ,
V (s) denotes expected value optimal solution state s, V called
optimal evaluation function (or value function MDP terminology). Note identical
34

fiHAO*

optimality equation hybrid-state MDPs defined Equation (1), latter restricted
discrete state space. keeping convention literature MDPs, treat
value-maximization problem even though AO* usually formalized solving cost-minimization
problem.
state-space search problems formalized AND/OR graphs, optimal solution
graph found using heuristic search algorithm AO* (Nilsson, 1980; Pearl, 1984).
heuristic search algorithms, advantage AO* dynamic programming find
optimal solution particular starting state without evaluating problem states. Therefore,
graph usually supplied explicitly search algorithm. implicit graph, G, specified
implicitly start node start state successor function generates successors
states state-action pair. search algorithm constructs explicit graph, G0 , initially
consists start state. tip leaf state explicit graph said terminal
goal state (or state action taken); otherwise, said
nonterminal. nonterminal tip state expanded adding explicit graph outgoing
k-connectors (one action) successor states already explicit graph.
AO* solves state-space search problem gradually building solution graph, beginning
start state. partial solution graph defined similarly solution graph, difference
tip states partial solution graph may nonterminal states implicit AND/OR graph.
partial solution graph defined follows:
start state belongs partial solution graph;
every non-tip state partial solution graph, exactly one outgoing k-connector (corresponding action) part partial solution graph successor states
belongs partial solution graph;
every directed path partial solution graph terminates tip state explicit graph.
value partial solution graph defined similarly value solution graph.
difference tip state partial solution graph nonterminal, value
propagated backwards. Instead, assume admissible heuristic estimate
H(s) maximal-value solution graph state s. heuristic evaluation function H said
admissible H(s) V (s) every state s. recursively calculate admissible heuristic
estimate V (s) optimal value state explicit graph follows:

0 terminal state,
V (s) =
nonterminal tip state,

H(s) isP
0
0
0
maxaA(s)
s0 P r(s |s, a) (R(s ) + V (s )) otherwise.

(3)

best partial solution graph determined time propagating heuristic estimates
tip states explicit graph start state. mark action maximizes
value state, best partial solution graph determined starting root
graph selecting best (i.e., marked) action reachable state.
Table 1 outlines AO* algorithm finding optimal solution graph acyclic AND/OR
graph. interleaves forward expansion best partial solution value update step
updates estimated state values best partial solution. simplest version AO*,
values expanded state ancestor states explicit graph updated.
fact, ancestor states need re-evaluated expanded state
reached taking marked actions (i.e., choosing best action state). Thus,
parenthetical remark step 2(b)i Table 1 indicates parent s0 state added
Z unless estimated value state changed state reached state
s0 choosing best action state s0 . AO* terminates policy expansion step

35

fiMeuleau, Benazera, Brafman, Hansen & Mausam

1. explicit graph G0 initially consists start state s0 .
2. best solution graph nonterminal tip state:
(a) Expand best partial solution: Expand nonterminal tip state best partial
solution graph add new successor states G0 . new state s0 added
G0 expanding s, s0 terminal state V (s0 ) := 0; else V (s0 ) := H(s0 ).
(b) Update state values mark best actions:
i. Create set Z contains expanded state ancestors explicit
graph along marked action arcs. (I.e., include ancestor states
expanded state reached following current best solution.)
ii. Repeat following steps Z empty.
A. Remove Z state descendant G0 occurs Z.
P
B. Set V (s) := maxaA(s) s0 P r(s0 |s, a) (R(s0 ) + V (s0 )) mark best action
s. (When determining best action resolve ties arbitrarily, give preference currently marked action.)
(c) Identify best solution graph nonterminal states fringe
3. Return optimal solution graph.
Table 1: AO* algorithm.
find nonterminal states fringe best solution graph. point, best solution
graph optimal solution.
Following literature AND/OR graph search, far referred solution found
AO* solution graph. following, AO* used solve MDP, sometimes
follow literature MDPs referring solution policy. sometimes refer
policy graph, indicate policy represented form graph.
3.2 Hybrid-State AO*
consider generalize AO* solve bounded-horizon hybrid-state MDP. challenge
face applying AO* problem challenge performing state-space search hybrid
state space.
solution adopt search aggregate state space represented AND/OR
graph node distinct value discrete component state.
words, node AND/OR graph represents region continuous state space
discrete value same. Given partition continuous state space, use AND/OR
graph search techniques solve MDP parts state space reachable
start state best policy.
However, AND/OR graph search techniques must modified important ways allow search
hybrid state space represented way. particular, longer correspondence nodes AND/OR graph individual states. node corresponds
continuous region state space, different actions may optimal different hybrid states associated search node. case rover planning, example,
best action likely depend much energy time remaining, energy time
continuous state variables.
address problem still find optimal solution, attach search node set
functions (of continuous variables) make possible associate different values, heuristics,
actions different hybrid states map search node. before, explicit

36

fiHAO*

search graph consists nodes edges AND/OR graph generated far,
describes states considered far search algorithm. difference
use complex state representation set continuous functions allows
representation reasoning continuous part state space associated search
node.
begin describing complex node data structure, describe HAO*
algorithm.
3.2.1 Data Structures
node n explicit AND/OR graph G0 consists following:
value discrete state variable.
Pointers parents children explicit graph policy graph.
Openn () {0, 1}: Open list. x X, Openn (x) indicates whether (n, x)
frontier explicit graph, i.e., generated yet expanded.
Closedn () {0, 1}: Closed list. x X, Closedn (x) indicates whether (n, x)
interior explicit graph, i.e., already expanded.
Note that, (n, x), Openn (x) Closedn (x) = . (A state cannot open
closed.) parts continuous state space associated node
neither open closed. explicit graph contains trajectory start state
particular hybrid state, hybrid state considered generated, even search
node corresponds generated; states neither open closed.
addition, non-terminal states open closed. Note refer open
closed nodes; instead, refer hybrid states associated nodes open
closed.
Hn (): heuristic function. x X, Hn (x) heuristic estimate optimal
expected cumulative reward state (n, x).
Vn (): value function. open state (n, x), Vn (x) = Hn (x). closed state
(n, x), Vn (x) obtained backing values successor states, Equation (4).
n () A: policy. Note defined closed states only.
Reachablen () {0, 1}: x X, Reachablen (x) indicates whether (n, x) reachable
executing current best policy beginning start state (n0 , x0 ).
assume various continuous functions, represent information hybrid states associated search node, partition state space associated node
discrete number regions, associate distinct value action region. Given
partitioning, HAO* algorithm expands evaluates regions hybrid state space,
instead individual hybrid states. finiteness partition important order ensure
search frontier extended finite number expansions, ensure HAO*
terminate finite number steps. implementation HAO*, described Section 4, use piecewise-constant partitioning continuous state space proposed Feng et
al. (2004). However, method discrete partitioning could used, provided condition
holds; example, Li Littman (2005) describe alternative method partitioning.
Note two forms state-space partitioning used algorithm. First, hybrid state
space partitioned finite number regions, one discrete state,

37

fiMeuleau, Benazera, Brafman, Hansen & Mausam

regions corresponds node AND/OR graph. Second, continuous state space associated particular node partitioned smaller regions based piecewise-constant
representation continuous function, one used Feng et al. (2004).
addition complex representation nodes AND/OR graph, algorithm
requires complex definition best (partial) solution. standard AO*, oneto-one correspondence nodes individual states means solution policy
represented entirely graph, called (partial) solution graph, single action
associated node. HAO* algorithm, continuum states associated
node, different actions may optimal different regions state space associated
particular node. HAO* algorithm, (partial) solution graph sub-graph explicit
graph defined follows:
start node belongs solution graph;
every non-tip node solution graph, one outgoing k-connectors part
solution graph, one action optimal hybrid state associated
node, successor nodes belongs solution graph;
every directed path solution graph terminates tip node explicit graph.
key difference definition may one optimal action associated
node, since different actions may optimal different hybrid states associated
node. policy represented solution graph, continuous functions n (.)
Reachablen (.). particular, (partial) policy specifies action reachable region
continuous state space. best (partial) policy one satisfies following optimality
equation:
Vn (x)

=

Vn (x)

= Hn (x) (n, x) nonterminal open state,
"

Z
X
=
max
Pr(n0 | n, x, a)
Pr(x0 | n, x, a, n0 ) (Rn0 (x0 ) + Vn0 (x0 )) dx0 .

Vn (x)

0 (n, x) terminal state,

aAn (x)

n0 N

(4)

x0

Note optimality equation satisfied regions state space reachable
start state, (n0 , x0 ) following optimal policy.
3.2.2 Algorithm
Table 2 gives high-level summary HAO* algorithm. outline, AO*
algorithm, consists iteration three steps; solution (or policy) expansion, use
dynamic programming update current value function policy, analysis reachability
identify frontier solution eligible expansion. detail, modified several
important ways allow search hybrid state space. following, discuss modifications
three steps.
Policy expansion nodes current solution graph identified one open
regions associated nodes selected expansion. is, one regions
hybrid state space intersection Open Reachable chosen expansion. actions
applicable states open regions simulated, results actions added
explicit graph. cases, means adding new node AND/OR graph.
cases, simply involves marking one regions continuous state space associated
existing node open. specifically, action leads new node, node added
explicit graph, states corresponding node reachable expanded
region(s) action consideration marked open. action leads
38

fiHAO*

1. explicit graph G0 initially consists start node corresponding start state (n0 , x0 ),
marked open reachable.
2. Reachablen (x) Openn (x) non-empty (n, x):
(a) Expand best partial solution: Expand one region(s) open states frontier
explicit state space reachable following best partial policy. Add new
successor states G0 . cases, requires adding new node AND/OR
graph. cases, simply involves marking one regions continuous
state space associated existing node open. States expanded region(s)
marked closed.
(b) Update state values mark best actions:
i. Create set Z contains node(s) associated expanded regions
states ancestor nodes explicit graph along marked action arcs.
ii. Decompose part explicit AND/OR graph consists nodes Z
strongly connected components.
iii. Repeat following steps Z empty.
A. Remove Z set nodes (1) belong connected
component, (2) descendant nodes occurs Z.
B. every node n connected component states (n, x)
expanded region node n, set
Vn (x) :=
"
max
aAn (x)

X

Pr(n0 | n, x, a)

Z


Pr(x0 | n, x, a, n0 ) (Rn0 (x0 ) + Vn0 (x0 )) dx0 ,

x0

n0 N

mark best action. (When determining best action resolve ties arbitrarily, give preference currently marked action.) Repeat
longer change value nodes.
(c) Identify best solution graph nonterminal states frontier. step
updates Reachablen (x).
3. Return optimal policy.
Table 2: HAO* algorithm.
existing node, region(s) Markov states node reachable expanded
region(s) marked closed, marked open. Expanded regions state space marked
closed. Thus, different regions associated node opened expanded
different times. process illustrated Figure 2. figure, nodes corresponding
distinct value discrete state represented rectangles, circular connectors represent
actions. node, see many distinct continuous regions exist. region
see whether closed (C) open (O), whether reachable initial state (R)
executing current best policy (OPT). instance, Figure 2(a), node At(Start)
single region marked closed reachable, node Lost two regions: smallest, open
reachable, largest, closed unreachable.
Dynamic programming standard AO*, value newly-expanded node n must
updated computing Bellman backup based value functions children n

39

fiMeuleau, Benazera, Brafman, Hansen & Mausam

At(Start)

At(Loc1)


At(Start)
C

C

R

R

Navigate
(Start, Loc1)

At(Loc1)

OPT

C

R

Navigate
(Start, Loc1)

OPT

R

Navigate
(Loc1, Loc2)

Lost


C

Lost


R



C

C

R

At(Loc2)

Panoramic
Camera



(a) expansion

Panoramic
Camera

(b) expansion

Figure 2: Expanding region state space. (a) expansion: nodes At(Start),
At(Loc1) Lost previously created. unique region At(Loc1)
next region expanded. (b) expansion: action Navigate(Loc1, Loc2)
applied expanded region added graph. action lead
either preexisting node Lost, new node At(Loc2). expanded region (in
At(Loc1)), well continuous regions reachable (in Lost At(Loc2)),
highlighted dotted framed. Following expansion, expanded region closed.
Discrete state At(Loc2) added graph reachable regions
open. Additionally, new open regions added node Lost.

explicit graph. expanded region state space associated node n,
action evaluated, best action selected, corresponding continuous value function
associated region. continuous-state value function computed evaluating
continuous integral Equation (4). use method computing integral.
implementation, use dynamic programming algorithm Feng et al. (2004). reviewed
Section 2.4, show continuous integral x0 computed exactly, long
transition reward functions satisfy certain conditions. Note that, hybrid-state
dynamic programming techniques Feng et al. (2004), dynamic programming backups may
increase number pieces value function attached updated regions (Figure 3(a)).
expanded regions continuous state space associated node n reevaluated, new values must propagated backward explicit graph. backward
propagation stops nodes value function modified, root node.
standard AO* algorithm, summarized Figure 1, assumes AND/OR graph
searches acyclic. extensions AO* searching AND/OR graphs contain
cycles. One line research concerned find acyclic solutions AND/OR graphs
contain cycles (Jimenez & Torras, 2000). Another generalization AO*, called LAO*, allows
solutions contain cycles loops order specify policies infinite-horizon MDPs (Hansen
& Zilberstein, 2001).

40

fiHAO*

At(Start)

At(Loc1)
C C C

At(Start)
C

C

R

R

Navigate
(Start, Loc1)

At(Loc1)

OPT

C C C

R R R

Navigate
(Loc1, Loc2)

Lost




C

OPT

C

R

At(Loc2)


OPT

R R R

Navigate
(Loc1, Loc2)

OPT

Navigate
(Start, Loc1)

At(Loc2)

Panoramic
Camera


R

(a) Dynamic programming

Lost




C

R

R

R

C

Panoramic
Camera

(b) Reachability analysis

Figure 3: Dynamic programming reachability analysis (Figure 2 continued). (a) Dynamic programming: optimal policy reevaluated Navigate(Loc1, Loc2) appears
optimal continuous states At(Loc2). Node At(Loc1) represented finer
partition continuous state space illustrate fact backup increased
number pieces value function associated expanded region. (b) Reachability analysis: newly created region At(Loc2) becomes reachable, well
regions Lost reached Navigate(Loc1, Loc2).

Given assumption every action positive resource consumption,
loops state space problem resources available decrease step.
surprisingly, loops AND/OR graph. possible AND/OR
graph represents projection state space onto smaller space consists
discrete component state. example, possible rover return
site visited before. rover actually state, since fewer resources
available. AND/OR graph represents projection state space include
continuous aspects state, resources, means rover visit state
projects node AND/OR graph state visited earlier, shown Figure 4.
result, loops AND/OR graph, even loops part AND/OR
graph corresponds solution. sense, phantom loops
appear projected state space, real state space.
Nevertheless must modify dynamic programming (DP) algorithm deal loops.
loops real state space, know exact value function
updated finite number backups performed correct order, one backup performed
state visited along path start state expanded node(s).
multiple states map AND/OR graph node, continuous region
state space associated particular node may need evaluated once. identify
AND/OR graph nodes need evaluated once, use following two-step
algorithm.

41

fiMeuleau, Benazera, Brafman, Hansen & Mausam

At(Start)

At(Location1)
energy = 80

At(Location1)
energy = 50

At(Location1)

At(Start)
energy = 100
At(Location2)
energy = 65

At(Location2)
energy = 35

At(Location2)

Figure 4: Phantom loops HAO*: solid boxes represent Markov states. Dashed boxes represent
search nodes, is, projection Markov states discrete components. Arrows
represent possible state transition. Bold arrows show instance phantom loop
search space.

First, consider part AND/OR graph consists ancestor nodes
expanded node(s). set Z nodes identified beginning DP step.
decompose part graph strongly connected components. graph strongly
connected components acyclic used prescribe order backups almost
way standard AO* algorithm. particular, nodes particular component
backed nodes descendant components backed up. Note
case acyclic graph, every strongly connected component single node. possible
connected component one node loops AND/OR graph.
loops AND/OR graph, primary change DP step algorithm
occurs time perform backups nodes connected component one
node. case, nodes connected component evaluated. Then, repeatedly
re-evaluated value functions nodes converge, is, change
values nodes. loops real state space, convergence
guaranteed occur finite number steps. Typically, occurs small number
steps. advantage decomposing AND/OR graph connected components
identifies loops localizes effect small number nodes. experiments test
domain, nodes graph need evaluated DP step,
small number nodes (and often none) need evaluated once.
Note decomposition nodes Z connected components method improving
efficiency dynamic programming step, required correctness. alternative repeatedly updating nodes Z values converge correct, although
likely result many useless updates already converged nodes.
Analysis reachability Change value function lead change optimal policy,
and, thus, change states visited best policy. This, turn, affect
open regions state space eligible expanded. final step, HAO* identifies
best (partial) policy recomputes Reachablen nodes states explicit graph,
follows (see Figure 3(b)). node n best (partial) solution graph, consider
parents n0 solution graph, actions lead one parents n.
Reachablen (x) support Pn (x),
X Z
Pn (x) =
Reachablen0 (x0 ) Pr(n | n0 , x0 , a) Pr(x | n0 , x0 , a, n)dx0 ,
(5)
(n0 ,a)n

X

42

fiHAO*

is, Reachablen (x) = {x X : Pn (x) > 0}. Equation (5), n set pairs (n0 , a)
best action n0 reachable resource level:
n = {(n0 , a) N : x X, Pn0 (x) > 0, n0 (x) = a, Pr(n | n0 , x, a) > 0} .
clear restrict attention state-action pairs n , only.
performing reachability analysis, HAO* identifies frontier state space
eligible expansion. HAO* terminates frontier empty, is, find
hybrid states intersection Reachable Open.
3.3 Convergence Error Bounds
next consider theoretical properties HAO*. First, reasonable assumptions,
prove HAO* converges optimal policy finite number steps. discuss
use HAO* find sub-optimal policies error bounds.
proof convergence finite number steps depends, among things,
assumption hybrid-state MDP finite branching factor. implementation,
means region state space represented hyper-rectangle, set
successor regions action represented finite set hyper-rectangles.
assumption assumption number actions finite, follows every
assignment n discrete variables, set
{x|(n, x)is reachable initial state using fixed sequence actions}
union finite number open closed hyper-rectangles. assumption viewed
generalization assumption finite branching factor discrete AND/OR graph upon
finite convergence proof AO* depends.
Theorem 1 heuristic functions Hn admissible (optimistic), actions positive resource consumptions, continuous backups action application computable exactly finite
time, branching factor finite, then:
1. step HAO*, Vn (x) upper-bound optimal expected return (n, x),
(n, x) expanded HAO*;
2. HAO* terminates finite number steps;
3. termination, Vn (x) equal optimal expected return (n, x), (n, x) reachable
optimal policy, i.e., Reachablen (x) > 0.
Proof: (1) proof induction. Every state (n, x) assigned initial heuristic estimate,
Vn (x) = Hn (x) Vn (x) admissibility heuristic evaluation function. make
inductive hypothesis point algorithm, Vn (x) Vn (x) every state (n, x).
backup performed state (n, x),
"

Z
X
Vn (x) =
max
Pr(n0 | n, x, a)
Pr(x0 | n, x, a, n0 ) (Rn0 (x0 ) + Vn0 (x0 )) dx0
aAn (x)

x0

n0 N

"


max
aAn (x)

X
n0 N

0

Z

Pr(n | n, x, a)

0

0

0

Pr(x | n, x, a, n ) (Rn0 (x ) +
x0

= Vn (x) ,
last equality restates Bellman optimality equation.

43

Vn0 (x0 )) dx0



fiMeuleau, Benazera, Brafman, Hansen & Mausam

(2) action positive, bounded below, resource consumption, resources
finite non-replenishable, complete implicit AND/OR graph must finite.
reason, graph turned finite graph without loops: Along directed loop
graph, amount maximal available resources must decrease positive
lower-bound amount resources consumed action. node graph may
expanded number times bounded number ancestor. (Each time new
ancestor discovered, may lead update set reachable regions node.)
Moreover, finite branching factor implies number regions considered within node
bounded (because finite ways reaching node, contributes finite
number hyper-rectangles). Thus, overall, number regions considered finite,
processing required region expansion finite (because action application backups
computed finite time). leads desired conclusion.
(3) search algorithm terminates policy start state (n0 , x0 ) complete,
is, lead unexpanded states. every state (n, x) reachable
following policy, contradictory suppose Vn (x) > Vn (x) since implies complete
policy better optimal. Bellman optimality equation Equation (1), know
Vn (x) Vn (x) every state complete policy. Therefore, Vn (x) = Vn (x).
HAO* converges optimal solution, stopping algorithm early allows flexible
trade-off solution quality computation time. assume that, state,
done action terminates execution zero reward (in rover problem, would
start safe sequence), evaluate current policy step algorithm
assuming execution ends time reach leaf policy graph. assumption,
error current policy step algorithm bounded. show
using decomposition value function described Chakrabarti et al.(1988) Hansen
Zilberstein (2001). note point algorithm, value function decomposed
two parts, gn (x) hn (x),
gn (x)

=

gn (x)

=

0 (n, x) open state, fringe greedy policy; otherwise,
Z
X
0

Pr(n | n, x, )
Pr(x0 | n, x, , n0 ) (Rn (x) + gn0 (x0 )) dx0 ,

(6)

x0

n0 N


hn (x)

= Hn (x) (n, x) open state, fringe greedy policy; otherwise,
Z
X
hn (x) =
Pr(n0 | n, x, )
Pr(x0 | n, x, , n0 ) hn0 (x0 )dx0 ,
(7)
n0 N

x0

action maximizes right-hand side Equation (4). Note Vn (x) =
gn (x) + hn (x). use decomposition value function bound error best policy
found far, follows.
Theorem 2 step HAO* algorithm, error current best policy bounded
hn0 (x0 ).
Proof: state (n, x) explicit search space, lower bound optimal value given
gn (x), value achieved current policy done action
executed fringe states, upper bound given Vn (x) = gn (x) + hn (x), established
Theorem 1. follows hn0 (x0 ) bounds difference optimal value
current admissible value state (n, x), including initial state (n0 , x) ).
Note error bound initial state hn0 (x0 ) = Hn0 (x0 ) start algorithm;
decreases progress algorithm; hn0 (x0 ) = 0 HAO* converges optimal
solution.
44

fiHAO*

3.4 Heuristic Function
heuristic function Hn focuses search reachable states likely useful.
informative heuristic, scalable search algorithm. implementation
HAO* rover planning problem, described detail next section, used
simple admissible heuristic function assigns node sum rewards associated
goals achieved far. Note heuristic function depends
discrete component state, continuous variables; is, function Hn (x)
constant values x. obvious heuristic admissible, since represents
maximum additional reward could achieved continuing plan execution. Although
obvious heuristic simple could useful, experimental results present
Section 4 show is. considered additional, informed heuristic function solved
relaxed, suitably discretized, version planning problem. However, taking account
time required compute heuristic estimate, simpler heuristic performed better.
3.5 Expansion Policy
HAO* works correctly converges optimal solution matter continuous region(s)
node(s) expanded iteration (step 2.a). quality solution may
improve quickly using heuristics choose region(s) fringe expand
next.
One simple strategy select node expand continuous regions node
open reachable. preliminary implementation, expanded (the open regions of)
node likely reached using current policy. Changes value
states greatest effect value earlier nodes. Implementing strategy requires
performing additional work involved maintaining probability associated state.
probabilities available, one could focus expanding promising node,
is, node integral Hn (x) times probability values x highest,
described Mausam, Benazera, Brafman, Meuleau, Hansen (2005).
Hansen Zilberstein (2001) observed that, case LAO*, algorithm efficient
expand several nodes fringe performing dynamic programming explicit
graph. cost performing update node largely dominates cost
expanding node. expand one node fringe iteration, might
perform DP backups expand several nodes common ancestors proceeding
DP. limit, might want expand nodes fringe algorithm iteration.
Indeed, variant LAO* proved efficient (Hansen & Zilberstein, 2001).
case LAO*, updates expensive loops implicit graph. HAO*,
update region induces call hybrid dynamic programming module open
region node. Therefore, technique likely produce benefit.
Pursuing idea, allowed algorithm expand nodes fringe
descendants fixed depth iteration. defined parameter, called expansion
horizon denoted k, represent, loosely speaking, number times whole fringe
expanded iteration. k = 1, HAO* expands open reachable regions
nodes fringe recomputing optimal policy. k = 2, expands regions
fringe children updating policy. k = 3 consider grandchildren regions fringe, on. k tends infinity, algorithm essentially
performs exhaustive search: first expands graph reachable nodes, performs one
pass (hybrid) dynamic programming graph determine optimal policy. balancing
node expansion update, expansion horizon allows tuning algorithm behavior
exhaustive search traditional heuristic search. experiments showed value k
5 10 optimal solve hardest benchmark problems (see section 4).

45

fiMeuleau, Benazera, Brafman, Hansen & Mausam

Start

ObsPt3

Unsafe

C4
Obs
Pt4

Featureless
C6

W2
W3

W1

Obs
Pt5

ObsPt2

Audience

Demo

label

: Waypoint
Name

: Rock
: IP + CHAMP

ObsPt1
Far

: Science Cam.

Figure 5: K9 rover (top left) developed Jet Propulsion Laboratory NASA Ames
Research Center prototype MER rovers. used test advanced rover
software, including automated planners rovers activities. Right: topological map
2004 demo problem. Arrows labeled IP + CHAMP represent opportunity
deploy arm rock (instrument placement) take picture
CHAMP Camera. Arrows labeled Science Cam represent opportunity take
remote picture rock Science Camera.

3.6 Updating Multiple Regions
expansion policies described based expanding open regions one several
nodes simultaneously. allow leveraging hybrid-state dynamic programming techniques
Feng et al. (2004) Li Littman (2005). techniques may compute single
iteration piecewise constant linear value functions cover large range continuous states,
possibly whole space possible values. particular, back one iteration
continuous states included given bounds.
Therefore, several open regions node expanded iteration
HAO*, update simultaneously backing-up subset continuous states
includes regions. instance, one may record lower bounds upper bounds
continuous variable expanded regions, compute value function covers
hyper-rectangle bounds.
modification algorithm impact convergence. long value
expanded regions computed, convergence proof holds. However, execution time may adversely affected expanded regions proper subset region continuous states

46

fiHAO*

(a) Value function Vn (.) initial node.
first plateau corresponds analyzing R1, second plateau analyzing R2, third plateau
analyzing R1 R2.

(b) policy n (.) starting
node shows partitions resource space different actions
optimal. Dark: action; Grey:
navigation R2; Light: analysis
R1.

Figure 6: (a) Optimal value function initial state simple rover problem possible
values continuous resources (time energy remaining). value function
partitioned 3476 pieces. (b) Optimal policy set states.

backed-up. case, values states open reachable uselessly computed,
deviates pure heuristic search algorithm.
However, modification may beneficial avoids redundant computation.
Hybrid-state dynamic programming techniques manipulate pieces value functions. Thus, several
expanded regions included piece value function, value computed
once. practice, benefit may outweigh cost evaluating useless regions. Moreover, cost
reduced storing value functions associated node graph,
computed values irrelevant regions saved case regions become eligible expansion
(i.e., open reachable) later. Thus, variant HAO* fully exploits hybrid-state dynamic
programming techniques.

4. Experimental Evaluation
section, describe performance HAO* solving planning problems simulated
planetary exploration rover two monotonic continuous-valued resources: time battery
power. Section 4.1 uses simple toy example problem illustrate basic steps
HAO* algorithm. Section 4.2 tests performance algorithm using realistic, real-size
NASA simulation rover analyzes results experiments. simulation uses
model K9 rover (see Figure 5) developed Intelligent Systems (IS) demo NASA
Ames Research Center October 2004 (Pedersen et al., 2005). complex real-size model
K9 rover uses command names understandable rovers execution language,
plans produced algorithm directly executed rover. experiments
reported Section 4.2, simplify NASA simulation model way.

47

fiMeuleau, Benazera, Brafman, Hansen & Mausam

Figure 7: First iteration HAO* toy problem. explicit graph marked dim edges
solution graph marked thick edges. Tip nodes 4, 5, 6 7 shown
constant heuristic functions expanded nodes 1, 2 3 shown backed
value functions.

planning problem consider, autonomous rover must navigate planar graph
representing surroundings authorized navigation paths, schedule observations
performed different rocks situated different locations. subset observational
goals achieved single run due limited resources. Therefore, oversubscribed
planning problem. problem planning uncertainty since action uncertain
positive resource consumptions probability failing.
significant amount uncertainty domain comes tracking mechanism used
rover. Tracking process rover recognizes rock based certain features
camera image associated rock. mission operations, problem instance
containing fixed set locations, paths, rocks built last panoramic camera image
sent rover. logical rock problem instance corresponds real rock,
rover must associate two basis features detected instruments,
including camera. rover moves camera image changes, rover must keep track
features image evolve. process uncertain subject faults result
losing track rock. practice, tracking modeled following way:
order perform measurement rock, rover must tracking rock.
navigate along path, must tracking one rocks enables following path.
set rocks enable path part problem definition given planner.
decision start tracking rock must made rover begins move.
rover starts moving, may keep track rock already tracked voluntarily stop
tracking it, cannot acquire new rock tracked initially.

48

fiHAO*

Figure 8: Second iteration HAO* toy problem.
rover may randomly lose track rocks navigating along path. probability losing track rock depends rock path followed, part
problem definition given planner.
way reacquire rock whose track lost, intentionally accident.
number rocks tracked strongly influences duration resource consumption
navigate actions. higher number rocks tracked, costly navigate
along path. rover stop regularly check record aspect
rock tracked. creates incentive limit number rocks tracked
rover given set goals chosen path intends follow.
So, rover initially selects set rocks track tries keep set small possible
given goals. starts moving, may lose track rocks, may cause
reconsider set goals pursue route get corresponding rocks.
purposely stop tracking rock longer necessary given goals left
achieve.
implementation HAO* uses dynamic programming algorithm developed Feng et
al. (2004) summarized Section 2.4 order perform backups hybrid state space,
partitions continuous state-space associated node piecewise-constant regions. uses
multiple-region updates described Section 3.6: upper bound resource
expanded regions computed, states included bounds minimal
possible resource levels updated.
experiments, use variant HAO* algorithm described Section 3.5,
parameter k sets number times whole fringe expanded iteration HAO*;
allows behavior algorithm tuned exhaustive search heuristic search.
used expansion horizon k = 2 simple example Section 4.1 default expansion
horizon k = 7 larger examples Section 4.2. Section 4.2.3 describes experiments
different expansion horizons.

49

fiMeuleau, Benazera, Brafman, Hansen & Mausam

Figure 9: Third iteration HAO* toy problem.
implementation HAO* uses simple heuristic described Section 3.4, augmented
small amount domain knowledge. value Hn (x) state (n, x) essentially equal
sum utilities goals yet achieved n. However, rover already moved
certain rock tracked state n, goals requiring rock tracked
included sum. reflects fact that, rover moved, cannot start tracking
rock more, thus goals require rock tracked unreachable. resulting
heuristic admissible (i.e., never underestimates value state), straightforward
compute. Note depend current resource levels, functions Hn (x)
constant values x.
4.1 Example
begin simple example rover planning problem order illustrate steps
algorithm. solve example using implementation HAO* use
solve realistic examples considered Section 4.2.
example, targets two rocks, R1 R2, positioned locations L1 L2,
respectively. rovers initial location L1, direct path L1 L2.
Analyzing rock R1 yields reward 10 analyzing rock R2 yields reward 20. rovers
action set simplified. Notably, features single action Pic(Rx) represents steps
analyzing rock Rx, stop tracking actions removed.
Figure 6 shows optimal value function optimal policy found HAO* starting
discrete state, resources ranging whole space possible values. Figures 7, 8 9
show step-by-step process HAO* solves problem. Using expansion horizon
k = 2, HAO* solves problem three iterations, follows:
Iteration 1: shown Figure 7, HAO* expands nodes 1, 2 3 computes heuristic
function new tip nodes 4, 5, 6 7. backup step yields value function estimates
nodes 1, 2 3. HAO* identifies best solution graph new fringe node 6.

50

fiHAO*

(a) 1012 pieces.

(b) 3465pieces.

(c) 6122pieces.

Figure 10: Optimal value functions initial state simple rover problem increasing initial resource levels (from left right). optimal return appears three
dimensional function carved reachable space heuristic function.
problem
name
Rover1
Rover2
Rover3
Rover4

rover
locations
7
7
9
11

paths

goals

fluents

actions

10
11
16
20

3
5
6
6

30
41
49
51

43
56
73
81

discrete
states
(approx.)
1.1 109
2.2 1012
5.6 1014
2.3 1015

reachable
discrete
states
613
5255
20393
22866

explicit
graph

optimal
policy

longest
branch

234
1068
2430
4321

50
48
43
44

35
35
43
43

Table 3: Size benchmark rover problems.
Iteration 2: shown Figure 8, HAO* expands nodes 6, 8, 9 10, starting
previous fringe node 6, computes heuristic functions new tip nodes 11, 12 13.
heuristic value node 12 zero because, state, rover lost track R2
already analyzed R1. backup step improves accuracy value function
several nodes. Node 11 new fringe node since 12 terminal node.
Iteration 3: shown Figure 9, HAO* expands node 11 node 14. search ends
iteration open node optimal solution graph.
comparison, Figure 10 shows value function found HAO* varies different initial
resource levels. figures, unreachable states assigned large constant heuristic value,
value function reachable states appears carved plateau heuristic.
4.2 Performance
Now, describe HAO*s performance solving four much larger rover planning problems using
NASA simulation model. characteristics problems displayed Tables 3. Columns
two six show size problems terms rover locations, paths, goals. show
total number fluents (Boolean state variables) actions problem. Columns seven
ten report size discrete state space. total number discrete states two raised
power number fluents. Although huge state space, limited number
states reached start state, depending initial resource levels. eighth
column Table 3 shows number reachable discrete states initial time energy levels
set maximum value. (The maximum initial resource levels based scenario
2004 demo represent several hours rover activity.) shows simple reachability
51

fiMeuleau, Benazera, Brafman, Hansen & Mausam

700

500
400
300
200
100
0

reachable
created
expanded
optimal policy

600
Number discrete states

600
Number discrete states

700

reachable
created
expanded
optimal policy

500
400
300
200
100

0

100000

200000
300000
Initial energy

400000

0

500000

0

2000

4000
6000
Initial time

8000

10000

8000

10000

8000

10000

8000

10000

(a) Rover1
6000

reachable
created
expanded
optimal policy

5000

Number discrete states

Number discrete states

6000

4000
3000
2000
1000
0

0

100000

200000
300000
Initial energy

400000

4000
3000
2000
1000
0

500000

reachable
created
expanded
optimal policy

5000

0

2000

4000
6000
Initial time

(b) Rover2
25000

reachable
created
expanded
optimal policy

20000

Number discrete states

Number discrete states

25000

15000
10000
5000
0

0

100000

200000 300000
Initial energy

400000

20000
15000
10000
5000
0

500000

reachable
created
expanded
optimal policy

0

2000

4000
6000
Initial time

(c) Rover3
25000

reachable
created
expanded
optimal policy

20000

Number discrete states

Number discrete states

25000

15000
10000
5000
0

0

100000

200000 300000
Initial energy

400000

20000
15000
10000
5000
0

500000

reachable
created
expanded
optimal policy

0

2000

4000
6000
Initial time

(d) Rover4
Figure 11: Number nodes created expanded HAO* vs. number reachable discrete states.
graphs left column obtained fixing initial time maximum value
varying initial energy. graphs right column obtained fixing
initial energy maximum value varying initial time. Results obtained
k = 7.
52

fiHAO*

analysis based resource availability makes huge difference. partly due fact
planning domain, close K9 execution language, allow many fluents
true simultaneously. Columns nine ten show number discrete states explicit
graph optimal policy. precisely, former number nodes created HAO*,
is, subset reachable discrete states. number reachable discrete states, thus
size graph explore, may seem small compared discrete combinatorial problems
solved AI techniques. iteration, continuous approximation two-dimensional
backup necessary evaluate hybrid state space associated graph. Finally, last
column Table 3 shows length longest branch optimal policy initial
resource levels set maximum value.
largest four instances (that is, Rover4) exactly problem October 2004
demo. considered large rover problem. example, much larger
problems faced MER rovers never visit one rock single planning cycle.
4.2.1 Efficiency Pruning
first set simulations, try evaluate efficiency heuristic pruning HAO*, is,
portion discrete search space spared exploration use admissible
heuristics. purpose, compare number discrete states reachable given
resource level number nodes created expanded HAO*. consider
number nodes optimal policy found algorithm.
Results four benchmark problems presented Figure 11. curves obtained
fixing one resource maximum possible value varying 0 maximum.
Therefore, represent problems mostly one resource constraining. result show,
notably, single resource enough constrain reachability state space significantly.
surprisingly, problems become larger initial resources increase, discrete
states become reachable. Despite simplicity heuristic used, HAO* able by-pass
significant part search space. Moreover, bigger problem, leverage
algorithm take simple heuristic.
results quite encouraging, number nodes created expanded
always reflect search time. Therefore, examine time takes HAO* produce solutions.
4.2.2 Search Time
Figure 12 shows HAO* search time set experiments. curves exhibit
monotonicity and, instead, appear show significant amount noise. surprising
search time always increase increase initial levels resource, although
search space bigger. shows search complexity depend size search
space alone. factors must explain complexity peaks observed Figure 12.
number nodes created expanded algorithm contain noise,
reason peaks computation time must time spent dynamic programming
backups. Moreover, search time appears closely related complexity optimal policy.
Figure 13 shows number nodes branches policy found algorithm, well
number goals pursued policy. shows that: (i) cases, increasing initial
resource level eliminates need branching reduces size optimal solution; (ii)
size optimal policy and, secondarily, number branches, explains peaks
search time curves. Therefore, question is: large solution graph induce long time
spent backups? two possible answers question: backups take longer
and/or backups performed. first explanation pretty intuitive.
policy graph contains many branches leading different combinations goals, value functions
contain many humps plateaus, therefore many pieces, impacts complexity
dynamic programming backups. However, time empirical evidence
53

fi18

18

16

16

14

14

12

12

Search time (s)

Search time (s)

Meuleau, Benazera, Brafman, Hansen & Mausam

10
8
6

10
8
6

4

4

2

2

0

0

100000

200000
300000
Initial energy

400000

0

500000

0

2000

4000
6000
Initial time

8000

10000

4000
6000
Initial time

8000

10000

180

160

160

140

140

120

120

Search time (s)

Search time (s)

(a) Rover1
180

100
80
60

100
80
60

40

40

20

20

0

0

100000

200000
300000
Initial energy

400000

0

500000

0

2000

25000

20000

20000
Search time (s)

Search time (s)

(b) Rover2
25000

15000
10000
5000
0

15000
10000
5000

0

100000

200000 300000
Initial energy

400000

0

500000

0

2000

4000
6000
Initial time

8000

10000

0

2000

4000
6000
Initial time

8000

10000

20000

20000

15000

15000
Search time (s)

Search time (s)

(c) Rover3

10000

5000

0

10000

5000

0

100000

200000 300000
Initial energy

400000

0

500000

(d) Rover4
Figure 12: HAO* search time. graphs left column obtained fixing initial time
maximum value, graphs right column obtained fixing
initial energy maximum. Results obtained k = 7.

54

fiHAO*

confirm hypothesis. Conversely, observe peak Figure 12 comes increase
number backups. work required explain this.
4.2.3 Expansion Horizon
results Section 4.2.1 show HAO* leverage even simple admissible heuristic prune
large portion search space. necessarily follow HAO* outperform
exhaustive search algorithm creates graph reachable states, executes one pass
dynamic programming graph find optimal policy. Although HAO* expands smaller
graph exhaustive search, must evaluate graph often. Section 3.5,
introduced parameter k expansion horizon order allow adjustment trade-off
time spent expanding nodes time spent evaluating nodes. study influence
parameter algorithm.
Figure 14 shows number nodes created expanded HAO* function
expansion horizon four benchmark problem instances. surprisingly, algorithm creates
expands nodes expansion horizon increases. Essentially, behaves
exhaustive search k increased. two smallest problem instances, large enough
values k, number visited states levels total number reachable states
reached. two largest problem instances, interrupt experiments k reached
25 search time became long.
Figure 15 shows effect expansion horizon search time HAO*. smallest
problem instance (Rover1), HAO* clear advantage exhaustive search (with
k > 20), even though explores fewer nodes. three larger problem instances, HAO*
clear advantage. Rover2 problem instance, search time HAO* levels
k = 25, indicating limit reachable states reached. However, duration
exhaustive search several times longer HAO* smaller settings k. benefits
HAO* clearer two largest problem instances. k increased, algorithm
quickly overwhelmed combinatorial explosion size search space, simulations
eventually need interrupted search time becomes long. problem
instances smaller settings k, HAO* able efficiently find optimal solutions.
Overall, results show clear benefit using admissible heuristics prune
search space, although expansion horizon must adjusted appropriately order HAO*
achieve favorable trade-off node-expansion time node-evaluation time.

5. Conclusion
introduced heuristic search approach finding optimal conditional plans domains characterized continuous state variables represent limited, consumable resources. HAO*
algorithm variant AO* algorithm that, best knowledge, first algorithm deal following: limited continuous resources, uncertain action outcomes,
over-subscription planning. tested HAO* realistic NASA simulation planetary rover,
complex domain practical importance, results demonstrate effectiveness solving
problems large solved straightforward application dynamic programming. effective heuristic search exploit resource constraints, well admissible
heuristic, order limit reachable state space.
implementation, HAO* algorithm integrated dynamic programming algorithm Feng et al. (2004). However HAO* integrated dynamic programming
algorithms solving hybrid-state MDPs. Feng et al. algorithm finds optimal policies
limiting assumptions transition probabilities discrete, rewards either piecewiseconstant piecewise-linear. recently-developed dynamic programming algorithms hybridstate MDPs make less restrictive assumptions, potential improve computational

55

fiMeuleau, Benazera, Brafman, Hansen & Mausam

30

3

20

2

10

1

0

0

100000

200000 300000
Initial energy

400000

40

0
500000

5

Nodes
Branches
Goals

4

30

3

20

2

10

1

0

0

2000

4000
6000
Initial time

8000

Number branches goals

4

50

Number nodes

40
Number nodes

5

Nodes
Branches
Goals

Number branches goals

50

0
10000

(a) Rover1

45

3

30

2

15

1

0

0

100000

200000 300000
Initial energy

400000

60

0
500000

5

Nodes
Branches
Goals

4

45

3

30

2

15

1

0

0

2000

4000
6000
Initial time

8000

Number branches goals

4

75

Number nodes

Nodes
Branches
Goals

60
Number nodes

5
Number branches goals

75

0
10000

(b) Rover2

45

3

30

2

15

1

0

0

100000

200000 300000
Initial energy

400000

60

0
500000

5

Nodes
Branches
Goals

4

45

3

30

2

15

1

0

0

2000

4000
6000
Initial time

8000

Number branches goals

4

75

Number nodes

Nodes
Branches
Goals

60
Number nodes

5
Number branches goals

75

0
10000

(c) Rover3

60

3

40

2

20

1

0

0

100000

200000 300000
Initial energy

400000

80

0
500000

5

Nodes
Branches
Goals

4

60

3

40

2

20

1

0

0

2000

4000
6000
Initial time

8000

Number branches goals

4

100

Number nodes

Nodes
Branches
Goals

80
Number nodes

5
Number branches goals

100

0
10000

(d) Rover4
Figure 13: Complexity optimal policy: number nodes, branches, goals optimal
policy setting Figure 11.

56

fiHAO*

700

Number discrete states

600
Number discrete states

6000

created
expanded

500
400
300
200
100
0

0

5

10
15
20
Expansion horizon

4000
3000
2000
1000
0

25

created
expanded

5000

0

5

10

(a) Rover1
14000

Number discrete states

Number discrete states

30

created
expanded

14000

10000
8000
6000
4000
2000
0

25

(b) Rover2
16000

created
expanded

12000

15
20
Expansion horizon

12000
10000
8000
6000
4000
2000

0

5

10
15
Expansion horizon

0

20

(c) Rover3

0

5

10
15
Expansion horizon

20

(d) Rover4

Figure 14: Influence expansion horizon number nodes visited algorithm.
efficiency (Li & Littman, 2005; Marecki et al., 2007). Integrating HAO* one algorithms
could improve performance further.
several interesting directions work could extended. developing HAO*, made assumptions every action consumes resource resources
non-replenishable. Without assumptions, state could revisited optimal
plan could loops well branches. Generalizing approach allow plans loops,
seems necessary handle replenishable resources, requires generalizing heuristic search
algorithm LAO* solve hybrid MDPs (Hansen & Zilberstein, 2001). Another possible extension
allow continuous action variables addition continuous state variables. Finally, heuristic
search approach could combined approaches improving scalability, hierarchical decomposition (Meuleau & Brafman, 2007). would allow handle even larger state
spaces result number goals over-subscription planning problem increased.
Acknowledgments
work funded NASA Intelligent Systems program, grant NRA2-38169. Eric Hansen
supported part NASA Summer Faculty Fellowship funding Mississippi
Space Grant Consortium. work performed Emmanuel Benazera working
NASA Ames Research Center Ronen Brafman visiting NASA Ames Research Center,
consultants Research Institute Advanced Computer Science. Ronen Brafman
supported part Lynn William Frankel Center Computer Science, Paul Ivanier
Center Robotics Production Management, ISF grant #110707. Nicolas Meuleau
consultant Carnegie Mellon University NASA Ames Research Center.

57

fiMeuleau, Benazera, Brafman, Hansen & Mausam

60

1800
1600
1400

40

Search time (s)

Search time (s)

50

30
20

1000
800
600
400

10
0

1200

200
0

5

10
15
20
Expansion horizon

0

25

0

5

10

(a) Rover1

30

30000
25000
Search time (s)

20000
Search time (s)

25

(b) Rover2

25000

15000
10000
5000
0

15
20
Expansion horizon

20000
15000
10000
5000

0

5

10
15
Expansion horizon

0

20

(c) Rover3

0

5

10
15
Expansion horizon

20

(d) Rover4

Figure 15: Influence expansion horizon overall search time.

References
Altman, E. (1999). Constrained Markov Decision Processes. Chapman HALL/CRC.
Bertsekas, D., & Tsitsiklis, J. (1996). Neural Dynamic Programming. Athena Scientific, Belmont,
MA.
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions
computational leverage. Journal Artificial Intelligence Research, 11, 194.
Boyan, J., & Littman, M. (2000). Exact solutions time-dependent MDPs. Advances Neural
Information Processing Systems 13, pp. 17. MIT Press, Cambridge.
Bresina, J., Dearden, R., Meuleau, N., Ramakrishnan, S., Smith, D., & Washington, R. (2002).
Planning continuous time resource uncertainty: challenge AI. Proceedings
Eighteenth Conference Uncertainty Artificial Intelligence, pp. 7784.
Bresina, J., Jonsson, A., Morris, P., & Rajan, K. (2005). Activity planning mars exploration
rovers. Proceedings Fifteenth International Conference Automated Planning
Scheduling, pp. 4049.
Chakrabarti, P., Ghose, S., & DeSarkar, S. (1988). Admissibility AO* heuristics overestimate. Aritificial Intelligence, 34, 97113.
Feng, Z., Dearden, R., Meuleau, N., & Washington, R. (2004). Dynamic programming structured continuous Markov decision problems. Proceedings Twentieth Conference
Uncertainty Artificial Intelligence, pp. 154161.
58

fiHAO*

Friedman, J., Bentley, J., & Finkel, R. (1977). algorithm finding best matches logarithmic
expected time. ACM Trans. Mathematical Software, 3(3), 209226.
Hansen, E., & Zilberstein, S. (2001). LAO*: heuristic search algorithm finds solutions
loops. Artificial Intelligence, 129, 3562.
Jimenez, P., & Torras, C. (2000). efficient algorithm searching implicit AND/OR graphs
cycles. Artificial Intelligence, 124, 130.
Kveton, B., Hauskrecht, M., & Guestrin, C. (2006). Solving factored MDPs hybrid state
action variables. Journal Artificial Intelligence Research, 27, 153201.
Li, L., & Littman, M. (2005). Lazy approximation solving continuous finite-horizon MDPs.
Proceedings Twentieth National Conference Artificial Intelligence, pp. 11751180.
Marecki, J., Koenig, S., & Tambe, M. (2007). fast analytical algorithm solving markov decision
processes real-valued resources. Proceedings 20th International Joint Conference
Artificial Intelligence (IJCAI-07, pp. 25362541.
Mausam, Benazera, E., Brafman, R., Meuleau, N., & Hansen, E. (2005). Planning continuous resources stochastic domains. Proceedings Nineteenth International Joint
Conference Artificial Intelligence, pp. 12441251. Professional Book Center, Denver, CO.
Meuleau, N., & Brafman, R. (2007). Hierarchical heuristic forward search stochastic domains.
Proceedings 20th International Joint Conference Artificial Intelligence (IJCAI-07),
pp. 25422549.
Munos, R., & Moore, A. (2002). Variable resolution discretization optimal control. Machine
Learning, 49 (2-3), 291323.
Nilsson, N. (1980). Principles Artificial Intelligence. Tioga Publishing Company, Palo Alto, CA.
Pearl, J. (1984). Heuristics: Intelligent Search Strategies Computer Problem Solving. AddisonWesley.
Pedersen, L., Smith, D., Deans, M., Sargent, R., Kunz, C., Lees, D., & Rajagopalan, S. (2005).
Mission planning target tracking autonomous instrument placement. Proceedings
2005 IEEE Aerospace Conference., Big Sky, Montana.
Puterman, M. (1994). Markov Decision Processes: Discrete Stochastic Dynamic Programming.
Wiley, New York, NY.
Rust, J. (1997). Using randomization break curse dimensionality. Econimetrica, 65, 487
516.
Smith, D. (2004). Choosing objectives over-subscription planning. Proceedings Fourteenth
International Conference Automated Planning Scheduling, pp. 393401.
Thiebaux, S., Gretton, C., Slaney, J., Price, D., & Kabanza, F. (2006). Decision-theoretic planning
non-Markovian rewards. Journal Artificial Intelligence Research, 25, 1774.
van den Briel, M., Sanchez, R., Do, M., & Kambhampati, S. (2004). Effective approaches partial
satisfation (over-subscription) planning. Proceedings Nineteenth National Conference
Artificial Intelligence, pp. 562569.

59


