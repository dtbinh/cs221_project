journal artificial intelligence

submitted published

learning document level semantic properties
free text annotations
r k branavan
harr chen
jacob eisenstein
regina barzilay

branavan csail mit edu
harr csail mit edu
jacobe csail mit edu
regina csail mit edu

computer science artificial intelligence laboratory
massachusetts institute technology
massachusetts avenue cambridge

abstract
presents method inferring semantic properties documents leveraging free text keyphrase annotations annotations becoming increasingly abundant due
recent dramatic growth semi structured user generated online content one especially
relevant domain product reviews often annotated authors pros cons
keyphrases real bargain good value annotations representative
underlying semantic properties however unlike expert annotations noisy lay authors
may use different labels denote property labels may missing learn
noisy annotations hidden paraphrase structure clusters keyphrases
paraphrase structure linked latent topic model review texts enabling system predict properties unannotated documents effectively aggregate semantic
properties multiple reviews implemented hierarchical bayesian model
joint inference joint inference increases robustness keyphrase clustering
encourages latent topics correlate semantically meaningful properties multiple evaluations demonstrate model substantially outperforms alternative approaches summarizing
single multiple documents set semantically salient keyphrases

introduction
identifying document level semantic properties implied text core natural
language understanding example given text restaurant review would useful
extract semantic level characterization authors reaction specific aspects restaurant food service quality see figure learning approaches dramatically
increased scope robustness semantic processing typically dependent
large expert annotated datasets costly produce zaenen
propose use alternative source annotations learning free text keyphrases produced novice users example consider lists pros cons often accompany
reviews products services end user annotations increasingly prevalent online
grow organically keep pace subjects interest socio cultural trends beyond
pragmatic considerations free text annotations appealing linguistic standpoint
capture intuitive semantic judgments non specialist language users many real world
datasets annotations created documents original author providing direct window
semantic judgments motivated document text

c

ai access foundation rights reserved

fib ranavan c hen e isenstein barzilay

pros cons great nutritional value
combines amazing product quick friendly service cleanliness great nutrition
pros cons bit pricey healthy
awesome place go health conscious really great low calorie dishes
calories fat grams per serving

figure excerpts online restaurant reviews pros cons phrase lists reviews assert
restaurant serves healthy food use different keyphrases additionally
first review discusses restaurants good service annotated
keyphrases

major obstacle computational use free text annotations inherently noisy fixed vocabulary explicit relationship annotation keyphrases
guarantee relevant semantic properties document annotated example
pros cons annotations accompanying restaurant reviews figure underlying
semantic idea expressed different ways keyphrases great nutritional value
healthy additionally first review discusses quality service annotated
contrast expert annotations would replace synonymous keyphrases single canonical label would fully label semantic properties described text expert annotations
typically used supervised learning methods demonstrate traditional
supervised approaches perform poorly free text annotations used instead clean expert
annotations
demonstrates handling free text annotation context
hidden topic analysis document text regularities text clarify noise
annotations example although great nutritional value healthy different
surface forms text documents annotated two keyphrases likely
similar modeling relationship document text annotations large dataset
possible induce clustering annotation keyphrases help overcome
inconsistency model addresses incompleteness novice
annotators fail label relevant semantic topics estimating topics predicted
document text alone
central idea document text associated annotations reflect
single underlying set semantic properties text semantic properties correspond
induced hidden topics similar growing body work latent topic
latent dirichlet allocation lda blei ng jordan however unlike existing work topic
modeling tie hidden topics text clusters observed keyphrases connection
motivated idea text associated annotations grounded shared set
semantic properties modeling properties directly ensure inferred hidden
topics semantically meaningful clustering free text annotations robust
noise
takes form hierarchical bayesian framework includes lda style
component word text generated mixture multinomials addition incorporate similarity matrix across universe annotation keyphrases



fil earning ocument l evel emantic p roperties f ree ext nnotations

constructed orthographic distributional features keyphrases model
matrix generated underlying clustering keyphrases keyphrases
clustered together likely produce high similarity scores generate words
document model two distributions semantic properties one governed annotation
keyphrases clusters background distribution cover properties mentioned
annotations latent topic word drawn mixture two distributions
learning model parameters noisily labeled training set apply model unlabeled
data
build system extracts semantic properties reviews products services
system uses training corpus includes user created free text annotations pros cons
review training yields two outputs clustering keyphrases semantic properties
topic model capable inducing semantic properties unlabeled text clustering
annotation keyphrases relevant applications content information retrieval
allowing users retrieve documents semantically relevant annotations even surface
forms differ query term topic model used infer semantic properties
unlabeled text
topic model used perform multi document summarization capturing key
semantic properties multiple reviews unlike traditional extraction approaches multidocument summarization induced topic model abstracts text review representation capturing relevant semantic properties enables comparison reviews even
use superficially different terminology describe set semantic properties
idea implemented review aggregation system extracts majority sentiment
multiple reviewers product service example output produced system
shown figure system applied reviews product categories allowing users
navigate semantic properties products total reviews
effectiveness confirmed several evaluations
summarization single multiple documents compare properties inferred model expert annotations yields substantially better
alternatives literature particular learning clustering free text
annotation keyphrases essential extracting meaningful semantic properties dataset
addition compare induced clustering gold standard clustering produced expert
annotators comparison shows tying clustering hidden topic model substantially
improves quality clustering induced system coheres well clustering
produced expert annotators
remainder structured follows section compares previous work topic modeling semantic property extraction multi document summarization
section describes properties free text annotations motivate model
described section method parameter estimation presented section
section describes implementation evaluation single document multi document
summarization systems techniques summarize contributions consider directions future work section code datasets expert annotations used
available online http groups csail mit edu rbg code precis



fib ranavan c hen e isenstein barzilay

related work
material presented section covers three lines related work first discuss work
bayesian topic modeling related technique learning free text annotations
next discuss state art methods identifying analyzing product properties
review text finally situate summarization work landscape prior
multi document summarization
bayesian topic modeling
recent work topic modeling literature demonstrated semantically salient topics
inferred unsupervised fashion constructing generative bayesian model document text one notable example line latent dirichlet allocation lda blei
et al lda framework semantic topics equated latent distributions words
text thus document modeled mixture topics class
used variety language processing tasks including topic segmentation purver kording
griffiths tenenbaum named entity resolution bhattacharya getoor sentiment
ranking titov mcdonald b word sense disambiguation boyd graber blei zhu

method similar lda assigns latent topic indicators word
dataset documents mixtures topics however lda model unsupervised
provide method linking latent topics external observed representations
properties interest contrast model exploits free text annotations dataset
ensure induced topics correspond semantically meaningful properties
combining topics induced lda external supervision first considered blei
mcauliffe supervised latent dirichlet allocation slda model induction
hidden topics driven annotated examples provided training stage perspective supervised learning succeeds hidden topics mediate
document annotations lexical features blei mcauliffe describe variational expectationmaximization procedure approximate maximum likelihood estimation parameters tested two polarity assessment tasks slda shows improvement model
topics induced unsupervised model added features supervised
model
key difference model slda assume access clean
supervision data training since annotations provided free text
nature incomplete fraught inconsistency substantial difference input
structure motivates need model simultaneously induces hidden structure freetext annotations learns predict properties text
property assessment review analysis
model applied task review analysis traditionally task identifying properties product review texts cast extraction hu liu liu
hu cheng popescu nguyen etzioni example hu liu employ
association mining identify noun phrases express key portions product reviews polarity extracted phrases determined seed set adjectives expanded via wordnet



fil earning ocument l evel emantic p roperties f ree ext nnotations

relations summary review produced extracting property phrases present verbatim
document
property extraction refined pine popescu et al another system
review analysis pine employs novel information extraction method identify noun phrases
could potentially express salient properties reviewed products candidates
pruned wordnet morphological cues opinion phrases identified set handcrafted rules applied syntactic dependencies extracted input document semantic
orientation properties computed relaxation labeling method finds optimal assignment polarity labels given set local constraints empirical demonstrate pine
outperforms hu lius system opinion extraction identifying polarity opinion words
two feature extraction methods informed human knowledge way opinions
typically expressed reviews hu liu human knowledge encoded
wordnet seed adjectives popescu et al opinion phrases extracted via handcrafted rules alternative learn rules feature extraction annotated
data end property identification modeled classification framework kim
hovy classifier trained corpus free text pro con keyphrases
specified review authors keyphrases compared sentences review
text sentences exhibit high word overlap previously identified phrases marked pros
cons according phrase polarity rest sentences marked negative examples
clearly accuracy resulting classifier depends quality automatically induced annotations analysis free text annotations several domains shows automatically mapping even manually extracted annotation keyphrases document text difficult
task due variability keyphrase surface realizations see section argue rest
beneficial explicitly address difficulties inherent free text annotations
end work distinguished two significant ways property extraction methods described first able predict properties beyond appear verbatim text
second learns semantic relationships different keyphrases allowing
us draw direct comparisons reviews even semantic ideas expressed
different surface forms
working related domain web opinion mining lu zhai describe system
generates integrated opinion summaries incorporate expert written articles e g review online magazine user generated ordinary opinion snippets e g mentions
blogs specifically expert article assumed structured segments collection
representative ordinary opinions aligned segment probabilistic latent semantic analysis
plsa used induce clustering opinion snippets cluster attached one
expert article segments clusters may unaligned segment indicating
opinions entirely unexpressed expert article ultimately integrated opinion summary combination single expert article multiple user generated opinion snippets
confirm supplement specific segments review
works final goal different aim provide highly compact summary multitude user opinions identifying underlying semantic properties rather supplementing
single expert article user opinions specifically leverage annotations users already
provide reviews thus obviating need expert article template opinion inte



fib ranavan c hen e isenstein barzilay

gration consequently suitable goal producing concise keyphrase
summarizations user reviews particularly review taken authoritative
work closest methodology review summarizer developed titov
mcdonald method summarizes review selecting list phrases
express writers opinions set predefined properties e g food ambiance restaurant
reviews system access numerical ratings set properties
training set providing examples appropriate keyphrases extract similar slda method
uses numerical ratings bias hidden topics towards desired semantic properties phrases
strongly associated properties via hidden topics extracted part summary
several important differences work summarization method
titov mcdonald method assumes predefined set properties thus cannot capture
properties outside set moreover consistent numerical annotations required training
method emphasizes use free text annotations finally since titov mcdonalds
extractive facilitate property comparison across multiple reviews
multidocument summarization
relates large body work multi document summarization researchers
long noted central challenge multi document summarization identifying redundant
information input documents radev mckeown carbonell goldstein mani
bloedorn barzilay mckeown elhadad task crucial significance
multi document summarizers operate related documents describe facts
multiple times fact common assume repetition information among related sources
indicator importance barzilay et al radev jing budzikowska nenkova
vanderwende mckeown many first cluster sentences together
extract generate sentence representatives clusters
identification repeated information equally central multi document
summarization method selects properties stated plurality users thereby eliminating rare erroneous opinions key difference existing summarization systems method identifying repeated expressions single semantic property
since existing work multi document summarization focuses topic independent
newspaper articles redundancy identified via sentence comparison instance radev et al
compare sentences cosine similarity corresponding word vectors alternatively methods compare sentences via alignment syntactic trees barzilay et al
marsi krahmer string tree comparison augmented
lexico semantic knowledge resources wordnet
described perform comparisons sentence level instead first abstract reviews set properties compare property overlap across
different documents relates domain dependent approaches text summarization radev mckeown white korelsky cardie ng pierce wagstaff elhadad
mckeown methods identify relations documents comparing
abstract representations cases abstract representation constructed shelf
information extraction tools template specifying types information select crafted
manually domain interest moreover training information extraction systems requires
corpus manually annotated relations interest contrast method require



fil earning ocument l evel emantic p roperties f ree ext nnotations

incompleteness
property
good food
good service
good price
bad food
bad service
bad price
average

recall

precision

f score

























inconsistency
keyphrase top keyphrase
count
coverage















table incompleteness inconsistency restaurant domain six major properties prevalent reviews incompleteness figures recall precision f score
author annotations manually clustered properties gold standard property
annotations inconsistency measured number different keyphrase realizations
least five occurrences associated property percentage frequency
commonly occurring keyphrases used annotate property
averages bottom row weighted according frequency property occurrence

manual template specification corpora annotated experts abstract representations
induce linguistically rich extraction templates nevertheless enable us
perform depth comparisons across different reviews

analysis free text keyphrase annotations
section explore characteristics free text annotations aiming quantify degree
noise observed data analysis motivate development learning
described section
perform investigation domain online restaurant reviews documents downloaded popular epinions website users website evaluate products providing
textual description opinion well concise lists keyphrases pros cons
summarizing review pros cons keyphrases appealing source annotations online
review texts however contributed independently multiple users thus unlikely
clean expert annotations analysis focus two features free text annotations incompleteness inconsistency measure incompleteness quantifies degree
label omission free text annotations inconsistency reflects variance keyphrase
vocabulary used annotators
test quality user generated annotations compare expert annotations produced systematic fashion annotation effort focused six properties
commonly mentioned review authors specifically shown table given
review property task assess whether reviews text supports property
annotations produced two judges guided standardized set instructions contrast
author annotations website judges conferred training session ensure consistency completeness two judges collectively annotated reviews annotated
http www epinions com



fib ranavan c hen e isenstein barzilay

property good price
relatively inexpensive dirt cheap relatively cheap great price fairly priced well priced reasonable
prices cheap prices affordable prices reasonable cost

figure examples many different paraphrases related property good price appear
pros cons keyphrases reviews used inconsistency analysis

cohens kappa measure inter annotator agreement ranges zero one
joint set indicating high agreement cohen average review text
annotated properties
separately one judges standardized free text pros cons annotations
reviews reviews keyphrases matched six properties standardization allows direct comparison properties judged supported reviews
text properties described reviews free text annotations many semantic properties judged present text user annotated average
keyphrases expressed relevant semantic properties per document text expressed
properties gap demonstrates frequency authors omitted relevant semantic
properties review annotations
incompleteness
measure incompleteness compare properties stated review authors form
pros cons stated review text judged expert annotators
comparison performed precision recall f score setting recall proportion
semantic properties text review author provided least one annotation
keyphrase precision proportion keyphrases conveyed properties judged supported
text f score harmonic mean comparison summarized
left half table
incompleteness demonstrate significant discrepancy user expert
annotations expected recall quite low property occurrences stated
review text without explicitly mentioned annotations precision scores indicate
converse true though lesser extent keyphrases express properties
mentioned text
interestingly precision recall vary greatly depending specific property
highest good food matching intuitive notion high food quality would key salient
property restaurant thus likely mentioned text annotations conversely recall good service lower users high quality service apparently
key point summarizing review keyphrases
inconsistency
lack unified annotation scheme restaurant review dataset apparent across
reviewers annotations feature unique keyphrase surface forms set total
keyphrase occurrences clearly many unique keyphrases express semantic property
figure good price expressed ten different ways quantify phenomenon judges


fil earning ocument l evel emantic p roperties f ree ext nnotations

figure cumulative occurrence counts top ten keyphrases associated good service
property percentages total separate keyphrase occurrences
property

manually clustered subset keyphrases associated six previously mentioned properties specifically keyphrases associated six major properties chosen accounting
keyphrase occurrences
use manually clustered annotations examine distributional pattern keyphrases
describe underlying property two different statistics first number
different keyphrases property gives lower bound number possible paraphrases
second measure often common keyphrase used annotate property
e coverage keyphrase metric gives sense diffuse keyphrases within
property specifically whether one single keyphrase dominates occurrences property
note value overestimate true coverage since considering tenth
keyphrase occurrences
right half table summarizes variability property paraphrases observe
property associated numerous paraphrases found multiple times
actual keyphrase set importantly frequent keyphrase accounted third
property occurrences strongly suggesting targeting labels learning
limited illustrate last point consider property good service whose
keyphrase realizations distributional histogram appears figure cumulative percentage
frequencies frequent keyphrases associated property plotted top four
keyphrases account three quarters property occurrences even within limited
set keyphrases consider analysis motivating need aggregate consideration
keyphrases
next section introduce model induces clustering among keyphrases
relating keyphrase clusters text directly addressing characteristics data



fib ranavan c hen e isenstein barzilay


x

h


c

z

w













keyphrase cluster model
keyphrase cluster assignment
keyphrase similarity values
document keyphrases
document keyphrase topics
probability selecting instead
selects word topics
background word topic model
word topic assignment
language topic
document words

dirichlet
x multinomial

beta x x

beta otherwise


k


k




x k l hd
otherwise

beta
cd n bernoulli
dirichlet

multinomial cd n
zd n
multinomial otherwise
k dirichlet
wd n multinomial zd n

figure plate diagram model shaded circles denote observed variables squares
denote hyperparameters dotted arrows indicate constructed deterministically x h use refer small constant probability mass



fil earning ocument l evel emantic p roperties f ree ext nnotations

model description
present generative bayesian model documents annotated free text keyphrases
model assumes annotated document generated set underlying semantic topics
semantic topics generate document text indexing language model
associated clusters keyphrases way model viewed extension
latent dirichlet allocation blei et al latent topics additionally biased
toward keyphrases appear training data however coupling flexible
words permitted drawn topics represented keyphrase annotations
permits model learn effectively presence incomplete annotations still
encouraging keyphrase clustering cohere topics supported document text
another critical aspect model desire ability use arbitrary comparisons
keyphrases addition information surface forms accommodate
goal treat keyphrase surface forms generated model rather acquire
real valued similarity matrix across universe possible keyphrases treat matrix
generated keyphrase clustering representation permits use surface
distributional features keyphrase similarity described section
advantage hierarchical bayesian easy change parts
model observed parts hidden training keyphrase annotations
observed hidden semantic topics coupled clusters keyphrases account
words related semantic topics topics may associated keyphrases test
time model presented documents keyphrase annotations hidden
model evaluated ability determine keyphrases applicable hidden
topics present document text
judgment whether topic applies given unannotated document probability mass assigned topic documents background topic distribution
annotations background topic distribution capture entirety documents
topics task involving reviews products services multiple topics may accompany
document case topic whose probability threshold tuned development
set predicted supported
keyphrase clustering
handle hidden paraphrase structure keyphrases one component model estimates
clustering keyphrases goal obtain clusters cluster correspond welldefined semantic topic e g healthy good nutrition grouped single
cluster overall joint model generative generative model clustering could easily
integrated larger framework would treat keyphrases
cluster generated parametric distribution however representation would
permit many powerful features assessing similarity pairs keyphrases string
overlap keyphrase co occurrence corpus mccallum bellare pereira
reason represent keyphrase real valued vector rather surface
form vector given keyphrase includes similarity scores respect every observed keyphrase similarity scores represented figure model similarity
scores generated cluster memberships represented x figure two keyphrases



fib ranavan c hen e isenstein barzilay

lexical

cosine similarity surface forms two keyphrases represented word frequency vectors

co occurrence

keyphrase represented vector co occurrence values
vector counts many times keyphrases appear documents
annotated keyphrase example similarity vector
good food may include entry tasty food value
would number documents annotated good food
contain tasty food text similarity two
keyphrases cosine similarity co occurrence vectors

table two sources information used compute similarity matrix experiments
final similarity scores linear combinations two values note cooccurrence similarity contains second order co occurrence information

figure surface plot keyphrase similarity matrix set restaurant reviews computed according table red indicates high similarity whereas blue indicates low
similarity diagram keyphrases grouped according expertcreated clustering keyphrases similar meaning close together strong series
similarity blocks along diagonal hint information could induce
reasonable clustering



fil earning ocument l evel emantic p roperties f ree ext nnotations

clustered together similarity score generated distribution encouraging high similarity otherwise distribution encouraging low similarity used
features used producing similarity matrix given table encompassing lexical
distributional similarity measures implemented system takes linear combination
two data sources weighting sources equally resulting similarity matrix keyphrases
restaurant domain shown figure
described next section clustering keyphrases model takes advantage
topic structure documents annotated keyphrases addition information
individual keyphrases sense differs traditional approaches paraphrase
identification barzilay mckeown lin pantel
document topic modeling
analysis document text probabilistic topic lda blei et al
lda framework word generated language model indexed
words topic assignment thus rather identifying single topic document lda identifies
distribution topics high probability topic assignments identify compact low entropy
language probability mass language model topic divided among
relatively small vocabulary
model operates similar manner identifying topic word denoted z
figure however lda learns distribution topics document deterministically construct document specific topic distribution clusters represented
documents keyphrases figure assigns equal probability topics
represented keyphrase annotations small probability topics generating
word topics way ties together clustering language
noted sometimes keyphrase annotation represent semantic
topics expressed text reason construct another background distribution topics auxiliary variable c indicates whether given words topic drawn
distribution derived annotations background model representing c
hidden variable allows us stochastically interpolate two language
addition given document likely discuss topics covered
keyphrase account model allowed leave clusters empty thus
leaving topics independent keyphrases
generative process
model assumes observed data generated stochastic process involving hidden
parameters section formally specify generative process specification guides
inference hidden parameters observed data following
l keyphrases vector length l denoting pairwise similarity score
interval every keyphrase
document bag words wd length nd nth word wd n
note model similarity score independent draw clearly assumption strong due
symmetry transitivity making similar assumptions independence related hidden variables
previously shown successful example toutanova johnson



fib ranavan c hen e isenstein barzilay

document set keyphrase annotations hd includes index document annotated keyphrase
number clusters k large enough encompass topics actual
clusters keyphrases well word topics
observed variables generated according following process
draw multinomial distribution k keyphrase clusters symmetric dirichlet
prior parameter
l
draw th keyphrases cluster assignment x multinomial
l l
x x draw beta beta encouraging scores biased
toward values close one
b x x draw beta beta encouraging scores biased
toward values close zero
k k
draw language model k symmetric dirichlet prior parameter

draw background topic model symmetric dirichlet prior parameter
b deterministically construct annotation topic model keyphrase cluster
assignments x observed document annotations hd specifically let h set
topics represented phrases hd distribution assigns equal probability
element h small probability mass topics
c draw weighted coin beta determine balance
annotation background topic
n nd
draw binary auxiliary variable cd n bernoulli determines whether
topic word wd n drawn annotation topic model background model
ii draw topic assignment zd n appropriate multinomial indicated
cd n
iii draw word wd n multinomial zd n language model indexed
words topic
variables subscripted zero fixed hyperparameters
making hard assignment zero probability topics creates parameter estimation
probability assigned topics represented keyphrase cluster memberships



fil earning ocument l evel emantic p roperties f ree ext nnotations

parameter estimation
make predictions unseen data need estimate parameters model bayesian
inference estimate distribution parameter conditioned observed data
hyperparameters inference intractable general case sampling approaches allow
us approximately construct distributions parameter interest
gibbs sampling perhaps generic straightforward sampling technique conditional distributions computed hidden variable given variables model
repeatedly sampling distributions turn possible construct markov chain
whose stationary distribution posterior model parameters gelman carlin stern
rubin use sampling techniques natural language processing previously
investigated many researchers including finkel grenager manning goldwater
griffiths johnson
present sampling equations hidden variables figure prior
keyphrase clusters sampled hyperprior keyphrase cluster assignments
x write p mean probability conditioned variables
p p p x

p
p x


dirichlet



multinomial x



dirichlet
count x conditional distribution derived conjugacy
multinomial dirichlet distribution first line follows bayes rule second
line conditional independence cluster assignments x given keyphrase distribution
resampling equations k derived similar manner
p dirichlet
p k dirichlet k k
p

count zn cn k

count wn zn k

building counts consider cases cn indicating topic zn
indeed drawn background topic model similarly building counts k
consider cases word wd n drawn topic k
resample employ conjugacy beta prior bernoulli observation likelihoods adding counts c prior
p beta

p
count c


n

n

p
n count cd n



fib ranavan c hen e isenstein barzilay

keyphrase cluster assignments represented x whose sampling distribution depends
z via
p x p x p x x p z c






p x
p x x
p zd n


cd n


multinomial x







beta x x
multinomial zd n



cd n

leftmost term equation prior x next term encodes dependence
similarity matrix cluster assignments slight abuse notation write x x
denote x x otherwise third term dependence word topics
zd n topic distribution compute final probability expression
possible setting x sample normalized multinomial
word topics z sampled according topic distribution background distribution
observed words w auxiliary variable c
p zd n p zd n cd n p wd n zd n

multinomial zd n multinomial wd n zd n

multinomial zd n multinomial wd n zd n

cd n
otherwise

x zd n sampled computing conditional likelihood possible setting
within constant proportionality sampling normalized multinomial
finally sample auxiliary variable cd n indicates whether hidden topic zd n
drawn c depends prior hidden topic assignments z
p cd n p cd n p zd n cd n

bernoulli cd n multinomial zd n

bernoulli cd n multinomial zd n

cd n
otherwise

compute likelihood cd n cd n within constant proportionality
sample normalized bernoulli distribution
finally model requires values fixed hyperparameters tuned
standard way development set performance appendix c lists hyperparameters
values used domain experiments
one main applications model predict properties supported documents
annotated keyphrases test time would compute posterior estimate
unannotated test document since annotations present property prediction
text component model estimate use gibbs sampling
procedure restricted zd n stipulation cd n fixed zero zd n
drawn particular treat language known accurately
integrate possible language use final samples language
training opposed point estimate topic probability exceeds
certain threshold topic predicted threshold tuned independently topic
development set empirical present section obtained manner


fil earning ocument l evel emantic p roperties f ree ext nnotations

figure summary reviews movie pirates caribbean worlds end p r ecis
summary documents list pros cons generated automatically system described generation numerical ratings
described snyder barzilay

evaluation summarization quality
model document analysis implemented p r ecis system performs single
multi document review summarization goal p r ecis provide users effective access
review data via mobile devices p r ecis contains information products services
ranging childcare products restaurants movies products system
contains collection reviews downloaded consumer websites epinions cnet
amazon p r ecis compresses data product short list pros cons
supported majority reviews example summary reviews movie
pirates caribbean worlds end shown figure contrast traditional multidocument summarizers output system sequence sentences rather list
phrases indicative product properties summarization format follows format pros cons
summaries individual reviewers provide multiple consumer websites moreover brevity
summary particularly suitable presenting small screens mobile
devices
automatically generate combined pros cons list product service first apply
model review model trained independently product domain e g movies
corresponding subset reviews free text annotations annotations provide
set keyphrases contribute clusters associated product properties
p r ecis accessible http groups csail mit edu rbg projects precis



fib ranavan c hen e isenstein barzilay

model trained labels review set properties since set possible properties
reviews product comparison among reviews straightforward
property count number reviews support select property part
summary supported majority reviews set semantic properties converted
pros cons list presenting common keyphrase property
aggregation technology applicable two scenarios system applied unannotated reviews inducing semantic properties document text conforms traditional way learning systems applied unlabeled data however model
valuable even individual reviews include pros cons keyphrase annotations due
high degree paraphrasing direct comparison keyphrases challenging see section
inferring clustering keyphrases model permits comparison keyphrase annotations
semantic level
remainder section provides set evaluations ability capture
semantic content document text keyphrase annotations section describes evaluation
systems ability extract meaningful semantic summaries individual documents
assesses quality paraphrase structure induced model section extends
evaluation systems ability summarize multiple review documents
single document evaluation
first evaluate model respect ability reproduce annotations present individual documents document text compare wide variety baselines
variations model demonstrating appropriateness task addition
explicitly evaluate quality paraphrase structure induced model comparing
gold standard clustering keyphrases provided expert annotators
e xperimental etup
section describe datasets evaluation techniques used experiments
system automatic methods comment hyperparameters tuned
model sampling initialized
statistic
reviews
avg review length
avg keyphrases review

restaurants




cell phones




digital cameras




table statistics datasets used evaluations
data sets evaluate system reviews three domains restaurants cell phones
digital cameras reviews downloaded epinions website used user authored
pros cons associated reviews keyphrases see section statistics datasets
provided table domains selected documents training
consider two strategies constructing test data first consider evaluating semantic
properties inferred system expert annotations semantic properties present
document end use expert annotations originally described section test



fil earning ocument l evel emantic p roperties f ree ext nnotations

set reiterate annotations reviews restaurant domain
hold development set review texts annotated six properties according
standardized guidelines strategy enforces consistency completeness ground truth
annotations differentiating free text annotations
unfortunately ability evaluate expert annotations limited cost producing annotations expand evaluation domains use author written keyphrase
annotations present original reviews annotations noisy presence
property annotation document strong evidence document supports property
inverse necessarily true lack annotation necessarily imply
respective property hold e g review good service related keyphrase may
still praise service body document
experiments free text annotations overcome pitfall restricting evaluation predictions individual properties documents annotated
property antonym instance evaluating prediction good service property
select documents annotated good service bad service related
keyphrases reason semantic property evaluated unique subset documents details development test sets presented appendix
ensure free text annotations reliably used evaluation compare
produced expert annotations whenever possible shown section free text
evaluations produce cohere well obtained expert annotations suggesting
labels used reasonable proxy expert annotation evaluations
evaluation methods first evaluation leverages expert annotations described section
one complication expert annotations marked level semantic properties
model makes predictions appropriateness individual keyphrases address
representing expert annotation commonly observed keyphrase
manually annotated cluster keyphrases associated semantic property example
annotation semantic property good food represented common keyphrase realization great food evaluation checks whether keyphrase within clusters
keyphrases predicted model
evaluation author free text annotations similar evaluation expert
annotations case annotation takes form individual keyphrases rather semantic
properties noted author generated keyphrases suffer inconsistency obtain consistent
evaluation mapping author generated keyphrase cluster keyphrases determined
expert annotator selecting common keyphrase realization
cluster example author may use keyphrase tasty maps semantic cluster
good food select common keyphrase realization great food expert
evaluation check whether keyphrase within clusters predicted model
model performance quantified recall precision f score computed
standard manner representative keyphrase predictions compared
corresponding references approximate randomization yeh noreen used
statistical significance testing test repeatedly performs random swaps individual
expert annotations available http groups csail mit edu rbg code precis
determination made mapping author keyphrases properties expert generated gold standard
clustering keyphrases much cheaper produce expert clustering keyphrases obtain expert
annotations semantic properties every document



fib ranavan c hen e isenstein barzilay

candidate system checks whether resulting performance gap remains least
large use test valid comparing nonlinear functions random variables f scores unlike common methods sign test previous work
used test include evaluations message understanding conference chinchor lewis
hirschman chinchor recently riezler maxwell advocated
use evaluating machine translation systems
parameter tuning initialization improve convergence rate perform two
initialization steps gibbs sampler first sampling done keyphrase clustering
component model ignoring document text second fix clustering sample
remaining model parameters two steps run iterations full joint model
sampled iterations inspection parameter estimates confirms model convergence ghz dual core desktop machine multithreaded c implementation model
training takes two hours dataset
model needs provided number clusters k set k large enough
model learn effectively development set restaurant data set k cell
phones digital cameras k set respectively values tuned
development set however found long k large enough accommodate significant number keyphrase clusters additional account topics keyphrases
specific value k affect performance hyperparameters
adjusted development set performance though tuning extensive
previously mentioned obtain document properties examining probability mass
topic distribution assigned property probability threshold set property via
development set optimizing maximum f score
r esults
section report performance model comparing array increasingly
sophisticated baselines model variations first demonstrate learning clustering annotation keyphrases crucial accurate semantic prediction next investigate impact
paraphrasing quality model accuracy considering expert generated gold standard clustering keyphrases another comparison point consider alternative automatically computed
sources paraphrase information
ease comparison experiments shown table table
summary baselines model variations table
comparison simple baselines first evaluation compares model four nave
baselines four treat keyphrases independent ignoring latent paraphrase structure
random keyphrase supported document probability one half
baseline computed expectation rather actually run baseline
expected recall expectation select half correct
keyphrases precision average proportion annotations test set
number possible annotations test set size n properties property
requirement could conceivably removed modeling cluster indices drawn dirichlet
process prior



fil earning ocument l evel emantic p roperties f ree ext nnotations

random

keyphrase supported document probability one half

keyphrase text

keyphrase supported document appears verbatim text

keyphrase classifier

separate support vector machine classifier trained keyphrase
positive examples documents labeled author
keyphrase documents considered negative examples
keyphrase supported document keyphrases classifier returns
positive prediction

heuristic keyphrase
classifier

similar keyphrase classifier except heuristic methods used attempt reduce noise training documents specifically wish
remove sentences discuss keyphrases positive examples
heuristic removes positive examples sentences
word overlap given keyphrase

model cluster text

keyphrase supported document paraphrases appear
text paraphrasing keyphrase clusters

model cluster classifier

separate classifier trained cluster keyphrases positive examples documents labeled author keyphrase
cluster documents negative examples keyphrases
cluster supported document clusters classifier returns
positive prediction keyphrase clustering model

heuristic model cluster
classifier

similar model cluster classifier except heuristic methods used reduce noise training documents specifically wish remove
positive examples sentences discuss keyphrases
clusters heuristic removes positive examples sentences
word overlap keyphrases given cluster
keyphrase clustering model

gold cluster model

variation model clustering keyphrases fixed
expert created gold standard text modeling parameters learned

gold cluster text

similar model cluster text except clustering keyphrases according expert produced gold standard

gold cluster classifier

similar model cluster classifier except clustering keyphrases
according expert produced gold standard

heuristic gold cluster
classifier

similar heuristic model cluster classifier except clustering
keyphrases according expert produced gold standard

independent cluster model

variation model clustering keyphrases first learned
keyphrase similarity information separately text
resulting independent clustering fixed text modeling parameters learned variations key distinction full model
lack joint learning keyphrase clustering text topics

independent cluster text

similar model cluster text except clustering keyphrases
according independent clustering

independent cluster
classifier

similar model cluster classifier except clustering keyphrases
according independent clustering

heuristic independent
cluster classifier

similar heuristic model cluster classifier except clustering
keyphrases according independent clustering

table summary baselines variations model compared


fib ranavan c hen e isenstein barzilay

method

















model
random
keyphrase text
keyphrase classifier
heuristic keyphrase classifier
model cluster text
model cluster classifier
heuristic model cluster classifier
gold cluster model
gold cluster text
gold cluster classifier
heuristic gold cluster classifier
independent cluster model
independent cluster text
independent cluster classifier
heuristic independent cluster classifier

recall

















restaurants
prec f score

















table comparison property predictions made model series baselines
model variations restaurant domain evaluated expert semantic annotations
divided according experiment methods model
significantly better approximate randomization indicated
p p



fil earning ocument l evel emantic p roperties f ree ext nnotations

method

















model
random
keyphrase text
keyphrase classif
heur keyphr classif
model cluster text
model cluster classif
heur model classif
gold cluster model
gold cluster text
gold cluster classif
heur gold classif
indep cluster model
indep cluster text
indep cluster classif
heur indep classif

recall

















restaurants
prec f score

















recall

















cell phones
prec f score

















digital cameras
recall prec f score

















table comparison property predictions made model series baselines
model variations three product domains evaluated author free text annotations divided according experiment methods
model significantly better approximate randomization indicated
p p methods perform significantly better
model p indicated



fib ranavan c hen e isenstein barzilay

p
ni
appears ni times expected precision
mn instance restaurants
gold standard evaluation six tested properties appeared total times
documents yielding expected precision
keyphrase text keyphrase supported document appears verbatim
text precision high recall low model unable detect
paraphrases keyphrase text instance first review figure
cleanliness would supported appears text however healthy would
supported even though synonymous great nutrition appear
keyphrase classifier separate discriminative classifier trained keyphrase positive examples documents labeled author keyphrase documents considered negative examples consequently particular keyphrase
documents labeled synonymous keyphrases would among negative examples
keyphrase supported document keyphrases classifier returns positive prediction
use support vector machines built svmlight joachims features
model e word counts partially circumvent imbalanced positive negative
data tuned prediction thresholds development set maximize f score
manner tuned thresholds model
heuristic keyphrase classifier baseline similar keyphrase classifier attempts mitigate noise inherent training data specifically given
positive example document may contain text unrelated given keyphrase attempt
reduce noise removing positive examples sentences word
overlap given keyphrase keyphrase supported document keyphrases
classifier returns positive prediction
lines tables present gold annotations original
authors annotations testing model outperforms three baselines evaluations
strong statistical significance
keyphrase text baseline fares poorly f score random baseline three
four evaluations expected recall baseline usually low requires
keyphrases appear verbatim text precision somewhat better presence
significant number false positives indicates presence keyphrase text
necessarily reliable indicator associated semantic property
interestingly one domain keyphrase text perform well digital cameras
believe prevalence specific technical terms keyphrases used
domain zoom battery life technical terms frequently used
review text making recall keyphrase text substantially higher domain
evaluations
note classifier reported initial publication branavan chen eisenstein barzilay
obtained default parameters maximum entropy classifier tuning classifiers parameters allowed us
significantly improve performance classifier baselines
general svms additional advantage able incorporate arbitrary features sake
comparison restrict features across methods
thank reviewer suggesting baseline



fil earning ocument l evel emantic p roperties f ree ext nnotations

keyphrase classifier baseline outperforms random keyphrase text baselines
still achieves consistently lower performance model four evaluations notably
performance heuristic keyphrase classifier worse keyphrase classifier except one case
alludes difficulty removing noise inherent document text
overall indicate methods learn predict keyphrases without accounting intrinsic hidden structure insufficient optimal property prediction leads us
toward extending present baselines clustering information
important assess consistency evaluation free text annotations table evaluation uses expert annotations table absolute scores
expert annotations dataset lower scores free text annotations ordering performance automatic methods across two evaluation scenarios
consistency maintained rest experiments well indicating purpose
relative comparison different automatic methods method evaluating
free text annotations reasonable proxy evaluation expert generated annotations
comparison clustering approaches previous section demonstrates
model outperforms baselines account paraphrase structure keyphrases
ask whether possible enhance baselines performance augmenting
keyphrase clustering induced model specifically introduce three systems none
true baselines since use information inferred model
model cluster text keyphrase supported document paraphrases
appears text paraphrasing clustering keyphrases
use paraphrasing information enhances recall potential cost precision depending
quality clustering example assuming healthy great nutrition
clustered together presence healthy text would indicate support great
nutrition vice versa
model cluster classifier separate discriminative classifier trained cluster
keyphrases positive examples documents labeled author keyphrase
cluster documents negative examples keyphrases cluster
supported document clusters classifier returns positive prediction keyphrase
clustering model keyphrase classifier use support vector machines trained word count features tune prediction thresholds individual cluster development set
another perspective model cluster classifier augments simplistic text modeling
portion model discriminative classifier discriminative training often considered powerful equivalent generative approaches mccallum et al
leading us expect high level performance system
heuristic model cluster classifier method similar model cluster classifier
additional heuristics used reduce noise inherent training data positive
example documents may contain text unrelated given cluster reduce noise
sentences word overlap clusters keyphrases removed
keyphrases cluster supported document clusters classifier returns positive prediction keyphrase clustering model


fib ranavan c hen e isenstein barzilay

lines tables present methods expected clustering
keyphrases baseline methods substantially improves recall low impact
precision model cluster text invariably outperforms keyphrase text recall keyphrase
text improved addition clustering information though precision worse cases
phenomenon holds even cameras domain keyphrase text already performs well
however model still significantly outperforms model cluster text evaluations
adding clustering information classifier baseline performance sometimes
better surprising model cluster classifier gains
benefit robust clustering learning sophisticated classifier assigning
properties texts resulting combined system complex model
potential yield better performance hand simple heuristic reduce
noise present training data consistently hurts performance classifier possibly
due reduction amount training data
overall enhanced performance methods contrast keyphrase baselines
aligned previous observations entailment dagan glickman magnini
confirming paraphrasing information contributes greatly improved performance semantic
inference tasks
impact paraphrasing quality previous section demonstrates one central
claims accounting paraphrase structure yields substantial improvements semantic inference noisy keyphrase annotations second key aspect
idea clustering quality benefits tying clusters hidden topics document
text evaluate claim comparing clustering independent clustering
baseline compare gold standard clustering produced expert human annotators test impact clustering methods substitute inferred clustering
alternative examine resulting semantic inferences change comparison
performed semantic inference mechanism model well model cluster
text model cluster classifier heuristic model cluster classifier baselines
add gold standard clustering model replace hidden variables correspond keyphrase clusters observed values set according gold standard clustering parameters trained modeling text model variation gold
cluster model predicts properties inference mechanism original model
baseline variations gold cluster text gold cluster classifier heuristic gold cluster classifier
likewise derived substituting automatically computed clustering gold standard clusters
additional clustering obtained keyphrase similarity information specifically modify original model learns keyphrase clustering isolation
text learns property language framework keyphrase clustering
entirely independent review text text modeling learned keyphrase
clustering fixed refer modification model independent cluster model
model treats document text mixture latent topics reminiscent
supervised latent dirichlet allocation slda blei mcauliffe labels acquired
performing clustering across keyphrases preprocessing step previous experiment introduce three baseline variations independent cluster text independent cluster
classifier heuristic independent cluster classifier
gold standard clustering created part evaluation procedure described section



fil earning ocument l evel emantic p roperties f ree ext nnotations

lines tables present experiments gold cluster model
produces f scores comparable original model providing strong evidence clustering
induced model sufficient quality semantic inference application expertgenerated clustering baselines lines yields less consistent overall
evaluation provides little reason believe performance would substantially improved
obtaining clustering closer gold standard
independent cluster model consistently reduces performance respect full joint
model supporting hypothesis joint learning gives rise better prediction independent
clustering baselines independent cluster text independent cluster classifier heuristic independent cluster classifier lines worse counterparts use model
clustering lines observation leads us conclude expert annotated
clustering improve independent clustering degrades
supports view joint learning clustering text important prerequisite
better property prediction
clustering
model clusters
independent clusters

restaurants



cell phones



digital cameras



table rand index scores clusters learned keyphrases text jointly compared clusters learned keyphrase similarity evaluation cluster quality
gold standard clustering

another way assessing quality automatically obtained keyphrase clustering
quantify similarity clustering produced expert annotators purpose
use rand index rand measure cluster similarity measure varies zero
one higher scores indicating greater similarity table shows rand index scores
full joint clustering well clustering obtained independent cluster model
every domain joint inference produces overall clustering improves upon keyphrasesimilarity scores confirm joint inference across keyphrases
document text produces better clustering considering features keyphrases alone
summarizing multiple reviews
last experiment examines multi document summarization capability system
study ability aggregate properties across set reviews compared baselines
aggregate directly free text annotations
data e valuation
selected restaurants five user written reviews restaurant ten annotators
asked annotate reviews five restaurants comprising reviews per annotator
used six salient properties annotation guidelines previous restaurant
annotation experiment see section constructing ground truth label properties
supported least three five reviews



fib ranavan c hen e isenstein barzilay

method
model
keyphrase aggregation
model cluster aggregation
gold cluster aggregation
indep cluster aggregation

recall






prec






f score






table comparison aggregated property predictions made model series
baselines use free text annotations methods model significantly better approximate randomization indicated p

make property predictions set reviews model baselines
presented automatic methods register prediction system judges
property supported least two five reviews recall precision f score
computed aggregate predictions six salient properties marked annotators
aggregation pproaches
evaluation run trained version model described section note
keyphrases provided model though provided baselines
obvious baseline summarizing multiple reviews would directly aggregate
free text keyphrases annotations presumably representative reviews semantic
properties unlike review text keyphrases matched directly first
baseline applies notion directly
keyphrase aggregation keyphrase supported restaurant least two five
reviews annotated verbatim keyphrase
simple aggregation obvious downside requiring strict matching independently authored reviews reason consider extensions aggregation
allow annotation paraphrasing
model cluster aggregation keyphrase supported restaurant least two
five reviews annotated keyphrase one paraphrases paraphrasing
according inferred clustering
gold cluster aggregation model cluster aggregation expert generated
clustering paraphrasing
independent cluster aggregation model cluster aggregation clustering
learned keyphrase similarity paraphrasing
three corroborating reviews required baseline systems produce positive predictions leading
poor recall setting presented appendix b



fil earning ocument l evel emantic p roperties f ree ext nnotations

r esults
table compares baselines model model outperforms annotationbased baselines despite access keyphrase annotations notably keyphrase aggregation performs poorly makes predictions requirement
exact keyphrase string match inclusion keyphrase clusters improves performance baseline however incompleteness keyphrase annotations see
section explains recall scores still low compared model incorporating
document text model obtains dramatically improved recall cost reduced precision
ultimately yielding significantly improved f score
demonstrate review summarization benefits greatly joint model
review text keyphrases nave approaches consider keyphrases yield inferior
even augmented paraphrase information

conclusions future work
shown free text keyphrase annotations provided novice users
leveraged training set document level semantic inference free text annotations
potential vastly expand set training data available developers semantic inference
systems however shown suffer lack consistency completeness
overcome inducing hidden structure semantic properties correspond
clusters keyphrases hidden topics text takes form
hierarchical bayesian model addresses text keyphrases jointly
model implemented system successfully extracts semantic properties unannotated restaurant cell phone camera reviews empirically validating experiments demonstrate necessity handling paraphrase structure free text keyphrase
annotations moreover better paraphrase structure learned joint framework
document text outperforms competitive baselines semantic
property extraction single multiple documents permits aggregation across
multiple keyphrases different surface forms multi document summarization
work extends actively growing literature document topic modeling topic modeling paraphrasing posit hidden layer captures relationship disparate surface
forms topic modeling set latent distributions lexical items paraphrasing
represented latent clustering phrases two latent structures linked
resulting increased robustness semantic coherence
see several avenues future work first model draws substantial power features measure keyphrase similarity ability use arbitrary similarity metrics desirable
however representing individual similarity scores random variables compromise
clearly independent believe could avoided modeling generation
entire similarity matrix jointly
related would treat similarity matrix across keyphrases indicator
covariance structure model would learn separate language keyphrase
keyphrases rated highly similar would constrained induce similar language
might possible gaussian process framework rasmussen
williams



fib ranavan c hen e isenstein barzilay

currently focus model identify semantic properties expressed given
document allows us produce summary properties however mentioned
section human authors give equal importance properties producing summary
pros cons one possible extension work would explicitly model likelihood
topic annotated document might avoid current post processing step
uses property specific thresholds compute final predictions model output
finally assumed semantic properties unstructured reality
properties related interesting ways trivially domain reviews would desirable
model antonyms explicitly e g restaurant review simultaneously labeled
good bad food relationships properties hierarchical structures could
considered suggests possible connections correlated topic model blei
lafferty

bibliographic note
portions work previously presented conference publication branavan et al
current article extends work several ways notably development evaluation
multi document review summarization system uses semantic properties induced
method section detailed analysis distributional properties free text annotations
section expansion evaluation include additional domain sets baselines
considered original section

acknowledgments
authors acknowledge support national science foundation nsf career grant iis microsoft faculty fellowship u office naval
onr quanta computer nokia corporation harr chen supported national defense science engineering nsf graduate fellowships thanks michael collins zoran
dzunic amir globerson aria haghighi dina katabi kristian kersting terry koo yoong keok
lee brian milch tahira naseem dan roy christina sauper benjamin snyder luke zettlemoyer
journal reviewers helpful comments suggestions thank marcia davidson
members nlp group mit help expert annotations opinions findings
conclusions recommendations expressed article authors necessarily reflect views nsf microsoft onr quanta nokia



fil earning ocument l evel emantic p roperties f ree ext nnotations

appendix development test set statistics
table lists semantic properties domain number documents used
evaluating properties noted section gold standard evaluation
complete testing every property document conversely free text evaluations
property use documents annotated property antonym
number documents differs semantic property
domain
restaurants gold
restaurants

cell phones

cameras

property
properties
good food
bad food
good price
bad price
good service
bad service
good reception
bad reception
good battery life
poor battery life
good price
bad price
small
large
good price
bad price
good battery life
poor battery life
great zoom
limited zoom

development documents


test documents










































table breakdown property development test sets used evaluations section



fib ranavan c hen e isenstein barzilay

appendix b additional multiple review summarization
table lists multi document experiment variation aggregation
require automatic method predict property three five reviews predict
property product rather two presented section baseline systems
change causes precipitous drop recall leading f score substantially worse
presented section contrast f score model consistent across
evaluations
method
model
keyphrase aggregation
model cluster aggregation
gold cluster aggregation
indep cluster aggregation

recall






prec






f score






table comparison aggregated property predictions made model series
baselines use free text annotations aggregation requires three five reviews
predict property rather two section methods
model significantly better approximate randomization indicated
p

appendix c hyperparameter settings
table lists values hyperparameters used experiments domain
values arrived tuning development set cases set
making beta uniform distribution
hyperparameters




restaurants




cell phones




cameras




table values hyperparameters used domain across experiments



fil earning ocument l evel emantic p roperties f ree ext nnotations

references
barzilay r mckeown k elhadad information fusion context multidocument summarization proceedings acl pp
barzilay r mckeown k r extracting paraphrases parallel corpus proceedings acl pp
bhattacharya getoor l latent dirichlet model unsupervised entity resolution
proceedings siam international conference data mining
blei lafferty j correlated topic advances nips pp
blei mcauliffe j supervised topic advances nips pp
blei ng jordan latent dirichlet allocation journal machine
learning
boyd graber j blei zhu x topic model word sense disambiguation
proceedings emnlp pp
branavan r k chen h eisenstein j barzilay r learning document level semantic properties free text annotations proceedings acl pp
carbonell j goldstein j use mmr diversity reranking reordering
documents producing summaries proceedings acm sigir pp
chinchor n statistical significance muc proceedings th conference
message understanding pp
chinchor n lewis hirschman l evaluating message understanding systems
analysis third message understanding conference muc computational linguistics
cohen j coefficient agreement nominal scales educational psychological
measurement
dagan glickman magnini b pascal recognising textual entailment challenge lecture notes computer science
elhadad n mckeown k r towards generating patient specific summaries medical
articles proceedings naacl workshop automatic summarization pp
finkel j r grenager manning c incorporating non local information information extraction systems gibbs sampling proceedings acl pp
gelman carlin j b stern h rubin b bayesian data analysis nd edition
texts statistical science chapman hall crc
goldwater griffiths l johnson contextual dependencies unsupervised
word segmentation proceedings acl pp
hu liu b mining summarizing customer reviews proceedings sigkdd
pp
joachims making large scale support vector machine learning practical pp
mit press



fib ranavan c hen e isenstein barzilay

kim hovy e automatic identification pro con reasons online reviews
proceedings coling acl pp
lin pantel p discovery inference rules question answering natural language
engineering
liu b hu cheng j opinion observer analyzing comparing opinions
web proceedings www pp
lu zhai c opinion integration semi supervised topic modeling proceedings www pp
mani bloedorn e multi document summarization graph search matching
proceedings aaai pp
marsi e krahmer e explorations sentence fusion proceedings european
workshop natural language generation pp
mccallum bellare k pereira f conditional random field discriminativelytrained finite state string edit distance proceedings uai pp
nenkova vanderwende l mckeown k compositional context sensitive multidocument summarizer exploring factors influence summarization proceedings
sigir pp
noreen e computer intensive methods testing hypotheses introduction john
wiley sons
popescu nguyen b etzioni opine extracting product features opinions reviews proceedings hlt emnlp pp
purver kording k p griffiths l tenenbaum j b unsupervised topic modelling multi party spoken discourse proceedings coling acl pp
radev jing h budzikowska centroid summarization multiple documents sentence extraction utility evaluation user studies proceedings
anlp naacl summarization workshop
radev mckeown k generating natural language summaries multiple line
sources computational linguistics
rand w objective criteria evaluation clustering methods journal
american statistical association
rasmussen c e williams c k gaussian processes machine learning mit
press
riezler maxwell j pitfalls automatic evaluation significance
testing mt proceedings acl workshop intrinsic extrinsic evaluation
measures machine translation summarization pp
snyder b barzilay r multiple aspect ranking good grief
proceedings naacl hlt pp
titov mcdonald r joint model text aspect ratings sentiment summarization proceedings acl pp



fil earning ocument l evel emantic p roperties f ree ext nnotations

titov mcdonald r b modeling online reviews multi grain topic
proceedings www pp
toutanova k johnson bayesian lda model semi supervised part ofspeech tagging advances nips pp
white korelsky cardie c ng v pierce wagstaff k multi document
summarization via information extraction proceedings hlt pp
yeh accurate tests statistical significance differences proceedings coling pp
zaenen mark barking wrong tree computational linguistics




