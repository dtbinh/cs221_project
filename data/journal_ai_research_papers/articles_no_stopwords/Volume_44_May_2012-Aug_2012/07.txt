Journal Artificial Intelligence Research 44 (2012) 491-532

Submitted 11/11; published 07/12

Riffled Independence Efficient Inference
Partial Rankings
Jonathan Huang

jhuang11@stanford.edu

James H. Clark Center
Stanford University, Stanford CA 94305, USA

Ashish Kapoor

akapoor@microsoft.com

Microsoft Research
One Microsoft Way
Redmond WA 98052-6399, USA

Carlos Guestrin

guestrin@cs.cmu.edu

Gates Hillman Complex, Carnegie Mellon University,
5000 Forbes Avenue, Pittsburgh, PA 15213, USA

Abstract
Distributions rankings used model data multitude real world settings
preference analysis political elections. Modeling distributions presents
several computational challenges, however, due factorial size set rankings
item set. challenges quite familiar artificial intelligence
community, compactly represent distribution combinatorially large
space, efficiently perform probabilistic inference representations.
respect ranking, however, additional challenge refer
human task complexity users rarely willing provide full ranking long list
candidates, instead often preferring provide partial ranking information.
Simultaneously addressing challenges i.e., designing compactly representable model amenable efficient inference learned using partial
ranking data difficult task, necessary would scale problems
nontrivial size. paper, show recently proposed riffled independence
assumptions cleanly efficiently address challenges. particular,
establish tight mathematical connection concepts riffled independence
partial rankings. correspondence allows us develop efficient
exact algorithms performing inference tasks using riffled independence based representations partial rankings, somewhat surprisingly, shows efficient inference
possible riffle independent models (in certain sense) observations
take form partial rankings. Finally, using inference algorithm, introduce
first method learning riffled independence based models partially ranked data.

1. Probabilistic Modeling Ranking Data: Three Challenges
Rankings arise number machine learning application settings preference analysis movies books (Lebanon & Mao, 2008) political election analysis (Gormley
& Murphy, 2007; Huang & Guestrin, 2010). many problems, great interest
build statistical models ranking data order make predictions, form recommendations, discover latent trends structure construct human-comprehensible data
summaries.
c
2012
AI Access Foundation. rights reserved.

fiHuang, Kapoor & Guestrin

Modeling distributions rankings difficult problem, however, due fact
number items ranked increases, number possible rankings increases
factorially. combinatorial explosion forces us confront three central challenges
dealing rankings. First, need deal storage complexity compactly represent distribution space rankings?1 algorithmic complexity efficiently answer probabilistic inference queries given distribution?
Finally, must contend refer human task complexity,
challenge stemming fact difficult accurately elicit full ranking
large list candidates human user; choosing list n! options easy task
users typically prefer provide partial information. Take American Psychological
Association (APA) elections, example, allow voters rank order candidates
favorite least favorite. 1980 election, five candidates, therefore
5! = 120 ways rank five candidates. Despite small candidate list, voters
election preferred specify top-k favorite candidates rather writing
full rankings ballots (see Figure 1). example, roughly third voters
simply wrote single favorite candidate 1980 election.
three intertwined challenges storage, algorithmic, human task complexity
central issues probabilistic modeling rankings, models efficiently
handle three sources complexity limited applicability. paper, examine
flexible intuitive class models rankings based generalization probabilistic
independence called riffled independence, proposed recent work (Huang & Guestrin,
2009, 2010). previous papers focused primarily representational (storage
complexity) issues, concentrate inference incomplete observations (i.e., partial
rankings), showing addition storage complexity, riffle independence based models
efficiently address issues algorithmic human task complexity.
fact two issues algorithmic human task complexity intricately linked
riffle independent models. considering partial rankings, give users flexibility
provide much little information care give. context partial
ranking data, relevant inference queries take form partial rankings.
example, might want predict voters second choice candidate given information
first choice. One main contributions paper show inference
partial ranking queries performed particularly efficiently riffle independent
models.
main contributions work follows:2
reveal natural fundamental connection riffle independent models
partial rankings. particular, show collection partial rankings
item set form complete characterization space observations upon
1. Note common wonder one would care represent distribution rankings
number sample rankings never nearly large. problem number samples always
much smaller n! however, means rankings never observed, limiting ability
estimate probability arbitrary ranking. way overcome paucity samples
exploit representational structure, much alignment solving storage complexity
issue.
2. paper extended presentation paper (Huang, Kapoor, & Guestrin, 2011) appeared
2011 Conference Uncertainty Artificial Intelligence (UAI) well results first
authors dissertation (Huang, 2011).

492

fiEfficient Inference Partial Rankings

First
Choice

Second
Choice

Third
Choice

Fourth
Choice

Fifth
Choice

#
votes

5

3

4

2

1

37

3

4

5

1

2

30

1

2

3

---

---

27

3

---

---

---

---

1198

4

1

3

---

---

15

1

3

---

---

---

302

3

1

2

5

4

186

Figure 1: Example partial ranking data (taken American Psychological Association
election dataset, 1980)

one efficiently condition riffle independent model. result, show
ranked items satisfy riffled independence relationship, conditioning
partial rankings done efficiently, running time O(n|H|), |H| denotes
number model parameters.
prove that, sense (which formalize), impossible efficiently condition
riffle independent models observations take form partial rankings.
propose first algorithm capable efficiently estimating structure
parameters riffle independent models heterogeneous collections partially
ranked data.
show results real voting preference data evidencing effectiveness
methods.

2. Riffled Independence Rankings
ranking, , items item set one-to-one mapping rank
set R = {1, . . . , n} denoted using vertical bar notation 1 (1)| 1 (2)| . . . | 1 (n).
say ranks item i1 (or over) item i2 rank i1 less
rank i2 . example, might {Corn, P eas, Apples, Oranges} ranking
Corn|P eas|Apples|Oranges encodes preference Corn Peas turn preferred Apples on. collection possible rankings item set denoted
(or Sn implicit).
Since n! rankings n items, intractable estimate even explicitly
represent arbitrary distributions Sn without making structural assumptions
underlying distribution. many possible simplifying assumptions one
make, focus approach proposed recent papers (Huang & Guestrin,
2009, 2010) ranks items assumed satisfy intuitive generalized notion
probabilistic independence known riffled independence. paper, argue
riffled independence assumptions particularly effective settings one would
make queries taking form partial rankings. remainder section,
review riffled independence.
493

fiHuang, Kapoor & Guestrin

riffled independence assumption posits rankings item set generated independently generating rankings smaller disjoint item subsets (say,
B) partition , piecing together full ranking interleaving (or riffle shuffling)
smaller rankings together. example, rank item set foods, one might first
rank vegetables fruits separately, interleave two subset rankings form
full ranking. formally define riffled independence, use notions relative rankings
interleavings.
Definition 1 (Relative ranking map). Given ranking subset ,
relative ranking items A, (), ranking, SA , (i) < (j)
(i) < (j).
Definition 2 (Interleaving map). Given ranking partition disjoint
sets B, interleaving B (denoted, AB ()) (binary) mapping
rank set R = {1, . . . , n} {A, B} indicating whether rank occupied
B. rankings, denote interleaving ranking vertical bar notation:
[AB ()](1)|[AB ()](2)| . . . |[AB ()](n).
Example 3. Consider partitioning item set vegetables = {Corn, P eas}
fruits B = {Apples, Oranges}, well full ranking four items: =
Corn|Oranges|P eas|Apples. case, relative ranking vegetables () =
Corn|P eas relative ranking fruits B () = Oranges|Apples. interleaving vegetables fruits AB () = A|B|A|B.
Definition 4 (Riffled Independence). Let h distribution consider subset
items complement B. sets B said riffle independent
h decomposes (or factors) as:
h() = mAB (AB ()) fA (A ()) gB (B ()),
distributions mAB , fA gB , defined interleavings relative rankings
B respectively. words, B riffle independent relative rankings
B, well interleaving mutually independent. refer mAB
interleaving distribution fA gB relative ranking distributions.
Riffled independence found approximately hold number real datasets
(Huang & Guestrin, 2012). relationships identified data, instead
exhaustively representing n! ranking probabilities, one represent factors
mAB , fA gB , distributions smaller sets.
2.1 Hierarchical Riffle Independent Models
relative ranking factors fA gB distributions rankings.
reduce parameter space, natural consider hierarchical decompositions item sets
nested collections partitions (like hierarchical clustering). example, Figure 2.1
shows hierarchical decomposition vegetables riffle independent fruits among
healthy foods, healthy foods are, turn, riffle independent subset
desserts: {Doughnuts, &M s}.
494

fiEfficient Inference Partial Rankings

{C,P,A,O,D,M}

{D,M}

{C,P,A,O}

Doughnuts, M&Ms

{C,P}

{A,O}

Corn, Peas

Apples, Oranges

Figure 2: example hierarchy six food items.
simplicity, restrict consideration binary hierarchies, defined tuples
form H = (HA , HB ), HA HB either (1) null, case H called leaf,
(2) hierarchies item sets B respectively. second case, B
assumed form nontrivial partitioning item set.
Definition 5. say distribution h factors riffle independently respect
hierarchy H = (HA , HB ) item sets B riffle independent respect h,
fA gB factor riffle independently respect subhierarchies HA HB ,
respectively.
Bayesian networks, hierarchies represent families distributions obeying
certain set (riffled) independence constraints parameterized locally. draw
model, one generates full rankings recursively starting drawing rankings
leaf sets, working tree, sequentially interleaving rankings reaching
root. parameters hierarchical models simply interleaving relative
ranking distributions internal nodes leaves hierarchy, respectively.
general, number total parameters required represent hierarchical riffle
independent model (as Bayesian networks) still scale exponentially number

items. example, number interleavings p items n p items np .
often case however, much fewer parameters necessary. example, thin
models (Huang & Guestrin, 2012), number items factored model
stage hierarchy never small constant k, always represented
(degree k) polynomial number parameters. use |H| refer number
parameters necessary representing distribution factors according hierarchy
H.
decomposing distributions rankings small pieces (like Bayesian networks
done distributions), hierarchical models allow better interpretability,
efficient probabilistic representation, low sample complexity, efficient MAP optimization,
and, show paper, efficient inference.
Example 6. Figure 3(a), reproduce hierarchical structure learned using
fully ranked subset APA data consisting 5000 training examples Huang
Guestrin (2012). five candidates election: (1) William Bevan, (2) Ira
Iscoe, (3) Charles Kiesler, (4) Max Siegle, (5) Logan Wright (Marden, 1995). Strikingly,
structure learned using algorithm (maximum likelihood) knows nothing
underlying politics APA, leaf nodes correspond exactly
political coalitions dominated APA 1980 election research psychologists
495

fiHuang, Kapoor & Guestrin

A={12345}

B={1345}

C={2}
Community
psychologists

mB,C()
.14

B|C|B|B|B

.19

B|B|C|B|B

.25

B|B|B|C|B

.25

B|B|B|B|C

.18



mD,E()



fC()

D|D|E|E

.28

2

1.00

D|E|D|E

.12

D|E|E|D

.12

D={13}

E={45}

E|D|D|E

.14

Research
psychologists

Clinical
psychologists

E|D|E|D

.12

E|E|D|D

.22

(a) Hierarchical structure learned
via MLE using 5000 full rankings
APA dataset.


C|B|B|B|B



fD()



fE()

1|3

.50

4|5

.48

3|1

.50

5|4

.52

(b) Riffle independent model parameters learned via MLE using
5000 full rankings APA dataset.

Figure 3: Example hierarchical model APA election. Candidates enumerated
as: (1) William Bevan, (2) Ira Iscoe, (3) Charles Kiesler, (4) Max Siegle, (5)
Logan Wright (Marden, 1995).

(candidates 1 3), clinical psychologists (candidates 4 5), community
psychologists (candidate 2).
Figure 3(b), plot corresponding parameter distributions learned via
maximum likelihood. three relative ranking distributions, corresponding
political party, well two interleaving distributions (one interleaving research
clinical psychologists, one interleaving community psychologist
remaining candidates). Since parameter distribution constrained sum 1,
total 11 free parameters.
2.2 Model Estimation
paper estimate riffle independent models based methods introduced
earlier work. Given hierarchial structure model, maximum likelihood parameter
estimates hierarchical riffle independent model straightforward compute via frequency estimates. estimate correct structure model challenging
problem. key insight lies noticing two subsets B riffle independent,
j, k B, independence relation (i) ((j) < (k)) must hold.
structure learning algorithms operate hunting tripletwise independence
relations within data. defer interested readers details (Huang & Guestrin,
2012).
496

fiEfficient Inference Partial Rankings

Note earlier work, assumed algorithms access dataset
consisting i.i.d. full rankings provided users. current work, relax
assumptions allowing users provide partially ranked data. One assumption throughout, however, user full ranking mind items. particular,
current work address incomplete ranking problem, users might
seen items (we discuss possible extensions incomplete ranking setting
Section 9.

3. Decomposable Observations
Given prior distribution, h, rankings observation O, Bayes rule tells us
posterior distribution, h(|O), proportional L(O|) h(), L(O|)
likelihood function. operation conditioning h observation typically computationally intractable since requires multiplying two n! dimensional functions, unless
one exploit structural decompositions problem. section, describe decomposition certain class likelihood functions space rankings
observations factored simpler parts. observation decomposable
way, show one efficiently condition riffle independent prior distribution
O. simplicity paper, focus primarily subset observations whose likelihood
functions encode membership subset rankings Sn .
Definition 7 (Subset observations). subset observation binary observation whose
likelihood proportional indicator function subset Sn i.e.,

1
.
L(O|) =
0 otherwise
running example, consider class first place observations throughout
chapter (we consider far general observation models later sections). first
place observation =Corn ranked first, example, associated collection
rankings placing item Corn first place (O = { : (Corn) = 1}). interested
computing posterior h(| O). Thus first place scenario, given voters
top choice would infer preferences remaining candidates.
Given partitioning item set two subsets B, sometimes possible
decompose (or factor ) subset observation involving items smaller subset observations involving A, B interleavings B independently. decompositions
often exploited efficient inference.
Example 8.
Consider first place observation
= Corn ranked first,
decomposed two independent observations observation
relative ranking Vegetables, observation interleaving Vegetables
Fruits:
497

fiHuang, Kapoor & Guestrin

OA = Corn ranked first among Vegetables,
OA,B = First place occupied Vegetable.
condition case, one updates relative ranking distribution
Vegetables (A) zeroing rankings vegetables place Corn first
place, updates interleaving distribution zeroing interleavings
place Vegetable first place, normalizes resulting distributions.
example nondecomposable observation observation
= Corn third place.
see decompose (with respect Vegetables Fruits), enough
notice interleaving Vegetables Fruits independent relative
ranking Vegetables. If, example, element interleaves (Vegetables)
B (Fruits) AB () = A|B|A|B, since (Corn) = 3, relative ranking
Vegetables constrained () = P eas|Corn. Since interleavings
relative rankings independent, see cannot decomposable.
Formally, use riffle independent factorizations define decomposability respect
hierarchy H item set.
Definition 9 (Decomposability). Given hierarchy H item set, subset observation decomposes respect H likelihood function L(O|) factors riffle
independently respect H.
subset observations prior decompose according hierarchy,
show (as Example 8) posterior decomposes.
Proposition 10. Let H hierarchy item set. Given prior distribution h
subset observation decompose respect H, posterior distribution
h(|O) factors riffle independently respect H.
Proof. Denote likelihood function corresponding L (in proof,
matter assumed subset observation result holds arbitrary
likelihoods).
use induction size item set n = ||. base case n = 1 trivially
true. Next consider general case n > 1. posterior distribution, Bayes rule,
written h(|O) L() h(). two cases. H leaf node,
posterior h0 trivially factors according H, done. Otherwise, L h
factor, assumption, according H = (HA , HB ) following way:
L() = mL (AB ())fL (A ())gL (B ()), h() = mh (AB ())fh (A ())gh (B ()).
Multiplying grouping terms, see posterior factors as:
h(|O) = [mL mh ](AB ()) [fL fh ](A ()) [gL gh ](B ()).
show h(|O) factors respect H, need demonstrate (by Definition 5)
distributions [fL fh ] [gL gh ] (after normalizing) factor respect HA
498

fiEfficient Inference Partial Rankings

HB , respectively. Since fL fh factor according hierarchy HA assumption
|A| < n since H leaf, invoke inductive hypothesis show
posterior distribution, proportional fL fh must factor according HA .
Similarly, distribution proportional gL gh must factor according HB .

4. Complete Decomposability
condition Proposition 10, prior observation must decompose respect exactly hierarchy, sufficient one efficient inference, might
first glance seem restrictive render proposition useless practice. overcome
limitation hierarchy specific decomposability, explore special family observations (which call completely decomposable) property decomposability
depend specifically particular hierarchy, implying particular
observations, efficient inference always possible (provided efficient representation
prior distribution possible).
illustrate observation decompose respect multiple hierarchies
item set, consider first place observation =Corn ranked first. argued
Example 8 decomposable observation. Notice however decomposability
particular observation depend items partitioned
hierarchy. Specifically, instead Vegetables Fruits, sets = {Corn, Apples}
B = {P eas, Oranges} riffle independent, similar decomposition would continue
hold, decomposing observation relative ranking items (Corn
first among items A), observation interleaving B (First place
occupied element A).
formally capture notion observation decompose respect
arbitrary underlying hierarchies, define complete decomposability:
Definition 11 (Complete decomposability). say subset observation completely decomposable decomposes respect every possible hierarchy item
set . denote collection possible completely decomposable (subset) observations C. See Figure 4 illustration set C.
Conceptually, completely decomposable observations correspond indicator functions
riffle independent possible. Complete decomposability guarantee
observation one always exploit available factorized structure prior
distribution order efficiently condition O.
Proposition 12. Let H binary hierarchy item set. Given prior h
factorizes respect H, completely decomposable observation O, posterior
h(|O) decomposes respect H.
Proof. Proposition 12 follows simple corollary Proposition 10.
Example 13. simplest example completely decomposable observation uniform observation Ounif = , includes possible rankings corresponds
uniform indicator function unif rankings. Given hierarchy H, unif shown
decompose riffle independently respect H, factor uniform,
hence Ounif completely decomposable.
499

fiHuang, Kapoor & Guestrin

H1

H6

H2

Completely
Decomposable
Observations

H5

H3

H4

Figure 4: diagram illustrating collection completely decomposable observations, C.
shaded region (labeled Hi ) represents family subset observations
Sn decompose respect hierarchy Hi . collection C
seen intersection shaded regions, subset observations
lie inside intersection ones conditioning performed
linear time (in number model parameters).

uniform observation course particularly interesting context Bayesian
inference, hand, given stringent conditions Definition 11,
obvious nontrivial completely decomposable observations even exist. Nonetheless,
exist nontrivial examples (such first place observations), next
section, exhibit rich general class completely decomposable observations.

5. Complete Decomposability Partial Ranking Observations
section discuss mathematical problem fully characterizing class
completely decomposable observations. main contribution section show
completely decomposable observations correspond precisely partial rankings
item set.
Partial rankings. begin discussion introducing partial rankings, allow
items tied respect ranking dropping verticals vertical bar
representation .
Definition 14 (Partial ranking observation). Let 1 , 2 ,. . . , r ordered collection
subsets partition (i.e., = j = 6= j). partial ranking
observation 3 corresponding partition collection rankings rank items
3. remarked Ailon (2007), note term partial ranking used confused
two standard objects: (1) Partial order, namely, reflexive, transitive anti-symmetric binary

500

fiEfficient Inference Partial Rankings

items j < j. denote partial ranking 1 |2 | . . . |r say
type = (|1 |, |2 |, . . . , |r |). denote collection partial rankings
(over n items) P.
partial ranking defined viewed coset subgroup =
S1 S2 Sr . Given type full ranking , one
partial ranking type containing , thus therefore equivalently denote partial
ranking 1 |2 | . . . |r , element 1 |2 | . . . |r . Note coset
notation allows multiple rankings refer partial ranking .
space partial rankings defined captures rich natural class
observations. particular, partial rankings encompass number commonly occurring
special cases, traditionally modeled isolation, work (as well
recent works Lebanon & Lafferty, 2003; Lebanon & Mao, 2008) used
unified setting.
Example 15. Partial ranking observations include:
(First place, Top-1 observations): First place observations correspond partial
rankings type = (1, n 1). observation Corn ranked first
written Corn|Peas,Apples,Oranges.
(Top-k observations): Top-k observations partial rankings type = (1, . . . , 1, n
k). generalize first place observations specifying items mapping
first k ranks, leaving n k remaining items implicitly ranked behind. example,
observation Corn ranked first Peas ranked second written
Corn|Peas|Apples,Oranges.
(Desired/less desired dichotomy): Partial rankings type = (k, n k) correspond
subset k items preferred desired remaining subset n k items.
example, partial rankings type (k, n k) might arise approval voting
voters mark subset approved candidates, implicitly indicating disapproval
remaining n k candidates.
(Ratings): Finally, partial rankings come form rating data where,
example, restaurants rated as, ?, ??, ? ? ?. corresponding partial ranking
would thus tie restaurants rated number stars, ranking
restaurants stars restaurants fewer stars.
(Trivial observations): Partial rankings type = (n) refer trivial observations
whose likelihood functions uniform entire space rankings, . trivial
observation rankings item set = {Corn, P eas, Apples}, example,
simply written simply Corn, P eas, Apples.
show partial ranking observations decompose, exhibit explicit factorization respect hierarchy H items. simplicity, begin considering
single layer case, items partitioned two leaf sets B. factorization depends following notions consistency relative rankings interleavings
partial ranking.
relation; (2) ranking subset [which discuss Section 9 incomplete rankings].
search engines, example, although top-k elements returned, remaining n k
implicitly assumed ranked behind [and therefore, search engines return partial rankings].

501

fiHuang, Kapoor & Guestrin

Definition 16 (Restriction consistency). Given partial ranking = 1 |2 | . . . |r
subset , define restriction partial ranking items
obtained intersecting A. Hence restriction is:
[S ]A = 1 A|2 A| . . . |r A.
Given ranking, items A, say consistent partial ranking
member restriction A, [S ]A .
Definition 17 (Interleaving consistency). Given interleaving AB two sets A, B
partition , say AB consistent partial ranking = 1 | . . . |r (with
type ) first 1 entries AB contain number Bs 1 ,
second 2 entries AB contain number Bs 2 , on. Given
partial ranking , denote collection consistent interleavings [S ]AB .
example, consider partial ranking
= Corn, Apples|P eas, Oranges,
places single vegetable single fruit first two ranks, single vegetable
single fruit last two ranks. Alternatively, partially specifies interleaving
AB|AB. full interleavings A|B|B|A B|A|B|A consistent (by dropping
vertical lines) A|A|B|B consistent (since places two vegetables first two
ranks).
Using notions consistency partial ranking, show partial ranking
observations decomposable respect binary partitioning (i.e., single layer
hierarchy) item set.
Proposition 18 (Single layer hierarchy). partial ranking observation
binary partitioning item set (A, B), indicator function , , factors riffle
independently as:
() = mAB (AB ()) fA (A ()) gB (B ()),

(5.1)

factors mAB , fA gB indicator functions consistent interleavings
relative rankings, [S ]AB , [S ]A [S ]B , respectively.
single layer decomposition Proposition 18 turned recursive decomposition partial ranking observations arbitrary binary hierarchies, establishes
main result. particular, given partial ranking prior distribution
factorizes according hierarchy H, first condition topmost interleaving distribution zeroing parameters corresponding interleavings consistent
, normalizing distribution. need condition subhierarchies
HA HB relative rankings B consistent , respectively.
Since consistent sets, [S ]A [S ]B , partial rankings themselves,
algorithm conditioning partial ranking applied recursively
subhierarchies HA HB . precise, show that:
Theorem 19. Every partial ranking completely decomposable (P C).
502

fiEfficient Inference Partial Rankings

prcondition (Prior hprior , Hierarchy H, Observation = 1 |2 | . . . |r )
isLeaf(H)
forall

hprior ()
hpost ()
;
0
otherwise
Normalize (hpost ) ;
return (hpost );
else
forall

mprior ( ) [S ]AB
mpost ( )
;
0
otherwise
Normalize (mpost ) ;
f (A ) prcondition (fprior , HA , [S ]A ) ;
g(B ) prcondition (gprior , HB , [S ]B ) ;
return (mpost , fpost , gpost );

Algorithm 1: Pseudocode prcondition, algorithm recursively conditioning hierarchical riffle independent prior distribution partial ranking observations. See Definitions 16
17 [S ]A , [S ]B , [S ]AB . runtime prcondition O(n |H|), |H|
number model parameters. Input: parameter distributions prior hprior represented explicit tabular form, observation form partial ranking. Output:
parameter distributions posterior hpost represented explicit tabular form.

Since proof Theorem 19 fairly straight forward given form factorization (Equation 5.1), deferred Appendix. consequence Theorem 19
Proposition 12, conditioning partial ranking observations performed efficiently.
See Algorithm 1 details recursive conditioning algorithm.
running time complexity conditioning partial ranking? recursion
Algorithm 1 operates parameter distribution once, setting probabilities
interleavings relative rankings distribution either zero not,
normalizing. decide whether zero probability not, one must check partial
ranking consistency either interleaving relative ranking, requires
O(n) time. Therefore, total, Algorithm 1 requires O(n |H|) time, |H|
total number model parameters. Notice complexity conditioning depends
linearly complexity prior whenever prior distribution compactly
represented, efficient inference partial ranking observations possible.
stated Section 2, |H| general scale exponentially n, thin chain models,
number items factored model stage never
small constant k, verifying interleaving relative ranking consistency performed
constant time, implying conditioning operation linear number model
parameters, guaranteed polynomial n.
Example 20. example, consider conditioning APA distribution Example 6 observation Candidate 3 ranked first place,
represented partial ranking = 3|1, 2, 4, 5. Recall candidate 3 Charles
Kiesler, research psychologist.
Figure 5(a) show structure parameters prior distribution
APA election data, highlighting particular interleavings relative rankings
503

fiHuang, Kapoor & Guestrin



mB,C()



C|B|B|B|B

.14

C|B|B|B|B

mB,C()
0

B|C|B|B|B

.19

B|C|B|B|B

.22

B|B|C|B|B

.25

B|B|C|B|B

.29

B|B|B|C|B

.25

B|B|B|C|B

.29

B|B|B|B|C

.18

B|B|B|B|C

.21



mD,E()



fC()



mD,E()



fC()

D|D|E|E

.28

2

1.00

D|D|E|E

.54

2

1.00

D|E|D|E

.12

D|E|D|E

.23

D|E|E|D

.12

D|E|E|D

.23

E|D|D|E

.14

E|D|D|E

0

E|D|E|D

.12

E|D|E|D

0

E|E|D|D

.22

E|E|D|D

0



fD()



fE()



fD()



fE()

1|3

.50

4|5

.48

1|3

0

4|5

.48

3|1

.50

5|4

.52

3|1

1.00

5|4

.52

(a) Structure parameters prior distribution (with consistent relative rankings interleavings highlighted).

(b) Structure parameters posterior distribution conditioning.

Figure 5: Example conditioning APA hierarchy (from Example 6) first place
observation Candidate 3 ranked first place.

consistent O. example, possible interleavings research psychologists
(D) clinical psychologists (E), interleavings consistent
rank research psychologist first among research clinical psychologists.
therefore three consistent interleavings: D|D|E|E, D|E|D|E, D|E|E|D.
Conditioning sets relative rankings interleavings consistent
zero normalizes resulting parameter distribution. resulting riffle
independent representation posterior distribution shown Figure 5(b).

5.1 Impossibility Result
interesting consider completely decomposable observations exist beyond partial
rankings. One main contributions show observations.
Theorem 21 (Converse Theorem 19). Every completely decomposable observation takes
form partial ranking (C P).
Together, Theorems 19 21 form significant insight nature rankings,
showing notions partial rankings riffled independence deeply connected.
fact, result shows even possible define partial rankings via complete
decomposability!
practical matter, Theorem 21 shows algorithm based simple
multiplicative updates parameters exactly condition observations
take form partial rankings. computational complexity conditioning
observations partial rankings remains open. conjecture approximate
inference approaches may necessary efficiently handling complex observations.
504

fiEfficient Inference Partial Rankings

5.2 Proof Impossiblity Result (Theorem 21)
turn proving Theorem 21. Since proof significantly longer less obvious
proof converse (Theorem 19), sketch main ideas drive proof
refer interested readers details Appendix.
Recall definition linear span set vectors vector space
intersection linear subspaces containing set vectors. prove Theorem 21,
introduce analogous concepts span set rankings.
Definition 22 (rspan pspan). Let X Sn collection rankings. define
pspan(X) intersection partial rankings containing X. Similarly, define
rspan(X) intersection completely decomposable observations containing X.
formally,
\
\
pspan(X) =
, rspan(X) =
O.
O:XO, OC

:XS

example, X = {Corn|P eas|Apples, Apples|P eas|Corn}, checked
partial ranking three items containing items X entire set itself.
Thus pspan(X) = Corn, P eas, Apples.
proof strategy establish two claims: (1) pspan set always
partial ranking, (2) fact, rspan pspan set X exactly
sets. Since claim (1) fact partial rankings involve riffled
independence, defer related proofs Appendix. Thus have:
Lemma 23. X Sn , pspan(X) partial ranking.
Proof. See Appendix.
following discussion instead sketch proof claim (2). first show, however,
Theorem 21 must hold indeed true claims (1) (2) hold.
Proof. (of Theorem 21): Given C, want show P. claim
(2), rspan(O) = pspan(O). Since element C, however,
= rspan(O), thus = pspan(O). Finally Lemma 23 (claim (2)) guarantees
pspan(O) partial ranking, conclude P.
proceed establish claim rspan(X) = pspan(X). following
proposition lists several basic properties rspan use several
proofs. follow directly definition write proofs.
Proposition 24.
I. (Monotonicity) X, X rspan(X).
II. (Subset preservation) X, X 0 X X 0 , rspan(X) rspan(X 0 ).
III. (Idempotence) X, rspan(rspan(X)) = rspan(X).
One inclusion proof rspan(X) = pspan(X) follows directly fact
P C (Theorem 19):
505

fiHuang, Kapoor & Guestrin

formPspan(X)
X0 X; 0;
, 0 0 Xt disagree relative ordering items a1 , a2
Xt ;
foreach Xt
Add partial ranking obtained deleting vertical bar items
a1 a2 Xt ;
+ 1;
return (any element Xt ) ;

Algorithm 2: Pseudocode computing pspan(X). formPspan(X) takes set partial
rankings (or full rankings) X input outputs partial ranking. algorithm iteratively
deletes vertical bars elements X agreement. Note necessary
keep track t, ease notation proofs. algorithm
direct way computing pspan(X), again, simplifies proof main theorem.

Lemma 25. subset orderings, X, rspan(X) pspan(X).
Proof. Fix subset X Sn let element rspan(X). would show
element pspan(X). Consider partial ranking P covers X
(i.e., 0 0 X). want see . Theorem 19, P C,
therefore, C. Since rspan(X), 0 0 X, conclude,
definition rspan, . Since holds partial ranking covering X,
pspan(X).
remains task establishing reverse inclusion:
Proposition 26. subset orderings, X, rspan(X) pspan(X).
prove Proposition 26, consider problem computing partial ranking span
(pspan) given set rankings X. Algorithm 2, show simple procedure based
iteratively finding rankings X disagree pairwise ranking two items,
replacing rankings partial ranking vertical bar two
elements removed. show algorithm provably outputs correct
result.
Proposition 27. Given set rankings X input, Algorithm 2 outputs pspan(X).
Proof. See Appendix.
final step able prove Proposition 26, prove following two
technical lemmas relate computation pspan Algorithm 2 riffled independence, really form heart argument. particular, completely
decomposable observation C, Lemma 28 shows ranking contained
force rankings contained O.
Lemma 28. Let C suppose exist 1 , 2 disagree relative
ranking items i, j . ranking obtained swapping relative ranking
items i, j within 3 must contained O.
506

fiEfficient Inference Partial Rankings

Proof. Let h indicator distribution corresponding observation O.
show swapping relative ranking items i, j 3 result ranking
assigned nonzero probability h, thus showing new ranking contained O.
Let = {i, j} B = \A. Since C, h must factor riffle independently according
partition (A, B). Thus,
h(1 ) = m(AB (1 )) f (A (1 )) g(B (1 )) > 0,
h(2 ) = m(AB (2 )) f (A (2 )) g(B (2 )) > 0.
Since 1 2 disagree relative ranking items A, factorization implies
particular f (A = i|j) > 0 f (A = j|i) > 0. Since h(3 ) > 0, must
m(AB (3 )), f (A (3 )), g(B (3 )) positive probability.
therefore swap relative ranking A, , obtain new ranking positive
probability since terms decomposition new ranking positive
probability.
Lemma 29 provides conditions removing vertical bar one
rankings X change support completely riffle independent distribution. illustrate example, consider completely decomposable observation
contains partial ranking = Corn, P eas|Apples, Oranges subset.
Lemma 29 guarantees that, if, addition, exists element disagrees
relative ordering of, say, P eas Oranges, fact partial ranking
0 0 = Corn, P eas, Apples, Oranges (with bar removed ) must
subset O. Formally,
Lemma 29. Let = 1 | . . . |i |i+1 | . . . |k partial ranking item set ,
0 0 = 1 | . . . |i i+1 | . . . |k , partial ranking sets i+1
merged. Let a1 ij=1 j a2 kj=i+1 j . element C
additionally exists ranking disagrees relative
ordering a1 , a2 , 0 0 O.
Proof. key strategy proof Lemma 29 argue large subsets rankings
must contained completely decomposable observation decomposing rankings
transpositions invoking technical lemma (Lemma 28) repeatedly.
See Appendix details.
use Lemma 29 show reverse inclusion Proposition 26
holds, establishing two sets rspan(X) pspan(X) fact equal thereby
proving desired result, C P.
Proof. (of Proposition 26) iteration t, Algorithm 2 producesS
set partial rankings,
Xt . denote union partial rankings time Xt Xt . Note
X0 = X XT = pspan(X). idea proof show iteration
t, following set inclusion holds: rspan(Xt ) rspan(Xt1 ). indeed holds,
507

fiHuang, Kapoor & Guestrin

final iteration , shown that:
pspan(X) = XT ,

(Proposition 27)

rspan(XT ),

(Monotonicity, Proposition 24)

rspan(X0 ),

(since rspan(Xt ) rspan(Xt1 ), shown below),

rspan(X)

(X0 = X, see Algorithm 2)

would prove Proposition.
remains show rspan(Xt ) rspan(Xt1 ). claim Xt rspan(Xt1 ).
Let Xt . Xt1 , since Xt1 rspan(Xt1 ), rspan(Xt1 )
proof done. Otherwise, Xt \Xt1 . second case, use fact
iteration t, vertical bar i+1 deleted partial ranking = 1 | . . . |i |i+1 | . . . |k (which subset Xt1 ) form partial ranking
0 0 = 1 | . . . |i i+1 | . . . |k . (which subset Xt ). Furthermore, order
vertical bar deleted algorithm, must existed partial
ranking (and therefore full ranking 0 ) disagreed relative ordering items a1 , a2 opposite sides bar. Since Xt \Xt1 assume
0 0 .
would apply Lemma 29. Note C Xt1 O,
O, since Xt1 . application Lemma 29 shows
0 0 therefore O.
shown fact holds observation C Xt1
O, therefore taking intersection supports C, see Xt
rspan(Xt1 ). Taking rspan sides yields:
rspan(Xt ) rspan(rspan(Xt1 )),
rspan(Xt1 ).

(Subset preservation, Proposition 24)

(Idempotence, Proposition 24)

5.3 Going Beyond Subset Observations
Though stated results far subset observations, comment
theory would look considered general likelihood functions.
order avoid confusion, refer general class functions call completely decomposable functions, instead completely decomposable subset observations
Definition 11.
Definition 30. function h : Sn R called completely decomposable function
factors riffle independently respect every hierarchy item set . denote
e
collection possible completely decomposable functions C.
e nearly same. quite simple restate Theorem 19
discuss, C C
respect general case completely decomposable functions:
Theorem. Every partial ranking indicator function completely decomposable function.
508

fiEfficient Inference Partial Rankings

Unfortunately, proof converse (Theorem 21) easily generalize,
instead used show support ({ Sn : h() > 0}) every completely
decomposable function partial ranking. natural, however, suspect full
converse indeed exist every completely decomposable function proportional
indicator function partial ranking. fact, suspected converse almost
holds. have:
Theorem. h completely decomposable function supported partial ranking
= 1 | . . . |r |i | =
6 2 = 1, . . . , r, h proportional indicator
function .
Proof. See Appendix.
Example 31. completely decomposable functions, possible away
assumption |i | =
6 2 i. example, function defined as:

2/3 = Corn|P eas|Apples
1/3 = P eas|Corn|Apples ,
h() =

0
otherwise
supported partial ranking = Corn, P eas|Apples (where |1 | = 2),
proportional indicator function (i.e., uniform rankings
assigned positive probability).
However, still possible show h completely decomposable function.
prove so, necessary establish three things: {Corn, P eas} {Apples}
riffle independent, {Corn, Apples} {P eas} riffle independent,
{P eas, Apples} {Corn} riffle independent. example, respect partitioning sets = {Corn, Apples} B = {P eas}, see
h() = m(AB ()) f (A ()) g(B ()),
where:

2/3
1/3
m(AB ) =

0

AB = A|B|A
AB = B|A|A ,
AB = A|A|B


f (A ) =

1
0

{AC} = A|C
,
otherwise

g(B ) = 1.

Therefore, |i | = 2, possible completely decomposable functions
uniform supports.
5.4 Conditioning Noisy Observations
conclude section remark handling noise observations.
assumed paper observed partial rankings always consistent users
underlying full ranking, situations one may wish model noisier
setting, partial rankings may misreported small probability.
natural model accounts noise, example, might be:

1
L(O|) =
.
(5.2)

|O|1 otherwise
509

fiHuang, Kapoor & Guestrin

prior distribution factorizes respect hierarchy H, conditioning
noisy likelihood Equation 5.2 results posterior distribution written
weighted mixture prior distribution posterior would resulted
conditioning noise-free observation. component posterior
distribution factorizes respect H, mixture factor general (and
factor according theory). result, iteratively conditioning multiple
partial rankings according noisy likelihood function would quickly lead
unmanageable number mixture components. therefore believe approximate
inference methods conditioning multiple noisy partial ranking observations fruitful
area research.

6. Model Estimation Partially Ranked Data
many ranking based applications, datasets predominantly composed partial rankings rather full rankings due fact humans, partial rankings typically
easier faster specify. addition, many datasets heterogeneous, containing partial
ranking different types. example, American Psychological Assoication well
Irish House Parliament elections, voters allowed specify top-k candidate
choices value k (see Figures 7(a) 7(b)). section use efficient
inference algorithm proposed Section 5 estimating riffle independent model
partially ranked data. estimating model using partially ranked data typically
considered difficult estimating one using full rankings, common practice (e.g., see Huang & Guestrin, 2010) simply ignore partial rankings
dataset. ability method incorporate available data however, lead
significantly improved model accuracy well wider applicability method.
section, propose first efficient method estimating structure parameters
hierarchical riffle independent model heterogeneous datasets consisting arbitrary
partial ranking types. Central approach idea given someones partial preferences, use efficient algorithms developed previous section infer full
preferences consequently apply previously proposed algorithms designed
work full rankings.
6.1 Censoring Interpretations Partial Rankings
model estimation problem full rankings stated follows. Given i.i.d. training
examples (1) , . . . , (m) (consisting full rankings) drawn hierarchical riffle independent distribution h, recover structure parameters h.
partial ranking setting, assume i.i.d. draws, training
example (i) undergoes censoring process producing partial ranking consistent (i) .
example, censoring might allow ranking top-k items (i)
observed. allow arbitrary types partial rankings arise via censoring,
make common assumption partial ranking type resulting censoring (i)
depend (i) itself.
510

fiEfficient Inference Partial Rankings

6.2 Algorithm
treat model estimation partial rankings problem missing data problem.
many problems, could determine full ranking corresponding observation data, could apply algorithms work completely observed
data setting. Since full rankings given, utilize Expectation-Maximization (EM)
approach use inference compute posterior distribution full rankings
given observed partial ranking. case, apply algorithms Huang
Guestrin (2010, 2012) designed estimate hierarchical structure
model parameters dataset full rankings.
Given initial model h collection training examples {O(1) , O(2) , . . . , O(m) }
consisting partial rankings, EM-based approach alternates following two
steps convergence achieved.
(E-step): observation, O(i) = (i) (i) , training examples, use
inference compute posterior distribution full ranking could
generated O(i) via censoring, h(|O(i) = (i) (i) ). Since observations take
form partial rankings hence completely decomposable, use efficient
algorithms Section 5 perform E-step.
(M-step): M-step, one maximizes expected log-likelihood training
data respect model. hierarchical structure model
provided, known beforehand, M-step performed using standard methods optimizing parameters. structure unknown, use structural
EM approach, analogous methods graphical models literature
structure learning incomplete data (Friedman, 1997, 1998).
Unfortunately, (riffled independence) structure learning algorithm Huang
Guestrin (2010) unable directly use posterior distributions computed
E-step. Instead, observing sampling riffle independent models
done efficiently exactly (as opposed to, example, MCMC methods), simply
sample full rankings posterior distributions computed E-step
pass full rankings structure learning algorithm Huang Guestrin
(2010). number samples necessary, instead scaling factorially, scales
according number samples required detect riffled independence (which
mild assumptions polynomial n, Huang & Guestrin, 2010).

7. Related Work
Rankings permutations recently become active area research machine
learning due part hinge role play information retrieval preference
elicitation. Algorithms RankSVM (Joachims, 2002) RankBoost (Freund,
Iyer, Schapire, & Singer, 2003), example, successful large scale ranking
problems appear web search. main aims work differ web
scale settings however instead seeking single optimal ranking respect
objective function, seek understanding large collection rankings via density
estimation. following, outline two major lines research influenced
work.
511

fiHuang, Kapoor & Guestrin

7.1 Additive Multiplicative Decompositions
paper builds particular upon thread recent work tractable models permutation data based function decompositions. Kondor, Howard, Jebara (2007)
Huang, Guestrin, Guibas (2008, 2009) considered additive decompositions distribution weighted sum Fourier basis functions. papers show low-frequency
Fourier assumptions often effective coping representational complexity
working distributions permutations. show particular conditioning
prior distributions low frequency likelihood functions often arise multiobject
tracking problems performed especially efficiently.
Unfortunately, low frequency assumptions applicable distributions defined
rankings, address ranking problems specifically, Huang Guestrin (2009,
2010) introduced concept riffled independence useful generalization probabilistic independence rankings. Using multiplicative decompositions based riffled
independence, showed possible learn hierarchical structure model
given fully ranked dataset. previous papers topic riffled independence
focused problems related efficiently representing distributions, main focus
current paper lies efficient reasoning/inference tackling human task complexity
considering partial rankings.
interesting note natural efficient condition Fourier based
representation low-frequency observations (involving small number items)
=Alice third place, multiplicative decomposition based riffled independence
would able efficiently condition observation. hand,
multiplicative decompositions allow us condition top-k observations efficiently (independently size k), whereas top-k observations would difficult handle
Fourier theoretic setting (except small k).
7.2 Mallows Models
work fits larger body research well known Mallows distribution
rankings, parameterized by:
h(; , 0 ) (,0 ) ,

(7.1)

function refers Kendalls tau distance metric rankings. Mallows
distribution (Equation 7.1) always shown special case hierarchical riffle independent model items sequentially factored model one
one (Huang, 2011) (see Figure 6).
Mallows models (as well similar distance based models) advantage
compactly represent distributions large n, admit conjugate prior
distributions (Meila, Phadnis, Patterson, & Bilmes, 2007). Estimating parameters
popular problem statisticians recovering optimal 0 data known
consensus ranking rank aggregation problem known N P -hard (Bartholdi,
Tovey, & Trick, 1989). Many authors focused approximation algorithms instead.
Gaussian distributions, Mallows models tend lack flexibility, Lebanon
Mao (2008) propose nonparametric model ranked (and partially ranked) data based
placing weighted Mallows kernels top training examples, which, show,
512

fiEfficient Inference Partial Rankings

{Corn,Peas,Apples,Oranges,Doughnuts}

{Corn}

{Peas,Apples,Oranges,Doughnuts}

{Peas}

{Apples,Oranges,Doughnuts}
{Apples}

{Oranges,Doughnuts}
{Oranges}

{Doughnuts}

Figure 6: Mallows model always factors according refer chain
structure items factored one one. Mallows distribution five items food item set mode (or central ranking)
0 = Corn|P eas|Apples|Oranges|Doughnuts, example, must factor according hierarchical structure.

realize far richer class distributions, learned efficiently. However,
address inference problem, immediately clear many Mallows models
papers whether one efficiently perform inference operations marginalization
conditioning models. Riffle independent models, hand, encompass
class distributions rich well interpretable, additionally,
identified precise conditions efficient conditioning possible (the conditions
observations take form partial rankings).
several recent works model partial rankings using Mallows based models.
Busse, Orbanz, Buhmann (2007) learned finite mixtures Mallows models topk data (also using EM approach). Lebanon Mao (2008), mentioned,
developed nonparametric model based Mallows models handle arbitrary
types partial rankings. settings, central problem marginalize Mallows
model full rankings consistent particular partial ranking.
efficiently, papers rely fact (first shown Fligner & Verducci, 1986)
marginalization step performed closed form. closed form equation Fligner
Verducci (1986), however, seen special case setting since Mallows
models always shown factor riffle independently according chain structure.
Specifically, compute sum rankings consistent partial ranking
, necessary condition , compute normalization constant
resulting function. conditioning step performed using methods
described paper, normalization constant computed multiplying
normalization constant factor hierarchical decomposition. Thus, instead
resorting complicated mathematics inversion combinatorics, theory
complete decomposability offers simple conceptual way understand Mallows models
conditioned efficiently partial ranking observations.
513

fiHuang, Kapoor & Guestrin

Finally recent related work, Lu Boutilier (2011) considered even general
class observations based DAG (directed acyclic graph) based observations
probabilities rankings consistent DAG relative ranking relations
set zero. Lu Boutilier show particular conditioning problem
DAG-based class observations #P -hard. additionally propose efficient
rejection sampling method performing probabilistic inference within general class
DAG observations prove sampling method exact class partial
rankings discussed paper.

8. Experiments
section, demonstrate method learning hierarchical riffle independent models partial rankings simulated data well real datasets taken different
domains. experiments, initialize distributions uniform, use random restarts.
8.1 Datasets
addition roughly 5000 full rankings, APA dataset 10,000 top-k rankings
5 candidates. previous work, used full rankings APA data (Huang
& Guestrin, 2010, 2012), able use entire dataset. Figure 7(a) plots,
k {1, . . . , 5}, number ballots APA data length k.
Likewise, Meath dataset (Gormley & Murphy, 2007) taken 2002
Irish Parliament election 60,000 top-k rankings 14 candidates. APA
data, used full rankings Meath data previous work, use
entire dataset. Figure 7(b) plots, k {1, . . . , 14}, number ballots
Meath data length k. particular, note vast majority ballots dataset
consist partial rather full rankings, half electorate preferring list
favorite three four candidates. run inference (Algorithm 1)
5000 top-k examples Meath data 10 seconds dual 3.0 GHz Pentium machine
unoptimized Python implementation. Using brute force inference, estimate
job would require roughly one hundred years.
extracted third dataset database searchtrails collected White
Drucker (2007), browsing sessions roughly 2000 users logged 20082009. many cases, users unlikely read articles news story twice,
often possible think order user reads collection
articles top-k ranking articles concerning particular story/topic. ability
model visit orderings would allow us make long term predictions user browsing
behavior, even recommend curriculums articles users. ran algorithms
roughly 300 visit orderings eight popular posts www.huffingtonpost.com
concerning Sarah Palin, popular subject 2008 U.S. presidential election. Since
user visited every article, full rankings data thus
even exist option learning using subset full rankings.
514

fiEfficient Inference Partial Rankings

number votes

number votes

6000
5000

4000
3000
2000
1000

20,000

10,000

0

0

1

2

2
3
4
5
number candidates

4

6

8 10 12 14
k

(a) APA election data

(b) Irish election data

Figure 7: Histograms top-k ballot lengths APA Irish election datasets. Whereas
majority electorate provided full rankings APA election data
(probably due fact five candidates), vast majority
voters Irish election data provided top-3 top-4 choices.
{12345}

{12345}
{12345}

{2}

{2345}

{2}

{2345}
{1345}

{345}

{1}
{3}

{45}

(a) Structure learned using
subset full rankings
(out 300 given training
examples)

{345}

{1}
{5}

{34}

(b) Structure learned using
training examples 1 iteration EM

{13}
Research

{2}
Community
psychologists

{45}
Clinical

(c) Structure learned using
training examples structural convergence (3 iterations)

Figure 8: Structure learning subset APA dataset (300 rankings, randomly
sampled, including full partial rankings).

8.2 APA Structure Learning Results
Due unordinarily large number full rankings APA data, gains made
additionally using partially ranked data insignificant. better illustrate benefits
partial rankings, subsampled dataset 300 rankings (including full partial
rankings) present results smaller dataset. Performing structure learning using
full rankings 300 training examples (consisting roughly 100 examples),
one obtains structure Figure 8(a), seen match correct
structure Figure 3(a) learned using 5000 full rankings. Figures 8(b) 8(c)
515

fiHuang, Kapoor & Guestrin

training time (seconds)

test log-likelihood

x 10 4
-2
-3
-4
-5

-6
EM

Flat-EM

4

training time (seconds)

test log-likelihood

-5
-5.2
-5.4
-5.6
-5.8
-6
k>2

10
5

0
EM

Flat-EM Uniform
Fill-In

(b) Training time comparison
EM approach FlatEM
Uniform Fill-In methods.

x 10

k>1

15

Uniform
Fill-In

(a) Test set log-likelihood comparison
EM approach FlatEM
Uniform Fill-In methods.

k>0

20

2.5
2
1.5

1

k>3

(c) Test set log-likelihoods, training
top-t rankings larger
fixed k.

k>0

k>1

k>2

k>3

(d) Training times, training
top-t rankings larger fixed
k.

Figure 9: APA experimental results experiment repeated 200 bootstrapped
resamplings data

plot results EM algorithm former displaying resulting structure
single EM iteration latter result structural convergence, occurs
third iteration, showing method learn correct structure given
300 training examples.
compared EM algorithm two alternative baseline approaches
refer plots FlatEM Uniform Fill-in. FlatEM algorithm
EM algorithm except two details: (1) performs conditioning exhaustively
instead exploiting factorized model structure, (2) performs M-step without
sampling. Uniform Fill-in approach treats every top-k ranking training set
uniform collection votes full rankings consistent top-k ranking,
accomplished using one iteration EM algorithm.
Figure 9(a) plot test set loglikelihoods corresponding approach, EM
FlatEM almost identical results performing much better
Uniform Fill-in approach. hand, Figure 9(b), compares running times
three approaches, shows FlatEM far costly (for datasets,
cannot even run reasonable amount time).
516

fiEfficient Inference Partial Rankings

1st iteration

2nd iteration

{0,1,2,3,4,5,6,7}

3rd iteration

{0,1,2,3,4,5,6,7}
{0,1,2,3,4,5,6,7}

{5}

{0,1,2,3,4,6,7}

{5}

{0,1,2,3,4,6,7}
{5}

{0,1,2,3,4,7}

{6}

{0,1,2,3,4,7}

{0,1,2,3,4,6,7}

{7}
{0,1,2,3,4,7}

{0,1,2,3,4}

{7}

{0,1,2,3,4}

{6}
{0,2,3}

{0,2,3}

{1,4}

Log likelihood: -818.6579
(a)

{0,2,3}

{7}

{1,4,6}

{1,4}

Log likelihood: -769.2369
(b)

Log likelihood: -767.2760
(c)

Figure 10: Iterations Structure EM Sarah Palin data structural changes
iteration highlighted red. Structural convergence occurs three
iterations. Note structure discovered using visit orders,
text information pages incorporated learning process.
figure best viewed color.

verify partial rankings indeed make difference APA data, plot
results estimating model subsets APA training data consisting top-k
rankings length larger fixed k. Figures 9(c) 9(d) show log-likelihood
running times k = 0, 1, 2, 3 k = 0 entire training set k = 3
subset training data consisting full rankings. results show,
including partial rankings indeed help average improving test log-likelihood
(with diminishing returns).
8.3 Structure Discovery EM Larger n.
experiments led several observations using EM learning partial
rankings. First, observe typical runs converge fixed structure quickly,
three EM iterations. Figure 10 shows progress EM Sarah Palin
data, whose structure converges third iteration. expected, log-likelihood
increases iteration, remark structure becomes interpretable
example, leaf set {0, 2, 3} corresponds three posts Palins wardrobe
election, posts leaf set {1, 4, 6} related verbal gaffes
made Palin campaign. Notice structure discovered purely using
data visit orders text information used experiments.
517

fiHuang, Kapoor & Guestrin

-2.72

x 104

test log-likelihood

# EM iterations
convergence

30

25
20
15

10

EM
decomposable
conditioning

-2.8

[Lebanon & Mao, 08]

-2.84
-2.88

5
0

-2.76

0

0 1 2 3 4 5 6 7 8 9 10 11 12
k

250 1000 4000 16000 64000
# partial rankings training set
(in addition full rankings)

(a)

(b)

Figure 11: (a): Number EM iterations required convergence training set
contains rankings length longer k. (b): Density estimation synthetic
data. plot test loglikelihood learning 343 full rankings
0 64,000 additional partial rankings.

5000 training
examples

4

Test log-likelihood

x 10
-4.5



-4.6

25000 training
examples

5

x 10
-1.26

[LM08]

[LM08]

-1.28



-1.3

-4.7



-1.32

[LM08]


-4.8

[LM08]

-1.34

-4.9

Full

Mixed (Full+Partial)

Full

Mixed (Full+Partial)

Figure 12: Density estimation small (5000 examples) large subsets (25000 examples) Meath data. compare method work Lebanon
Mao (2008) two settings: (1) training available data (2) training
subset full rankings.

Secondly, number EM iterations required reach convergence log-likelihood
depends types partial rankings observed. ran algorithm subsets
Meath dataset, time training = 2000 rankings length larger
518

fiEfficient Inference Partial Rankings

fixed k. Figure 11(a) shows number iterations required convergence
function k (with 20 bootstrap trials k). observe fastest convergence
datasets consisting almost-full rankings slowest convergence consisting
almost-empty rankings, almost 25 iterations necessary one trains using rankings
types. Finally remark model obtained first iteration EM
interesting thought result pretending voter completely
ambivalent regarding n k unspecified candidates.
8.4 Value Partial Rankings
verify larger n using partial rankings addition full rankings
allows us achieve better density estimates. first learned models synthetic data
drawn hierarchy, training using 343 full rankings plus varying numbers partial
ranking examples (ranging 0-64,000). repeat setting 20 bootstrap
trials, evaluation, compute log-likelihood testset 5000 examples.
speed, learn structure H fix H learn parameters trial.
Figure 11(b), plots test log-likelihood function number partial
rankings made available training set, shows indeed able learn
accurate distributions data form partial rankings made
available.
8.5 Comparing Nonparametric Model
Comparing performance riffle independent models approaches possible previous work since able handle partial rankings. Using
methods developed current paper, however, compare riffle independent models
state-of-the-art nonparametric estimator Lebanon Mao (2008) (to
hereby refer LM08 estimator) data (setting regularization parameter C =1,2,5, 10 via validation set). Figure 11(b) shows (naturally)
data drawn synthetically riffle independent model, EM method significantly outperforms LM08 estimator. remark theory, LM08 guaranteed
catch performance (under appropriate conditions) given enough training examples.
Meath data, approximately riffle independent, trained subsets
size 5,000 25,000 (testing remaining data). subset, evaluated EM
algorithm learning riffle independent model LM08 estimator (1)
using full ranking data, (2) using data. before, methods better
partial rankings made available.
smaller training set, riffle independent model performs well better
LM08 estimator. larger training set 25,000, see nonparametric
method starts perform slightly better average, advantage nonparametric
model guaranteed consistent, converging correct model given
enough data. advantage riffle independent models, however, simple,
interpretable, highlight global structures hidden within data.
519

fiHuang, Kapoor & Guestrin

9. Future Directions
remain several possible extensions current work. list open
questions extensions following.
9.1 Inference Incomplete Rankings
shown paper one exploit riffled independence structure condition
observation takes form partial ranking. space
partial rankings rich useful many settings, cover important class
observations: incomplete rankings, defined ranking (or partial
ranking) subset itemset . example, Theorem 21 shows conditioning problem pairwise observations form Apples preferred Bananas
nondecomposable. Note top-k rankings considered complete rankings since
implicitly rank items last n k positions.
then, tractably condition incomplete rankings? One possible approach
convert Fourier representation using methods (Huang & Guestrin, 2012),
conditioning pairwise ranking observation using Fourier domain conditioning
algorithm proposed (Huang et al., 2008). Fourier domain approach would useful one particularly interested low-order marginal probabilities posterior
distributions.
Fourier approach viable, another option may assume
posterior distribution takes particular riffle independent structure (in way
mean field methods graphical models literature would assume factorized
posterior). research question interest is: hierarchical structure used
purposes approximating posterior?
9.2 Reexamining Data Independence Assumptions
paper, assumed throughout training examples independent
identically distributed. However practice always safe assumptions
number factors impact validity both. example, internet survey
user must perform series preference ranking tasks sequence, concern
users prior ranking tasks may bias results future rankings.
Another source bias lies reference ranking may displayed,
user asked rearrange items dragging dropping. one hand, showing
everyone reference ranking may bias resulting data. hand,
showing every user different reference ranking may mean training examples
exactly identically distributed.
Yet another form bias lies partial ranking types reported data.
formulate EM algorithm, assumed users preferences influence
whether chooses to, say, report full ranking instead top-3 ranking. practice,
however, partial ranking types user preferences often correlated. Irish elections, example, typically one Sinn Fein candidate, rank
Sinn Fein first typically likely reported top-1 choice.
520

fiEfficient Inference Partial Rankings

Understanding, identifying, finally, learning spite different types biases
may occur eliciting preference data remains fundamental problem ranking.
9.3 Probabilistic Modeling Strategic Voting
interesting consider differences actual vote distributions considered
paper approximate riffle independent distributions. Take APA dataset,
example, optimal approximation riffle independent hierarchy reflects
underlying political coalitions within organization. Upon comparison
approximation empirical distribution, however, marked differences arise.
example, riffle independent approximation underestimates number votes obtained
candidate 3 (a research psychologist) ultimately election.
One possible explanation discrepancy may lie idea voters tend vote
strategically APA elections, placing stronger candidates opposing political coalitions
lower ranking, rather revealing true preferences. interesting line
future work lies detecting studying presence strategic voting election
datasets. Open questions include (1) verifying mathematically whether strategic voting
indeed exist in, say, APA election data, (2) so, strategic voting effect
strong enough overwhelm riffled independence structure learning algorithms,
(3) strategic voting manifest partial ranking votes.

10. Conclusion
probabilistic reasoning problems, often case certain data types suggest
certain distribution representations. example, sparse dependency structure data
often suggests Markov random field (or graphical model) representation (Friedman,
1997, 1998). low-order permutation observations (depending items
time), recent work (Huang et al., 2009; Kondor, 2008) shown Fourier domain
representation appropriate. preference ranking scenarios, one must contend
human task complexity difficulty involved human rank long list items
often leads partially, instead fully ranked data. paper, shown
data takes form partial rankings, hierarchical riffle independent models
natural representation.
conjugate priors, showed riffle independent model guaranteed
retain factorization structure conditioning partial ranking (which performed efficiently). surprisingly, work shows observations
take form partial rankings amenable simple multiplicative update based
conditioning algorithms. Finally, showed possible learn hierarchical riffle
independent models partially ranked data, significantly extending applicability
previous work.

Acknowledgments
project formulated largely conducted internship Jonathan Huang
Microsoft Research. Additional work supported part ONR MURI
N000140710747, ARO MURI W911NF0810242. Carlos Guestrin funded
521

fiHuang, Kapoor & Guestrin

part NSF Career IIS-064422. thank Eric Horvitz, Ryen White, Dan Liebling,
Yi Mao discussions.

Appendix A. Proofs
appendix, provide supplementary proofs theoretical results
paper.
A.1 Proof Theorem 19
prove Theorem 19 (as well later results), refer rank sets.
Definition 32. Given partial ranking type , denote rank set occupied
Ri . Note Ri depends
written R1 = {1, . . . , 1 },
P
R2 = {1 + 1, . . . , 1 + 2 }, . . . , Rr = { r1
i=1 + 1, . . . , n}.
refer following basic fact regarding rank sets:
Proposition 33. = 1 | . . . |r i, (i ) = Ri .
Proof. (of Theorem 19) use induction size itemset. cases n = 1, 2
trivial since every distribution S1 S2 factors riffle independently. consider
general case n > 2.
Fix partial ranking = 1 |2 | . . . |r type binary partition item
set subsets B. show indicator function factors as:
() = m(AB ()) f (A ()) g(B ()),

(A.1)

factors m, f g indicator functions set consistent interleavings,
[S ]AB , sets consistent relative rankings, [S ]A [S ]B , respectively.
Equation A.1 true, shown must decompose respect
top layer H. show decomposes hierarchically, must show
relative ranking factors fA gB decompose respect HA HB ,
subhierarchies item sets B. establish second step (assuming
Equation A.1 holds), note fA gB indicator functions restricted partial
rankings, [S ]A [S ]B , partial rankings smaller item sets
B. inductive hypothesis (and fact B assumed strictly
smaller sets ) shows functions fA gB factor according
respective subhierarchies.
turn establishing Equation A.1. suffices prove following two
statements equivalent:
I. ranking consistent partial ranking (i.e., ).
II. following three conditions hold:
(a) interleaving AB () consistent (i.e., AB () [S ]AB ),
(b) relative ranking () consistent (i.e., () [S ]A ),
(c) relative ranking B () consistent (i.e., B () [S ]B ).
522

fiEfficient Inference Partial Rankings

(I II): first show implies conditions (a), (b) (c).
(a) , i,
|j Ri : AB (j) = A| = |j Ri : 1 (j) A|,
= |k : k A|,

(by Definition 2)

(by Proposition 33)

= |i A|.
argument (replacing B) shows i, |j
Ri : AB (j) = B| = |i B|. two conditions (by Definition 17) show
AB consistent .
(b) , (by Definition 14) ranks items items j
< j. Intersecting A, see ranks item
item j i, j. Definition 2, () ranks item
item j i, j. finally Definition 16 again,
see () consistent partial ranking .
(c) (Same argument (b)).
(II I): assume conditions (a), (b), (c) hold, show .
Proposition 33 sufficient show item k , (k) Ri .
prove claim, show induction item k A, (k) Ri
(and similarly k B, (k) Ri ).
Base case. base case (i = 1), assume k 1 A, goal show
(k) R1 . condition (a), AB () [S ]AB . Definition 17,
means that: |1 A| = {j R1 : [AB ()](j) = A} = {j R1 : 1 (j) A}.
words, = |1 A| items lie rank set R1 = {1, . . . , 1 }.
show item k maps rank R1 , must show relative
ranking elements A, k among first m. condition (b), () [S ]A ,
implying item subset 1 occupies first positions relative
ranking A. Since k 1 A, item k among first items ranked ()
therefore (k) R1 . similar argument shows k 1 B implise
(k) R1 .
Inductive case. show k A, (k) Ri . condition (b),
() [S ]A , implying item subset (and hence, item k) occupies
first = |i A| positions relative ranking beyond items i1
j=1 (j
A). inductive hypothesis mutual exclusivity, items, together
i1
i1
j=1 (j B) occupy ranks j=1 Rj , therefore (k) R` ` i.
hand, condition (a) assures us |i A| = {j Ri : 1 (j) A}
words, ranks Ri occupied exactly items A. Therefore,
(k) Ri . Again, similar argument shows k B implies (k) Ri .

A.2 pspan Set Always Partial Ranking
reason pspan set rankings, first introduce basic concepts
regarding combinatorics partial rankings. collection partial rankings
523

fiHuang, Kapoor & Guestrin

forms partially ordered set (poset) 0 0 obtained 0 0
dropping vertical lines. example, S3 , 1|2|3 12|3. Hasse diagram
graph node corresponds partial ranking node x connected
node via edge x exists partial ranking z x z
(see Lebanon & Mao, 2008). top Hasse diagram partial ranking 1, 2, . . . , n
(i.e., ) bottom Hasse diagram lie full rankings. See Figure 13
example partial ranking lattice S3 .
Lemma 34. [Lebanon & Mao, 2008] Given two partial rankings , 0 0 ,
exists unique supremum 0 0 (a node Ssup sup Ssup sup
0 0 Ssup sup , node greater Ssup sup ). Similarly,
exists unique infimum 0 0 .
Lemma 35. Given two partial rankings , 0 0 , relation 0 0 holds
lies 0 0 Hasse diagram.
Proof. lies 0 0 Hasse diagram, 0 0 trivial since
obtained dropping vertical bars 0 0 . given lie
0 0 , would show 0 0 6 . Let Sinf inf unique infimum
0 0 guaranteed Lemma 34. definition Hasse diagram,
obtained dropping verticals vertical bar representation
Sinf inf . Since lie 0 0 , must vertical bar
dropped 0 0 dropped (if exist bar,
0 0 ), hence must exist pair items i, j separated single vertical
bar unseparated 0 0 . Therefore exists 0 0 (j) < (i)
even though exists . conclude 0 0 6 .
Lemma 36 (Lemma 23 main body). X Sn , pspan(X) partial ranking.
Proof. Consider subset X Sn . partial ranking containing every element X
must upper bound every element X Hasse diagram Lemma 35.
Lemma 34, must exist unique least upper bound (supremum) X, Ssup sup ,
common upper bound X, must ancestor Ssup sup
hence Ssup sup . therefore see partial ranking containing X must
superset Ssup sup . hand, Ssup sup partial ranking containing X.
Since pspan(X) intersection partial rankings containing X, pspan(X) =
Ssup sup therefore pspan(X) must partial ranking.
A.3 Proofs Claim rspan(X) = pspan(X)
simplify notation remaining proofs, introduce following definition.
Definition 37 (Ties). Given partial ranking = 1 | . . . |r , say items a1
a2 tied (written a1 a2 ) respect a1 , a2 i.
following basic properties tie relation straightforward.
Proposition 38.
524

fiEfficient Inference Partial Rankings

123
1|23

12|3

13|2

2|13

3|12

23|1

1|2|3

1|3|2

2|1|3

3|1|2

2|3|1

3|2|1

Figure 13: Hasse diagram lattice partial rankings S3 .
I. respect fixed partial ranking , tie relation, , equivalence relation
item set (i.e., reflexive, symmetric transitive).
II. exist , 0 disagree relative ranking items a1 a2 ,
a1 a2 respect .
III. 0 0 , a1 a2 respect , a1 a2 respect 0 0 .
IV. a1 a2 respect , (a1 ) < (a3 ) < (a2 ) item a3
, a1 a2 a3 .
Proposition 39. Given set rankings X input, Algorithm 2 outputs pspan(X).
Proof. prove three things, together prove proposition: (1) algorithm
terminates, (2) stage elements X contained pspan(X), (3)
upon termination, pspan(X) contained element X.
1. First note algorithm must terminate finitely many iterations
loop since stage least one vertical bar removed partial ranking,
vertical bars removed elements X,
disagreements relative ordering.
2. show stage algorithm, every element Xt subset
pspan(X). initialization, course, X0 , simply singleton
set consisting element X, therefore pspan(X).
Suppose pspan(X) every Xt . replaced
Xt+1 , want show pspan(X) well. Algorithm 2,
j, = 1 | . . . |j |j+1 | . . . |r , written 1 | . . . |j
j+1 | . . . |r , vertical bar j j+1 deleted due existence
partial ranking Xt , 0 0 Xt disagrees relative
ordering items a1 , a2 opposite sides bar. Since 0 0
subsets pspan(X) assumption, know a1 a2 respect pspan(X)
(Proposition 38, II). Suppose a1 a2 i0 . x
i0 , x a1 a2 respect pspan(X) (III)
Proposition 38. Moreover, (I, transitivity), see x respect
pspan(X). two elements i0 . (IV) Proposition 38,
items lying , i+1 , . . . , i0 thus tied respect pspan(X) therefore
removing bar items a1 a2 (producing, example, ) results
partial ranking subset pspan(X).
525

fiHuang, Kapoor & Guestrin

3. Finally, upon termination, ranking X contained element
Xt , would exist two items a1 , a2 whose relative ranking
disagree upon, contradiction. Therefore, every element Xt contains
every element X thus pspan(X) every Xt .

Lemma 40. Let = 1 | . . . |i |i+1 | . . . |k partial ranking item set ,
0 0 = 1 | . . . |i i+1 | . . . |k , partial ranking sets i+1
merged. Let a1 ij=1 j a2 kj=i+1 j . element C
additionally exists ranking disagrees relative
ordering a1 , a2 , 0 0 O.
Proof. fix completely decomposable work h, indicator
distribution corresponding O. Let 0 0 . prove lemma, need establish
h() > 0. Let 0 element 0 (k) = (k) k \(i i+1 ).
Since supp(h) assumption, h( 0 ) > 0.
Since 0 match items except i+1 , exists sequence
rankings 0 , 1 , 2 , . . . , = adjacent rankings sequence differ
pairwise exchange items b1 , b2 i+1 . show step
along sequence, h( ) > 0 implies h( t+1 ) > 0, prove h() > 0.
Suppose h( ) > 0 t+1 differ relative ranking
items b1 , b2 i+1 (without loss generality, assume (b2 ) < (b1 )
t+1 (b1 ) < t+1 (b2 )).
idea following paragraph use previous lemma (Lemma 28) prove
t+1 positive probability so, necessary argue
exists ranking 0 h( 0 ) > 0 0 (b1 ) < 0 (b2 ) (i.e., 0 disagrees
relative ranking b1 , b2 ). Let element . a1 , rearrange
a1 ranked first among elements . a2 i+1 , rearrange
a2 ranked last among elements i+1 . Note still element
possible rearrangements therefore h() > 0. assume (b2 ) < (b1 )
since otherwise shown wanted show. Thus relative ordering
a1 , a2 , b1 , b2 within a1 |b2 |b1 |a2 . Note treat case items a1 , a2 , b1 , b2
distinct, argument follows cases a1 = b2 a2 = b1 .
since disagrees relative ordering a1 , a2 assumption (and
hence disagrees ), apply Lemma 28 conclude swapping relative ordering
a1 , a2 within (obtaining a2 |b2 |b1 |a1 ) results ranking, 0 , h( 0 ) > 0.
Finally, observe 0 must disagree relative ranking a2 , b2 ,
invoking Lemma 28 shows swap relative ordering a2 , b2 within
(obtaining a1 |a2 |b1 |b2 ) result ranking 0 h( 0 ) > 0. element 0 ranks
b1 b2 , wanted show.
shown exist rankings disagree relative ordering b1
b2 positive probability h. applying Lemma 28 shows swap
relative ordering items b1 , b2 within obtain t+1 h( t+1 ) > 0,
concludes proof.
526

fiEfficient Inference Partial Rankings

A.4 Uniformity C Functions Partial Ranking
thus far shown element C must supported partial ranking.
following, show (up certain class exceptions), element must
assign uniform probability members partial ranking.
Theorem 41. h completely decomposable function supported partial ranking
= 1 | . . . |r |i | =
6 2 = 1, . . . , r, h uniform (i.e.,
1
Q
h() =
|i | ).


establish Theorem 41, must establish two supporting results: (1) Lemma 42
factors h r smaller completely decomposable functions, nonzero everywhere domain, (2) Theorem 43 establishes uniformity completely
decomposable function nonzero everywhere domain.
Lemma 42. completely decomposable Q
function, h, supported partial ranking
= 1 | . . . |r , must factor as: h() = ri=1 h((i )), factor distribution
h((i )) completely decomposable function Si .
Proof. Since h completely decomposable, (i ) riffle independent
(\i ) i. Since h supported partial ranking = )1 | . . . |r , however,
interleaving complement deterministic therefore conclude fact
(i ) fully independent
(\i ). Since (i ) (\i ) i,
Qr
factorization: h() = i=1 h((i )).
turn establishing factor h((i )) completely decomposable
observation. Fix = 1 (without loss generality) consider partition set 1
subsets B. would see sets B riffle independent
respect h((1 )). Since h assumed completely decomposable, know
riffle independent complement, B(\1 ). words, B = B(\1 ),
variables (), AB , B (the relative ranking A, interleaving
remaining items, relative ranking remaining items, respectively) mutually
independent. observe (1) interleaving B, AB , deterministic
function interleaving AB (2) relative ranking B, B , deterministic
function B , thus proving , AB B mutually independent hence
B riffle independent.
Theorem 43. Let h completely decomposable function h() > 0 Sn
n > 2. two rankings 1 , 2 differ single transposition,
h(1 ) = h(2 ).
proof strategy Theorem 43 involve examining ratio two
probabilities h(1 ) h(2 ). define operation transforming 1 2 new
rankings 10 20 ratio rankings preserved (i.e., h(1 )/h(2 ) =
h(10 )/h(20 )). performing sequence ratio-preserving operations, show that:
h(1 )
h(2 )
=
,
h(2 )
h(1 )
Theorem 43 easily follows.
527

fiHuang, Kapoor & Guestrin

use two types operations transform ranking new ranking: (1)
changing interleaving two sets B within ranking , (2), changing
relative ranking set within ranking . precisely, given ranking
partitioning item set subsets B, uniquely index triplet
(, , B ), = A,B (), = (), B = B (). two operations
defined follows:
1. Changing interleaving A, B within 0 : yields new ranking 0
indexed ( 0 , , B ).
0 (or 0 ): yields new
2. Changing relative ranking (or B) within
B
0
0
0
ranking indexed (, , B ) [or (, , B )].

use operations obtain 10 20 , interested conditions
transformation ratio-preserving (i.e., h(1 )/h(2 ) = h(10 )/h(20 )).
following lemma provides sufficient conditions ratio-preservation.
Lemma 44. Let h completely decomposable function consider 1 , 2 Sn
h(2 ) > 0. partitioning item set subsets B, have:
1. 1 2 match interleaving B (i.e., A,B (1 ) = AB (2 )),
h(10 )
h(1 )
0
0
h(2 ) = h(20 ) , 1 2 formed changing interleaving sets
B within 1 2 new interleaving 0 .
2. 1 2 match relative ranking (or B) (i.e., (1 ) = (2 ) (or
h( 0 )
h(1 )
= h(10 ) , 10 20 formed changing
B (1 ) = B (2 ))), h(
2)
2
0
relative ranking set (or B) within 1 2 new relative ranking
0
(or B ).
Proof. Since proofs parts 1 2 nearly identical, prove part 1 here.
Since h C, sets B riffle independent assumption, hence
factorizations:
h(1 )
m(1 ) f (1A ) g(2B )
=
.
h(2 )
m(2 ) f (2A ) g(2B )

1 2 match interleaving sets B, = 1 = 2 ,
thus interleaving terms, m(1 ) m(2 ) numerator
denominator.
hand, examine ratio h(10 ) h(20 ), see
interleaving terms must cancel:
h(1 )
m(10 ) f (1A ) g(2B )
=
.
h(2 )
m(20 ) f (2A ) g(2B )

therefore that:
f (1A ) g(piB
h(10 )
h(1 )
1 )
=
=
.

B
h(2 )
h(20 )
f (2 ) g(2 )

528

fiEfficient Inference Partial Rankings

established Lemma 44, turn establishing three short claims (using
lemma) allow us prove finally prove Theorem 43. interesting note
require n > 2 (strictly) claim III swap order j
numerator denominator. third item k proof thought
playing role dummy variable analogous temporary storage variables one
might use implementing swap function. necessity third item precisely
result hold special case n = 2.
Proposition 45. Let h : Sn R completely decomposable function n > 2
h() > 0 Sn . following equivalences (where
ratios, entries explicitly written assumed match identically
numerator denominator).
I.

II.

h(i|j| . . . |k| . . . )
h(i|j|k| . . . )
=
.
h(j|i| . . . |k| . . . )
h(j|i|k| . . . )
h(. . . |i| . . . |j| . . . )
h(i|j| . . . )
=
.
h(. . . |j| . . . |i| . . . )
h(j|i| . . . )

III.

h(j|i|k| . . . )
h(i|j|k| . . . )
=
.
h(j|i|k| . . . )
h(i|j|k| . . . )

Proof.
I. Equality holds since 1 2 match interleaving sets = {k}
B = \{k}. Thus change interleaving B 1 2
item k inserted rank 3 preserving ratio.
II. Equality holds II since 1 2 match interleaving sets = {i, j}
B = \{i, j}. Thus change interleaving B 1
2 items j occupy first two ranks preserving ratio
h(1 ) h(2 ).
III. following use 1 2 refer arguments numerator
denominator, respectively, preceding line.
h(i|j|k| . . . )
h(i|k|j| . . . )
=
,
h(j|i|k| . . . )
h(k|i|j| . . . )
h(j|i|k| . . . )
=
,
h(j|k|i| . . . )
h(i|j|k| . . . )
=
,
h(i|k|j| . . . )
h(k|j|i| . . . )
=
,
h(k|i|j| . . . )
h(j|i|k| . . . )
=
,
h(i|j|k| . . . )

(since 1 , 2 match relative ranking {j, k})
(since 1 , 2 match interleaving {j} \{j})
(since 1 , 2 match relative ranking {i, j})
(since 1 , 2 match relative ranking {i, k})
(since 1 , 2 match interleaving {k} \{k}).

529

fiHuang, Kapoor & Guestrin

Proof. (of Theorem 43) want show two rankings differ single transposition,
assigned equal probability h. Suppose 2 obtained
1 swapping ranks items j. Additionally, let k item besides j
(such item must exist since n > 2). following, use Proposition 45 show
h(1 )/h(2 ) = h(2 )/h(1 ). before, entries explicitly written
assumed match identically numerator denominator.
h(. . . |i| . . . |j| . . . )
h(i|j| . . . )
h(1 )
=
=
, (by Prop. 45, Part II)
h(2 )
h(. . . |j| . . . |i| . . . )
h(j|i| . . . )
h(i|j| . . . |k| . . . )
h(i|j|k| . . . )
=
=
, (by Prop. 45, Part I)
h(j|i| . . . |k| . . . )
h(j|i|k| . . . )
h(j|i|k| . . . )
, (by Prop. 45, Part III)
=
h(i|j|k| . . . )
h(j|i| . . . |k| . . . )
=
, (by Prop. 45, Part I)
h(i|j| . . . |k| . . . )
h(j|i| . . . )
h(. . . |j| . . . |i| . . . )
=
=
, (by Prop. 45, Part II)
h(i|j| . . . )
h(. . . |i| . . . |j| . . . )
h(2 )
=
.
h(1 )

Since assumed h(1 ) h(2 ) > 0, must conclude h(1 ) = h(2 ).
Finally, assemble supporting results prove Theorem 41.
Proof. (of Theorem 41) Lemma 42, completely decomposable function h must factor
as:
r

h() =
h((i )),
(A.2)
i=1

factor distribution h((i )) completely decomposable function Si .
assumption, |i | 6= 2. |i | = 1, corresponding factor h((i )) must trivially
uniform. Otherwise, |i | > 2. latter case, apply Theorem 43
h((i )) show must assign equal probability two rankings differ
single transposition. However, given rankings 1 , 2 Si , obtain sequence
transpositions transforms 1 2 , therefore, Theorem 43 fact implies
factor h((i )) constant inputs. proved factor Equation A.2
constant, conclude h must constant support.

References
Ailon, N. (2007). Aggregation partial rankings, p-ratings top-m lists. Proceedings
eighteenth annual ACM-SIAM symposium Discrete algorithms, SODA 07,
New Orleans, Louisiana.
Bartholdi, J. J., Tovey, C. A., & Trick, M. (1989). Voting schemes
difficult tell won. Social Choice Welfare, 6(2).
530

fiEfficient Inference Partial Rankings

Busse, L. M., Orbanz, P., & Buhmann, J. (2007). Cluster analysis heterogeneous rank
data. 24th Annual International Conference Machine Learning, Corvallis,
Oregon.
Fligner, M. A., & Verducci, J. S. (1986). Distance based ranking models. Journal
Royal Statistical Society, 48.
Freund, Y., Iyer, R., Schapire, R. E., & Singer, Y. (2003). efficient boosting algorithm
combining preferences. Journal Machine Learning Research (JMLR), 4, 933969.
Friedman, N. (1997). Learning belief networks presence missing values hidden variables. Proceedings Fourteenth International Conference Machine
Learning, ICML 97, pp. 125133, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.
Friedman, N. (1998). bayesian structural em algorithm. 14th Conference
Uncertainty Artificial Intelligence, UAI 98, Madison, Wisconsin.
Gormley, C., & Murphy, B. (2007). latent space model rank data. Proceedings
2006 conference Statistical network analysis, ICML06, pp. 90102, Berlin,
Heidelberg. Springer-Verlag.
Huang, J., Kapoor, A., & Guestrin, C. (2011). Efficient probabilistic inference partial
ranking queries. 27th Conference Uncertainty Artificial Intelligence, UAI
11, Barcelona, Spain.
Huang, J. (2011). Probabilistic Reasoning Learning Permutations: Exploiting Structural Decompositions Symmetric Group. Ph.D. thesis, Carnegie Mellon University.
Huang, J., & Guestrin, C. (2009). Riffled independence ranked data. Bengio, Y.,
Schuurmans, D., Lafferty, J., Williams, C. K. I., & Culotta, A. (Eds.), Advances
Neural Information Processing Systems 22, NIPS 08, pp. 799807. MIT Press.
Huang, J., & Guestrin, C. (2010). Learning hierarchical riffle independent groupings
rankings. Proceedings 27th Annual International Conference Machine
Learning, ICML 10, pp. 455462, Haifa, Israel.
Huang, J., & Guestrin, C. (2012). Uncovering riffled independence structure ranked
data. Electronic Journal Statistics, 6, 199230.
Huang, J., Guestrin, C., & Guibas, L. (2008). Efficient inference distributions permutations. Platt, J., Koller, D., Singer, Y., & Roweis, S. (Eds.), Advances Neural
Information Processing Systems 20, NIPS 07, pp. 697704. MIT Press, Cambridge,
MA.
Huang, J., Guestrin, C., & Guibas, L. J. (2009). Fourier theoretic probabilistic inference
permutations. Journal Machine Learning Research (JMLR), 10, 9971070.
Joachims, T. (2002). Optimizing search engines using clickthrough data. Proceedings
eighth ACM SIGKDD international conference Knowledge discovery data
mining, KDD 02, pp. 133142, New York, NY, USA. ACM.
Kondor, R., Howard, A., & Jebara, T. (2007). Multi-object tracking representations
symmetric group. Meila, M., & Shen, X. (Eds.), Proceedings Eleventh
531

fiHuang, Kapoor & Guestrin

International Conference Artificial Intelligence Statistics March 21-24, 2007,
San Juan, Puerto Rico, Vol. Volume 2 JMLR: W&CP.
Kondor, R. (2008). Group theoretical methods machine learning. Ph.D. thesis, Columbia
University.
Lebanon, G., & Lafferty, J. (2003). Conditional models ranking poset. S. Becker,
S. T., & Obermayer, K. (Eds.), Advances Neural Information Processing Systems
15, NIPS 02, pp. 415422, Cambridge, MA. MIT Press.
Lebanon, G., & Mao, Y. (2008). Non-parametric modeling partially ranked data. Platt,
J. C., Koller, D., Singer, Y., & Roweis, S. (Eds.), Advances Neural Information
Processing Systems 20, NIPS 07, pp. 857864, Cambridge, MA. MIT Press.
Lu, T., & Boutilier, C. (2011). Learning mallows models pairwise preferences.
28th Annual International Conference Machine Learning, ICML 11, Bellevue,
Washington.
Marden, J. I. (1995). Analyzing Modeling Rank Data. Chapman & Hall.
Meila, M., Phadnis, K., Patterson, A., & Bilmes, J. (2007). Consensus ranking
exponential model. Tech. rep. 515, University Washington, Statistics Department.
White, R., & Drucker, S. (2007). Investigating behavioral variability web search.
Proceedings 16th international conference World Wide Web, WWW 07,
Banff, Alberta, Canada. ACM.

532


