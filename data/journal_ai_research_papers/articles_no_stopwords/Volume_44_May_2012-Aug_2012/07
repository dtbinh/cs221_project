journal artificial intelligence

submitted published

riffled independence efficient inference
partial rankings
jonathan huang

jhuang stanford edu

james h clark center
stanford university stanford ca usa

ashish kapoor

akapoor microsoft com

microsoft
one microsoft way
redmond wa usa

carlos guestrin

guestrin cs cmu edu

gates hillman complex carnegie mellon university
forbes avenue pittsburgh pa usa

abstract
distributions rankings used model data multitude real world settings
preference analysis political elections modeling distributions presents
several computational challenges however due factorial size set rankings
item set challenges quite familiar artificial intelligence
community compactly represent distribution combinatorially large
space efficiently perform probabilistic inference representations
respect ranking however additional challenge refer
human task complexity users rarely willing provide full ranking long list
candidates instead often preferring provide partial ranking information
simultaneously addressing challenges e designing compactly representable model amenable efficient inference learned partial
ranking data difficult task necessary would scale
nontrivial size recently proposed riffled independence
assumptions cleanly efficiently address challenges particular
establish tight mathematical connection concepts riffled independence
partial rankings correspondence allows us develop efficient
exact performing inference tasks riffled independence representations partial rankings somewhat surprisingly shows efficient inference
possible riffle independent certain sense observations
take form partial rankings finally inference introduce
first method learning riffled independence partially ranked data

probabilistic modeling ranking data three challenges
rankings arise number machine learning application settings preference analysis movies books lebanon mao political election analysis gormley
murphy huang guestrin many great interest
build statistical ranking data order make predictions form recommendations discover latent trends structure construct human comprehensible data
summaries
c

ai access foundation rights reserved

fihuang kapoor guestrin

modeling distributions rankings difficult however due fact
number items ranked increases number possible rankings increases
factorially combinatorial explosion forces us confront three central challenges
dealing rankings first need deal storage complexity compactly represent distribution space rankings algorithmic complexity efficiently answer probabilistic inference queries given distribution
finally must contend refer human task complexity
challenge stemming fact difficult accurately elicit full ranking
large list candidates human user choosing list n options easy task
users typically prefer provide partial information take american psychological
association apa elections example allow voters rank order candidates
favorite least favorite election five candidates therefore
ways rank five candidates despite small candidate list voters
election preferred specify top k favorite candidates rather writing
full rankings ballots see figure example roughly third voters
simply wrote single favorite candidate election
three intertwined challenges storage algorithmic human task complexity
central issues probabilistic modeling rankings efficiently
handle three sources complexity limited applicability examine
flexible intuitive class rankings generalization probabilistic
independence called riffled independence proposed recent work huang guestrin
previous papers focused primarily representational storage
complexity issues concentrate inference incomplete observations e partial
rankings showing addition storage complexity riffle independence
efficiently address issues algorithmic human task complexity
fact two issues algorithmic human task complexity intricately linked
riffle independent considering partial rankings give users flexibility
provide much little information care give context partial
ranking data relevant inference queries take form partial rankings
example might want predict voters second choice candidate given information
first choice one main contributions inference
partial ranking queries performed particularly efficiently riffle independent

main contributions work follows
reveal natural fundamental connection riffle independent
partial rankings particular collection partial rankings
item set form complete characterization space observations upon
note common wonder one would care represent distribution rankings
number sample rankings never nearly large number samples
much smaller n however means rankings never observed limiting ability
estimate probability arbitrary ranking way overcome paucity samples
exploit representational structure much alignment solving storage complexity
issue
extended presentation huang kapoor guestrin appeared
conference uncertainty artificial intelligence uai well first
authors dissertation huang



fiefficient inference partial rankings

first
choice

second
choice

third
choice

fourth
choice

fifth
choice


votes





















































































figure example partial ranking data taken american psychological association
election dataset

one efficiently condition riffle independent model
ranked items satisfy riffled independence relationship conditioning
partial rankings done efficiently running time n h h denotes
number model parameters
prove sense formalize impossible efficiently condition
riffle independent observations take form partial rankings
propose first capable efficiently estimating structure
parameters riffle independent heterogeneous collections partially
ranked data
real voting preference data evidencing effectiveness
methods

riffled independence rankings
ranking items item set one one mapping rank
set r n denoted vertical bar notation n
say ranks item item rank less
rank example might corn p eas apples oranges ranking
corn p eas apples oranges encodes preference corn peas turn preferred apples collection possible rankings item set denoted
sn implicit
since n rankings n items intractable estimate even explicitly
represent arbitrary distributions sn without making structural assumptions
underlying distribution many possible simplifying assumptions one
make focus proposed recent papers huang guestrin
ranks items assumed satisfy intuitive generalized notion
probabilistic independence known riffled independence argue
riffled independence assumptions particularly effective settings one would
make queries taking form partial rankings remainder section
review riffled independence


fihuang kapoor guestrin

riffled independence assumption posits rankings item set generated independently generating rankings smaller disjoint item subsets say
b partition piecing together full ranking interleaving riffle shuffling
smaller rankings together example rank item set foods one might first
rank vegetables fruits separately interleave two subset rankings form
full ranking formally define riffled independence use notions relative rankings
interleavings
definition relative ranking map given ranking subset
relative ranking items ranking sa j
j
definition interleaving map given ranking partition disjoint
sets b interleaving b denoted ab binary mapping
rank set r n b indicating whether rank occupied
b rankings denote interleaving ranking vertical bar notation
ab ab ab n
example consider partitioning item set vegetables corn p eas
fruits b apples oranges well full ranking four items
corn oranges p eas apples case relative ranking vegetables
corn p eas relative ranking fruits b oranges apples interleaving vegetables fruits ab b b
definition riffled independence let h distribution consider subset
items complement b sets b said riffle independent
h decomposes factors
h mab ab fa gb b
distributions mab fa gb defined interleavings relative rankings
b respectively words b riffle independent relative rankings
b well interleaving mutually independent refer mab
interleaving distribution fa gb relative ranking distributions
riffled independence found approximately hold number real datasets
huang guestrin relationships identified data instead
exhaustively representing n ranking probabilities one represent factors
mab fa gb distributions smaller sets
hierarchical riffle independent
relative ranking factors fa gb distributions rankings
reduce parameter space natural consider hierarchical decompositions item sets
nested collections partitions hierarchical clustering example figure
shows hierarchical decomposition vegetables riffle independent fruits among
healthy foods healthy foods turn riffle independent subset
desserts doughnuts


fiefficient inference partial rankings

c p



c p

doughnuts ms

c p



corn peas

apples oranges

figure example hierarchy six food items
simplicity restrict consideration binary hierarchies defined tuples
form h ha hb ha hb null case h called leaf
hierarchies item sets b respectively second case b
assumed form nontrivial partitioning item set
definition say distribution h factors riffle independently respect
hierarchy h ha hb item sets b riffle independent respect h
fa gb factor riffle independently respect subhierarchies ha hb
respectively
bayesian networks hierarchies represent families distributions obeying
certain set riffled independence constraints parameterized locally draw
model one generates full rankings recursively starting drawing rankings
leaf sets working tree sequentially interleaving rankings reaching
root parameters hierarchical simply interleaving relative
ranking distributions internal nodes leaves hierarchy respectively
general number total parameters required represent hierarchical riffle
independent model bayesian networks still scale exponentially number

items example number interleavings p items n p items np
often case however much fewer parameters necessary example thin
huang guestrin number items factored model
stage hierarchy never small constant k represented
degree k polynomial number parameters use h refer number
parameters necessary representing distribution factors according hierarchy
h
decomposing distributions rankings small pieces bayesian networks
done distributions hierarchical allow better interpretability
efficient probabilistic representation low sample complexity efficient map optimization
efficient inference
example figure reproduce hierarchical structure learned
fully ranked subset apa data consisting training examples huang
guestrin five candidates election william bevan ira
iscoe charles kiesler max siegle logan wright marden strikingly
structure learned maximum likelihood knows nothing
underlying politics apa leaf nodes correspond exactly
political coalitions dominated apa election psychologists


fihuang kapoor guestrin



b

c
community
psychologists

mb c


b c b b b



b b c b b



b b b c b



b b b b c





md e



fc

e e







e e



e e





e

e e




psychologists

clinical
psychologists

e e



e e



hierarchical structure learned
via mle full rankings
apa dataset


c b b b b



fd



fe

















b riffle independent model parameters learned via mle
full rankings apa dataset

figure example hierarchical model apa election candidates enumerated
william bevan ira iscoe charles kiesler max siegle
logan wright marden

candidates clinical psychologists candidates community
psychologists candidate
figure b plot corresponding parameter distributions learned via
maximum likelihood three relative ranking distributions corresponding
political party well two interleaving distributions one interleaving
clinical psychologists one interleaving community psychologist
remaining candidates since parameter distribution constrained sum
total free parameters
model estimation
estimate riffle independent methods introduced
earlier work given hierarchial structure model maximum likelihood parameter
estimates hierarchical riffle independent model straightforward compute via frequency estimates estimate correct structure model challenging
key insight lies noticing two subsets b riffle independent
j k b independence relation j k must hold
structure learning operate hunting tripletwise independence
relations within data defer interested readers details huang guestrin



fiefficient inference partial rankings

note earlier work assumed access dataset
consisting full rankings provided users current work relax
assumptions allowing users provide partially ranked data one assumption throughout however user full ranking mind items particular
current work address incomplete ranking users might
seen items discuss possible extensions incomplete ranking setting
section

decomposable observations
given prior distribution h rankings observation bayes rule tells us
posterior distribution h proportional l h l
likelihood function operation conditioning h observation typically computationally intractable since requires multiplying two n dimensional functions unless
one exploit structural decompositions section describe decomposition certain class likelihood functions space rankings
observations factored simpler parts observation decomposable
way one efficiently condition riffle independent prior distribution
simplicity focus primarily subset observations whose likelihood
functions encode membership subset rankings sn
definition subset observations subset observation binary observation whose
likelihood proportional indicator function subset sn e



l
otherwise
running example consider class first place observations throughout
chapter consider far general observation later sections first
place observation corn ranked first example associated collection
rankings placing item corn first place corn interested
computing posterior h thus first place scenario given voters
top choice would infer preferences remaining candidates
given partitioning item set two subsets b sometimes possible
decompose factor subset observation involving items smaller subset observations involving b interleavings b independently decompositions
often exploited efficient inference
example
consider first place observation
corn ranked first
decomposed two independent observations observation
relative ranking vegetables observation interleaving vegetables
fruits


fihuang kapoor guestrin

oa corn ranked first among vegetables
oa b first place occupied vegetable
condition case one updates relative ranking distribution
vegetables zeroing rankings vegetables place corn first
place updates interleaving distribution zeroing interleavings
place vegetable first place normalizes resulting distributions
example nondecomposable observation observation
corn third place
see decompose respect vegetables fruits enough
notice interleaving vegetables fruits independent relative
ranking vegetables example element interleaves vegetables
b fruits ab b b since corn relative ranking
vegetables constrained p eas corn since interleavings
relative rankings independent see cannot decomposable
formally use riffle independent factorizations define decomposability respect
hierarchy h item set
definition decomposability given hierarchy h item set subset observation decomposes respect h likelihood function l factors riffle
independently respect h
subset observations prior decompose according hierarchy
example posterior decomposes
proposition let h hierarchy item set given prior distribution h
subset observation decompose respect h posterior distribution
h factors riffle independently respect h
proof denote likelihood function corresponding l proof
matter assumed subset observation holds arbitrary
likelihoods
use induction size item set n base case n trivially
true next consider general case n posterior distribution bayes rule
written h l h two cases h leaf node
posterior h trivially factors according h done otherwise l h
factor assumption according h ha hb following way
l ml ab fl gl b h mh ab fh gh b
multiplying grouping terms see posterior factors
h ml mh ab fl fh gl gh b
h factors respect h need demonstrate definition
distributions fl fh gl gh normalizing factor respect ha


fiefficient inference partial rankings

hb respectively since fl fh factor according hierarchy ha assumption
n since h leaf invoke inductive hypothesis
posterior distribution proportional fl fh must factor according ha
similarly distribution proportional gl gh must factor according hb

complete decomposability
condition proposition prior observation must decompose respect exactly hierarchy sufficient one efficient inference might
first glance seem restrictive render proposition useless practice overcome
limitation hierarchy specific decomposability explore special family observations call completely decomposable property decomposability
depend specifically particular hierarchy implying particular
observations efficient inference possible provided efficient representation
prior distribution possible
illustrate observation decompose respect multiple hierarchies
item set consider first place observation corn ranked first argued
example decomposable observation notice however decomposability
particular observation depend items partitioned
hierarchy specifically instead vegetables fruits sets corn apples
b p eas oranges riffle independent similar decomposition would continue
hold decomposing observation relative ranking items corn
first among items observation interleaving b first place
occupied element
formally capture notion observation decompose respect
arbitrary underlying hierarchies define complete decomposability
definition complete decomposability say subset observation completely decomposable decomposes respect every possible hierarchy item
set denote collection possible completely decomposable subset observations c see figure illustration set c
conceptually completely decomposable observations correspond indicator functions
riffle independent possible complete decomposability guarantee
observation one exploit available factorized structure prior
distribution order efficiently condition
proposition let h binary hierarchy item set given prior h
factorizes respect h completely decomposable observation posterior
h decomposes respect h
proof proposition follows simple corollary proposition
example simplest example completely decomposable observation uniform observation ounif includes possible rankings corresponds
uniform indicator function unif rankings given hierarchy h unif shown
decompose riffle independently respect h factor uniform
hence ounif completely decomposable


fihuang kapoor guestrin

h

h

h

completely
decomposable
observations

h

h

h

figure diagram illustrating collection completely decomposable observations c
shaded region labeled hi represents family subset observations
sn decompose respect hierarchy hi collection c
seen intersection shaded regions subset observations
lie inside intersection ones conditioning performed
linear time number model parameters

uniform observation course particularly interesting context bayesian
inference hand given stringent conditions definition
obvious nontrivial completely decomposable observations even exist nonetheless
exist nontrivial examples first place observations next
section exhibit rich general class completely decomposable observations

complete decomposability partial ranking observations
section discuss mathematical fully characterizing class
completely decomposable observations main contribution section
completely decomposable observations correspond precisely partial rankings
item set
partial rankings begin discussion introducing partial rankings allow
items tied respect ranking dropping verticals vertical bar
representation
definition partial ranking observation let r ordered collection
subsets partition e j j partial ranking
observation corresponding partition collection rankings rank items
remarked ailon note term partial ranking used confused
two standard objects partial order namely reflexive transitive anti symmetric binary



fiefficient inference partial rankings

items j j denote partial ranking r say
type r denote collection partial rankings
n items p
partial ranking defined viewed coset subgroup
sr given type full ranking one
partial ranking type containing thus therefore equivalently denote partial
ranking r element r note coset
notation allows multiple rankings refer partial ranking
space partial rankings defined captures rich natural class
observations particular partial rankings encompass number commonly occurring
special cases traditionally modeled isolation work well
recent works lebanon lafferty lebanon mao used
unified setting
example partial ranking observations include
first place top observations first place observations correspond partial
rankings type n observation corn ranked first
written corn peas apples oranges
top k observations top k observations partial rankings type n
k generalize first place observations specifying items mapping
first k ranks leaving n k remaining items implicitly ranked behind example
observation corn ranked first peas ranked second written
corn peas apples oranges
desired less desired dichotomy partial rankings type k n k correspond
subset k items preferred desired remaining subset n k items
example partial rankings type k n k might arise approval voting
voters mark subset approved candidates implicitly indicating disapproval
remaining n k candidates
ratings finally partial rankings come form rating data
example restaurants rated corresponding partial ranking
would thus tie restaurants rated number stars ranking
restaurants stars restaurants fewer stars
trivial observations partial rankings type n refer trivial observations
whose likelihood functions uniform entire space rankings trivial
observation rankings item set corn p eas apples example
simply written simply corn p eas apples
partial ranking observations decompose exhibit explicit factorization respect hierarchy h items simplicity begin considering
single layer case items partitioned two leaf sets b factorization depends following notions consistency relative rankings interleavings
partial ranking
relation ranking subset discuss section incomplete rankings
search engines example although top k elements returned remaining n k
implicitly assumed ranked behind therefore search engines return partial rankings



fihuang kapoor guestrin

definition restriction consistency given partial ranking r
subset define restriction partial ranking items
obtained intersecting hence restriction
r
given ranking items say consistent partial ranking
member restriction
definition interleaving consistency given interleaving ab two sets b
partition say ab consistent partial ranking r
type first entries ab contain number bs
second entries ab contain number bs given
partial ranking denote collection consistent interleavings ab
example consider partial ranking
corn apples p eas oranges
places single vegetable single fruit first two ranks single vegetable
single fruit last two ranks alternatively partially specifies interleaving
ab ab full interleavings b b b b consistent dropping
vertical lines b b consistent since places two vegetables first two
ranks
notions consistency partial ranking partial ranking
observations decomposable respect binary partitioning e single layer
hierarchy item set
proposition single layer hierarchy partial ranking observation
binary partitioning item set b indicator function factors riffle
independently
mab ab fa gb b



factors mab fa gb indicator functions consistent interleavings
relative rankings ab b respectively
single layer decomposition proposition turned recursive decomposition partial ranking observations arbitrary binary hierarchies establishes
main particular given partial ranking prior distribution
factorizes according hierarchy h first condition topmost interleaving distribution zeroing parameters corresponding interleavings consistent
normalizing distribution need condition subhierarchies
ha hb relative rankings b consistent respectively
since consistent sets b partial rankings
conditioning partial ranking applied recursively
subhierarchies ha hb precise
theorem every partial ranking completely decomposable p c


fiefficient inference partial rankings

prcondition prior hprior hierarchy h observation r
isleaf h
forall

hprior
hpost


otherwise
normalize hpost
return hpost
else
forall

mprior ab
mpost


otherwise
normalize mpost
f prcondition fprior ha
g b prcondition gprior hb b
return mpost fpost gpost

pseudocode prcondition recursively conditioning hierarchical riffle independent prior distribution partial ranking observations see definitions
b ab runtime prcondition n h h
number model parameters input parameter distributions prior hprior represented explicit tabular form observation form partial ranking output
parameter distributions posterior hpost represented explicit tabular form

since proof theorem fairly straight forward given form factorization equation deferred appendix consequence theorem
proposition conditioning partial ranking observations performed efficiently
see details recursive conditioning
running time complexity conditioning partial ranking recursion
operates parameter distribution setting probabilities
interleavings relative rankings distribution zero
normalizing decide whether zero probability one must check partial
ranking consistency interleaving relative ranking requires
n time therefore total requires n h time h
total number model parameters notice complexity conditioning depends
linearly complexity prior whenever prior distribution compactly
represented efficient inference partial ranking observations possible
stated section h general scale exponentially n thin chain
number items factored model stage never
small constant k verifying interleaving relative ranking consistency performed
constant time implying conditioning operation linear number model
parameters guaranteed polynomial n
example example consider conditioning apa distribution example observation candidate ranked first place
represented partial ranking recall candidate charles
kiesler psychologist
figure structure parameters prior distribution
apa election data highlighting particular interleavings relative rankings


fihuang kapoor guestrin



mb c



c b b b b



c b b b b

mb c


b c b b b



b c b b b



b b c b b



b b c b b



b b b c b



b b b c b



b b b b c



b b b b c





md e



fc



md e



fc

e e







e e







e e



e e



e e



e e



e e



e e



e e



e e



e e



e e





fd



fe



fd



fe

































structure parameters prior distribution consistent relative rankings interleavings highlighted

b structure parameters posterior distribution conditioning

figure example conditioning apa hierarchy example first place
observation candidate ranked first place

consistent example possible interleavings psychologists
clinical psychologists e interleavings consistent
rank psychologist first among clinical psychologists
therefore three consistent interleavings e e e e e e
conditioning sets relative rankings interleavings consistent
zero normalizes resulting parameter distribution resulting riffle
independent representation posterior distribution shown figure b

impossibility
interesting consider completely decomposable observations exist beyond partial
rankings one main contributions observations
theorem converse theorem every completely decomposable observation takes
form partial ranking c p
together theorems form significant insight nature rankings
showing notions partial rankings riffled independence deeply connected
fact shows even possible define partial rankings via complete
decomposability
practical matter theorem shows simple
multiplicative updates parameters exactly condition observations
take form partial rankings computational complexity conditioning
observations partial rankings remains open conjecture approximate
inference approaches may necessary efficiently handling complex observations


fiefficient inference partial rankings

proof impossiblity theorem
turn proving theorem since proof significantly longer less obvious
proof converse theorem sketch main ideas drive proof
refer interested readers details appendix
recall definition linear span set vectors vector space
intersection linear subspaces containing set vectors prove theorem
introduce analogous concepts span set rankings
definition rspan pspan let x sn collection rankings define
pspan x intersection partial rankings containing x similarly define
rspan x intersection completely decomposable observations containing x
formally


pspan x
rspan x

xo oc

xs

example x corn p eas apples apples p eas corn checked
partial ranking three items containing items x entire set
thus pspan x corn p eas apples
proof strategy establish two claims pspan set
partial ranking fact rspan pspan set x exactly
sets since claim fact partial rankings involve riffled
independence defer related proofs appendix thus
lemma x sn pspan x partial ranking
proof see appendix
following discussion instead sketch proof claim first however
theorem must hold indeed true claims hold
proof theorem given c want p claim
rspan pspan since element c however
rspan thus pspan finally lemma claim guarantees
pspan partial ranking conclude p
proceed establish claim rspan x pspan x following
proposition lists several basic properties rspan use several
proofs follow directly definition write proofs
proposition
monotonicity x x rspan x
ii subset preservation x x x x rspan x rspan x
iii idempotence x rspan rspan x rspan x
one inclusion proof rspan x pspan x follows directly fact
p c theorem


fihuang kapoor guestrin

formpspan x
x x
xt disagree relative ordering items
xt
foreach xt
add partial ranking obtained deleting vertical bar items
xt

return element xt

pseudocode computing pspan x formpspan x takes set partial
rankings full rankings x input outputs partial ranking iteratively
deletes vertical bars elements x agreement note necessary
keep track ease notation proofs
direct way computing pspan x simplifies proof main theorem

lemma subset orderings x rspan x pspan x
proof fix subset x sn let element rspan x would
element pspan x consider partial ranking p covers x
e x want see theorem p c
therefore c since rspan x x conclude
definition rspan since holds partial ranking covering x
pspan x
remains task establishing reverse inclusion
proposition subset orderings x rspan x pspan x
prove proposition consider computing partial ranking span
pspan given set rankings x simple procedure
iteratively finding rankings x disagree pairwise ranking two items
replacing rankings partial ranking vertical bar two
elements removed provably outputs correct

proposition given set rankings x input outputs pspan x
proof see appendix
final step able prove proposition prove following two
technical lemmas relate computation pspan riffled independence really form heart argument particular completely
decomposable observation c lemma shows ranking contained
force rankings contained
lemma let c suppose exist disagree relative
ranking items j ranking obtained swapping relative ranking
items j within must contained


fiefficient inference partial rankings

proof let h indicator distribution corresponding observation
swapping relative ranking items j ranking
assigned nonzero probability h thus showing ranking contained
let j b since c h must factor riffle independently according
partition b thus
h ab f g b
h ab f g b
since disagree relative ranking items factorization implies
particular f j f j since h must
ab f g b positive probability
therefore swap relative ranking obtain ranking positive
probability since terms decomposition ranking positive
probability
lemma provides conditions removing vertical bar one
rankings x change support completely riffle independent distribution illustrate example consider completely decomposable observation
contains partial ranking corn p eas apples oranges subset
lemma guarantees addition exists element disagrees
relative ordering say p eas oranges fact partial ranking
corn p eas apples oranges bar removed must
subset formally
lemma let k partial ranking item set
k partial ranking sets
merged let ij j kj j element c
additionally exists ranking disagrees relative
ordering
proof key strategy proof lemma argue large subsets rankings
must contained completely decomposable observation decomposing rankings
transpositions invoking technical lemma lemma repeatedly
see appendix details
use lemma reverse inclusion proposition
holds establishing two sets rspan x pspan x fact equal thereby
proving desired c p
proof proposition iteration producess
set partial rankings
xt denote union partial rankings time xt xt note
x x xt pspan x idea proof iteration
following set inclusion holds rspan xt rspan xt indeed holds


fihuang kapoor guestrin

final iteration shown
pspan x xt

proposition

rspan xt

monotonicity proposition

rspan x

since rspan xt rspan xt shown

rspan x

x x see

would prove proposition
remains rspan xt rspan xt claim xt rspan xt
let xt xt since xt rspan xt rspan xt
proof done otherwise xt xt second case use fact
iteration vertical bar deleted partial ranking k subset xt form partial ranking
k subset xt furthermore order
vertical bar deleted must existed partial
ranking therefore full ranking disagreed relative ordering items opposite sides bar since xt xt assume

would apply lemma note c xt
since xt application lemma shows
therefore
shown fact holds observation c xt
therefore taking intersection supports c see xt
rspan xt taking rspan sides yields
rspan xt rspan rspan xt
rspan xt

subset preservation proposition

idempotence proposition

going beyond subset observations
though stated far subset observations comment
theory would look considered general likelihood functions
order avoid confusion refer general class functions call completely decomposable functions instead completely decomposable subset observations
definition
definition function h sn r called completely decomposable function
factors riffle independently respect every hierarchy item set denote
e
collection possible completely decomposable functions c
e nearly quite simple restate theorem
discuss c c
respect general case completely decomposable functions
theorem every partial ranking indicator function completely decomposable function


fiefficient inference partial rankings

unfortunately proof converse theorem easily generalize
instead used support sn h every completely
decomposable function partial ranking natural however suspect full
converse indeed exist every completely decomposable function proportional
indicator function partial ranking fact suspected converse almost
holds
theorem h completely decomposable function supported partial ranking
r
r h proportional indicator
function
proof see appendix
example completely decomposable functions possible away
assumption
example function defined

corn p eas apples
p eas corn apples
h


otherwise
supported partial ranking corn p eas apples
proportional indicator function e uniform rankings
assigned positive probability
however still possible h completely decomposable function
prove necessary establish three things corn p eas apples
riffle independent corn apples p eas riffle independent
p eas apples corn riffle independent example respect partitioning sets corn apples b p eas see
h ab f g b




ab



ab b
ab b
ab b


f




ac c

otherwise

g b

therefore possible completely decomposable functions
uniform supports
conditioning noisy observations
conclude section remark handling noise observations
assumed observed partial rankings consistent users
underlying full ranking situations one may wish model noisier
setting partial rankings may misreported small probability
natural model accounts noise example might


l



otherwise


fihuang kapoor guestrin

prior distribution factorizes respect hierarchy h conditioning
noisy likelihood equation posterior distribution written
weighted mixture prior distribution posterior would resulted
conditioning noise free observation component posterior
distribution factorizes respect h mixture factor general
factor according theory iteratively conditioning multiple
partial rankings according noisy likelihood function would quickly lead
unmanageable number mixture components therefore believe approximate
inference methods conditioning multiple noisy partial ranking observations fruitful
area

model estimation partially ranked data
many ranking applications datasets predominantly composed partial rankings rather full rankings due fact humans partial rankings typically
easier faster specify addition many datasets heterogeneous containing partial
ranking different types example american psychological assoication well
irish house parliament elections voters allowed specify top k candidate
choices value k see figures b section use efficient
inference proposed section estimating riffle independent model
partially ranked data estimating model partially ranked data typically
considered difficult estimating one full rankings common practice e g see huang guestrin simply ignore partial rankings
dataset ability method incorporate available data however lead
significantly improved model accuracy well wider applicability method
section propose first efficient method estimating structure parameters
hierarchical riffle independent model heterogeneous datasets consisting arbitrary
partial ranking types central idea given someones partial preferences use efficient developed previous section infer full
preferences consequently apply previously proposed designed
work full rankings
censoring interpretations partial rankings
model estimation full rankings stated follows given training
examples consisting full rankings drawn hierarchical riffle independent distribution h recover structure parameters h
partial ranking setting assume draws training
example undergoes censoring process producing partial ranking consistent
example censoring might allow ranking top k items
observed allow arbitrary types partial rankings arise via censoring
make common assumption partial ranking type resulting censoring
depend


fiefficient inference partial rankings


treat model estimation partial rankings missing data
many could determine full ranking corresponding observation data could apply work completely observed
data setting since full rankings given utilize expectation maximization em
use inference compute posterior distribution full rankings
given observed partial ranking case apply huang
guestrin designed estimate hierarchical structure
model parameters dataset full rankings
given initial model h collection training examples
consisting partial rankings em alternates following two
steps convergence achieved
e step observation training examples use
inference compute posterior distribution full ranking could
generated via censoring h since observations take
form partial rankings hence completely decomposable use efficient
section perform e step
step step one maximizes expected log likelihood training
data respect model hierarchical structure model
provided known beforehand step performed standard methods optimizing parameters structure unknown use structural
em analogous methods graphical literature
structure learning incomplete data friedman
unfortunately riffled independence structure learning huang
guestrin unable directly use posterior distributions computed
e step instead observing sampling riffle independent
done efficiently exactly opposed example mcmc methods simply
sample full rankings posterior distributions computed e step
pass full rankings structure learning huang guestrin
number samples necessary instead scaling factorially scales
according number samples required detect riffled independence
mild assumptions polynomial n huang guestrin

related work
rankings permutations recently become active area machine
learning due part hinge role play information retrieval preference
elicitation ranksvm joachims rankboost freund
iyer schapire singer example successful large scale ranking
appear web search main aims work differ web
scale settings however instead seeking single optimal ranking respect
objective function seek understanding large collection rankings via density
estimation following outline two major lines influenced
work


fihuang kapoor guestrin

additive multiplicative decompositions
builds particular upon thread recent work tractable permutation data function decompositions kondor howard jebara
huang guestrin guibas considered additive decompositions distribution weighted sum fourier basis functions papers low frequency
fourier assumptions often effective coping representational complexity
working distributions permutations particular conditioning
prior distributions low frequency likelihood functions often arise multiobject
tracking performed especially efficiently
unfortunately low frequency assumptions applicable distributions defined
rankings address ranking specifically huang guestrin
introduced concept riffled independence useful generalization probabilistic independence rankings multiplicative decompositions riffled
independence showed possible learn hierarchical structure model
given fully ranked dataset previous papers topic riffled independence
focused related efficiently representing distributions main focus
current lies efficient reasoning inference tackling human task complexity
considering partial rankings
interesting note natural efficient condition fourier
representation low frequency observations involving small number items
alice third place multiplicative decomposition riffled independence
would able efficiently condition observation hand
multiplicative decompositions allow us condition top k observations efficiently independently size k whereas top k observations would difficult handle
fourier theoretic setting except small k
mallows
work fits larger body well known mallows distribution
rankings parameterized
h



function refers kendalls tau distance metric rankings mallows
distribution equation shown special case hierarchical riffle independent model items sequentially factored model one
one huang see figure
mallows well similar distance advantage
compactly represent distributions large n admit conjugate prior
distributions meila phadnis patterson bilmes estimating parameters
popular statisticians recovering optimal data known
consensus ranking rank aggregation known n p hard bartholdi
tovey trick many authors focused approximation instead
gaussian distributions mallows tend lack flexibility lebanon
mao propose nonparametric model ranked partially ranked data
placing weighted mallows kernels top training examples


fiefficient inference partial rankings

corn peas apples oranges doughnuts

corn

peas apples oranges doughnuts

peas

apples oranges doughnuts
apples

oranges doughnuts
oranges

doughnuts

figure mallows model factors according refer chain
structure items factored one one mallows distribution five items food item set mode central ranking
corn p eas apples oranges doughnuts example must factor according hierarchical structure

realize far richer class distributions learned efficiently however
address inference immediately clear many mallows
papers whether one efficiently perform inference operations marginalization
conditioning riffle independent hand encompass
class distributions rich well interpretable additionally
identified precise conditions efficient conditioning possible conditions
observations take form partial rankings
several recent works model partial rankings mallows
busse orbanz buhmann learned finite mixtures mallows topk data em lebanon mao mentioned
developed nonparametric model mallows handle arbitrary
types partial rankings settings central marginalize mallows
model full rankings consistent particular partial ranking
efficiently papers rely fact first shown fligner verducci
marginalization step performed closed form closed form equation fligner
verducci however seen special case setting since mallows
shown factor riffle independently according chain structure
specifically compute sum rankings consistent partial ranking
necessary condition compute normalization constant
resulting function conditioning step performed methods
described normalization constant computed multiplying
normalization constant factor hierarchical decomposition thus instead
resorting complicated mathematics inversion combinatorics theory
complete decomposability offers simple conceptual way understand mallows
conditioned efficiently partial ranking observations


fihuang kapoor guestrin

finally recent related work lu boutilier considered even general
class observations dag directed acyclic graph observations
probabilities rankings consistent dag relative ranking relations
set zero lu boutilier particular conditioning
dag class observations p hard additionally propose efficient
rejection sampling method performing probabilistic inference within general class
dag observations prove sampling method exact class partial
rankings discussed

experiments
section demonstrate method learning hierarchical riffle independent partial rankings simulated data well real datasets taken different
domains experiments initialize distributions uniform use random restarts
datasets
addition roughly full rankings apa dataset top k rankings
candidates previous work used full rankings apa data huang
guestrin able use entire dataset figure plots
k number ballots apa data length k
likewise meath dataset gormley murphy taken
irish parliament election top k rankings candidates apa
data used full rankings meath data previous work use
entire dataset figure b plots k number ballots
meath data length k particular note vast majority ballots dataset
consist partial rather full rankings half electorate preferring list
favorite three four candidates run inference
top k examples meath data seconds dual ghz pentium machine
unoptimized python implementation brute force inference estimate
job would require roughly one hundred years
extracted third dataset database searchtrails collected white
drucker browsing sessions roughly users logged many cases users unlikely read articles news story twice
often possible think order user reads collection
articles top k ranking articles concerning particular story topic ability
model visit orderings would allow us make long term predictions user browsing
behavior even recommend curriculums articles users ran
roughly visit orderings eight popular posts www huffingtonpost com
concerning sarah palin popular subject u presidential election since
user visited every article full rankings data thus
even exist option learning subset full rankings


fiefficient inference partial rankings

number votes

number votes

























number candidates






k

apa election data

b irish election data

figure histograms top k ballot lengths apa irish election datasets whereas
majority electorate provided full rankings apa election data
probably due fact five candidates vast majority
voters irish election data provided top top choices





















structure learned
subset full rankings
given training
examples








b structure learned
training examples iteration em





community
psychologists


clinical

c structure learned
training examples structural convergence iterations

figure structure learning subset apa dataset rankings randomly
sampled including full partial rankings

apa structure learning
due unordinarily large number full rankings apa data gains made
additionally partially ranked data insignificant better illustrate benefits
partial rankings subsampled dataset rankings including full partial
rankings present smaller dataset performing structure learning
full rankings training examples consisting roughly examples
one obtains structure figure seen match correct
structure figure learned full rankings figures b c


fihuang kapoor guestrin

training time seconds

test log likelihood

x






em

flat em



training time seconds

test log likelihood







k





em

flat em uniform
fill

b training time comparison
em flatem
uniform fill methods

x

k



uniform
fill

test set log likelihood comparison
em flatem
uniform fill methods

k









k

c test set log likelihoods training
top rankings larger
fixed k

k

k

k

k

training times training
top rankings larger fixed
k

figure apa experimental experiment repeated bootstrapped
resamplings data

plot em former displaying resulting structure
single em iteration latter structural convergence occurs
third iteration showing method learn correct structure given
training examples
compared em two alternative baseline approaches
refer plots flatem uniform fill flatem
em except two details performs conditioning exhaustively
instead exploiting factorized model structure performs step without
sampling uniform fill treats every top k ranking training set
uniform collection votes full rankings consistent top k ranking
accomplished one iteration em
figure plot test set loglikelihoods corresponding em
flatem almost identical performing much better
uniform fill hand figure b compares running times
three approaches shows flatem far costly datasets
cannot even run reasonable amount time


fiefficient inference partial rankings

st iteration

nd iteration



rd iteration





































log likelihood










log likelihood
b

log likelihood
c

figure iterations structure em sarah palin data structural changes
iteration highlighted red structural convergence occurs three
iterations note structure discovered visit orders
text information incorporated learning process
figure best viewed color

verify partial rankings indeed make difference apa data plot
estimating model subsets apa training data consisting top k
rankings length larger fixed k figures c log likelihood
running times k k entire training set k
subset training data consisting full rankings
including partial rankings indeed help average improving test log likelihood
diminishing returns
structure discovery em larger n
experiments led several observations em learning partial
rankings first observe typical runs converge fixed structure quickly
three em iterations figure shows progress em sarah palin
data whose structure converges third iteration expected log likelihood
increases iteration remark structure becomes interpretable
example leaf set corresponds three posts palins wardrobe
election posts leaf set related verbal gaffes
made palin campaign notice structure discovered purely
data visit orders text information used experiments


fihuang kapoor guestrin



x

test log likelihood

em iterations
convergence









em
decomposable
conditioning



lebanon mao












k


partial rankings training set
addition full rankings



b

figure number em iterations required convergence training set
contains rankings length longer k b density estimation synthetic
data plot test loglikelihood learning full rankings
additional partial rankings

training
examples



test log likelihood

x






training
examples



x


lm

lm













lm




lm





full

mixed full partial

full

mixed full partial

figure density estimation small examples large subsets examples meath data compare method work lebanon
mao two settings training available data training
subset full rankings

secondly number em iterations required reach convergence log likelihood
depends types partial rankings observed ran subsets
meath dataset time training rankings length larger


fiefficient inference partial rankings

fixed k figure shows number iterations required convergence
function k bootstrap trials k observe fastest convergence
datasets consisting almost full rankings slowest convergence consisting
almost empty rankings almost iterations necessary one trains rankings
types finally remark model obtained first iteration em
interesting thought pretending voter completely
ambivalent regarding n k unspecified candidates
value partial rankings
verify larger n partial rankings addition full rankings
allows us achieve better density estimates first learned synthetic data
drawn hierarchy training full rankings plus varying numbers partial
ranking examples ranging repeat setting bootstrap
trials evaluation compute log likelihood testset examples
speed learn structure h fix h learn parameters trial
figure b plots test log likelihood function number partial
rankings made available training set shows indeed able learn
accurate distributions data form partial rankings made
available
comparing nonparametric model
comparing performance riffle independent approaches possible previous work since able handle partial rankings
methods developed current however compare riffle independent
state art nonparametric estimator lebanon mao
hereby refer lm estimator data setting regularization parameter c via validation set figure b shows naturally
data drawn synthetically riffle independent model em method significantly outperforms lm estimator remark theory lm guaranteed
catch performance appropriate conditions given enough training examples
meath data approximately riffle independent trained subsets
size testing remaining data subset evaluated em
learning riffle independent model lm estimator
full ranking data data methods better
partial rankings made available
smaller training set riffle independent model performs well better
lm estimator larger training set see nonparametric
method starts perform slightly better average advantage nonparametric
model guaranteed consistent converging correct model given
enough data advantage riffle independent however simple
interpretable highlight global structures hidden within data


fihuang kapoor guestrin

future directions
remain several possible extensions current work list open
questions extensions following
inference incomplete rankings
shown one exploit riffled independence structure condition
observation takes form partial ranking space
partial rankings rich useful many settings cover important class
observations incomplete rankings defined ranking partial
ranking subset itemset example theorem shows conditioning pairwise observations form apples preferred bananas
nondecomposable note top k rankings considered complete rankings since
implicitly rank items last n k positions
tractably condition incomplete rankings one possible
convert fourier representation methods huang guestrin
conditioning pairwise ranking observation fourier domain conditioning
proposed huang et al fourier domain would useful one particularly interested low order marginal probabilities posterior
distributions
fourier viable another option may assume
posterior distribution takes particular riffle independent structure way
mean field methods graphical literature would assume factorized
posterior question interest hierarchical structure used
purposes approximating posterior
reexamining data independence assumptions
assumed throughout training examples independent
identically distributed however practice safe assumptions
number factors impact validity example internet survey
user must perform series preference ranking tasks sequence concern
users prior ranking tasks may bias future rankings
another source bias lies reference ranking may displayed
user asked rearrange items dragging dropping one hand showing
everyone reference ranking may bias resulting data hand
showing every user different reference ranking may mean training examples
exactly identically distributed
yet another form bias lies partial ranking types reported data
formulate em assumed users preferences influence
whether chooses say report full ranking instead top ranking practice
however partial ranking types user preferences often correlated irish elections example typically one sinn fein candidate rank
sinn fein first typically likely reported top choice


fiefficient inference partial rankings

understanding identifying finally learning spite different types biases
may occur eliciting preference data remains fundamental ranking
probabilistic modeling strategic voting
interesting consider differences actual vote distributions considered
approximate riffle independent distributions take apa dataset
example optimal approximation riffle independent hierarchy reflects
underlying political coalitions within organization upon comparison
approximation empirical distribution however marked differences arise
example riffle independent approximation underestimates number votes obtained
candidate psychologist ultimately election
one possible explanation discrepancy may lie idea voters tend vote
strategically apa elections placing stronger candidates opposing political coalitions
lower ranking rather revealing true preferences interesting line
future work lies detecting studying presence strategic voting election
datasets open questions include verifying mathematically whether strategic voting
indeed exist say apa election data strategic voting effect
strong enough overwhelm riffled independence structure learning
strategic voting manifest partial ranking votes

conclusion
probabilistic reasoning often case certain data types suggest
certain distribution representations example sparse dependency structure data
often suggests markov random field graphical model representation friedman
low order permutation observations depending items
time recent work huang et al kondor shown fourier domain
representation appropriate preference ranking scenarios one must contend
human task complexity difficulty involved human rank long list items
often leads partially instead fully ranked data shown
data takes form partial rankings hierarchical riffle independent
natural representation
conjugate priors showed riffle independent model guaranteed
retain factorization structure conditioning partial ranking performed efficiently surprisingly work shows observations
take form partial rankings amenable simple multiplicative update
conditioning finally showed possible learn hierarchical riffle
independent partially ranked data significantly extending applicability
previous work

acknowledgments
project formulated largely conducted internship jonathan huang
microsoft additional work supported part onr muri
n aro muri w nf carlos guestrin funded


fihuang kapoor guestrin

part nsf career iis thank eric horvitz ryen white dan liebling
yi mao discussions

appendix proofs
appendix provide supplementary proofs theoretical

proof theorem
prove theorem well later refer rank sets
definition given partial ranking type denote rank set occupied
ri note ri depends
written r
p
r rr r
n
refer following basic fact regarding rank sets
proposition r ri
proof theorem use induction size itemset cases n
trivial since every distribution factors riffle independently consider
general case n
fix partial ranking r type binary partition item
set subsets b indicator function factors
ab f g b



factors f g indicator functions set consistent interleavings
ab sets consistent relative rankings b respectively
equation true shown must decompose respect
top layer h decomposes hierarchically must
relative ranking factors fa gb decompose respect ha hb
subhierarchies item sets b establish second step assuming
equation holds note fa gb indicator functions restricted partial
rankings b partial rankings smaller item sets
b inductive hypothesis fact b assumed strictly
smaller sets shows functions fa gb factor according
respective subhierarchies
turn establishing equation suffices prove following two
statements equivalent
ranking consistent partial ranking e
ii following three conditions hold
interleaving ab consistent e ab ab
b relative ranking consistent e
c relative ranking b consistent e b b


fiefficient inference partial rankings

ii first implies conditions b c

j ri ab j j ri j
k k

definition

proposition


argument replacing b shows j
ri ab j b b two conditions definition
ab consistent
b definition ranks items items j
j intersecting see ranks item
item j j definition ranks item
item j j finally definition
see consistent partial ranking
c argument b
ii assume conditions b c hold
proposition sufficient item k k ri
prove claim induction item k k ri
similarly k b k ri
base case base case assume k goal
k r condition ab ab definition
means j r ab j j r j
words items lie rank set r
item k maps rank r must relative
ranking elements k among first condition b
implying item subset occupies first positions relative
ranking since k item k among first items ranked
therefore k r similar argument shows k b implise
k r
inductive case k k ri condition b
implying item subset hence item k occupies
first positions relative ranking beyond items
j j
inductive hypothesis mutual exclusivity items together


j j b occupy ranks j rj therefore k r
hand condition assures us j ri j
words ranks ri occupied exactly items therefore
k ri similar argument shows k b implies k ri

pspan set partial ranking
reason pspan set rankings first introduce basic concepts
regarding combinatorics partial rankings collection partial rankings


fihuang kapoor guestrin

forms partially ordered set poset obtained
dropping vertical lines example hasse diagram
graph node corresponds partial ranking node x connected
node via edge x exists partial ranking z x z
see lebanon mao top hasse diagram partial ranking n
e bottom hasse diagram lie full rankings see figure
example partial ranking lattice
lemma lebanon mao given two partial rankings
exists unique supremum node ssup sup ssup sup
ssup sup node greater ssup sup similarly
exists unique infimum
lemma given two partial rankings relation holds
lies hasse diagram
proof lies hasse diagram trivial since
obtained dropping vertical bars given lie
would let sinf inf unique infimum
guaranteed lemma definition hasse diagram
obtained dropping verticals vertical bar representation
sinf inf since lie must vertical bar
dropped dropped exist bar
hence must exist pair items j separated single vertical
bar unseparated therefore exists j
even though exists conclude
lemma lemma main body x sn pspan x partial ranking
proof consider subset x sn partial ranking containing every element x
must upper bound every element x hasse diagram lemma
lemma must exist unique least upper bound supremum x ssup sup
common upper bound x must ancestor ssup sup
hence ssup sup therefore see partial ranking containing x must
superset ssup sup hand ssup sup partial ranking containing x
since pspan x intersection partial rankings containing x pspan x
ssup sup therefore pspan x must partial ranking
proofs claim rspan x pspan x
simplify notation remaining proofs introduce following definition
definition ties given partial ranking r say items
tied written respect
following basic properties tie relation straightforward
proposition


fiefficient inference partial rankings


























figure hasse diagram lattice partial rankings
respect fixed partial ranking tie relation equivalence relation
item set e reflexive symmetric transitive
ii exist disagree relative ranking items
respect
iii respect respect
iv respect item

proposition given set rankings x input outputs pspan x
proof prove three things together prove proposition
terminates stage elements x contained pspan x
upon termination pspan x contained element x
first note must terminate finitely many iterations
loop since stage least one vertical bar removed partial ranking
vertical bars removed elements x
disagreements relative ordering
stage every element xt subset
pspan x initialization course x simply singleton
set consisting element x therefore pspan x
suppose pspan x every xt replaced
xt want pspan x well
j j j r written j
j r vertical bar j j deleted due existence
partial ranking xt xt disagrees relative
ordering items opposite sides bar since
subsets pspan x assumption know respect pspan x
proposition ii suppose x
x respect pspan x iii
proposition moreover transitivity see x respect
pspan x two elements iv proposition
items lying thus tied respect pspan x therefore
removing bar items producing example
partial ranking subset pspan x


fihuang kapoor guestrin

finally upon termination ranking x contained element
xt would exist two items whose relative ranking
disagree upon contradiction therefore every element xt contains
every element x thus pspan x every xt

lemma let k partial ranking item set
k partial ranking sets
merged let ij j kj j element c
additionally exists ranking disagrees relative
ordering
proof fix completely decomposable work h indicator
distribution corresponding let prove lemma need establish
h let element k k k
since supp h assumption h
since match items except exists sequence
rankings adjacent rankings sequence differ
pairwise exchange items b b step
along sequence h implies h prove h
suppose h differ relative ranking
items b b without loss generality assume b b
b b
idea following paragraph use previous lemma lemma prove
positive probability necessary argue
exists ranking h b b e disagrees
relative ranking b b let element rearrange
ranked first among elements rearrange
ranked last among elements note still element
possible rearrangements therefore h assume b b
since otherwise shown wanted thus relative ordering
b b within b b note treat case items b b
distinct argument follows cases b b
since disagrees relative ordering assumption
hence disagrees apply lemma conclude swapping relative ordering
within obtaining b b ranking h
finally observe must disagree relative ranking b
invoking lemma shows swap relative ordering b within
obtaining b b ranking h element ranks
b b wanted
shown exist rankings disagree relative ordering b
b positive probability h applying lemma shows swap
relative ordering items b b within obtain h
concludes proof


fiefficient inference partial rankings

uniformity c functions partial ranking
thus far shown element c must supported partial ranking
following certain class exceptions element must
assign uniform probability members partial ranking
theorem h completely decomposable function supported partial ranking
r
r h uniform e

q
h



establish theorem must establish two supporting lemma
factors h r smaller completely decomposable functions nonzero everywhere domain theorem establishes uniformity completely
decomposable function nonzero everywhere domain
lemma completely decomposable q
function h supported partial ranking
r must factor h ri h factor distribution
h completely decomposable function si
proof since h completely decomposable riffle independent
since h supported partial ranking r however
interleaving complement deterministic therefore conclude fact
fully independent
since
qr
factorization h h
turn establishing factor h completely decomposable
observation fix without loss generality consider partition set
subsets b would see sets b riffle independent
respect h since h assumed completely decomposable know
riffle independent complement b words b b
variables ab b relative ranking interleaving
remaining items relative ranking remaining items respectively mutually
independent observe interleaving b ab deterministic
function interleaving ab relative ranking b b deterministic
function b thus proving ab b mutually independent hence
b riffle independent
theorem let h completely decomposable function h sn
n two rankings differ single transposition
h h
proof strategy theorem involve examining ratio two
probabilities h h define operation transforming
rankings ratio rankings preserved e h h
h h performing sequence ratio preserving operations
h
h


h
h
theorem easily follows


fihuang kapoor guestrin

use two types operations transform ranking ranking
changing interleaving two sets b within ranking changing
relative ranking set within ranking precisely given ranking
partitioning item set subsets b uniquely index triplet
b b b b two operations
defined follows
changing interleaving b within yields ranking
indexed b
yields
changing relative ranking b within
b



ranking indexed b b

use operations obtain interested conditions
transformation ratio preserving e h h h h
following lemma provides sufficient conditions ratio preservation
lemma let h completely decomposable function consider sn
h partitioning item set subsets b
match interleaving b e b ab
h
h


h h formed changing interleaving sets
b within interleaving
match relative ranking b e
h
h
h formed changing
b b h



relative ranking set b within relative ranking

b
proof since proofs parts nearly identical prove part
since h c sets b riffle independent assumption hence
factorizations
h
f g b


h
f g b

match interleaving sets b
thus interleaving terms numerator
denominator
hand examine ratio h h see
interleaving terms must cancel
h
f g b


h
f g b

therefore
f g pib
h
h





b
h
h
f g



fiefficient inference partial rankings

established lemma turn establishing three short claims
lemma allow us prove finally prove theorem interesting note
require n strictly claim iii swap order j
numerator denominator third item k proof thought
playing role dummy variable analogous temporary storage variables one
might use implementing swap function necessity third item precisely
hold special case n
proposition let h sn r completely decomposable function n
h sn following equivalences
ratios entries explicitly written assumed match identically
numerator denominator


ii

h j k
h j k


h j k
h j k
h j
h j


h j
h j

iii

h j k
h j k


h j k
h j k

proof
equality holds since match interleaving sets k
b k thus change interleaving b
item k inserted rank preserving ratio
ii equality holds ii since match interleaving sets j
b j thus change interleaving b
items j occupy first two ranks preserving ratio
h h
iii following use refer arguments numerator
denominator respectively preceding line
h j k
h k j


h j k
h k j
h j k


h j k
h j k


h k j
h k j


h k j
h j k


h j k

since match relative ranking j k
since match interleaving j j
since match relative ranking j
since match relative ranking k
since match interleaving k k



fihuang kapoor guestrin

proof theorem want two rankings differ single transposition
assigned equal probability h suppose obtained
swapping ranks items j additionally let k item besides j
item must exist since n following use proposition
h h h h entries explicitly written
assumed match identically numerator denominator
h j
h j
h


prop part ii
h
h j
h j
h j k
h j k


prop part
h j k
h j k
h j k
prop part iii

h j k
h j k

prop part
h j k
h j
h j


prop part ii
h j
h j
h


h

since assumed h h must conclude h h
finally assemble supporting prove theorem
proof theorem lemma completely decomposable function h must factor

r

h
h



factor distribution h completely decomposable function si
assumption corresponding factor h must trivially
uniform otherwise latter case apply theorem
h must assign equal probability two rankings differ
single transposition however given rankings si obtain sequence
transpositions transforms therefore theorem fact implies
factor h constant inputs proved factor equation
constant conclude h must constant support

references
ailon n aggregation partial rankings p ratings top lists proceedings
eighteenth annual acm siam symposium discrete soda
orleans louisiana
bartholdi j j tovey c trick voting schemes
difficult tell social choice welfare


fiefficient inference partial rankings

busse l orbanz p buhmann j cluster analysis heterogeneous rank
data th annual international conference machine learning corvallis
oregon
fligner verducci j distance ranking journal
royal statistical society
freund iyer r schapire r e singer efficient boosting
combining preferences journal machine learning jmlr
friedman n learning belief networks presence missing values hidden variables proceedings fourteenth international conference machine
learning icml pp san francisco ca usa morgan kaufmann publishers inc
friedman n bayesian structural em th conference
uncertainty artificial intelligence uai madison wisconsin
gormley c murphy b latent space model rank data proceedings
conference statistical network analysis icml pp berlin
heidelberg springer verlag
huang j kapoor guestrin c efficient probabilistic inference partial
ranking queries th conference uncertainty artificial intelligence uai
barcelona spain
huang j probabilistic reasoning learning permutations exploiting structural decompositions symmetric group ph thesis carnegie mellon university
huang j guestrin c riffled independence ranked data bengio
schuurmans lafferty j williams c k culotta eds advances
neural information processing systems nips pp mit press
huang j guestrin c learning hierarchical riffle independent groupings
rankings proceedings th annual international conference machine
learning icml pp haifa israel
huang j guestrin c uncovering riffled independence structure ranked
data electronic journal statistics
huang j guestrin c guibas l efficient inference distributions permutations platt j koller singer roweis eds advances neural
information processing systems nips pp mit press cambridge

huang j guestrin c guibas l j fourier theoretic probabilistic inference
permutations journal machine learning jmlr
joachims optimizing search engines clickthrough data proceedings
eighth acm sigkdd international conference knowledge discovery data
mining kdd pp york ny usa acm
kondor r howard jebara multi object tracking representations
symmetric group meila shen x eds proceedings eleventh


fihuang kapoor guestrin

international conference artificial intelligence statistics march
san juan puerto rico vol jmlr w cp
kondor r group theoretical methods machine learning ph thesis columbia
university
lebanon g lafferty j conditional ranking poset becker
obermayer k eds advances neural information processing systems
nips pp cambridge mit press
lebanon g mao non parametric modeling partially ranked data platt
j c koller singer roweis eds advances neural information
processing systems nips pp cambridge mit press
lu boutilier c learning mallows pairwise preferences
th annual international conference machine learning icml bellevue
washington
marden j analyzing modeling rank data chapman hall
meila phadnis k patterson bilmes j consensus ranking
exponential model tech rep university washington statistics department
white r drucker investigating behavioral variability web search
proceedings th international conference world wide web www
banff alberta canada acm




