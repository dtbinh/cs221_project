Journal Artificial Intelligence Research 44 (2012) 533-585

Submitted 03/12; published 07/12

Domain Function: Dual-Space Model
Semantic Relations Compositions
Peter D. Turney

peter.turney@nrc-cnrc.gc.ca

National Research Council Canada
Ottawa, Ontario, Canada, K1A 0R6

Abstract
Given appropriate representations semantic relations carpenter wood
mason stone (for example, vectors vector space model), suitable
algorithm able recognize relations highly similar (carpenter
wood mason stone; relations analogous). Likewise, representations
dog, house, kennel, algorithm able recognize semantic
composition dog house, dog house, highly similar kennel (dog house kennel
synonymous). seems two tasks, recognizing relations compositions,
closely connected. However, now, best models relations significantly
different best models compositions. paper, introduce dual-space
model unifies two tasks. model matches performance best
previous models relations compositions. dual-space model consists space
measuring domain similarity space measuring function similarity. Carpenter
wood share domain, domain carpentry. Mason stone share
domain, domain masonry. Carpenter mason share function,
function artisans. Wood stone share function, function materials.
composition dog house, kennel domain overlap dog house
(the domains pets buildings). function kennel similar function
house (the function shelters). combining domain function similarities various
ways, model relations, compositions, aspects semantics.

1. Introduction
distributional hypothesis words occur similar contexts tend similar
meanings (Harris, 1954; Firth, 1957). Many vector space models (VSMs) semantics use
wordcontext matrix represent distribution words contexts, capturing
intuition behind distributional hypothesis (Turney & Pantel, 2010). VSMs achieved
impressive results level individual words (Rapp, 2003), clear
extend level phrases, sentences, beyond. example, know
represent dog house vectors, represent dog house ?
One approach representing dog house treat unit, way handle
individual words. call holistic noncompositional approach representing
phrases. holistic approach may suitable phrases, scale up.
vocabulary N individual words, N 2 two-word phrases, N 3 threeword phrases, on. Even large corpus text, possible
phrases never appear corpus. People continually inventing new phrases,
able understand new phrases although never heard before;
able infer meaning new phrase composition meanings
c
2012
National Research Council Canada. Reprinted permission.

fiTurney

component words. scaling problem could viewed issue data sparsity,
better think problem linguistic creativity (Chomsky, 1975; Fodor &
Lepore, 2002). master natural language, algorithms must able represent phrases
composing representations individual words. cannot treat n-grams (n > 1)
way treat unigrams (individual words). hand, holistic approach ideal
idiomatic expressions (e.g., kick bucket) meaning cannot inferred
component words.
creativity novelty natural language require us take compositional approach majority n-grams encounter. Suppose vector representations dog house. compose representations represent dog
house ? One strategy represent dog house average vectors dog
house (Landauer & Dumais, 1997). simple proposal actually works, limited degree
(Mitchell & Lapata, 2008, 2010). However boat house house boat would represented
average vector, yet different meanings. Composition averaging
deal order sensitivity phrase meaning. Landauer (2002) estimates
80% meaning English text comes word choice remaining 20% comes
word order.
Similar issues arise representation semantic relations. Given vectors
carpenter wood, represent semantic relations carpenter
wood ? treat carpenter :wood unit search paraphrases relations
carpenter wood (Turney, 2006b). large corpus, could find phrases
carpenter cut wood, carpenter used wood, wood carpenter.
variation holistic approach enable us recognize semantic relations
carpenter wood highly similar relations mason stone.
However, holistic approach semantic relations suffers data sparsity
linguistic creativity problems holistic approach semantic composition.
could represent relation carpenter wood averaging vectors.
might enable us recognize carpenter wood mason stone,
would incorrectly suggest carpenter wood stone mason. problem
order sensitivity arises semantic relations arose semantic composition.
Many ideas proposed composing vectors (Landauer & Dumais, 1997;
Kintsch, 2001; Mitchell & Lapata, 2010). Erk Pado (2008) point two problems
common several proposals. First, often adaptive
capacity represent variety possible syntactic relations phrase. example,
phrase horse draws, horse subject verb draws, whereas object
verb phrase draws horse. composition vectors horse draws
must able adapt variety syntactic contexts order properly model
given phrases. Second, single vector weak handle long phrase, sentence,
document. single vector encode fixed amount structural information
dimensionality fixed, upper limit sentence length, hence
amount structure encoded (Erk & Pado, 2008, p. 898). fixed dimensionality
allow information scalability.
Simple (unweighted) averaging vectors lacks adaptive capacity, treats
kinds composition way; flexibility represent different
modes composition. good model must capacity adapt different situations.
534

fiDomain Function: Dual-Space Model

example, weighted averaging, weights tuned different syntactic
contexts (Mitchell & Lapata, 2008, 2010).
Information scalability means size semantic representations grow
proportion amount information representing. size
representation fixed, eventually information loss. hand,
size representations grow exponentially.
One case problem information scalability arises approaches
map multiple vectors single vector. example, represent dog house adding
vectors dog house (mapping two vectors one), may information
loss. increase number vectors mapped single vector,
eventually reach point single vector longer contain information
multiple vectors. problem avoided try map multiple vectors
single vector.
Suppose k-dimensional vector floating point elements b bits each.
vector hold kb bits information. Even allow b grow, k fixed,
eventually information loss. vector space model semantics, vectors
resistance noise. perturb vector noise threshold ,
significant change meaning represents. Therefore think
vector hypersphere radius , rather point. may put
bounds [r, +r] range values elements vector.1 finite
number N hyperspheres radius packed bounded k-dimensional
space (Conway & Sloane, 1998). According information theory, finite set
N messages, need log2 (N ) bits encode message. Likewise,
finite set N vectors, vector represents log2 (N ) bits information.
Therefore information capacity single vector bounded k-dimensional space
limited log2 (N ) bits.
Past work suggests recognizing relations compositions closely connected
tasks (Kintsch, 2000, 2001; Mangalath, Quesada, & Kintsch, 2004). goal research
unified model handle compositions relations, resolving
issues linguistic creativity, order sensitivity, adaptive capacity, information scalability.
considerations led us dual-space model, consisting domain space
measuring domain similarity (i.e., topic, subject, field similarity) function space
measuring function similarity (i.e., role, relationship, usage similarity).
analogy : b :: c : (a b c d; example, traffic street water
riverbed), b relatively high domain similarity (traffic street come
domain transportation) c relatively high domain similarity (water
riverbed come domain hydrology). hand, c relatively
high function similarity (traffic water similar roles respective domains;
things flow) b relatively high function similarity (street
riverbed similar roles respective domains; things carry
things flow). combining domain function similarity appropriate ways,
1. models vectors normalized unit length (e.g., models use cosine measure
similarity), elements must lie within range [1, +1]. element outside range,
length vector greater one. general, floating point representations minimum
maximum values.

535

fiTurney

recognize semantic relations traffic street analogous
relations water riverbed.
semantic composition, appropriate way combine similarities may depend
syntax composition. Lets focus noun-modifier composition example.
noun-modifier phrase ab (for instance, brain doctor), head noun b (doctor)
modified adjective noun (brain). Suppose word c (neurologist)
synonymous ab. functional role noun-modifier phrase ab determined
head noun b (a brain doctor kind doctor) b relatively high degree
function similarity c (doctor neurologist function doctors).
b high degree domain similarity c (brain, doctor, neurologist come
domain clinical neurology). combining domain function similarity,
recognize brain doctor synonymous neurologist.
Briefly, proposal compose similarity measures instead composing vectors.
is, apply various mathematical functions combine cosine similarity measures,
instead applying functions directly vectors. addresses information
loss problem, preserve vectors individual component words. (We
map multiple vectors single vector.) Since two different spaces,
flexibility address problem adaptive capacity.2 model compositional,
resolves linguistic creativity problem. deal order sensitivity combining
similarity measures ways recognize effects word order.
might argued present model semantic composition,
way compare words form two phrases order derive measure similarity
phrases. example, Section 4.3 derive measure similarity phrases
environment secretary defence minister, actually provide representation
phrase environment secretary. hand, past work problem
semantic composition (reviewed Section 2.1) yields representation composite
phrase environment secretary different union representations
component words, environment secretary.
argument based assumption goal semantic composition
create single, general-purpose, stand-alone representation phrase, composite,
distinct union representations component words. assumption
necessary approach use assumption. believe
assumption held back progress problem semantic composition.
argue present model semantic composition, composition similarities, composition vectors. Vectors represent individual words,
similarities inherently represent relations two (or more) things. Composing vectors
yield stand-alone representation phrase, composing similarities necessarily
yields linking structure connects phrase phrases. Similarity composition
result stand-alone representation phrase, practical applications
require stand-alone representations. Whatever practical tasks performed
stand-alone representations phrases, believe performed equally well (or better)
similarity composition. discuss issue depth Section 6.
2. Two similarity spaces give us options similarity composition one space, two types
characters (0 1) give us options generating strings one type character (0 alone).

536

fiDomain Function: Dual-Space Model

next section surveys related work modeling semantic composition
semantic relations. Section 3 describes build domain function space. test
hypothesis value two separate spaces, create mono space,
merger domain function spaces. present four sets experiments dual-space model Section 4. evaluate dual-space approach
multiple-choice analogy questions SAT (Turney, 2006b), multiple-choice nounmodifier composition questions derived WordNet (Fellbaum, 1998), phrase similarity rating problems (Mitchell & Lapata, 2010), similarity versus association problems
(Chiarello, Burgess, Richards, & Pollock, 1990). discuss experimental results
Section 5. Section 6 considers theoretical questions dual-space model. Limitations model examined Section 7. Section 8 concludes.
paper assumes familiarity vector space models semantics.
overview semantic VSMs, see papers Handbook Latent Semantic Analysis
(Landauer, McNamara, Dennis, & Kintsch, 2007), review Mitchell Lapatas
(2010) paper, survey Turney Pantel (2010).

2. Related Work
examine related work semantic composition relations. introduction, mentioned four problems semantic models, yield four desiderata
semantic model:
1. Linguistic creativity: model able handle phrases (in case
semantic composition) word pairs (in case semantic relations)
never seen before, familiar component words.
2. Order sensitivity: model sensitive order words
phrase (for composition) word pair (for relations), order affects
meaning.
3. Adaptive capacity: phrases, model flexibility represent
different kinds syntactic relations. word pairs, model
flexibility handle variety tasks, measuring degree relational
similarity two pairs (see Section 4.1) versus measuring degree phrasal
similarity two pairs (see Section 4.3).
4. Information scalability: phrases, model scale neither loss
information exponential growth representation size number component
words phrases increases. n-ary semantic relations (Turney, 2008a),
model scale neither loss information exponential growth
representation size n, number terms relations, increases.
review past work light four considerations.
2.1 Semantic Composition
Let ab phrase, noun-modifier phrase, assume vectors
b represent component words b. One earliest proposals semantic
composition represent ab vector c average b (Landauer &
537

fiTurney

Dumais, 1997). using cosine measure vector similarity, taking average
set vectors (or centroid) adding vectors, c = a+b. Vector addition
works relatively well practice (Mitchell & Lapata, 2008, 2010), although lacks order
sensitivity, adaptive capacity, information scalability. Regarding order sensitivity
adaptive capacity, Mitchell Lapata (2008, 2010) suggest using weights, c = a+b,
tuning weights different values different syntactic relations. experiments
(Mitchell & Lapata, 2010), weighted addition performed better unweighted addition.
Kintsch (2001) proposes variation additive composition
c sum a,
P
b, selected neighbours ni b, c = + b + ni . neighbours vectors
words given vocabulary (i.e., rows given wordcontext matrix).
neighbours chosen manner attempts address order sensitivity adaptive
capacity, still problem information scalability due fixed dimensionality.
Utsumi (2009) presents similar model, different way selecting neighbours.
Mitchell Lapata (2010) found simple additive model peformed better
additive model included neighbours.
Mitchell Lapata (2008, 2010) suggest element-wise multiplication composition
operation, c = b, ci = ai bi . vector addition, element-wise multiplication suffers lack order sensitivity, adaptive capacity, information scalability.
Nonetheless, experimental evaluation seven compositional models two noncompositional models, element-wise multiplication best performance (Mitchell &
Lapata, 2010).
Another approach use tensor product composition (Smolensky, 1990; Aerts
& Czachor, 2004; Clark & Pulman, 2007; Widdows, 2008), outer product,
C = b. outer product two vectors (a b), n elements, n n
matrix (C). outer product three vectors n n n third-order tensor.
results information scalability problem: representations grow exponentially large
phrases grow longer.3 Furthermore, outer product perform well
element-wise multiplication Mitchell Lapatas (2010) experiments. Recent work
tensor products (Clark, Coecke, & Sadrzadeh, 2008; Grefenstette & Sadrzadeh, 2011)
attempted address issue information scalability.
Circular convolution similar outer product, outer product matrix
compressed back vector, c = ~ b (Plate, 1995; Jones & Mewhort, 2007).
avoids information explosion, results information loss. Circular convolution
performed poorly Mitchell Lapatas (2010) experiments.
Baroni Zamparelli (2010) Guevara (2010) suggest another model composition
adjective-noun phrases. core strategy share use holistic vectors
train compositional model. partial least squares regression (PLSR), learn
linear model maps vectors component nouns adjectives linear
approximations holistic vectors phrases. linguistic creativity problem
avoided linear model needs holistic vectors training;
need holistic vectors plausible adjective-noun phrases. Given phrase
training data, linear model predicts holistic vector phrase, given
3. ways avoid exponential growth; example, third-order tensor rank 1
three modes may compactly encoded three component vectors. Kolda Bader (2009)
discuss compact tensor representations.

538

fiDomain Function: Dual-Space Model

component vectors adjective noun. works well adjective-noun
phrases, clear generalize parts speech longer phrases.
One application semantic composition measuring similarity phrases (Erk &
Pado, 2008; Mitchell & Lapata, 2010). Kernel methods applied closely
related task identifying paraphrases (Moschitti & Quarteroni, 2008), emphasis
kernel methods syntactic similarity, rather semantic similarity.
Neural network models combined vector space models task
language modeling (Bengio, Ducharme, Vincent, & Jauvin, 2003; Socher, Manning, & Ng,
2010; Socher, Huang, Pennington, Ng, & Manning, 2011), impressive results. goal
language model estimate probability phrase decide several
phrases likely. VSMs improve probability estimates language model
measuring similarity words phrases smoothing probabilities
groups similar words. However, language model, words considered similar
degree exchanged without altering probability given phrase,
without regard whether exchange alters meaning phrase.
function similarity, measures degree words similar functional roles,
language models missing anything domain similarity.
Erk Pado (2008) present model similar two parts,
vector space measuring similarity model selectional preferences. vector
space similar domain space model selectional preferences plays role
similar function space. individual word represented triple, = ha, R, R1 i,
consisting words vector, a, selectional preferences, R, inverse selectional
preferences, R1 . phrase ab represented pair triples, hA0 , B 0 i. triple A0
modified form triple represents individual word a. modifications
adjust representation model meaning altered relation b
phrase ab. Likewise, triple B 0 modified form triple B represents b,
B 0 takes account affects b.
transformed A0 represent influence b meaning a,
vector transformed new vector a0 A0 . Let rb vector represents
typical words consistent selectional preferences b. vector a0
composition rb . Erk Pado (2008) use element-wise multiplication
composition, a0 = rb . intention make typical vector x would
expected phrase xb. Likewise, b0 B 0 , b0 = b ra
Erk Pados (2008) model related models (Thater, Furstenau, & Pinkal, 2010)
address linguistic creativity, order sensitivity, adaptive capacity, information scalability,
suitable measuring similarity semantic relations. Consider
analogy traffic street water riverbed. Let hA0 , B 0 represent traffic :street
let hC 0 , D0 represent water :riverbed. transformation A, B, C, A0 , B 0 ,
C 0 , D0 reinforces connection traffic street water
riverbed, help us recognize relational similarity traffic :street
water :riverbed. course, models designed relational similarity,
surprising. However, goal find unified model handle
compositions relations.
539

fiTurney

2.2 Semantic Relations
semantic relations, make general observations order sensitivity. Let
: b c : two word pairs let simr (a : b, c : d) < measure degree
similarity relations : b c : d. : b :: c : good analogy,
simr (a : b, c : d) relatively high value. general, good model relational
similarity respect following equalities inequalities:

simr (a : b, c : d) = simr (b : a, : c)

(1)

simr (a : b, c : d) = simr (c : d, : b)

(2)

simr (a : b, c : d) 6= simr (a : b, : c)

(3)

simr (a : b, c : d) 6= simr (a : d, c : b)

(4)

example, given carpenter :wood mason :stone make good analogy, follows
Equation 1 wood :carpenter stone :mason make equally good analogy. Also,
according Equation 2, mason :stone carpenter :wood make good analogy.
hand, suggested Equation 3, carpenter :wood analogous stone :mason.
Likewise, indicated Equation 4, poor analogy assert carpenter stone
mason wood.
Rosario Hearst (2001) present algorithm classifying word pairs according
semantic relations. use lexical hierarchy map word pairs feature
vectors. classification scheme implicitly tell us something similarity. Two word
pairs semantic relation class implicitly relationally similar
two word pairs different classes. consider relational similarity
implied Rosario Hearsts (2001) algorithm, see problem order
sensitivity: Equation 4 violated.
Let simh (x, y) < measure degree hierarchical similarity
words x y. simh (x, y) relatively high, x share common hypernym
relatively close given lexical hierarchy. essence, intuition behind
Rosario Hearsts (2001) algorithm is, simh (a, c) simh (b, d) high,
simr (a : b, c : d) high. is, simh (a, c) simh (b, d) high enough,
: b c : assigned relation class.
example, consider analogy mason stone carpenter wood. common hypernym mason carpenter artisan; see simh (mason, carpenter)
high. common hypernym stone wood material; hence simh (stone, wood)
high. seems good analogy indeed characterized high values simh (a, c)
simh (b, d). However, symmetry simh (x, y) leads problem. simh (b, d) high,
simh (d, b) must high, implies simr (a : d, c : b) high. is,
incorrectly conclude mason wood carpenter stone (see Equation 4).
later work classifying semantic relations used different algorithms,
underlying intuition hierarchical similarity (Rosario, Hearst, & Fillmore,
2002; Nastase & Szpakowicz, 2003; Nastase, Sayyad-Shirabad, Sokolova, & Szpakowicz,
2006). use similar intuition here, since similarity function space closely related
540

fiDomain Function: Dual-Space Model

hierarchical similarity, simh (x, y), see later (Section 4.4). However, including
domain space relational similarity measure saves us violating Equation 4.
Let simf (x, y) < function similarity measured cosine vectors x
function space. Let simd (x, y) < domain similarity measured cosine
vectors x domain space. past researchers (Rosario & Hearst, 2001; Rosario
et al., 2002; Nastase & Szpakowicz, 2003; Veale, 2004; Nastase et al., 2006), look
high values simf (a, c) simf (b, d) indicators simr (a : b, c : d) high,
look high values simd (a, b) simd (c, d). Continuing previous example,
conclude mason wood carpenter stone, wood
belong domain masonry stone belong domain carpentry.
Let determiner (e.g., the, a, an). Hearst (1992) showed patterns form
X (a bird crow) kind X (the crow kind
bird) used infer X hypernym (bird hypernym crow).
pairpattern matrix VSM rows word pairs columns
various X . . . patterns. Turney, Littman, Bigham, Shnayder (2003) demonstrated
pairpattern VSM used measure relational similarity. Suppose
pair-pattern matrix X word pair : b corresponds row vector xi c :
corresponds xj . approach measure relational similarity simr (a : b, c : d)
cosine xi xj .
first patterns pairpattern matrices generated hand (Turney
et al., 2003; Turney & Littman, 2005), later work (Turney, 2006b) used automatically
generated patterns. authors used variations technique (Nakov & Hearst,
2006, 2007; Davidov & Rappoport, 2008; Bollegala, Matsuo, & Ishizuka, 2009; Seaghdha
& Copestake, 2009). models suffer linguistic creativity problem.
models noncompositional (holistic), cannot scale handle
huge number possible pairs. Even largest corpus cannot contain pairs
human speaker might use daily conversation.
Turney (2006b) attempted handle linguistic creativity problem within holistic
model using synonyms. example, corpus contain traffic street within
certain window text, perhaps might contain traffic road. contain
water riverbed, perhaps water channel. However, best partial
solution. Turneys (2006b) algorithm required nine days process 374 multiple choice SAT
analogy questions. Using dual-space model, without specifying advance word
pairs might face, answer 374 questions seconds (see Section 4.1).
Compositional models scale better holistic models.
Mangalath et al. (2004) presented model semantic relations represents word
pairs vectors ten abstract relational categories, hyponymy, meronymy, taxonomy, degree. approach construct kind second-order vector space
elements vectors degrees similarity, calculated cosines
first-order wordcontext matrix.
instance, carpenter :wood represented second-order vector composed
ten cosines calculated first-order vectors. second-order vector, value
element corresponding to, say, meronymy would cosine two first-order vectors, x
y. vector x would sum first-order vectors carpenter wood.
vector would sum several vectors words related meronymy,
541

fiTurney

part, whole, component, portion, contains, constituent, segment. cosine
x would indicate degree carpenter wood related meronymy.
Mangalath et al.s (2004) model suffers information scalability order sensitivity
problems. Information loss takes place first-order vectors summed
high-dimensional first-order space reduced ten-dimensional second-order
space. order sensitivity problem second-order vectors violate Equation 3,
pairs c : : c represented second-order vector.
natural proposal represent word pair : b way would represent
phrase ab. is, whatever compositional model phrases could
applied word pairs. However problems compositional model order
sensitivity information scalability carry word pairs. example, represent
: b c = + b c = b, violate Equation 3, + b = b +
b = b a.

3. Three Vector Spaces
section, describe three vector space models. three spaces consist word
context matrices, rows correspond words columns correspond
contexts words occur. differences among three spaces kinds
contexts. Domain space uses nouns context, function space uses verb-based patterns
context, mono space merger domain function contexts. Mono space
created order test hypothesis useful separate domain
function spaces; mono space serves baseline.
3.1 Constructing WordContext Matrices
Building three spaces involves series steps. three main steps,
substeps. first last steps three spaces;
differences spaces result differences second step.
1. Find terms contexts: input: corpus lexicon, output: terms contexts.
1.1. Extract terms lexicon find frequencies corpus.
1.2. Select terms given frequency candidate rows frequency
matrix.
1.3. selected term, find phrases corpus contain term within
given window size.
1.4. Use tokenizer split phrases tokens.
1.5. Use part-of-speech tagger tag tokens phrases.
2. Build termcontext frequency matrix: input: terms contexts, output:
sparse frequency matrix.
2.1. Convert tagged phrases contextual patterns (candidate columns).
2.2. contextual pattern, count number terms (candidate rows)
generated pattern rank patterns descending order counts.
2.3. Select top nc contextual patterns columns matrix.
542

fiDomain Function: Dual-Space Model

2.4. initial set rows (from Step 1.2), drop row match
top nc contextual patterns, yielding final set nr rows.
2.5. row (term) column (contextual pattern), count number
phrases (from Step 1.5) containing given term matching given
pattern, output resulting numbers sparse frequency matrix.
3. Weight elements smooth matrix: input: sparse frequency matrix,
output: singular value decomposition (SVD) weighted matrix.
3.1. Convert raw frequencies positive pointwise mutual information (PPMI)
values.
3.2. Apply SVD PPMI matrix output SVD component matrices.
input corpus Step 1 collection web pages gathered university websites
webcrawler.4 corpus contains approximately 51010 words, comes
280 gigabytes plain text. facilitate finding term frequencies sample phrases,
indexed corpus Wumpus search engine (Buttcher & Clarke, 2005).5 rows
matrices selected terms (words phrases) WordNet lexicon.6
found selecting terms WordNet resulted subjectively higher quality
simply selecting terms high corpus frequencies.
Step 1.1, extract unique words phrases (n-grams) index.sense file
WordNet 3.0, skipping n-grams contain numbers (only letters, hyphens, spaces
allowed n-grams). find n-gram corpus frequencies querying Wumpus
n-gram. n-grams frequency least 100 least 2 characters
candidate rows Step 1.2. selected n-gram, query Wumpus find
maximum 10,000 phrases Step 1.3.7 phrases limited window 7 words
left n-gram 7 words right, total window size 14 + n words.
use OpenNLP 1.3.0 tokenize part-of-speech tag phrases (Steps 1.4 1.5).8
tagged phrases come 46 gigabytes.9
Step 2.1, generate contextual patterns part-of-speech tagged phrases.
Different kinds patterns created three different kinds spaces. details
step given following subsections. phrase may yield several patterns.
three spaces 100,000 rows, maximum 10,000 phrases
per row several patterns per phrase. result millions distinct patterns,
filter patterns Steps 2.2 2.3. select top nc patterns shared
largest number rows. Given large number patterns, may fit
RAM. work limited RAM, use Linux sort command, designed
efficiently sort files large fit RAM. row, make file
distinct patterns generated row. concatenate files
4.
5.
6.
7.

corpus collected Charles Clarke University Waterloo.
Wumpus available http://www.wumpus-search.org/.
WordNet available http://wordnet.princeton.edu/.
limit 10,000 phrases per n-gram required make Wumpus run tolerable amount time.
Finding phrases time-consuming step construction spaces. use solid-state
drive (SSD) speed step.
8. OpenNLP available http://incubator.apache.org/opennlp/.
9. tagged phrases available author request.

543

fiTurney

rows alphabetically sort patterns concatenated file. sorted file,
identical patterns adjacent, makes easy count number occurrences
pattern. counting, second sort operation yields ranked list patterns,
select top nc .
possible candidate rows Step 1.2 might match
patterns Step 2.3. rows would zeros matrix, remove
Step 2.4. Finally, output sparse frequency matrix F nr rows nc
columns. i-th row corresponds n-gram wi j-th column corresponds
contextual pattern cj , value element fij F number phrases
containing wi (from Step 1.5) generate pattern cj (in Step 2.1). Step 3.2, use
SVDLIBC 1.34 calculate singular value decomposition, format output
sparse matrix Step 2.5 chosen meet requirements SVDLIBC.10
Step 3.1, apply positive pointwise mutual information (PPMI) sparse frequency matrix F. variation pointwise mutual information (PMI) (Church &
Hanks, 1989; Turney, 2001) PMI values less zero replaced
zero (Niwa & Nitta, 1994; Bullinaria & Levy, 2007). Let X matrix results
PPMI applied F. new matrix X number rows columns
raw frequency matrix F. value element xij X defined follows:
fij
pij = Pnr Pnc

j=1 fij

i=1

(5)

Pnc

j=1 fij
pi = Pnr Pnc

(6)

Pnr
f
Pncij
= Pnr i=1

(7)

i=1

pj

i=1



j=1 fij
j=1 fij

pij
pi pj



pmiij = log

pmiij pmiij > 0
xij =
0 otherwise

(8)
(9)

definition, pij estimated probability word wi occurs context
cj , pi estimated probability word wi , pj estimated probability
context cj . wi cj statistically independent, pij = pi pj (by definition
independence), thus pmiij zero (since log(1) = 0). product pi pj
would expect pij wi occurs cj pure random chance. hand,
interesting semantic relation wi cj , expect pij larger
would wi cj indepedent; hence find pij > pi pj ,
thus pmiij positive. word wi unrelated (or incompatible with) context cj ,
may find pmiij negative. PPMI designed give high value xij
interesting semantic relation wi cj ; otherwise, xij value
zero, indicating occurrence wi cj uninformative.
10. SVDLIBC available http://tedlab.mit.edu/dr/svdlibc/.

544

fiDomain Function: Dual-Space Model

Finally, Step 3.2, apply SVDLIBC X. SVD decomposes X product
three matrices UVT , U V column orthonormal form (i.e., columns
orthogonal unit length, UT U = VT V = I) diagonal matrix
singular values (Golub & Van Loan, 1996). X rank r, rank r. Let
k , k < r, diagonal matrix formed top k singular values, let Uk
Vk matrices produced selecting corresponding columns U V.
matrix Uk k VkT matrix rank k best approximates original matrix X,
sense minimizes approximation errors. is, X = Uk k VkT minimizes
kX XkF matrices X rank k, k . . . kF denotes Frobenius norm (Golub
& Van Loan, 1996). final output three matrices, Uk , k , Vk , form
truncated SVD, X = Uk k VkT .
3.2 Domain Space
intuition behind domain space domain topic word characterized
nouns occur near it. use relatively wide window ignore syntactic
context nouns appear.
domain space, Step 2.1, tagged phrase generates two contextual
patterns. contextual patterns simply first noun left given n-gram
(if one) first noun right (if one). Since window size 7
words side n-gram, usually nouns sides n-gram.
nouns may either common nouns proper nouns. OpenNLP uses Penn Treebank
tags (Santorini, 1990), include several different categories noun tags.
noun tags begin capital N, simply extract first words left right
n-gram tags begin N. extracted nouns converted lower
case. noun appears sides n-gram, one contextual pattern
generated. extracted patterns always unigrams; noun compound,
component noun closest n-gram extracted.
Table 1 shows examples n-gram boat. Note window 7 words
count punctuation, number tokens window may greater
number words window. see Table 1 row vector
n-gram boat frequency matrix F nonzero values (for example)
columns lake summer (assuming contextual patterns make
filtering Step 2.3).
Step 2.3, set nc 50,000. Step 2.4, drop rows zero,
left nr equal 114,297. PPMI (which sets negative elements zero)
149,673,340 nonzero values, matrix density 2.62%. Table 2 shows
contextual patterns first five columns last five columns (the columns
order ranks Step 2.2). Count column table gives number rows
(n-grams) generate pattern (that is, counts mentioned Step 2.2).
last patterns begin c counts ties broken
alphabetical order.
545

fiTurney

Tagged phrases
would/MD visit/VB Big/NNP Lake/NNP and/CC take/VB our/PRP$
boat/NN on/IN this/DT huge/JJ beautiful/JJ lake/NN ./. There/EX
was/VBD

Patterns
lake

2

the/DT large/JJ paved/JJ parking/NN lot/NN in/IN the/DT boat/NN
ramp/NN area/NN and/CC walk/VB south/RB along/IN the/DT

lot
ramp

3

building/VBG permit/NN ./. / Anyway/RB ,/, we/PRP should/MD
have/VB a/DT boat/NN next/JJ summer/NN with/IN skiing/NN
and/CC tubing/NN paraphernalia/NNS ./.

permit
summer

1

Table 1: Examples Step 2.1 domain space n-gram boat. three tagged
phrases generate five contextual patterns.

Column
1
2
3
4
5

Pattern
time
part
years
way
name

Count
91,483
84,445
84,417
84,172
81,960

Column
49,996
49,997
49.998
49,999
50,000

Pattern
clu
co-conspirator
conciseness
condyle
conocer

Count
443
443
443
443
443

Table 2: Contextual patterns first last columns domain space. CLU
abbreviation Chartered Life Underwriter terms, condyle round
bump bone forms joint another bone, conocer
Spanish verb know, sense acquainted person.

3.3 Function Space
concept function space function role word characterized
syntactic context relates verbs occur near it. use narrow
window function space domain space, based intuition proximity
verb important determining functional role given word. distant verb
less likely characterize function word. generate relatively complex patterns
function space, try capture syntactic patterns connect given word
nearby verbs.
Step 2.1, tagged phrase generates six contextual patterns. given
tagged phrase, first step cut window 3 tokens given n-gram
3 tokens it. remaining tokens left n-gram punctuation, punctuation everything left punctuation removed.
remaining tokens right n-gram punctuation, punctuation everything right punctuation removed. Lets call remaining tagged phrase
truncated tagged phrase.
Next replace given n-gram truncated tagged phrase generic marker,
546

fiDomain Function: Dual-Space Model

X. simplify part-of-speech tags reducing first character
(Santorini, 1990). example, various verb tags (VB, VBD, VBG, VBN, VBP,
VBZ) reduced V. truncated tagged phrase contains V tag, generates
zero contextual patterns. phrase contains V tag, generate two types
contextual patterns, general patterns specific patterns.
general patterns, verbs (every token V tag) tags removed
(naked verbs) tokens reduced naked tags (tags without words).
specific patterns, verbs, modals (tokens tags), prepositions (tokens tags),
(tokens tags) tags removed tokens reduced naked
tags. (See Table 3 examples.)
general specific patterns, left X, trim leading naked tags.
right X, trim trailing naked tags. tag to, replace
remaining naked tags to. sequence N tags (N N N N N) likely
compound noun, reduce sequence single N.
given truncated tagged phrase, two patterns, one general pattern
one specific pattern. either patterns tokens left right
sides X, make two patterns duplicating X splitting pattern
point two Xs. one new patterns verb, drop
it. Thus may three specific patterns three general patterns
given truncated tagged phrase. specific general patterns same, one
generated.
Table 3 shows examples n-gram boat. Note every pattern must contain
generic marker, X, least one verb.
Truncated tagged phrases
the/DT canals/NNS by/IN boat/NN
and/CC wandering/VBG the/DT

Patterns
X C wandering
X C wandering

Types
general
specific

2

a/DT charter/NN fishing/VBG boat/NN
captain/NN named/VBN Jim/NNP

fishing X N named
fishing X
X N named

general
general
general

3

used/VBN from/IN a/DT
and/CC lowered/VBD to/TO

used X C lowered
used X
X C lowered
used X C lowered
used X
X C lowered

general
general
general
specific
specific
specific

1

boat/NN

Table 3: Examples Step 2.1 function space n-gram boat. three truncated
tagged phrases generate eleven contextual patterns.

Step 2.3, set nc 50,000. Step 2.4, rows zero dropped,
nr 114,101. PPMI, 68,876,310 nonzero values, yielding matrix density
1.21%. Table 4 shows contextual patterns first last five columns.
547

fiTurney

last patterns begin counts ties broken
alphabetical order.
Column
1
2
3
4
5

Pattern
X
X N
X
X
X

Count
94,312
82,171
79,131
72,637
72,497

Column
49,996
49,997
49,998
49,999
50,000

Pattern
since X N
sinking X
supplied X
supports X N
suppressed X

Count
381
381
381
381
381

Table 4: Contextual patterns first last columns function space.
contextual patterns function space complex patterns
domain space. motivation greater complexity observation mere
proximity enough determine functional roles, although seems sufficient determining domains. example, consider verb gives. word X occurs near
gives, X could subject, direct object, indirect object verb. determine
functional role X, need know case applies. syntactic context connects X gives provides information. contextual pattern X gives implies
X subject, gives X implies X object, likely direct object, gives X
suggests X indirect object. Modals prepositions supply information
functional role X context given verb. verb gives appears 43
different contextual patterns (i.e., 43 50,000 columns function space correspond
syntactic patterns contain gives).
Many row vectors function space matrix correspond verbs. might
seem surprising characterize function verb syntactic relation
verbs, consider example, verb run. row vector run
PPMI matrix function space 1,296 nonzero values; is, run characterized
1,296 different contextual patterns.
Note appearing contextual pattern different nonzero value
contextual pattern. character string word run appears 62 different
contextual patterns, run X. row vector word run nonzero
values 1,296 contextual patterns (columns), X.
3.4 Mono Space
Mono space simply merger domain space function space. Step 2.3,
take union 50,000 domain space columns 50,000 function space columns,
resulting total nc 100,000 columns. Step 2.4, total nr 114,297
rows. mono matrix PPMI 218,222,254 nonzero values, yielding density
1.91%. values mono frequency matrix F equal corresponding values
domain function matrices. rows mono space matrix
corresponding rows function space matrix. rows, corresponding values
zeros (but nonzero elements rows, correspond values
domain matrix).
548

fiDomain Function: Dual-Space Model

3.5 Summary Spaces
Table 5 summarizes three matrices. following four sets experiments, use
three matrices (the domain, function, mono matrices) cases;
generate different matrices set experiments. Three four sets experiments
involve datasets used past researchers. made special
effort ensure words three datasets corresponding rows three
matrices. intention three matrices adequate handle
applications without special customization.
Space
domain
function
mono

Rows (nr )
114,297
114,101
114,297

Columns (nc )
50,000
50,000
100,000

Nonzeros (after PPMI)
149,673,340
68,876,310
218,222,254

Density (after PPMI)
2.62%
1.21%
1.91%

Table 5: Summary three spaces.

3.6 Using Spaces Measure Similarity
following experiments, measure similarity two terms, b, cosine
angle corresponding row vectors, b:
sim(a, b) = cos(a, b) =


b

kak kbk

(10)

cosine angle two vectors inner product vectors,
normalized unit length. cosine ranges 1 vectors point
opposite directions ( 180 degrees) +1 point direction ( 0
degrees). vectors orthogonal ( 90 degrees), cosine zero. raw
frequency vectors, necessarily cannot negative elements, cosine cannot
negative, weighting smoothing often introduce negative elements. PPMI weighting
yield negative elements, truncated SVD generate negative elements, even
input matrix negative values.
semantic similarity two terms given cosine two corresponding rows
Uk pk (see Section 3.1). two parameters Uk pk need set.
parameter k controls number latent factors parameter p adjusts weights
factors, raising corresponding singular values pk power p.
parameter k well-known literature (Landauer et al., 2007), p less familiar.
use p suggested Caron (2001). following experiments (Section 4),
explore range values p k.
Suppose take word w list words descending order
cosines w, using Uk pk calculate cosines. p high, go list,
cosines nearest neighbours w decrease slowly. p low, decrease
quickly. is, high p results broad, fuzzy neighbourhood low p yields sharp,
crisp neighbourhood. parameter p controls sharpness similarity measure.
549

fiTurney

reduce running time SVDLIBC, limit number singular values
1500, usually results less 1500 singular values. example, SVD
domain space 1477 singular values. long k greater 1477,
experiment range k values without rerunning SVDLIBC. generate Uk pk
U1477 p1477 simply deleting 1477 k columns smallest singular values.
experiments, vary k 100 1400 increments 100 (14 values k)
vary p 1 +1 increments 0.1 (21 values p). p 1,
give weight factors smaller singular values; p +1, factors
larger singular values weight. Caron (2001) observes researchers use
either p = 0 p = 1; is, use either Uk Uk k .
Let simf (a, b) < function similarity measured cosine vectors b
function space. Let simd (a, b) < domain similarity measured cosine
vectors b domain space. similarity measure combines simd (a, b)
simf (a, b), four parameters tune, kd pd domain space kf pf
function space.
one space, feasible us explore 14 21 = 294 combinations parameter
values, two spaces 294 294 = 86, 436 combinations values. make search
tractable, initialize parameters middle ranges (kf = kd = 700
pf = pd = 0) alternate tuning simd (a, b) (i.e., kd pd ) holding
simf (a, b) (i.e., kf pf ) fixed tuning simf (a, b) holding simd (a, b) fixed. stop
search improvement performance training data. almost
cases, local optimum found one pass; is, tuned parameters
once, improvement try tune second time. Thus typically
evaluate 294 3 = 882 parameter values (3 tune one similarity, tune other,
try first see improvement possible).11
could use standard numerical optimization algorithm tune four parameters, algorithm use takes advantage background knowledge
optimization task. know small variations parameters make small changes
performance, need make fine-grained search, know
simd (a, b) simf (a, b) relatively independent, optimize separately.
rows matrices based terms WordNet index.sense file.
file, nouns singular forms verbs stem forms. calculate
sim(a, b), first look exact matches b terms correspond
rows given matrix (domain, function, mono). exact match found,
use corresponding row vector matrix. Otherwise, look alternate forms
terms, using validForms function WordNet::QueryData Perl interface
WordNet.12 automatically converts plural nouns singular forms verbs
stem forms. none alternate forms exact match row matrix,
map term zero vector length k.

11. use Perl Data Language (PDL) searching parameters, calculating cosines, operations
vectors matrices. See http://pdl.perl.org/.
12. WordNet::QueryData available http://search.cpan.org/dist/WordNet-QueryData/.

550

fiDomain Function: Dual-Space Model

3.7 Composing Similarities
approach semantic relations compositions combine two similarities,
simd (a, b) simf (a, b), various ways, depending task hand syntax
phrase hand. general, want combined similarity high
component similarities high, want values component similarities
balanced. achieve balance, use geometric mean combine similarities, instead
arithmetic mean. geometric mean suitable negative numbers,
cosine negative cases; hence define geometric mean zero
component similarities negative:

geo(x1 , x2 , . . . , xn ) =

(x1 x2 . . . xn )1/n xi > 0 = 1, . . . , n
0 otherwise

(11)

3.8 Element-wise Multiplication
One successful approaches composition, far, element-wise multiplication, c = b, ci = ai bi (Mitchell & Lapata, 2008, 2010). approach
makes sense elements vectors negative. elements
b positive, relatively large values ai bi reinforce other, resulting
large value ci . makes intuitive sense. ai bi highly negative,
ci highly positive, although intuition says ci highly negative. Mitchell
Lapata (2008, 2010) designed wordcontext matrices ensure vectors
negative elements.
values matrix Uk pk typically half positive half negative.
use element-wise multiplication baseline following experiments.
fair baseline, cannot simply apply element-wise multiplication row vectors Uk pk .
One solution would use PPMI matrix, X, negative elements,
would allow element-wise multiplication take advantage smoothing effect
SVD. solution use row vectors X = Uk k VkT . Although PPMI matrix,
X, sparse (see Table 5), X Uk pk density 100%.
Let a0 b0 vectors X correspond terms b. row
vectors benefit smoothing due truncated SVD, elements almost
positive. negative elements, set zero. Let c0 = a0 b0 .
apply element-wise multiplication vectors, multiply Vk kp1 ,
resulting vector c = c0 Vk p1
compared row vectors matrix
k
p
Uk k :
p1

X(Vk p1
k ) = (Uk k Vk )(Vk k )

=
=
=

Uk k VkT Vk kp1
Uk k p1
k
Uk pk

(12)
(13)
(14)
(15)

Note that, since Vk column orthonormal, VkT Vk equals Ik , k k identity matrix.
551

fiTurney

Similarly, row vector Uk pk , find counterpart a0 X multiplying

1p
k Vk :

(Uk pk )(k1p VkT ) = Uk pk 1p
k Vk

(16)

= Uk k VkT

(17)

= X

(18)

Let nn(x) (nn nonnegative) function converts negative elements vector
x zero:

nn(hx1 , . . . , xn i) = hy1 , . . . , yn

xi xi > 0
yi =
0 otherwise

(19)
(20)

version element-wise multiplication may expressed follows:
p1
1p

c = (nn(a1p
k Vk ) nn(bk Vk )) Vk k

(21)

Another way deal element-wise multiplication would use nonnegative
matrix factorization (NMF) (Lee & Seung, 1999) instead SVD. yet found
implementation NMF scales matrix sizes (Table 5).
past experiments smaller matrices, SVD NMF similar performance.

4. Experiments Varieties Similarities
section presents four sets experiments. first set experiments presents dualspace model semantic relations evaluates model multiple choice analogy
questions SAT. second set presents model semantic composition
evaluates multiple choice questions constructed WordNet. third
set applies dual-space model phrase similarity dataset Mitchell Lapata
(2010). final set uses three classes word pairs Chiarello et al. (1990) test
hypothesis dual-space model, domain space function space capture
intuitive concepts association similarity.
4.1 Similarity Relations
evaluate dual-space model applied task measuring similarity
semantic relations. use set 374 multiple-choice analogy questions SAT
college entrance exam (Turney, 2006b). Table 6 gives example one questions.
task select choice word pair analogous (most relationally similar)
stem word pair.
Let : b represent stem pair (e.g., lull :trust). answer SAT questions
selecting choice pair c : maximizes relational similarity, simr (a : b, c : d), defined
follows:
552

fiDomain Function: Dual-Space Model

Stem:
Choices:

Solution:

(1)
(2)
(3)
(4)
(5)
(3)

lull:trust
balk:fortitude
betray:loyalty
cajole:compliance
hinder:destination
soothe:passion
cajole:compliance

Table 6: example question 374 SAT analogy questions. Lulling person
trust analogous cajoling person compliance.

sim1 (a : b, c : d) = geo(simf (a, c), simf (b, d))

(22)

sim2 (a : b, c : d) = geo(simd (a, b), simd (c, d))

(23)

sim3 (a : b, c : d) = geo(simd (a, d), simd (c, b))

sim1 (a : b, c : d) sim2 (a : b, c : d) sim3 (a : b, c : d)
simr (a : b, c : d) =
0 otherwise

(24)
(25)

intent sim1 measure function similarity across two pairs. domain
similarity inside two pairs measured sim2 , whereas domain similarity across
two pairs given sim3 . relational similarity, simr , simply function similarity,
sim1 , subject constraint domain similarity inside pairs, sim2 , must
less domain similarity across pairs, sim3 .
Figure 1 conveys main ideas behind Equations 22 25. want high function
similarities (indicated F) : c b : d, measured sim1 . prefer
relatively high domain similarities (marked D) : b c : (measured sim2 ),
contrast relatively low domain similarities ( D) : c : b (as given sim3 ).13
Using example Table 6, see lulling person trust analogous
cajoling person compliance, since functional role lull similar functional
role cajole (both involve manipulating person) functional role trust similar
functional role compliance (both states person in).
captured sim1 . constraint sim2 (a : b, c : d) sim3 (a : b, c : d) implies
domain similarities lull :trust (the domain confidence loyalty) cajole :compliance
(the domain obedience conformity) greater equal domain
similarities lull :compliance cajole :trust.
Analogy way mapping knowledge source domain target domain
(Gentner, 1983). source domain mapped c target domain,
play role source domain c plays target domain.
theory behind sim1 . b source domain c target
13. recently came across rectangular structure Lepage Shin-ichis (1996) paper
morphological analogy (see Figure 1). Although algorithm task differ considerably
algorithm task Lepage Shin-ichi (1996), independently discovered
underlying structure analogical reasoning.

553

fiTurney






b


F

c

F





simr (a : b, c : d)
relational similarity

Figure 1: diagram reasoning behind Equations 22 25. F represents high
function similarity, means high domain similarity, indicates low
domain similarity.

domain, internal domain similarity b internal domain similarity
c less cross-domain similarities. motivates constraint
sim2 sim3 . definition natural expression Gentners (1983) theory analogy.
Recall four equations introduced Section 2.2. repeat equations
convenience:

simr (a : b, c : d) = simr (b : a, : c)

(26)

simr (a : b, c : d) = simr (c : d, : b)

(27)

simr (a : b, c : d) 6= simr (a : b, : c)

(28)

simr (a : b, c : d) 6= simr (a : d, c : b)

(29)

Inspection show definition relational similarity Equation 25 satisfies
requirements Equations 26, 27, 28, 29. understood considering
Figure 1. Equation 26 tells us rotate Figure 1 vertical axis without
altering network similarities, due symmetry figure. Equation 27 tells
us rotate Figure 1 horizontal axis without altering network
similarities.
hand, cannot swap c holding b fixed,
would change F links (although would change links).
words, sim1 sim3 would changed, although sim2 would affected.
Therefore Equation 28 satisfied.
Also, cannot swap b holding c fixed, would change
links (although would change F links). words, sim2
sim3 would changed, although sim1 would affected. Therefore Equation 29
554

fiDomain Function: Dual-Space Model

satisfied. see sim1 would violate Equation 29, due symmetry
cosines, simf (b, d) = simf (d, b). constraint sim2 (a : b, c : d) sim3 (a : b, c : d) breaks
symmetry.
Another way break symmetry, Equation 29 satisfied, would use
similarity measure inherently asymmetric, skew divergence. Equation 25,
symmetry broken natural way considering domain function similarity
apply analogies, need introduce inherently asymmetric measure. Also,
note symmetries Equations 26 27 desirable; wish break
symmetries.
would reasonable include simd (a, c) simd (b, d) sim3 , decided
leave out. seems us function similarities simf (a, c) simf (b, d),
high values good analogy, might cause simd (a, c) simd (b, d)
relatively high, even though cross domains. people observe certain kind
abstract function similarity frequently, function similarity might become popular
topic discussion, could result high domain similarity.
example, carpenter :wood analogous mason :stone. domain carpenter :wood
carpentry domain mason :stone masonry. functional role carpenter
similar functional role mason, artisans. Although carpenter
mason belong different domains, high degree abstract function similarity
may result discussions mention together, discussions specialized trades, skilled manual labour, construction industry, workplace injuries.
words, high function similarity two words may cause rise domain
similarity. Therefore include simd (a, c) simd (b, d) sim3 .
five choices SAT question relational similarity zero, skip
question. use ten-fold cross-validation set parameters SAT questions.
parameter values selected nine ten folds, kd = 800, pd = 0.1, kf = 300,
pf = 0.5. parameters determined, 374 SAT questions answered
seconds. Equation 25 correctly answers 191 questions, skips 2 questions,
incorrectly answers 181 questions, achieving accuracy 51.1%.
4.1.1 Comparison Past Work
comparison, average score senior highschool students applying US universities
57.0%. ACL Wiki lists many past results 374 SAT questions.14 Table 7
shows top ten results time writing. table, dual-space refers dualspace model using Equation 25. Four past results achieved accuracy 51.1%
higher. four used holistic approaches hence able address issue
linguistic creativity. best previous algorithm attains accuracy 56.1% (210 correct,
4 skipped, 160 incorrect) (Turney, 2006b). difference 51.1% 56.1%
statistically significant 95% confidence level, according Fishers Exact Test.
majority algorithms Table 7 unsupervised, Dual-Space, PairClass
(Turney, 2008b), BagPack (Herdagdelen & Baroni, 2009) use limited supervision. PairClass BagPack answer given SAT question learning binary classification model
specific given question. training set given question consists one
14. See http://aclweb.org/aclwiki/index.php?title=SAT Analogy Questions.

555

fiTurney

Algorithm
LSA+Predication
KNOW-BEST
k-means
BagPack
VSM
Dual-Space
BMI
PairClass
PERT
LRA
Human

Reference
Mangalath et al. (2004)
Veale (2004)
Bicici Yuret (2006)
Herdagdelen Baroni (2009)
Turney Littman (2005)
Bollegala et al. (2009)
Turney (2008b)
Turney (2006a)
Turney (2006b)
Average US college applicant

Accuracy
42.0
43.0
44.0
44.1
47.1
51.1
51.1
52.1
53.5
56.1
57.0

95% confidence
37.247.4
38.048.2
39.049.3
39.049.3
42.252.5
46.156.5
46.156.5
46.957.3
48.558.9
51.061.2
52.062.3

Table 7: top ten results 374 SAT questions, ACL Wiki. 95%
confidence intervals calculated using Binomial Exact Test.

positive training example, stem pair question, ten randomly selected pairs
(assumed) negative training examples. induced binary classifier used assign
probabilities five choices probable choice guess. Dual-Space uses
training set tune four numerical parameters. three algorithms best
described weakly supervised.
4.1.2 Sensitivity Parameters
see sensitive dual-space model values parameters, perform
two exhaustive grid searches, one coarse, wide grid another fine, narrow
grid. point grids, evaluate dual-space model using whole set
374 SAT questions. narrow grid search centred parameter values
selected nine ten folds previous experiment, kd = 800, pd = 0.1,
kf = 300, pf = 0.5. searches evaluate 5 values parameter, yielding total
54 = 625 parameter settings. Table 8 shows values explored two grid
searches Table 9 presents minimum, maximum, average, standard deviation
accuracy two searches.
Grid
Coarse

Fine

Parameter
kd
pd
kf
pf
kd
pd
kf
pf

100
-1.0
100
-1.0
600
-0.3
100
0.3

425
-0.5
425
-0.5
700
-0.2
200
0.4

Values
750 1075
0.0
0.5
750 1075
0.0
0.5
800
900
-0.1
0.0
300
400
0.5
0.6

1400
1.0
1400
1.0
1000
0.1
500
0.7

Table 8: range parameter values two grid searches.
556

fiDomain Function: Dual-Space Model

Grid
Coarse
Fine

Minimum
31.0
42.5

Accuracy
Maximum Average
48.7
40.7
51.6
47.3

Standard deviation
4.1
2.0

Table 9: sensitivity dual-space model parameter settings.
accuracy attained heuristic search (described Section 3.6) ten-fold
cross-validation, 51.1% (Table 7), near best accuracy fine grid search using
whole set 374 SAT questions, 51.6% (Table 9). evidence heuristic search
effective. Accuracy coarse search varies 31.0% 48.7%, demonstrates
importance tuning parameters. hand, accuracy fine search
spans narrower range lower standard deviation, suggests dualspace model overly sensitive relatively small variations parameter values;
is, parameters reasonably stable. (That nine ten folds cross-validation
select parameters evidence stability.)
4.1.3 Parts Speech
Since domain space based nouns function space based verbs, interesting
know performance dual-space model varies different parts speech.
answer this, manually labeled 374 SAT questions part-of-speech labels.
labels single pair ambiguous, labels become unambiguous context
whole question. example, lull :trust could noun :verb, context
Table 6, must verb :noun.
Table 10 splits results various parts speech. None differences
table statistically significant 95% confidence level, according Fishers
Exact Test. larger varied set questions needed determine part
speech affects dual-space model.
Parts speech
noun:noun
noun:adjective adjective:noun
noun:verb verb:noun
adjective:adjective
verb:adjective adjective:verb
verb:verb
verb:adverb adverb:verb


Right
97
35
27
9
12
11
0
191

Accuracy
50.8
53.0
49.1
37.5
60.0
64.7
0.0
51.1

Wrong
93
31
28
15
7
6
1
181

Skipped
1
0
0
0
1
0
0
2

Total
191
66
55
24
20
17
1
374

Table 10: Performance dual-space model various parts speech.

4.1.4 Order Sensitivity
seems function space work Equation 25. use sim1 alone,
dropping constraint sim2 sim3 , accuracy drops 51.1% 50.8%.
557

fiTurney

drop statistically significant. hypothesize small drop due design
SAT test, primarily intended test students understanding functional
roles, domains.
verify hypothesis, reformulated SAT questions would test
function domain comprehension. method first expand choice pair
c : including stem pair : b, resulting full explicit analogy : b :: c : d.
expanded choice, : b :: c : d, generate another choice, : :: c : b. Table 11 shows
reformulation Table 6. Due symmetry, sim1 must assign similarity
: b :: c : : :: c : b. new ten-choice test evaluates function domain
similarities.
Choices:

Solution:

(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
(9)
(10)
(6)

lull:trust::balk:fortitude
lull:fortitude::balk:trust
lull:loyalty::betray:trust
lull:trust::betray:loyalty
lull:compliance::cajole:trust
lull:trust::cajole:compliance
lull:destination::hinder:trust
lull:trust::hinder:destination
lull:trust::soothe:passion
lull:passion::soothe:trust
lull:trust::cajole:compliance

Table 11: expanded SAT question, designed test function domain comprehension. Choices (5) (6) similarity according sim1 .

task expanded ten-choice SAT questions original
five-choice questions, select best analogy. solution Table 11
solution Table 6, except stem pair explicit Table 11. signficant
change five new distractors added choices. answer ten-choice
questions selecting choice : b :: c : maximizes simr (a : b, c : d).
ten-choice reformulated SAT test, simr (Equation 25) attains accuracy
47.9%, whereas sim1 alone (Equation 22) achieves 27.5%. difference statistically
significant 95% confidence level, according Fishers Exact Test. stringent
test supports claim function similarity insufficient itself.
test value two separate spaces, use single space
simd simf Equation 25. model still four parameters tune, kd , pd , kf ,
pf , matrix used similarities. best result accuracy
40.4% ten-question reformulated SAT test, using function space simd
simf . significantly 47.9% accuracy dual-space model simd
based domain space simf based function space (95% confidence level, Fishers
Exact Test).
Table 12 summarizes results. cases matrix simd used,
model based sim1 alone (Equation 22). cases, model based
simr (Equation 25). five-choice ten-choice SAT questions, original
558

fiDomain Function: Dual-Space Model

dual-space model accurate modified models. Significant column
indicates whether accuracy modified model significantly less original
dual-space model (95% confidence level, Fishers Exact Test). difficult ten-choice
questions clearly show value two distinct spaces.
Algorithm
dual-space
modified dual-space
modified dual-space
modified dual-space
modified dual-space
modified dual-space
modified dual-space
dual-space
modified dual-space
modified dual-space
modified dual-space
modified dual-space
modified dual-space
modified dual-space

Accuracy
51.1
47.3
43.6
37.7
50.8
41.7
35.8
47.9
40.4
38.2
34.8
27.5
25.1
14.4

Significant

yes
yes

yes
yes
yes
yes
yes
yes
yes
yes

Questions
five-choice
five-choice
five-choice
five-choice
five-choice
five-choice
five-choice
ten-choice
ten-choice
ten-choice
ten-choice
ten-choice
ten-choice
ten-choice

Matrix simd
domain space
function space
mono space
domain space
used
used
used
domain space
function space
mono space
domain space
used
used
used

Matrix simf
function space
function space
mono space
domain space
function space
mono space
domain space
function space
function space
mono space
domain space
function space
mono space
domain space

Table 12: Accuracy original five-choice questions reformulated ten-choice
questions. modified models, intentionally use wrong matrix (or
matrix) simd simf . modified models show accuracy decreases
one space used.

4.1.5 Summary
dual-space model performs well current state-of-the-art holistic model
addresses issue linguistic creativity. results reformulated SAT questions
support claim value two separate spaces.
mentioned Section 2.2, task classifying word pairs according
semantic relations (Rosario & Hearst, 2001; Rosario et al., 2002; Nastase & Szpakowicz,
2003) closely connected problem measuring relational similarity. Turney (2006b)
applied measure relational similarity relation classification using cosine similarity
measure nearness nearest neighbour supervised learning algorithm. dualspace model (Equation 25) suitable relation classification nearest neighbour
algorithm.
4.2 Similarity Compositions
second set experiments, apply dual-space model noun-modifier compositions. Given vectors dog, house, kennel, would able recognize
dog house kennel synonymous. compare dual-space model holistic
approach, vector addition, element-wise multiplication. approaches evaluated
559

fiTurney

using multiple-choice questions automatically generated WordNet, using
WordNet::QueryData Perl interface WordNet. Table 13 gives example one
noun-modifier questions.
Stem:
Choices:

Solution:

(1)
(2)
(3)
(4)
(5)
(6)
(7)
(1)

dog house
kennel
dog
house
canine
dwelling
effect
largeness
kennel

Table 13: example multiple-choice noun-modifier composition question.
questions, stem bigram choices unigrams. Choice (1)
correct answer, (2) modifier, (3) head noun. Choice (4) synonym
hypernym modifier (5) synonym hypernym head noun.
synonyms hypernyms found, noun randomly chosen. last two choices, (6)
(7), randomly selected nouns. Choices (2) (4) either nouns adjectives,
choices must nouns.
stem bigram choice unigrams must corresponding rows function
space (the space least number rows). stem bigram must noun sense
WordNet (it may senses parts speech). solution unigram, (1),
must member synset (synonym set) first noun sense stem bigram
(the frequent dominant sense bigram, bigram used noun),
cannot simply hyphenation (dog-house) concatenation (doghouse)
stem bigram.
requirements result total 2180 seven-choice questions, randomly
split 680 training (parameter tuning) 1500 testing.15 questions deliberately designed difficult. particular, approaches strongly attracted
choices (2) (3). Furthermore, attempt ensure stem bigrams
compositional; may idiomatic expressions compositional approach
could possibly get right. want bias questions imposing theories
distinguishing compositions idioms construction.
Let ab represent noun-modifier bigram (dog house) let c represent unigram
(kennel). answer multiple-choice questions selecting unigram maximizes
compositional similarity, simc (ab, c), defined follows:

sim1 (ab, c) = geo(simd (a, c), simd (b, c), simf (b, c))

sim1 (ab, c) 6= c b 6= c
simc (ab, c) =
0 otherwise
15. questions available online appendix http://jair.org/.

560

(30)
(31)

fiDomain Function: Dual-Space Model

Equations 30 31 illustrated Figure 2.


6=

b



F

6=

c
simc (ab, c)
noun-modifier compositional similarity

Figure 2: diagram Equations 30 31.
thinking behind sim1 c (kennel ) high domain similarity
modifier (dog) head noun b (house); furthermore, function
bigram ab (dog house) determined head noun b (house), head noun
high function similarity c (kennel ). add constraints 6= c b 6= c
sim1 tends high values sim1 (ab, a) sim1 (ab, b).16 seems
plausible humans use constraints this: reason dog house cannot mean
thing house, extra word dog dog house would serve purpose;
would meaningless noise.17
constraints 6= c b 6= c could expressed terms similarities,
simd (a, c) < simd (b, c) < t, high threshold (e.g., = 0.9), would
add another parameter model. decided keep model relatively simple.
seven choices noun-modifier question compositional similarity
zero, skip question. training set, best parameter settings kd = 800,
pd = 0.3, kf = 100, pf = 0.6. testing set, Equation 31 correctly answers 874
questions, skips 22 questions, incorrectly answers 604, yielding accuracy 58.3%.
4.2.1 Comparison Approaches
Mitchell Lapata (2010) compared many different approaches semantic composition
experiments, considered one task (the task examine Section 4.3).
paper, chosen compare smaller number approaches larger
number tasks. include element-wise multiplication experiments,
approach best performance Mitchell Lapatas (2010) experiments. Vector
16. spite constraints, still worthwhile include head noun modifier distractors
multiple-choice questions, enables us experimentally evaluate impact
distractors various algorithms constraints removed (see Table 15). Also, future users
dataset may find way avoid distractors without explicit constraints.
17. philosophy language, Grice (1989) argued proper interpretation language requires us
charitably assume speakers generally insert random words speech.

561

fiTurney

addition included due historical importance simplicity. Although Mitchell
Lapata (2010) found weighted addition better unweighted addition,
include weighted addition experiments, perform well
element-wise multiplication Mitchell Lapatas (2010) experiments. include
holistic model noncompositional baseline.
Table 14 compares dual-space model holistic model, element-wise multiplication, vector addition. latter three models, try three spaces.
Algorithm
dual-space
holistic
holistic
holistic
multiplication
multiplication
multiplication
addition
addition
addition

Space
domain function
mono
domain
function
mono
domain
function
mono
domain
function

Accuracy
58.3
81.6
79.1
67.5
55.7
57.5
46.3
48.3
50.1
39.8

Table 14: Results noun-modifier questions.
table, dual-space refers dual-space model using Equation 31. holistic
model, ab represented corresponding row vector given space. Recall
Section 3.1 that, Step 1.1, rows matrices correspond n-grams WordNet,
n may greater one. Thus, example, dog house corresponding
row vector three spaces. holistic model simply uses row vector
representation dog house. element-wise multiplication, ab represented using
Equation 21. vector addition model, ab represented + b, vectors
normalized unit length added. four models use constraints
6= c b 6= c. four models use training data parameter tuning.
difference dual-space model (58.3%) best variation elementwise multiplication (57.5%) statistically significant 95% confidence level, according Fishers Exact Test. However, difference dual-space model
(58.3%) best variation vector addition (50.1%) significant.
4.2.2 Limitations Holistic Approach
three spaces, holistic model significantly better models,
inability address issue linguistic creativity major limitation. 2180 multiplechoice questions used experiments intentionally constructed
requirement stem bigram must corresponding row function space (see
above). done could use holistic model baseline; however,
gives misleading impression holistic model serious competitor
compositional approaches. design, Table 14 shows holistic model achieve
ideal (but unrealistic) conditions.
562

fiDomain Function: Dual-Space Model

Mitchell Lapatas (2010) dataset, used experiments Section 4.3, illustrates
limitations holistic model. dataset consists 324 distinct pairs bigrams,
composed 216 distinct bigrams. 216 bigrams, 28 (13%) occur WordNet.
324 pairs bigrams, 13 (4%) contain bigrams occur WordNet. Given
matrices use (with rows based WordNet), holistic approach would
reduced random guessing 96% pairs Mitchell Lapatas (2010) dataset.
might argued failure holistic approach Mitchell Lapatas
(2010) dataset due decision base rows matrices terms WordNet.
However, suppose attempt build holistic model frequent bigrams. Web
1T 5-gram corpus (Brants & Franz, 2006) includes list bigrams appeared 40
times terabyte text, total 314,843,401 bigrams. Using compositional
approach, matrices use represent majority bigrams.
hand, holistic approach would require matrix 314,843,401 rows,
considerably beyond current state art.
One possibility build matrix holistic approach needed, given
input set n-grams, instead building large, static, multipurpose matrix.
two problems idea. First, slow. Turney (2006b) used approach
SAT analogy questions, required nine days run, whereas dual-space model
process SAT questions seconds, given static, multipurpose matrix. Second,
requires large corpus, corpus size must grow exponentially n, length
phrases. Longer phrases rare, larger corpora needed gather sufficient
data model phrases. Larger corpora result longer processing times.
given application, may wise predefined list bigrams holistic
representations, would wise expect list sufficient cover
bigrams would seen practice. creativity human language use requires
compositional models (Chomsky, 1975; Fodor & Lepore, 2002). Although holistic model
included baseline experiments, competitor models;
supplement models.
4.2.3 Impact Constraints
use sim1 alone (Equation 30), dropping constraints 6= c b 6= c, accuracy
drops signficantly, 58.3% 13.7%. However, models benefit greatly
constraints. Table 15, take best variation model Table 14
look happens constraints dropped.

Algorithm
dual-space
holistic
multiplication
addition

Space
domain function
mono
domain
domain

constraints
58.3
81.6
57.5
50.1

Accuracy
constraints
13.7
49.6
8.2
2.5

difference
-44.6
-32.0
-49.3
-47.6

Table 15: impact constraints, 6= c b 6= c, accuracy.

563

fiTurney

4.2.4 Element-wise Multiplication
Section 3.8, argued c = b suitable row vectors matrix Uk pk
suggested Equation 21 alternative. use c = b domain
space, instead Equation 21, performance drops significantly, 57.5% 21.5%.
4.2.5 Impact Idioms
gap holistic model models may due idiomatic
bigrams testing questions. One successful approaches determining
whether multiword expression (MWE) compositional noncompositional (idiomatic)
compare holistic vector representation compositional vector representation
(for example, high cosine two vectors suggests MWE compositional,
idiomatic) (Biemann & Giesbrecht, 2011; Johannsen, Alonso, Rishj, & Sgaard, 2011).
However, approach suitable here, want assume
gap entirely due idiomatic bigrams; instead, would estimate much
gap due idiomatic bigrams.
WordNet contains clues use indicators bigram might less
compositional bigrams (allowing compositionality matter degree).
One clue whether WordNet gloss bigram contains either head noun
modifier. example, gloss dog house outbuilding serves shelter
dog, contains modifier, dog. suggests dog house may compositional.
classified 1500 testing set questions head (the first five characters
head noun bigram match first five characters word bigrams gloss),
modifier (the first five characters modifier bigram match first five characters
word bigrams gloss), (both head modifier match), neither
(neither head modifier match). four classes approximately equally
distributed testing questions (424 head, 302 modifier, 330 both, 444 neither).
match first five characters allow cases brain surgeon, gloss
someone surgery nervous system (especially brain). bigram
classified both, first five characters surgeon match first five characters
surgery.
Table 16 shows accuracy models varies four classes questions. three compositional models (dual-space, multiplication, addition), neither class significantly less accurate three classes (Fishers Exact Test,
95% confidence), difference significant holistic model. three
compositional models, neither class 17% 20% less accurate classes.
supports view significant fraction wrong answers compositional
models due noncompositional bigrams.
Another clue compositionality WordNet whether head noun hypernym
bigram. example, surgeon hypernym brain surgeon. classified
1500 testing set questions hyper (the head noun member synset
immediate hypernym first noun sense bigram; look
hypernym hierarchy look senses bigram) (not hyper).
testing set, 621 questions hyper 879 not.
564

fiDomain Function: Dual-Space Model

Algorithm
dual-space
holistic
multiplication
addition

Space
domain function
mono
domain
domain


63.0
82.7
61.8
53.6

head
63.0
83.7
63.7
56.8

Accuracy
modifier neither
64.6
45.9
82.1
78.4
62.9
44.8
56.3
36.7


58.3
81.6
57.5
50.1

Table 16: variation accuracy different classes bigram glosses.
Table 17 gives accuracy models classes. table
general pattern Table 16. three compositional models significantly lower
accuracy class, decreases 6% 8%. significant difference
holistic model.
Algorithm
dual-space
holistic
multiplication
addition

Space
domain function
mono
domain
domain

Accuracy
hyper
62.0 55.6
81.0 82.0
61.8 54.5
54.8 46.8


58.3
81.6
57.5
50.1

Table 17: variation accuracy different classes bigram hypernyms.

4.2.6 Order Sensitivity
Note vector addition element-wise multiplication lack order sensitivity, Equation 31 sensitive order, simc (ab, c) 6= simc (ba, c). see impact
reformulating noun-modifier questions test order-sensitivity. First
expand choice unigram c including stem bigram ab, resulting explicit
comparison ab c. expanded choice, ab c, generate another choice,
ba c. increases number choices seven fourteen. Due symmetry,
vector addition element-wise multiplication must assign similarity
ab c ba c.
Table 18 compares dual-space model element-wise multiplication vector addition, using reformulated fourteen-choice noun-modifier questions. holistic model
included table rows matrices reversed ba
bigrams (which may seen another illustration limits holistic model).
stricter test, dual-space model significantly accurate element-wise
multiplication vector addition (Fishers Exact Test, 95% confidence).
dual-space model perform well fourteen-choice questions, need
simd simf . drop simd Equation 31 (function alone Table 18),
ignoring modifier paying attention head noun. Accuracy drops
41.5% 25.7%. drop simf Equation 31 (domain alone Table 18),
equation becomes symmetrical, similarity assigned ab c
565

fiTurney

Algorithm
dual-space
multiplication
modified dual-space
modified dual-space
addition

Space
domain function
domain
function alone
domain alone
domain

Accuracy
41.5
27.4
25.7
25.7
22.5

Table 18: Results reformulated fourteen-choice noun-modifier questions.
ba c. Accuracy drops 41.5% 25.7%.18 dual-space model significantly
accurate either modified dual-space models (Fishers Exact Test, 95%
confidence).
4.2.7 Summary
reformulated fourteen-choice noun-modifier questions (Table 18), dual-space
significantly better element-wise multiplication vector addition. original
seven-choice questions (Table 14), difference large, questions
test order. Unlike element-wise multiplication vector addition, dual-space
model addresses issue order sensitivity. Unlike holistic model, dual-space
addresses issue linguistic creativity.
4.3 Similarity Phrases
subsection, apply dual-space model measuring similarity phrases,
using Mitchell Lapatas (2010) dataset human similarity ratings pairs phrases.
dataset includes three types phrases, adjective-noun, noun-noun, verb-object.
108 pairs type (108 3 = 324 pairs phrases). pair phrases
rated 18 human subjects. ratings use 7 point scale, 1 signifies lowest
degree similarity 7 signifies highest degree. Table 19 gives examples.
Let ab represent first phrase pair phrases (environment secretary) let cd
represent second phrase (defence minister). rate similarity phrase pairs
simp (ab, cd), defined follows:
simp (ab, cd) = geo(simd (a, c), simd (b, d), simf (a, c), simf (b, d))

(32)

equation based instructions human participants (Mitchell & Lapata,
2010, Appendix B), imply function domain similarity must high
phrase pair get high similarity rating. Figure 3 illustrates reasoning behind
equation. want high domain function similarities corresponding
components phrases ab cd.
18. coincidence modified dual-space models accuracy 25.7% fourteenchoice questions. Although aggregate accuracy same, individual questions, two models
typically select different choices.

566

fiDomain Function: Dual-Space Model

Participant
114
114
114
109
109
109
111
111
111

Phrase type
adjective-noun
adjective-noun
adjective-noun
noun-noun
noun-noun
noun-noun
verb-object
verb-object
verb-object

Group
2
2
2
0
0
0
2
2
2

Phrase pair
certain circumstance particular case
large number great majority
evidence low cost
environment secretary defence minister
action programme development plan
city centre research work
lift hand raise head
satisfy demand emphasise need
people increase number

Similarity
6
4
2
6
4
1
7
4
1

Table 19: Examples phrase pair similarity ratings Mitchell Lapatas (2010)
dataset. Similarity ratings vary 1 (lowest) 7 (highest).



b

F

F

c


simp (ab, cd)
phrasal similarity

Figure 3: diagram Equation 32.

4.3.1 Experimental Setup
Mitchell Lapata (2010) divided dataset development set (for tuning parameters) evaluation set (for testing tuned models). development set
6 ratings phrase pair evaluation set 12 ratings phrase pair.
development evaluation sets contain phrase pairs, judgments
different participants. Thus 6324 = 1, 944 rated phrase pairs development
set 12 324 = 3, 888 ratings evaluation set.19
challenging evaluation, divide dataset phrase pairs rather
participants. development set 108 phrase pairs 18 ratings
evaluation set 216 phrase pairs 18 ratings each. three phrase types,
randomly select 36 phrase pairs development set (3 36 = 108 phrase pairs)
19. information paragraph based Section 4.3 paper Mitchell Lapata (2010)
personal communication Jeff Mitchell June, 2010.

567

fiTurney

72 evaluation set (3 72 = 216 phrase pairs). Thus 18 108 = 1, 944
ratings development set 18 216 = 3, 888 evaluation set.
Mitchell Lapata (2010) use Spearmans rank correlation coefficient (Spearmans
rho) evaluate performance various vector composition algorithms task
emulating human similarity ratings. given phrase type, 108 phrase pairs
divided 3 groups 36 pairs each. group evaluation set, 12 people
gave similarity ratings pairs given group. group 36 pairs given
different group 12 people. score algorithm given phrase type
average three rho values, one rho three groups. 12 people rating 36
pairs group, 12 36 = 432 ratings. human ratings represented
vector 432 numbers. algorithm generates one rating pair group,
yielding 36 numbers. make algorithms ratings comparable human ratings,
algorithms ratings duplicated 12 times, yielding vector 432 numbers. Spearmans
rho calculated two vectors 432 ratings. 3 phrase types 3 rho
values 432 ratings per rho value, 3,888 ratings.20
believe evaluation method underestimates performance algorithms. Combining ratings different people one vector 432 numbers
allow correlation adapt different biases. one person gives consistently low ratings
another person gives consistently high ratings, people ranking,
ranking matches algorithms ranking, algorithm get high
score. fair evaluation, score algorithm calculating one rho value
human participant given phrase type, calculate average
rho values participants.
given phrase type, 108 phrase pairs divided 3 groups 36 pairs each.
development set, randomly select 12 phrase pairs 3 groups
(3 12 = 36 phrase pairs per phrase type). leaves 24 phrase pairs 3
groups evaluation set (3 24 = 72 phrase pairs per phrase type). human
participants ratings represented vector 24 numbers. algorithms ratings
represented vector 24 numbers. rho value calculated two vectors
24 numbers input. given phrase type, algorithms score average 54
rho values (18 participants per group 3 groups = 54 rho values). 3 phrase types
54 rho values 24 ratings per rho value, 3,888 ratings.
4.3.2 Comparison Approaches
Table 20 compares dual-space model vector addition element-wise multiplication.
use development set tune parameters three approaches. vector
addition, ab represented + b cd represented c + d. similarity ab
cd given cosine two vectors. Element-wise multiplication uses Equation 21
represent ab cd. dual-space model uses Equation 32.
average correlation dual-space model (0.48) significantly average
correlation vector addition using function space (0.51). Element-wise multiplication
mono space (0.47) significantly vector addition using function space (0.51).
20. information paragraph based personal communication Jeff Mitchell June, 2010.
Mitchell Lapatas (2010) paper describe Spearmans rho applied.

568

fiDomain Function: Dual-Space Model

Algorithm
human
dual-space
addition
addition
addition
multiplication
multiplication
multiplication

Correlation
ad-nn nn-nn
0.56
0.54
0.48
0.54
0.47
0.61
0.32
0.55
0.49
0.55
0.43
0.57
0.35
0.58
0.39
0.45

phrase type
vb-ob avg
0.57
0.56
0.43
0.48
0.42
0.50
0.41
0.42
0.48
0.51
0.41
0.47
0.39
0.44
0.27
0.37

Comment
leave-one-out correlation subjects
domain function space
mono space
domain space
function space
mono space
domain space
function space

Table 20: Performance models evaluation dataset.
difference dual-space model (0.48) element-wise multiplication mono
space (0.47) signficant. average correlation algorithm based 162 rho
values (3 phrase types 3 groups 18 participants = 162 rho values = 162 participants).
calculate statistical significance using paired t-test 95% significance level,
based 162 pairs rho values.
4.3.3 Order Sensitivity
Mitchell Lapatas (2010) dataset test order sensitivity. Given phrase pair
ab cd, test order sensitivity adding new pair ab dc. assume
new pairs would given rating 1 human participants. Table 21,
show happens transformation applied examples Table 19.
save space, give examples participant number 114.
Participant
114
114
114
114
114
114

Phrase type
adjective-noun
adjective-noun
adjective-noun
adjective-noun
adjective-noun
adjective-noun

Group
2
2
2
2
2
2

Phrase pair
certain circumstance particular case
certain circumstance case particular
large number great majority
large number majority great
evidence low cost
evidence cost low

Similarity
6
1
4
1
2
1

Table 21: Testing order sensitivity adding new phrase pairs.
Table 22 gives results new, expanded dataset. stringent
dataset, dual-space model performs significantly better vector addition
vector multiplication. Unlike element-wise multiplication vector addition, dualspace model addresses issue order sensitivity.
manually inspected new pairs automatically rated 1 found
rating 1 reasonable cases, although cases could disputed. example,
original noun-noun pair tax charge interest rate generates new pair tax charge
rate interest original verb-object pair produce effect achieve result generates
new pair produce effect result achieve. seems natural tendency correct
569

fiTurney

Algorithm
human
dual-space
addition
addition
addition
multiplication
multiplication
multiplication

Correlation
ad-nn nn-nn
0.71
0.81
0.66
0.37
0.22
0.25
0.15
0.22
0.23
0.23
0.20
0.24
0.18
0.22
0.18
0.19

phrase type
vb-ob avg
0.73
0.75
0.62
0.55
0.19
0.22
0.18
0.18
0.19
0.22
0.18
0.21
0.18
0.19
0.12
0.17

Comment
leave-one-out correlation subjects
domain function space
mono space
domain space
function space
mono space
domain space
function space

Table 22: Performance dataset expanded test order sensitivity.
incorrectly ordered pairs minds assign higher ratings
deserve. predict human ratings new pairs would vary greatly, depending
instructions given human raters. instructions emphasized
importance word order, new pairs would get low ratings. prediction supported
results SemEval 2012 Task 2 (Jurgens, Mohammad, Turney, & Holyoak, 2012),
instructions raters emphasized importance word order wrongly
ordered pairs received low ratings.
4.3.4 Summary
dataset test order sensitivity, vector addition performs slightly better
dual-space model. dataset tests order sensitivity, dual-space
model surpasses vector addition element-wise multiplication large margin.
4.4 Domain versus Function Associated versus Similar
Chiarello et al. (1990) created dataset 144 word pairs labeled similar-only,
associated-only, similar+associated (48 pairs three classes). Table 23
shows examples dataset. labeled pairs created cognitive
psychology experiments human subjects. experiments, found evidence
processing associated words engages left right hemispheres brain
ways different processing similar words. is, seems
fundamental neurological difference two types semantic relatedness.21
hypothesize similarity domain space, simd (a, b), measure degree
two words associated similarity function space, simf (a, b), measure
degree two words similar. test hypothesis, define similar-only,
simso (a, b), associated-only, simao (a, b), similar+associated, simsa (a, b), follows:

ratio(x, y) =

x/y x > 0 > 0
0 otherwise

(33)

21. controversy among cognitive scientists distinction semantic similarity
association (McRae, Khalkhali, & Hare, 2011).

570

fiDomain Function: Dual-Space Model

Word pair
table:bed
music:art
hair:fur
house:cabin
cradle:baby
mug:beer
camel:hump
cheese:mouse
ale:beer
uncle:aunt
pepper:salt
frown:smile

Class label
similar-only
similar-only
similar-only
similar-only
associated-only
associated-only
associated-only
associated-only
similar+associated
similar+associated
similar+associated
similar+associated

Table 23: Examples word pairs Chiarello et al. (1990), labeled similar-only,
associated-only, similar+associated. full dataset Appendix.

simso (a, b) = ratio(simf (a, b), simd (a, b))

(34)

simao (a, b) = ratio(simd (a, b), simf (a, b))

(35)

simsa (a, b) = geo(simd (a, b), simf (a, b))

(36)

intention simso high simf high simd low, simao high
simd high simf low, simsa high simd simf high.
illustrated Figure 4.






F

F

F

b

b

b

simso (a, b)

simao (a, b)

simsa (a, b)

similar-only

associated-only

similar+associated

Figure 4: Diagrams Equations 34, 35, 36.

571

fiTurney

4.4.1 Evaluation
experiments three preceding subsections, three sets parameter
settings dual-space model. Table 24 shows parameter values. effect,
three sets parameter setttings give us three variations similarity measures, simso ,
simao , simsa . evaluate three variations see well correspond
labels Chiarello et al.s (1990) dataset.
Similarity
simr (a : b, c : d)
simc (ab, c)
simp (ab, cd)

Description
similarity relations
similarity noun-modifier compositions
similarity phrases

Section
4.1
4.2
4.3

kd
800
800
200

pd
-0.1
0.3
0.3

kf
300
100
600

pf
0.5
0.6
0.6

Table 24: Parameter settings dual-space model.
given similarity measure, simso , sort 144 word pairs descending order similarities look top N pairs see many
desired label; case simso , would see majority
top N label similar-only. Table 25 shows percentage pairs
desired labels three variations three similarity measures. Note
random guessing would yield 33%, since three classes pairs size.

Source parameters
simr (a : b, c : d)

simc (ab, c)

simp (ab, cd)

N
10
20
30
10
20
30
10
20
30

Percentage top N desired label
similar-only associated-only similar+associated
70
90
90
80
85
80
63
77
73
90
90
80
80
70
70
70
67
73
50
90
80
65
80
80
47
77
73

Table 25: Percentage top N word pairs desired labels.
three sets parameter settings, Table 25 displays high density desired
labels tops sorted lists. density slowly decreases move
lists. evidence three similarity measures capturing three classes
Chiarello et al. (1990).
another test hypothesis, use three similarity measures create feature
vectors three elements word pair. is, word pair : b represented
feature vector hsimso (a, b), simao (a, b), simsa (a, b)i. use supervised learning
ten-fold cross-validation classify feature vectors three classes Chiarello
et al. (1990). learning algorithm, use logistic regression, implemented
572

fiDomain Function: Dual-Space Model

Weka.22 results summarized Table 26. results lend support
hypothesis similarity domain space, simd (a, b), measure degree
two words associated similarity function space, simf (a, b), measure
degree two words similar.

Source parameters
simr (a : b, c : d)
simc (ab, c)
simp (ab, cd)

Accuracy
61.1
59.0
58.3

similar-only
0.547
0.583
0.472

F-measure
associated-only similar+associated
0.660
0.625
0.702
0.490
0.699
0.563

average
0.611
0.592
0.578

Table 26: Performance logistic regression three similarity measures features.

Table 25, similar-only seems sensitive parameter settings associatedonly similar+associated. hypothesize function similarity
difficult measure domain similarity. Note construction function
space (Section 3.3) complex construction domain space (Section 3.2).
Intuitively, seems easier identify domain thing identify functional
role. Gentners (1991) work suggests children master domain similarity
become competent function similarity.

5. Discussion Experiments
section discusses results previous section.
5.1 Summary Results
Section 4.1, used 374 multiple-choice analogy questions evaluate dual-space
model relational similarity, simr (a : b, c : d). difference performance
dual-space model (51.1% accuracy) best past result (56.1% accuracy), using
holistic model, statistically significant. Experiments reformulated version
questions, designed test order sensitivity, supported hypothesis
domain function space required. Function space sensitive order
merging two spaces (mono space) causes significant drop performance.
Section 4.2, automatically generated 2,180 multiple-choice noun-modifier composition questions WordNet, evaluate dual-space model noun-modifier compositional similarity, simc (ab, c). difference performance dual-space
model (58.3% accuracy) state-of-the-art element-wise multiplication model (57.5%
accuracy) statistically significant. best performance obtained holistic model (81.6%), model address issue linguistic creativity.
experiments suggest significant fraction gap holistic model
models due noncompositional phrases. limitation element-wise multiplication model lack sensitivity order. Experiments reformulated version
22. Weka available http://www.cs.waikato.ac.nz/ml/weka/.

573

fiTurney

questions, designed test order sensitivitiy, demonstrated statistically significant
advantage dual-space model element-wise multiplication vector addition
models.
Section 4.3, used Mitchell Lapatas (2010) dataset 324 pairs phrases
evaluate dual-space model phrasal similarity, simp (ab, cd). reformulated version
dataset, modified test order sensitivitiy, showed statistically significant advantage
dual-space model element-wise multiplication vector addition models.
Section 4.4, used Chiarello et al.s (1990) dataset 144 word pairs, labeled
similar-only, associated-only, similar+associated, test hypothesis similarity
domain space, simd (a, b), measure degree two words associated
similarity function space, simf (a, b), measure degree two words
similar. experimental results support hypothesis. interesting
Chiarello et al. (1990) argue fundamental neurological difference way
people process two kinds semantic relatedness.
experiments support claim dual-space model address issues
linguistic creativity, order sensitivity, adaptive capacity. Furthermore, dual-space
model provides unified approach semantic relations semantic composition.
5.2 Corpus-based Similarity versus Lexicon-based Similarity
results Section 4.4 suggest function similarity may correspond kind
taxonomical similarity often associated lexicons, WordNet (Resnik,
1995; Jiang & Conrath, 1997; Leacock & Chodrow, 1998; Hirst & St-Onge, 1998).
word pairs Table 23 labeled similar-only kinds words typically
share common hypernym taxonomy. example, table:bed share hypernym
furniture. believe correct, necessarily imply lexiconbased similarity measures would better corpus-based approach,
used here.
various similarities Section 4, arguably relational similarity, simr (a : b, c : d),
makes use function similarity. itself, function similarity achieves 50.8%
SAT questions (original five-choice version; see Table 12). However, best performance
achieved SAT questions using WordNet 43.0% (Veale, 2004). difference
statistically significant 95% confidence level, based Fishers Exact Test.
Consider analogy traffic street water riverbed. One SAT questions
involves analogy, traffic :street stem pair water :riverbed correct
choice. simr (a : b, c : d) (Equation 25) function similarity (Equation 22)
make correct choice. recognize traffic water high degree
function similarity; fact, similarity used hydrodynamic models traffic flow
(Daganzo, 1994). However, must climb WordNet hierachy way entity
find shared hypernym traffic water. believe manually
generated lexicon capture functional similarity discovered
large corpus.

6. Theoretical Considerations
section examines theoretical questions dual-space model.
574

fiDomain Function: Dual-Space Model

6.1 Vector Composition versus Similarity Composition
dual-space model, phrase stand-alone, general-purpose representation,
composite phrase, apart representations component words. composite
meaning constructed context given task. example, task measure
similarity relation dog :house relation bird :nest, compose
meanings dog house one way (see Section 4.1); task measure similarity
phrase dog house word kennel, compose meanings dog
house another way (see Section 4.2); task measure similarity phrase
dog house phrase canine shelter, compose meanings dog house
third way (see Section 4.3). composition construction explicitly ties together
two things compared, depends nature comparison
desired, task performed. hypothesize single stand-alone,
task-independent representation constructed suitable purposes.
noted introduction, composition vectors result stand-alone
representation phrase, composing similarities necessarily yields linking structure
connects phrase phrases. linking structures seen Figures
1 4. Intuitively, seems important part understand phrase
connecting phrases. Part understanding dog house connection
kennel. Dictionaries make kinds connections explicit. perspective,
idea explicit linking structure seems natural, given making connnections among
words phrases essential aspect meaning understanding.
6.2 General Form Similarities Dual-Space Model
subsection, present general scheme ties together various similarities
defined Section 4. scheme includes similarities chunks text
arbitrary size. scheme encompasses phrasal similarity, relational similarity,
compositional similarity.
Let chunk text (an ordered set words), ht1 , t2 , . . . , tn i, ti
word. represent semantics = hD, Fi, F matrices.
row vector di D, = 1, 2, . . . , n, row vector domain space represents
domain semantics word ti . row vector F, = 1, 2, . . . , n, row vector
function space represents function semantics word ti . keep notation
simple, parameters, kd pd domain space kf pf function space,
implicit. Assume row vectors F normalized unit length. Note
size representation scales linearly n, number words t, hence
information scalability. large values n, inevitably duplicate
words t, representation could easily compressed sublinear size without loss
information.
Let t1 t2 two chunks text representations T1 = hD1 , F1 T2 =
hD2 , F2 i, t1 contains n1 words t2 n2 words. Let D1 D2
parameters, kd pd , let F1 F2 parameters, kf pf . D1
n1 kd , D2 n2 kd , F1 n1 kf , F2 n2 kf . Note D1 DT
1 n1 n1
matrix cosines two row vectors D1 . is, element i-th
575

fiTurney


row j-th column D1 DT
1 cos(di , dj ). Likewise, D1 D2 n1 n2 matrix
cosines row vector D1 row vector D2 .
Suppose wish measure similarity, sim(t1 , t2 ), two chunks
text, t1 t2 . paper, restricted similarity measures following
general form:





sim(t1 , t2 ) = f (D1 DT
1 , 1 2 , 2 2 , F1 F1 , F1 F2 , F2 F2 )

(37)

words, input composition function f cosines (and implicit
parameters, kd , pd , kf , pf ); f operate directly row vectors D1 ,
D2 , F1 , F2 . contrast much work discussed Section 2.1, composition
operation shifted representations, T1 T2 , similarity measure,
f . exact specification f depends task hand. T1 T2 sentences,
envision structure f determined syntactic structures
two sentences.23
Consider relational similarity (Section 4.1):

sim1 (a : b, c : d) = geo(simf (a, c), simf (b, d))

(38)

sim2 (a : b, c : d) = geo(simd (a, b), simd (c, d))

(39)

sim3 (a : b, c : d) = geo(simd (a, d), simd (c, b))

sim1 (a : b, c : d) sim2 (a : b, c : d) sim3 (a : b, c : d)
simr (a : b, c : d) =
0 otherwise

(40)
(41)

fits form Equation 37 t1 = ha, bi t2 = hc, di. see


sim1 based cosines F1 FT
2 , sim2 based cosines D1 D1 D2 D2 ,

sim3 based cosines D1 D2 .
Consider compositional similarity (Section 4.2):

sim1 (ab, c) = geo(simd (a, c), simd (b, c), simf (b, c))

sim1 (ab, c) 6= c b 6= c
simc (ab, c) =
0 otherwise

(42)
(43)

seen instance Equation 37 t1 = ha, bi t2 = hci.

case, sim1 based cosines D1 DT
2 F1 F2 . constraints, 6= c b 6= c,

expressed terms cosines D1 D2 , simd (a, c) 6= 1 simd (b, c) 6= 1.
(Equivalently, could use cosines F1 FT
2 .) Similar analyses apply similarities
Sections 4.3 4.4; similarities instances Equation 37.
Although representations T1 T2 sizes linear functions numbers phrases t1 t2 , size composition Equation 37 quadratic
function numbers phrases t1 t2 . However, specific instances general
equation may less quadratic size, may possible limit growth
23. Note requirement two chunks text, t1 t2 , number
words. is, n1 necessarily equal n2 . Section 4.2, n1 6= n2 .

576

fiDomain Function: Dual-Space Model

linear function. Also, general, quadratic growth often acceptable practical
applications (Garey & Johnson, 1979).
function words (e.g., prepositions, conjunctions), one option would treat
words. would represented vectors similarities would calculated function domain spaces. Another possibility would
use function words hints guide construction composition function f .
function words would correspond vectors; instead would contribute determining linking structure connects two given chunks text. first option
appears elegant, choice options made empirically.
6.3 Automatic Composition Similarities
Section 4, manually constructed functions combined similarity measures,
using intuition background knowledge. Manual construction scale
task comparing two arbitrarily chosen sentences. However, good reasons
believing construction composition functions automated.
Turney (2008a) presents algorithm solving analogical mapping problems,
analogy solar system Rutherford-Bohr model atom. Given
list terms solar system domain, {planet, attracts, revolves, sun, gravity, solar system, mass}, list terms atomic domain, {revolves, atom, attracts,
electromagnetism, nucleus, charge, electron}, automatically generate one-to-one
mapping one domain other, {solar system atom, sun nucleus, planet
electron, mass charge, attracts attracts, revolves revolves, gravity electromagnetism}. twenty analogical mapping problems, attains accuracy 91.5%,
compared average human accuracy 87.6%.
algorithm scores quality candidate analogical mapping composing
similarities mapped terms. composition function addition individual
component similarities holistic relational similarities. algorithm searches
space possible mappings mapping maximizes composite similarity
measure. is, analogical mapping treated argmax problem, argument
maximized mapping function. effect, output algorithm (an analogical
mapping) automically generated composition similarities. mapping structures
found algorithm essentially linking structures see
Figures 1 4.
believe variation Turneys (2008a) algorithm could used automatically compose similarities dual-space model; example, possible
identify paraphrases using automatic similarity composition. proposal search
composition maximizes composite similarity, subject various constraints (such
constraints based syntax sentences). Turney (2008a) points analogical
mapping could used align words two sentences, experimentally
evaluate suggestion.
Recent work (Lin & Bilmes, 2011) shown argmax problems solved efficiently effectively framed monotone submodular function maximization
problems. believe automatic composition similarities fit naturally
framework, would result highly scalable algorithms semantic composition.
577

fiTurney

Regarding information scalability, dual-space model suffer information
loss (unlike approaches represent compositions vectors fixed dimensionality),
sizes representations grow lengths phrases grow. growth
might quadratic, exponential. questions automate
composition similarities, may impact computational complexity
scaling longer phrases, evidence questions tractable.

7. Limitations Future Work
One area future work experiment longer phrases (more two words)
sentences, discussed Section 6.3. interesting topic research parsing might
used constrain automatic search similarity composition functions.
focused two spaces, domain function, seems likely us
model spaces would yield better performance. currently experimenting quad-space model includes domain (noun-based contextual patterns),
function (verb-based), quality (adjective-based), manner (adverb-based) spaces.
preliminary results quad-space promising. Quad-space seems related
Pustejovskys (1991) four-part qualia structure.
Another issue avoided morphology. discussed Section 3.6, used
validForms function WordNet::QueryData Perl interface WordNet map
morphological variations words base forms. implies that, example,
singular noun plural form semantic representation.
certainly simplification sophisticated model would use different representations
different morphological forms word.
avoided issue polysemy. possible extend past work
polysemy VSMs dual-space model (Schutze, 1998; Pantel & Lin, 2002; Erk
& Pado, 2008).
paper, treated holistic model dual-space model
competitors, certain cases, idiomatic expressions, holistic
approach required. Likewise, holistic approach limited inability handle
linguistic creativity. considerations suggest holistic dual-space models
must integrated. another topic future work.
Arguably limitation dual-space model four parameters
tune (kd , pd , kf , pf ). hand, perhaps model adaptive capacity
must parameters tune. research needed.
number design decisions made construction domain function
space, especially conversion phrases contextual patterns (Sections 3.2 3.3).
decisions guided intuitions. expect exploration experimental evaluation design space fruitful area future research.
construction function space (Section 3.3) specific English. may generalize readily Indo-European languages, languages may present
challenge. another topic future research.
composite similarities use geometric mean combine domain
function similarities, see reason restrict possible composition functions.
578

fiDomain Function: Dual-Space Model

Equation 37 allows composition function f . Exploring space possible composition
functions another topic future work.
Another question formal logic textual entailment integrated
approach. dual-space model seems suitable recognizing paraphrases,
obvious way handle entailment. generally, focused various
kinds similarity, scale phrases (red ball) sentences (The ball
red), encounter truth falsity. Gardenfors (2004) argues spatial models
bridge low-level connectionist models high-level symbolic models. claims
spatial models best questions similarity symbolic models best
questions truth. yet know join two kinds models.

8. Conclusions
goal research develop model unifies semantic relations
compositions, addressing linguistic creativity, order sensitivity, adaptive capacity, information scalability. believe dual-space model achieves goal,
although certainly room improvement research.
many kinds wordcontext matrices, based various notions context;
Sahlgren (2006) gives good overview types context explored
past work. novelty dual-space model includes two distinct
complementary wordcontext matrices work together synergistically.
two distinct spaces, two distinct similarity measures,
combined many different ways. multiple similarity measures, similarity composition becomes viable alternative vector composition. example, instead multiplying vectors, c = b, multiply similarities, simsa (a, b) =
geo(simd (a, b), simf (a, b)). results suggest fruitful new way look
problems semantics.

Acknowledgments
Thanks George Foster, Yair Neuman, David Jurgens, reviewers JAIR
helpful comments earlier version paper. Thanks Charles Clarke
corpus used build three spaces, Stefan Buttcher Wumpus,
creators WordNet making lexicon available, developers OpenNLP,
Doug Rohde SVDLIBC, Jeff Mitchell Mirella Lapata sharing data
answering questions evaluation methodology, Christine Chiarello, Curt
Burgess, Lorie Richards, Alma Pollock making data available, Jason Rennie
WordNet::QueryData Perl interface WordNet, developers Perl Data
Language.

References
Aerts, D., & Czachor, M. (2004). Quantum aspects semantic analysis symbolic
artificial intelligence. Journal Physics A: Mathematical General, 37, L123
L132.
579

fiTurney

Baroni, M., & Zamparelli, R. (2010). Nouns vectors, adjectives matrices: Representing adjective-noun constructions semantic space. Proceedings 2010
Conference Empirical Methods Natural Language Processing (EMNLP 2010),
pp. 11831193.
Bengio, Y., Ducharme, R., Vincent, P., & Jauvin, C. (2003). neural probabilistic language
model. Journal Machine Learning Research, 3, 11371155.
Bicici, E., & Yuret, D. (2006). Clustering word pairs answer analogy questions.
Proceedings Fifteenth Turkish Symposium Artificial Intelligence Neural
Networks (TAINN 2006), Akyaka, Mugla, Turkey.
Biemann, C., & Giesbrecht, E. (2011). Distributional semantics compositionality 2011:
Shared task description results. Proceedings Workshop Distributional
Semantics Compositionality (DiSCo 2011), pp. 2128, Portland, Oregon.
Bollegala, D., Matsuo, Y., & Ishizuka, M. (2009). Measuring similarity implicit
semantic relations Web. Proceedings 18th International Conference
World Wide Web (WWW 2009), pp. 651660.
Brants, T., & Franz, A. (2006). Web 1T 5-gram Version 1. Linguistic Data Consortium,
Philadelphia.
Bullinaria, J., & Levy, J. (2007). Extracting semantic representations word cooccurrence statistics: computational study. Behavior Research Methods, 39 (3),
510526.
Buttcher, S., & Clarke, C. (2005). Efficiency vs. effectiveness terabyte-scale information retrieval. Proceedings 14th Text REtrieval Conference (TREC 2005),
Gaithersburg, MD.
Caron, J. (2001). Experiments LSA scoring: Optimal rank basis.. Proceedings
SIAM Computational Information Retrieval Workshop, pp. 157169, Raleigh,
NC.
Chiarello, C., Burgess, C., Richards, L., & Pollock, A. (1990). Semantic associative
priming cerebral hemispheres: words do, words dont . . . sometimes,
places. Brain Language, 38, 75104.
Chomsky, N. (1975). Logical Structure Linguistic Theory. Plenum Press.
Church, K., & Hanks, P. (1989). Word association norms, mutual information, lexicography. Proceedings 27th Annual Conference Association Computational Linguistics, pp. 7683, Vancouver, British Columbia.
Clark, S., Coecke, B., & Sadrzadeh, M. (2008). compositional distributional model
meaning. Proceedings 2nd Symposium Quantum Interaction, pp. 133140,
Oxford, UK.
Clark, S., & Pulman, S. (2007). Combining symbolic distributional models meaning.
Proceedings AAAI Spring Symposium Quantum Interaction, pp. 5255,
Stanford, CA.
Conway, J. H., & Sloane, N. J. A. (1998). Sphere Packings, Lattices Groups. Springer.
580

fiDomain Function: Dual-Space Model

Daganzo, C. F. (1994). cell transmission model: dynamic representation highway
traffic consistent hydrodynamic theory. Transportation Research Part B:
Methodological, 28 (4), 269287.
Davidov, D., & Rappoport, A. (2008). Unsupervised discovery generic relationships using
pattern clusters evaluation automatically generated SAT analogy questions.
Proceedings 46th Annual Meeting ACL HLT (ACL-HLT-08), pp.
692700, Columbus, Ohio.
Erk, K., & Pado, S. (2008). structured vector space model word meaning context.
Proceedings 2008 Conference Empirical Methods Natural Language
Processing (EMNLP-08), pp. 897906, Honolulu, HI.
Fellbaum, C. (Ed.). (1998). WordNet: Electronic Lexical Database. MIT Press.
Firth, J. R. (1957). synopsis linguistic theory 19301955. Studies Linguistic
Analysis, pp. 132. Blackwell, Oxford.
Fodor, J., & Lepore, E. (2002). Compositionality Papers. Oxford University Press.
Gardenfors, P. (2004). Conceptual Spaces: Geometry Thought. MIT Press.
Garey, M. R., & Johnson, D. S. (1979). Computers Intractability: Guide Theory
NP-Completeness. Freeman.
Gentner, D. (1983). Structure-mapping: theoretical framework analogy. Cognitive
Science, 7 (2), 155170.
Gentner, D. (1991). Language career similarity. Gelman, S., & Byrnes, J.
(Eds.), Perspectives Thought Language: Interrelations Development, pp.
225277. Cambridge University Press.
Golub, G. H., & Van Loan, C. F. (1996). Matrix Computations (Third edition). Johns
Hopkins University Press, Baltimore, MD.
Grefenstette, E., & Sadrzadeh, M. (2011). Experimenting transitive verbs DisCoCat. Proceedings GEMS 2011 Workshop GEometrical Models Natural
Language Semantics.
Grice, H. P. (1989). Studies Way Words. Harvard University Press, Cambridge,
MA.
Guevara, E. (2010). regression model adjective-noun compositionality distributional
semantics. Proceedings 2010 Workshop GEometrical Models Natural
Language Semantics (GEMS 2010), pp. 3337.
Harris, Z. (1954). Distributional structure. Word, 10 (23), 146162.
Hearst, M. (1992). Automatic acquisition hyponyms large text corpora. Proceedings 14th Conference Computational Linguistics (COLING-92), pp. 539545.
Herdagdelen, A., & Baroni, M. (2009). Bagpack: general framework represent semantic
relations. Proceedings EACL 2009 Geometrical Models Natural Language
Semantics (GEMS) Workshop, pp. 3340.
581

fiTurney

Hirst, G., & St-Onge, D. (1998). Lexical chains representations context detection
correction malapropisms. Fellbaum, C. (Ed.), WordNet: Electronic
Lexical Database, pp. 305332. MIT Press.
Jiang, J. J., & Conrath, D. W. (1997). Semantic similarity based corpus statistics
lexical taxonomy. Proceedings International Conference Research
Computational Linguistics (ROCLING X), pp. 1933, Tapei, Taiwan.
Johannsen, A., Alonso, H. M., Rishj, C., & Sgaard, A. (2011). Shared task system
description: Frustratingly hard compositionality prediction. Proceedings
Workshop Distributional Semantics Compositionality (DiSCo 2011), pp. 29
32, Portland, Oregon.
Jones, M. N., & Mewhort, D. J. K. (2007). Representing word meaning order information composite holographic lexicon. Psychological review, 114, 137.
Jurgens, D. A., Mohammad, S. M., Turney, P. D., & Holyoak, K. J. (2012). SemEval-2012
Task 2: Measuring degrees relational similarity. Proceedings First Joint
Conference Lexical Computational Semantics (*SEM), pp. 356364, Montreal,
Canada.
Kintsch, W. (2000). Metaphor comprehension: computational theory. Psychonomic Bulletin & Review, 7 (2), 257266.
Kintsch, W. (2001). Predication. Cognitive Science, 25 (2), 173202.
Kolda, T., & Bader, B. (2009). Tensor decompositions applications. SIAM Review,
51 (3), 455500.
Landauer, T. K. (2002). computational basis learning cognition: Arguments
LSA. Ross, B. H. (Ed.), Psychology Learning Motivation: Advances
Research Theory, Vol. 41, pp. 4384. Academic Press.
Landauer, T. K., & Dumais, S. T. (1997). solution Platos problem: latent semantic analysis theory acquisition, induction, representation knowledge.
Psychological Review, 104 (2), 211240.
Landauer, T. K., McNamara, D. S., Dennis, S., & Kintsch, W. (2007). Handbook Latent
Semantic Analysis. Lawrence Erlbaum, Mahwah, NJ.
Leacock, C., & Chodrow, M. (1998). Combining local context WordNet similarity
word sense identification. Fellbaum, C. (Ed.), WordNet: Electronic Lexical
Database. MIT Press.
Lee, D. D., & Seung, H. S. (1999). Learning parts objects nonnegative matrix
factorization. Nature, 401, 788791.
Lepage, Y., & Shin-ichi, A. (1996). Saussurian analogy: theoretical account
application. Proceedings 16th International Conference Computational
Linguistics (COLING 1996), pp. 717722.
Lin, H., & Bilmes, J. (2011). class submodular functions document summarization.
49th Annual Meeting Association Computational Linguistics: Human
Language Technologies (ACL-HLT), pp. 510520.
582

fiDomain Function: Dual-Space Model

Mangalath, P., Quesada, J., & Kintsch, W. (2004). Analogy-making predication using
relational information LSA vectors. Proceedings 26th Annual Meeting
Cognitive Science Society, p. 1623, Austin, TX.
McRae, K., Khalkhali, S., & Hare, M. (2011). Semantic associative relations adolescents young adults: Examining tenuous dichotomy. Reyna, V., Chapman,
S., Dougherty, M., & Confrey, J. (Eds.), Adolescent Brain: Learning, Reasoning,
Decision Making, pp. 3966. APA, Washington, DC.
Mitchell, J., & Lapata, M. (2008). Vector-based models semantic composition. Proceedings ACL-08: HLT, pp. 236244, Columbus, Ohio. Association Computational
Linguistics.
Mitchell, J., & Lapata, M. (2010). Composition distributional models semantics.
Cognitive Science, 34 (8), 13881429.
Moschitti, A., & Quarteroni, S. (2008). Kernels linguistic structures answer extraction. Proceedings 46th Annual Meeting Association Computational
Linguistics Human Language Technologies: Short Papers, p. 113116, Columbus,
OH.
Nakov, P., & Hearst, M. (2006). Using verbs characterize noun-noun relations. Proceedings 12th International Conference Artificial Intelligence: Methodology,
Systems, Applications (AIMSA 2006), pp. 233244, Varna, Bulgaria.
Nakov, P., & Hearst, M. (2007). UCB: System description SemEval Task 4. Proceedings Fourth International Workshop Semantic Evaluations (SemEval 2007),
pp. 366369, Prague, Czech Republic.
Nastase, V., Sayyad-Shirabad, J., Sokolova, M., & Szpakowicz, S. (2006). Learning nounmodifier semantic relations corpus-based WordNet-based features. Proceedings 21st National Conference Artificial Intelligence (AAAI-06), pp.
781786.
Nastase, V., & Szpakowicz, S. (2003). Exploring noun-modifier semantic relations.
Proceedings Fifth International Workshop Computational Semantics (IWCS5), pp. 285301, Tilburg, Netherlands.
Niwa, Y., & Nitta, Y. (1994). Co-occurrence vectors corpora vs. distance vectors
dictionaries. Proceedings 15th International Conference Computational
Linguistics, pp. 304309, Kyoto, Japan.
Seaghdha, D., & Copestake, A. (2009). Using lexical relational similarity classify
semantic relations. Proceedings 12th Conference European Chapter
Association Computational Linguistics (EACL-09), Athens, Greece.
Pantel, P., & Lin, D. (2002). Discovering word senses text. Proceedings Eighth
ACM SIGKDD International Conference Knowledge Discovery Data Mining,
pp. 613619, Edmonton, Canada.
Plate, T. (1995). Holographic reduced representations. IEEE Transactions Neural Networks, 6 (3), 623641.
Pustejovsky, J. (1991). generative lexicon. Computational Linguistics, 17 (4), 409441.
583

fiTurney

Rapp, R. (2003). Word sense discovery based sense descriptor dissimilarity. Proceedings Ninth Machine Translation Summit, pp. 315322.
Resnik, P. (1995). Using information content evaluate semantic similarity taxonomy.
Proceedings 14th International Joint Conference Artificial Intelligence
(IJCAI-95), pp. 448453, San Mateo, CA. Morgan Kaufmann.
Rosario, B., & Hearst, M. (2001). Classifying semantic relations noun-compounds
via domain-specific lexical hierarchy. Proceedings 2001 Conference
Empirical Methods Natural Language Processing (EMNLP-01), pp. 8290.
Rosario, B., Hearst, M., & Fillmore, C. (2002). descent hierarchy, selection
relational semantics. Proceedings 40th Annual Meeting Association
Computational Linguistics (ACL-02), pp. 247254.
Sahlgren, M. (2006). Word-Space Model: Using distributional analysis represent syntagmatic paradigmatic relations words high-dimensional vector spaces.
Ph.D. thesis, Department Linguistics, Stockholm University.
Santorini, B. (1990). Part-of-speech tagging guidelines Penn Treebank Project. Tech.
rep., Department Computer Information Science, University Pennsylvania.
(3rd revision, 2nd printing).
Schutze, H. (1998). Automatic word sense discrimination. Computational Linguistics, 24 (1),
97124.
Smolensky, P. (1990). Tensor product variable binding representation symbolic
structures connectionist systems. Artificial Intelligence, 159216.
Socher, R., Huang, E. H., Pennington, J., Ng, A. Y., & Manning, C. D. (2011). Dynamic
pooling unfolding recursive autoencoders paraphrase detection. Advances
Neural Information Processing Systems (NIPS 2011), pp. 801809.
Socher, R., Manning, C. D., & Ng, A. Y. (2010). Learning continuous phrase representations
syntactic parsing recursive neural networks. Proceedings NIPS-2010
Deep Learning Unsupervised Feature Learning Workshop.
Thater, S., Furstenau, H., & Pinkal, M. (2010). Contextualizing semantic representations
using syntactically enriched vector models. Proceedings 48th Annual Meeting
Association Computational Linguistics, pp. 948957.
Turney, P. D. (2001). Mining Web synonyms: PMI-IR versus LSA TOEFL.
Proceedings Twelfth European Conference Machine Learning (ECML-01),
pp. 491502, Freiburg, Germany.
Turney, P. D. (2006a). Expressing implicit semantic relations without supervision.
Proceedings 21st International Conference Computational Linguistics
44th Annual Meeting Association Computational Linguistics (Coling/ACL06), pp. 313320, Sydney, Australia.
Turney, P. D. (2006b). Similarity semantic relations. Computational Linguistics, 32 (3),
379416.
Turney, P. D. (2008a). latent relation mapping engine: Algorithm experiments.
Journal Artificial Intelligence Research, 33, 615655.
584

fiDomain Function: Dual-Space Model

Turney, P. D. (2008b). uniform approach analogies, synonyms, antonyms, associations. Proceedings 22nd International Conference Computational
Linguistics (Coling 2008), pp. 905912, Manchester, UK.
Turney, P. D., & Littman, M. L. (2005). Corpus-based learning analogies semantic
relations. Machine Learning, 60 (13), 251278.
Turney, P. D., Littman, M. L., Bigham, J., & Shnayder, V. (2003). Combining independent
modules solve multiple-choice synonym analogy problems. Proceedings
International Conference Recent Advances Natural Language Processing
(RANLP-03), pp. 482489, Borovets, Bulgaria.
Turney, P. D., & Pantel, P. (2010). frequency meaning: Vector space models
semantics. Journal Artificial Intelligence Research, 37, 141188.
Utsumi, A. (2009). Computational semantics noun compounds semantic space model.
Proceedings 21st International Joint Conference Artificial Intelligence
(IJCAI-09), pp. 15681573.
Veale, T. (2004). WordNet sits SAT: knowledge-based approach lexical analogy.
Proceedings 16th European Conference Artificial Intelligence (ECAI 2004),
pp. 606612, Valencia, Spain.
Widdows, D. (2008). Semantic vector products: initial investigations. Proceedings
2nd Symposium Quantum Interaction, Oxford, UK.

585


