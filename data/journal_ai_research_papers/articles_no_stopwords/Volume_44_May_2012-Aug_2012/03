journal artificial intelligence

submitted published

domain function dual space model
semantic relations compositions
peter turney

peter turney nrc cnrc gc ca

national council canada
ottawa ontario canada k r

abstract
given appropriate representations semantic relations carpenter wood
mason stone example vectors vector space model suitable
able recognize relations highly similar carpenter
wood mason stone relations analogous likewise representations
dog house kennel able recognize semantic
composition dog house dog house highly similar kennel dog house kennel
synonymous seems two tasks recognizing relations compositions
closely connected however best relations significantly
different best compositions introduce dual space
model unifies two tasks model matches performance best
previous relations compositions dual space model consists space
measuring domain similarity space measuring function similarity carpenter
wood share domain domain carpentry mason stone share
domain domain masonry carpenter mason share function
function artisans wood stone share function function materials
composition dog house kennel domain overlap dog house
domains pets buildings function kennel similar function
house function shelters combining domain function similarities
ways model relations compositions aspects semantics

introduction
distributional hypothesis words occur similar contexts tend similar
meanings harris firth many vector space vsms semantics use
wordcontext matrix represent distribution words contexts capturing
intuition behind distributional hypothesis turney pantel vsms achieved
impressive level individual words rapp clear
extend level phrases sentences beyond example know
represent dog house vectors represent dog house
one representing dog house treat unit way handle
individual words call holistic noncompositional representing
phrases holistic may suitable phrases scale
vocabulary n individual words n two word phrases n threeword phrases even large corpus text possible
phrases never appear corpus people continually inventing phrases
able understand phrases although never heard
able infer meaning phrase composition meanings
c

national council canada reprinted permission

fiturney

component words scaling could viewed issue data sparsity
better think linguistic creativity chomsky fodor
lepore master natural language must able represent phrases
composing representations individual words cannot treat n grams n
way treat unigrams individual words hand holistic ideal
idiomatic expressions e g kick bucket meaning cannot inferred
component words
creativity novelty natural language require us take compositional majority n grams encounter suppose vector representations dog house compose representations represent dog
house one strategy represent dog house average vectors dog
house landauer dumais simple proposal actually works limited degree
mitchell lapata however boat house house boat would represented
average vector yet different meanings composition averaging
deal order sensitivity phrase meaning landauer estimates
meaning english text comes word choice remaining comes
word order
similar issues arise representation semantic relations given vectors
carpenter wood represent semantic relations carpenter
wood treat carpenter wood unit search paraphrases relations
carpenter wood turney b large corpus could phrases
carpenter cut wood carpenter used wood wood carpenter
variation holistic enable us recognize semantic relations
carpenter wood highly similar relations mason stone
however holistic semantic relations suffers data sparsity
linguistic creativity holistic semantic composition
could represent relation carpenter wood averaging vectors
might enable us recognize carpenter wood mason stone
would incorrectly suggest carpenter wood stone mason
order sensitivity arises semantic relations arose semantic composition
many ideas proposed composing vectors landauer dumais
kintsch mitchell lapata erk pado point two
common several proposals first often adaptive
capacity represent variety possible syntactic relations phrase example
phrase horse draws horse subject verb draws whereas object
verb phrase draws horse composition vectors horse draws
must able adapt variety syntactic contexts order properly model
given phrases second single vector weak handle long phrase sentence
document single vector encode fixed amount structural information
dimensionality fixed upper limit sentence length hence
amount structure encoded erk pado p fixed dimensionality
allow information scalability
simple unweighted averaging vectors lacks adaptive capacity treats
kinds composition way flexibility represent different
modes composition good model must capacity adapt different situations


fidomain function dual space model

example weighted averaging weights tuned different syntactic
contexts mitchell lapata
information scalability means size semantic representations grow
proportion amount information representing size
representation fixed eventually information loss hand
size representations grow exponentially
one case information scalability arises approaches
map multiple vectors single vector example represent dog house adding
vectors dog house mapping two vectors one may information
loss increase number vectors mapped single vector
eventually reach point single vector longer contain information
multiple vectors avoided try map multiple vectors
single vector
suppose k dimensional vector floating point elements b bits
vector hold kb bits information even allow b grow k fixed
eventually information loss vector space model semantics vectors
resistance noise perturb vector noise threshold
significant change meaning represents therefore think
vector hypersphere radius rather point may put
bounds r r range values elements vector finite
number n hyperspheres radius packed bounded k dimensional
space conway sloane according information theory finite set
n messages need log n bits encode message likewise
finite set n vectors vector represents log n bits information
therefore information capacity single vector bounded k dimensional space
limited log n bits
past work suggests recognizing relations compositions closely connected
tasks kintsch mangalath quesada kintsch goal
unified model handle compositions relations resolving
issues linguistic creativity order sensitivity adaptive capacity information scalability
considerations led us dual space model consisting domain space
measuring domain similarity e topic subject field similarity function space
measuring function similarity e role relationship usage similarity
analogy b c b c example traffic street water
riverbed b relatively high domain similarity traffic street come
domain transportation c relatively high domain similarity water
riverbed come domain hydrology hand c relatively
high function similarity traffic water similar roles respective domains
things flow b relatively high function similarity street
riverbed similar roles respective domains things carry
things flow combining domain function similarity appropriate ways
vectors normalized unit length e g use cosine measure
similarity elements must lie within range element outside range
length vector greater one general floating point representations minimum
maximum values



fiturney

recognize semantic relations traffic street analogous
relations water riverbed
semantic composition appropriate way combine similarities may depend
syntax composition lets focus noun modifier composition example
noun modifier phrase ab instance brain doctor head noun b doctor
modified adjective noun brain suppose word c neurologist
synonymous ab functional role noun modifier phrase ab determined
head noun b brain doctor kind doctor b relatively high degree
function similarity c doctor neurologist function doctors
b high degree domain similarity c brain doctor neurologist come
domain clinical neurology combining domain function similarity
recognize brain doctor synonymous neurologist
briefly proposal compose similarity measures instead composing vectors
apply mathematical functions combine cosine similarity measures
instead applying functions directly vectors addresses information
loss preserve vectors individual component words
map multiple vectors single vector since two different spaces
flexibility address adaptive capacity model compositional
resolves linguistic creativity deal order sensitivity combining
similarity measures ways recognize effects word order
might argued present model semantic composition
way compare words form two phrases order derive measure similarity
phrases example section derive measure similarity phrases
environment secretary defence minister actually provide representation
phrase environment secretary hand past work
semantic composition reviewed section yields representation composite
phrase environment secretary different union representations
component words environment secretary
argument assumption goal semantic composition
create single general purpose stand alone representation phrase composite
distinct union representations component words assumption
necessary use assumption believe
assumption held back progress semantic composition
argue present model semantic composition composition similarities composition vectors vectors represent individual words
similarities inherently represent relations two things composing vectors
yield stand alone representation phrase composing similarities necessarily
yields linking structure connects phrase phrases similarity composition
stand alone representation phrase practical applications
require stand alone representations whatever practical tasks performed
stand alone representations phrases believe performed equally well better
similarity composition discuss issue depth section
two similarity spaces give us options similarity composition one space two types
characters give us options generating strings one type character alone



fidomain function dual space model

next section surveys related work modeling semantic composition
semantic relations section describes build domain function space test
hypothesis value two separate spaces create mono space
merger domain function spaces present four sets experiments dual space model section evaluate dual space
multiple choice analogy questions sat turney b multiple choice nounmodifier composition questions derived wordnet fellbaum phrase similarity rating mitchell lapata similarity versus association
chiarello burgess richards pollock discuss experimental
section section considers theoretical questions dual space model limitations model examined section section concludes
assumes familiarity vector space semantics
overview semantic vsms see papers handbook latent semantic analysis
landauer mcnamara dennis kintsch review mitchell lapatas
survey turney pantel

related work
examine related work semantic composition relations introduction mentioned four semantic yield four desiderata
semantic model
linguistic creativity model able handle phrases case
semantic composition word pairs case semantic relations
never seen familiar component words
order sensitivity model sensitive order words
phrase composition word pair relations order affects
meaning
adaptive capacity phrases model flexibility represent
different kinds syntactic relations word pairs model
flexibility handle variety tasks measuring degree relational
similarity two pairs see section versus measuring degree phrasal
similarity two pairs see section
information scalability phrases model scale neither loss
information exponential growth representation size number component
words phrases increases n ary semantic relations turney
model scale neither loss information exponential growth
representation size n number terms relations increases
review past work light four considerations
semantic composition
let ab phrase noun modifier phrase assume vectors
b represent component words b one earliest proposals semantic
composition represent ab vector c average b landauer


fiturney

dumais cosine measure vector similarity taking average
set vectors centroid adding vectors c b vector addition
works relatively well practice mitchell lapata although lacks order
sensitivity adaptive capacity information scalability regarding order sensitivity
adaptive capacity mitchell lapata suggest weights c b
tuning weights different values different syntactic relations experiments
mitchell lapata weighted addition performed better unweighted addition
kintsch proposes variation additive composition
c sum
p
b selected neighbours ni b c b ni neighbours vectors
words given vocabulary e rows given wordcontext matrix
neighbours chosen manner attempts address order sensitivity adaptive
capacity still information scalability due fixed dimensionality
utsumi presents similar model different way selecting neighbours
mitchell lapata found simple additive model peformed better
additive model included neighbours
mitchell lapata suggest element wise multiplication composition
operation c b ci ai bi vector addition element wise multiplication suffers lack order sensitivity adaptive capacity information scalability
nonetheless experimental evaluation seven compositional two noncompositional element wise multiplication best performance mitchell
lapata
another use tensor product composition smolensky aerts
czachor clark pulman widdows outer product
c b outer product two vectors b n elements n n
matrix c outer product three vectors n n n third order tensor
information scalability representations grow exponentially large
phrases grow longer furthermore outer product perform well
element wise multiplication mitchell lapatas experiments recent work
tensor products clark coecke sadrzadeh grefenstette sadrzadeh
attempted address issue information scalability
circular convolution similar outer product outer product matrix
compressed back vector c b plate jones mewhort
avoids information explosion information loss circular convolution
performed poorly mitchell lapatas experiments
baroni zamparelli guevara suggest another model composition
adjective noun phrases core strategy share use holistic vectors
train compositional model partial least squares regression plsr learn
linear model maps vectors component nouns adjectives linear
approximations holistic vectors phrases linguistic creativity
avoided linear model needs holistic vectors training
need holistic vectors plausible adjective noun phrases given phrase
training data linear model predicts holistic vector phrase given
ways avoid exponential growth example third order tensor rank
three modes may compactly encoded three component vectors kolda bader
discuss compact tensor representations



fidomain function dual space model

component vectors adjective noun works well adjective noun
phrases clear generalize parts speech longer phrases
one application semantic composition measuring similarity phrases erk
pado mitchell lapata kernel methods applied closely
related task identifying paraphrases moschitti quarteroni emphasis
kernel methods syntactic similarity rather semantic similarity
neural network combined vector space task
language modeling bengio ducharme vincent jauvin socher manning ng
socher huang pennington ng manning impressive goal
language model estimate probability phrase decide several
phrases likely vsms improve probability estimates language model
measuring similarity words phrases smoothing probabilities
groups similar words however language model words considered similar
degree exchanged without altering probability given phrase
without regard whether exchange alters meaning phrase
function similarity measures degree words similar functional roles
language missing anything domain similarity
erk pado present model similar two parts
vector space measuring similarity model selectional preferences vector
space similar domain space model selectional preferences plays role
similar function space individual word represented triple ha r r
consisting words vector selectional preferences r inverse selectional
preferences r phrase ab represented pair triples ha b triple
modified form triple represents individual word modifications
adjust representation model meaning altered relation b
phrase ab likewise triple b modified form triple b represents b
b takes account affects b
transformed represent influence b meaning
vector transformed vector let rb vector represents
typical words consistent selectional preferences b vector
composition rb erk pado use element wise multiplication
composition rb intention make typical vector x would
expected phrase xb likewise b b b b ra
erk pados model related thater furstenau pinkal
address linguistic creativity order sensitivity adaptive capacity information scalability
suitable measuring similarity semantic relations consider
analogy traffic street water riverbed let ha b represent traffic street
let hc represent water riverbed transformation b c b
c reinforces connection traffic street water
riverbed help us recognize relational similarity traffic street
water riverbed course designed relational similarity
surprising however goal unified model handle
compositions relations


fiturney

semantic relations
semantic relations make general observations order sensitivity let
b c two word pairs let simr b c measure degree
similarity relations b c b c good analogy
simr b c relatively high value general good model relational
similarity respect following equalities inequalities

simr b c simr b c



simr b c simr c b



simr b c simr b c



simr b c simr c b



example given carpenter wood mason stone make good analogy follows
equation wood carpenter stone mason make equally good analogy
according equation mason stone carpenter wood make good analogy
hand suggested equation carpenter wood analogous stone mason
likewise indicated equation poor analogy assert carpenter stone
mason wood
rosario hearst present classifying word pairs according
semantic relations use lexical hierarchy map word pairs feature
vectors classification scheme implicitly tell us something similarity two word
pairs semantic relation class implicitly relationally similar
two word pairs different classes consider relational similarity
implied rosario hearsts see order
sensitivity equation violated
let simh x measure degree hierarchical similarity
words x simh x relatively high x share common hypernym
relatively close given lexical hierarchy essence intuition behind
rosario hearsts simh c simh b high
simr b c high simh c simh b high enough
b c assigned relation class
example consider analogy mason stone carpenter wood common hypernym mason carpenter artisan see simh mason carpenter
high common hypernym stone wood material hence simh stone wood
high seems good analogy indeed characterized high values simh c
simh b however symmetry simh x leads simh b high
simh b must high implies simr c b high
incorrectly conclude mason wood carpenter stone see equation
later work classifying semantic relations used different
underlying intuition hierarchical similarity rosario hearst fillmore
nastase szpakowicz nastase sayyad shirabad sokolova szpakowicz
use similar intuition since similarity function space closely related


fidomain function dual space model

hierarchical similarity simh x see later section however including
domain space relational similarity measure saves us violating equation
let simf x function similarity measured cosine vectors x
function space let simd x domain similarity measured cosine
vectors x domain space past researchers rosario hearst rosario
et al nastase szpakowicz veale nastase et al look
high values simf c simf b indicators simr b c high
look high values simd b simd c continuing previous example
conclude mason wood carpenter stone wood
belong domain masonry stone belong domain carpentry
let determiner e g hearst showed patterns form
x bird crow kind x crow kind
bird used infer x hypernym bird hypernym crow
pairpattern matrix vsm rows word pairs columns
x patterns turney littman bigham shnayder demonstrated
pairpattern vsm used measure relational similarity suppose
pair pattern matrix x word pair b corresponds row vector xi c
corresponds xj measure relational similarity simr b c
cosine xi xj
first patterns pairpattern matrices generated hand turney
et al turney littman later work turney b used automatically
generated patterns authors used variations technique nakov hearst
davidov rappoport bollegala matsuo ishizuka seaghdha
copestake suffer linguistic creativity
noncompositional holistic cannot scale handle
huge number possible pairs even largest corpus cannot contain pairs
human speaker might use daily conversation
turney b attempted handle linguistic creativity within holistic
model synonyms example corpus contain traffic street within
certain window text perhaps might contain traffic road contain
water riverbed perhaps water channel however best partial
solution turneys b required nine days process multiple choice sat
analogy questions dual space model without specifying advance word
pairs might face answer questions seconds see section
compositional scale better holistic
mangalath et al presented model semantic relations represents word
pairs vectors ten abstract relational categories hyponymy meronymy taxonomy degree construct kind second order vector space
elements vectors degrees similarity calculated cosines
first order wordcontext matrix
instance carpenter wood represented second order vector composed
ten cosines calculated first order vectors second order vector value
element corresponding say meronymy would cosine two first order vectors x
vector x would sum first order vectors carpenter wood
vector would sum several vectors words related meronymy


fiturney

part whole component portion contains constituent segment cosine
x would indicate degree carpenter wood related meronymy
mangalath et al model suffers information scalability order sensitivity
information loss takes place first order vectors summed
high dimensional first order space reduced ten dimensional second order
space order sensitivity second order vectors violate equation
pairs c c represented second order vector
natural proposal represent word pair b way would represent
phrase ab whatever compositional model phrases could
applied word pairs however compositional model order
sensitivity information scalability carry word pairs example represent
b c b c b violate equation b b
b b

three vector spaces
section describe three vector space three spaces consist word
context matrices rows correspond words columns correspond
contexts words occur differences among three spaces kinds
contexts domain space uses nouns context function space uses verb patterns
context mono space merger domain function contexts mono space
created order test hypothesis useful separate domain
function spaces mono space serves baseline
constructing wordcontext matrices
building three spaces involves series steps three main steps
substeps first last steps three spaces
differences spaces differences second step
terms contexts input corpus lexicon output terms contexts
extract terms lexicon frequencies corpus
select terms given frequency candidate rows frequency
matrix
selected term phrases corpus contain term within
given window size
use tokenizer split phrases tokens
use part speech tagger tag tokens phrases
build termcontext frequency matrix input terms contexts output
sparse frequency matrix
convert tagged phrases contextual patterns candidate columns
contextual pattern count number terms candidate rows
generated pattern rank patterns descending order counts
select top nc contextual patterns columns matrix


fidomain function dual space model

initial set rows step drop row match
top nc contextual patterns yielding final set nr rows
row term column contextual pattern count number
phrases step containing given term matching given
pattern output resulting numbers sparse frequency matrix
weight elements smooth matrix input sparse frequency matrix
output singular value decomposition svd weighted matrix
convert raw frequencies positive pointwise mutual information ppmi
values
apply svd ppmi matrix output svd component matrices
input corpus step collection web gathered university websites
webcrawler corpus contains approximately words comes
gigabytes plain text facilitate finding term frequencies sample phrases
indexed corpus wumpus search engine buttcher clarke rows
matrices selected terms words phrases wordnet lexicon
found selecting terms wordnet resulted subjectively higher quality
simply selecting terms high corpus frequencies
step extract unique words phrases n grams index sense file
wordnet skipping n grams contain numbers letters hyphens spaces
allowed n grams n gram corpus frequencies querying wumpus
n gram n grams frequency least least characters
candidate rows step selected n gram query wumpus
maximum phrases step phrases limited window words
left n gram words right total window size n words
use opennlp tokenize part speech tag phrases steps
tagged phrases come gigabytes
step generate contextual patterns part speech tagged phrases
different kinds patterns created three different kinds spaces details
step given following subsections phrase may yield several patterns
three spaces rows maximum phrases
per row several patterns per phrase millions distinct patterns
filter patterns steps select top nc patterns shared
largest number rows given large number patterns may fit
ram work limited ram use linux sort command designed
efficiently sort files large fit ram row make file
distinct patterns generated row concatenate files





corpus collected charles clarke university waterloo
wumpus available http www wumpus search org
wordnet available http wordnet princeton edu
limit phrases per n gram required make wumpus run tolerable amount time
finding phrases time consuming step construction spaces use solid state
drive ssd speed step
opennlp available http incubator apache org opennlp
tagged phrases available author request



fiturney

rows alphabetically sort patterns concatenated file sorted file
identical patterns adjacent makes easy count number occurrences
pattern counting second sort operation yields ranked list patterns
select top nc
possible candidate rows step might match
patterns step rows would zeros matrix remove
step finally output sparse frequency matrix f nr rows nc
columns th row corresponds n gram wi j th column corresponds
contextual pattern cj value element fij f number phrases
containing wi step generate pattern cj step step use
svdlibc calculate singular value decomposition format output
sparse matrix step chosen meet requirements svdlibc
step apply positive pointwise mutual information ppmi sparse frequency matrix f variation pointwise mutual information pmi church
hanks turney pmi values less zero replaced
zero niwa nitta bullinaria levy let x matrix
ppmi applied f matrix x number rows columns
raw frequency matrix f value element xij x defined follows
fij
pij pnr pnc

j fij





pnc

j fij
pi pnr pnc



pnr
f
pncij
pnr





pj





j fij
j fij

pij
pi pj



pmiij log

pmiij pmiij
xij
otherwise




definition pij estimated probability word wi occurs context
cj pi estimated probability word wi pj estimated probability
context cj wi cj statistically independent pij pi pj definition
independence thus pmiij zero since log product pi pj
would expect pij wi occurs cj pure random chance hand
interesting semantic relation wi cj expect pij larger
would wi cj indepedent hence pij pi pj
thus pmiij positive word wi unrelated incompatible context cj
may pmiij negative ppmi designed give high value xij
interesting semantic relation wi cj otherwise xij value
zero indicating occurrence wi cj uninformative
svdlibc available http tedlab mit edu dr svdlibc



fidomain function dual space model

finally step apply svdlibc x svd decomposes x product
three matrices uvt u v column orthonormal form e columns
orthogonal unit length ut u vt v diagonal matrix
singular values golub van loan x rank r rank r let
k k r diagonal matrix formed top k singular values let uk
vk matrices produced selecting corresponding columns u v
matrix uk k vkt matrix rank k best approximates original matrix x
sense minimizes approximation errors x uk k vkt minimizes
kx xkf matrices x rank k k kf denotes frobenius norm golub
van loan final output three matrices uk k vk form
truncated svd x uk k vkt
domain space
intuition behind domain space domain topic word characterized
nouns occur near use relatively wide window ignore syntactic
context nouns appear
domain space step tagged phrase generates two contextual
patterns contextual patterns simply first noun left given n gram
one first noun right one since window size
words side n gram usually nouns sides n gram
nouns may common nouns proper nouns opennlp uses penn treebank
tags santorini include several different categories noun tags
noun tags begin capital n simply extract first words left right
n gram tags begin n extracted nouns converted lower
case noun appears sides n gram one contextual pattern
generated extracted patterns unigrams noun compound
component noun closest n gram extracted
table shows examples n gram boat note window words
count punctuation number tokens window may greater
number words window see table row vector
n gram boat frequency matrix f nonzero values example
columns lake summer assuming contextual patterns make
filtering step
step set nc step drop rows zero
left nr equal ppmi sets negative elements zero
nonzero values matrix density table shows
contextual patterns first five columns last five columns columns
order ranks step count column table gives number rows
n grams generate pattern counts mentioned step
last patterns begin c counts ties broken
alphabetical order


fiturney

tagged phrases
would md visit vb big nnp lake nnp cc take vb prp
boat nn dt huge jj beautiful jj lake nn ex
vbd

patterns
lake



dt large jj paved jj parking nn lot nn dt boat nn
ramp nn area nn cc walk vb south rb along dt

lot
ramp



building vbg permit nn anyway rb prp md
vb dt boat nn next jj summer nn skiing nn
cc tubing nn paraphernalia nns

permit
summer



table examples step domain space n gram boat three tagged
phrases generate five contextual patterns

column






pattern
time
part
years
way
name

count






column






pattern
clu
co conspirator
conciseness
condyle
conocer

count






table contextual patterns first last columns domain space clu
abbreviation chartered life underwriter terms condyle round
bump bone forms joint another bone conocer
spanish verb know sense acquainted person

function space
concept function space function role word characterized
syntactic context relates verbs occur near use narrow
window function space domain space intuition proximity
verb important determining functional role given word distant verb
less likely characterize function word generate relatively complex patterns
function space try capture syntactic patterns connect given word
nearby verbs
step tagged phrase generates six contextual patterns given
tagged phrase first step cut window tokens given n gram
tokens remaining tokens left n gram punctuation punctuation everything left punctuation removed
remaining tokens right n gram punctuation punctuation everything right punctuation removed lets call remaining tagged phrase
truncated tagged phrase
next replace given n gram truncated tagged phrase generic marker


fidomain function dual space model

x simplify part speech tags reducing first character
santorini example verb tags vb vbd vbg vbn vbp
vbz reduced v truncated tagged phrase contains v tag generates
zero contextual patterns phrase contains v tag generate two types
contextual patterns general patterns specific patterns
general patterns verbs every token v tag tags removed
naked verbs tokens reduced naked tags tags without words
specific patterns verbs modals tokens tags prepositions tokens tags
tokens tags tags removed tokens reduced naked
tags see table examples
general specific patterns left x trim leading naked tags
right x trim trailing naked tags tag replace
remaining naked tags sequence n tags n n n n n likely
compound noun reduce sequence single n
given truncated tagged phrase two patterns one general pattern
one specific pattern patterns tokens left right
sides x make two patterns duplicating x splitting pattern
point two xs one patterns verb drop
thus may three specific patterns three general patterns
given truncated tagged phrase specific general patterns one
generated
table shows examples n gram boat note every pattern must contain
generic marker x least one verb
truncated tagged phrases
dt canals nns boat nn
cc wandering vbg dt

patterns
x c wandering
x c wandering

types
general
specific



dt charter nn fishing vbg boat nn
captain nn named vbn jim nnp

fishing x n named
fishing x
x n named

general
general
general



used vbn dt
cc lowered vbd

used x c lowered
used x
x c lowered
used x c lowered
used x
x c lowered

general
general
general
specific
specific
specific



boat nn

table examples step function space n gram boat three truncated
tagged phrases generate eleven contextual patterns

step set nc step rows zero dropped
nr ppmi nonzero values yielding matrix density
table shows contextual patterns first last five columns


fiturney

last patterns begin counts ties broken
alphabetical order
column






pattern
x
x n
x
x
x

count






column






pattern
since x n
sinking x
supplied x
supports x n
suppressed x

count






table contextual patterns first last columns function space
contextual patterns function space complex patterns
domain space motivation greater complexity observation mere
proximity enough determine functional roles although seems sufficient determining domains example consider verb gives word x occurs near
gives x could subject direct object indirect object verb determine
functional role x need know case applies syntactic context connects x gives provides information contextual pattern x gives implies
x subject gives x implies x object likely direct object gives x
suggests x indirect object modals prepositions supply information
functional role x context given verb verb gives appears
different contextual patterns e columns function space correspond
syntactic patterns contain gives
many row vectors function space matrix correspond verbs might
seem surprising characterize function verb syntactic relation
verbs consider example verb run row vector run
ppmi matrix function space nonzero values run characterized
different contextual patterns
note appearing contextual pattern different nonzero value
contextual pattern character string word run appears different
contextual patterns run x row vector word run nonzero
values contextual patterns columns x
mono space
mono space simply merger domain space function space step
take union domain space columns function space columns
resulting total nc columns step total nr
rows mono matrix ppmi nonzero values yielding density
values mono frequency matrix f equal corresponding values
domain function matrices rows mono space matrix
corresponding rows function space matrix rows corresponding values
zeros nonzero elements rows correspond values
domain matrix


fidomain function dual space model

summary spaces
table summarizes three matrices following four sets experiments use
three matrices domain function mono matrices cases
generate different matrices set experiments three four sets experiments
involve datasets used past researchers made special
effort ensure words three datasets corresponding rows three
matrices intention three matrices adequate handle
applications without special customization
space
domain
function
mono

rows nr




columns nc




nonzeros ppmi




density ppmi




table summary three spaces

spaces measure similarity
following experiments measure similarity two terms b cosine
angle corresponding row vectors b
sim b cos b


b

kak kbk



cosine angle two vectors inner product vectors
normalized unit length cosine ranges vectors point
opposite directions degrees point direction
degrees vectors orthogonal degrees cosine zero raw
frequency vectors necessarily cannot negative elements cosine cannot
negative weighting smoothing often introduce negative elements ppmi weighting
yield negative elements truncated svd generate negative elements even
input matrix negative values
semantic similarity two terms given cosine two corresponding rows
uk pk see section two parameters uk pk need set
parameter k controls number latent factors parameter p adjusts weights
factors raising corresponding singular values pk power p
parameter k well known literature landauer et al p less familiar
use p suggested caron following experiments section
explore range values p k
suppose take word w list words descending order
cosines w uk pk calculate cosines p high go list
cosines nearest neighbours w decrease slowly p low decrease
quickly high p broad fuzzy neighbourhood low p yields sharp
crisp neighbourhood parameter p controls sharpness similarity measure


fiturney

reduce running time svdlibc limit number singular values
usually less singular values example svd
domain space singular values long k greater
experiment range k values without rerunning svdlibc generate uk pk
u p simply deleting k columns smallest singular values
experiments vary k increments values k
vary p increments values p p
give weight factors smaller singular values p factors
larger singular values weight caron observes researchers use
p p use uk uk k
let simf b function similarity measured cosine vectors b
function space let simd b domain similarity measured cosine
vectors b domain space similarity measure combines simd b
simf b four parameters tune kd pd domain space kf pf
function space
one space feasible us explore combinations parameter
values two spaces combinations values make search
tractable initialize parameters middle ranges kf kd
pf pd alternate tuning simd b e kd pd holding
simf b e kf pf fixed tuning simf b holding simd b fixed stop
search improvement performance training data almost
cases local optimum found one pass tuned parameters
improvement try tune second time thus typically
evaluate parameter values tune one similarity tune
try first see improvement possible
could use standard numerical optimization tune four parameters use takes advantage background knowledge
optimization task know small variations parameters make small changes
performance need make fine grained search know
simd b simf b relatively independent optimize separately
rows matrices terms wordnet index sense file
file nouns singular forms verbs stem forms calculate
sim b first look exact matches b terms correspond
rows given matrix domain function mono exact match found
use corresponding row vector matrix otherwise look alternate forms
terms validforms function wordnet querydata perl interface
wordnet automatically converts plural nouns singular forms verbs
stem forms none alternate forms exact match row matrix
map term zero vector length k

use perl data language pdl searching parameters calculating cosines operations
vectors matrices see http pdl perl org
wordnet querydata available http search cpan org dist wordnet querydata



fidomain function dual space model

composing similarities
semantic relations compositions combine two similarities
simd b simf b ways depending task hand syntax
phrase hand general want combined similarity high
component similarities high want values component similarities
balanced achieve balance use geometric mean combine similarities instead
arithmetic mean geometric mean suitable negative numbers
cosine negative cases hence define geometric mean zero
component similarities negative

geo x x xn

x x xn n xi n
otherwise



element wise multiplication
one successful approaches composition far element wise multiplication c b ci ai bi mitchell lapata
makes sense elements vectors negative elements
b positive relatively large values ai bi reinforce resulting
large value ci makes intuitive sense ai bi highly negative
ci highly positive although intuition says ci highly negative mitchell
lapata designed wordcontext matrices ensure vectors
negative elements
values matrix uk pk typically half positive half negative
use element wise multiplication baseline following experiments
fair baseline cannot simply apply element wise multiplication row vectors uk pk
one solution would use ppmi matrix x negative elements
would allow element wise multiplication take advantage smoothing effect
svd solution use row vectors x uk k vkt although ppmi matrix
x sparse see table x uk pk density
let b vectors x correspond terms b row
vectors benefit smoothing due truncated svd elements almost
positive negative elements set zero let c b
apply element wise multiplication vectors multiply vk kp
resulting vector c c vk p
compared row vectors matrix
k
p
uk k
p

x vk p
k uk k vk vk k





uk k vkt vk kp
uk k p
k
uk pk






note since vk column orthonormal vkt vk equals ik k k identity matrix


fiturney

similarly row vector uk pk counterpart x multiplying

p
k vk

uk pk k p vkt uk pk p
k vk



uk k vkt



x



let nn x nn nonnegative function converts negative elements vector
x zero

nn hx xn hy yn

xi xi
yi
otherwise




version element wise multiplication may expressed follows
p
p

c nn p
k vk nn bk vk vk k



another way deal element wise multiplication would use nonnegative
matrix factorization nmf lee seung instead svd yet found
implementation nmf scales matrix sizes table
past experiments smaller matrices svd nmf similar performance

experiments varieties similarities
section presents four sets experiments first set experiments presents dualspace model semantic relations evaluates model multiple choice analogy
questions sat second set presents model semantic composition
evaluates multiple choice questions constructed wordnet third
set applies dual space model phrase similarity dataset mitchell lapata
final set uses three classes word pairs chiarello et al test
hypothesis dual space model domain space function space capture
intuitive concepts association similarity
similarity relations
evaluate dual space model applied task measuring similarity
semantic relations use set multiple choice analogy questions sat
college entrance exam turney b table gives example one questions
task select choice word pair analogous relationally similar
stem word pair
let b represent stem pair e g lull trust answer sat questions
selecting choice pair c maximizes relational similarity simr b c defined
follows


fidomain function dual space model

stem
choices

solution








lull trust
balk fortitude
betray loyalty
cajole compliance
hinder destination
soothe passion
cajole compliance

table example question sat analogy questions lulling person
trust analogous cajoling person compliance

sim b c geo simf c simf b



sim b c geo simd b simd c



sim b c geo simd simd c b

sim b c sim b c sim b c
simr b c
otherwise




intent sim measure function similarity across two pairs domain
similarity inside two pairs measured sim whereas domain similarity across
two pairs given sim relational similarity simr simply function similarity
sim subject constraint domain similarity inside pairs sim must
less domain similarity across pairs sim
figure conveys main ideas behind equations want high function
similarities indicated f c b measured sim prefer
relatively high domain similarities marked b c measured sim
contrast relatively low domain similarities c b given sim
example table see lulling person trust analogous
cajoling person compliance since functional role lull similar functional
role cajole involve manipulating person functional role trust similar
functional role compliance states person
captured sim constraint sim b c sim b c implies
domain similarities lull trust domain confidence loyalty cajole compliance
domain obedience conformity greater equal domain
similarities lull compliance cajole trust
analogy way mapping knowledge source domain target domain
gentner source domain mapped c target domain
play role source domain c plays target domain
theory behind sim b source domain c target
recently came across rectangular structure lepage shin ichis
morphological analogy see figure although task differ considerably
task lepage shin ichi independently discovered
underlying structure analogical reasoning



fiturney






b


f

c

f





simr b c
relational similarity

figure diagram reasoning behind equations f represents high
function similarity means high domain similarity indicates low
domain similarity

domain internal domain similarity b internal domain similarity
c less cross domain similarities motivates constraint
sim sim definition natural expression gentners theory analogy
recall four equations introduced section repeat equations
convenience

simr b c simr b c



simr b c simr c b



simr b c simr b c



simr b c simr c b



inspection definition relational similarity equation satisfies
requirements equations understood considering
figure equation tells us rotate figure vertical axis without
altering network similarities due symmetry figure equation tells
us rotate figure horizontal axis without altering network
similarities
hand cannot swap c holding b fixed
would change f links although would change links
words sim sim would changed although sim would affected
therefore equation satisfied
cannot swap b holding c fixed would change
links although would change f links words sim
sim would changed although sim would affected therefore equation


fidomain function dual space model

satisfied see sim would violate equation due symmetry
cosines simf b simf b constraint sim b c sim b c breaks
symmetry
another way break symmetry equation satisfied would use
similarity measure inherently asymmetric skew divergence equation
symmetry broken natural way considering domain function similarity
apply analogies need introduce inherently asymmetric measure
note symmetries equations desirable wish break
symmetries
would reasonable include simd c simd b sim decided
leave seems us function similarities simf c simf b
high values good analogy might cause simd c simd b
relatively high even though cross domains people observe certain kind
abstract function similarity frequently function similarity might become popular
topic discussion could high domain similarity
example carpenter wood analogous mason stone domain carpenter wood
carpentry domain mason stone masonry functional role carpenter
similar functional role mason artisans although carpenter
mason belong different domains high degree abstract function similarity
may discussions mention together discussions specialized trades skilled manual labour construction industry workplace injuries
words high function similarity two words may cause rise domain
similarity therefore include simd c simd b sim
five choices sat question relational similarity zero skip
question use ten fold cross validation set parameters sat questions
parameter values selected nine ten folds kd pd kf
pf parameters determined sat questions answered
seconds equation correctly answers questions skips questions
incorrectly answers questions achieving accuracy
comparison past work
comparison average score senior highschool students applying us universities
acl wiki lists many past sat questions table
shows top ten time writing table dual space refers dualspace model equation four past achieved accuracy
higher four used holistic approaches hence able address issue
linguistic creativity best previous attains accuracy correct
skipped incorrect turney b difference
statistically significant confidence level according fishers exact test
majority table unsupervised dual space pairclass
turney b bagpack herdagdelen baroni use limited supervision pairclass bagpack answer given sat question learning binary classification model
specific given question training set given question consists one
see http aclweb org aclwiki index php title sat analogy questions



fiturney


lsa predication
know best
k means
bagpack
vsm
dual space
bmi
pairclass
pert
lra
human

reference
mangalath et al
veale
bicici yuret
herdagdelen baroni
turney littman
bollegala et al
turney b
turney
turney b
average us college applicant

accuracy












confidence












table top ten sat questions acl wiki
confidence intervals calculated binomial exact test

positive training example stem pair question ten randomly selected pairs
assumed negative training examples induced binary classifier used assign
probabilities five choices probable choice guess dual space uses
training set tune four numerical parameters three best
described weakly supervised
sensitivity parameters
see sensitive dual space model values parameters perform
two exhaustive grid searches one coarse wide grid another fine narrow
grid point grids evaluate dual space model whole set
sat questions narrow grid search centred parameter values
selected nine ten folds previous experiment kd pd
kf pf searches evaluate values parameter yielding total
parameter settings table shows values explored two grid
searches table presents minimum maximum average standard deviation
accuracy two searches
grid
coarse

fine

parameter
kd
pd
kf
pf
kd
pd
kf
pf



















values
























table range parameter values two grid searches


fidomain function dual space model

grid
coarse
fine

minimum



accuracy
maximum average





standard deviation



table sensitivity dual space model parameter settings
accuracy attained heuristic search described section ten fold
cross validation table near best accuracy fine grid search
whole set sat questions table evidence heuristic search
effective accuracy coarse search varies demonstrates
importance tuning parameters hand accuracy fine search
spans narrower range lower standard deviation suggests dualspace model overly sensitive relatively small variations parameter values
parameters reasonably stable nine ten folds cross validation
select parameters evidence stability
parts speech
since domain space nouns function space verbs interesting
know performance dual space model varies different parts speech
answer manually labeled sat questions part speech labels
labels single pair ambiguous labels become unambiguous context
whole question example lull trust could noun verb context
table must verb noun
table splits parts speech none differences
table statistically significant confidence level according fishers
exact test larger varied set questions needed determine part
speech affects dual space model
parts speech
noun noun
noun adjective adjective noun
noun verb verb noun
adjective adjective
verb adjective adjective verb
verb verb
verb adverb adverb verb


right









accuracy









wrong









skipped









total









table performance dual space model parts speech

order sensitivity
seems function space work equation use sim alone
dropping constraint sim sim accuracy drops


fiturney

drop statistically significant hypothesize small drop due design
sat test primarily intended test students understanding functional
roles domains
verify hypothesis reformulated sat questions would test
function domain comprehension method first expand choice pair
c including stem pair b resulting full explicit analogy b c
expanded choice b c generate another choice c b table shows
reformulation table due symmetry sim must assign similarity
b c c b ten choice test evaluates function domain
similarities
choices

solution













lull trust balk fortitude
lull fortitude balk trust
lull loyalty betray trust
lull trust betray loyalty
lull compliance cajole trust
lull trust cajole compliance
lull destination hinder trust
lull trust hinder destination
lull trust soothe passion
lull passion soothe trust
lull trust cajole compliance

table expanded sat question designed test function domain comprehension choices similarity according sim

task expanded ten choice sat questions original
five choice questions select best analogy solution table
solution table except stem pair explicit table signficant
change five distractors added choices answer ten choice
questions selecting choice b c maximizes simr b c
ten choice reformulated sat test simr equation attains accuracy
whereas sim alone equation achieves difference statistically
significant confidence level according fishers exact test stringent
test supports claim function similarity insufficient
test value two separate spaces use single space
simd simf equation model still four parameters tune kd pd kf
pf matrix used similarities best accuracy
ten question reformulated sat test function space simd
simf significantly accuracy dual space model simd
domain space simf function space confidence level fishers
exact test
table summarizes cases matrix simd used
model sim alone equation cases model
simr equation five choice ten choice sat questions original


fidomain function dual space model

dual space model accurate modified significant column
indicates whether accuracy modified model significantly less original
dual space model confidence level fishers exact test difficult ten choice
questions clearly value two distinct spaces

dual space
modified dual space
modified dual space
modified dual space
modified dual space
modified dual space
modified dual space
dual space
modified dual space
modified dual space
modified dual space
modified dual space
modified dual space
modified dual space

accuracy















significant

yes
yes

yes
yes
yes
yes
yes
yes
yes
yes

questions
five choice
five choice
five choice
five choice
five choice
five choice
five choice
ten choice
ten choice
ten choice
ten choice
ten choice
ten choice
ten choice

matrix simd
domain space
function space
mono space
domain space
used
used
used
domain space
function space
mono space
domain space
used
used
used

matrix simf
function space
function space
mono space
domain space
function space
mono space
domain space
function space
function space
mono space
domain space
function space
mono space
domain space

table accuracy original five choice questions reformulated ten choice
questions modified intentionally use wrong matrix
matrix simd simf modified accuracy decreases
one space used

summary
dual space model performs well current state art holistic model
addresses issue linguistic creativity reformulated sat questions
support claim value two separate spaces
mentioned section task classifying word pairs according
semantic relations rosario hearst rosario et al nastase szpakowicz
closely connected measuring relational similarity turney b
applied measure relational similarity relation classification cosine similarity
measure nearness nearest neighbour supervised learning dualspace model equation suitable relation classification nearest neighbour

similarity compositions
second set experiments apply dual space model noun modifier compositions given vectors dog house kennel would able recognize
dog house kennel synonymous compare dual space model holistic
vector addition element wise multiplication approaches evaluated


fiturney

multiple choice questions automatically generated wordnet
wordnet querydata perl interface wordnet table gives example one
noun modifier questions
stem
choices

solution










dog house
kennel
dog
house
canine
dwelling
effect
largeness
kennel

table example multiple choice noun modifier composition question
questions stem bigram choices unigrams choice
correct answer modifier head noun choice synonym
hypernym modifier synonym hypernym head noun
synonyms hypernyms found noun randomly chosen last two choices
randomly selected nouns choices nouns adjectives
choices must nouns
stem bigram choice unigrams must corresponding rows function
space space least number rows stem bigram must noun sense
wordnet may senses parts speech solution unigram
must member synset synonym set first noun sense stem bigram
frequent dominant sense bigram bigram used noun
cannot simply hyphenation dog house concatenation doghouse
stem bigram
requirements total seven choice questions randomly
split training parameter tuning testing questions deliberately designed difficult particular approaches strongly attracted
choices furthermore attempt ensure stem bigrams
compositional may idiomatic expressions compositional
could possibly get right want bias questions imposing theories
distinguishing compositions idioms construction
let ab represent noun modifier bigram dog house let c represent unigram
kennel answer multiple choice questions selecting unigram maximizes
compositional similarity simc ab c defined follows

sim ab c geo simd c simd b c simf b c

sim ab c c b c
simc ab c
otherwise
questions available online appendix http jair org






fidomain function dual space model

equations illustrated figure




b



f



c
simc ab c
noun modifier compositional similarity

figure diagram equations
thinking behind sim c kennel high domain similarity
modifier dog head noun b house furthermore function
bigram ab dog house determined head noun b house head noun
high function similarity c kennel add constraints c b c
sim tends high values sim ab sim ab b seems
plausible humans use constraints reason dog house cannot mean
thing house extra word dog dog house would serve purpose
would meaningless noise
constraints c b c could expressed terms similarities
simd c simd b c high threshold e g would
add another parameter model decided keep model relatively simple
seven choices noun modifier question compositional similarity
zero skip question training set best parameter settings kd
pd kf pf testing set equation correctly answers
questions skips questions incorrectly answers yielding accuracy
comparison approaches
mitchell lapata compared many different approaches semantic composition
experiments considered one task task examine section
chosen compare smaller number approaches larger
number tasks include element wise multiplication experiments
best performance mitchell lapatas experiments vector
spite constraints still worthwhile include head noun modifier distractors
multiple choice questions enables us experimentally evaluate impact
distractors constraints removed see table future users
dataset may way avoid distractors without explicit constraints
philosophy language grice argued proper interpretation language requires us
charitably assume speakers generally insert random words speech



fiturney

addition included due historical importance simplicity although mitchell
lapata found weighted addition better unweighted addition
include weighted addition experiments perform well
element wise multiplication mitchell lapatas experiments include
holistic model noncompositional baseline
table compares dual space model holistic model element wise multiplication vector addition latter three try three spaces

dual space
holistic
holistic
holistic
multiplication
multiplication
multiplication
addition
addition
addition

space
domain function
mono
domain
function
mono
domain
function
mono
domain
function

accuracy











table noun modifier questions
table dual space refers dual space model equation holistic
model ab represented corresponding row vector given space recall
section step rows matrices correspond n grams wordnet
n may greater one thus example dog house corresponding
row vector three spaces holistic model simply uses row vector
representation dog house element wise multiplication ab represented
equation vector addition model ab represented b vectors
normalized unit length added four use constraints
c b c four use training data parameter tuning
difference dual space model best variation elementwise multiplication statistically significant confidence level according fishers exact test however difference dual space model
best variation vector addition significant
limitations holistic
three spaces holistic model significantly better
inability address issue linguistic creativity major limitation multiplechoice questions used experiments intentionally constructed
requirement stem bigram must corresponding row function space see
done could use holistic model baseline however
gives misleading impression holistic model serious competitor
compositional approaches design table shows holistic model achieve
ideal unrealistic conditions


fidomain function dual space model

mitchell lapatas dataset used experiments section illustrates
limitations holistic model dataset consists distinct pairs bigrams
composed distinct bigrams bigrams occur wordnet
pairs bigrams contain bigrams occur wordnet given
matrices use rows wordnet holistic would
reduced random guessing pairs mitchell lapatas dataset
might argued failure holistic mitchell lapatas
dataset due decision base rows matrices terms wordnet
however suppose attempt build holistic model frequent bigrams web
gram corpus brants franz includes list bigrams appeared
times terabyte text total bigrams compositional
matrices use represent majority bigrams
hand holistic would require matrix rows
considerably beyond current state art
one possibility build matrix holistic needed given
input set n grams instead building large static multipurpose matrix
two idea first slow turney b used
sat analogy questions required nine days run whereas dual space model
process sat questions seconds given static multipurpose matrix second
requires large corpus corpus size must grow exponentially n length
phrases longer phrases rare larger corpora needed gather sufficient
data model phrases larger corpora longer processing times
given application may wise predefined list bigrams holistic
representations would wise expect list sufficient cover
bigrams would seen practice creativity human language use requires
compositional chomsky fodor lepore although holistic model
included baseline experiments competitor
supplement
impact constraints
use sim alone equation dropping constraints c b c accuracy
drops signficantly however benefit greatly
constraints table take best variation model table
look happens constraints dropped


dual space
holistic
multiplication
addition

space
domain function
mono
domain
domain

constraints





accuracy
constraints





difference





table impact constraints c b c accuracy



fiturney

element wise multiplication
section argued c b suitable row vectors matrix uk pk
suggested equation alternative use c b domain
space instead equation performance drops significantly
impact idioms
gap holistic model may due idiomatic
bigrams testing questions one successful approaches determining
whether multiword expression mwe compositional noncompositional idiomatic
compare holistic vector representation compositional vector representation
example high cosine two vectors suggests mwe compositional
idiomatic biemann giesbrecht johannsen alonso rishj sgaard
however suitable want assume
gap entirely due idiomatic bigrams instead would estimate much
gap due idiomatic bigrams
wordnet contains clues use indicators bigram might less
compositional bigrams allowing compositionality matter degree
one clue whether wordnet gloss bigram contains head noun
modifier example gloss dog house outbuilding serves shelter
dog contains modifier dog suggests dog house may compositional
classified testing set questions head first five characters
head noun bigram match first five characters word bigrams gloss
modifier first five characters modifier bigram match first five characters
word bigrams gloss head modifier match neither
neither head modifier match four classes approximately equally
distributed testing questions head modifier neither
match first five characters allow cases brain surgeon gloss
someone surgery nervous system especially brain bigram
classified first five characters surgeon match first five characters
surgery
table shows accuracy varies four classes questions three compositional dual space multiplication addition neither class significantly less accurate three classes fishers exact test
confidence difference significant holistic model three
compositional neither class less accurate classes
supports view significant fraction wrong answers compositional
due noncompositional bigrams
another clue compositionality wordnet whether head noun hypernym
bigram example surgeon hypernym brain surgeon classified
testing set questions hyper head noun member synset
immediate hypernym first noun sense bigram look
hypernym hierarchy look senses bigram hyper
testing set questions hyper


fidomain function dual space model


dual space
holistic
multiplication
addition

space
domain function
mono
domain
domain







head





accuracy
modifier neither















table variation accuracy different classes bigram glosses
table gives accuracy classes table
general pattern table three compositional significantly lower
accuracy class decreases significant difference
holistic model

dual space
holistic
multiplication
addition

space
domain function
mono
domain
domain

accuracy
hyper











table variation accuracy different classes bigram hypernyms

order sensitivity
note vector addition element wise multiplication lack order sensitivity equation sensitive order simc ab c simc ba c see impact
reformulating noun modifier questions test order sensitivity first
expand choice unigram c including stem bigram ab resulting explicit
comparison ab c expanded choice ab c generate another choice
ba c increases number choices seven fourteen due symmetry
vector addition element wise multiplication must assign similarity
ab c ba c
table compares dual space model element wise multiplication vector addition reformulated fourteen choice noun modifier questions holistic model
included table rows matrices reversed ba
bigrams may seen another illustration limits holistic model
stricter test dual space model significantly accurate element wise
multiplication vector addition fishers exact test confidence
dual space model perform well fourteen choice questions need
simd simf drop simd equation function alone table
ignoring modifier paying attention head noun accuracy drops
drop simf equation domain alone table
equation becomes symmetrical similarity assigned ab c


fiturney


dual space
multiplication
modified dual space
modified dual space
addition

space
domain function
domain
function alone
domain alone
domain

accuracy






table reformulated fourteen choice noun modifier questions
ba c accuracy drops dual space model significantly
accurate modified dual space fishers exact test
confidence
summary
reformulated fourteen choice noun modifier questions table dual space
significantly better element wise multiplication vector addition original
seven choice questions table difference large questions
test order unlike element wise multiplication vector addition dual space
model addresses issue order sensitivity unlike holistic model dual space
addresses issue linguistic creativity
similarity phrases
subsection apply dual space model measuring similarity phrases
mitchell lapatas dataset human similarity ratings pairs phrases
dataset includes three types phrases adjective noun noun noun verb object
pairs type pairs phrases pair phrases
rated human subjects ratings use point scale signifies lowest
degree similarity signifies highest degree table gives examples
let ab represent first phrase pair phrases environment secretary let cd
represent second phrase defence minister rate similarity phrase pairs
simp ab cd defined follows
simp ab cd geo simd c simd b simf c simf b



equation instructions human participants mitchell lapata
appendix b imply function domain similarity must high
phrase pair get high similarity rating figure illustrates reasoning behind
equation want high domain function similarities corresponding
components phrases ab cd
coincidence modified dual space accuracy fourteenchoice questions although aggregate accuracy individual questions two
typically select different choices



fidomain function dual space model

participant










phrase type
adjective noun
adjective noun
adjective noun
noun noun
noun noun
noun noun
verb object
verb object
verb object

group










phrase pair
certain circumstance particular case
large number great majority
evidence low cost
environment secretary defence minister
action programme development plan
city centre work
lift hand raise head
satisfy demand emphasise need
people increase number

similarity










table examples phrase pair similarity ratings mitchell lapatas
dataset similarity ratings vary lowest highest



b

f

f

c


simp ab cd
phrasal similarity

figure diagram equation

experimental setup
mitchell lapata divided dataset development set tuning parameters evaluation set testing tuned development set
ratings phrase pair evaluation set ratings phrase pair
development evaluation sets contain phrase pairs judgments
different participants thus rated phrase pairs development
set ratings evaluation set
challenging evaluation divide dataset phrase pairs rather
participants development set phrase pairs ratings
evaluation set phrase pairs ratings three phrase types
randomly select phrase pairs development set phrase pairs
information paragraph section mitchell lapata
personal communication jeff mitchell june



fiturney

evaluation set phrase pairs thus
ratings development set evaluation set
mitchell lapata use spearmans rank correlation coefficient spearmans
rho evaluate performance vector composition task
emulating human similarity ratings given phrase type phrase pairs
divided groups pairs group evaluation set people
gave similarity ratings pairs given group group pairs given
different group people score given phrase type
average three rho values one rho three groups people rating
pairs group ratings human ratings represented
vector numbers generates one rating pair group
yielding numbers make ratings comparable human ratings
ratings duplicated times yielding vector numbers spearmans
rho calculated two vectors ratings phrase types rho
values ratings per rho value ratings
believe evaluation method underestimates performance combining ratings different people one vector numbers
allow correlation adapt different biases one person gives consistently low ratings
another person gives consistently high ratings people ranking
ranking matches ranking get high
score fair evaluation score calculating one rho value
human participant given phrase type calculate average
rho values participants
given phrase type phrase pairs divided groups pairs
development set randomly select phrase pairs groups
phrase pairs per phrase type leaves phrase pairs
groups evaluation set phrase pairs per phrase type human
participants ratings represented vector numbers ratings
represented vector numbers rho value calculated two vectors
numbers input given phrase type score average
rho values participants per group groups rho values phrase types
rho values ratings per rho value ratings
comparison approaches
table compares dual space model vector addition element wise multiplication
use development set tune parameters three approaches vector
addition ab represented b cd represented c similarity ab
cd given cosine two vectors element wise multiplication uses equation
represent ab cd dual space model uses equation
average correlation dual space model significantly average
correlation vector addition function space element wise multiplication
mono space significantly vector addition function space
information paragraph personal communication jeff mitchell june
mitchell lapatas describe spearmans rho applied



fidomain function dual space model


human
dual space
addition
addition
addition
multiplication
multiplication
multiplication

correlation
ad nn nn nn

















phrase type
vb ob avg

















comment
leave one correlation subjects
domain function space
mono space
domain space
function space
mono space
domain space
function space

table performance evaluation dataset
difference dual space model element wise multiplication mono
space signficant average correlation rho
values phrase types groups participants rho values participants
calculate statistical significance paired test significance level
pairs rho values
order sensitivity
mitchell lapatas dataset test order sensitivity given phrase pair
ab cd test order sensitivity adding pair ab dc assume
pairs would given rating human participants table
happens transformation applied examples table
save space give examples participant number
participant







phrase type
adjective noun
adjective noun
adjective noun
adjective noun
adjective noun
adjective noun

group







phrase pair
certain circumstance particular case
certain circumstance case particular
large number great majority
large number majority great
evidence low cost
evidence cost low

similarity







table testing order sensitivity adding phrase pairs
table gives expanded dataset stringent
dataset dual space model performs significantly better vector addition
vector multiplication unlike element wise multiplication vector addition dualspace model addresses issue order sensitivity
manually inspected pairs automatically rated found
rating reasonable cases although cases could disputed example
original noun noun pair tax charge interest rate generates pair tax charge
rate interest original verb object pair produce effect achieve generates
pair produce effect achieve seems natural tendency correct


fiturney


human
dual space
addition
addition
addition
multiplication
multiplication
multiplication

correlation
ad nn nn nn

















phrase type
vb ob avg

















comment
leave one correlation subjects
domain function space
mono space
domain space
function space
mono space
domain space
function space

table performance dataset expanded test order sensitivity
incorrectly ordered pairs minds assign higher ratings
deserve predict human ratings pairs would vary greatly depending
instructions given human raters instructions emphasized
importance word order pairs would get low ratings prediction supported
semeval task jurgens mohammad turney holyoak
instructions raters emphasized importance word order wrongly
ordered pairs received low ratings
summary
dataset test order sensitivity vector addition performs slightly better
dual space model dataset tests order sensitivity dual space
model surpasses vector addition element wise multiplication large margin
domain versus function associated versus similar
chiarello et al created dataset word pairs labeled similar
associated similar associated pairs three classes table
shows examples dataset labeled pairs created cognitive
psychology experiments human subjects experiments found evidence
processing associated words engages left right hemispheres brain
ways different processing similar words seems
fundamental neurological difference two types semantic relatedness
hypothesize similarity domain space simd b measure degree
two words associated similarity function space simf b measure
degree two words similar test hypothesis define similar
simso b associated simao b similar associated simsa b follows

ratio x

x x
otherwise



controversy among cognitive scientists distinction semantic similarity
association mcrae khalkhali hare



fidomain function dual space model

word pair
table bed
music art
hair fur
house cabin
cradle baby
mug beer
camel hump
cheese mouse
ale beer
uncle aunt
pepper salt
frown smile

class label
similar
similar
similar
similar
associated
associated
associated
associated
similar associated
similar associated
similar associated
similar associated

table examples word pairs chiarello et al labeled similar
associated similar associated full dataset appendix

simso b ratio simf b simd b



simao b ratio simd b simf b



simsa b geo simd b simf b



intention simso high simf high simd low simao high
simd high simf low simsa high simd simf high
illustrated figure






f

f

f

b

b

b

simso b

simao b

simsa b

similar

associated

similar associated

figure diagrams equations



fiturney

evaluation
experiments three preceding subsections three sets parameter
settings dual space model table shows parameter values effect
three sets parameter setttings give us three variations similarity measures simso
simao simsa evaluate three variations see well correspond
labels chiarello et al dataset
similarity
simr b c
simc ab c
simp ab cd

description
similarity relations
similarity noun modifier compositions
similarity phrases

section




kd




pd




kf




pf




table parameter settings dual space model
given similarity measure simso sort word pairs descending order similarities look top n pairs see many
desired label case simso would see majority
top n label similar table shows percentage pairs
desired labels three variations three similarity measures note
random guessing would yield since three classes pairs size

source parameters
simr b c

simc ab c

simp ab cd

n










percentage top n desired label
similar associated similar associated




























table percentage top n word pairs desired labels
three sets parameter settings table displays high density desired
labels tops sorted lists density slowly decreases move
lists evidence three similarity measures capturing three classes
chiarello et al
another test hypothesis use three similarity measures create feature
vectors three elements word pair word pair b represented
feature vector hsimso b simao b simsa b use supervised learning
ten fold cross validation classify feature vectors three classes chiarello
et al learning use logistic regression implemented


fidomain function dual space model

weka summarized table lend support
hypothesis similarity domain space simd b measure degree
two words associated similarity function space simf b measure
degree two words similar

source parameters
simr b c
simc ab c
simp ab cd

accuracy




similar




f measure
associated similar associated







average




table performance logistic regression three similarity measures features

table similar seems sensitive parameter settings associatedonly similar associated hypothesize function similarity
difficult measure domain similarity note construction function
space section complex construction domain space section
intuitively seems easier identify domain thing identify functional
role gentners work suggests children master domain similarity
become competent function similarity

discussion experiments
section discusses previous section
summary
section used multiple choice analogy questions evaluate dual space
model relational similarity simr b c difference performance
dual space model accuracy best past accuracy
holistic model statistically significant experiments reformulated version
questions designed test order sensitivity supported hypothesis
domain function space required function space sensitive order
merging two spaces mono space causes significant drop performance
section automatically generated multiple choice noun modifier composition questions wordnet evaluate dual space model noun modifier compositional similarity simc ab c difference performance dual space
model accuracy state art element wise multiplication model
accuracy statistically significant best performance obtained holistic model model address issue linguistic creativity
experiments suggest significant fraction gap holistic model
due noncompositional phrases limitation element wise multiplication model lack sensitivity order experiments reformulated version
weka available http www cs waikato ac nz ml weka



fiturney

questions designed test order sensitivitiy demonstrated statistically significant
advantage dual space model element wise multiplication vector addition

section used mitchell lapatas dataset pairs phrases
evaluate dual space model phrasal similarity simp ab cd reformulated version
dataset modified test order sensitivitiy showed statistically significant advantage
dual space model element wise multiplication vector addition
section used chiarello et al dataset word pairs labeled
similar associated similar associated test hypothesis similarity
domain space simd b measure degree two words associated
similarity function space simf b measure degree two words
similar experimental support hypothesis interesting
chiarello et al argue fundamental neurological difference way
people process two kinds semantic relatedness
experiments support claim dual space model address issues
linguistic creativity order sensitivity adaptive capacity furthermore dual space
model provides unified semantic relations semantic composition
corpus similarity versus lexicon similarity
section suggest function similarity may correspond kind
taxonomical similarity often associated lexicons wordnet resnik
jiang conrath leacock chodrow hirst st onge
word pairs table labeled similar kinds words typically
share common hypernym taxonomy example table bed share hypernym
furniture believe correct necessarily imply lexiconbased similarity measures would better corpus
used
similarities section arguably relational similarity simr b c
makes use function similarity function similarity achieves
sat questions original five choice version see table however best performance
achieved sat questions wordnet veale difference
statistically significant confidence level fishers exact test
consider analogy traffic street water riverbed one sat questions
involves analogy traffic street stem pair water riverbed correct
choice simr b c equation function similarity equation
make correct choice recognize traffic water high degree
function similarity fact similarity used hydrodynamic traffic flow
daganzo however must climb wordnet hierachy way entity
shared hypernym traffic water believe manually
generated lexicon capture functional similarity discovered
large corpus

theoretical considerations
section examines theoretical questions dual space model


fidomain function dual space model

vector composition versus similarity composition
dual space model phrase stand alone general purpose representation
composite phrase apart representations component words composite
meaning constructed context given task example task measure
similarity relation dog house relation bird nest compose
meanings dog house one way see section task measure similarity
phrase dog house word kennel compose meanings dog
house another way see section task measure similarity phrase
dog house phrase canine shelter compose meanings dog house
third way see section composition construction explicitly ties together
two things compared depends nature comparison
desired task performed hypothesize single stand alone
task independent representation constructed suitable purposes
noted introduction composition vectors stand alone
representation phrase composing similarities necessarily yields linking structure
connects phrase phrases linking structures seen figures
intuitively seems important part understand phrase
connecting phrases part understanding dog house connection
kennel dictionaries make kinds connections explicit perspective
idea explicit linking structure seems natural given making connnections among
words phrases essential aspect meaning understanding
general form similarities dual space model
subsection present general scheme ties together similarities
defined section scheme includes similarities chunks text
arbitrary size scheme encompasses phrasal similarity relational similarity
compositional similarity
let chunk text ordered set words ht tn ti
word represent semantics hd f matrices
row vector di n row vector domain space represents
domain semantics word ti row vector f n row vector
function space represents function semantics word ti keep notation
simple parameters kd pd domain space kf pf function space
implicit assume row vectors f normalized unit length note
size representation scales linearly n number words hence
information scalability large values n inevitably duplicate
words representation could easily compressed sublinear size without loss
information
let two chunks text representations hd f
hd f contains n words n words let
parameters kd pd let f f parameters kf pf
n kd n kd f n kf f n kf note dt
n n
matrix cosines two row vectors element th


fiturney


row j th column dt
cos di dj likewise n n matrix
cosines row vector row vector
suppose wish measure similarity sim two chunks
text restricted similarity measures following
general form





sim f dt
f f f f f f



words input composition function f cosines implicit
parameters kd pd kf pf f operate directly row vectors
f f contrast much work discussed section composition
operation shifted representations similarity measure
f exact specification f depends task hand sentences
envision structure f determined syntactic structures
two sentences
consider relational similarity section

sim b c geo simf c simf b



sim b c geo simd b simd c



sim b c geo simd simd c b

sim b c sim b c sim b c
simr b c
otherwise




fits form equation ha bi hc di see


sim cosines f ft
sim cosines

sim cosines
consider compositional similarity section

sim ab c geo simd c simd b c simf b c

sim ab c c b c
simc ab c
otherwise




seen instance equation ha bi hci

case sim cosines dt
f f constraints c b c

expressed terms cosines simd c simd b c
equivalently could use cosines f ft
similar analyses apply similarities
sections similarities instances equation
although representations sizes linear functions numbers phrases size composition equation quadratic
function numbers phrases however specific instances general
equation may less quadratic size may possible limit growth
note requirement two chunks text number
words n necessarily equal n section n n



fidomain function dual space model

linear function general quadratic growth often acceptable practical
applications garey johnson
function words e g prepositions conjunctions one option would treat
words would represented vectors similarities would calculated function domain spaces another possibility would
use function words hints guide construction composition function f
function words would correspond vectors instead would contribute determining linking structure connects two given chunks text first option
appears elegant choice options made empirically
automatic composition similarities
section manually constructed functions combined similarity measures
intuition background knowledge manual construction scale
task comparing two arbitrarily chosen sentences however good reasons
believing construction composition functions automated
turney presents solving analogical mapping
analogy solar system rutherford bohr model atom given
list terms solar system domain planet attracts revolves sun gravity solar system mass list terms atomic domain revolves atom attracts
electromagnetism nucleus charge electron automatically generate one one
mapping one domain solar system atom sun nucleus planet
electron mass charge attracts attracts revolves revolves gravity electromagnetism twenty analogical mapping attains accuracy
compared average human accuracy
scores quality candidate analogical mapping composing
similarities mapped terms composition function addition individual
component similarities holistic relational similarities searches
space possible mappings mapping maximizes composite similarity
measure analogical mapping treated argmax argument
maximized mapping function effect output analogical
mapping automically generated composition similarities mapping structures
found essentially linking structures see
figures
believe variation turneys could used automatically compose similarities dual space model example possible
identify paraphrases automatic similarity composition proposal search
composition maximizes composite similarity subject constraints
constraints syntax sentences turney points analogical
mapping could used align words two sentences experimentally
evaluate suggestion
recent work lin bilmes shown argmax solved efficiently effectively framed monotone submodular function maximization
believe automatic composition similarities fit naturally
framework would highly scalable semantic composition


fiturney

regarding information scalability dual space model suffer information
loss unlike approaches represent compositions vectors fixed dimensionality
sizes representations grow lengths phrases grow growth
might quadratic exponential questions automate
composition similarities may impact computational complexity
scaling longer phrases evidence questions tractable

limitations future work
one area future work experiment longer phrases two words
sentences discussed section interesting topic parsing might
used constrain automatic search similarity composition functions
focused two spaces domain function seems likely us
model spaces would yield better performance currently experimenting quad space model includes domain noun contextual patterns
function verb quality adjective manner adverb spaces
preliminary quad space promising quad space seems related
pustejovskys four part qualia structure
another issue avoided morphology discussed section used
validforms function wordnet querydata perl interface wordnet map
morphological variations words base forms implies example
singular noun plural form semantic representation
certainly simplification sophisticated model would use different representations
different morphological forms word
avoided issue polysemy possible extend past work
polysemy vsms dual space model schutze pantel lin erk
pado
treated holistic model dual space model
competitors certain cases idiomatic expressions holistic
required likewise holistic limited inability handle
linguistic creativity considerations suggest holistic dual space
must integrated another topic future work
arguably limitation dual space model four parameters
tune kd pd kf pf hand perhaps model adaptive capacity
must parameters tune needed
number design decisions made construction domain function
space especially conversion phrases contextual patterns sections
decisions guided intuitions expect exploration experimental evaluation design space fruitful area future
construction function space section specific english may generalize readily indo european languages languages may present
challenge another topic future
composite similarities use geometric mean combine domain
function similarities see reason restrict possible composition functions


fidomain function dual space model

equation allows composition function f exploring space possible composition
functions another topic future work
another question formal logic textual entailment integrated
dual space model seems suitable recognizing paraphrases
obvious way handle entailment generally focused
kinds similarity scale phrases red ball sentences ball
red encounter truth falsity gardenfors argues spatial
bridge low level connectionist high level symbolic claims
spatial best questions similarity symbolic best
questions truth yet know join two kinds

conclusions
goal develop model unifies semantic relations
compositions addressing linguistic creativity order sensitivity adaptive capacity information scalability believe dual space model achieves goal
although certainly room improvement
many kinds wordcontext matrices notions context
sahlgren gives good overview types context explored
past work novelty dual space model includes two distinct
complementary wordcontext matrices work together synergistically
two distinct spaces two distinct similarity measures
combined many different ways multiple similarity measures similarity composition becomes viable alternative vector composition example instead multiplying vectors c b multiply similarities simsa b
geo simd b simf b suggest fruitful way look
semantics

acknowledgments
thanks george foster yair neuman david jurgens reviewers jair
helpful comments earlier version thanks charles clarke
corpus used build three spaces stefan buttcher wumpus
creators wordnet making lexicon available developers opennlp
doug rohde svdlibc jeff mitchell mirella lapata sharing data
answering questions evaluation methodology christine chiarello curt
burgess lorie richards alma pollock making data available jason rennie
wordnet querydata perl interface wordnet developers perl data
language

references
aerts czachor quantum aspects semantic analysis symbolic
artificial intelligence journal physics mathematical general l
l


fiturney

baroni zamparelli r nouns vectors adjectives matrices representing adjective noun constructions semantic space proceedings
conference empirical methods natural language processing emnlp
pp
bengio ducharme r vincent p jauvin c neural probabilistic language
model journal machine learning
bicici e yuret clustering word pairs answer analogy questions
proceedings fifteenth turkish symposium artificial intelligence neural
networks tainn akyaka mugla turkey
biemann c giesbrecht e distributional semantics compositionality
shared task description proceedings workshop distributional
semantics compositionality disco pp portland oregon
bollegala matsuo ishizuka measuring similarity implicit
semantic relations web proceedings th international conference
world wide web www pp
brants franz web gram version linguistic data consortium
philadelphia
bullinaria j levy j extracting semantic representations word cooccurrence statistics computational study behavior methods

buttcher clarke c efficiency vs effectiveness terabyte scale information retrieval proceedings th text retrieval conference trec
gaithersburg md
caron j experiments lsa scoring optimal rank basis proceedings
siam computational information retrieval workshop pp raleigh
nc
chiarello c burgess c richards l pollock semantic associative
priming cerebral hemispheres words words dont sometimes
places brain language
chomsky n logical structure linguistic theory plenum press
church k hanks p word association norms mutual information lexicography proceedings th annual conference association computational linguistics pp vancouver british columbia
clark coecke b sadrzadeh compositional distributional model
meaning proceedings nd symposium quantum interaction pp
oxford uk
clark pulman combining symbolic distributional meaning
proceedings aaai spring symposium quantum interaction pp
stanford ca
conway j h sloane n j sphere packings lattices groups springer


fidomain function dual space model

daganzo c f cell transmission model dynamic representation highway
traffic consistent hydrodynamic theory transportation part b
methodological
davidov rappoport unsupervised discovery generic relationships
pattern clusters evaluation automatically generated sat analogy questions
proceedings th annual meeting acl hlt acl hlt pp
columbus ohio
erk k pado structured vector space model word meaning context
proceedings conference empirical methods natural language
processing emnlp pp honolulu hi
fellbaum c ed wordnet electronic lexical database mit press
firth j r synopsis linguistic theory studies linguistic
analysis pp blackwell oxford
fodor j lepore e compositionality papers oxford university press
gardenfors p conceptual spaces geometry thought mit press
garey r johnson computers intractability guide theory
np completeness freeman
gentner structure mapping theoretical framework analogy cognitive
science
gentner language career similarity gelman byrnes j
eds perspectives thought language interrelations development pp
cambridge university press
golub g h van loan c f matrix computations third edition johns
hopkins university press baltimore md
grefenstette e sadrzadeh experimenting transitive verbs discocat proceedings gems workshop geometrical natural
language semantics
grice h p studies way words harvard university press cambridge

guevara e regression model adjective noun compositionality distributional
semantics proceedings workshop geometrical natural
language semantics gems pp
harris z distributional structure word
hearst automatic acquisition hyponyms large text corpora proceedings th conference computational linguistics coling pp
herdagdelen baroni bagpack general framework represent semantic
relations proceedings eacl geometrical natural language
semantics gems workshop pp


fiturney

hirst g st onge lexical chains representations context detection
correction malapropisms fellbaum c ed wordnet electronic
lexical database pp mit press
jiang j j conrath w semantic similarity corpus statistics
lexical taxonomy proceedings international conference
computational linguistics rocling x pp tapei taiwan
johannsen alonso h rishj c sgaard shared task system
description frustratingly hard compositionality prediction proceedings
workshop distributional semantics compositionality disco pp
portland oregon
jones n mewhort j k representing word meaning order information composite holographic lexicon psychological review
jurgens mohammad turney p holyoak k j semeval
task measuring degrees relational similarity proceedings first joint
conference lexical computational semantics sem pp montreal
canada
kintsch w metaphor comprehension computational theory psychonomic bulletin review
kintsch w predication cognitive science
kolda bader b tensor decompositions applications siam review

landauer k computational basis learning cognition arguments
lsa ross b h ed psychology learning motivation advances
theory vol pp academic press
landauer k dumais solution platos latent semantic analysis theory acquisition induction representation knowledge
psychological review
landauer k mcnamara dennis kintsch w handbook latent
semantic analysis lawrence erlbaum mahwah nj
leacock c chodrow combining local context wordnet similarity
word sense identification fellbaum c ed wordnet electronic lexical
database mit press
lee seung h learning parts objects nonnegative matrix
factorization nature
lepage shin ichi saussurian analogy theoretical account
application proceedings th international conference computational
linguistics coling pp
lin h bilmes j class submodular functions document summarization
th annual meeting association computational linguistics human
language technologies acl hlt pp


fidomain function dual space model

mangalath p quesada j kintsch w analogy making predication
relational information lsa vectors proceedings th annual meeting
cognitive science society p austin tx
mcrae k khalkhali hare semantic associative relations adolescents young adults examining tenuous dichotomy reyna v chapman
dougherty confrey j eds adolescent brain learning reasoning
decision making pp apa washington dc
mitchell j lapata vector semantic composition proceedings acl hlt pp columbus ohio association computational
linguistics
mitchell j lapata composition distributional semantics
cognitive science
moschitti quarteroni kernels linguistic structures answer extraction proceedings th annual meeting association computational
linguistics human language technologies short papers p columbus
oh
nakov p hearst verbs characterize noun noun relations proceedings th international conference artificial intelligence methodology
systems applications aimsa pp varna bulgaria
nakov p hearst ucb system description semeval task proceedings fourth international workshop semantic evaluations semeval
pp prague czech republic
nastase v sayyad shirabad j sokolova szpakowicz learning nounmodifier semantic relations corpus wordnet features proceedings st national conference artificial intelligence aaai pp

nastase v szpakowicz exploring noun modifier semantic relations
proceedings fifth international workshop computational semantics iwcs pp tilburg netherlands
niwa nitta co occurrence vectors corpora vs distance vectors
dictionaries proceedings th international conference computational
linguistics pp kyoto japan
seaghdha copestake lexical relational similarity classify
semantic relations proceedings th conference european chapter
association computational linguistics eacl athens greece
pantel p lin discovering word senses text proceedings eighth
acm sigkdd international conference knowledge discovery data mining
pp edmonton canada
plate holographic reduced representations ieee transactions neural networks
pustejovsky j generative lexicon computational linguistics


fiturney

rapp r word sense discovery sense descriptor dissimilarity proceedings ninth machine translation summit pp
resnik p information content evaluate semantic similarity taxonomy
proceedings th international joint conference artificial intelligence
ijcai pp san mateo ca morgan kaufmann
rosario b hearst classifying semantic relations noun compounds
via domain specific lexical hierarchy proceedings conference
empirical methods natural language processing emnlp pp
rosario b hearst fillmore c descent hierarchy selection
relational semantics proceedings th annual meeting association
computational linguistics acl pp
sahlgren word space model distributional analysis represent syntagmatic paradigmatic relations words high dimensional vector spaces
ph thesis department linguistics stockholm university
santorini b part speech tagging guidelines penn treebank project tech
rep department computer information science university pennsylvania
rd revision nd printing
schutze h automatic word sense discrimination computational linguistics

smolensky p tensor product variable binding representation symbolic
structures connectionist systems artificial intelligence
socher r huang e h pennington j ng manning c dynamic
pooling unfolding recursive autoencoders paraphrase detection advances
neural information processing systems nips pp
socher r manning c ng learning continuous phrase representations
syntactic parsing recursive neural networks proceedings nips
deep learning unsupervised feature learning workshop
thater furstenau h pinkal contextualizing semantic representations
syntactically enriched vector proceedings th annual meeting
association computational linguistics pp
turney p mining web synonyms pmi ir versus lsa toefl
proceedings twelfth european conference machine learning ecml
pp freiburg germany
turney p expressing implicit semantic relations without supervision
proceedings st international conference computational linguistics
th annual meeting association computational linguistics coling acl pp sydney australia
turney p b similarity semantic relations computational linguistics

turney p latent relation mapping engine experiments
journal artificial intelligence


fidomain function dual space model

turney p b uniform analogies synonyms antonyms associations proceedings nd international conference computational
linguistics coling pp manchester uk
turney p littman l corpus learning analogies semantic
relations machine learning
turney p littman l bigham j shnayder v combining independent
modules solve multiple choice synonym analogy proceedings
international conference recent advances natural language processing
ranlp pp borovets bulgaria
turney p pantel p frequency meaning vector space
semantics journal artificial intelligence
utsumi computational semantics noun compounds semantic space model
proceedings st international joint conference artificial intelligence
ijcai pp
veale wordnet sits sat knowledge lexical analogy
proceedings th european conference artificial intelligence ecai
pp valencia spain
widdows semantic vector products initial investigations proceedings
nd symposium quantum interaction oxford uk




