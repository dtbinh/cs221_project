Journal Artificial Intelligence Research 44 (2012) 196

Submitted 01/12; published 05/12

C OLIN: Planning Continuous Linear Numeric Change
Amanda Coles
Andrew Coles
Maria Fox
Derek Long

AMANDA . COLES @ KCL . AC . UK
ANDREW. COLES @ KCL . AC . UK
MARIA . FOX @ KCL . AC . UK
DEREK . LONG @ KCL . AC . UK

Department Informatics, Kings College London,
Strand, London WC2R 2LS, UK

Abstract
paper describe COLIN, forward-chaining heuristic search planner, capable reasoning COntinuous LINear numeric change, addition full temporal semantics
PDDL 2.1. work make two advances state-of-the-art terms expressive reasoning capabilities planners: handling continuous linear change, handling duration-dependent effects combination duration inequalities, require tightly coupled temporal numeric reasoning planning. COLIN combines FF-style
forward chaining search, use Linear Program (LP) check consistency
interacting temporal numeric constraints state. LP used compute bounds
values variables state, reducing range actions need considered
application. addition, develop extension Temporal Relaxed Planning Graph heuristic CRIKEY 3, support reasoning directly continuous change. extend range task
variables considered suitable candidates specifying gradient continuous numeric
change effected action. Finally, explore potential employing mixed integer programming tool optimising timestamps actions plan, solution
found. support this, contribute selection extended benchmark domains
include continuous numeric effects. present results COLIN demonstrate scalability
range benchmarks, compare existing state-of-the-art planners.

1. Introduction
considerable progress development automated planning techniques
domains involving independent temporal metric conditions effects (Eyerich, Mattmuller,
& Roger, 2009; Coles, Fox, Long, & Smith, 2008a; Gerevini, Saetti, & Serina, 2006; Edelkamp,
2003; Coles, Fox, Long, & Smith, 2008b). development powerful heuristics propositional
planning shown offer benefits solution extended planning problems, including
planning uncertainty (Palacios & Geffner, 2009), planning numbers planning
time. However, combination integration metric temporal features, metric
quantities change time-dependent ways, remains challenge received relatively little
attention.
Interaction time numbers planning problems occur many ways.
simplest case, using PDDL 2.1 (Fox & Long, 2003), numeric effects actions updated
instantaneously, start end points actions known (and fixed)
point action execution. corpus domains past International Planning Competitions
adhere restrictions. Time numbers interact least two complex ways. First,
actions variable, possibly constrained, durations (instantaneous) effects
c
2012
AI Access Foundation. rights reserved.

fiC OLES , C OLES , F OX & L ONG

actions depend values durations. allows domain models capture effects
processes discretised step effects, adjusted according demands specific problem
instances. Second, effects actions considered continuous across execution,
values metric variables time point depend long continuous effects
acting them.
example, problem sand loaded lorry modelled amount
sand loaded depends time spent loading. first approach capture increase
quantity loaded sand step function applied end loading action. second
approach, process loading sand modelled continuous linear function time
spent loading, amount sand lorry observed point throughout
loading process. safety device must engaged lorry three-quarters
full, second models allow planner necessary access
underlying process behaviour make good planning choices integrate action
solutions. alternative models exploiting duration-dependent effects split loading
action two parts around time point safety device must engaged,
alternatives become complicated relatively modest changes domain.
Continuous change forms common many important problems. include: energy management, consumption replenishment restricted continuous resources
fuel, tracking progress chemicals storage tanks chemical plants, choreographing robot motion execution tasks, managing efficient use time.
cases, model using discrete time-independent change adequate planning. However, discretisation always practical: find reasonable solution (or, indeed, find one all) identifying
appropriate granularity discretisation non-trivial, perhaps requiring range choices
fine-grained make discrete model infeasibly large. cases, numeric
change cannot appropriately discretised, unavoidably necessary access
values numeric variables execution actions, order manage interactions
numeric values.
paper present planner, COLIN, capable reasoning variable, durationdependent, linear change linear continuous numeric effects. key advance COLIN makes
able reason time-dependent change use linear programs combine metric temporal conditions effects representation. COLIN satisficing
planner attempts build good quality solutions complex class problems. Since COLIN
forward-searching planner requires representation states, means compute progression states heuristic function guide search path initial goal
state. COLIN built planner CRIKEY 3 (Coles, Fox, Long et al., 2008a). However, CRIKEY 3
requires numeric change discrete cannot reason continuous numeric change, duration dependent change (where duration actions fixed state action
begins). able reason successfully problems characterised continuous change, coping efficiently wide range practical problems inspired real applications,
major contribution made COLIN.
organisation paper follows. Section 2 explain features PDDL 2.1
COLIN handle, contrast repertoire CRIKEY 3. Section 4 define
problem addressed COLIN. Section 5 outline background temporal
metric planning supports COLIN, before, Section 6, describing details foundations COLIN lie CRIKEY 3. COLIN inherits representation states CRIKEY 3,
2

fiC OLIN : P LANNING C ONTINUOUS C HANGE

well machinery confirming temporal consistency plans basis
heuristic function. Section 7 describe systems literature addressed similar
hybrid discrete-continuous planning problems COLIN designed handle. Section 8
explains state progression extended COLIN handle linear continuous change, Section 9 describes heuristic guides search solutions. Section 10 consider several
elements COLIN improve efficiency plan quality, without affecting fundamental
behaviour planner. Since time-dependent numeric change little explored,
benchmarks existence allow full quantitative evaluation. therefore present
collection continuous domains used analysis, show COLIN
fares these. appendix containing explanations technical detail detailed
summaries background work COLIN depends, ensures paper complete
self-contained.

2. Language Features C RIKEY 3 COLIN
COLIN builds CRIKEY 3 handling continuous features PDDL 2.1. C RIKEY 3 restricted management discrete change, COLIN handle full range linear continuous numeric effects. metric functions PDDL 2.1 repertoire COLIN
scale-up scale-down, non-linear updates, general form plan metrics. Managing plan metrics defined terms domain variables remains challenge planning
yet fully confronted contemporary planner. COLIN handle restricted
form quality metric, exploits instrumented variable called total-cost. allows
COLIN minimise overall cost shortest plan find using total-time (the default
metric used temporal planners).
common CRIKEY 3, COLIN cope Timed Initial Literals, important feature
introduced PDDL 2.2 (Hoffmann & Edelkamp, 2005). PDDL 2.1 backward compatible
McDermotts PDDL (McDermott, 2000) therefore supports ADL (Pednault, 1989). COLIN
handle full ADL, deal restricted form conditional effect seen
airplane-landing problem described section 11. restricted form allows cost action
dependent state applied. general forms conditional effect cannot
handled.
collection features, COLIN able fully manage discrete continuous
numeric change occur directly result actions. PDDL + (Fox & Long, 2006)
supports modelling continuous change brought exogenous processes events.
triggered actions, model independent continuous behaviour brought
world rather planners direct action. key additional features PDDL +
support processes events. COLIN handle features restricted
management continuous change expressed durative action device.
detailed explanations syntaxes semantics PDDL 2.1 PDDL +, including
semantics implementations state representation state progression must constructed, readers refer work Fox Long (2003, 2006).
3

fiC OLES , C OLES , F OX & L ONG

Language
PDDL 2.1
PDDL 2.1

Language Feature
Numeric conditions effects
Continuous numeric effects

C RIKEY 3
yes


COLIN
yes
yes

PDDL 2.1

PDDL 2.1

General plan metrics
Use total-cost
Assign (to discrete variables)
Scale-up/down
#t
Durative actions



yes


yes


yes
yes

yes
yes

PDDL 2.1

Duration inequalities

limited

yes

PDDL 2.2

TILs
Conditional Effects
ADL

yes



yes
partial


PDDL 2.1
PDDL 2.1
PDDL 2.1
PDDL 2.1

PDDL
PDDL

Comment
Basic treatment follows Metric-FF
Modification state representation
Modification heuristic

Section
Appendix B
Section 8
Section 9

Limited form
Treatment follows Metric-FF

Section 10

continuous effects
Includes required concurrency
COLIN handles
duration-dependent effects

limited effects

Section 6
Appendix C
Sections 8 9
Section 6
Section 10

Table 1: Language features handled CRIKEY 3 COLIN.

3. Motivation
number accounts planning successfully applied real problems,
frequency applications reported increasing. following examples involve domains hybrid discrete-continuous dynamics. dynamics typically dealt
discretising time, packaging continuous numeric effects step functions, integrating
propositional planning techniques specialised solvers. examples hybrid
discrete-continuous reasoning could exploited improve plan quality solution time.
Operations refineries (Boddy & Johnson, 2002; Lamba, Dietz, Johnson, & Boddy, 2003)
chemical plants (Penna, Intrigila, Magazzeni, & Mercorio, 2010), continuous
processes reflect flows materials, mixing chemical reactions, heating cooling.
Management power thermal energy aerospace applications power management critical, management solar panel arrays International Space
Station (Knight, Schaffer, & B.Clement, 2009; Reddy, Frank, Iatauro, Boyce, Kurklu, AiChang, & Jonsson, 2011). example, Knight et al. (2009) rely high-fidelity power
model (TurboSpeed) provide support reasoning continuous power supply
different configurations solar panels. Power management critical problem
space applications (including planetary rovers landers, inspiring temporal-metriccontinuous Rovers domain used one benchmark evaluation domains Section 11).
Chien et al. (2010) describe planner used support operations Earth Observing 1 (EO1), management thermal energy generated instruments sufficiently important on-board planner uses (highly constrained) CPU cycles model
track value. EO-1 inspires temporal-metric-continuous Satellite benchmark described
Section 11.
Management non-renewable power contexts, battery powered devices.
battery management problem described Fox et al. (2011) relies non-linear model,
4

fiC OLIN : P LANNING C ONTINUOUS C HANGE

COLIN must currently reduce discrete linear approximation, coupled iterated validation solution refinement, order optimise power use. Battery management
example continuous problem cannot solved continuous dynamics
removed.
Assignment time-dependent costs Aircraft Landing domain (Dierks, 2005),
continuous processes govern changing costs use runway landing
time deviates optimal landing time aircraft. problem inspires
Aircraft-Landing benchmark domain described Section 11.
Choreography mobile robotic systems: many cases, operations robotic platforms
involve careful management motion alongside tasks, continuous motion
robot constrains accessibility specific tasks, inspection observation.
Existing examples hybrid discrete-continuous planning models reasoning problems kind include work using flow tubes capture constraints continuous
processes (Leaute & Williams, 2005; Li & Williams, 2008). Problems involving autonomous
underwater vehicles (AUVs) inspired temporal-metric-continuous AUV benchmark presented Section 11.

4. Problem Definition
COLIN designed solve class problems temporal metric, feature linear
continuous metric change. refer class temporal-metric-continuous problems,
contains substantial subset problems expressed PDDL 2.1.
step towards class temporal-metric-continuous problems, recall definition
simple temporal-metric planning problem one time-dependent metric
change. Simple temporal-metric problems represented tuple hI, A, G, i, where:
initial state: set propositions assignment values set numeric
variables. Either sets may empty. notational convenience, refer
vector numeric values given state v.
A, set actions, hdur , pre ` , eff ` , pre , pre , eff i, where:
pre ` (pre ) start (end) conditions a: state starts (ends),
conditions must hold (for detailed account subtleties semantics
action application, see Fox & Long, 2003).
eff ` (eff ) start (end) effects a: starting (ending) updates world state
according effects. given collection effects eff x , x {`, a}, consists of:
eff
x , propositions deleted world state;
eff +
x , propositions added world state;
eff nx , effects acting upon numeric variables.
pre invariant conditions a: must hold every point open interval
start end a.
dur duration constraints a, calculated basis world state
started, constraining length time pass start end
a. refer special parameter ?duration, denoting duration a.
5

fiC OLES , C OLES , F OX & L ONG

G, goal: set propositions conditions numeric variables.
optionally , metric optimisation function, defined function values numeric
variables end plan, special variable total-time, denoting makespan
plan.
solution problem time-stamped sequence actions, associated durations,
transforms initial state state satisfying goal, respecting conditions imposed.
durations actions must specified explicitly, since possible action specifications
satisfied different duration values.
PDDL 2.1 numeric conditions used pre ` , pre , pre , dur G expressed
form:
hf (v), op, ci, op {, <, =, >, }, c <
v vector metric fluents planning problem, f (v) function applied
vector numeric fluents c arbitrary constant. Numeric effects used eff ` eff
expressed as:
hv, op, f (v)i, op {=, +=, =, -=, =}
restricted form numeric expressions set expressions Linear Normal Form (LNF).
expressions f (v) weighted sum variables plus constant, expressible
form w v + c, vector constants, w. notable consequence permitting dur take
form set LNF constraints ?duration ?duration need evaluate single
fixed value. instance, may constrain value ?duration lie within range values,
e.g. (?duration v1 ) (?duration v2 ), numeric variables v1 v2 . Restricting
conditions effects use LNFs allows metric expressions captured linear
program model, fact exploit COLIN.
class temporal-metric problems extended temporal-metric-continuous problems
two additions:
1. action described additional component: set linear continuous
numeric effects, cont, form hv, ki, k <, denoting increases v rate k
per unit time. corresponds PDDL 2.1 effect (increase (v) (* #t k)).
2. start end effects actions (eff n` eff na may, additionally, include parameter
?duration, denoting duration action, hence written:
hv, op, w v + k.(?duration) + ci s.t. op {+=, =, -=}, c, k <
temporal-metric-continuous problems relationship time numbers complex temporal-metric problems. first extension allows value variable v depend
length time elapsed since continuous effect acting upon began. second extension implies that, ?duration fixed, value variables depend duration
assigned action. fact , planners allow literal ?duration appear effects,
even actions value parameter constrained take single fixed value
duration constraint (e.g. (= ?duration 10)). typical idiom name intended value
duration metric fluent initial state (e.g. (= (durationOfAction) 10)) use
fluent effects.
6

fiC OLIN : P LANNING C ONTINUOUS C HANGE

(:durative-action saveHard
:parameters ()
:duration (= ?duration 10)
:condition
(and (at start (canSave))
(over (>= (money) 0)))
:effect
(and (at start (not (canSave)))
(at end (canSave))
(at start (saving))
(at end (not (saving)))
(increase (money) (* #t 1))))

(:durative-action lifeAudit
:parameters ()
:duration (= ?duration (patience))
:condition
(and (at start (saving))
(at end (boughtHouse))
(at end (>= (money) 0)))
:effect (and (at end (happy)))))

(:durative-action takeMortgage
:parameters (?m - mortgage)
:duration (= ?duration (durationFor ?m))
:condition
(and (at start (saving))
(at start (>= (money) (depositFor ?m)))
(over (<= (money) (maxSavings ?m))))
:effect
(and (at start (decrease (money) (depositFor ?m)))
(decrease (money) (* #t (interestRateFor ?m)))
(at end (boughtHouse))))

Figure 1: Actions Borrower Domain.

Temporal-metric-continuous problems form significant subset problems expressible
PDDL + language (Fox & Long, 2006), including linear continuous change within durative
actions. problems include non-linear continuous change, explicitly represent
events processes, although use certain modelling tricks capture similar behaviours.
4.1 Example Problem
running example temporal-metric-continuous domain use problem shown Figure 1. this, Borrower Domain, borrower use mortgage buy house. domain
simplified order focus attention key aspects continuous reasoning proposed realistic application. Furthermore, domain exploit variable duration actions,
even though ability handle key feature COLIN. example illustrates required
concurrency, means interesting interactions multiple actions affecting single continuous variable, allows us demonstrate differences alternative heuristics described
Section 9. Management required concurrency key feature COLIN, domains
variable durations discussed later paper.
domain, obtain mortgage necessary appropriate active savings plan
able lay deposit. conditions achieved saving hard, action
cannot applied parallel itself, preventing borrower building capital
arbitrarily high rate multiple parallel applications saveHard. sake example
restrict saving periods durations 10 years produce interesting interactions
7

fiC OLES , C OLES , F OX & L ONG

(:objects shortMortgage longMortgage - mortgage)
(:init (= (money) 0)
(canSave)
(= (patience) 4)
(= (depositFor shortMortgage) 5)
(= (durationFor shortMortgage) 10)
(= (interestRateFor shortMortgage) 0.5)
(= (maxSavings shortMortgage) 6)
(= (depositFor longMortgage) 1)
(= (durationFor longMortgage) 12)
(= (interestRateFor longMortgage) 0.75)
(= (maxSavings longMortgage) 6))
(:goal (and (happy)))
(:metric minimize (total-time))

Figure 2: example problem Borrower Domain.

durations mortgages sample problem. person starts saving tied
10-year savings plan.
constraint able start mortgage leads required concurrency saving
taking mortgage. effects saving repaying interest therefore combine yield
different linear effects value money variable, saving action requires
variable remain non-negative throughout duration saveHard action. Furthermore,
order qualify tax relief, mortgage carries maximum allowed level savings throughout
mortgage (which prevents mortgage taken late savings plan). Finally,
lifeAudit action places constraint gap end saving action
point mortgage completed (and ensures borrower end
debt). action acknowledges borrowers happy manage complete
mortgages within short periods (limited patience) save hard.
simple problem instance consider shown Figure 2. Two possible solutions
shown Figure 3. first solution borrower takes longer mortgage,
advantage start earlier requires lower deposit. Money rises rate 1
first part saving action, decreases 1 mortgage starts. rises rate
0.25 (the difference saving mortgage rates) saving action concludes,
continues decrease rate 0.75 mortgage ends. life audit action must start
saving action cannot end end mortgage action. second solution
borrower takes shorter mortgage, cannot start early requires much larger
deposit. consequence, life audit cannot start first saving action: mortgage
finishes late included inside life audit beginning within first saving action. meet
initial condition life audit, borrower must therefore perform second saving action
follow first. Clearly first solution preferable since interested minimising
makespan.
8

fiC OLIN : P LANNING C ONTINUOUS C HANGE

money

12 units
1 unit
takeMortgage longMortgage
10 units
saveHard
lifeAudit
4 units

money

10 units
takeMortgage shortMortgage
5 units
10 units

10 units

saveHard

saveHard
lifeAudit
4 units

Figure 3: Possible solutions Borrower problem.

5. Background Metric Temporal Planning
recent work discrete numeric planning built ideas introduced planner MetricFF (Hoffmann, 2003). discrete numeric planning problem introduces numeric variables
planning domain hold real numeric value (or undefined, yet
given value). Actions conditions expressed terms variables, effects
act upon them. provide heuristic guidance, Metric-FF introduced extension relaxed
planning graph (RPG) heuristic (Hoffmann & Nebel, 2001), Metric RPG heuristic, supporting
computation relaxed plan problems involving discrete numeric change.
propositional RPG heuristic, performs forwards-reachability analysis delete effects
actions relaxed (ignored). numeric effects, ignoring decrease effects always
relax problem, conditions require variable hold value less given constant.
Thus, reachability analysis extends forwards, upper- lower- bounds values
numeric variables computed: decrease effects effect upon upper bound increase
effects effect upon lower bound, assignment effects replace value upper
(lower) bound incumbent lower (greater) value (respectively) would
assigned. Deciding whether precondition satisfied given layer performed (optimistically)
9

fiC OLES , C OLES , F OX & L ONG

basis these: condition w v c1 , optimistically high value w v
computed using upper bound fluent v assigned value v corresponding
weight w positive, or, otherwise, using lower bound.
alternative use Metric RPG proposed LPRPG (Coles, Fox, Long et al.,
2008b), linear program constructed incrementally capture interactions
actions. approach restricted actions linear effects, general Metric-FF,
provides accurate heuristic guidance handling metric problems perform
significantly better problems metric resources must exchanged one another order
complete solution.
Numeric planning gives opportunity define metric optimisation functions terms
metric variables within problem description. example, objective minimise fuel consumption defined domains quantity fuel available metric variable.
optimisation function include special variable total-time, representing makespan
(execution duration) plan. planners restricted weighted sum across variables
(although PDDL 2.1 syntax allows unrestricted expression across variables). general,
planners yet capable optimising metric functions effectively: task finding plan
remains difficult. However, planners attempt optimise functions,
notable LPG (Gerevini & Serina, 2000) (and, domains numeric effects
count action cost, LAMA, due Richter & Westphal, 2010).
Although introduction PDDL 2.1 led increased interest temporal planning, earlier
work planning time influential. IxTeT (Ghallab & Laruelle, 1994) introduced
chronicles, consisting temporal assertions constraints set state variables, timelines chronicles single state variables. Timelines since widely used
planners followed different trajectory development led PDDL family languages (Pell, Gat, Keesing, Muscettola, & Smith, 1997; Frank & Jonsson, 2003; Cesta,
Cortellessa, Fratini, & Oddi, 2009). IxTeT pioneered use many important techniques,
including simple temporal networks linear constraints.
language introduced planner Temporal Graph Plan (TGP) (Smith & Weld, 1999)
allowed (constant) durations attached actions. semantics actions required
preconditions, pre, true entire duration action, effects actions,
eff, become available instantaneously ends. values affected variables treated
undefined inaccessible execution, although intended semantics (at least TGP)
values considered unobservable intervals and, therefore, plans
conformant respect possible values variables intervals. GP
solves problems using temporally extended version Graphplan planning graph (Blum
& Furst, 1995) reason temporal constraints. temporal heuristic effective form
temporal planning developed Haslum Geffner (2001) Vidal Geffner (2006)
explored constraint propagation approach handling problems.
Even using expressive temporal model defined PDDL 2.1, many temporal planners make use restricted TGP semantics, exploiting simplification PDDL 2.1 encoding
known action compression. compression performed setting pre weakest
preconditions actions, eff + (eff ) strongest add (delete) effects. propo1. Conditions w v c rewritten form negating sides. Further, stating w v = c
rewritten pair conditions, w v c (w v) c

10

fiC OLIN : P LANNING C ONTINUOUS C HANGE

Action
Q, P

P



P
Action B
R
P

R

Q,S

Action C

Action


T, G

Ordering achiever precondition
Ordering deleter precondition

Figure 4: problem SAPA.
sitional case, terms action representation introduced earlier, are:
pre = pre ` ((pre pre ) \ eff +
`)

+
eff + = (eff +
` \ eff ) eff
+

+
eff = ((eff
` \ eff ` ) eff ) \ eff

Many modern temporal planners, MIPS - XXL (Edelkamp & Jabbar, 2006) earlier versions LPG (Gerevini & Serina, 2000), make use action compression technique. However,
applying compression lead incompleteness (Coles, Fox, Halsey, Long, & Smith, 2008)
(in particular, failure solve certain temporal problems). issues surrounding incompleteness
first discussed reference planner CRIKEY (Fox, Long, & Halsey, 2004) and, later,
problem structures causing said introduce required concurrency (Cushing, Kambhampati, Mausam, & Weld, 2007). Borrower domain one example problem
compression prevents solution. lifeAudit takeMortgage actions initial
preconditions satisfied inside interval saveHard action, since action
adds saving start, deletes end.
Required concurrency critical ingredient planning continuous effects,
change occurs change occurs important throughout execution actions. order avoid producing poor quality plans or, indeed, excluding possible solutions, must allow
concurrency actions wherever problem description permits it. nave extension
compression approach would discretise continuous numeric change step function effects
occurring ends relevant actions, precluding possibility managing interaction
numeric variables execution actions continuous effects. therefore build
approach planner capable reasoning required concurrency. Borrower domain, mortgage action must overlap saving action, cannot early (to meet
deposit requirement) late (to meet maximum savings constraint ensure
life audit performed early possible). example illustrates, problems include
reasoning continuous linear change typically require concurrency.
Several planners are, currently, capable reasoning PDDL 2.1 startend semantics,
opposed relying compression approach. earliest PDDL 2.1 planner reasons successfully semantics VHPOP (Younes & Simmons, 2003), partial-order planner.
11

fiC OLES , C OLES , F OX & L ONG

planner depends heuristic guidance based relaxed planning graph used
FF, guidance fail problems required concurrency. Nevertheless, search space
explored VHPOP includes interleavings action start end points allow solution
problems required concurrency. V HPOP suffers problems encountered
earlier partial-order planners performance scales poorly many domains. PSYS (Garrido,
Fox, & Long, 2002; Garrido, Onainda, & Barber, 2001) Graphplan-inspired planner
produce plans domains required concurrency. Time represented successive layers
graph, using uniform time increment successive layers. approach similar way
TGP uses plan graph represent temporal structure, TPSYS supports model actions
separates start end effects actions dictated PDDL 2.1 semantics.
Another planner adopts Graphplan-based approach temporal planning LPGP (Long
& Fox, 2003a), case time successive layers variable. Instead using layers
graph represent passage fixed-duration increments time, used represent
successive happenings time points state changes occur. time successive state changes allowed vary within constraints imposed action durations whose end
points fixed particular happenings. linear program constructed, incrementally, model
constraints solution program interleaved selection action choices.
approach suffers weaknesses Graphplan planner: exhaustive iterative
deepening search impractical large problems, computation storage mutex relations becomes expensive larger problems. Nevertheless, LPGP provides useful approach
treatment PDDL 2.1 durative actions, splitting end points treated
instantaneous snap actions. solution (original) planning problem expressed
terms these, subject four conditions:
1. start snap-action paired end snap-action (and end applied without
corresponding start applied earlier);
2. start end action, invariants action pre respected;
3. actions must currently executing state considered goal state;
4. step plan occurs preceding step, time start end
action respect duration constraints.
APA (Do & Kambhampati, 2003) one earliest forward-search planners solve temporal PDDL 2.1 problems. works priority queue events. durative action started
end point queued time future executed. choice points
planner include starting new action, special wait action, advances time
next entry queue, corresponding action end point executed. allows SAPA
reason concurrency solve problems required concurrency. Unfortunately,
search space include necessary interleavings achieve complete search. example,
consider problem illustrated Figure 4. solve problem, action must start, action
B must start early enough allow C complete ends (and deletes P ) late enough
action start B ends end ends. actions required order
allow applied, achieving goal G. SAPA starts action A, queue contain
end A. choices open start B immediately, end early
allow execute successfully, else complete A, advances time far allow B
12

fiC OLIN : P LANNING C ONTINUOUS C HANGE

Light Match
Light
Unused Match

Light
Unused Match

Light

Mend Fuse
Needs Fixing Fuse

Light

Needs Fixing Fuse

Fixed Fuse

Figure 5: Required Concurrency

exploit effect P A, preventing C executed. fact, simpler problem defeats SAPA:
B end condition Q instead end effect G C dispensed
with. However, additional complexity existing example impossible infer
start B examination B alone, timing constraints start B
depends actions C immediately obvious temporal constraints
affect placement B. difficulty adopting waiting approach hard
anticipate long wait next interesting time point depends interaction actions
yet even selected.
different approach forward-search temporal planning explored CRIKEY family
planners (Coles, Fox, Halsey et al., 2008; Coles, Fox, Long et al., 2008a). planners use
action splitting approach used LPGP, work heuristically guided forward search.
heuristics planners use relaxed planning graph starting point (Hoffmann & Nebel,
2001), extend adding guidance temporal structure plan, pruning
choices easily demonstrated violate temporal constraints inferring choices
temporal constraints imply them. planners use Simple Temporal Network model solve
temporal constraints action end points accumulated successive
action choices. Split actions used extend LPG temporal version respects
semantics PDDL 2.1 (Gerevini, Saetti, & Serina, 2010) (earlier versions LPG use compressed action models described above). Recent work Haslum (2009) explored ways
heuristics temporal planning constructed, remaining admissible.
Temporal Fast Downward (Eyerich et al., 2009), based Helmerts Fast Downward planner (Helmert, 2006), uses approach slight refinement compressed action model,
allowing required concurrency managed. authors demonstrate planner
solve Match problem shown Figure 5. mistakenly claim SAPA cannot solve
problem cannot consider applying action starting ending lighting
match: fact, SAPA apply mend fuse action match lit, much
way done Temporal Fast Downward. problem planners face situations
action must started time last happening, next queued
event: neither planner includes choice search space.
Huang et al. (2009) developed temporal planner exploiting planning-as-SATisfiability
paradigm. uses Graphplan-to-SAT encoding, starting LPGP action-splitting compilation, using fixed time increment successive layers graph. approach
13

fiC OLES , C OLES , F OX & L ONG

adequate problems appropriate time increment identified, possible, general, time-dependent effects domain. Furthermore, approach
ineffective significant difference durations actions, time increment becomes short relative actions. planner produce optimal (makespan)
plans using iterative deepening search. planner combines existing ideas achieve objectives
mainly interest relationship SAT-based approaches temporal
planning, TM - LPSAT discussed below.
C RIKEY 3, planners mentioned, capable solving simple temporal
planning problems described above. restricted management discrete change.
Duration-dependent change cannot handled planners. fact, planners
manage kind reasoning numbers outside durations actions. COLIN therefore
significantly extends competence PDDL-compliant temporal planners.

6. C RIKEY3: Forward-Chaining Temporal Planner
Temporal forward-chaining planners two kinds choices make construction
plans. Firstly, non-temporal case, choice must made actions apply (these
choices considered planning element problem). Secondly, choices must
made apply actions (these seen scheduling choices construction solutions). CRIKEY 3 (Coles, Fox, Long et al., 2008a), temporal forward-chaining planner,
exploits distinction choices, using separate procedures make planning decisions (which actions start end) scheduling decisions (when place actions
timeline). decisions must checked consistency respect existing
temporal constraints confirm actions completely scheduled. section,
briefly describe CRIKEY 3 performs planning scheduling, since architecture forms
basis COLIN work subsequently described paper. Full details temporal
management CRIKEY 3 provided Coles et al.
CRIKEY 3 uses forward-chaining heuristic state-space search drive planning decisions.
makes use Enforced Hill-Climbing (EHC) algorithm introduced (Hoffmann & Nebel,
2001) repeated, convenience, Algorithm 1. EHC incomplete, solution cannot
found CRIKEY 3 plans again, using weighted A* search. discuss search described within basic enforced hill-climbing algorithm extended perform temporal
planning. order this, number modifications required. particular:
1. get applicable actions(S): planner must reason two actions per durative action,
start action end action, rather applying action immediately considering
finished (as non-temporal case).
2. get applicable actions(S), apply(a, S): invariant conditions durative actions must
maintained throughout execution, requires active invariants recorded
state order prevent application actions conflict them.
3. goal state(S): state goal state (i.e. path solution plan)
actions must completed.
14

fiC OLIN : P LANNING C ONTINUOUS C HANGE

Algorithm 1: Enforced Hill-Climbing Algorithm
Data: P = hA, I, Gi - planning problem
Result: P , solution plan
1 best heuristic evaluate heuristic(I);
2 best heuristic = 0
3
return [];
4 closed {I};
5 open list [hI, []i];
6 open list 6= []
7
hS, P element removed front open list;
8
applic(S) get applicable actions(S);
apply helpful filter (applic(S));
9
10
foreach applic(S)
11
0 apply(a, S);
12
0 6 closed
13
add 0 closed ;
14
P 0 P followed a;
valid plan(P 0 )
15
16
goal state(S 0 )
17
return P 0 ;
18
19
20
21
22
23
24
25

26

h evaluate heuristic(S 0 );
h < best heuristic
open list [hS 0 , P 0 i];
best heuristic h;
break;
else
h <
append hS 0 , P 0 onto open list;

return f ailure;

4. valid plan(P ): temporal (scheduling) constraints candidate plans must respected.
particular, duration constraints durative actions must satisfied. discussed
Section 6.1.
consider modifications turn. First, durative actions compiled
two non-temporal actions. modified version LPGP action compilation (Long & Fox,
2003a) used this, described Coles et al. (2008). durative action a, form
hdur , pre ` , eff ` , pre , pre , eff i, split two non-temporal (in fact, instantaneous) snap
actions form hpre, eff i:
a` = hpre ` , eff `
aa = hpre , eff
15

fiC OLES , C OLES , F OX & L ONG

performing search snap actions, taking appropriate care ensure
constraints satisfied, restrictions expressivity imposed use action compression avoided. becomes possible search plan start end points
different actions coordinated, solving problems required concurrency. price
search space much larger: original action replaced two snap-actions,
length solution plans doubled. circumstances blow-up avoided
identifying actions compression safe (Coles, Coles, Fox, & Long, 2009a), i.e.
use action compression compromise soundness completeness.
approach described Coles et al., actions still split start end snap-actions,
end points compression-safe actions inserted either effects needed
invariants would otherwise violated another action chosen application. consequence,
one search decision point needed per compression-safe action (choose apply start),
rather two. Recent versions CRIKEY 3 COLIN make use restricted action
compression technique search.
split actions start end points, modifications basic search algorithm
needed handle constraints arise consequence. CRIKEY 3 makes use extended
state representation, adding two elements state tuple. resulting state defined
= hF, P, E, i, where:
F represents facts hold current world state: set propositions
currently true, W , vector, v, recording values numeric variables.
P ordered list snap actions, representing plan reach initial state.
E ordered list start events, recording actions started yet finished;
collection temporal constraints actions plan reach F .
purpose start event list E record information currently executing
actions, assist formation sound plans. entry e E tuple hop, , dmin, dmax
where:
op identifier action, start snap-action op` added plan;
index snap-action added plan reach S;
dmin, dmax minimum maximum duration op, determined state
op started.
minimum maximum duration action depend state applied
(e.g. duration recharge action may depend level charge time execution),
durations must computed based state preceding step i. However, given action
started, bounds duration remain fixed. PDDL 2.1 allows actions durations
constrained conditions hold end action, actions supported
planners.
extended state definition leads corresponding extensions get applicable actions(S).
before, snap-action deemed logically applicable state preconditions pre
satisfied S. However, additional condition must satisfied: effects must violate
16

fiC OLIN : P LANNING C ONTINUOUS C HANGE

active invariants. invariants active given state determined E denote
invariants state event list E as:
inv(S) = e.op.pre
eE

apply end snap-action, aa , required entry e E whose operator entry
op equal a. prevents planner attempting apply ends actions
yet started.
Assuming action, a, found applicable chosen step plan, function
apply(a, S), applied temporally-extended state, S, yields successor 0 = hF 0 , P 0 , E 0 , 0 i.
first two elements updated non-temporal case: F 0 = apply(a, F ), P 0 = P +[a].
obtain 0 , begin setting 0 = . Furthermore, > 0:
0 = 0 { t(i) t(i 1)}
t(i) variable representing time step scheduled executed.
is, new step must come least (a small unit time) preceding step. separation
respects requirement interfering actions must separated least (Fox & Long, 2003),
strictly stronger required actions actually mutually exclusive.
accurate realisation PDDL 2.1 semantics could implemented, would incur cost
offering little apparent benefit. Finally, resulting value E 0 (and whether 0 changed
further) depends whether start end snap-action:
start action a` applied, E 0 = E + [ha, i, dmin, dmax i], dmin dmax correspond lower- upper-bounds duration a, evaluated context
valuation F .
end action aa applied, start entry {e E | e.op = a} chosen, E 0
assigned value E 0 = E \ e. often case one instance
action open, one choice pairing, case multiple instances
action executing concurrently, search branches choice
e. e chosen, final modification made 0 encode duration constraints
action finished:
0 = 0 {e.dmin t(i) t(e.i) e.dmax }
information encoded state currently executing actions, extension
needed goal state(S) minor: state goal state satisfies non-temporal version
goal state(S), event list state, E, empty.
search strategy leads natural way handle PDDL 2.2 Timed Initial Literals (TILs)
directly. Dummy TIL actions introduced, comprising effects TILs time
point, added plan earlier TIL actions already added,
delete invariants open action. special case, TIL actions create
entry E: facts F amended execution. do, however, produce
updated set temporal constraints. snap actions, TIL added step plan,
TIL must fall earlier preceding step. Then, 0 = 0 {ts t(i) t() ts},
17

fiC OLES , C OLES , F OX & L ONG

ts time-stamp TIL prescribed happen, name denoting
start plan t() = 0. seen, constraints ensure TIL occur
appropriate time, step prior TIL must occur it, step
TIL must occur it.
changes described subsection ensure plans produced CRIKEY 3 logically sound: check logical applicability, coupled maintenance E throughout
search, ensures preconditions, either propositional numeric, broken. Use
get applicable actions(S) guarantees actions logically applicable: guarantee adding snap-action plan, judged applicable way, violate
temporal constraints. example, possible preconditions satisfied plan
P = [a` , b` , ba , aa ], P logically sound. However, duration b greater
duration P temporally sound. next section discuss function
valid plan(P ) modified identify reject temporally inconsistent plans.
6.1 Temporal Plan Consistency
state temporally consistent steps [0...n 1] plan, P , reaches
assigned values [t(0)...t(n 1)], representing times execution corresponding
steps, respecting temporal constraints, . checked use valid plan(P 0 ),
called line 15 Algorithm 1 function call trivial non-temporal case,
temporal case serves check temporal consistency plan. state temporal
constraints cannot satisfied immediately pruned search, since extension action
sequence lead solution plan valid.
temporal constraints built CRIKEY 3 state expressed form:
lb t(b) t(a) ub

lb, ub < 0 lb ub

constraints conveniently expressible Simple Temporal Problem (STP) (Dechter,
Meiri, & Pearl, 1989). variables within STP consist timestamps actions,
inequality constraints specified form. Crucially, purposes,
validity STP (and assignment timestamps events therein) determined
polynomial time solving shortest-path problem within Simple Temporal Network (STN),
directed-graph representation STP. event STP represented vertex
STN. additional node t() represent time 0 time first action
plan, t(0), constrained fall within t(). constraint form adds two edges
graph: one b weight ub, one b weight lb. Attempting
solve shortest-path problem t() event yields one two outcomes: either
terminates successfully, providing time-stamp step, terminates unsuccessfully due
presence negative-cost cycle within STN indicating temporal inconsistency (any
schedule would require least one step scheduled itself).
CRIKEY 3, STP used check temporal consistency choices made reach
step S, based temporal constraints must hold plan P reach S,
additional constraints determined E: list actions started, yet
finished. variables vars STP partitioned two sets: variables, t(i)
step P f variables, one f (i) entry hop, i, dmin, dmax E. variables
correspond times steps already added plan, might times
18

fiC OLIN : P LANNING C ONTINUOUS C HANGE

start end points actions. time points might correspond starts actions
yet finished subset actions (only) associated f variables
associated pending end times actions. consistency terminology
introduced CRIKEY 3 (Coles, Fox, Long et al., 2008a), use refer time
next event plan occur (which could execution last actions applied).
time point next choice made, either start new action
completion existing one, therefore seen time associated final state,
S, generated current plan head. ever one timepoint called value
moves forward plan head extends. constraints follows:
, constraining variables ensure temporal consistency steps
plan reach (and include constraints introduced timed initial literals);
{dmin f (i) t(i) dmax | hop, i, dmin, dmax E} is, future
action end point committed (but yet applied), recorded duration
constraint must respected;
{ f (i) t(n 1) | hop, i, dmin, dmax E} is, future action end point
must come last step current plan, ensure future.
t(now ) t(n 1) is, current time (the time next event
plan occur) least last event plan.
Solving STP confirms temporal consistency decisions made far. STP
cannot solved, state pruned: plan induced startend action representation temporally invalid. last two categories constraints particularly important:
without them, pruning could undertaken basis plan P reach S. Including
them, however, allows STP identify cases end point action never added
plan, would lead temporal inconsistency. goal states cannot contain
executing actions (i.e. E must empty), allows CRIKEY 3 prune states earlier
definitely path state end points added plan.
Timed initial literals easily managed STP using dummy TIL actions described
earlier. constraints dummy TIL action already applied included .
dummy TIL action yet occur automatically treated end action yet
applied. Thus, f variable added each, so, last step plan far
constrained come TIL event yet happen.

7. Planning Continuous Numeric Change
challenging variants temporal numeric problems combine two arrive problems time-dependent metric fluents. Although problems exhibiting hybrid discrete-continuous
dynamics studied research communities time, example, verification (Yi, Larsen, & Pettersson, 1997; Henzinger, Ho, & Wong-Toi, 1995; Henzinger, 1996),
timed automata capture exactly kind behaviour, relatively little work
continuous dynamics planning community.
PDDL 2.1 model mixed discrete-continuous change extends propositional state transition model include continuous change state variables. state transition system
19

fiC OLES , C OLES , F OX & L ONG

discrete changes transition instantaneously states. system particular state, continuous change occur state variables time passes. soon discrete
change occurs system changes state. PDDL + (Fox & Long, 2006) extended allow
exogenous events processes (controlled nature) well durative actions. leads
formal semantics based theory Hybrid Automata (Henzinger, 1996). action
causes discrete state change might trigger continuous process. continues time
event triggered leading new state. time later another action might taken.
Early work exploring planning continuous processes includes Zeno system Penberthy Weld (1994), processes described using differential equations. Zeno suffers
limitations partial order planners time, unable solve large
planning problems without significant aid carefully crafted heuristic function. importantly, fundamental constraint behaviour allow concurrent actions apply
continuous effects variable. imposes significant restriction kinds
problems solved, making Zeno much less expressive COLIN. constraint
follows, part, way model requires effects specified differential equations, rather continuous update effects, simultaneous equations must consistent
one another rather accumulating additive effects. authors say must specify
entire continuous behaviour interval [of durative action] semantics insist
continuous behaviours result direct, explicit action.
Another early planner handle continuous processes McDermotts PTOP system (McDermott, 2003), heuristic search planner, using regression-based heuristic. plausible
progression technique used within PTOP guide search sufficiently powerful recognise
interactions could prevent future application actions, thereby restricting scalability
problems form consider here. PTOP competed International Planning Competition 2004, solved small subset problems (although, interestingly,
solved involved expressive combination ADL temporal windows planner
could manage). PTOP interesting variant heuristic forward search approach, since
avoids grounding representation, using approach similar means-ends linear planning approach generate relaxed plan estimates number actions required achieve
goal given state.
7.1 TM-LPSAT
recently, Shin Davis developed TM - LPSAT (Shin & Davis, 2005), based earlier
LPSAT system (Wolfman & Weld, 1999). - LPSAT first planner implement PDDL +
semantics. implemented compilation scheme horizon-bounded continuous
planning problem compiled collection SAT formulas enforce PDDL + semantics,
together associated set linear metric constraints numeric variables. compiled
formulation passed SAT-based arithmetic constraint solver, LPSAT. L PSAT consists
DPLL solver LP solver. SAT-solver passes triggered constraints LP-solver,
hands back conflict sets form nogoods constraints cannot resolved.
solution horizon increased process repeats, otherwise solution decoded
plan. order support concurrency compilation exploits LPGP separation action
start end points. different versions TM - LPSAT exploiting different solvers: LPSAT
MathSAT-04 (Audemard, Bertoli, Cimatti, Kornilowicz, & Sebastiani, 2002)
20

fiC OLIN : P LANNING C ONTINUOUS C HANGE

exploited. novelty TM - LPSAT lies compilation decoding phases, since solvers
well-established systems.
compilation scheme TM - LPSAT implements full PDDL + semantics. Although
includes events processes, specific PDDL +, TM - LPSAT handle variable duration durative actions, durative actions continuous effects duration-dependent end-effects.
continuous effects concurrent actions quantity two time-points summed
actions active quantity period. Therefore, TM - LPSAT supports concurrent
updates continuous variables.
- LPSAT interesting approach, theory capable solving large class problems
varied continuous dynamics. However, reported empirical data suggests planner
slow unable solve problems requiring plans steps. possible
experiment publicly available implementation system.
7.2 Kongming
Hui Li Brian Williams explored planning hybrid systems (Li & Williams, 2008, 2011).
work focussed model-based control, using techniques based constraint reasoning.
continuous dynamics system modelled flow tubes capture envelopes
continuous behaviours (Leaute & Williams, 2005). dimensions tubes function
time (typically expanding allowed extend), requirement made
successive continuous behaviours must connected connecting start one tube (the precondition surface) cross-section preceding tube; i.e. intersection two spaces must
non-empty. relevant work area development planner Kongming,
described Li Williams.
Kongming solves class control planning problems continuous dynamics. based
construction fact action layers flow tubes, within iterative plan graph structure
introduced Graphplan (Blum & Furst, 1995). graph developed, every action produces
flow tube contains valid trajectories develop time. Starting feasible
region, actions whose preconditions intersect feasible region applied reachable states time point computed using state equations system. initial
state system variables single known values. valid trajectory must pass
sequence flow tubes, must meet constraints specified dynamics actions
selected. mutex relation used Graphplan extended continuous dynamics well
propositional fragment language. graph iteratively extended Graphplan,
search plan conducted successive extension.
plan-graph encoding problem continuous dynamics translated Mixed
Logical-Quadratic Program (MLQP). metric objective functions used planner optimise behaviour defined terms quadratic functions state variables. example
problem considered Li Williams (2008) 2-d representation simple autonomous underwater vehicle (AUV) problem AUV glide, ascend descend avoiding
obstacles. language used version PDDL 2.1 extended enable dynamics encoded.
continuous nature problem lies fact that, continuous action, AUV
one continuous range positions determined control system. Kongming
depends translation planning problems MLQPs constraints describing dynamics problem must linear. Since effects continuous actions involve product rate
21

fiC OLES , C OLES , F OX & L ONG

change time, one values treated variable. Kongming
rate change variable, time discretised, contrasts COLIN rates
change remain constant continuously variable length intervals. discretisation time
Kongming exploited support state updates within plan graph: successive layers graph
separated constant uniform time increment. approach suffers disadvantage
duration plan limited number happenings plan, since solver cannot
realistically solve problems tens layers plan graph.
Kongming support concurrent continuous updates state variable, so,
respect, PDDL 2.1 expressive extended language used Kongming. part
due difficulty resolving precisely semantics dynamics described
actions used Kongming. dynamic constraint specifies limits rate change
specific variable: unclear whether concurrent actions combined taking union
intersection bounds constraint specifies rate change given fluent.

7.3 UPMurphi
One recently developed planner uses PDDL 2.1 reasons continuous processes
UPMurphi (Penna, Intrigila, Magazzeni, & Mercorio, 2009). UPMurphi takes completely different approach considered far. Instead reasoning continuous change directly,
UPMurphi works guessing discretisation iteratively refining solution discretised problem validate original problem specification. iterative driver
coarseness discretisation, well planning horizon, making interestingly different
basic architecture TM - LPSAT.
UPMurphi begins continuous representation problem starts discretising it.
First actions discretised taking specific values feasible ranges. results
several versions action. UPMurphi explores state space, explicitly constructing
current discretisation. Plans constructed using planning-as-model-checking
paradigm (Cimatti, Giunchiglia, Giunchiglia, & Traverso, 1997): heuristic guide
search. plan found validated original continuous model, using
plan validator (Fox, Howey, & Long, 2005). invalid, discretisation refined
search resumes. UPMurphi fails find plan one discretisation starts finer grained
discretisation. Subsequent refinements lead ever denser feasible regions, increasingly
complex construct.
UPMurphi used build partial policies handle uncertainty likely arise
practice execution hybrid control plans. controller table initially synthesised,
consisting (state,action) pairs plan first constructs. However, table might lack
states could visited controller, robust. subsequent step
robustify controller randomly perturbing states finding new paths
new states. perturbed states reachable, probability distribution
used identify likely ones. called safe states. controller table
extended safe (state, action) pairs. controller table, policy, referred
Universal Plan.
22

fiC OLIN : P LANNING C ONTINUOUS C HANGE

7.4 Approaches Continuous Reasoning
completely different way manage continuous quantities model continuous resource consumption production terms uncertainty amount consumed produced.
approach taken HAO* algorithm (Meuleau, Benazera, Brafman, Hansen, & Mausam,
2009) Markov Decision Process (MDP) constructed consisting hybrid states.
state contains set propositional variables collection distributions resource
consumption production values. states hybrid, standard value iteration approaches cannot used find policies. hybrid AO* approach described used
find best feasible policy. feasible region constructed HAO* continuous distribution
resource values resource considered uncontrollable (unlike Kongming,
assumed executive maintains control values region eventually
chosen).
Planning continuous processes important applications and, many application areas planning, led development systems combine generic planning
technology carefully tuned domain-specific performance achieve necessary combination problem coverage performance. good example work Boddy
Johnson (2002) colleagues (Lamba et al., 2003) planning oil refinery operations. work
uses quadratic program solver, coupled heuristically guided assignment discrete decision
variables (corresponding actions), solve real problems.

8. COLIN: Forward Chaining Planning Continuous Linear Change
section describe CRIKEY 3 extended reason duration-dependent
continuous numeric change, building planner COLIN ( COntinuous LINear dynamics).
decided give planner specific name highlight capabilities. demonstrated Section 4.1, key difference introduced continuous numeric change logical numeric
constraints longer neatly separated temporal constraints: values numeric
variables state depend timestamps durations actions, vice versa. relative
benefits handling temporal numeric constraints together, rather separating out,
apparent motivating domains outlined Section 3 amply rehearsed
paper describing PDDL + (Fox & Long, 2006).
need cope integrated numeric temporal constraints raises number important
issues planning domains. First, checking whether action choice consistent
longer achieved using STP, numeric constraints interact temporal
constraints, STP sufficiently expressive capture this. Second, changing values
numeric variables time brings new challenges determining action applicability:
precondition satisfied immediately following application action, might become
satisfied allowing certain amount time elapse. Finally, need provide
heuristic guidance. cover first two issues section, defer discussion
heuristic guidance next.
8.1 Temporal-Numeric Plan Consistency Linear Programming
begin problem temporal-numeric plan consistency, techniques used dealing
issue amended use solving issues encountered determining
23

fiC OLES , C OLES , F OX & L ONG

action applicability. Considering definition STP given Section 6.1, make observation STP could equally well written linear program (LP). CRIKEY 3,
STP efficiently solved using shortest-path algorithm. However, observation becomes
important wish reason continuous change numeric resources alongside temporal constraints. case, use LP capture temporal constraints numeric
constraints, including interaction two. describe LP built,
serving replacement valid plan(S) function called search, invokes
STP solver CRIKEY 3. diagram structure LP create shown Figure 6,
plan P = [a0 , ..., an2 , an1 ] reach state S, an1 action recently added
plan. (For simplicity, shows case event queue E empty.)
construction LP begins variables (a subset of) constraints
STP. STP variable ti (the time-stamp (snap) action ai ) corresponding LP variable
stepi (shown across top Figure 6), STP variable ei (for future end action
step i) corresponding LP variable estep . construct constraints corresponding
total-ordering action steps, STP: step P still sequenced (i.e.
stepi stepi1 n > > 0), future end snap-action later stepn1
(i.e. estepi stepn1 estep variables).
extend LP numeric constraints problem, beginning effects
actions. Since numeric effects discrete continuous, create two additional
vectors variables per step plan. first these, vi , represents values state
variables v immediately prior ai executed (in case step 0, vi equal values
v initial state, I). second, vi0 , contains values v immediately ai executed.
Figure 6, variables v0 enumerated v0 ...vm1 and, similarly, v0 0 shown
0
v00 ...vm1
. avoid proliferation indices index values
time stamp Figure 6, vi ith value v time step corresponding layer
variable appears. use two vectors layer required order represent
discrete changes caused actions: snap-action cause value variable different
immediately execution. represent within LP, action step effect
variable v vi0 = vi 2 . Otherwise, discrete effect hv 0 +=w v + k.(?duration) + ci,
constraint introduced define value vi0 :3
vi0 = vi + w v + k.(ce(i) cs(i)) + c
functions cs(i) ce(i) denote time-stamp variables corresponding start
end action step i. step end action, ce(i) = step , cs(i)
step variable start action finished step i. Similarly, step initiates action,
cs(i) = step , ce(i) either estep action yet finished or, otherwise,
step variable end action started step i. Therefore, substituting ce(i) cs(i)
?duration captures relationship effect action duration.
2. Note identities implemented efficiently simply introducing unnecessary additional
variable. Similarly, variable subject effects conditions added LP,
introduced becomes relevant.
3. effects using operator -=, i.e. decrease effects, first term right-hand side negated.
assignment effects, operator =, first term right-hand side (i.e. vi ) omitted entirely (the value
v assignment depend value v beforehand).

24

fiC OLIN : P LANNING C ONTINUOUS C HANGE

Metric fluents v 0 v m1
Snapactions 0 n1 corresponding timepoint variables

v0

v0

v0

v0

v0

v0

v0

v0

v1

v1

v1

v1

v1

v1

v1

v1

v

v

v

v

v

v

a0

v3

v3

v3
step 2

v3

v m1

v m1

State 2

State 1

v
n1

step n1

2

v3
...

v m1

2

...

v m1

2

a2

...

v m1

2

...


v m1

step 1

2

...

...

step 0

a1

v3

...

...
State 0

2

v3

v3

v m1

2

v m1
State n

...

2

...

v

Active continuous change affecting (some) variables
Actions cause instantaneous step changes fluent values
Temporal Constraints
Actions sequenced separated:
step i+1 stepi >=
action starts durative action, a, ended action j:
dmin(a) <= step j step <= dmax(a)
Metric Variable Constraints: Step Effects

Metric Variable Constraints: Continuous Effects

Variables updated action effects:

Variables updated active continuous effects:
v j = vj + vj (stepi+1 stepi )

v j state i+1 v j state updated effect
including timedependent stepeffects

values state i+1, v j determined
accumulated effects active continuous effects

Figure 6: Diagrammatic Representation LP Used COLIN. Note subscripts attached
v v 0 fluents diagram indices vector fluents state,
indices step represent different time steps plan. metric fluents
notionally indexed time step, shown diagram order
avoid clutter.

Continuous numeric change occurs steps plan, rather instant
execution step itself. capture continuous effects, building LP consider
step turn, start plan, recording gradient total (linear) continuous
change acting upon variable v v, v denotes gradient active ai1
execution action ai . restrictions language handled COLIN, described
Section 4, total-order constraints snap-actions, value variable vi
known constant within interval successive actions: continuous change
linear. gradient variable v changed either starting action (initiating
25

fiC OLES , C OLES , F OX & L ONG

adjustment prevailing continuous effect v given dv
dt += k, k <) ending
action (terminating effect initiated start). values constants computed
follows4 :
variables, v0 = 0; is, continuous numeric change active
variable start plan.
ai continuous numeric effect v vi+1 = vi ;
ai initiates continuous numeric effect,

dv
dt

ai terminates continuous numeric effect,

+= k, vi+1 = vi + k;
dv
dt

+= k, vi+1 = vi k;

basis values, add constraints LP:
vi+1 = vi0 + vi+1 (step i+1 step )
Again, distinction vi vi0 important: vi determined basis continuous
change interval steps 1, immediately prior discrete effect
may occur step.
created variables represent values fluents step introduced
constraints capture effects actions them, consider constraints arise
preconditions snap-action, invariants must respected starts
ends actions, constraints durations actions plan.
numeric precondition form hv, {, =, }, w v + ci, must hold order apply step i,
add constraint LP:
vi {, =, }w vi + c
action starting stepi ending stepj , invariants added LP
0
form, vectors variables [vi0 , vj1
] [vi+1 , vj ] (vi v0 j excluded
PDDL 2.1 semantics require invariants action hold end points).
case end action (starting i) yet appeared plan, invariants
imposed vectors variables vi0 onwards: must end future, invariants
must violated step current plan point started.
Finally, add duration constraints. action starting stepi , denote variable
corresponding time finishes ce(i), ce(i) = step j end action
inserted plan step j, ce(i) = estep otherwise (as defined above). Then,
duration constraint a, form h?duration, {, =, }, w v + ci, add constraint:
ce(i) step {, =, }w vi + c
process constructs LP captures numeric temporal constraints govern
plan, interactions them. STP CRIKEY 3, solution LP
contains values variables [step 0 ...step n ], i.e. assignment time-stamps actions
plan. prevent LP assigning variables arbitrarily large (but valid) values, set
4. Variables trivially shown constant (i.e. action effect referring variable)
removed LP replaced throughout values initial state.

26

fiC OLIN : P LANNING C ONTINUOUS C HANGE

Plan Action

Delta value LP Variable
m0 = 0

saveHard start

LP Constraints

step0

=0

m0

=0

m00
m1 = 1
takeMortgage start
m2 =

1
4

= m0

0

step1

step0 +

step0 + 10

m1

= m00 + 1.(step1 step0 )

0

m01

= m1 1

0

step2
m2

lifeAudit start
m3 =

1
4

saveHard end

= step1 +
=

m01

+

1
4 .(step2

step0 + 10 step1 + 12

step1 )

m02

= m2

step3

step2 +

m3

= m02 + 41 .(step3 step2 )

m5 = 0

6

6

step3 +
=

m03



3
4 .(step3

= step1 + 12 step2 + 4

step2 )

m04
lifeAudit end

6

0

= m3

step4
m4

takeMortgage end

0

= step0 + 10 step1 + 12
step2 + 4

m03
m4 = 34

6

= m4

step5

step4 +

= step2 + 4

m5

= m04 0.(step3 step2 )

0

m05

= m5

Table 2: Variables constraints Borrower problem
LP objective function minimise step n , last step plan far.
purposes valid plan(S) function, LP built plan P reach state cannot
solved, prune state search space need consider further:
path legal goal state. way, LP scheduler used replacement
STP order determine plan validity.
8.2 Example: LP Borrower Problem
order illustrate LP construction plan consider example Borrower problem introduced Section 4.1. Recall one solution plan problem following structure:
0:
1:
2:
3:
4:
5:

saveHard start
takeMortgage start longMortgage
lifeAudit start
saveHard end
takeMortgage end longMortgage
lifeAudit end.

LP six-step Borrower solution plan contains variables constraints shown
Table 2. six step variables represent time-stamps six snap-actions plan,
variable represents money saved Borrower. initial state, = 0,
27

fiC OLES , C OLES , F OX & L ONG

hence m0 = 0. Starting saveHard action instantaneous numeric effects, introducing
constraint m00 = m0 (if effect m, instance instantaneous increase
savings k, constraint would m00 = m0 + k). Due invariant condition
saveHard action, savings remain zero, constraint m00 0 added: seen
constraint duplicated mi m0i execution saveHard action,
ensure invariant continues hold. Notice, also, action takeMortgage started,
invariant action (the savings level remains less equal maxSavings cap)
appears, applies values execution. Additional constraints capture
discrete change connecting value m0i mi . cases example values
equal, one constraint shows discrete effect: m01 = m1 1 captures deduction
deposit caused initiating takeMortgage action.
previously described, temporal constraints LP take two forms. First,
constraints form step i+1 step + , forcing step i+1 follow step , enforcing sequencing snap-actions. Second, duration constraints restrict duration actions, e.g.
step3 = step0 + 10 forces step3 (the end point saveHard) occurs precisely 10 units (the
duration saveHard) step0 , start snap-action.
final constraints consider modelling continuous numeric change. first
constraint type gives value m1 execution saveHard start
execution takeMortgage start. constraint, m1 = m00 + 1.(step1 step0 ), based
value m1 , 1: action currently executing continuous change
saveHard, increases 1 per unit time. second constraint, m2 =
m01 + 41 .(step2 step1 ), based value m2 (1 34 ) = 14 , found adding
active gradients actions started yet finished. illustrates
two actions active linear continuous effects variable simultaneously. Note
saveHard end applied (at step3 ) gradient continuous change (m4 ) becomes
43 active continuous effect takeMortgage action.
Solving temporal constraints problem without considering metric fluents yields
solution step0 = 0, step1 = , step2 = 8 + 2, step3 = 10, step4 = 12 +
step5 = 12 + 2. Unfortunately, proposal violates constraint m01 0, since:
m01 = m1 1 = m00 + 1.(step1 step0 ) 1 = m0 + 1 = 0 + 1 = 1
1. constraint start time takeMortgage action cannot identified
dependent discrete initial effect action, active continuous effect
saveHard action invariant saveHard. simple example illustrates strength
using LP perform scheduling alongside resolution numeric constraints:
timestamps satisfy temporal numeric constraints.
8.3 TemporalNumeric Search
performing state-space search, state, S, snapshot world along plan trajectory, coming one action step another. absence continuous numeric change,
valuations define known precisely: propositions hold, values
numeric variables v. presence continuous numeric change, however,
hold: variable v undergoing continuous numeric change (or subject active durationdependent change) valuations state depend snap-actions applied far,
28

fiC OLIN : P LANNING C ONTINUOUS C HANGE

times snap-actions applied much time passed since
last action applied. Within representation state time-stamps snap-actions
plan fixed (during plan-construction, LP used confirm plan
scheduled subject current constraints), valuation numeric fluents constrained
within ranges determined constraints temporal variables interactions
them.
consequence flexibility commitment values temporal continuously
changing variables, COLIN requires different state representation one used CRIKEY 3.
Rather representing values numeric variables single vector v, use two
vectors: vmax vmin . hold maximum minimum values, respectively,
numeric variable S. computation bounds variables achieved using small
extension LP described Section 8.1. state S, reached plan P (where
last step P ), add another vector variables LP, denoted vnow , another time-stamp
variable, step . variables vnow represent values state variable point
(at time step ) along state trajectory following . numeric variables time-stamp
constrained additional action appended plan:
must follow previous step, i.e. stepnow stepn
must precede coincide ends actions started yet
finished, i.e. estep(i), estep(i) step
variable vnow vnow , compute value based continuous numeric
change:
vnow = vn0 + vnow (stepnow stepn )
Finally, every invariant condition hv, {, =, }, w v + ci action started
yet finished:
vnow {, =, }w vnow + c
LP used find upper lower bounds variables. variables vnow vnow , two calls made LP solver: one objective set maximise vnow ,
one minimise vnow . taken values vmax vmin S. simplest case, variable v subject (direct indirect) continuous duration-dependent
change, value v time-independent, vmax = vmin , value determined
successive application effects actions P , i.e. mechanism used
CRIKEY 3, indeed classical (non-temporal) planning.
Since upper lower bounds value variable, rather fixed assignment, action applicability function, get applicable actions(S), must modified. CRIKEY 3,
action said applicable state preconditions satisfied. COLIN, definition
means numeric precondition satisfied different. preserve completeness,
employ mechanism used metric relaxed planning graphs, discussed detail
Section B. Specifically, numeric precondition w x c, calculate optimistic value
w x using upper bound v x corresponding weight w positive, or, otherwise,
using lower bound. Then, resulting value greater equal c, precondition
considered satisfied. (As before, numeric conditions w x c, equivalent precondition appropriate form obtained multiplying sides inequality 1
29

fiC OLES , C OLES , F OX & L ONG

Plan Action

Delta value LP Variable
m0 = 0

saveHard start

LP Constraints

step0

=0

m0

=0

m00
m1 = 1
takeMortgage start
mnow =

1
4

= m0

0

step1

step0 +

step0 + 10

m1

= m00 + 1.(step1 step0 )

0

m01

= m1 1

0

step1 +

stepnow



mnow

=

m0now

m01

+

1
4 .(stepnow

= mnow

step1 )

6

step0 + 10 step1 + 12
0

6

0

6

Table 3: Variables constraints first stages Borrower Problem
constraints form w x = c replaced equivalent pair conditions w x c,
w x c.)
test applicability action relaxed, serves filter, eliminating actions
certainly inapplicable. instance, precondition + b 3 could satisfied upper
bounds b 2, even assignment timestamps actions within LP attain
= 2 conflicts needed attain b 1. rely subsequent LP consistency check
determine whether actions truly applicable. Nonetheless, filtering applicable actions
basis variable bounds state useful tool reducing number candidates
must individually verified LP.
8.3.1 E XAMPLE U SE B ORROWER P ROBLEM
briefly illustrate way variable constructed used context
Borrower problem. Consider situation selection first two actions (saveHard start
takeMortgage start). LP construction yields constraints shown Table 3. Solving
LP minimum maximum values stepnow gives values 1 + 10 respectively,
meaning earliest time third action applied 1 + latest
10.5 Similarly, solving LP minimum maximum values mnow gives bounds

4 6. information could, principle, constrain actions applied current
state.
8.4 Comments LP Efficiency
LP solved every node search space, important process made
efficient possible. adding variable vectors LP step i, necessary
consider state variable, v, become unstable prior step i, one
following effects acting it:
1. direct continuous numeric change, i.e. changing v according gradient;
5. practice, efficiency, COLIN actually solve LP minimum maximum values stepnow ,
uses variable communicate constraints metric variables state.

30

fiC OLIN : P LANNING C ONTINUOUS C HANGE

2. direct duration-dependent change, i.e. change v dependent duration action
(whose duration non-fixed);
3. discrete change, magnitude change based one variables
falling either previous two categories.
variables meet one conditions omitted LP, values
calculated based successive effects actions applied step i, substituted
constant within LP constraints referring them. reduces number state variables
constraints must added LP reduces number times LP must
solved state find variable bounds: irrelevant variables eliminated
vector vnow . similar simplification that, applying plan a0 ...an1 reaches state
vmin = vmax , continuous numeric change acting v, v become stable, i.e.
value independent times assigned preceding plan steps. case, first
step k v becomes unstable, value v determined simple application
discrete effects, hence v omitted vj , vj0 , n 1 < j.
opportunity exploit LP solved state similar solved
parent state: represents plan, extra snap-action appended end.
lower bounds time-stamp variables LP therefore based values computed
parent states. Suppose state expanded reach state 0 applying snap action, a,
step plan. point, LP corresponding plan built solved
objective minimise step . Assuming plan indeed scheduled (if cannot,
0 pruned successors generated it), value objective function
stored 0 lower bound time-stamp a. states subsequently reached 0 ,
stored value used LP lower bound step appending actions plan
constrain hence increase value step , never remove constraints
order allow decrease.
well storing lower bounds time-stamp variables, make use bounds
vmin ,vmax state 0 generating successors it. state reached via plan
length i, applying action leads state 0 new action step i+1 inherits
constraints imposed previously step calculating variable bounds 0 . Therefore,
values vmax vmin serve upper lower bounds (respectively) vi+1 LP
built determine feasibility 0 . Similarly, combine discrete numeric effects
values vmax vmin give bounds v0 i+1 . variable v subject
effect, optimistically large (small) outcome effect computed basis vmax
0 . Otherwise, variables upon
vmin , taken upper (lower) bound vi+1
0
discrete effect, vi+1 = vi .
Finally, presence timed initial literals (TILs) allows us impose stricter bounds
time-stamp variables. step j plan dummy action corresponding TIL time t,
upper bound step , < j, lower bound step k , j < k (or estep
variable) + . Similarly, plan yet contain step corresponding TIL time
t, upper bound step variables . Furthermore, TIL time corresponds
deadline deletes fact p present initial state, never added action,
never reinstated TIL. case:
plan step requires p precondition, step ;
31

fiC OLES , C OLES , F OX & L ONG

estep end action end condition p, estep ;
estep end action invariant condition p, estep t.

9. Heuristic Computation
search algorithms described far paper make use heuristic guide planner
efficiently search space towards goal. introduced necessary machinery
support linear continuous numeric duration-dependent effects turn attention
construction informed heuristic face time-dependent change.
Appendices B C revisit standard Metric-FF Relaxed-Planning Graph (RPG)
heuristic Temporal RPG (TRPG) used CRIKEY 3, provide details approaches reference. depend initial construction reachability graph,
based plan graph introduced Graphplan (Blum & Furst, 1995). graph consists alternating layers facts (f l) actions (al). TRPG, convenience, index layers
earliest time could represent, although still enumerated consecutive integers
finitely many times relevant process construction. section
explain heuristic computation techniques introduced planners modified
reason interacting temporalnumeric behaviour. describe two variants heuristic:
basic version, active continuous change relaxed discrete step changes, refined
variant relaxation replaced careful approximation continuous
values. show, using Borrower example, benefits refined approach.
heuristics based underlying use relaxed plan step-count. use relaxed
plan makespan tie-breaker ordering plans step-count. Step-count dominates
heuristic first priority find feasible solution planning problem means
attempting minimise number choices must made resolved search.
course, emphasis rapidly finding feasible plan compromise quality plan,
particularly problems step-count poorly correlated makespan. Subsequent
attempts improve quality initial feasible solution, either iteratively improving
solution search using bound derived feasible solution prune
search space, possible, consider work.
9.1 Basic Integrated Heuristic Computation Continuous Numeric Effects
first version COLIN (Coles, Coles, Fox, & Long, 2009b) introduced three significant modifications TRPG used CRIKEY 3, order generate heuristic values presence
continuous duration-dependent effects. first modification simply equips heuristic
means approximate effects continuous change.
action continuous effect equivalent dv
dt += k relaxed instantaneous
start effect hv, +=, k dmax (a)i. is, effect changing variable treated
integral effect upper bound duration action applied
start action. ensures behaviour relaxed, contrast to, say,
applying effect end action. dmax (a) calculated point
action added TRPG, based maximum duration constraints refer
variables cannot change time (that is, state-independent).
32

fiC OLIN : P LANNING C ONTINUOUS C HANGE

constraints exist, duration allowed infinite (and variables affected continuous
effects action similarly uninformed bounds).
action discrete duration-dependent effect variable v then, calculating
maximum (minimum) effect upon v (as discussed, non-temporal case, Appendix B), ?duration variable relaxed whichever dmin(a) dmax (a) gives
largest (smallest) effect. Relaxation effect achieved without changing timing,
associated start end action indicated action specification.
second modification affects action continuous numeric effect variable either end precondition invariant refers numeric variable.
invariant end precondition places constraint way process governed
action affect value variable, constraint reflected corresponding upper
lower bounds value variable. Specifically, action decreases v rate k
invariant end precondition v c, upper bound v end action must
least k.(dmin(a) elapsed (a)) + c, elapsed (a) maximum amount time
could executing state evaluated (0 currently executing, otherwise,
maximum entries E). condition ensures variable could achieve
necessary value support application action. might appear strange bound
set higher c, reason relaxation accumulates increase effects ignores
decrease effects assessing upper bound, necessary, end action,
accumulated increases value variable allow outstanding consumption
order still meet c bound end action. corresponding condition
required action increases v rate k, invariant end precondition v c,
lower bound v cannot k.(dmin(a) elapsed (a)) + c. conditions
added explicit additional preconditions aa purposes constructing TRPG.
third modification deals problem constructing appropriate initialisation
bounds numeric variables first layer TRPG. CRIKEY 3 values
initialised actual values metric variables, since values current state
change time passes without actions applied. true COLIN, since
actions started, yet finished, govern process, cause variables
change simply consequence time passing. basic heuristic proposed relies
able integrate continuous numeric change, determine variable bounds fl (0.0)
two stages. First, bounds variable v set according obtained LP
Section 8.3. Then, entry e E, corresponding start action, a,
continuous effect v positive gradient k, upper bound v f l(0.0) increased
k.remaining(e). Here, remaining(e) maximum amount time could elapse
state evaluated future end snap-action paired start event e. maximum
remaining execution time calculated subtracting lower bound amount time
elapsed since start action maximum duration. case
gradient negative, lower bound decreased.
9.2 Refined Integrated Heuristic
Time-dependent change arises two sources: continuous numeric effects, initiated start snapactions, discrete duration-dependent effects apply either end durative actions.
33

fiC OLES , C OLES , F OX & L ONG

purposes refined heuristic described section, treat continuous effects
discrete duration-dependent effects ends actions way, attaching
continuous linear effect acting relevant variable effects appropriate snap-action,
a, denoting set continuous effects g(a). continuous effects, cont(a), initiated
a` , cont(a) g(a` ). is, gradient effects start include continuous
effects a. duration-dependent effects end snap-action aa split effect two
parts:
discrete effect aa , hv, {+=, -=, =}, w v + k.dmin(a) + ci
gradient effect v, added g(aa ). effect defined hv, ki original effect used
operator += = otherwise, hv, ki.
Thus, instantaneously, end aa , effect available assuming smallest possible
duration used. executes greater duration, continuous effect applied
gradient change taken coefficient k ?duration variable
corresponding effect a.
Unfortunately, treatment proposed cannot applied duration-dependent start effects, since effects always available start action, regardless duration. Thus,
employ approach taken basic heuristic used COLIN: calculating maximum (minimum) effect a` affected variable, v, ?duration variable substituted
whichever dmin(a) dmax (a) gives largest (smallest) effect.
collection linear continuous effects, g(a), associated snap-action,
a, adjust construction TRPG. First, identify, variable, v, associated
maximum rate change, vmax (t), following layer al(t). set sum
positive rates change, affecting v, snap-actions al(t):
v max (t) =

X

X

aal(t)

hv,kig(a)

k

definition relies restriction one instance action execute
time. restriction hold, clear finite bound p(a) number instances
action execute concurrently, incorporate calculation v max (t)
follows:
X
X
v max (t) =
p(a)
k
aal(t)

hv,kig(a)

finite bound exists, action could, principle, applied arbitrarily many times
parallel hence set v max (t) = .6 Following layer al(t) v max (t) =
longer need reason upper bound continuous change v since upper bound
v become immediately layer. noted degradation
behaviour will, worst case, lead heuristic behaviour basic heuristic
where, again, arbitrarily many copies action execute concurrently, magnitude
increase decrease effects becomes unbounded. extension heuristic consider
6. note that, experience, presence infinitely self-overlapping actions continuous numeric change
often bug domain encoding: difficult envisage real situation parallel production
unbounded.

34

fiC OLIN : P LANNING C ONTINUOUS C HANGE

continuous effects refined way worsen guidance situation.
remainder section, consider variables whose values modified actions
finite bounds number concurrently executing copies allowed.
Armed upper bound value rate change variable following layer al(t),
deduce maximum value variable time t0 > t, simply applying
appropriate change maximum value variable time t. remaining challenge
decide far advance t0 construction TRPG. construction TRPG
CRIKEY 3 time constrained advance next action end point, depending
whether new facts available following recent action layer (lines 2934 Algorithm 2). order manage effects active continuous processes, add third possibility:
time advance earliest value accumulated effect active continuous change
variable satisfy previously unsatisfied precondition. set preconditions interest
always finite, so, assuming variable subject non-zero effect, bound
relevant advance always defined (or, set preconditions empty, advance required).
compute value time follows. numeric precondition may written
constraint vector numeric variables, v, form w v c, vectors constants w
c. define function ub follows:
X w[i] y[i] w[i] 0
ub(w, x, y) =
w[i] x[i] otherwise
w[i]w

upper bound w v t0 then: ub(w, vmin (t0 ), vmax (t0 )).
earliest point numeric precondition w v c become satisfied
smallest value t0 ub(w, vmin (t0 ), vmax (t0 )) c.
example, suppose action precondition x + 2y z c, w =
h1, 2, 1i (assuming x, z numeric fluents case). Substituting
previous equation yields:
ub(h1, 2, 1i, hx, y, zimin (t0 ), hx, y, zimax (t0 )) = 1.xmax (t0 ) + 2.ymax (t0 ) 1.zmin (t0 )
= 1.(xmax (t) (t0 ) + xmax (t + ))
+2.(y max (t) (t0 ) + ymax (t + ))
1.(z min (t) (t0 ) + zmin (t + ))
(The values x, z based starting points + accounts
instantaneous changes triggered actions al(t).) value t0 produced computation
infinite, maximum possible rate increase expression x + 2y z must zero.7
Otherwise, t0 time new numeric precondition first become satisfied due
active continuous effects and, earlier earliest point action end point
applied, next fact layer TRGP f l(t0 ).
9.2.1 MPROVING B OUNDS VARIABLES FACT-L AYER Z ERO
Previously, setting bounds fact-layer zero could thought consisting two stages:
finding initial bounds using LP then, passage time could cause bounds
diverge due active continuous numeric change, integrating change prior setting
7. find t0 requires simple rearrangement formula extract t0 directly.

35

fiC OLES , C OLES , F OX & L ONG

bounds layer zero TRPG. explicit model numeric gradients planning
graph, reconsider approach. intuition behind new approach
follows:
1. variable v, create associated variable tnow (v) LP, solve LP
minimise value variable.
2. Fixing value tnow (v) lower-bound, maximise minimise value v find
bounds point used bounds v fl (0.0).
3. v > 0 current state, vmax (t) values TRPG offset v or,
similarly, v < 0, vmin (t) values offset.
first steps based ideas described Section 8.3, process subtly
different trying determine bounds v given point time, rather
appear reachable. before, tnow (v) must still come recent plan step
used determine value v. reflected pair constraints:
tnow (v) step
vnow = vi0 + vnow (tnow (v) step )
Additionally, since variable associated single v, rather
appropriate v, constrain if, necessarily, v cannot referred (either
precondition, duration within effect) least certain steps plan, rather
weaker requirement recent step. purposes, observe
actions referring v require, delete add fact p, possible interaction p
require-delete-add form, tnow (v) must come plan step adds p.
formally, require-delete-add idiom holds p p true initial state, action
preconditions/effects p, interaction action p characterised
one following patterns:
+
1. p pre ` (a), p eff
` (a), p eff ` (a)
+
2. p pre (a), p eff
(a), p eff (a)
+
3. p pre ` (a), p eff
` (a), p eff (a)

(An action may exhibit either first two interactions, third.)
LP variable corresponding point p added, denote step p ,
determined one two ways. First, p present state evaluated, step p LP
variable corresponding plan step recently added p. Otherwise, case 4 above,
know p eff +
(a) action currently executing. case, step p LP
variable estep corresponding end a. defined variable, add constraint
LP:
tnow (v) step p +
Solving LP objective minimise tnow (v) finds earliest possible time
v referred to. Then, fixing tnow (v) minimised value, minimise maximise
36

fiC OLIN : P LANNING C ONTINUOUS C HANGE

bounds vnow . gives us bounds v appropriate early possible
actions plan far.
obtained variable bounds LP must, before, account fact
passage time causes bounds change active continuous numeric change. Whereas
integrated change prior TRPG, mechanism handling gradients directly TRPG expansion. Thus, start-event-queue entry e E corresponding
start action, A, continuous effect v positive (negative) gradient k,
add gradient effect upper (lower) bound v TRPG. previously restricted integrated effect e remaining(e), maximum remaining time action
must end, limit long gradient effect active: starts al(0.0) finishes
al(remaining(e)). Then, given fact layer value vmax (t) updated accordingly:
vmax (t)+=

XX

{k | hv, ki g(op(e)) k > 0 remaining(e)}

eE

Similarly, vmin(t) amended account effects hv, ki, k < 0.
9.3 Using Two Variants Integrated Heuristic Borrower Problem
illustrate computation two heuristic functions choice point Borrower
problem. example shows refined heuristic guides planner shorter makespan
plan basic heuristic, improved heuristic information leads selection
better choices helpful actions. Consider situation following execution first action,
saveHard start. Figure 7 (top) shows TRPG relaxed plan constructed using basic
heuristic.
heuristic generates cost state 5: four actions shown relaxed plan,
together extra one end saveHard action already started. relaxed plan
generates two helpful actions, start lifeAudit start takeMortgage short. attempt start lifeAudit action quickly dismissed temporally inconsistent, depending
boughtHouse becoming true ends, helpful action chosen. Unfortunately, action selected interaction saving process deposit
requirement (at least five savings must acquired) forces action start earlier
time 5. constraint invisible TRPG, continuous effect saveHard
abstracted start effect, full ten savings therefore appear available immediately.
plan constructed using short mortgage, introducing second saving action
shown lower plan Figure 3. start short mortgage pushed
late life audit cannot overlap end first saveHard action finish
mortgage action.
lower part Figure 7 shows happens refined heuristic used solve
problem. saveHard action starts before, time heuristic relax
behaviour continuous savings process long mortgage, requires smaller deposit
initiate it, becomes available short mortgage. consequence this, relaxed
plan selects long mortgage, action starts early enough life audit overlap
end end saveHard action. planner correctly guided optimal plan,
shown top Figure 3. crucial difference two heuristics, refined
heuristic able access accurate information value savings timepoints
37

fiC OLES , C OLES , F OX & L ONG

0: saveHard_start

saving

10


lifeAudit_start
takeMortgage_start short

money : [20,20]

money : [ ,10]

saveHard_end

canSave

takeMortgage_start long

10+

10+2
lifeAudit_end

saveHard_start
boughtHouse

happy

takeMortgage_end short

0: saveHard_start

saving

1



5

lifeAudit_start
takeMortgage_start long

money : [ ,t]

money : [0.75t,10]

takeMortgage_start short

12

10

money : [t,10]

12+
lifeAudit_end

saveHard_end

canSave

takeMortgage_end long

boughtHouse

happy

Figure 7: TRPG relaxed plan Borrower problem, following initial execution
saveHard start time 0, constructed using original version COLIN (top
described Section 9.1) revised version (bottom described Section 9.2).
Action layers depicted rounded rectangles fact layers ovals. action
layers labelled times constructed reachability analysis.

start savehard action. leads finer-grained structure TRPG,
seen fact six action layers arrival goal, rather four
case basic heuristic used. estimated makespan final plan 12 + ,
makespan according basic heuristic 10 + 2. basic heuristic leads non-optimal
solution requires extra saveHard action, giving solution makespan 20 + 2,
contrast makespan 12 + optimal plan.
benefit refined heuristic, extra work involved constructing modified
TRPG, better helpful actions chosen makespan estimate therefore accurate. choice similar length plans made based makespan. TRPG, constructed
refined heuristic Borrower problem, even contain short mortgage action
early enough layer considered relaxed plan.
38

fiC OLIN : P LANNING C ONTINUOUS C HANGE

10. Improving Performance
section present two techniques use improve performance COLIN. first
technique, described Section 10.1, generalisation earlier exploitation one-shot actions (Coles et al., 2009a) situation encapsulate continuous processes, leading
faster plan construction problems action types. second technique, described
Section 10.2, exploits LP defines constraints within final plan optimise plan
metric. leads better quality plans many cases.
10.1 Reasoning One-Shot Actions
earlier work (Coles et al., 2009a) observed common modelling device
planning domains leads use actions applied once. call actions
one-shot actions. arise, particular, collection resources
used once. key difference one-shot actions imply TRPG continuous
effects generated one-shot actions lapse certain point reached:
one-shot action continuous numeric effect v, a` first appears action layer
al(t), gradient v due effect finishes, latest, al(t + dmax (a)).
end aa one-shot action duration-dependent effect v, (implicit)
continuous effect acting v finishes, latest, layer al(t + dmax (a))
termination point implied, cases, fact action one-shot.
modify TRPG construction reflect restrictions extending data recorded
action layer include, snap-action action a, maximum remaining execution
time a, denoted rem(t, a). one-shot actions, layer al(t) a` first appears,
rem(t, a` ) = dmax (a), aa first appears, rem(t, aa ) = dmax (a) dmin(a). actions
one-shot rem(t, a` ) rem(t, aa ) initialised . make three minor
changes layer update rules accommodate rem values. First, calculating active
gradient variable v following action layer al(t):
X
X
v max (t) =
p(a)
k
aal(t)|rem(a,t)>0

hv,kig(a)

seen, subset actions execution time remaining considered. Second,
next action layer al(t + t) following al(t), value positive rem decremented
t, amount time elapsed since previous layer. Third, consequence this,
additional criterion must considered calculating time-stamp next fact-layer, t0 ,
described Section 9.2. Since time remaining complete action may expire, may
need insert additional fact layer denote point rem value reaches 0
continuous effects acting one variables need recalculated. time-stamp
earliest layer is:
t0 = + min{rem(t, a) > 0 | al(t)}
One-shot actions exploited still improving upper bound duration
action a. case actions state-dependent duration constraints (i.e. upperbound calculated based variables subjected effects actions), dmax (a) may
39

fiC OLES , C OLES , F OX & L ONG

gross over-estimate duration a. Suppose maximum duration bounded
formula w v + c. layer al(t) a` appears, compute maximum duration
a, started layer, based variable bounds recorded f l(t). could
use value determine bound remaining execution time a. However, future
layer f l(t0 ), variable bounds might changed, beginning al(t0 ), calculating
maximum duration based f l(t0 ), would allowed execute possibly longer period
time, allowing continuous effects persist longer.
remain faithful relaxation, possibility exploiting increased duration
(by starting t0 ) must included TRPG, well allowing possibility start
t, thereby obtaining effects sooner. Therefore, one-shot action allowed start
earliest layer al(t) preconditions satisfied, giving initial maximum duration
dmax (a, t) based fact later f l(t). But, later fact layer f l(t0 ) admits greater duration
(dmax (a, t0 ), value dmax action layer t0 ), remaining execution time
reconsidered. First, simple case, variables duration constraint changed f l(t0 ),
subject active continuous effects. case, apply pair dummy effects
fact layer t00 = t0 + dmax (a, t):
hrem(a` , t00 ) += (dmax (a, t0 ) dmax (a, t))i

hrem(aa , t00 ) += (dmax (a, t0 ) dmax (a, t))i.
Note increase rem values delayed layer t00 because, order benefit
longer duration a, must started layer t0 .
complex case, variables duration constraint changed f l(t0 )
duration affected continuous effects variables depends on.
situation, subsequent fact layer might admit marginally bigger duration last.
avoid recalculate new duration repeatedly, schedule pair dummy effects
based global, layer-independent, maximum value duration a:
hrem(a` , t00 ) += (dmax (a) dmax (a, t))i

hrem(aa , t00 ) += (dmax (a) dmax (a, t))i.
relaxation weaker might be, efficient compute.
10.2 Plan Optimisation
plan metric specified PDDL 2.1 problem files indicate measure quality
use evaluating plans. metric expressed terms task numeric variables
total execution time plan (by referring variable total-time). use LP
COLIN offers opportunity optimisation plan respect metric: plan
0
consisting n steps, numeric variables vn1
end plan, stepn1
time-stamp final step (i.e. action dictating makespan plan) LP objective
set minimise function these. LP must solved minimise time-stamp
last action (the makespan plan) order arrive lower bound time next
action. However, solved optimise plan metric.
40

fiC OLIN : P LANNING C ONTINUOUS C HANGE

Although possible consider ways use metric-optimising LP value plan
construction, guide search, focussed much limited, less costly, use:
attempt post hoc optimisation, attempting exploit flexibility temporal structure
final plan optimise plan quality last stage plan construction.
order post hoc optimisation useful, planning problems must property
possible vary quality metric plan scheduling actions occur
different times. possible wide range interesting situations, scheduling aircraft
land close given target time possible, taking images satellites certain times day
view clearer, minimising wasted fuel penalising time elapsing starting
engine plane take off. last represents general class problems
may desirable minimise amount time two activities: different metric
total time taken plan execution. capture interesting cases, first extend
language supported COLIN allow limited subset ADL conditional effects, allow
conditions action executed vary effects action metric value
plan. Second, discuss MILP built, based LP described Section 8.1,
support post hoc plan optimisation.
planner handles conditional effects standard compilation. However, conditional
effects metric variables appear plan quality metric preconditions
actions dealt differently. call variables metric tracking variables
exploit fact rescheduling plan affect values variables without changing
validity plan. example shown Figure 10.2 action land airplane,
conditional effects metric tracking variable total-cost. domain structured
land actions must start beginning plan, end points represent actual
landing times aircraft problem. duration actions set correspond
earliest latest possible points plane could land. seen, propositional
effects action whether plane lands early late: plane landed,
longer flying. However, numeric effects, effects metric tracking variable
total-cost, depend duration action and, particular, whether plane landed
early late. plane lands early, penalty paid certain rate per unit time plane
lands desired target value. plane lands late, fixed cost paid, addition
penalty (at different rate) per unit time plane lands desired target value.
considering action single action, pair conditional effects, planner decide
upon actions needed construct sound plan (in planes landed) whilst
leaving subsequent optimisation phase decision whether plane landed
early, late, time.
general, straightforward exploit LP described Section 8.1 attempt reschedule actions plan optimise value plan metric (provided metric function
linear). However, plan contains actions conditional effects metric tracking variables,
becomes possible exploit representation effects extended LP, using integer
variables, order offer powerful optimisation step. conditional effect either
activated not: introduce 0-1 variable represent case effect.
variable connected corresponding constraints determine whether condition
associated effect true not.
deal two kinds constraints 0-1 variables MILP encoding plan optimisation problem: one special case actions scheduled fixed time-windows
41

fiC OLES , C OLES , F OX & L ONG

(:durative-action land
:parameters (?p - plane ?r - runway)
:duration (and (>= ?duration (earliest ?p)) (<= ?duration (latest ?p)))
:condition
(and
(at start (takeOff))
(over (flying ?p))
(at end (scheduled ?p ?r)))
:effect
(and
(at start (flying ?p))
(at end (landed ?p))
(at end (not (flying ?p)))
(when (at end (< (?duration) (target ?p)))
(at end
(increase (total-cost)
(* (earlyPenaltyRate ?p) (- (target ?p) ?duration)))))
(when (at end (> ?duration (target ?p)))
(at end
(increase (total-cost)
(+ (latePenalty ?p)
(* (latePenaltyRate ?p) (- ?duration (target ?p)))))))
)
)

Figure 8: PDDL Domain Conditional Effects Airplane Landing Problem. literal
takeOff special proposition manipulated dummy action force landing
actions anchored point time.

governed timed initial literals affect whether conditions satisfied
case satisfaction conditions determined status continuous effects
controlled actions plan (so, example, cost action might depend whether
continuously changing value passed threshold time action executed).
cases handled straightforward encoding linkage value
0-1 condition variable corresponding conditions (the details given Appendix D).
extended conditional effects allow affect ?duration variable
action, similar devices encoding MILP.
MILP solved single final step construction plan, optimising
plan metric quality rescheduling actions best exploit precise timing actions
interaction, limited conditional effects, plan quality.

11. Continuous Linear Benchmark Domains
COLIN one first planners support PDDL 2.1 models featuring continuous linear change
duration-dependent effects8 currently benchmarks available exploit fea8. Specifically, duration-dependent effects depend non-fixed durations.

42

fiC OLIN : P LANNING C ONTINUOUS C HANGE

tures. support evaluation foster future comparisons planners designed
solve problems, produced number domains features9 .
first domains extension Metric Time variant Rovers domain,
2002 International Planning Competition (IPC 2002) (Long & Fox, 2003b). focus
action navigate, responsible moving rover one location another. original
model, discrete effect, start action, decrease energy level rover
8 units, coupled precondition must least 8 units energy available.
replace continuous numeric effect energy, condition energy
must least zero action. duration original action specified 5,
use effect gradient 8/5. Written thus, action net effect conditions:
energy decreased 8 units, must become negative. continuous change models
accurately use power navigate action: whilst power use may actually linear,
closer linear instantaneous. make model still realistic,
introduce new action domain: journey-recharge, shown Figure 9. exploiting
interaction continuous numeric effects variable, use action capture
option rover tilting solar panels face sun whilst navigating two points.
account power use reorienting solar panels, start end action, 0.2
units energy used. benefit consumption that, whilst action executing,
energy rover increased according constant positive gradient. final modification
domain, alter duration constraint existing recharge action. original
encoding, constraint is:
(= ?duration (/ (- 80 (energy ?x)) (recharge-rate ?x))).

forces duration action sufficient restore level charge 80 (full capacity). new formulation, replace = <= duration constraint specifies
maximum duration battery charged: need restored full capacity
every time action applied. Following three modifications, domain used
standard IPC 2002 benchmark problems. addition this, created
problems considering single rover, issue battery power management much
greater importance.
next domains extension Time variant Satellite domain,
taken IPC 2002. Here, continuous variant domain, make three key changes
domain model. First, original formulation, proposition used indicate whether
power available operate instrumentation given satellite. Switching instrument
required deleted fact, switching added again. Thus,
scope parallel power usage, instrumentation effectively used unit power. Now, use
numeric variable represent power, preconditions effects variable replacing
preconditions effects proposition previously used. Second, exploiting potential
differing power requirements, instruments operated one two modes:
cooled, uncooled. cooled mode, active sensor cooling used reduce sensor noise, enabling
images taken less time. cooling, however, requires additional energy. Third,
finally, compulsory sunrise phase start plan, satellites
9. P DDL domain problem descriptions evaluation tasks available online appendix maintained
JAIR paper.

43

fiC OLES , C OLES , F OX & L ONG

(:durative-action journey-recharge
:parameters (?x - rover ?y - waypoint ?z - waypoint)
:duration (>= ?duration 0.2)
:condition (and (over (moving ?x ?y ?z))
(over (<= (energy ?x) 80))
(at start (>= (energy ?x) 0.2))
(at end
(>= (energy ?x) 0.2))
)
:effect (and (at start (decrease (energy ?x) 0.2))
(increase (energy ?x) (* #t (recharge-rate ?x)))
(at end
(decrease (energy ?x) 0.2))
)
)

Figure 9: journey-recharge action continuous-numeric Rovers domain

move shaded planet, direct sunlight. leads increase
power availability, modelled linear continuous numeric effect attached action, sunrise,
must applied. Interaction effect preconditions powering instruments
ensures operated sooner power available. problem files use
domain slightly modified versions IPC competition problems, updated define power
availability numeric variable encode power requirements cooled uncooled
sensor operation. problems domain characteristics similar
Borrower problem used running example.
exploring use continuous numeric effects, next domain models operations
cooperating Autonomous Underwater Vehicles (AUVs). AUVs move waypoints
underwater perform two sorts science gathering operations. first taking water
sample given waypoint, performed AUV appropriate location,
whose water sample chamber empty. second taking image target interest.
requires two AUVs cooperate: one illuminate target torch, one take
image it. AUV domain inspired problem described Maria Fox
invited lecture 2009 International Conference Automated Planning Scheduling.
data acquired, must communicated ship surface. Satellite
Rovers domains, AUVs energy-constrained finite battery power
power usage actions continuous throughout execution. interesting continuous
numeric aspects domain arise use model drift. introduce variable
record far AUV drifted nominal position, update two ways. First,
activity plan contained within action drift small, positive continuous numeric
effect drifted distance. Second, add localise action sets drifted distance
zero, duration (and hence energy requirements) depending drifted distance prior
application. drifting affects domain actions. simplest case, sample
water take image given location, AUV cannot drifted two metres, hence
introducing need first localise case. interestingly, AUV shining
torch, drifting affects much light falling target. Thus, shine-torch action
AUV ?v three effects amount light falling given target ?t:
44

fiC OLIN : P LANNING C ONTINUOUS C HANGE

start: (increase (light-level ?t) (- 1000 (distance-from-waypoint ?v)))
throughout: (decrease (light-level ?t) (* #t (fall-off)))
end: decrease (light-level ?t) remaining contribution ?v making
illumination.
constant (fall-off) pessimistically derived formul involving inverse-square
law, giving linear approximation decay illumination levels due drift. Then,
take-image action itself, duration function (light-level ?t): less light available, longer requires take image.
final domain use Airplane Landing domain (Dierks, 2005), first posed challenge Kim Larsen invited lecture 2009 International Conference Automated
Planning Scheduling. problem models scheduling landing aircraft airport
runway. plane, three landing times specified: earliest possible landing time,
latest possible landing time, target (desired) landing time. Since time must allowed
airplanes clear runway landed, use runway heavily subscribed resource, possible planes land ideal time. Planes can, therefore,
land early late, incurs penalty. penalty modelled duration-dependent
effect, shown earlier paper (Figure 10.2 Section 10). able construct
set airplane landing problems using real data Edinburgh Airport arrivals board. Results
running COLIN problems reported Section 12.

12. Evaluation
COLIN temporal planner, able solve problems required concurrency, handle
discrete continuous metric variables. first question address costly
extension underlying CRIKEY 3 system allow COLIN manage continuous effects? COLIN
particularly powerful planner general PDDL 2.1 planners similar
expressive power available comparison continuous problems. However, extensions
necessary support continuous reasoning add overhead cost solving problems
continuous effects. compare performance COLIN temporal
planners selection temporal problems without continuous effects (Section 12.1) order
evaluate much overhead paid COLIN setting managing (redundant) structures,
comparison state-of-the-art planners pay price.
move considering performance COLIN problems continuous dynamics. second question is: much improvement obtain using refined
heuristic instead basic heuristic, dealing problems continuous change?
planners discussed Section 7 able scale large complex problems, compare
two versions COLIN. present performances new benchmark problems continuous processes, setting foundation future comparative evaluation alternative approaches
problems.
third question considered concerns quality solutions produced COLIN,
comparison optimal solutions found. COLIN satisficing planner
perform efficiently wide range continuous planning problems, interested
understanding much solution quality must sacrificed order obtain efficiency
achieved COLIN.
45

fiC OLES , C OLES , F OX & L ONG

Finally, consider question: expensive move solving STP (sufficient purely discrete temporal planning) solving LP (necessary handling continuous
effects)? particular, practical solve multiple LPs performing heuristic state evaluations?
Since LP construction solution central architecture COLIN important
relied upon scale appropriately range complexity problems COLIN
expected solve.
following experiments consider large number domains domain variants.
temporal comparisons use Simple Time Time variants Depots, Driverlog, Rovers,
Satellite Zeno, IPC 2002, Airport Pipes-No-Tankage IPC 2004.
Airport variant used Strips Temporal variant.
comparisons basic refined heuristics continuous domains, use
new continuous benchmark domains introduced Section 11: Airplane Landing, Rovers, Satellite
Cooled (the Satellite variant sensor cooling) AUV domain.
post-hoc optimisation experiments use Airplane Landing problem, Cafe domain introduced empirical analysis CRIKEY 2 (Coles, Fox, Halsey et al., 2008), variant
Airport amount fuel burned minimised, version Satellite time
windows, rewards obtained scheduling observations tighter windows.
cases use competition benchmark sets instances available. continuous Rovers Satellite domains used IPC 2002 Complex Time problem sets.
instances work continuous domain variants possible get better makespan plans
them, respecting continuous dynamics, possible instances
solved using discrete domain variants. generated increasing sized instances Airplane Landing domain number planes landed increased (in nth instance
problem, n planes must landed). wrote problem generator AUV domain
increases number AUVs, waypoints goals instances (they range 2 AUVs, 4
waypoints 1 goal, 6 AUVs, 16 waypoints 6 goals). experiments run 3.4GHz
Pentium machine, limited 30 minutes 1GB memory.
12.1 Comparison Existing Temporal Planners
temporal planners actually solve full range temporal problems. already
observed, many temporal planners cannot solve problems required concurrency. Even within
class problems required concurrency, easier problems, solved
left packing actions within plan harder ones possible. left
packing mean actions must executed concurrently actions plan
started time other. property means approach adopted Sapa,
extending forward search include choice either start new action else advance time
earliest point currently executing action terminates, sufficient solve problem.
contrast, problem cannot left packed require possibility advancing time
intermediate point execution action order coordinate correct interleaving
actions it. describe problems requiring temporal coordination. One
planners handle problems requiring temporal coordination LPG-s (Gerevini
et al., 2010).
therefore compare COLIN LPG-td, LPG-s, Sapa temporal baseline planner developed temporal satisficing track 2008 International Planning Competition. Neither
46

fiC OLIN : P LANNING C ONTINUOUS C HANGE

Depots Simple Time

Driverlog Simple Time
100

Colin Solution Time (s)

Colin Solution Time (s)

100

10

1

0.1

10

1

0.1
LPG-TD used
Temp-Baseline used

0.01
0.01

0.1

1
10
Best Solution Time (s)

LPG-TD used
LPG.s used
Temp-Baseline used
0.01
0.01

100

0.1

Rovers Simple Time

100

Satellite Simple Time
100

Colin Solution Time (s)

100

Colin Solution Time (s)

1
10
Best Solution Time (s)

10

1

0.1

10

1

0.1
LPG-TD used
Temp-Baseline used

0.01
0.01

0.1

1
10
Best Solution Time (s)

LPG-TD used
Temp-Baseline used
0.01
0.01

100

0.1

1
10
Best Solution Time (s)

100

Zeno Simple Time
1000

Colin Solution Time (s)

100

10

1

0.1

0.01
0.01

LPG-TD used
Temp-Baseline used

0.1

1
10
Best Solution Time (s)

100

1000

Figure 10: Comparison time taken solve problems simple temporal planning benchmarks.
COLIN compared best LPG -td, LPG .s, Sapa temporal baseline planner,
problem file shape colour points indicate planner
best therefore used plot. Planners appearing particular dataset
best problems collection.

temporal baseline planner, Sapa LPG-td solve problems requiring kind temporal
coordination. temporal baseline planner compiles away temporal information, using action
47

fiC OLES , C OLES , F OX & L ONG

Depots Time

Driverlog Time
100

Colin Solution Time (s)

Colin Solution Time (s)

100

10

1

0.1

10

1

0.1
LPG-TD used
Temp-Baseline used

0.01
0.01

0.1

1
10
Best Solution Time (s)

LPG-TD used
Temp-Baseline used
0.01
0.01

100

0.1

Rovers Time
100

Colin Solution Time (s)

Colin Solution Time (s)

100

Satellite Time

100

10

1

0.1

10

1

0.1
LPG-TD used
LPG.s used

0.01
0.01

1
10
Best Solution Time (s)

0.1

1
10
Best Solution Time (s)

LPG-TD used
LPG.s used
Temp-Baseline used
0.01
0.01

100

0.1

1
10
Best Solution Time (s)

100

Figure 11: Comparison time taken solve problems complex temporal planning benchmarks (first set). COLIN compared best LPG-td, LPG.s, Sapa temporal
baseline planner, problem file shape colour points indicate
best. Planners appearing particular dataset best
problems collection.

compression, solves problems non-temporal metric propositional problems.
solutions found, using Metric-FF core planning system, temporal information
reintroduced annotating plan suitable timestamps based critical path analysis.
details published planner, source code brief information available
IPC 2008 web site. approach cannot therefore solve problems required concurrency, fast effective simpler problems temporal actions sequenced.
straightforward identify many cases action compression applied safely
analysis implemented COLIN reduce overhead reasoning action end points
unnecessary. Therefore, behaviour temporal baseline planner similar
COLIN actions safely compressed. Figures 10, 11 12 show CPU time
comparisons COLIN best performances Sapa, LPG-td, LPG-s temporal baseline planner, across wide representative collection temporal benchmark domains.
Figure 10 shows performance simple temporal problems, action durations fixed,
Figures 11 12 show results complex temporal problems, including
48

fiC OLIN : P LANNING C ONTINUOUS C HANGE

Airport Strips Temporal
1000

100

100
Colin Solution Time (s)

Colin Solution Time (s)

Zeno Time
1000

10

1

0.1

0.01
0.01

1
10
Best Solution Time (s)

100

1

0.1

LPG-TD used
LPG.s used

0.1

10

0.01
0.01

1000

LPG-TD used
Temp-Baseline used

0.1

Pipes No-Tankage Temporal

1
10
Best Solution Time (s)

100

1000

Pipes Tankage Temporal
1000

100

Colin Solution Time (s)

Colin Solution Time (s)

100
10

1

10

1

0.1
0.1

LPG-TD used
Temp-Baseline used
0.01
0.01

0.1

1
10
Best Solution Time (s)

0.01
0.01

100

LPG-TD used
Temp-Baseline used

0.1

1
10
Best Solution Time (s)

100

1000

Figure 12: Comparison time taken solve problems complex temporal planning benchmarks (second set). COLIN compared best LPG-td, LPG.s, Sapa temporal baseline planner, problem file shape colour points indicate
best. Planners appearing particular dataset best
problems collection.

duration actions determined context executed (although none
action effects depend this), problems metric variables. None problems
feature required concurrency forms temporal coordination. figures, planners
appearing dataset best problems domain.
Analysis Figures 1012 shows COLIN indeed pay overhead computation time
solution temporal problems feature continuous dynamics. overhead particularly significant simple temporal problems interesting temporal structure
temporal baseline planner tends perform well. overhead paid COLIN lower
complex temporal problems, temporal reasoning required sometimes challenging. makespan results Figures 13, 14 15 show COLIN produces good quality
plans, especially complex temporal problems, although temporal baseline planner still
competitive terms CPU time makespan. suggests temporal structure,
even complex temporal benchmarks, quite simple planner well ignoring
temporal structure present, rather trying reason generating plans.
49

fiC OLES , C OLES , F OX & L ONG

Depots Simple Time

Driverlog Simple Time

100

300

250
Colin Solution Quality

Colin Solution Quality

80

60

40

20

LPG-TD used
LPG.s used
Sapa used
Temp-Baseline used

0
0

20

40
60
Best Solution Quality

80

200

150

100

LPG.s used
Sapa used
Temp-Baseline used

50

0
100

0

50

100
150
200
Best Solution Quality

300

Satellite Simple Time

400

400

350

350

300

300
Colin Solution Quality

Colin Solution Quality

Rovers Simple Time

250

250
200
150
100

250
200
150
100

LPG-TD used
LPG.s used
Sapa used
Temp-Baseline used

50
0
0

50

100

150
200
250
Best Solution Quality

300

350

LPG-TD used
LPG.s used
Temp-Baseline used

50
0
400

0

50

100

150
200
250
Best Solution Quality

300

350

400

Zeno Simple Time
6000

Colin Solution Quality

5000

4000

3000

2000

LPG-TD used
LPG.s used
Sapa used
Temp-Baseline used

1000

0
0

1000

2000
3000
4000
Best Solution Quality

5000

6000

Figure 13: Comparison plan quality simple temporal planning benchmarks. COLIN compared best LPG-td, LPG.s, Sapa temporal baseline planner, problem file shape colour points indicate best. Planners
appearing particular dataset best problems
collection.

detailed results experiments, showing raw runtime quality comparisons planners used experiment, presented Appendix E.
50

fiC OLIN : P LANNING C ONTINUOUS C HANGE

Depots Time

Driverlog Time

400

1000

350
800
Colin Solution Quality

Colin Solution Quality

300
250
200
150

600

400

100
200

LPG-TD used
LPG.s used
Sapa used

50
0

LPG-TD used
LPG.s used
Sapa used
Temp-Baseline used

0
0

50

100

150
200
250
Best Solution Quality

300

350

400

0

200

Rovers Time

400
600
Best Solution Quality

800

1000

Satellite Time

400

600

350

500
Colin Solution Quality

Colin Solution Quality

300
250
200
150

400

300

200

100
LPG-TD used
LPG.s used
Sapa used

50

LPG-TD used
LPG.s used
Sapa used
Temp-Baseline used

100

0

0
0

50

100

150
200
250
Best Solution Quality

300

350

400

0

100

200
300
400
Best Solution Quality

500

600

Figure 14: Comparison plan quality complex temporal planning benchmarks (first set).
COLIN compared best LPG -td, LPG .s, Sapa temporal baseline planner,
problem file shape colour points indicate best.
Planners appearing particular dataset best problems
collection.

12.2 Solving Problems Continuous Linear Change Duration-Dependent Effects
focus section examining scalability COLIN continuous benchmark
domains developed and, specifically, comparing two variants TRPG discussed Section 9. are: basic heuristic, discretises time, refined heuristic,
capable handling continuous numeric change directly. continuous benchmarks,
described Section 11, characterised sophisticated temporal structure (including required
concurrency) giving rise interesting opportunities concurrent behaviour. problems time-dependent effects continuous effects, reach temporal
planners used last experiment. problems used experiment designed rely
exploitation features, baseline planner ignored continuous dynamics
would unable solve problems.
Results comparing basic refined heuristics shown Figure 16. Beginning
Airplane Landing domain Rovers domain variant, performance either
51

fiC OLES , C OLES , F OX & L ONG

Zeno Time

Airport Strips Temporal

400

1000

350
800
Colin Solution Quality

Colin Solution Quality

300
250
200
150

600

400

100
200

LPG-TD used
LPG.s used
Sapa used

50
0

LPG-TD used
LPG.s used
Temp-Baseline used

0
0

50

100

150
200
250
Best Solution Quality

300

350

400

0

200

50

40

40

30

20

10

800

1000

Pipes No-Tankage Temporal

50

Colin Solution Quality

Colin Solution Quality

Pipes No-Tankage Temporal

400
600
Best Solution Quality

30

20

10

LPG-TD used
LPG.s used
Temp-Baseline used

0

LPG-TD used
LPG.s used
Temp-Baseline used

0
0

10

20
30
Best Solution Quality

40

50

0

10

20
30
Best Solution Quality

40

50

Figure 15: Comparison plan quality complex temporal planning benchmarks (second
set). COLIN compared best LPG-td, LPG.s, Sapa temporal baseline
planner, problem file shape colour points indicate
best. Planners appearing particular dataset best
problems collection.

heuristic used: relaxed plans found same. expected, two
domains interaction time numbers relatively limited. Airplane Landing
problem, action durations affect variable used measure plan cost used
preconditions. Thus, selection actions TRPG unaffected. Rovers domain, continuous change arises consuming power navigate actions, producing power
recharging. Capturing time-dependent nature precisely effect relaxed plans, nature relaxation leads rarely require recharge actions,
conditions needed affected whether effects integrated
not. Nevertheless, two domains illustrate guaranteed like-for-like situations,
heuristic guidance same, refined heuristic negligibly expensive
compute, despite additional overheads tracking gradient effects TRPG expanded.
seen COLIN scales well across Airplane Landing instances, although
manages solve 9 14 Rovers problems (these well within two minutes).
52

fiC OLIN : P LANNING C ONTINUOUS C HANGE

Rovers Continuous Time

Airplane Landing Edinburgh Time
1000

100

Basic Heuristic
Refined Heuristic

Refined Heuristic
Basic Heuristic
100
10

Time

Time (s)

10
1

1

0.1
0.1

0.01

0.01
5

10

15

20
25
30
Problem Number

35

40

45

2

50

4

6

Satellite Cooled Time
1000

12

14

Satellite Cooled Makespan
1000

Basic Heuristic
Refined Heuristic

Basic Heuristic
Refined Heuristic

900
800

Makespan

100

Time (s)

8
10
Problem Number

10

700
600
500

1

400
300

0.1

200
2

4

6

8

10
12
Problem Number

14

16

18

20

2

4

6

AUV Time

10
12
Problem Number

14

16

18

20

AUV Makespan

10000

600

Basic Heuristic
Refined Heuristic
1000

500

100

400
Makespan

Time

8

10

300

1

200

0.1

100

0.01

Basic Heuristic
Refined Heuristic

0
5

10

15
20
Problem Number

25

30

5

10

15
20
Problem Number

25

30

Figure 16: Comparison Basic Refined TRPG variants continuous domains. boxed
graphs makespan comparisons Satellite Cooled AUV domains, placed
right corresponding runtime graphs.

Satellite Cooled domain, runtime taken find plans using refined
heuristic comparable using basic heuristic: problems (e.g. 13, 18)
slower; others (e.g. 12, 15) faster. interesting comparison make
makespan data (shown right). seen, refined heuristic generally produces
53

fiC OLES , C OLES , F OX & L ONG

better quality plans. difference quality due refined heuristic better capturing
relationship time numbers, leading better actions chosen relaxed plan.
way example, consider state reached beginning sunrise action:
basic heuristic, LP used obtain bounds power availability state,
free reign much time allow elapse. lower-bound found slightly
zero (corresponding allowing time elapse), upper-bound found
peak power availability (corresponding applying entirety sunrise action).
building TRPG bounds, cooled sensor operation immediately available,
hence goals always achieved first actions using sensor cooling: duration
actions lower, making attractive. resulting relaxed plan, hence
helpful actions, therefore lead search use sensor cooling.
refined heuristic, LP used obtain bounds power availability
state, bounds must obtained soonest possible point. Thus, lowerbound still slightly zero, upper bound slightly
zero. positive gradients effect power availability variables included
TRPG, influencing layers different actions become applicable. Specifically,
actions without sensor cooling lower power requirements, hence appear earlier
layers. Then, goals first achieved actions using sensor cooling (where increased
duration acquiring image without cooling compensated sufficiently able
start taking image sooner) relaxed plan, hence helpful actions, use
sensor cooling goals. seen situation closely analogous
differences alternative mortgages Borrower domain.
extent trade-off influences plan quality varies problems, depending
initial orientation satellites, images required. least benefit arises
satellite requires substantial reorientation point towards first target case,
time taken allows energy level rise sufficiently support sensor cooling. greatest benefit
arises opposite situation, satellite requires minimal reorientation then, switching
sensor cooled mode require substantial amount time elapse support
energy requirement precondition.
aid understanding scalability implications results, Satellite problems
based used 2002 IPC, similar fundamental size. However, continuous
reasoning added makes underlying problems fundamentally much
difficult solve.
AUV domain, use refined heuristic increases problem coverage, 30
problems solved rather 27. Applying Wilcoxon Matched-Pairs Signed-Ranks Test
paired time-taken data mutually solved problems, find reject null hypothesis
refined heuristic better basic heuristic, p 0.05. Observing performance planner, difference performance arises due way drifting
process handled two approaches. Specifically, accounted difference
bounds fact layer zero TRPG calculated. Consider state action
AUV communicate image data started. domain encoding ensures
communication completed, AUV cannot perform activities. point, prior
evaluating state using TRPG heuristic, LP used give bounds values state
54

fiC OLIN : P LANNING C ONTINUOUS C HANGE

variable. Considering variable recording far communicating AUV drifted
variable (distance-from-waypoint auv0), abbreviated dfw0:
basic heuristic employs approach set Section 8.3. single timestamp
variable introduced, must come action started, along additional
variable constraint dfw0. Maximising minimising value additional
variable yields bounds dfw0. lower bound infinitesimally larger
prior starting action, due time elapsed. upper bound corresponds
allowing large amount time elapse.
refined heuristic employs approach set Section 9.2.1. Here, timestamp
variable introduced task variable, case concerned tnow (dfw0).
prior case, constrained action applied. Additionally,
however, domain model enforces action refer value
variable communicate action finished, specific tnow must come
(future) end action applied. bounds dfw0 found following
remaining steps Section 9.2.1: LP solved minimise value tnow variable,
value variable fixed minimum LP solved maximise
minimise value dfw0. Critically, tnow variable must come end
action applied, rather start, lower bound dfw0 larger.
increase lower bound dfw0 affects whether, TRPG, preconditions
form (<= (dfw0) c) considered satisfied initial fact layer. satisfied,
delayed earliest layer localise action reduces value dfw0.
difference affect relaxed plan found: solution extraction, action requiring
(<= (dfw0) c) chosen, localise action necessary achieve TRPG,
action added relaxed plan. cannot come earlier end
communicate action applied, is, point bounds dfw0 calculated,
sort localisation necessary ultimately applied. Thus, bounds
refined heuristic lead better relaxed plans found, containing localise actions
would otherwise omitted.
give indication difficulty problems, AUV problems range problems
2 AUVs, 5 waypoints, 2 objectives 2 goals harder end 6 AUVs, 15
waypoints, 6 objectives 7 goals. major hurdle preventing COLIN scaling even
larger problems inability see implicit deadline created shine-torch
action started. AUV shining torch finite energy, planner starts shinetorch action one AUV, preparation another AUV take image, adds
plan actions involving second AUV unrelated taking image, delay
lead insufficient energy shine torch long enough gain required
exposure photograph taking action eventually started. leads planner dead
end forced resort best-first search, much less effective EHC
domain. implicit deadlines occur many planning problems temporal coordination
issues COLIN faces could avoided using branch-ordering heuristic promotes
actions whose applicability time-limited due ends currently-executing actions, perhaps
relaxing unnecessary ordering constraints imposed COLIN due total order search.
scope paper, interesting avenues future work.
55

fiC OLES , C OLES , F OX & L ONG

(:durative-action burning-fuel
:parameters (?a - airplane)
:duration
(>= ?duration (* 60 (engines ?a)))
:condition (and (at start (not-burning-fuel ?a))
(at end (taking-off ?a)))
:effect (and (at start (can-start-engines ?a))
(at start (not (not-burning-fuel ?a)))
(increase (wasted-fuel) (* #t (engines ?a)))
)
)

Figure 17: burning-fuel action added Airport domain
12.3 Post Hoc Plan Optimisation
section evaluate effectiveness post hoc plan optimisation strategy. described
Section 10, plan optimisation phase occurs planning complete never change
actions plan. lifting Partial Order prior scheduling (Veloso, Perez, &
Carbonell, 1990), provide scheduler little flexibility order actions.
long ordering constraints remaining (greedy) partial-order lifting respected,
scheduler reduce plan cost altering time-points actions occur and,
possible, durations. Minimising objective plan makespan effect
plan quality domains metric sensitive times actions applied,
since, default, COLIN minimises makespan solution final LP completed plan.
benchmark domains literature, make use one existing suitable
domain introduce new variations existing benchmarks, order test feature.
first domain, existing domain property, Airplane Landing
domain, used earlier section, described Section 11. Here, penalties incurred
landing depend whether, extent, early late. Therefore, given
sequence landings, times assigned impact quality plan.
next two benchmark problems variants problems introduced International
Planning Competitions 2002 (Long & Fox, 2003b) 2004 (Hoffmann & Edelkamp, 2005).
First, consider modified version Satellite domain. modify domain adding
time windows (modelled using TILs) clear view given objective.
photograph objective taken time window, quality plan improves,
better quality picture preferable. problem introduce three time windows
objective, bounded random duration, taking photograph objective
preferred. second adapted benchmark taken IPC2004 Airport domain.
Airplane Landing problem described previously concerned scheduling landing times
aircraft, Airport domain concerned coordinating ground traffic: moving planes
gates runways, eventually take-off, whilst respecting physical separation must
maintained aircraft safety reasons. add domain metric minimise
total amount fuel burnt aircrafts engines starting eventually takes
off. capture PDDL 2.1, add action shown Figure 17. action must occur
planes engines started cannot finish plane started take-off
(hence duration least startup action). two points increases
56

fiC OLIN : P LANNING C ONTINUOUS C HANGE

amount fuel wasted rate proportional number engines fitted aircraft:
larger planes (for number engines greater) waste fuel per unit time.
Satellite Airport domains use standard problem sets competitions,
adding minor changes needed support modifications made, whilst leaving underlying
problems unaltered.
final domain consider cafe domain, first used evaluate CRIKEY (Coles, Fox,
Halsey et al., 2008). domain, tea toast must made delivered table
cafe. kitchen, however, one plug socket, preventing two items made
concurrently. restriction allows problem number interesting metric functions:
minimise total time serve customers (the plan makespan), minimise time
delivery tea toast given table, minimise amount items cooled
delivered table. consider latter two variants here.
results experiments presented Figure 18. Starting top-left,
Airplane Landing domain, post hoc optimisation gives modest improvement plan quality.
due limited scope optimisation: even partial-order lifting, order
planes going land fixed plan, adjusted precise times
planes going land within ordering.
Moving Airport domain variant burning-fuel action Figure 18 top-right
post hoc scheduling able give large improvements plan quality. original plans,
optimisation, burning-fuel action given plane started point prior
relevant can-start-engines fact needed ended point relevant
taking-off fact true, necessarily timely manner. Following post hoc optimisation,
due objective function used, burning-fuel action starts late possible finishes
early possible.
cafe domain, results two metrics used shown central graphs
Figure 18. two diagonal lines correspond original plans. given problem, two
plans identical: evaluation metric differs. two lower lines show quality
plan scheduling respect relevant metric. Observing post-scheduled plans,
actions scheduled one would intuitively expect. minimising total delivery
window times, items given table delivered succession, even first item loses heat
waiting second item prepared. contrast, minimising heat loss items
delivered tables soon prepared, even delay two
items delivered.
Finally, results variant Satellite domain observation windows shown
bottom-left Figure 18. Whilst marked improvements previous two
domains, scheduler able make headway better scheduling observations.
original plan given problem will, satellite, fix observations make,
order made. remains enough flexibility able improve plan
quality, reducing plan cost around factor 2.
12.4 Comparison Optimal Solutions
investigated difference quality optimal solutions solutions produced
COLIN order form impression close optimal COLIN get. this, ran
COLIN admissible heuristic uses makespan estimate produced TRPG, using
57

fiC OLES , C OLES , F OX & L ONG

Airplane Landing Edinburgh
20000

Airport Fuel Loss
4500

colin-standard
colin-optimise

18000

4000

16000

3500

14000

3000

Solution Quality

Solution Quality

colin-standard
colin-optimise

12000
10000
8000

2500
2000
1500

6000

1000

4000

500

2000
0

0
5

10

15

20
25
30
Problem Number

35

40

45

50

2

4

6

Cafe (Delivery Window)
1200

8

10
12
Problem Number

14

16

18

Cafe (Heat Loss)

colin-standard
colin-optimise

80

100

colin-standard
colin-optimised

70

80

1000
Delivery Window Metric

20

Heat Loss Metric

60
800

600

60

50
40

40
30

400
20
200

20

10

0

0
2

4

6

8

10
12
Problem Number

14

16

18

20

0
2

4

6

8
10
12
Problem Number

14

16

18

20

Satellite Reward
1200

colin-standard
colin-optimise

Solution Quality

1000

800

600

400

200

0
2

4

6

8

10
12
Problem Number

14

16

18

20

Figure 18: Quality plans produced Colin without post hoc optimisation. four
graphs, lower better.

value used COLIN results presented Figures 16 18. call
variant optimalCOLIN.
AUV Rover domains, variable-duration actions domain
durations chosen small actions used plan. -length actions
might chosen, example, relocalise slightly drifted, recharge used
negligible amount power. domains, optimal search consider plans comprising
almost entirely actions duration optimal makespan. example scale this,
58

fiC OLIN : P LANNING C ONTINUOUS C HANGE

Problem
[2-5] instance
01
02
03
04
05
06
07

optimalCOLIN
makespan time (secs)
20.001
0.00
30.001
0.00
30.001
0.02
40.001
0.29
40.003
3.94
50.003
69.93
-

COLIN

makespan
20.001
34.004
38.007
44.006
48.009
62.01
66.014

time (secs)
0.01
0.01
0.01
0.01
0.02
0.03
0.03

Table 4: Comparison makespans solution time airplane landing problems solved optimalCOLIN COLIN using refined heuristic. Problem 7 could solved
optimalCOLIN within 1 hour bound.

AUV problem 1, solved COLIN, find plan makespan 34.031. Careful analysis
hand suggests plan cannot improved, optimal. OptimalCOLIN must consider plans
34,031 steps order prove plan optimal. means problem
completely reach optimal planning.
similar problem arises Rovers domain, recharge action little
long, series -long recharge actions applied, reaching ostensibly different states,
without making progress. Clearly, potential -duration actions arise
continuous temporal domain. problem search-space explosion arise
temporal domain orders magnitude differences longest shortest
possible actions.
However, Airplane-Landing Satellite Cooling domains, variable-duration
actions domains made arbitrarily short search. Therefore, optimalCOLIN
principle able solve problems domains. fact, given 4 Gb memory 1 hour
runtime instance, able solve 6 airplane landing instances, shown Table 4.
table shows, time required solve problems increases fast: problem 5 could
solved 3.94 seconds, problem 6 69.93 seconds, problem 7 could solved within
hour available. basis decided unnecessary extend time available
optimalCOLIN would unlikely cope large instances.
Table 4 shows COLIN sacrifices optimality speed. sacrifice important,
pay terms time required solve problems. COLIN able solve 62 airplane
landing problems, instance taking 33.02 seconds solve.
found optimalCOLIN could report candidate solution first Satellite domain instance, within 368 seconds. However, could prove within time available solution
optimal, include it.
12.5 Costs Associated LP Scheduling
transition CRIKEY 3 COLIN switch solving STP state solving
LP. important issue consider impact time taken evaluate
59

fiC OLES , C OLES , F OX & L ONG

feasibility plan constructed reach every state considered search. default mode
operation, COLIN uses STP evaluate state unless temporalnumeric constructs
necessitate use LP. evaluate whether appropriate (or whether always using
LP would faster), compare overheads STP solving LP solving equivalent
problems, created variant COLIN that, every state S, schedules plan reach independently using three different schedulers: original STP solver used standard version
COLIN, equivalent LP solved using CPLEX (IBM ILOG CPLEX Optimization Studio)
equivalent LP solved using CLP (Lougee-Heimer, 2003). STP solver used incremental STP algorithm due Cesta Oddi (1996), previously used CRIKEY 3. LP
solvers used tighter variable bounds described Section 8.4. order evaluate
cost associated use LP instead STP, modified COLIN collect data revealing
costs technique applied node evaluated search plan.
possible compare performance straightforwardly, simply running COLIN using STP
versus COLIN LP, minor variations caused numerical accuracy lead
different trajectories followed, masking intended comparison. aside, interesting observe minor (and essentially uncontrollable) differences computed makespans
relaxed plans lead significant variations performance (relaxed plans equal h-values
sorted makespan estimates search).
wish compare STP LP approaches, necessary consider domains
reason: is, without continuous-numeric duration-dependent effects.
order consider problems scheduling interesting necessary (in contrast
temporally simple problems Section 12.1) consider domains required concurrency.
Currently benchmarks exist, planners attempt solve problems.
use representatives competition domains features: compiled timed initial
literal domains IPC2004, use Airport (with Time Windows) PipesNoTankage (with deadlines). use Match-Lift Driverlog Shift domains (Halsey, 2005).
completeness, include results domain scheduler strictly necessary:
PipesNoTankage Temporal domain IPC2004.
Figure 19 shows mean time spent scheduling per state, using approach, problems
domains. exclude graph data problems solved
planner less second, accuracy profiling data sufficiently reliable
measure time spent scheduler overall time taken small. Since
interesting variation results domains present data together across three
graphs, sorted scheduling time per node using CPLEX. intended nominal
analogue hard scheduling problems given planning problem are. increase
scheduling time CPLEX generally corresponds increase scheduling time CLP
STP solver, except easier problems noise sufficient tip balance
figures small. Note differing y-axis scales three graphs, sorting problems
according difficulty allows us display data appropriate range distinguish
results. sake maintaining reasonable y-axis ranges final problem, problem 60,
omitted graphs; problem figures CPLEX 239ms, CLP 139ms
STP 38ms.
results Figure 19 are, course, indicative scalability COLIN, running
three schedulers state, significantly slower usual configuration. practice,
domains continuous duration dependent effects, COLIN automatically
60

fiC OLIN : P LANNING C ONTINUOUS C HANGE

Low Difficulty
0.4

STP
CLP
0.3 CPLEX

MST/State (ms)

0.35
0.25
0.2
0.15
0.1
0.05

1

2

3

4

5

6

7

8

9 10 11 12
Problem Number

13

14

15

16

17

18

19

20

33

34

35

36

37

38

39

40

58

59

Medium Difficulty

MST/State (ms)

1.4

STP
CLP
1.2 CPLEX
1
0.8
0.6
0.4
0.2
21

22

23

24

25

26

27

28

29 30 31 32
Problem Number
High Difficulty

40
MST/State (ms)

35
30

STP
CLP
CPLEX

25
20
15
10
5
0
41

42

43

44

45

46

47

48

49 50 51 52
Problem Number

53

54

55

56

57

Figure 19: Mean Scheduling Time (MST) per State Temporal Planning Problems. problem
number appears leftmost three corresponding columns case.

disable LP scheduler use efficient STP solver. Further, planner
run profiling enabled, subject significant overheads.
61

fiC OLES , C OLES , F OX & L ONG

&
&
&
&
&
&

!"
#"$
# !
" '
(' )
#
)*')' +
(,#
(,
!"
#"$
(' )
(' )

!"

#"$%#

!"
2
#"$%#!"

& # !
-.# **
&
')&
( ' )/ ' - / (
0
-. 0 '
# 11

Figure 20: Time spent various activities solvers, CPLEX CLP, viewed proportion total time spent CPLEX. slice labelled MILPSolverCPX/CLP
time spent destructor MILP solver CPLEX CLP: housekeeping operation implementations (which written C++).

Considering relative performance STP LP solvers, clear overheads incurred necessary (for domains continuous effects) move using LP rather
STP. mean ratio time spent scheduling problems CPLEX
spent using STP solver 5.81, figure CLP 3.71. Analysis data suggests
ratios change problem difficulty increases, rather overhead constant factor
harder problems.
Despite increased scheduling overheads still worth noting solving scheduling problem relatively small fraction cost reasoning done state. plan
given state scheduled check feasibility state evaluated using temporal RPG heuristic described Section 9. well known, analysis performance
forward search planners, majority search time spent evaluating
heuristic. give indication relative cost scheduling versus heuristic computation
give admissible estimate mean fraction time spent, per-state, running scheduler
versus computing heuristic. estimate guaranteed overestimate true mean
states scheduler demonstrate temporal problem solution:
states RPG heuristic never evaluated, heuristic evaluation actually applied
fewer states scheduler. Nonetheless, data shows that, across problems, using
STP solver scheduling accounts average less 5% state evaluation time. CLP
CPLEX figures 13% 18% respectively. suggests that, although scheduling
add overhead solving problems, relatively small compared cost heuristic
computation.
perhaps surprising observation made Figure 19 CLP generally solves
scheduling problems much efficiently CPLEX. Given reputation CPLEX
highly efficient commercial LP solver wanted investigate case problems.
62

fiC OLIN : P LANNING C ONTINUOUS C HANGE

performed analysis profiling data, breaking results function call,
observe time spent various aspects constructing solving LP thorough CLP
CPLEX library calls. data, presented Figure 20 shows time spent function
fraction total time taken CPLEX schedule plans (each summed across problems).
used section CLP data represents time saved using CLP versus CPLEX.
presentation means equally sized slices pies represent length time
taken either solvers respective methods.
important insight gain data time
LP solvers spent solve function, indeed observed search portion
negligible: barely visible. majority time is, fact, spent adding rows LP
matrix, i.e. adding constraints LP actually solved. Comparing CPLEX CLP,
takes 6 times longer, average, add row matrix. LPs created
identical, hence involve adding number rows matrix. portion
chart corresponds methods, many take longer search,
pre-processing steps adding new columns (variables) setting upper bounds. Since
adding rows matrix significant portion time taken constructing-then-solving
LPs COLIN, results large overhead. LPs created COLIN small simple
solve, compared difficult industrial-sized problems CPLEX designed.
results suggest that, fact, best type LP solver use task relatively
light-weight LP solver, overheads, create models efficiently, even perhaps
would scale large-scale problems. notable, although less marked, difference
two LP solvers time spent destructor, called free memory used
LP solver state evaluated. Here, takes 23 times longer, average,
call destructor CPLEX destructor CLP. less impact rowadding overheads, since LP deleted per state, rather per LP constraint.
general, would normally noticeable issue solving single difficult LP. However,
COLIN, number LPs solved equal number states evaluated, overhead
become noticeable.
One interesting outcome study if, future, COLIN extended
non-linear continuous change, requiring use mathematical programming solver state
(along research developments), overheads may well prohibitive. search
within solver, greater overhead would occur due change, fact
major contributor time overheads using LP.

13. Conclusions
range problems solved effectively planners grows, range
opportunities technology applied real problems. recent years, planning extended solve problems real temporal structure, requiring temporal coordination, problems
include metric resources interactions use causal structure plans.
shown range extended still further, include linear continuous process
effects. extension power planners demands several steps. first model
extension form allows relationship constraints imposed plans
new expressiveness, actions used solve problem, properly expressed.
second step develop means represent world state consistently, order
63

fiC OLES , C OLES , F OX & L ONG

characterise space search plan conducted. third step develop
way compute progression states using action models extended representation.
step complete, is, principle, possible plan: search space constructed
searched using classic simple search techniques. practice, process unlikely lead
solutions many interesting problems fourth step, order make search possible
large spaces, construct informed heuristic guide search.
paper built earlier work completed first steps, adding third
fourth steps allow us solve planning problems continuous effects. tools
used achieve well-established Operations Research tools: LP solvers extensions
MILP solvers. contributions made show tools harnessed
check consistency states, model state progression compute heuristics successfully guide search large spaces develop planning problems.
additional contribution established collection benchmark problems
direction research planning. planning community witnessed creation
benchmarks propagation powerful aid development technology, supporting
clear empirical evaluation challenging researchers improve results others.
shown COLIN solve interesting complex problems, remains much room
improvement. Apart extending capability planner improving informedness
heuristic improving early pruning dead end states, opportunity
extend still range problems expressed solved. particular,
interested problems non-linear continuous effects, power thermal curves.
seems possible non-linear effects might approached similar approach used
COLIN, adapting NLP solver role LP solver COLIN. Alternatively, might
possible approximate non-linear effects piecewise linear effects, much way
AUV domain described paper, performing process automatically.
Planning becoming increasingly key technology robotic systems become powerful
complex begin see limits low level control strategies managing
control systems. Autonomy demands powerful predictive control planning
offers possible solutions problem. Planning continuous effects important
tool collection offer tackling new demands.

Acknowledgments

authors wish thank handling editor, Malte Helmert, anonymous reviewers
considerable contributions paper. authors wish thank members Planning Group helpful discussions long gestation work.
authors wish acknowledge EPSRC support work, specifically
grants EP/G023360/2 EP/H029001/2.
64

fiC OLIN : P LANNING C ONTINUOUS C HANGE

Appendix A. Glossary
Name
(i, v)
(i, v)
Action compression

al

ce(i)
cs(i)

dec(i, v)

dmin (dmax)

E
eff +
x
eff
x
eff nx
estepi

elapsed(a)

f (i)

Description
lower bound assignment effects variable v due actions layer reachability graph.
upper bound assignment effects variable v due actions layer reachability graph.
technique simplifying structure durative actions
treating simple non-durative action union
effects ends durative action union
preconditions.
Action layer reachability graph constructed heuristic purposes.

First Use
62

Function returning variable corresponding end time
snap-action position current plan.
Function returning variable corresponding start time
snap-action position current plan.

21

Set (discrete) decreasing effects variable v layer
reachability graph.
rate change variable v (associated state
achieved execution plan.
minimum (maximum) duration action. use dmin(a)
(dmax(a)) relevant action required explicit
dmin(a, t) (dmax(a, t)) value anchored action
layer al(t).

62

event list recording action start times durative actions
whose end points yet included plan.
Propositional add effects action, x, present, indicates whether start end action.
Propositional delete effects action, x, present,
indicates whether start end action.
Numeric effects action, x, present, indicates
whether start end action.
name LP variable corresponding time
durative action finish, started ith step plan,
finished within plan constructed far.
maximum time action could executing
state heuristically evaluated.

13

variable STN CRIKEY 3 corresponds time
currently incomplete action eventually finish.

65

62
9

26

21

21
14

4
4
4
20

27

15

fiC OLES , C OLES , F OX & L ONG

Name
fl

Description
Fact layer reachability graph constructed heuristic purposes.

First Use
26

inc(i, v)

Set (discrete) increasing effects variable v layer
reachability graph.
invariants active state S.

62

Left packing

structure plans concurrency concurrent actions start simultaneously.

39



name variable created represent time end
current plan STP LP used check temporal consistency state.

15

hop, , dmin, dmax

Event record CRIKEY state, containing durative action, op,
started step i, minimum maximum duration
action.

14

pre
pre
pre `
p(a)

Conditions required complete action.
Invariant conditions durative action.
Conditions required initiate action.
bound number instances durative action
may execute concurrently.

4
5
4
28

remaining(e)

maximum amount remaining time action
event record e could continue executing following state
heuristically evaluated.
Information associated durative action al(t) reachability analysis constructed COLIN, indicating much time
could continue execute layer.

28

stepi

name LP variable corresponding time
action ai applied plan.

20

t(i)

variable STP CRIKEY 3 represents time
step plan executed.
property planning problems require concurrency order manage interactions actions
deadlines.
Action effects refer ?duration, causing numeric fluents
change different amounts according length action
causing effect.

14

inv(S)

rem(t, a)

Temporal coordination

Time-dependent change

66

14

33

39

2

fiC OLIN : P LANNING C ONTINUOUS C HANGE

Name

Description
Used describe continuous change: complete account
use semantics, see original discussion use
PDDL 2.1 (Fox & Long, 2003).

First Use
3

ub(w, x, y)

Function used calculate bounds effects continuous numeric change.

29

v

Used represent vector metric fluents associated
planning domain, values state. vector treated
indexable: v[i] ith entry v.
vector values metric fluents start state, immediately following step effects application action.
vectors lower upper bounds values numeric variables state (during plan construction).

5

Symbol used represent vector constants equal dimension
size vector metric fluents relevant planning
problem.

5

#t

v0
vmin , vmax

w

67

21
24

fiC OLES , C OLES , F OX & L ONG

Appendix B. Metric Relaxed Planning Graph Heuristic
Relaxed Planning Graph (RPG) heuristic Metric-FF (Hoffmann, 2003)
popular numeric planning heuristic last decade, widely used many planners.
intuition behind heuristic generalise delete-relaxation include numeric variables.
case propositions, relaxation simply ignore propositional delete effects so,
(relaxed) actions applied, set true propositions non-decreasing. case numbers,
relaxation replaces exact assignments numeric variables bound constraints upper
lower bounds. Applying relaxed actions extends bounds reducing lower bounds
decrease effects increasing upper bounds increase effects. Checking whether numeric
precondition satisfied simply matter testing whether constraint satisfied
value within bounds. delete-relaxed problem solved (non-optimally) polynomial
time, number actions resulting relaxed plan taken heuristic estimate
distance evaluated state goal.
purpose RPG support heuristic computation. Relaxed planning undertaken
two phases: graph expansion, solution extraction. graph expansion phase purpose
build RPG, identifying facts actions become reachable. RPG consists
alternate fact layers, consisting propositions hold optimistic bounds v, action
layers, containing actions whose preconditions satisfied preceding fact layer. case
propositional preconditions, precondition satisfied relevant fact contained
previous layer. case numeric preconditions, satisfied assignment
variables appearing precondition, consistent upper lower bounds, lead
satisfied. define function ub(w, x, y) as:
X w[j] y[j] w[j] 0
ub(w, x, y) =
w[j] x[j] otherwise
w[j]w

(this function defined Section 9.2).
Then, denoting fact layer set propositions, f l(i), upper lower variable bounds
(vmin (i), vmax (i)), precondition w v c action layer considered true iff:
ub(w, vmin (i), vmax (i)) c
seed graph construction, fact layer 0 contains facts true S. Thus, action layer
0 consists actions whose preconditions satisfied fact layer 0. Fact layer 1 set
optimistic outcome taking fact layer 0, applying actions action layer 0.
formally, considering propositions, applying actions action layer i, i.e. actions al(i)
leads fact layer + 1 where:
f l(i + 1) = f l(i) {eff + (a) | al(i)}
Considering numbers, action layer set optimistic increase decrease effects
variable v across actions are, respectively:
inc(i, v) = {(ub(w, vmin (i), vmax (i)) + c) > 0 | al(i) s.t. hv, +=, w v + ci eff n (a)}
dec(i, v) = {(ub(w, vmax (i), vmin (i)) + c) < 0 | al(i) s.t. hv, +=, w v + ci eff n (a)}
68

fiC OLIN : P LANNING C ONTINUOUS C HANGE

exchange minimum maximum bounds v two expressions important:
causes expression extreme possible appropriate direction. Similarly,
optimistic upper lower bounds v, following available assignment effects, are:
(i, v) = max{(ub(w, vmin (i), vmax (i)) + c) | al(i) s.t.hv, =, w v + ci eff n (a)}
(i, v) = min{(ub(w, vmax (i), vmin (i)) + c) | al(i) s.t.hv, =, w v + ci eff n (a)}
new bounds become:
vmax (i + 1)[j] = max{a (i, v[j]), vmax (i)[j] +
vmin (i + 1)[j] = min{a (i, v[j]), vmin (i)[j] +

X

X

inc(i, v[j])}

dec(i, v[j])}

is, find upper (lower) bounds v[j] next layer, choice
applying largest (smallest) single assignment effect, sum increase (decrease) effects. computed bounds variables layer + 1, graph expansion continues
iteratively, finding actions applicable action layer + 1, hence facts layer + 2,
on. Graph expansion terminates one two cases: either fact layer satisfies propositional
numeric goals, addition layers would never lead preconditions
satisfied condition signalled new propositions appearing accumulation
larger smaller bounds variables would lead numeric preconditions becoming
satisfied. case, relaxed problem cannot solved hence, original problem,
plan starting reach G. heuristic value state set .
Assuming graph expansion terminates goals reached, second phase extract
solution planning graph. recursive procedure, regressing goals back
initial fact layer. fact layer augmented set goals (facts numeric preconditions)
achieved layer. Beginning inserting top-level goals G planning
graph first layers appeared, solution extraction repeatedly picks latest
outstanding goal planning graph selects way achieve it. propositional goals,
single action (with effect adding goal) chosen, preconditions inserted goals
achieved (again, earliest possible layers). satisfy numeric goal w v c layer
i, actions effects acting upon variables (with non-zero coefficients) v chosen,
net increase w v, k, sufficient allow residual precondition w v c k
satisfied fact layer 1. point, residual precondition added goal achieved
layer 1 (or earlier possible), preconditions actions chosen support
precondition added goals achieved previous layers.
Solution extraction terminates outstanding goals achieved fact layer 0, since
true state evaluated need supporting actions. actions selected
solution extraction form relaxed plan goal. length (number actions)
relaxed plan forms heuristic estimate, h(S). Additionally, actions relaxed plan
chosen action layer 0 form basis helpful actions S, used restrict
states explored enforced hill-climbing search: action effect common
actions chosen action layer 0 considered helpful.
69

fiC OLES , C OLES , F OX & L ONG

Appendix C. Temporal Reasoning Relaxed Planning Graphs
Several approaches proposed building temporal relaxed planning graphs (TRPGs).
three additional features TRPGs attempt manage, compared RPGs:
1. temporal structure durative actions: aa applied a` applied
it.
2. Action durations: end effects actions available appropriate delay
started.
3. PDDL 2.1 startend semantics, allowing effects preconditions attached
starts ends actions.
TRPG employed Sapa (Do & Kambhampati, 2003) satisfies first two these,
third. Sapa, action compressed temporally-extended action obeying
TGP semantics, discarding delete effects, relaxation, building TGP -style planning
graph (Smith & Weld, 1999). use compression time-stamped TGP representation
captures durations start-before-end relationships, use compression causes
heuristic find false dead-ends cases required concurrency.
TRGP used CRIKEY (Coles, Fox, Halsey et al., 2008) avoids action compression,
ignores durations actions. non-temporal RPG built terms snap-actions used
search, additional precondition end snap-action particular dummy fact,
added corresponding start, appeared preceding fact layer. use snap-actions
means preconditions effects lost (ensuring heuristic longer identifies false
dead-ends created approach used Sapa), limitation heuristic
forced separation start end action, ordering constraint.
CRIKEY 3 (Coles, Fox, Long et al., 2008a), heuristic constructed combine
strengths earlier heuristics, accounting durations actions, whilst
respecting startend semantics. briefly describe construction TRPG, since
basis heuristic used COLIN. structure TRPG similar constructed
Metric-FF, instead fact layer assigned index, assigned time-stamp
(indicating minimum amount time must pass initial layer facts
layer question appear). capture durations actions, record, end action aa ,
earliest layer tmin (aa ) appear. value set 0 actions already
executing state evaluated (as need first insert start action
RPG). actions, value initialised , commencing TRPG construction.
build TRPG follow Algorithm 2. First, number initialisation steps performed.
time-zero fact layer fl (0) initialised (at line 1) contain facts true 10 . set
ea initialised contain end snap-actions must appear TRPG action
executing, end reachable (i.e. appear TRPG), else state dead end.
ea empty, satisfies goals G (line 14), TRPG need built, since plan
complete.
Following initialisation, TRPG expanded, beginning = 0 using fact layer
f l(t) determine action layer al(t). preconditions action satisfied fact layer
10. simplicity omit handling numeric fluents explanation performed exactly
earlier description RPG heuristic implemented Metric-FF.

70

fiC OLIN : P LANNING C ONTINUOUS C HANGE

Algorithm 2: Building Temporal RPG CRIKEY 3.
Data: = hF, E, - state evaluated
Result: R = hfls, alsi, relaxed planning graph
1 fl (0) F ;
2 fls hfl (0)i;
3 als h i;
4 0;
5 ea ;
6 prev al ;
7 prev fl fl (0);
8 foreach aa
9
{e E | e.op = a} =
10
tmin (aa ) ;
11
else
12
tmin (aa ) 0;
13
ea ea {aa };
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37

G fl (0) ea = return goal state;
<
fl (t + ) prev fl ;
al (t) {aa | pre(aa ) fl (t) tmin (aa ) t};
foreach aa al (t) prev al
fl (t + ) fl (t + ) eff + (aa );
al (t) al (t) {a` | pre(a` ) fl (t)};
foreach a` al (t) prev al
fl (t + ) fl (t + ) eff + (a` );
tmin (aa ) = min[tmin (aa ), + dmin(a)];
als als + al (t);
fls fls + fls(t + );
prev al al (t);
G fl (t + ) ea al (t)
return R = hfls, alsi;
prev fl 6= fl (t + )
prev fl fl (t + );
+ ;
else
prev fl fl (t + );
ep = {tmin (aa ) | pre(aa ) fl (t) tmin (aa ) > t};
ep 6= min[ep];
else ;
return dead end;

71

fiC OLES , C OLES , F OX & L ONG

fl (t) whether appear al (t) depends whether start end snap-action.
first simpler case (line 20) that, start snap-action a` applicable, added al (t)
tmin (aa ) set + dmin(a), dmin(a) priori lower bound duration
a. state-independent measure minimum duration a, i.e. minimum duration
constraint referring constants, taken value dmin(a). Otherwise,
minimum duration constraints depends state action applied,
dmin(a) = : certain time must elapse start end
action. state-dependent terms cannot evaluated since TRPG determines relaxed state,
real state.
second case, covering end snap-actions, preconditions end action aa
become satisfied fact layer fl (t), addition aa al (t) depends whether start
action occurred sufficiently far past (line 17). tmin (aa ) aa added
al(t); < tmin (aa ) < , aa postponed al(tmin (aa )); otherwise, start
action yet appear, aa postponed relevant start appears.
determined actions newly appear al (t), fact layer fl (t + ) updated
non-temporal RPG case, taking fl (t) (optimistically) applying effects
actions al (t). f l(t + ) al(t) contain necessary goals end snap-actions
(line 27) must decided fact layer consider next. Clearly, infeasible create
new fact layers spacing fl (0) fact layer goals appear. Fortunately,
unnecessary, many fact action layers graph would identical.
Instead, determine next fact layer consider follows:
new facts fl (t+) true fl (t) (line 29), next layer expand
fl (t + ) appearance new potentially useful facts makes necessary consider
whether actions become applicable layer.
fl (t + ) = fl (t), know visiting fl (t + ) futile. case (line 34),
time-stamp next fact layer visit earliest future point postponed
end action becomes applicable:
min{tmin (aa ) | pre(aa ) f l(t) tmin (aa ) > t}
minimum values (or undefined) state pruned
procedure exits early, signalling result search procedure.
TRPG successfully constructed (that is, starting state dead end) graph
returned contains finite set fact action layers, associated real time value.
Assuming graph expansion terminates goals reached, relaxed solution extracted.
solution extraction procedure used Metric-FF needs one minor modification suitable
use TRPG: end action aa chosen support goal given fact layer,
action already executing state evaluated, corresponding start a` must
scheduled selection (at layer first appeared). purpose corresponds
dummy facts CRIKEY: end action chosen, start must executed.
final remark TRPG, timed initial literals (TILs) included employing
machinery introduced delay ends actions appropriate layer. dummy TIL
actions {TILj ...TILm } yet applied tmin (TILj ) = 0.0, since TILj could applied
first action layer. intuition state evaluated snapshot world,
72

fiC OLIN : P LANNING C ONTINUOUS C HANGE

taken earlier end previous action, later point
next TIL event occurs (due constraints discussed Section 6.1). minimum timestamps
later TILs, TILk {TILj+1 ...TILm }, set relative time point:
tmin (TILk ) = ts(TILk ) ts(TILj ).

Appendix D. Post-Hoc Plan Optimisation
appendix contains details MILP construction briefly described Section 10.2.
D.1 Optimising Time Windows
First let us consider simple case action conditional effect metric-tracking
variable reward (where objective problem maximise reward), effect
occurs depends truth value single proposition p time specifier ts relative
action (either start, all, end):
(when (ts (p)) (at end (increase (reward) k))).
case p manipulated actions, without allowing MILP introduce new
actions completely change order plan steps (with complexity modifications
would entail), little scope optimisation. case truth value p dictated
timed initial literals (TILs), interesting case: changing time-stamps
start end (LP variables step step j ), condition satisfied,
direct effect metric function. relationship encoded within LP. way
example, consider case p becomes true time false b; then, again, becomes
true c false d. case, two time-windows could potentially satisfy
condition effect. Whether action wholly partially within one
windows depends time-specifier attached p:
ts =at start, a` (step ) lie within one time windows;
ts =at end, aa (step j ) lie within one time windows;
otherwise, ts =over all, a` aa lie within one time windows.
three cases, question must answered value variable lie within
known range? case requires conjunction two conditions hold and,
two cases, one hold. given step variable step , time window (a, b),
introduce (MI)LP binary variable switch ab corresponding observation,
constraints take logical form:
switch ab (step > a) (step < b)
Thus, switch variable takes value 1, time-stamp point p needed must
fall within time-window [a, b] vice versa. introducing two additional binary variables,
denoted ga lb, logical constraint represented series inequalities (using N
73

fiC OLES , C OLES , F OX & L ONG

denote large number):
step (a + ) switch ab
step + (b ) switch ab
step + N ga
step N lb
switch ab ga lb







0
0

b
1

first two constraints encode forwards implication: switch ab set 1, step
lie range [a + , b ] (a non-zero amount separation, epsilon, needed
PDDL semantics avoid inspecting value p time changed
TIL). latter three constraints encode reverse implication: step strictly greater
strictly less b, ga lb hold value 1 thus, switch ab .
Returning example, time specifier all, windows (a, b)
(c, d), constraints added are:
switch ab1
switch ab2
switch ab
switch cd1
switch cd2
switch cd
switch p









(step > a) (step < b)
(step j > a) (step j < b)
(switch ab1 switch ab2 )
(step > c) (step < d)
(step j > c) (step j < d)
(switch cd1 switch cd2 )
(switch ab switch cd )

is, switch ab 1 entirety action falls within (a, b), switch cd 1 falls
within (c, d) switch p 1 either hold. final switch variable used capture
benefit effect itself: holds value 1, increase value reward end
plan k, is, apply conditional effect. variable reward already appear
objective function form LP variable reward 0n , n last step plan.
Thus, modify constraints define reward 0n k switch p added value.
change ensure variable providing value reward objective function
include reward k condition time executed holds.
Generalising, extend case conditional effect depends truth
formula f consisting conjunction time-specified propositional facts [(ts1 p1 )...(tsj pj )].
(tsi pi ) f , create constraints, indicated above, switch variable switch pi
take value 1 pi holds time-specifier tsi . gives us list switch variables
= [switch p1 ...switch pj ]. Then, encode fact conjunction f must hold, create
variable switch f add constraints:
j switch f + 1.switch p1 + ... + 1.switch pj 0
switch f + 1.switch p1 + ... + 1.switch pj 1 j
Defined thus, switch f takes value 1 iff switch variables takes value 1,
precisely case conjunct satisfied. Then, much before, updating
constraint dictating value LP variable reward 0n , add k switch f value.
D.2 Optimising Numeric-Dependent Conditions
Perhaps complex case time windows conditions conditional effect
depend values numeric variables domain. (The PDDL 2.2 definition (Hoffmann
74

fiC OLIN : P LANNING C ONTINUOUS C HANGE

& Edelkamp, 2005) include case TILs change values numeric variables11 ,
consider case here.) simple case, time-specifier numeric
conditions either start end. complicated case one
time-specifiers all. case, potentially, snap-actions plan,
start end action condition belongs, could affect whether condition
associated effect met. must therefore check status condition
point execution action. Suppose action O, stepk stepl
variables denoting start end time-stamps action, conditional effect
numeric precondition LNF:
(over (>= (w v) c)) (at end (increase (reward) k)).
encode this, need add constraints ensure conditional outcome occurs iff w
v c times within O. Since change linear, (as conditions
O) need check values numeric variables immediately immediately
action time step within O, immediately following start immediately
end itself. Thus, variables corresponding values v must examine
list:
0
0
, vl ].
, ..., vl1 , vl1
e = [vk0 , vk+1 , vk+1
stated earlier, case start/at end conditions somewhat easier: start,
e = [vk ], end, e = [vl ]. Irrespective time specifier, basis list e,
capture whether condition met, adding switch variable switch indicate whether
condition met vectors, switch variables [switch t1 ...switch tn ] element [1..s]
list e, indicating whether met single vector. constraints (where
small number) then:


w.e[x] N + (N + c) switch

x[1...s]



w.e[x] (c ) + N switch tx

x[1...s]

switch + 1.switch t0 + ... + 1.switch ts 1 s.
first quantification ensures switch = 1, lower bound c imposed w v
element e. second quantification ensures vector v index x e satisfies
w v c, corresponding switch variable switch tx take value 1. third
constraint ensures switch variables switch tx take value 1, switch must, too, set
1. appropriately constrained switch variable update constraint governing
value LP variable reward 0n (the value reward end plan) increase
value k switch.
D.3 Optimising Time-Dependent Conditions
final extension allow conditional effects refer truth values timed
propositions, values numeric variables, value duration action.
11. Timed Initial Fluents used domain models, unofficial extension language.
semantics extension straightforward Timed Initial Literals.

75

fiC OLES , C OLES , F OX & L ONG

situation appears example airplane landing problem (Section 10.2) value
(total-cost) updated conditional effect, condition effect depend
?duration. consider example order show MILP extended
handle updates. First, previous cases, need add constraints ensure
MILP solver chooses obtain conditioned outcome conditioned effect, condition
must met. So, example, introduce new variable binary switch variable
condition, new constraints. land action plane ?p, starting finishing
time-stamps action step n step respectively, add pair constrained switch variables.
sake example give meaningful names early late. constraints
added LP then:
target p step + step n
step step n
target p + step step n
step step n






N early
N (N + target p) early
N late
(target p + ) late

new constraints ensure plane lands early, variable early take value
1, vice versa. Similarly, lands late, late must take value 1 vice versa. case
example, conditional effects action mutually exclusive, though true
general case.
defined early late switch variables, objective function MILP must
augmented reflect conditional outcomes action. Two terms must added one
switch variable effect obtained switch variable 1. Abbreviating terms
earlyPenaltyRate, latePenaltyRate latePenalty epr, lpr lp, respectively,
objective terms plane p are:
early (epr p) (target p (step step n ))
late (lpr p) ((step step n ) target p) + late (lp p)
Note unlike previous cases, objective function quadratic: objective
contains terms switch variable multiplied constant step variable.
arises as, unlike previous cases, conditional effect duration dependent fixed, constant value k. Whilst raises computational cost optimising MILP, cost acceptable: incurred once, solution plan found.

Appendix E. Details Empirical Evaluation Colin
graphs presented show detailed runtime quality comparisons analysed Section 12. comparative data graphed. Since graphs sometimes superimpose curves one
another, making difficult see COLIN performing, Tables 613 show raw time
quality results COLIN compared average best times qualities problems. Best
times qualities reported corresponding quality time (respectively)
solution, planner(s) generated best result.

76

fiC OLIN : P LANNING C ONTINUOUS C HANGE

Depots Simple Time
1000

100

100

10

Time (s)

Time (s)

Driverlog Simple Time
1000

Colin
LPG-TD
LPG.s
Sapa
Temp-Baseline

10

1

1

0.1

0.1

0.01

Colin
LPG-TD
LPG.s
Sapa
Temp-Baseline

0.01
5

10
Problem Number

15

20

2

4

6

Rovers Simple Time
100

10

Time (s)

Time (s)

10
12
Problem Number

14

16

18

20

14

16

18

20

Satellite Simple Time
100

Colin
LPG-TD
LPG.s
Sapa
Temp-Baseline

10

8

1

0.1

Colin
LPG-TD
LPG.s
Sapa
Temp-Baseline

1

0.1

0.01

0.01
2

4

6

8

10
12
Problem Number

14

16

18

20

14

16

18

20

2

4

6

8

10
12
Problem Number

Zeno Simple Time
10000

1000

Colin
LPG-TD
LPG.s
Sapa
Temp-Baseline

Time (s)

100

10

1

0.1

0.01
2

4

6

8

10
12
Problem Number

Figure 21: Comparison time taken solve problems various simple temporal planning benchmarks planners COLIN, LPG-td, LPG.s, Sapa temporal baseline planner.
Planners appearing particular dataset solve problems
collection.

77

fiC OLES , C OLES , F OX & L ONG

Depots Simple Time
600

Driverlog Simple Time
3000

Colin
LPG-TD
LPG.s
Sapa
Temp-Baseline

500

2500

2000
Makespan

400
Makespan

Colin
LPG-TD
LPG.s
Sapa
Temp-Baseline

300

1500

200

1000

100

500

0

0
5

10
Problem Number

15

20

2

4

6

Rovers Simple Time
400

400

250

Makespan

Makespan

300

10
12
Problem Number

14

16

18

20

14

16

18

20

Satellite Simple Time
500

Colin
LPG-TD
LPG.s
Sapa
Temp-Baseline

350

8

200
150

Colin
LPG-TD
LPG.s
Sapa
Temp-Baseline

300

200

100
100
50
0

0
2

4

6

8

10
12
Problem Number

14

16

18

20

14

16

18

20

2

4

6

8

10
12
Problem Number

Zeno Simple Time
7000
6000

Colin
LPG-TD
LPG.s
Sapa
Temp-Baseline

Makespan

5000
4000
3000
2000
1000
0
2

4

6

8

10
12
Problem Number

Figure 22: Comparison plan quality problems various simple temporal planning benchmarks planners COLIN, LPG-td, LPG.s, Sapa temporal baseline planner.
Planners appearing particular dataset solve problems
collection.

78

fiC OLIN : P LANNING C ONTINUOUS C HANGE

Depots Time
100

Driverlog Time
10000

Colin
LPG-TD
LPG.s
Sapa
Temp-Baseline

10

Colin
LPG-TD
LPG.s
Sapa
Temp-Baseline

1000

Time (s)

Time (s)

100

1

10

1
0.1
0.1

0.01

0.01
5

10
Problem Number

15

20

2

4

6

8

Rovers Time
100

14

16

18

20

14

16

18

20

Satellite Time
1000

Colin
LPG-TD
LPG.s
Sapa

100

Time (s)

10

Time (s)

10
12
Problem Number

1

Colin
LPG-TD
LPG.s
Sapa
Temp-Baseline

10

1
0.1
0.1

0.01

0.01
2

4

6

8

10
12
Problem Number

14

16

18

20

2

4

6

8

10
12
Problem Number

Figure 23: Comparison time taken solve problems complex temporal planning benchmarks (first set) planners COLIN, LPG-td, LPG.s, Sapa temporal baseline
planner. Planners appearing particular dataset solve problems
collection.

79

fiC OLES , C OLES , F OX & L ONG

Zeno Time
1000

Airport Strips Temporal
1000

Colin
LPG-TD
LPG.s
Sapa

100

100

Time (s)

10

Time (s)

Colin
LPG-TD
LPG.s
Temp-Baseline

10

1

1

0.1

0.1

0.01

0.01
2

4

6

8

10
12
Problem Number

14

16

18

20

5

10

15

Pipes No-Tankage Temporal
10000

1000

35

40

45

50

35

40

45

50

Pipes Tankage Temporal
10000

Colin
LPG-TD
LPG.s
Temp-Baseline

1000

Colin
LPG-TD
LPG.s
Temp-Baseline

100
Time (s)

100
Time (s)

20
25
30
Problem Number

10

10

1

1

0.1

0.1

0.01

0.01
5

10

15

20
25
30
Problem Number

35

40

45

50

5

10

15

20
25
30
Problem Number

Figure 24: Comparison time taken solve problems complex temporal planning benchmarks (second set) planners COLIN, LPG-td, LPG.s, Sapa temporal baseline
planner. Planners appearing particular dataset solve problems
collection.

80

fiC OLIN : P LANNING C ONTINUOUS C HANGE

Depots Time
2000

Driverlog Time

Colin
LPG-TD
LPG.s
Sapa
Temp-Baseline

1500

7000
6000

Colin
LPG-TD
LPG.s
Sapa
Temp-Baseline

Makespan

Makespan

5000

1000

4000
3000
2000

500

1000
0

0
2

4

6

8

10
12
Problem Number

14

16

18

20

2

4

6

8

Rovers Time
800

1400

600

1200

500

1000

Makespan

Makespan

14

16

18

20

14

16

18

20

Satellite Time
1600

Colin
LPG-TD
LPG.s
Sapa

700

10
12
Problem Number

400

800

300

600

200

400

100

200

0

Colin
LPG-TD
LPG.s
Sapa
Temp-Baseline

0
2

4

6

8

10
12
Problem Number

14

16

18

20

2

4

6

8

10
12
Problem Number

Figure 25: Comparison plan quality complex temporal planning benchmarks (first set)
planners COLIN, LPG-td, LPG.s, Sapa temporal baseline planner. Planners
appearing particular dataset solve problems collection.

81

fiC OLES , C OLES , F OX & L ONG

Zeno Time
400

Airport Strips Temporal
1000

Colin
LPG-TD
LPG.s
Sapa

350

800

Colin
LPG-TD
LPG.s
Temp-Baseline

250

Makespan

Makespan

300

200
150

600

400

100
200
50
0

0
2

4

6

8

10
12
Problem Number

14

16

18

20

5

10

15

Pipes No-Tankage Temporal
140
120

35

40

45

50

35

40

45

50

Pipes Tankage Temporal

Colin
LPG-TD
LPG.s
Temp-Baseline

140
120

Colin
LPG-TD
LPG.s
Temp-Baseline

100
Makespan

100
Makespan

20
25
30
Problem Number

80
60

80
60

40

40

20

20

0

0
5

10

15

20
25
30
Problem Number

35

40

45

50

5

10

15

20
25
30
Problem Number

Figure 26: Comparison plan quality complex temporal planning benchmarks (second
set) planners COLIN, LPG-td, LPG.s, Sapa temporal baseline planner. Planners appearing particular dataset solve problems collection.

82

fiC OLIN : P LANNING C ONTINUOUS C HANGE

COLIN

Time
Quality
depotssimpletime
1
0.03
36.008
2
0.03
54.013
3
13.69 170.037
4
11.67
82.031
5
6
7
2.56
51.024
8
9
10
1.39
90.025
11
12
13
0.17
85.026
14
15
16
0.25
99.025
17
0.48
61.016
18
19
20
21
1.69
96.029
22
driverlogsimpletime
1
0.02
92.006
2
0.16
169.021
3
0.04
67.012
4
0.25
129.018
5
0.11
106.017
6
0.09
114.011
7
0.05
58.011
8
0.19
151.023
9
0.42
213.026
10
0.04
84.021
11
0.04
108.019
12
3.22
380.041
13
1.09
283.039
14
1.45
240.036
15
0.25
283.043
16
17
18
19
20

Average
Time
Quality

Best
Time

Planner

Quality

Planner

LPG -td,TBL

28.000 (0.0)
46.090 (0.374)
80.002 (0.11)
56.24 (0.42)
115.000 (0.11)
156.004 (205.29)
45.001 (0.11)
87.003 (0.48)
190.000 (0.29)
52.000 (0.05)
152.000 (0.26)
133.000 (0.62)
64.001 (0.33)
64.001 (0.96)
186.000 (0.45)
46.000 (0.08)
28.050 (10.728)
105.000 (1.04)
94.000 (0.19)
101.002 (27.84)
73.000 (0.56)
329.007 (111.46)

LPG -s+td

91.001 (0.04)
104.001 (0.03)
40.02 (0.116)
99.001 (0.02)
75.08 (0.957)
64.070 (0.963)
49.090 (0.3)
77.090 (0.869)
150.002 (0.13)
49.090 (0.404)
85.090 (0.474)
274.3 (0.05)
240.003 (0.76)
163.22 (4.388)
157.210 (13.457)
1510.000 (50.78)
653.008 (16.28)
361.67 (79.06)
1478.000 (47.08)
478.000 (6.19)

LPG -s

0.040
0.097
3.462
3.097
1.620
103.400
0.677
0.223
0.625
4.100
7.933
5.880
0.475
0.400
2.655
0.928
2.716
3.217
0.517
10.443
17.448
54.667

34.834
64.243
110.817
74.818
131.250
171.002
59.049
95.784
213.300
81.465
212.845
188.251
79.265
83.767
212.502
85.273
55.261
109.144
118.787
238.291
85.872
406.609

0.0 (28.000)
0.01 (54.11)
0.02 (82.000)
0.04 (88.000)
0.11 (115.000)
1.51 (186.000)
0.01 (82.17)
0.09 (105.000)
0.29 (190.000)
0.03 (88.2)
0.26 (152.000)
0.62 (133.000)
0.02 (82.17)
0.1 (107.3)
0.45 (186.000)
0.04 (98.18)
0.23 (61.000)
1.03 (117.43)
0.14 (164.36)
0.75 (209.000)
0.19 (90.18)
8.95 (536.000)

0.024
0.166
0.039
0.150
0.223
0.241
0.086
0.238
0.301
0.131
0.153
1.012
0.650
1.790
3.299
112.980
7.133
64.420
47.080
6.190

96.025
127.460
59.215
112.460
92.054
93.031
63.838
153.071
204.692
93.645
104.058
339.586
274.338
291.536
278.920
1849.014
790.049
644.560
1478.000
478.000

0.0 (91.05)
0.01 (110.19)
0.01 (40.04)
0.01 (110.15)
0.01 (83.17)
0.02 (74.000)
0.01 (51.09)
0.01 (167.24)
0.04 (232.26)
0.01 (71.11)
0.01 (119.18)
0.05 (274.3)
0.31 (299.000)
0.24 (391.000)
0.14 (278.34)
50.78 (1510.000)
1.86 (1052.000)
21.03 (869.000)
47.08 (1478.000)
6.19 (478.000)

TBL
LPG -td
LPG -td
LPG -td
LPG -td
TBL
LPG -td
LPG -td
TBL
LPG -td
LPG -td
TBL
TBL
LPG -td
TBL
LPG -td
TBL
TBL
LPG -td
TBL
LPG -td
TBL
LPG -td,TBL
LPG -s,TBL

TBL
LPG -td,TBL
LPG -td
TBL
TBL
LPG -td,TBL
TBL
TBL
TBL
LPG -td
LPG -td
TBL
LPG -td
LPG -td
LPG -td
LPG -td
LPG -td

Sapa
LPG -s

TBL
LPG -td
LPG -s
LPG -s
LPG -s
LPG -td
LPG -td
LPG -td
LPG -td
LPG -s
LPG -s
LPG -td
LPG -td

Sapa
LPG -td
LPG -td
LPG -s
LPG -td
LPG -s

LPG -s

Sapa
LPG -s

Sapa
Sapa
Sapa
Sapa
LPG -s
Sapa
Sapa
TBL
LPG -s
Sapa
Sapa
LPG -td
LPG -s
TBL
LPG -td
LPG -td

Table 6: Results Simple Domains: Best results show best time (corresponding quality)
planner(s) achieved time best quality (corresponding time) planner(s)
achieving quality. TBL Temporal Baseline planner following tables.

83

fiC OLES , C OLES , F OX & L ONG

COLIN

Time
Quality
roverssimpletime
1
0.02
67.007
2
0.02
48.006
3
0.02
73.01
4
0.02
50.005
5
0.04
126.014
6
0.09
186.028
7
0.04
107.013
8
0.06
119.018
9
0.09
176.028
10
0.11
155.019
11
0.11
161.025
12
0.05
103.014
13
0.22
198.029
14
0.12
170.021
15
0.21
208.038
16
0.26
203.032
17
0.23
267.036
18
0.49
217.039
19
0.72
339.047
20
9.51
392.063
satellitesimpletime
1
0.01
41.008
2
0.01
65.012
3
0.02
50.01
4
0.04
87.019
5
0.05
74.016
6
0.07
72.019
7
0.09
72.022
8
0.14
84.024
9
0.21
95.028
10
0.26
101.029
11
0.37
113.031
12
2.49
137.041
13
13.38 214.061
14
5.11
166.039
15
6.47
183.056
16
5.78
170.045
17
4.52
133.041
18
0.82
107.031
19
27.98 349.075
20

Average
Time
Quality

Best
Time

Planner

Quality

Planner

TBL
TBL
LPG -td
TBL
LPG -td,TBL
LPG -td
TBL
TBL
LPG -td,TBL
TBL
TBL
TBL
TBL
TBL
TBL
LPG -td
LPG -td
LPG -td
LPG -td
LPG -td

67.007 (0.02)
45.000 (0.02)
67.089993 (0.15)
45.039997 (0.081)
107.15 (0.01)
183.29 (0.02)
91.001 (0.04)
113.19 (0.02)
156.18999 (0.508)
139.14 (25.24)
161.025 (0.11)
88.08 (1.035)
173.17998 (6.716)
139.000 (0.04)
175.21999 (2.776)
186.20998 (6.67)
218.22993 (9.01)
140.28 (0.15)
297.5 (0.38)
351.000 (0.49)

COLIN

TBL

41.000 (0.01)
65.000 (0.02)
29.000 (0.02)
58.000 (0.01)
61.001 (0.09)
58.09 (0.01)
46.001 (0.13)
41.000 (0.03)
51.001 (0.33)
63.000 (0.05)
72.000 (0.07)
98.000 (0.11)
99.002 (2.75)
68.000 (0.13)
58.001 (2.54)
73.002 (4.12)
92.002 (4.29)
75.002 (1.42)
114.003 (2.57)
137.24 (31.225)

0.032
0.030
0.040
0.030
0.220
0.058
0.146
0.247
0.148
5.110
0.085
0.253
1.565
0.239
0.671
1.464
1.992
2.158
1.058
4.317

72.834
48.223
76.238
51.621
131.461
229.580
97.851
131.066
166.296
158.878
179.322
112.041
227.708
157.483
207.310
208.513
255.936
170.494
319.138
380.932

0.0 (67.08)
0.01 (47.06)
0.0 (77.000)
0.01 (50.06)
0.01 (107.15)
0.01 (277.000)
0.01 (98.13)
0.02 (113.19)
0.02 (159.000)
0.02 (141.23)
0.02 (185.26)
0.01 (90.11)
0.06 (190.33)
0.02 (145.22)
0.04 (215.29)
0.04 (242.000)
0.11 (259.000)
0.1 (160.000)
0.23 (319.000)
0.49 (351.000)

0.022
0.042
0.049
0.080
0.103
0.128
0.179
0.266
0.517
0.563
0.806
2.183
8.604
4.859
10.845
13.916
16.070
2.084
9.740
9.739

43.032
66.310
45.040
78.268
75.259
72.846
67.655
73.267
70.866
79.678
90.667
119.533
165.517
111.078
152.335
143.923
124.495
93.269
170.718
231.793

0.0 (46.07)
0.01 (65.012)
0.01 (58.09)
0.01 (58.000)
0.01 (82.13)
0.01 (58.09)
0.02 (75.12)
0.03 (41.000)
0.04 (58.000)
0.05 (63.000)
0.07 (72.000)
0.11 (98.000)
0.2 (208.000)
0.13 (68.000)
0.17 (183.000)
0.23 (137.000)
0.21 (142.000)
0.1 (101.000)
0.17 (130.000)
0.26 (196.000)

COLIN ,TBL

TBL
LPG -td,TBL
TBL
TBL
TBL
LPG -td,TBL
LPG -td,TBL
LPG -td
LPG -td,TBL
LPG -td
LPG -td
LPG -td
LPG -td
LPG -td
LPG -td
LPG -td
LPG -td
LPG -td

LPG -s

Sapa
Sapa
TBL
TBL
LPG -s
TBL
Sapa
Sapa
COLIN

Sapa
Sapa
LPG -td
Sapa
Sapa
Sapa
TBL
TBL
LPG -td
LPG -td
LPG -td
LPG -td
LPG -td
LPG -s

TBL
LPG -s
LPG -td
LPG -s
LPG -td
LPG -td
LPG -td
LPG -s
LPG -td
LPG -s
LPG -s
LPG -s
LPG -s
LPG -s

Sapa

Table 7: Results Simple Domains: Best results show best time (corresponding quality)
planner(s) achieved time best quality (corresponding time) planner(s)
achieving quality.

84

fiC OLIN : P LANNING C ONTINUOUS C HANGE

COLIN

Time
zenosimpletime
1
0.01
2
0.02
3
0.02
4
0.04
5
0.03
6
0.05
7
0.08
8
0.15
9
0.31
10
0.12
11
0.26
12
0.3
13
6.19
14
15
19.48
16
477.47
17
759.19
18
19
48.95
20
163.2

Quality
173.001
592.008
350.007
885.013
656.011
995.016
931.014
895.013
1583.027
1191.022
796.015
1208.027
2062.04
2836.051
3173.045
4947.071
4309.079
5364.096

Average
Time
Quality
0.016
0.033
0.047
0.062
0.121
0.191
0.218
0.440
1.021
0.696
1.785
1.900
5.261
360.254
10.105
127.658
246.098
43.737
22.740
59.687

178.602
666.022
356.619
1226.041
868.832
1456.434
1006.035
869.291
1268.456
1461.053
823.433
1617.856
1303.254
1577.821
2561.808
2394.072
5232.603
2983.101
5043.146
5315.165

Best
Time

Planner

Quality

Planner

0.0 (180)
0.01 (866.05)
0.01 (280.04)
0.01 (936.000)
0.0 (400.06)
0.01 (603.06)
0.01 (706.08)
0.02 (836.07)
0.03 (789.12)
0.03 (743.13)
0.03 (763.1)
0.03 (1199.13)
0.03 (923.14)
0.3 (2068.18)
0.46 (2254.18)
0.86 (1702.24)
2.36 (3436.34)
2.61 (3453.3)
8.91 (3769.36)
6.43 (4578.4)

TBL
TBL
TBL
LPG -td,TBL
TBL
TBL
TBL
TBL
TBL
TBL
TBL
TBL
TBL
TBL
TBL
TBL
TBL
TBL
TBL
TBL

173.001 (0.01)
592.008 (0.02)
280.04 (0.01)
885.013 (0.04)
400.06 (0.0)
603.06 (0.01)
706.08 (0.01)
836.07 (0.02)
789.12 (0.03)
743.13 (0.03)
510.050 (8.037)
1166.120 (8.568)
923.14 (0.03)
1169.100 (1433.534)
2254.18 (0.46)
1702.24 (0.86)
3436.34 (2.36)
2383.003 (121.90)
3769.36 (8.91)
4578.4 (6.43)

COLIN
COLIN

TBL
COLIN

TBL
TBL
TBL
TBL
TBL
TBL
Sapa
Sapa
TBL
Sapa
TBL
TBL
TBL
LPG -s
TBL
TBL

Table 8: Results Simple Domains: Best results show best time (corresponding quality)
planner(s) achieved time best quality (corresponding time) planner(s)
achieving quality.

85

fiC OLES , C OLES , F OX & L ONG

COLIN

Time
Quality
airportstripstemporal
1
0.06
64.007
2
0.06
185.008
3
0.09
202.011
4
0.30
127.019
5
0.26
227.02
6
0.30
301.04
7
0.30
301.04
8
0.52
538.059
9
1.40
516.058
10
0.33
126.017
11
0.35
228.02
12
0.36
265.035
13
0.44
311.034
14
0.71
528.057
15
0.58
365.049
16
1.68
625.076
17
11.47 603.084
18
58.89 777.095
19
38.65 574.076
20
21
21.65
366.1
22
22.52 487.147
23
23.68 510.166
24
31.00 826.156
25
90.81 934.204
26
27
28
29
30
31
32
33
34
35
36
46.83 408.108
37
78.05 672.136
38
49.89 847.217
39
40
41
52.64 895.241
42
43
44
46

Average
Time
Quality
0.030
0.030
0.040
0.120
0.103
0.130
0.130
0.237
1.672
0.130
0.135
0.155
0.170
0.465
0.257
2.738
6.093
25.983
226.607
66.563
80.240
6.178
6.425
8.307
34.577
53.480
5.190
94.760
94.955
5.420
6.820
443.075
14.950
15.710
17.110
16.063
36.820
17.833
476.650
91.395
24.803
841.960
35.450
701.750
289.270

64.026
185.022
200.533
127.070
227.055
251.363
251.363
432.165
433.918
126.062
228.055
239.352
254.852
426.657
357.633
536.440
542.947
652.658
453.467
643.625
305.519
428.104
369.363
493.835
650.231
480.785
511.111
623.910
596.410
641.000
834.000
724.570
859.000
879.000
887.000
350.705
620.047
580.741
962.000
510.004
596.749
579.000
639.506
331.000
737.000

Best
Time
0.01 (64.07)
0.01 (185.000)
0.01 (200.12)
0.02 (127.19)
0.02 (227.2)
0.02 (240.41)
0.02 (240.41)
0.06 (402.6)
0.10 (402.000)
0.01 (126.17)
0.02 (228.2)
0.02 (228.37)
0.03 (237.37)
0.07 (390.57)
0.06 (273.48)
0.14 (558.000)
0.61 (569.000)
0.25 (733.000)
0.50 (413.000)
0.27 (740.000)
0.33 (285.97)
0.78 (286.26)
0.49 (265.28)
0.61 (377.18)
2.47 (539.000)
3.88 (584.000)
7.50 (505.33)
7.53 (706.000)
4.47 (687.000)
5.42 (641.000)
6.82 (834.000)
7.78 (815.000)
14.95 (859.000)
15.71 (879.000)
17.11 (887.000)
1.36 (322.000)
32.41 (831.000)
3.61 (573.000)
476.65 (962.000)
182.79 (584.000)
21.77 (573.000)
841.96 (579.000)
70.90 (573.000)
701.75 (331.000)
289.27 (737.000)

Planner

Quality

Planner

TBL

64.000 (0.02)
185.000 (0.01)
200.000 (0.03)
127.000 (0.04)
227.000 (0.05)
232.000 (0.07)
232.000 (0.07)
394.000 (0.09)
402.000 (0.10)
126.000 (0.05)
228.000 (0.07)
228.37 (0.02)
230.002 (0.14)
390.57 (0.07)
273.48 (0.06)
404.68 (0.24)
417.7 (2.16)
447.88 (18.81)
413.000 (0.50)
450.87 (55.03)
285.000 (0.77)
286.26 (0.78)
264.004 (510.166)
376.003 (826.156)
477.49 (10.45)
377.57 (103.08)
504.004 ()
541.82 (181.99)
505.82 (185.44)
641.000 (5.42)
834.000 (6.82)
634.14 (878.37)
859.000 (14.95)
879.000 (15.71)
887.000 (17.11)
322.000 (1.36)
357.006 (672.136)
322.006 (847.217)
962.000 (476.65)
436.007 ()
322.006 (895.241)
579.000 (841.96)
573.000 (70.90)
331.000 (701.75)
737.000 (289.27)

LGP -td

LGP -td,TBL

TBL
TBL
TBL
TBL
TBL
TBL
LGP -td
TBL
TBL
TBL
TBL
TBL
TBL
LGP -td
LGP -td
LGP -td
LGP -td
LGP -td
TBL
TBL
TBL
TBL
LGP -td
LGP -td
TBL
LGP -td
LGP -td
LGP -td
LGP -td
LGP -td
LGP -td
LGP -td
LGP -td
LGP -td
LGP -td
LGP -td
LGP -td
LGP -td
LGP -td
LGP -td
LGP -td
LGP -td
LGP -td

LGP -td
LGP -td
LGP -td
LGP -td
LGP -td
LGP -td
LGP -td
LGP -td
LGP -td
LGP -td

TBL
LGP -s

TBL
TBL
TBL
TBL
TBL
LGP -td
TBL
LGP -td
TBL
LGP -s
LGP -s
TBL
TBL
LGP -s
TBL
TBL
LGP -td
LGP -td
TBL
LGP -td
LGP -td
LGP -td
LGP -td
LGP -s
LGP -s
LGP -td
LGP -s
LGP -s
LGP -td
LGP -td
LGP -td
LGP -td

Table 9: Results Complex Domains: Best results show best time (corresponding quality) planner(s) achieved time best quality (corresponding time)
planner(s) achieving quality.

86

fiC OLIN : P LANNING C ONTINUOUS C HANGE

COLIN

Time
Quality
depotstime
1
0.03
59.746
2
0.14
66.79
3
4
67.41
258.125
5
6
7
35.56
129.741
8
9
10
43.34
296.354
11
12
13
0.27
108.493
14
15
16
0.60
50.588
17
18
19
20
21
2.08
76.753
22
driverlogtime
1
0.02
303.006
2
0.17
462.019
3
0.03
202.009
4
0.14
474.019
5
0.20
343.019
6
0.03
275.007
7
0.12
316.012
8
0.43
699.024
9
0.57
730.028
10
0.21
294.031
11
0.16
522.026
12
5.90
1066.038
13
2.84
1052.031
14
6.51
787.034
15
0.27
212.042
16
17
18
19
20

Average
Time
Quality

Best
Time

Planner

Quality

Planner

TBL
TBL
TBL
LGP -td
LGP -td
LGP -td
TBL
LGP -td,TBL
LGP -td
TBL
LGP -td
LGP -td
TBL
TBL
LGP -td
TBL
TBL
LGP -td
TBL
LGP -td
TBL
LGP -td

55.181 (0.01)
60.556 (0.02)
127.592 (0.03)
160.139 (0.04)
777.544 (3.12)
287.543 (2.35)
107.340 (0.12)
108.399 (0.10)
1655.27 (0.97)
151.173 (11.364)
599.323 (1.42)
132.893 (4.33)
82.834 (1.373)
157.701 (1.02)
382.973 (4.16)
26.390 (0.56)
46.2 (0.20)
395.768 (0.89)
366.474 (1.22)
592.561 (7.98)
54.731 (36.794)
343.110 (3.86)

LGP -td

TBL

302 (0.01)
294.090 (0.678)
173.020 (0.131)
402.001 (0.04)
161.090 (0.973)
260 (0.02)
268.1 (0.01)
388.110 (1.244)
591 (0.02)
220.110 (0.415)
306.13 (0.486)
627.260 (1151.634)
597.180 (4.193)
625.150 (7.34)
212.042 (0.27)
3652.008 (28.03)
2238 (3.31)
1476.68 (78.94)
3993 (63.87)
1691.007 (66.56)

LGP -td

0.036
0.103
0.087
17.043
2.970
2.350
8.933
0.240
0.650
11.027
1.020
5.387
0.417
0.427
2.290
4.754
1.977
3.077
0.520
3.743
9.693
47.470

57.698
71.719
141.362
186.587
781.855
287.543
141.337
119.161
1783.468
226.698
726.121
163.032
99.751
216.623
399.697
76.900
87.495
535.942
428.210
880.639
71.388
445.232

0.00 (58.9311)
0.01 (73.2211)
0.03 (127.592)
0.04 (160.139)
2.82 (786.1666)
2.35 (287.543)
0.02 (142.158)
0.10 (108.3988)
0.33 (1911.6665)
0.03 (294.293)
0.32 (608.9167)
0.46 (165.5889)
0.02 (104.436)
0.10 (329.133)
0.42 (416.4197)
0.03 (166.236)
0.19 (88.26)
0.75 (709.8509)
0.14 (486.722)
0.93 (1050.2797)
0.16 (101.373)
3.86 (343.1095)

0.017
0.178
0.044
0.128
0.249
0.280
0.072
0.361
0.193
0.159
0.181
233.369
1.743
3.248
59.469
66.385
9.580
48.780
110.925
41.060

302.625
442.260
279.018
453.060
250.058
266.432
350.041
695.275
748.078
328.450
515.068
1230.325
1218.305
1125.322
766.155
5120.504
2787.062
2319.229
4005.005
3828.003

0.00 (302.05)
0.01 (438.19)
0.01 (173.06)
0.01 (441.15)
0.01 (195.18)
0.02 (260)
0.01 (268.1)
0.01 (894.24)
0.02 (591)
0.01 (394.11)
0.01 (512.18)
0.05 (829.32)
0.32 (903)
0.41 (1161)
0.10 (939.43)
28.03 (3652.0081)
3.31 (2238)
27.48 (2885)
63.87 (3993)
15.56 (5965)

LGP -td,TBL

TBL
TBL
TBL
LGP -td
LGP -td,TBL
TBL
LGP -td
TBL
TBL
TBL
LGP -td
LGP -td
TBL
LGP -s
LGP -td
LGP -td
LGP -td
LGP -td

LGP -td

TBL
LGP -td

TBL
LGP -td
LGP -s
LGP -td

TBL
Sapa
LGP -s
LGP -s
Sapa
LGP -s
LGP -s
LGP -s
LGP -td
TBL
LGP -s
LGP -s
Sapa
LGP -td

Sapa
Sapa
LGP -s
Sapa
LGP -td
TBL
Sapa
LGP -td
Sapa
Sapa
Sapa
Sapa
Sapa
COLIN
LGP -s
LGP -td

TBL
LGP -td
LGP -s

Table 10: Results Complex Domains: Best results show best time (corresponding quality) planner(s) achieved time best quality (corresponding time)
planner(s) achieving quality.

87

fiC OLES , C OLES , F OX & L ONG

COLIN

Time
Quality
pipesnotankagetemporal
1
0.03
6.003
2
0.04
20.011
3
0.05
12.008
4
0.07
22.012
5
0.05
14.006
6
0.05
16.009
7
0.06
12.007
8
0.06
16.009
9
0.08
24.013
10
0.08
28.015
11
0.16
11.026
12
1.77
22.554
13
0.27
16.535
14
0.38
14.532
15
0.14
11.526
16
6.78
30.07
17
4.25
11.023
18
1.08
12.529
19
0.19
9.525
20
0.46
17.039
21
0.09
9.017
22
23
0.19
14.018
24
40.46
29.032
25
26
1.89
32.053
27
0.29
11.527
28
0.77
22.551
29
2.46
21.546
30
31
9.21
18.357
32
13.01
35.038
33
10.53
21.874
34
20.52
30.709
35
36
37
38
39
1.19
13.857
40
41
103.46
4.43
49
50

Average
Time
Quality
0.015
0.058
0.040
0.058
0.043
0.060
0.075
0.100
0.113
0.120
300.430
302.110
0.745
0.380
0.785
2.667
1.143
0.900
0.247
7.977
0.080
2.975
0.240
27.015
4.850
4.140
1.613
16.697
5.560
4.665
3.240
4.485
10.530
34.070
24.200
78.245
31.760
64.855
0.967
10.410
34.820
346.235
15.400

7.506
51.526
17.520
83.021
13.017
24.023
16.015
16.520
29.026
36.037
9.544
16.971
15.687
14.446
16.700
31.401
10.544
18.221
9.898
26.681
9.062
36.700
21.340
101.016
32.165
28.148
13.572
42.977
18.272
32.205
16.098
51.504
21.874
39.582
17.334
18.078
36.000
12.822
18.356
34.665
4.173
16.493
18.380

Time
0.00 (6.02)
0.01 (20.09)
0.01 (16.07)
0.01 (16.07)
0.01 (12.000)
0.01 (18.08)
0.01 (12.05)
0.03 (18.000)
0.02 (20.09)
0.02 (28.13)
0.05 (8.15)
0.28 (17.33)
0.05 (11.21)
0.15 (14.000)
0.12 (14.27)
0.48 (40.000)
0.09 (8.15)
0.20 (18.35)
0.11 (9.17)
0.46 (0.46)
0.02 (9.17)
2.05 (23.4)
0.19 (14.018)
13.57 (173.000)
1.12 (42.000)
1.46 (27.39)
0.29 (11.527)
0.77 (22.551)
0.34 (14.27)
1.39 (27.41)
0.17 (16.9367)
0.89 (17.31)
10.53 (21.874)
1.40 (23.370033)
48.40 (24.000)
6.49 (21.156633)
31.76 (36.000)
0.32 (13.6433)
0.17 (12.21)
10.41 (34.665)
0.10 (4.09)
2.15 (16.32)
15.40 (18.38)

Best
Planner
TBL
TBL
TBL
TBL
LGP -td,TBL
TBL
TBL
LGP -td
TBL
TBL
TBL
TBL
TBL
LGP -td
TBL
LGP -td
TBL
TBL
TBL
COLIN

TBL
TBL
COLIN
LGP -td
LGP -td

TBL
COLIN
COLIN

TBL
TBL
TBL
TBL
COLIN

TBL
LGP -td
TBL
LGP -td
TBL
TBL
LGP -td
TBL
TBL
TBL

Quality

Planner

6.000 (0.01)
20.011 (0.04)
12.008 (0.05)
16.07 (0.01)
12.000 (0.01)
16.009 (0.05)
12.007 (0.06)
16.001 (0.19)
20.09 (0.02)
28.015 (0.08)
8.15 (0.05)
11.002 (1200.92)
11.21 (0.05)
13.25 (0.99)
11.526 (0.14)
27.53 (3.41)
8.002 (11.023)
12.529 (1.08)
9.17 (0.11)
14.003 (17.039)
9.000 (0.13)
23.4 (2.05)
14.018 (0.19)
29.032 (40.46)
22.33 (8.58)
25.000 (9.07)
10.19 (0.48)
22.551 (0.77)
14.27 (0.34)
27.41 (1.39)
13.000 (0.34)
17.31 (0.89)
21.874 (10.53)
23.370033 (1.40)
10.669 ()
15.000 (150.00)
36.000 (31.76)
12.000 (129.39)
12.21 (0.17)
34.665 (10.41)
4.000 (0.90)
16.32 (2.15)
18.38 (15.40)

LGP -td
COLIN
COLIN

TBL
LGP -td
COLIN
COLIN
LGP -s

TBL
COLIN

TBL
LGP -s

TBL
TBL
COLIN

TBL
LGP -s
COLIN

TBL
LGP -s
LGP -td

TBL
COLIN
COLIN

TBL
LGP -td

TBL
COLIN

TBL
TBL
LGP -td
TBL
COLIN

TBL
LGP -s
LGP -td
LGP -td
LGP -td

TBL
LGP -td
LGP -td

TBL
TBL

Table 11: Results Complex Domains: Best results show best time (corresponding quality) planner(s) achieved time best quality (corresponding time)
planner(s) achieving quality.

88

fiC OLIN : P LANNING C ONTINUOUS C HANGE

COLIN

Time
Quality
pipestankagetemporal
1
0.04
6.003
2
0.12
24.013
3
0.23
12.008
4
0.52
18.009
5
0.09
14.007
6
0.09
16.01
7
0.26
16.008
8
0.30
18.012
9
10
4.40
36.02
11
0.40
13.031
12
13
1.51
13.53
14
53.09
22.052
15
59.61
17.541
17
18
33.06
11.527
19
20.29
16.537
20
2.54
12.029
21
22
23
24
23.25
31.564
25
983.78
28.558
26
32.55
33.053
27
2.30
10.024
29
191.49
23.548
30
58.98
28.555
31
187.22
32.713
32
33
34
160.42
25.038
37
39
9.14
15.191
40
51.82
21.877
41
269.57
4.93
49
50
82.82
22.378

Average
Time
Quality
0.020
0.147
0.180
0.275
0.153
0.160
1.175
1.275
238.057
26.135
316.585
12.480
6.420
334.625
64.690
1500.780
105.157
252.310
118.323
1.830
77.570
41.080
112.000
710.420
389.255
7.000
364.450
354.695
249.670
696.495
304.470
432.870
598.180
298.620
51.820
138.057
91.770
82.820

8.006
63.371
15.520
26.020
20.017
17.018
18.522
23.031
66.741
57.571
27.601
27.105
13.265
19.357
14.886
31.000
28.626
17.582
24.610
14.595
25.690
30.000
37.782
56.483
31.991
16.012
28.274
30.777
31.041
34.158
21.133
40.065
20.665
22.925
21.877
6.687
17.340
22.378

Time
0.00 (6.02)
0.02 (22.1)
0.06 (16.07)
0.04 (16.07)
0.02 (14.06)
0.02 (14.06)
0.07 (18.08)
0.30 (18.012)
0.95 (104.000)
0.32 (54.26)
0.40 (13.031)
5.08 (43.000)
1.51 (13.53)
9.17 (19.37)
59.61 (17.541)
1500.78 (31.000)
33.06 (11.527)
17.59 (11.21)
2.54 (12.029)
0.11 (10.19)
53.50 (30.000)
41.08 (30.000)
23.25 (31.564)
527.52 (116.500)
32.55 (33.053)
2.30 (10.024)
191.49 (23.548)
58.98 (28.555)
0.84 (31.7833)
179.24 (35.3167)
304.47 (21.133333)
31.22 (33.99)
598.18 (20.665)
9.14 (15.191)
51.82 (21.877)
32.63 (6.13)
91.77 (17.34)
82.82 (22.378)

Best
Planner
TBL
TBL
TBL
TBL
TBL
TBL
TBL
COLIN
LGP -td

TBL
COLIN
LGP -td
COLIN

TBL
COLIN
LGP -td
COLIN

TBL
COLIN

TBL
LGP -td
LGP -td
COLIN
LGP -td
COLIN
COLIN
COLIN
COLIN

TBL
TBL
TBL
TBL
LGP -td
COLIN
COLIN

TBL
TBL
COLIN

Quality

Planner

6.000 (0.03)
22.1 (0.02)
12.008 (0.23)
16.07 (0.04)
14.007 (0.09)
14.06 (0.02)
16.000 (0.31)
18.012 (0.30)
46.22 (6.34)
36.02 (4.40)
13.003 (1235.31)
11.21 (19.88)
13.000 (11.33)
18.000 (1276.24)
12.23 (69.77)
31.000 (1500.78)
11.527 (33.06)
11.21 (17.59)
12.029 (2.54)
10.19 (0.11)
21.38 (101.64)
30.000 (41.08)
31.564 (23.25)
24.39 (619.96)
30.93 (745.96)
10.024 (2.30)
23.548 (191.49)
28.555 (58.98)
29.833 (810.62)
33.000 (1213.75)
21.133333 (304.47)
25.038 (160.42)
20.665 (598.18)
15.191 (9.14)
21.877 (51.82)
4.93 (269.57)
17.34 (91.77)
22.378 (82.82)

LGP -s

TBL
COLIN

TBL
COLIN

TBL
LGP -td
COLIN

TBL
COLIN
LGP -s

TBL
LGP -td
LGP -td

TBL
LGP -td
COLIN

TBL
colin
TBL
TBL
LGP -td
COLIN

TBL
TBL
COLIN
COLIN
COLIN
LGP -td
LGP -td

TBL
COLIN
LGP -td
COLIN
COLIN
COLIN

TBL
COLIN

Table 12: Results Complex Domains: Best results show best time (corresponding quality) planner(s) achieved time best quality (corresponding time)
planner(s) achieving quality.

89

fiC OLES , C OLES , F OX & L ONG

COLIN

Time
roverstime
1
0.03
2
0.01
3
0.03
4
0.03
5
0.06
6
40.93
7
0.07
8
0.17
9
10
0.29
11
0.15
12
0.86
13
14
15
16
17
0.70
18
1.35
19
20
92.59
satellitetime
1
0.02
2
0.01
3
0.01
4
0.03
5
0.11
6
0.10
7
0.12
8
0.30
9
0.38
10
0.64
11
0.94
12
3.57
13
21.89
14
6.19
15
8.76
16
22.18
17
15.29
18
3.85
19
57.66
20

Quality
67.007
48.006
63.01
52.006
125.014
273.118
105.017
149.944
177.022
170.881
122.022

230.036
245.864
390.804
129.596
182.916
78.616
140.42
290.12
114.38
126.544
139.608
175.74
295.549
283.351
336.599
433.308
267.73
292.436
336.356
232.66
169.256
520.602

Average
Time
Quality

Best
Time

Planner

Quality

Planner

LGP -td

67.007 (67.007)
46.0007 (0.02)
63.01 (0.03)
52 (0.02)
107.12 (0.702)
273.118 (40.93)
90.538574 (0.411)
134 (0.04)
128.6895 (0.24)
154.48767 (1.448)
170.881 (0.15)
114.130005 (0.682)
237.8161 (0.86)
137.7917 (0.62)
205.3755 (0.10)
210 (0.46)
230.036 (0.70)
155.0909 (0.33)
394.5915 (0.61)
390.804 (92.59)

COLIN

0.048
0.027
0.062
0.040
0.258
14.010
0.138
0.494
0.150
0.534
0.170
0.476
0.560
0.865
0.500
0.460
2.223
1.817
0.610
49.107

73.524
56.269
70.525
52.733
129.069
299.794
107.121
160.906
137.345
192.855
195.395
134.818
292.200
206.147
239.333
210.000
341.230
220.290
394.591
505.667

0.02 (80)
0.01 (48.006)
0.02 (80)
0.02 (52)
0.02 (113.2)
0.08 (284.965)
0.03 (138.9286)
0.04 (134)
0.06 (146)
0.06 (190.3077)
0.07 (200.5)
0.04 (145.5294)
0.26 (346.5833)
0.35 (268.7368)
0.10 (205.3755)
0.46 (210)
0.34 (392.353)
0.33 (155.0909)
0.61 (394.5915)
3.38 (502.7423)

0.026
0.038
0.068
0.099
0.154
0.138
0.265
0.469
0.813
0.949
1.703
9.147
29.900
10.618
34.225
34.882
94.390
5.306
49.424
30.264

193.553
225.716
168.343
319.688
262.900
255.894
222.818
205.241
307.457
262.206
370.878
423.836
504.815
387.271
333.289
510.100
380.053
290.099
527.707
854.958

0.01 (205.28)
0.00 (235.12)
0.01 (78.616)
0.01 (359.28)
0.01 (254.563)
0.01 (264.51)
0.02 (296.16)
0.03 (226.503)
0.05 (351.8529)
0.04 (231.485)
0.07 (283.916)
0.13 (389.4588)
0.23 (464.408)
0.15 (461.855)
0.18 (267.4431)
0.23 (602.7849)
0.24 (378.459)
0.10 (324.406)
0.19 (352.355)
0.24 (584.663)

COLIN
LGP -td
LGP -s, LGP -td
LGP -td
LGP -td
LGP -td
LGP -td
LGP -td
LGP -td
LGP -td
LGP -td
LGP -td
LGP -td
LGP -td
LGP -td
LGP -td
LGP -td
LGP -td
LGP -td
LGP -s, LGP -td,TBL
LGP -td
COLIN ,TBL

TBL
TBL
TBL
TBL
TBL
LGP -td,TBL
LGP -td
TBL
LGP -td
LGP -td
LGP -td
LGP -td
LGP -td
LGP -td
TBL
LGP -td
LGP -td

129.596 (0.02)
182.916 (0.01)
78.616 (0.01)
140.42 (0.03)
162.39699 (0.528)
114.38 (0.10)
126.544 (0.12)
139.608 (0.30)
175.74 (0.38)
231.485 (0.04)
283.351 (0.94)
336.599 (3.57)
433.308 (21.89)
267.73 (6.19)
264.6641 (2.48)
336.356 (22.18)
232.66 (15.29)
169.256 (3.85)
352.355 (0.19)
498.90106 (113.277)

LGP -s
COLIN
LGP -td

Sapa
COLIN

Sapa
LGP -td
LGP -s

Sapa
COLIN

Sapa
LGP -s
LGP -s
LGP -td
LGP -td
COLIN
LGP -td
LGP -td
COLIN
COLIN
COLIN
COLIN
COLIN

Sapa
COLIN
COLIN
COLIN
COLIN
LGP -td
COLIN
COLIN
COLIN
COLIN
LGP -s
COLIN
COLIN
COLIN
LGP -td

Table 13: Results Complex Domains: Best results show best time (corresponding quality) planner(s) achieved time best quality (corresponding time)
planner(s) achieving quality.

90

Sapa

fiC OLIN : P LANNING C ONTINUOUS C HANGE

COLIN

Time
zenotime
1
0.01
2
0.01
3
0.03
4
0.08
5
0.04
6
0.08
7
0.06
8
8.53
9
0.22
10
0.41
11
0.41
12
10.34
13
1.28
14
371.61
15
6.95
16
130.33
17
443.79
18
188.85
19
20

Quality
3.672
23.435
10.089
21.287
8.196
21.966
33.31
43.722
39.949
36.458
22.264
72.139
86.473
117.625
381.626
117.052
77.332
89.082

Average
Time
Quality
0.018
0.019
0.033
0.084
0.075
0.075
0.110
2.250
0.194
0.310
0.271
2.874
1.592
256.813
5.524
43.183
161.243
77.357
73.040
80.660

3.489
23.672
13.336
22.340
22.016
20.921
24.863
30.689
48.528
33.487
25.576
51.105
58.235
72.619
241.318
86.693
118.179
70.542
137.909
91.146

Best
Time

Planner

Quality

Planner

0.01 (0.01)
0.01 (0.01)
0.01 (14.4211)
0.01 (21.708)
0.02 (27.0419)
0.02 (20.8864)
0.01 (25.6744)
0.02 (24.2375)
0.03 (72.1579)
0.06 (46.1848)
0.05 (46.1576)
0.09 (42.1671)
0.07 (42.0593)
0.62 (58.8983)
0.87 (274.8496)
3.07 (67.554)
3.97 (117.1512)
4.48 (56.8345)
9.83 (168.0886)
28.69 (78.4703)

COLIN , LPG -td

3.424 (0.01)
23.431 (0.01)
10.089 (0.03)
21.287 (0.08)
8.196 (0.04)
16.578 (0.17)
18.283 (0.05)
24.238 (0.02)
23.238 (0.395)
20.864 (0.14)
13.666 (0.443)
38.992 (0.846)
42.059 (0.07)
39.064 (651.493)
117.171 (8.644)
54.717 (23.883)
77.332 (443.79)
56.835 (4.48)
107.729 (136.25)
78.470 (28.69)

LPG -td

COLIN , LPG -td
LPG -td
LPG -s
LPG -td
LPG -td
LPG -td
LPG -td
LPG -td
LPG -td
LPG -td
LPG -td
LPG -td
LPG -td
LPG -td
LPG -td
LPG -td
LPG -td
LPG -td
LPG -td

LPG -td
COLIN
COLIN
COLIN

Sapa
LPG -s
LPG -td

Sapa
LPG -s

Sapa
Sapa
LPG -td
Sapa
Sapa
Sapa
COLIN
LPG -td
LPG -s
LPG -td

Table 14: Results Complex Domains: Best results show best time (corresponding quality) planner(s) achieved time best quality (corresponding time)
planner(s) achieving quality.

91

fiC OLES , C OLES , F OX & L ONG

References
Audemard, G., Bertoli, P., Cimatti, A., Kornilowicz, A., & Sebastiani, R. (2002). SAT-based approach solving formulas boolean linear mathematical propositions. Proceedings 18th International Conference Automated Deduction, Vol. 2392, pp. 193208.
Springer-Verlag, LNAI Series.
Blum, A., & Furst, M. (1995). Fast Planning Planning Graph Analysis. Proceedings
International Joint Conference Artificial Inteligence (IJCAI).
Boddy, M. S., & Johnson, D. P. (2002). New Method Global Solution Large Systems Continuous Constraints. Proceedings 1st International Workshop Global
Constraint Optimization Constraint Satisfaction (COCOS), Vol. 2861 Lecture Notes
Computer Science, pp. 142156. Springer.
Cesta, A., & Oddi, A. (1996). Gaining Efficiency Flexibility Simple Temporal Problem.
Proceedings 3rd International Workshop Temporal Representation Reasoning
(TIME).
Cesta, A., Cortellessa, G., Fratini, S., & Oddi, A. (2009). Developing End-to-End Planning
Application Timeline Representation Framework. Proceedings 21st Conference
Innovative Applications Artificial Intelligence (IA*AI).
Chien, S. A., Tran, D., Rabideau, G., Schaffer, S. R., Mandl, D., & Frye, S. (2010). Timeline-Based
Space Operations Scheduling External Constraints. Proceedings International
Conference AI Planning Scheduling (ICAPS), pp. 3441.
Cimatti, A., Giunchiglia, F., Giunchiglia, E., & Traverso, P. (1997). Planning via Model Checking:
Decision Procedure R. Recent Advances AI Planning, 4th European Conference
Planning, ECP, pp. 130142.
Coles, A. I., Fox, M., Halsey, K., Long, D., & Smith, A. J. (2008). Managing concurrency
temporal planning using planner-scheduler interaction. Artificial Intelligence, 173, 144.
Coles, A. I., Fox, M., Long, D., & Smith, A. J. (2008a). Planning Problems Requiring Temporal
Coordination. Proceedings 23rd AAAI Conference Artificial Intelligence (AAAI
08).
Coles, A. I., Fox, M., Long, D., & Smith, A. J. (2008b). Hybrid Relaxed Planning GraphLP
Heuristic Numeric Planning Domains. Proceedings 18th International Conference Automated Planning Scheduling (ICAPS), pp. 5259.
Coles, A. J., Coles, A. I., Fox, M., & Long, D. (2009a). Extending Use Inference Temporal Planning Forwards Search. Proceedings 19th International Conference
Automated Planning Scheduling (ICAPS 09).
Coles, A. J., Coles, A. I., Fox, M., & Long, D. (2009b). Temporal Planning Domains Linear
Processes. Proceedings 21st International Joint Conference Artificial Intelligence
(IJCAI). AAAI Press.
Cushing, W., Kambhampati, S., Mausam, & Weld, D. (2007). temporal planning really
temporal planning?. Proceedings International Joint Conference AI (IJCAI), pp.
18521859.
92

fiC OLIN : P LANNING C ONTINUOUS C HANGE

Dechter, R., Meiri, I., & Pearl, J. (1989). Temporal Constraint Networks. Proceedings Principles Knowledge Representation Reasoning (KR), pp. 8393. Toronto, Canada.
Dierks, H. (2005). Finding Optimal Plans Domains Restricted Continuous Effects
UPPAAL-Cora. ICAPS Workshop Verification Validation Model-Based Planning
Scheduling Systems.
Do, M. B., & Kambhampati, S. (2003). Sapa: Multi-objective Metric Temporal Planner. Journal
Artificial Intelligence Research (JAIR), 20, 155194.
Edelkamp, S. (2003). Taming numbers durations model-checking integrated planning
system. Journal Artificial Intelligence Research (JAIR), 20, 195238.
Edelkamp, S., & Jabbar, S. (2006). Cost-Optimal External Planning. Proceedings 21st
National (American) Conference Artificial Intelligence (AAAI). AAAI Press.
Eyerich, P., Mattmuller, R., & Roger, G. (2009). Using Context-enhanced Additive Heuristic
Temporal Numeric Planning. Proceedings 19th International Conference
Automated Planning Scheduling (ICAPS 2009). AAAI Press.
Fox, M., & Long, D. (2003). PDDL2.1: extension PDDL expressing temporal planning
domains. Journal Artificial Intelligence Research (JAIR), 20, 61124.
Fox, M., & Long, D. (2006). Modelling Mixed Discrete-Continuous Domains Planning. Journal
Artificial Intelligence Research (JAIR), 27, 235297.
Fox, M., Howey, R., & Long, D. (2005). Validating Plans Context Processes Exogenous
Events. Proceedings 20th National Conference Artificial Intelligence 17th
Innovative Applications Artificial Intelligence Conference (AAAI), pp. 11511156.
Fox, M., Long, D., & Halsey, K. (2004). Investigation Expressive Power PDDL2.1.
Proceedings 16th European Conference Artificial Intelligence (ECAI).
Fox, M., Long, D., & Magazzeni, D. (2011). Automatic Construction Efficient Multiple Battery
Usage Policies. Proceedings 21st International Conference Automated Planning
Scheduling (ICAPS).
Frank, J., & Jonsson, A. K. (2003). Constraint-Based Attribute Interval Planning. Constraints,
8(4), 339364.
Garrido, A., Fox, M., & Long, D. (2002). Temporal Planning System Durative Actions
PDDL2.1. Proceedings 15th Eureopean Conference Artificial Intelligence
(ECAI), pp. 586590.
Garrido, A., Onainda, E., & Barber, F. (2001). Temporal Planning System Time-Optimal
Planning. Proceedings 10th Portuguese Conference Artificial Intelligence, pp.
379392. Springer.
Gerevini, A., Saetti, A., & Serina, I. (2006). Approach Temporal Planning Scheduling
Domains Predictable Exogenous Events. Journal Artificial Intelligence Research
(JAIR), 25, 187231.
Gerevini, A., Saetti, A., & Serina, I. (2010). Temporal Planning Problems Requiring Concurrency Action Graphs Local Search. Proceedings 20th International
Conference Automated Planning Scheduling (ICAPS).
93

fiC OLES , C OLES , F OX & L ONG

Gerevini, A., & Serina, I. (2000). Fast Plan Adaptation Planning Graphs: Local Systematic Search Techniques. Proceedings 5th International Conference Artificial
Intelligence Planning Systems (AIPS), pp. 112121.
Ghallab, M., & Laruelle, H. (1994). Representation Control IxTeT, Temporal Planner.
Proceedings 2nd International Conference Artificial Intelligence Planning Systems
(AIPS), pp. 6167.
Halsey, K. (2005). CRIKEY!: co-ordination temporal planning. Ph.D. thesis, University
Durham.
Haslum, P., & Geffner, H. (2001). Heuristic planning time resources. Proceedings
6th European Conference Planning (ECP01), pp. 121132.
Haslum, P. (2009). Admissible Makespan Estimates PDDL2.1 Temporal Planning. Proceedings ICAPS Workshop Heuristics Domain-Independent Planning.
Helmert, M. (2006). Fast Downward Planning System. Journal Artificial Intelligence (JAIR),
26, 191246.
Henzinger, T. (1996). Theory Hybrid Automata. Proceedings 11th Annual Symposium Logic Computer Science. Invited tutorial., pp. 278292. IEEE Computer Society
Press.
Henzinger, T., Ho, P.-H., & Wong-Toi, H. (1995). user guide HYTECH. E. Brinksma,
W.R. Cleaveland, K.G. Larsen, T. Margaria, B. Steffen, editors, Tool Algorithms
Construction Analysis Systems: (TACAS 95), volume 1019 Lecture Notes
Computer Science, pp. 4171.
Hoffmann, J. (2003). Metric-FF Planning System: Translating Ignoring Delete Lists Numeric State Variables. Journal Artificial Intelligence Research (JAIR), 20, 291341.
Hoffmann, J., & Edelkamp, S. (2005). Deterministic Part IPC-4: Overview. Journal
Artificial Intelligence Research (JAIR), 24, 519579.
Hoffmann, J., & Nebel, B. (2001). planning system: Fast plan generation heuristic
search. Journal Artificial Intelligence Research (JAIR), 14, 253302.
Huang, R., Chen, Y., & Zhang, W. (2009). Optimal Temporally Expressive Planner: Initial
Results Application P2P Network Optimization. Proceedings International
Conference Automated Planning Scheduling (ICAPS).
Knight, R., Schaffer, S., & B.Clement (2009). Power planning international space station
domain. Proceedings 6th International Workshop Planning Scheduling
Space (IWPSS).
Lamba, N., Dietz, M., Johnson, D. P., & Boddy, M. S. (2003). Method Global Optimization
Large Systems Quadratic Constraints. Proceedings 2nd International Workshop
Global Optimization Constraint Satisfaction (COCOS), Vol. 3478 Lecture Notes
Computer Science, pp. 6170. Springer.
Leaute, T., & Williams, B. (2005). Coordinating Agile Systems Model-based Execution
Temporal Plans. Proceedings 20th National Conference AI (AAAI).
Li, H., & Williams, B. (2008). Generative systems hybrid planning based flow tubes. Proc.
18th Int. Conf. Aut. Planning Scheduling (ICAPS).
94

fiC OLIN : P LANNING C ONTINUOUS C HANGE

Li, H., & Williams, B. (2011). Hybrid Planning Temporally Extended Goals Sustainable
Ocean Observing. Proceedings International Conference Association
Advancement AI (AAAI): Special Track Sustainability AI.
Long, D., & Fox, M. (2003a). Exploiting Graphplan Framework Temporal Planning.
Proceedings 13th International Conference Automated Planning Scheduling
(ICAPS), pp. 5261.
Long, D., & Fox, M. (2003b). 3rd International Planning Competition: Results Analysis.
Journal Artificial Intelligence Research (JAIR), 20, 159.
Lougee-Heimer, R. (2003). Common Optimization INterface Operations Research. IBM
Journal Research Development, 47(1), 5766.
McDermott, D. (2003). Reasoning Autonomous Processes Estimated Regression
Planner. Proceedings 13th International Conference Automated Planning
Scheduling (ICAPS).
McDermott, D. V. (2000). 1998 AI Planning Systems Competition. AI Magazine, 21(2), 3555.
Meuleau, N., Benazera, E., Brafman, R. I., Hansen, E. A., & Mausam (2009). Heuristic Search
Approach Planning Continuous Resources Stochastic Domains. Journal Artificial
Intelligence Research (JAIR), 34, 2759.
Palacios, H., & Geffner, H. (2009). Compiling Uncertainty Away Conformant Planning Problems
Bounded Width. Journal Artificial Intelligence Research (JAIR), 35, 623675.
Pednault, E. P. D. (1989). ADL: Exploring Middle Ground STRIPS Situation
Calculus. Proceedings International Conference Knowledge Representation (KR),
pp. 324332.
Pell, B., Gat, E., Keesing, R., Muscettola, N., & Smith, B. D. (1997). Robust Periodic Planning
Execution Autonomous Spacecraft. Proceedings International Joint Conference
AI (IJCAI), pp. 12341239.
Penberthy, S., & Weld, D. (1994). Temporal Planning Continuous Change. Proceedings
12th National Conference AI (AAAI), pp. 10101015. AAAI/MIT Press.
Penna, G. D., Intrigila, B., Magazzeni, D., & Mercorio, F. (2009). UPMurphi: Tool Universal Planning PDDL+ Problems. Proceedings 19th International Conference
Automated Planning Scheduling (ICAPS 2009), pp. 1923. AAAI Press.
Penna, G. D., Intrigila, B., Magazzeni, D., & Mercorio, F. (2010). PDDL+ Benchmark Problem:
Batch Chemical Plant. Proceedings International Conference AI Planning
Scheduling (ICAPS), pp. 222225.
Reddy, S. Y., Frank, J. D., Iatauro, M. J., Boyce, M. E., Kurklu, E., Ai-Chang, M., & Jonsson,
A. K. (2011). Planning Solar Array Operations International Space Station. ACM
Transactions Intelligent Systems Technology, 2, 124.
Richter, S., & Westphal, M. (2010). LAMA Planner: Guiding Cost-Based Anytime Planning
Landmarks. Journal Artificial Intelligence Research (JAIR), 39, 127177.
Shin, J., & Davis, E. (2005). Processes Continuous Change SAT-based Planner. Artificial
Intelligence, 166, 194253.
95

fiC OLES , C OLES , F OX & L ONG

Smith, D., & Weld, D. S. (1999). Temporal Planning Mutual Exclusion Reasoning. Proceedings 16th International Joint Conference AI (IJCAI), pp. 326337.
Veloso, M., Perez, M., & Carbonell, J. (1990). Nonlinear planning parallel resource allocation.
Proceedings DARPA Workshop Innovative Approaches Planning, Scheduling
Control, pp. 207212.
Vidal, V., & Geffner, H. (2006). Branching pruning: optimal temporal POCL planner based
constraint programming. Artificial Intelligence, 170(3), 298335.
Wolfman, S., & Weld, D. (1999). LPSAT System Application Resource Planning.
Proceedings 16th International Joint Conference Artificial Intelligence (IJCAI).
Yi, W., Larsen, K., & Pettersson, P. (1997). UPPAAL Nutshell. International Journal
Software Tools Technology Transfer, 1(1).
Younes, H. L. S., & Simmons, R. G. (2003). VHPOP: Versatile heuristic partial order planner..
Journal Artificial Intelligence Research (JAIR), 20, 405430.

96


