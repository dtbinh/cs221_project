Journal Artificial Intelligence Research 3 (1995) 1-24

Submitted 1/95; published 6/95

Induction First-Order Decision Lists:
Results Learning Past Tense English Verbs

Raymond J. Mooney
Mary Elaine Califf

Department Computer Sciences, University Texas
Austin, TX 78712-1188

mooney@cs.utexas.edu
mecaliff@cs.utexas.edu

Abstract

paper presents method inducing logic programs examples learns
new class concepts called first-order decision lists, defined ordered lists clauses
ending cut. method, called Foidl, based Foil (Quinlan, 1990)
employs intensional background knowledge avoids need explicit negative examples. particularly useful problems involve rules specific exceptions,
learning past-tense English verbs, task widely studied context
symbolic/connectionist debate. Foidl able learn concise, accurate programs
problem significantly fewer examples previous methods (both connectionist
symbolic).

1. Introduction

Inductive logic programming (ILP) growing subtopic machine learning studies
induction Prolog programs examples presence background knowledge
(Muggleton, 1992; Lavrac & Dzeroski, 1994). Due expressiveness first-order logic,
ILP methods learn relational recursive concepts cannot represented
attribute/value representations assumed machine-learning algorithms. ILP methods successfully induced small programs sorting list manipulation (Shapiro,
1983; Sammut & Banerji, 1986; Muggleton & Buntine, 1988; Quinlan & Cameron-Jones,
1993) well produced encouraging results important applications predicting protein secondary structure (Muggleton, King, & Sternberg, 1992) automating
construction natural-language parsers (Zelle & Mooney, 1994b).
However, current ILP techniques make important assumptions restrict application. three common assumptions:
1. Background knowledge provided extensional form set ground literals.
2. Explicit negative examples target predicate available.
3. target program expressed \pure" Prolog clause-order irrelevant
procedural operators cut (!) disallowed.
currently well-known successful ILP systems, Golem (Muggleton & Feng,
1990) Foil (Quinlan, 1990), make three assumptions. However,
assumptions brings significant limitations since:
1. adequate extensional representation background knowledge frequently infinite
intractably large.

c 1995 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiMooney & Califf

2. Explicit negative examples frequently unavailable adequate set negative
examples computed using closed-world assumption infinite intractably large.
3. Concise representation many concepts requires use clause-ordering and/or
cuts (Bergadano, Gunetti, & Trinchero, 1993).
paper presents new ILP method called Foidl (First-Order Induction Decision Lists) helps overcome limitations incorporating following
properties:
1. Background knowledge represented intensionally logic program.
2. explicit negative examples need supplied constructed. assumption
output completeness used instead implicitly determine hypothesized
clause overly-general and, so, quantify degree over-generality simply
estimating number negative examples covered.
3. learned program represented first-order decision list, ordered set
clauses ending cut. representation useful problems
best represented general rules specific exceptions.
name implies, Foidl closely related Foil follows similar top-down,
greedy specialization guided information-gain heuristic. However, algorithm
substantially modified address three advantages listed above. use intensional
background knowledge fairly straightforward incorporated previous Foil
derivatives (Lavrac & Dzeroski, 1994; Pazzani & Kibler, 1992; Zelle & Mooney, 1994b),
development Foidl motivated failure observed applying existing ILP methods particular problem, learning past tense English
verbs. problem studied fairly extensively using connectionist symbolic methods (Rumelhart & McClelland, 1986; MacWhinney & Leinbach, 1991; Ling,
1994); however, previous efforts used specially-designed feature-based encodings impose fixed limit length words fail capture position-independence
underlying transformation. believed representing problem constructing logic program predicate past(X,Y) X words represented
lists letters (e.g past([a,c,t], [a,c,t,e,d]), past([a,c,h,e], [a,c,h,e,d]),
past([a,r,i,s,e], [a,r,o,s,e])) would produce much better results. However, due
limitations mentioned above, unable get reasonable results either Foil
Golem. However, overcoming limitations, Foidl able learn highly accurate programs past-tense problem many fewer examples required
previous methods.
remainder paper organized follows. Section 2 provides important background material Foil past-tense learning problem. Section 3 presents
Foidl algorithm details incorporates three advantages discussed above. Section 4 presents results learning past-tense English verbs demonstrating
Foidl out-performs previous methods problem. Section 5 reviews related work,
Section 6 discusses limitations future directions, Section 7 summarizes presents
conclusions.
2

fiInduction First-Order Decision Lists: Learning English Past Tense

2. Background

Since Foidl based Foil, section presents brief review important ILP system; Quinlan (1990), Quinlan Cameron-Jones (1993), Cameron-Jones Quinlan
(1994) provide complete description. section presents brief review
previous work English past tense problem.

2.1 FOIL

Foil learns function-free, first-order, Horn-clause definition target predicate terms
background predicates. input consists extensional definitions
predicates tuples constants specified types. example, input appropriate
learning definition list membership is:
member(Elt,Lst): { <a,[a]>, <a,[a,b]>, <b,[a,b]>, <a,[a,b,c]>, ...}
components(Lst,Elt,Lst): { <[a],a,[]>, <[a,b],a,[b]>, <[a,b,c],a,[b,c]> ...}

Elt type denoting possible elements includes a,b,c, d; Lst
type defined consisting lists containing three elements;
components(A,B,C) background predicate true iff list whose first element B whose rest list C (this must provided place function
list construction). Foil requires negative examples target concept,
supplied directly computed using closed-world assumption. example,
closed-world assumption would produce pairs form <Elt,Lst> explicitly provided positive examples (e.g., <b,[a]>).
Given input, Foil learns program one clause time using greedy-covering
algorithm summarized follows:
Let positives-to-cover = positive examples.
positives-to-cover empty
Find clause, C , covers preferably large subset positives-to-cover
covers negative examples.
Add C developing definition.
Remove examples covered C positives-to-cover.
example, clause might learned member one iteration loop is:
member(A,B) :- components(B,A,C).

since covers positive examples element first one list
cover negatives. clause could learned cover remaining examples is:
member(A,B) :- components(B,C,D), member(A,D).

Together two clauses constitute correct program member.
\find clause" step implemented general-to-specific hill-climbing search
adds antecedents developing clause one time. step, evaluates possible
literals might added selects one maximizes information-gain heuristic.
algorithm maintains set tuples satisfy current clause includes bindings
new variables introduced body. following pseudocode summarizes
procedure:
3

fiMooney & Califf

Initialize C R(V1; V2; :::; Vk) :-. R target predicate arity k.
Initialize contain positive tuples positives-to-cover negative tuples.
contains negative tuples
Find best literal L add clause.
Form new training set containing tuple satisfies L,
tuples form b (t b concatenated) b set bindings
new variables introduced L literal satisfied
(i.e., matches tuple extensional definition predicate).
Replace .
Foil considers adding literals possible variablizations predicate long
type restrictions satisfied least one arguments existing variable
bound head previous literal body. Literals evaluated based
number positive negative tuples covered, preferring literals cover many positives
negatives. Let T+ denote number positive tuples set define:
(T ) = , log2 (T+ =jT j):
(1)
chosen literal one maximizes:
gain(L) = (I (T ) , (T ));
(2)
number tuples extensions (i.e., number current
positive tuples covered L).
Foil includes many additional features as: heuristics pruning space
literals searched, methods including equality, negation failure, useful literals
immediately provide gain (determinate literals), pre-pruning post-pruning
clauses prevent over-fitting, methods ensuring induced programs
terminate. papers referenced consulted details
features.
0

0

0

0

2.2 Learning Past Tense English Verbs

Rumelhart McClelland (1986) first build computational model pasttense learning using classic perceptron algorithm special phonemic encoding
words employing so-called Wickelphones Wickelfeatures. general goal show
connectionist models could account interesting language-learning behavior
previously thought require explicit rules. model heavily criticized opponents
connectionist approach language acquisition relatively poor results achieved
heavily-engineered representations training techniques employed (Pinker &
Prince, 1988; Lachter & Bever, 1988). MacWhinney Leinbach (1991) attempted
address criticisms using standard multi-layer backpropagation learning
algorithm simpler UNIBET encoding phonemes (in 36 phonemes
encoded single ASCII character).
Ling Marinov (1993) Ling (1994) criticize current connectionist models past-tense acquisition heavily-engineered representations poor experimental
methodology. present systematic results system called SPA (Symbolic Pattern Associator) uses slightly modified version C4.5 (Quinlan, 1993) build
4

fiInduction First-Order Decision Lists: Learning English Past Tense

forest decision trees maps fixed-length input pattern fixed-length output pattern. Ling's (1994) head-to-head results show SPA generalizes significantly better
backpropagation number variations problem employing different phonemic
encodings (e.g., 76% vs. 56% given 500 training examples).
However, previous work encodes problem fixed-length pattern association fails capture generativity position-independence true transformation. example, use 15-letter patterns like:
a,c,t,_,_,_,_,_,_,_,_,_,_,_,_ => a,c,t,e,d,_,_,_,_,_,_,_,_,_,_

UNIBET phonemic encoding:
&,k,t,_,_,_,_,_,_,_,_,_,_,_,_ => &,k,t,I,d,_,_,_,_,_,_,_,_,_,_

separate decision tree output unit used predict character output
pattern input characters. Therefore, learning general rules, \add
`ed'," must repeated position word end, words longer 15
characters cannot handled. Also, best results SPA exploit highly-engineered
feature template modified version C4.5's default leaf-labeling strategy tailor
string transformation problems.
Although ILP methods seem appropriate problem, initial attempts
apply Foil Golem past-tense learning gave disappointing results (Califf,
1994). Below, discuss three problems listed introduction contribute
diculty applying current ILP methods problem.
principle, background predicate append sucient constructing accurate
past-tense programs incorporated ability include constants arguments
or, equivalently, ability add literals bind variables specific constants (called
theory constants Foil). However, background predicate allow appending
empty list appropriate. use predicate called split(A, B, C)
splits list two non-empty sublists B C. intensional definition split is:
split([X, | Z], [X] , [Y | Z]).
split([X | Y], [X | W], Z) :- split(Y,W,Z).

Using split, \add `ed"' rule represented as:
past(A,B) :- split(B,A,[e,d]).

which, Foil, learned form:
past(A,B) :- split(B,A,C), C = [e,d].

Providing extensional definition split includes possible strings 15 fewer
characters (at least 1021 strings) clearly intractable. However, providing partial definition includes possible splits strings actually appear training corpus
possible generally sucient. Therefore, providing adequate extensional background
knowledge cumbersome requires careful engineering; however, major
problem.
Supplying appropriate set negative examples problematic. Using closedworld assumption produce pairs words training set second
past-tense first feasible useful. case, clause:
5

fiMooney & Califf

past(A,B) :- split(B,A,C).

likely learned since covers positives (if any)
negatives since unlikely word prefix another word past
tense. However, clause useless producing past tense novel verbs, and,
domain, accuracy must measured ability actually generate correct output
novel inputs, rather ability classify pre-supplied tuples arguments positive
negative. obvious solution supplying strings 15 characters less
negative examples past tense word clearly intractable. Providing specially
constructed \near-miss" negative examples past([a,c,h,e],[a,c,h,e,e,d]),
helpful, requires careful engineering exploits detailed prior knowledge
problem.
order address problem negative examples, Quinlan (1994) applied
Foil problem, employed different target predicate representing pasttense transformation.1 used three-place predicate past(X,Y,Z) true iff
input word X transformed past-tense form removing current ending
substituting ending Z; example: past([a,c,t], [], [e,d]), past([a,r,i,s,e],
[i,s,e], [o,s,e]). simple preprocessor map data two-place predicate
form. Since sample 500 verb pairs contains 30-40 different end fragments,
results manageable number closed-world negatives, approximately 1000
every positive example training set. Using approach UNIBET phonemic
encodings, Quinlan obtained slightly better results Ling's best SPA results exploited highly-engineered feature template (83.3% vs. 82.8% 500 training examples)
significantly better SPA's normal results (76.3%). Although three-place target predicate incorporates knowledge desired transformation, arguably
requires less representation engineering previous methods.
However, Quinlan (1994) notes results still hampered Foil's inability
exploit clause order. example, using normal alphabetic encoding, Foil quickly
learns clause sucient regular verbs:
past(A,B,C) :- B=[], C=[e,d].

However, since clause still covers fair number negative examples due many
irregular verbs, continues add literals. result, Foil creates number specialized
versions clause together still fail capture generality underlying
default rule. problem compounded Foil's inability add constraints
\does end `e'." Since Foil separates addition literals containing variables
binding variables constants using literals form V = c, cannot learn clauses
like:
past(A,B,C) :- B=[], C=[e,d], not(split(A,D,[e])).

Since word split several ways, clearly equivalent learnable
clause:
past(A,B,C) :- B=[], C=[e,d], not(split(A,D,E)), E /= [e].

1. Quinlan's work problem motivated early attempts use Foil.

6

fiInduction First-Order Decision Lists: Learning English Past Tense

Consequently, must approximate true rule learning many clauses form:
past(A,B,C) :- B=[], C=[e,d], split(A,D,E), E = [b].
past(A,B,C) :- B=[], C=[e,d], split(A,D,E), E = [d].
...

result, Foil generated overly-complex programs containing 40 clauses
phonemic alphabetic versions problem.
However, experienced Prolog programmer would exploit clause order cuts
write concise program first handles most-specific exceptions falls
more-general default rules exceptions fail apply. example, program:
past(A,B)
past(A,B)
past(A,B)
past(A,B)

::::-

split(A,C,[e,e,p]), split(B,C,[e,p,t]), !.
split(A,C,[y]), split(B,C,[i,e,d]), !.
split(A,C,[e]), split(B,A,[d]), !.
split(B,A,[e,d]).

summarized as:
word ends \eep," replace \eep" \ept" (e.g., sleep, slept),
else, word ends \y," replace \y" \ied"
else, word ends \e," add \d"
else, add \ed."
Foidl directly learn programs form, i.e., ordered sets clauses ending
cut. call programs first-order decision lists due similarity propositional
decision lists introduced Rivest (1987). Foidl uses normal binary target predicate
requires explicit negative examples. Therefore, believe requires significantly
less representation engineering previous work area.

3. FOIDL Induction Algorithm

stated introduction, Foidl adds three major features Foil: 1) Intensional
specification background knowledge, 2) Output completeness substitute explicit
negative examples, 3) Support learning first-order decision lists. following
subsections describe modifications made incorporate features.

3.1 Intensional Background

described above, Foil assumes background predicates provided extensional
definitions; however, burdensome frequently intractable. Providing intensional definition form general Prolog clauses generally preferable. example,
instead providing numerous tuples components predicates, easier give
intensional definition:
components([A | B], A, B).

Intentional background definitions restricted function-free pure Prolog
exploit features language.
7

fiMooney & Califf

Modifying Foil use intensional background straightforward. Instead matching
literal set tuples determine whether covers example,
Prolog interpreter used attempt prove literal satisfied using
intensional definitions. Unlike Foil, expanded tuples maintained positive
negative examples target concept reproved alternative specialization
developing clause. Therefore, pseudocode learning clause simply:
Initialize C R(V1; V2; :::; Vk) :-. R target predicate arity k.
Initialize contain examples positives-to-cover negative examples.
contains negative tuples
Find best literal L add clause.
Let subset examples still proved instances
target concept using specialized clause.
Replace
Since expanded tuples produced, information-gain heuristic picking best
literal simply:
gain(L) = jT j (I (T ) , (T )):
(3)
0

0

0

0

3.2 Output Completeness Implicit Negatives

order overcome need explicit negative examples, mode declaration
target concept must provided (i.e., specification whether argument input (+)
output (-)). assumption output completeness made, indicating
every unique input pattern training set, training set includes correct
output patterns. Therefore, output program produces given input
assumed represent negative example. require positive
examples part training set, unique input pattern training
set, positive examples input pattern (if any) must training
set. assumption trivially met predicate represents function single
unique output input.
example, assumption output completeness mode declaration past(+,-)
indicates correct past-tense forms included input word
training set. predicates representing functions, past, implies
output example unique outputs implicitly represent negative examples. However, output completeness applied non-functional cases
append(-,-,+), indicating possible pairs lists appended
together produce list included training set (e.g., append([],[a,b],[a,b]),
append([a],[b],[a,b]), append([a,b],[],[a,b])).
Given output completeness assumption, determining clause overly-general
straightforward. positive example, output query made determine
outputs given input (e.g., past([a,c,t], X)). outputs generated
positive examples, clause still covers negative examples requires
specialization. Note intensional interpretation learned clauses required order
answer output queries.
addition, order compute gain alternative literals specialization,
negative coverage clause needs quantified. incorrect answer output
8

fiInduction First-Order Decision Lists: Learning English Past Tense

query ground (i.e., contains variables) clearly counts single negative example (e.g., past([a,c,h,e], [a,c,h,e,e,d])). However, output queries frequently
produce answers universally quantified variables. example, given overly-general
clause past(A,B) :- split(A,C,D)., query past([a,c,t], X) generates answer
past([a,c,t], Y). implicitly represents coverage infinite number negative
examples. order quantify negative coverage, Foidl uses parameter u represent
bound number possible terms. Since set possible terms (the Herbrand
universe background knowledge together examples) generally infinite, u
meant represent heuristic estimate finite number terms ever
actually occur practice (e.g., number distinct words English). negative coverage represented non-ground answer output query estimated uv , p,
v number variable arguments answer p number positive
examples answer unifies. uv term stands number unique
ground outputs represented answer (e.g., answer append(X,Y,[a,b]) stands
2
u different ground outputs) p term stands number represent
positive examples. allows Foidl quantify coverage large numbers implicit
negative examples without ever explicitly constructing them. generally sucient
estimate u fairly large constant (e.g., 1000), empirically method
sensitive exact value long significantly greater number ground
outputs ever generated clause.
Unfortunately, estimate sensitive enough. example, clauses
past(A,B) :- split(A,C,D).
past(A,B) :- split(B,A,C).

cover u implicit negative examples output query past([a,c,t], X) since first
produces answer past([a,c,t], Y) second produces answer past([a,c,t],
[a,c,t | Y]). However, second clause clearly better since least requires output input sux added. Since presumably words
words start \a-c-t" (assuming total number words finite),
first clause considered cover negative examples. Therefore, arguments
partially instantiated, [a,c,t | Y], counted fraction
variable calculating v . Specifically, partially instantiated output argument scored
fraction subterms variables, e.g., [a,c,t | Y] counts 1=4
variable argument. Therefore, first clause scored covering u implicit negatives second covering u1=4. Given reasonable values u number
positives covered clause, literal split(B,A,C) preferred.
revised specialization algorithm incorporates implicit negatives is:
Initialize C R(V1; V2; :::; Vk) :-. R target predicate arity k.
Initialize contain examples positives-to-cover output queries
positive examples.
contains output queries
Find best literal L add clause.
Let subset positive examples still proved instances
target concept using specialized clause, plus output queries
0

9

fiMooney & Califf

still produce incorrect answers.
Replace .
0

Literals scored described previous section except jT j computed
number positive examples plus sum number implicit negatives covered
output query .

3.3 First-Order Decision Lists

described above, first-order decision lists ordered sets clauses ending
cut. answering output query, cuts simply eliminate first answer
produced trying clauses order. Therefore, representation similar
propositional decision lists (Rivest, 1987), ordered lists pairs (rules)
form (ti ; ci) test ti conjunction features ci category label
example assigned category first pair whose test satisfies.
original algorithm Rivest (1987) CN2 (Clark & Niblett, 1989), rules
learned order appear final decision list (i.e., new rules appended
end list learned). However, Webb Brkic (1993) argue learning
decision lists reverse order since preference functions tend learn general
rules first, best positioned default cases towards end. introduce
algorithm, prepend, learns decision lists reverse order present results indicating
cases learns simpler decision lists superior predictive accuracy. Foidl
seen generalizing prepend first-order case target predicates representing
functions. learns ordered sequence clauses reverse order, resulting program
produces first output generated first satisfied clause.
basic operation algorithm best illustrated concrete example.
alphabetic past-tense, current algorithm easily learns partial clause:
past(A,B) :- split(B,A,C), C = [e,d].

However, discussed section 2.2, clause still covers negative examples due irregular verbs. However, produces correct ground output subset examples (i.e.,
regular verbs).2 indication best terminate clause handle
examples, add earlier clauses decision list handle remaining examples.
fact produces incorrect answers output queries safely ignored
decision-list framework since handled earlier clauses. Therefore,
examples correctly covered clause removed positives-to-cover new
clause begun. literals provide best gain are:
past(A,B) :- split(B,A,C), C = [d].

since many irregulars add \d" (since end \e"). clause
produces correct ground output subset examples; however,
complete since produces incorrect output examples correctly covered previously
learned clause (e.g., past([a,c,t], [a,c,t,d])). Therefore, specialization continues
cases eliminated. results clause:
2. Note untrue literals added initially empty clause.

10

fiInduction First-Order Decision Lists: Learning English Past Tense

past(A,B) :- split(B,A,C), C = [d], split(A,D,E), E = [e].

added front decision list examples covers removed
positives-to-cover. approach ensures every new clause produces correct outputs
new subset examples doesn't result incorrect output examples already
correctly covered previously learned clauses. process continues adding clauses
front decision list exceptions handled positives-to-cover
empty.
resulting clause-specialization algorithm summarized follows:
Initialize C R(V1; V2; :::; Vk) :-. R target predicate arity k.
Initialize contain examples positives-to-cover output queries
positive examples.
contains output queries
Find best literal L add clause.
Let subset positive examples whose output query still produces
first answer unifies correct answer, plus output queries
either
1) Produce non-ground first answer unifies correct answer,
2) Produce incorrect answer produce correct answer using
previously learned clause.
Replace .
0

0

many cases, algorithm able learn accurate, compact, first-order decision lists
past tense, \expert" program shown section 2.2. However, due highly irregular verbs, algorithm encounter local-minima unable find literals
provide positive gain still covering required minimum number examples.3
originally handled terminating search memorizing remaining uncovered examples specific exceptions top decision list (e.g., past([a,r,i,s,e],
[a,r,o,s,e]) :- !.). However, result premature termination prevents
algorithm finding low-frequency regularities. example, alphabetic version, system get stuck trying learn complex rule double final
consonant (e.g., grab ! grabbed) fail learn rule changing \y" \ied" since
actually less frequent.
current version, Foil, tests learned clause meets minimum-accuracy
threshold; however, unlike Foil, counting errors incorrect outputs queries correctly answered previously learned clauses. meet threshold, clause
thrown positive examples covers memorized top decision
list. algorithm continues learn clauses remaining positive examples.
allows Foidl memorize dicult irregularities, consonant doubling,
still continue learn rules changing \y" \ied."
minimum-accuracy threshold met, decision-list property exploited
final attempt still learn completely accurate program. negatives covered
clause examples correctly covered previously learned clauses, Foidl
3. Foil, Foidl includes parameter minimum number examples clause must cover
(normally set 2).

11

fiMooney & Califf

treats \exceptions exception rule" returns positives-tocover covered correctly subsequently learned clauses. example, Foidl
frequently learns clause:
past(A, B) :- split(A, C, [y]), split(B, C, [i, e, d]).

changing \y" \ied." However, clause incorrectly covers examples
correctly covered previously learned \add `ed"' rule (e.g., bay ! bayed; delay !
delayed). Since exceptions \y" \ied" rule small percentage words
end \y," system keeps rule returns examples add \ed"
positives-to-cover. Subsequently, rules as:
past(A, B) :- split(B, A, [e, d]), split(A, D, [a, y]).

learned recover examples, resulting program completely consistent
training data. setting minimum clause-accuracy threshold 50%, Foidl
applies uncovering technique results covering examples
uncovers, thereby guaranteeing progress towards fitting training examples.

3.4 Algorithmic Implementation Details

section brie discusses additional details Foidl algorithm implementation. includes discussion use modes, types, weak literals, theory
constants. current version Foil includes features basically
form.
Foidl makes use types modes limit space literals searched. argument predicate typed literals whose previously-bound arguments
correct type tested specializing clause. example, split given
types split(word,prefix,suffix), preventing system splitting prefixes
suxes exploring arbitrary substrings word regularities. predicate
given mode declaration, literals whose input arguments previouslybound variables tested. example, split given mode split(+,-,-), preventing
clause creating new strings appending together previously generated prefixes
suxes.
case literal provides positive information gain, Foidl gives small bonus literals
introduce new variables. However, number weak literals added
row limited user parameter (normally set 1). example, allows
system split word possible prefixes suxes, even though may provide
gain substrings constrained subsequent literals.
Theory constants provided type, literals tested binding
existing variable constant appropriate type. example, literal X=[e,d]
generated X type suffix. runs past-tense, theory constants included
every prefix sux occurs least two words training data. helps
control training time limiting number literals searched, affect
literals actually chosen since minimum-clause-coverage test prevents Foidl
choosing literals don't cover least two examples anyway.
12

fiInduction First-Order Decision Lists: Learning English Past Tense

Foidl currently implemented Common Lisp Quintus Prolog. Unlike
current Prolog version, Common Lisp version supports learning recursive clauses4
output-completeness non-functional target predicates. However, Common Lisp
version significantly slower since relies un-optimized Prolog interpreter
compiler written Lisp (from Norvig, 1992). Consequently, presented results
Prolog version running Sun SPARCstation 2.5

4. Experimental Results
test Foidl's performance English past tense task, ran experiments using
data Ling (1994) made available appendix.

4.1 Experimental Design
data used consist 6939 English verb forms normal alphabetic form
UNIBET phoneme representation along label indicating verb form (base, past
tense, past participle, etc), label indicating whether form regular irregular,
Francis-Kucera frequency verb. data include 1390 distinct pairs base
past tense verb forms. ran three different experiments. one used phonetic
forms verbs. second used phonetic forms regular verbs only,
easiest form task problem
Ling provides learning curves. Finally, ran trials using alphabetic forms verbs.
training testing followed standard paradigm splitting data testing
training sets training progressively larger samples training set. results
averaged 10 trials, testing set trial contained 500 verbs.
order better separate contribution using implicit negatives contribution decision list representation, ran experiments IFoil, variant
system uses intensional background output completeness assumption,
build decision lists.
ran experiments Foil, Foidl, IFoil compared
results Ling. Foil experiments run using Quinlan's representation described
section 2.2. Quinlan (1994), negative examples provided using randomlyselected 25% could generated using closed world assumption.6
experiments Foidl IFoil used standard default values various numeric
parameters (term universe size, 1000; minimum clause coverage, 2; weak literal limit, 1).
differences among Foil, IFoil, Foidl tested significance using twotailed paired t-test.
4. Handling intensional interpretation recursive clauses target predicate requires additional
complexities discussed paper since relevant decision-lists,
generally recursive.
5. versions available anonymous FTP net.cs.utexas.edu directory
pub/mooney/foidl.
6. replicated Quinlan's approach since memory limitations prevented us using 100% generated negatives larger training sets.

13

fiMooney & Califf

100

80

Accuracy

60

40
FOIDL
IFOIL
FOIL
SPA
Neural Network

20

0
0

100

200

300
Training Examples

400

500

Figure 1: Accuracy phonetic past tense task using verbs

4.2 Results
results phonetic task using regular irregular verbs presented
Figure 1. graph shows results Foil, IFoil, Foidl along
best results Ling, provide learning curve task. expected,
Foidl out-performed systems task, surpassing Ling's best results 500
examples 100 examples. IFoil performed quite poorly, barely beating neural
network results despite effectively 100% negatives opposed Foil's 25%.
poor performance due least part overfitting training data, IFoil
lacks noise-handling techniques Foil6. Foil advantage three-place
predicate, gives bias toward learning suxes. IFoil's poor performance
task shows implicit negatives sucient,
bias decision lists three-place predicate noise-handling needed.
differences Foil Foidl significant 0.01 level. Foidl
IFoil significant 0.001 level. differences Foil IFoil
significant 100 training examples less, significant 0.001 level
250 500 examples.
Figure 2 presents accuracy results phonetic task using regulars only. curves
SPA neural net results reported Ling. again, Foidl outperformed systems. particular task demonstrated one problems
using closed-world negatives. regular past tense task, second argument Quinlan's 3-place predicate always same: empty list. Therefore, constants
generated positive examples, Foil never produce rules ground second argument, since cannot create negative examples constants second
argument. prevents system learning rule generate past tense. order
14

fiInduction First-Order Decision Lists: Learning English Past Tense

100

80

Accuracy

60

40
FOIDL
IFOIL
FOIL
SPA
Neural Network

20

0
0

50

100

150

200
250
300
Training Examples

350

400

450

500

Figure 2: Accuracy phonetic past tense task using regulars
obtain results reported here, introduced extra constants second argument
(specifically constants third argument), enabling closed world assumption
generate appropriate negatives. task, IFoil seem gain advantage
Foil able effectively use negatives. regularity data
allows IFoil Foil achieve 90% accuracy 500 examples. differences
Foil Foidl significant 0.001 level, IFoil
Foidl. differences IFoil Foil significant 25 examples,
significant 0.02 level 500 examples, significant 0.001 level
50-250 training examples.
Results alphabetic version appear Figure 3. task
typically considered literature, interest concerned
incorporating morphology natural language understanding systems deal
text. dicult task, primarily consonant doubling.
results Foidl, IFoil, Foil. alphabetic task even
irregular full phonetic task, IFoil overfits data performs quite poorly.
differences Foil Foidl significant 0.001 level 25, 50, 250,
500 examples, 0.1 level 100 examples. differences
IFoil Foidl significant 0.001 level. Foil IFoil
significant 25 training examples significant 0.01 level 50
training examples, significant 0.001 level 100 examples.
three tasks, Foidl clearly outperforms systems, demonstrating
first order decision list bias good one learning task. sucient set
negatives necessary, five systems provide way: neural
network SPA learn multiple-class classification tasks (which phoneme belongs
position); Foil uses three-place predicate closed world negatives; IFoil
15

fiMooney & Califf

90

80

70

Accuracy

60

50

40

30

FOIDL
IFOIL
FOIL

20

10

0
0

50

100

150

200
250
300
Training Examples

350

400

450

500

Figure 3: Accuracy alphabetic past tense task
Foidl, course, use output completeness assumption. primary importance
implicit negatives provide advantage propositional
neural network systems, enable first order systems perform task
all. Without them, knowledge task required. Foidl's decision lists give
significant added advantage, though advantage less apparent regular phonetic
task, exceptions.
Clearly, Foidl produces accurate rules systems, another consideration complexity rule sets. ILP systems, two good measures
complexity number rules number literals generated. Figure 4 shows
number rules generated Foil, IFoil, Foidl phonetic task using verbs.
number literals generated appears Figure 5. Since interested generalization since Foil attempt fit training data, results
include rules Foidl IFoil add order memorize individual exceptions.7 Although numbers comparable examples, increasing numbers
examples, programs Foil IFoil generate grow much faster Foidl's programs.
large number rules/literals learned IFoil show tendency overfit data.
Foidl generates comprehensible programs. following example program generated alphabetic version task using 250 examples (again excluding
memorized examples).
past(A,B) :- split(A,C,[e,p]), split(B,C,[p,t]),!.
past(A,B) :- split(A,C,[y]), split(B,C,[i,e,d]), split(A,D,[r,y]),!.
past(A,B) :- split(A,C,[y]), split(B,C,[i,e,d]), split(A,D,[l,y]),!.

7. large number irregular pasts English, Foidl memorizes average 38 verbs per
trial 500 examples.

16

fiInduction First-Order Decision Lists: Learning English Past Tense

80

70

FOIDL
IFOIL
FOIL

Number Rules

60

50

40

30

20

10

0
0

50

100

150

200
250
300
Training Examples

350

400

450

500

Figure 4: Number rules created phonetic past tense task

350

300

FOIDL
IFOIL
FOIL

Number Literals

250

200

150

100

50

0
0

50

100

150

200
250
300
Training Examples

350

400

450

500

Figure 5: Number literals created phonetic past tense task
17

fiMooney & Califf

past(A,B)
past(A,B)
past(A,B)
past(A,B)

::::-

split(B,A,[m,e,d]), split(A,C,[m]), split(A,[s],D),!.
split(B,A,[r,e,d]), split(A,C,[u,r]),!.
split(B,A,[d]), split(A,C,[e]),!.
split(B,A,[e,d]),!.

training times various systems considered research dicult
compare. Ling provide timing results, though probably assume based
research comparing symbolic neural learning algorithms (Shavlik, Mooney, & Towell,
1991) SPA runs fairly quickly since based C4.5 backpropagation took
considerably longer. tests Foil Foidl directly comparable
run different architectures. Foil runs done Sparc 5.
500 examples, Foil averaged 48 minutes phonetic task verbs. Foidl
experiments ran Sparc 2 averaged 1071 minutes task. Even allowing
differences speed two machines (about factor two), Foidl quite
bit slower, probably due largely cost using intentional background part
implementation Prolog opposed C.

5. Related Work

5.1 Related Work ILP

Although three features mentioned introduction distinguishes Foidl
work Inductive Logic Programming, number related pieces research
mentioned. use intensional background knowledge least distinguishing feature
since number ILP systems incorporate aspect. Focl (Pazzani & Kibler,
1992), mFoil (Lavrac & Dzeroski, 1994), Grendel (Cohen, 1992), Forte (Richards &
Mooney, 1995), Chillin (Zelle & Mooney, 1994a) use intensional background
degree context Foil-like algorithm. ILP systems employ
intensional background include early ones Shapiro (1983) Sammut Banerji (1986)
recent ones Bergadano et al. (1993) Stahl, Tausend, Wirth (1993).
use implicit negatives significantly novel. described section 3.2,
approach considerably different explicit construction using closed-world assumption, therefore employed explicit construction sucient negative examples intractable. Bergadano et al. (1993) allows user supply intensional definition
negative examples covers large set ground instances (e.g (past([a,c,t],X),
not(equal(X,[a,c,t,e,d])))); however, equivalent output completeness, user
would explicitly provide separate intensional negative definition positive
example. non-monotonic semantics used eliminate need negative examples
Claudien (De Raedt & Bruynooghe, 1993) effect output completeness
assumption case arguments target relation outputs. However,
output completeness permits exibility allowing arguments specified
inputs counting negative examples extra outputs generated specific
inputs training set. Flip (Bergadano, 1993) provides method learning functional programs without negative examples making assumption equivalent output
completeness functional case. Output completeness general permits learning non-functional programs well. Also, unlike Foidl, none previous
18

fiInduction First-Order Decision Lists: Learning English Past Tense

methods provide way quantifying implicit negative coverage context heuristic
top-down specialization algorithm.
notion first-order decision list unique Foidl. ILP system
attempts learn programs exploit clause-order cuts Bergadano et al.
(1993). paper discusses many problems learning arbitrary programs cuts,
brute-force search used approach intractable realistic problems.
Instead addressing general problem learning arbitrary programs cuts, Foidl
tailored specific problem learning first-order decision lists, use cuts
stylized manner particularly useful functional problems involve rules
exceptions. Bain Muggleton (1992) Bain (1992) discuss technique uses
negation failure handle exceptions. However, using negation failure significantly
different decision lists since simply prevents clause covering exceptions rather
learning additional clause over-rides existing clause specifies
correct output set exceptions.

5.2 Related Work Past-Tense Learning
shortcomings previous work past-tense learning reviewed section 2.2,
results section 4 clearly demonstrate generalization advantage Foidl exhibits
problem. However, couple issues deserve additional discussion.
previous work problem concerned modelling various
psychological phenomenon, U-shaped learning curve children exhibit
irregular verbs acquiring language. paper addressed issue psychological validity, rather focused performance accuracy exposure fixed
number training examples. Therefore, make specific psychological claims based
current results.
However, humans obviously produce correct past tense arbitrarily-long novel
words, Foidl easily model fixed-length feature-based representations clearly
cannot. Ling developed version SPA eliminates position dependence fixed
word-length (Ling, 1995) using sliding window used NETtalk (Sejnowski
& Rosenberg, 1987). large window used includes 15 letters either side
current position (padded blanks necessary) order always include entire
word examples corpus. results approach significantly better
normal SPA still inferior Foidl's results. Also, approach still requires
fixed-sized input window prevents handling arbitrary-length irregular verbs.
Recurrent neural networks could used avoid word-length restrictions (Cotrell &
Plunkett, 1991), although appears one yet applied standard
present-tense past-tense mapping problem. However, believe diculty training
recurrent networks relatively poor ability maintain state information arbitrarily
long would limit performance task.
Another issue comprehensibility transparency learned result.
Foidl's programs past-tense short, concise, readable; unlike complicated networks, decision forests, pure logic programs generated previous approaches.
Ling Marinov (1993) discusses possibility transforming SPA's decision forest
19

fiMooney & Califf

comprehensible first-order rules; however, approach directly learning first-order
rules data seems clearly preferable.

6. Future Work
One obvious topic future research Foidl's cognitive modelling abilities context
past-tense task. Incorporating over-fitting avoidance methods may allow system
model U-shaped learning curve manner analogous demonstrated Ling
Marinov (1993). ability model human results generating past tense
novel psuedo-verbs (e.g., spling ! splang) could examined compared SPA
(Ling & Marinov, 1993) connectionist methods.
Although first-order decision lists represent fairly general class programs, currently
convincing experimental results past-tense problem. Many realistic
problems consist rules exceptions, experimental results additional applications needed support general utility representation.
Despite advantages, use intensional background knowledge ILP incurs
significant performance cost, since examples must continually reproved testing
alternative literals specialization. computation accounts training
time Foidl. One approach improving computational eciency would maintain
partial proofs examples incrementally update proofs additional literals
added clause. approach would Foil's approach maintaining
tuples, would require using meta-interpreter Prolog, incurs significant
overhead. Ecient use intensional knowledge ILP could greatly benefit work
rapid incremental compilation logic programs, i.e., incrementally updating compiled code
account small changes definition predicate.
Foidl could potentially benefit methods handling noisy data preventing
over-fitting. Pruning methods employed Foil related systems (Quinlan, 1990; Lavrac
& Dzeroski, 1994) could easily incorporated. decision list framework, alternative
simply ignoring incorrectly covered examples noise treat exceptions
handled subsequently learned clauses (as uncovering technique discussed
section 3.3).
Theoretical results learnability restricted classes first-order decision lists
another interesting area research. Given results PAC-learnability propositional decision lists (Rivest, 1987) restricted classes ILP problems (Dzeroski, Muggleton, & Russell, 1992; Cohen, 1994), appropriately restricted class first-order decision
lists PAC-learnable.

7. Conclusions
paper addressed two main issues: appropriateness first-order learner
popular past-tense problem, problems previous ILP systems handling
functional tasks whose best representation rules exceptions. results clearly
demonstrate ILP system outperforms decision-tree neural-network
systems previously applied past-tense task. important since
results showing first-order learner performs significantly better apply20

fiInduction First-Order Decision Lists: Learning English Past Tense

ing propositional learners best feature-based encoding problem. research
demonstrates ecient effective algorithm learning concise,
comprehensible symbolic programs small interesting subproblem language acquisition. Finally, work shows possible eciently learn logic programs
involve cuts exploit clause order particular class problems, demonstrates usefulness intensional background implicit negatives. Solutions many
practical problems seem require general default rules characterizable exceptions,
therefore may best learned using first-order decision lists.

Acknowledgements
basic research paper conducted first author leave
University Sydney supported grant Prof. J.R. Quinlan Australian
Research Council. Thanks Ross Quinlan providing enjoyable productive
opportunity Ross Mike Cameron-Jones important discussions
pointers greatly aided development Foidl. Thanks Ross aiding us
running Foil experiments. Discussions John Zelle Cindi Thompson
University Texas uenced work. Partial support provided
grant IRI-9310819 National Science Foundation MCD fellowship
University Texas awarded second author.

References

Bain, M. (1992). Experiments non-monotonic first-order induction. Muggleton, S.
(Ed.), Inductive Logic Programming, pp. 423{435. Academic Press, New York, NY.
Bain, M., & Muggleton, S. (1992). Non-monotonic learning. Muggleton, S. (Ed.), Inductive Logic Programming, pp. 145{162. Academic Press, New York, NY.
Bergadano, F. (1993). interactive system learn functional logic programs. Proceedings Thirteenth International Joint Conference Artificial intelligence, pp.
1044{1049 Chambery, France.
Bergadano, F., Gunetti, D., & Trinchero, U. (1993). diculties learning logic programs cut. Journal Artificial Intelligence Research, 1, 91{107.
Califf, M. E. (1994). Learning past tense English verbs: inductive logic programming approach. Unpublished project report.
Cameron-Jones, R. M., & Quinlan, J. R. (1994). Ecient top-down induction logic
programs. SIGART Bulletin, 5 (1), 33{42.
Clark, P., & Niblett, T. (1989). CN2 induction algorithm. Machine Learning, 3,
261{284.
Cohen, W. W. (1994). Pac-learning nondeterminate clauses. Proceedings Twelfth
National Conference Artificial Intelligence, pp. 676{681 Seattle, WA.
21

fiMooney & Califf

Cohen, W. (1992). Compiling prior knowledge explicit bias. Proceedings
Ninth International Conference Machine Learning, pp. 102{110 Aberdeen,
Scotland.
Cotrell, G., & Plunkett, K. (1991). Learning past tense recurrent network: Acquiring mapping meaning sounds. Proceedings Thirteenth Annual
Conference Cognitive Science Society, pp. 328{333 Chicago, IL.
De Raedt, L., & Bruynooghe, M. (1993). theory clausal discovery. Proceedings
Thirteenth International Joint Conference Artificial intelligence, pp. 1058{1063
Chambery, France.
Dzeroski, S., Muggleton, S., & Russell, S. (1992). Pac-learnability determinate logic
programs.. Proceedings 1992 Workshop Computational Learning Theory
Pittsburgh, PA.
Lachter, J., & Bever, T. (1988). relation linguistic structure associative
theories language learning: constructive critique connectionist learning
models. Pinker, S., & Mehler, J. (Eds.), Connections Symbols, pp. 195{247.
MIT Press, Cambridge, MA.
Lavrac, N., & Dzeroski, S. (Eds.). (1994). Inductive Logic Programming: Techniques
Applications. Ellis Horwood.
Ling, C. X. (1994). Learning past tense English verbs: symbolic pattern associator vs. connectionist models. Journal Artificial Intelligence Research, 1, 209{229.
Ling, C. X. (1995). Personal communication.
Ling, C. X., & Marinov, M. (1993). Answering connectionist challenge: symbolic
model learning past tense English verbs. Cognition, 49 (3), 235{290.
MacWhinney, B., & Leinbach, J. (1991). Implementations conceptualizations: Revising verb model. Cognition, 40, 291{296.
Muggleton, S., & Buntine, W. (1988). Machine invention first-order predicates inverting resolution. Proceedings Fifth International Conference Machine
Learning, pp. 339{352 Ann Arbor, MI.
Muggleton, S., & Feng, C. (1990). Ecient induction logic programs. Proceedings
First Conference Algorithmic Learning Theory Tokyo, Japan. Ohmsha.
Muggleton, S., King, R., & Sternberg, M. (1992). Protein secondary structure prediction
using logic-based machine learning. Protein Engineering, 5 (7), 647{657.
Muggleton, S. H. (Ed.). (1992). Inductive Logic Programming. Academic Press, New York,
NY.
Norvig, P. (1992). Paradigms Artificial Intelligence Programming: Case Studies Common Lisp. Morgan Kaufmann, San Mateo, CA.
22

fiInduction First-Order Decision Lists: Learning English Past Tense

Pazzani, M., & Kibler, D. (1992). utility background knowledge inductive learning.
Machine Learning, 9, 57{94.
Pinker, S., & Prince, A. (1988). language connectionism: Analysis parallel
distributed model language acquisition. Pinker, S., & Mehler, J. (Eds.), Connections Symbols, pp. 73{193. MIT Press, Cambridge, MA.
Quinlan, J. R. (1993). C4.5: Programs Machine Learning. Morgan Kaufmann, San
Mateo,CA.
Quinlan, J. R. (1994). Past tenses verbs first-order learning. Zhang, C., Debenham,
J., & Lukose, D. (Eds.), Proceedings Seventh Australian Joint Conference
Artificial Intelligence, pp. 13{20 Singapore. World Scientific.
Quinlan, J. R., & Cameron-Jones, R. M. (1993). FOIL: midterm report. Proceedings
European Conference Machine Learning, pp. 3{20 Vienna.
Quinlan, J. (1990). Learning logical definitions relations. Machine Learning, 5 (3),
239{266.
Richards, B. L., & Mooney, R. J. (1995). Automated refinement first-order Horn-clause
domain theories. Machine Learning, press.
Rivest, R. L. . (1987). Learning decision lists. Machine Learning, 2 (3), 229{246.
Rumelhart, D. E., & McClelland, J. (1986). learning past tense English verbs.
Rumelhart, D. E., & McClelland, J. L. (Eds.), Parallel Distributed Processing, Vol.
II, pp. 216{271. MIT Press, Cambridge, MA.
Sammut, C., & Banerji, R. B. (1986). Learning concepts asking questions. Michalski,
R. S., Carbonell, J. G., & Mitchell, T. M. (Eds.), Machine Learning: AI Approach,
Vol. II, pp. 167{191. Morgan Kaufman.
Sejnowski, T. J., & Rosenberg, C. (1987). Parallel networks learn pronounce English
text. Complex Systems, 1, 145{168.
Shapiro, E. (1983). Algorithmic Program Debugging. MIT Press, Cambridge, MA.
Shavlik, J. W., Mooney, R. J., & Towell, G. G. (1991). Symbolic neural learning
algorithms: experimental comparison. Machine Learning, 6, 111{143.
Stahl, I., Tausend, B., & Wirth, R. (1993). Two methods improving inductive logic
programming systems. Machine Learning: ECML-93, pp. 41{55 Vienna.
Webb, G. I., & Brkic, N. (1993). Learning decision lists prepending inferred rules.
Proceedings Australian Workshop Machine Learning Hybrid Systems,
pp. 6{10 Melbourne, Australia.
Zelle, J. M., & Mooney, R. J. (1994a). Combining top-down bottom-up methods
inductive logic programming. Proceedings Eleventh International Conference
Machine Learning New Brunswick, NJ.
23

fiMooney & Califf

Zelle, J. M., & Mooney, R. J. (1994b). Inducing deterministic Prolog parsers treebanks:
machine learning approach. Proceedings Twelfth National Conference
Artificial Intelligence, pp. 748{753 Seattle, WA.

24


