journal artificial intelligence

submitted published

induction first order decision lists
learning past tense english verbs

raymond j mooney
mary elaine califf

department computer sciences university texas
austin tx

mooney cs utexas edu
mecaliff cs utexas edu

abstract

presents method inducing logic programs examples learns
class concepts called first order decision lists defined ordered lists clauses
ending cut method called foidl foil quinlan
employs intensional background knowledge avoids need explicit negative examples particularly useful involve rules specific exceptions
learning past tense english verbs task widely studied context
symbolic connectionist debate foidl able learn concise accurate programs
significantly fewer examples previous methods connectionist
symbolic

introduction

inductive logic programming ilp growing subtopic machine learning studies
induction prolog programs examples presence background knowledge
muggleton lavrac dzeroski due expressiveness first order logic
ilp methods learn relational recursive concepts cannot represented
attribute value representations assumed machine learning ilp methods successfully induced small programs sorting list manipulation shapiro
sammut banerji muggleton buntine quinlan cameron jones
well produced encouraging important applications predicting protein secondary structure muggleton king sternberg automating
construction natural language parsers zelle mooney b
however current ilp techniques make important assumptions restrict application three common assumptions
background knowledge provided extensional form set ground literals
explicit negative examples target predicate available
target program expressed pure prolog clause order irrelevant
procedural operators cut disallowed
currently well known successful ilp systems golem muggleton feng
foil quinlan make three assumptions however
assumptions brings significant limitations since
adequate extensional representation background knowledge frequently infinite
intractably large

c ai access foundation morgan kaufmann publishers rights reserved

fimooney califf

explicit negative examples frequently unavailable adequate set negative
examples computed closed world assumption infinite intractably large
concise representation many concepts requires use clause ordering
cuts bergadano gunetti trinchero
presents ilp method called foidl first order induction decision lists helps overcome limitations incorporating following
properties
background knowledge represented intensionally logic program
explicit negative examples need supplied constructed assumption
output completeness used instead implicitly determine hypothesized
clause overly general quantify degree generality simply
estimating number negative examples covered
learned program represented first order decision list ordered set
clauses ending cut representation useful
best represented general rules specific exceptions
name implies foidl closely related foil follows similar top
greedy specialization guided information gain heuristic however
substantially modified address three advantages listed use intensional
background knowledge fairly straightforward incorporated previous foil
derivatives lavrac dzeroski pazzani kibler zelle mooney b
development foidl motivated failure observed applying existing ilp methods particular learning past tense english
verbs studied fairly extensively connectionist symbolic methods rumelhart mcclelland macwhinney leinbach ling
however previous efforts used specially designed feature encodings impose fixed limit length words fail capture position independence
underlying transformation believed representing constructing logic program predicate past x x words represented
lists letters e g past c c e past c h e c h e
past r e r e would produce much better however due
limitations mentioned unable get reasonable foil
golem however overcoming limitations foidl able learn highly accurate programs past tense many fewer examples required
previous methods
remainder organized follows section provides important background material foil past tense learning section presents
foidl details incorporates three advantages discussed section presents learning past tense english verbs demonstrating
foidl performs previous methods section reviews related work
section discusses limitations future directions section summarizes presents
conclusions


fiinduction first order decision lists learning english past tense

background

since foidl foil section presents brief review important ilp system quinlan quinlan cameron jones cameron jones quinlan
provide complete description section presents brief review
previous work english past tense

foil

foil learns function free first order horn clause definition target predicate terms
background predicates input consists extensional definitions
predicates tuples constants specified types example input appropriate
learning definition list membership
member elt lst b b b b c
components lst elt lst b b b c b c

elt type denoting possible elements includes b c lst
type defined consisting lists containing three elements
components b c background predicate true iff list whose first element b whose rest list c must provided place function
list construction foil requires negative examples target concept
supplied directly computed closed world assumption example
closed world assumption would produce pairs form elt lst explicitly provided positive examples e g b
given input foil learns program one clause time greedy covering
summarized follows
let positives cover positive examples
positives cover empty
clause c covers preferably large subset positives cover
covers negative examples
add c developing definition
remove examples covered c positives cover
example clause might learned member one iteration loop
member b components b c

since covers positive examples element first one list
cover negatives clause could learned cover remaining examples
member b components b c member

together two clauses constitute correct program member
clause step implemented general specific hill climbing search
adds antecedents developing clause one time step evaluates possible
literals might added selects one maximizes information gain heuristic
maintains set tuples satisfy current clause includes bindings
variables introduced body following pseudocode summarizes
procedure


fimooney califf

initialize c r v v vk r target predicate arity k
initialize contain positive tuples positives cover negative tuples
contains negative tuples
best literal l add clause
form training set containing tuple satisfies l
tuples form b b concatenated b set bindings
variables introduced l literal satisfied
e matches tuple extensional definition predicate
replace
foil considers adding literals possible variablizations predicate long
type restrictions satisfied least one arguments existing variable
bound head previous literal body literals evaluated
number positive negative tuples covered preferring literals cover many positives
negatives let denote number positive tuples set define
log jt j

chosen literal one maximizes
gain l

number tuples extensions e number current
positive tuples covered l
foil includes many additional features heuristics pruning space
literals searched methods including equality negation failure useful literals
immediately provide gain determinate literals pre pruning post pruning
clauses prevent fitting methods ensuring induced programs
terminate papers referenced consulted details
features








learning past tense english verbs

rumelhart mcclelland first build computational model pasttense learning classic perceptron special phonemic encoding
words employing called wickelphones wickelfeatures general goal
connectionist could account interesting language learning behavior
previously thought require explicit rules model heavily criticized opponents
connectionist language acquisition relatively poor achieved
heavily engineered representations training techniques employed pinker
prince lachter bever macwhinney leinbach attempted
address criticisms standard multi layer backpropagation learning
simpler unibet encoding phonemes phonemes
encoded single ascii character
ling marinov ling criticize current connectionist past tense acquisition heavily engineered representations poor experimental
methodology present systematic system called spa symbolic pattern associator uses slightly modified version c quinlan build


fiinduction first order decision lists learning english past tense

forest decision trees maps fixed length input pattern fixed length output pattern ling head head spa generalizes significantly better
backpropagation number variations employing different phonemic
encodings e g vs given training examples
however previous work encodes fixed length pattern association fails capture generativity position independence true transformation example use letter patterns
c c e

unibet phonemic encoding
k k

separate decision tree output unit used predict character output
pattern input characters therefore learning general rules add
ed must repeated position word end words longer
characters cannot handled best spa exploit highly engineered
feature template modified version c default leaf labeling strategy tailor
string transformation
although ilp methods seem appropriate initial attempts
apply foil golem past tense learning gave disappointing califf
discuss three listed introduction contribute
diculty applying current ilp methods
principle background predicate append sucient constructing accurate
past tense programs incorporated ability include constants arguments
equivalently ability add literals bind variables specific constants called
theory constants foil however background predicate allow appending
empty list appropriate use predicate called split b c
splits list two non empty sublists b c intensional definition split
split x z x z
split x x w z split w z

split add ed rule represented
past b split b e

foil learned form
past b split b c c e

providing extensional definition split includes possible strings fewer
characters least strings clearly intractable however providing partial definition includes possible splits strings actually appear training corpus
possible generally sucient therefore providing adequate extensional background
knowledge cumbersome requires careful engineering however major

supplying appropriate set negative examples problematic closedworld assumption produce pairs words training set second
past tense first feasible useful case clause


fimooney califf

past b split b c

likely learned since covers positives
negatives since unlikely word prefix another word past
tense however clause useless producing past tense novel verbs
domain accuracy must measured ability actually generate correct output
novel inputs rather ability classify pre supplied tuples arguments positive
negative obvious solution supplying strings characters less
negative examples past tense word clearly intractable providing specially
constructed near miss negative examples past c h e c h e e
helpful requires careful engineering exploits detailed prior knowledge

order address negative examples quinlan applied
foil employed different target predicate representing pasttense transformation used three place predicate past x z true iff
input word x transformed past tense form removing current ending
substituting ending z example past c e past r e
e e simple preprocessor map data two place predicate
form since sample verb pairs contains different end fragments
manageable number closed world negatives approximately
every positive example training set unibet phonemic
encodings quinlan obtained slightly better ling best spa exploited highly engineered feature template vs training examples
significantly better spa normal although three place target predicate incorporates knowledge desired transformation arguably
requires less representation engineering previous methods
however quinlan notes still hampered foil inability
exploit clause order example normal alphabetic encoding foil quickly
learns clause sucient regular verbs
past b c b c e

however since clause still covers fair number negative examples due many
irregular verbs continues add literals foil creates number specialized
versions clause together still fail capture generality underlying
default rule compounded foil inability add constraints
end e since foil separates addition literals containing variables
binding variables constants literals form v c cannot learn clauses

past b c b c e split e

since word split several ways clearly equivalent learnable
clause
past b c b c e split e e e

quinlan work motivated early attempts use foil



fiinduction first order decision lists learning english past tense

consequently must approximate true rule learning many clauses form
past b c b c e split e e b
past b c b c e split e e


foil generated overly complex programs containing clauses
phonemic alphabetic versions
however experienced prolog programmer would exploit clause order cuts
write concise program first handles specific exceptions falls
general default rules exceptions fail apply example program
past b
past b
past b
past b



split c e e p split b c e p
split c split b c e
split c e split b
split b e

summarized
word ends eep replace eep ept e g sleep slept
else word ends replace ied
else word ends e add
else add ed
foidl directly learn programs form e ordered sets clauses ending
cut call programs first order decision lists due similarity propositional
decision lists introduced rivest foidl uses normal binary target predicate
requires explicit negative examples therefore believe requires significantly
less representation engineering previous work area

foidl induction

stated introduction foidl adds three major features foil intensional
specification background knowledge output completeness substitute explicit
negative examples support learning first order decision lists following
subsections describe modifications made incorporate features

intensional background

described foil assumes background predicates provided extensional
definitions however burdensome frequently intractable providing intensional definition form general prolog clauses generally preferable example
instead providing numerous tuples components predicates easier give
intensional definition
components b b

intentional background definitions restricted function free pure prolog
exploit features language


fimooney califf

modifying foil use intensional background straightforward instead matching
literal set tuples determine whether covers example
prolog interpreter used attempt prove literal satisfied
intensional definitions unlike foil expanded tuples maintained positive
negative examples target concept reproved alternative specialization
developing clause therefore pseudocode learning clause simply
initialize c r v v vk r target predicate arity k
initialize contain examples positives cover negative examples
contains negative tuples
best literal l add clause
let subset examples still proved instances
target concept specialized clause
replace
since expanded tuples produced information gain heuristic picking best
literal simply
gain l jt j









output completeness implicit negatives

order overcome need explicit negative examples mode declaration
target concept must provided e specification whether argument input
output assumption output completeness made indicating
every unique input pattern training set training set includes correct
output patterns therefore output program produces given input
assumed represent negative example require positive
examples part training set unique input pattern training
set positive examples input pattern must training
set assumption trivially met predicate represents function single
unique output input
example assumption output completeness mode declaration past
indicates correct past tense forms included input word
training set predicates representing functions past implies
output example unique outputs implicitly represent negative examples however output completeness applied non functional cases
append indicating possible pairs lists appended
together produce list included training set e g append b b
append b b append b b
given output completeness assumption determining clause overly general
straightforward positive example output query made determine
outputs given input e g past c x outputs generated
positive examples clause still covers negative examples requires
specialization note intensional interpretation learned clauses required order
answer output queries
addition order compute gain alternative literals specialization
negative coverage clause needs quantified incorrect answer output


fiinduction first order decision lists learning english past tense

query ground e contains variables clearly counts single negative example e g past c h e c h e e however output queries frequently
produce answers universally quantified variables example given overly general
clause past b split c query past c x generates answer
past c implicitly represents coverage infinite number negative
examples order quantify negative coverage foidl uses parameter u represent
bound number possible terms since set possible terms herbrand
universe background knowledge together examples generally infinite u
meant represent heuristic estimate finite number terms ever
actually occur practice e g number distinct words english negative coverage represented non ground answer output query estimated uv p
v number variable arguments answer p number positive
examples answer unifies uv term stands number unique
ground outputs represented answer e g answer append x b stands

u different ground outputs p term stands number represent
positive examples allows foidl quantify coverage large numbers implicit
negative examples without ever explicitly constructing generally sucient
estimate u fairly large constant e g empirically method
sensitive exact value long significantly greater number ground
outputs ever generated clause
unfortunately estimate sensitive enough example clauses
past b split c
past b split b c

cover u implicit negative examples output query past c x since first
produces answer past c second produces answer past c
c however second clause clearly better since least requires output input sux added since presumably words
words start c assuming total number words finite
first clause considered cover negative examples therefore arguments
partially instantiated c counted fraction
variable calculating v specifically partially instantiated output argument scored
fraction subterms variables e g c counts
variable argument therefore first clause scored covering u implicit negatives second covering u given reasonable values u number
positives covered clause literal split b c preferred
revised specialization incorporates implicit negatives
initialize c r v v vk r target predicate arity k
initialize contain examples positives cover output queries
positive examples
contains output queries
best literal l add clause
let subset positive examples still proved instances
target concept specialized clause plus output queries




fimooney califf

still produce incorrect answers
replace


literals scored described previous section except jt j computed
number positive examples plus sum number implicit negatives covered
output query

first order decision lists

described first order decision lists ordered sets clauses ending
cut answering output query cuts simply eliminate first answer
produced trying clauses order therefore representation similar
propositional decision lists rivest ordered lists pairs rules
form ti ci test ti conjunction features ci category label
example assigned category first pair whose test satisfies
original rivest cn clark niblett rules
learned order appear final decision list e rules appended
end list learned however webb brkic argue learning
decision lists reverse order since preference functions tend learn general
rules first best positioned default cases towards end introduce
prepend learns decision lists reverse order present indicating
cases learns simpler decision lists superior predictive accuracy foidl
seen generalizing prepend first order case target predicates representing
functions learns ordered sequence clauses reverse order resulting program
produces first output generated first satisfied clause
basic operation best illustrated concrete example
alphabetic past tense current easily learns partial clause
past b split b c c e

however discussed section clause still covers negative examples due irregular verbs however produces correct ground output subset examples e
regular verbs indication best terminate clause handle
examples add earlier clauses decision list handle remaining examples
fact produces incorrect answers output queries safely ignored
decision list framework since handled earlier clauses therefore
examples correctly covered clause removed positives cover
clause begun literals provide best gain
past b split b c c

since many irregulars add since end e clause
produces correct ground output subset examples however
complete since produces incorrect output examples correctly covered previously
learned clause e g past c c therefore specialization continues
cases eliminated clause
note untrue literals added initially empty clause



fiinduction first order decision lists learning english past tense

past b split b c c split e e e

added front decision list examples covers removed
positives cover ensures every clause produces correct outputs
subset examples incorrect output examples already
correctly covered previously learned clauses process continues adding clauses
front decision list exceptions handled positives cover
empty
resulting clause specialization summarized follows
initialize c r v v vk r target predicate arity k
initialize contain examples positives cover output queries
positive examples
contains output queries
best literal l add clause
let subset positive examples whose output query still produces
first answer unifies correct answer plus output queries

produce non ground first answer unifies correct answer
produce incorrect answer produce correct answer
previously learned clause
replace




many cases able learn accurate compact first order decision lists
past tense expert program shown section however due highly irregular verbs encounter local minima unable literals
provide positive gain still covering required minimum number examples
originally handled terminating search memorizing remaining uncovered examples specific exceptions top decision list e g past r e
r e however premature termination prevents
finding low frequency regularities example alphabetic version system get stuck trying learn complex rule double final
consonant e g grab grabbed fail learn rule changing ied since
actually less frequent
current version foil tests learned clause meets minimum accuracy
threshold however unlike foil counting errors incorrect outputs queries correctly answered previously learned clauses meet threshold clause
thrown positive examples covers memorized top decision
list continues learn clauses remaining positive examples
allows foidl memorize dicult irregularities consonant doubling
still continue learn rules changing ied
minimum accuracy threshold met decision list property exploited
final attempt still learn completely accurate program negatives covered
clause examples correctly covered previously learned clauses foidl
foil foidl includes parameter minimum number examples clause must cover
normally set



fimooney califf

treats exceptions exception rule returns positives tocover covered correctly subsequently learned clauses example foidl
frequently learns clause
past b split c split b c e

changing ied however clause incorrectly covers examples
correctly covered previously learned add ed rule e g bay bayed delay
delayed since exceptions ied rule small percentage words
end system keeps rule returns examples add ed
positives cover subsequently rules
past b split b e split

learned recover examples resulting program completely consistent
training data setting minimum clause accuracy threshold foidl
applies uncovering technique covering examples
uncovers thereby guaranteeing progress towards fitting training examples

algorithmic implementation details

section brie discusses additional details foidl implementation includes discussion use modes types weak literals theory
constants current version foil includes features basically
form
foidl makes use types modes limit space literals searched argument predicate typed literals whose previously bound arguments
correct type tested specializing clause example split given
types split word prefix suffix preventing system splitting prefixes
suxes exploring arbitrary substrings word regularities predicate
given mode declaration literals whose input arguments previouslybound variables tested example split given mode split preventing
clause creating strings appending together previously generated prefixes
suxes
case literal provides positive information gain foidl gives small bonus literals
introduce variables however number weak literals added
row limited user parameter normally set example allows
system split word possible prefixes suxes even though may provide
gain substrings constrained subsequent literals
theory constants provided type literals tested binding
existing variable constant appropriate type example literal x e
generated x type suffix runs past tense theory constants included
every prefix sux occurs least two words training data helps
control training time limiting number literals searched affect
literals actually chosen since minimum clause coverage test prevents foidl
choosing literals cover least two examples anyway


fiinduction first order decision lists learning english past tense

foidl currently implemented common lisp quintus prolog unlike
current prolog version common lisp version supports learning recursive clauses
output completeness non functional target predicates however common lisp
version significantly slower since relies un optimized prolog interpreter
compiler written lisp norvig consequently presented
prolog version running sun sparcstation

experimental
test foidl performance english past tense task ran experiments
data ling made available appendix

experimental design
data used consist english verb forms normal alphabetic form
unibet phoneme representation along label indicating verb form base past
tense past participle etc label indicating whether form regular irregular
francis kucera frequency verb data include distinct pairs base
past tense verb forms ran three different experiments one used phonetic
forms verbs second used phonetic forms regular verbs
easiest form task
ling provides learning curves finally ran trials alphabetic forms verbs
training testing followed standard paradigm splitting data testing
training sets training progressively larger samples training set
averaged trials testing set trial contained verbs
order better separate contribution implicit negatives contribution decision list representation ran experiments ifoil variant
system uses intensional background output completeness assumption
build decision lists
ran experiments foil foidl ifoil compared
ling foil experiments run quinlan representation described
section quinlan negative examples provided randomlyselected could generated closed world assumption
experiments foidl ifoil used standard default values numeric
parameters term universe size minimum clause coverage weak literal limit
differences among foil ifoil foidl tested significance twotailed paired test
handling intensional interpretation recursive clauses target predicate requires additional
complexities discussed since relevant decision lists
generally recursive
versions available anonymous ftp net cs utexas edu directory
pub mooney foidl
replicated quinlan since memory limitations prevented us generated negatives larger training sets



fimooney califf





accuracy




foidl
ifoil
foil
spa
neural network











training examples





figure accuracy phonetic past tense task verbs


phonetic task regular irregular verbs presented
figure graph shows foil ifoil foidl along
best ling provide learning curve task expected
foidl performed systems task surpassing ling best
examples examples ifoil performed quite poorly barely beating neural
network despite effectively negatives opposed foil
poor performance due least part overfitting training data ifoil
lacks noise handling techniques foil foil advantage three place
predicate gives bias toward learning suxes ifoil poor performance
task shows implicit negatives sucient
bias decision lists three place predicate noise handling needed
differences foil foidl significant level foidl
ifoil significant level differences foil ifoil
significant training examples less significant level
examples
figure presents accuracy phonetic task regulars curves
spa neural net reported ling foidl outperformed systems particular task demonstrated one
closed world negatives regular past tense task second argument quinlan place predicate empty list therefore constants
generated positive examples foil never produce rules ground second argument since cannot create negative examples constants second
argument prevents system learning rule generate past tense order


fiinduction first order decision lists learning english past tense





accuracy




foidl
ifoil
foil
spa
neural network















training examples









figure accuracy phonetic past tense task regulars
obtain reported introduced extra constants second argument
specifically constants third argument enabling closed world assumption
generate appropriate negatives task ifoil seem gain advantage
foil able effectively use negatives regularity data
allows ifoil foil achieve accuracy examples differences
foil foidl significant level ifoil
foidl differences ifoil foil significant examples
significant level examples significant level
training examples
alphabetic version appear figure task
typically considered literature interest concerned
incorporating morphology natural language understanding systems deal
text dicult task primarily consonant doubling
foidl ifoil foil alphabetic task even
irregular full phonetic task ifoil overfits data performs quite poorly
differences foil foidl significant level
examples level examples differences
ifoil foidl significant level foil ifoil
significant training examples significant level
training examples significant level examples
three tasks foidl clearly outperforms systems demonstrating
first order decision list bias good one learning task sucient set
negatives necessary five systems provide way neural
network spa learn multiple class classification tasks phoneme belongs
position foil uses three place predicate closed world negatives ifoil


fimooney califf







accuracy









foidl
ifoil
foil

















training examples









figure accuracy alphabetic past tense task
foidl course use output completeness assumption primary importance
implicit negatives provide advantage propositional
neural network systems enable first order systems perform task
without knowledge task required foidl decision lists give
significant added advantage though advantage less apparent regular phonetic
task exceptions
clearly foidl produces accurate rules systems another consideration complexity rule sets ilp systems two good measures
complexity number rules number literals generated figure shows
number rules generated foil ifoil foidl phonetic task verbs
number literals generated appears figure since interested generalization since foil attempt fit training data
include rules foidl ifoil add order memorize individual exceptions although numbers comparable examples increasing numbers
examples programs foil ifoil generate grow much faster foidl programs
large number rules literals learned ifoil tendency overfit data
foidl generates comprehensible programs following example program generated alphabetic version task examples excluding
memorized examples
past b split c e p split b c p
past b split c split b c e split r
past b split c split b c e split l

large number irregular pasts english foidl memorizes average verbs per
trial examples



fiinduction first order decision lists learning english past tense





foidl
ifoil
foil

number rules

























training examples









figure number rules created phonetic past tense task





foidl
ifoil
foil

number literals























training examples









figure number literals created phonetic past tense task


fimooney califf

past b
past b
past b
past b



split b e split c split
split b r e split c u r
split b split c e
split b e

training times systems considered dicult
compare ling provide timing though probably assume
comparing symbolic neural learning shavlik mooney towell
spa runs fairly quickly since c backpropagation took
considerably longer tests foil foidl directly comparable
run different architectures foil runs done sparc
examples foil averaged minutes phonetic task verbs foidl
experiments ran sparc averaged minutes task even allowing
differences speed two machines factor two foidl quite
bit slower probably due largely cost intentional background part
implementation prolog opposed c

related work

related work ilp

although three features mentioned introduction distinguishes foidl
work inductive logic programming number related pieces
mentioned use intensional background knowledge least distinguishing feature
since number ilp systems incorporate aspect focl pazzani kibler
mfoil lavrac dzeroski grendel cohen forte richards
mooney chillin zelle mooney use intensional background
degree context foil ilp systems employ
intensional background include early ones shapiro sammut banerji
recent ones bergadano et al stahl tausend wirth
use implicit negatives significantly novel described section
considerably different explicit construction closed world assumption therefore employed explicit construction sucient negative examples intractable bergadano et al allows user supply intensional definition
negative examples covers large set ground instances e g past c x
equal x c e however equivalent output completeness user
would explicitly provide separate intensional negative definition positive
example non monotonic semantics used eliminate need negative examples
claudien de raedt bruynooghe effect output completeness
assumption case arguments target relation outputs however
output completeness permits exibility allowing arguments specified
inputs counting negative examples extra outputs generated specific
inputs training set flip bergadano provides method learning functional programs without negative examples making assumption equivalent output
completeness functional case output completeness general permits learning non functional programs well unlike foidl none previous


fiinduction first order decision lists learning english past tense

methods provide way quantifying implicit negative coverage context heuristic
top specialization
notion first order decision list unique foidl ilp system
attempts learn programs exploit clause order cuts bergadano et al
discusses many learning arbitrary programs cuts
brute force search used intractable realistic
instead addressing general learning arbitrary programs cuts foidl
tailored specific learning first order decision lists use cuts
stylized manner particularly useful functional involve rules
exceptions bain muggleton bain discuss technique uses
negation failure handle exceptions however negation failure significantly
different decision lists since simply prevents clause covering exceptions rather
learning additional clause rides existing clause specifies
correct output set exceptions

related work past tense learning
shortcomings previous work past tense learning reviewed section
section clearly demonstrate generalization advantage foidl exhibits
however couple issues deserve additional discussion
previous work concerned modelling
psychological phenomenon u shaped learning curve children exhibit
irregular verbs acquiring language addressed issue psychological validity rather focused performance accuracy exposure fixed
number training examples therefore make specific psychological claims
current
however humans obviously produce correct past tense arbitrarily long novel
words foidl easily model fixed length feature representations clearly
cannot ling developed version spa eliminates position dependence fixed
word length ling sliding window used nettalk sejnowski
rosenberg large window used includes letters side
current position padded blanks necessary order include entire
word examples corpus significantly better
normal spa still inferior foidl still requires
fixed sized input window prevents handling arbitrary length irregular verbs
recurrent neural networks could used avoid word length restrictions cotrell
plunkett although appears one yet applied standard
present tense past tense mapping however believe diculty training
recurrent networks relatively poor ability maintain state information arbitrarily
long would limit performance task
another issue comprehensibility transparency learned
foidl programs past tense short concise readable unlike complicated networks decision forests pure logic programs generated previous approaches
ling marinov discusses possibility transforming spa decision forest


fimooney califf

comprehensible first order rules however directly learning first order
rules data seems clearly preferable

future work
one obvious topic future foidl cognitive modelling abilities context
past tense task incorporating fitting avoidance methods may allow system
model u shaped learning curve manner analogous demonstrated ling
marinov ability model human generating past tense
novel psuedo verbs e g spling splang could examined compared spa
ling marinov connectionist methods
although first order decision lists represent fairly general class programs currently
convincing experimental past tense many realistic
consist rules exceptions experimental additional applications needed support general utility representation
despite advantages use intensional background knowledge ilp incurs
significant performance cost since examples must continually reproved testing
alternative literals specialization computation accounts training
time foidl one improving computational eciency would maintain
partial proofs examples incrementally update proofs additional literals
added clause would foil maintaining
tuples would require meta interpreter prolog incurs significant
overhead ecient use intensional knowledge ilp could greatly benefit work
rapid incremental compilation logic programs e incrementally updating compiled code
account small changes definition predicate
foidl could potentially benefit methods handling noisy data preventing
fitting pruning methods employed foil related systems quinlan lavrac
dzeroski could easily incorporated decision list framework alternative
simply ignoring incorrectly covered examples noise treat exceptions
handled subsequently learned clauses uncovering technique discussed
section
theoretical learnability restricted classes first order decision lists
another interesting area given pac learnability propositional decision lists rivest restricted classes ilp dzeroski muggleton russell cohen appropriately restricted class first order decision
lists pac learnable

conclusions
addressed two main issues appropriateness first order learner
popular past tense previous ilp systems handling
functional tasks whose best representation rules exceptions clearly
demonstrate ilp system outperforms decision tree neural network
systems previously applied past tense task important since
showing first order learner performs significantly better apply

fiinduction first order decision lists learning english past tense

ing propositional learners best feature encoding
demonstrates ecient effective learning concise
comprehensible symbolic programs small interesting subproblem language acquisition finally work shows possible eciently learn logic programs
involve cuts exploit clause order particular class demonstrates usefulness intensional background implicit negatives solutions many
practical seem require general default rules characterizable exceptions
therefore may best learned first order decision lists

acknowledgements
basic conducted first author leave
university sydney supported grant prof j r quinlan australian
council thanks ross quinlan providing enjoyable productive
opportunity ross mike cameron jones important discussions
pointers greatly aided development foidl thanks ross aiding us
running foil experiments discussions john zelle cindi thompson
university texas uenced work partial support provided
grant iri national science foundation mcd fellowship
university texas awarded second author

references

bain experiments non monotonic first order induction muggleton
ed inductive logic programming pp academic press york ny
bain muggleton non monotonic learning muggleton ed inductive logic programming pp academic press york ny
bergadano f interactive system learn functional logic programs proceedings thirteenth international joint conference artificial intelligence pp
chambery france
bergadano f gunetti trinchero u diculties learning logic programs cut journal artificial intelligence
califf e learning past tense english verbs inductive logic programming unpublished project report
cameron jones r quinlan j r ecient top induction logic
programs sigart bulletin
clark p niblett cn induction machine learning

cohen w w pac learning nondeterminate clauses proceedings twelfth
national conference artificial intelligence pp seattle wa


fimooney califf

cohen w compiling prior knowledge explicit bias proceedings
ninth international conference machine learning pp aberdeen
scotland
cotrell g plunkett k learning past tense recurrent network acquiring mapping meaning sounds proceedings thirteenth annual
conference cognitive science society pp chicago il
de raedt l bruynooghe theory clausal discovery proceedings
thirteenth international joint conference artificial intelligence pp
chambery france
dzeroski muggleton russell pac learnability determinate logic
programs proceedings workshop computational learning theory
pittsburgh pa
lachter j bever relation linguistic structure associative
theories language learning constructive critique connectionist learning
pinker mehler j eds connections symbols pp
mit press cambridge
lavrac n dzeroski eds inductive logic programming techniques
applications ellis horwood
ling c x learning past tense english verbs symbolic pattern associator vs connectionist journal artificial intelligence
ling c x personal communication
ling c x marinov answering connectionist challenge symbolic
model learning past tense english verbs cognition
macwhinney b leinbach j implementations conceptualizations revising verb model cognition
muggleton buntine w machine invention first order predicates inverting resolution proceedings fifth international conference machine
learning pp ann arbor mi
muggleton feng c ecient induction logic programs proceedings
first conference algorithmic learning theory tokyo japan ohmsha
muggleton king r sternberg protein secondary structure prediction
logic machine learning protein engineering
muggleton h ed inductive logic programming academic press york
ny
norvig p paradigms artificial intelligence programming case studies common lisp morgan kaufmann san mateo ca


fiinduction first order decision lists learning english past tense

pazzani kibler utility background knowledge inductive learning
machine learning
pinker prince language connectionism analysis parallel
distributed model language acquisition pinker mehler j eds connections symbols pp mit press cambridge
quinlan j r c programs machine learning morgan kaufmann san
mateo ca
quinlan j r past tenses verbs first order learning zhang c debenham
j lukose eds proceedings seventh australian joint conference
artificial intelligence pp singapore world scientific
quinlan j r cameron jones r foil midterm report proceedings
european conference machine learning pp vienna
quinlan j learning logical definitions relations machine learning

richards b l mooney r j automated refinement first order horn clause
domain theories machine learning press
rivest r l learning decision lists machine learning
rumelhart e mcclelland j learning past tense english verbs
rumelhart e mcclelland j l eds parallel distributed processing vol
ii pp mit press cambridge
sammut c banerji r b learning concepts asking questions michalski
r carbonell j g mitchell eds machine learning ai
vol ii pp morgan kaufman
sejnowski j rosenberg c parallel networks learn pronounce english
text complex systems
shapiro e algorithmic program debugging mit press cambridge
shavlik j w mooney r j towell g g symbolic neural learning
experimental comparison machine learning
stahl tausend b wirth r two methods improving inductive logic
programming systems machine learning ecml pp vienna
webb g brkic n learning decision lists prepending inferred rules
proceedings australian workshop machine learning hybrid systems
pp melbourne australia
zelle j mooney r j combining top bottom methods
inductive logic programming proceedings eleventh international conference
machine learning brunswick nj


fimooney califf

zelle j mooney r j b inducing deterministic prolog parsers treebanks
machine learning proceedings twelfth national conference
artificial intelligence pp seattle wa




