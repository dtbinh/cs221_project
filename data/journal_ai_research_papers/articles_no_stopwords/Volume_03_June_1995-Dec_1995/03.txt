Journal Artificial Intelligence Research 3 (1995) 383-403

Submitted 6/95; published 12/95

Rule-based Machine Learning Methods Functional
Prediction

Sholom M. Weiss

weiss@cs.rutgers.edu

Nitin Indurkhya

nitin@cs.usyd.edu.au

Department Computer Science, Rutgers University
New Brunswick, New Jersey 08903, USA
Department Computer Science, University Sydney
Sydney, NSW 2006, AUSTRALIA

Abstract

describe machine learning method predicting value real-valued function, given values multiple input variables. method induces solutions
samples form ordered disjunctive normal form (DNF) decision rules. central
objective method representation induction compact, easily interpretable
solutions. rule-based decision model extended search eciently similar cases prior approximating function values. Experimental results real-world data
demonstrate new techniques competitive existing machine learning
statistical methods sometimes yield superior regression performance.

1. Introduction
problem approximating values continuous variable described statistical literature regression. Given samples output (response) variable input
(predictor) variables x = fx1 :::xn g, regression task find mapping = f(x). Relative space possibilities, finite samples far complete, predefined
model needed concisely map x y. Accuracy prediction, i.e. generalization new
cases, primary concern. Regression differs classification output variable regression problems continuous, whereas classification strictly categorical.
perspective, classification thought subcategory regression.
machine learning researchers emphasized connection describing regression
\learning classify among continuous classes" (Quinlan, 1993).
traditional approach problem classical linear least-squares regression
(Scheffe, 1959). Developed refined many years, linear regression proven quite
effective many real-world applications. Clearly elegant computationally simple linear model limits, complex models may fit data better.
increasing computational power computers larger volumes data, interest grown pursuing alternative nonlinear regression methods. Nonlinear regression
models explored statistics research community many new effective
methods emerged (Efron, 1988), including projection pursuit (Friedman & Stuetzle,
1981) MARS (Friedman, 1991). Methods nonlinear regression developed outside mainstream statistics research community. neural network trained
back-propagation (McClelland & Rumelhart, 1988) one model. models
c 1995 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiWeiss & Indurkhya
found numerical analysis (Girosi & Poggio, 1990). overview many different
regression models, application classification, available literature (Ripley,
1993). methods produce solutions terms weighted models.
real-world, classification problems commonly encountered regression problems. accounts greater attention paid classification regression. many important problems real world regression type.
instance, problems involving time-series usually involve prediction real values. Besides
fact regression problems important own, another reason need
focus regression regression methods used solve classification problems.
example, neural networks often applied classification problems.
issue interpretable solutions important consideration leading
development \symbolic learning methods." popular format interpretable solutions
disjunctive normal form (DNF) model (Weiss & Indurkhya, 1993a). Decision trees
rules examples DNF models. Decision rules similar characteristics decision
trees, potential advantages: (a) stronger model (b) often better
explanatory capabilities. Unlike trees, DNF rules need mutually exclusive. Thus,
solution space includes tree solutions. rules potentially compact
predictive trees. Decision rules may offer greater explanatory capabilities
trees tree grows size, interpretability diminishes.
Among symbolic learning methods, decision tree induction, using recursive partitioning, highly developed. Many methods developed within machine learning
community, ID3 decision tree induction (Quinlan, 1986), applied exclusively classification tasks. Less widely known decision trees effective
regression. CART program, developed statistical research community, induces classification regression trees (Breiman, Friedman, Olshen, & Stone, 1984).
regression trees strictly binary trees, representation naturally follows
intensive modeling using continuous variables.1
terms performance, regression trees often competitive performance
regression methods (Breiman et al., 1984). Regression trees noted particularly
strong many higher order dependencies among input variables (Friedman,
1991). advantages regression tree model similar advantages enjoyed
classification trees models. Two principal advantages cited: (a) dynamic
feature selection (b) explanatory capabilities. Tree induction methods extremely
effective finding key attributes high dimensional applications. applications,
key features small subset original feature set. Another characteristic
decision trees often cited capability explanation terms acceptable
people. negative side, decision trees cannot represent compactly many simple
functions, example linear functions. second weakness regression tree
model discrete, yet predicts continuous variable. function approximation,
expectation smooth continuous function, decision tree provides discrete regions
discontinuous boundaries. though, regression trees often produce
strong results, many applications advantages strongly outweigh potential
disadvantages.
1. comparative study (Fayyad & Irani, 1992) suggests binary classification trees somewhat
predictive even categorical variables.

384

fiRule-based Functional Prediction
paper describe new method inducing regression rules. method
takes advantage close relationship classification regression provides
uniform general model dealing problems. Additional gains obtained
extending method manner preserves strengths partitioning
schemes compensating weaknesses. Rules used search
relevant cases, subset cases help determine function value. Thus,
model's interpretability traded better performance. Empirical
results suggest methods effective induce solutions often
superior decision trees.

2. Measuring Performance

objective regression minimize distance sample output values,
yi predicted values yi . Two measures distance commonly used. classical
regression measure equation 1, average squared distance yi yi , i.e.
variance. leads elegant formulation linear least squares model. mean
absolute distance (deviation) equation 2 used least absolute deviation regression,
perhaps intuitive measure.
mean absolute distance (deviation) equation 2 used studies.
measure average error prediction yi n cases.
0

0

Xn (yi , yi)2
i=1
n
X
MAD = 1 jyi , yi j

V ariance = n1

0

(1)

(2)
n i=1
regression problem sometimes described signal noise problem. model
extended include stochastic component equation 3. Thus, true function may
produce zero error distance. contrast classification labels assumed
correct, regression predicted values could explained number factors
including random noise component, , signal, y.
0

= f (x1 : : : xn) +
(3)
prediction primary concern, estimates based training cases alone
inadequate. principles predicting performance new cases analogous
classification, mean absolute distance used error rate. best
estimate true performance model error rate large set independent test
cases. large samples data unavailable, process train test simulated
random resampling. experiments, used (10-fold) cross-validation
estimate predictive performance.

3. Regression Tree Induction

section, contrast regression tree induction classification tree induction.
classification trees, regression trees induced recursive partitioning. solution takes
385

fiWeiss & Indurkhya
form equation 4, Ri disjoint regions, ki constant values, yji refers
y-values training cases fall within region Ri .

x Ri f (x) = ki = medianfyji g
(4)
Regression trees representation classification trees except terminal nodes. decision terminal node assign case constant value.
single best constant value median training cases falling terminal node
partition, median minimizer mean absolute distance. Figure 1
example binary regression tree. cases reaching shaded terminal node 1 (x13)
assigned constant value y=10.
1
x1<=3

x1>3

1

2

y=10

x2<=1

x2>1

2

3

y=2

y=5

Figure 1: Example Regression Tree
Tree induction methods usually proceed (a) finding covering set training
cases (b) pruning tree best size. Although classification trees
widely studied, similar approach applied regression trees. assume
reader familiar classification trees, cite differences binary
tree induction (Breiman et al., 1984; Quinlan, 1986; Weiss & Kulikowski, 1991). many
respects, regression tree induction straightforward. classification trees, error
rate poor choice node splitting, alternative functions entropy gini
employed. regression tree induction, minimized function, i.e. absolute distance,
satisfactory. node, single best split minimizes mean absolute
distance selected. Splitting continues fewer minimum number cases
covered node, cases within node identical value y.
goal find tree generalizes best new cases, often
full covering tree, particularly presence noise weak features. pruning strategies
employed classification trees equally valid regression trees. covering
procedures, substantial difference error rate measured terms
mean absolute distance. One popular method weakest-link pruning strategy (Breiman
et al., 1984). weakest-link pruning, tree recursively pruned ratio delta/n
minimized, n number pruned nodes delta increase error.
386

fiRule-based Functional Prediction
x13 ! y=10
x21 ! y=2
Otherwise y=5
Figure 2: Example Regression Rules
Weakest link pruning several desirable characteristics: (a) prunes training cases
only, remaining test cases relatively independent (b) compatible
resampling.

4. Regression Rule Induction
tree rule induction models find solutions disjunctive normal form, model
equation 4 applicable both. rule rule-set represents single partition
region Ri . However, unlike tree regions, regions rules need disjoint.
non-disjoint regions, several rules may satisfied single sample. mechanism
needed resolve con icts ki , constant values assigned, multiple rules,
Ri regions, invoked. One standard model (Weiss & Indurkhya, 1993a) order
rules. ordered rule-sets referred decision lists. first rule
satisfied selected, equation 5.

< j x Ri Rj f (x) = ki
(5)
Figure 2 example ordered rule-set corresponding tree Figure 1.
cases satisfying rule 3, rules 1 2, assigned value y=5.
Given model regression rule sets, problem find procedures effectively
induce solutions. rule-based regression, covering strategy analogous classification tree strategy could specified. rule could induced adding single component
time, added component single best minimizer distance. usual,
constant value ki median region formed current rule. rule
extended, fewer cases covered. fewer minimal number cases covered,
rule extension terminates. covered cases removed rule induction continue
remaining cases. regression analogue rule induction procedures
classification (Michalski, Mozetic, Hong, & Lavrac, 1986; Clark & Niblett, 1989).
However, instead approach, propose novel strategy mapping regression
covering problem classification problem.

4.1 Reformulation Regression Problem

motivation mapping regression classification based number factors
related extra information given regression problem: natural ordering yi
magnitude: > j yi > yj .
Let fCi g set consisting arbitrary number classes, class containing
approximately equal values fyi g. solve classification problem, expect
classes different other, patterns found distinguish
387

fiWeiss & Indurkhya
1. Generate set Pseudo-classes using P-class algorithm (Figure 4).
2. Generate covering rule-set transformed classification
problem using rule induction method Swap-1
(Weiss & Indurkhya, 1993a).
3. Initialize current rule set covering rule set save it.
4. current rule set pruned, iteratively following:
a) Prune current rule set.
b) Optimize pruned rule set (Figure 5) save it.
c) Make pruned rule set new current rule set.
5. Use test cases cross-validation pick best saved rule sets.
Figure 3: Overview Method Learning Regression Rules
classes. expect classes formed ordering fyi g reasonable classification problem? numbers reasons answer yes, particularly
rule induction procedure.
obvious situation classical linear relationship. instance,
definition, ordering fx1i . . . xni g corresponds ordering yi . Although classical
methods strong compactly determining linear functions, interest modern
methods centers around potential finding nonlinear relationships. nonlinear
functions, know usually ordering fx1i . . . xni g corresponding
fyig. Still, expect true function smooth, local region ordering
relationship hold. terms classification, know class Cj similar values
quite different class Ck much lower values y. nonlinear function
within class similar values y, similar values fx1i . . . xni g.
correspond local region function. However, true
identical values different fx1i . . . xni g multiple clusters
found within class. rule induction methods cover class single
rule, expectation multiple patterns found cover clusters.
cases assigned (pseudo-)classes, classification problem
solved following stages: (a) find covering set (b) prune rule set
appropriate size, improved results achieved additional technique considered:
(c) refine optimize rule set. overall method outlined Figure 3.

4.2 Generating Pseudo-classes
previous section, described motivation pseudo-classes. specification
classes use information beyond ordering y. assumptions
true nature underlying function made. Within environment,
goal make values within one class similar values across classes
dissimilar. wish assign values classes overall distance
yi class mean minimum.
388

fiRule-based Functional Prediction
Input: fyi g set output values
Initialize n := number cases, k := number classes
Classi
Classi := next n/k cases list sorted values
end-for
Compute Errnew
Repeat
Errold = Errnew
Casej
Classi
1. Dist[Casej , Mean(Classi,1 )] < Dist[Casej , Mean(Classi )]
Move Casej Classi,1
2. Dist[Casej , Mean(Classi+1 )] < Dist[Casej , Mean(Classi )]
Move Casej Classi+1
Next Casej
Compute Errnew
Errnew less Errold
Figure 4: Composing Pseudo-Classes (P-Class)
Figure 4 describes algorithm (P-Class) assigning values fyi g k classes. Essentially algorithm following: (a) sorts values; (b) assigns approximately
equal numbers contiguous sorted yi class; (c) moves yi contiguous class
reduces global distance Err yi mean assigned class.
Classes identical means merged. P-Class variation k-means clustering, statistical method minimizes distance measure (Hartigan & Wong, 1979).
Alternative methods depend distance measures (Lebowitz, 1985) may
used.
Given fixed number k classes, procedure relatively quickly assign yi
classes overall distances minimized. underlying function
unknown, critical global minimum assignment yi. procedure
matches well stated goals ordering yi values. obvious remaining question
determine k, number classes? Unfortunately, direct answer,
experimentation necessary. However, shall see Section 7, empirical
evidence suggesting results quite similar within local neighborhood values
k. Moreover, relatively large values k, entail increased computational complexity
rule induction, typically necessary noise-free functions modeled
exactly. Analogous comparisons neural nets increasing numbers hidden units,
trends increasing numbers partitions become evident experimentation.
One additional variation classification theme arises rule induction schemes
cover one class time. classes must ordered, last class typically
389

fiWeiss & Indurkhya
becomes default class cover situations rule classes satisfied.
regression, one default partition class unlikely best covering solution,
instead remaining cases last class repeatedly partitioned (by P-Class)
2 classes fewer cases remain.
interesting characteristic transformation regression problem
uniform general model relates classification
regression. yi values discrete categorical, P-Class merely restates standard
classification problem. example, values yi either 0 1, result
P-Class 2 non-empty classes.

4.3 Covering Rule Set
transformation, rule induction algorithms classification applied.
consider induction methods fully cover class moving induce
rules next class. step covering algorithm, problem considered
binary classification problem current class Ci versus Cj j > i, i.e.
current class versus remaining classes. rule induced, corresponding cases
removed remaining cases considered. class covered,
next class considered. example covering algorithm used Swap-1
(Weiss & Indurkhya, 1993a), procedure used paper. covering
method identical classification regression. However, one distinction
regression classes transient labels replaced median values
cases covered induced rule. rules ordered multiple rules
may satisfied, medians derived instances rule
first satisfied.
Although procedure may yield good, compact covering sets, additional procedures
necessary complete solution.

4.4 Pruning Rule Set
Typical real-world applications noisy features fully predictive. covering
set, particularly one composed many continuous variables, far over-specialized
produce best results. classification, relatively classes specified advance.
regression, expect many smaller groups values yi likely quite
different.
noted earlier regression trees usual classification pruning techniques
applied substitution mean absolute distance classification error rate.
weakest-link tree pruning, ratio delta/n recursively minimized
weakest-link rule pruning. intuitive rationale remove parts rule set
least impact increasing error. Pruning rule sets usually accomplished
either deleting complete rules single rule components (Quinlan, 1987; Weiss & Indurkhya,
1993a). general, rule pruning (for classification regression) less natural
far computationally expensive tree pruning. Tree pruning natural ow
set subset. Thus tree pruned bottom up, typically considering
effect removing subtree. Non-disjoint rules natural pruning order,
390

fiRule-based Functional Prediction
example every component rule candidate pruning may affect rules
follow specified rule order.
major difference pruning regression rules vs. classification rules.
classification, deleting rule rule component effect class labels.
regression, pruning change median-values regions. Even deletion
rule affect region medians rules ordered multiple rules may
satisfied. characteristic rule pruning regression adds substantial complexity
task. However, assuming median-values remain unchanged
evaluation candidate rules prune, pruning procedure achieve reasonable
computational eciency expense loss accuracy evaluation.
best rule component deletion selected, medians regions
re-evaluated.
Even classification rules, rule pruning inherent weaknesses. example,
rule deletion often create gap coverage. classification rules though, quite
feasible develop additional procedure refine optimize rule set. large
extent, overcomes cited weakness pruned rules sets. similar refinement
optimization procedure developed regression described next.

4.5 Rule Refinement Optimization

Given rule set RSi , improved? question applies rule set, although
mostly motivated trying improve pruned rules sets fRSo . . . RSi . . . RSng.
combinatorial optimization problem. Using error measure Err(RS), improve
RSi without changing size, i.e. number rules components? Figure 5 describes
algorithm minimizes Err(RS), MAD model prediction sample cases,
local swapping, i.e. replacing single rule component best alternative.
variation techniques used Swap-1 (Weiss & Indurkhya, 1993a).
central theme hold model configuration constant make single local
improvement configuration. Local modifications made improvements possible. Making local changes configuration widely-used optimization
technique approximate global optimum applied quite successfully,
example find near-optimum solutions traveling salesman problems (Lin & Kernighan,
1973). analogous local optimization technique, called backfitting, used
context nonlinear statistical regression (Hastie & Tibshirani, 1990).
Variations selection next improvement move could include:
1. First local improvement encountered (such backfitting)
2. Best local improvement (such Swap-1)
experiments rule induction methods, results consistently better
(2); (1) ecient, (pruned) rule induction environment mostly stable
relatively local improvements prior convergence. less stable environment,
large numbers possible configuration changes, (2) may feasible even better.
pruned rule set environment, covering procedure effective, pruned
solution relatively close local minimum solution. Weakest-link pruning
391

fiWeiss & Indurkhya
Input: RS rule set consisting rules Ri ,
set training cases
:= TRUE
(D TRUE)
RSnew := RS single best replacement
component RS reduces Err(RS)
cases using current Median(Ri )
replacement found
:= FALSE
else
RS := RSnew ; recompute Medians(Ri )
endwhile
return rule set RS
Figure 5: Optimization Rule Component Swapping
results series pruned rule sets RSi number far fewer sets would
result single prune rule rule component. RSi optimized prior
continuing pruning process. However, rule set optimization usually suspended
substantial segments covering set already pruned.
(1) used, either sequentially ordered evaluations (as backfitting) stochastic evaluations considered. Empirical evidence optimization literature supports superiority stochastic evaluation (Jacoby, Kowalik, & Pizzo, 1972).
improvements may obtained occasionally making random changes configuration
(Kirpatrick, Gelatt, & Vecchi, 1983). general combinatorial optimization techniques must substantially reworked fit specific problem type. expected
applied throughout problem solving.
result pruning covering rule set, RSo , series progressively smaller rule
sets fRSo . . . RSi . . . RSn g. objective pick best one, usually form
error estimation. Model complexity future performance highly related.
complex simple model yield poor results, objective find
right size model. Independent test cases resampling cross-validation effective
estimating future performance. absence estimates, approximations,
GCV (Craven & Wahba, 1979; Friedman, 1991), described equation 6,
used statistics literature estimate performance2. measures training error
model complexity used estimates. C(M), measure model complexity
expressed terms parameters estimated (such number weights neural net)
tests performed, C(M) assumed less n, number cases.
2. GCV acronym generalized cross-validation, apparent error training cases used
true cross-validation resampling.

392

fiRule-based Functional Prediction

Xn
GCV (M ) =

jy ,y j
0





n
C(M)
i=1 1 , n

(6)

experiments used cross-validated estimates guide final model selection
process, measures GCV may used.

4.6 Potential Problems Rule-based Regression

Regression rules, trees, induced recursive partitioning methods approximate function constant-value regions. relatively strong dynamic feature
selection high-dimensional applications, sometimes using highly predictive
features. essential weakness methods approximation partition
region constant value. continuous function even moderately sized sample,
approximation lead increased error.
deal limitation, instead constant-value functions, linear functions
substituted partition (Quinlan, 1993). However, linear function obvious
weakness true function may far linear even restricted context
single region. general, use linearity compromises highly non-parametric
nature DNF model. better strategy might examine alternative non-linear
methods.

5. Alternative Rules: k-Nearest Neighbors
k-nearest neighbor method one simplest regression methods, relying table
lookup. classify unknown case x, k cases closest new case
found sample data base stored cases. predicted y(x) equation 7 mean
values k-nearest neighbors. nearest neighbors found distance
metric euclidean distance (usually feature normalization). method
non-parametric highly non-linear nature

yknn(x) = K1

XK yk K nearest neighbours x

k=1

(7)

major problem approach limit effect irrelevant features.
limited forms feature selection sometimes employed preprocessing stage,
method cannot determine features weighted others.
result, procedure sensitive distance measure used. high-dimensional
feature space, k-nearest neighbor methods may perform poorly. limitations
precisely partitioning methods address. Thus, theory, two methods
potentially complement one another.

6. Model Combination

practice, one learning model always superior others, learning strategy
examines results different models may better. Moreover, combining
393

fiWeiss & Indurkhya
different models, enhanced results may achieved. general approach combining
learning models scheme referred stacking (Wolpert, 1992). Additional studies
performed applying scheme regression problems (Breiman, 1993; LeBlanc &
Tibshirani, 1993). Using small training samples simulated data, linear combinations
regression methods, improved results reported. Let Mi i-th model trained
sample, wi , weight given Mi .3 new case vector
x, predictions different models combined Equation 8 produce
estimate y. models may use representation, k-nearest neighbors
variable-size k, perhaps variable-size decision trees. models could completely
different, combining decision trees linear regression models. Different models
applied independently find solutions, later weighted vote taken reach
combined solution. method model combination contrast usual approach
evaluation different models, single best performing model selected.

y=

XK wkMk(x)

k=1

(8)

stacking shown give improved results simulated data, major
drawback properties combined models retained. Thus interpretable models combined, result may interpretable all.
possible compensate weaknesses one model introducing another model
controlled fashion.
suggested earlier, partitioning regression methods k-nearest neighbor regression
methods complementary. Hence one might expect suitably combining two
methods, one might obtain better performance. one recent study (Quinlan, 1993), model
trees (i.e., regression trees linear combinations leaf nodes) nearest neighbor
methods combined. combination method described equation 9,
N (x)k one K nearest neighbors x, V(x) y-value stored instance
x, T(x) result applying model tree x.

= K1

XK V (N (x)k) , (T (N (x)k) , (x))

k=1

(9)

k-nearest neighbors found independently induced regression tree (results
reported K=3). sense, approach similar combination method
equation 8. k-nearest neighbors passed tree, results used
refine nearest neighbor answer. Thus, combination model formed
independently computing global solution, later combining results.
However, strong reasons determining global nearest neighbor solution independently. While, limit, large samples, non-parametric k-nearest
neighbor methods correctly fit function, practice though, weaknesses
substantial. Finding effective global distance measure may easy, particularly
presence many noisy features. Hence different technique combining two
methods needed.
3. weights obtained minimize least squared error constraints (Breiman,
1993).

394

fiRule-based Functional Prediction

6.1 Integrating Rules Table-lookup

Consider following strategy: determine y-value case x falls region Ri ,
instead assigning single constant value ki region Ri , ki determined
(x), mean k-nearest
median value training cases region, assign yknn
(training set) instances x region Ri . Thus regression trees, equation
10. regression rules, equation 11.
(x)
x Ri f (x) = yknn

(10)

(x)
< j x Ri Rj f (x) = yknn

(11)

interesting aspect strategy k-nearest neighbor results need
considered cases covered particular partition. increases interaction models eliminates independent computation two models,
model rationale and, shall show, empirical results, supportive
approach.
representation potentially alleviates weakness partitions
assigned single constant values. Moreover, global distance measure difficulties k-nn methods may relieved table lookup reduced
partitioned related groupings.
rationale hybrid partition k-nn scheme. Note unlike stacking,
hybrid models independently determined, interact strongly one
another. However, must demonstrated methods fact complementary,
preserving strengths partitioning schemes compensating weaknesses
would introduced constant values used region. respect model
combination, two principal questions need addressed empirical experimentation:

results improved relative using model alone?
methods competitive alternative regression methods?

7. Results
Experiments conducted assess competitiveness rule-based regression compared
procedures (including less interpretable ones), well evaluate performance integrated partition k-nn regression method. Experiments performed
using seven datasets, six described previous studies (Quinlan, 1993). addition six datasets, new experiments done large telecommunications
application, labeled pole. seven datasets, one continuous
real-valued response variable. Experimental results reported terms MAD,
measured using 10-fold cross-validation. pole, 5,000 cases used training
10,000 independent testing. features different datasets mixture
continuous categorical features. pole, 48 features continuous. Descriptions
395

fiWeiss & Indurkhya

Dataset Cases Vars
price
servo
cpu
mpg
peptide
housing
pole

159
167
209
392
431
506
15000

16
19
6
13
128
13
48

Table 1: Dataset Characteristics
datasets found literature (Quinlan, 1993).4 Table 1 summarizes
key characteristics datasets used study.
Table 2 summarizes original results reported (Quinlan, 1993). include modeltrees (MT), regression trees linear fits terminal nodes; neural nets
(NNET); 3-nearest neighbors (3-nn); combined results model-trees 3-nearest
neighbors (MT/3-nn).5
Table 3 summarizes additional results obtained. include CART
regression tree (RT); 5-nearest neighbors euclidean distance (5-nn); rule regression
using Swap-1; rule regression 5-nn applied rule region (Rule/5-nn); MARS.
5-nn used expectation nearest neighbor method incrementally
improves constant-value region region moderately large sample neighbors
average.
rule-based method, parameter m, number pseudo-classes, must
determined. found using cross-validation independent test cases (in
experiments, cross-validation used). Figure 6 represents typical plot relative
error vs. number pseudo-classes (Weiss & Indurkhya, 1993b). number
partitions increases, results improve reach relative plateau deteriorate
somewhat. Similar complexity plots found models, example neural nets
(Weiss & Kapouleas, 1989).
MARS procedure several adjustable parameters.6 parameter mi, values
tried 1 (additive modeling), 2, 3, 4 number inputs. df, default value
3.0 tried well optimal value estimated cross-validation. parameter nk
varied 20 100 steps 10. Lastly, piece-wise linear well piece-wise cubic
solutions tried. setting parameters, cross-validated
accuracy monitored, value best MARS model reported.
method, besides MAD, relative error reported. relative
error simply estimated true mean absolute distance (measured cross-validation)
normalized initial mean absolute distance median. Analogous classifi4. peptide dataset slightly modified version one Quinlan refers lhrh-att paper.
version used experiments, cases missing values removed.
5. peptide slightly modified version lhrh-att dataset, result listed one
provided Quinlan personal communication.
6. particular program used MARS 3.5.

396

fiRule-based Functional Prediction

Relative Error
0.65

0.6

0.55

0.5

0.45

0.4

0.35

0.3
2

3

4

5
6
7
Number Pseudo-Classes

8

9

10

Figure 6: Prototypical Performance Varying Pseudo-Classes

Dataset MT NNET 3-nn MT/3-nn
price 1562
servo
.45
cpu
28.9
mpg
2.11
peptide .95
housing 2.45

1833
.30
28.7
2.02
2.29

1689
.52
34.0
2.72
2.90

1386
.30
28.1
2.18
2.32

Table 2: Previous Results
cation, predictions must fewer errors simply predicting largest class,
regression must better average distance median
meaningful results.
comparing performance two methods dataset, standard error
method independently estimated, larger one used comparisons.
difference performance greater 2 standard errors, difference
considered statistically significant. significance test, one must consider
overall pattern performance relative advantages competing solutions (Weiss
& Indurkhya, 1994).
dataset, Figure 7 plots relative best error found ratio best
reported result model's result. relative best error 1 indicates result
best reported result regression model. model results compared
best results regression rules, 5-nn, mixed model. graph indicates
397

fiWeiss & Indurkhya

Dataset

RT

5-nn

Rule

Rule/5-nn

MARS

MAD Error MAD Error MAD Error MAD Error MAD Error
price
1660 .40 1643 .40 1335 .32 1306 .31 1559 .38
servo
.195 .21 .582 .63 .235 .25 .227 .24 .212 .23
cpu
30.5 .39 29.4 .38 27.62 .35 26.32 .34 27.29 .35
mpg
2.28 .35 2.14 .33 2.17 .33 2.04 .31 1.94 .30
peptide .97
.46
.95
.45
.86
.40
.86
.40
.98
.46
housing 2.74 .42 2.77 .42 2.51 .38 2.35 .36 2.24 .34
pole
4.10 .14 5.91 .20 3.76 .13 3.70 .12 7.41 .25
Table 3: Performance Additional Methods
Relative Best Erate
1.2
5-nn
rule
1

rule/5-nn

0.8

0.6

0.4

0.2

0
servo

house

mpg

cpu

price

peptide

pole

Figure 7: Relative Best Erates 5-nn, Rules, Rule/5-nn
trends across datasets helps assess overall pattern performance. respect,
Rule Rule/5nn exhibit excellent performance across many applications.
empirical results allow us consider several relevant questions regarding rulebased regression:
1. rule-based regression perform compared tree-based regression? Comparing
results Rule RT, one see except servo, Rule consistently
better RT remaining six datasets. difference performance
398

fiRule-based Functional Prediction
tests significant. results significance tests, general trend (which
seen visually Figure 7) leads us conclude rule-based regression
definitely competitive trees often yields superior performance.
2. integrating 5nn rules lead improved performance relative using
model alone? comparison Rule/5nn 5nn shows datasets, Rule/5nn
significantly better. comparing Rule/5nn Rule, results indicate
three datasets (mpg, pole housing), Rule/5nn significantly better Rule,
remaining three datasets same. overall pattern
performance appears favor Rule/5nn Rule. Thus empirical results
indicate method improved results relative using model alone.
general trend seen Figure 7.
3. new methods competitive alternative regression methods? Among previous reported results, MT/3nn best performer. alternatives consider
are: Regression Trees (RT) MARS. None three methods significantly
better Rule/5nn datasets consideration except RT
significantly better servo. Furthermore, Rule/5nn significantly better
MT/3nn three five datasets (servo, cpu mpg) comparison possible. overall trend favor Rule/5nn. Comparing RT Rule/5nn,
find except servo, Rule/5nn significantly better RT remaining datasets. Comparing MARS Rule/5nn, find three datasets
(price, peptide pole), Rule/5nn significantly better. Hence empirical results overwhelmingly suggest new method competitive alternative
regression methods, hints superiority methods.

8. Discussion

considered new model rule-based regression provided comparisons
tree-based regression. many applications, strong explanatory capabilities high dimensional feature selection make DNF model quite advantageous. particularly
true knowledge-based applications, example equipment repair medical diagnosis,
contrast pure pattern recognition applications speech recognition.
rules similar trees, rule representation potentially compact
rules mutually exclusive. potential finding compact
solution particularly important problems model interpretation crucial.
Note space rules includes space trees. Thus, tree solution
best, theoretically rule induction procedure potential find it.
experiments, regression rules generally outperformed regression trees.
Fewer constant regions required estimated error rates generally lower.
Finding DNF regions substantially computationally expensive regression rules regression trees. regression rules, fairly complex optimization
techniques necessary. addition, experiments must performed find appropriate number pseudo-classes. matter scale: scale application
versus scale available computing. Excluding telecommunications application,
none cited applications takes 15 minutes cpu time SS-20 sin399

fiWeiss & Indurkhya
gle pseudo-classification problem full cross-validation.7 computing power increases
timing distinction less important. Even small percentage gain quite valuable appropriate application (Apte, Damerau, & Weiss, 1994) computational
requirements secondary factor.
provided results several real-world datasets. Mostly, involve nonlinear relationships. One may wonder rule-based method would perform data
obvious linear relationships. earlier experiments data exhibiting linear
relationships (for example, drug study data (Efron, 1988)), rule-based solutions
slightly better trees. However, true test real-world data which, often involve
complex non-linear relationships. Comparisons alternative models help assess
effectiveness new techniques.
Looking Figure 7 Tables 2 3, see pure rule-based solutions
competitive models. Additional gains made rules used
obtaining function values directly, instead used find relevant cases
used compute function value. results experiments support
view strategy combining different methods improve predictive performance.
Strategies similar applied classification problems (Ting, 1994;
Widmer, 1993) similar conclusions drawn results. results indicate
strategy useful regression context too. empirical results support
contention regression, partitioning methods nearest neighbor methods
complementary. solution found partitioning alone, incremental
improvement observed substituting average k-nearest neighbors
median partition. perspective nearest neighbor regression methods,
sample cases compartmentalized, simplifying table lookup new case.
conclusive, hints combination strategy effective
small moderate samples: likely sample size grows large, increased
numbers partitions, terms rules terminal nodes, compensate single
constant-valued regions. conjecture supported large-sample pole application,
incremental gain addition k-nn small.8
experiments used k-nn k=5. Depending application, different
value k might produce better results. optimal value might estimated crossvalidation strategy systematically varies k picks value gives best
results overall. However, unclear whether increased computational effort result
significant performance gain.
Another practical issue large samples storage requirement: cases must
stored. serious drawback real-world applications limited memory.
However, tried experiments cases associated partition replaced
fewer number \typical cases". results considerable savings terms storage
requirements. Results slightly weaker (though significantly different).
would appear gains might obtained restricting k-nn consider
features appear path leaf node examination. might
seem good idea attempts ensure features relevant
7. 10-fold cross-validation requires solving problem essentially 11 times: training cases
10 times group test cases.
8. Although small, difference tests significant sample large.

400

fiRule-based Functional Prediction
cases node, used distance calculations. However, found results
weaker.
number regression techniques presented others demonstrate
advantages combined models. combine methods independently
invoked. Instead typical election one winner, alternative models
combined weighted. combination techniques advantage
outputs different models treated independent variables. combined
form post-processing, model outputs available.
way contradict value alternative combination techniques.
approaches show improved results various applications. conclude, however,
advantages complex regression procedures dynamically mix
alternative models. procedures may particularly strong fundamental rationale choice methods partitioning methods, properties
combined models must preserved.
presented regression problem one output variable. classical form linear models regression trees. issue multiple outputs
directly addressed although extensions feasible. issue experimentation await future work. model regression provide basis efforts,
leveraging current strong methods classification rule induction.

References

Apte, C., Damerau, F., & Weiss, S. (1994). Automated Learning Decison Rules Text
Categorization. ACM Transactions Oce Information Systems, 12 (3), 233{251.
Breiman, L. (1993). Stacked regression. Tech. rep., U. CA. Berkeley.
Breiman, L., Friedman, J., Olshen, R., & Stone, C. (1984). Classification Regression
Tress. Wadsworth, Monterrey, Ca.
Clark, P., & Niblett, T. (1989). CN2 induction algorithm. Machine Learning, 3,
261{283.
Craven, P., & Wahba, G. (1979). Smoothing noisy data spline functions. estimating
correct degree smoothing method generalized cross-validation. Numer.
Math., 31, 317{403.
Efron, B. (1988). Computer-intensive methods statistical regression. SIAM Review,
30 (3), 421{449.
Fayyad, U., & Irani, K. (1992). attribute selection problem decision tree generation.
Proceedings AAAI-92, pp. 104{110 San Jose.
Friedman, J. (1991). Multivariate adaptive regression splines. Annals Statistics, 19 (1),
1{141.
Friedman, J., & Stuetzle, W. (1981). Projection pursuit regression. J. Amer. Stat. Assoc.,
76, 817{823.
401

fiWeiss & Indurkhya
Girosi, F., & Poggio, T. (1990). Networks best approximation property. Biological
Cybernetics, 63, 169{176.
Hartigan, J., & Wong, M. (1979). k-means clustering algorithm, ALGORITHM 136.
Applied Statistics, 28 (1).
Hastie, T., & Tibshirani, R. (1990). Generalized Additive Models. Chapman Hall.
Jacoby, S., Kowalik, J., & Pizzo, J. (1972). Iterative Methods Non-linear Optimization
Problems. Prentice-Hall, New Jersey.
Kirpatrick, S., Gelatt, C., & Vecchi, M. (1983). Optimization simulated annealing.
Science, 220, 671.
LeBlanc, M., & Tibshirani, R. (1993). Combining estimates regression classification.
Tech. rep., Department Statistics, U. Toronto.
Lebowitz, M. (1985). Categorizing numeric information generalization. Cognitive Science, 9, 285{308.
Lin, S., & Kernighan, B. (1973). ecient heuristic traveling salesman problem.
Operations Research, 21 (2), 498{516.
McClelland, J., & Rumelhart, D. (1988). Explorations Parallel Distributed Processing.
MIT Press, Cambridge, Ma.
Michalski, R., Mozetic, I., Hong, J., & Lavrac, N. (1986). multi-purpose incremental learning system AQ15 testing application three medical domains.
Proceedings AAAI-86, pp. 1041{1045 Philadelphia, Pa.
Quinlan, J. (1986). Induction decision trees. Machine Learning, 1, 81{106.
Quinlan, J. (1987). Simplifying decision trees. International Journal Man-Machine
Studies, 27, 221{234.
Quinlan, J. (1993). Combining instance-based model-based learning. International
Conference Machine Learning, pp. 236{243.
Ripley, B. (1993). Statistical aspects neural networks. Proceedings Seminair Europeen de Statistique London. Chapman Hall.
Scheffe, H. (1959). Analysis Variance. Wiley, New York.
Ting, K. (1994). problem small disjuncts: remedy decision trees. Proceedings
10th Canadian Conference Artificial Intelligence, pp. 91{97.
Weiss, S., & Indurkhya, N. (1993a). Optimized Rule Induction. IEEE Expert, 8 (6), 61{69.
Weiss, S., & Indurkhya, N. (1993b). Rule-based regression. Proceedings 13th
International Joint Conference Artificial Intelligence, pp. 1072{1078.
402

fiRule-based Functional Prediction
Weiss, S., & Indurkhya, N. (1994). Decision tree pruning: Biased optimal?. Proceedings
AAAI-94, pp. 626{632.
Weiss, S., & Kapouleas, I. (1989). empirical comparison pattern recognition, neural
nets, machine learning classification methods. International Joint Conference
Artificial Intelligence, pp. 781{787 Detroit, Michigan.
Weiss, S., & Kulikowski, C. (1991). Computer Systems Learn: Classification Prediction Methods Statistics, Neural Nets, Machine Learning, Expert Systems.
Morgan Kaufmann.
Widmer, G. (1993). Combining knowledge-based instance-based learning exploit
qualitative knowledge. Informatica, 17, 371{385.
Wolpert, D. (1992). Stacked generalization. Neural Networks, 5, 241{259.

403


