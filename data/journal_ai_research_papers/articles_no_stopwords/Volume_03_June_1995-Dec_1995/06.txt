Journal Artificial Intelligence Research 3 (1995) 431-465

Submitted 9/95; published 12/95

OPUS: Efficient Admissible Algorithm
Unordered Search
Geoffrey I. Webb

webb@deakin.edu.au

Deakin University, School Computing Mathematics
Geelong, Vic, 3217, Australia.

Abstract
OPUS branch bound search algorithm enables efficient admissible search
spaces order search operator application significant.
algorithms search efficiency demonstrated respect large machine learning
search spaces. use admissible search potential value machine learning
community means exact learning biases employed complex learning
tasks precisely specified manipulated. OPUS potential application
areas artificial intelligence, notably, truth maintenance.

1. Introduction
Many artificial intelligence problems involve search. Consequently, development
appropriate search algorithms central advancement field. Due complexity search spaces involved, heuristic search often employed. However, heuristic
algorithms cannot guarantee find targets seek. contrast, admissible search algorithm one guaranteed uncover nominated target,
exists (Nilsson, 1971). greater utility usually obtained significant computational
cost.
paper describes OPUS (Optimized Pruning Unordered Search) family
search algorithms. algorithms provide efficient admissible search search spaces
order application search operators significant. search efficiency
achieved use branch bound techniques employ domain specific pruning
rules provide tightly focused traversal search space.
algorithms wide applicability, within beyond scope
artificial intelligence, paper focuses application classification learning.
particular significance, demonstrated algorithms efficiently process many
common classification learning problems. contrasts seemingly widespread
assumption sizes search spaces involved machine learning require use
heuristic search.
use admissible search potential value machine learning enables better
experimental evaluation alternative learning biases. Search used machine learning
attempt uncover classifiers satisfy learning bias. heuristic search
used difficult determine whether search technique introduces additional implicit
biases cannot properly identified. implicit biases may confound experimental
results. contrast, admissible search employed experimenter assured
c
1995
AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiWebb

search technique introducing confounding unidentified implicit biases
experimental situation.
use OPUS admissible search already led developments machine
learning may otherwise possible. particular, Webb (1993) compared
classifiers developed true optimization Laplace accuracy estimate obtained heuristic search sought failed optimize measure. general,
latter proved higher predictive accuracy former. surprising result,
could obtained without use admissible search, led Quinlan
Cameron-Jones (1995) develop theory oversearching.
paper offers two distinct contributions fields computing, artificial intelligence machine learning. First, offers new efficient admissible search algorithm
unordered search. Second, demonstrates admissible search possible range
machine learning tasks previously thought susceptible efficient exploration
non-admissible heuristic search.

2. Unordered Search Spaces
search problems, order operators applied significant.
example, attempting stack blocks matters whether red block placed
blue block blue block placed green. attempting navigate
point point B, possible move point C point B moving
point C. However, search problems, order operators applied
significant. example, searching space logical expressions, effect
conjoining expression expression B conjoining result expression
C identical result obtained conjoining C followed B. sequences
operations result expressions equivalent meaning. general, search space
unordered sequence operator applications state S, states
reached permutation identical. type search problem,
search unordered search spaces, subject investigation.
Special cases search unordered search spaces provided subset
selection (Narendra & Fukunaga, 1977) minimum test-set (Moret & Shapiro, 1985)
search problems. Subset selection involves selection subset objects maximizes
evaluation criterion. minimum test-set problem involves selection set
tests maximizes evaluation criterion. search problems encountered many
domains including machine learning, truth maintenance pattern recognition. Rymon
(1992) demonstrated Reiters (1987) de Kleer, Mackworth, Reiters (1990)
approaches diagnosis recast subset selection problems.
OPUS algorithms traverse search space using search tree. root
search tree initial state. Branches denote application search operators
nodes lead denote states result application operators.
Different variants OPUS suited optimization search satisficing search.
optimization search, goal state optimal solution. satisficing search, goal
state acceptable solution. possible search space may include multiple goal
states.
432

fiAn Efficient Admissible Algorithm Unordered Search

OPUS algorithms take advantage properties unordered search spaces
optimize effect pruning search tree may occur. particular,
expanding node n search tree, OPUS algorithms seek identify search operators
excluded consideration search tree descending n without
excluding sole goal node search tree. OPUS algorithms differ
previous admissible search algorithms employed machine learning (Clearwater & Provost,
1990; Murphy & Pazzani, 1994; Rymon, 1992; Segal & Etzioni, 1994; Webb, 1990)
operators identified, removed consideration branches
search tree descend current node. contrast, algorithms
remove single branch time without altering operators considered sibling
branches, thereby pruning fewer nodes search space.
possible apply operator path search
space, search unordered operators considered subset selection problem
select subset operators whose application (in order) leads goal state. single
operator may applied multiple times single path search space, search
unordered operators considered sub-multiset selection problemselect
multiset operators whose application leads desired result.
search tree traverses unordered search space multiple applications
single operator allowed may envisioned Figure 1. example includes
four search operators, named a, b, c d. node search tree labeled
set operators reached. Thus, initial state labeled empty
set. depth one sets containing single operator, depth two sets containing
two operators on, depth four. two nodes identical labels represent
equivalent states.
considerable duplication nodes search tree (the label {a, b, c, d} occurs
24 times). Figure 1 (and following figures), number unique nodes listed
depth search tree. number derived number
combinations considered, derivation indicated.
common search prune regions search tree basis investigations determine goal state cannot lie within regions. Figure 2 shows
search tree sub-tree {c} pruned. Note that, due duplication inherent
search tree, number unique nodes remaining tree identical
unpruned tree. However, deemed node descending {c}
may goal, nodes elsewhere search tree identical labels (are
reached via identical sets operator applications) nodes occur pruned
region tree could pruned. Figure 3 shows search tree remaining
nodes {c} duplicates deleted. seen number
unique nodes remaining search tree (the tree depths 2, 3 4) pruned
half. Similar results obtained case multiple applications
single operator allowed nodes consequently labeled multisets.
OPUS algorithms provide pruning rulesmechanisms identifying sections
search tree may pruned. Rather, take pruning rules input seek
optimize effect pruning action results application rules.
OPUS algorithms designed use admissible pruning rules.
used solely admissible pruning rules algorithms admissible. is,
433

fiWebb

{ a, b }

{a}

{ a, c }

{ a, }

{ a, b }

{b}

{ b, c }

{ b,d }

{ a, b, c }

{ a, b, c, }

{ a, b, }

{ a, b, c, }

{ a, b, c }

{ a, b, c, }

{ a, c, }

{ a, b, c, }

{ a, b, }

{ a, b, c, }

{ a, c, }

{ a, b, c, }

{ a, b, c }

{ a, b, c, }

{ a, b, }

{ a, b, c, }

{ a, b, c }

{ a, b, c, }

{ b, c, }

{ a, b, c, }

{ a, b, }

{ a, b, c, }

{ c, b, }

{ a, b, c, }

{ a, b, c }

{ a, b, c, }

{ a, c, }

{ a, b, c, }

{ a, b, c }

{ a, b, c, }

{ b, c, }

{ a, b, c, }

{ a, c, }

{ a, b, c, }

{}
{ a, c }

{c}

{ b, c }

{ c, }

{ a, }

{d}

{ b, }

{ c, }
4C =1
0

4C =4
1

4C =6
2

{ b, c, }

{ a, b, c, }

{ a, b, }

{ a, b, c, }

{ a, c, }

{ a, b, c, }

{ a, b, }

{ a, b, c, }

{ b, c, }

{ a, b, c, }

{ a, c, }

{ a, b, c, }

{ b, c, }

{ a, b, c, }

4C =4
3

4C =1
4

Figure 1: Simple unordered operator search tree
guaranteed find goal state one exists search space. However, algorithms
may used non-admissible pruning heuristics obtain efficient non-admissible
search.
OPUS algorithms admissible (when used admissible pruning
rules), systematic (Pearl, 1984). is, addition guaranteeing goal
found one exists, algorithms guarantee state visited
search (so long possible reach single node application
different sets operators).

3. Fixed-order Search
number recent machine learning algorithms performed restricted admissible search
(Clearwater & Provost, 1990; Rymon, 1993; Schlimmer, 1993; Segal & Etzioni, 1994; Webb,
1990). algorithms based organization search tree, that,
considering search problem illustrated Figures 1 3, traverse search space
manner depicted Figure 4. organization achieved arranging operators
predefined order, applying node operators higher
order operator appears path leading node. strategy
434

fiAn Efficient Admissible Algorithm Unordered Search

{ a, b }

{ a}

{ a, c }

{ a, }

{ a, b }

{ b}

{ b, c }

{ b, }

{ a, b , c }

{ a, b , c, }

{ a, b , }

{ a, b , c, }

{ a, b , c }

{ a, b , c, }

{ a, c, }

{ a, b , c, }

{ a, b , }

{ a, b , c, }

{ a, c, }

{ a, b , c, }

{ a, b , c }

{ a, b , c, }

{ a, b , }

{ a, b , c, }

{ a, b , c }

{ a, b , c, }

{ b, c, }

{ a, b , c, }

{ a, b , }

{ a, b , c, }

{ c, b, }

{ a, b , c, }

{ a, b , }

{ a, b , c, }

{ a, c, }

{ a, b , c, }

{ a, b , }

{ a, b , c, }

{ b, c, }

{ a, b , c, }

{ a, c, }

{ a, b , c, }

{ b, c, }

{ a, b , c, }

{}

{c}

{ a, }

{ d}

{ b, }

{ c, }
4C =1
0

4C =4
1

4C =6
2

4C =4
3

4C =1
4

Figure 2: Simple unordered operator search tree pruning beyond application operator c

called fixed-order search. (Fixed-order search used non-admissible search,
example, Buchanan, Feigenbaum, & Lederberg, 1971).
Figure 5 illustrates effect pruning sub-tree descending operator c,
fixed-order search. seen, substantially less effective optimized
pruning illustrated Figure 3. Schlimmer (1993) ensures pruning effect illustrated
Figure 3 obtained within efficient search tree organization illustrated Figure 4,
maintaining explicit representation nodes pruned. resulting search tree
depicted Figure 6. approach requires considerable computational overhead
identifying marking pruned states following every pruning action, restrictive
storage overhead maintaining representation. (One search problems tackled
contains 2162 states. represent whether state pruned requires single bit.
Thus, 2162 bits would required represent required information problem,
requirement well beyond capacity computational machinery foreseeable
future.) Further, open debate whether approach truly prune identified
nodes search space. Nodes pruned still need
generated encountered previously unexplored regions search tree order
435

fiWebb

{ a, b }

{ a, b , }

{ a}
{ a, b , }
{ a, }

{ a, b }

{ a, b , }

{ b}
{ a, b , }
{ b, }
{}

{c}

{ a, b , }
{ a, }
{ a, b , }
{ d}

4C =1
0

4C =4
1

{ b, }

3C =3
2

3C =1
3

3C =0
4

Figure 3: Simple unordered operator search tree maximal pruning beyond application
operator c

{ a, b }

{ a, b, c }

{ a, b , c, }

{ a, b, }
{ a}

{ a, c }

{ a, c, }

{ a, }
{ b, c }
{}

{ b, c, }

{ b}
{ b ,d }
{c}

{ c, }

{ d}
4C =1
0

4 C =4
1

4C =6
2

4C =4
3

4C =1
4

Figure 4: Static search tree organization used fixed-order search

436

fiAn Efficient Admissible Algorithm Unordered Search

{ a, b, c }
{ a, b }
{ a}

{ a, c }

{ a, b , c, }

{ a, b, }
{ a, c, }

{ a, }
{}

{ b}

{ b, c }

{ b, c, }

{ b ,d }
{c}
{ d}

4C =1
0

4 C =4
1

4C =4
3

5

4C =1
4

Figure 5: Effect pruning fixed-order search
{ a, b }

{ a, b, }

{a}
{ a, }
{}

{b}

{ b,d }

{c}
{d}
4C =1
0

4 C =4
1

3 C =3
2

3 C =1
3

3C =0
4

Figure 6: Optimal pruning fixed-order search
checked list pruned nodes. Consider, example, node labeled {a}
Figure 5. expanding node necessary generate node labeled {a,
c}, even node marked pruned. generated possible
identify node pruned. node could principle pruned
anyway application variant technique identified prunable
first place. Viewed light, argued Schlimmers (1993) approach
reduce number nodes must generated fixed-order search.
saves computational cost determining nodes whether
require pruning not. (This assumes optimistic pruning mechanism able
determine node n search space pruned node n
pruned, irrespective n encountered search tree. optimistic
pruning mechanism deficient cannot this, Schlimmers (1993) approach
increase amount true pruning performed extent overcomes
deficiency.)

4. Feature Subset Selection Algorithm
Fixed-order search traverses search space naive mannerthe topology search
tree determined advance takes account efficiency resulting search.
contrast, Feature Subset Selection (FSS) algorithm (Narendra & Fukunaga, 1977)
performs branch bound search unordered search spaces, traversing search space
437

fiWebb

{c}

{ ,b }

{ a,b, }

{ a}

{}

{ ,d }
{ b}

{ b, }

{ d}
4C =1
0

4 C =4
1

4C =6
2

4C =4
3

4C =1
4

Figure 7: Pruning FSS-like search
visit state dynamically organizing search tree
maximize proportion search space placed unpromising operators.
viewed form fixed-order search order altered node
search tree manipulate topology search tree sake search
efficiency. Unlike Schlimmer (1993), pruning mechanism ensures nodes
identified prunable generated.
power measure illustrated Figure 7. figure, fixed-order search
performed simple example problem illustrated Figures 1 6, order
changed operator pruned, c, placed first. seen, achieves
amount pruning achieved optimal pruning. effect achieved
negligible computational storage overhead.
However, FSS severely limited applicability
restricted optimization search;
restricted tasks operator may applied (subset
selection);
restricted search single solution;
requires values states search space monotonically decreasing.
is, value state cannot increase result operator application;

form pruning supports optimistic pruning.

5. OPUS Algorithms
OPUS algorithms generalize idea search space reorganization FSS. Two
variations OPUS defined. OPUSs variant satisficing search (search
qualified object sought). OPUSo variant optimization search (search
object optimizes evaluation function sought). Whereas FSS uses node values
pruning, OPUSo uses optimistic evaluation search space node.
removes requirement values states search space monotonically
decreasing opens possibility performing types pruning addition
optimistic pruning.
438

fiAn Efficient Admissible Algorithm Unordered Search

analysis follow, comments apply equally variants name
OPUS employed. comment applies one variant algorithm,
distinguished respective superscript.
OPUS uses branch bound (Lawler & Wood, 1966) search strategy traverses
search space manner similar illustrated Figure 4 guarantee
two equivalent nodes search space visited. However, organizes
search tree optimize effect pruning, achieving effect illustrated
Figure 6 without significant computational storage overhead.
Rather maintaining operator order, OPUS maintains node, n, set
operators n.active applied search space n. node
expanded, operators n.active examined determine pruned.
operators pruned removed n.active. New nodes created
operators remaining n.active sets active operators initialized
ensure every combination operators considered one node
search tree.
kept mind possible many states search space may
goal states. satisficing search states satisfy given criteria goal states.
optimization search, states optimize evaluation criteria goal states.
efficiency sake, OPUS algorithms allow sections search space pruned even
contain goal state, long remain goal states remaining search
space.
5.1 OPUSs
OPUSs algorithm presented Figure 8. description OPUSs follows
conventions employed search algorithm descriptions provided Pearl (1984).
definition OPUSs assumes single operator cannot applied
along single path search space. operator may applied multiple
times, order Steps 8a 8b reversed. Unless otherwise specified,
following discussion OPUS assumes operator may applied
along single path.
desired obtain solutions satisfy search criterion,
Step 2 altered exit successfully, returning set solutions;
Step 6b altered exit, rather add current node set
solutions;
domain specific pruning mechanisms employed Step 7 modified
goal state may pruned search space.
form search could used assumption-based truth maintenance system find
set maximally general consistent assumptions. would provide efficient search
without need maintain search explicit database inconsistent assumptions
ATMS no-good set (de Kleer et al., 1990). Unless otherwise specified,
discussion OPUS assumes single solution sought.
algorithm specify order nodes selected expansion
Step 3. Nodes may selected random, domain specific selection function,
439

fiWebb

Data structure:
node, n, search tree associated three items information:
n.state state search space associated node;
n.active set operators explored sub-tree descending node;
n.mostRecentOperator operator applied parent nodes state create current nodes state.
Algorithm:
1. Initialize list called OP EN unexpanded nodes follows,
(a) Set OP EN contain one node, start node s.
(b) Set s.active set operators, {o1 , o2 , ...on }
(c) Set s.state start state.
2. OP EN empty, exit failure; goal state exists.
3. Remove OP EN node n, next node expanded.
4. Initialize n.active set containing operators yet examined, called
RemainingOperators.
5. Initialize {} set nodes, called N ewN odes, contain descendants n
pruned.
6. Generate children n performing following steps every operator n.active,
(a) Generate n0 , node n0 .state set state formed application
n.state.
(b) n0 .state goal state, exit successfully goal represented n0 .
(c) Set n0 .mostRecentOperator o.
(d) Add n0 N ewN odes.
7. node n0 N ewN odes pruning rules determine sole remaining goal state accessible n0 using operators RemainingOperators, prune
nodes search tree n0 search tree n follows,
(a) Remove n0 N ewN odes.
(b) Remove n0 .mostRecentOperator RemainingOperators.
8. Allocate remaining operators remaining nodes processing node n0
N ewN odes turn follows,
(a) Remove n0 .mostRecentOperator RemainingOperators.
(b) Set n0 .active RemainingOperators.
9. Add nodes N ewN odes OP EN .
10. Go Step 2.

Figure 8: OPUSs Algorithm
440

fiAn Efficient Admissible Algorithm Unordered Search

order nodes placed OP EN . First-in-first-out node selection results
breadth-first search last-in-first-out node selection results depth-first search.
order processing unspecified Steps 7, 8 9. Depending upon
domain, practical advantage may obtained specific orderings steps.
OPUSs used machine learning context search space generalizations may formed deletion conjuncts highly specific classification
rule. goal search uncover set general rules cover
identical objects training data covered original rule (Webb, 1994a).
5.2 OPUSo
number changes warranted OPUS applied optimization search.
following definition OPUSo, variant OPUS optimization search, assumes
two domain specific functions available. first functions, value(n), returns
value state node n, higher value returned, higher
preference state1 . second function, optimisticV alue(n, o) returns value
exists node, b, created application combination
operators set operators state node n, b represents best solution
(maximizes value search space), optimisticV alue(n, o) less value(b).
used pruning sections search tree. general, lower values returned
optimisticV alue, greater efficiency pruning. time, possible prune
node optimistic value less equal best value node
explored date.
OPUSo able take advantage presence optimistic values optimize
effect pruning beyond obtained solely maximizing proportion
search space placed nodes immediately pruned. Generalizing heuristic
used FSS, nodes lower optimistic values given active operators thus
greater proportions search space placed beneath nodes higher
optimistic values. achieved order processing Step 9. rationale
strategy lower optimistic value higher probability
node associated search tree pruned expanded. Maximizing
proportion search space located nodes low optimistic value maximizes
proportion search space pruned thus explicitly explored.
Figure 9 illustrates effect respect simple machine learning tasksearch
propositional expression describes target examples non-target
examples. seven search operators represent conjunction specific proposition
male, f emale, single, married, young, mid old, respectively. Search starts
expression anything. total 128 expressions may formed conjunction
combination expressions. Twelve objects defined:
male, single, young, TARGET
male, single, mid, TARGET
male, single, old, TARGET
male, married, young, NON-TARGET
1. ease exposition, assumed optimization means maximization value. would
trivial transform algorithm discussion accommodate forms optimization.

441

fiWebb

male, married, mid, NON-TARGET
male, married, old, NON-TARGET
female, single, young, NON-TARGET
female, single, mid, NON-TARGET
female, single, old, NON-TARGET
female, married, young, NON-TARGET
female, married, mid, NON-TARGET
female, married, old, NON-TARGET.
objects, first three distinguished targets. value expression
determined two functions, negCover posCover. negCover expression
number non-target objects matches. posCover expression
number target objects matches. expression anything matches objects.
value expression negCover equal zero. Otherwise value equals
posCover. preference function avoids expressions cover negative objects
favors, expressions cover negative objects, expressions cover
positive objects. optimistic value node equals posCover nodes
expression.
Figure 9 depicts nine nodes considered OPUSo search task.
node following listed:
expression;
number target number non target objects matched (cover);
value;
potential value;
operators placed nodes set active operators hence included
search tree node.
search space traversed follows. first node, anything, expanded, producing seven children values optimistic values determined. node
pruned potential values greater best value far encountered.
active operators distributed, maximizing proportion search space
placed nodes low optimistic values. two nodes highest optimistic
values, male single, one receives active operator receives first
sole active operator. One expanded. one active
operators, single, nodes generated. other, male, expanded, generating single node, male single, value 3. Immediately node generated,
remaining open nodes pruned none optimistic value greater
new maximum value, 3.
Note nodes pruned node malesingle considered as,
point, node encountered lower optimistic value best actual
value. Consequently, search tree distributed accord potential value,
set active operators node male would {f emale, single, married, young, mid, old}.
Instead considering single node male expanded, would necessary
442

fiAn Efficient Admissible Algorithm Unordered Search

male
Cover=3/3
Value=infinity
PotVal=3
AO={single}

male single
Cover=3/0
Value=3
PotVal=3
AO={}

female
Cover=0/6
Value=infinity
PotVal=0
AO={male, married,
single, young, mid old }
single
Cover=3/3
Value=infinity
PotVal=3
AO={}
anything
Cover=3/9
Value=infinity
PotVal=3
AO={male, female,
married, single,
young, mid, old}

married
Cover=0/6
Value=infinity
PotVal=0
AO={male, single, young,
mid, old }
young
Cover=2/2
Value=infinity
PotVal=2
AO={male, single,
mid, old}
mid
Cover=2/2
Value=infinity
PotVal=2
AO={male, single, old}
old
Cover=2/2
Value=infinity
PotVal=2
AO={male, single}

Figure 9: Effect pruning search tree ordered optimistic value

443

fiWebb

consider six nodes. search space complex continued depth three
beyond, would commensurate increase proportion search space
explored unnecessarily.
Note search example terminate goal node first
encountered, system cannot determine goal node nodes
might higher values explored pruned.
5.2.1 OPUSo Algorithm
OPUSo, algorithm achieving effect, defined Figure 10. Note
optimistic pruning need performed Step 8 performed Step 10a,
irrespective.
definition OPUSo assumes single operator cannot applied
along single path search space. allow multiple applications single
operator, order Steps 9a 9b reversed.
algorithm could modified identify return maximal solutions
modification similar outlined allow OPUSs return solutions.
possible improve performance OPUSo lower limit
acceptable solution. Then, objective search find highest valued node
long value greater pre-specified minimum. case, nodes whose
potential value less equal minimum may pruned Step 10a.
OPUSs, OPUSo specify order nodes OPEN
expanded (Step 4). Selection node highest optimistic value minimize
size search tree. single node n optimizes optimistic value,
search cannot terminate n expanded. node lower
optimistic value may yield solution value higher optimistic value n.
However, expansion n may yield solution value higher candidates optimistic values, allowing candidates discarded without expansion.
Thus, selecting single node highest optimistic value optimal respect
number nodes expanded maximizes number nodes may pruned
without expansion. multiple nodes maximize optimistic value, least one
must expanded search terminate (and search
terminate expansion node leads node value equal optimistic
value.)
many cases important consider number nodes explored
algorithm, rather number nodes expanded. node explored evaluated.
Every time node expanded, children explored. Many children
may pruned, however, never expanded. addition minimizing number
nodes expanded, form best-first search minimize number nodes
explored (within constraint nodes equal optimistic values
possible anticipate one select order minimize number nodes explored).
due strategy algorithm employs distribute operators beneath nodes.
nodes OPUSo expands best-first search highest optimistic
value. OPUSo always allocates fewer active operators node higher optimistic value
node lower optimistic value. number nodes examined node
444

fiAn Efficient Admissible Algorithm Unordered Search

Algorithm:
1. Initialize list called OP EN unexpanded nodes follows,
(a) Set OP EN contain one node, start node s.
(b) Set s.active set operators, {o1 , o2 , ...on }
(c) Set s.state start state.
2. Initialize BEST , best node examined far, s.
3. OP EN empty, exit successfully solution represented BEST .
4. Remove OP EN node n, next node expanded.
5. Initialize n.active set containing operators yet examined, called
RemainingOperators.
6. Initialize {} set nodes, called N ewN odes, contain descendants n
pruned.
7. Generate children n performing following steps every operator n.active,
(a) Generate n0 , node n0 .state set state formed application
n.state.
(b) value(n0 ) greater value(BEST )
i. Set BEST n0 .
ii. Prune search tree removing OP EN nodes x
optimisticV alue(x, x.active) less value(BEST ).
(c) Add n0 N ewN odes.
(d) Set n0 .mostRecentOperator o.
8. node n0 N ewN odes pruning rules determine sole remaining goal state accessible n0 using operators RemainingOperators, prune
nodes search tree n0 search tree n follows,
(a) Remove n0 N ewN odes.
(b) Remove n0 .mostRecentOperator RemainingOperators.
9. Allocate remaining operators remaining nodes processing node n0
N ewN odes turn follows, time selecting previously unselected node minimizes optimisticV alue(n0 , RemainingOperators),
(a) Remove n0 .mostRecentOperator RemainingOperators.
(b) Set n0 .active RemainingOperators.
10. Perform optimistic pruning adding remaining nodes OP EN processing
node n0 N ewN odes turn follows,
(a) optimisticV alue(n0 , n0 .active) greater value(BEST ),
i. Add n0 OP EN .
11. Go Step 3.

Figure 10: OPUSo algorithm

445

fiWebb

n expanded equals number active operators n. Hence, number nodes
examined nodes expanded minimized (within constraints use
information derived current state operators
active state).
However, best-first approach minimizes number nodes expanded, may
storage optimal due large potential storage overheads. storage overhead
concern, depth-first rather best-first traversal may employed, cost
potential increase number nodes must expanded. depth-first search
employed, nodes added OP EN order optimistic value Step 10.
ensure nodes open single depth expanded best-first manner,
benefits outlined above.
5.2.2 Relation Previous Search Algorithms
OPUSo viewed amalgamation FSS (Narendra & Fukunaga, 1977) A*
(Hart, Nilsson, & Raphael, 1968). FSS performs branch bound search unordered
search spaces, traversing search space visit state dynamically organizing search tree maximize proportion search space placed
unpromising operators. However, FSS requires values states search
space monotonically decreasing. is, value state cannot increase result
operator application. OPUSo generalizes FSS employing actual
values states optimistic evaluation nodes search tree, manner similar
A*. consequence, two minor constraints upon values states
optimistic values nodes search spaces OPUSo search.
requirements
least one goal state g node n, g lies node n search
tree, optimistic value n lower value g;
states maximal value goal states.
follows OPUSo wider applicability FSS.
OPUSo differs FSS integrating pruning mechanisms optimistic
pruning search process. facility crucial searching large search spaces
encountered machine learning.
innovation OPUS algorithms use restricted set operators
available node search tree enable focused pruning would otherwise
case. may circumstances would possible reach goal
state node n, application operators active
n. pruning rules able take account active operators provide pruning
contextpruning would otherwise possible. Similarly, set active
operators used calculate concise estimate optimistic value
would otherwise possible.
OPUSo differs A* manner dynamically organizes search tree
maximize proportion search space placed unpromising operators.
differs A* A* relies upon value node equivalent
446

fiAn Efficient Admissible Algorithm Unordered Search

sum costs operations lead node, whereas OPUS allows method
determining nodes value.
Rymon (1993) discusses dynamic organization search tree admissible search
unordered search spaces purpose altering topology data structure (SE-tree) produced. contrasts use dynamic organization search
tree OPUSo increase search efficiency.
5.3 OPUS Non-admissible Search
pointed above, although OPUS algorithms designed admissible
search, applied non-admissible pruning rules may used
non-admissible search. may useful efficient heuristic search required.
non-admissible heuristic search strategies embed heuristics search technique.
example, beam search relies upon use fixed maximum number alternative options
considered stage search. heuristic prune
n best solutions stage search. precise implications heuristic
particular search task may difficult evaluate. contrast, use OPUS
non-admissible pruning rules places non-admissible heuristic clearly defined rule
may manipulated suit circumstances particular search problem.
Another feature OPUSo stages available best solution encountered date search. means search terminated
time. terminated prematurely, current best solution would returned
understanding solution may optimal. algorithm employed
context, may desirable employ best-first search, opening nodes highest
actual (as opposed optimistic) value first, assumption lead
early investigation high valued nodes.
5.4 Complexity Efficiency Considerations
OPUS ensures state examined (unless identical states
formed different combinations operator applications), using similar search tree organization strategy fixed-order search. differs, however, instead placing
largest subsection search space highest ordered operator, second
largest subsection second highest ordered operator, on, whenever pruning
occurs, largest possible proportion search space placed pruned node,
hence immediately pruned.
n operators active node e expanded, search tree
including node contain every combination number operators (the
application none operators results e). Thus, search tree including
e contain 2n nodes. Exactly half these, 2n1 , label including single
operator o. OPUS ensures operator pruned node e expanded,
nodes containing removed search tree e never examined
(except, course, node reached single application must examined
order determine pruned). Thus, search tree node almost
exactly halved single operator pruned. subsequent operator pruned
node reduces remaining search tree proportion. Thus, size
447

fiWebb

remaining search tree divided almost exactly 2p , p number operators
pruned.
contrast, number nodes pruned fixed-order search depends upon
ranking operator within fixed operator ranking scheme. highest
ranked operator proportion search tree pruned OPUS.
general, operator pruned, nodes whose labels include operator
combination exclusively lower ranked operators pruned. effect illustrated
Figure 5 pruning {c} removes {c, d} search tree. Thus,
2nr 1, nodes immediately pruned search tree, n number
operators active node expanded r ranking within operators
operator pruned, highest rank 1. contrasts 2n1 1
nodes pruned OPUS.
However, difference number nodes explored two strategies
quite great analysis might suggest, (assuming availability reasonable
optimistic pruning mechanism) fixed-order search prune operator every time
examined deeper search tree combination higher ranked operators.
Thus, Figure 5, eventually examined, pruning would occur nodes
{a, b, c}, {a, c} {b, c}. Thus, {a, b, c, d}, {a, c, d} {b, c, d} would pruned
search tree. words, fixed-order search, operator pruned
considered combination lower ranked operator, considered
every combination number higher ranked operators. 2r1 combinations
higher ranked operator. follows fixed-order search considers many nodes
OPUS single operator pruned. Thus, operator pruned
node n, OPUS explores 2r1 less nodes n fixed-order search.
rank order operators pruned tend grow number operators
grows, follows that, average case, advantage accrued use OPUS
grow exponentially number operators grows. OPUS tend greatest
relative advantage largest search spaces.
Note OPUS always able guarantee maximal possible pruning
occurs result single pruning action. example, OPUS used
search space subsets set items, determined superset
set node may solution, items active current
node, supersets contain items active may explored elsewhere
search tree. algorithm could prune supersets could perform
pruning OPUS. might claimed Schlimmers (1993) search method
performs pruning, recalled prevent pruned nodes
generated elsewhere tree, rather, ensures nodes pruned
generated. OPUS, armed suitable pruning rules, able prune
nodes encountered. OPUS maximizes pruning performed within constraints
localized information access.
However, constraint provided active operators prevents OPUS
performing pruning, enables perform pruning would otherwise possible. necessary considering whether prune
node determine whether nodes reached active operators may contain
solution. Thus, continue example subset search, even supersets set
448

fiAn Efficient Admissible Algorithm Unordered Search

current node n potential solutions, still possible prune search tree
n supersets potential solutions contain items active
n. Schlimmers (1993) approach allow pruning context.
illustrate effect, let us revisit search space examined Figures 1 7. Even
though search space {c} pruned, optimal pruning cannot take
account optimistic evaluations nodes mechanism
information communicated optimistic evaluation function (other
actually exploring space node evaluated, defeats purpose
optimistic evaluation). example, evaluating optimistic value node {a},
optimistic evaluation function cannot return different value would case
{c} pruned. contrast, optimistic evaluation function employed
OPUSo take account taking active operators current node
consideration. optimistic evaluation function described Section 6.1 below.
often possible use information particular operators available
search tree node substantially improve quality optimistic evaluation
node.
noted algorithm employ backtracking guarantee minimize number nodes expanded depth-first search.
poor node chosen expansion depth-first search, system stuck
explore search space node return explore alternatives.
algorithm guarantee poor selection unless optimistic evaluation function
high enough accuracy prevent need backtracking. follows algorithm
requires backtracking guarantee minimize number nodes
expanded. Thus, OPUS heuristic respect minimizing computational complexity
depth-first search.
storage requirements OPUS depend upon whether depth, breadth bestfirst search employed. depth-first search employed, maximum storage requirement
less maximum depth search tree multiplied maximum branching factor. However, breadth best-first search employed, worst case, storage
requirement exponential. stage search, storage requirement
storing frontier nodes search. number frontier nodes cannot exceed
number leaf nodes complete search tree. search operator may
applied (subset selection), pruning, number leaf nodes
2n1 , n number operators. assertion justified follows.
order operators considered invariant, nodes reached via last operator considered leaf node. search admissible, last operator must
considered every combination operators. 2n1 , combinations
operators. order operators considered alter number
leaf nodes absence pruning. search limit number
applications single operator (sub-multiset selection) upper limit
potential storage requirements.
Irrespective storage requirements, worst case OPUS explore
every node search space. occur pruning possible search.
operators applied per solution, number nodes search space
equal 2n , n number operators. Thus, worst case computational
449

fiWebb

complexity OPUS exponential, irrespective whether depth, breadth best-first
search employed.
OPUS clearly inappropriate, terms computational and, using breadth
best-first search, storage requirements, search problems substantial proportions search space cannot pruned. domains substantial pruning
possible, however, average case complexity (computational and/or storage) may turn
polynomial. Experimental evidence indeed case machine
learning tasks presented Section 6.3.
5.5 Search Efficiency OPUS Might Improved
noted Section 5.4, OPUS algorithms always able guarantee
maximum possible amount pruning performed. noted, one restriction upon
amount pruning performed localization inherent use active operators.
localization allows pruning would otherwise possible,
potential restrict number supersets set operators pruned
node pruned. may value developing mechanisms enable
pruning propagated beyond node pruning action occurs
sub-tree node.
Another aspect algorithms positive negative aspects type
information returned pruning mechanisms. mechanisms allow pruning
branch search tree long least one goal branch.
contrasts alternative strategy pruning branches lead
goal. strategy used beneficial, maximizes amount pruning
performed. However, always possible branch containing goal could
found little exploration pruned favor branch containing goal
requires extensive exploration uncover. potential gain augmenting
current pruning mechanisms means estimating search cost uncovering
goal beneath branch tree.

6. Evaluating Effectiveness OPUS Algorithms
Theoretical analysis demonstrated OPUS explore fewer nodes fixed-order
search magnitude advantage increase size search
space increases. However, precise magnitude gain depend upon extent
distribution within search tree pruning actions. interest,
number distinct elements OPUS algorithms, includingoptimistic pruning;
pruning (pruning addition optimistic pruning); dynamic reorganization
search tree; maximization proportion search space placed nodes
low optimistic value. following experiments evaluate magnitude advantage
OPUS obtained real world search tasks explore relative contribution
distinct elements OPUS algorithms.
end, OPUSo applied class real search tasksfinding pure conjunctive
expressions maximize Laplace accuracy estimate respect training set
preclassified example objects. is, example, search task CN2 purports
heuristically approximate (Clark & Niblett, 1989) forming disjuncts disjunc450

fiAn Efficient Admissible Algorithm Unordered Search

tive classifier. Machine learning systems employed OPUSo manner develop
rules inclusion sets decision rules (Webb, 1993) decision lists (Webb,
1994b). (The current experiments performed using Cover learning system, which,
default, performs repeated search pure conjunctive classifiers within CN2-like covering algorithm develops disjunctive rules. extended search disjunctive
rules used experiments, makes difficult compare alternative search
algorithms. because, two alternative algorithms find different pure conjunctive
rules first disjunct, subsequent search explore different search spaces.)
Numerous efficient admissible search algorithms exist developing classifiers
consistent training set examples. two classic algorithms purpose
least generalization algorithm (Plotkin, 1970) version space algorithm (Mitchell,
1977). least generalization algorithm finds specialized class description
covers objects training set containing positive examples. version space algorithm finds class descriptions complete consistent respect training
set positive negative examples. Hirsh (1994) generalized version space
algorithm find class descriptions complete consistent within defined
bounds training examples. least generalization version space algorithms
usually require strong inductive bias class description language (restriction
types class descriptions considered) find useful class descriptions
(Mitchell, 1980). SE-tree-based learning (Rymon, 1993) demonstrates admissible search
set consistent class descriptions within complex class description languages
may usefully employed least generalization version space algorithms. Oblow
(1992) describes algorithm employs admissible search pure conjunctive terms
within heuristic outer search k-DNF class descriptions consistent
training set.
However, many learning tasks desirable consider class descriptions
inconsistent training set. One reason training set may contain
noise (examples inaccurate). Another reason may possible accurately describe target class available language expressing class descriptions.
case necessary consider approximations target class. reason
training set may contain insufficient information reliably determine exact
class description. case, best solution may approximation known
incorrect strong evidence level error low.
Clearwater Provost (1990) Segal Etzioni (1994) use admissible fixedorder search explore classifiers inconsistent training set. However,
admissible search Clearwater Provost (1990) computationally feasible large
search spaces. Segal Etzioni (1994) bound depth search space considered
order maintain computational tractability. Smyth Goodman (1992) use optimistic
pruning search optimal rules, structure search ensure states
searched multiple times. previous admissible search algorithm
employed machine learning find classifiers inconsistent training
set maximize arbitrary preference function. following experiments seek
demonstrate search feasible using OPUS.
allowed class description may inconsistent training set,
helpful employ explicit preference function. function applied
451

fiWebb

class description returns measure desirability. evaluation usually take
account well description fits training set may include bias toward
particular types class descriptions, example, preference syntactic simplicity.
preference function expresses inductive bias (Mitchell, 1980).
OPUSo may employed admissible search contexts, provided search space
defined may traversed finite number unordered search operators.
example, OPUSo may employed search class description language
pure-conjunctive descriptions examining search space starting general
possible class description true employing search operators, effect
conjoining specific clause current description. search may performed
arbitrary preference function, provided appropriate optimistic evaluation functions
defined.
next section describes experiments OPUSo applied manner.
6.1 Search Task
pure conjunctive expressions consisted conjunctions clauses form attribute 6=
value. attributes two values, language expressive
language allowing conjunctions clauses form attribute = value. Indeed,
equivalent expressiveness language supports internal disjunction. example, respect attribute values x, z, language restricted
conjunctions equality expressions cannot express 6= x, whereas language restricted
conjunctions inequality expressions express = x using expression 6= 6= z.
internal disjunctive (Michalski, 1984) terms, 6= x equivalent = z.
noted
attributes two values search space conjunctions inequality expressions far larger search space conjunctions equality
expressions. attribute, size search space multiplied 2n
former n + 1 latter, n number values attribute.
software employed experimentation used search smaller
search spaces equality expressions effects demonstrated
following experiments.
Search starts general expression, true. operator performs conjunction current expression term 6= v, attribute v
single value attribute.
Laplace (Clark & Boswell, 1991) preference function used determine goal
search. function provides conservative estimate predictive accuracy
class description, e. defined
value(e) =

posCover(e) + 1
posCover(e) + negCover(e) + noOf Classes

posCover(e) number positive objects covered e; negCover(e)
number negative objects covered e; noOf Classes number classes
learning task.
452

fiAn Efficient Admissible Algorithm Unordered Search

Laplace preference function trades-off accuracy generality. favors class
descriptions cover positive objects class descriptions cover fewer,
favors class descriptions lower proportion cover negative
higher. following study, Laplace preference function employed
pruning mechanism Step 10a OPUSo algorithm pruned sections
search space optimistic values less equal value class description
covered objects. solution value higher obtained
class description covered objects, rule developed class.
optimistic value function derived observation cover specializations expression must subsets cover expression. Thus, specializations
expression may cover positive objects, may cover fewer negative objects
covered original expression. Laplace preference function maximized positive cover maximized negative cover minimized, specialization
expression node may higher value obtained positive
cover expression smallest negative cover within sub-tree node.
smallest negative cover within sub-tree node n obtained expression
formed applying operators active n expression n.
pruning performed application cannotImprove(n1 , n2 ),
boolean function true two nodes n1 n2 search tree n2
either child sibling n1 specialization n2 may higher value
highest value search tree n1 inclusive excluding search tree
n2 . function may defined
cannotImprove(x, y) neg(x) neg(y) pos(x) pos(y)
neg(n) denotes set negative objects covered description node n
pos(n) denotes set positive objects covered description node n.
cannotImprove(n1 , n2 ) search n2 cannot lead higher valued result
obtained search specializations n1 excluding nodes search space
n2 . shown n1 parent n2 child node follows.
n1 parent n2 expression n2 must specialization expression
n1 operators available n2 must available n1 . expression g
specialization, s, neg(g) neg(s) neg(g) = neg(s) (as specialization
decrease cover). follows specialization n2 , n3 , obtained
applications operators O, must specialization n1 obtained application
operators O, n4 , generalization n3 identical negative cover
n3 . n4 generalization n3 , must cover positive objects covered n3 .
Therefore, n4 must equal greater positive cover equal negative cover n3
consequently must equal greater value. follows must possible
reach n1 node least great value greatest valued node n2
without applying operator led n1 n2 .
Next consider case n1 n2 siblings. follows definition
cannotImprove neg(n1 ) neg(n2 ) pos(n1 ) pos(n2 ). Let operators o1
o2 led parent node p n1 n2 , respectively. follows
o2 cannot exclude negative objects expressions p excluded o1
o1 cannot exclude positive objects expressions p excluded
453

fiWebb

Table 1: Summary experimental data sets

Domain
Audiology
House Votes 84
Lenses
Lymphography
Monk 1
Monk 2
Monk 3
Multiplexor (F11)
Mushroom
Primary Tumor
Slovenian Breast Cancer
Soybean Large
Tic Tac Toe
Wisconsin Breast Cancer

Description
Medical diagnosis
Predict US Senators political
affiliation voting record.
Spectacle lens prescription.
Medical diagnosis.
Artificial data.
Artificial data.
Artificial data.
Artificial data, requiring disjunctive concept description.
Identify poison mushrooms.
Medical diagnosis.
Medical prognosis.
Botanical diagnosis.
Identify lost positions.
Medical diagnosis.

#
Values
162
48

#
Objects
226
435

#
Classes
24
2

9
60
17
17
17
22

24
148
556
601
554
500

3
4
2
2
2
2

126
42
57
135
27
91

8124
339
286
307
958
699

2
22
2
19
2
2

o2 . Therefore, application o2 n1 effect negative cover
expression may reduce positive cover. expression e reached n2
sequence operator applications O, application n1 cannot result expression
lower positive higher negative cover e.
cannotImprove function employed prune nodes Step 8 OPUSo
algorithm.
6.2 Experimental Method
search performed fourteen data sets UCI repository machine learning
databases (Murphy & Aha, 1993). data sets repository
researcher could time experiments identify capable readily expressed
categorical attribute-value learning tasks. fourteen data sets described
Table 1. number attribute values (presented column 3) treats missing values
distinct values. space class descriptions OPUS considers domain (and
hence size search space examined pure conjunctive rule developed)
2n , n number attribute values. Thus, Audiology domain,
class description developed, search space size 2162 . Columns 4 5 present
number objects number classes represented data set, respectively.
search repeated class data set. search,
objects belonging class question treated positive objects
objects data set treated negative objects. search performed using
454

fiAn Efficient Admissible Algorithm Unordered Search

following search methodsOPUSo; OPUSo without optimistic pruning; OPUSo
without pruning; OPUSo without optimistic reordering; fixed-order search,
performed Clearwater Provost (1990), Rymon (1993), Schlimmer (1993), Segal
Etzioni (1994) Webb (1990).
Optimistic pruning disabled removing condition Step 10a OPUSo
algorithm. words, Step 10(a)i always performed.
pruning disabled removing Step 8 OPUSo algorithm.
Optimistic reordering disabled changing Step 9 process node predetermined fixed-order, rather order optimistic value. treatment,
topology search tree organized fixed-order, operators pruned
node removed consideration entire subtree node.
Fixed-order search emulated disabling Step 8b disabling optimistic reordering, described above.
algorithms extent under-specified. OPUSo, optimistic pruning
pruning leave unspecified order operators leading nodes
equal optimistic values considered Step 9. ambiguities resolved
following experiments ordering operators leading nodes higher actual
values first. two operators tied optimistic actual values, operator
mentioned first names file describes data selected first.
optimistic reordering fixed-order search leave unspecified fixed-order
employed traversing search space. fixed-order search representative previous approaches unordered search employed machine learning, thus
important obtain realistic evaluation performance, ten alternative random
orders generated employed fixed-order search task. While, due
high variability performance different orderings, would desirable
explore ten alternative orderings, infeasible due tremendous
computational demands algorithm. comparison optimistic reordering
considered less crucial, used solely evaluate effectiveness one aspect
OPUSo algorithm, thus, due tremendous computational expense
algorithm, single fixed ordering used, employing order attribute values
mentioned names file.
algorithms leave unspecified order nodes equal optimistic
values selected OP EN best-first search, directly expanded
depth-first search. best first search nodes equal optimistic values removed
OP EN last-in-first-off order. depth-first search, nodes equal optimistic
value expanded order employed allocating operators Step 9.
Note fixed-order search OPUSo disabled optimistic reordering conditions used optimistic pruning. Note fixed-order search
ordered topology search tree manner depicted Figure 4, explored
tree either best depth-first manner.
6.3 Experimental Results
Tables 2 3 present number nodes examined search experiment.
data set total number nodes explored condition indicated.
455

fiWebb

Table 2: Number nodes explored best-first search

Data set
Audiology
House Votes 84
Lenses
Lymphography
Monk 1
Monk 2
Monk 3
Multiplexor (F11)
Mushroom
Primary Tumor
Slovenian B. C.
Soybean Large
Tic Tac Toe
Wisconsin B. C.

OPUSo
7,044
533
41
1,142
357
4,326
281
2,769
391
10,892
17,418
8,304
2,894
447,786


optimistic
pruning

661
176
1,143
9,156
6,578
25,775
96,371
392
10,893
4,810,129
8338
4,222,641




pruning
24,199
554
41
1,684
371
4,335
281
2,769
788
13,137
32,965
21,418
2,902
1,159,011


optimistic
reordering

355,040
38
658,335
925
10,012
682
4,932
233,579
4,242,978

21,551,436
16,559


Fixed-order
(mean)

1,319,911
64
2,251,652
788
5,895
656
4,948

29,914,840
42,669,822

16,471


Execution terminated exceeding virtual memory limit 250 megabytes.
one ten runs completed.

fixed-order search, mean ten runs presented. Tables 4 5 present
fixed-order search number runs completed successfully, minimum number
nodes examined successful run, mean number nodes examined successful runs
(repeated Tables 2 3) standard deviations runs. Every node
generated Step 7a counted tally number nodes explored. hyphen
() indicates search could completed number open nodes made
system exceed predefined virtual memory limit 250 megabytes. asterisk (*)
indicates search terminated due exceeding pre-specified compute time
limit twenty-four CPU hours. (For comparison, longest CPU time taken data
set OPUSo sixty-seven CPU seconds Wisconsin Breast Cancer data
depth-first search.)
noted one pure conjunctive rule developed class.
separate search performed rule, number searches performed equals
number classes. Thus, Audiology data using best-first search OPUSo explored
7,044 nodes perform 24 admissible searches 2162 node search space.
two search tasks OPUSo best-first search explore nodes
alternative. Lenses data, OPUSo explores 41 nodes optimistic reordering
explores 38. Monk 2 data, OPUSo explores 4,326 nodes best ten fixedorder runs different random fixed orders explores 4,283 nodes. possible
outcomes arisen situations two sibling nodes share optimistic
value. case, two approaches select different nodes expand first, one
456

fiAn Efficient Admissible Algorithm Unordered Search

Table 3: Number nodes explored depth-first search

Data set
Audiology
House Votes 84
Lenses
Lymphography
Monk 1
Monk 2
Monk 3
Multiplexor (F11)
Mushroom
Primary Tumor
Slovenian B. C.
Soybean Large
Tic Tac Toe
Wisconsin B. C.

OPUSo
7,011
568
38
1,200
364
16,345
289
2,914
386
18,209
30,647
9,562
3,876
465,058


optimistic
pruning
*
17,067,302
513
39,063,303
54,218
85,425
63,057
188,120
*
34,325,234
172,073,241
*
11,496,736
*



pruning
17,191
596
38
1,825
378
16,427
289
2,914
761
23,668
61,391
23,860
4,010
1,211,211


optimistic
reordering
3,502,475
10,046
38
728,276
980
12,879
588
6,961
1,562,006
3,814,422
271,328,080
17,138,467
93,521
*

Fixed-order
(mean)
*
3,674,418
66
22,225,745
1,348
12,791
1,236
6,130
132,107,513
31,107,648
308,209,464
*
110,664
*

* Execution terminated exceeding 24 CPU hour limit.
three ten runs completed.

Table 4: Number nodes explored best-first fixed-order search
Data set
Audiology
House Votes 84
Lenses
Lymphography
Monk 1
Monk 2
Monk 3
Multiplexor (F11)
Mushroom
Primary Tumor
Slovenian B. C.
Soybean Large
Tic Tac Toe
Wisconsin B. C.

Runs
0
10
10
10
10
10
10
10
0
10
1
0
10
0

Minimum

451,038
51
597,842
463
4,283
527
4,210

10,552,129
42,669,822

8,046


Mean

sd


1,319,911
64
2,251,652
788
5,895
656
4,948

29,914,840
42,669,822

16,471



624,957
9
1,454,583
225
931
110
364

12,390,146
0

5,300


Execution terminated ten runs exceeding
virtual memory limit 250 megabytes.

457

fiWebb

Table 5: Number nodes explored depth-first fixed-order search
Data set
Audiology
House Votes 84
Lenses
Lymphography
Monk 1
Monk 2
Monk 3
Multiplexor (F11)
Mushroom
Primary Tumor
Slovenian B. C.
Soybean Large
Tic Tac Toe
Wisconsin B. C.

Runs
0
10
10
10
10
10
10
10
3
10
10
0
10
0

Minimum
*
1,592,391
50
484,694
553
9,274
627
4,467
105,859,320
10,458,421
110,101,761
*
49,328
*

Mean

sd

*
3,674,418
66
22,225,745
1,348
12,791
1,236
6,130
132,107,513
31,107,648
308,209,464
*
110,664
*

*
2,086,159
12
27,250,834
922
2,686
891
1,164
22,749,211
14,907,744
303,800,659
*
65,809
*

* Execution terminated ten runs exceeding
24 CPU hour limit.

may turn better choice other, leading exploration fewer
nodes. test plausibility explanation, OPUSo run Lenses
data set Step 8 altered ensure two siblings equal optimistic value
ordered order employed optimistic reordering.
resulted exploration 36 nodes, fewer alternative. OPUSo
fixed-order run fixed-order using order attribute declaration data
file determine operator order OPUSo using order order siblings
equal optimistic values, numbers nodes explored Monk 2 data 4,302
OPUSo 8,812 fixed-order search.
notable effect apparent small search spaces.
significant suggests effect small magnitude resulting
poor choice node expand two nodes equal optimistic value.
expected. Consider case two nodes n1 n2 equal highest
optimistic value, v, n1 leads goal whereas n2 not. n2 expanded first,
long child n2 optimistic value greater equal v, next node
expanded n1 , n1 node highest optimistic value. (If
child n2 optimistic value greater v, optimistic evaluation function cannot
good, fact n2 optimistic value v means node n2
value greater v.) Thus, number unnecessary node expansions due
effect never exceed number times nodes equal highest optimistic
values encountered search.
contrast case best-first search, discussed Section 5.4, OPUS
heuristic respect minimizing number nodes expanded depth-first search.
458

fiAn Efficient Admissible Algorithm Unordered Search

Nonetheless, one search task, Monk 2 data set, OPUSo explore
nodes depth-first search (16,345) alternative (both optimistic reordering
fixed-order search explore 12,879 12,791 nodes respectively). results
demonstrate heuristic optimal data. noted, however,
single exception depth-first search occurs relatively small search
space. suggests efficient exploration search space poor choice
node much minimize damage done poor choice, even
backtracking case depth-first search.
five data sets (House Votes 84, Lymphography, Mushroom, Primary Tumor
Soybean Large), disabling optimistic pruning little effect best-first search. Disabling optimistic pruning always large effect depth-first search. best-first
search smallest increase caused disabling optimistic pruning increase
one node Lymphography Mushroom data sets. data sets
possible complete search without optimistic pruning, biggest effect
almost 1,500 fold increase number nodes explored Tic Tac Toe
data. depth-first search, data sets processing could completed
without optimistic pruning, smallest increase five-fold increase Monk 2
data largest increase 30,000 fold increase Lymphography data.
seven data sets (House Votes 84, Lenses, Monk 1, Monk 2, Monk 3, F11 Multiplexor,
Tic Tac Toe) disabling pruning little effect best-first depth-first
search. largest effects 2.5 fold increases Soybean Large Wisconsin
Breast Cancer data sets best-first search Audiology, Soybean Large
Wisconsin Breast Cancer data sets depth-first search.
results apparent data sets
pruning technique little effect (so long employed),
data sets pruning halves amount search space explored
data sets optimistic pruning reduces amount search space explored
thousandths would otherwise explored.
effect optimistic reordering highly variable. two search tasks (bestfirst search Lenses data set depth-first search Monk 2 data set) use
actually resulted slight increase number nodes explored. discussed
above. many cases, however, effect disabling optimistic reordering far greater
disabling optimistic pruning. Processing could completed without
optimistic reordering three best-first search tasks one depth-first search
tasks. tasks search could completed, largest effect best-first
search 2,500 fold increase number nodes explored Soybean Large data.
depth-first search, tasks search could completed, largest
effect 8,000 fold increase Slovenian Beast Cancer data. would
desirable evaluate effect alternative fixed-orderings operators results,
seems optimistic reordering critical general success algorithm.
one data set (the Monk 2 data depth-first search), fixed-order search
average explores substantially nodes OPUSo. asserted Section 5.4
average case advantage use OPUSo opposed fixed-order search
tend grow exponentially number search operators increases. number
search operators search tasks equal number attribute values
459

fiLog Advantage

Webb

20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0
-1













best-first search
depth-first search





10 20 30 40 50 60 70 80 90 100 110 120 130 140
Search Space Size

Figure 11: Plot difference nodes explored fixed-order OPUSo search
search space size.

corresponding data sets. Analysis Tables 2 3 reveals relative advantage
OPUSo data sets fewest attribute values (Lenses, Monk 1, 2 3,
F11 Multiplexor) approximately two-fold reduction number nodes examined.
number attribute values increases, relative advantage. four
data sets greatest number attribute values (Audiology, Mushroom, Soybean
Large Wisconsin Breast Cancer) one case (depth-first search Mushroom
data) fixed-order search terminate. one case, OPUSo enjoys 350,000-fold
advantage. results lend credibility claim OPUSos average case advantage
fixed-order search exponential respect size search space.
illustrated Figure 11. figure, searches fixed-order search terminated
within resource constraints, size search space plotted log2 (f /o)
f number nodes explored fixed-order search number nodes
explored OPUSo.
seems clear results admissible fixed-order search practical
many search tasks within scope current technology.
interesting observe best-first search, four artificial data
sets (Monk 1, Monk 2, Monk 3 F11 Multiplexor) fixed-order search often explores
slightly fewer nodes OPUSo optimistic reordering disabled. difference two types search latter deletes pruned operators sets
active operators higher ordered operators whereas former not. Thus
latter prunes nodes search tree pruning operation. seems
460

fiAn Efficient Admissible Algorithm Unordered Search

counter-intuitive increased pruning sometimes lead exploration
nodes. understand effect necessary recall pruning prune
solutions search tree long alternative solutions available.
artificial data sets question, retaining alternative solutions search tree cases
leads slight increase search efficiency alternative encountered earlier
first solution. Despite minor advantage number artificial data sets
fixed-order search OPUSo optimistic reordering disabled, latter enjoys large
advantage data sets processing could completed. House
Votes 84 data, fixed-order search explores 3.5 times many nodes best-first
search 350 times many depth-first search.
seen reason believe average case number
nodes explored OPUSo polynomial respect search space size
machine learning search tasks. numbers nodes explored three largest
search spaces certainly suggestive exponential explosion numbers
nodes examined (Audiology2162 nodes search space: 7,044 7,011 nodes examined. Soybean Large2135 nodes search space: 8,304 9,562 nodes examined.
Mushroom2126 nodes search space: 391 386 nodes examined.)
interesting little difference number nodes explored OPUSo
using either best depth-first search data sets. Surprisingly, slightly fewer nodes
explored depth-first search three data sets (Audiology, Lenses Mushroom). similar reasons presented context occasional slight advantage enjoyed fixed-order search OPUSo optimistic reordering
disabled. cases depth-first search fortuitously encounters alternative solutions
found best-first search. evaluate plausibility explanation, OPUSo
run three data sets question using fixed-order ordering order operators
equal optimistic values. resulting numbers nodes explored Audiology:
6678, Lenses: 36 Mushroom: 385. seen, numbers cases lower
numbers nodes explored depth-first search. case OPUSo
outperformed best-first strategies, effect appears small magnitude
thus significant small numbers nodes need explored. four
data sets depth-first search explores substantially nodes best-first search
(Slovenian Breast Cancer, 75%; Monk 2, 275%; Primary Tumor, 67%; Tic Tac Toe,
33%).
6.4 Summary Experimental Results
experiments demonstrate admissible search pure conjunctive classifiers feasible using OPUSo types learning task contained UCI repository.
support theoretical findings OPUSo general explore fewer
nodes fixed-order search magnitude advantage tend grow
exponentially respect size search space.
Optimistic pruning pruning demonstrated individually provide
large decreases number nodes explored search spaces little
effect others. Optimistic reordering demonstrated large impact upon
number nodes explored.
461

fiWebb

results respect search largest search spaces suggest
average case complexity algorithm less exponential respect search
space size.

7. Summary Future Research
OPUS algorithms potential application many areas endeavor.
used replace admissible search algorithms unordered search spaces maintain
explicit lists pruned nodes, currently used ATMS (de Kleer, 1986).
may support admissible search number application domains, learning
classifiers inconsistent training set, previously tackled
heuristic search.
addition applications admissible search, OPUS algorithms may
used efficient non-admissible search application non-admissible pruning
rules. OPUSo algorithm able return solution prematurely terminated
time, although solution may non-optimal.
availability admissible search important step forward machine learning
research. studies paper employed OPUSo optimize Laplace
preference function, algorithm could used optimize learning bias. means
first time possible isolate effect explicit learning bias
implicit learning bias might introduced heuristic search algorithm
interaction explicit bias.
application OPUSo provide admissible search machine learning already
proved productive. Webb (1993) used OPUSo demonstrate heuristic search
fails optimize Laplace accuracy estimate within covering algorithm frequently
results inference better classifiers found admissible search optimize preference function. explain result Quinlan Cameron-Jones
(1995) developed theory oversearching.
research reported herein demonstrated OPUS provide efficient admissible search pure conjunctive classifiers categorical attribute-value data sets
UCI repository. would interesting see techniques extended
powerful machine learning paradigms continuous attribute-value first-order logic
domains.
research demonstrated power pruning. issue given
scant attention context search machine learning. Although presented
context admissible search, pruning rules presented equally applicable
heuristic search. development pruning rules may prove important
machine learning tackles ever complex search spaces.
OPUS provides efficient admissible search unordered search spaces. creating
machine learning system necessary consider search (the
explicit learning biases) search (appropriate search algorithms).
assumed previously algorithms must necessarily heuristic techniques
approximating desired explicit biases. Admissible search decouples two issues
removing confounding factors may introduced search algorithm.
guaranteeing search uncovers defined target, admissible search makes possible
462

fiAn Efficient Admissible Algorithm Unordered Search

systematically study explicit learning biases. supporting efficient admissible search,
OPUS first time brings machine learning ability clearly explicitly
manipulate precise inductive bias employed complex machine learning task.

Acknowledgements
research supported Australian Research Council. grateful
Riichiro Mizoguchi pointing potential application OPUS truth maintenance. grateful Mike Cameron-Jones, Jon Patrick, Ron Rymon, Richard
Segal, Jason Wells, Leslie Wells Simon Yip numerous helpful comments previous
drafts paper. especially indebted anonymous reviewers whose insightful,
extensive detailed comments greatly improved quality paper.
Breast Cancer, Lymphography Primary Tumor data sets provided
Ljubljana Oncology Institute, Slovenia. Thanks UCI Repository, maintainers,
Patrick Murphy David Aha, donors, providing access data sets used
herein.

References
Buchanan, B. G., Feigenbaum, E. A., & Lederberg, J. (1971). heuristic programming
study theory formation science. IJCAI-71, pp. 4050.
Clark, P., & Boswell, R. (1991). Rule induction CN2: recent improvements.
Proceedings Fifth European Working Session Learning, pp. 151163.
Clark, P., & Niblett, T. (1989). CN2 induction algorithm. Machine Learning, 3,
261284.
Clearwater, S. H., & Provost, F. J. (1990). RL4: tool knowledge-based induction.
Proceedings Second Intl. IEEE Conf. Tools AI, pp. 2430 Los Alamitos, CA.
IEEE Computer Society Pres.
de Kleer, J. (1986). assumption-based TMS. Artificial Intelligence, 28, 127162.
de Kleer, J., Mackworth, A. K., & Reiter, R. (1990). Characterizing diagnoses. Proceedings AAAI-90, pp. 324330 Boston, MA.
Hart, P., Nilsson, N., & Raphael, B. (1968). formal basis heuristic determination
minimum cost paths. IEEE Transactions System Sciences Cybernetics,
SSC-4 (2), 100107.
Hirsh, H. (1994). Generalizing version spaces. Artificial Intelligence, 17, 546.
Lawler, E. L., & Wood, D. E. (1966). Branch bound methods: survey. Operations
Research, 149, 699719.
Michalski, R. S. (1984). theory methodology inductive learning. Michalski,
R. S., Carbonell, J. G., & Mitchell, T. M. (Eds.), Machine Learning: Artificial
Intelligence Approach, pp. 83129. Springer-Verlag, Berlin.
463

fiWebb

Mitchell, T. M. (1977). Version spaces: candidate elimination approach rule learning.
Proceedings Fifth International Joint Conference Artificial Intelligence,
pp. 305310.
Mitchell, T. M. (1980). need biases learning generalizations. Technical report
CBM-TR-117, Rutgers University, Department Computer Science, New Brunswick,
NJ.
Moret, B. M. E., & Shapiro, H. D. (1985). minimizing set tests. SIAM Journal
Scientific Statistical Computing, 6 (4), 9831003.
Murphy, P., & Aha, D. (1993). UCI repository machine learning databases. [Machinereadable data repository]. University California, Department Information
Computer Science, Irvine, CA.
Murphy, P., & Pazzani, M. (1994). Exploring decision forest: empirical investigation
Occams Razor decision tree induction. Journal Artificial Intelligence Research,
1, 257275.
Narendra, P., & Fukunaga, K. (1977). branch bound algorithm feature subset
selection. IEEE Transactions Computers, C-26, 917922.
Nilsson, N. J. (1971). Problem-solving Methods Artificial Intelligence. McGraw-Hill, New
York.
Oblow, E. M. (1992). Implementing Valiants learnability theory using random sets. Machine Learning, 8, 4573.
Pearl, J. (1984). Heuristics: Intelligent Search Strategies Computer Problem Solving.
Addison-Wesley, Reading, Mass.
Plotkin, G. D. (1970). note inductive generalisation. Meltzer, B., & Mitchie, D.
(Eds.), Machine Intelligence 5, pp. 153163. Edinburgh University Press, Edinburgh.
Quinlan, J. R., & Cameron-Jones, R. M. (1995). Oversearching layered search
empirical learning. IJCAI95, pp. 10191024 Montreal. Morgan Kaufmann.
Reiter, R. (1987). theory diagnosis first principles. Artificial Intelligence, 32,
5795.
Rymon, R. (1992). Search systematic set enumeration. Proceedings KR-92, pp.
268275 Cambridge, MA.
Rymon, R. (1993). SE-tree based characterization induction problem. Proceedings 1993 International Conference Machine Learning San Mateo, Ca.
Morgan Kaufmann.
Schlimmer, J. C. (1993). Efficiently inducing determinations: complete systematic
search algorithm uses optimal pruning. Proceedings 1993 International
Conference Machine Learning, pp. 284290 San Mateo, Ca. Morgan Kaufmann.
464

fiAn Efficient Admissible Algorithm Unordered Search

Segal, R., & Etzioni, O. (1994). Learning decision lists using homogeneous rules. AAAI94.
Smyth, P., & Goodman, R. M. (1992). information theoretic approach rule induction
databases. IEEE Transactions Knowledge Data Engineering, 4 (2), 301
316.
Webb, G. I. (1990). Techniques efficient empirical induction. Barter, C. J., &
Brooks, M. J. (Eds.), AI88 Proceedings Third Australian Joint Conference
Artificial Intelligence, pp. 225239 Adelaide. Springer-Verlag.
Webb, G. I. (1993). Systematic search categorical attribute-value data-driven machine
learning. Rowles, C., Liu, H., & Foo, N. (Eds.), AI93 Proceedings Sixth
Australian Joint Conference Artificial Intelligence, pp. 342347 Melbourne. World
Scientific.
Webb, G. I. (1994a). Generality significant complexity: Toward alternatives
Occams Razor. Zhang, C., Debenham, J., & Lukose, D. (Eds.), AI94 Proceedings Seventh Australian Joint Conference Artificial Intelligence, pp. 6067
Armidale. World Scientific.
Webb, G. I. (1994b). Recent progress learning decision lists prepending inferred
rules. SPICIS94: Proceedings Second Singapore International Conference
Intelligent Systems, pp. B280B285 Singapore.

465


