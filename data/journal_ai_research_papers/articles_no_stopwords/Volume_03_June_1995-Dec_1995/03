journal artificial intelligence

submitted published

rule machine learning methods functional
prediction

sholom weiss

weiss cs rutgers edu

nitin indurkhya

nitin cs usyd edu au

department computer science rutgers university
brunswick jersey usa
department computer science university sydney
sydney nsw australia

abstract

describe machine learning method predicting value real valued function given values multiple input variables method induces solutions
samples form ordered disjunctive normal form dnf decision rules central
objective method representation induction compact easily interpretable
solutions rule decision model extended search eciently similar cases prior approximating function values experimental real world data
demonstrate techniques competitive existing machine learning
statistical methods sometimes yield superior regression performance

introduction
approximating values continuous variable described statistical literature regression given samples output response variable input
predictor variables x fx xn g regression task mapping f x relative space possibilities finite samples far complete predefined
model needed concisely map x accuracy prediction e generalization
cases primary concern regression differs classification output variable regression continuous whereas classification strictly categorical
perspective classification thought subcategory regression
machine learning researchers emphasized connection describing regression
learning classify among continuous classes quinlan
traditional classical linear least squares regression
scheffe developed refined many years linear regression proven quite
effective many real world applications clearly elegant computationally simple linear model limits complex may fit data better
increasing computational power computers larger volumes data interest grown pursuing alternative nonlinear regression methods nonlinear regression
explored statistics community many effective
methods emerged efron including projection pursuit friedman stuetzle
mars friedman methods nonlinear regression developed outside mainstream statistics community neural network trained
back propagation mcclelland rumelhart one model
c ai access foundation morgan kaufmann publishers rights reserved

fiweiss indurkhya
found numerical analysis girosi poggio overview many different
regression application classification available literature ripley
methods produce solutions terms weighted
real world classification commonly encountered regression accounts greater attention paid classification regression many important real world regression type
instance involving time series usually involve prediction real values besides
fact regression important another reason need
focus regression regression methods used solve classification
example neural networks often applied classification
issue interpretable solutions important consideration leading
development symbolic learning methods popular format interpretable solutions
disjunctive normal form dnf model weiss indurkhya decision trees
rules examples dnf decision rules similar characteristics decision
trees potential advantages stronger model b often better
explanatory capabilities unlike trees dnf rules need mutually exclusive thus
solution space includes tree solutions rules potentially compact
predictive trees decision rules may offer greater explanatory capabilities
trees tree grows size interpretability diminishes
among symbolic learning methods decision tree induction recursive partitioning highly developed many methods developed within machine learning
community id decision tree induction quinlan applied exclusively classification tasks less widely known decision trees effective
regression cart program developed statistical community induces classification regression trees breiman friedman olshen stone
regression trees strictly binary trees representation naturally follows
intensive modeling continuous variables
terms performance regression trees often competitive performance
regression methods breiman et al regression trees noted particularly
strong many higher order dependencies among input variables friedman
advantages regression tree model similar advantages enjoyed
classification trees two principal advantages cited dynamic
feature selection b explanatory capabilities tree induction methods extremely
effective finding key attributes high dimensional applications applications
key features small subset original feature set another characteristic
decision trees often cited capability explanation terms acceptable
people negative side decision trees cannot represent compactly many simple
functions example linear functions second weakness regression tree
model discrete yet predicts continuous variable function approximation
expectation smooth continuous function decision tree provides discrete regions
discontinuous boundaries though regression trees often produce
strong many applications advantages strongly outweigh potential
disadvantages
comparative study fayyad irani suggests binary classification trees somewhat
predictive even categorical variables



firule functional prediction
describe method inducing regression rules method
takes advantage close relationship classification regression provides
uniform general model dealing additional gains obtained
extending method manner preserves strengths partitioning
schemes compensating weaknesses rules used search
relevant cases subset cases help determine function value thus
model interpretability traded better performance empirical
suggest methods effective induce solutions often
superior decision trees

measuring performance

objective regression minimize distance sample output values
yi predicted values yi two measures distance commonly used classical
regression measure equation average squared distance yi yi e
variance leads elegant formulation linear least squares model mean
absolute distance deviation equation used least absolute deviation regression
perhaps intuitive measure
mean absolute distance deviation equation used studies
measure average error prediction yi n cases




xn yi yi

n
x
mad jyi yi j

v ariance n






n
regression sometimes described signal noise model
extended include stochastic component equation thus true function may
produce zero error distance contrast classification labels assumed
correct regression predicted values could explained number factors
including random noise component signal


f x xn

prediction primary concern estimates training cases alone
inadequate principles predicting performance cases analogous
classification mean absolute distance used error rate best
estimate true performance model error rate large set independent test
cases large samples data unavailable process train test simulated
random resampling experiments used fold cross validation
estimate predictive performance

regression tree induction

section contrast regression tree induction classification tree induction
classification trees regression trees induced recursive partitioning solution takes


fiweiss indurkhya
form equation ri disjoint regions ki constant values yji refers
values training cases fall within region ri

x ri f x ki medianfyji g

regression trees representation classification trees except terminal nodes decision terminal node assign case constant value
single best constant value median training cases falling terminal node
partition median minimizer mean absolute distance figure
example binary regression tree cases reaching shaded terminal node x
assigned constant value

x

x







x

x









figure example regression tree
tree induction methods usually proceed finding covering set training
cases b pruning tree best size although classification trees
widely studied similar applied regression trees assume
reader familiar classification trees cite differences binary
tree induction breiman et al quinlan weiss kulikowski many
respects regression tree induction straightforward classification trees error
rate poor choice node splitting alternative functions entropy gini
employed regression tree induction minimized function e absolute distance
satisfactory node single best split minimizes mean absolute
distance selected splitting continues fewer minimum number cases
covered node cases within node identical value
goal tree generalizes best cases often
full covering tree particularly presence noise weak features pruning strategies
employed classification trees equally valid regression trees covering
procedures substantial difference error rate measured terms
mean absolute distance one popular method weakest link pruning strategy breiman
et al weakest link pruning tree recursively pruned ratio delta n
minimized n number pruned nodes delta increase error


firule functional prediction
x
x
otherwise
figure example regression rules
weakest link pruning several desirable characteristics prunes training cases
remaining test cases relatively independent b compatible
resampling

regression rule induction
tree rule induction solutions disjunctive normal form model
equation applicable rule rule set represents single partition
region ri however unlike tree regions regions rules need disjoint
non disjoint regions several rules may satisfied single sample mechanism
needed resolve con icts ki constant values assigned multiple rules
ri regions invoked one standard model weiss indurkhya order
rules ordered rule sets referred decision lists first rule
satisfied selected equation

j x ri rj f x ki

figure example ordered rule set corresponding tree figure
cases satisfying rule rules assigned value
given model regression rule sets procedures effectively
induce solutions rule regression covering strategy analogous classification tree strategy could specified rule could induced adding single component
time added component single best minimizer distance usual
constant value ki median region formed current rule rule
extended fewer cases covered fewer minimal number cases covered
rule extension terminates covered cases removed rule induction continue
remaining cases regression analogue rule induction procedures
classification michalski mozetic hong lavrac clark niblett
however instead propose novel strategy mapping regression
covering classification

reformulation regression

motivation mapping regression classification number factors
related extra information given regression natural ordering yi
magnitude j yi yj
let fci g set consisting arbitrary number classes class containing
approximately equal values fyi g solve classification expect
classes different patterns found distinguish


fiweiss indurkhya
generate set pseudo classes p class figure
generate covering rule set transformed classification
rule induction method swap
weiss indurkhya
initialize current rule set covering rule set save
current rule set pruned iteratively following
prune current rule set
b optimize pruned rule set figure save
c make pruned rule set current rule set
use test cases cross validation pick best saved rule sets
figure overview method learning regression rules
classes expect classes formed ordering fyi g reasonable classification numbers reasons answer yes particularly
rule induction procedure
obvious situation classical linear relationship instance
definition ordering fx xni g corresponds ordering yi although classical
methods strong compactly determining linear functions interest modern
methods centers around potential finding nonlinear relationships nonlinear
functions know usually ordering fx xni g corresponding
fyig still expect true function smooth local region ordering
relationship hold terms classification know class cj similar values
quite different class ck much lower values nonlinear function
within class similar values similar values fx xni g
correspond local region function however true
identical values different fx xni g multiple clusters
found within class rule induction methods cover class single
rule expectation multiple patterns found cover clusters
cases assigned pseudo classes classification
solved following stages covering set b prune rule set
appropriate size improved achieved additional technique considered
c refine optimize rule set overall method outlined figure

generating pseudo classes
previous section described motivation pseudo classes specification
classes use information beyond ordering assumptions
true nature underlying function made within environment
goal make values within one class similar values across classes
dissimilar wish assign values classes overall distance
yi class mean minimum


firule functional prediction
input fyi g set output values
initialize n number cases k number classes
classi
classi next n k cases list sorted values
end
compute errnew
repeat
errold errnew
casej
classi
dist casej mean classi dist casej mean classi
move casej classi
dist casej mean classi dist casej mean classi
move casej classi
next casej
compute errnew
errnew less errold
figure composing pseudo classes p class
figure describes p class assigning values fyi g k classes essentially following sorts values b assigns approximately
equal numbers contiguous sorted yi class c moves yi contiguous class
reduces global distance err yi mean assigned class
classes identical means merged p class variation k means clustering statistical method minimizes distance measure hartigan wong
alternative methods depend distance measures lebowitz may
used
given fixed number k classes procedure relatively quickly assign yi
classes overall distances minimized underlying function
unknown critical global minimum assignment yi procedure
matches well stated goals ordering yi values obvious remaining question
determine k number classes unfortunately direct answer
experimentation necessary however shall see section empirical
evidence suggesting quite similar within local neighborhood values
k moreover relatively large values k entail increased computational complexity
rule induction typically necessary noise free functions modeled
exactly analogous comparisons neural nets increasing numbers hidden units
trends increasing numbers partitions become evident experimentation
one additional variation classification theme arises rule induction schemes
cover one class time classes must ordered last class typically


fiweiss indurkhya
becomes default class cover situations rule classes satisfied
regression one default partition class unlikely best covering solution
instead remaining cases last class repeatedly partitioned p class
classes fewer cases remain
interesting characteristic transformation regression
uniform general model relates classification
regression yi values discrete categorical p class merely restates standard
classification example values yi
p class non empty classes

covering rule set
transformation rule induction classification applied
consider induction methods fully cover class moving induce
rules next class step covering considered
binary classification current class ci versus cj j e
current class versus remaining classes rule induced corresponding cases
removed remaining cases considered class covered
next class considered example covering used swap
weiss indurkhya procedure used covering
method identical classification regression however one distinction
regression classes transient labels replaced median values
cases covered induced rule rules ordered multiple rules
may satisfied medians derived instances rule
first satisfied
although procedure may yield good compact covering sets additional procedures
necessary complete solution

pruning rule set
typical real world applications noisy features fully predictive covering
set particularly one composed many continuous variables far specialized
produce best classification relatively classes specified advance
regression expect many smaller groups values yi likely quite
different
noted earlier regression trees usual classification pruning techniques
applied substitution mean absolute distance classification error rate
weakest link tree pruning ratio delta n recursively minimized
weakest link rule pruning intuitive rationale remove parts rule set
least impact increasing error pruning rule sets usually accomplished
deleting complete rules single rule components quinlan weiss indurkhya
general rule pruning classification regression less natural
far computationally expensive tree pruning tree pruning natural ow
set subset thus tree pruned bottom typically considering
effect removing subtree non disjoint rules natural pruning order


firule functional prediction
example every component rule candidate pruning may affect rules
follow specified rule order
major difference pruning regression rules vs classification rules
classification deleting rule rule component effect class labels
regression pruning change median values regions even deletion
rule affect region medians rules ordered multiple rules may
satisfied characteristic rule pruning regression adds substantial complexity
task however assuming median values remain unchanged
evaluation candidate rules prune pruning procedure achieve reasonable
computational eciency expense loss accuracy evaluation
best rule component deletion selected medians regions
evaluated
even classification rules rule pruning inherent weaknesses example
rule deletion often create gap coverage classification rules though quite
feasible develop additional procedure refine optimize rule set large
extent overcomes cited weakness pruned rules sets similar refinement
optimization procedure developed regression described next

rule refinement optimization

given rule set rsi improved question applies rule set although
mostly motivated trying improve pruned rules sets frso rsi rsng
combinatorial optimization error measure err rs improve
rsi without changing size e number rules components figure describes
minimizes err rs mad model prediction sample cases
local swapping e replacing single rule component best alternative
variation techniques used swap weiss indurkhya
central theme hold model configuration constant make single local
improvement configuration local modifications made improvements possible making local changes configuration widely used optimization
technique approximate global optimum applied quite successfully
example near optimum solutions traveling salesman lin kernighan
analogous local optimization technique called backfitting used
context nonlinear statistical regression hastie tibshirani
variations selection next improvement move could include
first local improvement encountered backfitting
best local improvement swap
experiments rule induction methods consistently better
ecient pruned rule induction environment mostly stable
relatively local improvements prior convergence less stable environment
large numbers possible configuration changes may feasible even better
pruned rule set environment covering procedure effective pruned
solution relatively close local minimum solution weakest link pruning


fiweiss indurkhya
input rs rule set consisting rules ri
set training cases
true
true
rsnew rs single best replacement
component rs reduces err rs
cases current median ri
replacement found
false
else
rs rsnew recompute medians ri
endwhile
return rule set rs
figure optimization rule component swapping
series pruned rule sets rsi number far fewer sets would
single prune rule rule component rsi optimized prior
continuing pruning process however rule set optimization usually suspended
substantial segments covering set already pruned
used sequentially ordered evaluations backfitting stochastic evaluations considered empirical evidence optimization literature supports superiority stochastic evaluation jacoby kowalik pizzo
improvements may obtained occasionally making random changes configuration
kirpatrick gelatt vecchi general combinatorial optimization techniques must substantially reworked fit specific type expected
applied throughout solving
pruning covering rule set rso series progressively smaller rule
sets frso rsi rsn g objective pick best one usually form
error estimation model complexity future performance highly related
complex simple model yield poor objective
right size model independent test cases resampling cross validation effective
estimating future performance absence estimates approximations
gcv craven wahba friedman described equation
used statistics literature estimate performance measures training error
model complexity used estimates c measure model complexity
expressed terms parameters estimated number weights neural net
tests performed c assumed less n number cases
gcv acronym generalized cross validation apparent error training cases used
true cross validation resampling



firule functional prediction

xn
gcv

jy j






n
c
n



experiments used cross validated estimates guide final model selection
process measures gcv may used

potential rule regression

regression rules trees induced recursive partitioning methods approximate function constant value regions relatively strong dynamic feature
selection high dimensional applications sometimes highly predictive
features essential weakness methods approximation partition
region constant value continuous function even moderately sized sample
approximation lead increased error
deal limitation instead constant value functions linear functions
substituted partition quinlan however linear function obvious
weakness true function may far linear even restricted context
single region general use linearity compromises highly non parametric
nature dnf model better strategy might examine alternative non linear
methods

alternative rules k nearest neighbors
k nearest neighbor method one simplest regression methods relying table
lookup classify unknown case x k cases closest case
found sample data base stored cases predicted x equation mean
values k nearest neighbors nearest neighbors found distance
metric euclidean distance usually feature normalization method
non parametric highly non linear nature

yknn x k

xk yk k nearest neighbours x

k



major limit effect irrelevant features
limited forms feature selection sometimes employed preprocessing stage
method cannot determine features weighted others
procedure sensitive distance measure used high dimensional
feature space k nearest neighbor methods may perform poorly limitations
precisely partitioning methods address thus theory two methods
potentially complement one another

model combination

practice one learning model superior others learning strategy
examines different may better moreover combining


fiweiss indurkhya
different enhanced may achieved general combining
learning scheme referred stacking wolpert additional studies
performed applying scheme regression breiman leblanc
tibshirani small training samples simulated data linear combinations
regression methods improved reported let mi th model trained
sample wi weight given mi case vector
x predictions different combined equation produce
estimate may use representation k nearest neighbors
variable size k perhaps variable size decision trees could completely
different combining decision trees linear regression different
applied independently solutions later weighted vote taken reach
combined solution method model combination contrast usual
evaluation different single best performing model selected



xk wkmk x

k



stacking shown give improved simulated data major
drawback properties combined retained thus interpretable combined may interpretable
possible compensate weaknesses one model introducing another model
controlled fashion
suggested earlier partitioning regression methods k nearest neighbor regression
methods complementary hence one might expect suitably combining two
methods one might obtain better performance one recent study quinlan model
trees e regression trees linear combinations leaf nodes nearest neighbor
methods combined combination method described equation
n x k one k nearest neighbors x v x value stored instance
x x applying model tree x

k

xk v n x k n x k x

k



k nearest neighbors found independently induced regression tree
reported k sense similar combination method
equation k nearest neighbors passed tree used
refine nearest neighbor answer thus combination model formed
independently computing global solution later combining
however strong reasons determining global nearest neighbor solution independently limit large samples non parametric k nearest
neighbor methods correctly fit function practice though weaknesses
substantial finding effective global distance measure may easy particularly
presence many noisy features hence different technique combining two
methods needed
weights obtained minimize least squared error constraints breiman




firule functional prediction

integrating rules table lookup

consider following strategy determine value case x falls region ri
instead assigning single constant value ki region ri ki determined
x mean k nearest
median value training cases region assign yknn
training set instances x region ri thus regression trees equation
regression rules equation
x
x ri f x yknn



x
j x ri rj f x yknn



interesting aspect strategy k nearest neighbor need
considered cases covered particular partition increases interaction eliminates independent computation two
model rationale shall empirical supportive

representation potentially alleviates weakness partitions
assigned single constant values moreover global distance measure difficulties k nn methods may relieved table lookup reduced
partitioned related groupings
rationale hybrid partition k nn scheme note unlike stacking
hybrid independently determined interact strongly one
another however must demonstrated methods fact complementary
preserving strengths partitioning schemes compensating weaknesses
would introduced constant values used region respect model
combination two principal questions need addressed empirical experimentation

improved relative model alone
methods competitive alternative regression methods


experiments conducted assess competitiveness rule regression compared
procedures including less interpretable ones well evaluate performance integrated partition k nn regression method experiments performed
seven datasets six described previous studies quinlan addition six datasets experiments done large telecommunications
application labeled pole seven datasets one continuous
real valued response variable experimental reported terms mad
measured fold cross validation pole cases used training
independent testing features different datasets mixture
continuous categorical features pole features continuous descriptions


fiweiss indurkhya

dataset cases vars
price
servo
cpu
mpg
peptide
housing
pole

















table dataset characteristics
datasets found literature quinlan table summarizes
key characteristics datasets used study
table summarizes original reported quinlan include modeltrees mt regression trees linear fits terminal nodes neural nets
nnet nearest neighbors nn combined model trees nearest
neighbors mt nn
table summarizes additional obtained include cart
regression tree rt nearest neighbors euclidean distance nn rule regression
swap rule regression nn applied rule region rule nn mars
nn used expectation nearest neighbor method incrementally
improves constant value region region moderately large sample neighbors
average
rule method parameter number pseudo classes must
determined found cross validation independent test cases
experiments cross validation used figure represents typical plot relative
error vs number pseudo classes weiss indurkhya b number
partitions increases improve reach relative plateau deteriorate
somewhat similar complexity plots found example neural nets
weiss kapouleas
mars procedure several adjustable parameters parameter mi values
tried additive modeling number inputs df default value
tried well optimal value estimated cross validation parameter nk
varied steps lastly piece wise linear well piece wise cubic
solutions tried setting parameters cross validated
accuracy monitored value best mars model reported
method besides mad relative error reported relative
error simply estimated true mean absolute distance measured cross validation
normalized initial mean absolute distance median analogous classifi peptide dataset slightly modified version one quinlan refers lhrh att
version used experiments cases missing values removed
peptide slightly modified version lhrh att dataset listed one
provided quinlan personal communication
particular program used mars



firule functional prediction

relative error
























number pseudo classes







figure prototypical performance varying pseudo classes

dataset mt nnet nn mt nn
price
servo

cpu

mpg

peptide
housing



















table previous
cation predictions must fewer errors simply predicting largest class
regression must better average distance median
meaningful
comparing performance two methods dataset standard error
method independently estimated larger one used comparisons
difference performance greater standard errors difference
considered statistically significant significance test one must consider
overall pattern performance relative advantages competing solutions weiss
indurkhya
dataset figure plots relative best error found ratio best
reported model relative best error indicates
best reported regression model model compared
best regression rules nn mixed model graph indicates


fiweiss indurkhya

dataset

rt

nn

rule

rule nn

mars

mad error mad error mad error mad error mad error
price

servo

cpu

mpg

peptide









housing
pole

table performance additional methods
relative best erate

nn
rule


rule nn










servo

house

mpg

cpu

price

peptide

pole

figure relative best erates nn rules rule nn
trends across datasets helps assess overall pattern performance respect
rule rule nn exhibit excellent performance across many applications
empirical allow us consider several relevant questions regarding rulebased regression
rule regression perform compared tree regression comparing
rule rt one see except servo rule consistently
better rt remaining six datasets difference performance


firule functional prediction
tests significant significance tests general trend
seen visually figure leads us conclude rule regression
definitely competitive trees often yields superior performance
integrating nn rules lead improved performance relative
model alone comparison rule nn nn shows datasets rule nn
significantly better comparing rule nn rule indicate
three datasets mpg pole housing rule nn significantly better rule
remaining three datasets overall pattern
performance appears favor rule nn rule thus empirical
indicate method improved relative model alone
general trend seen figure
methods competitive alternative regression methods among previous reported mt nn best performer alternatives consider
regression trees rt mars none three methods significantly
better rule nn datasets consideration except rt
significantly better servo furthermore rule nn significantly better
mt nn three five datasets servo cpu mpg comparison possible overall trend favor rule nn comparing rt rule nn
except servo rule nn significantly better rt remaining datasets comparing mars rule nn three datasets
price peptide pole rule nn significantly better hence empirical overwhelmingly suggest method competitive alternative
regression methods hints superiority methods

discussion

considered model rule regression provided comparisons
tree regression many applications strong explanatory capabilities high dimensional feature selection make dnf model quite advantageous particularly
true knowledge applications example equipment repair medical diagnosis
contrast pure pattern recognition applications speech recognition
rules similar trees rule representation potentially compact
rules mutually exclusive potential finding compact
solution particularly important model interpretation crucial
note space rules includes space trees thus tree solution
best theoretically rule induction procedure potential
experiments regression rules generally outperformed regression trees
fewer constant regions required estimated error rates generally lower
finding dnf regions substantially computationally expensive regression rules regression trees regression rules fairly complex optimization
techniques necessary addition experiments must performed appropriate number pseudo classes matter scale scale application
versus scale available computing excluding telecommunications application
none cited applications takes minutes cpu time ss sin

fiweiss indurkhya
gle pseudo classification full cross validation computing power increases
timing distinction less important even small percentage gain quite valuable appropriate application apte damerau weiss computational
requirements secondary factor
provided several real world datasets mostly involve nonlinear relationships one may wonder rule method would perform data
obvious linear relationships earlier experiments data exhibiting linear
relationships example drug study data efron rule solutions
slightly better trees however true test real world data often involve
complex non linear relationships comparisons alternative help assess
effectiveness techniques
looking figure tables see pure rule solutions
competitive additional gains made rules used
obtaining function values directly instead used relevant cases
used compute function value experiments support
view strategy combining different methods improve predictive performance
strategies similar applied classification ting
widmer similar conclusions drawn indicate
strategy useful regression context empirical support
contention regression partitioning methods nearest neighbor methods
complementary solution found partitioning alone incremental
improvement observed substituting average k nearest neighbors
median partition perspective nearest neighbor regression methods
sample cases compartmentalized simplifying table lookup case
conclusive hints combination strategy effective
small moderate samples likely sample size grows large increased
numbers partitions terms rules terminal nodes compensate single
constant valued regions conjecture supported large sample pole application
incremental gain addition k nn small
experiments used k nn k depending application different
value k might produce better optimal value might estimated crossvalidation strategy systematically varies k picks value gives best
overall however unclear whether increased computational effort
significant performance gain
another practical issue large samples storage requirement cases must
stored serious drawback real world applications limited memory
however tried experiments cases associated partition replaced
fewer number typical cases considerable savings terms storage
requirements slightly weaker though significantly different
would appear gains might obtained restricting k nn consider
features appear path leaf node examination might
seem good idea attempts ensure features relevant
fold cross validation requires solving essentially times training cases
times group test cases
although small difference tests significant sample large



firule functional prediction
cases node used distance calculations however found
weaker
number regression techniques presented others demonstrate
advantages combined combine methods independently
invoked instead typical election one winner alternative
combined weighted combination techniques advantage
outputs different treated independent variables combined
form post processing model outputs available
way contradict value alternative combination techniques
approaches improved applications conclude however
advantages complex regression procedures dynamically mix
alternative procedures may particularly strong fundamental rationale choice methods partitioning methods properties
combined must preserved
presented regression one output variable classical form linear regression trees issue multiple outputs
directly addressed although extensions feasible issue experimentation await future work model regression provide basis efforts
leveraging current strong methods classification rule induction

references

apte c damerau f weiss automated learning decison rules text
categorization acm transactions oce information systems
breiman l stacked regression tech rep u ca berkeley
breiman l friedman j olshen r stone c classification regression
tress wadsworth monterrey ca
clark p niblett cn induction machine learning

craven p wahba g smoothing noisy data spline functions estimating
correct degree smoothing method generalized cross validation numer
math
efron b computer intensive methods statistical regression siam review

fayyad u irani k attribute selection decision tree generation
proceedings aaai pp san jose
friedman j multivariate adaptive regression splines annals statistics

friedman j stuetzle w projection pursuit regression j amer stat assoc



fiweiss indurkhya
girosi f poggio networks best approximation property biological
cybernetics
hartigan j wong k means clustering
applied statistics
hastie tibshirani r generalized additive chapman hall
jacoby kowalik j pizzo j iterative methods non linear optimization
prentice hall jersey
kirpatrick gelatt c vecchi optimization simulated annealing
science
leblanc tibshirani r combining estimates regression classification
tech rep department statistics u toronto
lebowitz categorizing numeric information generalization cognitive science
lin kernighan b ecient heuristic traveling salesman
operations
mcclelland j rumelhart explorations parallel distributed processing
mit press cambridge
michalski r mozetic hong j lavrac n multi purpose incremental learning system aq testing application three medical domains
proceedings aaai pp philadelphia pa
quinlan j induction decision trees machine learning
quinlan j simplifying decision trees international journal man machine
studies
quinlan j combining instance model learning international
conference machine learning pp
ripley b statistical aspects neural networks proceedings seminair europeen de statistique london chapman hall
scheffe h analysis variance wiley york
ting k small disjuncts remedy decision trees proceedings
th canadian conference artificial intelligence pp
weiss indurkhya n optimized rule induction ieee expert
weiss indurkhya n b rule regression proceedings th
international joint conference artificial intelligence pp


firule functional prediction
weiss indurkhya n decision tree pruning biased optimal proceedings
aaai pp
weiss kapouleas empirical comparison pattern recognition neural
nets machine learning classification methods international joint conference
artificial intelligence pp detroit michigan
weiss kulikowski c computer systems learn classification prediction methods statistics neural nets machine learning expert systems
morgan kaufmann
widmer g combining knowledge instance learning exploit
qualitative knowledge informatica
wolpert stacked generalization neural networks




