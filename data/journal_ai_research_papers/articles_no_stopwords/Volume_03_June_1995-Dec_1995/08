Journal Artificial Intelligence Research 3 (1995) 271-324

Submitted 2/95; published 11/95

Flexibly Instructable Agents
Scott B. Huffman

Price Waterhouse Technology Centre, 68 Willow Road
Menlo Park, CA 94025 USA

John E. Laird

Artificial Intelligence Laboratory
University Michigan, 1101 Beal Ave.
Ann Arbor, MI 48109{2110 USA

huffman@tc.pw.com
laird@eecs.umich.edu

Abstract

paper presents approach learning situated, interactive tutorial instruction within ongoing agent. Tutorial instruction exible (and thus powerful)
paradigm teaching tasks allows instructor communicate whatever types
knowledge agent might need whatever situations might arise. support exibility, however, agent must able learn multiple kinds knowledge broad
range instructional interactions. approach, called situated explanation, achieves
learning combination analytic inductive techniques. combines
form explanation-based learning situated instruction full suite
contextually guided responses incomplete explanations. approach implemented
agent called Instructo-Soar learns hierarchies new tasks domain knowledge interactive natural language instructions. Instructo-Soar meets
three key requirements exible instructability distinguish previous systems:
(1) take known unknown commands instruction point; (2) handle
instructions apply either current situation hypothetical situation specified language (as in, instance, conditional instructions); (3) learn,
instructions, class knowledge uses perform tasks.

1. Introduction
intelligent, autonomous agents future called upon perform wide
varying range tasks, wide range circumstances, course
lifetimes. Performing tasks requires knowledge. number possible tasks
circumstances large variable time (as general agent), becomes
nearly impossible preprogram knowledge required. Thus, knowledge must
added agent's lifetime. Unfortunately, knowledge cannot added
current intelligent systems perform; must shut programmed
new task.
work examines alternative: intelligent agents taught perform tasks
tutorial instruction, part ongoing performance. Tutorial instruction
highly interactive dialogue focuses specific task(s) performed.
working tasks, student may receive instruction needed complete tasks
understand aspects domain previous instructions. situated, interactive
form instruction produces strong human learning (Bloom, 1984). Although

c 1995 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiHuffman & Laird

received little attention AI, potential powerful knowledge source
artificial agents well.
Much tutorial instruction's power comes communicative exibility: instructor communicate whatever type knowledge student may need whatever
situation needed. challenge designing tutorable agent support
breadth interaction learning abilities required exible communication.
paper, present theory learning tutorial instruction within ongoing
agent. developing theory, given special attention supporting communicative exibility instructor (the human user). began identifying properties
tutorial instruction instructor's perspective. properties,
derived set requirements instructable agent must meet support exible instructability. requirements drove development theory evaluation.
Finally, implemented theory instructable agent called Instructo-Soar
(Huffman, 1994; Huffman & Laird, 1993, 1994), evaluated performance.1
Identifying requirements exible instructability provides target { set evaluation
criteria { instructable agents. requirements encompass ways agent interacts
instructor, comprehends instructions, learns them. general
requirements common interactive learning systems; e.g., agent expected
learn general knowledge instructions, learn quickly (with minimal number
examples), integrate learned previous knowledge, etc. requirements
specific tutorial instruction.
theory learning tutorial instruction specifies analytic inductive
learning techniques combined within agent meet requirements, producing
general learning wide range instructional interactions. present learning
framework called situated explanation utilizes situation instruction applies
larger instructional context (the instruction's type place current dialogue)
guide learning process. Situated explanation combines form explanation-based
learning (DeJong & Mooney, 1986; Mitchell, Keller, & Kedar-Cabelli, 1986) situated individual instruction, full suite contextually guided responses
incomplete explanations. responses include delaying explanation information available, inducing knowledge complete explanations, completing explanations
instruction, abandoning explanation favor weaker learning methods.
Previous explanation-based learning systems employed one cases static
sequence options, chosen dynamically among options based
context example. dynamic selection required exible instructability. learning framework cast within computational model general intelligent
behavior called problem space computational model.
Instructo-Soar implemented agent embodies theory. interactive natural language instructions, Instructo-Soar learns perform new tasks, extends
known tasks apply new situations, acquires variety types domain
knowledge. allows exible instruction previous instructable systems (e.g.,
learning apprentice systems, Mitchell, Mahadevan, & Steinberg, 1990) meeting three
1. work inspired human students, taken cues psychological effects
appropriate. theory's potential cognitive model discussed elsewhere (Huffman, 1994;
Huffman, Miller, & Laird, 1993).

272

fiFlexibly Instructable Agents

Push green button.
Thats new one. that?
Move grey table.
Ok. next?
Move green button.
that?
Move arm up.
Oh, see! next?
Move down.
Ok. next?
operator finished.

turn light, push button.
mean pushing button
causes light come on?
Yes.
...
Never grasp red blocks.
not?
Red blocks explosive.
light turn on.
...

Figure 1: example tutorial instruction.
key requirements tutorial instruction: (1) take known unknown commands
instruction point; (2) handle instructions apply either current situation hypothetical situation specified language (as in, instance, conditional
instructions); (3) learn, instructions, class knowledge uses
perform tasks.
follows, first discuss properties requirements tutorial instruction.
Then, present approach implementation Instructo-Soar, including
series examples illustrating instructional capabilities supported. conclude
discussion limitations areas research.

2. Properties Tutorial Instruction
Tutorial instruction situated, interactive instruction given agent attempts
perform tasks. situated applies particular task situations arise
domain. interactive agent may request instruction needed. type
instruction common task-oriented dialogues experts apprentices (Grosz,
1977). example tutorial instruction given Instructo-Soar robotic domain
shown Figure 1.
Tutorial instruction number properties make exible easy
instructor produce:
P1. Situation specificity. Instructions given particular tasks particular situations. teach task, instructor need provide suggestions specific
situation hand, rather producing global procedure includes general conditions applicability step, handles possible contingencies, etc.
situation help disambiguate otherwise ambiguous instruction. number authors discussed advantages situation-specific knowledge elicitation
(e.g., Davis, 1979; Gruber, 1989).
P2. Situation specification needed. Although instructions typically apply
situation hand, instructor free specify situations needed;
instance, specifying contingencies using conditional instructions.
P3. Incremental as-needed elicitation. Knowledge elicited incrementally part
agent's ongoing performance. Instructions given agent unable
273

fiHuffman & Laird

perform task; thus, directly address points agent's knowledge
lacking.
P4. Task structuring instructor. instructor structure larger tasks
smaller subtasks way desired. instance, task requiring ten primitive steps
may taught simple sequence ten steps, two subtasks five steps
each, etc. agent know instructed action subtask is,
perform situation hand, ask instruction.
P5. Knowledge-level interaction. instructor provides knowledge agent
knowledge level (Newell, 1981). is, instructions refer objects
actions world, symbol-level structures (e.g., data structures) within
agent. interaction occurs natural language, language instructor
uses talk task, rather requiring artificial terminology syntax
specify agent's internal data processes.
Tutorial instructions provide knowledge applicable agent's current situation
closely related one. Thus, type instruction appropriate tasks
local control structure, control decisions made based presently available
information. Local control structure characteristic constructive synthesis tasks,
primitive steps composed one another form complete solution.
work focuses type task.2

3. Requirements Instructable Agent
Although easing instructor's burden providing knowledge, properties tutorial
instruction described place severe requirements instructable agent. general,
agent must solve three conceptually distinct problems: must (1) comprehend individual instructions produce behavior, (2) support exible dialogue instructor,
(3) produce general learning interaction. properties tutorial instruction described previous section place requirements solutions
problems. follows, identify key requirements problem turn.

3.1 Comprehending Instructions: Mapping Problem

mapping problem involves comprehending instructions given natural language transforming information contain agent's internal representation
2. contrast, problem solving methods constraint satisfaction heuristic classification involve global
control strategies. strategies either follow fixed global regime require aggregation
information multiple problem solving states make control decisions. possible produce
global control strategy using combination local decisions (Yost & Newell, 1989). However, teaching
global method casting purely sequence local decisions may dicult. types
instruction, beyond scope work, required teach global methods natural way.
acquire knowledge tasks involve known global control strategy, may ecient use
method-based knowledge acquisition tool (e.g., Birmingham & Klinker, 1993; Birmingham & Siewiorek,
1989; Eshelman, Ehret, McDermott, & Tan, 1987; Marcus & McDermott, 1989; Musen, 1989)
control strategy built in.

274

fiFlexibly Instructable Agents

language. required agent apply information communicated instructions
knowledge level (property P5, above) internal processing.
Solving mapping problem general involves complexities natural language comprehension. Carpenter (1976) point out, instructions linguistically complex dicult interpret independent diculty task
instructed. Even linguistically simple instructions, actions objects often incompletely specified, requiring use context domain knowledge produce complete
interpretation (Chapman, 1990; DiEugenio & Webber, 1992; Frederking, 1988; Martin &
Firby, 1991).
general requirement mapping problem tutorable agent straightforward:

M1 . tutorable agent must able comprehend map aspects instruction
fall within scope information possibly represent.

agent cannot expected interpret aspects fall outside representation abilities (these abilities may augmented instruction, occurs building
existing abilities). detailed analysis could break general requirement
set specific ones.
work focused mapping problem. Rather, agent implemented uses fairly standard natural language processing techniques handle instructions
express sucient range actions situations demonstrate capabilities.
concentrated efforts interaction transfer problems.

3.2 Supporting Interactive Dialogue: Interaction Problem
interaction problem problem supporting exible dialogue instructor.
properties tutorial instruction indicate dialogue occurs agent's
ongoing performance address lacks knowledge (property P3); within dialogue,
agent must handle instructions apply different kinds situations (properties
P1 P2) structure tasks different ways (property P4).
instructable agent moves toward solving interaction problem degree
supports properties. work, concentrate instructor's utterances
within dialogue, since exibility instructor goal. considered
potential complexity agent's utterances (e.g., give instructor various kinds
feedback) much detail.
properties exible interaction specified terms individual instruction
events, instruction event utterance single instruction particular
point discourse. support truly exible dialogue, instructable agent must
able handle instruction event coherent current discourse point.
instruction event initiated either student teacher, carries knowledge
type applied particular task situation. Thus, exible tutorable agent
support instruction events with:

I1. Flexible initiation. Instruction events initiated agent instructor.
275

fiHuffman & Laird

I2. Flexibility knowledge content. knowledge carried instruction event
piece types knowledge agent uses applicable
way within ongoing task discourse context.

I3. Situation exibility. instruction event apply either current task
situation specified hypothetical situation.

following sections discuss requirements detail.
3.2.1 Flexible Initiation

human tutorial dialogues, initiation instruction mixed student teacher.
One study indicates teacher initiation prevalent early instruction; student
initiation increases student learns more, drops student
masters task (Emihovich & Miller, 1988).
Instructor-initiated instruction dicult support instruction events
interrupt agent's ongoing processing. Upon interrupting agent, instruction event
may alter agent's knowledge way could change invalidate reasoning
agent previously engaged. diculties, instructable systems
date fully supported instructor-initiated instruction.3 Likewise, InstructoSoar handle instructor-initiated instruction.
Agent-initiated instruction directed (at least) two possible ways: verification
impasses. learning apprentice systems, LEAP (Mitchell et al., 1990)
DISCIPLE (Kodratoff & Tecuci, 1987b) ask instructor verify alter reasoning
step. advantage approach step examined instructor;
disadvantage, course, step must examined. alternative approach
drive instruction requests impasses agent's task performance (Golding, Rosenbloom, & Laird, 1987; Laird, Hucka, Yager, & Tuck, 1990). approach used
Instructo-Soar. impasse indicates agent's knowledge lacking needs
instruction. advantage approach agent learns, becomes
autonomous; need instruction decreases time. disadvantage
lacks knowledge recognized reaching impasses; e.g., impasse occur
performance correct inecient.
3.2.2 Flexibility Knowledge Content

exible tutorable agent must handle instruction events involving knowledge
applicable way within ongoing task discourse context. requirement
dicult meet general, wide range knowledge may relevant
particular situation. requires robust ability relate utterance ongoing
discourse task situation. instructable systems met requirement fully.
However, define constrained form requirement, limited instructions command actions (i.e., imperatives). Imperative commands especially
prevalent tutorial instruction procedures. Supporting exible knowledge content
3. systems learned purely observing expert (e.g., Dent, Boticario, McDermott, Mitchell &
Zabowski, 1992; Redmond, 1992). Observation type instructor-initiatedness, instruction
interactive dialogue.

276

fiFlexibly Instructable Agents

commands means allowing instructor give relevant command point
dialogue teaching task. call ability command exibility.
command given, three possibilities: (1) commanded action
known, agent performs it; (2) commanded action known, agent
know perform current situation (extra, unknown steps needed);
(3) commanded action unknown. Thus, command exibility allows instructor
teaching procedure skip steps (2) command subtask unknown (3)
point. cases, agent asks instruction. interaction pattern
results, procedures commanded taught needed,
observed human instruction. Wertsch (1979) notes \...adults spontaneously follow
communication strategy use directives children understand
guide children behaviors necessary carry directives."
Command exibility gives instructor great exibility teaching set tasks instructions hierarchically structure tasks whatever way instructor
wishes. mathematical analysis (Huffman, 1994) revealed number possible
sequences instructions used teach given procedure grows exponentially
number actions procedure. procedure 6 primitive actions,
100 possible instruction sequences; 7, 400.
3.2.3 Situation Flexibility

exible tutorable agent must handle instructions apply either current task
situation hypothetical situation instructor specifies. Instructors make
frequent use options. instance, analysis protocol student
taught use ight simulator revealed 119 508 instructions (23%) involved
hypothetical situations, remainder applying current situation time
given.
Instructions apply current situation, imperative commands (e.g.,
\Move yellow table"), called implicitly situated (Huffman & Laird, 1992). Since
instruction says nothing situation applied,
current situation (the task performed current state) implied.
contrast, instructions specify elements situation meant
apply explicitly situated (Huffman & Laird, 1992). agent meant carry
instructions immediately (as implicitly situated instruction), rather
situation arises one specified. Examples include conditionals instructions
purpose clauses (DiEugenio, 1993), following:4


using chocolate chips, add coconut mixture pressing
pie pan.



restart this, hit R shift-R.



get interval want, center joystick again.

4. examples taken protocol tutorial instruction written source instruction (a
cookbook).

277

fiHuffman & Laird

number researchers pointed (Ford & Thompson, 1986; Haiman, 1978;
Johnson-Laird, 1986), conditional clauses introduce shared reference speaker
hearer forms explicit background interpreting evaluating consequent.5
Here, clauses italics indicate hypothetical situation command
remainder instruction meant apply. cases, situation partially
specified, remainder drawn current situation, \When using chocolate
chips [and cooking recipe, current point process]..."
general, hypothetical situation may created referred across multiple
utterances. agent presented handles implicitly single explicitly situated
instructions, deal hypothetical situations exist multiple
instructions.

3.3 Producing General Learning: Transfer Problem

transfer problem problem learning generally applicable knowledge instructions, transfer appropriate situations future. general learning
based instructions apply specific situations (property P1, above). Many types
knowledge may learned, since instructions provide type knowledge
agent lacking (property P3).
Solving problem involves simply memorizing instructions future use;
rather, conditions applying instruction must determined situation.
Consider, example, following exchange instructor agent:
Block open oce door.
that?
Pick red block.
Now, drop here, next door.
proper conditions performing \pick up" action? Simple memorization yields poor learning; e.g., whenever blocking open office door, pick
red block. However, block's color, even fact block, irrelevant case. Rather, fact block weighs (say) five pounds,
giving enough friction oor hold open door, crucial. Thus, proper
learning might be:
trying block open door,
object obj picked up,
obj weighs 5 pounds
propose picking obj.
Here, original instruction generalized (color red isa block drop out)
specialized (weight > 5 added).
transfer problem places number demands tutorable agent:
T1. General learning specific cases. agent instructed particular
situation, expected learn general knowledge apply suciently
similar situations.
5. types conditionals follow pattern (Akatsuka, 1986), relevant
tutorial instruction.

278

fiFlexibly Instructable Agents

T2. Fast learning. instructable agent expected learn new procedures quickly.
T3.

T4.
T5.

T6.

Typically, task taught once.
Maximal use prior knowledge. agent must apply prior knowledge
learning instruction. maxim machine learning systems general (if
knowledge, use it), particularly relevant learning instruction
learning expected happen quickly.
Incremental learning. agent must able continually increase knowledge
instruction. New knowledge must smoothly integrated agent's
existing knowledge part ongoing performance.
Knowledge-type exibility. Since type knowledge (e.g., control knowledge,
causal knowledge, etc.) might communicated instructions, exible tutorable
agent must able learn type knowledge uses. make testable
criterion laying types knowledge agent based particular
computational model.
Dealing incorrect knowledge. agent's knowledge clearly incomplete
(otherwise, would need instruction); may incorrect. general tutorable agent must able perform learn effectively despite incorrect knowledge.

T7. Learning instruction coexisting learning sources.

addition instruction, complete agent able learn
sources knowledge available. might include learning observation/demonstrations, experimentation environment, analogy, etc.
theory learning tutorial instruction presented focuses extending
incomplete knowledge instruction { requirements T1 T5 list. Handling incorrect knowledge (T6) learning sources (T7) planned extensions
progress.
Table 1 summarizes requirements must met instructable agent support exible tutorial instruction, indicates requirements targeted InstructoSoar. made two simplifications using requirements evaluate InstructoSoar. First, treat requirement binary; is, either completely met
unmet. reality, requirements could broken finer-grained pieces evaluated separately. Second, treat requirement independently. table indicates
Instructo-Soar's performance requirement alone, account potential interactions them. interactions complex; instance,
pursuing fast learning (T2), agent may sacrifice good general learning (T1)
bases generalizations examples. addressed tradeoffs
evaluation Instructo-Soar.

4. Related Work

Although extensive research agents learn tutorial instruction per se, learning instruction-like input long-time goal AI (Carbonell,
279

fiHuffman & Laird

Problem Requirement
Mapping M1 Mapping representable
information
Interaction I1 Flexible initiation instruction
I2 Flexibility instructional knowledge
content
I3 Situation exibility
implicitly situated
explicitly situated single utterance
explicitly situated multiple utterance
Transfer T1 General learning specific cases

T2
T3
T4
T5
T6
T7

Instructo-Soar?



(as needed show
capabilities)

(only agent-initiated)
partial (command exibility)

partial
yes
yes

yes
(via situated explanation)
Fast learning
yes
(new procedures
taught once)
Maximal use prior knowledge
yes
Incremental learning
yes
Knowledge-type exibility
yes
(learns PSCM
knowledge types)
Ability deal incorrect knowledge
(only extending incomplete knowledge)
Learning instruction coexisting
(not demonstrated)
learning sources

Table 1: requirements exible tutorable agent, Instructo-Soar's performance them.
Michalski, & Mitchell, 1983; McCarthy, 1968; Rychener, 1983). Early non-interactive systems learned declarative, ontological knowledge language (Haas & Hendrix, 1983; Lindsay, 1963), simple tasks unsituated descriptions (Lewis, Newell, & Polk, 1989; Simon,
1977; Simon & Hayes, 1976), task heuristics non-operational advice (Hayes-Roth,
Klahr, & Mostow, 1981; Mostow, 1983).
work concentrated behaving based interactive natural language instructions. SHRDLU (Winograd, 1972) performed natural language commands small
amount rote learning { e.g., learning new goal specifications directly transforming
sentences state descriptions. recent systems act response language
(concentrating mapping problem) minimal learning include SONJA
(Chapman, 1990), AnimNL (DiEugenio & Webber, 1992), Homer (Vere & Bickmore,
1990).
recent work focused learning situated natural language instructions. Martin Firby (1991) discuss approach interpreting learning
elliptical instructions (e.g., \Use shovel") matching instruction expectations
generated task execution failure. Alterman et al.'s FLOBN (Alterman, Zito-Wolf, &
Carpenter, 1991; Carpenter & Alterman, 1994) searches instructions environment
280

fiFlexibly Instructable Agents

order operate devices. FLOBN learns relating device's instructions known
procedures operating similar devices. systems target learning exible
interactive instructions types instructions imperatives, however.
bulk work learning instruction-like input rubric
learning apprentice systems (LASs), closely related programming-by-demonstration
(PbD) systems (Cypher, 1993) { employed, instance, recent work learning
within software agents (Dent et al., 1992; Maes, 1994; Maes & Kozierok, 1993; Mitchell,
Caruana, Freitag, McDermott, & Zabowski, 1994). systems learn interacting
expert; either observing expert solving problems (Cypher, 1993; Donoho & Wilkins,
1994; Mitchell et al., 1990; Redmond, 1992; Segre, 1987; VanLehn, 1987; Wilkins, 1990),
attempting solve problems allowing expert guide critique decisions
made (Golding et al., 1987; Gruber, 1989; Kodratoff & Tecuci, 1987b; Laird
et al., 1990; Porter, Bareiss, & Holte, 1990; Porter & Kibler, 1986). LAS learned
particular types knowledge: e.g., operator implementations (Mitchell et al., 1990), goal
decomposition rules (Kodratoff & Tecuci, 1987b), operational versions functional goals
(Segre, 1987), control knowledge control features (Gruber, 1989), procedure schemas (a
combination goal decomposition control knowledge) (VanLehn, 1987), useful macrooperations (Cypher, 1993), heuristic classification knowledge (Porter et al., 1990; Wilkins,
1990), etc.
Tutorial instruction exible type instruction supported past
LASs, three reasons. First, instructor may command unknown tasks tasks
unachieved preconditions agent instruction point (command exibility). Past
LASs limit input particular commands/observations particular times (e.g., commanding observing directly executable actions) typically allow unknown
commands all. Second, tutorial instruction allows use explicitly situated instructions (situation exibility), teach contingencies may present
current situation; past LASs not. Third, tutorial instruction requires types
task knowledge learned (knowledge-type exibility). Past LASs learn subset
types knowledge use perform tasks.

5. Theory Learning Tutorial Instruction

theory learning tutorial instruction consists learning framework, situated
explanation, placed within computational model general agenthood, problem space
computational model. first describe computational model learning
framework.

5.1 Problem Space Computational Model

computational model (CM) \a set operations entities interpreted
computational terms" (Newell et al., 1990, p. 6). computational model general
instructable agent must meet two requirements:
1. Support general computation/agenthood.
2. Close correspondence knowledge level. tutorial instructions
provide knowledge knowledge level (Newell, 1981), CM com281

fiHuffman & Laird

ponents knowledge level, dicult mapping learning
instructions be. addition, close correspondence knowledge level
allow us use CM identify types knowledge agent uses.
Many potential CMs ruled requirements. Standard programming languages (e.g., Lisp) theoretical CMs Turing machines push-down automata
support general computation, operations constructs symbol level,
without direct correspondence knowledge level. Similarly, connectionist neural
network models computation (e.g., Rumelhart & McClelland, 1986) employ (by design)
computational operations entities level far knowledge level. Thus,
models appropriate top-level CM instructable agent. However, higher levels description computational system implemented lower levels
(Newell, 1990), CMs might used implementation substrate higher
level CM instructable agent.
Another alternative logic, entities well matched knowledge
level (e.g., propositions, well-formed formulas). Logics specify set legal operations
(e.g., modus ponens), thus defining space possibly computed. However,
logic provides notion computed; is, logics alone specify
control logical operations' application. desirable CM instructable
agent include control knowledge, control knowledge crucial type knowledge
general agenthood, communicated instructions.
Since one goals identify agent's knowledge types, might appear
selecting theory knowledge representation would appropriate selecting
computational model. theories define functions structures used represent knowledge (e.g., KL-ONE, Brachman, 1980); define possible content
structures (e.g., conceptual dependency theory, Schank, 1975; CYC, Guha & Lenat,
1990). However, computational structure must added theories produce working agents. Thus, rather alternative specifying computational model, theory
knowledge representation addition. content theory knowledge representation would provide fine-grained breakdown knowledge learned
instructable agent within category knowledge specified CM.
employed particular content theory work thus far, however.
computational model adopted called problem space computational model
(PSCM) (Newell et al., 1990; Yost, 1993). PSCM general formulation computation knowledge-level agent, many applications built within (Rosenbloom, Laird, & Newell, 1993a). specifies agent terms computation within
problem spaces, without reference symbol level structures used implementation.
components approximate knowledge level (Newell et al., 1990), PSCM
apt choice identifying agent's knowledge types. Soar (Laird, Newell, & Rosenbloom, 1987) symbol level implementation PSCM.
schematic PSCM agent shown Figure 2. Perception motor modules
connect agent external environment. PSCM agent reaches goal moving
sequence states problem space. progresses toward goals sequentially
applying operators current state. Operators transform state, may produce
motor commands. PSCM, operators powerful simple STRIPS operators
(Fikes, Hart, & Nilsson, 1972), perform arbitrary computation (e.g.,
282

fiFlexibly Instructable Agents

external environment

perceptual modules

motor modules

Figure 2: processing PSCM-based agent. Triangles represent problem spaces;
squares, states; arrows, operators; ovals, impasses.
include conditional effects, multiple substeps, reactivity different situations, etc.).
PSCM agent reaches impasse immediately available knowledge
sucient either select fully apply operator. occurs, another problem
space context { subgoal { created, goal resolving impasse. second
context may impasse well, causing third context arise, on.
computational entities PSCM mediated agent's knowledge
states operators. small set basic PSCM-level operations entities
agent performs:
1. State inference. Simple monotonic inferences always applied
made without using PSCM operator. inferences augment agent's representation state inferring state properties based state properties
(including delivered perception). instance, agent might know
block held gripper closed positioned directly block.
2. Operator selection. agent must select operator apply, given current
state. process involves two types knowledge:
2.1. Proposal knowledge: Indicates operators deemed appropriate current situation. knowledge similar \precondition" knowledge simple STRIPS
operators.
2.2. Control knowledge: Orders proposed operators; e.g., \A better B"; \C
best"; \D rejected."
283

fiHuffman & Laird

3. Operator application. selected, operator may applied directly,
indirectly via subgoal:
3.1. Operator effects. operator applied directly current problem space.
agent knowledge effects operator state motor
commands produced (if any).
3.2. Sub-operator selection. operator applied reaching impasse selecting operators subgoal. Here, knowledge apply operator selection
knowledge (2, above) sub-operators.
4. Operator termination. operator must terminated application
completed. termination conditions, goal concept (Mitchell et al., 1986),
operator indicate state conditions operator meant achieve.
example, termination conditions pick-up(blk) might blk held
arm raised.6
functions performed agent using knowledge; thus, define set
knowledge types present within PSCM agent. knowledge types (five types total)
summarized Table 2. Soar implementation PSCM, knowledge
within Soar agents types.
Soar's implementation PSCM, learning occurs whenever results returned
subgoal resolve impasses. learning process, called chunking, creates rules
(called chunks) summarize processing subgoal leading creation
result. Depending type result, chunks may correspond five types
PSCM knowledge. similar situations arise future, chunks allow impasse
caused original subgoal avoided producing results directly. Chunking
form explanation-based learning (Rosenbloom & Laird, 1986). Although
summarization mechanism, taking inductive deductive steps subgoals,
chunking produce inductive deductive learning (Miller, 1993; Rosenbloom
& Aasman, 1990). Chunking occurs continuously, making learning part ongoing
activity Soar/PSCM agent.
PSCM clarifies task instructable agent: must able learn
five types PSCM knowledge instruction. next section discusses learning
process itself.

5.2 Learning Instructions Situated Explanation

Learning instruction involves analytic learning (learning based prior knowledge) inductive learning (going beyond prior knowledge). Analytic learning needed
agent must learn instructions combine known elements { e.g., learning pick objects combining known steps pick particular object. agent's
prior knowledge elements used produce better faster learning. Inductive learning needed agent must learn new task goals domain knowledge
6. PSCM operators explicit termination knowledge string conditional
effects take place time, respond (or wait for) external environment, etc.
STRIPS operators, contrast, need explicit termination knowledge, defined
single list effects, \terminated" definition applying effects.

284

fiFlexibly Instructable Agents

Entity Knowledge type Example
state
inference
gripper closed & directly obj ! holding obj.
operator proposal
goal pick obj table-x, docked tablex, propose moving table-x.
operator control
goal pick small metal obj table-x, prefer moving table-x fetching magnet.
operator effects
effect operator move table-x robot
becomes docked table-x.
operator termination
Termination conditions pick obj gripper
raised & holding obj.
Table 2: five types knowledge PSCM agents.
beyond scope prior knowledge. goal research produce
powerful analytic inductive techniques, rather specify techniques
come together produce variety learning variety instructional situations faced
instructable agent. resulting approach called situated explanation.
Instruction requirements T1 T3 specify general learning (T1) must occur
single, specific examples (T2), making maximal use prior knowledge (T3).
requirements bode strongly learning approach based explanation. use
explanation produce general learning common theme machine learning (e.g.,
DeJong & Mooney, 1986; Fikes et al., 1972; Minton, Carbonell, Knoblock, Kuokka, Etzioni,
& Gil, 1989; Rosenbloom, Laird, & Newell, 1988; Schank & Leake, 1989; many others)
cognitive science (Anderson, 1983; Chi, Bassok, Lewis, Reimann, & Glaser, 1989; Lewis,
1988; Rosenbloom & Newell, 1986). Forming explanations enables general learning
specific cases (requirement T1) explanation indicates features case
important generalized. Learning explaining typically requires
single example (requirement T2) prior knowledge employed construct
explanation (requirement T3) provides strong bias allows fast learning.
Thus, use explanation-based method core learning instruction approach, fall back inductive methods explanation fails. standard
explanation-based learning, explaining reasoning step involves forming \proof" (using
prior knowledge) step leads current state reasoning toward current
goal. proof path reasoning current state goal, step
explained, diagrammed Figure 3. General learning produced forming
rule includes causally required features state, goal, step appearing
proof; features appear generalized away.
Figure 3 indicates three key elements explanation: step explained,
endpoints explanation (a state goal G reached), steps
required complete explanation. form elements explanation take
situated explanation instruction?


Step explained. situated explanation, step explained individual
instruction given agent.
285

fiHuffman & Laird

reasoning step
explained
(indicated instruction I)

steps, agents knowledge

...



G

(Mk )

Figure 3: Caricature explanation reasoning step applies situation starting
state , goal G achieved.





Alternatively, entire instruction episode { e.g., full sequence instructions
new procedure { could explained once. Applying explanation single steps
results knowledge applicable step (as Golding et al., 1987; Laird et al.,
1990); explaining full sequences reasoning steps results learning schemas
encode whole reasoning episode (as Mooney, 1990; Schank & Leake, 1989; VanLehn, 1987). Learning factored pieces knowledge rather monolithic schemas
allows reactive behavior, since knowledge accessed locally based current situation (Drummond, 1989; Laird & Rosenbloom, 1990). meshes
PSCM's local control structure. Explaining individual instructions supported
psychological results self-explanation effect, shown subjects
self-explain instructional examples re-deriving individual lines example. \Students virtually never ect overall solution try recognize
plan spans lines" (VanLehn Jones, 1991, p. 111).
Endpoints explanation. endpoints explanation { state goal G
achieved { correspond situation instruction applies to. Situation
exibility (requirement I3 ) stipulates situation may either current
state world goal pursued hypothetical situation specified explicitly instruction. instruction specify situational
features implicitly situated, applies agent's current situation. Alternatively, instruction specify features G, making two kinds explicitly
situated instructions. example, \If light on, push button" indicates
hypothetical state light on; \To turn machine, ip switch" indicates
hypothetical goal turning machine. situation [S; G] produced
instruction, based current task situation situation features
instruction specifies.
required steps. complete explanation instruction, agent must
bring prior knowledge bear complete path instruction
achievement situation goal. PSCM agent's knowledge applies current
situation select apply operators make inferences. explaining
instruction , knowledge applied internally situation [S; G] associated
. is, explanation takes form forward internal projection within
situation. depicted Figure 3, agent \imagines" state ,
runs forward, applying instructed step knowledge
subsequent states/operators. knowledge includes normally used
286

fiFlexibly Instructable Agents

external world knowledge operators' expected effects used
produce effects projected world. G reached within projection,
projected path , step instructed , G comprises
explanation . indicating features , , G causally required
success, explanation allows agent learn general knowledge (as
standard EBL, realized agent Soar's chunking mechanism, Rosenbloom &
Laird, 1986). However, agent's prior knowledge may insucient, causing
incomplete explanation, described below.
Combining elements produces approach learning tutorial instruction
conceptually quite simple. instruction received, agent first
determines situation meant apply to, attempts explain
step indicated leads goal achievement situation (or prohibits it, negative
instructions). explanation made, produces general learning knowledge
IK indicating key features situation instruction cause success.
explanation cannot completed, indicates agent missing one
pieces prior knowledge MK (of PSCM type) needed explain instruction.
Missing knowledge (in Figure 3, missing arrows) causes incomplete explanation precluding achievement G projection. instance, agent may know key
effect operator, crucial state inference, needed reach G. radically,
action commanded may completely unknown thus inexplicable.
shown Figure 4, four general options learning agent might follow
cannot complete explanation. (O1) could delay explanation later,
hope missing knowledge (MK ) learned meantime. Alternatively,
(O2-O3) could try complete explanation somehow learning missing
knowledge. missing knowledge could learned (O2) inductively (e.g., inducing
\gap" explanation, described VanLehn, Jones & Chi, 1992, many
others), or, (O3) instructable agent's case, instruction. Finally, (O4)
could abandon explanation altogether try learn desired knowledge another
way instead.
Given incomplete explanation, would dicult choose option
follow. Identifying missing knowledge MK general case dicult credit assignment problem (with algorithmic solution), nothing incomplete
explanation predicts whether MK learned later explanation delayed. Thus, past machine learning systems responded incomplete explanations
either single way, multiple ways, tried fixed sequence.
Many authors (Bergadano & Giordana, 1988; Hall, 1988; VanLehn, 1987; VanLehn, Jones,
& Chi, 1992; Widmer, 1989), instance, describe systems make inductions complete incomplete explanations (option O2). diculty determining missing
knowledge, systems either base induction multiple examples, and/or bias
induction underlying theory teacher's help. SIERRA (VanLehn, 1987),
example, induces multiple partially explained examples, constrains induction
requiring examples unexplainable piece missing knowledge (the disjunct, SIERRA's terminology). SWALE (Schank & Leake,
1989) uses underlying theory \anomalies" explanations complete incomplete explanations events. OCCAM (Pazzani, 1991b) uses options O2 O4 static order:
287

fiHuffman & Laird

delay explanation Mk learned

O1

delay:


instruction
context








G

incomplete explanation
(missing knowledge Mk)





k

G

learn Mk inductively complete explanation

O2
k

k

?

induce





G


k

learn Mk instruction complete explanation

O3









k

G



G

abandon explanation (learn another way)

O4







k

G

Figure 4: Options faced incomplete explanation missing knowledge
MK .
first attempts fill gaps incomplete explanation inductively, biased
naive theory; fails, abandons explanation falls back correlational learning
methods. PET (Porter & Kibler, 1986) example system delays explanation
reasoning step learns knowledge (option O1).
However, indicated Figure 4, instructable agent additional information
available besides incomplete explanation itself. Namely, instructional context
(that is, type instruction place within dialogue) often indicates
option appropriate given incomplete explanation. Thus, situated explanation includes four options dynamically selects based
instructional context. situated explanation instruction situation [S; G],
missing knowledge MK precludes completing explanation learn knowledge IK ,
options O1-O4 take following form:

O1. Delay explanation later. instructional context indicate likelihood missing knowledge MK learned later. instance, instruction
given teaching new procedure cannot immediately explained re-

maining steps procedure unknown, known later (assuming
instructor completes teaching procedure). cases, agent discards
current, incomplete explanation simply memorizes 's use [S; G] (rote learning). Later, MK learned, recalled explained [S; G], causing IK
learned.
288

fiFlexibly Instructable Agents

Given instruction knowledge IK learned:
Determine situation [S; G] (current hypothetical) applies

Explain [S; G] forward projecting !; ; G
! Success (G met): learn IK complete explanation ( EBL).
! Failure: missing knowledge MK . Options:
O1. Delay explanation later.
O2. Induce MK , completing explanation.
O3. Take instruction learn MK , completing explanation.
O4. Abandon explanation; instead, learn IK inductively.
Table 3: Situated explanation.

O2. Induce MK , completing explanation. cases, instructional context
localizes missing knowledge MK part particular operator. instance,

purpose clause instruction (\To X, Y") suggests single operator
cause X occur. localization tightly constrains \gap"
incomplete explanation, agent use heuristics induce strong guess
MK needed span gap. Inducing MK allows explanation
completed IK learned.
O3. Take instruction learn MK , completing explanation. default response agent (when options deemed appropriate) ask
instructor explain further. instruction teach agent MK .
Again, learning MK allows explanation completed IK learned.
O4. Abandon explanation learn IK another way. instructional
context indicate missing knowledge MK would dicult learn.
occurs either instructor refuses give information asked
to, agent projected multiple operators may missing pieces
knowledge (multiple potential MK s). Since unknown whether MK ever
acquired, agent abandons explanation altogether. Instead, attempts
learn IK directly (using inductive heuristics), without explanation base
learning on.
options made clearer examples presented following sections.
Situated explanation summarized Table 3. Unlike knowledge acquisition approaches, include explicit check consistency newly learned knowledge
added agent's knowledge base. Kodratoff Tecuci (1987a) point out, techniques situated explanation biased toward consistency acquire
new knowledge current knowledge insucient, use current knowledge
deriving new knowledge. However, domains, explicit consistency checks (such
used Wilkins' (1990) ODYSSEUS) may required.
Situated explanation meets requirement learning incremental (T4)
occurs ongoing processing agent adds new pieces knowledge
289

fiHuffman & Laird

T1
T2
T3
T4
T5

I2
I3

General learning specific cases
Fast learning (each task instructed once)
Maximal use prior knowledge
Incremental learning
Knowledge-type exibility
a. state inference
b. operator proposal
c. operator control
d. operator effects
e. operator termination
Command exibility
a. known command
b. skipped steps
c. unknown command
Situation exibility
a. implicitly situated
b. explicitly situated: hypothetical state
hypothetical goal

Table 4: Expanded requirements tutorial instruction met Instructo-Soar.
agent's memory modular way. local control structure PSCM allows new
knowledge added independent current knowledge. con ict
pieces knowledge (for example, proposing two different operators situation),
impasse arise reasoned resolved instruction.

6.

Instructo-Soar

Instructo-Soar instructable agent built within Soar { thus, PSCM {
uses situated explanation learn tutorial instruction.7 Instructo-Soar engages

interactive dialogue instructor, receiving natural language instructions
learning perform tasks extend knowledge domain. section
next describe Instructo-Soar meets targeted requirements tutorial instruction,
shown expanded form Table 4. section describes system's basic
performance learning new procedures, extending procedures new situations,
imperative commands (implicitly situated instructions); next describes learning
types knowledge handling explicitly situated instructions.

7. overview Soar, systems built within it, see (Rosenbloom, Laird, & Newell, 1993b).

290

fiFlexibly Instructable Agents

Figure 5: robotic domain Instructo-Soar applied.

6.1 Domain Agent's Initial Knowledge
primary domain Instructo-Soar applied simulated robotic
world shown Figure 5.8 agent simulated Hero robot, room tables,
buttons, blocks different sizes materials, electromagnet, light. magnet
toggled closing gripper around it. red button toggles light off; green
button toggles dim bright, on.
Instructo-Soar consists set problem spaces within Soar contain three
main categories knowledge: natural language processing knowledge, originally developed
NL-Soar (Lewis, 1993); knowledge obtaining using instruction; knowledge task domain itself. task knowledge extended learning
instruction. Instructo-Soar expand natural language capabilities per se
takes instruction, although learn sentences map onto new operators
learns. complete, noiseless perception world, recognize set basic
object properties (e.g., type, color, size) relationships (e.g., robot docked-at table,
8. techniques applied limited way ight domain (Pearson, Huffman, Willis,
Laird, & Jones, 1993), Soar controls ight simulator instructions given taking off.

291

fiHuffman & Laird

Pick red block.
Move yellow table.
Move arm red block.
Move up.
Move down.
Close hand.
Move up.
operator finished.

Figure 6: Instructions given Instructo-Soar teach pick block.
gripper holding object, objects above, directly-above, left-of, right-of one another).
set properties relations extended instruction, described below.
agent begins knowledge set primitive operators map
natural language sentences, execute. include moving tables, opening
closing hand, moving arm up, down, above, left of, right things.
agent internally project operators. However, effects
various conditions unknown. instance, agent know operators
affect light magnet, magnet attract metal objects. Also, agent
begins knowledge complex operators (that involve combinations primitive
operators), picking arranging objects, pushing buttons, etc.

6.2 Learning New Procedures Delayed Explanation
Instructo-Soar learns new procedures (PSCM operators) instructions

shown Figure 6, picking block. Since \pick up" known procedure
initially, told \Pick red block," agent realizes must learn new
operator.
perform PSCM operator, operator must selected, implemented, terminated. select operator future based command requires knowledge
operator's argument structure (a template), natural language maps
structure. Thus, learn new operator, agent must learn four things:
1. Template: Knowledge operator's arguments instantiated.
picking blocks, agent acquires new operator single argument,
object picked up.
2. Mapping natural language: mapping natural language semantic
structures instantiation new operator, operator selected
commanded future. picking blocks, agent learns map
semantic object \Pick ..." single argument new operator template.
3. Implementation: perform operator. New operators performed
executing sequence smaller operators. implementation takes form
selection knowledge sub-operators (e.g., move proper table, move
arm, etc.)
292

fiFlexibly Instructable Agents

4. Termination conditions: Knowledge recognize new operator achieved
{ goal concept new operator. \pick up," termination conditions
include holding desired block, arm raised.
Requirement T2 (\fast learning") stipulates first execution new procedure, agent must able perform least task without re-instructed.
Thus, agent must learn, form, four parts new operator
first execution.
general implementation new operator learned situated explanation
steps. first execution new operator, though, instructions
performing cannot explained, agent yet know goal
operator (e.g., agent know termination conditions \pick up")
steps following current one reach goal. However, instructional context
{ explaining instructed steps procedure learned { clear missing
knowledge remaining steps procedure's goal acquired later,
instructor expected teach procedure completion. Thus, agent delays
explanation (option O1) memorizes implementation instruction rote,
episodic form. end first execution new procedure, agent induces
procedure's goal { termination conditions { using set simple inductive heuristics.
later executions procedure, original instructions recalled explained
learn general implementation.
describe details process using \pick up" example.
6.2.1 First Execution

example, shown Figure 6, begins instruction \Pick red block."
agent comprehends instruction, producing semantic structure resolving \the red
block" block environment. However, semantic structure correspond
known operator, indicating agent must learn new operator (which
calls, say, new-op14). learn template new operator, agent simply assumes
argument structure command used request operator required
argument structure operator itself. case, template new operator
generated argument structure directly corresponds semantic arguments
\pick up" command (here, one argument, object). agent learns mapping
semantic structure new operator's template, used presented
similar requests future. simple approach learning templates mappings
sucient imperative sentences direct arguments, fail commands
complex arguments, path constraints (\Move dynamite room,
keeping far heater possible").
Next, new operator selected execution. Since implementation unknown,
agent immediately reaches impasse asks instructions. instruction
Figure 6 given, comprehended executed turn. instructions provide
implementation new operator. implicitly situated { applies
current situation agent finds itself.
point, agent may given another command cannot directly completed { one requests either another unknown procedure known procedure
293

fiHuffman & Laird

instruction
explained

steps

...



?
?
?

G?

Figure 7: Instructions teaching new operator cannot explained termination
conditions new operator learned.
agent know perform current situation due skipped steps.
command exibility (requirement I2). example, within instructions \pick up,"
command \Move red block" cannot completed skipped step
(the arm must raised move something). impasse arises instructor
indicates needed step (\move up"), continues instructing \pick up."
Ultimately, implementation new operator learned proper level
generality explaining instructed step. However, illustrated Figure 7,
initial execution forming explanation impossible, goal new
operator steps (further instructions) needed reach yet known.
Since missing pieces explanation expected available later, agent
delays explanation resorts rote learning instructed step.
Instructo-Soar, rote learning occurs side effect language comprehension.
reading sentence, agent learns set rules encode sentence's
semantic features. rules allow NL-Soar resolve referents later sentences, implementing simple version Grosz's focus space mechanism (Grosz, 1977). rules record
instruction, indexed goal applies place instruction
sequence. result essentially episodic case records specific, lock-step sequence instructions given perform new operator. instance, recorded
\to pick-up (that is, new-op14) red block, rb1, first told move yellow table, yt1." course, information contained within case could generalized,
point generalization would purely heuristic, agent cannot
explain steps episode. Thus, Instructo-Soar takes conservative approach
leaving case rote form.
Finally, agent told \The operator finished," indicating goal
new operator achieved. instruction triggers agent learn termination
conditions new operator. Learning termination conditions inductive concept
formation problem: agent must induce features hold current
state imply positive instance new operator's goal achieved. Standard concept
learning approaches may used here, long produce strong hypothesis within
small number examples (due \fast learning" requirement, T2). Instructo-Soar
uses simple heuristic strongly bias induction: hypothesizes everything
changed initial state new operator requested current
state part new operator's termination conditions. case, changes
robot docked table, holding block, block gripper
air.
294

fiFlexibly Instructable Agents

heuristic gives reasonable guess, clearly simple. Conditions
changed may matter; e.g., perhaps doesn't matter picking blocks
robot ends table. Unchanged conditions may matter; e.g., learning build
\stoplight," block colors important although change. Thus, agent
presents induced set termination conditions instructor possible alteration
verification. instructor add remove conditions. example, \pick
up" case instructor might say \The robot need docked yellow table"
remove condition deemed unnecessary, verifying termination conditions.
Instructo-Soar performs induction EBL (chunking) overgeneral theory
make inductive leaps (similar to, e.g., Miller, 1993; Rosenbloom & Aasman, 1990;
VanLehn, Ball, & Kowalski, 1990). type inductive learning advantage
agent alter bias ect available knowledge. case, agent
uses instruction (the instructor's indications features add remove) alter
induction. knowledge sources could employed (but current
implementation) include analogy known operators (e.g., pick actions
domains), domain-specific heuristics, etc.
first execution new operator, then, agent:
Carries sequence instructions achieving new operator.
Learns operator template new operator.
Learns mapping natural language new operator.
Learns rote execution sequence new operator.
Learns termination conditions new operator.
Since agent learned necessary parts operator, able
perform task without instruction. However, since implementation
operator rote, perform exact task. learned generally
pick things yet.
6.2.2 Generalizing New Operator's Implementation

agent knows goal concept full (though rote) implementation sequence
new operator. Thus, information needs explain instruction
implementation sequence leads goal achievement, provided underlying domain
knowledge sucient.
instruction explained recalling episodic memory internally projecting effects rest path achievement termination conditions
new operator. projection \proof" instructed operator lead
goal achievement situation. Soar's chunking mechanism essentially computes
weakest preconditions situation instruction required success (similar
standard EBL) form general rule proposing instructed operator. rule learned
instruction \Move yellow table" shown Figure 8. rule generalizes
original instruction dropping table's color, specializes adding facts
table object sitting object small (only small objects
295

fiHuffman & Laird



goal new-op-14(?obj),
?obj table ?t, small(?obj),
robot docked ?t,
gripper status(open),
propose operator move-to-table(?t).
Figure 8: general operator proposal rule learned instruction \Move
yellow table" (new-op-14 newly learned \pick up" operator).
grasped gripper). rule tests gripper open,
condition important grasping block instructed case.9
learning general proposal rules step instruction sequence, agent
perform task without reference rote case. instance, asked \Pick
green block," agent selects new-op14, instantiated green block. Then,
general sub-operator proposal rules one Figure 8 fire one one, match
current situation, implement operator. performing implementation
steps, agent recognizes termination conditions met (the gripper raised
holding green block), new-op14 terminated.
Since general proposal rules implementing task directly conditional
state, agent perform task starting state along implementation
path react unexpected conditions (e.g., another robot stealing block).
contrast, rote implementation initially learned applied starting
original starting state, reactive steps conditional
current state.

6.3 Recall Strategies

described agent recalls explains step new operator's implementation sequence, operator's termination conditions induced.
still two open issues: (A) point learning termination conditions
agent perform recall/projection?, (B) many steps recalled
projected sequence time?
investigate issues, implemented two different recall/project strategies:
1. Immediate/complete recall. agent recalls attempts explain full
sequence instructions new operator immediately learning new
operator's termination conditions.
2. Lazy/single-step recall. agent recalls attempts explain single instructions sequence asked perform operator starting
initial state. is, point execution operator, agent
9. technical details Soar's chunking mechanism forms rules found (Huffman,
1994; Laird, Congdon, Altmann, & Doorenbos, 1993).

296

fiFlexibly Instructable Agents

recalls next instruction, attempts explain forward projecting it. However, projection result path goal achievement without
instructions recalled, rather recalling next instruction
sequence continue forward projection, agent gives explaining
instruction simply executes external world.
strategies represent extremes continuum strategies.10 strategy
use parameter agent; dynamically select strategies
running. possible extension would reason time pressure different
situations select appropriate strategy. Next, brie describe implications
recall strategy.
6.3.1 Immediate/Complete Recall Strategy

Immediate/complete recall explanation involves internally projecting multiple operators
(the full instruction sequence) immediately first execution new operator.
projection begins state agent new operator first suggested.
projection successfully achieves termination conditions new operator,
agent learns general implementation rules every step. advantage strategy
agent learns general implementation new operator immediately
first execution (e.g., agent pick objects right away).
strategy three important disadvantages. First, requires agent reconstruct initial state commanded perform new operator.
reconstruction may dicult amount information state large (although
small robotic domain used here).
Second, recall projection entire sequence instructed steps time-consuming,
requiring time proportional length instruction sequence. process,
agent's performance tasks hand suspended. suspension could awkward
agent pressure act quickly.
Third, illustrated Figure 9, multiple step projections susceptible compounding errors underlying domain knowledge. projection successive operator
begins state ects agent's knowledge effects prior operators
sequence. knowledge incomplete incorrect, state move
ecting actual effects prior operators. Minor domain knowledge problems knowledge individual operators, alone would produce error
single step explanation, may combine within projection cause error.
lead incomplete explanations (more rarely) spuriously successful explanations (e.g.,
reaching success early instruction sequence).
6.3.2 Lazy/Single-Step Recall Strategy

lazy/single-step recall strategy, agent waits recall explain instructions
asked perform new operator second time initial state. addition,
agent recalls single instruction internally project time. recalled
10. implemented lazy/complete recall strategy, described (see Huffman,
1994, details).

297

fiHuffman & Laird

opX
op1

S1

op1

op2

S2
S2

op3

S3

...

G tc

opX

Internally projected path
(reflecting agents incorrect
knowledge operator effects)

Sx

states reflecting
actual operator effects

Sx

states reflecting
projected operator effects

G tc

state meets
termination conditions
current goal

op2

S3
op3

... Sn (= G )
tc

Correct path (reflecting
actual operator effects)

Figure 9: Multiple step projections result incomplete explanations due compounding errors domain knowledge.
operator projected, agent applies whatever general knowledge rest
implementation new operator. general knowledge, however,
include rote memories past instructions. is, agent know
rest path complete new operator using general knowledge, recall
instructions sequence rote memories. Rather, internal projection
terminated single recalled operator applied external world.
strategy addresses three disadvantages immediate/complete strategy.
First, require reconstruction original instruction state; rather, waits
similar state occur again.
Second, recalling projecting single instruction time require timeconsuming introspection suspends agent's ongoing activity. \pick up,"
instance, Table 5 shows longest time agent's external action (movements
instruction requests) suspended using strategy (as measured Soar decision cycles,
last 35 milliseconds Instructo-Soar SGI R4400 Indigo).
immediate/complete strategy external actions 304 decision cycles (about
11 seconds Indigo) immediately following first execution, order recall
explain complete instruction sequence. Using lazy/single-step strategy,
one instruction ever recalled/explained time action taken world;
thus, longest time without action 75 decision cycles (about 2 seconds).
total recall/explanation time proportional length instruction sequence
cases (304 vs. 294 decision cycles), lazy/single-step strategy, time
interleaved execution instructions rather fully taken first
execution.
Third, lazy/single-step strategy overcomes problem compounding domain
theory errors beginning projection instruction current state
world external execution previous instructions. Thus, beginning state
projection correctly ects effects previous operators implementation
sequence.
major disadvantage strategy requires number executions
new operator equal length instruction sequence order learn whole
298

fiFlexibly Instructable Agents

Immediate/complete Lazy/single-step
Largest time without external action 304
75
Largest total recall/explanation time 304 (end 1st exec'n) 294 (during 2nd exec'n.)
execution

Decision cycles (log scale)

Table 5: Timing comparison, Soar decision cycles, learning \pick up" using immediate/complete lazy/single-step recall strategies.
1000

(r = 0.98)
(r = 0.98)

1000

100

100

10
1

10

Execution number (log scale), "pick up"
(a)

1

10

Execution number (log scale), "move left of"
(b)

Figure 10: Decision cycles versus execution number learn (a) pick (b) move
objects left one another, using lazy/single-step strategy.
general implementation. limiting recall single step allows single
sub-operator per execution generalized. disadvantage, however, leads two
interesting learning characteristics:
Back-to-front generalization. Generalized learning starts end implementation sequence moves towards beginning. second execution
new operator, path goal known last instruction sequence
(it leads directly goal completion), general proposal instruction
learned. third execution, second last instruction projected,
proposal learned previously last operator applies, leading goal achievement
allowing general proposal second last instruction learned.
pattern continues back entire sequence full implementation
learned generally. Figure 10 shows, resulting learning curves closely approximate power law practice (Rosenbloom & Newell, 1986) (r = 0:98 (a)
(b)).
Effectiveness hierarchical instruction. Due back-to-front effect,
agent learns new procedure quickly steps taught using hierarchical organization taught sequence. Figure 11 depicts
at, nine-step instruction sequence teaching Instructo-Soar move one block
299

fiHuffman & Laird

1
move left of(block, block2)

6

4

2

move arm

move table (table)
3

graphical

10

moveleftof(arm,block2)

7

5

move (block)

Figure 11:

8

move arm

close gripper

view

move arm

move table (table)





move-left-of(block,block2).

open gripper

9



instruction

sequence



1
move left of(block, block2)

2

11

9
moveleftof(arm,block2)

pick (block)

put (block)

10
move table (table)
3

4
move table (table)

8

6

5
move (block)

Figure 12:

12
move arm

grasp (block)

graphical

move arm

13
open gripper

7
move arm

view

close gripper



hierarchical instruction sequence
move-left-of(block,block2). New operators shown bold.



left another; Figure 12 depicts hierarchical instruction sequence procedure, contains 13 instructed steps, maximum 3 subsequence.
breaking instruction sequence shorter subsequences, hierarchical organization allows multiple subtrees hierarchy generalized execution.
General learning N step operator takes N executions using
instruction
HpN subtasks
sequence. Taught hierarchically


H
-level
hierarchy

p
subsequence, H H N executions required full generalization. hierarchy Figure 12 irregular structure, results apspeedup
length every subsequence small (in case, smaller N ). Empirically,
sequence Figure 11 takes nine (N ) executions generalize, whereas hierarchical sequence takes six. Hierarchical organization additional advantage
operators learned used future instructions.
300

fiFlexibly Instructable Agents

6.4 Supporting Command Flexibility

Command exibility (requirement I2 ) stipulates instructor may request either
unknown procedure, known procedure agent know perform
current state (skipping steps), point. lead multiple levels embedded
instruction. seen, Instructo-Soar learns completely new procedures
instructions unknown commands. addition, agent asked perform
known procedure unfamiliar situation { one agent know
step take { learns extend knowledge procedure situation.
example contained instructions \Pick red block," agent
asked \Move red block." agent knows perform operator
arm raised. However, case arm lowered, agent reaches
impasse asks instruction.11 told \Move up," agent internally
projects raising arm, allows achieve moving red block.
projection learns general rule: move arm trying move object
table agent docked at. rule extends \move above" procedure
cover situation.
operator { even one previously learned instruction { may require extension
apply new situation. agent learns general implementation
new operator, reason possible situations operator
might performed, limits explanations series situations arises
actual execution new operator learned.
Newly learned operators may included instructions later operators, leading
learning operator hierarchies. One hierarchy operators learned Instructo-Soar
shown Figure 13. Learning procedural hierarchies identified fundamental
component children's skill acquisition tutorial instruction (Wood, Bruner, & Ross,
1976). learning hierarchy Figure 13, Instructo-Soar learned four new operators, extension known operator (move above), extension new operator
(extending \pick up" work robot already holding block). command
exibility, hierarchy taught exponentially many different ways (Huffman, 1994). instance, new operators appear sub-operators (e.g., grasp)
taught either teaching higher operators (e.g., pick up).

6.5 Abandoning Explanation Domain Knowledge Incomplete

general operator implementation learning described thus far depends explaining
instructions using prior domain knowledge (as opposed learning operator termination conditions, inductive). domain knowledge incomplete, making
explanation impossible? sequences multiple operators, pinpointing knowledge
missing extremely dicult credit assignment problem (sequences known contain
one operator, however, constrained case, described next section).
11. Another option would search; i.e., apply weak method means-ends analysis.
example, search would easy; cases, could costly. event, since goal
Instructo-Soar investigate use instruction, agent always asks instructions
reaches impasse task performance. Nothing Instructo-Soar precludes use search
knowledge sources, however.

301

fiHuffman & Laird

lineup(block1,block2,block3)

move left of(block2,block1)

pick (block)

move arm

moveleftof(arm,block1)

move table (table)

grasp (block)

put (blockX)

move left of(block3,block2)

put (block)

move arm

(lg. metal)

open gripper
grasp (magnet)

move (block)

move arm

(small)

move table (table)

move (blk/mag)

move arm

close gripper

move arm

Figure 13: hierarchy operators learned Instructo-Soar. Primitive operators
light print; learned operators bold.
general, explanation failure detected end projection instruction sequence could caused missing knowledge operator sequence.
Thus, faced incomplete explanation sequence multiple instructions,
Instructo-Soar abandons explanation instead tries induce knowledge directly
instructions (option O4).
example, consider case Instructo-Soar's knowledge secondary operator effects (frame axiom type knowledge) removed teaching procedure. example, although agent knows closing hand causes
status closed, longer knows closing hand around block causes block
held. Now, agent taught new procedure, pick red block.
first execution, agent attempts recall explain instructions usual,
fails missing knowledge. is, block picked
projection instructions, since agent's knowledge indicate held.
agent records fact procedure's instructions cannot explained.
Later, agent asked perform procedure, recalls instructions. However, recalls explaining instructions failed past. Thus,
abandons explanation instead attempts induce general proposal rule directly
instruction.12
12. Since incomplete explanation procedure may indicate effect(s) operator
instruction sequence unknown, another alternative (not yet implemented Instructo-Soar) would
agent observe effects operator sequence performed, comparing
observations effects predicted domain knowledge. differences would allow agent

302

fiFlexibly Instructable Agents

G: newop14 ("pick up")
object: <redblock>

<redblock> <yellowtable>

OP: movetotable
destination: <yellowtable>

Figure 14: use OP-to-G-path heuristic, OP \move yellow table,"
G \pick red block."
\pick up" example, agent first recalls command move yellow
table. learn proposal rule operator (call OP ), agent must induce set
conditions state performing OP contribute achieving \pick
up" goal (call G). Instructo-Soar uses two simple heuristics induce state
conditions:
OP-to-G-path. object Obj 1 filling slot OP , object Obj 2
attached G, include shortest existing path (heuristically length less
three) relationships Obj 1 Obj 2 set induced conditions.
heuristic captures intuition operator involves object,
relationship objects relevant goal probably important. Figure 14
shows operation \move yellow table." figure indicates,
path G's object, red block, destination OP , yellow table,
relationship block table.
OP-features-unachieved. termination condition (essentially, primary
effect) OP achieved state OP performed considered
important condition.
heuristic captures intuition primary effects OP probably
important; therefore, matters achieved OP selected.
example, OP 's primary effect robot ends docked table; thus,
fact robot initially docked table added inferred set
conditions proposing OP .
heuristics implemented Soar operators compute appropriate conditions. set conditions induced, presented instructor, add
remove conditions verifying them. Upon verification, rule learned proposing OP
(e.g., move-to-table(?t)) induced conditions hold (e.g., goal pick-up(?b),
?b isa block, on(?b,?t)). rule similar rule learned explanation (Figure 8), applies picking block (overspecific), stipulate
learn new operator effects could complete explanation procedure. Learning effects
operators observation explored number researchers (Carbonell & Gil, 1987;
Pazzani, 1991b; Shen, 1993; Sutton & Pinette, 1985; Thrun & Mitchell, 1993).

303

fiHuffman & Laird

object must small (overgeneral). similar induction occurs step \pick up,"
agent learns general implementation full \pick up" operator. However,
unless corrections made instructor, induced implementation correct
one learned explanation; instance, applies (wrongly) block instead
small object. complex domain, inferring implementation rules would
even less successful. surprisingly, psychological research shows human subjects'
learning procedural instructions degrades lack domain knowledge (Kieras
& Bovair, 1984).
Returning targeted instruction requirements Table 4, Instructo-Soar's learning procedures illustrates (T1) general learning specific instructions, (T2) fast learning (because procedure need instructed once) (T3) using prior domain
knowledge construct explanations, (T4) incremental learning agent's ongoing performance. Two types PSCM knowledge learned: (T5(b)) operator proposals
sub-operators procedure, (T5(e)) procedure's termination conditions.
learning involves either delayed explanation, domain knowledge inadequate,
abandoning explanation favor simple induction. instructions (I3 (a)) implicitly situated imperative commands, either (I2(a)) known procedures, (I2(b)) known
procedures steps skipped, (I2(c)) unknown procedures.

7. Beyond Imperative Commands

Next, turn learning remaining types PSCM knowledge (T5(a,c,d)) various
kinds explicitly situated instructions (I3(b)). explicitly situated instruction,
Instructo-Soar constructs hypothetical situation (goal state) includes
objects, properties, relationships mentioned explicitly instruction well
features current situation needed carry instruction.13
hypothetical situation used context situated explanation instruction.

7.1 Hypothetical Goals Learning Effects Operators

goal explicitly specified instruction purpose clause (DiEugenio, 1993): \To
X, Y." basic knowledge learned instruction operator
proposal rule goal achieve X.
Consider example Instructo-Soar's domain:
> turn light, push red button.

agent taught push buttons, know red button's
effect light. purpose clause instruction example, agent creates
hypothetical situation goal stated purpose clause (here, \turn light"),
state current state, goal achieved (here, light off).
Within situation, agent attempts explain instruction forward projecting
action pushing red button.
agent knew pushing red button toggles light, projection,
light would come on. Thus, explanation would succeed, general operator
13. See (Huffman, 1994) details features determined.

304

fiFlexibly Instructable Agents

proposal rule would learned, proposed pushing red button light
goal turn on.
However, since actuality agent missing knowledge (MK ) pushing
button affects light, light come within projection. explanation
incomplete.
Instructo-Soar's explanation sequence operators fails, agent
try induce missing knowledge needed complete explanation,
could associated multiple operators. Rather, explanation simply
abandoned, described Section 6.5. However, case, unexplainable sequence
contains one operator. addition, form instruction gives agent
strong expectation operator's intended effect. Based purpose clause,
agent expects specified action (pushing button) cause achievement
specified goal (turning light). DiEugenio (1993) found empirically type
expectation holds 95% naturally occurring purpose clauses.
expectation constrains \gap" incomplete explanation: state
pushing button state light on, one action performed
produce effect. Based constrained gap, agent attempts induce missing
knowledge MK order complete explanation (option O2). straightforward
inference MK simply unknown effect single action produce
expected goal conditions { e.g., pushing button cause light come on.
instructor asked verify inference.14
verified, Instructo-Soar heuristically guesses state conditions
effect occur. uses OP-to-G-path heuristic naive causality
theory (Pazzani, 1991a) guess causes inferred operator effect. Here, OP-toG-path notices light red button table. addition,
agent includes fact inferred effect hold (the light off)
operator caused it. result presented instructor:
think push button causes:
light
following conditions:
light currently on, light table, button table
right conditions?
Here, heuristics recognized matters button pushed (the red
one). instructor add condition saying ``The button must red.''
instructor verifies conditions, agent adds new piece operator effect
knowledge memory:


projecting push-button(?b),
?l isa light status off, table ?t,
?b isa button color red, table ?t,
light ?l status on.

14. inference rejected, agent abandons explanation directly induces proposal rule
pushing button instruction, described Section 6.5.

305

fiHuffman & Laird

Immediately learned, rule applies light forward projection
current instruction. light comes on, completing instruction's explanation
achieving goal. explanation, agent learns proposal rule proposes
pushing red button goal turn light. Thus, agent acquired
new knowledge multiple levels; inferring unknown effect operator supported
learning proposal operator.
example illustrates (I3 (b)) use hypothetical goal instructions use
option O2 dealing incomplete explanations { inferring missing knowledge {
learn new operator effects (T5(d)), thus extending domain knowledge.

7.2 Hypothetical States Learn Contingencies

Instructors use instructions hypothetical states (e.g., conditionals: \If [state conditions], ...") either teach general policies (\If lights leave
room, turn off.") teach contingencies performing task. InstructoSoar handles these; here, describe latter.
contingency instruction indicates course action followed current
task performed future situation different current situation. Instructors
often use contingency instructions teach situations differ current
one crucial way alter agent's behavior. Contingency instructions
common human instruction; Ford Thompson (1986) found 79%
conditional statements instruction manual communicated contingency options
student.
Consider interaction:
> Grasp blue block.

That's new one me. that?

> blue block metal, pick magnet.

blue block made metal, instructor communicating were,
different course action would required.
conditional instruction \If blue block metal, pick magnet,"
agent needs learn operator proposal rule picking magnet appropriate conditions. agent begins constructing hypothetical situation
\pick magnet" applies. \If blue block metal" indicates hypothetical state
variant current state blue block material metal.
current goal (\Grasp blue block") goal hypothetical situation.
Within situation, agent projects picking magnet explain
allow block grasped. However, agent missing much knowledge needed
complete explanation. know goal concept \Grasp" yet, rest
instructions reach goal.
Since instruction explained contingency, rest instructions
agent given \Grasp blue block" may (and case, not) apply
contingent situation, block metal. normal grasp sequence,
instance, agent learns close hand around grasped object, grasping
metal object, hand closed around magnet. Since knowledge complete
306

fiFlexibly Instructable Agents

grasping metal object needed explain contingency instruction, agent
know might learn missing knowledge, abandons explanation
(option O4). Instead, uses heuristics described Section 6.5 directly induce
operator proposal rule \Grasp magnet." addition conditions generated
heuristics, conditions indicated antecedent instruction included.
result presented instructor alteration verification:
I'm guessing conditions \pick magnet"
goal \grasp block" are:
block metal
right?
> Right.

interaction agent learns rule proposes picking magnet
goal grasp metal block. learning completed, since agent yet
finished grasping blue block, continues receive instruction task.
contingencies indicated point. Learning contingencies illustrates (I3(b))
handling hypothetical state instructions.

7.3 Learning Reject Operators

final examples illustrate learning reject operator { type operator control
knowledge PSCM. examples detail remaining option dealing
incomplete explanations: (O3) completing explanation instruction.
Consider instructions:
> Never grasp green blocks.

Why?
(a) > Trust me.
(b) > Green blocks explosive.
negative imperative prohibits step applying hypothetical situation
might apply. Thus, Instructo-Soar creates hypothetical situation
prohibited action might executed; case, state graspable green block.
Since goal specified instruction, current goal, default
goal \maintaining happiness" (which always considered one agent's current goals)
used. hypothetical situation, agent internally projects \grasp" action,
expecting \unhappy" result. However, resulting state, agent grasping
green block, acceptable according agent's knowledge. Thus, projection
explain action prohibited.
agent deals incomplete explanation asking instruction,
attempt learn MK complete explanation. However, instructor decline
give information saying (a) Trust me. Although instructor provide MK , prohibition single operator (grasping green block)
explained, agent induce plausible MK complete explanation (option
O2). Since agent knows final state prohibited operator meant
\unhappy", simply induces state avoided. converse learning recognize desired goal reached (learning operator's termination
307

fiHuffman & Laird

conditions). agent conservatively guesses features hypothetical
state (here, green block held), taken together, make state
avoided. inference conservative, current implementation
instructor even asked verify it. state inference rule results follows:


goal ``happiness'',
?b isa block color green,
holding(gripper,?b),
state fails achieve ``happiness''.

rule applies final state projection \Never grasp..." state's failure
achieve happiness completes agent's explanation \Never grasp...,"
learns rule rejects proposed operator grasping green block.
Alternatively, instructor could provide instruction, (b) Green blocks
explosive. instruction provide missing knowledge MK needed complete incomplete explanation (option O3). (b), agent learns state inference
rule: blocks color green explosiveness high. Instructo-Soar learns state
inferences simple statements (b), conditionals (e.g., \If magnet
powered directly metal block, magnet stuck block") essentially translating utterance directly rule.15 state inference instructions
used introduce new features extend agent's representation vocabulary
(e.g, stuck-to).
rule learned \Green blocks explosive" adds explosiveness high
block agent simulated grasping hypothetical situation. agent knows
touching explosive object may cause explosion { negative result. negative
result completes explanation \Never grasp...," agent learns avoid
grasping objects explosiveness high.
Completing explanation instruction (as (b)) produce
general learning heuristically inferring missing knowledge (as (a)). (b),
agent later told Blue blocks explosive, avoid grasping well.
general, multiple levels instruction lead higher quality learning single level
learning based explanation composed strong lower-level knowledge
(MK ) rather inductive heuristics alone. MK (here, state inference rule)
available future use.
agent learned reject \grasp" operator recognize
bad state performing would lead to, agent recognize bad state
reached another path. instance, agent led individual
steps grasping explosive block without instructor ever mentioning \grasp."
agent finally asked \Close gripper" around explosive object, so,
immediately recognizes undesirable state arrived reverses
close-gripper action. process, learns reject close-gripper hand
around explosive object, future reach undesirable state
path.
15. translation occurs chunking, uninteresting way. Instructo-Soar use explanation learn state inferences. extension would try explain inference holds using
deeper causal theory.

308

fiFlexibly Instructable Agents

Notice effect situated nature Instructo-Soar's learning. agent
learns avoid operators lead bad state arise agent's
performance. initial learning bad state recognitional rather predictive.
Alternatively, agent first learns bad state, could extensive reasoning
determine every possible operator could lead state, every possible
previous state, learn reject operators appropriate times. unsituated
reasoning would expensive; agent would reason huge number
possible situations. addition, whenever new operators learned, agent would
reason possible situations could arise, learn
could ever led bad state. Rather costly reasoning, Instructo-Soar simply
learns situations arise.
Another alternative completely avoiding bad states would think
effects every action taking it, see bad state result. highly cautious
execution strategy would appropriate dangerous situations, appropriate
safer situations agent time pressure. (Moving less
cautious execution strategies currently implemented Instructo-Soar.)
\Never grasp..." examples illustrated agent's learning one type
operator control knowledge, namely operator rejection (T5(c)), learning state inferences
(T5(a)), use instruction complete incomplete explanations (option O3).
final category learning discuss second type operator control knowledge.

7.4 Learning Operator Comparison Knowledge
Another type control knowledge besides operator rejection rules operator comparison
rules, compare two operators express preference one given
situation. Instructo-Soar learns operator comparison rules asking instructor's
feedback multiple operators proposed point achieve particular
goal. Multiple operators proposed, instance, agent taught two
different methods achieving goal (e.g., pick metal block either using
magnet directly gripper). instructor asked either select one
proposed operators indicate action appropriate. Selecting one
proposed choices causes agent learn rule prefers selected operator
proposed operators situations current situation. Alternatively,
instructor indicates operator outside set proposed operators,
Instructo-Soar attempts explain operator usual way, learn general
rule proposing it. addition, agent learns rules preferring instructed operator
currently proposed operators.
two weaknesses Instructo-Soar's learning operator comparison rules.
First, instructor required indicate preference step needed complete procedure, rather simply choosing overall methods. is,
instructor cannot say \Use method grab block gripper, instead
using magnet," must indicate preference individual step method
employing gripper. PSCM, knowledge steps procedure
accessed independently, separate proposal rules, rather aggregate method.
Independent access improves exibility reactivity { agent combine steps
309

fiHuffman & Laird

different methods needed based current situation { higher level grouping
steps would simplify instruction selecting complete methods.
second weakness although agent uses situated explanation explain
selection instructor makes, explain selection better
possibilities. Preferences viable operators often based global considerations;
e.g., \Prefer actions lead overall faster/cheaper goal achievement." Learning based
type global preference (which turn may learned instruction)
point research.

8. Discussion Results
shown Instructo-Soar learns various kinds instructions. Although
domain used demonstrate behavior simple, enough complexity exhibit
variety different types instructional interactions occur tutorial instruction.
11 requirements tutorial instruction places instructable agent (listed
Table 1), Instructo-Soar meets 7 (listed expanded form Table 4) either fully partially. Three particular distinguish Instructo-Soar previous instructable
systems:






Command exibility: instructor give command task
instruction point, whether agent knows task perform
current situation.

Situation exibility: agent learn implicitly situated instructions
explicitly situated instructions specifying either hypothetical goals states.

Knowledge-type exibility: agent able learn types knowledge uses task performance (the five PSCM types) instruction.

Earlier, claimed handling tutorial instruction's exibility requires breadth
learning interaction capabilities. Combining command, situation, knowledge-type
exibility, Instructo-Soar displays 18 distinct instructional capabilities, listed Table 6. variety instructional behavior require 18 different learning techniques,
arises one general technique, situated explanation PSCM-based agent, applied
range instructional situations.
series examples illustrated situated explanation uses instruction's
situation context learning process. First, situation instruction applies provides endpoints attempting explain instruction. Second,
instructional context indicate option follow explanation cannot
completed. context learning new procedure indicates delaying explanation
(option O1) best, since full procedure eventually taught. step cannot
explained previously taught procedure, missing knowledge could anywhere
procedure, best abandon explanation (option O4) learn another way. Instructions provide explicit context, purpose clause, localize missing
knowledge giving strong expectations single operator achieve single
goal. localization makes plausible induce missing knowledge complete
310

fiFlexibly Instructable Agents

1.
2.
3.
4.
5.
6.
7.

Instructional capability
Learning completely new procedures
Extending procedure apply new situation
Hierarchical instruction: handling instructions
procedure embedded instruction others
Altering induced knowledge based
instruction
Learning procedures inductively domain
knowledge incomplete
Learning avoid prohibited actions
general learning due instruction

8. Learning avoid indirect achievement bad
state
9. Inferences simple specific statements
10. Inferences simple generic statements
11. Inferences conditionals
12. Learning operator perform hypothetical
goal
13. Learning operator perform hypothetical
state: general policy (active times)
14. Learning operator perform hypothetical
state: contingency within particular procedure
15. Learning operator effects
16. Learning non-perceivable operator effects associated inferences recognize
17. Learning control knowledge: learning set
operators prefer
18. Learning control knowledge: learning operators
indifferent

Example
pick

move move
teaching pick within line

removing docked-at pick
up's termination conditions
learning secondary operator
effects knowledge removed
\Never grasp red blocks."
Avoid grasping \Red
blocks explosive."
closing hand around explosive
block
\The grey block metal."
\White magnets powered."
\if condition [and condition]*
concluded state feature"
\To turn light, push
red button."
\If light bright, dim
light."
\If block metal, grasp
magnet" pick
pushing red button turns
light
magnet becomes stuck-to
metal block moved
two ways grasp small metal
block
two ways grasp small metal
block

Table 6: Instructional capabilities demonstrated Instructo-Soar.

311



fiHuffman & Laird

explanation (option O2). cases, default ask instruction missing
knowledge complete explanation (option O3).

8.1 Empirical Evaluation

empirical evaluations machine learning systems take one four forms, appropriate addressing different evaluation questions:
A. Comparison systems. technique useful evaluating overall
performance compares state art. used
systems available learning task.
B. Comparison altered version system. technique evaluates
impact component system overall performance. Typically,
system compared version without key component (sometimes called
\lesion study").
C. Measuring performance systematically generated series problems. technique evaluates method affected different dimensions input (e.g.,
noise training data).
D. Measuring performance known hard problems. Known hard problems provide
evaluation overall performance extreme conditions. instance, concept
learners' performance often measured standard, dicult datasets.
evaluation techniques applied limited ways Instructo-Soar.
dicult apply great depth two reasons. First, whereas machine
learning efforts concentrate depth single type learning single type input,
tutorial instruction requires breadth learning range instructional interactions. Whereas depth measured quantitative performance, breadth measured
(possibly qualitative) coverage { here, coverage 7 11 instructability requirements. Second, tutorial instruction extensively studied machine learning,
battery standard systems problems available. Nonetheless, evaluation
techniques (B), (C), (D) applied Instructo-Soar address specific
evaluation questions:
B. Comparison altered version: removed frame-axiom knowledge illustrate
effect prior knowledge agent's performance, described Section 6.5.
Without prior knowledge, agent unable explain instructions must resort
inductive methods. Thus, removing frame-axiom knowledge increased amount
instruction required reduced learning quality. compared versions
agent use different instruction recall strategies (Section 6.3).
C. Performance systematically varied input: examined effects varying three
dimensions instructions given agent. First, compared learning curves
instruction sequences different lengths (Section 6.2). graphs Figure 10
show, Instructo-Soar's execution time instructed procedure varies
number instructions sequence used teach it. Total execution time drops
312

fiFlexibly Instructable Agents

time procedure executed, according power law function, procedure learned general form. Second, compared teaching procedure
hierarchical subtasks versus using instruction sequence. Based
power law result, predicted hierarchical instruction would allow faster general
learning instruction. prediction confirmed empirically. Third,
examined number instruction orderings used teach given procedure Instructo-Soar order measure value supporting command
exibility. Rather experimental measurement, performed mathematical
analysis. analysis showed due command exibility, number instruction sequences used teach given procedure large, growing
exponentially number primitive steps procedure (Huffman, 1994).
D. Performance known hard problem: Since learning tutorial instruction
extensively studied machine learning, standard, dicult
problems. created comprehensive instruction scenario crossing command
exibility, situation exibility, knowledge-type exibility requirements. scenario, described detail (Huffman, 1994), contains 100 instructions demonstrates 17 Instructo-Soar's 18 instructional capabilities Table 6 (it
include learning indifference selecting two operators). agent learns
4,700 chunks scenario, including examples type PSCM
knowledge, extend agent's domain knowledge significantly.

9. Limitations Research
work's limitations fall three major categories: limitations tutorial instruction
teaching technique, limitations agent's general capabilities, limitations
incomplete solutions mapping, interaction, transfer problems. discuss
turn.

9.1 Limitations Tutorial Instruction

Tutorial instruction highly interactive situated. However, much human
instruction either non-interactive unsituated (or both), considered work. non-interactive instruction, content ow information
student controlled primarily information source. Examples include classroom
lectures, instruction manuals, textbooks. One issue using type instruction
locating extracting information needed particular problems (Carpenter
& Alterman, 1994). Non-interactive instruction contain situated information (e.g.,
worked-out example problems, Chi et al., 1989; VanLehn, 1987) unsituated information
(e.g., general expository text).
Unsituated instruction conveys general abstract knowledge applied
large number different situations. general-purpose knowledge often described
\declarative" (Singley & Anderson, 1989). example, physics class, students
taught F = a; general equation applies specific ways great variety
situations. advantage unsituated instruction precisely ability compactly
communicate abstract knowledge broadly applicable (Sandberg & Wielinga, 1991).
313

fiHuffman & Laird

However, use abstract knowledge, students must learn applies specific
situations (Singley & Anderson, 1989).

9.2 Limitations Agent

agent's inherent limitations constrain taught. developed
theory learning tutorial instruction within particular computational model
agents (the PSCM), within computational model, implemented agent
particular set capabilities demonstrate theory. Thus, weaknesses
computational model specific implemented agent must examined.
9.2.1 Computational Model

problem space computational model well suited situated instruction
elements' close correspondence knowledge level (facilitating mapping
instructions elements), inherently local control structure. However,
PSCM's local application knowledge makes dicult learn global control regimes
instruction, must translated series local decisions
result local learning.
second weakness PSCM provides theory functional types
knowledge used intelligent agent, gives indication possible content
knowledge. content theory knowledge would allow finer grained analysis
agent's instructability, within larger-grained knowledge types analysis provided
PSCM.
9.2.2 Implemented Agent's Capabilities

Producing definitive agent goal work. Rather, InstructoSoar agent's capabilities developed needed demonstrate instructional learning capabilities. Thus, limited number ways.16 instance,
performs simple actions serially static world. would sucient dynamic
domain ying airplane, multiple goals multiple levels granularity,
involving achievement and/or maintenance conditions environment, may
active (Pearson et al., 1993). Instructo-Soar's procedures implemented
series locally decided steps, precluding instruction containing procedure-wide (i.e., nonlocal) path constraints (e.g., \Go room, don't walk carpeting!").
single agent world, precluding instructions involve cooperation
agents (e.g., two robots carrying couch) instructions require reasoning
agents' potential actions (e.g., \Don't go alley, enemy
may block in.")
agent complete perception (clearly unrealistic real physical domains),
never told look, asked notice feature overlooked. contrast, instruction protocols show human students often told attend
features notice. Instructo-Soar's world noise-free, agent need
16. limitations particular agent implemented here, Soar, used
build powerful agents (e.g., Jones et al., 1993; Pearson et al., 1993).

314

fiFlexibly Instructable Agents

reason receive instruction failed actions. complete perception
noise-free environment, agent explicitly reason uncertainty
perceptions actions, demonstrated handling instructions explicitly
describe uncertain probabalistic outcomes.17 agent reason time
(as, e.g., Vere Bickmore's (1990) Homer does), cannot taught perform tasks
time-dependent way. keep track states seen actions performs
(other episodic instruction memory), cannot asked \do
before." Similarly, cannot learn procedures defined particular sequence
actions, rather set state conditions achieve. example, cannot taught
dance, dancing result net change external world. Finally, whenever agent know next, asks instruction.
never tries determine solution search weak methods means-ends
analysis. Adding capability would decrease need instruction.
addition agent's capabilities, Instructo-Soar limited solutions
mapping, interaction, transfer problems incomplete various ways.
limitations discussed next.

9.3 Mapping Problem
Instructo-Soar employs straightforward approach mapping instructions

agent's internal language, leaves problems mapping dicult natural language constructions unaddressed. relevant problems include reference resolution, incompleteness, use domain knowledge comprehension. Mapping
even require instruction, interaction resolve referent:
> Grab explosive block.

one that?
> red one.

type interaction supported Instructo-Soar.
addition general linguistic problems, Instructo-Soar makes limited
use semantic information learning new operators. example, first reads
\Move red block left yellow block," creates new operator, make
use semantic information communicated \Move...to left of." complete
agent would try glean information could semantics unfamiliar
command.

9.4 Interaction Problem
agent's shortcomings interaction problem center around three requirements:
(I1) exible initiation instruction, (I2) full exibility knowledge content, (I3)
situation exibility. (I1 ): Instructo-Soar, instruction initiated agent.
17. instruction protocols analyzed, instructions incomplete (missing conditions
Instructo-Soar learns), rarely described uncertainty explicitly.

315

fiHuffman & Laird

limits instructor's ability drive interaction interrupt agent's actions
instruction: \No! Don't push button!"18
(I2): Instructo-Soar provides exibility commands, instructions
communicate kinds information. Similar notion discourse coherence (Mann
& Thompson, 1988), fully exible tutorable agent needs support instruction event
knowledge coherence; is, instruction event delivering knowledge makes
sense current context. great variety knowledge could relevant
point makes requirement dicult.
(I3): Instructo-Soar provides situation exibility handling implicitly
explicitly situated instructions, hypothetical situations referred within
single instruction. Human tutors often refer one hypothetical situation course
multiple instructions.

9.5 Transfer Problem

work focused primarily transfer problem { producing general learning
tutorial instruction { requirements met. However, inductive
heuristics Instructo-Soar uses powerful.
addition, two transfer problem requirements achieved. First, (T7)
Instructo-Soar yet demonstrated instructional learning coexistence learning knowledge sources. Nothing Instructo-Soar's theory precludes coexistence, however. Learning knowledge sources could invoked possibly
enhanced instruction. instance, instructor might invoke learning observation pointing set objects saying \This tower"; similarly, instruction
containing metaphor could invoke analogical learning. One application instruction
could potentially enhance learning mechanisms within \personal assistant" software
agents learn observing users (e.g., Maes, 1994; Mitchell et al., 1994). Adding
ability learn verbal instructions addition observations would allow users
explicitly train agents situations learning observation alone may
dicult slow.
Second, (T6) Instructo-Soar cannot recover incorrect knowledge leads
either invalid explanations incorrect external performance. incorrect knowledge
may part agent's initial domain theory, may learned faulty
instruction. Inability recover incorrect knowledge precludes instruction general
case exceptions; instance, \Never grasp red blocks," later, \It's ok
grasp ones safety signs them." order avoid learning anything incorrect,
whenever Instructo-Soar attempts induce new knowledge, asks instructor's
verification adding knowledge long-term memory. Human students
ask much verification; appear jump conclusions, alter later
prove incorrect based information.
Rather always verifying knowledge learned, next generation instructable
agents learn reasonable inferences without verification (although may ask
verifications extreme cases). recently produced agent (Pearson &
18. recently added simple interruptability capability new version Instructo-Soar
incorporates recovery incorrect knowledge (Pearson & Huffman, 1995).

316

fiFlexibly Instructable Agents

Huffman, 1995) incorporates current research incremental recovery incorrect
knowledge (Pearson & Laird, 1995). agent learns correct overgeneral knowledge
infers completing explanations instructions. correction process triggered
using overgeneral knowledge results incorrect performance (e.g., action
agent expects succeed not). long run, believe work could push
research incremental theory revision error recovery, instructable agents
taught many types knowledge may need revision.

10. Conclusion

Although much work machine learning aims depth particular kind learning,

Instructo-Soar demonstrates breadth { interaction instructor learn variety

types knowledge { arising one underlying technique. kind breadth
crucial building instructable agent great variety instructions
variety knowledge communicate. instructable agents begin
basic knowledge domain, Instructo-Soar uses analytic, explanationbased approach learn instructions, makes use knowledge.
instructions may either implicitly explicitly situated, Instructo-Soar situates
explanations instruction within situation indicated instruction. Finally,
agent's knowledge often deficient explaining instructions, InstructoSoar employs four different options dealing incomplete explanations, selects
options dynamically depending instructional context.
availability effectiveness, tutorial instruction potentially powerful knowledge source intelligent agents. Instructo-Soar illustrates simple
domain. Realizing instruction's potential fielded applications require linguistically able agents incorporate robust techniques acquiring knowledge
instruction, refining knowledge needed based performance
instruction.

Acknowledgements
work performed first author graduate student University
Michigan. sponsored NASA/ONR contract NCC 2-517, University
Michigan Predoctoral Fellowship. Thanks Paul Rosenbloom, Randy Jones,
anonymous reviewers helpful comments earlier drafts.

References

Akatsuka, N. (1986). Conditionals discourse-bound. Traugott, E. C. (Ed.),
Conditionals, pp. 333{51. Cambridge Univ. Press, Cambridge.
Alterman, R., Zito-Wolf, R., & Carpenter, T. (1991). Interaction, comprehension,
instruction usage. Journal Learning Sciences, 1 (3&4), 273{318.
Anderson, J. R. (1983). architecture cognition. Harvard University Press, Cambridge,
MA.
317

fiHuffman & Laird

Bergadano, F., & Giordana, A. (1988). knowledge intensive approach concept induction. Proceedings International Conference Machine Learning, pp.
305{317.
Birmingham, W., & Klinker, G. (1993). Knowledge acquisition tools explicit problemsolving methods. Knowledge Engineering Review, 8 (1).
Birmingham, W., & Siewiorek, D. (1989). Automated knowledge acquisition computer
hardware synthesis system. Knowledge Acquisition, 1, 321{340.
Bloom, B. S. (1984). 2 sigma problem: search methods group instruction
effective one-to-one tutoring. Educational Researcher, 13 (6), 4{16.
Brachman, R. J. (1980). introduction KL-ONE. Brachman, R. J. (Ed.), Research
Natural Language Understanding, pp. 13{46. Bolt, Beranek Newman Inc.,
Cambridge, MA.
Carbonell, J. G., & Gil, Y. (1987). Learning experimentation. Proceedings
International Workshop Machine Learning, pp. 256{265.
Carbonell, J. G., Michalski, R. S., & Mitchell, T. M. (1983). overview machine
learning. Michalski, R. S., Carbonell, J. G., & Mitchell, T. M. (Eds.), Machine
Learning: artificial intelligence approach. Morgan Kaufmann.
Carpenter, T., & Alterman, R. (1994). reading agent. Proceedings Twelfth
National Conference Artificial Intelligence Seattle, WA.
Chapman, D. (1990). Vision, Instruction, Action. Ph.D. thesis, Massachusetts Institute
Technology, Artificial Intelligence Laboratory.
Chi, M. T. H., Bassok, M., Lewis, M. W., Reimann, P., & Glaser, R. (1989). Selfexplanations: students study use examples learning solve problems.
Cognitive Science, 13, 145{182.
Cypher, A. (Ed.). (1993). Watch do: Programming demonstration. MIT Press,
Cambridge, Mass.
Davis, R. (1979). Interactive transfer expertise: Acquisition new inference rules.
Artificial Intelligence, 12 (2), 409{427.
DeJong, G. F., & Mooney, R. J. (1986). Explanation-based learning: alternative view.
Machine Learning, 1 (2), 145{176.
Dent, L., Boticario, J., McDermott, J., Mitchell, T., & Zabowski, D. (1992). personal
learning apprentice. Proceedings International Joint Conference Artificial
Intelligence.
DiEugenio, B. (1993). Understanding natural language instructions: computational approach purpose clauses. Ph.D. thesis, University Pennsylvania. IRCS Report
93-52.
318

fiFlexibly Instructable Agents

DiEugenio, B., & Webber, B. (1992). Plan recognition understanding instructions.
Hendler, J. (Ed.), Proceedings First International Conference Artificial Intelligence Planning Systems, pp. 52{61 College Park, MD.
Donoho, S. K., & Wilkins, D. C. (1994). Exploiting ordering observed problem-solving
steps knowledge ase refinement: apprenticeship approach. Proceedings
12th National Conference Artifical Intelligence Seattle, WA.
Drummond, M. (1989). Situated control rules. Proceedings First International Conference Principles Knowledge Representation Toronto, Canada. Morgan Kaufmann.
Emihovich, C., & Miller, G. E. (1988). Talking turtle: discourse analysis Logo
instruction. Discourse Processes, 11, 183{201.
Eshelman, L., Ehret, D., McDermott, J., & Tan, M. (1987). MOLE: tenacious knowledgeacquisition tool. International Journal Man-Machine Studies, 26 (1), 41{54.
Fikes, R. E., Hart, P. E., & Nilsson, N. J. (1972). Learning executing generalized robot
plans. Artificial Intelligence, 3, 251{288.
Ford, C. A., & Thompson, S. A. (1986). Conditionals discourse: text-based study
English. Traugott, E. C. (Ed.), Conditionals, pp. 353{72. Cambridge
Univ. Press, Cambridge.
Frederking, R. E. (1988). Integrated natural language dialogue: computational model.
Kluwer Academic Press, Boston.
Golding, A., Rosenbloom, P. S., & Laird, J. E. (1987). Learning search control outside
guidance. Proceedings Tenth International Joint Conference Artificial
Intelligence, pp. 334{337.
Grosz, B. J. (1977). Representation use focus dialogue understanding. Ph.D.
thesis, University California, Berkeley.
Gruber, T. (1989). Automated knowledge acquisition strategic knowledge. Machine
Learning, 4 (3-4), 293{336.
Guha, R. V., & Lenat, D. B. (1990). Cyc: mid-term report. AI Magazine, 11 (3), 32{59.
Haas, N., & Hendrix, G. G. (1983). Learning told: Acquiring knowledge
information management. Michalski, R. S., Carbonell, J. G., & Mitchell, T. M.
(Eds.), Machine Learning: artificial intelligence approach. Morgan Kaufmann.
Haiman, J. (1978). Conditionals topics. Language, 54, 564{89.
Hall, R. J. (1988). Learning failing explain. Machine Learning, 3 (1), 45{77.
Hayes-Roth, F., Klahr, P., & Mostow, D. J. (1981). Advice taking knowledge refinement:
iterative view skill acquisition. Anderson, J. R. (Ed.), Cognitive skills
acquisition, pp. 231{253. Lawrence Erlbaum Associates, Hillsdale, NJ.
319

fiHuffman & Laird

Huffman, S. B. (1994). Instructable autonomous agents. Ph.D. thesis, University Michigan, Dept. Electrical Engineering Computer Science.
Huffman, S. B., & Laird, J. E. (1992). Dimensions complexity learning interactive
instruction. Erickson, J. (Ed.), Proceedings Cooperative Intelligent Robotics
Space III, SPIE Volume 1829.
Huffman, S. B., & Laird, J. E. (1993). Learning procedures interactive natural language instructions. Utgoff, P. (Ed.), Machine Learning: Proceedings Tenth
International Conference.
Huffman, S. B., & Laird, J. E. (1994). Learning highly exible tutorial instruction.
Proceedings 12th National Conference Artificial Intelligence (AAAI-94)
Seattle, WA.
Huffman, S. B., Miller, C. S., & Laird, J. E. (1993). Learning instruction: knowledgelevel capability within unified theory cognition. Proceedings Fifteenth
Annual Conference Cognitive Science Society, pp. 114{119.
Johnson-Laird, P. N. (1986). Conditionals mental models. Traugott, E. C. (Ed.),
Conditionals. Cambridge Univ. Press, Cambridge.
Jones, R. M., Tambe, M., Laird, J. E., & Rosenbloom, P. S. (1993). Intelligent automated agents ight training simulators. Proceedings Third Conference
Computer Generated Forces, pp. 33{42 Orlando, FL.
Just, M. A., & Carpenter, P. A. (1976). Verbal comprehension instructional situations.
Klahr, D. (Ed.), Cognition Instruction. Lawrence Erlbaum Associates, Hillsdale,
NJ.
Kieras, D. E., & Bovair, S. (1984). role mental model learning operate
device. Cognitive Science, 8, 255{273.
Kodratoff, Y., & Tecuci, G. (1987a). DISCIPLE-1: Interactive apprentice system weak
theory fields. Proceedings Tenth International Joint Conference Artificial
Intelligence, pp. 271{273.
Kodratoff, Y., & Tecuci, G. (1987b). Techniques design DISCIPLE learning apprentice. International Journal Expert Systems, 1 (1), 39{66.
Laird, J. E., Congdon, C. B., Altmann, E., & Doorenbos, R. (1993). Soar user's manual,
version 6..
Laird, J. E., Hucka, M., Yager, E. S., & Tuck, C. M. (1990). Correcting extending
domain knowledge using outside guidance. Proceedings Seventh International
Conference Machine Learning.
Laird, J. E., Newell, A., & Rosenbloom, P. S. (1987). Soar: architecture general
intelligence. Artificial Intelligence, 33 (1), 1{64.
320

fiFlexibly Instructable Agents

Laird, J. E., & Rosenbloom, P. S. (1990). Integrating execution, planning, learning
Soar external environments. Proceedings Eighth National Conference
Artificial Intelligence, pp. 1022{1029. AAAI Press.
Lewis, C. (1988). learn why: Analysis-based generalization procedures.
Cognitive Science, 12, 211{256.
Lewis, R. L. (1993). Architecturally-Based Theory Human Sentence Comprehension.
Ph.D. thesis, Carnegie Mellon University, School Computer Science.
Lewis, R. L., Newell, A., & Polk, T. A. (1989). Toward Soar theory taking instructions immediate reasoning tasks. Proceedings Annual Conference
Cognitive Science Society.
Lindsay, R. K. (1963). Inferential memory basis machines understand natural
language. Feigenbaum, E. A., & Feldman, J. (Eds.), Computers Thought, pp.
217{233. R. Oldenbourg KG.
Maes, P. (1994). Agents reduce work information overload. Communications
ACM, 37 (7).
Maes, P., & Kozierok, R. (1993). Learning interface agents. Proceedings National
Conference Artificial Intelligence, pp. 459{465.
Mann, W. C., & Thompson, S. A. (1988). Rhetorical structure theory: Toward functional
theory text organization. Text, 8 (3), 243{281.
Marcus, S., & McDermott, J. (1989). SALT: knowledge acquisition language proposeand-revise systems. Artificial Intelligence, 39 (1), 1{37.
Martin, C. E., & Firby, R. J. (1991). Generating natural language expectations
reactive execution system. Proceedings Thirteenth Annual Conference
Cognitive Science Society, pp. 811{815.
McCarthy, J. (1968). advice taker. Minsky, M. (Ed.), Semantic Information Processing, pp. 403{410. MIT Press, Cambridge, Mass.
Miller, C. M. (1993). model concept acquisition context unified theory
cognition. Ph.D. thesis, University Michigan, Dept. Computer Science
Electrical Engineering.
Minton, S., Carbonell, J. G., Knoblock, C. A., Kuokka, D. R., Etzioni, O., & Gil, Y. (1989).
Explanation-based learning: problem-solving perspective. Artificial Intelligence, 40,
63{118.
Mitchell, T., Caruana, R., Freitag, D., McDermott, J., & Zabowski, D. (1994). Experience
learning personal assistant. Communications ACM, 37 (7).
Mitchell, T. M., Keller, R. M., & Kedar-Cabelli, S. T. (1986). Explanation-based generalization: unifying view. Machine Learning, 1.
321

fiHuffman & Laird

Mitchell, T. M., Mahadevan, S., & Steinberg, L. I. (1990). LEAP: learning apprentice
system VLSI design. Kodratoff, Y., & Michalski, R. S. (Eds.), Machine Learning:
artificial intelligence approach, Vol. III. Morgan Kaufmann.
Mooney, R. J. (1990). Learning plan schemata observation: Explanation-based learning plan recognition. Cognitive Science, 14, 483{509.
Mostow, D. J. (1983). Learning told: Machine transformation advice
heuristic search procedure. Michalski, R. S., Carbonell, J. G., & Mitchell, T. M.
(Eds.), Machine Learning: artificial intelligence approach. Morgan Kaufmann.
Musen, M. A. (1989). Automated support building extending expert models. Machine Learning, 4 (3-4), 347{376.
Newell, A. (1981). knowledge level. AI Magazine, 2 (2), 1{20.
Newell, A. (1990). Unified Theories Cognition. Harvard University Press, Cambridge,
Massachusetts.
Newell, A., Yost, G., Laird, J. E., Rosenbloom, P. S., & Altmann, E. (1990). Formulating
problem space computational model. Proceedings 25th Anniversary
Symposium, School Computer Science, Carnegie Mellon University.
Pazzani, M. (1991a). computational theory learning causal relationships. Cognitive
Science, 15, 401{424.
Pazzani, M. (1991b). Learning predict explain: integration similarity-based,
theory driven, explanation-based learning. Journal Learning Sciences, 1 (2),
153{199.
Pearson, D. J., & Huffman, S. B. (1995). Combining learning instruction recovery
incorrect knowledge. Gordon, D., & Shavlik, J. (Eds.), Proceedings 1995
Machine Learning Workshop Agents Learn Agents.
Pearson, D. J., Huffman, S. B., Willis, M. B., Laird, J. E., & Jones, R. M. (1993).
symbolic solution intelligent real-time control. IEEE Robotics Autonomous
Systems, 11, 279{291.
Pearson, D. J., & Laird, J. E. (1995). Toward incremental knowledge correction agents
complex environments. Muggleton, S., Michie, D., & Furukawa, K. (Eds.), Machine
Intelligence, Vol. 15. Oxford University Press.
Porter, B. W., Bareiss, R., & Holte, R. C. (1990). Concept learning heuristic classification weak-theory domains. Artificial Intelligence, 45 (3), 229{263.
Porter, B. W., & Kibler, D. F. (1986). Experimental goal regression: method learning
problem-solving heuristics. Machine Learning, 1, 249{286.
Redmond, M. A. (1992). Learning observing understanding expert problem solving.
Ph.D. thesis, Georgia Institute Technology.
322

fiFlexibly Instructable Agents

Rosenbloom, P. S., & Aasman, J. (1990). Knowledge level inductive uses chunking
(EBL). Proceedings National Conference Artificial Intelligence.
Rosenbloom, P. S., & Laird, J. E. (1986). Mapping explanation-based generalization onto
Soar. Proceedings National Conference Artificial Intelligence, pp. 561{567.
Rosenbloom, P. S., Laird, J. E., & Newell, A. (1988). chunking skill knowledge.
Bouma, H., & Elsendoorn, A. G. (Eds.), Working Models Human Perception,
pp. 391{410. Academic Press, London, England.
Rosenbloom, P. S., Laird, J. E., & Newell, A. (Eds.). (1993a). Soar Papers: Research
integrated intelligence. MIT Press, Cambridge, Mass.
Rosenbloom, P. S., Laird, J. E., & Newell, A. (Eds.). (1993b). Soar Papers: Research
integrated intelligence. MIT Press, Cambridge, Mass.
Rosenbloom, P. S., & Newell, A. (1986). chunking goal hierarchies: generalized
model practice. Michalski, R. S., Carbonell, J. G., & Mitchell, T. M. (Eds.),
Machine Learning: artificial intelligence approach, Volume II. Morgan Kaufmann.
Rumelhart, D. E., & McClelland, J. L. (Eds.). (1986). Parallel distributed processing: Explorations microstructure cognition. MIT Press, Cambridge, MA.
Rychener, M. D. (1983). instructible production system: retrospective analysis.
Michalski, R. S., Carbonell, J. G., & Mitchell, T. M. (Eds.), Machine Learning:
artificial intelligence approach, pp. 429{460. Morgan Kaufmann.
Sandberg, J., & Wielinga, B. (1991). situated cognition?. Proceedings
International Joint Conference Artificial Intelligence, pp. 341{346.
Schank, R. C. (1975). Conceptual Information Processing. American Elsevier, New York.
Schank, R. C., & Leake, D. B. (1989). Creativity learning case-based explainer.
Artificial Intelligence, 40, 353{385.
Segre, A. M. (1987). learning apprentice system mechanical assembly. Third IEEE
Conference Artificial Intelligence Applications, pp. 112{117.
Shen, W. (1993). Discovery autonomous learning environment. Machine Learning, 12, 143{165.
Simon, H. A. (1977). Artificial intelligence systems understand. Proceedings
Fifth International Joint Conference Artificial Intelligence, pp. 1059{1073.
Simon, H. A., & Hayes, J. R. (1976). Understanding complex task instructions. Klahr,
D. (Ed.), Cognition Instruction. Lawrence Erlbaum Associates, Hillsdale, NJ.
Singley, M. K., & Anderson, J. R. (1989). transfer cognitive skill. Harvard University
Press.
323

fiHuffman & Laird

Sutton, R. S., & Pinette, B. (1985). learning world models connectionist networks.
Proceedings Seventh Annual Conference Cognitive Science Society, pp.
54{64.
Thrun, S. B., & Mitchell, T. M. (1993). Integrating inductive neural network learning
explanation-based learning. Proceedings International Joint Conference
Artificial Intelligence, pp. 930{936.
VanLehn, K. (1987). Learning one subprocedure per lesson. Artificial Intelligence, 31 (1),
1{40.
VanLehn, K., Ball, W., & Kowalski, B. (1990). Explanation-based learning correctness:
Towards model self-explanation effect. Proceedings 12th Annual
Conference Cognitive Science Society, pp. 717{724.
VanLehn, K., & Jones, R. (1991). Learning physics via explanation-based learning correctness analogical search control. Proceedings International Machine
Learning Workshop.
VanLehn, K., Jones, R. M., & Chi, M. T. H. (1992). model self-explanation effect.
Journal Learning Sciences, 2 (1), 1{59.
Vere, S., & Bickmore, T. (1990). basic agent. Computational Intelligence, 6, 41{60.
Wertsch, J. V. (1979). social interaction higher psychological processes: clarification application Vygotsky's theory. Human Development, 22, 1{22.
Widmer, G. (1989). tight integration deductive inductive learning. Proceedings
International Workshop Machine Learning, pp. 11{13.
Wilkins, D. C. (1990). Knowledge base refinement improving incomplete incorrect
domain theory. Kodratoff, Y., & Michalski, R. S. (Eds.), Machine Learning:
Artificial Intelligence Approach, Volume III, pp. 493{514. Morgan Kaufmann.
Winograd, T. (1972). Understanding Natural Language. Academic Press, New York.
Wood, D., Bruner, J. S., & Ross, G. (1976). role tutoring problem solving. Journal
Child Psychology Psychiatry, 17, 89{100.
Yost, G. R. (1993). Acquiring knowledge Soar. IEEE Expert, 8 (3), 26{34.
Yost, G. R., & Newell, A. (1989). problem space approach expert system specification.
Proceedings International Joint Conference Artificial Intelligence, pp.
621{7.

324


