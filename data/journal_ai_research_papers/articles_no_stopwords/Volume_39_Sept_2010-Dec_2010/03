journal artificial intelligence

submitted published

clustering want
inducing ideal clustering minimal feedback
sajib dasgupta
vincent ng

sajib hlt utdallas edu
vince hlt utdallas edu

human language technology institute
university texas dallas
west campbell road mail station ec
richardson tx u

abstract
traditional text clustering largely focused grouping documents
topic conceivable user may want cluster documents along dimensions
authors mood gender age sentiment without knowing users intention
clustering group documents along prominent dimension
may one user desires address clustering documents
along user desired dimension previous work focused learning similarity metric
data manually annotated users intention human construct
feature space interactive manner clustering process goal
reducing reliance human knowledge fine tuning similarity function selecting
relevant features required approaches propose novel active clustering
allows user easily select dimension along wants
cluster documents inspecting small number words demonstrate
viability variety commonly used sentiment datasets

introduction
text clustering one major application domains demonstrating viability
clustering traditional text clustering largely focused
grouping documents topic conceivable user may want cluster documents along dimensions authors mood gender age sentiment since
virtually existing text clustering produce one clustering given
set documents natural question clustering necessarily one user desires words text clustering produce clustering along
user desired dimension
answer question depends large extent whether user successfully communicate intention clustering traditionally
achieved designing good similarity function capture similarity
pair documents ideal clustering produced typically involves
identify set features useful inducing desired clusters liu li
lee yu however manually identifying right set features timeconsuming knowledge intensive may even require lot domain expertise
fact resulting similarity function typically easily portable domains
particularly unappealing machine learning perspective overcome weakness
c

ai access foundation rights reserved

fidasgupta ng

researchers attempted learn similarity metric side information xing ng
jordan russell constraints pairs documents must must
appear cluster wagstaff cardie rogers schrodl
contrast recent work focused active clustering clustering
incorporate user feedback clustering process help ensure documents grouped according user desired dimension one way
user incrementally construct set relevant features interactive fashion bekkerman raghavan allan eguchi raghavan allan roth small
another way user correct mistakes made clustering
clustering iteration specifying whether two existing clusters merged
split balcan blum major drawback associated active clustering
involve considerable amount human feedback needs
provided iteration clustering process furthermore identifying clusters
merging splitting balcan blums may easy appears
merge split decision user makes sample large number
documents cluster read documents base decision
extent documents dis similar
article attack clustering documents according user interest
different angle aim knowledge lean
produce clustering documents along user desired dimension
without relying human knowledge fine tuning similarity function selecting
relevant features unlike existing approaches end propose novel active
clustering assumes input simple feature representation composed
unigrams simple similarity function e dot product operates
inducing important clustering dimensions given set documents
clustering dimension represented small number automatically selected words
representative dimension user choose dimension along
wants cluster documents examining automatically selected words
comparison aforementioned feedback mechanisms arguably much simpler
require user cursory look small number features
dimension opposed user generate feature space
interactive manner identify clusters need merged split clustering
iteration
evaluate active clustering task sentiment clustering
goal cluster set documents e g reviews according polarity
e g thumbs thumbs expressed author without labeled
data decision focus sentiment clustering motivated several reasons
one reason relatively little work sentiment clustering
mentioned existing work text clustering focused topic clustering
high accuracies achieved even datasets large number classes
e g newsgroups despite large amount recent work sentiment analysis
use term clustering dimension refer dimension along set documents
clustered example set movie reviews clustered according genre e g action romantic
documentary sentiment e g positive negative neutral



fiinducing ideal clustering minimal feedback

review
sound system seem little better
cds skipping much bottom line
didnt fix cds still skipping noticeably
although bad
review
john lynch wrote classic spanish american revolutions
describes events led independence latin america spain
book starts rio de la plata ends mexico central america
curiously one note common pattern highly stratified societies lead spanish
reluctance spanish monarchy later even liberals led independence
interested better understanding latin great book must
lynch cleverly combines historical economic facts hispanic american societies

table snippets two reviews illustrate two challenges polarity classification
one reviews sentimentally ambiguous review
objective materials review significantly outnumber subjective counterparts
review

opinion mining much focused supervised methods see pang lee
comprehensive survey field
another equally important reason focus sentiment clustering concerned challenges task presents natural language processing nlp
researchers broadly speaking complexity sentiment clustering arises
two sources first reviews sentimentally ambiguous containing positive negative sentiment bearing words phrases review table shows snippet
review dvd domain illustrates sentimental ambiguity
phrases little better skipping bad convey positive sentiment
phrases didnt fix skipping noticeably negative sentiment bearing hence
unless sentiment analyzer performs deeper linguistic analysis difficult
analyzer determine polarity review second objective materials review tend significantly outnumber subjective counterparts reviewer typically
devotes large portion review describing features product assigning rating consequently sentiment analyzer uses word phrase
feature representation composed mostly features irrelevant respect
polarity determination shown review table snippet book review
illustrates see three words phrases classic great
book cleverly review correspond objective materials
aforementioned complications present significant challenges even supervised polarity classification systems let alone sentiment clustering
access labeled data illustrate difficulty two complications impose sentiment clustering consider task clustering set movie
reviews since review may contain description plot authors sentiment
clustering may cluster reviews along plot dimension sentiment
dimension without knowing users intention clustered along


fidasgupta ng

prominent dimension assuming usual bag words representation prominent
dimension likely plot uncommon review devoted almost
exclusively plot author briefly expressing sentiment end
review even reviews contain mostly subjective materials prominent
dimension may still sentiment owing aforementioned sentimental ambiguity
presence positive negative sentiment bearing words reviews renders sentiment dimension hidden e less prominent far clustering
concerned
sum contributions article five fold
propose novel active clustering cluster set documents
along user desired dimension without labeled data side information
manually specified automatically acquired must link cannot link constraints
comparison existing active clustering approaches appeal
requiring much simpler human feedback
demonstrate viability evaluating performance
sentiment datasets via set human experiments typically
absent papers involve incorporating user feedback
led deeper understanding spectral clustering specifically
propose novel application top eigenvectors produced spectral clustering
use unveil important clustering dimensions text
collection
implications domain adaptation topic recently
received lot attention nlp community specifically
sentiment dimension manually identified one domain used automatically
identify sentiment dimension similar domain
preliminary datasets possess one clustering dimension e g
collection book dvd reviews clustered sentiment
type product concerned indicate capable producing
multiple clusterings dataset one along dimension hence
potentially reveal information dataset possible traditional
clustering produce single clustering data
ability produce multiple clusterings particularly useful feature user
idea wants documents clustered due
lack knowledge data instance even user knowledge
data knows wants documents clustered help
unveil hidden dimensions previously aware may
interest
rest article organized follows section presents basics spectral
clustering facilitate discussion active clustering section
describe human experiments evaluation several sentiment datasets
section significance work section finally discuss related work
section conclude section


fiinducing ideal clustering minimal feedback

spectral clustering
given clustering task important question ask clustering
use popular choice k means nevertheless well known k means
major drawback able separate data points linearly separable
given feature space e g see dhillon guan kulis cai han
moreover since k means clusters documents directly given feature space
text applications typically comprises hundreds thousands features performance
could adversely affected curse dimensionality spectral clustering
developed response k means section first present
one commonly used spectral clustering section
provide intuition behind spectral clustering section finally describe two ways
use resulting eigenvectors produce clustering section

let x x xn set n data points clustered x x similarity
function defined x similarity matrix captures pairwise similarities
e si j xi xj many clustering spectral clustering
takes input outputs k way partition c c c ck e ki ci x
j j ci cj equivalently one think spectral clustering learning
partitioning function f rest article represented vector
f k indicates cluster xi assigned note
cluster labels interchangeable even renamed without loss
generality
among well known spectral clustering e g weiss shi malik
kannan vempala vetta adopt one proposed ng jordan
weiss arguably widely used main steps ng et
al spectral clustering
create diagonal matrix whose th entry sum th row
construct laplacian matrix l sd
eigenvalues eigenvectors l
create matrix eigenvectors correspond largest eigenvalues
data point rank reduced point dimensional space normalize
point unit length retaining sign value
apply k means cluster data points resulting eigenvectors
words spectral clustering clusters data points low dimensional space
dimension corresponds top eigenvector laplacian matrix
follow ng et al employ normalized dual form usual laplacian



fidasgupta ng

intuition behind spectral clustering
may immediately clear spectral clustering produces meaningful partitioning set points theoretical justifications behind spectral clustering
since mathematics quite involved provide intuitive justification
clustering technique way sufficient reader understand active
clustering section refer interested reader shi maliks
seminal spectral clustering details since apply spectral clustering
produce way clustering given set data points rest article
center discussion way clustering subsection
spectral clustering employs graph theoretic notion grouping specifically set
data points arbitrary feature space represented undirected weighted graph
node corresponds data point edge weight two nodes xi
xj similarity si j
given graph formulation reasonable way produce way partitioning
data points minimize similarity resulting two clusters c c
hence reasonable objective function minimize cut value
x
cut c c
si j f f j
j

without loss generality define f follows

c
f
c
mentioned use cluster labels interchangeable
fact renamed whatever way want
one minimizing cut value noticed wu leahy
objective favors producing unbalanced clusters one contains
small number nodes words bias towards isolating small set
nodes mentioned shi malik surprising since
number edges involved cut hence cut value tends increase sizes
two clusters become relatively balanced
closer examination minimum cut criterion reveals minimizes inter cluster similarity makes attempt maximize intra cluster similarity
address weakness shi malik propose minimize instead normalized
cut value n cut takes account inter cluster dissimilarity intra cluster
similarity specifically
cut c c
cut c c


assoc c c c assoc c c c
p
assoc b computed xi xj b si j total connection nodes
nodes b given definition cut resulting unbalanced clusters
longer small n cut value see reason consider case c consists
one node case assoc c c c cut c c making n cut c c large
n cut c c



fiinducing ideal clustering minimal feedback

algebra express n cut follows
n cut c c

f f
f df

subject constraints df
rp



pic

ic
rp
f



pic
ic

c
c

defined section first constraint specifies
df orthogonal intuitively understood follows since constant
vector entries cannot used induce partition constraint
avoids trivial solution points assigned cluster
unfortunately papadimitriou proves minimizing normalized cut np complete
even special case graphs regular grids see shi malik
proof hence following shi malik relax minimization dropping
second constraint allowing entry f take real value rather one
two discrete values seeking real valued solution following
minn

f

f f
f df



subject
df
assuming g f rewrite
minn

g

gt g
gt g



subject
g
following standard rayleigh ritz theorem one prove solution
g eigenvector corresponds second smallest eigenvalue
equivalently eigenvector corresponds second largest
eigenvector sd laplacian matrix l defined section
simplicity henceforth refer eigenvector corresponds n th largest
eigenvalue l simply n th eigenvector denote en
besides normalized cut ratio cut chan schlag zien average association shi malik
min max cut ding zha gu simon used objective functions
spectral clustering
given involves minimizing rayleigh quotient may seem somewhat unintuitive
solution second eigenvector l rather first eigenvector reason attributed
constraint associated specifies solution g perpendicular
first eigenvector l



fidasgupta ng

idea behind spectral clustering second eigenvector l approximate solution minimizing normalized cut course since second
eigenvector real valued solution convert partitioning function
used cluster data points section explains two simple ways
converting eigenvector partitioning function
turns eigenvectors l convey useful information
data specifically impose additional constraint forcing solution orthogonal second eigenvector l solution becomes third
eigenvector hence third eigenvector thought suboptimal solution
meaning used impose reasonably good partition
data points perhaps importantly since eigenvectors l orthogonal
l symmetric clustering produced third eigenvector
likely correspond different dimension data produced second
eigenvector
generally limit solution space real valued vectors
orthogonal first eigenvectors l solution constrained optimization
th eigenvector l words top eigenvectors
l intuitively thought revealing important dimension data although
subsequent eigenvectors progressively less ideal far clustering concerned
clustering eigenvectors
ng et al point different authors still disagree eigenvectors
use derive clusters subsection describe two common
methods determining eigenvectors use method
derive clusters selected eigenvector methods serve baselines
evaluation
method second eigenvector
since shi malik second eigenvector e approximate solution
minimizing normalized cut perhaps surprising
e commonly chosen eigenvector deriving partition however since e
real valued solution constrained optimization need specify
derive clusters
clustering e trivial since linearization points one simple way
determine threshold partitioning however follow ng et al
cluster points means one dimensional space
method top eigenvectors
recall section eigen decomposing laplacian matrix data point
represented co ordinates second method use means cluster data
points dimensional space effectively exploiting top eigenvectors
fact since f g pre multiply second eigenvector l get
solution following ng et al employ second eigenvector l directly
clustering ignoring term



fiinducing ideal clustering minimal feedback

active clustering
mentioned sentiment clustering challenging part due fact
reviews clustered along one dimension section describe
active clustering makes easy user specify dimension
along wants cluster data points sentiment recall first
applies spectral clustering reveal important dimensions data
lets user select desired dimension e sentiment motivate importance
user feedback helps understand two baseline clustering described
section spectral methods rely user feedback may yield sentiment clustering begin consider first
method second eigenvector used induce partition recall
second eigenvector reveals prominent dimension data hence sentiment
prominent dimension happen non sentiment bearing words
outnumber sentiment bearing words bag words representation review
resulting clustering reviews may sentiment oriented similar line
reasoning used explain second baseline clustering
clusters top eigenvectors may work well since eigenvector corresponds different dimension particular correspond
non sentiment dimensions represent review may hamper accurate computation similarity two reviews far clustering along sentiment
dimension concerned rest section discuss detail major steps
active clustering allows easy incorporation user feedback
step identify important clustering dimensions
rely simple method identifying important clustering dimensions given
text collection employ top eigenvectors laplacian important clustering dimensions method motivated fact e second eigenvector
laplacian optimal real valued solution objective function spectral
clustering minimizes e normalized cut shi malik therefore optimal
clustering dimension importantly exploit rarely utilized observation discussed
section remaining eigenvectors suboptimal solutions ei suboptimal increases top eigenvectors e small values
less suboptimal may still yield reasonably good though optimal clusterings
data therefore serve good clustering dimensions existing applications
spectral clustering mainly clustered data points space defined top
eigenvectors attempted use ei separately
produce clusterings unlike note first eigenvector constant vector
simply assigns data points cluster therefore typically ignored
step identify relevant features partition
given eigen decomposition step first obtain second th
eigenvectors correspond important dimensions data next
question determine dimension captures user interest one way


fidasgupta ng

user inspect partitions reviews decide
corresponds closely sentiment clustering main drawback associated
kind user feedback user may read large number reviews
order make decision hence reduce human effort employ alternative
procedure identify informative features characterizing partition
user inspect features rather reviews make easy
human identify clustering dimension features chosen
useful distinguishing reviews two clusters
identify rank informative features employ method call maximum
margin feature ranking mmfr recall maximum margin classifier e g support
vector machine separates data points two classes maximizing margin
separation specifically maximum margin hyperplane defined w x b
x feature vector representing arbitrary data point w weight vector b
scalar parameters learned solving following constrained optimization

x


min kwk c


subject
ci w xi b

n

ci class th training point xi degree misclassification xi c regularization parameter balances training error model
complexity
use w identify informative features partition note
informative features large absolute weight values feature large
positive negative weight strongly indicative positive negative class exploit
observation identify informative features partition training
binary svm classifier partition data points cluster assumed
class value sorting features according svm learned feature
weights generating two ranked lists informative features top bottom
f features respectively
given ranked lists generated partitions user select one
partitions dimensions relevant sentiment inspecting many features
ranked lists needed picking relevant dimension user
label one two feature lists associated dimension positive
negative since feature list represents one clusters cluster associated
positive list labeled positive cluster associated negative list
labeled negative
note commonly used feature selection techniques log likelihood ratio information
gain applied identify informative features see yang pedersen
overview
notion svm feature weights measures feature informativeness explored
work see instance work fung gilad bachrach navot tishby
kugler aoki kuroyanagi iwata nugroho details
svm classifiers article trained svmlight package joachims
learning parameters set default values



fiinducing ideal clustering minimal feedback

comparison existing user feedback mechanisms assisting clustering
requires comparatively little human intervention require user select
dimension examining small number features opposed user construct
feature space identify clusters need merged split required
methods
step identify unambiguous reviews
caveat however mentioned introduction many reviews contain
positive negative sentiment bearing words ambiguous reviews likely
clustered incorrectly unambiguous counterparts since ranked lists
features derived partition presence ambiguous reviews
adversely affect identification informative features mmfr
remove ambiguous reviews deriving informative features partition
employ simple method identifying unambiguous reviews computation
eigenvalues data point factors orthogonal projections
data points affinity ambiguous data points receive orthogonal
projections positive negative data points hence near zero
values pivot eigenvectors words points near zero values
eigenvectors ambiguous large absolute values therefore sort
data points according corresponding values eigenvector keep
top n bottom n data points induce informative features
resulting data points present user select
desired partition
step cluster along selected eigenvector
finally employ means cluster reviews along eigenvector selected
user regardless whether review ambiguous

evaluation
section describe experiments aim evaluate effectiveness active
clustering provide insights
experimental setup
begin discussing details datasets document preprocessing method
implementation spectral clustering evaluation metrics
note somewhat arbitrary choice underlying choice merely assumption
fraction reviews unambiguous see evaluation section reviews
classified according polarity high accuracy consequently features induced
resulting clusters high quality additional experiments revealed list top ranking
features change significantly induced smaller number unambiguous reviews



fidasgupta ng

datasets
use five sentiment datasets including widely used movie review dataset mov
pang lee vaithyanathan well four datasets containing reviews four
different types products amazon books boo dvds dvd electronics ele
kitchen appliances kit blitzer dredze pereira dataset
labeled reviews positives negatives illustrate difference
topic clustering sentiment clustering topic clustering pol dataset created taking documents two sections
newsgroups discuss issues cryptography politics namely sci crypt
talks politics
document preprocessing
preprocess document first tokenize downcase represent
vector unstemmed unigrams assumes value indicates
presence absence document addition remove vector punctuation
numbers words length one words occur single review
following common practice information retrieval community exclude
words high document frequency many stopwords domain specific
general purpose words e g movies movie domain preliminary examination
evaluation datasets reveals words typically comprise vocabulary
decision exactly many terms remove dataset subjective large
corpus typically requires removals small corpus consistent simply
sort vocabulary document frequency remove top henceforth
refer document representation bag words bow representation
spectral learning setup
following common practice spectral learning text domains e g kamvar klein
manning cai et al compute similarity two reviews
taking dot product feature vectors ng et al spectral clustering
set diagonal entries similarity matrix addition set
words consider second fifth eigenvectors assuming
sufficient capturing desired clusterings
evaluation metrics
employ two evaluation metrics first report dataset terms
accuracy percentage documents label assigned system
gold standard label second following kamvar et al evaluate
clusters produced gold standard clusters adjusted
rand index ari corrected chance version rand index
specifically given set n data points two clusterings points u v
note setting somewhat arbitrary choice number eigenvectors
used active clustering



fiinducing ideal clustering minimal feedback

u u u um clusters v v v vn n clusters ari
computed follows
nij


p bj
p

j
ari u v p p b
p
ai p
j

j
j

p

ij

n

bj
n




formula nij number common objects ui vj whereas ai bj
number objects ui vj respectively ari ranges better clusterings
higher ari values
baseline systems
subsection describe baseline first two baseline systems
ones described section last two arguably sophisticated clustering
employed attempt strengthen baseline
clustering second eigenvector
first baseline adopt shi maliks cluster reviews
second eigenvector e described section pol
sentiment datasets expressed terms accuracy ari shown row tables
b respectively owing randomness choice seeds means
experimental involving means averaged ten independent runs
see baseline achieves accuracy pol much lower
accuracies sentiment datasets performance trend
observed ari provide suggestive evidence producing sentimentbased clustering requires different features producing topic clustering
many cases salient features tend topic difference
sentiment clustering topic clustering illuminated
experiments section
addition worth noting baseline achieves much lower accuracies
ari values boo dvd ele remaining two sentiment datasets since
e captures prominent dimension suggest sentiment dimension
prominent dimension three datasets fact intuitively
plausible instance book domain positive book reviews typically contain
short description content reviewer briefly expressing sentiment
somewhere review similarly electronics domain electronic product reviews
typically aspect oriented reviewer talking pros cons
aspect product e g battery durability since reviews likely contain
positive negative sentiment bearing words sentiment clustering unlikely
captured e
note clustering one dimensional space baseline yields stable regardless
choice seeds ten runs exhibit nearly zero variance



fidasgupta ng

system variation
nd eigenvector
top five eigenvectors
interested reader model
nmf
system

pol






mov






accuracy
kit boo






dvd






ele






ari
dvd






ele








system variation
nd eigenvector
top five eigenvectors
interested reader model
nmf
system

pol






adjusted rand index
mov kit boo








b

table terms accuracy b adjusted rand index six datasets
obtained bag words document representation strongest
dataset boldfaced

clustering top five eigenvectors
second baseline represent data point top five eigenvectors e e
e cluster means five dimensional space described
section hence thought ensemble clustering
decision collectively made five eigenvectors
shown row tables b comparison first baseline
see improvements accuracy ari pol three sentiment datasets
first baseline performs poorly e boo dvd ele drastic
improvement observed ele however performance remaining two sentiment
datasets deteriorates attributed fact boo dvd
ele e capture sentiment dimension since eigenvector
ensemble see improvements hand e already captured
sentiment dimension mov kit employing additional dimensions
may sentiment related may introduce noise computation
similarities reviews
first eigenvector produce trivial clustering data points reside
cluster commonly used combination top eigenvectors create low dimensional
space data points clustered see work ng et al details
clustering five dimensional space observe highly sensitive
choice seeds instance variances accuracy observed ten runs pol mov
kit boo dvd ele respectively



fiinducing ideal clustering minimal feedback

clustering interested reader model
third baseline kamvar et al unsupervised clustering according authors ideally suited text clustering recently proved
special case ratio cut optimization kulis basu dhillon mooney specifically introduce laplacian inspired interested reader model
laplacian computed dmax dmax defined section
except si j one js k nearest neighbors j one k
nearest neighbors dmax maximum rowsum identity matrix since
performance highly sensitive k tested values k report row tables b best somewhat disappointingly despite
algorithmic sophistication fact reporting best baseline
offer consistent improvements previous two comparison first
baseline achieves better performance pol worse performance sentiment
datasets first baseline boo dvd ele particularly poor
clustering non negative matrix factorization
non negative matrix factorization nmf recently shown xu liu gong
effective document clustering implementing
evaluate six datasets shown row tables b best obtained running five times comparison first baseline
nmf achieves better performance ele comparable performance mov worse
performance remaining datasets
active clustering
subsection describe human automatic experiments evaluating active
clustering
human experiments
unlike four baselines active clustering requires users specify
four dimensions defined second fifth eigenvectors closely
related sentiment inspecting set features derived unambiguous reviews
dimension mmfr better understand easy human select
desired dimension given features performed experiment independently
five humans computer science graduate students affiliated
computed agreement rate
specifically dataset showed human judge top features
cluster according mmfr see tables subset features induced
six datasets lightly shared columns correspond sentiment
dimension selected majority human judges addition informed
matrix factorization use code downloaded http www csie ntu edu tw cjlin nmf index html
human judges reported inspecting top features sufficient identifying
sentiment dimension note user clustering may request inspect many
features wants



fidasgupta ng

e
c
serder
armenian
turkey
armenians
muslims
sdpa
argic
davidian
dbd ura
troops
c
sternlight

pgp
crypto


likely
access
idea
cryptograph

pol
e
e
c
c
beyer
serbs
arabs
palestinians
andi
muslims

wrong
israelis
department
tim
bosnia
uci
live
ab
matter
z virginia
freedom
holocaust
politics
c
escrow
sternlight

access
net
des
privacy
uk
systems
pgp

c
standard
sternlight
des
escrow
employer
net
york
jake
code


e
c
escrow
serial

chips
ensure
care
strong
police
omissions
excepted
c
internet
uucp
uk
net
quote
ac
co

ai
mit

table top ten features induced dimension pol domain shaded
columns correspond dimensions selected human judges e e top eigenvectors c c clusters



fiinducing ideal clustering minimal feedback

e
c
relationship
son
tale
husband
perfect
drama
focus
strong
beautiful
nature
c
worst
stupid
waste
bunch

video
worse
boring
guess
anyway

mov
e
e
c
c
production
jokes
earth
kids
sequences
live
aliens
animation
war
disney
crew
animated
alien
laughs
planet
production
horror
voice
evil
hilarious
c
sex
romantic
school
relationship
friends
jokes
laughs
sexual
cute
mother

c
thriller
killer
murder
crime
police
car
dead
killed
starts
violence

e
c
starts
person
saw
feeling
lives
told
happen

felt
happened
c
comic
sequences
michael
supporting
career
production
peter
style
latest
entertaining

table top ten features induced dimension mov domain shaded
columns correspond dimensions selected human judges e e top eigenvectors c c clusters



fidasgupta ng

boo
e
c
history
must
modern
important
text
reference
excellent
provides
business


e
c
series
man
history
character
death

war
seems
political
american

e
c
loved
highly
easy
enjoyed
children

although
excellent
understand
three

e
c
must
wonderful
old
feel
away
children
year
someone
man
made

c
plot

thought
boring
got
character


ending
fan

c
buy
bought
information
easy
money
recipes
pictures
look
waste
copy

c
money
bad
nothing
waste
buy
anything

already
instead
seems

c
boring
series
history

information

highly
page
excellent


table top ten features induced dimension boo domain shaded
columns correspond dimensions selected human judges e e top eigenvectors c c clusters



fiinducing ideal clustering minimal feedback

ele
e
c
mouse
cable
cables
case
red
monster
picture
kit
overall
paid

e
c
music
really
ipod

little
headphones
hard
excellent
need
fit

e
c
easy
used
card
fine


fine
drive
computer
install

e
c
amazon
cable
card
recommend
dvd
camera
fast
far
printer
picture

c
working
never

phone
days
headset
money
months
return
second

c
worked

never
item
amazon
working
support
months
returned
another

c
money
worth
amazon

return
years
much
headphones
sony
received

c
phone

worked
power
battery
unit
set
phones
range
little

table top ten features induced dimension ele domain shaded
columns correspond dimensions selected human judges e e top eigenvectors c c clusters



fidasgupta ng

e
c
love
clean
nice
size
set
kitchen
easily
sturdy
recommend
price
c
months
still
back
never
worked
money

amazon
return
machine

kit
e
e
c
c
works
really
water
nice
clean
works
work

ice
quality
makes
small
thing
sturdy
need
little
keep
think
best
item
c
price
item
set
ordered
amazon
gift
got
quality
received
knives

c

years
love
never
clean
months

pan

pans

e
c
pan
oven
cooking
made
pans
better
heat
cook

clean
c
love
coffee

recommend
makes

size
little
maker
cup

table top ten features induced dimension kit domain shaded
columns correspond dimensions selected human judges e e top eigenvectors c c clusters



fiinducing ideal clustering minimal feedback

e
c
worth
bought
series
money
season
fan
collection
music
tv
thought
c
young

actors
men
cast
seems
job
beautiful
around
director

dvd
e
e
c
c
music
video
collection
music
excellent
found
wonderful
feel
must
bought
loved
workout
perfect
daughter
highly
recommend
makes

special
disappointed
c
worst
money
thought
boring
nothing
minutes
waste
saw
pretty
reviews

c
series
cast
fan
stars
original
comedy
actors
worth
classic
action

e
c
money
quality
video
worth
found
version
picture
waste
special
sound
c
saw
watched
loved
enjoy
whole
got
family
series
season
liked

table top ten features induced dimension dvd domain shaded
columns correspond dimensions selected human judges e e top eigenvectors c c clusters



fidasgupta ng

judge





agreement

pol







mov







kit







boo







dvd







ele







table human agreement rate shown eigenvectors selected five judges
intended dimension example pol judge told intended
clustering politics vs science determined one dimension
relevant intended clustering instructed rank dimensions
terms relevance relevant one would appear first list
dimensions expressed terms ids eigenvectors selected
five judges dataset shown table agreement rate shown
last row table computed highest ranked dimension selected
judge see perfect agreement achieved four five sentiment
datasets remaining two datasets near perfect agreement achieved
together fact took five six minutes identify relevant
dimension indicate asking human determine intended dimension
solely informative features viable task
clustering
next cluster documents dataset dimension selected
majority human judges clustering shown row tables
b comparison best baseline dataset see
performs substantially better boo dvd ele almost level mov
kit slightly worse pol note improvements observed boo dvd
ele attributed failure e capture sentiment dimension perhaps
importantly exploiting human feedback achieved stable
performance across datasets four baselines
identification unambiguous documents
recall features largest mmfr computed unambiguous
documents get idea accurate identifying unambiguous
documents table accuracy obtained unambiguous documents
dataset clustered eigenvector selected majority judges
see accuracy dataset higher corresponding accuracy
shown row table fact accuracy achieved
first baseline since clustering one dimensional space
sensitive choice seeds yielding zero variance ten independent runs



fiinducing ideal clustering minimal feedback

accuracy

pol


mov


kit


boo


dvd


ele


table accuracies unambiguous documents

labels

pol


mov


kit


boo


dvd


ele


table transductive svm
one dataset suggests method identifying unambiguous documents
reasonably accurate
note crucial able achieve high accuracy unambiguous documents clustering accuracy low features induced clusters may
accurate representation corresponding dimension human judge may
difficult time identifying intended dimension fact human judges reported difficulty identifying correct dimension ele dataset attributed
part low accuracy achieved unambiguous documents
user feedback versus labeled data
recall four baselines unsupervised whereas characterized
semi supervised relies user feedback select intended dimension hence
surprising see average clustering performance
better baselines
fairer comparison conduct another experiment compare
semi supervised sentiment classification system uses transductive svm underlying semi supervised learner specifically goal
experiment determine many labeled documents needed order transductive learner achieve level performance answer
question first give transductive learner access documents
dataset unlabeled data next randomly sample unlabeled documents assign
true label train classifier compute accuracy
documents keep adding labeled data iteration reaches
accuracy achieved experiment shown table
owing randomness involved selection unlabeled documents
averaged ten independent runs see user feedback equivalent
effort hand annotating documents per dataset average
multiple relevant eigenvectors
seen table human judges selected one eigenvector
datasets e g pol mov ele however never took
account extra eigenvectors previous experiments better understand


fidasgupta ng

system

pol
acc ari


mov
acc ari


ele
acc ari


table obtained multiple relevant eigenvectors pol mov
ele datasets

accuracy

pol


mov


kit


boo


dvd


ele


table supervised classification accuracies
whether extra eigenvectors help improve accuracy ari conduct another
experiment apply means cluster documents space defined
selected eigenvectors table shows accuracy ari averaged
ten independent runs see pol considerably better
obtained highest ranked eigenvector used suggesting
extra eigenvectors contain useful information however mov ele drop
slightly addition extra eigenvectors indicating extra sentiment
dimensions useful
supervised classification
next present supervised classification five sentiment datasets
one expect largely unsupervised offer comparable performance
fully supervised believe fully supervised enable
reader get sense work stands among existing work identifying
sentiment datasets specifically report table averaged fold crossvalidation accuracies svm classifier trained nine folds tested
remaining fold fold experiment see lag behind supervised
datasets
alternative document representations
experiments represented document bag words
frequent words removed course way represent
document subsection examine two alternative document representations
attempt better understand effect document representation classification
first document representation represent document unigrams
appear remove frequent words document vector
bag words boaw representation motivated fact frequencies
function words shown many studies useful features kinds non topic classification e g finn kushmerick stein argamon
frieder abbasi chen salem koppel schler argamon


fiinducing ideal clustering minimal feedback

system variation
nd eigenvector
top five eigenvectors
interested reader model
nmf
system

pol






mov






accuracy
kit boo






dvd






ele








system variation
nd eigenvector
top five eigenvectors
interested reader model
nmf
system

pol






adjusted rand index ari
mov kit
boo dvd

















ele






b

table terms accuracy b adjusted rand index six datasets
obtained bag words document representation strongest
dataset boldfaced

accuracy ari obtained running four baselines well system
document representation shown tables b respectively comparing tables see words used features best
accuracy achieved dataset drops high frequency words
removed spectral clustering applied similar trends observed ari
shown tables b b overall substantiate hypothesis
retaining high frequency words document representation adverse effect
performance clustering
next experiment another representation specifically one document represented sentiment bearing words contains understand
motivation behind bag sentiment words bosw representation recall introduction one way encourage clustering produce user desired
clustering design feature space contains features
useful producing user desired clustering since desire sentiment clustering design feature space composed solely sentiment bearing words since
hand crafted subjectivity lexicon e lexicon word manually labeled
prior polarity english readily available automatically construct feature
space consists words positive negative polarity according
subjectivity lexicon represent document resulting feature space
prior polarity word polarity computed without regard context word
appears



fidasgupta ng

system variation
nd eigenvector
top five eigenvectors
interested reader model
nmf
system

mov






kit






accuracy
boo dvd








ele








system variation
nd eigenvector
top five eigenvectors
interested reader model
nmf
system

adjusted rand index ari
mov kit boo dvd ele









b

table terms accuracy b adjusted rand index five
sentiment datasets obtained bag sentiment words document representation
strongest dataset boldfaced

goal determine whether bosw document representation improve
sentiment clustering obtained bow representation
identify sentiment bearing words experiment employ subjectivity lexicon introduced work wilson wiebe hoffmann lexicon contains
words hand labeled prior polarity positive negative
neutral create subjectivity lexicon l retain words
wilson et al lexicon positive negative polarity bosw
representation document composed words appear
l document
accuracy ari baselines system obtained employing
bosw representation shown tables b respectively consider first
second eigenvector baseline nmf interested reader model comparison
corresponding tables b bow representation
used see performance improves boo dvd ele datasets
cases drops mov kit datasets top five eigenvectors baseline
performance increases dvd slightly mov drops remaining datasets
finally bosw representation causes performance system drop
datasets
overall seem suggest whether bosw representation document yields better clustering bow representation rather dependent
underlying domain clustering nevertheless see best
see http www cs pitt edu mpqa



fiinducing ideal clustering minimal feedback

clustering accuracy ari achieved sentiment dataset bosw representation significantly lower obtained bow representation speculate
two reasons poorer first general purpose subjectivity lexicon
cover sentiment bearing words particular words sentiment oriented
context particular domain neutral polarity otherwise may omitted bosw document representation second non sentiment bearing words
might useful identifying sentiment
domain adaptation
mentioned introduction majority existing approaches sentiment classification supervised one weakness supervised approaches given
domain one needs go expensive process collecting large amount
annotated data order train accurate polarity classifier one may argue
active clustering suffers weakness user needs identify
sentiment dimension domain one way address weakness domain
adaptation specifically investigate whether sentiment dimension manually identified one domain henceforth source domain used automatically identify
sentiment dimension domain henceforth target domain hypothesize
domain adaptation feasible especially two domains sentimentally similar e significant overlap features characterize sentiment
dimensions two domains
propose following method automatically identifying sentiment
dimension target domain sentiment dimension manually identified
source domain x assume sentiment dimension domain x defined
x
x
eigenvector ex moreover assume c e c e two vectors top ranked
features obtained mmfr characterize two clusters induced ex
features cluster given target domain first compute similarity
ex ys top eigenvectors ey ey similarity two
eigenvectors ex ey defined
x



x



x



x



max c e c e c e c e c e c e c e c e
similarity function computes similarity two feature vectors
experiments simply set dot product allows us capture
degree overlap two feature vectors posit eigenvector
ey ey highest overlap one defines sentiment dimension
determine effectiveness method compare automatically selected
eigenvector human selected eigenvector domain shown
table row column j indicates sentiment dimension
target domain j successfully identified sentiment dimension manually
collecting annotated data trivial dealing review data necessarily
true kinds data instance people express opinions sentiment political blogs
floor debates associated postings transcripts may explicitly annotated
sentiment labels
note two arguments max function correspond two different ways creating
mapping feature vectors two domains



fidasgupta ng

domain
mov
dvd
boo
ele
kit

mov


n
n
n

dvd



n


boo
n


n
n

ele
n
n
n



kit
n





table domain adaptation

identified source domain n indicates failure instance know
sentiment dimension dvd domain human feedback domain
adaptation method used correctly identify sentiment domain mov
vice versa however domain adaptation method successful
instance knowing sentiment dimension mov allow us correctly predict
sentiment dimension ele interestingly ignore boo kit pair domain
adaptation exhibits symmetry symmetry mean domain x used
identify correct sentiment dimension domain domain used
identify correct sentiment dimension domain x intuitively makes sense
x successfully used identify sentiment dimension likely
two domains share lot sentiment words consequently adapt x
likely successful boo kit pair represents case domain adaptation
successful one direction domain adaptation successful boo kit
similarity sentiment dimensions two domains high see
discussion next paragraph details contributes failure adaptation
direction
mentioned beginning subsection hypothesize domain adaptation likely successful two domains consideration similar
test hypothesis table similarity manually
identified eigenvector corresponding automatically identified eigenvector
pair domains three points deserve mention first long similarity value
least domain adaptation successful long similarity value
domain adaptation unsuccessful hence substantiate hypothesis
domain adaptation likely successful two domains consideration
similar would interesting see two thresholds
used predict whether domain adaptation successful given pair domains
second domain adaptation directions likely successful similarity
value sufficiently high mentioned similarity value high
two domains share many sentiment words common may turn contribute
successful domain adaptation directions five domains considering
long similarity value least domain adaptation directions
successful third worth reiterating even similarity value falls
threshold imply domain adaptation fail mentioned
sentiment dimension domain correctly identified long similarity


fiinducing ideal clustering minimal feedback

domain
mov
dvd
boo
ele
kit

mov






dvd






boo






ele






kit






boo






ele






kit






boo






ele






kit








domain
mov
dvd
boo
ele
kit

mov






dvd





b

domain
mov
dvd
boo
ele
kit

mov






dvd





c

table similarity domain adaptation shows similarity
sentiment eigenvector source domain eigenvector similar target
domain b shows similarity sentiment eigenvector source domain
second similar eigenvector target domain c shows similarity gap
difference corresponding entries b

sentiment dimension domain x highest among four eigenvectors
case boo kit domain pair
far attempted correlate success domain adaptation similarity manually selected eigenvector source domain eigenvector
similar target domain may worth consider similarity
manually selected eigenvector second similar eigenvector
target domain gap similarity may give indication success domain adaptation determine whether better correlation success
domain adaptation similarity gap compute similarity
eigenvector manually selected source domain second similar eigenvector
target domain see table b well similarity gap see table c
simply difference corresponding entries tables b
see table c appears correlation success
domain adaptation gap values particular gap value least domain


fidasgupta ng

adaptation successful however gap value domain adaptation unsuccessful nevertheless gap values help predict domain pairs
success domain adaptation cannot predicted similarity values table
e g domain pairs low similarity yet domain adaptable moreover
fail predict success domain adaptation many domain pairs specifically
gap value
subjectivity lexicon versus human feedback
one might argue access subjectivity lexicon could use automatically identify right sentiment dimension thus obviating need human feedback
altogether subsection investigate whether indeed feasible use handbuilt general purpose sentiment lexicon identify eigenvector corresponds
sentiment dimension domain
experiment use subjectivity lexicon l described section
mentioned l contains words wilson et al subjectivity
lexicon marked prior polarity positive negative procedure
automatically identifying sentiment dimension l similar one described
domain adaptation section second fifth eigenvectors first
compute similarity eigenvector l choose eigenvector
highest similarity l domain adaptation compute similarity
l eigenvector ex
x

x

x

x

max c l c e c l c e c l c e c l c e
c l c l represent words l labeled positive negative rex
x
spectively c e c e top ranked features obtained mmfr
characterize two clusters induced ex features cluster similarity function computes similarity two feature vectors domain
adaptation simply set dot product
indicate successfully identified right eigenvector l
five domains note l general purpose e domain independent
lexicon containing generic sentiment bearing words good enough identify
correct sentiment dimension five different domains worth noting sentiment
dimension mov domain highest similarity l e five
domains suggesting highest ranked sentiment features mov domain according mmfr largely generic dvd second largest similarity l
followed boo kit ele comparatively low similarity values
kit ele indicative fact highest ranked sentiment features
largely domain specific
finally although subjectivity lexicon obviates need human feedback
emphasize undermine contribution feedback oriented clustering
technique following reasons first thinking text mining perspective would
good knowledge free possible employing handcrafted subjectivity lexicon makes system resource dependent fact subjectivity
lexicon may readily available vast majority natural languages second


fiinducing ideal clustering minimal feedback

want method potentially applicable non sentiment domains e g spam vs
spam faced hand built lexicon may
available
single data multiple clusterings
mentioned previously set documents clustered along different dimensions
example movie reviews clustered sentiment positive vs negative genre
e g action romantic documentary natural question produce different
clusterings given set documents corresponds different dimension
vast majority existing text clustering answer
cluster along exactly one dimension typically prominent dimension
hand since induces important clustering dimensions
dataset principle used produce distinct clustering
hypothesize generate multiple clusterings given dataset along important
dimensions
test claim produce multiple clusterings evaluate
four datasets possess multiple clustering dimensions namely mov dvd boodvd dvd ele mov kit example boo dvd dataset consists
reviews taken boo dvd domains hence augmented dataset
composed reviews two contributing domains
clustered according topic e g book vs dvd sentiment note
four pairs domains used create augmented datasets chosen carefully
specifically two augmented datasets mov dvd boo dvd created
constituent domains mutually domain adaptable according table
remaining two dvd ele mov kit created constituent domains
domain adaptable goal see whether active clustering able
produce topic sentiment clusterings datasets different levels
sentimental similarity
clustering procedure almost identical one described section essence
compute top five eigenvectors laplacian matrix learn top ranked
features corresponding e e according mmfr ask human judges
identify eigenvectors corresponding topic dimension sentiment
dimension use means produce two clusterings reviews one according
selected topic dimension selected sentiment dimension
section conducted human automatic experiments determine viability

reason employing augmented datasets obviate need
additional human annotations guarantee least two dimensions along
clusters formed thus allowing us directly test ability produce multiple clusterings
possible evaluate ability generate multiple clusterings mov
dataset clustering along genre sentiment decided leave future investigation since
documents mov annotated genre information
confused topic sentiment mixture mei ling wondra su zhai
goal first use topic mine major aspects product online review
assign ratings extracted aspect hand goal design clustering
capable generating multiple clusterings dataset



fidasgupta ng

judge





agreement

mov dvd







boo dvd







dvd ele







mov kit







dvd ele







mov kit









judge





agreement

mov dvd







boo dvd






b

table human agreement rate selecting topic dimension b sentiment
dimension augmented datasets shown eigenvectors selected
human judges

human experiments
employed five human judges involved human experiments section
independently determine topic dimension sentiment dimension
four augmented datasets top features according mmfr
human judge identifies one relevant eigenvector particular dimension
ask rank eigenvectors according relevance finally take topic sentiment
dimension ranked first largest number judges human selected
topic sentiment dimension
tables b respectively topic sentiment dimensions expressed
terms ids eigenvectors selected five judges augmented
dataset shown tables human agreement rate computed
highest ranked dimension selected judge several points
human experiments deserve mention
first dataset human judges managed one eigenvector
top five corresponds topic least one eigenvector corresponds
sentiment perhaps importantly human agreement rate least
achieved four datasets respect selecting eigenvector correspond
topic sentiment dimensions together provide suggestive evidence
eigen decomposition procedure active clustering effective
enough unearth topic sentiment dimensions present


fiinducing ideal clustering minimal feedback

dataset proposal incorporating user feedback via inspecting small
number features viable
second topic sentiment prominent dimensions datasets
fact second eigenvector captures topic dimension four datasets suggests
topic prominent dimension sentiment fact human judges
reported topic dimension identified quite easily achieving perfect agreement
identifying topic dimension provides empirical evidence speculation
topic typically though prominent dimension sentiment
dimensions exist dataset
third reasonably high human agreement rate identifying sentiment dimension achieved perfect agreement two datasets agreement rate
remaining two see table b details human judges reported difficult identify sentiment dimension especially two datasets composed
sentimentally dissimilar domains
attempt gain insight judges found difficult identify
sentiment dimension tables top ranked features induced
dimension mmfr four augmented datasets lightly shaded columns
correspond eigenvectors chosen topic dimension darkly shaded columns
correspond eigenvectors chosen sentiment dimension examining
believe points deserve mention
first top features generated sentiment eigenvector mov dvd
boo dvd two datasets composed sentimentally similar constituent domains
clearly sentiment oriented making relatively easy human judges determine
sentiment eigenvector case dvd ele mov kit two datasets
composed dissimilar domains top features noisier e many
necessarily sentiment oriented thus making tougher judges locate
sentiment eigenvector fact one see top features generated
sentiment eigenvector tables mov dvd boo dvd
clearly sentiment oriented dvd ele mov kit
surprising sentimentally dissimilar constituent domains noisier top features generated sentiment eigenvector
however constituent domains sentimentally similar tend many
sentiment bearing words common implies sentiment bearing words
appear frequently augmented datasets constituent datasets
hence combining two domains helps boost influence sentiment bearing
words increasing chance appearing higher list features ranked
mmfr reinforcement effect intuitively explains sentiment eigenvector
clearly dominated sentiment words datasets composed sentimentally similar domains hand constituent domains sentimentally dissimilar tend
many sentiment bearing words common influence
sentiment bearing words present one two constituent domains
diluted larger number non sentiment bearing words combining
two domains words features clearly sentiment oriented
one rather domains may longer appear sufficiently high ranked list
features fact saw tables sentiment eigenvector contaminated


fidasgupta ng

e
c
roles
drama
murder
meets
crime
supporting
involving
convincing
tale
lead
c
bought
season
buy
disappointed
fan
amazon
buying
copy
dvds
watched

mov dvd
e
e
c
c
wonderful recommend
excellent
fan
beautiful
liked
personal
book
collection
read
view
excellent
art
amazing
highly
definitely
fantastic
highly
deal
absolutely
c
stupid
boring
dull
mean
terrible
save
lame
run
guys
except

c
buy
house
rent
waste
wait
kill
murder
obvious
season
dvds

e
c
kids
children
loved
child
son
daughter
boy
school
wonderful
heart
c
quality
dark
war
horror
release
fan
earth
production
suspense
sound

table top ten features induced dimension mov dvd domain
lightly darkly shaded columns correspond topic sentiment dimensions respectively
selected human judges e e top eigenvectors c c clusters



fiinducing ideal clustering minimal feedback

e
c
reader
important
subject
understanding
modern
information
examples
political
business
nature
c
saw
watched
actors
liked
music
season
humor
comedy
favorite
ending

boo dvd
e
e
c
c
bought
excellent
disappointed wonderful
easy
highly
information collection
price
music
waste
special
workout
classic
helpful
video
expected
perfect
reviews
amazing

e
c
loved
enjoyed
children
year
wonderful
child
fun
son
friends
highly

c
young
men
cast
role
actors
script
scene
war
performance
action

c
version
quality
waste
worst
review
original
edition
collection
amazon
format

c
boring
ending
waste
reviews

novel
maybe

stupid
finish

table top ten features induced dimension boo dvd domain
lightly darkly shaded columns correspond topic sentiment dimensions respectively
selected human judges e e top eigenvectors c c clusters



fidasgupta ng

e
c
funny
acting
family
actors
action
plot
enjoy
young
wonderful
comedy
c
unit
battery
purchased
device

tried
working
plug
charge
computer

dvd ele
e
e
c
c
easy
fine
small

perfect
worked
excellent
months
highly
easy
nice
working
low
computer
comfortable
day
ipod
card
headphones
drive
c
amazon
item
review
company
return
took
check
saw
card
worked

c
amazon
tv
purchase
disappointed
item
purchased
reviews
wanted
received
ipod

e
c
video
card
camera
fast
easy
cable
picture
pictures

digital
c
phone
waste
unit
battery
getting
low
power
hear
worst
batteries

table top ten features induced dimension dvd ele domain
lightly darkly shaded columns correspond topic sentiment dimensions respectively
selected human judges e e top eigenvectors c c clusters



fiinducing ideal clustering minimal feedback

e
c
james
directed
sex
hour
drama
relationship
death
direction
tv
michael
c
food
recommend
pot
purchased
mine
kitchen
mixer
handle
size
store

mov kit
e
e
c
c
pan
coffee
cooking
clean
clean
machine
pans
ice
cook
maker
heat
plastic
oven
cup
heavy
fill
food
months
stick
working
c
months
purchased
worked
broke
amazon
coffee
replacement
month
tried
service

c
item
price
sheets
ordered
amazon
received
beautiful
dishes
arrived
sets

e
c
price
clean
kitchen
knife
knives
size
sharp
dishwasher
cutting
attractive
c
pan
toaster
oven
pans
heat
return
bottom
worked
read
toast

table top ten features induced dimension mov kit domain
lightly darkly shaded columns correspond topic sentiment dimensions respectively
selected human judges e e top eigenvectors c c clusters



fidasgupta ng

nd eigenvector
top five eigenvectors
interested reader model
nmf
system

mov dvd
acc ari






boo dvd
acc ari






dvd ele
acc ari






mov kit
acc ari






dvd ele
acc ari






mov kit
acc ari








nd eigenvector
top five eigenvectors
interested reader model
nmf
system

mov dvd
acc ari






boo dvd
acc ari





b

table topic clustering b sentiment clustering
four augmented datasets strongest dataset boldfaced

number features necessarily sentiment bearing make difficult
human judges identify sentiment dimension
another interesting point note datasets seems
one eigenvector correspond sentiment instance boo dvd dataset
five human judges agreed e e correspond sentiment dimension
closer examination two eigenvectors shown table reveals interesting
pattern e positive features c came dvd domain negative
features c came boo domain whereas e positive features
c came boo domain negative features c came dvd
words e partitions reviews according positive dvd negative
boo whereas e reverse suggests eigen decomposition procedure
smart enough merge positive negative sentiment bearing words
two domains together perhaps even importantly e e
partitioning reviews along sentiment dimension topic dimension
clustering
rows tables b topic sentiment clustering
four baseline text clustering described section note
baselines produce one clustering documents per dataset
hence baseline topic clustering produced comparing
clustering gold standard topic clustering sentiment


fiinducing ideal clustering minimal feedback

clustering produced comparing clustering gold standard
sentiment clustering
see topic table baseline cluster
second eigenvector achieves best average clustering four
augmented datasets potentially attributed fact e corresponds
topic dimension four datasets according human judges described
human experiments however clustering e produce best clustering
four datasets fact interested reader model achieves best
mov dvd dvd ele mov kit nevertheless boo dvd
worst among baselines true top five eigenvectors baseline
nmf yielded poor mov dvd addition nmfs
boo dvd mov kit promising
far sentiment baseline clustering concerned see rows
table b best average performance achieved nmf except three cases
nmf mov dvd mov kit well top five eigenvectors mov dvd
baseline particularly promising accuracy low fifties
ari close zero
topic sentiment clustering produced shown
row tables b specifically obtained grouping
reviews according eigenvectors manually selected topic sentiment dimensions respectively hence unlike baselines topic clustering
sentiment clustering produced different
cases human judges selected one eigenvector dimension use eigenvector ranked first frequently see
accuracies topic clustering reasonably high ranging
suggest possible achieve high performance topic
precisely domain clustering dataset even another prominent clustering dimension e sentiment present hand despite existence eigenvectors
clearly capture sentiment dimension datasets e g e mov dvd
dataset sentiment clustering accuracies ari values lower
topic clustering potentially attributed reason mentioned
introduction fact reviews sentimentally ambiguous makes non trivial
classify comparison four baselines achieves best
average performance four datasets comparatively stable performance
across datasets
worth noting sentiment clustering produced
mov dvd boo dvd higher dvd ele mov kit
perhaps surprising discussed human judges found difficult
identify sentiment eigenvector dvd ele mov kit mov dvd
boo dvd owing part fact many top ranked features sentiment
eigenvector dvd ele mov kit sentiment oriented turn
attributed fact datasets correspond domain pairs
sentimentally dissimilar mentioned two sentimentally dissimilar constituent
domains tend many sentiment bearing words common consequently
influence sentiment bearing words present one two constituent


fidasgupta ng

domains diluted larger number non sentiment bearing words
combining two domains making difficult produce good sentiment
clustering hand combining two domains helps boost influence
sentiment bearing words increasing chance appearing higher
list features ranked mmfr producing good sentiment clustering
interestingly achieves better topic clustering two
datasets dvd ele mov kit achieves poorer sentiment clustering fact topic clustering accuracies dvd ele mov kit
near perfect dvd ele mov kit respectively
means coincidence constituent domains augmented dataset highly
dissimilar e word usage tends differ considerably topic clusters well separated hence high topic clustering
achieved similar line reasoning explain finds comparatively
difficult produce good topic clustering mov dvd boo dvd
constituent domains similar
seem suggest higher topic accuracy ari implies lower
sentiment accuracy ari vice versa speculate constituent
domains similar sentiment bearing features tend similar
sentiment tend good topic tend poor additional
experiments needed determine reason
overall provide supporting evidence feedback oriented
produce multiple clusterings dataset particular even though sentimentbased clustering accuracies high topic clustering accuracies
augmented datasets current level performance arguably reasonable especially considering fact sentiment clustering challenging task
traditional clustering fail even produce one clustering
multiple relevant eigenvectors
recall table b four augmented datasets least one
judge indicated one eigenvector relevant sentiment dimension
however producing sentiment clustering system table b used eigenvector ranked frequently human judges
better understand whether relevant eigenvectors help improve sentiment clustering repeat experiment apply means
cluster documents space defined eigenvectors determined
relevant least one judge specifically cluster following set
eigenvectors mov dvd boo dvd dvd ele
mov kit
accuracy ari experiment shown table comparison
last row table b see additional relevant eigenvectors
yields better boo dvd dataset may easy
determine reason believe poorer observed boo dvd
attributed impurity e captures sentiment topic
discussed hand additional sentiment eigenvectors chosen


fiinducing ideal clustering minimal feedback

system

mov dvd
acc ari


boo dvd
acc ari


dvd ele
acc ari


mov kit
acc ari


table sentiment clustering obtained multiple relevant eigenvectors four augmented datasets
three augmented datasets seem impurity
capture sentiment dimension one constituent domains

significance work
believe significant following aspects
producing clustering according user interest proposed novel framework enabled spectral clustering take account human
feedback produce clustering along dimension interest user
particularly appealing aspect concerned relatively minimal human feedback demands user needs take cursory look
small number features representative induced dimension
worth noting human inspect select automatically induced
clustering dimension form interaction human clustering
enables human easily engage clustering tasks help improve performance easy low effort manner believe
belongs emerging family interactive allows user
make small guiding tweaks thereby get much better would otherwise
possible future information retrieval
inducing human interpretable clustering dimensions dimensions produced spectral clustering dimensionality reduction e g latent
semantic indexing lsi deerwester dumais furnas landauer harshman
generally considered non interpretable sebastiani unlike dimension
original feature space typically corresponds word type therefore interpreted human easily preliminary study challenge
common wisdom context text clustering dimension
low dimensional space induced spectral clustering interpreted
human believe ability produce human interpretable dimensions enables us
employ spectral clustering perhaps dimensionality reduction clustering text processing intelligent manner especially
case respect selecting dimensions pertinent task
hand example existing applications spectral clustering topic
clustering task e g xu et al cai liu hu deng guo xu
dimensions low dimensional space typically used since
showed dimensions produced spectral clustering dataset
necessarily topic related potentially improve topic clustering


fidasgupta ng

employing non topic related dimensions clustering process addition
since induced dimensions correspond non topic dimensions
use produce non topic clusterings particular given recent surge
interest nlp community text classification along non topic dimensions
sentiment gender e g garera yarowsky jurafsky ranganath
mcfarland offers solution tasks rely
labeled data unlike majority existing approaches non topic text classification supervised nature overall believe nlp researchers
fully exploited power spectral clustering hence rewards
understanding spectral clustering light may significant
producing multiple clusterings majority existing text clustering
produce single clustering dataset potentially
used produce multiple clusterings one along important clustering
dimensions induced via novel application spectral clustering
finally worth mentioning task inducing clustering dimensions reminiscent influential topic modeling task blei ng jordon whose goal
discover major topics set documents unsupervised manner note
two tasks fundamentally different topic model attempts discover major
topics set documents dimension model aims discover major clustering
dimensions nevertheless two bear resemblance many ways
first employ clustering discover information text collection unsupervised manner second display learned information human
representative words topic model represents induced topic words
representative topic dimension model represents induced clustering
dimension words representative two document clusters involved dimension finally induced topics clustering dimensions human recognizable
human needed assign labels believe induction
clustering dimensions potential substantially enhance capability existing
text analysis discover knowledge text collection unsupervised
manner complementing information induced topic model

related work
introduction discussed related work producing user desired clustering
section focus discussing related work topic clustering classification sentiment classification active learning producing multiple clusterings computational
stylistics
topic text clustering traditional text clustering focused primarily topic clustering owing large part darpas topic detection
tracking initiative many different clustering used including non hierarhical k means expectation maximization em
hierarchical single link complete link group average singlepass hatzivassiloglou gravano maganti cluster given set


fiinducing ideal clustering minimal feedback

documents feature space typically spanned unigrams however
clustering high dimensional space allow distance two documents reliably computed due curse dimensionality consequently
recent work focused development cluster documents lowdimensional space constructed via dimensionality reduction representative members
family dimensionality reduction clustering include traditional lsi deerwester et al well recently proposed
arguably better performing spectral clustering shi malik
ng et al non negative matrix factorization xu et al locality preserving indexing et al locality discriminating indexing hu et al despite
development clustering primarily evaluated
respect ability produce topic clusterings
topic text classification yang liu put text classification
inherently supervised learning task fact arguably one popular
tasks supervised learning techniques applied information retrieval
community see sebastiani comprehensive overview related work
machine learning text classification nevertheless annotated documents
needed training high performance supervised text classifier expensive
obtain researchers investigated possibility performing text
classification little even labeled data attempts led development
general purpose semi supervised text classification combine labeled
unlabeled data transduction joachims b em nigam mccallum thrun
mitchell latter used combination active learning mccallum nigam recently sandler proposed unsupervised text
classification mixture modeling lsi dimensionality
reduction
sentiment classification mentioned introduction despite large amount
recent work sentiment analysis opinion mining much focused supervised
methods see pang lee comprehensive survey field one weakness existing supervised polarity classification systems typically
domain language specific hence given domain language one needs
go expensive process collecting large amount annotated data order
train high performance polarity classifier recent attempts made
leverage existing sentiment corpora lexicons automatically create annotated resources
domains languages however methods require existence
parallel corpus machine translation engine projecting translating annotations lexicons
resource rich language target language banea mihalcea wiebe hassan
wan domain similar enough target domain blitzer et al
target domain language fails meet requirement sentiment
clustering unsupervised polarity classification become appealing alternatives unfortunately exceptions e g semi supervised sentiment analysis riloff wiebe
sindhwani melville dasgupta ng li zhang sindhwani
tasks largely investigated nlp community turneys work
perhaps one notable examples unsupervised polarity classification however


fidasgupta ng

system learns semantic orientation phrases review unsupervised manner information used predict polarity review heuristically
domain adaptation domain adaptation known transfer learning one
focal areas machine learning nlp recent years goal
leverage labeled data available one domain source domain build
classifier another domain target domain techniques domain adaptation
applied nlp tasks including part speech tagging noun phrase chunking
syntactic parsing named entity recognition word sense disambiguation e g daume iii
marcu chan ng duame iii jiang zhai b
particular relevance work domain adaptation techniques specifically developed
text sentiment classification e g blitzer mcdonald pereira finn
kushmerick blitzer et al gao fan jiang han ling dai xue yang
yu tan cheng wang xu worth noting domain adaptation
setting different traditional setting traditionally sophisticated classifiers
automatically constructed mapping features two domains used
adaptation process setting however simply utilize sentiment dimension
manually selected source domain automatically identify sentiment
dimension target domain
active clustering active learning heavily investigated machine learning paradigm
aims achieve better generalization bounds lower annotation costs cohn atlas
ladner traditional active learning setting human requested
annotate data points classifier uncertain e g cohn et al
recent active learning involved asking human identify label
features useful classification task hand e g bekkerman et al
raghavan allan druck settles mccallum roth small
mentioned introduction active learning applied clustering setting
goal encouraging produce user intended clustering
data clustered along multiple dimensions different variants active clustering
proposed request human label pair data points must link
cannot link indicate whether two points must must reside cluster
e g wagstaff et al bilenko basu mooney others human
determine whether two clusters merged split hierarchical clustering
process e g balcan blum active clustering yet another variant
ask human select clustering desires set automatically produced
clusterings
generation multiple clusterings notion text collections may clustered
multiple independent ways discussed literature computational stylistics
see lim lee kim biber kurjian grieve smith tambouratzis
vassiliou gries wulff davies example machine learning
attempts design producing multiple clusterings dataset
operate semi supervised setting e g gondek hofmann
davidson qi totally unsupervised e g caruana elhawary nguyen
smith jain meka dhillon instance caruana et al meta
clustering produces different clusterings dataset running k means


fiinducing ideal clustering minimal feedback

times time random selection seeds random weighting features
goal present local minimum found k means possible clustering however
propose mechanism determining clusterings one
user desires relies spectral clustering rather k means
producing multiple clusterings fills gap soliciting user feedback determine
user desired clustering

conclusions future work
unsupervised clustering typically group objects along prominent dimension part owing objective simultaneously maximizing inter cluster similarity intra cluster dissimilarity hence users intended clustering dimension
prominent dimension unsupervised clustering fail
miserably address proposed active clustering
allows us mine user intended possibly hidden dimension data produce
desired clustering mechanism differs competing methods requires
limited feedback select intended dimension user needs inspect
small number features demonstrated viability via set human automatic
experiments challenging yet investigated task sentiment clustering obtaining promising additional experiments provided suggestive evidence
domain adaptation successfully applied identify sentiment dimension
domain domains consideration sentimentally similar hand crafted
subjectivity lexicon available used replace user feedback needed select
sentiment eigenvector domain potentially used
produce multiple clusterings datasets possess multiple clustering dimensions
equally importantly empirically demonstrated possible human
interpret dimension produced spectral clustering contrary common
wisdom dimensions automatically constructed rank reduced space noninterpretable believe nlp researchers fully exploited power spectral
clustering hence rewards understanding spectral clustering light
may significant finally proposal represent induced clustering dimension
sets informative features facilitates exploratory text analysis potentially enhancing
capability existing text analysis complementing information provided
unsupervised e g topic model
future work plan explore several extensions active clustering
first active clustering potentially used produce multiple clusterings dataset one interesting future direction would examine theoretical
guarantees determining whether able produce distinct clusterings qualitatively strong see dasgupta ng b example second plan use
combination existing feedback oriented methods e g bekkerman et al
roth small improving performance instance instead
user construct relevant feature space scratch simply extend set
informative features identified user selected dimension third since none
steps specifically designed sentiment classification plan apply
non topic text classification tasks recently received lot

fidasgupta ng

terest nlp community gender classification e task determining
gender author document finally plan adopt richer representation
document exploits features polarity oriented words obtained hand built
machine learned sentiment lexicons e g hu liu wiebe wilson bruce bell
martin andreevskaia bergler mohammad dunne dorr rao
ravichandran derived finer grained e sentential sub sentential
phrase sentiment analysis methods e g wilson et al kennedy inkpen
polanyi zaenen mcdonald hannan neylon wells reynar choi
cardie richer features may make easier user identify
desired dimension method

bibliographic note
portions work previously presented conference publication dasgupta
ng b current article extends work several ways notably
detailed introduction spectral clustering section inclusion two
baseline systems section investigation effect document representation
clustering performance section addition three sections focusing
issues domain adaptation section employing manually constructed subjectivity
lexicon section producing multiple clusterings dataset section well
description significance work section

acknowledgments
authors acknowledge support national science foundation nsf grant iis thank four anonymous reviewers helpful comments
unanimously recommending article publication jair opinions findings
conclusions recommendations expressed article authors
necessarily reflect views official policies expressed implied nsf

references
abbasi chen h salem sentiment analysis multiple languages feature
selection opinion classification web forums acm transactions information
systems
andreevskaia bergler mining wordnet fuzzy sentiment sentiment
tag extraction wordnet glosses proceedings th conference
european chapter association computational linguistics eacl pp

balcan f blum clustering interactive feedback proceedings
th international conference algorithmic learning theory alt pp

banea c mihalcea r wiebe j hassan multilingual subjectivity analysis machine translation proceedings conference empirical
methods natural language processing emnlp pp


fiinducing ideal clustering minimal feedback

bekkerman r raghavan h allan j eguchi k interactive clustering
text collections according user specified criterion proceedings th
international joint conference artificial intelligence ijcai pp
biber kurjian j towards taxonomy web registers text types
multidimensional analysis language computers
bilenko basu mooney r j integrating constraints machine learning
semi supervised clustering proceedings st international conference
machine learning icml pp
blei ng jordon latent dirichlet allocation journal
machine learning
blitzer j dredze pereira f biographies bollywood boom boxes
blenders domain adaptation sentiment classification proceedings th
annual meeting association computational linguistics acl pp
blitzer j mcdonald r pereira f domain adaptation structural correspondence learning proceedings conference empirical methods
natural language processing emnlp pp
cai x han j document clustering locality preserving indexing
ieee transactions knowledge data engineering
caruana r elhawary f nguyen n smith c meta clustering
proceedings th ieee international conference data mining icdm pp

chan p k schlag f zien j spectral k way ratio cut partitioning
clustering ieee transactions computer aided design
chan ng h domain adaptation active learning word sense
disambiguation proceedings th annual meeting association
computational linguistics acl pp
choi cardie c learning compositional semantics structural inference subsentential sentiment analysis proceedings conference
empirical methods natural language processing emnlp pp
cohn atlas l ladner r improving generalization active learning
machine learning
dasgupta ng v mine easy classify hard semi supervised automatic sentiment classification proceedings joint conference
th annual meeting acl th international joint conference
natural language processing afnlp acl ijcnlp pp
dasgupta ng v b topic wise sentiment wise otherwise identifying
hidden dimension unsupervised text classification proceedings
conference empirical methods natural language processing emnlp pp

dasgupta ng v mining clustering dimensions proceedings th
international conference machine learning icml pp


fidasgupta ng

dasgupta ng v b towards subjectifying text clustering proceedings
rd annual international acm sigir conference development
information retrieval sigir pp
daume iii h marcu domain adaptation statistical classifiers journal
artificial intelligence
davidson qi z finding alternative clusterings constraints proceedings th ieee international conference data mining icdm pp
deerwester dumais furnas g w landauer k harshman r
indexing latent semantic analysis journal american society information
science
dhillon guan kulis b kernel k means spectral clustering normalized cuts proceedings th acm sigkdd international conference
knowledge discovery data mining kdd pp
ding c x zha h gu simon h min max cut
graph partitioning data clustering proceedings international
conference data mining icdm pp
druck g settles b mccallum active learning labeling features proceedings conference empirical methods natural language processing
emnlp pp
duame iii h frustratingly easy domain adaptation proceedings th
annual meeting association computational linguistics acl pp
finn kushmerick n learning classify documents according genre
journal american society information science technology

fung g disputed federalist papers svm feature selection via concave minimization proceedings conference diversity computing pp

gao j fan w jiang j han j knowledge transfer via multiple model local
structure mapping proceeding th acm sigkdd international conference
knowledge discovery data mining kdd pp
garera n yarowsky modeling latent biographic attributes conversational
genres proceedings joint conference th annual meeting
acl th international joint conference natural language processing
afnlp acl ijcnlp pp
gilad bachrach r navot tishby n margin feature selection theory
proceedings st international conference machine
learning icml pp
gondek hofmann non redundant data clustering proceedings
th ieee international conference data mining icdm pp
gries wulff davies corpus linguistic applications current studies
directions rodopi


fiinducing ideal clustering minimal feedback

grieve smith envelope variation multidimensional register genre
analyses language computers
hatzivassiloglou v gravano l maganti investigation linguistic
features clustering topical document clustering proceedings
rd annual international acm sigir conference development
information retrieval sigir pp
x cai liu h w locality preserving indexing document
representation proceedings th annual international acm sigir conference development information retrieval sigir pp
hu j deng w guo j xu w locality discriminating indexing document
classification proceedings th annual international acm sigir conference
development information retrieval sigir poster pp

hu liu b mining opinion features customer reviews proceedings
th national conference artificial intelligence aaai pp
jain p meka r dhillon simultaneous unsupervised learning disparate
clusterings proceedings siam international conference data mining sdm
pp
jiang j zhai c instance weighting domain adaptation nlp proceedings th annual meeting association computational linguistics
acl pp
jiang j zhai c b two stage domain adaptation statistical
classifiers proceedings th conference information knowledge
management cikm pp
joachims making large scale svm learning practical scholkopf b
smola eds advances kernel methods support vector learning pp
mit press
joachims b transductive inference text classification support vector
machines proceedings th international conference machine learning
icml pp
jurafsky ranganath r mcfarland extracting social meaning identifying interactional style spoken conversation proceedings human language
technologies annual conference north american chapter
association computational linguistics naacl hlt pp
kamvar klein manning c spectral learning proceedings th
international joint conference artificial intelligence ijcai pp
kannan r vempala vetta clusterings good bad spectral
journal acm
kennedy inkpen sentiment classifiation movie reviews contextual
valence shifters computational intelligence


fidasgupta ng

koppel schler j argamon computational methods authorship attribution journal american society information science technology

kugler aoki k kuroyanagi iwata nugroho feature subset
selection support vector machines confident margin proceedings
ieee international joint conference neural networks ijcnn pp
kulis b basu dhillon mooney r semi supervised graph clustering kernel machine learning
li zhang sindhwani v non negative matrix tri factorization
sentiment classification lexical prior knowledge proceedings joint
conference th annual meeting acl th international joint
conference natural language processing afnlp acl ijcnlp pp

lim c lee k kim g multiple sets features automatic genre classification web documents information processing management
ling x dai w xue g yang q yu spectral domain transfer learning
proceeding th acm sigkdd international conference knowledge
discovery data mining kdd pp
liu b li x lee w yu p text classification labeling words
proceedings th national conference artificial intelligence aaai pp

mccallum k nigam k employing em pool active learning
text classification proceedings th international conference machine
learning icml pp madison wi morgan kaufmann
mcdonald r hannan k neylon wells reynar j structured
fine coarse sentiment analysis proceedings th annual meeting
association computational linguistics acl pp
mei q ling x wondra su h zhai c sentiment mixture modeling
facets opinions weblogs proceedings th world wide web conference
www pp
mohammad dunne c dorr b generating high coverage semantic orientation lexicons overtly marked words thesaurus proceedings
conference empirical methods natural language processing emnlp
pp
ng jordan weiss spectral clustering analysis
advances neural information processing systems nips
nigam k mccallum thrun mitchell text classification labeled
unlabeled documents em machine learning
pang b lee l opinion mining sentiment analysis foundations trends
information retrieval


fiinducing ideal clustering minimal feedback

pang b lee l vaithyanathan thumbs sentiment classification
machine learning techniques proceedings conference empirical
methods natural language processing emnlp pp association computational linguistics
polanyi l zaenen contextual valence shifters computing attitude
affect text theory applications springer verlag
raghavan h allan j interactive asking incorporating
feature feedback support vector machines proceedings th annual
international acm sigir conference development information
retrieval sigir pp
rao ravichandran semi supervised polarity lexicon induction proceedings th conference european chapter association computational linguistics eacl pp
riloff e wiebe j learning extraction patterns subjective expressions
proceedings conference empirical methods natural language
processing emnlp pp
roth small k interactive feature space construction semantic information proceedings th conference computational natural language
learning conll pp
sandler use linear programming unsupervised text classification
proceedings th acm sigkdd international conference knowledge
discovery data mining kdd pp
sebastiani f machine learning automated text categorization acm computing
surveys
shi j malik j normalized cuts image segmentation ieee transactions
pattern analysis machine intelligence
sindhwani v melville p document word co regularization semi supervised
sentiment analysis proceedings th ieee international conference data
mining icdm pp
stein argamon frieder effect ocr errors stylistic text
classification proceedings th annual international acm sigir conference
development information retrieval sigir poster pp

tambouratzis g vassiliou employing thematic variables enhancing classification accuracy within author discrimination experiments literary linguistic
computing
tan cheng x wang xu h adapting naive bayes domain adaptation
sentiment analysis proceedings st european conference information
retrieval ecir pp
turney p thumbs thumbs semantic orientation applied unsupervised classification reviews proceedings th annual meeting
association computational linguistics acl pp


fidasgupta ng

wagstaff k cardie c rogers schrodl constrained k means clustering
background knowledge proceedings th international conference
machine learning icml pp
wan x bilingual knowledge ensemble techniques unsupervised chinese sentiment analysis proceedings conference empirical methods
natural language processing emnlp pp
weiss segmentation eigenvectors unifying view proceedings
international conference computer vision iccv pp
wiebe j wilson bruce r bell martin learning subjective
language computational linguistics
wilson wiebe j hoffmann p recognizing contextual polarity
phrase level sentiment analysis proceedings joint human language technology conference conference empirical methods natural language
processing hlt emnlp pp
wu z leahy r optimal graph theoretic appproach data clustering
application image segmentation ieee transactions pattern analysis
machine intelligence
xing e p ng jordan russell j distance metric learning
application clustering side information advances neural information
processing systems nips pp
xu w liu x gong document clustering non negative matrix
factorization proceedings th annual international acm sigir conference
development information retrieval sigir pp
yang liu x examination text categorization methods proceedings nd annual international acm sigir conference
development information retrieval sigir pp
yang pedersen j comparative study feature selection text categorization proceedings th international conference machine learning
icml pp




