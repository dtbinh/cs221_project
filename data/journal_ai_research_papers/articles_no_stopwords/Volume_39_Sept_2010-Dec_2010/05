journal artificial intelligence

submitted published

intrusion detection continuous time bayesian networks
jing xu
christian r shelton

jingxu cs ucr edu
cshelton cs ucr edu

department computer science engineering
university california riverside
riverside ca usa

abstract
intrusion detection systems idss fall two high level categories network systems
nids monitor network behaviors host systems hids monitor system calls
work present general technique systems use anomaly detection
identifies patterns conforming historic norm types systems rates change
vary dramatically time due burstiness components due service difference
efficiently model systems use continuous time bayesian networks ctbns avoid
specifying fixed update interval common discrete time build generative
normal training data abnormal behaviors flagged likelihood
norm nids construct hierarchical ctbn model network packet traces
use rao blackwellized particle filtering learn parameters illustrate power
method experiments detecting real worms identifying hosts two publicly
available network traces mawi dataset lbnl dataset hids develop novel
learning method deal finite resolution system log file time stamps without losing
benefits continuous time model demonstrate method detecting intrusions
darpa bsm dataset

introduction
misuse abuse computer systems critical issue system administrators goal
detect attacks attempt compromise performance quality particular host machine
time consuming error prone acquire labeled data contains good bad
behaviors build classifier additionally frequency attacks developed make maintaining database previously seen attacks inefficient even infeasible
anomaly detection identify attacks even attack type unknown beforehand unsupervised learning allows anomaly detector adapt changing environments thereby extending
domain usefulness modeling normal behavior historic clean data identify
abnormal activity without direct prior model attack simply comparing deviation
learned norm
network intrusion detection system nids network packet traces monitored
network traffic traces collect information networks data stream provide external
view network behavior host intrusion detection system hids internal state
computing system analyzed system call logs convenient way monitoring executing
programs behavior operating system calls
systems composed activities happen dramatically different time granularity
users alternate busily computer resting busy period burst
action may cause peak network traffic flow operating system usage however
c

ai access foundation rights reserved

fix u helton

resting period computer maintains regular running pattern network system activities much less intense e g automatically checking email every minutes even within
global modes variations therefore dynamic model requires discretizing
time efficient develop intrusion detection techniques continuous time bayesian
networks ctbns nodelman shelton koller data types although two
data completely different formats semantic meaning demonstrate flexibility
continuous time generative model ctbn describe
first effort detect anomalies network traffic traces nids abnormal traffic must
differ way normal traffic patterns difference may subtle
difficult detect subtle attack longer attack take
stress patience attacker looking summarized information flow statistics
helpful especially stealthy worms mingle well normal traffic sacrificing
spreading speed scale therefore feel looking abnormalities detailed network
traffic flow level utile method finding attacks network flow given host machine
sequence continuous time asynchronous events furthermore events form complex
structured system statistical dependencies relate network activities packet emissions
connections employ ctbns reason structured stochastic network processes
ctbn model contains number observed network events packet emissions concurrent port connections changes allow model descriptive add latent
variables tie activity variables together exact inference method longer feasible
therefore use rao blackwellized particle filtering rbpf estimate parameters
second effort detect intrusions system call logs hids system log file contains ordered list calls made computers operating system executing program
focus analyzing ordering context sequence rather simply counting
overall statistics ctbn natural way modeling sequential data finite
resolution computer clock system calls issued within clock tick assigned
time stamp therefore data stream consists long periods time activity followed
sequences calls order correctly recorded exact timing information lost
poses challenge ctbn reasoning present learning method type
data without resorting time discretization
validate nids technique mawi dataset lbnl dataset hids
technique darpa bsm dataset applications give good compared
method
section discuss related work intrusion detection section review continuoustime markov processes continuous time bayesian networks section describe ctbn
model rbpf inference nids section describe
ctbn model parameter estimation hids including deal imprecise timing measurements section experimental applications

related work
much previous work intrusion detection focuses one area detecting
network traffic mining system call logs work eskin arnold prerau portnoy
stolfo similar apply method kinds


fii ntrusion etection ctbn

data map data elements feature space detect anomalies determining points
lie sparse regions cluster estimation k nearest neighbors one class svm
use data dependent normalization feature map network traffic data spectrum kernel
system call traces
nids
network traffic data build upon previous work xu shelton made
assumption network activities independent across different ports allowed us
factorize model port level submodels standard exact inference techniques could used
parameter learning remove restriction application specific
reason traffic independent ports tying traffic together model describes
complicated structural dependencies among variables derive rao blackwellized particle
filtering estimate parameters model work differs
interested intrusion detection host identity recognition well
signature detection share many assumptions karagiannis
papagiannaki faloutsos particular assume access
internals machines networks rules methods malan
smith cha qin lee eskin et al however differ
rely preset values require human intervention interpretation
assume access network wide traffic information network wide data human
intervention advantages lead difficulties data collation face
attack increased human effort chose leave solution
many learning adaptive methods proposed network data
example zuev moore soule salamatian taft emilion papagiannali classification task requires labeled data dewaele
fukuda borgnat profile statistical characteristics anomalies random projection techniques sketches reduce data dimensionality multi resolution non gaussian
marginal distribution extract anomalies different aggregation levels goal papers
usually detect attacks rather classify non attacks traffic type applied attack
detection would risk missing types attacks furthermore frequently treat
network activity separately instead considering temporal context
lakhina crovella diot nice summary adaptive statistical methods
look anomaly detection instead classification use entropy method
entire network traffic many methods ye emran chen vilbert
use statistical tests subspace methods assume features connections
packets distributed normally rieck laskov model language features
n grams words connection payloads xu zhang bhattacharyya use
unsupervised methods concentrate clustering traffic across whole network similarly
soule salamatian taft build anomaly detector markov
network traffic patterns whole function host level
work soule et al similar statistical flavor work
fit distribution case histogram modeled dirichlet distribution network data
however model flow level statistics whereas work level individual connections
additionally attempting network wide clustering flows instead anomaly detection


fix u helton

work moore zuev traffic graphical
particular naive bayes networks goal categorize network traffic instead detecting
attacks kruegel mutz robertson valeur present bayesian detecting
event classification task care whether host attack
interval
work lazarevic ertoz kumar ozgur srivastava similar work
one papers attempt attacks host level employ nearest neighbor
mahalanobis distance density local outliers method features
connections although methods make standard assumption data
therefore miss temporal context connection use features compared
features compare section closest prior work agosta
duik wasser chandrashekar livadas present adaptive detector whose threshold
time varying similar work rely model
employ host internal states cpu loads available us
great variety previous work work novel detects
anomalies host level timing features network activities consider
connection packet isolation rather complex context capture statistical
dynamic dependencies packets connections sequences network traffic
anomalous group
hids
previous work detecting intrusions system call logs roughly grouped two categories sequence feature sequence methods focus sequential order
events feature methods treat system calls independent data elements
method belongs former category since use ctbn model dynamics sequences
time delay embedding tide sequence time delay embedding stide two examples
sequence methods forrest hofmeyr somayaji longstaff hofmeyr forrest somayaji generalize data building database storing previously seen
system call sub sequences test looking subsequences database methods
straightforward often achieve good compare experiments tandon
chan look richer set attributes return value arguments associated
system call make use system call names
feature methods hu liao vemuri use dataset use
darpa bsm dataset training data noisy try classification
hyperplane robust support vector machines rsvms separate normal system call profiles
intrusive ones eskin works noisy data make assumption
training data contains large portion normal elements anomalies present mixture
distribution normal abnormal data calculate likelihood change data point
moved normal part abnormal part get optimum data partition
yeung ding try use techniques provide dynamic static behavioral system call data dynamic method hidden markov model hmm
used model normal system events likelihood calculated testing sequence
compared certain threshold work system call traces close


fii ntrusion etection ctbn

framework since build dynamic model sequential data compute
likelihood testing example score different ctbn continuous time dynamics rather time sliced behaviors static method represent
normal behavior command occurrence frequency distribution measure distance
testing example norm cross entropy dataset use kdd archive dataset
work
simma et al use continuous time model reason network traffic apply
method dependences exterprise level services model non markovian
deals network events basic observational unit
estimate parameters large network build network traffic data use
rao blackwellized particle filters rbpfs doucet de freitas murphy russel propose
rbpf dynamic bayesian networks works discrete time fashion exploiting
structure dbn ng pfeffer dearden extend rbpf continuous time dynamic systems apply method k experimental mars rover nasa ames
center model hybrid system containing discrete continuous variables
use particle filters discrete variables unscented filters continuous variables
work similar apply rbpf ctbn model contains discrete
variables evidence continuous time opposed snapshots system
state

continuous time bayesian networks
begin briefly reviewing definition markov processes continuous time bayesian
networks ctbns
homogeneous markov process
finite state continuous time homogeneous markov process xt described initial distribution px given state space v al x x xn n n matrix transition intensities



qx


qx
q x x



q x x
qx



q xn x

q xn x

q x xn
q x xn




qxn







p
qxi xj intensity rate transition state xi state xj qxi j qxi xj
transient behavior xt described follows variable x stays state x time
exponentially distributed parameter qx probability density function f xt remaining
x duration fx q qx exp qx expected time next transition
given state currently x qx upon transitioning x shifts state x probability
xx qxx qx note given qx xx qxx iosmorphic sometime gives formulae
terms xx simplifies expression
distribution state process x future time px computed
directly qx px distribution x time represented vector letting


fix u helton

exp matrix exponential
px px exp qx
complete data
complete data hmp represented set trajectories n trajectory
complete set state transitions xd td x meaning x stayed state xd
duration td transitioned state x therefore know exact state variable
x time
sufficient statistics likelihood
given hmp full data likelihood single state transition xd td x

lx q qxd exp qxd td xd x
likelihood function decomposed transition


lx q
lx q
lx
dd

dd


x x
qxm x exp qx x
xx

x x x

x

take log function get log likelihood
lx q lx q lx
x
x

x ln qx qx x
x x ln xx
x x

x

x x


x sufficient statistics hmp
model x x number
p
times x transitions state x x denote x x x x total number
times system leaves state x x total duration x stays state x
learning complete data
estimate parameters transition intensity matrix q maximize log likelihood function yields maximum likelihood estimates
qx

x

x

xx

x x

x

incomplete data
incomplete data hmp composed partially observed trajectories n
trajectory consists set sd td dt observations sd subsystem
nonempty subset states x process triplets specifies interval
evidence states variable x subsystem sd time td time td dt
observations may duration free e observe x sd time know
long stayed called point evidence generalized triplet
notation described setting duration partially observed trajectory
observe sequences subsystems observe state transitions within subsystems


fii ntrusion etection ctbn

expected sufficient statistics expected likelihood
consider possible completions partially observed trajectory specify transitions
consistent partial trajectory combining partial trajectory completion
get full trajectory define n completions partial trajectories
given model distriubtion given
data expected sufficient statistics respect probability density possible completions data x x x x expected log likelihood
e lx q e lx q e lx
x
x
x ln qx qx x

x x ln xx
x x

x

learning incomplete data
expectation maximization em used local maximum likelihood
partial trajectory em iterates following e step step
convergence derived likelihood function
e step given current hmp parameters compute expected sufficient statistics x
x x x data set complex part give
details
step computed expected sufficient statistics update model parameters
next em iteration
x x
x
xx

qx
x
x
calculate expected sufficient statistics forward backward
message passing method
trajectory devided n intervals interval separated
adjacent event changes assume trajectory spans time interval let v w
observed evidence time v w including events time stamp v w let
v w set evidence excluding v w let subsystem states
restricted interval
define
p xt p xt
vectors indexed possible assignments xt similarly define corresponding
distribution excludes certain point evidence follows
p xt

p xt

denote j vector except j th position denote ij matrix
except element th row j th column
able derived expected sufficient statistics time
z
e x
p xt x dt

n
z ti
x


p xt x dt
p
ti




fix u helton

constant fraction beginning last line serves make total expected time
j sum integral interval expressed
z

w

z

w

v exp qs v xx exp qs w w dt

p xt x dt
v

v

qs qx except elements correspond transitions set

equation expected transition counts similarly defined
n

e x x

x
qx x


ti x x
p

n
z ti
x

ti exp qs ti x x exp qs ti ti dt


ti

integrals appearing e e computed via standard ode solver
runge kutta method press teukolsky vetterling flannery method uses
adaptive step size move quickly times expected changes slowly
times rapid transitions
remaining calculate let qss transitioning intensity
matrix hmp one subsystem another matrix qx
elements corresponding transitions non zero
ti ti exp qsi ti ti
ti ti qsi si
ti exp qsi ti ti ti
ti qsi si ti
forward backward calculation trivial answer queries
p xt x


xx
p

continuous time bayesian networks
hmps good modeling many dynamic systems limitations
systems multiple components state space grows exponentially number
variables hmp model variable independencies therefore use unified
state x represent joint behavior involving components system
section continuous time bayesian network used address issue
nodelman et al extend theory hmps present continuous time bayesian networks ctbns model joint dynamics several local variables allowing transition
model local variable x markov process whose parametrization depends
subset variables u


fii ntrusion etection ctbn

definition
first give definition inhomogeneous markov process called conditional markov process critical concept us formally introduce ctbn framework
definition nodelman shelton koller conditional markov process x inhomogeneous markov process whose intensity matrix varies function current values set
discrete conditioning variables u parametrized conditional intensity matrix cim
qx u set homogeneous intensity matrices qx u one instantiation values u u
call u parents x set u empty cim simply standard intensity
matrix
cims provide way model temporal behavior one variable conditioned
variables putting local together joint structured model continuous
time bayesian network
definition nodelman et al continuous time bayesian network n set stochastic processes x consists two components initial distribution px specified bayesian
network b set random variables x continuous transition model specified
directed possibly cyclic graph g whose nodes x x ux denotes parents x g
variable x x associated conditional intensity matrix qx ux
dynamics ctbn quantitatively defined graph instantaneous evolution
variable depends current value parents graph quantitative description
variables dynamics given set intensity matrices one value parents
means transition behavior variable controlled current values parents
standard notion separation bayesian networks carries ctbns
graphs cyclic variables represent processes single random variables implications
little different variable process still independent non descendants given parents
still independent everything given markov blanket variable parent
child parent child cycles cause parents children provided
considered definitions still hold importantly notion given works
full trajectory variable question known therefore x grandchildren
independent given xs childrens values single instant rather independent
given xs childrens full trajectories time last time interest
amalgamate variables ctbn together get single homogeneous markov
process joint state space joint state intensity matrix rate assigned
transition involves changing one variables value exact time
intensities found looking value corresponding conditional intensity matrix
variable changes diagonal elements negative row sums
forward sampling done quickly ctbn without generating full joint intensity
matrix keep track next event time variable sampled relevant exponential distribution given current values parent select earliest
event time change variable sampling multinomial distribution implied row
variables relevant intensity matrix next event time variable changed
children must resampled variables time must resampled due
memoriless property exponential distribution way sequence events trajectory
sampled


fix u helton

learning
context ctbns model parameters consist ctbn structure g initial distribution p parameterized regular bayesian network conditional intensity matrices cims
variable network section assume ctbn structure known us
focus parameter learning assume model irreducible initial
distribution p becomes less important context ctbn inference learning especially
time range becomes significantly large therefore parameter learning context
estimate conditional intensity matrices qxi ui variable xi ui set
parent variables xi
l earning c omplete data
nodelman et al presented efficient way learn ctbn model fully observed
trajectories complete data know full instantiations variables whole
trajectory know cim governing transition dynamics variable
time sufficient statistics x x u number times x transitions state x
x given parent instantiation u x u p
total duration x stays state x
given parent instantiation u denote x u x x x u
likelihood function decomposed

ln q
lxi qxi ui lxi xi ui

xi x


lx qx u

yy
u

x u

qx u

exp qx u x u



x


lx

yy
u



xx
u x x u



x x x

put functions together take log get log likelihood component
single variable x
lx q lx q lx
xx

x u ln qx u q x u x u
u



x

xx x
u

x x u ln xx u



x x x

maximizing log likelihood function model parameters estimated
qx u

x u

x u

xx u



x x u

x u



fii ntrusion etection ctbn

l earning ncomplete data
nodelman shelton koller present expectation maximization em
learn ctbn model partially observed trajectories expected sufficient statistics
x x u expected number times x transitions state x x parent set
u takes values u x u expected p
amount time x stays state x
parent instantiation u denote x u x x x u expected log likelihood
decomposed way equation except sufficient statistics x x u x u
x u replaced expected sufficient statistics x x u x u x u
em ctbn works essentially way hmp expectation step calculate expected sufficient statistics inference method described
section maximization step update model parameters

qx u

x u

x u

xx u

x x u

x u

inference
given ctbn model partially observed data would query model
example may wish calculate expected sufficient statistics em
e xact nference
nodelman et al provide exact inference expectation maximization
reason learn parameters partially observed data exact inference requires flattening variables single markov process performing inference
hmp makes state space grow exponentially large therefore exact
inference method feasible small state spaces
pproximate nference
issue addressed much work done ctbn approximate inference nodelman koller shelton present expectation propagation saria
nodelman koller give another message passing adapts time granularity cohn el hay friedman kupferman provide mean field variational
el hay friedman kupferman gibbs sampling method monte
carlo expectation maximization fan shelton give another sampling
uses importance sampling el hay cohn friedman kupferman describe different expectation propagation
estimate parameters build two applications nids hids
employ inference including exact inference rao blackwellized particle filtering
rbpf depending model size ng et al extended rbpf ctbns
model hybrid system containing discrete continuous variable used particle
filters discrete variables unscented filters continuous variable work
similar work method applying rbpf ctbns model contains discrete
variables evidence continuous intervals


fix u helton

port









description
world wide web http
http alternate
http protocol tls ssl
authentication service
talarian tcp
pop protocol tls ssl
unknown
unknown

port









description
world wide wed http
netbios session service
http protocol tls ssl
microsoft ds
msnp
gadget gate way
c license manager
post office protocol version

figure ranking frequent ports mawi dataset left lbnl dataset right

ctbn applications
although inference learning well developed ctbns
applications real world nodelman horvitz used ctbns
reason users presence availability time ng et al used ctbns monitor
mobile robot nodelman et al used ctbns model life event history fan shelton
modeled social networks via ctbns previous work xu shelton presented
nids host machine ctbns include hids

anomaly detection network traffic
section present detect anomalies network traffic data ctbns
focus single host network sequence timing events e g packet
transimission connection establishment important network traffic flow matters
many connections initiated past minute timing
evenly spaced trace probably normal came quick burst suspicious
similarly sequence important connections made sequentially increasing ports
likely scanning virus whereas set ports random order likely
normal traffic merely simple examples would detect complex
patterns
typical machine network may diverse activities service types e g
http smtp destination port number roughly describes type service particular network activity belongs worms propagate malicious traffic toward certain well known
ports affect quality associated services looking traffic associated different
ports sensitive subtle variations appear aggregate trace information
across ports figure shows popular ports ranked frequencies network
traffic datasets use described depth later services extent
independent therefore model ports traffic ctbn submodel
denote whole observed traffic sequences particular host j traffic
associated port j


fii ntrusion etection ctbn

g

n
h

pin

pout

cinc

cdec

figure ctbn model network traffic plate model n number port

ctbn model network traffic
use port level submodel previous work xu shelton latent
variable h four fully observed toggle variables pin pout cinc cdec
nodes packet pin packet pout represent transmission packet
host intrinsic state transmission packet essentially instantaneous
event therefore events transitions without state modeled
toggle variable event evidence change state variable rate
transition associated state required
nodes connection increase cin connection decrease cdec together describe status
number concurrent connections c active host notice c increase
decrease one given event beginning ending time connection assume
arrival connection termination existing connection independent
number connections thus intensity connection starts stops
connections therefore modeled toggle variables
node h states represent different abstract attributes machines internal state
toggle variables pin pout cinc cdec allowed change states
h required rate states hidden states per toggle
variable chosen balance expressive power model efficiency
previous work assumed traffic associated different ports independent
port level submodels isolated remove restriction introducing
another latent variable g ties port submodels together full model shown figure
parameter learning rbpf
calculate expected sufficient statistics e step em parameter learning exact
inference nodelman et al flattens variables joint intensity matrix
reasons resulting homogeneous markov process time complexity exponential
number variables example port network contains variables
total approximate inference techniques clique tree nodelman et al
message passing nodelman et al saria et al importance sampling fan


fix u helton

shelton gibbs sampling el hay et al overcome sacrificing
accuracy
notice model nice tree structure makes rao blackwellized particle
filtering rbpf perfect fit rbpf uses particle filter sample portion variables
analytically integrates rest decomposes model structure efficiently thus reduces
sampling space
denote n port level hidden variables h hn posterior
distribution
qn
whole model factorized p g h hn p g p hi g note
g hi processes probability density complete trajectories use
particle filter estimate gs conditional distribution p g set sampled trajectories
g difficult sample directly posterior distribution use importance sampler
sample particle proposal distribution particles weighted ratio
likelihood posterior distribution likelihood proposal distribution doucet
et al since variable g latent parents use forward sampling
sample particles p g weight particle simply likelihood
conditioned trajectory g fan shelton port level submodel dseparated rest network given full trajectory g see section separation
ctbns since small hidden states marginalized exactly
calculate p g portion trajectory submodel exactly
marginalizing hi recursions section
expected sufficient statistics ess variable x ctbn tx u x u expected amount time x stays state x given parent instantiation u mx u x x u
expected number transitions state x x given xs parent instantiation u let g p g

particles define likelihood weights wi pp g g
let
p
w wi sum weights general importance sampling allows expected
sufficient statistic estimated following way ss sufficient statistic

e g h hn p g h hn ss g h hn
egp g eh hn p h hn g ss g h hn
x
wi eh hn p h hn gi ss g h hn

w


expected sufficient statistics whole model two categories depend
g ess g depend port model k ess g hk k ess g simply
summation counts amount time g stays state number times g
transitions one state another particles weighted particle weights

egp g ss g

x
wi ss g
w






fii ntrusion etection ctbn

function wholemodel estep
input current model evidence
output expected sufficient statistics ess
ess ess g ess g ess sn g
initialize ess empty
particle g g g g p g
sj sn
p j g ess sj g submodel estep g sj j
sj sn
q
ess sj g ess sj g k j p k g ess sj g

essgi countgss gq
ess g ess g j p j g essgi
return ess

figure rao blackwellized particle filtering estep whole model
ess g hk k calculated submodel independently
eg h hn p g h hn ss g hk k
z
x
wi

p hk g k ss g hk k dhk
w
hk

q
z
x j p j g
p hk g k ss g hk k dhk

w
p
hk

z
x



p j g
p hk k g ss g hk k dhk
w
hk




j k

integrals possible trajectories hidden process hk first line holds
separation need average submodel k given assignment g second
line expands weight last line combines weight term submodel k terms
integral get likelihood hk submodel data constant proportionality
p
cancel subsequent maximization reconstructed noting x tx u x u
total time
r interval
last integral hk p hk k g ss g hk k dhk p j g calculated
technique described nodelman et al exact ess calculation calculations
similar integrals section except intensity matrices change interval
interval function sampled trajectory gi
full e step shown figure sk represents variables submodel
k function submodel estep calculates expected sufficient statistics likelihood
subnet model equation function countgss counts empirical time transition statistics
sampled trajectory g equation
em use ess true sufficient statistics maximize likelihood
respect parameters regular ctbn variable x hidden variable g
h equation performs maximization toggle variables e g pi likelihood


fix u helton

component toggle variable


mp

qpi ui exp qpi u u u

u

found setting qx u value qpi u x tieing parameters
simplifying product x equation thus maximum likelihood parameter estimate
qpi u

mpi
u u

mpi number events variable pi qpi u parameter rate
switching
synchronize particles end window see section resample
normal particle filter points propagate particles forward stop
end window resample weights continue set
particles general particles aligned time except resampling points
online testing likelihood
ctbn model fitted historic data detect attacks computing likelihood
window data see section model likelihood falls threshold
flag window anomalous otherwise mark normal
experiments fix window fixed time length tw therefore
window interest starts time wish calculate p tw
represents observed connections packets time time use rbpf
estimate probability samples time represent prior distribution p g
propagating forward across window length tw produces set trajectories g
g submodel k evalute p k tw g exact marginalization sum
vector tw forward message weighted average samples g k product
submodel probabilities estimate p tw

anomaly detection system calls
turn detecting anomalies system call logs
ctbn model system calls
system call logs monitor kernel activities machines record detailed information
sequence system calls operating system many malicious attacks host revealed
directly internal logs
analyze audit log format suns solaris basic security module bsm praudit audit
logs user level kernel event record least three tokens header subject return
event begins header format header record length bytes audit record version
number event description event description modifier time date subject line consists
subject user audit id effective user id effective group id real user id real group id process
id session id terminal id consisting device machine name return return
value indicating success event closes record


fii ntrusion etection ctbn

h





sn

figure ctbn model system call data



sk

ti ti

ti

ti

ti ti

time

figure system call traces finite resolution clock resolution
construct ctbn model similar port level network model individual system calls
sn event description fields header token transiently observed
happen instantaneously duration treat toggle variables packets
network model introduce hidden variable h parent system calls variables
allow correlations among hidden variable designed model internal state
machine although semantic meaning imposed method put together system
call model looks figure
state space hidden variable h size transition rate matrix h



qh


qh
q h h



q h h
qh



q hm h

q hm h

q h hm
q h hm




qhm







transition intensity rate toggle variable given current value parent h
qs hi
estimate ctbn model parameters use expectation maximization em
expected sufficient statistics need calculate model
mhi hj expected number times h transitions state j
thi expected amount time h stays state
ms hi expected number times system call evoked h state


fix u helton

maximum likelihood parameters
mhi hj
thi
ms hi


thi

q hi hj
qs hi

parameter estimation finite resolution clocks
finite resolution computer clocks multiple instantaneous events system calls
occur within single clock tick therefore audit logs batch system calls may recorded
executed time point rather real time stamp finite
time accuracy however correct order events kept logs know exactly
system call follows recorded order audit logs thus system
call timings partially observed type partial observation previously
considered ctbn inference typical trajectory system call data shown
figure batch system calls evoked time ti next clock tick
followed quiet period arbitrary length yet another bunch events time
ti
let denote evidence interval denote evidence
denote evidence define vectors
ti p ht ti


p ht




ht value h prior transition ti ht value afterward


define vectors
ti p hti


ti p ti hti
evidence transition time ti included follow forward backward
compute ti ti ti event split interval ti ti
spike period ti ti one resolution clock batch
system calls quite period ti ti events exist propagations
separately
spike period ti ti observed event sequence sk construct
artificial markov process x following intensity matrix




qx



qh q


qh q













qh qk



qh










fii ntrusion etection ctbn





qh


p
qh ss qs h
q h h



qh

q hm h

q h h
p
ss qs h







qhm

q hm h

q h hm
q h hm


p
ss qs hm











qi


qsi h






qsi h

















qsi hm







x tracks evidence sequence sk qx square block matrix dimension
k block matrix subsystem x k blocks states
first block represents state h events second block represents h exactly
one event happens third block represents h followed happens
last block represents h events finish executing order subsystem
zero transition intensities everywhere except along sequence pass diagonal qh
matrix qh except transition intensities system call variables
subtracted full system includes transitions observed
transition rates set zero force system agree evidence conditioning
change diagonal elements rate matrix nodelman et al within
k states block h freely change value therefore non diagonal elements
qh intensities qh upon transitioning x transit state
another according event sequence therefore blocks matrices except
immediate right diagonal blocks transition behavior described matrix
qi qi intensities non diagonal entries h change simultaneously
diagonal element qi h h intensities event si happening given current value
hidden state h
take forward pass example describe propagation backward pass
performed similarly right ti ti dimensions expand k dimensions
form ti non zero probabilities first states ti describes
distribution subsystem x ti eqx represents probability distribution time ti
given prefix observed sequence occurred take last state probabilities
condition entire sequence happening thus resulting dimensional vector ti
quiet period ti ti evidence observed therefore ti propagated
ti qh rate matrix conditioned h events occuring
ti ti exp qh ti ti
done full forward backward pass whole trajectory calculate expected sufficient statistics mhi hj thi ms hi refer work nodelman
et al


fix u helton

testing likelihood
learned model normal process system call logs calculate
log likelihood future process model log likelihood compared
predefined threshold threshold possible anomaly indicated single
hidden variable calculations done exactly

evaluation
evaluate methodology constructed experiments two different types data network
traffic traces system call logs following sections experiment
tasks
dynamic bayesian network dbn another popular technique graphical modeling
temporal data slice time events without state changes instantaneous events
difficult model reasonable time resolution multiple events
variable one time period standard way encoding dbn use toggle
variable records parity number events time interval furthermore
nids events bursty active times multiple packets emited per second
inactive times may activity hours finding suitable sampling rate
maintains efficency model difficult hids acute
know way modeling timing ambiguity dbn without throwing away timing
information adding mathematical framework essentially turns dbn ctbn
described general could suitable way apply dbn
without essentially turning dbn ctbn finely slicing time applying
numeric tricks speed inference amount converting stochastic matrices rate
matrices numeric integration matrix exponential
compared current adaptive methods individually include nearest neighbor support vector machines sequence time delaying embedding give
details methods
experiment network traffic
section present experiment nids
datasets
verify two publicly available real network traffic trace repositories mawi
working group backbone traffic mawi lbnl icsi internal enterprise traffic lbnl
mawi backbone traffic part wide project collected raw daily packet
header traces since records network traffic inter pacific tunnel
japan usa dataset uses tcpdump ip anonymizing tools record minute
traces every day consists mostly traffic japanese universities experiment
use traces january st th connections total time
one hour
lbnl traces recorded medium sized site emphasis characterizing internal enterprise traffic publicly released anonymized form lbnl data collects


fii ntrusion etection ctbn

packets flowing source destination
packets flowing destination source
connections source last seconds
connections destination last seconds
different services source last seconds
different services destination last seconds
connections source last connections
connections destination last connections
connections port source last connections
connections port destination last connections
figure features nearest neighbor work lazarevic et al
hours network traces thousands internal hosts publicly released take
one hour traces january th latest date available total connections
w orm etection
start worm detection split traffic traces host half training
half testing learn ctbn model training data hosts since
network data available clean traffic known intrusions inject real attack traces
testing data particular inject ip scanner w mydoom slammer slide
fixed time window testing traces report single log likelihood value sliding
window compare predefined threshold threshold predict
abnormal time period define ground truth window abnormal attack
traffic exists interval normal otherwise window size use seconds
consider windows contain least one network event
compare method employing rbpf previous factored ctbn model xu
shelton connection counting nearest neighbor parzen window detector yeung chow
one class svm spectrum string kernel leslie eskin noble
connection counting method straightforward score window number
initiated connections window worms aggregate many connections short time
method captures particular anomaly well
make nearest neighbor competitive try extract reasonable set features follow
feature selection work lazarevic et al use total features
features available data available shown figure notice
features associated connection record apply nearest neighbor method
window testing framework first calculate nearest distance connection inside
window training set composed normal traffic assign maximum
among score window similarly parzen window apply
feature set assign maximum density among connections inside window
score window
besides feature would see sequence
approaches compare methods widely used network anomaly
detection treat traffic traces stream data sequential contexts


fix u helton















nearest neighbor
connection count
parzen window
svmspectrum
ctbn factored
ctbn rbpf










false positive rate







nearest neighbor
connection count
parzen window
svmspectrum
ctbn factored
ctbn rbpf































nearest neighbor
connection count
parzen window
svmspectrum
ctbn factored
ctbn rbpf








false positive rate

ip scanning









nearest neighbor
connection count
parzen window
svmspectrum
ctbn factored
ctbn rbpf










false positive rate

mydoom





true positive rate









false positive rate





slammer





nearest neighbor
connection count
parzen window
svmspectrum
ctbn factored
ctbn rbpf



mydoom

true positive rate

true positive rate

lbnl

ip scanning



false positive rate

true positive rate



true positive rate

true positive rate

mawi

explored one class svm spectrum string kernel chosen comparison
implemented spectrum kernel libsvm library chang lin give network
activities connection starting ending packet emmision receipt inside portlevel submodel distinct symbol sequence symbols fed inputs
decision surface trained normal training traffic testing sliding window
distance window string decision hyperplane reported window score
tried experiments edit distance kernel dominated spectrum
kernel report





nearest neighbor
connection count
parzen window
svmspectrum
ctbn factored
ctbn rbpf










false positive rate





slammer

figure roc curves testing ip scanning attack mydoom attack slammer attack
top mawi bottom lbnl

injecting attack traffic randomly pick starting point somewhere first half
test trace insert worm traffic duration equal times length full testing
trace shorter harder detect anomaly choose
experiments work challenge detection tasks scaled back rates
worms running full speed worm easy detect method slows
thus blends background traffic better becomes difficult detect let
scaling rate e g indicates worm running one tenth normal speed
method set state space variable g variable h use
samples particle filtering resample particles every seconds svm
spectrum kernel method choose sub sequence length parameter
roc curves methods figure curves overall performance active hosts dataset point curves corresponds


fii ntrusion etection ctbn








true positive rate

true positive rate

different threshold ctbn method performs except
single case mydoom attack background lbnl traffic many cases
advantages ctbn pronounced
mawi data factored non factored ctbn perform comparably
believe data captures connections traverse trans pacific link
therefore connections machine represented makes reasoning
global pattern interaction machine difficult lbnl data one attack ip
scanning shows advantage non factored model one attack mydoom shows distinct
advantage one attack slammer indicates advantage depending desired false
positive rate demonstrate advantage jointly modeling traffic across ports
although clear advantage uniform traffic patterns attack types












connection count
ctbn rbpf








false positive rate



connection count
ctbn rbpf










false positive rate





figure roc curves testing slammer attack mawi dataset demonstrating
effect slowing attack rate left right

roc curves shift scale back worm running speed figure
firewalls built sensitive block malicious traffic worms act stealthy
sneak demonstrate robustness method compared best competitor
connection counts speed worms attack
h ost dentification
identifying individual hosts network traffic patterns another useful application
model instance household usually installs network router family members
computer connected router outside internet network traffic going
router behaves coming one peer actually coming different people
dad possibly read sports news kids surf social networks interesting well
useful tell family member contributing current network traffic host identification
used combat identity theft network identity abused attacker host
identification techniques help network administrator tell whether current network traffic
host consistent usual pattern


fix u helton

first set experiments construct host model fitting competition
hosts picked worm detection tasks lbnl dataset compose testing pool learn
coupled ctbn model host split test traces clean particular host
segments lengths seconds segments compute log likelihood
segment learned model hosts including label segment
host achieves highest value compute confusion matrix c whose element cij
equals fraction test traces host model j highest log likelihood expect
see highest hit rates fall diagonals ideally host best described
model table shows dataset lbnl vast majority traffic windows
assigned correct host exception host diagonals distinctly higher
elements row comparison performed experiment svm
spectrum kernel method selected sub sequence length parameter
tried multiple methods normalization distance hyperplane
variations parameters produced poor almost windows assigned
single host omit table
host


























































































figure confusion matrix lbnl host identification ctbn
second experiment host traffic differentiation task mingle network traffic
another host analyzed host expect detection method successfully tell apart
two verify idea pick one host among choose lbnl dataset
split traffic evenly training testing learn model training data
testing data randomly choose period inject another hosts traffic worm
goal identify period abnormal since hosts traffic longer behavior
figure displays two combination tests parameters injecting
traffic worm left graph nearest neighbor parzen
window curve overlap ctbn curves overlap right graph coupled ctbn curve
substantially outperforms curves
experiment system call logs
section present experiment hids













nearest neighbor
connection count
parzen window
svmspectrum
ctbn factored
ctbn rbpf









false positive rate



true positive rate

true positive rate

ntrusion etection ctbn

nearest neighbor
connection count
parzen window
svmspectrum
ctbn factored
ctbn rbpf













false positive rate





figure roc curves testing host identification lbnl data left host
nearest neighbor curve parzen window curve overlap ctbn curves overlap
right host

week








normal
processes








attack
processes








system call
close
ioctl
mmap
open
fcntl
stat
access

occurrence








system call
execve
chdir
chroot
unlink
chown
mkdir
chmod

occurrence








figure left darpa bsm process summary right darpa bsm system call summary
dataset
dataset used darpa intrusion detection evaluation data set mit lincoln
laboratory seven weeks training data contain labeled network attacks midst
normal background data publicly available darpa website solaris basic security
module bsm praudit audit data system call logs provided analysis follow
kang fuller honavar cross index bsm logs produce labeled list file
labels individual processes resulting statistics shown left table figure
frequency system calls appearing dataset summarized descending order
right figure
nomaly etection
experimental goal detect anomalous processes train ctbn model normal
processes test mixture normal attack processes state space









true positive rate

true positive rate

x u helton



ctbn
svmspectrum
stide
nearest neighbor









false positive rate





ctbn
svmspectrum
stide
nearest neighbor












false positive rate





figure roc curves bsm data detection left training week combined testing
week right training week test week stide curve
ctbn curve overlap

hidden variable h set log likelihood whole process learned model
represents score process compare score predefined threshold classify
process normal one system abuse
implement sequence time delaying embedding stide stide frequency threshold
stide comparison warrender forrest pearlmutter two build
database previously seen normal sequences system calls compare testing sequences
straightforward perform well empirically system call log
datasets choose parameter k sequence length h locality frame length
stide shown following resulting graphs since overlapped
stide almost cases
approaches compare nearest neighbor one class svm spectrum
string kernel edit distance kernel follow hu et al transform process
feature vector consisting occurrence numbers system call process nearest
distance testing process training set processes assigned score
one class svm processes composed strings system calls normal processes used
learning bounding surface signed distance assigned score set subsequence length parameter since edit distance kernel
dominated spectrum kernel
figure displays two experiment settings left graph train
model normal processes week test processes weeks
right graph train normal processes week test processes week
richest attack processes attacks relatively rare compared normal
traffic interested region roc curves small false positive rates
curves area false positive rate falls region
ctbn method beats nearest neighbor svm spectrum kernel experiments stide
performs slightly better method combined test achieves accuracy


fii ntrusion etection ctbn

experiment week testing week advantage ctbn model
stide easily combined prior knowledge data sources
network data nids demonstrate loss performance flexibility

conclusions
realm temporal reasoning introduced two additions ctbn literature first
demonstrated rao blackwellized particle filter continuous evidence second demonstrated learn reason data contains imprecise timings still refraining
discretizing time
realm intrusion detection demonstrated framework performs well two
related tasks different data types concentrating purely event timing without
consideration complex features able perform existing methods continuoustime nature model aided greatly modeling bursty event sequences occur systems
logs network traffic resort time slicing producing rapid slices
inefficient quite periods lengthy slices miss timing bursty events
combination two sources information system calls network events would
straight forward model produced believe would accurate
detection collection data difficult however leave interesting next step

acknowledgments
project supported intel uc micro air force office scientific
fa defense advanced project agency hr

references
agosta j duik wasser c chandrashekar j livadas c adaptive anomaly
detector worm detection workshop tackling computer systems
machine learning techniques
hofmeyr forrest somayaji intrusion detection sequences system
calls journal computer security
cha b host anomaly detection performance analysis system call neuro fuzzy
soundex n gram technique systems communications icw
chang c c lin c j libsvm library support vector machines http
www csie ntu edu tw cjlin libsvm
cohn el hay friedman n kupferman r mean field variational approximation
continous time bayesian networks uncertainty artificial intelligence
dewaele g fukuda k borgnat p extracting hidden anomalies sketch non
gaussian multiresulotion statistical detection procedures acm sigcomm
doucet de freitas n murphy k russel rao blackwellised particle filtering
dynamic bayesian networks uncertainty artificial intelligence


fix u helton

el hay cohn friedman n kupferman r continuous time belief propagation
proceedings twenty seventh international conference machine learning
el hay friedman n kupferman r gibbs sampling factorized continous time
markov processes uncertainty artificial intelligence
eskin e anomaly detection noisy data learned probability distributions
international conference machine learning
eskin e arnold prerau portnoy l stolfo geometric framework
unsupervised anomaly detection detecting intrusions unlabeled data barbara
jajodia eds applications data mining computer security kluwer
fan shelton c r sampling approximate inference continuous time bayesian
networks symposium artificial intelligence mathematics
fan shelton c r learning continuous time social network dynamics proceedings twenty fifth international conference uncertainty artificial intelligence
forrest hofmeyr somayaji longstaff sense self unix processes ieee symposium security privacy pp
hu w liao vemuri v robust support vector machines anomaly detection
computer security international conference machine learning applications
kang k fuller honavar v learning classifiers misuse detetction
bag system calls representation ieee international conferences intelligence
security informatics
karagiannis papagiannaki k faloutsos blinc multilevel traffic classification
dark acm sigcomm
kruegel c mutz robertson w valeur f bayesian event classification intrusion
detection annual computer security applications conference
lakhina crovella diot c mining anomalies traffic feature distributions
acm sigcomm pp
lazarevic ertoz l kumar v ozgur srivastava j compare study anomaly
detection schemes network intrusion detection siam international conference data
mining
lbnl
lbnl icsi enterprise tracing project
enterprise tracing overview html

http www icir org

leslie c eskin e noble w spectrum kernel string kernel svm protein
classification pacific symposium biocomputing
malan j smith host detection worms peer peer cooperation workshop rapid malcode
mawi mawi working group traffic archive http mawi nezu wide ad jp mawi
moore w zuev internet traffic classification bayesian analysis techniques
acm sigmetrics
ng b pfeffer dearden r continuous time particle filtering national conference
artificial intelligence pp


fii ntrusion etection ctbn

nodelman u horvitz e continuous time bayesian networks inferring users presence activities extensions modeling evaluation tech rep msr tr
microsoft
nodelman u koller shelton c r expectation propagation continuous time
bayesian networks uncertainty artificial intelligence pp
nodelman u shelton c r koller continuous time bayesian networks uncertainty artificial intelligence pp
nodelman u shelton c r koller learning continuous time bayesian networks
uncertainty artificial intelligence pp
nodelman u shelton c r koller expectation maximization complex duration
distributions continuous time bayesian networks uncertainty artificial intelligence
pp
press w h teukolsky vetterling w flannery b p numerical recipes c
second edition cambridge university press
qin x lee w attack plan recognition prediction causal networks annual
computer security application conference pp
rieck k laskov p language detection unknown attacks network
traffic journal computer virology
saria nodelman u koller reasoning right time granularity uncertainty
artificial intelligence
simma goldszmidt maccormick j barham p black r isaacs r mortier r
ct representing reasoning events continuous time uncertainty artificial intelligence
soule salamatian l taft n emilion r papagiannali k flow classification
histogram acm sigmetrics
soule salamatian k taft n combining filtering statistical methods
anomaly detection internet measurement conference pp
tandon g chan p k learning useful system call attributes anomaly detection
florida artificial intelligence society conference pp
warrender c forrest pearlmutter b detecting intrusions system calls alternative data ieee symposium security privacy ieee computer society
xu j shelton c r continuous time bayesian networks host level network intrusion
detection european conference machine learning
xu k zhang z l bhattacharyya profiling internet backbone traffic behavior
applications acm sigcomm
ye n emran chen q vilbert multivariate statistical analysis audit trails
host intrusion detection ieee transactions computers
yeung chow c parzen window network intrusion detectors international
conference pattern recognition


fix u helton

yeung ding user profiling intrusion detection dynamic static
behavioral advances knowledge discovery data mining
zuev moore internet traffic classification bayesian analysis techniques
acm sigmetrics




