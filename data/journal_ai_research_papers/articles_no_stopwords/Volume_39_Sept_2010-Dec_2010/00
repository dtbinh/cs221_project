Journal Artificial Intelligence Research 39 (2010) 1-49

Submitted 05/10; published 09/10

Planning Noisy Probabilistic Relational Rules
Tobias Lang
Marc Toussaint

tobias.lang@tu-berlin.de
mtoussai@cs.tu-berlin.de

Machine Learning Robotics Group
Technische Universitat Berlin
Franklinstrae 28/29, 10587 Berlin, Germany

Abstract
Noisy probabilistic relational rules promising world model representation several reasons. compact generalize world instantiations. usually
interpretable learned effectively action experiences complex
worlds. investigate reasoning rules grounded relational domains. algorithms exploit compactness rules efficient flexible decision-theoretic planning.
first approach, combine rules Upper Confidence Bounds applied
Trees (UCT) algorithm based look-ahead trees. second approach converts
rules structured dynamic Bayesian network representation predicts effects
action sequences using approximate inference beliefs world states. evaluate
effectiveness approaches planning simulated complex 3D robot manipulation scenario articulated manipulator realistic physics domains
probabilistic planning competition. Empirical results show methods solve
problems existing methods fail.

1. Introduction
Building systems act autonomously complex environments central goal Artificial Intelligence. Nowadays, A.I. systems par particularly intelligent humans
specialized tasks playing chess. hopelessly inferior almost humans, however, deceivingly simple tasks everyday-life, clearing desktop,
preparing cup tea manipulating chess figures: current state art reasoning, planning, learning, perception, locomotion, manipulation far removed
human-level abilities, cannot yet contemplate working actual domain interest (Pasula, Zettlemoyer, & Kaelbling, 2007). Performing common object manipulations
indeed challenging task real world: choose large number
distinct actions uncertain outcomes number possible situations basically
unseizable.
act real world, accomplish two tasks. First, need understand
world works: example, pile plates stable place big plates
bottom; hard job build tower balls; filling tea cup may lead
dirty table cloth. Autonomous agents need learn world knowledge experience
adapt new environments rely human hand-crafting. paper,
employ recent solution learning (Pasula et al., 2007). know possible
effects actions, face second challenging problem: use acquired
knowledge reasonable time find sequence actions suitable achieve goals?
c
2010
AI Access Foundation. rights reserved.

fiLang & Toussaint

paper investigates novel algorithms tackle second task, namely planning.
pursue model-based approach planning complex domains. contrast modelfree approaches compute policies directly experience respect fixed goals
(also called habit-based decision making), follow purposive decision-making approach
(Botvinick & An, 2009) use learned models plan goal current state
hand. particular, simulate probabilistic effects action sequences. approach
interesting parallels recent neurobiology cognitive science results suggesting
behavior intelligent mammals driven internal simulation emulation:
found motor structures cortex activated planning,
execution motor commands suppressed (Hesslow, 2002; Grush, 2004).
Probabilistic relational world model representations received significant attention
last years. enable generalize object identities unencountered situations objects similar types account indeterministic action effects noise.
review several approaches together related work Section 2. Noisy
indeterministic deictic (NID) rules (Pasula et al., 2007) capture world dynamics
elegant compact way. particularly appealing learned effectively
experience. existing approach planning rules relies growing
full look-ahead trees grounded domain. Due large action space
stochasticity world, computational burden plan single action
method given situation overwhelmingly large. paper proposes two novel
ways reasoning efficiently grounded domain using learned NID rules, enabling fast
planning complex environments varying goals. First, apply existing Upper
Confidence bounds applied Trees (UCT) algorithm (Kocsis & Szepesvari, 2006) NID
rules. contrast full-grown look-ahead trees, UCT samples actions selectively, thereby
cutting suboptimal parts tree early. Second, introduce Probabilistic Relational
Action-sampling DBNs planning Algorithm (PRADA) uses probabilistic inference
cope uncertain action outcomes. Instead growing look-ahead trees sampled successor states previous approaches, PRADA applies approximate inference
techniques propagate effects actions. particular, make three contributions
PRADA: (i) Following idea framing planning probabilistic inference problem (Shachter, 1988; Toussaint, Storkey, & Harmeling, 2010), convert NID rules
dynamic Bayesian network (DBN) representation. (ii) derive approximate inference method cope state complexity time-slice resulting network.
Thereby, efficiently predict effects action sequences. (iii) planning based
sampling action-sequences, propose sampling distribution plans takes predicted state distributions account. evaluate planning approaches simulated
complex 3D robot manipulation environment realistic physics, articulated humanoid manipulating objects different types (see Fig. 4). domain contains billions
world states large number potential actions. learn NID rules experience
environment apply planning approaches different planning
scenarios increasing difficulty. Furthermore, provide results approaches
planning domains recent international probabilistic planning competition.
purpose, discuss relation NID rules probabilistic planning
domain definition language (PPDDL) used specification domains.
2

fiPlanning Noisy Probabilistic Relational Rules

begin paper discussing related work Section 2 reviewing
background work, namely stochastic relational representations, NID rules, formalization decision-theoretic planning graphical models Section 3. Section 4,
present two planning algorithms build look-ahead trees cope stochastic
actions. Section 5, introduce PRADA uses approximate inference planning.
Section 6, present empirical evaluation demonstrating utility planning
approaches. Finally, conclude outline future directions research.

2. Related Work
problem decision-making planning stochastic relational domains approached different ways. field relational reinforcement learning (RRL) (Dzeroski,
de Raedt, & Driessens, 2001; van Otterlo, 2009) investigates value functions Q-functions
defined possible ground states actions relational domain. key
idea describe important world features terms abstract logical formulas enabling
generalization objects situations. Model-free RRL approaches learn value functions
states actions directly experience. Q-function estimators include relational
regression trees (Dzeroski et al., 2001) instance-based regression using distance metrics relational states graph kernels (Driessens, Ramon, & Gartner, 2006).
Model-free approaches enable planning specific problem type used training
examples, e.g. on(X, ), thus may inappropriate situations goals
agent change quickly, e.g. on(X, ) inhand(X). contrast, model-based RRL
approaches first learn relational world model state transition experiences
use model planning, example form relational probability trees
individual state attributes (Croonenborghs, Ramon, Blockeel, & Bruynooghe, 2007)
SVMs using graph kernels (Halbritter & Geibel, 2007). stochastic relational NID rules
Pasula et al. (2007) particularly appealing action model representation,
shown empirically learn dynamics complex environments.
probabilistic relational world model available (either learned handcrafted),
one pursue decision-theoretic planning different ways. Within machine learning
community, popular direction research formalizes problem relational Markov
decision process (RMDP) develops dynamic programming algorithms compute solutions, i.e. policies complete state action spaces. Many algorithms reason
lifted abstract representation without grounding referring particular problem instances. Boutilier, Reiter, Price (2001) introduce Symbolic Dynamic Programming,
first exact solution technique RMDPs uses logical regression construct
minimal logical partitions state space required make necessary value function
distinctions. approach implemented difficult keep firstorder state formulas consistent manageable size. Based ideas, Kersting, van
Otterlo, de Raedt (2004) propose exact value iteration algorithm RMDPs using
logic-programming, called ReBel. employ restricted language represent RMDPs
reason efficiently state formulas. Holldobler Skvortsova (2004)
present first-order value iteration algorithm (FOVIA) using different restricted language.
Karabaev Skvortsova (2005) extend FOVIA combining first-order reasoning
actions heuristic search restricted states reachable initial
3

fiLang & Toussaint

state. Wang, Joshi, Khardon (2008) derive value iteration algorithm based using
first-order decision diagrams (FODDs) goal regression. introduce reduction operators FODDs keep representation small, may require complex reasoning;
empirical evaluation provided. Joshi, Kersting, Khardon (2009) apply
model checking reduce FODDs generalize arbitrary quantification.
techniques form interesting research direction reason exactly
abstract RMDPs. employ different methods ensure exact regression theorem proving, logical simplification, consistency checking. Therefore, principled approximations techniques discover good policies difficult domains
likewise worth investigating. instance, Gretton Thiebaux (2004) employ first-order
regression generate suitable hypothesis language use policy induction; thereby, approach avoids formula rewriting theorem proving, still
requiring model-checking. Sanner Boutilier (2007, 2009) present first-order approximate linear programming approach (FOALP). Prior producing plans, approximate
value function based linear combinations abstract first-order value functions,
showing impressive results solving RMDPs millions states. Fern, Yoon,
Givan (2006) consider variant approximate policy iteration (API) replace
value-function learning step learning step policy space. make use
policy-space bias described generic relational knowledge representation simulate trajectories improve learned policy. Kersting Driessens (2008) describe
non-parametric policy gradient approach deal propositional, continuous
relational domains unified way.
Instead working lifted representation, one may reason grounded domain.
makes straightforward account two special characteristics NID rules:
noise outcome uniqueness requirement rules. grounding RMDP
specifies rewards set goal states, one might principle apply traditional A.I. planning methods used propositional representations (Weld, 1999; Boutilier,
Dean, & Hanks, 1999). Traditionally, planning often cast search problem
state action space, restricting oneself portion state space considered contain goal states reachable current state within limited
horizon. Much research within planning community focused deterministic domains thus cant applied straightforwardly stochastic worlds. common approach
probabilistic planning, however, determinize planning problem apply deterministic planners (Kuter, Nau, Reisner, & Goldman, 2008). Indeed, FF-Replan (Yoon,
Fern, & Givan, 2007) extension using hindsight optimization (Yoon, Fern, Givan, &
Kambhampati, 2008) shown impressive performance many probabilistic planning
competition domains. common variant FF-Replan considers probabilistic outcome action separate deterministic action, ignoring respective probabilities.
runs deterministic Fast-Forward (FF) planner (Hoffmann & Nebel, 2001)
determinized problem. uses relaxation planning problem: ignores delete
effects actions applies clever heuristics prune search space. FF-Replan outputs
sequence actions expected states. time action execution leads state
plan, FF-Replan replan, i.e., recompute new plan scratch
current state. good performance FF-Replan many probabilistic domains
explained structure problems (Little & Thiebaux, 2007).
4

fiPlanning Noisy Probabilistic Relational Rules

argued FF-Replan less appropriate domains probability
reaching dead-end non-negligible outcome probabilities actions need
taken account construct good policy.
Many participants recent probabilistic planning competition (IPPC, 2008)
extend FF-Replan deal probabilities action outcomes (see competition
website brief descriptions algorithms). winner competition, RFF
(Teichteil-Konigsbuch, Kuter, & Infantes, 2010), computes robust policy offline generating successive execution paths leading goal using FF. resulting policy
low probability failing. LPPFF uses subgoals generated determinization
probabilistic planning problem divide smaller manageable problems. HMDPPs
strategy similar all-outcomes-determinization FF-Replan, accounts
probability associated outcome. SEH (Wu, Kalyanam, & Givan, 2008) extends
heuristic function FF-Replan cope local optima plans using stochastic
enforced hill-climbing.
common approach reasoning general reward-maximization context
avoids explicitly dealing uncertainty build look-ahead trees sampling successor
states. Two algorithms follow idea, namely SST (Kearns, Mansour, & Ng, 2002)
UCT (Kocsis & Szepesvari, 2006), investigated paper.
Another approach Buffet Aberdeen (2009) directly optimizes parameterized
policy using gradient descent. factor global policy simple approximate policies
starting action sample trajectories cope probabilistic effects.
Instead sampling state transitions, propose planning algorithm PRADA
paper (based Lang & Toussaint, 2009a) accounts uncertainty principled
way using approximate inference. Domshlak Hoffmann (2007) propose interesting
planning approach comes closest work. introduce probabilistic extension planner, using complex algorithms building probabilistic relaxed planning
graphs. construct dynamic Bayesian networks (DBNs) hand-crafted STRIPS operators reason actions states using weighted model counting. DBN
representation, however, inadequate type stochastic relational rules use,
reasons naive DBN model discuss Sec. 5.1 inappropriate. Planning inference approaches (Toussaint & Storkey, 2006) spread information
backwards DBNs calculate posteriors actions (resulting policies
complete state spaces). use backward propagation even full planning
inference relational domains open issue.
approaches working grounded representation common number
states actions grow exponentially number objects. apply
domains many objects, approaches need combined complementary
methods reduce state action space complexity relational domains.
instance, one focus envelopes states high-utility subsets state
space (Gardiol & Kaelbling, 2003), one ground representation respect
relevant objects (Lang & Toussaint, 2009b), one exploit equivalence actions
(Gardiol & Kaelbling, 2007), particularly useful combination ignoring
certain predicates functions relational logic language (Gardiol & Kaelbling, 2008).
5

fiLang & Toussaint

3. Background
section, set theoretical background planning algorithms
present subsequent sections. First, describe relational representations define world
states actions. present noisy indeterministic deictic (NID) rules detail
thereafter define problem decision-theoretic planning stochastic relational
domains. Finally, briefly review dynamic Bayesian networks.
3.1 State Action Representation
relational domain represented relational logic language L: set logical
predicates P set logical functions F contain relationships properties
hold domain objects. set logical predicates comprises possible actions
domain. concrete instantiation relational domain made finite set
objects O. arguments predicate function concrete, i.e. taken O,
call grounded. concrete world state fully described conjunction grounded
(potentially negated) predicates function values. Concrete actions described
positive grounded predicates A. arguments predicates functions
abstract logical variables represent object. predicate function
abstract arguments, call abstract. Abstract predicates functions enable
generalization objects situations. speak grounding formula
apply substitution maps variables appearing objects O.
relational model transition dynamics specifies P (s0 |a, s), probability
successor state s0 action performed state s. paper, usually
non-deterministic distribution. typically defined compactly terms formulas
abstract predicates functions. enables abstraction object identities
concrete domain instantiations. instance, consider set N cups: effects trying
grab cups may described single abstract model instead
using N individual models. apply given world state, one needs ground
respect objects domain. NID rules elegant way specify
model described following.
3.2 Noisy Indeterministic Deictic Rules
want learn relational model stochastic world use planning. Pasula
et al. (2007) recently introduced appealing action model representation based
noisy indeterministic deictic (NID) rules combine several advantages:
relational representation enabling generalization objects situations,
indeterministic action outcomes probabilities account stochastic domains,
deictic references actions reduce action space,
noise outcomes avoid explicit modeling rare overly complex outcomes,
existence effective learning algorithm.
6

fiPlanning Noisy Probabilistic Relational Rules

Table 1 shows exemplary NID rule complex robot manipulation domain.
Fig. 1 depicts situation rule used prediction. Formally, NID rule
r given

pr,1
: r,1 (X )




..
.
(1)
ar (X ) : r (X )

:
r,mr (X )
p

r,m
r


pr,0
: r,0
X set logical variables rule (which represent (sub-)set abstract
objects). rules define world models formulas abstract, i.e.,
arguments logical variables. rule r consists preconditions, namely action
ar applied X state P
context r fulfilled, mr +1 different outcomes
associated probabilities pr,i 0, i=0 pr,i = 1. outcome r,i (X ) describes
predicates functions change rule applied. context r (X ) outcomes
r,i (X ) conjunctions (potentially negated) literals constructed predicates
P well equality statements comparing functions F constant values. Besides
explicitely stated outcomes r,i (i > 0), so-called noise outcome r,0 models implicitly
potential outcomes rule. particular, includes rare overly
complex outcomes typical noisy domains, want cover explicitly
compactness generalization reasons. instance, context rule depicted
Fig. 1 potential, highly improbable outcome grab blue cube pushing
objects table: noise outcome allows account without burden
explicitly stating it.
arguments action a(Xa ) may true subset Xa X variables X
rule. remaining variables called deictic references = X \ Xa denote
objects relative agent action performed. Using deictic references
advantage decrease arity action predicates. turn reduces size
action space least order magnitude, significant effects
planning problem. instance, consider binary action predicate world
n objects n2 groundings contrast unary action predicate n
groundings.
above, let denote substitution maps variables constant objects, : X O.
Applying abstract rule r(X ) yields ground rule r((X )). say ground rule r
covers state ground action |= r = ar . Let set ground NID
rules. define (a) := {r | r , ar = a} set rules provide predictions
action a. r rule (a) cover state s, call unique covering rule
s. state-action pair (s, a) unique covering rule r, calculate P (s0 | s, a)
taking outcomes r account weighted respective probabilities,
r

0

0

P (s |s, a) = P (s |s, r) =


X

pr,i P (s0 |r,i , s) + pr,0 P (s0 |r,0 , s),

(2)

i=1

where, > 0, P (s0 | r,i , s) deterministic distribution one unique
state constructed taking changes r,i account. distribution given
7

fiLang & Toussaint

Table 1: Example NID rule complex robot manipulation scenario, models
try grab ball X. cube implicitly defined one X (deictic
referencing). X ends robots hand high probability, might
fall table. small probability something unpredictable happens.
Confer Fig. 1 example application.

grab(X) : on(X, ), ball(X), cube(Y ), table(Z)

0.7 : inhand(X), on(X, )
0.2 : on(X, Z), on(X, )


0.1 : noise

Figure 1: NID rule defined Table 1 used predict effects action
grab(ball) situation left side. right side depicts possible
successor states predicted rule. noise outcome indicated
question mark define unique successor state.

noise outcome, P (s0 | r,0 , s), unknown needs estimated. Pasula et al. use
worst case constant bound pmin P (s0 |r,0 , s) lower bound P (s0 |s, a). Alternatively,
come well-defined distribution, one may assign low probability many
successor states. described detail Sec. 5.2, planning algorithm PRADA
exploits factored state representation grounded relational domain achieve
predicting state attribute change low probability.
state-action pair (s, a) unique covering rule r (e.g. two rules cover
(s, a) providing conflicting predictions), one predict effects means
noisy default rule r explains effects changing state attributes noise:
P (s0 |s, r ) = P (s0 | r ,0 , s). Essentially, using r expresses know
happen. meaningful thus disadvantageous planning. (Hence, one
bias NID rules learner learn rules contexts likely mutually
exclusive.) reason, concept unique covering rules crucial planning
NID rules. Here, pay price using deictic references: using
abstract NID rule prediction, always ensure deictic references
unique groundings. may require examining large part state representation,
8

fiPlanning Noisy Probabilistic Relational Rules

proper storage ground state efficient indexing techniques logical formula
evaluation needed.
ability learn models environment experience crucial requirement
autonomous agents. problem learning rule-sets general NP-hard, efficiency guarantees sample complexity given many learning subtasks
suitable restrictions (Walsh, 2010). Pasula et al. (2007) proposed supervised batch
learning algorithm complete NID rules. algorithm learns structure rules
well parameters experience triples (s, a, s0 ), stating observed successor
state s0 action applied state s. performs greedy search space
rule-sets. optimizes tradeoff maximizing likelihood experience
triples minimizing complexity current hypothesis rule-set optimizing
scoring metric
X
X
S() =
log P (s0 | s, rs,a )
P EN (r) ,
(3)
(s,a,s0 )

r

rs,a either unique covering rule (s, a) noisy default rule r
scaling parameter controls influence regularization. P EN (r) penalizes
complexity rule defined total number literals r.
noise outcome NID rules crucial learning. learning algorithm initialized rule-set comprising noisy default rule r iteratively adds
new rules modifies existing ones using set search operators. noise outcome
allows avoiding overfitting, need model rare overly complex outcomes
explicitly. drawback successor state distribution P (s0 | r,0 , s) unknown.
deal problem, learning algorithm uses lower bound pmin approximate
distribution, described above. algorithm uses greedy heuristics attempt
learn complete rules, guarantees behavior given. Pasula et al., however, report impressive results complex noisy environments. Sec. 6.1, confirm
results simulated noisy robot manipulation scenario. major motivation employing NID rules learn observed actions state transitions.
Furthermore, planning approach PRADA exploit simple structure (which
similar probabilistic STRIPS operators) convert DBN representation.
provide detailed comparison NID rules PPDDL Appendix B. NID
rules support features sophisticated domain description language
PPDDL, compactly capture dynamics many interesting planning domains.
3.3 Decision-Theoretic Planning
problem decision-theoretic planning find actions given state
expected maximize future rewards states actions (Boutilier et al., 1999).
classical planning, reward usually defined terms clear-cut goal
either fulfilled fulfilled state. expressed means logical
formula . Typically, formula partial state description exists
one state holds. example, goal might put romance
books specific shelf, matter remaining books lying. case,
planning involves finding sequence actions executing starting
9

fiLang & Toussaint

result world state s0 s0 |= . stochastic domains, however, outcomes
actions uncertain. Probabilistic planning inherently harder deterministic
counterpart (Littman, Goldsmith, & Mundhenk, 1997). particular, achieving goal
state certainty typically unrealistic. Instead, one may define lower bound
probability achieving goal state. second source uncertainty next uncertain
action outcomes uncertainty initial state s. ignore latter
following always assume deterministic initial states. see later, however,
straightforward incorporate uncertainty initial state using one proposed
planning approaches.
Instead classical planning task finished achieved state
goal fulfilled, task may ongoing. instance, goal might
keep desktop tidy. formalized means reward function states,
yields high reward desirable states (for simplicity, assume rewards
depend actions). approach taken reinforcement learning formalisms
(Sutton & Barto, 1998). Classical planning goals easily formalized
reward function. cast scenario planning stochastic relational domain
relational Markov decision process (RMDP) framework (Boutilier et al., 2001). follow
notation van Otterlo (2009) define RMDP 4-tuple (S, A, T, R). contrast
enumerated state spaces, state space relational structure defined
logical predicates P functions F, yield ground atoms arguments taken
set domain objects O. action space defined positive predicates
arguments O. : [0, 1] transition distribution R : R
reward function. R make use factored relational representation
abstract states actions, discussed following. Typically,
state space action space relational domain large. Consider
instance domain 5 objects use 3 binary predicates represent states:
2
case, number states 235 = 275 . Relational world models encapsulate transition
probabilities compact way exploiting relational structure. example, NID rules
described Eq. (2) achieve generalized partial world state descriptions
form conjunctions abstract literals. compactness models, however,
carry directly planning problem.
(deterministic) policy : tells us action take given state.
fixed horizon discount
0 < < 1, interested maximizing
Pd factor
r . value factored state defined
discounted total reward r =


t=0
expected return state following policy :
V (s) = E[r | s0 = s; ] .

(4)

solution RMDP, thus problem planning, optimal policy
maximizes expected return. defined Bellman equation:
X


V (s) = R(s) + max[
P (s0 | s, a)V (s0 )] .
aA

10

s0

(5)

fiPlanning Noisy Probabilistic Relational Rules

Similarly, one define value Q (s, a) action state expected return
action taken state s, using policy select subsequent actions:
Q (s, a) = E[r | s0 = s, a0 = a; ]
X
= R(s) +
V (s0 )P (s0 | s, a) .

(6)
(7)

s0

Q-values optimal policy let us define optimal action optimal
value state


= argmax Q (s, a)



(8)

aA




V (s) = max Q (s, a) .
aA

(9)

enumerated unstructured state spaces, state Q-values computed using dynamic programming methods resulting optimal policies complete state space.
Recently, promising approaches exploiting relational structure proposed apply similar ideas solve approximate solutions RDMPs abstract level (without
referring concrete objects O) (see related work Sec. 2). Alternatively, one may
reason grounded relational domain. makes straightforward account
noise outcome uniqueness requirement NID rules. Usually, one focuses estimating optimal action values given state. approach appealing agents
varying goals, quickly coming plan problem hand
appropriate computing abstract policy complete state space. Although
grounding simplifies problem, decision-theoretic planning propositionalized representation challenging task complex stochastic domains. Sections 4 5,
present different algorithms reasoning grounded relational domain estimating
optimal Q-values actions (and action-sequences) given state.
3.4 Dynamic Bayesian Networks
Dynamic Bayesian networks (DBNs) model development stochastic systems
time. PRADA planning algorithm introduce Sec. 5 makes use
kind graphical model evaluate stochastic effects action sequences factored
grounded relational world states. Therefore, briefly review Bayesian networks
dynamic extension here.
Bayesian network (BN) (Jensen, 1996) compact representation joint probability distribution set random variables X means directed acyclic graph
G. nodes G represent random variables, edges define dependencies thereby express conditional independence assumptions. value x variable
X X depends values immediate ancestors G, called
parents P a(X) X. Conditional probability functions node define P (X | P a(X)).
case discrete variables, may defined form conditional probability tables.
BN compact representation distribution X nodes
parents conditional probability functions significant local structure.
play crucial role development graphical models PRADA.
11

fiLang & Toussaint

DBN (Murphy, 2002) extends BN formalism model dynamic system evolving
time. Usually, focus discrete-time stochastic processes. underlying
system (in case, world state) represented BN B, DBN maintains
copy BN every time-step. DBN defined pair BNs (B0 , B ),
B0 (deterministic uncertain) prior defines state system
initial state = 0, B two-slice BN defines dependencies two
successive time-steps + 1. implements first-order Markov assumption:
variables time + 1 depend variables time + 1 variables t.

4. Planning Look-Ahead Trees
plan NID rules, one treat domain described
ulary relational Markov decision process discussed Sec.
present two value-based reinforcement learning algorithms
generative model build look-ahead trees starting initial
used estimate values actions states.

relational logic vocab3.3. following,
employ NID rules
state. trees

4.1 Sparse Sampling Trees
Sparse Sampling Tree (SST) algorithm (Kearns et al., 2002) MDP planning samples
randomly sparse, full-grown look-ahead trees states starting given state
root. suffices compute near-optimal actions state MDP. Given
planning horizon branching factor b, SST works follows (see Fig. 2): tree
node (representing state), (i) SST takes possible actions account, (ii)
action takes b samples successor state distribution using generative model
transitions, e.g. transition model MDP, build tree nodes next
level. Values tree nodes computed recursively leaves root using
Bellman equation: given node, Q-value possible action estimated
averaging values b children states action; then, maximizing
Q-value actions chosen estimate value given node. SST
favorable property independent total number states MDP,
examines restricted subset state space. Nonetheless, exponential
time horizon taken account.
Pasula et al. (2007) apply SST planning NID rules. sampling noise
outcome planning SST, assume stay state, discount
estimated value. refer adaptation speak SST planning
remainder paper. action unique covering rule, use noisy
default rule r predict effects. always better perform othing action
instead staying state get punished. Hence, SST planning one
discard actions given state unique covering rules.
SST near-optimal, practice feasible small branching factor
b planning horizon d. Let number actions a. number nodes
horizon (ba)d . (This number reduced outcome rule sampled
multiple times.) illustration, assume 10 possible actions per time-step
set parameters = 4 b = 4 (the choice Pasula et al. experiments). plan
single action given state, one visit (10 4)4 = 2, 560, 000 states. smaller
12

fiPlanning Noisy Probabilistic Relational Rules

Figure 2: SST planning algorithm samples sparse, full-grown look-ahead trees
estimate values actions states.

choices b lead faster planning, result significant accuracy loss realistic
domains. Kearns et al. note, SST useful special structure permits
compact representation available. Sec. 5, introduce alternative planning
approach based approximate inference exploits structure NID rules.
4.2 Sampling Trees Upper Confidence Bounds
Upper Confidence Bounds applied Trees (UCT) algorithm (Kocsis & Szepesvari,
2006) samples search tree subsequent states starting current state root.
contrast SST generates b successor states every action state, idea
UCT choose actions selectively given state thus sample selectively
successor state distribution. UCT tries identify large subsets suboptimal actions early
sampling procedure focus promising parts look-ahead tree instead.
UCT builds look-ahead tree repeatedly sampling simulated episodes
initial state using generative model, e.g. transition model MDP. episode
sequence states, rewards actions limited horizon d: s0 , r0 , a1 , s1 , r1 , a2 . . . sd , rd .
simulated episode, values tree nodes (representing states) updated
online simulation policy improved respect new values. result,
distinct value estimated state-action pair tree Monte-Carlo simulation.
precisely, UCT follows following policy tree node s: exist actions
explored yet, UCT samples one using uniform
distribution. Otherwise, actions explored least once, UCT selects
action maximizes upper confidence bound QO
U CT (s, a) estimated action
13

fiLang & Toussaint

value QU CT (s, a),

QO
U CT (s, a)

= QU CT (s, a) + c

log ns
,
ns,a

U CT (s) = argmax QO
U CT (s, a) ,

(10)
(11)



ns,a counts number times actionPa selected state s, ns
counts total number visits state s, ns = ns,a . bias parameter c defines
influence number previous action selections thereby controls extent
upper confidence bound.
end episode, value encountered state-action pair (st , ), 0
< d, updated using total discounted rewards:
nst ,at nst ,at + 1 ,
QU CT (st , ) QU CT (st , ) +

(12)
1
nst ,at


X

[

0

rt0 QU CT (st , )] .

(13)

t0 =t

policy UCT implements exploration-exploitation tradeoff: balances
exploring currently suboptimal-looking actions selected seldom thus far
exploiting currently best-looking actions get precise estimates values.
total number episodes controls accuracy UCTs estimates balanced
overall running time.
UCT achieved remarkable results challenging domains game Go
(Gelly & Silver, 2007). best knowledge, first apply UCT
planning stochastic relational domains, using NID rules generative model. adapt
UCT cope noise outcomes fashion SST: assume stay
state discount obtained rewards. Thus, UCT takes actions unique
covering rules account, reasons SST does.

5. Planning Approximate Inference
Uncertain action outcomes characterize complex environments, make planning relational domains substantially difficult. sampling-based approaches discussed
previous section tackle problem repeatedly generating samples outcome
distribution action using transition probabilities MDP. leads lookahead trees easily blow planning horizon. Instead sampling successor
states, one may maintain distribution states, so-called belief. following,
introduce approach planning grounded stochastic relation domains propagates beliefs states sense state monitoring. First, show create
compact graphical models NID rules. develop approximate inference method
efficiently propagate beliefs. hand, describe Probabilistic Relational
Action-sampling DBNs planning Algorithm (PRADA), samples action-sequences
informed way evaluates using approximate inference DBNs. Then,
example presented illustrate reasoning PRADA. Finally, discuss PRADA
comparison approaches previous section, SST UCT, present simple
extension PRADA.
14

fiPlanning Noisy Probabilistic Relational Rules

(a)

(b)

Figure 3: Graphical models NID rules: (a) Naive DBN; (b) DBN exploiting NID factorization

5.1 Graphical Models NID Rules
Decision-theoretic problems agents need choose appropriate actions represented means Markov chains dynamic Bayesian networks (DBNs)
augmented decision nodes specify agents actions (Boutilier et al., 1999).
following, discuss convert NID rules DBNs PRADA algorithm
use plan probabilistic inference. denote random variables upper case letters
(e.g. S), values corresponding lower case letters (e.g., dom(S)), variable
vectors bold upper case letters (e.g. = (S1 , S2 , S3 )) value vectors bold lower
case letters (e.g. = (s1 , s2 , s3 )). use column notation, e.g. s2:4 = (s2 , s3 , s4 ).
naive way convert NID rules DBNs shown Fig. 3(a). States represented
vector = (S1 , . . . , SN ) ground predicate P binary Si
ground function F Sj range according represented
function. Actions represented integer variable indicates action
vector ground action predicates A. reward gained state represented
U may depend subset state variables. possible express
arbitrary reward expectations P (U | S) binary U (Cooper, 1988). define
transition dynamics using NID rules naive model? Assume given set
fully abstract NID rules. compute groundings rules w.r.t. objects
domain get set K different ground NID rules. parents state variable
Si0 successor time-step include action variable respective variable Si
predecessor time-step. parents Si0 determined follows:
rule r literal corresponding Si0 appears outcomes r, variables
Sk corresponding literals preconditions r parents Si0 . typically Si0
manipulated several actions turn modeled several rules, total
number parents Si0 large. problem worsened usage deictic
references NID rules, increase total number K ground rules .
resulting local structure conditional probability function Si0 complex, one
account uniqueness covering rules. complex dependencies
two time-slices make representation unfeasible planning.
15

fiLang & Toussaint

Therefore, exploit structure NID rules model state transition
compact graphical model shown Fig. 3(b) representing joint distribution
P (u0 , s0 , o, r, | a, s)

=

P (u0 | s0 ) P (s0 | o, r, s) P (o | r) P (r | a, ) P ( | s) ,

(14)

explain detail following. before, assume given set
fully abstract NID rules, compute set K different ground NID rules
w.r.t. objects domain. addition S, S0 , A, U U 0 above, use
binary random variable rule model event context holds,
case required literals hold. Let I() indicator function 1
argument evaluates true 0 otherwise. Then,


K
K


^
P ( | s) =
P (i |s(i ) ) =
Sj = sri ,j .
(15)

i=1

i=1

j(i )

V

use express logical conjunction 1 n . function () yields set
indices state variables s, depends. sri denotes configuration
state variables corresponding literals context ri . use integer-valued
variable R ranging K +1 possible values identify rule predicts effects
action. exists, unique covering rule current state-action pair,
i.e., rule r (a) modeling action whose context holds:


^
P (R = r|a, ) = r (a) r = 1
r 0 = 0 .
(16)
r0 (a)\{r}

unique covering rule exists, predict changes indicated special value
R = 0 (assuming execute action, similarly SST UCT do):


^
^
P (R = 0 | a, ) =
r = 1
r 0 = 0 .
(17)
r0 (a)\{r}

r(a)

integer-valued variable represents outcome action predicted
rule. ranges possible values maximum number outcomes
rules have. ensure sound semantics, introduce empty dummy outcomes
zero-probability rules whose number outcomes less . probability
outcome defined corresponding rule:
P (O = | r) = pr,o .
define probability successor state

P (s0 | o, s, r) =
P (s0i | o, si , r) ,

(18)

(19)



one unique state constructed taking changes according
r,o account: outcome specifies value Si0 , value probability
16

fiPlanning Noisy Probabilistic Relational Rules

one. Otherwise, value state variable persists previous time-step.
rules usually change small subset s, persistence often applies. resulting
dependency P (s0i | o, r, si ) variable Si0 time-step + 1 compact. contrast
naive DBN Fig. 3(a), three parents, namely variables outcome,
rule predecessor previous time-step. simplifies specification
conditional probability function 0 significantly enables efficient inference,
see later. probability reward given


^
P (U 0 = 1 | s0 ) =
Sj0 = j .
(20)
j(U 0 )

function (U 0 ) yields set indices state variables s0 , U 0 depends.
configuration variables corresponds planning goal denoted
. Uncertain initial states naturally accounted specifying priors P (s0 ).
renounce specification prior here, however, initial state s0 always given
experiments later enable comparison look-ahead tree based approaches SST
UCT require deterministic initial states (which might sampled
prior). choice distribution P (a) used sampling actions described
Sec. 5.3.
simplicity ignored derived predicates functions defined
terms predicates functions presentation graphical model. Derived
concepts may increase compactness rules. dependencies among concepts acyclic,
straightforward include derived concepts model intra-state dependencies
corresponding variables. Indeed, use derived predicates experiments.
interested inferring posterior state distributions P (st | a0:t1 ) given sequence previous actions (where omit conditioning initial state simplicity).
Exact inference intractable graphical model. constructing junction tree,
get cliques comprise whole Markov slices (all variables representing state
certain time-step): consider eliminating state variables St+1 . Due moralization,
outcome variable connected state variables St . elimination O,
variables St form clique. Thus, make use approximate inference
techniques. General loopy belief propagation (LBP) unfeasible due deterministic
dependencies small cycles inhibit convergence. conducted preliminary tests small networks damping factor, without success. interesting
open question whether ways alternate propagating deterministic information running LBP remaining parts network, e.g., whether methods
MC-SAT (Poon & Domingos, 2007) successfully applied decision-making contexts ours. next subsection, propose different approximate inference scheme
using factored frontier (FF). algorithm describes forward inference procedure
computes exact marginals next time-step subject factored approximation
previous time-step. Here, advantage exploit structure
involved DBNs come formulas marginals. related passing
forward messages. contrast LBP, information propagated backwards. Note
approach condition rewards (as full planning inference) samples
actions, backward reasoning uninformative.
17

fiLang & Toussaint

5.2 Approximate Inference
following, present efficient method approximate inference previously
proposed DBNs exploiting factorization NID rules. focus mathematical
derivations. illustrative example provided Sec. 5.4.
follow idea factored frontier (FF) algorithm (Murphy & Weiss, 2001)
approximate belief product marginals:

P (st | a0:t1 )
P (sti | a0:t1 ) .
(21)


define
(sti ) := P (sti | a0:t1 )
(st ) := P (st | a0:t1 )

N


(22)
(sti )

(23)

i=1

derive filter DBN model Fig. 3(b). interested inferring
state distribution time + 1 given action sequence a0:t calculate marginals
state attributes
t+1
(st+1
| a0:t )
) = P (si
X
=
P (st+1
| rt , a0:t1 ) P (rt | a0:t ) .


(24)
(25)

rt

Eq. (25), use rules prediction, weighted respective posteriors P (rt | a0:t ).
reflects fact depending state use different rules model
action. weight P (rt | a0:t ) 0 rules modeling action . remaining
rules model , weights correspond posterior parts
state space according rule used prediction.
compute first term (25)
X
P (st+1
| rt , a0:t1 ) =
P (st+1
| rt , sti ) P (sti | rt , a0:t1 )


sti



X

P (st+1
| rt , sti ) (sti ) .


(26)

sti

Here, sum possible values variable Si previous time-step t. Intuitively, take account potential pasts arrive value st+1
next

, st ) enables us easily predict probabilities
time-step. resulting term P (st+1
|
r


next time-step discussed below. prediction weighted marginal
(sti ) respective previous value. approximation (26) assumes sti conditionally independent rt . true general choice rule prediction
depends current state thus attribute Si . improve approximation one examine whether sti part context rt : case, infer
state sti knowing rt . However, found approximation sufficient.
18

fiPlanning Noisy Probabilistic Relational Rules

one would expect, calculate successor state distribution P (st+1
| rt , sti )


taking different outcomes r account weighted respective probabilities
P (o | rt ),
X
P (st+1
| rt , sti ) =
P (st+1
| o, rt , sti ) P (o | rt ) .
(27)




shows us update belief Sit+1 predict rule rt . P (st+1
| o, rt , sti )

t+1
deterministic distribution. changes value Si , si set accordingly. Otherwise, value sti persists.
Lets turn computation second term Eq. (25), P (rt | a0:t ), posterior
rules. trick use context variables exploit assumption
rule r models state transition uniquely covers (at , st ), indicated
appropriate assignment . reduced expression
involving marginals (). start
X
P (Rt = r | a0:t ) =
P (Rt = r | , a0:t ) P (t | a0:t )





^

= I(r (at )) P tr = 1,

tr0 = 0 | a0:t1

r0 (at )\{r}




^

= I(r (at )) P (tr = 1 | a0:t1 ) P

tr0 = 0 | tr = 1, a0:t1 .

r0 (at )\{r}

(28)
simplify summation , consider unique assignment
context variables r used prediction: provided models action, indicated
I(r (at )), case context tr holds, contexts tr0
competing rules r0 action hold.
calculate second term (28) summing states
X

X
P (tr = 1 | a0:t1 ) =
P (tr = 1 | st ) (st )
P (tr = 1 | st )
(stj )
(29)
st

=



st

(Sjt = sr,j )

.

j

(30)

j(tr )

approximation (29) assumption. (30), sr denotes configuration
state variables according context r (15). sum variables
context r. variables rs context remain: terms (Sjt = sr,j ) correspond
probabilities respective literals.
third term (28) joint posterior contexts competing rules r0
given rs context already holds. interested situation none
contexts hold. calculate


^

P
tr0 = 0 | tr = 1, a0:t1
P (tr0 = 0 | tr = 1, a0:t1 ) ,
(31)
r0 (at )\{r}

r0 (at )\{r}

19

fiLang & Toussaint

approximating product individual posteriors. latter computed
X
P (tr0 = 0 | tr = 1, a0:t1 ) =
P (tr0 = 0 | st ) P (st | tr = 1, a0:t1 )
(32)



r r0
1.0 Q

1.0 i(t ), (Si = sr0 ,i ) otherwise
,

r0


(33)

i6(r )

if-condition expresses logical contradiction contexts r r0 .
contexts contradict, r0 context surely hold given rs context holds.
Otherwise, know state attributes apppearing contexts r r0
hold condition r = 1. Therefore, examine remaining state
attributes r0 context. Again, approximate posterior marginals.
Finally, compute reward probability straightforwardly
X

P (U = 1 | st )P (st | a0:t1 , s0 )
(Sit = ) ,
(34)
P (U = 1 | a0:t1 ) =
st

i(U )

denotes configuration state variables corresponding planning goal
(20). above, summation states simplified assumption resulting
product marginals required state attributes.
overall computational costs propagating effects action quadratic
number rules action (for rule calculate probability
none others applies) linear maximum numbers context literals
manipulated state attributes rules.
inference framework requires approximation distribution P (s0 | r,0 , s)
(cf. Eq. (2)) cope noise outcome NID rules. training data used
learn rules, estimate predicates functions change value time follows: let
Sc contain corresponding variables. estimate rule r average number
N r changed state attributes noise outcome applies. Due factored frontier
approach, consider noise effects variable independently. approximate
r
probability Si Sc changes rs noise outcome | SNC | . case change,
changed values Si equal probability.
5.3 Planning
DBN representation Fig. 3(b) together approximate inference method described last subsection enable us derive novel planning algorithm stochastic
relational domains: Probabilistic Relational Action-sampling DBNs planning Algorithm (PRADA) plans sampling action sequences informed way based predicted
beliefs states evaluating action sequences using approximate inference.
precisely, sample sequences actions a0:T 1 length . 0 < ,
infer posteriors states P (st | a0:t1 , s0 ) rewards P (ut | a0:t1 , s0 ) (in sense
filtering state monitoring). Then, calculate value action sequence
discount factor 0 < < 1
Q(s0 , a0:T 1 ) :=


X

P (U = 1 | a0:t1 , s0 ) .

t=0

20

(35)

fiPlanning Noisy Probabilistic Relational Rules

choose first action best sequence = argmaxa0:T 1 Q(a0:T 1 , s0 ),
value exceeds certain threshold (e.g., = 0). Otherwise, continue sampling actionsequences either action found planning given up. quality found
plan controlled total number action-sequence samples traded
time available planning.
aim strategy sample good action sequences high probability.
propose choose equal probability among actions unique covering
rule current state. Thereby, avoid use noisy default rule r
models action effects noise thus poor use planning. action time t,
PRADA samples distribution


^
X

tr0 = 0 | a0:t1 .
(36)
P tr = 1,
Psample
(a)
r(a)

r0 (a)\{r}

sum rules action a: rule add posterior
unique covering rule, i.e. context tr holds, contexts tr0 competing
rules r0 hold. sampling distribution takes current state distribution
account. Thus, probability sample action sequence predicting state sequence
s0 , . . . , sT depends likelihood state sequence given a: likely required outcomes are, likely next actions sampled. Using policy,
PRADA miss actions SST UCT explore, following proposition
states (proof Appendix A).
Proposition 1: set action sequences PRADA samples non-zero probability
super-set ones SST UCT.
experiments, replan action executed without reusing knowledge previous time-steps. simple strategy helps get general impression
PRADAs planning performance complexity. strategies easily conceivable.
instance, one might execute entire sequence without replanning, trading faster
computation times potential loss achieved reward. noisy environments,
might seem better strategy combine reuse previous plans replanning.
instance, one could omit first action previous plan, executed,
examine suitability remaining actions new state. consider
single best action sequence, many planning domains might beneficial
marginalize sequences first action. instance, action a1
might lead number reasonable sequences, none best, another
action a2 first one good sequence, many bad ones case one
might favor a1 .
5.4 Illustrative Example
Let us consider small planning problem Table 2 illustrate reasoning procedure
PRADA. domain noisy cubeworld represented predicates table(X), cube(X),
on(X, ), inhand(X) clear(X) Y.on(Y, X) robot perform two types
actions: may either lift cube X means action grab(X) put cube
21

fiLang & Toussaint

held hand top another object X using puton(X). start state s0 shown 2(a)
contains three cubes a, b c stacked pile table t. goal shown 2(b)
get middle cube b on-top top cube a. world model provides three abstract
NID rules predict action effects, shown Table 2(c). first rule uncertain
outcomes: models grab object another object. contrast, grabbing
clear object (Rule 2) putting object somewhere (Rule 3) always leads
successor state.
First, PRADA constructs DBN represent planning problem. purpose,
computes grounded rules respect objects = {a, b, c, t} shown 2(d).
potential grounded rules ignored: one deduce abstract rules
predicates changeable. combination specifications s0 , prunes
grounded rules. instance, know s0 table. Thus, ground rule
action argument X = needs constructed rules require cube(X).
Based DBN, PRADA samples action-sequences evaluates expected
rewards. following, investigate procedure sampling action-sequence
(grab(b), puton(a)). Table 2(e) presents inferred values DBN variables
auxiliary quantities. marginals (Eq. (22)) state variables = 0
set deterministically according s0 . calculate posteriors context variables
P ( | a0:t1 ) according Eq. (30). example, = 0 one rule
probability 1.0 actions grab(a), grab(b) grab(c). contrast,
rules non-zero probability various puton() actions. help Eq. (33),
calculate probability rule r unique covering rule respective
action (listed Unique rule; note condition fixed action thus
far): case context r r holds, contexts r0 competing rules
r0 action hold. = 0, posterior r alone.
resulting probabilities used calculate sampling distribution Eq. (36): first,
compute probability action unique covering rule simple
sum probabilities previous step (listed Action coverage table); then,
normalize values get sampling distribution Psample (). = 0, results
sampling distribution uniform three actions unique rules. Assume
sample a0 = grab(b) (grabbing blue cube b). Variable R specifies ground rules
use predicting state marginals next time-step. infer posterior
according Eq. (28). Here, P (R0 = (1, b/act) | a0 ) = 1.0.
Things get interesting = 1. Here, observe effects factored
frontier. instance, consider calculating posterior context r ground rule
r = (1, b/att) (grabbing blue cube b yellow a) using Eq. (30),
P ((1,b/att) | a0 ) (on(a, b)) (on(b, t)) (cube(a)) (cube(b)) (table(t))
= 0.2 0.2 1.0 1.0 1.0 = 0.04.
contrast, exact value P ((1,b/att) | a0 ) = 0.2, according third outcome
abstract Rule 1 used predict a0 . imprecision due ignoring correlations:
regards marginals on(a, b) on(b, t) independent, fact fully
correlated.
= 1, action grab(a) three ground rules non-zero context probabilities
(grabbing either b, c t). due three different outcomes abstract
22

fiPlanning Noisy Probabilistic Relational Rules

Table 2: Example PRADAs factored frontier inference
(a) Start state
s0 = {on(a, b), on(b, c), on(c, t),
cube(a), cube(b), cube(c), table(t)}

(b) Goal
= {on(b, a)}

(c) Abstract NID rules example situations
Rule 1:
grab(X) : on(Y, X), on(X, Z), cube(X), cube(Y ), table(T )

0.5 : inhand(X), on(Y, Z), on(Y, X), on(X, Z)
0.3 : inhand(X), on(Y, ), on(Y, X), on(X, Z)


0.2 : on(X, ), on(X, Z)

Rule 2:
grab(X) : cube(X), clear(X), on(X, )

1.0 : inhand(X), on(X, )


(e) Inferred posteriors PRADAs

inference

action-sequence
(grab(b), puton(a))
t=0

t=1

t=2

State marginals
on(a, b)
on(a, c)
on(a, t)
on(b, a)
on(b, c)
on(b, t)
on(c, t)
inhand(b)
clear(a)
clear(b)
clear(c)
Goal U

1.0
0.0
0.0
0.0
1.0
0.0
1.0
0.0
1.0
0.0
0.0
0.0

0.2
0.5
0.3
0.0
0.0
0.2
1.0
0.8
1.0
0.8
0.5
0.0

0.2
0.5
0.3
0.8
0.0
0.2
1.0
0.16
0.2
0.8
0.5
0.8

P ( | a0:t1 )
(1,b/act)
(1,b/att)
(1,c/btt)
(2,a/b)
(2,a/c)
(2,a/t)
(2,b/t)
(2,c/t)
(3,a/b)
(3,c/b)
(3,t/b)

1.0
0.0
1.0
1.0
0.0
0.0
0.0
0.0
0.0
0.0
0.0

0.0
0.04
0.5
0.2
0.5
0.3
0.16
0.5
0.8
0.8
0.8

Unique rule
(1, b/act)
1.0
0.0
(1, b/att)
0.0 0.0336
(1, c/att)
0.0
0.25
(1, c/btt)
1.0
0.0
(2, a/b)
1.0
0.07
(2, a/c)
0.0
0.28
(2, a/t)
0.0
0.12
(2, b/t)
0.0 0.154
(2, c/t)
0.0
0.25
(3, a/b)
0.0
0.8
(3, c/b)
0.0
0.8
(3, t/b)
0.0
0.8
Action coverage
grab(a)
1.0
0.47
grab(b)
1.0 0.187
grab(c)
1.0
0.5
puton(a)
0.0
0.8
puton(c)
0.0
0.8
puton(t)
0.0
0.8
Sample distribution
Psample (grab(a))
0.33 0.132
Psample (grab(b))
0.33 0.0526
Psample (grab(c))
0.33 0.141
Psample (puton(a))
0.0 0.225
Psample (puton(c))
0.0 0.225
Psample (puton(t))
0.0 0.225
P (Rt = rt | a0:t )
Rt = (1, b/act)
1.0
0.0
Rt = (3, a/b)
0.0
0.8
Rt = 0
0.0
0.2

Rule 3:
puton(X) : inhand(Y ), cube(Y )

1.0 : on(Y, X), inhand(X)


(d) Grounded NID rules
Grounded Rule Action
Substitution
(1, a/bbt)
grab(a) {X a, b, Z b, t}
(1, a/bct)
grab(a) {X a, b, Z c, t}
...
(1, c/bbt)
grab(c) {X c, b, Z b, t}
(2, a/b)
grab(a)
{X a, b}
(2, a/c)
grab(a)
{X a, c}
(2, a/t)
grab(a)
{X a, t}
...
(2, c/t)
grab(c)
{X c, t}
(3, a/b)
puton(a)
{X a, b}
(3, a/c)
puton(a)
{X a, c}
...
(3, t/c)
puton(t)
{X a, c}

23

fiLang & Toussaint

Rule 1. example, calculate probability rule (2, a/c) (grabbing c)
unique covering rule grab(a) = 1
P ((2,a/c) ,(2,a/b) , (2,a/t) | a0 )
P ((2,a/c) | a0 ) (1. P ((2,a/b) | a0 )) (1. P ((2,a/t) | a0 ))
= 0.5 (1. 0.2) (1. 0.3) = 0.28 .
calculations, determine sampling distribution = 1. Assume
sample action puton(a). results rule (3/a, b) (putting b a) used
prediction 0.8 probability since probability unique covering rule
action puton(a). remaining mass 0.2 posterior assigned parts
state space unique covering rule available puton(a). case, use
default rule R = 0 (corresponding performing action) probability
0.2 values state variables persist.
Finally, let us infer marginals = 2 using Eq. (25). example, calculate
(inhand(b)t=2 ). Let i(b) brief inhand(b). sum ground rules rt=1 taking
potential values i(b)t=1 i(b)t=1 previous time-step = 1 account,
X
(i(b)t=2 )
P (rt=1 | a0:1 ) ( P (i(b)t=2 | rt=1 , i(b)t=1 ) (i(b)t=1 )
rt=1

+ P (i(b)t=2 | rt=1 , i(b)t=1 ) (i(b)t=1 ) )
= 0.8 (0.0 0.2 + 0.0 0.8) + 0.2 (0.0 0.2 + 1.0 0.8) = 0.16 .
discussed above, ground rule (3/a, b) default rule play role
prediction. effect, belief b inhand decreases 0.8 0.16 tried
put b a, expected. Similarly, calculate posterior on(b, a) 0.8.
expected probability reach goal performing actions grab(b)
puton(a). (Here, PRADAs inferred value coincides true posterior.)
comparison, probability reach goal 1.0 performing actions
grab(a), puton(t), grab(b) puton(a), i.e., clear b grab it. plan
safer, i.e., higher probability, takes actions.
5.5 Comparison Planning Approaches
prominent difference presented planning approaches way
account stochasticity action effects. one hand, SST UCT repeatedly take samples successor state distributions estimate value action
building look-ahead trees. hand, PRADA maintains beliefs states
propagates indetermistic action effects forward. precisely, PRADA SST follow
opposite approaches: PRADA samples actions calculates state transitions approximately means probabilistic inference, SST considers actions (and thus exact
action search) samples state transitions. price considering actions
SSTs overwhelmingly large computational cost. UCT remedies issue samples action sequences thus state transitions selectively: uses previously sampled episodes
build upper confidence bounds estimates action values specific states,
used adapt policy next episode. straightforward translate
24

fiPlanning Noisy Probabilistic Relational Rules

adaptive policy PRADA since PRADA works beliefs states instead states
directly. Therefore, chose simple policy PRADA sample randomly
actions unique covering rule state (in form sampling distribution
account beliefs states).
PRADA returns whole plan transform world state one goal
fulfilled probability exceeding given threshold , spirit conformant planning probabilistic planning observability (Kushmerick, Hanks, & Weld, 1995).
Due outcome-sampling, SST UCT cannot return plan straightforward way. Instead, provide policy many successor states based estimates
action-values look-ahead tree. estimates states deeper tree
less reliable built less episodes. action executed
new state observed, estimates reused. Thus far, PRADA take
knowledge gained previous action-sequence samples account adapt policy.
elegant way achieve better exploit goal knowledge might use backpropagation
DBNs plan completely inference (Toussaint & Storkey, 2006).
beyond scope paper, clear principled way
large state action spaces relational domains. Alternatively, PRADA could give high
weight second action previous best plan. Sec. 5.6, show another
simple way make use previous episodes find better plans.
PRADA afford simple action-sampling strategy evaluates large numbers
action-sequences efficiently grow look-ahead trees account
indeterministic effects. points important difference: three algorithms faced
search spaces action sequences exponential horizon. calculate
value given action sequence, however, SST UCT still need exponential time due
outcome sampling. contrast, PRADA propagates state transitions forward
thus linear horizon.
approximate planning algorithms, neither SST, UCT PRADA expected perform ideally situations. SST UCT sample action outcomes hence
face problems important outcomes small probability. instance, consider
agent wants escape room two locked doors. hits first door
made wood chance 0.05 break escape. second door made
iron chance 0.001 break. SST UCT may take long time
detect 50 times better repeatedly hit wooden door. contrast, PRADA
recognizes immediately reasoned actions takes
outcomes account. hand, PRADAs approximate inference procedure correlations among state variables get lost SST UCT preserve
sample complete successor states. impair PRADAs planning performance
situations correlations crucial. Consider following simple domain two
state attributes b. agent choose two actions modeled rules

action1 :
action2 :






0.5 : a, b
,
0.5 : a, b



0.5 : a, b
0.5 : b,




25

.

fiLang & Toussaint

goal make attributes either true false, i.e., = (a b) (a b).
actions, resulting marginals (a) = 0.5, (a) = 0.5, (b) = 0.5
(b) = 0.5. Due factored frontier, PRADA cannot distinguish actions
although action1 achieve goal, action2 not.
PRADAs estimated probabilities states rewards may differ significantly
true values. harm performance many domains experiments
indicate (Sec. 6). suppose reason PRADAs estimated probabilities imprecise, enable correct ranking action sequences planning,
interested choosing best action instead calculating correctly value.
difference proposed algorithms way handle noise
outcome rules: PRADA assigns small probability successor states spirit
noise outcome. contrast, SST UCT make sense sample
distribution, single successor state extremely low probability
inadequate estimate state action values. Hence, use described workaround
assume stay state, discounting obtained rewards.
straightforward PRADA deal uncertain initial states. Uncertainty
initial states common complex environments may instance caused partial
observability noisy sensors. uncertainty natural representation belief
state PRADA works on. contrast, SST UCT cannot account uncertain initial
states directly, would sample prior distribution.
5.6 Extension: Adaptive PRADA
present simple extension PRADA increase planning accuracy.
exploit fact PRADA evaluates complete sequences actions contrast SST
UCT actions taken > 0 depend sampled outcomes. Adaptive
PRADA (A-PRADA) examines best action sequence found PRADA. PRADA
chooses first action sequence without reasoning, A-PRADA inspects
single action sequence decides simulation whether deleted.
resulting shortened sequence may lead increased expected reward. case
actions significant effects achieving goal decrease success
probability. actions omitted, states high reward reached earlier
rewards discounted less. instance, consider goal grab blue ball:
action sequence grabs red cube, puts onto table grabs blue
ball improved omitting first two actions unrelated goal.
precisely, A-PRADA takes PRADAs action sequence aP highest value
investigates iteratively action whether deleted. action
deleted plan resulting plan higher reward likelihood. idea
formalized Algorithm 1. crucial calculation algorithm compute values
Q(s0 , a0:T 1 ) defined Eq. (28) restated convenience:
0

Q(s ,

0:T 1

)=


X

P (U = 1 | a0:t1 , s0 ) .

t=1

PRADAs approximate inference procedure particularly suitable calculating required P (U = 1 | a0:t1 , s0 ). performs calculation time linear length
26

fiPlanning Noisy Probabilistic Relational Rules

Algorithm 1 Adaptive PRADA (A-PRADA)
Input: PRADAs plan aP
Output: A-PRADAs plan aA
1: aA aP
2: = 0 = 1
3:
true
4:
Let plan length .
5:
a0:t1 a0:t1
B Omit

t+1:T 1
t:T 2
6:

aA
7:
1 othing
8:
Q(s0 , a) > Q(s0 , aA )
9:
aA
10:
else
11:
break
12:
end
13:
end
14: end
15: return aA

action sequence, SST UCT would require time exponential
outcome sampling.

6. Evaluation
implemented presented planning algorithms learning algorithm
NID rules C++. code available www.user.tu-berlin.de/lang/prada/.
evaluate approaches two different scenarios. first intrinsically noisy complex simulated environment learn NID rules experience use
plan. Second, apply algorithms benchmarks Uncertainty Part
International Planning Competition 2008.
6.1 Simulated Robot Manipulation Environment
perform experiments simulated complex robot manipulation environment
robot manipulates objects scattered table (Fig. 4). report results three
series experiments different tasks increasing difficulty, first describe domain
detail. use 3D rigid-body dynamics simulator (ODE) enables realistic behavior objects. simulator available www.user.tu-berlin.de/lang/DWSim/.
Objects cubes balls different sizes colors. robot grab objects
put top objects table. actions robot affected
noise. domain, towers objects straight-lined; easier put object
top big cube top small cube difficult put something
top ball; piles objects may topple over; objects may fall table case
become reach robot.
represent domain predicates on(X, ), inhand(X), upright(X), out(X) (if
object fallen table), function size(X) unary typing predicates cube(X),
ball(X), table(X). predicates obtained querying state simulator
27

fiLang & Toussaint

Figure 4: simulated robot plays cubes balls different sizes scattered
table. Objects fallen table cannot manipulated anymore.

translating according simple hand-made guidelines, thereby sidestepping difficult
problem converting agents observations internal representation. instance,
on(a, b) holds b exert friction forces z-coordinate greater
one b, x- y-coordinates similar. Besides primitive
concepts, use derived predicate clear(X) Y.on(Y, X). found
predicate enable compact accurate rules, reflected values
objective function rule learning algorithm given Eq. (3).
define three different types actions. actions correspond motor primitives
whose effects want learn exploit. grab(X) action triggers robot open
hand, move hand next X, let grab X raise robot arm again.
execution action influenced factors. example, different
object held hand before, fall either table third
object ; objects top X, likely fall down.
puton(X) action centers robots hand certain distance X, opens
raises hand again. instance, object Z X, object
potentially inhand may end Z Z might fall X. othing() action triggers
movement robots arm. robot might choose action thinks
action could harmful respect expected reward. emphasize
actions always execute, regardless state world. Also, actions
rather unintuitive humans trying grab table put object top
carried out. robot learn effects motor primitives.
Due intrinsic noise complexity, simulated robot manipulation scenario
challenging domain learning compact world models well planning.
objects f different object sizes, action space contains 2o+1 actions
2
state space huge f 2o +6o different states (not excluding states one would classify
impossible given intuition real world physics).
use rule learning algorithm Pasula et al. (2007) parameter
settings learn three different sets fully abstract NID rules. rule-set learned
28

fiPlanning Noisy Probabilistic Relational Rules

independent training sets 500 experience triples (s, a, s0 ) specify world
changed state successor state s0 action executed, assuming full
observability. Training data learn rules generated world six cubes four
balls two different sizes performing random actions slight bias build high
piles. resulting rule-sets contain 9, 10 10 rules respectively. rule-sets provide
approximate partial models true world dynamics. generalize situations
experiences, may account situations completely different
agent seen before. enforce compactness avoid overfitting, rules
regularized; hence, learning algorithm may sometimes favor model rarely experienced
state transitions low-probability outcomes general rules, thereby trading
accuracy compactness. combination general noisiness world
causes need carefully account probabilities world reasoning
rules.
perform three series experiments planning tasks increasing difficulty.
series, test planners different worlds varying numbers cubes
balls. Thus, transfer knowledge gained training world different, similar
worlds using abstract NID rules. object number, create five different worlds.
Per rule-set world, perform three independent runs different random seeds.
evaluate different planning approaches, compute mean performances
planning times fixed (but randomly generated) set 45 trials (3 learned rule-sets,
5 worlds, 3 random seeds).
choose parameters planning algorithms follows. SST, report results different branching factors b, far resulting runtimes allow. Similarly, UCT
(A-)PRADA parameter balances planning time quality
found actions. UCT, number episodes, (A-)PRADA
number sampled action-sequences. Depending experiment, set
heuristically tradeoff planning time quality reasonable.
particular, fair comparison pay attention UCT, PRADA A-PRADA get
planning times, reported otherwise. Furthermore, UCT set
bias parameter c 1.0 found heuristically perform best. planners
experiments, set discounting factor future rewards = 0.95. crucial
parameter planning horizon d, heavily influences planning time. course,
cannot known a-priori. Therefore, reported otherwise, deliberately set larger
required UCT (A-)PRADA suggest algorithms effective
estimated. Indeed, found experiments long
small, exact choice significant effects UCTs (A-)PRADAs
planning quality unlike effects planning times. contrast, set horizon
SST always small possible, case planning times still large.
planning algorithm find suitable action given situation, restart
planning procedure: SST builds new tree, UCT runs episodes (A-)PRADA
takes new action-sequence samples. given situation 10 planning runs suitable
action still found, trial fails.
Furthermore, use FF-Replan (Yoon et al., 2007) baseline. discuss
detail related work Sec. 2, FF-Replan determinizes planning problem,
thereby ignoring outcome probabilities. FF-Replan shown impressive results
29

fiLang & Toussaint

domains probabilistic planning competitions. domains carefully designed
humans: action dynamics definitions complete, accurate consistent
used true world dynamics according experiments contrast learned
NID rules use estimate approximate partial models robot manipulation
domain. able use derived predicate clear(X) FF-Replan implementation
experiments, included appropriate literals predicate hand
outcomes rules SST, UCT (A-)PRADA implementations infer
values automatically definition clear(X). report results FF-Replan
(almost original) learned rules using all-outcomes determinization scheme, denoted
FF-Replan-All below. (Using single-outcome schemes always led worse performance.)
rules general (putting restrictions arguments
deictic references); case, actions appear applicable given state make
sense intuitive human perspective hurts FF-Replan much
methods, resulting large planning times FF-Replan. instance, rule may model
toppling small tower including object X trying put object top
tower: one outcome might specify end X. possible
cube, course, learning algorithm may choose omit typing predicate
cube(X) due regularization, prefers compact rules none experiences might
require additional predicate. Therefore, created modified rule-sets hand
introduced typing predicates appropriate make contexts distinct. Below,
denote results modified rule-sets FF-Replan-All* FF-Replan-Single*,
using all-outcomes single most-probable outcome determinization schemes.
6.1.1 High Towers
first series experiments, investigate building high towers planning
task work Pasula et al. (2007). precisely, reward state defined
average height objects. constitutes easy planning problem many different
actions may increase reward (object identities matter) small planning
horizon sufficient. set SST horizon = 4 (Pasula et al. choice) different
branching factors b UCT (A-)PRADA horizon = 6. experiments, initial
states contain already stacked objects, reward performing actions
0. Table 3 Fig. 5 present results. SST competitive. branching factor
b > 1, slower UCT (A-)PRADA least order magnitude.
b = 1, performance poor. series experiments, designed worlds 10
objects contain many big cubes. explains relatively good performance SST
worlds, number good plans large. mentioned above, control UCT,
PRADA A-PRADA times available planning. three
approaches perform far better SST almost experiments. difference
UCT, PRADA A-PRADA never significant.
series experiments indicates planning approaches using full-grown lookahead trees SST inappropriate even easy planning problems. contrast, approaches exploit look-ahead trees clever way UCT seem best
choice easy tasks require small planning horizon solved many
alternative good plans. performance planning approaches using approximate
30

fiPlanning Noisy Probabilistic Relational Rules

Table 3: High towers problem. Reward denotes discounted total reward different
numbers objects (cubes/balls table). reward performing actions
0. data points averages 45 trials created 3 learned rule-sets,
5 worlds 3 random seeds. Standard deviations mean estimators
shown. FF-Replan-All* FF-Replan-Single* use hand-made modifications
original learned rule-sets. Fig. 5 visualizes results.
Objects

Planner
FF-Replan-All
FF-Replan-All*
FF-Replan-Single*

6+1

SST (b=1)
SST (b=2)
SST (b=3)
UCT
PRADA
A-PRADA

SST (b=1)
SST (b=2)
SST (b=3)
UCT
PRADA
A-PRADA

SST (b=1)
SST (b=2)
SST (b=3)
UCT
PRADA
A-PRADA

6.65 1.01
6.29 0.80
4.48 0.94

41.07 9.63
7.54 4.09
4.61 2.75

1.19
1.01
0.94
0.99
1.25
1.27

9.03 0.80
121.40 11.12
595.43 55.95
7.45 0.19
6.01 0.07
6.36 0.07

5.10 1.01
3.08 0.87
2.82 0.87

76.86 20.98
28.65 16.81
1.72 0.27








1.07
1.21
0.87
1.07
1.21
1.47

23.57 3.48
335.5 52.4
1613.3 249.2
15.54 0.40
15.24 0.27
16.30 0.27

6.97 1.21
7.36 1.07
5.76 1.21

121.99 27.43
33.45 12.80
4.14 1.08








119.26 10.59
1748.7 170.2
8424 851
31.71 5.83
31.58 1.14
35.22 0.40

9.62
12.36
11.09
17.11
16.10
16.29

FF-Replan-All
FF-Replan-All*
FF-Replan-Single*
10+1

Trial time (s)

11.68
12.90
12.80
16.01
15.54
16.12

FF-Replan-All
FF-Replan-All*
FF-Replan-Single*
8+1

Reward

15.12
14.48
16.48
17.71
16.21
16.78

31








1.34
1.20
1.19
1.08
1.07
1.14

fiLang & Toussaint

FF-Replan-All
FF-Replan-All*
FF-Replan-Single*
SST b=1
SST b=2
SST b=3
UCT
PRADA
A-PRADA

15
10
5

1000

Trial time (s)

Discounted total reward

10000

100
10
1

6

8
Objects

10

6

(a) Reward

8
Objects

10

(b) Time

Figure 5: High towers problem Visualization results presented Table 3. reward
performing actions 0. data points averages 45 trials created
3 learned rule-sets, 5 worlds 3 random seeds. Error bars standard
deviations mean estimators shown. Please note log-scale (b).

inference, PRADA A-PRADA, however, comes close one UCT, showing
suitability scenarios.
FF-Replan focuses exploiting conjunctive goal structures cannot deal quantified goals. grounded reward structure task consists disjunction
different tower combinations, FF-Replan pick arbitrary tower combination
goal. Therefore, apply FF-Replan sample tower combinations according rewards achieve (i.e., situations high towers probable) exclude
combinations balls bottom towers prohibited reward
structure. Yoon et al. note, obvious pitfall [goal formula sampling] approach
groundings goal reachable much expensive reach
initial state. FF-Replan cannot find plan, execute action,
sample new ground goal formula next time-step, preserving already achieved
tower structures.
FF-Replan performs significantly worse previous planning approaches.
major reason FF-Replan often comes plans exploiting low-probability
outcomes rules contrast SST, UCT (A-)PRADA reason
probabilities. illustrate this, consider example rule Fig. 1 models putting
ball top cube. two explicit outcomes: ball usually ends
cube; sometimes, however, falls table. FF-Replan misuse rule tricky
way put ball table ignoring often fail. results FFReplan-Single* show, taking probable outcomes account remedy
problem: often two three outcomes similar probabilities
choice seems unjustified; sometimes, intuitively expected outcome split
different outcomes low probabilities, however vary features irrelevant
planning problem (such upright()).
32

fiPlanning Noisy Probabilistic Relational Rules

Table 4: Desktop clearance problem. Reward denotes discounted total reward different numbers objects (cubes/balls table). reward performing
actions 0. data points averages 45 trials created 3 learned rulesets, 5 worlds 3 random seeds. Standard deviations mean estimators
shown. FF-Replan-All* FF-Replan-Single* use hand-made modifications
original learned rule-sets. Fig. 6 visualizes results.
Obj.

Planner
FF-Replan-All
FF-Replan-All*
FF-Replan-Single*

6+1

SST (b=1)
UCT
PRADA
A-PRADA

SST (b=1)
UCT
PRADA
A-PRADA

SST (b=1)
UCT
PRADA
A-PRADA

3.81 0.67
5.86 0.87
6.53 1.07

19.1 6.5
1.1 0.7
0.7 0.8

0.75
0.86
0.86
0.80

1382.6 80.4
52.2 0.7
40.9 0.7
42.3 0.7

5.93 1.00
6.21 1.05
6.02 0.94

29.8 8.7
3.5 0.6
0.8 0.7






2.01
1.08
1.54
1.57

8157 978
151.4 2.0
154.5 1.9
157.4 2.0

3.30 0.74
3.53 0.87
3.91 0.86

60.9 12.1
20.7 5.4
5.2 1.3


10.13 0.80
12.81 1.14
13.91 1.12

> 8h
415.7 7.4
385.3 4.7
394.5 4.0

8.43
10.29
14.63
14.87

FF-Replan-All
FF-Replan-All*
FF-Replan-Single*
10+1

Trial time (s)

5.35
9.60
10.94
12.79

FF-Replan-All
FF-Replan-All*
FF-Replan-Single*
8+1

Reward






6.1.2 Desktop Clearance
task second series experiments clear desktop. Objects lying
splattered table beginning. object cleared part tower
containing objects class. object class simply defined terms
color additionally provided state representation robot. reward
robot defined number cleared objects. experiments, classes contain
2-4 objects 1 ball (in order enable successful piling). starting situations contain piles, objects different classes. Thus, reward
performing actions 0. Desktop clearance difficult building high towers,
number good plans yielding high rewards significantly reduced.
set planning horizon = 6 optimal SST required clear
class 4 objects, namely grabing putting three objects. above, contrast set
= 10 UCT (A-)PRADA show deal overestimation
usually unknown optimal horizon d. Table 4 Fig. 6 present results. horizon
= 6 overburdens SST seen large planning times. Even b = 1, SST
takes almost 40 minutes average worlds 6 objects, 2 hours worlds
8 objects. Therefore, try SST greater b. contrast, planning times
33

fi16

10000

14

FF-Replan-All
FF-Replan-All*
FF-Replan-Single*
SST b=1
UCT
PRADA
A-PRADA

12
10
8
6
4
2
6

8
Objects

1000
Trial time (s)

Discounted total reward

Lang & Toussaint

100
10
1
6

10

(a) Reward

8
Objects

10

(b) Time

Figure 6: Desktop clearance problem. Visualization results presented Table 4.
reward performing actions 0. data points averages 45 trials
created 3 learned rule-sets, 5 worlds 3 random seeds. Error bars
standard deviations mean estimators shown. Note log-scale (b).

UCT, PRADA A-PRADA, controlled enable
reasonable performance, two orders magnitude smaller, although overestimating
planning horizon: trial take average 45s worlds 6 objects, 2 12
minutes worlds 8 objects 6-7 minutes worlds 10 objects. Nonetheless, UCT,
PRADA A-PRADA perform significantly better SST. worlds, PRADA
A-PRADA turn outperform UCT, particular worlds many objects. A-PRADA
finds best plans among planners. planners gain reward worlds 8 objects
comparison worlds 6 objects, number objects cleared increases
well number classes thus good plans. worlds 10 objects contain
numbers object classes worlds 8 objects, objects,
making planning difficult.
Overall, findings Desktop clearance experiments indicate SST
inappropriate, UCT achieves good performance planning scenarios require medium
planning horizons several, many alternative plans. Approaches
using approximate inference PRADA A-PRADA, however, seem appropriate scenarios intermediate difficulty.
Furthermore, results indicate FF-Replan inadequate clearance task.
sample target classes randomly provide goal structure FF-Replan; tower
structure within target class turn randomly chosen. bad performance
FF-Replan due reasons described previous experiments; particular
plans FF-Replan often rely low-probability outcomes.
34

fiPlanning Noisy Probabilistic Relational Rules

Table 5: Reverse tower problem. trial times numbers executed actions given
successful trials different numbers objects (cubes table).
data points averages 45 trials created 3 learned rule-sets, 5 worlds
3 random seeds. Standard deviations mean estimators shown. FFReplan-All* FF-Replan-Single* use hand-made modifications original
learned rule-sets.
Objects

5+1

6+1

7+1

Planner

Success rate

Trial time (s)

Executed actions

FF-Replan-All
FF-Replan-All*
FF-Replan-Single*

0.02
1.00
0.67

7.1 0.0
26.7 2.7
7.0 0.9

12.0 0.10
13.1 0.9
13.6 1.1

SST (b=1)
SST (b=2)
UCT
PRADA
A-PRADA

0.00
0.00
0.38
0.71
0.82

> 1 day
2504.9 491.1
27.0 1.8
25.4 0.8

19.5 4.0
13.2 0.7
10.9 0.8

FF-Replan-All
FF-Replan-All*
FF-Replan-Single*

0.00
1.00
0.64

589.2 73.7
52.7 5.3

12.0 0.8
17.3 2.1

UCT
PRADA
A-PRADA

0.00
0.47
0.56

>4 h
66.4 3.9
77.5 8.3

13.6 0.9
14.4 2.5

FF-Replan-All
FF-Replan-All*
FF-Replan-Single*

0.00
0.42
0.56

2234.2 81.1
687.4 86.4

15.1 1.3
17.5 2.0

PRADA
A-PRADA

0.24
0.23

871.3 126.6
783.7 132.6

18.2 1.2
15.1 1.8

6.1.3 Reverse Tower
explore limits UCT, PRADA A-PRADA, conducted final series
experiments task reverse towers C cubes requires least 2C
actions (each cube needs grabbed put somewhere least once). Apart
long planning horizon, difficult due noise simulated world: towers
become unstable topple cubes falling table. decrease noise
slightly obtain reliable results, forbid robot grab objects clear
(i.e., objects). set limit 50 executed actions trial. thereafter
reversed tower still built, trial fails. trial fails one required
objects falls table.
Table 5 presents results. cannot get SST optimal planning horizon = 10
solve problem even five cubes. Although space possible actions reduced
due mentioned restriction, SST enormous runtimes. b = 1, SST find
suitable actions (no leaves goal state) several starting situations increased
planning horizon leads high probability sampling least one unfavorable outcome
required action. b 2, single tree traversal SST takes day.
found UCT require large planning times order achieve reasonable success
rate. Therefore, set planning horizons optimal UCT. worlds 5 cubes, UCT
optimal = 10 success rate 40% taking average 40
35

fiLang & Toussaint

minutes case success. 6 cubes, however, UCT optimal = 12 never succeeds
even planning times exceed 4 hours. contrast, afford overestimating
horizon = 20 PRADA A-PRADA. worlds 5 cubes, PRADA A-PRADA
achieve success rates 71% 82% respectively less half minute. A-PRADAs
average number executed actions case success almost optimal. worlds 6
cubes, success rates PRADA A-PRADA still 50%, taking bit
minute average case success. trials fail, often due
cubes falling table cannot find appropriate actions. Cubes
falling table main reason success rates PRADA A-PRADA
drop 23% 24% respectively worlds 7 cubes towers become rather unstable.
Planning times successful trials, however, increase 13 minutes indicating
limitations planning approaches. Nonetheless, mean number executed
actions successful trials still almost optimal A-PRADA.
Overall, Reverse tower experiments indicate planning approaches using lookahead trees fail tasks require long planning horizons achieved
plans. Given huge action state spaces relational domains, chances
UCT simulates episode exactly required actions successor states
small. Planning approaches using approximate inference PRADA A-PRADA
crucial advantage stochasticity actions affect runtime
exponentially planning horizon. course, search space action-sequences still
exponential planning horizon problems requiring long horizons hard
solve them. experiments show using simple, though principled
extension A-PRADA, gain significant performance improvements.
results show FF-Replan fails provide good plans using original
learned rule-sets. surprising characteristics Reverse tower task seem
favor FF-Replan comparison methods: single conjunctive goal
structure number good plans small plans require long horizons.
results FF-Replan-All* FF-Replan-Single* indicate, FF-Replan achieve
good performance adapted rule-sets modified hand restrict
number possible actions state. constitutes proof concept
FF-Replan, shows difficulty applying FF-Replan learned rule-sets.

6.1.4 Summary
results demonstrate successful planning learned world models (here
form rules) may require explicitly account quantification predictive uncertainty. concretely, methods applying look-ahead trees (UCT) approximate
inference ((A-)PRADA) outperform FF-Replan different tasks varying difficulty. Furthermore, (A-)PRADA solve planning tasks long horizons, UCT fails.
one post-processes learned rules hand clarify application contexts
planning problem uses conjunctive goal structure requires long plans,
FF-Replan performs better UCT (A-)PRADA.
36

fiPlanning Noisy Probabilistic Relational Rules

6.2 IPPC 2008 Benchmarks
second part evaluation, apply proposed approaches benchmarks
latest international probabilistic planning competition, Uncertainty Part
International Planning Competition 2008 (IPPC, 2008). involved domains differ
many characteristics, number actions, required planning horizons
reward structures. competition results show, planning algorithm performs
best everywhere. Thus, benchmarks give idea types problems SST,
UCT (A-)PRADA may useful. convert PPDDL domain specifications
NID rules along lines described Sec. B.1. resulting rule-sets used run
implementations SST, UCT (A-)PRADA benchmark problems.
seven benchmark domains consists 15 problem instances. instance
specifies goal starting state. Instances vary problem size,
reward structures (including action costs), direct comparison always
possible. competition, instance considered independently: planners
given restricted amount time (10 minutes problems 1-5 domain 40
minutes others) cover many repetitions problem instance
possible maximum 100 trials. Trials differed random seeds resulting
potentially different state transitions. planners evaluated respect
number trials ending goal state collected reward averaged trials.
Eight planners entered competition, including FF-Replan official participant. discussed related work Sec. 2. results,
voluminous presented here, refer reader website competition. Below, provide qualitative comparison methods results
planners. attempt direct quantitative comparison several reasons. First,
different hardware prevents timing comparisons. Second, competition participants
frequently able successfully cover trials single instances domain.
difficult tell reasons results tables: planner might
overburdened problem, might faced temporary technical problems
client-server architecture framework competition could cope certain
PPDDL constructs could rewritten simpler format.
Third importantly, optimized implementations reuse previous planning efforts. Instead, fully replan single action (within trial
across trials). competition evaluation scheme puts replanners disadvantage (in
particular replan single action). Instead replanning, good strategy
competition spend planning time starting first trial reuse
resulting insights (such conditional plans value functions) subsequent trials
minimum additional planning. Indeed, strategy often adopted
many trial time results indicate. acknowledge fair procedure evaluate
planners compute policies large parts state-space acting. feel,
however, counter idea approaches: UCT (A-)PRADA
meant flexible planning varying goals different situations. Thus,
interested average time compute good actions successfully solve problem
instance prior knowledge available.
37

fiLang & Toussaint

Table 6: Benchmarks IPPC 2008. first column table specifies problem
instance. Suc. success rate. trial time number executed
actions given successful trials. applicable, reward
trials shown. results achieved full replanning within trial
across trials.
(a) Search Rescue
Planner

Suc. Trial Time (s)

Actions

(c) Blocksworld

Reward

SST
UCT
01
PRADA
A-PRADA

100
54
100
100

37.90.1
1.40.1
1.10.1
1.10.1

SST
UCT
PRADA
A-PRADA

100
56
100
100

220.20.1
4.10.3
1.60.1
1.60.1

9.80.2
12.20.6
12.90.7
12.80.4

SST
UCT
PRADA
A-PRADA

71
57
99
99

955.50.5
12.90.6
1.40.1
1.40.1

9.80.2 166285
13.60.6 68063
18.01.0 148088
17.91.1 148088

Actions

Reward

UCT
04 PRADA
A-PRADA

61
100
100

24.91.6
1.40.0
1.40.0

16.10.8 720057
11.90.4 146089
11.50.3 150087

SST
UCT
01
PRADA
A-PRADA

0
0
100
100





257.86.3 46.81.0
143.83.1 43.11.1



1.000.0
1.000.0

05

UCT
PRADA
A-PRADA

46
89
92

40.12.1
6.80.3
6.50.3

16.81.4 60064
21.80.9 124083
21.00.9 132081

02

PRADA
A-PRADA

100
100

285.27.8 46.21.3
215.84.2 39.60.9

20.000.0
20.000.0

06

UCT
PRADA
A-PRADA

39
83
84

71.75.6
10.10.9
10.00.9

19.51.3 41059
24.31.3 124090
23.71.2 124090

03

UCT
PRADA
A-PRADA

100
100
50

07

UCT
PRADA
A-PRADA

53
98
98

230.313.2
10.10.4
9.90.4

21.51.4 54062
18.50.8 147088
18.00.8 149087

04

PRADA
A-PRADA

28
60

959.035.5 76.13.2
519.215.3 72.02.4

0.30.5
0.60.1

UCT
PRADA
A-PRADA

34
59
59

332.924.1 21.711.5
20.20.8 30.41.7
19.90.8 29.91.7

05

08

UCT
PRADA
A-PRADA

54
61
2

9972776 37.93.5
345.48.5 68.41.6
528.638.8 38.00.0

606149
46524
41134

UCT
09 PRADA
A-PRADA

30
63
65

752.872.3
30.21.2
30.01.1

08

PRADA
A-PRADA

3
10

336188 87.02.3
157948 85.32.7

0.190.1
0.290.3

09

PRADA
A-PRADA

28
0

144925 85.91.5
(1750.3)


136531
112630

10

PRADA
A-PRADA

21
21

97.910.2
92.19.8

26.82.8
26.72.8

18027
18027

11

PRADA
A-PRADA

17
18

151.712.3
154.111.9

302.5
30.22.6

25029
25029

12

PRADA
A-PRADA

38
21

210.872.1 30.110.5 636253
219.828.5 30.72.8 55655

02

03

9.20.2 144090
11.40.3 90070
10.50.4 146089
10.40.4 146089

Planner

156083
880100
146089
144090

36059
91082
91082

0
100
100
100


9.90.3
8.50.2
8.00.2

02

UCT
PRADA
A-PRADA

100
57
65

64.12.2 12.40.3
30.10.7
90.2
33.70.8 11.40.3

03

UCT
PRADA
A-PRADA

89
19
21

390.58.5 18.60.4
119.24.9 12.30.5
121.05.3 14.30.7

UCT
04 PRADA
A-PRADA

82
6
4

149719 26.00.5
2967143 17.51.1
244.243.6 15.52.8



0.80.0
0.60.0

03

10

57.03.3 21.51.8 -9.60.0

Suc. Trial Time (s)

1285.28.1 32.80.0 929.82.1
165.72.9 52.51.1 865.13.3
457.87.1 35.00.7 754.121.5

(e) Exploding Blocksworld
Planner

Actions

SST
UCT
PRADA
A-PRADA

01





17.80.4 23.00.7
18.40.5 22.30.8

Planner

26.42.4 36048
27.51.6 93080
27.51.6 101084

Suc. Trial Time (s)

Reward

0
0
53
63

PRADA

Suc. Trial Time (s)

(d) Boxworld

(b) Triangle-Tireworld
Planner

Actions

SST
UCT
01
PRADA
A-PRADA


6.90.2
6.40.2
6.10.2

38

Suc. Trial Time (s)
86071224
111.814.0
3.60.0
3.90.0

Actions

SST
UCT
01
PRADA
A-PRADA

5
3
62
61

02

PRADA
A-PRADA

28
29

11.90.3 14.40.5
12.70.2 13.20.5

03

PRADA
A-PRADA

36
30

14.30.3 12.60.6
16.80.3 12.50.5

04

PRADA
A-PRADA

27
26

30.31.2 14.80.5
14.91.1 15.20.5

05

PRADA
A-PRADA

100
100

06

PRADA
A-PRADA

51
61

128.52.9 16.90.7
97.55.3 17.30.8

07

PRADA
A-PRADA

14
72

125.06.9 15.30.4
154.85.5 17.61.0

5.50.1
5.50.1

9.60.6
9.30.4
8.60.8
8.40.8

6.60.1
6.60.1

fiPlanning Noisy Probabilistic Relational Rules

Therefore, single problem instance perform 100 trials different random
seeds using full replanning. trial aborted goal state reached within
maximum number actions varying slightly benchmark (about 50 actions).
present success rates mean estimators trial times, executed actions
rewards standard deviations Table 6 problem instances least
one trial successfully covered reasonable time.
Search Rescue (Table 6(a)) domain SST (with branching factor
1) able find plans within reasonable time significantly larger runtimes
UCT (A-)PRADA. success rates rewards indicate PRADA APRADA superior UCT scale rather big problem instances. give idea
w.r.t. IPPC evaluation scheme: UCT solves successfully 54 trials first instance
within 10 minutes full replanning, PRADA A-PRADA solve trials
full replanning. fact, despite replanning single action, PRADA A-PRADA
show success rates best planners benchmark except large
problem instances (within competition, participants FSP-RBH FSP-RDH
achieved comparably satisfactory results). conjecture success methods
due fact domain requires account carefully outcome probabilities,
involve long planning horizons.
Triangle-Tireworld (Table 6(b)) domain UCT outperforms PRADA
A-PRADA, although higher computational cost. depth-first-like style
planning UCT seems useful domain. give idea w.r.t. IPPC evaluation
scheme: UCT performs 60 successful trials first instance within 10 minutes,
PRADA A-PRADA achieve 72 74 trials resp. using full replanning; UCT solves
trials difficult instances. required planning horizons increase quickly
problem instances. approaches cannot cope large problem instances,
three competition participants (RFF-BG, RFF-PG, HMDPP) could cover.
methods face problems required planning horizons large,
number plans non-zero probability small. becomes evident
Blocksworld benchmark (Table 6(c)). domain different robot manipulation environment first evaluation Sec. 6.1. latter considerably
stochastic provides actions given situation (e.g., may grab objects within
pile). Blocksworld domain approaches inferior FF-Replan.
give idea w.r.t. IPPC evaluation scheme: UCT perform single successful
trial first instance within 10 minutes, PRADA A-PRADA achieve 16
17 trials resp. using full replanning.
Boxworld domain (Table 6(d)), approaches exploit fact
delivery boxes (almost) independent delivery boxes (in problem
instances helped intermediate rewards delivered boxes). contrast
UCT, PRADA A-PRADA scale relatively large problem instances. PRADA
A-PRADA solve 100 trials first problem instance, requiring average 4.3
min 2.4 min resp. full replanning. two competition participants solved
trials successfully domain (RFF-BG RFF-PG). give idea w.r.t. IPPC
evaluation scheme: UCT perform single successful trial within 10 minutes,
PRADA completes 2 A-PRADA 4 trials. small number explained
large plan lengths single action computed full replanning.
39

fiLang & Toussaint

Finally, Exploding Blocksworld domain (Table 6(e)) PRADA A-PRADA
perform better good competition participants. give idea w.r.t. IPPC
evaluation scheme: UCT achieves single successful trial within 10 minutes,
PRADA A-PRADA complete 56 61 trials resp..
perform experiments either SysAdmin Schedule domain. PPDDL specifications cannot converted NID rules due involved
universal effects. contrast, possible Boxworld domain despite
universal effects there: Boxworld problem instances, universally quantified
variables always refer exactly one object exploit conversion NID rules.
(Note understood trick implement deictic references PPDDL
means universal effects. according action operator, however, odd semantics:
boxes could end two different cities time.) Furthermore, ignored
Rectangle-Tireworld domain, together Triangle-Tireworld domain makes
2-Tireworlds benchmark, problem instances faulty goal descriptions:
include not(dead) (this critical name winner competition
personally communicated Olivier Buffet).
6.2.1 Summary
majority PPDDL descriptions IPPC benchmarks converted
NID rules, indicating broad spectrum planning problems covered
NID rules. results demonstrate approaches perform comparably better
state-of-the-art planners many traditional hand-crafted planning problems.
hints generality methods probabilistic planning beyond type robotic
manipulation domains considered Sec. 6.1. methods perform particularly well
domains outcome probabilities need carefully accounted for. face problems
required planning horizons large, number plans non-zero
probability small; avoided intermediate rewards.

7. Discussion
presented two approaches planning probabilistic relational rules grounded
domains. methods designed work learned rules provide approximate
partial models noisy worlds. first approach adaptation UCT algorithm
samples look-ahead trees cope action stochasticity. second approach,
called PRADA, models uncertainty states explicitly terms beliefs employs
approximate inference graphical models planning. combine planning
algorithms existing rule learning algorithm, intelligent agent (i) learn
compact model dynamics complex noisy environment (ii) quickly derive appropriate actions varying goals. Results complex simulated robotics domain show
methods outperform state-of-the-art planner FF-Replan number different planning tasks. contrast FF-Replan, methods reason probabilities
action outcomes. necessary world dynamics noisy partial
approximate world models available.
However, planners perform remarkably well many traditional probabilistic
planning problems. demonstrated results IPPC benchmarks,
40

fiPlanning Noisy Probabilistic Relational Rules

shown PPDDL descriptions converted large extent kind rules
planners use. hints general-purpose character particularly PRADA
potential benefits techniques probabilistic planning. instance, methods
expected perform similarly well large propositional MDPs exhibit
relational structure.
far, planning approaches deal reasonable time problems containing
10-15 objects (implying billions world states) requiring planning horizons
15-20 time-steps. Nonetheless, approaches still limited rely
reasoning grounded representation. many objects need represented
representation language gets rich, approaches need combined
methods reduce state action space complexity (Lang & Toussaint, 2009b).
7.1 Outlook
current form, approximate inference procedure PRADA relies specific
compact DBNs compiled rules. development similar factored frontier filters
arbitrary DBNs, e.g. derived general PPDDL descriptions, promising.
Similarly, adaptation PRADAs factored frontier techniques existing probabilistic
planners worth investigation.
Using probabilistic relational rules backward planning appears appealing.
straightforward learn NID rules regress actions providing reversed triples (s0 , a, s)
rule learning algorithm, stating predecessor state state s0 action
applied before. Backward planning, combined forward planning,
received lot attention classical planning may fruitful planning
look-ahead trees well planning using approximate inference. means propagating backwards DBNs, one may ultimately derive algorithms calculate
posteriors actions, leading true planning inference (instead sampling actions).
important direction improving PRADA algorithm make adapt
action-sequence sampling strategy experience previous samples. introduced simple extension, A-PRADA, achieve this, sophisticated methods
conceivable. Learning rule-sets online exploiting immediately planning method important direction future research order enable acting
real world, want behave effectively right start. Improving
rule framework efficient effective planning another interesting issue.
instance, instead using noisy default rule, one may use mixture models deal
actions several (non-unique) covering rules, general use parallel rules work
different hierarchical levels different aspects underlying system.

Acknowledgments
thank anonymous reviewers careful thorough comments
greatly improved paper. thank Sungwook Yoon providing us implementation
FF-Replan. thank Olivier Buffet answering questions probabilistic
planning competition 2008. work supported German Research Foundation
(DFG), Emmy Noether fellowship 409/1-3.
41

fiLang & Toussaint

Appendix A. Proof Proposition 1
Proposition 1 (Sec. 5.3) set action sequences PRADA samples non-zero
probability super-set ones SST UCT.
Proof: Let a0:T 1 action sequence sampled SST (or UCT). Thus,
exists state sequence s0:T rule sequence r0:T 1 every state st
(t < ), action unique covering rule rt predicts successor state st+1
probability pt > 0. For, pt = 0, st+1 would never sampled SST (or UCT).
show t, 0 < : P (st | a0:t1 , s0 ) > 0. case

Psample (at ) > 0 unique covering rule rt st eventually sampled.
P (s0 ) = 1 > 0 obvious. assume P (st | a0:t1 , s0 ) > 0. execute ,
get P (st+1 | a0:t , s0 ) pt P (st | a0:t1 , s0 ) > 0. posterior P (st+1 | a0:t , s0 ) greater
(first inequality) due persistence previous states non-zero probability
lead st+1 given .
set action sequences PRADA samples larger SST (or UCT)
SST (or UCT) refuses model noise outcomes rules. Assume action state
state unique covering rule. episode
simulated means rule predictions noise outcome, action never
sampled SST (or UCT) (as required states never sampled). contrast, PRADA
models effects noise outcome giving low probability possible
successor states heuristic described above.

Appendix B. Relation NID rules PPDDL
use NID rules (Sec. 3.2) relational model transition dynamics probabilistic actions. Besides allowing negative literals preconditions, NID rules extend
probabilistic STRIPS operators (Kushmerick et al., 1995; Blum & Langford, 1999) two
special constructs, namely deictic references noise outcomes, crucial learning compact rule-sets. alternative language specify probabilistic relational planning
problems used International Probabilistic Planning Competitions (IPPC, 2008)
probabilistic planning domain definition language (PPDDL) (Younes & Littman, 2004).
PPDDL probabilistic extension subset PDDL, derived deterministic
action description language (ADL). ADL, turn, introduced universal conditional
effects negative precondition literals (deterministic) STRIPS representation.
Thus, PPDDL allows usage syntactic constructs beyond expressive
power NID rules; however, many PPDDL descriptions converted NID rules.
taking closer look convert PPDDL NID rule representations
other, clarify meant action formalisms, giving
intuition line thinking using either these. understand abstract
action abstract action predicate, e.g. pickup(X). Intuitively, defines certain type
action. stochastic state transitions according abstract action specified
abstract NID rules well abstract PPDDL action operators (also called schemata).
Typically, several different abstract NID rules model abstract action, specifying
state transitions different contexts. contrast, usually one abstract PPDDL action
42

fiPlanning Noisy Probabilistic Relational Rules

operator used model abstract action: context-dependent effects modeled
means conditional universal effects.
make predictions specific situation concrete action (a grounded action
predicate pickup(greenCube)), strategy within NID rule framework
ground set abstract NID rules examine ground rules cover state-action
pair. exactly one ground rule, chosen prediction.
rule one (the contexts NID rules mutually
exclusive), one chooses noisy default rule, essentially saying one know
happen (other strategies conceivable, pursued here). contrast,
usually exactly one operator per abstract action PPDDL domains,
need concept operator uniqueness distinguish ground actions
operators.
B.1 Converting PPDDL NID rules
following, discuss convert PPDDL features NID rule representation.
may impossible convert PPDDL action operator single NID rule,
one may often translate set rules polynomial increase size
representation. Table 7 provides example converted PPDDL action operator
IPPC domain Exploding Blocksworld. NID rules support many,
features sophisticated domain description language PPDDL provides, using rules
lead compact representations possible domains. experiments, however,
show dynamics many interesting planning domains specified compactly.
Furthermore, additional expressive power rule contexts gained using derived
predicates allow bring various kinds logical formulas quantification.
Conditional Effects conditional effect PPDDL operator takes form C
E. accounted two NID rules: first rule adds C context
E outcomes, second adds C context ignores E.
Universal Effects PPDDL allows define universal effects. specify effects
objects meet preconditions. example reboot action SysAdmin
domain IPPC 2008 competition: specifies every computer one
rebooted independently go probability 0.2 connected computer
already down. cannot expressed NID rule framework.
refer objects action arguments via deictic references, require
deictic references unique. reboot action, would need unique way refer
computer cannot achieved without significant modifications (for
example, enumerating computers via separate predicates).
Disjunctive Preconditions Quantification PPDDL operators allow disjunctive preconditions, including implications. instance, Search-and-rescue domain
IPPC 2008 competition defines action operator goto(X) precondition
(X 6= base) humanAlive(). disjunction B ( B) accounted
either using two NID rules, first rule context second
rule B. Alternatively, one may introduce derived predicate C B.
general, trick derived predicates allows overcome syntactical limitations NID
43

fiLang & Toussaint

Table 7: Example converting PPDDL action operator NID rules. putDownoperator IPPC benchmark domain Exploding Blocksworld (a) contains
conditional effect accounted two NID rules either exclude
(b) include (c) condition context.
( : action putDown

(a)

: parameters (?b block)
: precondition (and (holding ?b) (noDestroyedT able))
: ef f ect (and (emptyhand) (onT able ?b) (not (holding ?b))
(probabilistic 2/5 (when (noDetonated ?b) (and (not (noDestroyedT able)) (not (noDetonated?b))))))
)
(b)
putDown(X) : block(X), holding(X), noDestroyedT able(), noDetonated(X)

1.0 : emptyhand(X), onT able(X), holding(X)

(c)
putDown(X) : block(X), holding(X), noDestroyedT able(), noDetonated(X)

0.6 : emptyhand(X), onT able(X), holding(X)

0.4 : emptyhand(X), onT able(X), holding(X), noDestroyedT able(), noDetonated(X)

rules bring various kinds logical formulas quantifications. discussed
Pasula et al. (2007), derived predicates important prerequisite able learn
compact accurate rules.
Types Terms may typed PPDDL, e.g. driveT o(C city). Typing objects
variables predicates functions achieved NID rules usage typing
predicates within context, e.g. using additional predicate city(C).
State Transition Rewards PPDDL, one encode Markovian rewards associated
state transitions (including action costs negative rewards) using fluents update
rules action effects. One achieve NID rules associating rewards
outcomes rules.
B.2 Converting NID rules PPDDL
show following way NID rules used SST, UCT PRADA
planning time handled via polynomial blowup representational size.
basic building blocks NID rule, i.e. context well outcomes, transfer
one-to-one PPDDL action operators. deictic references, uniqueness requirement
covering rules noise outcome need special attention.
Deictic References Deictic references NID rules allow refer objects
action arguments. PPDDL, one refer objects means universal
conditional effects. important restriction, however: deictic reference needs
pick single unique object order apply. picks none many, rule fails
apply. two ways ensure uniqueness requirement within PPDDL. First,
44

fiPlanning Noisy Probabilistic Relational Rules

allowing quantified preconditions, explicit uniqueness precondition deictic
reference introduced. Using universal quantification, constrains objects
satisfying preconditions identical, i.e., X, : (X, ) (Y, )
X = , variables. Alternatively, uniqueness deictic references
achieved careful planning problem specification, however cannot
guaranteed learning rules.
Uniqueness covering rules contexts NID rules mutually
exclusive. want use rule prediction (as planning), need ensure
uniquely covers given state-action pair. procedural evaluation process NID
rules encoded declaratively PPDDL using modified conditions explicitly
negate contexts competing rules. instance, three NID rules
potentially overlapping contexts A, B, C (propositional simplicity), PPDDL
action operator may define four conditions: c1 = {A B C}, c2 = {A B C},
c3 = {A B C}, c4 = {(A B C) (A B) (A C) (B C)}. Conditions c1 ,
c2 c3 test uniqueness corresponding NID rules subsume outcomes.
Condition c4 tests non-uniqueness (either covering rule multiple covering rules)
models potential changes noise, analogous situations NID rule context
noisy default rule would used.
Noise outcome noise outcome NID rule subsumes seldom utterly complex
outcomes. relaxes frame assumption: even explicitly stated things may change
certain probability. comes price difficulty ensure well-defined
successor state distribution P (s0 | s, a). contrast, PPDDL needs explicitly specify
everything might change. may important reason difficult come
effective learning algorithm PPDDL.
principle PPDDL provide noise outcome, way approaches
account planning encoded PPDDL. either treat noise outcome
effects (in SST UCT; basically noop operator then) trivially
translated PPDDL; consider probability state attribute change
independently (in PRADA) encoded PPDDL independent universal
probabilistic effects.
noise outcome allows always make predictions arbitrary action:
multiple covering rules, may use (albeit informative) prediction
default rule. cases dealt PPDDL action operators using explicit
conditions described previous paragraph.

References
Blum, A., & Langford, J. (1999). Probabilistic planning graphplan framework.
Proc. Fifth European Conference Planning (ECP), pp. 319332.
Botvinick, M. M., & An, J. (2009). Goal-directed decision making prefrontal cortex:
computational framework. Advances Neural Information Processing Systems
(NIPS), pp. 169176.
45

fiLang & Toussaint

Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions computational leverage. Journal Artificial Intelligence Research,
11, 194.
Boutilier, C., Reiter, R., & Price, B. (2001). Symbolic dynamic programming first-order
MDPs. Proc. Int. Conf. Artificial Intelligence (IJCAI), pp. 690700.
Buffet, O., & Aberdeen, D. (2009). factored policy-gradient planner. Artificial Intelligence Journal, 173 (5-6), 722747.
Cooper, G. (1988). method using belief networks influence diagrams. Proc.
Fourth Workshop Uncertainty Artificial Intelligence, pp. 5563.
Croonenborghs, T., Ramon, J., Blockeel, H., & Bruynooghe, M. (2007). Online learning
exploiting relational models reinforcement learning. Proc. Int. Conf.
Artificial Intelligence (IJCAI), pp. 726731.
Domshlak, C., & Hoffmann, J. (2007). Probabilistic planning via heuristic forward search
weighted model counting. Journal Artificial Intelligence Research, 30, 565620.
Driessens, K., Ramon, J., & Gartner, T. (2006). Graph kernels Gaussian processes
relational reinforcement learning. Machine Learning, 64 (1-3), 91119.
Dzeroski, S., de Raedt, L., & Driessens, K. (2001). Relational reinforcement learning.
Machine Learning, 43, 752.
Fern, A., Yoon, S., & Givan, R. (2006). Approximate policy iteration policy language
bias: solving relational markov decision processes. Journal Artificial Intelligence
Research, 25 (1), 75118.
Gardiol, N. H., & Kaelbling, L. P. (2003). Envelope-based planning relational MDPs.
Proc. Conf. Neural Information Processing Systems (NIPS).
Gardiol, N. H., & Kaelbling, L. P. (2007). Action-space partitioning planning. Proc.
AAAI Conf. Artificial Intelligence (AAAI), pp. 980986.
Gardiol, N. H., & Kaelbling, L. P. (2008). Adaptive envelope MDPs relational
equivalence-based planning. Tech. rep. MIT-CSAIL-TR-2008-050, MIT CS & AI Lab,
Cambridge, MA.
Gelly, S., & Silver, D. (2007). Combining online offline knowledge UCT. Proc.
Int. Conf. Machine Learning (ICML), pp. 273280.
Gretton, C., & Thiebaux, S. (2004). Exploiting first-order rgeression inductive policy
selection. Proc. Conf. Uncertainty Artificial Intelligence (UAI), pp.
217225.
Grush, R. (2004). Conscious thought simulation behaviour perception. Behaviorial
brain sciences, 27, 377442.
46

fiPlanning Noisy Probabilistic Relational Rules

Halbritter, F., & Geibel, P. (2007). Learning models relational MDPs using graph kernels.
Proc. Mexican Conference Artificial Intelligence (MICAI), pp. 409419.
Hesslow, G. (2002). Conscious thought simulation behaviour perception. Trends
Cognitive Science, 6 (6), 242247.
Hoffmann, J., & Nebel, B. (2001). planning system: Fast plan generation
heuristic search. Journal Artificial Intelligence Research, 14, 253302.
Holldobler, S., & Skvortsova, O. (2004). logic-based approach dynamic programming.
AAAI-Workshop: Learning planning MDPs, pp. 3136.
IPPC

(2008).
Sixth International Planning Competition,
http://ippc-2008.loria.fr/wiki/index.php/Main Page.

Uncertainty

Part..

Jensen, F. (1996). introduction Bayesian networks. Springer Verlag, New York.
Joshi, S., Kersting, K., & Khardon, R. (2009). Generalized first-order decision diagrams
first-order MDPs. Proc. Int. Conf. Artificial Intelligence (IJCAI), pp.
19161921.
Karabaev, E., & Skvortsova, O. (2005). heuristic search algorithm solving first-order
MDPs. Proc. Conf. Uncertainty Artificial Intelligence (UAI), pp.
292299.
Kearns, M. J., Mansour, Y., & Ng, A. Y. (2002). sparse sampling algorithm nearoptimal planning large Markov decision processes. Machine Learning, 49 (2-3),
193208.
Kersting, K., & Driessens, K. (2008). Nonparametric policy gradients: unified treatment propositional relational domains. Proc. Int. Conf. Machine
Learning (ICML), pp. 456463.
Kersting, K., van Otterlo, M., & de Raedt, L. (2004). Bellman goes relational. Proc.
Int. Conf. Machine Learning (ICML), pp. 465472.
Kocsis, L., & Szepesvari, C. (2006). Bandit based monte-carlo planning. Proc.
European Conf. Machine Learning (ECML), pp. 837844.
Kushmerick, N., Hanks, S., & Weld, D. (1995). algorithm probabilistic planning.
Artificial Intelligence, 78 (1-2), 239286.
Kuter, U., Nau, D. S., Reisner, E., & Goldman, R. P. (2008). Using classical planners
solve nondeterministic planning problems. Proc. Int. Conf. Automated
Planning Scheduling (ICAPS), pp. 190197.
Lang, T., & Toussaint, M. (2009a). Approximate inference planning stochastic relational worlds. Proc. Int. Conf. Machine Learning (ICML), pp. 585592.
Lang, T., & Toussaint, M. (2009b). Relevance grounding planning relational domains.
Proc. European Conf. Machine Learning (ECML), pp. 736751.
47

fiLang & Toussaint

Little, I., & Thiebaux, S. (2007). Probabilistic planning vs replanning. ICAPS-Workshop
International Planning Competition: Past, Present Future.
Littman, M. L., Goldsmith, J., & Mundhenk, M. (1997). computational complexity
probabilistic planning. Journal Artificial Intelligence Research, 9, 136.
Murphy, K. P. (2002). Dynamic Bayesian Networks: Representation, Inference Learning. Ph.D. thesis, UC Berkeley.
Murphy, K. P., & Weiss, Y. (2001). factored frontier algorithm approximate inference DBNs. Proc. Conf. Uncertainty Artificial Intelligence (UAI),
pp. 378385.
Pasula, H. M., Zettlemoyer, L. S., & Kaelbling, L. P. (2007). Learning symbolic models
stochastic domains. Journal Artificial Intelligence Research, 29, 309352.
Poon, H., & Domingos, P. (2007). Sound efficient inference probabilistic
deterministic dependencies. Proc. AAAI Conf. Artificial Intelligence
(AAAI).
Sanner, S., & Boutilier, C. (2007). Approximate solution techniques factored first-order
MDPs. Proc. Int. Conf. Automated Planning Scheduling (ICAPS),
pp. 288295.
Sanner, S., & Boutilier, C. (2009). Practical solution techniques first-order MDPs.
Artificial Intelligence, 173 (5-6), 748788.
Shachter, R. (1988). Probabilistic inference influence diagrams. Operations Research,
36, 589605.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: Introduction. MIT
Press.
Teichteil-Konigsbuch, F., Kuter, U., & Infantes, G. (2010). Aggregation generating
policies MDPs. appear Proc. Int. Conf. Autonomous Agents
Multiagent Systems.
Toussaint, M., & Storkey, A. (2006). Probabilistic inference solving discrete continuous state Markov decision processes. Proc. Int. Conf. Machine Learning
(ICML), pp. 945952.
Toussaint, M., Storkey, A., & Harmeling, S. (2010). Expectation-maximization methods
solving (PO)MDPs optimal control problems. Chiappa, S., & Barber, D.
(Eds.), Inference Learning Dynamic Models. Cambridge University Press.
van Otterlo, M. (2009). Logic Adaptive Behavior. IOS Press, Amsterdam.
Walsh, T. J. (2010). Efficient learning relational models sequential decision making.
Ph.D. thesis, Rutgers, State University New Jersey, New Brunswick, NJ.
48

fiPlanning Noisy Probabilistic Relational Rules

Wang, C., Joshi, S., & Khardon, R. (2008). First order decision diagrams relational
MDPs. Journal Artificial Intelligence Research, 31, 431472.
Weld, D. S. (1999). Recent advances AI planning. AI Magazine, 20 (2), 93123.
Wu, J.-H., Kalyanam, R., & Givan, R. (2008). Stochastic enforced hill-climbing. Proc.
Int. Conf. Automated Planning Scheduling (ICAPS), pp. 396403.
Yoon, S. W., Fern, A., & Givan, R. (2007). FF-Replan: baseline probabilistic planning.
Proc. Int. Conf. Automated Planning Scheduling (ICAPS), pp. 352
359.
Yoon, S. W., Fern, A., Givan, R., & Kambhampati, S. (2008). Probabilistic planning via
determinization hindsight. Proc. AAAI Conf. Artificial Intelligence
(AAAI), pp. 10101016.
Younes, H. L., & Littman, M. L. (2004). PPDDL1.0: extension PDDL expressing
planning domains probabilistic effects. Tech. rep., Carnegie Mellon University.

49


