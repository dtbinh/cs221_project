journal artificial intelligence

submitted published

noisy probabilistic relational rules
tobias lang
marc toussaint

tobias lang tu berlin de
mtoussai cs tu berlin de

machine learning robotics group
technische universitat berlin
franklinstrae berlin germany

abstract
noisy probabilistic relational rules promising world model representation several reasons compact generalize world instantiations usually
interpretable learned effectively action experiences complex
worlds investigate reasoning rules grounded relational domains exploit compactness rules efficient flexible decision theoretic
first combine rules upper confidence bounds applied
trees uct look ahead trees second converts
rules structured dynamic bayesian network representation predicts effects
action sequences approximate inference beliefs world states evaluate
effectiveness approaches simulated complex robot manipulation scenario articulated manipulator realistic physics domains
probabilistic competition empirical methods solve
existing methods fail

introduction
building systems act autonomously complex environments central goal artificial intelligence nowadays systems par particularly intelligent humans
specialized tasks playing chess hopelessly inferior almost humans however deceivingly simple tasks everyday life clearing desktop
preparing cup tea manipulating chess figures current state art reasoning learning perception locomotion manipulation far removed
human level abilities cannot yet contemplate working actual domain interest pasula zettlemoyer kaelbling performing common object manipulations
indeed challenging task real world choose large number
distinct actions uncertain outcomes number possible situations basically
unseizable
act real world accomplish two tasks first need understand
world works example pile plates stable place big plates
bottom hard job build tower balls filling tea cup may lead
dirty table cloth autonomous agents need learn world knowledge experience
adapt environments rely human hand crafting
employ recent solution learning pasula et al know possible
effects actions face second challenging use acquired
knowledge reasonable time sequence actions suitable achieve goals
c

ai access foundation rights reserved

filang toussaint

investigates novel tackle second task namely
pursue model complex domains contrast modelfree approaches compute policies directly experience respect fixed goals
called habit decision making follow purposive decision making
botvinick use learned plan goal current state
hand particular simulate probabilistic effects action sequences
interesting parallels recent neurobiology cognitive science suggesting
behavior intelligent mammals driven internal simulation emulation
found motor structures cortex activated
execution motor commands suppressed hesslow grush
probabilistic relational world model representations received significant attention
last years enable generalize object identities unencountered situations objects similar types account indeterministic action effects noise
review several approaches together related work section noisy
indeterministic deictic nid rules pasula et al capture world dynamics
elegant compact way particularly appealing learned effectively
experience existing rules relies growing
full look ahead trees grounded domain due large action space
stochasticity world computational burden plan single action
method given situation overwhelmingly large proposes two novel
ways reasoning efficiently grounded domain learned nid rules enabling fast
complex environments varying goals first apply existing upper
confidence bounds applied trees uct kocsis szepesvari nid
rules contrast full grown look ahead trees uct samples actions selectively thereby
cutting suboptimal parts tree early second introduce probabilistic relational
action sampling dbns prada uses probabilistic inference
cope uncertain action outcomes instead growing look ahead trees sampled successor states previous approaches prada applies approximate inference
techniques propagate effects actions particular make three contributions
prada following idea framing probabilistic inference shachter toussaint storkey harmeling convert nid rules
dynamic bayesian network dbn representation ii derive approximate inference method cope state complexity time slice resulting network
thereby efficiently predict effects action sequences iii
sampling action sequences propose sampling distribution plans takes predicted state distributions account evaluate approaches simulated
complex robot manipulation environment realistic physics articulated humanoid manipulating objects different types see fig domain contains billions
world states large number potential actions learn nid rules experience
environment apply approaches different
scenarios increasing difficulty furthermore provide approaches
domains recent international probabilistic competition
purpose discuss relation nid rules probabilistic
domain definition language ppddl used specification domains


fiplanning noisy probabilistic relational rules

begin discussing related work section reviewing
background work namely stochastic relational representations nid rules formalization decision theoretic graphical section section
present two build look ahead trees cope stochastic
actions section introduce prada uses approximate inference
section present empirical evaluation demonstrating utility
approaches finally conclude outline future directions

related work
decision making stochastic relational domains approached different ways field relational reinforcement learning rrl dzeroski
de raedt driessens van otterlo investigates value functions q functions
defined possible ground states actions relational domain key
idea describe important world features terms abstract logical formulas enabling
generalization objects situations model free rrl approaches learn value functions
states actions directly experience q function estimators include relational
regression trees dzeroski et al instance regression distance metrics relational states graph kernels driessens ramon gartner
model free approaches enable specific type used training
examples e g x thus may inappropriate situations goals
agent change quickly e g x inhand x contrast model rrl
approaches first learn relational world model state transition experiences
use model example form relational probability trees
individual state attributes croonenborghs ramon blockeel bruynooghe
svms graph kernels halbritter geibel stochastic relational nid rules
pasula et al particularly appealing action model representation
shown empirically learn dynamics complex environments
probabilistic relational world model available learned handcrafted
one pursue decision theoretic different ways within machine learning
community popular direction formalizes relational markov
decision process rmdp develops dynamic programming compute solutions e policies complete state action spaces many reason
lifted abstract representation without grounding referring particular instances boutilier reiter price introduce symbolic dynamic programming
first exact solution technique rmdps uses logical regression construct
minimal logical partitions state space required make necessary value function
distinctions implemented difficult keep firstorder state formulas consistent manageable size ideas kersting van
otterlo de raedt propose exact value iteration rmdps
logic programming called rebel employ restricted language represent rmdps
reason efficiently state formulas holldobler skvortsova
present first order value iteration fovia different restricted language
karabaev skvortsova extend fovia combining first order reasoning
actions heuristic search restricted states reachable initial


filang toussaint

state wang joshi khardon derive value iteration
first order decision diagrams fodds goal regression introduce reduction operators fodds keep representation small may require complex reasoning
empirical evaluation provided joshi kersting khardon apply
model checking reduce fodds generalize arbitrary quantification
techniques form interesting direction reason exactly
abstract rmdps employ different methods ensure exact regression theorem proving logical simplification consistency checking therefore principled approximations techniques discover good policies difficult domains
likewise worth investigating instance gretton thiebaux employ first order
regression generate suitable hypothesis language use policy induction thereby avoids formula rewriting theorem proving still
requiring model checking sanner boutilier present first order approximate linear programming foalp prior producing plans approximate
value function linear combinations abstract first order value functions
showing impressive solving rmdps millions states fern yoon
givan consider variant approximate policy iteration api replace
value function learning step learning step policy space make use
policy space bias described generic relational knowledge representation simulate trajectories improve learned policy kersting driessens describe
non parametric policy gradient deal propositional continuous
relational domains unified way
instead working lifted representation one may reason grounded domain
makes straightforward account two special characteristics nid rules
noise outcome uniqueness requirement rules grounding rmdp
specifies rewards set goal states one might principle apply traditional methods used propositional representations weld boutilier
dean hanks traditionally often cast search
state action space restricting oneself portion state space considered contain goal states reachable current state within limited
horizon much within community focused deterministic domains thus cant applied straightforwardly stochastic worlds common
probabilistic however determinize apply deterministic planners kuter nau reisner goldman indeed replan yoon
fern givan extension hindsight optimization yoon fern givan
kambhampati shown impressive performance many probabilistic
competition domains common variant replan considers probabilistic outcome action separate deterministic action ignoring respective probabilities
runs deterministic fast forward planner hoffmann nebel
determinized uses relaxation ignores delete
effects actions applies clever heuristics prune search space replan outputs
sequence actions expected states time action execution leads state
plan replan replan e recompute plan scratch
current state good performance replan many probabilistic domains
explained structure little thiebaux


fiplanning noisy probabilistic relational rules

argued replan less appropriate domains probability
reaching dead end non negligible outcome probabilities actions need
taken account construct good policy
many participants recent probabilistic competition ippc
extend replan deal probabilities action outcomes see competition
website brief descriptions winner competition rff
teichteil konigsbuch kuter infantes computes robust policy offline generating successive execution paths leading goal resulting policy
low probability failing lppff uses subgoals generated determinization
probabilistic divide smaller manageable hmdpps
strategy similar outcomes determinization replan accounts
probability associated outcome seh wu kalyanam givan extends
heuristic function replan cope local optima plans stochastic
enforced hill climbing
common reasoning general reward maximization context
avoids explicitly dealing uncertainty build look ahead trees sampling successor
states two follow idea namely sst kearns mansour ng
uct kocsis szepesvari investigated
another buffet aberdeen directly optimizes parameterized
policy gradient descent factor global policy simple approximate policies
starting action sample trajectories cope probabilistic effects
instead sampling state transitions propose prada
lang toussaint accounts uncertainty principled
way approximate inference domshlak hoffmann propose interesting
comes closest work introduce probabilistic extension planner complex building probabilistic relaxed
graphs construct dynamic bayesian networks dbns hand crafted strips operators reason actions states weighted model counting dbn
representation however inadequate type stochastic relational rules use
reasons naive dbn model discuss sec inappropriate inference approaches toussaint storkey spread information
backwards dbns calculate posteriors actions resulting policies
complete state spaces use backward propagation even full
inference relational domains open issue
approaches working grounded representation common number
states actions grow exponentially number objects apply
domains many objects approaches need combined complementary
methods reduce state action space complexity relational domains
instance one focus envelopes states high utility subsets state
space gardiol kaelbling one ground representation respect
relevant objects lang toussaint b one exploit equivalence actions
gardiol kaelbling particularly useful combination ignoring
certain predicates functions relational logic language gardiol kaelbling


filang toussaint

background
section set theoretical background
present subsequent sections first describe relational representations define world
states actions present noisy indeterministic deictic nid rules detail
thereafter define decision theoretic stochastic relational
domains finally briefly review dynamic bayesian networks
state action representation
relational domain represented relational logic language l set logical
predicates p set logical functions f contain relationships properties
hold domain objects set logical predicates comprises possible actions
domain concrete instantiation relational domain made finite set
objects arguments predicate function concrete e taken
call grounded concrete world state fully described conjunction grounded
potentially negated predicates function values concrete actions described
positive grounded predicates arguments predicates functions
abstract logical variables represent object predicate function
abstract arguments call abstract abstract predicates functions enable
generalization objects situations speak grounding formula
apply substitution maps variables appearing objects
relational model transition dynamics specifies p probability
successor state action performed state usually
non deterministic distribution typically defined compactly terms formulas
abstract predicates functions enables abstraction object identities
concrete domain instantiations instance consider set n cups effects trying
grab cups may described single abstract model instead
n individual apply given world state one needs ground
respect objects domain nid rules elegant way specify
model described following
noisy indeterministic deictic rules
want learn relational model stochastic world use pasula
et al recently introduced appealing action model representation
noisy indeterministic deictic nid rules combine several advantages
relational representation enabling generalization objects situations
indeterministic action outcomes probabilities account stochastic domains
deictic references actions reduce action space
noise outcomes avoid explicit modeling rare overly complex outcomes
existence effective learning


fiplanning noisy probabilistic relational rules

table shows exemplary nid rule complex robot manipulation domain
fig depicts situation rule used prediction formally nid rule
r given

pr
r x







ar x r x


r mr x
p

r
r


pr
r
x set logical variables rule represent sub set abstract
objects rules define world formulas abstract e
arguments logical variables rule r consists preconditions namely action
ar applied x state p
context r fulfilled mr different outcomes
associated probabilities pr pr outcome r x describes
predicates functions change rule applied context r x outcomes
r x conjunctions potentially negated literals constructed predicates
p well equality statements comparing functions f constant values besides
explicitely stated outcomes r called noise outcome r implicitly
potential outcomes rule particular includes rare overly
complex outcomes typical noisy domains want cover explicitly
compactness generalization reasons instance context rule depicted
fig potential highly improbable outcome grab blue cube pushing
objects table noise outcome allows account without burden
explicitly stating
arguments action xa may true subset xa x variables x
rule remaining variables called deictic references x xa denote
objects relative agent action performed deictic references
advantage decrease arity action predicates turn reduces size
action space least order magnitude significant effects
instance consider binary action predicate world
n objects n groundings contrast unary action predicate n
groundings
let denote substitution maps variables constant objects x
applying abstract rule r x yields ground rule r x say ground rule r
covers state ground action r ar let set ground nid
rules define r r ar set rules provide predictions
action r rule cover state call unique covering rule
state action pair unique covering rule r calculate p
taking outcomes r account weighted respective probabilities
r





p p r


x

pr p r pr p r





p r deterministic distribution one unique
state constructed taking changes r account distribution given


filang toussaint

table example nid rule complex robot manipulation scenario
try grab ball x cube implicitly defined one x deictic
referencing x ends robots hand high probability might
fall table small probability something unpredictable happens
confer fig example application

grab x x ball x cube table z

inhand x x
x z x


noise

figure nid rule defined table used predict effects action
grab ball situation left side right side depicts possible
successor states predicted rule noise outcome indicated
question mark define unique successor state

noise outcome p r unknown needs estimated pasula et al use
worst case constant bound pmin p r lower bound p alternatively
come well defined distribution one may assign low probability many
successor states described detail sec prada
exploits factored state representation grounded relational domain achieve
predicting state attribute change low probability
state action pair unique covering rule r e g two rules cover
providing conflicting predictions one predict effects means
noisy default rule r explains effects changing state attributes noise
p r p r essentially r expresses know
happen meaningful thus disadvantageous hence one
bias nid rules learner learn rules contexts likely mutually
exclusive reason concept unique covering rules crucial
nid rules pay price deictic references
abstract nid rule prediction ensure deictic references
unique groundings may require examining large part state representation


fiplanning noisy probabilistic relational rules

proper storage ground state efficient indexing techniques logical formula
evaluation needed
ability learn environment experience crucial requirement
autonomous agents learning rule sets general np hard efficiency guarantees sample complexity given many learning subtasks
suitable restrictions walsh pasula et al proposed supervised batch
learning complete nid rules learns structure rules
well parameters experience triples stating observed successor
state action applied state performs greedy search space
rule sets optimizes tradeoff maximizing likelihood experience
triples minimizing complexity current hypothesis rule set optimizing
scoring metric
x
x

log p rs
p en r



r

rs unique covering rule noisy default rule r
scaling parameter controls influence regularization p en r penalizes
complexity rule defined total number literals r
noise outcome nid rules crucial learning learning initialized rule set comprising noisy default rule r iteratively adds
rules modifies existing ones set search operators noise outcome
allows avoiding overfitting need model rare overly complex outcomes
explicitly drawback successor state distribution p r unknown
deal learning uses lower bound pmin approximate
distribution described uses greedy heuristics attempt
learn complete rules guarantees behavior given pasula et al however report impressive complex noisy environments sec confirm
simulated noisy robot manipulation scenario major motivation employing nid rules learn observed actions state transitions
furthermore prada exploit simple structure
similar probabilistic strips operators convert dbn representation
provide detailed comparison nid rules ppddl appendix b nid
rules support features sophisticated domain description language
ppddl compactly capture dynamics many interesting domains
decision theoretic
decision theoretic actions given state
expected maximize future rewards states actions boutilier et al
classical reward usually defined terms clear cut goal
fulfilled fulfilled state expressed means logical
formula typically formula partial state description exists
one state holds example goal might put romance
books specific shelf matter remaining books lying case
involves finding sequence actions executing starting


filang toussaint

world state stochastic domains however outcomes
actions uncertain probabilistic inherently harder deterministic
counterpart littman goldsmith mundhenk particular achieving goal
state certainty typically unrealistic instead one may define lower bound
probability achieving goal state second source uncertainty next uncertain
action outcomes uncertainty initial state ignore latter
following assume deterministic initial states see later however
straightforward incorporate uncertainty initial state one proposed
approaches
instead classical task finished achieved state
goal fulfilled task may ongoing instance goal might
keep desktop tidy formalized means reward function states
yields high reward desirable states simplicity assume rewards
depend actions taken reinforcement learning formalisms
sutton barto classical goals easily formalized
reward function cast scenario stochastic relational domain
relational markov decision process rmdp framework boutilier et al follow
notation van otterlo define rmdp tuple r contrast
enumerated state spaces state space relational structure defined
logical predicates p functions f yield ground atoms arguments taken
set domain objects action space defined positive predicates
arguments transition distribution r r
reward function r make use factored relational representation
abstract states actions discussed following typically
state space action space relational domain large consider
instance domain objects use binary predicates represent states

case number states relational world encapsulate transition
probabilities compact way exploiting relational structure example nid rules
described eq achieve generalized partial world state descriptions
form conjunctions abstract literals compactness however
carry directly
deterministic policy tells us action take given state
fixed horizon discount
interested maximizing
pd factor
r value factored state defined
discounted total reward r



expected return state following policy
v e r



solution rmdp thus optimal policy
maximizes expected return defined bellman equation
x


v r max
p v
aa







fiplanning noisy probabilistic relational rules

similarly one define value q action state expected return
action taken state policy select subsequent actions
q e r
x
r
v p






q values optimal policy let us define optimal action optimal
value state


argmax q





aa




v max q
aa



enumerated unstructured state spaces state q values computed dynamic programming methods resulting optimal policies complete state space
recently promising approaches exploiting relational structure proposed apply similar ideas solve approximate solutions rdmps abstract level without
referring concrete objects see related work sec alternatively one may
reason grounded relational domain makes straightforward account
noise outcome uniqueness requirement nid rules usually one focuses estimating optimal action values given state appealing agents
varying goals quickly coming plan hand
appropriate computing abstract policy complete state space although
grounding simplifies decision theoretic propositionalized representation challenging task complex stochastic domains sections
present different reasoning grounded relational domain estimating
optimal q values actions action sequences given state
dynamic bayesian networks
dynamic bayesian networks dbns model development stochastic systems
time prada introduce sec makes use
kind graphical model evaluate stochastic effects action sequences factored
grounded relational world states therefore briefly review bayesian networks
dynamic extension
bayesian network bn jensen compact representation joint probability distribution set random variables x means directed acyclic graph
g nodes g represent random variables edges define dependencies thereby express conditional independence assumptions value x variable
x x depends values immediate ancestors g called
parents p x x conditional probability functions node define p x p x
case discrete variables may defined form conditional probability tables
bn compact representation distribution x nodes
parents conditional probability functions significant local structure
play crucial role development graphical prada


filang toussaint

dbn murphy extends bn formalism model dynamic system evolving
time usually focus discrete time stochastic processes underlying
system case world state represented bn b dbn maintains
copy bn every time step dbn defined pair bns b b
b deterministic uncertain prior defines state system
initial state b two slice bn defines dependencies two
successive time steps implements first order markov assumption
variables time depend variables time variables

look ahead trees
plan nid rules one treat domain described
ulary relational markov decision process discussed sec
present two value reinforcement learning
generative model build look ahead trees starting initial
used estimate values actions states

relational logic vocab following
employ nid rules
state trees

sparse sampling trees
sparse sampling tree sst kearns et al mdp samples
randomly sparse full grown look ahead trees states starting given state
root suffices compute near optimal actions state mdp given
horizon branching factor b sst works follows see fig tree
node representing state sst takes possible actions account ii
action takes b samples successor state distribution generative model
transitions e g transition model mdp build tree nodes next
level values tree nodes computed recursively leaves root
bellman equation given node q value possible action estimated
averaging values b children states action maximizing
q value actions chosen estimate value given node sst
favorable property independent total number states mdp
examines restricted subset state space nonetheless exponential
time horizon taken account
pasula et al apply sst nid rules sampling noise
outcome sst assume stay state discount
estimated value refer adaptation speak sst
remainder action unique covering rule use noisy
default rule r predict effects better perform othing action
instead staying state get punished hence sst one
discard actions given state unique covering rules
sst near optimal practice feasible small branching factor
b horizon let number actions number nodes
horizon ba number reduced outcome rule sampled
multiple times illustration assume possible actions per time step
set parameters b choice pasula et al experiments plan
single action given state one visit states smaller


fiplanning noisy probabilistic relational rules

figure sst samples sparse full grown look ahead trees
estimate values actions states

choices b lead faster significant accuracy loss realistic
domains kearns et al note sst useful special structure permits
compact representation available sec introduce alternative
approximate inference exploits structure nid rules
sampling trees upper confidence bounds
upper confidence bounds applied trees uct kocsis szepesvari
samples search tree subsequent states starting current state root
contrast sst generates b successor states every action state idea
uct choose actions selectively given state thus sample selectively
successor state distribution uct tries identify large subsets suboptimal actions early
sampling procedure focus promising parts look ahead tree instead
uct builds look ahead tree repeatedly sampling simulated episodes
initial state generative model e g transition model mdp episode
sequence states rewards actions limited horizon r r sd rd
simulated episode values tree nodes representing states updated
online simulation policy improved respect values
distinct value estimated state action pair tree monte carlo simulation
precisely uct follows following policy tree node exist actions
explored yet uct samples one uniform
distribution otherwise actions explored least uct selects
action maximizes upper confidence bound qo
u ct estimated action


filang toussaint

value qu ct

qo
u ct

qu ct c

log ns

ns

u ct argmax qo
u ct






ns counts number times actionpa selected state ns
counts total number visits state ns ns bias parameter c defines
influence number previous action selections thereby controls extent
upper confidence bound
end episode value encountered state action pair st
updated total discounted rewards
nst nst
qu ct st qu ct st



nst


x





rt qu ct st





policy uct implements exploration exploitation tradeoff balances
exploring currently suboptimal looking actions selected seldom thus far
exploiting currently best looking actions get precise estimates values
total number episodes controls accuracy ucts estimates balanced
overall running time
uct achieved remarkable challenging domains game go
gelly silver best knowledge first apply uct
stochastic relational domains nid rules generative model adapt
uct cope noise outcomes fashion sst assume stay
state discount obtained rewards thus uct takes actions unique
covering rules account reasons sst

approximate inference
uncertain action outcomes characterize complex environments make relational domains substantially difficult sampling approaches discussed
previous section tackle repeatedly generating samples outcome
distribution action transition probabilities mdp leads lookahead trees easily blow horizon instead sampling successor
states one may maintain distribution states called belief following
introduce grounded stochastic relation domains propagates beliefs states sense state monitoring first create
compact graphical nid rules develop approximate inference method
efficiently propagate beliefs hand describe probabilistic relational
action sampling dbns prada samples action sequences
informed way evaluates approximate inference dbns
example presented illustrate reasoning prada finally discuss prada
comparison approaches previous section sst uct present simple
extension prada


fiplanning noisy probabilistic relational rules



b

figure graphical nid rules naive dbn b dbn exploiting nid factorization

graphical nid rules
decision theoretic agents need choose appropriate actions represented means markov chains dynamic bayesian networks dbns
augmented decision nodes specify agents actions boutilier et al
following discuss convert nid rules dbns prada
use plan probabilistic inference denote random variables upper case letters
e g values corresponding lower case letters e g dom variable
vectors bold upper case letters e g value vectors bold lower
case letters e g use column notation e g
naive way convert nid rules dbns shown fig states represented
vector sn ground predicate p binary si
ground function f sj range according represented
function actions represented integer variable indicates action
vector ground action predicates reward gained state represented
u may depend subset state variables possible express
arbitrary reward expectations p u binary u cooper define
transition dynamics nid rules naive model assume given set
fully abstract nid rules compute groundings rules w r objects
domain get set k different ground nid rules parents state variable
si successor time step include action variable respective variable si
predecessor time step parents si determined follows
rule r literal corresponding si appears outcomes r variables
sk corresponding literals preconditions r parents si typically si
manipulated several actions turn modeled several rules total
number parents si large worsened usage deictic
references nid rules increase total number k ground rules
resulting local structure conditional probability function si complex one
account uniqueness covering rules complex dependencies
two time slices make representation unfeasible


filang toussaint

therefore exploit structure nid rules model state transition
compact graphical model shown fig b representing joint distribution
p u r



p u p r p r p r p



explain detail following assume given set
fully abstract nid rules compute set k different ground nid rules
w r objects domain addition u u use
binary random variable rule model event context holds
case required literals hold let indicator function
argument evaluates true otherwise


k
k



p
p
sj sri j






j

v

use express logical conjunction n function yields set
indices state variables depends sri denotes configuration
state variables corresponding literals context ri use integer valued
variable r ranging k possible values identify rule predicts effects
action exists unique covering rule current state action pair
e rule r modeling action whose context holds



p r r r r
r

r r

unique covering rule exists predict changes indicated special value
r assuming execute action similarly sst uct




p r
r
r

r r

r

integer valued variable represents outcome action predicted
rule ranges possible values maximum number outcomes
rules ensure sound semantics introduce empty dummy outcomes
zero probability rules whose number outcomes less probability
outcome defined corresponding rule
p r pr
define probability successor state

p r
p si r







one unique state constructed taking changes according
r account outcome specifies value si value probability


fiplanning noisy probabilistic relational rules

one otherwise value state variable persists previous time step
rules usually change small subset persistence often applies resulting
dependency p r si variable si time step compact contrast
naive dbn fig three parents namely variables outcome
rule predecessor previous time step simplifies specification
conditional probability function significantly enables efficient inference
see later probability reward given



p u
sj j

j u

function u yields set indices state variables u depends
configuration variables corresponds goal denoted
uncertain initial states naturally accounted specifying priors p
renounce specification prior however initial state given
experiments later enable comparison look ahead tree approaches sst
uct require deterministic initial states might sampled
prior choice distribution p used sampling actions described
sec
simplicity ignored derived predicates functions defined
terms predicates functions presentation graphical model derived
concepts may increase compactness rules dependencies among concepts acyclic
straightforward include derived concepts model intra state dependencies
corresponding variables indeed use derived predicates experiments
interested inferring posterior state distributions p st given sequence previous actions omit conditioning initial state simplicity
exact inference intractable graphical model constructing junction tree
get cliques comprise whole markov slices variables representing state
certain time step consider eliminating state variables st due moralization
outcome variable connected state variables st elimination
variables st form clique thus make use approximate inference
techniques general loopy belief propagation lbp unfeasible due deterministic
dependencies small cycles inhibit convergence conducted preliminary tests small networks damping factor without success interesting
open question whether ways alternate propagating deterministic information running lbp remaining parts network e g whether methods
mc sat poon domingos successfully applied decision making contexts next subsection propose different approximate inference scheme
factored frontier describes forward inference procedure
computes exact marginals next time step subject factored approximation
previous time step advantage exploit structure
involved dbns come formulas marginals related passing
forward messages contrast lbp information propagated backwards note
condition rewards full inference samples
actions backward reasoning uninformative


filang toussaint

approximate inference
following present efficient method approximate inference previously
proposed dbns exploiting factorization nid rules focus mathematical
derivations illustrative example provided sec
follow idea factored frontier murphy weiss
approximate belief product marginals

p st
p sti



define
sti p sti
st p st

n



sti





derive filter dbn model fig b interested inferring
state distribution time given action sequence calculate marginals
state attributes

st

p si
x

p st
rt p rt





rt

eq use rules prediction weighted respective posteriors p rt
reflects fact depending state use different rules model
action weight p rt rules modeling action remaining
rules model weights correspond posterior parts
state space according rule used prediction
compute first term
x
p st
rt
p st
rt sti p sti rt


sti



x

p st
rt sti sti




sti

sum possible values variable si previous time step intuitively take account potential pasts arrive value st
next

st enables us easily predict probabilities
time step resulting term p st

r


next time step discussed prediction weighted marginal
sti respective previous value approximation assumes sti conditionally independent rt true general choice rule prediction
depends current state thus attribute si improve approximation one examine whether sti part context rt case infer
state sti knowing rt however found approximation sufficient


fiplanning noisy probabilistic relational rules

one would expect calculate successor state distribution p st
rt sti


taking different outcomes r account weighted respective probabilities
p rt
x
p st
rt sti
p st
rt sti p rt





shows us update belief sit predict rule rt p st
rt sti


deterministic distribution changes value si si set accordingly otherwise value sti persists
lets turn computation second term eq p rt posterior
rules trick use context variables exploit assumption
rule r state transition uniquely covers st indicated
appropriate assignment reduced expression
involving marginals start
x
p rt r
p rt r p







r p tr

tr

r r






r p tr p

tr tr

r r


simplify summation consider unique assignment
context variables r used prediction provided action indicated
r case context tr holds contexts tr
competing rules r action hold
calculate second term summing states
x

x
p tr
p tr st st
p tr st
stj

st





st

sjt sr j



j



j tr

approximation assumption sr denotes configuration
state variables according context r sum variables
context r variables rs context remain terms sjt sr j correspond
probabilities respective literals
third term joint posterior contexts competing rules r
given rs context already holds interested situation none
contexts hold calculate




p
tr tr
p tr tr

r r

r r



filang toussaint

approximating product individual posteriors latter computed
x
p tr tr
p tr st p st tr




r r
q

si sr otherwise


r




r

condition expresses logical contradiction contexts r r
contexts contradict r context surely hold given rs context holds
otherwise know state attributes apppearing contexts r r
hold condition r therefore examine remaining state
attributes r context approximate posterior marginals
finally compute reward probability straightforwardly
x

p u st p st
sit

p u
st

u

denotes configuration state variables corresponding goal
summation states simplified assumption resulting
product marginals required state attributes
overall computational costs propagating effects action quadratic
number rules action rule calculate probability
none others applies linear maximum numbers context literals
manipulated state attributes rules
inference framework requires approximation distribution p r
cf eq cope noise outcome nid rules training data used
learn rules estimate predicates functions change value time follows let
sc contain corresponding variables estimate rule r average number
n r changed state attributes noise outcome applies due factored frontier
consider noise effects variable independently approximate
r
probability si sc changes rs noise outcome snc case change
changed values si equal probability

dbn representation fig b together approximate inference method described last subsection enable us derive novel stochastic
relational domains probabilistic relational action sampling dbns prada plans sampling action sequences informed way predicted
beliefs states evaluating action sequences approximate inference
precisely sample sequences actions length
infer posteriors states p st rewards p ut sense
filtering state monitoring calculate value action sequence
discount factor
q


x

p u







fiplanning noisy probabilistic relational rules

choose first action best sequence argmaxa q
value exceeds certain threshold e g otherwise continue sampling actionsequences action found given quality found
plan controlled total number action sequence samples traded
time available
aim strategy sample good action sequences high probability
propose choose equal probability among actions unique covering
rule current state thereby avoid use noisy default rule r
action effects noise thus poor use action time
prada samples distribution



x

tr

p tr
psample

r

r r

sum rules action rule add posterior
unique covering rule e context tr holds contexts tr competing
rules r hold sampling distribution takes current state distribution
account thus probability sample action sequence predicting state sequence
st depends likelihood state sequence given likely required outcomes likely next actions sampled policy
prada miss actions sst uct explore following proposition
states proof appendix
proposition set action sequences prada samples non zero probability
super set ones sst uct
experiments replan action executed without reusing knowledge previous time steps simple strategy helps get general impression
pradas performance complexity strategies easily conceivable
instance one might execute entire sequence without replanning trading faster
computation times potential loss achieved reward noisy environments
might seem better strategy combine reuse previous plans replanning
instance one could omit first action previous plan executed
examine suitability remaining actions state consider
single best action sequence many domains might beneficial
marginalize sequences first action instance action
might lead number reasonable sequences none best another
action first one good sequence many bad ones case one
might favor
illustrative example
let us consider small table illustrate reasoning procedure
prada domain noisy cubeworld represented predicates table x cube x
x inhand x clear x x robot perform two types
actions may lift cube x means action grab x put cube


filang toussaint

held hand top another object x puton x start state shown
contains three cubes b c stacked pile table goal shown b
get middle cube b top top cube world model provides three abstract
nid rules predict action effects shown table c first rule uncertain
outcomes grab object another object contrast grabbing
clear object rule putting object somewhere rule leads
successor state
first prada constructs dbn represent purpose
computes grounded rules respect objects b c shown
potential grounded rules ignored one deduce abstract rules
predicates changeable combination specifications prunes
grounded rules instance know table thus ground rule
action argument x needs constructed rules require cube x
dbn prada samples action sequences evaluates expected
rewards following investigate procedure sampling action sequence
grab b puton table e presents inferred values dbn variables
auxiliary quantities marginals eq state variables
set deterministically according calculate posteriors context variables
p according eq example one rule
probability actions grab grab b grab c contrast
rules non zero probability puton actions help eq
calculate probability rule r unique covering rule respective
action listed unique rule note condition fixed action thus
far case context r r holds contexts r competing rules
r action hold posterior r alone
resulting probabilities used calculate sampling distribution eq first
compute probability action unique covering rule simple
sum probabilities previous step listed action coverage table
normalize values get sampling distribution psample
sampling distribution uniform three actions unique rules assume
sample grab b grabbing blue cube b variable r specifies ground rules
use predicting state marginals next time step infer posterior
according eq p r b act
things get interesting observe effects factored
frontier instance consider calculating posterior context r ground rule
r b att grabbing blue cube b yellow eq
p b att b b cube cube b table

contrast exact value p b att according third outcome
abstract rule used predict imprecision due ignoring correlations
regards marginals b b independent fact fully
correlated
action grab three ground rules non zero context probabilities
grabbing b c due three different outcomes abstract


fiplanning noisy probabilistic relational rules

table example pradas factored frontier inference
start state
b b c c
cube cube b cube c table

b goal
b

c abstract nid rules example situations
rule
grab x x x z cube x cube table

inhand x z x x z
inhand x x x z


x x z

rule
grab x cube x clear x x

inhand x x


e inferred posteriors pradas

inference

action sequence
grab b puton






state marginals
b
c

b
b c
b
c
inhand b
clear
clear b
clear c
goal u








































p
b act
b att
c btt
b
c

b
c
b
c b
b

























unique rule
b act


b att

c att


c btt


b


c





b

c


b


c b


b


action coverage
grab


grab b

grab c


puton


puton c


puton


sample distribution
psample grab

psample grab b

psample grab c

psample puton

psample puton c

psample puton

p rt rt
rt b act


rt b


rt



rule
puton x inhand cube

x inhand x


grounded nid rules
grounded rule action
substitution
bbt
grab x b z b
bct
grab x b z c

c bbt
grab c x c b z b
b
grab
x b
c
grab
x c

grab
x

c
grab c
x c
b
puton
x b
c
puton
x c

c
puton
x c



filang toussaint

rule example calculate probability rule c grabbing c
unique covering rule grab
p c b
p c p b p

calculations determine sampling distribution assume
sample action puton rule b putting b used
prediction probability since probability unique covering rule
action puton remaining mass posterior assigned parts
state space unique covering rule available puton case use
default rule r corresponding performing action probability
values state variables persist
finally let us infer marginals eq example calculate
inhand b let b brief inhand b sum ground rules rt taking
potential values b b previous time step account
x
b
p rt p b rt b b
rt

p b rt b b

discussed ground rule b default rule play role
prediction effect belief b inhand decreases tried
put b expected similarly calculate posterior b
expected probability reach goal performing actions grab b
puton pradas inferred value coincides true posterior
comparison probability reach goal performing actions
grab puton grab b puton e clear b grab plan
safer e higher probability takes actions
comparison approaches
prominent difference presented approaches way
account stochasticity action effects one hand sst uct repeatedly take samples successor state distributions estimate value action
building look ahead trees hand prada maintains beliefs states
propagates indetermistic action effects forward precisely prada sst follow
opposite approaches prada samples actions calculates state transitions approximately means probabilistic inference sst considers actions thus exact
action search samples state transitions price considering actions
ssts overwhelmingly large computational cost uct remedies issue samples action sequences thus state transitions selectively uses previously sampled episodes
build upper confidence bounds estimates action values specific states
used adapt policy next episode straightforward translate


fiplanning noisy probabilistic relational rules

adaptive policy prada since prada works beliefs states instead states
directly therefore chose simple policy prada sample randomly
actions unique covering rule state form sampling distribution
account beliefs states
prada returns whole plan transform world state one goal
fulfilled probability exceeding given threshold spirit conformant probabilistic observability kushmerick hanks weld
due outcome sampling sst uct cannot return plan straightforward way instead provide policy many successor states estimates
action values look ahead tree estimates states deeper tree
less reliable built less episodes action executed
state observed estimates reused thus far prada take
knowledge gained previous action sequence samples account adapt policy
elegant way achieve better exploit goal knowledge might use backpropagation
dbns plan completely inference toussaint storkey
beyond scope clear principled way
large state action spaces relational domains alternatively prada could give high
weight second action previous best plan sec another
simple way make use previous episodes better plans
prada afford simple action sampling strategy evaluates large numbers
action sequences efficiently grow look ahead trees account
indeterministic effects points important difference three faced
search spaces action sequences exponential horizon calculate
value given action sequence however sst uct still need exponential time due
outcome sampling contrast prada propagates state transitions forward
thus linear horizon
approximate neither sst uct prada expected perform ideally situations sst uct sample action outcomes hence
face important outcomes small probability instance consider
agent wants escape room two locked doors hits first door
made wood chance break escape second door made
iron chance break sst uct may take long time
detect times better repeatedly hit wooden door contrast prada
recognizes immediately reasoned actions takes
outcomes account hand pradas approximate inference procedure correlations among state variables get lost sst uct preserve
sample complete successor states impair pradas performance
situations correlations crucial consider following simple domain two
state attributes b agent choose two actions modeled rules

action
action






b

b



b
b








filang toussaint

goal make attributes true false e b b
actions resulting marginals b
b due factored frontier prada cannot distinguish actions
although action achieve goal action
pradas estimated probabilities states rewards may differ significantly
true values harm performance many domains experiments
indicate sec suppose reason pradas estimated probabilities imprecise enable correct ranking action sequences
interested choosing best action instead calculating correctly value
difference proposed way handle noise
outcome rules prada assigns small probability successor states spirit
noise outcome contrast sst uct make sense sample
distribution single successor state extremely low probability
inadequate estimate state action values hence use described workaround
assume stay state discounting obtained rewards
straightforward prada deal uncertain initial states uncertainty
initial states common complex environments may instance caused partial
observability noisy sensors uncertainty natural representation belief
state prada works contrast sst uct cannot account uncertain initial
states directly would sample prior distribution
extension adaptive prada
present simple extension prada increase accuracy
exploit fact prada evaluates complete sequences actions contrast sst
uct actions taken depend sampled outcomes adaptive
prada prada examines best action sequence found prada prada
chooses first action sequence without reasoning prada inspects
single action sequence decides simulation whether deleted
resulting shortened sequence may lead increased expected reward case
actions significant effects achieving goal decrease success
probability actions omitted states high reward reached earlier
rewards discounted less instance consider goal grab blue ball
action sequence grabs red cube puts onto table grabs blue
ball improved omitting first two actions unrelated goal
precisely prada takes pradas action sequence ap highest value
investigates iteratively action whether deleted action
deleted plan resulting plan higher reward likelihood idea
formalized crucial calculation compute values
q defined eq restated convenience


q






x

p u



pradas approximate inference procedure particularly suitable calculating required p u performs calculation time linear length


fiplanning noisy probabilistic relational rules

adaptive prada prada
input pradas plan ap
output pradas plan aa
aa ap


true

let plan length


b omit





aa

othing

q q aa

aa

else

break

end

end
end
return aa

action sequence sst uct would require time exponential
outcome sampling

evaluation
implemented presented learning
nid rules c code available www user tu berlin de lang prada
evaluate approaches two different scenarios first intrinsically noisy complex simulated environment learn nid rules experience use
plan second apply benchmarks uncertainty part
international competition
simulated robot manipulation environment
perform experiments simulated complex robot manipulation environment
robot manipulates objects scattered table fig report three
series experiments different tasks increasing difficulty first describe domain
detail use rigid body dynamics simulator ode enables realistic behavior objects simulator available www user tu berlin de lang dwsim
objects cubes balls different sizes colors robot grab objects
put top objects table actions robot affected
noise domain towers objects straight lined easier put object
top big cube top small cube difficult put something
top ball piles objects may topple objects may fall table case
become reach robot
represent domain predicates x inhand x upright x x
object fallen table function size x unary typing predicates cube x
ball x table x predicates obtained querying state simulator


filang toussaint

figure simulated robot plays cubes balls different sizes scattered
table objects fallen table cannot manipulated anymore

translating according simple hand made guidelines thereby sidestepping difficult
converting agents observations internal representation instance
b holds b exert friction forces z coordinate greater
one b x coordinates similar besides primitive
concepts use derived predicate clear x x found
predicate enable compact accurate rules reflected values
objective function rule learning given eq
define three different types actions actions correspond motor primitives
whose effects want learn exploit grab x action triggers robot open
hand move hand next x let grab x raise robot arm
execution action influenced factors example different
object held hand fall table third
object objects top x likely fall
puton x action centers robots hand certain distance x opens
raises hand instance object z x object
potentially inhand may end z z might fall x othing action triggers
movement robots arm robot might choose action thinks
action could harmful respect expected reward emphasize
actions execute regardless state world actions
rather unintuitive humans trying grab table put object top
carried robot learn effects motor primitives
due intrinsic noise complexity simulated robot manipulation scenario
challenging domain learning compact world well
objects f different object sizes action space contains actions

state space huge f different states excluding states one would classify
impossible given intuition real world physics
use rule learning pasula et al parameter
settings learn three different sets fully abstract nid rules rule set learned


fiplanning noisy probabilistic relational rules

independent training sets experience triples specify world
changed state successor state action executed assuming full
observability training data learn rules generated world six cubes four
balls two different sizes performing random actions slight bias build high
piles resulting rule sets contain rules respectively rule sets provide
approximate partial true world dynamics generalize situations
experiences may account situations completely different
agent seen enforce compactness avoid overfitting rules
regularized hence learning may sometimes favor model rarely experienced
state transitions low probability outcomes general rules thereby trading
accuracy compactness combination general noisiness world
causes need carefully account probabilities world reasoning
rules
perform three series experiments tasks increasing difficulty
series test planners different worlds varying numbers cubes
balls thus transfer knowledge gained training world different similar
worlds abstract nid rules object number create five different worlds
per rule set world perform three independent runs different random seeds
evaluate different approaches compute mean performances
times fixed randomly generated set trials learned rule sets
worlds random seeds
choose parameters follows sst report different branching factors b far resulting runtimes allow similarly uct
prada parameter balances time quality
found actions uct number episodes prada
number sampled action sequences depending experiment set
heuristically tradeoff time quality reasonable
particular fair comparison pay attention uct prada prada get
times reported otherwise furthermore uct set
bias parameter c found heuristically perform best planners
experiments set discounting factor future rewards crucial
parameter horizon heavily influences time course
cannot known priori therefore reported otherwise deliberately set larger
required uct prada suggest effective
estimated indeed found experiments long
small exact choice significant effects ucts pradas
quality unlike effects times contrast set horizon
sst small possible case times still large
suitable action given situation restart
procedure sst builds tree uct runs episodes prada
takes action sequence samples given situation runs suitable
action still found trial fails
furthermore use replan yoon et al baseline discuss
detail related work sec replan determinizes
thereby ignoring outcome probabilities replan shown impressive


filang toussaint

domains probabilistic competitions domains carefully designed
humans action dynamics definitions complete accurate consistent
used true world dynamics according experiments contrast learned
nid rules use estimate approximate partial robot manipulation
domain able use derived predicate clear x replan implementation
experiments included appropriate literals predicate hand
outcomes rules sst uct prada implementations infer
values automatically definition clear x report replan
almost original learned rules outcomes determinization scheme denoted
replan single outcome schemes led worse performance
rules general putting restrictions arguments
deictic references case actions appear applicable given state make
sense intuitive human perspective hurts replan much
methods resulting large times replan instance rule may model
toppling small tower including object x trying put object top
tower one outcome might specify end x possible
cube course learning may choose omit typing predicate
cube x due regularization prefers compact rules none experiences might
require additional predicate therefore created modified rule sets hand
introduced typing predicates appropriate make contexts distinct
denote modified rule sets replan replan single
outcomes single probable outcome determinization schemes
high towers
first series experiments investigate building high towers
task work pasula et al precisely reward state defined
average height objects constitutes easy many different
actions may increase reward object identities matter small
horizon sufficient set sst horizon pasula et al choice different
branching factors b uct prada horizon experiments initial
states contain already stacked objects reward performing actions
table fig present sst competitive branching factor
b slower uct prada least order magnitude
b performance poor series experiments designed worlds
objects contain many big cubes explains relatively good performance sst
worlds number good plans large mentioned control uct
prada prada times available three
approaches perform far better sst almost experiments difference
uct prada prada never significant
series experiments indicates approaches full grown lookahead trees sst inappropriate even easy contrast approaches exploit look ahead trees clever way uct seem best
choice easy tasks require small horizon solved many
alternative good plans performance approaches approximate


fiplanning noisy probabilistic relational rules

table high towers reward denotes discounted total reward different
numbers objects cubes balls table reward performing actions
data points averages trials created learned rule sets
worlds random seeds standard deviations mean estimators
shown replan replan single use hand made modifications
original learned rule sets fig visualizes
objects

planner
replan
replan
replan single



sst b
sst b
sst b
uct
prada
prada

sst b
sst b
sst b
uct
prada
prada

sst b
sst b
sst b
uct
prada
prada

















































































replan
replan
replan single


trial time








replan
replan
replan single


reward
























filang toussaint

replan
replan
replan single
sst b
sst b
sst b
uct
prada
prada







trial time

discounted total reward










objects





reward


objects



b time

figure high towers visualization presented table reward
performing actions data points averages trials created
learned rule sets worlds random seeds error bars standard
deviations mean estimators shown please note log scale b

inference prada prada however comes close one uct showing
suitability scenarios
replan focuses exploiting conjunctive goal structures cannot deal quantified goals grounded reward structure task consists disjunction
different tower combinations replan pick arbitrary tower combination
goal therefore apply replan sample tower combinations according rewards achieve e situations high towers probable exclude
combinations balls bottom towers prohibited reward
structure yoon et al note obvious pitfall goal formula sampling
groundings goal reachable much expensive reach
initial state replan cannot plan execute action
sample ground goal formula next time step preserving already achieved
tower structures
replan performs significantly worse previous approaches
major reason replan often comes plans exploiting low probability
outcomes rules contrast sst uct prada reason
probabilities illustrate consider example rule fig putting
ball top cube two explicit outcomes ball usually ends
cube sometimes however falls table replan misuse rule tricky
way put ball table ignoring often fail ffreplan single taking probable outcomes account remedy
often two three outcomes similar probabilities
choice seems unjustified sometimes intuitively expected outcome split
different outcomes low probabilities however vary features irrelevant
upright


fiplanning noisy probabilistic relational rules

table desktop clearance reward denotes discounted total reward different numbers objects cubes balls table reward performing
actions data points averages trials created learned rulesets worlds random seeds standard deviations mean estimators
shown replan replan single use hand made modifications
original learned rule sets fig visualizes
obj

planner
replan
replan
replan single



sst b
uct
prada
prada

sst b
uct
prada
prada

sst b
uct
prada
prada























































h









replan
replan
replan single


trial time






replan
replan
replan single


reward






desktop clearance
task second series experiments clear desktop objects lying
splattered table beginning object cleared part tower
containing objects class object class simply defined terms
color additionally provided state representation robot reward
robot defined number cleared objects experiments classes contain
objects ball order enable successful piling starting situations contain piles objects different classes thus reward
performing actions desktop clearance difficult building high towers
number good plans yielding high rewards significantly reduced
set horizon optimal sst required clear
class objects namely grabing putting three objects contrast set
uct prada deal overestimation
usually unknown optimal horizon table fig present horizon
overburdens sst seen large times even b sst
takes almost minutes average worlds objects hours worlds
objects therefore try sst greater b contrast times








replan
replan
replan single
sst b
uct
prada
prada










objects


trial time

discounted total reward

lang toussaint








reward


objects



b time

figure desktop clearance visualization presented table
reward performing actions data points averages trials
created learned rule sets worlds random seeds error bars
standard deviations mean estimators shown note log scale b

uct prada prada controlled enable
reasonable performance two orders magnitude smaller although overestimating
horizon trial take average worlds objects
minutes worlds objects minutes worlds objects nonetheless uct
prada prada perform significantly better sst worlds prada
prada turn outperform uct particular worlds many objects prada
finds best plans among planners planners gain reward worlds objects
comparison worlds objects number objects cleared increases
well number classes thus good plans worlds objects contain
numbers object classes worlds objects objects
making difficult
overall findings desktop clearance experiments indicate sst
inappropriate uct achieves good performance scenarios require medium
horizons several many alternative plans approaches
approximate inference prada prada however seem appropriate scenarios intermediate difficulty
furthermore indicate replan inadequate clearance task
sample target classes randomly provide goal structure replan tower
structure within target class turn randomly chosen bad performance
replan due reasons described previous experiments particular
plans replan often rely low probability outcomes


fiplanning noisy probabilistic relational rules

table reverse tower trial times numbers executed actions given
successful trials different numbers objects cubes table
data points averages trials created learned rule sets worlds
random seeds standard deviations mean estimators shown ffreplan replan single use hand made modifications original
learned rule sets
objects







planner

success rate

trial time

executed actions

replan
replan
replan single













sst b
sst b
uct
prada
prada







day








replan
replan
replan single











uct
prada
prada





h






replan
replan
replan single











prada
prada










reverse tower
explore limits uct prada prada conducted final series
experiments task reverse towers c cubes requires least c
actions cube needs grabbed put somewhere least apart
long horizon difficult due noise simulated world towers
become unstable topple cubes falling table decrease noise
slightly obtain reliable forbid robot grab objects clear
e objects set limit executed actions trial thereafter
reversed tower still built trial fails trial fails one required
objects falls table
table presents cannot get sst optimal horizon
solve even five cubes although space possible actions reduced
due mentioned restriction sst enormous runtimes b sst
suitable actions leaves goal state several starting situations increased
horizon leads high probability sampling least one unfavorable outcome
required action b single tree traversal sst takes day
found uct require large times order achieve reasonable success
rate therefore set horizons optimal uct worlds cubes uct
optimal success rate taking average


filang toussaint

minutes case success cubes however uct optimal never succeeds
even times exceed hours contrast afford overestimating
horizon prada prada worlds cubes prada prada
achieve success rates respectively less half minute pradas
average number executed actions case success almost optimal worlds
cubes success rates prada prada still taking bit
minute average case success trials fail often due
cubes falling table cannot appropriate actions cubes
falling table main reason success rates prada prada
drop respectively worlds cubes towers become rather unstable
times successful trials however increase minutes indicating
limitations approaches nonetheless mean number executed
actions successful trials still almost optimal prada
overall reverse tower experiments indicate approaches lookahead trees fail tasks require long horizons achieved
plans given huge action state spaces relational domains chances
uct simulates episode exactly required actions successor states
small approaches approximate inference prada prada
crucial advantage stochasticity actions affect runtime
exponentially horizon course search space action sequences still
exponential horizon requiring long horizons hard
solve experiments simple though principled
extension prada gain significant performance improvements
replan fails provide good plans original
learned rule sets surprising characteristics reverse tower task seem
favor replan comparison methods single conjunctive goal
structure number good plans small plans require long horizons
replan replan single indicate replan achieve
good performance adapted rule sets modified hand restrict
number possible actions state constitutes proof concept
replan shows difficulty applying replan learned rule sets

summary
demonstrate successful learned world
form rules may require explicitly account quantification predictive uncertainty concretely methods applying look ahead trees uct approximate
inference prada outperform replan different tasks varying difficulty furthermore prada solve tasks long horizons uct fails
one post processes learned rules hand clarify application contexts
uses conjunctive goal structure requires long plans
replan performs better uct prada


fiplanning noisy probabilistic relational rules

ippc benchmarks
second part evaluation apply proposed approaches benchmarks
latest international probabilistic competition uncertainty part
international competition ippc involved domains differ
many characteristics number actions required horizons
reward structures competition performs
best everywhere thus benchmarks give idea types sst
uct prada may useful convert ppddl domain specifications
nid rules along lines described sec b resulting rule sets used run
implementations sst uct prada benchmark
seven benchmark domains consists instances instance
specifies goal starting state instances vary size
reward structures including action costs direct comparison
possible competition instance considered independently planners
given restricted amount time minutes domain
minutes others cover many repetitions instance
possible maximum trials trials differed random seeds resulting
potentially different state transitions planners evaluated respect
number trials ending goal state collected reward averaged trials
eight planners entered competition including replan official participant discussed related work sec
voluminous presented refer reader website competition provide qualitative comparison methods
planners attempt direct quantitative comparison several reasons first
different hardware prevents timing comparisons second competition participants
frequently able successfully cover trials single instances domain
difficult tell reasons tables planner might
overburdened might faced temporary technical
client server architecture framework competition could cope certain
ppddl constructs could rewritten simpler format
third importantly optimized implementations reuse previous efforts instead fully replan single action within trial
across trials competition evaluation scheme puts replanners disadvantage
particular replan single action instead replanning good strategy
competition spend time starting first trial reuse
resulting insights conditional plans value functions subsequent trials
minimum additional indeed strategy often adopted
many trial time indicate acknowledge fair procedure evaluate
planners compute policies large parts state space acting feel
however counter idea approaches uct prada
meant flexible varying goals different situations thus
interested average time compute good actions successfully solve
instance prior knowledge available


filang toussaint

table benchmarks ippc first column table specifies
instance suc success rate trial time number executed
actions given successful trials applicable reward
trials shown achieved full replanning within trial
across trials
search rescue
planner

suc trial time

actions

c blocksworld

reward

sst
uct

prada
prada











sst
uct
prada
prada
















sst
uct
prada
prada
















actions

reward

uct
prada
prada













sst
uct

prada
prada




















uct
prada
prada















prada
prada












uct
prada
prada















uct
prada
prada







uct
prada
prada















prada
prada










uct
prada
prada













uct
prada
prada













uct
prada
prada











prada
prada












prada
prada













prada
prada















prada
prada















prada
prada
















planner






















uct
prada
prada












uct
prada
prada









uct
prada
prada




















suc trial time





e exploding blocksworld
planner

actions

sst
uct
prada
prada










planner





suc trial time

reward






prada

suc trial time

boxworld

b triangle tireworld
planner

actions

sst
uct

prada
prada








suc trial time





actions

sst
uct

prada
prada








prada
prada









prada
prada









prada
prada









prada
prada






prada
prada









prada
prada


















fiplanning noisy probabilistic relational rules

therefore single instance perform trials different random
seeds full replanning trial aborted goal state reached within
maximum number actions varying slightly benchmark actions
present success rates mean estimators trial times executed actions
rewards standard deviations table instances least
one trial successfully covered reasonable time
search rescue table domain sst branching factor
able plans within reasonable time significantly larger runtimes
uct prada success rates rewards indicate prada aprada superior uct scale rather big instances give idea
w r ippc evaluation scheme uct solves successfully trials first instance
within minutes full replanning prada prada solve trials
full replanning fact despite replanning single action prada prada
success rates best planners benchmark except large
instances within competition participants fsp rbh fsp rdh
achieved comparably satisfactory conjecture success methods
due fact domain requires account carefully outcome probabilities
involve long horizons
triangle tireworld table b domain uct outperforms prada
prada although higher computational cost depth first style
uct seems useful domain give idea w r ippc evaluation
scheme uct performs successful trials first instance within minutes
prada prada achieve trials resp full replanning uct solves
trials difficult instances required horizons increase quickly
instances approaches cannot cope large instances
three competition participants rff bg rff pg hmdpp could cover
methods face required horizons large
number plans non zero probability small becomes evident
blocksworld benchmark table c domain different robot manipulation environment first evaluation sec latter considerably
stochastic provides actions given situation e g may grab objects within
pile blocksworld domain approaches inferior replan
give idea w r ippc evaluation scheme uct perform single successful
trial first instance within minutes prada prada achieve
trials resp full replanning
boxworld domain table approaches exploit fact
delivery boxes almost independent delivery boxes
instances helped intermediate rewards delivered boxes contrast
uct prada prada scale relatively large instances prada
prada solve trials first instance requiring average
min min resp full replanning two competition participants solved
trials successfully domain rff bg rff pg give idea w r ippc
evaluation scheme uct perform single successful trial within minutes
prada completes prada trials small number explained
large plan lengths single action computed full replanning


filang toussaint

finally exploding blocksworld domain table e prada prada
perform better good competition participants give idea w r ippc
evaluation scheme uct achieves single successful trial within minutes
prada prada complete trials resp
perform experiments sysadmin schedule domain ppddl specifications cannot converted nid rules due involved
universal effects contrast possible boxworld domain despite
universal effects boxworld instances universally quantified
variables refer exactly one object exploit conversion nid rules
note understood trick implement deictic references ppddl
means universal effects according action operator however odd semantics
boxes could end two different cities time furthermore ignored
rectangle tireworld domain together triangle tireworld domain makes
tireworlds benchmark instances faulty goal descriptions
include dead critical name winner competition
personally communicated olivier buffet
summary
majority ppddl descriptions ippc benchmarks converted
nid rules indicating broad spectrum covered
nid rules demonstrate approaches perform comparably better
state art planners many traditional hand crafted
hints generality methods probabilistic beyond type robotic
manipulation domains considered sec methods perform particularly well
domains outcome probabilities need carefully accounted face
required horizons large number plans non zero
probability small avoided intermediate rewards

discussion
presented two approaches probabilistic relational rules grounded
domains methods designed work learned rules provide approximate
partial noisy worlds first adaptation uct
samples look ahead trees cope action stochasticity second
called prada uncertainty states explicitly terms beliefs employs
approximate inference graphical combine
existing rule learning intelligent agent learn
compact model dynamics complex noisy environment ii quickly derive appropriate actions varying goals complex simulated robotics domain
methods outperform state art planner replan number different tasks contrast replan methods reason probabilities
action outcomes necessary world dynamics noisy partial
approximate world available
however planners perform remarkably well many traditional probabilistic
demonstrated ippc benchmarks


fiplanning noisy probabilistic relational rules

shown ppddl descriptions converted large extent kind rules
planners use hints general purpose character particularly prada
potential benefits techniques probabilistic instance methods
expected perform similarly well large propositional mdps exhibit
relational structure
far approaches deal reasonable time containing
objects implying billions world states requiring horizons
time steps nonetheless approaches still limited rely
reasoning grounded representation many objects need represented
representation language gets rich approaches need combined
methods reduce state action space complexity lang toussaint b
outlook
current form approximate inference procedure prada relies specific
compact dbns compiled rules development similar factored frontier filters
arbitrary dbns e g derived general ppddl descriptions promising
similarly adaptation pradas factored frontier techniques existing probabilistic
planners worth investigation
probabilistic relational rules backward appears appealing
straightforward learn nid rules regress actions providing reversed triples
rule learning stating predecessor state state action
applied backward combined forward
received lot attention classical may fruitful
look ahead trees well approximate inference means propagating backwards dbns one may ultimately derive calculate
posteriors actions leading true inference instead sampling actions
important direction improving prada make adapt
action sequence sampling strategy experience previous samples introduced simple extension prada achieve sophisticated methods
conceivable learning rule sets online exploiting immediately method important direction future order enable acting
real world want behave effectively right start improving
rule framework efficient effective another interesting issue
instance instead noisy default rule one may use mixture deal
actions several non unique covering rules general use parallel rules work
different hierarchical levels different aspects underlying system

acknowledgments
thank anonymous reviewers careful thorough comments
greatly improved thank sungwook yoon providing us implementation
replan thank olivier buffet answering questions probabilistic
competition work supported german foundation
dfg emmy noether fellowship


filang toussaint

appendix proof proposition
proposition sec set action sequences prada samples non zero
probability super set ones sst uct
proof let action sequence sampled sst uct thus
exists state sequence rule sequence r every state st
action unique covering rule rt predicts successor state st
probability pt pt st would never sampled sst uct
p st case

psample unique covering rule rt st eventually sampled
p obvious assume p st execute
get p st pt p st posterior p st greater
first inequality due persistence previous states non zero probability
lead st given
set action sequences prada samples larger sst uct
sst uct refuses model noise outcomes rules assume action state
state unique covering rule episode
simulated means rule predictions noise outcome action never
sampled sst uct required states never sampled contrast prada
effects noise outcome giving low probability possible
successor states heuristic described

appendix b relation nid rules ppddl
use nid rules sec relational model transition dynamics probabilistic actions besides allowing negative literals preconditions nid rules extend
probabilistic strips operators kushmerick et al blum langford two
special constructs namely deictic references noise outcomes crucial learning compact rule sets alternative language specify probabilistic relational
used international probabilistic competitions ippc
probabilistic domain definition language ppddl younes littman
ppddl probabilistic extension subset pddl derived deterministic
action description language adl adl turn introduced universal conditional
effects negative precondition literals deterministic strips representation
thus ppddl allows usage syntactic constructs beyond expressive
power nid rules however many ppddl descriptions converted nid rules
taking closer look convert ppddl nid rule representations
clarify meant action formalisms giving
intuition line thinking understand abstract
action abstract action predicate e g pickup x intuitively defines certain type
action stochastic state transitions according abstract action specified
abstract nid rules well abstract ppddl action operators called schemata
typically several different abstract nid rules model abstract action specifying
state transitions different contexts contrast usually one abstract ppddl action


fiplanning noisy probabilistic relational rules

operator used model abstract action context dependent effects modeled
means conditional universal effects
make predictions specific situation concrete action grounded action
predicate pickup greencube strategy within nid rule framework
ground set abstract nid rules examine ground rules cover state action
pair exactly one ground rule chosen prediction
rule one contexts nid rules mutually
exclusive one chooses noisy default rule essentially saying one know
happen strategies conceivable pursued contrast
usually exactly one operator per abstract action ppddl domains
need concept operator uniqueness distinguish ground actions
operators
b converting ppddl nid rules
following discuss convert ppddl features nid rule representation
may impossible convert ppddl action operator single nid rule
one may often translate set rules polynomial increase size
representation table provides example converted ppddl action operator
ippc domain exploding blocksworld nid rules support many
features sophisticated domain description language ppddl provides rules
lead compact representations possible domains experiments however
dynamics many interesting domains specified compactly
furthermore additional expressive power rule contexts gained derived
predicates allow bring kinds logical formulas quantification
conditional effects conditional effect ppddl operator takes form c
e accounted two nid rules first rule adds c context
e outcomes second adds c context ignores e
universal effects ppddl allows define universal effects specify effects
objects meet preconditions example reboot action sysadmin
domain ippc competition specifies every computer one
rebooted independently go probability connected computer
already cannot expressed nid rule framework
refer objects action arguments via deictic references require
deictic references unique reboot action would need unique way refer
computer cannot achieved without significant modifications
example enumerating computers via separate predicates
disjunctive preconditions quantification ppddl operators allow disjunctive preconditions including implications instance search rescue domain
ippc competition defines action operator goto x precondition
x base humanalive disjunction b b accounted
two nid rules first rule context second
rule b alternatively one may introduce derived predicate c b
general trick derived predicates allows overcome syntactical limitations nid


filang toussaint

table example converting ppddl action operator nid rules putdownoperator ippc benchmark domain exploding blocksworld contains
conditional effect accounted two nid rules exclude
b include c condition context
action putdown



parameters b block
precondition holding b nodestroyedt able
ef f ect emptyhand ont able b holding b
probabilistic nodetonated b nodestroyedt able nodetonated b

b
putdown x block x holding x nodestroyedt able nodetonated x

emptyhand x ont able x holding x

c
putdown x block x holding x nodestroyedt able nodetonated x

emptyhand x ont able x holding x

emptyhand x ont able x holding x nodestroyedt able nodetonated x

rules bring kinds logical formulas quantifications discussed
pasula et al derived predicates important prerequisite able learn
compact accurate rules
types terms may typed ppddl e g drivet c city typing objects
variables predicates functions achieved nid rules usage typing
predicates within context e g additional predicate city c
state transition rewards ppddl one encode markovian rewards associated
state transitions including action costs negative rewards fluents update
rules action effects one achieve nid rules associating rewards
outcomes rules
b converting nid rules ppddl
following way nid rules used sst uct prada
time handled via polynomial blowup representational size
basic building blocks nid rule e context well outcomes transfer
one one ppddl action operators deictic references uniqueness requirement
covering rules noise outcome need special attention
deictic references deictic references nid rules allow refer objects
action arguments ppddl one refer objects means universal
conditional effects important restriction however deictic reference needs
pick single unique object order apply picks none many rule fails
apply two ways ensure uniqueness requirement within ppddl first


fiplanning noisy probabilistic relational rules

allowing quantified preconditions explicit uniqueness precondition deictic
reference introduced universal quantification constrains objects
satisfying preconditions identical e x x
x variables alternatively uniqueness deictic references
achieved careful specification however cannot
guaranteed learning rules
uniqueness covering rules contexts nid rules mutually
exclusive want use rule prediction need ensure
uniquely covers given state action pair procedural evaluation process nid
rules encoded declaratively ppddl modified conditions explicitly
negate contexts competing rules instance three nid rules
potentially overlapping contexts b c propositional simplicity ppddl
action operator may define four conditions c b c c b c
c b c c b c b c b c conditions c
c c test uniqueness corresponding nid rules subsume outcomes
condition c tests non uniqueness covering rule multiple covering rules
potential changes noise analogous situations nid rule context
noisy default rule would used
noise outcome noise outcome nid rule subsumes seldom utterly complex
outcomes relaxes frame assumption even explicitly stated things may change
certain probability comes price difficulty ensure well defined
successor state distribution p contrast ppddl needs explicitly specify
everything might change may important reason difficult come
effective learning ppddl
principle ppddl provide noise outcome way approaches
account encoded ppddl treat noise outcome
effects sst uct basically noop operator trivially
translated ppddl consider probability state attribute change
independently prada encoded ppddl independent universal
probabilistic effects
noise outcome allows make predictions arbitrary action
multiple covering rules may use albeit informative prediction
default rule cases dealt ppddl action operators explicit
conditions described previous paragraph

references
blum langford j probabilistic graphplan framework
proc fifth european conference ecp pp
botvinick j goal directed decision making prefrontal cortex
computational framework advances neural information processing systems
nips pp


filang toussaint

boutilier c dean hanks decision theoretic structural assumptions computational leverage journal artificial intelligence

boutilier c reiter r price b symbolic dynamic programming first order
mdps proc int conf artificial intelligence ijcai pp
buffet aberdeen factored policy gradient planner artificial intelligence journal
cooper g method belief networks influence diagrams proc
fourth workshop uncertainty artificial intelligence pp
croonenborghs ramon j blockeel h bruynooghe online learning
exploiting relational reinforcement learning proc int conf
artificial intelligence ijcai pp
domshlak c hoffmann j probabilistic via heuristic forward search
weighted model counting journal artificial intelligence
driessens k ramon j gartner graph kernels gaussian processes
relational reinforcement learning machine learning
dzeroski de raedt l driessens k relational reinforcement learning
machine learning
fern yoon givan r approximate policy iteration policy language
bias solving relational markov decision processes journal artificial intelligence

gardiol n h kaelbling l p envelope relational mdps
proc conf neural information processing systems nips
gardiol n h kaelbling l p action space partitioning proc
aaai conf artificial intelligence aaai pp
gardiol n h kaelbling l p adaptive envelope mdps relational
equivalence tech rep mit csail tr mit cs ai lab
cambridge
gelly silver combining online offline knowledge uct proc
int conf machine learning icml pp
gretton c thiebaux exploiting first order rgeression inductive policy
selection proc conf uncertainty artificial intelligence uai pp

grush r conscious thought simulation behaviour perception behaviorial
brain sciences


fiplanning noisy probabilistic relational rules

halbritter f geibel p learning relational mdps graph kernels
proc mexican conference artificial intelligence micai pp
hesslow g conscious thought simulation behaviour perception trends
cognitive science
hoffmann j nebel b system fast plan generation
heuristic search journal artificial intelligence
holldobler skvortsova logic dynamic programming
aaai workshop learning mdps pp
ippc


sixth international competition
http ippc loria fr wiki index php main page

uncertainty

part

jensen f introduction bayesian networks springer verlag york
joshi kersting k khardon r generalized first order decision diagrams
first order mdps proc int conf artificial intelligence ijcai pp

karabaev e skvortsova heuristic search solving first order
mdps proc conf uncertainty artificial intelligence uai pp

kearns j mansour ng sparse sampling nearoptimal large markov decision processes machine learning

kersting k driessens k nonparametric policy gradients unified treatment propositional relational domains proc int conf machine
learning icml pp
kersting k van otterlo de raedt l bellman goes relational proc
int conf machine learning icml pp
kocsis l szepesvari c bandit monte carlo proc
european conf machine learning ecml pp
kushmerick n hanks weld probabilistic
artificial intelligence
kuter u nau reisner e goldman r p classical planners
solve nondeterministic proc int conf automated
scheduling icaps pp
lang toussaint approximate inference stochastic relational worlds proc int conf machine learning icml pp
lang toussaint b relevance grounding relational domains
proc european conf machine learning ecml pp


filang toussaint

little thiebaux probabilistic vs replanning icaps workshop
international competition past present future
littman l goldsmith j mundhenk computational complexity
probabilistic journal artificial intelligence
murphy k p dynamic bayesian networks representation inference learning ph thesis uc berkeley
murphy k p weiss factored frontier approximate inference dbns proc conf uncertainty artificial intelligence uai
pp
pasula h zettlemoyer l kaelbling l p learning symbolic
stochastic domains journal artificial intelligence
poon h domingos p sound efficient inference probabilistic
deterministic dependencies proc aaai conf artificial intelligence
aaai
sanner boutilier c approximate solution techniques factored first order
mdps proc int conf automated scheduling icaps
pp
sanner boutilier c practical solution techniques first order mdps
artificial intelligence
shachter r probabilistic inference influence diagrams operations

sutton r barto g reinforcement learning introduction mit
press
teichteil konigsbuch f kuter u infantes g aggregation generating
policies mdps appear proc int conf autonomous agents
multiagent systems
toussaint storkey probabilistic inference solving discrete continuous state markov decision processes proc int conf machine learning
icml pp
toussaint storkey harmeling expectation maximization methods
solving po mdps optimal control chiappa barber
eds inference learning dynamic cambridge university press
van otterlo logic adaptive behavior ios press amsterdam
walsh j efficient learning relational sequential decision making
ph thesis rutgers state university jersey brunswick nj


fiplanning noisy probabilistic relational rules

wang c joshi khardon r first order decision diagrams relational
mdps journal artificial intelligence
weld recent advances ai ai magazine
wu j h kalyanam r givan r stochastic enforced hill climbing proc
int conf automated scheduling icaps pp
yoon w fern givan r replan baseline probabilistic
proc int conf automated scheduling icaps pp

yoon w fern givan r kambhampati probabilistic via
determinization hindsight proc aaai conf artificial intelligence
aaai pp
younes h l littman l ppddl extension pddl expressing
domains probabilistic effects tech rep carnegie mellon university




