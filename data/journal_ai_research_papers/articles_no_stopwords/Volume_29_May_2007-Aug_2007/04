Journal Artificial Intelligence Research 29 (2007) 105-151

Submitted 05/06; published 06/07

Combination Strategies Semantic Role Labeling
Mihai Surdeanu
Llus Marquez
Xavier Carreras
Pere R. Comas

surdeanu@lsi.upc.edu
lluism@lsi.upc.edu
carreras@lsi.upc.edu
pcomas@lsi.upc.edu

Technical University Catalonia,
C/ Jordi Girona, 1-3
08034 Barcelona, SPAIN

Abstract
paper introduces analyzes battery inference models problem semantic role labeling: one based constraint satisfaction, several strategies model
inference meta-learning problem using discriminative classiers. classiers
developed rich set novel features encode proposition sentence-level
information. knowledge, rst work that: (a) performs thorough analysis learning-based inference models semantic role labeling, (b) compares several
inference strategies context. evaluate proposed inference strategies
framework CoNLL-2005 shared task using automatically-generated syntactic
information. extensive experimental evaluation analysis indicates
proposed inference strategies successful outperform current best results
reported CoNLL-2005 evaluation exercise proposed approaches
advantages disadvantages. Several important traits state-of-the-art SRL combination strategy emerge analysis: (i) individual models combined
granularity candidate arguments rather granularity complete solutions;
(ii) best combination strategy uses inference model based learning; (iii)
learning-based inference benets max-margin classiers global feedback.

1. Introduction
Natural Language Understanding (NLU) subeld Articial Intelligence (AI)
deals extraction semantic information available natural language texts.
knowledge used develop high-level applications requiring textual document
understanding, Question Answering Information Extraction. NLU complex
AI-complete problem needs venture well beyond syntactic analysis natural
language texts. state art NLU still far reaching goals, recent
research made important progress subtask NLU: Semantic Role Labeling.
task Semantic Role Labeling (SRL) process detecting basic event structures
whom, where. See Figure 1 sample sentence annotated
event frame.
1.1 Motivation
SRL received considerable interest past years (Gildea & Jurafsky, 2002;
Surdeanu, Harabagiu, Williams, & Aarseth, 2003; Xue & Palmer, 2004; Pradhan, Hac
2007
AI Access Foundation. rights reserved.

fiSurdeanu, Marquez, Carreras, & Comas

cioglu, Krugler, Ward, Martin, & Jurafsky, 2005a; Carreras & Marquez, 2005).
shown identication event frames signicant contribution many
NLU applications Information Extraction (Surdeanu et al., 2003), Question Answering (Narayanan & Harabagiu, 2004), Machine Translation (Boas, 2002), Summarization (Melli, Wang, Liu, Kashani, Shi, Gu, Sarkar, & Popowich, 2005), Coreference
Resolution (Ponzetto & Strube, 2006b, 2006a).
syntactic perspective, machine-learning SRL approaches classied
one two classes: approaches take advantage complete syntactic analysis text,
pioneered Gildea Jurafsky (2002), approaches use partial syntactic analysis,
championed previous evaluations performed within Conference Computational
Natural Language Learning (CoNLL) (Carreras & Marquez, 2004, 2005). wisdom
extracted rst representation indicates full syntactic analysis signicant
contribution SRL performance, using hand-corrected syntactic information (Gildea
& Palmer, 2002). hand, automatically-generated syntax available,
quality information provided full syntax decreases state-ofthe-art full parsing less robust performs worse tools used partial
syntactic analysis. real-world conditions, dierence two SRL
approaches (with full partial syntax) high. interestingly, two SRL
strategies perform better different semantic roles. example, models use full
syntax recognize agent theme roles better, whereas models based partial syntax
better recognizing explicit patient roles, tend farther predicate
accumulate parsing errors (Marquez, Comas, Gimenez, & Catala, 2005).
1.2 Approach
article explore implications observations studying strategies
combining output several independent SRL systems, take advantage
dierent syntactic views text. given sentence, combination models receive
labeled arguments individual systems, produce overall argument structure
corresponding sentence. proposed combination strategies exploit several levels
information: local global features (from individual models) constraints
argument structure. work, investigate three dierent approaches:
rst combination model parameters estimate; makes use
argument probabilities output individual models constraints argument
structures build overall solution sentence. call model inference
constraint satisfaction.
second approach implements cascaded inference model local learning: rst,
type argument, classier trained oine decides whether candidate
nal argument. Next, candidates passed previous step
combined solution consistent constraints argument structures.
refer model inference local learning.
third inference model global: number online ranking functions, one
argument type, trained score argument candidates correct
argument structure complete sentence globally ranked top. call
model inference global learning.
106

fiCombination Strategies Semantic Role Labeling



NP

NP

VP
NP

PP

luxury auto maker last year sold 1,214 cars U.S.
A0
Agent

AMTMP
Temporal
Marker

P

A1
Predicate Object

AMLOC
Locative
Marker

Figure 1: Sample sentence PropBank corpus.
proposed combination strategies general depend way
candidate arguments collected. empirically prove experimenting
individual SRL systems developed house, 10 best systems
CoNLL-2005 shared task evaluation.
1.3 Contribution
work introduced paper several novel points. knowledge,
rst work thoroughly explores inference model based meta-learning (the
second third inference models introduced) context SRL. investigate metalearning combination strategies based rich, global representations form local
global features, form structural constraints solutions. empirical
analysis indicates combination strategies outperform current state
art. Note combination strategies proposed paper re-ranking
approaches (Haghighi, Toutanova, & Manning, 2005; Collins, 2000). Whereas re-ranking
selects overall best solution pool complete solutions individual models,
combination approaches combine candidate arguments, incomplete solutions,
different individual models. show approach better potential, i.e., upper
limit F1 score higher performance better several corpora.
second novelty paper performs comparative analysis several combination strategies SRL, using framework i.e., pool
candidates evaluation methodology. large number combination
approaches previously analyzed context SRL larger context
predicting structures natural language texts e.g., inference based constraint satisfaction (Koomen, Punyakanok, Roth, & Yih, 2005; Roth & Yih, 2005), inference based
local learning (Marquez et al., 2005), re-ranking (Collins, 2000; Haghighi et al., 2005) etc.
still clear strategy performs best semantic role labeling. paper
107

fiSurdeanu, Marquez, Carreras, & Comas

provide empirical answers several important questions respect. example,
combination strategy based constraint satisfaction better inference model based
learning? Or, important global feedback learning-based inference model?
analysis indicates following issues important traits state-of-the-art
combination SRL system: (i) individual models combined argument granularity
rather granularity complete solutions (typical re-ranking); (ii) best
combination strategy uses inference model based learning; (iii) learning-based
inference benets max-margin classiers global feedback.
paper organized follows. Section 2 introduces semantic corpora used
training evaluation. Section 3 overviews proposed combination approaches.
individual SRL models introduced Section 4 evaluated Section 5. Section 6
lists features used three combination models introduced paper.
combination models described Section 7. Section 8 introduces empirical analysis proposed combination methods. Section 9 reviews related work
Section 10 concludes paper.

2. Semantic Corpora
paper used PropBank, approximately one-million-word corpus annotated
predicate-argument structures (Palmer, Gildea, & Kingsbury, 2005). date, PropBank addresses predicates lexicalized verbs. Besides predicate-argument structures,
PropBank contains full syntactic analysis sentences, extends Wall Street
Journal (WSJ) part Penn Treebank, corpus previously annotated
syntactic information (Marcus, Santorini, & Marcinkiewicz, 1994).
given predicate, survey carried determine predicate usage, and,
required, usages divided major senses. However, senses divided
syntactic grounds semantic, following assumption syntactic frames
direct reection underlying semantics. arguments predicate numbered sequentially A0 A5. Generally, A0 stands agent, A1 theme direct
object, A2 indirect object, benefactive instrument, semantics tend verb
specic. Additionally, predicates might adjunctive arguments, referred AMs.
example, AM-LOC indicates locative AM-TMP indicates temporal. Figure 1 shows
sample PropBank sentence one predicate (sold) 4 arguments. regular
adjunctive arguments discontinuous, case trailing argument fragments
prexed C-, e.g., [A1 funds] [predicate expected] [CA1 begin operation
around March 1]. Finally, PropBank contains argument references (typically pronominal),
share label actual argument prexed R-.1
paper use syntactic information Penn Treebank. Instead,
develop models using automatically-generated syntax named-entity (NE) labels,
made available CoNLL-2005 shared task evaluation (Carreras & Marquez, 2005).
CoNLL data, use syntactic trees generated Charniak parser (Char1. original PropBank annotations, co-referenced arguments appear single item, differentiation referent reference. use version data used CoNLL
shared tasks, reference arguments automatically separated corresponding referents
simple pattern-matching rules.

108

fiCombination Strategies Semantic Role Labeling

niak, 2000) develop two individual models based full syntactic analysis, chunk
i.e., basic syntactic phrase labels clause boundaries construct partial-syntax
model. individual models use provided NE labels.
Switching hand-corrected automatically-generated syntactic information means
PropBank assumption argument (or argument fragment discontinuous arguments) maps one syntactic phrase longer holds, due errors syntactic
processors. analysis PropBank data indicates 91.36% semantic
arguments matched exactly one phrase generated Charniak parser. Essentially, means SRL approaches make assumption semantic
argument maps one syntactic construct recognize almost 9% arguments.
statement made approaches based partial syntax caveat
setup arguments match sequence chunks. However, one expects
degree compatibility syntactic chunks semantic arguments higher
due ner granularity syntactic elements chunking algorithms perform better full parsing algorithms. Indeed, analysis PropBank data
supports observation: 95.67% semantic arguments matched sequence
chunks generated CoNLL syntactic chunker.
Following CoNLL-2005 setting evaluated system PropBank
fresh test set, derived Brown corpus. second evaluation allows us
investigate robustness proposed combination models.

3. Overview Combination Strategies
paper introduce analyze three combination strategies problem
semantic role labeling. three combination strategies implemented shared
framework detailed Figure 2 consists several stages: (a) generation candidate arguments, (b) candidate scoring, nally (c) inference. clarity, describe
rst proposed combination framework, i.e., vertical ow Figure 2. Then, move
overview three combination methodologies, shown horizontally Figure 2.
candidate generation step, merge solutions three individual SRL models
unique pool candidate arguments. individual SRL models range complete
reliance full parsing using partial syntactic information. example, Model 1
developed sequential tagger (using B-I-O tagging scheme) partial
syntactic information (basic phrases clause boundaries), whereas Model 3 uses full
syntactic analysis text handles arguments map exactly one syntactic
constituent. detail individual SRL models Section 4 empirically evaluate
Section 5.
candidate scoring phrase, re-score candidate arguments using local
information, e.g., syntactic structure candidate argument, global information,
e.g., many individual models generated similar candidate arguments. describe
features used candidate scoring Section 6.
Finally, inference stage combination models search best solution
consistent domain constraints, e.g., two arguments predicate cannot
overlap embed, predicate may one core argument (A0-5), etc.
109

fiSurdeanu, Marquez, Carreras, & Comas

Reliance full syntax

Model 1

Model 2

Model 3

Candidate
Generation
Candidate Argument
Pool

Constraint
Satisfaction
Engine

Solution

Inference
Constraint Satisfaction

Learning
(batch)

Learning
(online)

Dynamic
Programming
Engine

Dynamic
Programming
Engine

Solution

Candidate
Scoring

Inference

Solution

Inference
Local Learning

Inference
Global Learning

Figure 2: Overview proposed combination strategies.

combination approaches proposed paper share candidate argument pool. guarantees results obtained dierent strategies
corpus comparable. hand, even though candidate generation
step shared, three combination methodologies dier signicantly scoring
inference models.
rst combination strategy analyzed, inference constraint satisfaction, skips
candidate scoring step completely uses instead probabilities output individual SRL models candidate argument. individual models raw activations
actual probabilities convert probabilities using softmax function (Bishop,
1995), passing inference component. inference implemented using
Constraint Satisfaction model searches solution maximizes certain
compatibility function. compatibility function models probability
global solution consistency solution according domain constraints.
combination strategy based technique presented Koomen et al. (2005).
main dierence two systems candidate generation step: use
three independent individual SRL models, whereas Komen et al. used SRL model
110

fiCombination Strategies Semantic Role Labeling

trained dierent syntactic views data, i.e., top parse trees generated
Charniak Collins parsers (Charniak, 2000; Collins, 1999). Furthermore, take
argument candidates set complete solutions generated individual models, whereas Komen et al. take dierent syntactic trees, constructing
complete solution. obvious advantage inference model Constraint Satisfaction unsupervised: learning necessary candidate scoring,
scores individual models used. hand, Constraint Satisfaction
model requires individual models provide raw activations, and, moreover,
raw activations convertible true probabilities.
second combination strategy proposed article, inference local learning,
re-scores candidates pool using set binary discriminative classiers.
classiers assign argument score measuring condence argument
part correct, global solution. classiers trained batch mode
completely decoupled inference module. inference component implemented
using CKY-based dynamic programming algorithm (Younger, 1967). main advantage
strategy candidates re-scored using signicantly information
available individual model. example, incorporate features count
number individual systems generated given candidate argument, several
types overlaps candidate arguments predicate arguments
predicates, structural information based full partial syntax, etc.
describe rich feature set used scoring candidate arguments Section 6. Also,
combination approach depend argument probabilities individual
SRL models (but incorporate features, available). combination approach
complex previous strategy additional step requires
supervised learning: candidate scoring. Nevertheless, mean additional
corpus necessary: using cross validation, candidate scoring classiers trained
corpus used train individual SRL models. Moreover, show Section 8
obtain excellent performance even candidate scoring classiers trained
signicantly less data individual SRL models.
Finally, inference strategy global learning investigates contribution global
information inference model based learning. strategy incorporates global
information previous inference model two ways. First importantly, candidate scoring trained online global feedback inference component.
words, online learning algorithm corrects mistakes found comparing
correct solution one generated inference. Second, integrate global information actual inference component: instead performing inference proposition
independently, whole sentence once. allows implementation
additional global domain constraints, e.g., arguments attached dierent predicates
overlap.
combination strategies proposed described detail Section 7 evaluated
Section 8.
111

fiSurdeanu, Marquez, Carreras, & Comas

4. Individual SRL Models
section introduces three individual SRL models used combination strategies discussed paper. rst two models variations algorithm:
model SRL problem sequential tagging task, semantic argument
matched sequence non-embedding phrases, Model 1 uses partial syntax
(chunks clause boundaries), whereas Model 2 uses full syntax. third model takes
traditional approach assuming exists one-to-one mapping
semantic arguments syntactic phrases.
important note combination strategies introduced later paper
independent individual SRL models used. fact, Section 8 describe
experiments use individual models best performing SRL
systems CoNLL-2005 evaluation (Carreras & Marquez, 2005). Nevertheless,
choose focus mainly individual SRL approaches presented section
completeness show state-of-the-art performance possible relatively simple
SRL models.
4.1 Models 1 2
models approach SRL sequential tagging task. pre-processing step,
input syntactic structures traversed order select subset constituents organized
sequentially (i.e., non embedding). output process sequential tokenization
input sentence verb predicates. Labeling tokens appropriate
tags allows us codify complete argument structure predicate sentence.
precisely, given verb predicate, sequential tokens selected follows:
First, input sentence split disjoint sequential segments using markers
segment start/end verb position boundaries clauses include
corresponding predicate constituent. Second, segment, set top-most
non-overlapping syntactic constituents completely falling inside segment selected
tokens. Finally, tokens labeled B-I-O tags, depending
beginning, inside, outside predicate argument. Note strategy provides set
sequential tokens covering complete sentence. Also, independent syntactic
annotation explored, assuming provides clause boundaries.
Consider example Figure 3, depicts PropBank annotation two verb
predicates sentence (release hope) corresponding partial full parse
trees. Since verbs main clause sentence, two segments
sentence considered predicates, i.e., dening left right contexts
verbs ([w1 :Others, ..., w3 :just] [w5 :from, ..., w20 :big-time] predicate release,
[w1 :Others, ..., w8 :,] [w10 :the, ..., w20 :big-time] predicate hope). Figure 4
shows resulting tokenization predicates two alternative syntactic structures. case, correct argument annotation recovered cases, assuming
perfect labeling tokens.
worth noting resulting number tokens annotate much lower
number words cases. Also, codications coming full parsing
substantially fewer tokens coming partial parsing. example,
predicate hope, dierence number tokens two syntactic views
112

fiCombination Strategies Semantic Role Labeling

Clause
NP



6

VP
Clause

VP
1

NP

3

ADVP

4

II

NP

PP

VP

,

,

5

2

Others ,

released majors , hope senior league

bridge back bigtime.

Clause
8

1

NP



3

ADVP

2 , II
Others ,

A1

AMTMP

III

VP

IV 4

PP

V

5

NP

VI

7

VP

NP

,
6 VII
released majors , hope senior league
P
A0

Clause

VP

NP

VIII

ADVP PP

NP

bridge back bigtime.

A2
P

A1

Figure 3: Annotation example sentence two alternative syntactic structures.
lower tree corresponds partial parsing annotation (PP) base chunks
clause structure, upper represents full parse tree (FP). Semantic roles
two predicates (release hope) provided sentence.
encircled nodes trees correspond selected nodes process
sequential tokenization sentence. mark selected nodes
predicate release Western numerals nodes selected hope
Roman numerals. See Figure 4 details.

particularly large (8 vs. 2 tokens). Obviously, coarser token granularity, easier
problem assigning correct output labelings (i.e., less tokens label
long-distance relations among sentence constituents better captured).
hand, coarser granularity tends introduce unrecoverable errors
pre-processing stage. clear trade-o, dicult solve advance.
using two models combination scheme take advantage diverse sentence
tokenizations (see Sections 7 8).
Compared common tree node labeling approaches (e.g., following
Model 3), B-I-O annotation tokens advantage permitting correctly annotate arguments match unique syntactic constituent. bad side,
heuristic pre-selection candidate nodes predicate, i.e., nodes
sequentially cover sentence, makes number unrecoverable errors higher. Another source errors common strategies errors introduced real partial/full
parsers. calculated due syntactic errors introduced pre-processing
stage, upper-bound recall gures 95.67% Model 1 90.32% Model 2 using
datasets dened Section 8.
113

fiSurdeanu, Marquez, Carreras, & Comas

words
1: Others
2: ,
3:
4: released
5:
6:
7: majors
8: ,
9: hope
10:
11: senior
12: league
13:
14:
15:
16: bridge
17: back
18:
19:
20: big-time

releasePP
1: B A1
2:
3: B AM-TMP

4: B A2
5: A2
6:
7:

tokens
releaseFP
hopePP
1: B A1
I: B A0
2:
II: A0
3: B AM-TMP
III: A0

IV: A0
V: A0
4: B A2
VI: A0
5:

hopeFP

I: B A0

VII: A0




VIII: B A1

II: B A1

6:
8:

Figure 4: Sequential tokenization sentence Figure 3 according two syntactic
views predicates (PP stands partial parsing FP full parsing).
sentence semantic role annotations vertically displayed. token
numbered indexes appear tree nodes Figure 3 contains
B-I-O annotation needed codify proper semantic role structure.

Approaching SRL sequential tagging task new. Hacioglu, Pradhan, Ward,
Martin, Jurafsky (2004) presented system based sequential tagging base chunks
B-I-O labels, best performing SRL system CoNLL-2004 shared
task (Carreras & Marquez, 2004). novelty approach resides fact
sequence syntactic tokens label extracted hierarchical syntactic annotation
(either partial full parse tree) restricted base chunks (i.e., token
may correspond complex syntactic phrase even clause).
4.1.1 Features
tokens selected labeled B-I-O tags, converted training
examples considering rich set features, mainly borrowed state-of-the-art systems (Gildea & Jurafsky, 2002; Carreras, Marquez, & Chrupala, 2004; Xue & Palmer,
2004). features codify properties from: (a) focus token, (b) target predicate,
(c) sentence fragment token predicate, (d) dynamic context,
i.e., B-I-O labels previously generated. describe four feature sets next.2
2. Features extracted partial parsing Named Entities common Model 1 2, features
coming full parse trees apply Model 2.

114

fiCombination Strategies Semantic Role Labeling

Constituent structure features:
Constituent type head: extracted using head-word rules Collins (1999).
rst element PP chunk, head rst NP extracted.
example, type constituent U.S. Figure 1 PP, head
U.S. instead in.
First last words POS tags constituent, e.g., in/IN U.S./NNP
constituent U.S. Figure 1.
POS sequence: less 5 tags long, e.g., INDTNNP sample
constituent.
2/3/4-grams POS sequence.
Bag-of-words nouns, adjectives, adverbs. example, bag-of-nouns
constituent luxury auto maker {luxury, auto, maker}.
TOP sequence: sequence types top-most syntactic elements constituent
(if less 5 elements long). case full parsing corresponds
right-hand side rule expanding constituent node. example, TOP
sequence constituent U.S. INNP.
2/3/4-grams TOP sequence.
Governing category described Gildea Jurafsky (2002), indicates NP
arguments dominated sentence (typical subjects) verb phrase (typical
objects). example, governing category constituent 1,214 cars
Figure 1 VP, hints corresponding semantic role object.
NamedEntity, indicating constituent embeds strictly matches named entity
along type. example, constituent U.S. embeds locative
named entity: U.S..
TMP, indicating constituent embeds strictly matches temporal keyword
(automatically extracted AM-TMP arguments training set). Among
common temporal cue words extracted are: year, yesterday, week, month,
etc. used total 109 cue words.
Previous following words POS tag constituent. example,
previous word constituent last year Figure 1 maker/NN, next
one sold/VBD.
features characterizing focus constituents extracted two previous
following tokens, provided inside boundaries current segment.
Predicate structure features:
Predicate form, lemma, POS tag, e.g., sold, sell, VBD predicate
Figure 1.
Chunk type cardinality verb phrase verb included: single-word
multi-word. example, predicate Figure 1 included single-word VP
chunk.
115

fiSurdeanu, Marquez, Carreras, & Comas

predicate voice. distinguish voice types: active, passive, copulative,
innitive, progressive.
Binary ag indicating verb start/end clause.
Sub-categorization rule, i.e., phrase structure rule expands predicates
immediate parent, e.g., NP NP VP predicate Figure 1.
Predicate-constituent features:
Relative position, distance words chunks, level embedding (in number
clause-levels) respect constituent. example, constituent
U.S. Figure 1 appears predicate, distance 2 words 1 chunk,
level embedding 0.
Constituent path described Gildea Jurafsky (2002) 3/4/5-grams
path constituents beginning verb predicate ending constituent.
example, syntactic path constituent luxury auto maker
predicate sold Figure 1 NP VP VBD.
Partial parsing path described Carreras et al. (2004) 3/4/5-grams path
elements beginning verb predicate ending constituent. example,
path NP + PP + NP + VP VBD indicates current NP token
predicate PP, NP, constituents right (positive sign)
level token path descends clause VP
nd predicate. dierence previous constituent path
arrows anymore introduce horizontal (left/right) movements
syntactic level.
Syntactic frame described Xue Palmer (2004). syntactic frame captures
overall sentence structure using predicate constituent pivots.
example, syntactic frame predicate sold constituent
U.S. NPNPVPNPPP, current predicate constituent emphasized.
Knowing noun phrases predicate lowers probability
constituent serves agent (or A0).
Dynamic features:
BIOtag previous token. training, correct labels left context
used. testing, feature dynamically codied tag previously
assigned SRL tagger.
4.1.2 Learning Algorithm Sequence Tagging
used generalized AdaBoost real-valued weak classiers (Schapire & Singer, 1999)
base learning algorithm. version algorithm learns xed-depth small decision
trees weak rules, combined ensemble constructed AdaBoost.
implemented simple one-vs-all decomposition address multi-class classication.
way, separate binary classier learned B-X I-X argument label
plus extra classier decision.
116

fiCombination Strategies Semantic Role Labeling

AdaBoost binary classiers used labeling test sequences, left right,
using recurrent sliding window approach information tags assigned
preceding tokens. explained previous list features, left tags already assigned
dynamically codied features. Empirically, found optimal left context
taken account reduces previous token.
tested two dierent tagging procedures. First, greedy left-to-right assignment
best scored label token. Second, Viterbi search label sequence
maximizes probability complete sequence. case, classiers predictions
converted probabilities using softmax function described Section 7.1.
signicant improvements obtained latter. selected former,
faster, basic tagging algorithm experiments.
Finally, tagging model enforces three basic constraints: (a) B-I-O output labeling must codify correct structure; (b) arguments cannot overlap clause chunk
boundaries; (c) verb, A0-5 arguments present PropBank frames (taking
union rolesets dierent verb senses) considered.
4.2 Model 3
third individual SRL model makes strong assumption predicate argument
maps one syntactic constituent. example, Figure 1 A0 maps noun phrase,
AM-LOC maps prepositional phrase, etc. assumption holds well hand-corrected
parse trees simplies signicantly SRL process one syntactic constituent correctly classied order recognize one semantic argument.
hand, approach limited using automatically-generated syntactic trees.
example, 91.36% arguments mapped one syntactic constituents
produced Charniak parser.
Using bottom-up approach, Model 3 maps argument rst syntactic constituent exact boundaries climbs high possible
tree across unary production chains. currently ignore arguments map
single syntactic constituent. argument-constituent mapping performed
training set preprocessing step. Figure 1 shows mapping example semantic
arguments one verb corresponding sentence syntactic structure.
mapping process completes, Model 3 extracts rich set lexical, syntactic,
semantic features. features inspired previous work parsing
SRL (Collins, 1999; Gildea & Jurafsky, 2002; Surdeanu et al., 2003; Pradhan et al.,
2005a). describe complete feature set implemented Model 3 next.
4.2.1 Features
Similarly Models 1 2 group features three categories, based properties
codify: (a) argument constituent, (b) target predicate, (c) relation
constituent predicate syntactic constituents.
Constituent structure features:
syntactic label candidate constituent.

constituent head word, suffixes length 2, 3, 4, lemma, POS tag.
117

fiSurdeanu, Marquez, Carreras, & Comas

constituent content word, suffixes length 2, 3, 4, lemma, POS tag, NE
label. Content words, add informative lexicalized information dierent
head word, detected using heuristics Surdeanu et al. (2003). example,
head word verb phrase placed auxiliary verb had, whereas
content word placed. Similarly, content word prepositional phrases
preposition (which selected head word), rather head
word attached phrase, e.g., U.S. prepositional phrase U.S..
first last constituent words POS tags.
NE labels included candidate phrase.
Binary features indicate presence temporal cue words, i.e., words appear
often AM-TMP phrases training. used list temporal cue words
Models 1 2.
Treebank syntactic label added feature indicate number
labels included candidate phrase.
TOP sequence constituent (constructed similarly Model 2).
phrase label, head word POS tag constituent parent, left sibling,
right sibling.
Predicate structure features:
predicate word lemma.
predicate voice. denition Models 1 2.
binary feature indicate predicate frequent (i.e., appears
twice training data) not.
Sub-categorization rule. denition Models 1 2.
Predicate-constituent features:
path syntactic tree argument phrase predicate
chain syntactic labels along traversal direction (up down).
computed similarly Model 2.
length syntactic path.
number clauses (S* phrases) path. store overall clause count
number clauses ascending descending part path.
number verb phrases (VP) path. Similarly feature, store
three numbers: overall verb count, verb count ascending/descending
part path.
Generalized syntactic paths. generalize path syntactic tree,
appears 3 elements, using two templates: (a) Arg Ancestor Ni
Pred, Arg argument label, Pred predicate label, Ancestor
label common ancestor, Ni instantiated labels
118

fiCombination Strategies Semantic Role Labeling

Pred Ancestor full path; (b) Arg Ni Ancestor Pred, Ni
instantiated labels Arg Ancestor full path.
example, path NP VP SBAR VP argument label rst NP,
predicate label last VP, common ancestors label rst S. Hence,
using last template, path generalized following three features: NP
VP VP, NP SBAR VP, NP VP. generalization reduces
sparsity complete constituent-predicate path feature using dierent strategy
Models 1 2, implement n-gram based approach.
subsumption count, i.e., dierence depths syntactic tree
argument predicate constituents. value 0 two phrases share
parent.
governing category, similar Models 1 2.

surface distance predicate argument phrases encoded as:
number tokens, verb terminals (VB*), commas, coordinations (CC) argument predicate phrases, binary feature indicate two
constituents adjacent. example, surface distance argument
candidate Others predicate hope Figure 3 example: Others,
released majors, hope senior league... 7 tokens, 1 verb, 2 commas,
0 coordinations. features, originally proposed Collins (1999) dependency parsing model, capture robust, syntax-independent information
sentence structure. example, constituent unlikely argument
verb another verb appears two phrases.

binary feature indicate argument starts predicate particle, i.e.,
token seen RP* POS tag directly attached predicate training.
motivation feature avoid inclusion predicate particles
argument constituent. example, without feature, SRL system tend
incorrectly include predicate particle argument text: take [A1
organization], marked text commonly incorrectly parsed
prepositional phrase large number prepositional phrases directly attached
verb arguments corresponding predicate.
4.2.2 Classifier
Similarly Models 1 2, Model 3 trains one-vs-all classiers using AdaBoost
common argument labels. reduce sample space, Model 3 selects training examples
(both positive negative) from: (a) rst clause includes predicate,
(b) phrases appear left predicate sentence. 98%
argument constituents fall one classes.
prediction time classiers combined using simple greedy technique
iteratively assigns predicate argument classied highest condence.
predicate consider candidates attributes, numbered attributes
indicated corresponding PropBank frame. Additionally, greedy strategy enforces
limited number domain knowledge constraints generated solution: (a) arguments
overlap form, (b) duplicate arguments allowed A0-5, (c)
119

fiSurdeanu, Marquez, Carreras, & Comas

predicate numbered arguments, i.e., A0-5, subset present
PropBank frame. constraints somewhat dierent constraints used
Models 1 2: (i) Model 3 use B-I-O representation hence constraint
B-I-O labeling correct apply; (ii) Models 1 2 enforce
constraint numbered arguments duplicated implementation
straightforward architecture.

5. Performance Individual Models
section analyze performance three individual SRL models proposed.
three SRL systems trained using complete CoNLL-2005 training set (PropBank/Treebank sections 2 21). avoid overtting syntactic processors i.e.,
part-of-speech tagger, chunker, Charniaks full parser partitioned PropBank
training set folds fold used output syntactic processors
trained four folds. models tuned separate development partition (Treebank section 24) evaluated two corpora: (a) Treebank section
23, consists Wall Street Journal (WSJ) documents, (b) three sections
Brown corpus, semantically annotated PropBank team CoNLL-2005 shared
task evaluation.
classiers individual models developed using AdaBoost decision trees depth 4 (i.e., branch may represent conjunction 4 basic
features). classication model trained 2,000 rounds. applied
simplications keep training times memory requirements inside admissible bounds:
(a) trained frequent argument labels: top 41 Model 1, top 35
Model 2, top 24 Model 3; (b) discarded features occurring less 15
times training set, (c) Model 3 classier, limited number
negative training samples rst 500,000 negative samples extracted PropBank
traversal3 .
Table 1 summarizes results three models WSJ Brown corpora.
include percentage perfect propositions detected model (PProps),
i.e., predicates recognized arguments, overall precision, recall, F1
measure4 . results summarized Table 1 indicate individual systems
solid performance. Although none would rank top 3 CoNLL2005 evaluation (Carreras & Marquez, 2005), performance comparable best
individual systems presented evaluation exercise5 . Consistently systems
evaluated Brown corpus, models experience severe performance drop
corpus, due lower performance linguistic processors.
expected, models based full parsing (2 3) perform better model
based partial syntax. But, interestingly, dierence large (e.g., less 2 points
3. distribution samples Model 3 classifiers biased towards negative samples because,
worst case, syntactic constituent sentence predicate potential argument.
4. significance intervals F1 measure obtained using bootstrap resampling (Noreen,
1989). F1 rates outside intervals assumed significantly different related F1
rate (p < 0.05).
5. best performing SRL systems CoNLL combination several subsystems. See section 9
details.

120

fiCombination Strategies Semantic Role Labeling

WSJ
Model 1
Model 2
Model 3
Brown
Model 1
Model 2
Model 3

PProps
48.45%
52.04%
45.28%

Precision
78.76%
79.65%
80.32%

Recall
72.44%
74.92%
72.95%

F1
75.47 0.8
77.21 0.8
76.46 0.6

30.85%
36.44%
29.48%

67.72%
71.82%
72.41%

58.29%
64.03%
59.67%

62.65 2.1
67.70 1.9
65.42 2.1

Table 1: Overall results individual models WSJ Brown test sets.
Model 1 F1
Model 2 F1
Model 3 F1

A0
83.37
86.65
86.14

A1
75.13
77.06
75.83

A2
67.33
65.04
65.55

A3
61.92
62.72
65.26

A4
72.73
72.43
73.85

Table 2: F1 scores individual systems A04 arguments WSJ test.
F1 WSJ corpus), evincing base syntactic chunks clause boundaries
enough obtain competitive performance. importantly, full-parsing models
always better partial-syntax model. Table 2 lists F1 measure three
models rst numbered arguments. Table 2 shows Model 2, overall
best performing individual system, achieves best F-measure A0 A1 (typically
subjects direct objects), Model 1, partial-syntax model, performs best
A2 (typically indirect objects, instruments, benefactives). explanation
behavior indirect objects tend farther predicates accumulate
parsing errors. models based full syntax, Model 2 better recall
whereas Model 3 better precision, Model 3 lters candidate arguments
match single syntactic constituent. Generally, Table 2 shows models
strong weak points. justication focus combination
strategies combine several independent models.

6. Features Combination Models
detailed Section 3, paper analyze two classes combination strategies
problem semantic role labeling: (a) inference model constraint satisfaction,
nds set candidate arguments maximizes global cost function, (b)
two inference strategies based learning, candidates scored ranked using
discriminative classiers. perspective feature space, main dierence
two types combination models input rst combination strategy limited argument probabilities produced individual systems,
whereas last class combination approaches incorporates much larger feature set
ranking classiers. robustness, paper use features extracted solutions provided individual systems, hence independent
121

fiSurdeanu, Marquez, Carreras, & Comas

1111
0000
0000
1111
0000
1111
0000
1111
0000
1111
0000
1111
0000
1111
0000
1111
0000
1111
0000
1111
0000
1111
0000
1111
0000
1111
0000
1111
0000
1111
0000
1111
A0

A0

V

V

A1

A1

V

A1

A2

A4

M1

M2

M3

Figure 5: Sample solutions proposed predicate three individual SRL models:
M1, M2 M3. Argument candidates displayed vertically system.

individual models6 . describe features next. examples given section
based Figures 5 6.
Voting features features quantify votes received argument
individual systems. set includes following features:
label candidate argument, e.g., A0 rst argument proposed system
M1 Figure 5.
number systems generated argument label span.
example shown Figure 5, feature value 1 argument A0 proposed
M1 2 M1s A1, system M2 proposed argument.
unique ids systems generated argument label
span, e.g., M1 M2 argument A1 proposed M1 M2 Figure 5.
argument sequence predicate systems generated argument label span. example, argument sequence generated
system M1 proposition illustrated Figure 5 is: A0 - V - A1 - A2. feature attempts capture information proposition level, e.g., combination model
might learn trust model M1 argument sequence A0 - V - A1 - A2,
M2 another sequence, etc.
Same-predicate overlap features features measure overlap dierent
arguments produced individual SRL models predicate:
6. exception argument probabilities, required constraint satisfaction model.

122

fiCombination Strategies Semantic Role Labeling

number unique ids systems generated argument
span different label. example shown Figure 5, features
values 1 M2 argument A2 proposed M1, model M2 proposed
argument A4 span.
number unique ids systems generated argument included
current argument. candidate argument A0 proposed model M1
Figure 5, features values 1 M3, M3 generated argument A0,
included M1s A0.
spirit, generate number unique ids systems
generated argument contains current argument, number
unique ids systems generated argument overlaps
include contain current argument.
Other-predicate overlap features features quantify overlap dierent arguments produced individual SRL models predicates. generate
features previous feature group, dierence compare arguments generated dierent predicates. motivation overlap features that,
according PropBank annotations, form overlap allowed among arguments
attached predicate, inclusion containment permitted
arguments assigned dierent predicates. overlap features meant detect
domain constraints satised candidate argument, indication,
evidence strong, candidate incorrect.
Partial-syntax features features codify structure argument
distance argument predicate using partial syntactic information,
i.e., chunks clause boundaries (see Figure 6 example). Note features
inherently dierent features used Model 1, Model 1 evaluates
individual chunk part candidate argument, whereas codify properties
complete argument constituent. describe partial-syntax features below.
Length tokens chunks argument constituent, e.g., 4 1 argument
A0 Figure 6.
sequence chunks included argument constituent, e.g., PP NP
argument AM-LOC Figure 6. chunk sequence large, store n-grams
length 10 start end sequence.
sequence clause boundaries, i.e., clause beginning ending, included
argument constituent.
named entity types included argument constituent, e.g., LOCATION
AM-LOC argument Figure 6.
Position argument: before/after predicate sentence, e.g., A1
Figure 6.
Boolean ag indicate argument constituent adjacent predicate,
e.g., false A0 true A1 Figure 6.
123

fiSurdeanu, Marquez, Carreras, & Comas

Clause

NP

NP

VP

NP

PP

NP

luxury auto maker last year sold 1,214 cars U.S.
A0

AMTMP

P

A1

AMLOC

Figure 6: Sample proposition partial syntactic information.
sequence chunks argument constituent predicate, e.g.,
chunk sequence predicate argument AM-LOC Figure 6 is: NP.
Similarly chunk sequence feature, sequence large, store
starting ending n-grams.
number chunks predicate argument, e.g., 1 AM-LOC
Figure 6.
sequence clause boundaries argument constituent predicate.
clause subsumption count, i.e., dierence depths clause
tree argument predicate constituents. value 0 two phrases
included clause.
Full-syntax features features codify structure argument constituent,
predicate, distance two using full syntactic information.
full-syntax features replicated Model 3 (see Section 4.2), assumes
one-to-one mapping semantic constituents syntactic phrases exists. Unlike Model 3
ignores arguments matched syntactic constituent,
exact mapping exist due inclusion candidates Models 1 2,
generate approximate mapping unmapped semantic constituent largest
phrase included given span left boundary semantic constituent. heuristic guarantees capture least semantic
constituents syntactic structure.
motivation partial full-syntax features learn preferences
individual SRL models. example, features combination classier might
learn trust model M1 arguments closer 3 chunks predicate, model
M2 predicate-argument syntactic path NP VP SBAR VP, etc.
Individual systems argument probabilities individual model outputs condence score proposed arguments. scores converted probabilities using softmax function described detail Section 7.1. combination
strategy based constraint satisfaction (Section 7.1) uses probabilities are,
two strategies based meta-learning (Section 7.2) discretize
probabilities include features. so, probability value matched
124

fiCombination Strategies Semantic Role Labeling

one probability intervals corresponding interval used feature.
probability intervals dynamically constructed argument label individual system corresponding system predictions argument label
uniformly distributed across intervals.
Section 8.4 empirically analyze contribution proposed feature
sets performance best combination model.

7. Combination Strategies
section detail combination strategies proposed paper: (a) combination
model constraint satisfaction, aims nding set candidate arguments
maximizes global cost function, (b) two combination models inference based
learning, candidates scored ranked using discriminative classiers.
previous section described complete feature set made available approaches.
focus machine learning paradigm deployed combination
models.
7.1 Inference Constraint Satisfaction
Constraint Satisfaction model selects subset candidate arguments maximizes
compatibility function subject fulllment set structural constraints
ensure consistency solution. compatibility function based probabilities
given individual SRL models candidate arguments. work use Integer
Linear Programming solve constraint satisfaction problem. approach rst
proposed Roth Yih (2004) applied semantic role labeling Punyakanok,
Roth, Yih, Zimak (2004), Koomen et al. (2005), among others. follow setting
Komen et al., taken reference.
rst step, scores model normalized probabilities. scores
yielded classiers signed unbounded real numbers, experimental evidence
shows condence predictions (taken absolute value raw scores)
correlates well classication accuracy. Thus, softmax function (Bishop, 1995)
used convert set unbounded scores probabilities. k possible
output labels given argument sco(li ) denotes score label li output
xed SRL model, estimated probability label is:
esco(li )
p(li ) = Pk
sco(lj )
j=1 e
parameter formula empirically adjusted avoid overly skewed
probability distributions normalize scores three individual models
similar range values. See details experimental setting Section 8.1.
Candidate selection performed via Integer Linear Programming (ILP). program
goal maximize compatibility function modeling global condence selected
set candidates, subject set linear constraints. variables involved
task take integer values may appear rst degree polynomials only.
abstract ILP process described simple fashion as: given set variables V = {v1 , . . . , vn }, aims maximize global compatibility label assignment
125

fiSurdeanu, Marquez, Carreras, & Comas

{l1 , . . . , ln } variables. local compatibility function cv (l) denes compatibility
assigning label l variable v. global compatibility function C(l1 , . . . , ln ) taken
sum local assignment compatibility, goal ILP process
written as:
argmax C(l1 , . . . , ln ) = argmax
l1 ,...,ln

l1 ,...,ln

n
X

cvi (li )

i=1

constraints described set accompanying integer linear equations involving variables problem.
one wants codify soft constraints instead hard, possibility considering penalty component compatibility function. case, constraint
r R seen function takes current label assignment outputs
real number, 0 constraint satised positive number not,
indicating penalty imposed compatibility function. new expression
compatibility function maximize is:
C(l1 , . . . , ln ) =

n
X
i=1

cvi (li )

X

r(l1 , . . . , ln )

rR

Note hard constraints simulated setting making output
large positive number violated.
particular problem, binary-valued variable vi N argument candidates generated SRL models, i.e., li labels {0, 1}. Given label
assignment, arguments li = 1 selected form solution, others
(those li = 0) ltered out. variable vi , probability
values, pij , calculated score model j argument i, according softmax
formula described above7 . rst approach, compatibility function cv (li ) equals
P
8
(
j=1 pij )li , number models, , 3 case .
denition, maximizing compatibility function equivalent maximizing
sum probabilities given models argument candidates considered
solution. Since function always positive, global score increases directly
number selected candidates. consequence, model biased towards
maximization number candidates included solution (e.g., tending select
lot small non-overlapping arguments). Following Koomen et al. (2005), bias
corrected adding new score oi , sums compatibility function
i-th candidate selected solution. global compatibility function needs
rewritten encompass new information. Formalized ILP equation, looks like:
argmax C(l1 , . . . , lN ) = argmax

L{0,1}N

L{0,1}N


N X
X

(

i=1 j=1

pij )li + oi (1 li )

7. model j propose argument consider pij = 0.
8. Instead accumulating probabilities models given candidate argument, one could consider
different variable model prediction introduce constraint forcing variables
take value end optimization problem. two alternatives equivalent.

126

fiCombination Strategies Semantic Role Labeling

constraints expressed separated integer linear equations. possible
dene priori value oi . Komen et al. used validation corpus empirically
estimate constant value oi (i.e., independent argument candidate)9 .
use exactly solution working single constant value,
refer O.
Regarding consistency constraints, considered following six:
1. Two candidate arguments verb overlap embed.
2. verb may two core arguments type label A0-A5.
3. argument R-X verb, X argument
verb.
4. argument C-X verb, X argument
C-X verb.
5. Arguments two dierent verbs overlap, embed.
6. Two dierent verbs share AM-X, R-AM-X C-X arguments.
Constraints 14 included reference work (Punyakanok et al., 2004).
constraints paper need checked since individual model
outputs consistent solutions. Constraints 5 6, restrict set compatible
arguments among dierent predicates sentence, original work.
Integer Linear Programming setting constraints written inequalities. example,
Ai argument label i-th candidate Vi verb predicate, constraint number
P
2 written as: (Ai =a Vi =v) li 1, given verb v argument label a.
constraints similar translations inequalities.
Constraint satisfaction optimization applied two dierent ways obtain
complete output annotation sentence. rst one, proceed verb verb
independently nd best selection candidate arguments using constraints
1 4. call approach local optimization. second scenario
candidate arguments sentence considered constraints 1 6
enforced. refer second strategy global optimization. scenarios
compatibility function same, constraints need rewriting global
scenario include information concrete predicate.
Section 8.3 extensively evaluate presented inference model based Constraint Satisfaction, describe experiments covering following topics: (a)
contribution proposed constraints; (b) performance local vs. global
optimization; (c) precisionrecall tradeo varying value bias-correction
parameter.
7.2 Inference Based Learning
combination model consists two stages: candidate scoring phase, scores
candidate arguments pool using series discriminative classiers, inference
stage, selects best overall solution consistent domain constraints.
9. Instead working constant, one could try set oi value candidate, taking
account contextual features candidate. plan explore option near future.

127

fiSurdeanu, Marquez, Carreras, & Comas

rst important component combination strategy candidate
scoring module, assigns candidate argument score equal condence
argument part global solution. formed discriminative functions,
one role label. Below, devise two dierent strategies train discriminative
functions.
scoring candidate arguments, nal global solution built inference
module, looks best scored argument structure satises domain specic
constraints. Here, global solution subset candidate arguments, score
dened sum condence values arguments form it. currently consider
three constraints determine solutions valid:
(a) Candidate arguments predicate overlap embed.
(b) predicate, duplicate arguments allowed numbered arguments A0-5.
(c) Arguments predicate embedded within arguments predicates
overlap.
set constraints extended rules, particular case,
know constraints, e.g., providing arguments indicated corresponding PropBank frame, already guaranteed individual models, others, e.g.,
constraints 3 4 previous sub-section, positive impact overall
performance (see Section 8.3 empirical analysis). inference algorithm use
bottom-up CKY-based dynamic programming strategy (Younger, 1967). builds
solution maximizes sum argument condences satisfying constraints,
cubic time.
Next, describe two dierent strategies train functions score candidate
arguments. rst local strategy: function trained binary batch classier,
independently combination process enforces domain constraints.
second global strategy: functions trained online rankers, taking account
interactions take place combination process decide one argument
another.
training strategies, discriminative functions employ representation arguments, using complete feature set described Section 6 (we analyze
contribution feature group Section 8). intuition rich feature
space introduced Section 6 allow gathering sucient statistics robust
scoring candidate arguments. example, scoring classiers might learn
candidate trusted if: (a) two individual systems proposed it, (b) label
A2 generated Model 1, (c) proposed Model 2 within certain
argument sequence.
7.2.1 Learning Local Classifiers
combination process follows cascaded architecture, learning component
decoupled inference module. particular, training strategy consists
training binary classier role label. target label-based classier
determine whether candidate argument actually belongs correct proposition
corresponding predicate, output condence value decision.
128

fiCombination Strategies Semantic Role Labeling

specic training strategy follows. training data consists pool
labeled candidate arguments (proposed individual systems). candidate either
positive, actually correct argument sentence, negative,
correct. strategy trains binary classier role label l, independently
labels. so, concentrates candidate arguments data
label l. forms dataset binary classication, specic label l. it,
binary classier trained using existing techniques binary classication,
requirement combination strategy needs condence values
binary prediction. Section 8 provide experiments using SVMs train local
classiers.
all, classier trained independently classiers inference module. Looking globally combination process, classier seen argument
ltering component decides candidates actual arguments using much richer
representation individual models. context, inference engine used
conict resolution engine, ensure combined solutions valid argument structures sentences.
7.2.2 Learning Global Rankers
combination process couples learning inference, i.e., scoring functions
trained behave accurately within inference module. words, training
strategy global: target train global function maps set argument
candidates sentence valid argument structure. setting, global function
composition scoring functions one label, previous strategy.
Unlike previous strategy, completely decoupled inference engine,
policy map set candidates solution determined inference
engine.
recent years, research active global learning methods tagging,
parsing and, general, structure prediction problems (Collins, 2002; Taskar, Guestrin, &
Koller, 2003; Taskar, Klein, Collins, Koller, & Manning, 2004; Tsochantaridis, Hofmann,
Joachims, & Altun, 2004). article, make use simplest technique global
learning: online learning approach uses Perceptron (Collins, 2002). general
idea algorithm similar original Perceptron (Rosenblatt, 1958): correcting
mistakes linear predictor made visiting training examples, additive
manner. key point learning global rankers relies criteria determines
mistake function trained, idea exploited
similar way multiclass ranking scenarios Crammer Singer (2003a, 2003b).
Perceptron algorithm combination system works follows (pseudocode
algorithm given Figure 7). Let 1 . . . L possible role labels, let
W = {w1 . . . wL } set parameter vectors scoring functions, one
label. Perceptron initializes vectors W zero, proceeds cycle
training examples, visiting one time. case, training example pair (y, A),
correct solution example set candidate arguments
it. Note sets labeled arguments, thus make use
set dierence. note particular argument, l label a,
129

fiSurdeanu, Marquez, Carreras, & Comas

Initialization: wl W wl = 0
Training :
= 1 . . .
training example (y, A)
= Inference(A, W)
\
let l label
wl = wl + (a)
\
let l label
wl = wl (a)
Output: W
Figure 7: Perceptron Global Learning Algorithm

(a) vector features described Section 6. example, Perceptron performs
two steps. First, predicts optimal solution according current setting
W. Note prediction strategy employs complete combination model, including
inference component. Second, Perceptron corrects vectors W according
mistakes seen y: arguments label l seen promoted vector
wl ; hand, arguments demoted wl . correction rule
moves scoring vectors towards missing arguments, away predicted arguments
correct. guaranteed that, Perceptron visits examples,
feedback rule improve accuracy global combination function
feature space almost linearly separable (Freund & Schapire, 1999; Collins, 2002).
all, training strategy global mistakes Perceptron corrects
arise comparing predicted structure correct one. contrast,
local strategy identies mistakes looking individually sign scoring predictions:
candidate argument (is not) correct solution current scorers predict
negative (positive) condence value, corresponding scorer corrected
candidate argument. Note criteria used generate training data
classiers trained locally. Section 8 compare approaches empirically.
nal note, simplicity described Perceptron simple form.
However, Perceptron version use experiments reported Section 8 incorporates two well-known extensions: kernels averaging (Freund & Schapire, 1999; Collins
& Duy, 2002). Similar SVM, Perceptron kernel method. is, represented dual form, dot product example vectors generalized
kernel function exploits richer representations. hand, averaging
technique increases robustness predictions testing. original form,
test predictions computed parameters result training process.
averaged version, test predictions computed average parameter
vectors generated training, every update. Details technique
found original article Freund & Schapire.
130

fiCombination Strategies Semantic Role Labeling

1

Development
Brown
WSJ

0.98
0.96

Acuracy

0.94
0.92
0.9
0.88
0.86
0.84
0.82
0

10

20

30

40

50

60

70

80

90

100

Reject Rate (%)

Figure 8: Rejection curves estimated output probabilities individual models.

8. Experimental Results
section analyze performance three combination strategies previously
described: (a) inference constraint satisfaction, (b) learning-based inference local
rankers, (c) learning-based inference global rankers. bulk experiments use candidate arguments generated three individual SRL models described
Section 4 evaluated Section 5.
8.1 Experimental Settings
combination strategies (with one exception, detailed below) trained using
complete CoNLL-2005 training set (PropBank/Treebank sections 2 21). minimize
overtting individual SRL models training data, partitioned training
corpus folds fold used output individual models
trained remaining four folds. models tuned separate development
partition (Treebank section 24) evaluated two corpora: (a) Treebank section 23,
(b) three annotated sections Brown corpus.
constraint satisfaction model, converted scores output arguments
three SRL models probabilities using softmax function explained Section 7.1.
development set (section 24) used tune parameter softmax formula
nal value 0.1 models. order assess quality procedure,
plot Figure 8 rejection curves estimated output probabilities respect
classication accuracy development test sets (WSJ Brown). calculate
plots, probability estimates three models put together set sorted
decreasing order. certain level rejection (n%), curve Figure 8 plots
percentage correct arguments lowest scoring n% subset rejected.
131

fiSurdeanu, Marquez, Carreras, & Comas

exceptions, curves increasing smooth, indicating good correlation
probability estimates classication accuracy.
last experiment, Section 8.6 analyze behavior proposed combination
strategies candidate pool signicantly larger. experiment used
top 10 best performing systems CoNLL-2005 shared task evaluation. setup
two signicant dierences experiments used in-house individual
systems: (a) access systems outputs PropBank development
section two test sections, (b) argument probabilities individual
models available. Thus, instead usual training set, train
combination models PropBank development section smaller feature set. Note
development set 3.45% size regular training set.
evaluated resulting combination models two testing sections: WSJ
Brown.
8.2 Lower Upper Bounds Combination Strategies
venture evaluation combination strategies, explore lower
upper bounds combinations models given corpus individual models.
analysis important order understand potential proposed approach
see close actually realizing it.
performance upper bound calculated oracle combination system
perfect ltering classier selects correct candidate arguments discards
others. comparison purposes, implemented second oracle system simulates re-ranking approach: predicate selects candidate frame i.e.,
complete set arguments corresponding predicate proposed single model
highest F1 score. Table 3 lists results obtained WSJ Brown corpora
two oracle systems using three individual models. combination system
oracle simulates combination strategies proposed paper, break
candidate frames work individual candidate arguments. Note precision
oracle combination system 100% case discontinuous arguments,
fragments pass oracle lter considered incorrect scorer corresponding argument complete, e.g., argument A1 appears without continuation
C-A1. re-ranking columns list results second oracle system, selects
entire candidate frames.
Table 3 indicates upper limit combination approaches proposed
paper relatively high: F1 combination oracle system 14 points higher
best individual system WSJ test set, 17 points higher
Brown corpus (see Table 1). Furthermore, analysis indicates potential
combination strategy higher re-ranking strategies, limited
performance best complete frame candidate pool. allowing recombination arguments individual candidate solutions threshold raised
signicantly: 6 F1 points WSJ 9 F1 points Brown.
Table 4 lists distribution candidate arguments individual models
selection performed combination oracle system. conciseness, list
core numbered arguments focus WSJ corpus. 3 indicates percent132

fiCombination Strategies Semantic Role Labeling

WSJ
Brown

PProps
70.76%
51.87%

Combination
Precision Recall
99.12%
85.22%
99.63%
74.32%

F1
91.64
85.14

PProps
63.51%
45.02%

Re-Ranking
Precision Recall
88.08%
82.84%
80.80%
71.70%

F1
85.38
75.98

Table 3: Performance upper limits detected two oracle systems.
A0
A1
A2
A3
A4

3
80.45%
69.82%
56.04%
56.03%
65.85%

2
12.10%
17.83%
22.32%
21.55%
20.73%

Model 1
3.47%
7.45%
12.20%
12.93%
6.10%

Model 2
2.14%
2.77%
4.95%
5.17%
2.44%

Model 3
1.84%
2.13%
4.49%
4.31%
4.88%

Table 4: Distribution individual systems arguments upper limit selection,
A0A4 WSJ test set.

age correct arguments 3 models agreed, 2 indicates percentage
correct arguments 2 models agreed, columns indicate percentage correct arguments detected single model. Table 4 indicates that, expected,
two individual models agreed large percentage correct arguments. Nevertheless, signicant number correct arguments, e.g., 22% A3, come single
individual system. proves that, order achieve maximum performance, one
look beyond simple voting strategies favor arguments high agreement
individual systems.
propose two lower bounds performance combination models using two
baseline systems:
rst baseline recall-oriented: merges arguments generated
individual systems. conict resolution, baseline uses approximate inference
algorithm consisting two steps: (i) candidate arguments sorted using radix
sort orders candidate arguments descending order of: (a) number models
agreed argument, (b) argument length tokens, (c) performance
individual system10 ; (ii) Candidates iteratively appended global solution
violate domain constraints arguments already
selected.
second baseline precision-oriented: considers arguments three
individual systems agreed. conict resolution uses strategy
previous baseline system.
Table 5 shows performance two baseline models. expected, precisionoriented baseline obtains precision signicantly higher best individual model
(Table 1), recall suers individual models agree fairly large
number candidate arguments. recall-oriented baseline balanced: expected
recall higher individual model precision drop much
10. combination produced highest-scoring baseline model.

133

fiSurdeanu, Marquez, Carreras, & Comas

WSJ
baseline
baseline
Brown
baseline
baseline

recall
precision

PProps
53.71%
35.43%

Prec.
78.09%
92.49%

Recall
78.77%
60.48%

F1
78.43 0.8
73.14 0.9

recall
precision

36.94%
20.52%

68.57%
88.74%

66.05%
46.35%

67.29
60.89

2.0
2.1

Table 5: Performance baseline models WSJ Brown test sets.
inference strategy lters many unlikely candidates. Overall, recalloriented baseline performs best, F1 1.22 points higher best individual
model WSJ corpus, 0.41 points lower Brown corpus.
8.3 Performance Combination System Constraint Satisfaction
Constraint Satisfaction setting arguments output individual Models 1, 2,
3 recombined expected better solution satises set constraints.
run inference model based Constraint Satisfaction described Section 7.1 using
Xpress-MP ILP solver11 . main results summarized Table 6. variants
presented table following: Pred-by-pred stands local optimization,
processes verb predicate independently others, Full sentence stands
global optimization, i.e., resolving verb predicates sentence
time. column labeled Constraints shows particular constraints applied
conguration. column presents value parameter correcting bias
towards candidate overgeneration. Concrete values empirically set maximize F1
measure development set. = 0 corresponds setting bias correction
applied.
clear conclusions drawn Table 6. First, observe optimization variant obtains F1 results individual systems (Table 1)
baseline combination schemes (Table 5). best combination model scores 2.61 F1 points
WSJ 1.49 Brown higher best individual system. Taking account
learning performed, clear Constraint Satisfaction simple yet formal
setting achieves good results.
somewhat surprising result performance improvements come constraints 1 2 (i.e., overlapping embedding among arguments verb,
repetition core arguments verb). Constraints 3 4 harmful,
sentence-level constraints (5 6) impact overall performance12 .
analysis proposed constraints yielded following explanations:
Constraint number 3 prevents assignment R-X argument referred
argument X present. makes inference miss easy R-X arguments
11. Xpress-MP Dash Optimization product free academic usage.
12. Section 8.5 see learning strategy incorporates global feedback, performing
sentence-level inference slightly better proceeding predicate predicate.

134

fiCombination Strategies Semantic Role Labeling

WSJ
Pred-by-pred

Full sentence

Brown
Full sentence

Constraints
1
1+2
1+2+3
1+2+4
1+2+3+4
1+2+5
1+2+6
1+2+5+6
1+2+5+6


0.30
0.30
0.25
0.30
0.30
0.30
0.30
0.30
0

PProps
52.29%
52.52%
52.31%
51.40%
51.19%
52.53%
52.48%
52.50%
54.49%

Precision
84.20%
84.61%
84.34%
84.13%
83.86%
84.63%
84.64%
84.65%
78.74%

Recall
75.64%
75.53%
75.48%
75.04%
74.99%
73.53%
75.51%
75.51%
79.78%

F1
79.69 0.8
79.81 0.6
79.67 0.7
79.32 0.8
79.18 0.7
79.82 0.7
79.81 0.8
79.82 0.6
79.26 0.7

1+2+5+6
1+2+5+6

0.30
0

35.70%
38.06%

78.18%
69.80%

62.06%
67.85%

69.19 2.1
68.81 2.2

Table 6: Results, WSJ Brown test sets, obtained multiple variants constraint satisfaction approach

X argument correctly identied (e.g., constituents start
{that, which, who} followed verb always R-A0). Furthermore, constraint
presents lot exceptions: 18.75% R-X arguments WSJ test set
referred argument X (e.g., law tells so),
therefore hard application constraint 3 prevents selection correct
R-X candidates. ocial evaluation script CoNLL-2005 (srl-eval)
require constraint satised consider solution consistent.
srl-eval script requires constraint number 4 (i.e., C-X tag accepted
without preceding X argument) fullled candidate solution considered
consistent. nds solution violating constraint behavior
convert rst C-X (without preceding X) X. turns simple
post-processing strategy better forcing coherent solution inference step
allows recover error argument completely
recognized labeled C-X tags.
Regarding sentence-level constraints, observed setting, inference using
local constraints (1+2) rarely produces solution inconsistencies sentence
level.13 makes constraint 5 useless since almost never violated. Constraint
number 6 (i.e., sharing AMs among dierent verbs) ad-hoc represents
less universal principle SRL. number exceptions constraint,
WSJ test set, 3.0% gold-standard data 4.8% output
inference uses local constraints (1+2). Forcing fulllment
constraint makes inference process commit many errors corrections, making
eect negligible.
13. fact partly explained small number overlapping arguments candidate pool
produced three individual models.

135

fiSurdeanu, Marquez, Carreras, & Comas

95
90

95

Precision
Recall
F1

90

80

80
%

85

%

85

Precision
Recall
F1

75

75

70

70

65

65

60
-0.4

-0.2

0

0.2

0.4

0.6

0.8

60
-0.4

1

Value

-0.2

0

0.2

0.4

0.6

0.8

1

Value

Figure 9: Precision-Recall plots, respect bias correcting parameter (O),
WSJ development test sets (left right plots, respectively).

Considering constraints universal, i.e., exceptions exist
gold standard, seems reasonable convert soft constraints. done
precomputing compatibility corpora counts using, instance, point-wise
mutual information, incorporating eect compatibility function explained
section 7.1. softening could, principle, increase overall recall combination.
Unfortunately, initial experiments showed dierences hard soft
variants.
Finally, dierences optimized values bias correcting parameter
= 0 clearly explained observing precision recall values. default
version tends overgenerate argument assignments, implies higher recall cost
lower precision. contrary, F1 optimized variant conservative
needs evidence select candidate. result, precision higher recall
lower. side eect restrictive argument assignments, number
correctly annotated complete propositions lower optimized setting.
preference high-precision vs. high-recall system mostly task-dependant.
interesting note constraint satisfaction setting, adjusting precision
recall tradeo easily done varying value bias correcting score.
Figure 9, plot precisionrecall curves respect dierent values parameter (the optimization done using constraints 1, 2, 5, 6). expected, high values
promote precision demote recall, lower values contrary. Also,
see wide range values combined F1 measure almost
constant (the approximate intervals marked using vertical lines), making possible
select dierent recall precision values global performance (F1 ) near optimal. Parenthetically, note optimal value estimated development set
(O = 0.3) generalizes well WSJ test set.
136

fiCombination Strategies Semantic Role Labeling

WSJ
Models
Models
Models
Models
Brown
Models
Models
Models
Models

1+2
1+3
2+3
1+2+3

PProps
49.28%
48.26%
49.36%
51.66%

Prec.
87.39%
86.80%
86.63%
87.47%

Recall
72.88%
73.20%
73.03%
74.67%

F1
79.48 0.6
79.42 0.6
79.25 0.7
80.56 0.6

F1 improvement
+2.27
+2.96
+2.04
+3.35

1+2
1+3
2+3
1+2+3

34.33%
31.22%
32.84%
34.33%

81.14%
80.43%
80.90%
81.75%

60.86%
59.07%
60.31%
61.32%

69.55 2.0
68.11 1.9
69.11 2.1
70.08 2.1

+1.85
+2.69
+1.41
+2.38

Table 7: Overall results learning-based inference local rankers WSJ
Brown test sets.

8.4 Performance Combination System Local Rankers
implemented candidate-scoring classiers combination strategy using Support Vector Machines (SVM) polynomial kernels degree 2, performed slightly
better types SVMs AdaBoost. implemented SVM classiers
SVMlight software14 . Outside changing default kernel polynomial
modied default parameters. experiments reported section,
trained models 4 possible combinations 3 individual systems, using
complete feature set introduced Section 6. dynamic programming engine used
actual inference processes predicate independently (similar Pred-by-pred
approach previous sub-section).
Table 7 summarizes performance combined systems WSJ Brown
corpora. Table 7 indicates combination strategy always successful: results
combination systems improve upon individual models (Table 1) F1
scores always better baselines (Table 5). last column table shows
F1 improvement combination model w.r.t. best individual model set.
expected, highest scoring combined system includes three individual models.
F1 measure 3.35 points higher best individual model (Model 2) WSJ test
set 2.38 points higher Brown test set. Note combination two
individual systems outperform current state art (see Section 9 details).
empirical proof robust successful combination strategies SRL problem
possible. Table 7 indicates that, even though partial parsing model (Model 1)
worst performing individual model, contribution ensemble important,
indicating information provides indeed complementary models.
instance, WSJ performance combination two best individual models
(Models 2+3) worse combinations using model 1 (Models 1+2 1+3).
14. http://svmlight.joachims.org/

137

fiSurdeanu, Marquez, Carreras, & Comas

WSJ
FS1
+ FS2
+ FS3
+ FS4
+ FS5
+ FS6
Brown
FS1
+ FS2
+ FS3
+ FS4
+ FS5
+ FS6

PProps
50.24%
50.39%
51.22%
50.66%
51.38%
51.66%

Prec.
86.47%
86.41%
86.13%
86.67%
87.21%
87.47%

Recall
73.51%
73.68%
74.35%
74.10%
74.61%
74.67%

F1
79.47 0.7
79.54 0.6
79.80 0.7
79.89 0.7
80.42 0.6
80.56 0.6

32.21%
32.84%
33.33%
33.33%
34.08%
34.33%

80.12%
80.80%
80.29%
81.10%
81.76%
81.75%

59.44%
59.94%
60.82%
60.50%
61.14%
61.32%

68.25 2.0
68.83 2.2
69.21 2.0
69.30 2.1
69.96 2.2
70.08 1.9

Table 8: Feature analysis learning-based inference local rankers.
Due simple architecture i.e., feedback conict resolution component
candidate ltering inference model good framework study contribution
features proposed Section 6. study group features 6 sets: FS1
voting features, FS2 overlap features arguments predicate, FS3
overlap features arguments predicates, FS4 partial-syntax features, FS5
full-syntax features, FS6 probabilities generated individual systems
candidate arguments. Using sets constructed 6 combination models
increasing number features made available argument ltering classiers, e.g.,
rst system uses FS1, second system adds FS2 rst systems features,
FS3 added third system, etc. Table 8 lists performance 6 systems
two test corpora. empirical analysis indicates feature sets
highest contribution are:
FS1, boosts F1 score combined system 2.26 points (WSJ) 0.55
points (Brown) best individual system. yet another empirical proof
voting successful combination strategy.
FS5, contribution 0.53 points (WSJ) 0.66 points (Brown) F1
score. numbers indicate ltering classier capable learning
preferences individual models certain syntactic structures.
FS3, contributes 0.26 points (WSJ) 0.38 points (Brown) F1 score.
results promote idea information overall sentence structure,
case inter-predicate relations, successfully used problem SRL.
knowledge, novel.
proposed features positive contribution performance combined
system. Overall, achieve F1 score 1.12 points (WSJ) 2.33 points (Brown)
higher best performing combined system CoNLL-2005 shared task evaluation
(see Section 9 details).
138

fiCombination Strategies Semantic Role Labeling

8.5 Performance Combination System Global Rankers
section report experiments global Perceptron algorithm described
Section 7.2.2, globally trains scoring functions rankers. Similar local
SVM models, use polynomial kernels degree 2. Furthermore, predictions test
time used averages parameter vectors, following technique Freund Schapire
(1999).
interested two main aspects. First, evaluate eect training
scoring functions Perceptron using two dierent update rules, one global
local. global feedback rule, detailed Section 7.2.2, corrects mistakes found
comparing correct argument structure one results inference (this
noted global feedback). contrast, local feedback rule corrects mistakes
found inference, candidate argument handled independently, ignoring
global argument structure generated (this noted local feedback). Second,
analyze eect using dierent constraints inference module. extent,
congured inference module two ways. rst processes predicates
sentence independently, thus might select overlapping arguments dierent predicates,
incorrect according domain constraints (this one noted Pred-by-pred
inference). second processes predicates jointly, enforces hierarchical structure
arguments, arguments never overlap, arguments predicate allowed
embed arguments predicates (this noted Full sentence inference).
perspective, model local update Pred-by-pred inference almost identical
local combination strategy described Section 8.4, unique dierence
use Perceptron instead SVM. apparently minute dierence turns
signicant empirical analysis allows us measure contribution
SVM margin maximization global feedback classier-based combination
strategy (see Section 8.7).
trained four dierent models: local global feedback, predicate-bypredicate joint inference. model trained 5 epochs training data,
evaluated development data training epoch. selected best
performing point development, evaluated models test data. Table 9
reports results test data.
Looking results, rst impression dierence F1 measure
signicant among dierent congurations. However, observations pointed out.
Global methods achieve much better recall gures, whereas local methods prioritize
precision system. Overall, global methods achieve balanced tradeo
precision recall, contributes better F1 measure.
Looking Pred-by-pred versus Full sentence inference, seen
global methods sensitive dierence. Note local model trained
independently inference module. Thus, adding constraints inference
engine change parameters local model. testing time, dierent
inference congurations aect results. contrast, global models trained
dependently inference module. moving Pred-by-pred Full sentence
inference, consistency enforced argument structures dierent predicates,
benets precision recall method. global learning algorithm
139

fiSurdeanu, Marquez, Carreras, & Comas

WSJ
Pred-by-pred, local
Full sentence, local
Pred-by-pred, global
Full sentence, global
Brown
Pred-by-pred, local
Full sentence, local
Pred-by-pred, global
Full sentence, global

PProps
50.71%
50.67%
53.45%
53.81%

Prec.
86.80%
86.80%
84.66%
84.84%

Recall
74.31%
74.29%
76.19%
76.30%

F1
80.07 0.7
80.06 0.7
80.20 0.7
80.34 0.6

33.33%
33.33%
35.20%
35.95%

80.62%
80.67%
77.65%
77.91%

60.77%
60.77%
62.70%
63.02%

69.30 1.9
69.32 2.0
69.38 1.9
69.68 2.0

Table 9: Test results combination system global rankers. Four congurations
evaluated, combine Pred-by-pred Full sentence inference local
global feedback.

improves precision recall coupled joint inference process
considers constraints solution.
Nevertheless, combination system local SVM classiers, presented previous section, achieves marginally better F1 score global learning method (80.56% vs.
80.34% WSJ). explained dierent machine learning algorithms (we discuss
issue detail Section 8.7). better F1 score accomplished much better
precision local approach (87.47% vs. 84.84% WSJ), whereas recall lower
local global approach (74.67% vs. 76.30% WSJ). hand,
global strategy produces completely-correct annotations (see PProps column)
local strategies investigated (see Tables 9 7). expected,
considering global strategy optimizes sentence-level cost function. Somewhat
surprisingly, number perfect propositions generated global strategy lower
number perfect propositions produced constraint-satisfaction approach.
discuss result Section 8.7.
8.6 Scalability Combination Strategies
combination experiments reported point used candidate arguments
generated three individual SRL models introduced Section 4. experiments provide empirical comparison three inference models proposed,
answer obvious scalability question: proposed combination approaches
scale number candidate arguments increases quality diminishes?
mainly interested answering question last two combination models (which
use inference based learning local global rankers) two reasons: (a)
performed better constraint satisfaction model previous experiments,
(b) requirements individual SRL systems outputs unlike
constraint satisfaction model requires argument probabilities individual models coupled pools candidates generated individual SRL
model.
140

fiCombination Strategies Semantic Role Labeling

koomen
pradhan+
haghighi
marquez
pradhan

surdeanu
tsai
che
moschitti
tjongkimsang
yi

ozgencil

WSJ
Prec.
Recall
82.28%
76.78%
82.95% 74.75%
79.54% 77.39%
79.55%
76.45%






50.14%

81.97%

73.27%

77.37

36.44%

73.73%

61.51%

67.07






45.28%
45.43%
47.81%
47.66%
45.85%

80.32%
82.77%
80.48%
76.55%
79.03%

72.95%
70.90%
72.79%
75.24%
72.03%

76.46
76.38
76.44
75.89
75.37

29.48%
30.47%
31.84%
30.85%
28.36%

72.41%
73.21%
71.13%
65.92%
70.45%

59.67%
59.49%
59.99%
61.83%
60.13%

65.42
65.64
65.09
63.81
64.88







F1
79.44
78.63
78.45
77.97

PProps
32.34%
38.93%
37.06%
36.44%

Brown
Prec.
Recall
73.38%
62.93%
74.49% 63.30%
70.24% 65.37%
70.79%
64.35%

PProps
53.79%
52.61%
56.52%
51.85%

F1
67.75
68.44
67.71
67.42

47.50%

77.51%

72.97%

75.17

31.09%

67.88%

59.03%

63.14

46.19%

74.66%

74.21%

74.44

31.47%

65.52%

62.93%

64.20

Table 10: Performance best systems CoNLL-2005. pradhan+ contains postevaluation improvements. top 5 systems actually combination models
themselves. second column marks systems used evaluation: pradhan, replaced improved version pradhan+,
yi, due format errors submitted data.

scalability analysis, use individual SRL models top 10 systems
CoNLL-2005 shared task evaluation. Table 10 summarizes performance systems
two test corpora used previous experiments. Table 10 indicates,
performance systems varies widely: dierence 5 F1 points WSJ
corpus 4 F1 points Brown corpus best worst system
set.
combination experiments generated 5 candidate pools using top 2, 4, 6,

8, 10 individual systems labeled
Table 10. make two changes
experimental setup used rst part section: (a) trained combined models PropBank development section access
individual systems outputs PropBank training partition; (b) feature
set introduced Section 6 use individual systems argument probabilities
raw activations individual models classiers available. Note
settings size training corpus 10 times smaller size
training set used previous experiments.
Table 11 shows upper limits setups using combination reranking oracle systems introduced Section 8.2. Besides performance numbers,
list Table 11 average number candidates per sentence setup, i.e., number
unique candidate arguments (# Args./Sent.) combination oracle number
unique candidate frames (# Frames/Sent.) re-ranking oracle. Table 12 lists
performance combined models local feedback (Section 7.2.1) global
feedback (Section 7.2.2). combination strategy global rankers uses joint inference
global feedback (see description previous sub-section).
141

fiSurdeanu, Marquez, Carreras, & Comas

WSJ
C2
C4
C6
C8
C10
Brown
C2
C4
C6
C8
C10

# Args./Sent.
8.53
9.78
10.23
10.74
11.33
7.42
8.99
9.62
10.24
10.86

Combination
Prec.
Recall
99.34%
82.71%
99.47%
87.26%
99.47%
88.02%
99.48%
88.63%
99.50% 89.02%
99.62%
99.65%
99.65%
99.66%
99.66%

71.34%
77.58%
79.38%
80.52%
81.72%

F1
90.27
92.96
93.39
93.75
93.97

Re-Ranking
# Frames/Sent.
Prec.
3.16
88.63%
4.44
91.08%
7.21
92.14%
8.11
92.88%
8.97
93.31%

83.14
87.24
88.37
89.08
89.80

3.02
4.55
7.09
8.19
9.21

82.45%
86.01%
88.19%
88.95%
89.65%

Recall
81.77%
86.12%
86.57%
87.33%
87.71%

F1
85.07
88.53
89.27
90.02
90.42

70.37%
75.98%
76.80%
78.04%
79.19%

75.94
80.68
82.10
83.14
84.10

Table 11: Performance upper limits determined oracle systems 10 best systems CoNLL-2005. Ck stands combination top k systems
Table 10. # Args./Sent. indicates average number candidate arguments
per sentence combination oracle; # Frames/Sent. indicates average
number candidate frames per sentence re-ranking oracle. latter
larger number systems combination average
multiple predicates per sentence.

WSJ
C2
C4
C6
C8
C10
Brown
C2
C4
C6
C8
C10

PProps
50.69%
55.14%
54.85%
54.36%
53.90%

Local
Prec.
86.60%
86.67%
87.45%
87.49%
87.48%

ranker
Recall
73.90%
76.63%
76.34%
76.12%
75.81%

F1
79.750.7
81.380.7
81.520.6
81.410.6
81.230.6

PProps
52.74
54.95
55.21
55.00
54.76

Global ranker
Prec.
Recall
84.07% 75.38%
84.00% 77.19%
84.24% 77.41%
84.42% 77.10%
84.02% 77.44%

F1
79.490.7
80.450.7
80.680.7
80.590.7
80.600.7

32.71%
35.95%
35.32%
35.95%
36.32%

79.56%
80.27%
80.94%
81.98%
82.61%

60.45%
63.16%
62.24%
61.87%
61.97%

68.701.8
70.692.0
70.371.8
70.522.2
70.812.0

35.32
39.30
37.44
38.43
37.44

74.88%
75.63%
76.12%
76.40%
75.94%

68.092.0
69.592.2
69.882.0
69.702.2
69.862.0

62.43%
64.45%
64.58%
64.08%
64.68%

Table 12: Local versus global ranking combinations 10 best systems CoNLL2005. Ck stands combination top k systems Table 10.

draw several conclusions experiments. First, performance upper limit re-ranking always lower argument-based combination
strategy, even number candidates large. example, 10 individual
models used, F1 upper limit approach Brown corpus 89.80 whereas
F1 upper limit re-ranking 84.10. However, enhanced potential combination approach imply signicant increase computational cost: Table 11 shows
142

fiCombination Strategies Semantic Role Labeling

number candidate arguments must handled combination approaches
much higher number candidate frames input re-ranking system, especially number individual models high. example, 10
individual models used, combination approaches must process around 11 arguments
per sentence, whereas re-ranking approaches must handle approximately 9 frames per sentence. intuition behind relatively small dierence computational cost that,
even though number arguments signicantly larger number frames,
dierence number unique candidates two approaches high
probability repeated arguments higher probability repeated
frames.
second conclusion combination models boost performance
corresponding individual systems. example, best 4-system combination achieves
F1 score approximately 2 points higher best individual model WSJ
Brown corpus. expected, combination models reach performance plateau
around 4-6 individual systems, quality individual models starts drop
signicantly. Nevertheless, considering top 4 individual systems use combination
strategies amount training data experiment quite small,
results show good potential combination models analyzed paper.
third observation relation previously observed local global
rankers holds: combination model local rankers better precision, model
global rankers always better recall generally better PProps score. Overall,
model local rankers obtains better F1 scores scales better number
individual systems increases. discuss dierences detail next
sub-section.
Finally, Table 11 indicates potential recall experiment (shown
left-most block table) higher potential recall combining three
individual SRL systems (see Table 3): 3.8% higher WSJ test set, 7.4% higher
Brown test set. expected, considering number quality
candidate arguments last experiment higher. However, even
improvement, potential recall combination strategies far 100%. Thus,
combining solutions N best state-of-the-art SRL systems still
potential properly solve SRL problem. Future work focus recallboosting strategies, e.g., using candidate arguments individual systems
individual complete solutions generated, step many candidate arguments
eliminated.
8.7 Discussion
experimental results presented section indicate proposed combination
strategies successful: three combination models provide statistically signicant improvements individual models baselines setups. immediate (but
somewhat shallow) comparison three combination strategies investigated indicates
that: (a) best combination strategy SRL problem max-margin local metalearner; (b) global ranking approach meta-learner important
143

fiSurdeanu, Marquez, Carreras, & Comas

contribution max-margin strategy; (c) constraint-satisfaction
model performs worst strategies tried.
However, experiments dierences combination approaches investigated small. reasonable observation combination strategy
advantages disadvantages dierent approaches suitable dierent
applications data. discuss dierences below.
argument probabilities individual systems available, combination model
based constraint satisfaction attractive choice: simple, unsupervised strategy obtains competitive performance. Furthermore, constraint satisfaction model
provides elegant customizable framework tune balance precision
recall (see Section 8.3). framework currently obtain highest recall
combination models: 3.48% higher best recall obtained meta-learning approaches WSJ corpus, 4.83% higher meta-learning models Brown
corpus. higher recall implies higher percentage predicates completely
correctly annotated: best PProps numbers Table 6 best combination
strategies. cause high dierence recall favor constraint satisfaction
approach candidate scoring learning-based inference acts implicitly
lter: candidates whose score i.e., classier condence candidate part
correct solution negative discarded, negatively aects overall recall.
Hence, constraint satisfaction better solution SRL-based NLP applications
require predicate-argument frames extracted high recall. example, Information Extraction, predicate-argument tuples ltered subsequent high-precision,
domain-specic constraints (Surdeanu et al., 2003), hence paramount SRL
model high recall.
Nevertheless, many cases argument probabilities individual SRL models
available, either models generate them, e.g., rule-based systems,
individual models available black boxes, oer access
internal information. conditions, showed combination strategies based
meta-learning viable alternative. fact, approaches obtain highest
F1 scores (see Section 8.4) obtain excellent performance even small amounts
training data (see Section 8.6). previously mentioned, candidate scoring acts
lter, learning-based inference tends favor precision recall: precision
2.82% higher best precision constraint-satisfaction models WSJ
corpus, 3.57% higher Brown corpus. preference precision recall
pronounced learning-based inference local rankers (Section 8.4)
inference model global rankers (Section 8.5). hypothesis causes
global-ranking model less precision-biased conguration ratio
errors positive versus negative samples balanced. Thinking strategy
Perceptron follows, local approach updates every candidate incorrect prediction
sign, whereas global approach updates candidates
complete solution, enforcing domain constraints. words, number
negative updates drives precision bias reduced global approach,
false positives generated ranking classiers eliminated
domain constraints. Thus, candidate scoring trained optimize accuracy,
144

fiCombination Strategies Semantic Role Labeling

WSJ
global feedback
max margin
Brown
global feedback
max margin

PProps
+3.10%
+0.95%

Prec.
-1.96%
+0.67%

Recall
+1.99%
+0.36%

F1
+0.27
+0.49

+2.62%
+1.00%

-2.71%
+1.13%

+2.25%
+0.55%

+0.38
+0.78

Table 13: Contribution global feedback max margin learning-based inference.
baseline Pred-by-pred, local model Table 9.

fewer candidate arguments eliminated meta-learner global rankers,
translates better balance precision recall.
Another important conclusion analysis global versus local ranking
learning-based inference max-margin approach candidate scoring classiers
important global feedback inference. fact, considering
dierence model predicate-by-predicate inference local feedback
Section 8.5 (Pred-by-pred, local) versus best model Section 8.4 (+FS6)
latter uses SVM classiers whereas former uses Perceptron, compute exact
contribution max margin global feedback15 . convenience, summarize
analysis Table 13. table indicates max margin yields consistent improvement
precision recall, whereas contribution global feedback reducing
dierence precision recall boosting recall decreasing precision.
benet max-margin classiers even evident Table 12, shows
local-ranking model max-margin classiers generalizes better global-ranking
model amount training data reduced signicantly.
Even though paper analyzed several combination approaches three
independent implementations, proposed models fact compatible other.
Various combinations proposed strategies immediately possible. example,
constraint satisfaction model applied output probabilities candidate
scoring component introduced Section 7.2. model eliminates dependency
output scores individual SRL models retains advantages
constraint satisfaction model, e.g., formal framework tune balance precision recall. Another possible combination approaches introduced paper
use max-margin classiers learning-based inference global feedback, e.g.,
using global training method margin maximization SVMstruct (Tsochantaridis et al., 2004). model would indeed increased training time16 , could
leverage advantages max-margin classiers inference global feedback
(summarized Table 13). Finally, another attractive approach stacking, i.e., N levels chained meta-learning. example, could cascade learning-based inference
model global rankers, boosts recall, learning-based inference local
rankers, favors precision.
15. contribution global feedback given model joint inference global feedback (Full
sentence, global) Section 8.5.
16. main reason chose Perceptron proposed online strategies.

145

fiSurdeanu, Marquez, Carreras, & Comas

9. Related Work
4 best performing systems CoNLL-2005 shared task included combination
dierent base subsystems increase robustness gain coverage independence
parse errors. Therefore, closely related work paper. rst
four rows Table 10 summarize results exactly experimental setting
one used paper.
Koomen et al. (2005) used 2 layer architecture close ours. pool candidates
generated by: (a) running full syntax SRL system alternative input information (Collins
parsing, 5-best trees Charniaks parser), (b): taking candidates pass
lter set dierent parse trees. combination candidates performed
elegant global inference procedure constraint satisfaction, which, formulated
Integer Linear Programming, solved eciently. dierent work,
break complete solutions number SRL systems investigate
meta-learning combination approach addition ILP inference. Koomen et al.s
system best performing system CoNLL-2005 (see Table 10).
Haghighi et al. (2005) implemented double re-ranking top several outputs
base SRL model. re-ranking performed, rst, set n-best solutions obtained
base system run single parse tree, and, then, set best-candidates
coming n-best parse trees. second-best system CoNLL-2005
(third row Table 10). Compared decomposition re-combination approach,
re-ranking setting advantage allowing denition global features
apply complete candidate solutions. According follow-up work authors
(Toutanova, Haghighi, & Manning, 2005), global features source major
performance improvements re-ranking system. contrast, focus features
exploit redundancy individual models, e.g., overlap individual
candidate arguments, add global information frame level complete
solutions provided individual models. main drawback re-ranking compared
approach dierent individual solutions combined re-ranking
forced select complete candidate solution. implies overall performance
strongly depends ability base model generate complete correct solution
set n-best candidates. drawback evident lower performance upper
limit re-ranking approach (see Tables 3 11) performance actual
system best combination strategy achieves F1 score 2 points higher
Haghighi et al. WSJ Brown17 .
Finally, Pradhan, Hacioglu, Ward, Martin, Jurafsky (2005b) followed stacking
approach learning two individual systems based full syntax, whose outputs used
generate features feed training stage nal chunk-by-chunk SRL system. Although
ne granularity chunking-based system allows recover parsing errors,
nd combination scheme quite ad-hoc forces break argument candidates
chunks last stage.
17. Recently, Yih Toutanova (2006) reported improved numbers system: 80.32 F1 WSJ
68.81 Brown. However, numbers directly comparable systems presented
paper fixed significant bug representation quotes input data, bug
still present data.

146

fiCombination Strategies Semantic Role Labeling

Outside CoNLL shared task evaluation, Roth Yih (2005) reached conclusion quality local argument classiers important global
feedback inference component. one conclusions drawn paper. contribution shown hypothesis holds complex
framework: combination several state-of-the-art individual models, whereas Roth
Yih experimented single individual model, numbered arguments, slightly
simplied problem representation: B-I-O basic chunks. Additionally, detailed
experiments allowed us show clearly contribution max margin higher
global learning several corpora several combinations individual systems.
Punyakanok, Roth, Yih (2005) showed performance individual SRL
models (particularly argument identication) signicantly improved full parsing
used argument boundaries restricted match syntactic constituents (similarly
Model 3). believe approach used Models 1 2, candidate
arguments match single syntactic constituent, increased robustness
built-in mechanism handle syntax errors, argument constituent incorrectly fragmented multiple phrases. empirical results support
claim: Model 2 performs better Model 3 models proposed Punyakanok
et al. second advantage strategy proposed paper model
deployed using full syntax (Model 2) partial syntax (Model 1).
Pradhan, Ward, Hacioglu, Martin, Jurafsky (2005c) implement SRL combination
strategy constituent level that, similarly approach, combines dierent syntactic
views data based full partial syntactic analysis. However, unlike approach,
Pradhan et al.s work uses simple greedy inference strategy based probabilities
candidate arguments, whereas paper introduce analyze three dierent
combination algorithms. analysis yielded combination system outperforms
current state art.
Previous work general eld predicting structures natural language
texts indicated combination several individual models improves overall performance given task. Collins (2000) rst proposed learning layer based ranking
improve performance generative syntactic parser. approach, reranker
trained select best solution pool solutions produced generative
parser. so, reranker dealt complete parse trees, represented
rich features exploited dependencies considered generative method.
hand, computationally feasible train reranker, base method
reduced number possible parse trees sentence exponential number (w.r.t.
sentence length) tens. recently, global discriminative learning methods
predicting structures proposed (Laerty, McCallum, & Pereira, 2001; Collins,
2002, 2004; Taskar et al., 2003, 2004; Tsochantaridis et al., 2004). train
single discriminative ranking function detect structures sentence. major property
methods model problem discriminatively, arbitrary
rich representations structures used. Furthermore, training process
methods global, parameters set maximize measures related
local accuracies (i.e., recognizing parts structure), related global
accuracy (i.e., recognizing complete structures). article, use global
rich representations major motivation.
147

fiSurdeanu, Marquez, Carreras, & Comas

10. Conclusions
paper introduces analyzes three combination strategies context semantic
role labeling: rst model implements inference strategy constraint satisfaction
using integer linear programming, second uses inference based learning
candidates scored using discriminative classiers using local information,
third last inference model builds previous strategy adding global feedback
conict resolution component ranking classiers. meta-learners used
inference process developed rich set features includes voting
statistics i.e., many individual systems proposed candidate argument overlap
arguments predicates sentence, structure distance
information coded using partial full syntax, probabilities individual SRL
models (if available). knowledge, rst work that: (a) introduces thorough
inference model based learning semantic role labeling, (b) performs comparative
analysis several inference strategies context SRL.
results presented suggest strategy decomposing individual solutions
performing learning-based re-combination constructing nal solution advantages approaches, e.g., re-ranking set complete candidate solutions.
course, task-dependant conclusion. case semantic role labeling, approach relatively simple since re-combination argument candidates fulll
set structural constraints generate consistent solution. target structure complex (e.g., full parse tree) re-combination step might complex
learning search perspectives.
evaluation indicates proposed combination approaches successful:
provide signicant improvements best individual model several baseline
combination algorithms setups. three combination strategies investigated,
best F1 score obtained learning-based inference using max-margin classiers.
proposed approaches advantages drawbacks (see Section 8.7
detailed discussion dierences among proposed inference models) several important features state-of-the-art SRL combination strategy emerge analysis:
(i) individual models combined granularity candidate arguments rather
granularity complete solutions frames; (ii) best combination strategy
uses inference model based learning; (iii) learning-based inference benets
max-margin classiers global feedback, (iv) inference sentence level (i.e.,
considering predicates time) proves slightly useful learning
performed globally, using feedback complete solution inference.
Last least, results obtained best combination strategy developed
work outperform current state art. results empirical proof
SRL system good performance built combining small number (three
experiments) relatively simple SRL models.

Acknowledgments
would thank JAIR reviewers valuable comments.
research partially supported European Commission (CHIL project,
148

fiCombination Strategies Semantic Role Labeling

IP-506909; PASCAL Network, IST-2002-506778) Spanish Ministry Education
Science (TRANGRAM, TIN2004-07925-C03-02). Mihai Surdeanu research fellow
within Ramon Cajal program Spanish Ministry Education Science.
grateful Dash Optimization free academic use Xpress-MP.

References
Bishop, C. (1995). Neural Networks Pattern Recognition. Oxford University Press.
Boas, H. C. (2002). Bilingual framenet dictionaries machine translation. Proceedings
LREC 2002.
Carreras, X., & Marquez, L. (2004). Introduction CoNLL-2004 shared task: Semantic
role labeling. Proceedings CoNLL 2004.
Carreras, X., & Marquez, L. (2005). Introduction conll-2005 shared task: Semantic
role labeling. Proceedings CoNLL-2005.
Carreras, X., Marquez, L., & Chrupala, G. (2004). Hierarchical recognition propositional
arguments perceptrons. Proceedings CoNLL 2004 Shared Task.
Charniak, E. (2000). maximum-entropy-inspired parser. Proceedings NAACL.
Collins, M. (1999). Head-Driven Statistical Models Natural Language Parsing. PhD
Dissertation, University Pennsylvania.
Collins, M. (2000). Discriminative reranking natural language parsing. Proceedings
17th International Conference Machine Learning, ICML-00, Stanford, CA
USA.
Collins, M. (2002). Discriminative training methods hidden markov models: Theory
experiments perceptron algorithms. Proceedings SIGDAT Conference
Empirical Methods Natural Language Processing, EMNLP-02.
Collins, M. (2004). Parameter estimation statistical parsing models: Theory practice distribution-free methods. Bunt, H., Carroll, J., & Satta, G. (Eds.), New
Developments Parsing Technology, chap. 2. Kluwer.
Collins, M., & Duy, N. (2002). New ranking algorithms parsing tagging: Kernels
discrete structures, voted perceptron. Proceedings 40th Annual
Meeting Association Computational Linguistics, ACL02.
Crammer, K., & Singer, Y. (2003a). family additive online algorithms category
ranking. Journal Machine Learning Research, 3, 10251058.
Crammer, K., & Singer, Y. (2003b). Ultraconservative online algorithms multiclass
problems. Journal Machine Learning Research, 3, 951991.
Freund, Y., & Schapire, R. E. (1999). Large margin classication using perceptron
algorithm. Machine Learning, 37 (3), 277296.
Gildea, D., & Jurafsky, D. (2002). Automatic labeling semantic roles. Computational
Linguistics, 28 (3).
149

fiSurdeanu, Marquez, Carreras, & Comas

Gildea, D., & Palmer, M. (2002). necessity syntactic parsing predicate argument
recognition. Proceedings 40th Annual Conference Association
Computational Linguistics (ACL-02).
Hacioglu, K., Pradhan, S., Ward, W., Martin, J. H., & Jurafsky, D. (2004). Semantic
role labeling tagging syntactic chunks. Proceedings 8th Conference
Computational Natural Language Learning (CoNLL-2004).
Haghighi, A., Toutanova, K., & Manning, C. (2005). joint model semantic role labeling.
Proceedings CoNLL-2005 Shared Task.
Koomen, P., Punyakanok, V., Roth, D., & Yih, W. (2005). Generalized inference
multiple semantic role labeling systems. Proceedings CoNLL-2005 Shared Task.
Laerty, J., McCallum, A., & Pereira, F. (2001). Conditonal random elds: Probabilistic models segmenting labeling sequence data. Proceedings 18th
International Conference Machine Learning, ICML-01.
Marcus, M., Santorini, B., & Marcinkiewicz, M. (1994). Building large annotated corpus
English: Penn Treebank. Computational Linguistics, 19 (2).
Marquez, L., Comas, P., Gimenez, J., & Catala, N. (2005). Semantic role labeling
sequential tagging. Proceedings CoNLL-2005 Shared Task.
Melli, G., Wang, Y., Liu, Y., Kashani, M. M., Shi, Z., Gu, B., Sarkar, A., & Popowich,
F. (2005). Description SQUASH, SFU question answering summary handler
DUC-2005 summarization task. Proceedings Document Understanding
Workshop, HLT/EMNLP Annual Meeting.
Narayanan, S., & Harabagiu, S. (2004). Question answering based semantic structures.
International Conference Computational Linguistics (COLING 2004).
Noreen, E. W. (1989). Computer-Intensive Methods Testing Hypotheses. John Wiley &
Sons.
Palmer, M., Gildea, D., & Kingsbury, P. (2005). Proposition Bank: annotated
corpus semantic roles. Computational Linguistics, 31 (1).
Ponzetto, S. P., & Strube, M. (2006a). Exploiting semantic role labeling, wordnet
wikipedia coreference resolution. Proceedings Human Language Technolgy
Conference North American Chapter Association Computational Linguistics.
Ponzetto, S. P., & Strube, M. (2006b). Semantic role labeling coreference resolution.
Companion Volume Proceedings 11th Meeting European Chapter
Association Computational Linguistics.
Pradhan, S., Hacioglu, K., Krugler, V., Ward, W., Martin, J. H., & Jurafsky, D. (2005a).
Support vector learning semantic argument classication. Machine Learning, 60,
1139.
Pradhan, S., Hacioglu, K., Ward, W., Martin, J. H., & Jurafsky, D. (2005b). Semantic role
chunking combining complementary syntactic views. Proceedings CoNLL-2005.
150

fiCombination Strategies Semantic Role Labeling

Pradhan, S., Ward, W., Hacioglu, K., Martin, J. H., & Jurafsky, D. (2005c). Semantic role
labeling using dierent syntactic views. Proceedings 43rd Annual Conference
Association Computational Linguistics.
Punyakanok, V., Roth, D., & Yih, W. (2005). necessity syntactic parsing semantic role labeling. Proceedings International Joint Conference Artificial
Intelligence (IJCAI).
Punyakanok, V., Roth, D., Yih, W., & Zimak, D. (2004). Semantic role labeling via integer linear programming inference. Proceedings International Conference
Computational Linguistics (COLING04).
Rosenblatt, F. (1958). perceptron: probabilistic model information storage
organization brain. Psychological Review, 65, 386407.
Roth, D., & Yih, W. (2004). linear programming formulation global inference
natural language tasks. Proceedings Annual Conference Computational
Natural Language Learning (CoNLL-2004), pp. 18, Boston, MA.
Roth, D., & Yih, W. (2005). Integer linear programming inference conditional random
elds. Proceedings International Conference Machine Learning (ICML).
Schapire, R. E., & Singer, Y. (1999). Improved boosting algorithms using condence-rated
predictions. Machine Learning, 37 (3).
Surdeanu, M., Harabagiu, S., Williams, J., & Aarseth, P. (2003). Using predicate-argument
structures information extraction. Proceedings 41st Annual Meeting
Association Computational Linguistics (ACL 2003).
Taskar, B., Guestrin, C., & Koller, D. (2003). Max-Margin Markov Networks. Proceedings
17th Annual Conference Neural Information Processing Systems, NIPS-03,
Vancouver, Canada.
Taskar, B., Klein, D., Collins, M., Koller, D., & Manning, C. (2004). Max-margin parsing.
Proceedings EMNLP-2004.
Toutanova, K., Haghighi, A., & Manning, C. (2005). Joint learning improves semantic role
labeling. Proceedings 43rd Annual Meeting Association Computational Linguistics (ACL05), pp. 589596, Ann Arbor, MI, USA. Association
Computational Linguistics.
Tsochantaridis, I., Hofmann, T., Joachims, T., & Altun, Y. (2004). Support vector machine
learning interdependent structured output spaces. Proceedings 21st
International Conference Machine Learning, ICML-04.
Xue, N., & Palmer, M. (2004). Calibrating features semantic role labeling. Proceedings
EMNLP-2004.
Yih, S. W., & Toutanova, K. (2006). Automatic semantic role labeling. Tutorial
Human Language Technolgy Conference North American Chapter
Association Computational Linguistics.
Younger, D. H. (1967). Recognition parsing context-free languages n3 time.
Information Control, 10 (2), 189208.

151


