journal artificial intelligence

submitted published

combination strategies semantic role labeling
mihai surdeanu
llus marquez
xavier carreras
pere r comas

surdeanu lsi upc edu
lluism lsi upc edu
carreras lsi upc edu
pcomas lsi upc edu

technical university catalonia
c jordi girona
barcelona spain

abstract
introduces analyzes battery inference semantic role labeling one constraint satisfaction several strategies model
inference meta learning discriminative classiers classiers
developed rich set novel features encode proposition sentence level
information knowledge rst work performs thorough analysis learning inference semantic role labeling b compares several
inference strategies context evaluate proposed inference strategies
framework conll shared task automatically generated syntactic
information extensive experimental evaluation analysis indicates
proposed inference strategies successful outperform current best
reported conll evaluation exercise proposed approaches
advantages disadvantages several important traits state art srl combination strategy emerge analysis individual combined
granularity candidate arguments rather granularity complete solutions
ii best combination strategy uses inference model learning iii
learning inference benets max margin classiers global feedback

introduction
natural language understanding nlu subeld articial intelligence ai
deals extraction semantic information available natural language texts
knowledge used develop high level applications requiring textual document
understanding question answering information extraction nlu complex
ai complete needs venture well beyond syntactic analysis natural
language texts state art nlu still far reaching goals recent
made important progress subtask nlu semantic role labeling
task semantic role labeling srl process detecting basic event structures
see figure sample sentence annotated
event frame
motivation
srl received considerable interest past years gildea jurafsky
surdeanu harabagiu williams aarseth xue palmer pradhan hac

ai access foundation rights reserved

fisurdeanu marquez carreras comas

cioglu krugler ward martin jurafsky carreras marquez
shown identication event frames signicant contribution many
nlu applications information extraction surdeanu et al question answering narayanan harabagiu machine translation boas summarization melli wang liu kashani shi gu sarkar popowich coreference
resolution ponzetto strube b
syntactic perspective machine learning srl approaches classied
one two classes approaches take advantage complete syntactic analysis text
pioneered gildea jurafsky approaches use partial syntactic analysis
championed previous evaluations performed within conference computational
natural language learning conll carreras marquez wisdom
extracted rst representation indicates full syntactic analysis signicant
contribution srl performance hand corrected syntactic information gildea
palmer hand automatically generated syntax available
quality information provided full syntax decreases state ofthe art full parsing less robust performs worse tools used partial
syntactic analysis real world conditions dierence two srl
approaches full partial syntax high interestingly two srl
strategies perform better different semantic roles example use full
syntax recognize agent theme roles better whereas partial syntax
better recognizing explicit patient roles tend farther predicate
accumulate parsing errors marquez comas gimenez catala

article explore implications observations studying strategies
combining output several independent srl systems take advantage
dierent syntactic views text given sentence combination receive
labeled arguments individual systems produce overall argument structure
corresponding sentence proposed combination strategies exploit several levels
information local global features individual constraints
argument structure work investigate three dierent approaches
rst combination model parameters estimate makes use
argument probabilities output individual constraints argument
structures build overall solution sentence call model inference
constraint satisfaction
second implements cascaded inference model local learning rst
type argument classier trained oine decides whether candidate
nal argument next candidates passed previous step
combined solution consistent constraints argument structures
refer model inference local learning
third inference model global number online ranking functions one
argument type trained score argument candidates correct
argument structure complete sentence globally ranked top call
model inference global learning


ficombination strategies semantic role labeling



np

np

vp
np

pp

luxury auto maker last year sold cars u

agent

amtmp
temporal
marker

p


predicate object

amloc
locative
marker

figure sample sentence propbank corpus
proposed combination strategies general depend way
candidate arguments collected empirically prove experimenting
individual srl systems developed house best systems
conll shared task evaluation
contribution
work introduced several novel points knowledge
rst work thoroughly explores inference model meta learning
second third inference introduced context srl investigate metalearning combination strategies rich global representations form local
global features form structural constraints solutions empirical
analysis indicates combination strategies outperform current state
art note combination strategies proposed ranking
approaches haghighi toutanova manning collins whereas ranking
selects overall best solution pool complete solutions individual
combination approaches combine candidate arguments incomplete solutions
different individual better potential e upper
limit f score higher performance better several corpora
second novelty performs comparative analysis several combination strategies srl framework e pool
candidates evaluation methodology large number combination
approaches previously analyzed context srl larger context
predicting structures natural language texts e g inference constraint satisfaction koomen punyakanok roth yih roth yih inference
local learning marquez et al ranking collins haghighi et al etc
still clear strategy performs best semantic role labeling


fisurdeanu marquez carreras comas

provide empirical answers several important questions respect example
combination strategy constraint satisfaction better inference model
learning important global feedback learning inference model
analysis indicates following issues important traits state art
combination srl system individual combined argument granularity
rather granularity complete solutions typical ranking ii best
combination strategy uses inference model learning iii learning
inference benets max margin classiers global feedback
organized follows section introduces semantic corpora used
training evaluation section overviews proposed combination approaches
individual srl introduced section evaluated section section
lists features used three combination introduced
combination described section section introduces empirical analysis proposed combination methods section reviews related work
section concludes

semantic corpora
used propbank approximately one million word corpus annotated
predicate argument structures palmer gildea kingsbury date propbank addresses predicates lexicalized verbs besides predicate argument structures
propbank contains full syntactic analysis sentences extends wall street
journal wsj part penn treebank corpus previously annotated
syntactic information marcus santorini marcinkiewicz
given predicate survey carried determine predicate usage
required usages divided major senses however senses divided
syntactic grounds semantic following assumption syntactic frames
direct reection underlying semantics arguments predicate numbered sequentially generally stands agent theme direct
object indirect object benefactive instrument semantics tend verb
specic additionally predicates might adjunctive arguments referred ams
example loc indicates locative tmp indicates temporal figure shows
sample propbank sentence one predicate sold arguments regular
adjunctive arguments discontinuous case trailing argument fragments
prexed c e g funds predicate expected ca begin operation
around march finally propbank contains argument references typically pronominal
share label actual argument prexed r
use syntactic information penn treebank instead
develop automatically generated syntax named entity ne labels
made available conll shared task evaluation carreras marquez
conll data use syntactic trees generated charniak parser char original propbank annotations co referenced arguments appear single item differentiation referent reference use version data used conll
shared tasks reference arguments automatically separated corresponding referents
simple pattern matching rules



ficombination strategies semantic role labeling

niak develop two individual full syntactic analysis chunk
e basic syntactic phrase labels clause boundaries construct partial syntax
model individual use provided ne labels
switching hand corrected automatically generated syntactic information means
propbank assumption argument argument fragment discontinuous arguments maps one syntactic phrase longer holds due errors syntactic
processors analysis propbank data indicates semantic
arguments matched exactly one phrase generated charniak parser essentially means srl approaches make assumption semantic
argument maps one syntactic construct recognize almost arguments
statement made approaches partial syntax caveat
setup arguments match sequence chunks however one expects
degree compatibility syntactic chunks semantic arguments higher
due ner granularity syntactic elements chunking perform better full parsing indeed analysis propbank data
supports observation semantic arguments matched sequence
chunks generated conll syntactic chunker
following conll setting evaluated system propbank
fresh test set derived brown corpus second evaluation allows us
investigate robustness proposed combination

overview combination strategies
introduce analyze three combination strategies
semantic role labeling three combination strategies implemented shared
framework detailed figure consists several stages generation candidate arguments b candidate scoring nally c inference clarity describe
rst proposed combination framework e vertical ow figure move
overview three combination methodologies shown horizontally figure
candidate generation step merge solutions three individual srl
unique pool candidate arguments individual srl range complete
reliance full parsing partial syntactic information example model
developed sequential tagger b tagging scheme partial
syntactic information basic phrases clause boundaries whereas model uses full
syntactic analysis text handles arguments map exactly one syntactic
constituent detail individual srl section empirically evaluate
section
candidate scoring phrase score candidate arguments local
information e g syntactic structure candidate argument global information
e g many individual generated similar candidate arguments describe
features used candidate scoring section
finally inference stage combination search best solution
consistent domain constraints e g two arguments predicate cannot
overlap embed predicate may one core argument etc


fisurdeanu marquez carreras comas

reliance full syntax

model

model

model

candidate
generation
candidate argument
pool

constraint
satisfaction
engine

solution

inference
constraint satisfaction

learning
batch

learning
online

dynamic
programming
engine

dynamic
programming
engine

solution

candidate
scoring

inference

solution

inference
local learning

inference
global learning

figure overview proposed combination strategies

combination approaches proposed share candidate argument pool guarantees obtained dierent strategies
corpus comparable hand even though candidate generation
step shared three combination methodologies dier signicantly scoring
inference
rst combination strategy analyzed inference constraint satisfaction skips
candidate scoring step completely uses instead probabilities output individual srl candidate argument individual raw activations
actual probabilities convert probabilities softmax function bishop
passing inference component inference implemented
constraint satisfaction model searches solution maximizes certain
compatibility function compatibility function probability
global solution consistency solution according domain constraints
combination strategy technique presented koomen et al
main dierence two systems candidate generation step use
three independent individual srl whereas komen et al used srl model


ficombination strategies semantic role labeling

trained dierent syntactic views data e top parse trees generated
charniak collins parsers charniak collins furthermore take
argument candidates set complete solutions generated individual whereas komen et al take dierent syntactic trees constructing
complete solution obvious advantage inference model constraint satisfaction unsupervised learning necessary candidate scoring
scores individual used hand constraint satisfaction
model requires individual provide raw activations moreover
raw activations convertible true probabilities
second combination strategy proposed article inference local learning
scores candidates pool set binary discriminative classiers
classiers assign argument score measuring condence argument
part correct global solution classiers trained batch mode
completely decoupled inference module inference component implemented
cky dynamic programming younger main advantage
strategy candidates scored signicantly information
available individual model example incorporate features count
number individual systems generated given candidate argument several
types overlaps candidate arguments predicate arguments
predicates structural information full partial syntax etc
describe rich feature set used scoring candidate arguments section
combination depend argument probabilities individual
srl incorporate features available combination
complex previous strategy additional step requires
supervised learning candidate scoring nevertheless mean additional
corpus necessary cross validation candidate scoring classiers trained
corpus used train individual srl moreover section
obtain excellent performance even candidate scoring classiers trained
signicantly less data individual srl
finally inference strategy global learning investigates contribution global
information inference model learning strategy incorporates global
information previous inference model two ways first importantly candidate scoring trained online global feedback inference component
words online learning corrects mistakes found comparing
correct solution one generated inference second integrate global information actual inference component instead performing inference proposition
independently whole sentence allows implementation
additional global domain constraints e g arguments attached dierent predicates
overlap
combination strategies proposed described detail section evaluated
section


fisurdeanu marquez carreras comas

individual srl
section introduces three individual srl used combination strategies discussed rst two variations
model srl sequential tagging task semantic argument
matched sequence non embedding phrases model uses partial syntax
chunks clause boundaries whereas model uses full syntax third model takes
traditional assuming exists one one mapping
semantic arguments syntactic phrases
important note combination strategies introduced later
independent individual srl used fact section describe
experiments use individual best performing srl
systems conll evaluation carreras marquez nevertheless
choose focus mainly individual srl approaches presented section
completeness state art performance possible relatively simple
srl

srl sequential tagging task pre processing step
input syntactic structures traversed order select subset constituents organized
sequentially e non embedding output process sequential tokenization
input sentence verb predicates labeling tokens appropriate
tags allows us codify complete argument structure predicate sentence
precisely given verb predicate sequential tokens selected follows
first input sentence split disjoint sequential segments markers
segment start end verb position boundaries clauses include
corresponding predicate constituent second segment set top
non overlapping syntactic constituents completely falling inside segment selected
tokens finally tokens labeled b tags depending
beginning inside outside predicate argument note strategy provides set
sequential tokens covering complete sentence independent syntactic
annotation explored assuming provides clause boundaries
consider example figure depicts propbank annotation two verb
predicates sentence release hope corresponding partial full parse
trees since verbs main clause sentence two segments
sentence considered predicates e dening left right contexts
verbs w others w w w big time predicate release
w others w w w big time predicate hope figure
shows resulting tokenization predicates two alternative syntactic structures case correct argument annotation recovered cases assuming
perfect labeling tokens
worth noting resulting number tokens annotate much lower
number words cases codications coming full parsing
substantially fewer tokens coming partial parsing example
predicate hope dierence number tokens two syntactic views


ficombination strategies semantic role labeling

clause
np





vp
clause

vp


np



advp



ii

np

pp

vp









others

released majors hope senior league

bridge back bigtime

clause




np





advp

ii
others



amtmp

iii

vp

iv

pp

v



np

vi



vp

np


vii
released majors hope senior league
p


clause

vp

np

viii

advp pp

np

bridge back bigtime


p



figure annotation example sentence two alternative syntactic structures
lower tree corresponds partial parsing annotation pp base chunks
clause structure upper represents full parse tree fp semantic roles
two predicates release hope provided sentence
encircled nodes trees correspond selected nodes process
sequential tokenization sentence mark selected nodes
predicate release western numerals nodes selected hope
roman numerals see figure details

particularly large vs tokens obviously coarser token granularity easier
assigning correct output labelings e less tokens label
long distance relations among sentence constituents better captured
hand coarser granularity tends introduce unrecoverable errors
pre processing stage clear trade dicult solve advance
two combination scheme take advantage diverse sentence
tokenizations see sections
compared common tree node labeling approaches e g following
model b annotation tokens advantage permitting correctly annotate arguments match unique syntactic constituent bad side
heuristic pre selection candidate nodes predicate e nodes
sequentially cover sentence makes number unrecoverable errors higher another source errors common strategies errors introduced real partial full
parsers calculated due syntactic errors introduced pre processing
stage upper bound recall gures model model
datasets dened section


fisurdeanu marquez carreras comas

words
others


released


majors

hope

senior
league



bridge
back


big time

releasepp
b

b tmp

b




tokens
releasefp
hopepp
b
b

ii
b tmp
iii

iv
v
b
vi


hopefp

b

vii




viii b

ii b




figure sequential tokenization sentence figure according two syntactic
views predicates pp stands partial parsing fp full parsing
sentence semantic role annotations vertically displayed token
numbered indexes appear tree nodes figure contains
b annotation needed codify proper semantic role structure

approaching srl sequential tagging task hacioglu pradhan ward
martin jurafsky presented system sequential tagging base chunks
b labels best performing srl system conll shared
task carreras marquez novelty resides fact
sequence syntactic tokens label extracted hierarchical syntactic annotation
partial full parse tree restricted base chunks e token
may correspond complex syntactic phrase even clause
features
tokens selected labeled b tags converted training
examples considering rich set features mainly borrowed state art systems gildea jurafsky carreras marquez chrupala xue palmer
features codify properties focus token b target predicate
c sentence fragment token predicate dynamic context
e b labels previously generated describe four feature sets next
features extracted partial parsing named entities common model features
coming full parse trees apply model



ficombination strategies semantic role labeling

constituent structure features
constituent type head extracted head word rules collins
rst element pp chunk head rst np extracted
example type constituent u figure pp head
u instead
first last words pos tags constituent e g u nnp
constituent u figure
pos sequence less tags long e g indtnnp sample
constituent
grams pos sequence
bag words nouns adjectives adverbs example bag nouns
constituent luxury auto maker luxury auto maker
top sequence sequence types top syntactic elements constituent
less elements long case full parsing corresponds
right hand side rule expanding constituent node example top
sequence constituent u innp
grams top sequence
governing category described gildea jurafsky indicates np
arguments dominated sentence typical subjects verb phrase typical
objects example governing category constituent cars
figure vp hints corresponding semantic role object
namedentity indicating constituent embeds strictly matches named entity
along type example constituent u embeds locative
named entity u
tmp indicating constituent embeds strictly matches temporal keyword
automatically extracted tmp arguments training set among
common temporal cue words extracted year yesterday week month
etc used total cue words
previous following words pos tag constituent example
previous word constituent last year figure maker nn next
one sold vbd
features characterizing focus constituents extracted two previous
following tokens provided inside boundaries current segment
predicate structure features
predicate form lemma pos tag e g sold sell vbd predicate
figure
chunk type cardinality verb phrase verb included single word
multi word example predicate figure included single word vp
chunk


fisurdeanu marquez carreras comas

predicate voice distinguish voice types active passive copulative
innitive progressive
binary ag indicating verb start end clause
sub categorization rule e phrase structure rule expands predicates
immediate parent e g np np vp predicate figure
predicate constituent features
relative position distance words chunks level embedding number
clause levels respect constituent example constituent
u figure appears predicate distance words chunk
level embedding
constituent path described gildea jurafsky grams
path constituents beginning verb predicate ending constituent
example syntactic path constituent luxury auto maker
predicate sold figure np vp vbd
partial parsing path described carreras et al grams path
elements beginning verb predicate ending constituent example
path np pp np vp vbd indicates current np token
predicate pp np constituents right positive sign
level token path descends clause vp
nd predicate dierence previous constituent path
arrows anymore introduce horizontal left right movements
syntactic level
syntactic frame described xue palmer syntactic frame captures
overall sentence structure predicate constituent pivots
example syntactic frame predicate sold constituent
u npnpvpnppp current predicate constituent emphasized
knowing noun phrases predicate lowers probability
constituent serves agent
dynamic features
biotag previous token training correct labels left context
used testing feature dynamically codied tag previously
assigned srl tagger
learning sequence tagging
used generalized adaboost real valued weak classiers schapire singer
base learning version learns xed depth small decision
trees weak rules combined ensemble constructed adaboost
implemented simple one vs decomposition address multi class classication
way separate binary classier learned b x x argument label
plus extra classier decision


ficombination strategies semantic role labeling

adaboost binary classiers used labeling test sequences left right
recurrent sliding window information tags assigned
preceding tokens explained previous list features left tags already assigned
dynamically codied features empirically found optimal left context
taken account reduces previous token
tested two dierent tagging procedures first greedy left right assignment
best scored label token second viterbi search label sequence
maximizes probability complete sequence case classiers predictions
converted probabilities softmax function described section
signicant improvements obtained latter selected former
faster basic tagging experiments
finally tagging model enforces three basic constraints b output labeling must codify correct structure b arguments cannot overlap clause chunk
boundaries c verb arguments present propbank frames taking
union rolesets dierent verb senses considered
model
third individual srl model makes strong assumption predicate argument
maps one syntactic constituent example figure maps noun phrase
loc maps prepositional phrase etc assumption holds well hand corrected
parse trees simplies signicantly srl process one syntactic constituent correctly classied order recognize one semantic argument
hand limited automatically generated syntactic trees
example arguments mapped one syntactic constituents
produced charniak parser
bottom model maps argument rst syntactic constituent exact boundaries climbs high possible
tree across unary production chains currently ignore arguments map
single syntactic constituent argument constituent mapping performed
training set preprocessing step figure shows mapping example semantic
arguments one verb corresponding sentence syntactic structure
mapping process completes model extracts rich set lexical syntactic
semantic features features inspired previous work parsing
srl collins gildea jurafsky surdeanu et al pradhan et al
describe complete feature set implemented model next
features
similarly group features three categories properties
codify argument constituent b target predicate c relation
constituent predicate syntactic constituents
constituent structure features
syntactic label candidate constituent

constituent head word suffixes length lemma pos tag


fisurdeanu marquez carreras comas

constituent content word suffixes length lemma pos tag ne
label content words add informative lexicalized information dierent
head word detected heuristics surdeanu et al example
head word verb phrase placed auxiliary verb whereas
content word placed similarly content word prepositional phrases
preposition selected head word rather head
word attached phrase e g u prepositional phrase u
first last constituent words pos tags
ne labels included candidate phrase
binary features indicate presence temporal cue words e words appear
often tmp phrases training used list temporal cue words

treebank syntactic label added feature indicate number
labels included candidate phrase
top sequence constituent constructed similarly model
phrase label head word pos tag constituent parent left sibling
right sibling
predicate structure features
predicate word lemma
predicate voice denition
binary feature indicate predicate frequent e appears
twice training data
sub categorization rule denition
predicate constituent features
path syntactic tree argument phrase predicate
chain syntactic labels along traversal direction
computed similarly model
length syntactic path
number clauses phrases path store overall clause count
number clauses ascending descending part path
number verb phrases vp path similarly feature store
three numbers overall verb count verb count ascending descending
part path
generalized syntactic paths generalize path syntactic tree
appears elements two templates arg ancestor ni
pred arg argument label pred predicate label ancestor
label common ancestor ni instantiated labels


ficombination strategies semantic role labeling

pred ancestor full path b arg ni ancestor pred ni
instantiated labels arg ancestor full path
example path np vp sbar vp argument label rst np
predicate label last vp common ancestors label rst hence
last template path generalized following three features np
vp vp np sbar vp np vp generalization reduces
sparsity complete constituent predicate path feature dierent strategy
implement n gram
subsumption count e dierence depths syntactic tree
argument predicate constituents value two phrases share
parent
governing category similar

surface distance predicate argument phrases encoded
number tokens verb terminals vb commas coordinations cc argument predicate phrases binary feature indicate two
constituents adjacent example surface distance argument
candidate others predicate hope figure example others
released majors hope senior league tokens verb commas
coordinations features originally proposed collins dependency parsing model capture robust syntax independent information
sentence structure example constituent unlikely argument
verb another verb appears two phrases

binary feature indicate argument starts predicate particle e
token seen rp pos tag directly attached predicate training
motivation feature avoid inclusion predicate particles
argument constituent example without feature srl system tend
incorrectly include predicate particle argument text take
organization marked text commonly incorrectly parsed
prepositional phrase large number prepositional phrases directly attached
verb arguments corresponding predicate
classifier
similarly model trains one vs classiers adaboost
common argument labels reduce sample space model selects training examples
positive negative rst clause includes predicate
b phrases appear left predicate sentence
argument constituents fall one classes
prediction time classiers combined simple greedy technique
iteratively assigns predicate argument classied highest condence
predicate consider candidates attributes numbered attributes
indicated corresponding propbank frame additionally greedy strategy enforces
limited number domain knowledge constraints generated solution arguments
overlap form b duplicate arguments allowed c


fisurdeanu marquez carreras comas

predicate numbered arguments e subset present
propbank frame constraints somewhat dierent constraints used
model use b representation hence constraint
b labeling correct apply ii enforce
constraint numbered arguments duplicated implementation
straightforward architecture

performance individual
section analyze performance three individual srl proposed
three srl systems trained complete conll training set propbank treebank sections avoid overtting syntactic processors e
part speech tagger chunker charniaks full parser partitioned propbank
training set folds fold used output syntactic processors
trained four folds tuned separate development partition treebank section evaluated two corpora treebank section
consists wall street journal wsj documents b three sections
brown corpus semantically annotated propbank team conll shared
task evaluation
classiers individual developed adaboost decision trees depth e branch may represent conjunction basic
features classication model trained rounds applied
simplications keep training times memory requirements inside admissible bounds
trained frequent argument labels top model top
model top model b discarded features occurring less
times training set c model classier limited number
negative training samples rst negative samples extracted propbank
traversal
table summarizes three wsj brown corpora
include percentage perfect propositions detected model pprops
e predicates recognized arguments overall precision recall f
measure summarized table indicate individual systems
solid performance although none would rank top conll evaluation carreras marquez performance comparable best
individual systems presented evaluation exercise consistently systems
evaluated brown corpus experience severe performance drop
corpus due lower performance linguistic processors
expected full parsing perform better model
partial syntax interestingly dierence large e g less points
distribution samples model classifiers biased towards negative samples
worst case syntactic constituent sentence predicate potential argument
significance intervals f measure obtained bootstrap resampling noreen
f rates outside intervals assumed significantly different related f
rate p
best performing srl systems conll combination several subsystems see section
details



ficombination strategies semantic role labeling

wsj
model
model
model
brown
model
model
model

pprops




precision




recall




f




















table overall individual wsj brown test sets
model f
model f
model f


























table f scores individual systems arguments wsj test
f wsj corpus evincing base syntactic chunks clause boundaries
enough obtain competitive performance importantly full parsing
better partial syntax model table lists f measure three
rst numbered arguments table shows model overall
best performing individual system achieves best f measure typically
subjects direct objects model partial syntax model performs best
typically indirect objects instruments benefactives explanation
behavior indirect objects tend farther predicates accumulate
parsing errors full syntax model better recall
whereas model better precision model lters candidate arguments
match single syntactic constituent generally table shows
strong weak points justication focus combination
strategies combine several independent

features combination
detailed section analyze two classes combination strategies
semantic role labeling inference model constraint satisfaction
nds set candidate arguments maximizes global cost function b
two inference strategies learning candidates scored ranked
discriminative classiers perspective feature space main dierence
two types combination input rst combination strategy limited argument probabilities produced individual systems
whereas last class combination approaches incorporates much larger feature set
ranking classiers robustness use features extracted solutions provided individual systems hence independent


fisurdeanu marquez carreras comas





































v

v





v













figure sample solutions proposed predicate three individual srl
argument candidates displayed vertically system

individual describe features next examples given section
figures
voting features features quantify votes received argument
individual systems set includes following features
label candidate argument e g rst argument proposed system
figure
number systems generated argument label span
example shown figure feature value argument proposed
system proposed argument
unique ids systems generated argument label
span e g argument proposed figure
argument sequence predicate systems generated argument label span example argument sequence generated
system proposition illustrated figure v feature attempts capture information proposition level e g combination model
might learn trust model argument sequence v
another sequence etc
predicate overlap features features measure overlap dierent
arguments produced individual srl predicate
exception argument probabilities required constraint satisfaction model



ficombination strategies semantic role labeling

number unique ids systems generated argument
span different label example shown figure features
values argument proposed model proposed
argument span
number unique ids systems generated argument included
current argument candidate argument proposed model
figure features values generated argument
included
spirit generate number unique ids systems
generated argument contains current argument number
unique ids systems generated argument overlaps
include contain current argument
predicate overlap features features quantify overlap dierent arguments produced individual srl predicates generate
features previous feature group dierence compare arguments generated dierent predicates motivation overlap features
according propbank annotations form overlap allowed among arguments
attached predicate inclusion containment permitted
arguments assigned dierent predicates overlap features meant detect
domain constraints satised candidate argument indication
evidence strong candidate incorrect
partial syntax features features codify structure argument
distance argument predicate partial syntactic information
e chunks clause boundaries see figure example note features
inherently dierent features used model model evaluates
individual chunk part candidate argument whereas codify properties
complete argument constituent describe partial syntax features
length tokens chunks argument constituent e g argument
figure
sequence chunks included argument constituent e g pp np
argument loc figure chunk sequence large store n grams
length start end sequence
sequence clause boundaries e clause beginning ending included
argument constituent
named entity types included argument constituent e g location
loc argument figure
position argument predicate sentence e g
figure
boolean ag indicate argument constituent adjacent predicate
e g false true figure


fisurdeanu marquez carreras comas

clause

np

np

vp

np

pp

np

luxury auto maker last year sold cars u


amtmp

p



amloc

figure sample proposition partial syntactic information
sequence chunks argument constituent predicate e g
chunk sequence predicate argument loc figure np
similarly chunk sequence feature sequence large store
starting ending n grams
number chunks predicate argument e g loc
figure
sequence clause boundaries argument constituent predicate
clause subsumption count e dierence depths clause
tree argument predicate constituents value two phrases
included clause
full syntax features features codify structure argument constituent
predicate distance two full syntactic information
full syntax features replicated model see section assumes
one one mapping semantic constituents syntactic phrases exists unlike model
ignores arguments matched syntactic constituent
exact mapping exist due inclusion candidates
generate approximate mapping unmapped semantic constituent largest
phrase included given span left boundary semantic constituent heuristic guarantees capture least semantic
constituents syntactic structure
motivation partial full syntax features learn preferences
individual srl example features combination classier might
learn trust model arguments closer chunks predicate model
predicate argument syntactic path np vp sbar vp etc
individual systems argument probabilities individual model outputs condence score proposed arguments scores converted probabilities softmax function described detail section combination
strategy constraint satisfaction section uses probabilities
two strategies meta learning section discretize
probabilities include features probability value matched


ficombination strategies semantic role labeling

one probability intervals corresponding interval used feature
probability intervals dynamically constructed argument label individual system corresponding system predictions argument label
uniformly distributed across intervals
section empirically analyze contribution proposed feature
sets performance best combination model

combination strategies
section detail combination strategies proposed combination
model constraint satisfaction aims nding set candidate arguments
maximizes global cost function b two combination inference
learning candidates scored ranked discriminative classiers
previous section described complete feature set made available approaches
focus machine learning paradigm deployed combination

inference constraint satisfaction
constraint satisfaction model selects subset candidate arguments maximizes
compatibility function subject fulllment set structural constraints
ensure consistency solution compatibility function probabilities
given individual srl candidate arguments work use integer
linear programming solve constraint satisfaction rst
proposed roth yih applied semantic role labeling punyakanok
roth yih zimak koomen et al among others follow setting
komen et al taken reference
rst step scores model normalized probabilities scores
yielded classiers signed unbounded real numbers experimental evidence
shows condence predictions taken absolute value raw scores
correlates well classication accuracy thus softmax function bishop
used convert set unbounded scores probabilities k possible
output labels given argument sco li denotes score label li output
xed srl model estimated probability label
esco li
p li pk
sco lj
j e
parameter formula empirically adjusted avoid overly skewed
probability distributions normalize scores three individual
similar range values see details experimental setting section
candidate selection performed via integer linear programming ilp program
goal maximize compatibility function modeling global condence selected
set candidates subject set linear constraints variables involved
task take integer values may appear rst degree polynomials
abstract ilp process described simple fashion given set variables v v vn aims maximize global compatibility label assignment


fisurdeanu marquez carreras comas

l ln variables local compatibility function cv l denes compatibility
assigning label l variable v global compatibility function c l ln taken
sum local assignment compatibility goal ilp process
written
argmax c l ln argmax
l ln

l ln

n
x

cvi li



constraints described set accompanying integer linear equations involving variables
one wants codify soft constraints instead hard possibility considering penalty component compatibility function case constraint
r r seen function takes current label assignment outputs
real number constraint satised positive number
indicating penalty imposed compatibility function expression
compatibility function maximize
c l ln

n
x


cvi li

x

r l ln

rr

note hard constraints simulated setting making output
large positive number violated
particular binary valued variable vi n argument candidates generated srl e li labels given label
assignment arguments li selected form solution others
li ltered variable vi probability
values pij calculated score model j argument according softmax
formula described rst compatibility function cv li equals
p


j pij li number case
denition maximizing compatibility function equivalent maximizing
sum probabilities given argument candidates considered
solution since function positive global score increases directly
number selected candidates consequence model biased towards
maximization number candidates included solution e g tending select
lot small non overlapping arguments following koomen et al bias
corrected adding score oi sums compatibility function
th candidate selected solution global compatibility function needs
rewritten encompass information formalized ilp equation looks
argmax c l ln argmax

l n

l n


n x
x



j

pij li oi li

model j propose argument consider pij
instead accumulating probabilities given candidate argument one could consider
different variable model prediction introduce constraint forcing variables
take value end optimization two alternatives equivalent



ficombination strategies semantic role labeling

constraints expressed separated integer linear equations possible
dene priori value oi komen et al used validation corpus empirically
estimate constant value oi e independent argument candidate
use exactly solution working single constant value
refer
regarding consistency constraints considered following six
two candidate arguments verb overlap embed
verb may two core arguments type label
argument r x verb x argument
verb
argument c x verb x argument
c x verb
arguments two dierent verbs overlap embed
two dierent verbs share x r x c x arguments
constraints included reference work punyakanok et al
constraints need checked since individual model
outputs consistent solutions constraints restrict set compatible
arguments among dierent predicates sentence original work
integer linear programming setting constraints written inequalities example
ai argument label th candidate vi verb predicate constraint number
p
written ai vi v li given verb v argument label
constraints similar translations inequalities
constraint satisfaction optimization applied two dierent ways obtain
complete output annotation sentence rst one proceed verb verb
independently nd best selection candidate arguments constraints
call local optimization second scenario
candidate arguments sentence considered constraints
enforced refer second strategy global optimization scenarios
compatibility function constraints need rewriting global
scenario include information concrete predicate
section extensively evaluate presented inference model constraint satisfaction describe experiments covering following topics
contribution proposed constraints b performance local vs global
optimization c precisionrecall tradeo varying value bias correction
parameter
inference learning
combination model consists two stages candidate scoring phase scores
candidate arguments pool series discriminative classiers inference
stage selects best overall solution consistent domain constraints
instead working constant one could try set oi value candidate taking
account contextual features candidate plan explore option near future



fisurdeanu marquez carreras comas

rst important component combination strategy candidate
scoring module assigns candidate argument score equal condence
argument part global solution formed discriminative functions
one role label devise two dierent strategies train discriminative
functions
scoring candidate arguments nal global solution built inference
module looks best scored argument structure satises domain specic
constraints global solution subset candidate arguments score
dened sum condence values arguments form currently consider
three constraints determine solutions valid
candidate arguments predicate overlap embed
b predicate duplicate arguments allowed numbered arguments
c arguments predicate embedded within arguments predicates
overlap
set constraints extended rules particular case
know constraints e g providing arguments indicated corresponding propbank frame already guaranteed individual others e g
constraints previous sub section positive impact overall
performance see section empirical analysis inference use
bottom cky dynamic programming strategy younger builds
solution maximizes sum argument condences satisfying constraints
cubic time
next describe two dierent strategies train functions score candidate
arguments rst local strategy function trained binary batch classier
independently combination process enforces domain constraints
second global strategy functions trained online rankers taking account
interactions take place combination process decide one argument
another
training strategies discriminative functions employ representation arguments complete feature set described section analyze
contribution feature group section intuition rich feature
space introduced section allow gathering sucient statistics robust
scoring candidate arguments example scoring classiers might learn
candidate trusted two individual systems proposed b label
generated model c proposed model within certain
argument sequence
learning local classifiers
combination process follows cascaded architecture learning component
decoupled inference module particular training strategy consists
training binary classier role label target label classier
determine whether candidate argument actually belongs correct proposition
corresponding predicate output condence value decision


ficombination strategies semantic role labeling

specic training strategy follows training data consists pool
labeled candidate arguments proposed individual systems candidate
positive actually correct argument sentence negative
correct strategy trains binary classier role label l independently
labels concentrates candidate arguments data
label l forms dataset binary classication specic label l
binary classier trained existing techniques binary classication
requirement combination strategy needs condence values
binary prediction section provide experiments svms train local
classiers
classier trained independently classiers inference module looking globally combination process classier seen argument
ltering component decides candidates actual arguments much richer
representation individual context inference engine used
conict resolution engine ensure combined solutions valid argument structures sentences
learning global rankers
combination process couples learning inference e scoring functions
trained behave accurately within inference module words training
strategy global target train global function maps set argument
candidates sentence valid argument structure setting global function
composition scoring functions one label previous strategy
unlike previous strategy completely decoupled inference engine
policy map set candidates solution determined inference
engine
recent years active global learning methods tagging
parsing general structure prediction collins taskar guestrin
koller taskar klein collins koller manning tsochantaridis hofmann
joachims altun article make use simplest technique global
learning online learning uses perceptron collins general
idea similar original perceptron rosenblatt correcting
mistakes linear predictor made visiting training examples additive
manner key point learning global rankers relies criteria determines
mistake function trained idea exploited
similar way multiclass ranking scenarios crammer singer b
perceptron combination system works follows pseudocode
given figure let l possible role labels let
w w wl set parameter vectors scoring functions one
label perceptron initializes vectors w zero proceeds cycle
training examples visiting one time case training example pair
correct solution example set candidate arguments
note sets labeled arguments thus make use
set dierence note particular argument l label


fisurdeanu marquez carreras comas

initialization wl w wl
training

training example
inference w

let l label
wl wl

let l label
wl wl
output w
figure perceptron global learning

vector features described section example perceptron performs
two steps first predicts optimal solution according current setting
w note prediction strategy employs complete combination model including
inference component second perceptron corrects vectors w according
mistakes seen arguments label l seen promoted vector
wl hand arguments demoted wl correction rule
moves scoring vectors towards missing arguments away predicted arguments
correct guaranteed perceptron visits examples
feedback rule improve accuracy global combination function
feature space almost linearly separable freund schapire collins
training strategy global mistakes perceptron corrects
arise comparing predicted structure correct one contrast
local strategy identies mistakes looking individually sign scoring predictions
candidate argument correct solution current scorers predict
negative positive condence value corresponding scorer corrected
candidate argument note criteria used generate training data
classiers trained locally section compare approaches empirically
nal note simplicity described perceptron simple form
however perceptron version use experiments reported section incorporates two well known extensions kernels averaging freund schapire collins
duy similar svm perceptron kernel method represented dual form dot product example vectors generalized
kernel function exploits richer representations hand averaging
technique increases robustness predictions testing original form
test predictions computed parameters training process
averaged version test predictions computed average parameter
vectors generated training every update details technique
found original article freund schapire


ficombination strategies semantic role labeling



development
brown
wsj




acuracy






























reject rate

figure rejection curves estimated output probabilities individual

experimental
section analyze performance three combination strategies previously
described inference constraint satisfaction b learning inference local
rankers c learning inference global rankers bulk experiments use candidate arguments generated three individual srl described
section evaluated section
experimental settings
combination strategies one exception detailed trained
complete conll training set propbank treebank sections minimize
overtting individual srl training data partitioned training
corpus folds fold used output individual
trained remaining four folds tuned separate development
partition treebank section evaluated two corpora treebank section
b three annotated sections brown corpus
constraint satisfaction model converted scores output arguments
three srl probabilities softmax function explained section
development set section used tune parameter softmax formula
nal value order assess quality procedure
plot figure rejection curves estimated output probabilities respect
classication accuracy development test sets wsj brown calculate
plots probability estimates three put together set sorted
decreasing order certain level rejection n curve figure plots
percentage correct arguments lowest scoring n subset rejected


fisurdeanu marquez carreras comas

exceptions curves increasing smooth indicating good correlation
probability estimates classication accuracy
last experiment section analyze behavior proposed combination
strategies candidate pool signicantly larger experiment used
top best performing systems conll shared task evaluation setup
two signicant dierences experiments used house individual
systems access systems outputs propbank development
section two test sections b argument probabilities individual
available thus instead usual training set train
combination propbank development section smaller feature set note
development set size regular training set
evaluated resulting combination two testing sections wsj
brown
lower upper bounds combination strategies
venture evaluation combination strategies explore lower
upper bounds combinations given corpus individual
analysis important order understand potential proposed
see close actually realizing
performance upper bound calculated oracle combination system
perfect ltering classier selects correct candidate arguments discards
others comparison purposes implemented second oracle system simulates ranking predicate selects candidate frame e
complete set arguments corresponding predicate proposed single model
highest f score table lists obtained wsj brown corpora
two oracle systems three individual combination system
oracle simulates combination strategies proposed break
candidate frames work individual candidate arguments note precision
oracle combination system case discontinuous arguments
fragments pass oracle lter considered incorrect scorer corresponding argument complete e g argument appears without continuation
c ranking columns list second oracle system selects
entire candidate frames
table indicates upper limit combination approaches proposed
relatively high f combination oracle system points higher
best individual system wsj test set points higher
brown corpus see table furthermore analysis indicates potential
combination strategy higher ranking strategies limited
performance best complete frame candidate pool allowing recombination arguments individual candidate solutions threshold raised
signicantly f points wsj f points brown
table lists distribution candidate arguments individual
selection performed combination oracle system conciseness list
core numbered arguments focus wsj corpus indicates percent

ficombination strategies semantic role labeling

wsj
brown

pprops



combination
precision recall





f



pprops



ranking
precision recall





f



table performance upper limits detected two oracle systems




















model






model






model






table distribution individual systems arguments upper limit selection
wsj test set

age correct arguments agreed indicates percentage
correct arguments agreed columns indicate percentage correct arguments detected single model table indicates expected
two individual agreed large percentage correct arguments nevertheless signicant number correct arguments e g come single
individual system proves order achieve maximum performance one
look beyond simple voting strategies favor arguments high agreement
individual systems
propose two lower bounds performance combination two
baseline systems
rst baseline recall oriented merges arguments generated
individual systems conict resolution baseline uses approximate inference
consisting two steps candidate arguments sorted radix
sort orders candidate arguments descending order number
agreed argument b argument length tokens c performance
individual system ii candidates iteratively appended global solution
violate domain constraints arguments already
selected
second baseline precision oriented considers arguments three
individual systems agreed conict resolution uses strategy
previous baseline system
table shows performance two baseline expected precisionoriented baseline obtains precision signicantly higher best individual model
table recall suers individual agree fairly large
number candidate arguments recall oriented baseline balanced expected
recall higher individual model precision drop much
combination produced highest scoring baseline model



fisurdeanu marquez carreras comas

wsj
baseline
baseline
brown
baseline
baseline

recall
precision

pprops



prec



recall



f



recall
precision
















table performance baseline wsj brown test sets
inference strategy lters many unlikely candidates overall recalloriented baseline performs best f points higher best individual
model wsj corpus points lower brown corpus
performance combination system constraint satisfaction
constraint satisfaction setting arguments output individual
recombined expected better solution satises set constraints
run inference model constraint satisfaction described section
xpress mp ilp solver main summarized table variants
presented table following pred pred stands local optimization
processes verb predicate independently others full sentence stands
global optimization e resolving verb predicates sentence
time column labeled constraints shows particular constraints applied
conguration column presents value parameter correcting bias
towards candidate overgeneration concrete values empirically set maximize f
measure development set corresponds setting bias correction
applied
clear conclusions drawn table first observe optimization variant obtains f individual systems table
baseline combination schemes table best combination model scores f points
wsj brown higher best individual system taking account
learning performed clear constraint satisfaction simple yet formal
setting achieves good
somewhat surprising performance improvements come constraints e overlapping embedding among arguments verb
repetition core arguments verb constraints harmful
sentence level constraints impact overall performance
analysis proposed constraints yielded following explanations
constraint number prevents assignment r x argument referred
argument x present makes inference miss easy r x arguments
xpress mp dash optimization product free academic usage
section see learning strategy incorporates global feedback performing
sentence level inference slightly better proceeding predicate predicate



ficombination strategies semantic role labeling

wsj
pred pred

full sentence

brown
full sentence

constraints





















pprops










precision










recall










f




























table wsj brown test sets obtained multiple variants constraint satisfaction

x argument correctly identied e g constituents start
followed verb r furthermore constraint
presents lot exceptions r x arguments wsj test set
referred argument x e g law tells
therefore hard application constraint prevents selection correct
r x candidates ocial evaluation script conll srl eval
require constraint satised consider solution consistent
srl eval script requires constraint number e c x tag accepted
without preceding x argument fullled candidate solution considered
consistent nds solution violating constraint behavior
convert rst c x without preceding x x turns simple
post processing strategy better forcing coherent solution inference step
allows recover error argument completely
recognized labeled c x tags
regarding sentence level constraints observed setting inference
local constraints rarely produces solution inconsistencies sentence
level makes constraint useless since almost never violated constraint
number e sharing ams among dierent verbs ad hoc represents
less universal principle srl number exceptions constraint
wsj test set gold standard data output
inference uses local constraints forcing fulllment
constraint makes inference process commit many errors corrections making
eect negligible
fact partly explained small number overlapping arguments candidate pool
produced three individual



fisurdeanu marquez carreras comas






precision
recall
f














precision
recall
f

































value















value

figure precision recall plots respect bias correcting parameter
wsj development test sets left right plots respectively

considering constraints universal e exceptions exist
gold standard seems reasonable convert soft constraints done
precomputing compatibility corpora counts instance point wise
mutual information incorporating eect compatibility function explained
section softening could principle increase overall recall combination
unfortunately initial experiments showed dierences hard soft
variants
finally dierences optimized values bias correcting parameter
clearly explained observing precision recall values default
version tends overgenerate argument assignments implies higher recall cost
lower precision contrary f optimized variant conservative
needs evidence select candidate precision higher recall
lower side eect restrictive argument assignments number
correctly annotated complete propositions lower optimized setting
preference high precision vs high recall system mostly task dependant
interesting note constraint satisfaction setting adjusting precision
recall tradeo easily done varying value bias correcting score
figure plot precisionrecall curves respect dierent values parameter optimization done constraints expected high values
promote precision demote recall lower values contrary
see wide range values combined f measure almost
constant approximate intervals marked vertical lines making possible
select dierent recall precision values global performance f near optimal parenthetically note optimal value estimated development set
generalizes well wsj test set


ficombination strategies semantic role labeling

wsj




brown










pprops





prec





recall





f





f improvement



































table overall learning inference local rankers wsj
brown test sets

performance combination system local rankers
implemented candidate scoring classiers combination strategy support vector machines svm polynomial kernels degree performed slightly
better types svms adaboost implemented svm classiers
svmlight software outside changing default kernel polynomial
modied default parameters experiments reported section
trained possible combinations individual systems
complete feature set introduced section dynamic programming engine used
actual inference processes predicate independently similar pred pred
previous sub section
table summarizes performance combined systems wsj brown
corpora table indicates combination strategy successful
combination systems improve upon individual table f
scores better baselines table last column table shows
f improvement combination model w r best individual model set
expected highest scoring combined system includes three individual
f measure points higher best individual model model wsj test
set points higher brown test set note combination two
individual systems outperform current state art see section details
empirical proof robust successful combination strategies srl
possible table indicates even though partial parsing model model
worst performing individual model contribution ensemble important
indicating information provides indeed complementary
instance wsj performance combination two best individual
worse combinations model
http svmlight joachims org



fisurdeanu marquez carreras comas

wsj
fs
fs
fs
fs
fs
fs
brown
fs
fs
fs
fs
fs
fs

pprops







prec







recall







f



































table feature analysis learning inference local rankers
due simple architecture e feedback conict resolution component
candidate ltering inference model good framework study contribution
features proposed section study group features sets fs
voting features fs overlap features arguments predicate fs
overlap features arguments predicates fs partial syntax features fs
full syntax features fs probabilities generated individual systems
candidate arguments sets constructed combination
increasing number features made available argument ltering classiers e g
rst system uses fs second system adds fs rst systems features
fs added third system etc table lists performance systems
two test corpora empirical analysis indicates feature sets
highest contribution
fs boosts f score combined system points wsj
points brown best individual system yet another empirical proof
voting successful combination strategy
fs contribution points wsj points brown f
score numbers indicate ltering classier capable learning
preferences individual certain syntactic structures
fs contributes points wsj points brown f score
promote idea information overall sentence structure
case inter predicate relations successfully used srl
knowledge novel
proposed features positive contribution performance combined
system overall achieve f score points wsj points brown
higher best performing combined system conll shared task evaluation
see section details


ficombination strategies semantic role labeling

performance combination system global rankers
section report experiments global perceptron described
section globally trains scoring functions rankers similar local
svm use polynomial kernels degree furthermore predictions test
time used averages parameter vectors following technique freund schapire

interested two main aspects first evaluate eect training
scoring functions perceptron two dierent update rules one global
local global feedback rule detailed section corrects mistakes found
comparing correct argument structure one inference
noted global feedback contrast local feedback rule corrects mistakes
found inference candidate argument handled independently ignoring
global argument structure generated noted local feedback second
analyze eect dierent constraints inference module extent
congured inference module two ways rst processes predicates
sentence independently thus might select overlapping arguments dierent predicates
incorrect according domain constraints one noted pred pred
inference second processes predicates jointly enforces hierarchical structure
arguments arguments never overlap arguments predicate allowed
embed arguments predicates noted full sentence inference
perspective model local update pred pred inference almost identical
local combination strategy described section unique dierence
use perceptron instead svm apparently minute dierence turns
signicant empirical analysis allows us measure contribution
svm margin maximization global feedback classier combination
strategy see section
trained four dierent local global feedback predicate bypredicate joint inference model trained epochs training data
evaluated development data training epoch selected best
performing point development evaluated test data table
reports test data
looking rst impression dierence f measure
signicant among dierent congurations however observations pointed
global methods achieve much better recall gures whereas local methods prioritize
precision system overall global methods achieve balanced tradeo
precision recall contributes better f measure
looking pred pred versus full sentence inference seen
global methods sensitive dierence note local model trained
independently inference module thus adding constraints inference
engine change parameters local model testing time dierent
inference congurations aect contrast global trained
dependently inference module moving pred pred full sentence
inference consistency enforced argument structures dierent predicates
benets precision recall method global learning


fisurdeanu marquez carreras comas

wsj
pred pred local
full sentence local
pred pred global
full sentence global
brown
pred pred local
full sentence local
pred pred global
full sentence global

pprops





prec





recall





f

























table test combination system global rankers four congurations
evaluated combine pred pred full sentence inference local
global feedback

improves precision recall coupled joint inference process
considers constraints solution
nevertheless combination system local svm classiers presented previous section achieves marginally better f score global learning method vs
wsj explained dierent machine learning discuss
issue detail section better f score accomplished much better
precision local vs wsj whereas recall lower
local global vs wsj hand
global strategy produces completely correct annotations see pprops column
local strategies investigated see tables expected
considering global strategy optimizes sentence level cost function somewhat
surprisingly number perfect propositions generated global strategy lower
number perfect propositions produced constraint satisfaction
discuss section
scalability combination strategies
combination experiments reported point used candidate arguments
generated three individual srl introduced section experiments provide empirical comparison three inference proposed
answer obvious scalability question proposed combination approaches
scale number candidate arguments increases quality diminishes
mainly interested answering question last two combination
use inference learning local global rankers two reasons
performed better constraint satisfaction model previous experiments
b requirements individual srl systems outputs unlike
constraint satisfaction model requires argument probabilities individual coupled pools candidates generated individual srl
model


ficombination strategies semantic role labeling

koomen
pradhan
haghighi
marquez
pradhan

surdeanu
tsai
che
moschitti
tjongkimsang
yi

ozgencil

wsj
prec
recall























































































f





pprops





brown
prec
recall







pprops





f





































table performance best systems conll pradhan contains postevaluation improvements top systems actually combination
second column marks systems used evaluation pradhan replaced improved version pradhan
yi due format errors submitted data

scalability analysis use individual srl top systems
conll shared task evaluation table summarizes performance systems
two test corpora used previous experiments table indicates
performance systems varies widely dierence f points wsj
corpus f points brown corpus best worst system
set
combination experiments generated candidate pools top

individual systems labeled
table make two changes
experimental setup used rst part section trained combined propbank development section access
individual systems outputs propbank training partition b feature
set introduced section use individual systems argument probabilities
raw activations individual classiers available note
settings size training corpus times smaller size
training set used previous experiments
table shows upper limits setups combination reranking oracle systems introduced section besides performance numbers
list table average number candidates per sentence setup e number
unique candidate arguments args sent combination oracle number
unique candidate frames frames sent ranking oracle table lists
performance combined local feedback section global
feedback section combination strategy global rankers uses joint inference
global feedback see description previous sub section


fisurdeanu marquez carreras comas

wsj
c
c
c
c
c
brown
c
c
c
c
c

args sent











combination
prec
recall





















f






ranking
frames sent
prec





























recall






f


















table performance upper limits determined oracle systems best systems conll ck stands combination top k systems
table args sent indicates average number candidate arguments
per sentence combination oracle frames sent indicates average
number candidate frames per sentence ranking oracle latter
larger number systems combination average
multiple predicates per sentence

wsj
c
c
c
c
c
brown
c
c
c
c
c

pprops






local
prec






ranker
recall






f






pprops






global ranker
prec
recall






f






















































table local versus global ranking combinations best systems conll ck stands combination top k systems table

draw several conclusions experiments first performance upper limit ranking lower argument combination
strategy even number candidates large example individual
used f upper limit brown corpus whereas
f upper limit ranking however enhanced potential combination imply signicant increase computational cost table shows


ficombination strategies semantic role labeling

number candidate arguments must handled combination approaches
much higher number candidate frames input ranking system especially number individual high example
individual used combination approaches must process around arguments
per sentence whereas ranking approaches must handle approximately frames per sentence intuition behind relatively small dierence computational cost
even though number arguments signicantly larger number frames
dierence number unique candidates two approaches high
probability repeated arguments higher probability repeated
frames
second conclusion combination boost performance
corresponding individual systems example best system combination achieves
f score approximately points higher best individual model wsj
brown corpus expected combination reach performance plateau
around individual systems quality individual starts drop
signicantly nevertheless considering top individual systems use combination
strategies amount training data experiment quite small
good potential combination analyzed
third observation relation previously observed local global
rankers holds combination model local rankers better precision model
global rankers better recall generally better pprops score overall
model local rankers obtains better f scores scales better number
individual systems increases discuss dierences detail next
sub section
finally table indicates potential recall experiment shown
left block table higher potential recall combining three
individual srl systems see table higher wsj test set higher
brown test set expected considering number quality
candidate arguments last experiment higher however even
improvement potential recall combination strategies far thus
combining solutions n best state art srl systems still
potential properly solve srl future work focus recallboosting strategies e g candidate arguments individual systems
individual complete solutions generated step many candidate arguments
eliminated
discussion
experimental presented section indicate proposed combination
strategies successful three combination provide statistically signicant improvements individual baselines setups immediate
somewhat shallow comparison three combination strategies investigated indicates
best combination strategy srl max margin local metalearner b global ranking meta learner important


fisurdeanu marquez carreras comas

contribution max margin strategy c constraint satisfaction
model performs worst strategies tried
however experiments dierences combination approaches investigated small reasonable observation combination strategy
advantages disadvantages dierent approaches suitable dierent
applications data discuss dierences
argument probabilities individual systems available combination model
constraint satisfaction attractive choice simple unsupervised strategy obtains competitive performance furthermore constraint satisfaction model
provides elegant customizable framework tune balance precision
recall see section framework currently obtain highest recall
combination higher best recall obtained meta learning approaches wsj corpus higher meta learning brown
corpus higher recall implies higher percentage predicates completely
correctly annotated best pprops numbers table best combination
strategies cause high dierence recall favor constraint satisfaction
candidate scoring learning inference acts implicitly
lter candidates whose score e classier condence candidate part
correct solution negative discarded negatively aects overall recall
hence constraint satisfaction better solution srl nlp applications
require predicate argument frames extracted high recall example information extraction predicate argument tuples ltered subsequent high precision
domain specic constraints surdeanu et al hence paramount srl
model high recall
nevertheless many cases argument probabilities individual srl
available generate e g rule systems
individual available black boxes oer access
internal information conditions showed combination strategies
meta learning viable alternative fact approaches obtain highest
f scores see section obtain excellent performance even small amounts
training data see section previously mentioned candidate scoring acts
lter learning inference tends favor precision recall precision
higher best precision constraint satisfaction wsj
corpus higher brown corpus preference precision recall
pronounced learning inference local rankers section
inference model global rankers section hypothesis causes
global ranking model less precision biased conguration ratio
errors positive versus negative samples balanced thinking strategy
perceptron follows local updates every candidate incorrect prediction
sign whereas global updates candidates
complete solution enforcing domain constraints words number
negative updates drives precision bias reduced global
false positives generated ranking classiers eliminated
domain constraints thus candidate scoring trained optimize accuracy


ficombination strategies semantic role labeling

wsj
global feedback
max margin
brown
global feedback
max margin

pprops



prec



recall



f















table contribution global feedback max margin learning inference
baseline pred pred local model table

fewer candidate arguments eliminated meta learner global rankers
translates better balance precision recall
another important conclusion analysis global versus local ranking
learning inference max margin candidate scoring classiers
important global feedback inference fact considering
dierence model predicate predicate inference local feedback
section pred pred local versus best model section fs
latter uses svm classiers whereas former uses perceptron compute exact
contribution max margin global feedback convenience summarize
analysis table table indicates max margin yields consistent improvement
precision recall whereas contribution global feedback reducing
dierence precision recall boosting recall decreasing precision
benet max margin classiers even evident table shows
local ranking model max margin classiers generalizes better global ranking
model amount training data reduced signicantly
even though analyzed several combination approaches three
independent implementations proposed fact compatible
combinations proposed strategies immediately possible example
constraint satisfaction model applied output probabilities candidate
scoring component introduced section model eliminates dependency
output scores individual srl retains advantages
constraint satisfaction model e g formal framework tune balance precision recall another possible combination approaches introduced
use max margin classiers learning inference global feedback e g
global training method margin maximization svmstruct tsochantaridis et al model would indeed increased training time could
leverage advantages max margin classiers inference global feedback
summarized table finally another attractive stacking e n levels chained meta learning example could cascade learning inference
model global rankers boosts recall learning inference local
rankers favors precision
contribution global feedback given model joint inference global feedback full
sentence global section
main reason chose perceptron proposed online strategies



fisurdeanu marquez carreras comas

related work
best performing systems conll shared task included combination
dierent base subsystems increase robustness gain coverage independence
parse errors therefore closely related work rst
four rows table summarize exactly experimental setting
one used
koomen et al used layer architecture close pool candidates
generated running full syntax srl system alternative input information collins
parsing best trees charniaks parser b taking candidates pass
lter set dierent parse trees combination candidates performed
elegant global inference procedure constraint satisfaction formulated
integer linear programming solved eciently dierent work
break complete solutions number srl systems investigate
meta learning combination addition ilp inference koomen et al
system best performing system conll see table
haghighi et al implemented double ranking top several outputs
base srl model ranking performed rst set n best solutions obtained
base system run single parse tree set best candidates
coming n best parse trees second best system conll
third row table compared decomposition combination
ranking setting advantage allowing denition global features
apply complete candidate solutions according follow work authors
toutanova haghighi manning global features source major
performance improvements ranking system contrast focus features
exploit redundancy individual e g overlap individual
candidate arguments add global information frame level complete
solutions provided individual main drawback ranking compared
dierent individual solutions combined ranking
forced select complete candidate solution implies overall performance
strongly depends ability base model generate complete correct solution
set n best candidates drawback evident lower performance upper
limit ranking see tables performance actual
system best combination strategy achieves f score points higher
haghighi et al wsj brown
finally pradhan hacioglu ward martin jurafsky b followed stacking
learning two individual systems full syntax whose outputs used
generate features feed training stage nal chunk chunk srl system although
ne granularity chunking system allows recover parsing errors
nd combination scheme quite ad hoc forces break argument candidates
chunks last stage
recently yih toutanova reported improved numbers system f wsj
brown however numbers directly comparable systems presented
fixed significant bug representation quotes input data bug
still present data



ficombination strategies semantic role labeling

outside conll shared task evaluation roth yih reached conclusion quality local argument classiers important global
feedback inference component one conclusions drawn contribution shown hypothesis holds complex
framework combination several state art individual whereas roth
yih experimented single individual model numbered arguments slightly
simplied representation b basic chunks additionally detailed
experiments allowed us clearly contribution max margin higher
global learning several corpora several combinations individual systems
punyakanok roth yih showed performance individual srl
particularly argument identication signicantly improved full parsing
used argument boundaries restricted match syntactic constituents similarly
model believe used candidate
arguments match single syntactic constituent increased robustness
built mechanism handle syntax errors argument constituent incorrectly fragmented multiple phrases empirical support
claim model performs better model proposed punyakanok
et al second advantage strategy proposed model
deployed full syntax model partial syntax model
pradhan ward hacioglu martin jurafsky c implement srl combination
strategy constituent level similarly combines dierent syntactic
views data full partial syntactic analysis however unlike
pradhan et al work uses simple greedy inference strategy probabilities
candidate arguments whereas introduce analyze three dierent
combination analysis yielded combination system outperforms
current state art
previous work general eld predicting structures natural language
texts indicated combination several individual improves overall performance given task collins rst proposed learning layer ranking
improve performance generative syntactic parser reranker
trained select best solution pool solutions produced generative
parser reranker dealt complete parse trees represented
rich features exploited dependencies considered generative method
hand computationally feasible train reranker base method
reduced number possible parse trees sentence exponential number w r
sentence length tens recently global discriminative learning methods
predicting structures proposed laerty mccallum pereira collins
taskar et al tsochantaridis et al train
single discriminative ranking function detect structures sentence major property
methods model discriminatively arbitrary
rich representations structures used furthermore training process
methods global parameters set maximize measures related
local accuracies e recognizing parts structure related global
accuracy e recognizing complete structures article use global
rich representations major motivation


fisurdeanu marquez carreras comas

conclusions
introduces analyzes three combination strategies context semantic
role labeling rst model implements inference strategy constraint satisfaction
integer linear programming second uses inference learning
candidates scored discriminative classiers local information
third last inference model builds previous strategy adding global feedback
conict resolution component ranking classiers meta learners used
inference process developed rich set features includes voting
statistics e many individual systems proposed candidate argument overlap
arguments predicates sentence structure distance
information coded partial full syntax probabilities individual srl
available knowledge rst work introduces thorough
inference model learning semantic role labeling b performs comparative
analysis several inference strategies context srl
presented suggest strategy decomposing individual solutions
performing learning combination constructing nal solution advantages approaches e g ranking set complete candidate solutions
course task dependant conclusion case semantic role labeling relatively simple since combination argument candidates fulll
set structural constraints generate consistent solution target structure complex e g full parse tree combination step might complex
learning search perspectives
evaluation indicates proposed combination approaches successful
provide signicant improvements best individual model several baseline
combination setups three combination strategies investigated
best f score obtained learning inference max margin classiers
proposed approaches advantages drawbacks see section
detailed discussion dierences among proposed inference several important features state art srl combination strategy emerge analysis
individual combined granularity candidate arguments rather
granularity complete solutions frames ii best combination strategy
uses inference model learning iii learning inference benets
max margin classiers global feedback iv inference sentence level e
considering predicates time proves slightly useful learning
performed globally feedback complete solution inference
last least obtained best combination strategy developed
work outperform current state art empirical proof
srl system good performance built combining small number three
experiments relatively simple srl

acknowledgments
would thank jair reviewers valuable comments
partially supported european commission chil project


ficombination strategies semantic role labeling

ip pascal network ist spanish ministry education
science trangram tin c mihai surdeanu fellow
within ramon cajal program spanish ministry education science
grateful dash optimization free academic use xpress mp

references
bishop c neural networks pattern recognition oxford university press
boas h c bilingual framenet dictionaries machine translation proceedings
lrec
carreras x marquez l introduction conll shared task semantic
role labeling proceedings conll
carreras x marquez l introduction conll shared task semantic
role labeling proceedings conll
carreras x marquez l chrupala g hierarchical recognition propositional
arguments perceptrons proceedings conll shared task
charniak e maximum entropy inspired parser proceedings naacl
collins head driven statistical natural language parsing phd
dissertation university pennsylvania
collins discriminative reranking natural language parsing proceedings
th international conference machine learning icml stanford ca
usa
collins discriminative training methods hidden markov theory
experiments perceptron proceedings sigdat conference
empirical methods natural language processing emnlp
collins parameter estimation statistical parsing theory practice distribution free methods bunt h carroll j satta g eds
developments parsing technology chap kluwer
collins duy n ranking parsing tagging kernels
discrete structures voted perceptron proceedings th annual
meeting association computational linguistics acl
crammer k singer family additive online category
ranking journal machine learning
crammer k singer b ultraconservative online multiclass
journal machine learning
freund schapire r e large margin classication perceptron
machine learning
gildea jurafsky automatic labeling semantic roles computational
linguistics


fisurdeanu marquez carreras comas

gildea palmer necessity syntactic parsing predicate argument
recognition proceedings th annual conference association
computational linguistics acl
hacioglu k pradhan ward w martin j h jurafsky semantic
role labeling tagging syntactic chunks proceedings th conference
computational natural language learning conll
haghighi toutanova k manning c joint model semantic role labeling
proceedings conll shared task
koomen p punyakanok v roth yih w generalized inference
multiple semantic role labeling systems proceedings conll shared task
laerty j mccallum pereira f conditonal random elds probabilistic segmenting labeling sequence data proceedings th
international conference machine learning icml
marcus santorini b marcinkiewicz building large annotated corpus
english penn treebank computational linguistics
marquez l comas p gimenez j catala n semantic role labeling
sequential tagging proceedings conll shared task
melli g wang liu kashani shi z gu b sarkar popowich
f description squash sfu question answering summary handler
duc summarization task proceedings document understanding
workshop hlt emnlp annual meeting
narayanan harabagiu question answering semantic structures
international conference computational linguistics coling
noreen e w computer intensive methods testing hypotheses john wiley
sons
palmer gildea kingsbury p proposition bank annotated
corpus semantic roles computational linguistics
ponzetto p strube exploiting semantic role labeling wordnet
wikipedia coreference resolution proceedings human language technolgy
conference north american chapter association computational linguistics
ponzetto p strube b semantic role labeling coreference resolution
companion proceedings th meeting european chapter
association computational linguistics
pradhan hacioglu k krugler v ward w martin j h jurafsky
support vector learning semantic argument classication machine learning

pradhan hacioglu k ward w martin j h jurafsky b semantic role
chunking combining complementary syntactic views proceedings conll


ficombination strategies semantic role labeling

pradhan ward w hacioglu k martin j h jurafsky c semantic role
labeling dierent syntactic views proceedings rd annual conference
association computational linguistics
punyakanok v roth yih w necessity syntactic parsing semantic role labeling proceedings international joint conference artificial
intelligence ijcai
punyakanok v roth yih w zimak semantic role labeling via integer linear programming inference proceedings international conference
computational linguistics coling
rosenblatt f perceptron probabilistic model information storage
organization brain psychological review
roth yih w linear programming formulation global inference
natural language tasks proceedings annual conference computational
natural language learning conll pp boston
roth yih w integer linear programming inference conditional random
elds proceedings international conference machine learning icml
schapire r e singer improved boosting condence rated
predictions machine learning
surdeanu harabagiu williams j aarseth p predicate argument
structures information extraction proceedings st annual meeting
association computational linguistics acl
taskar b guestrin c koller max margin markov networks proceedings
th annual conference neural information processing systems nips
vancouver canada
taskar b klein collins koller manning c max margin parsing
proceedings emnlp
toutanova k haghighi manning c joint learning improves semantic role
labeling proceedings rd annual meeting association computational linguistics acl pp ann arbor mi usa association
computational linguistics
tsochantaridis hofmann joachims altun support vector machine
learning interdependent structured output spaces proceedings st
international conference machine learning icml
xue n palmer calibrating features semantic role labeling proceedings
emnlp
yih w toutanova k automatic semantic role labeling tutorial
human language technolgy conference north american chapter
association computational linguistics
younger h recognition parsing context free languages n time
information control




