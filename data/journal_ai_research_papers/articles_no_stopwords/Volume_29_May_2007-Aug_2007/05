journal artificial intelligence

submitted published

generalized architecture
pedro f felzenszwalb

pff cs uchicago edu

department computer science
university chicago
chicago il

david mcallester

mcallester tti c org

toyota technological institute chicago
chicago il

abstract
consider computing lightest derivation global structure
set weighted rules large variety inference ai formulated
framework generalize search heuristics derived abstractions
broad class lightest derivation describe searches
lightest derivations hierarchy abstractions generalization gives
searching graphs bottom fashion
discuss described provide general architecture addressing pipeline passing information back forth
stages processing perceptual system consider examples computer vision natural language processing apply hierarchical search
estimating boundaries convex objects grayscale images compare
search methods second set experiments demonstrate use
compositional model finding salient curves images

introduction
consider class defined set weighted rules composing structures
larger structures goal lightest least cost derivation
global structure derivable given rules large variety classical inference
ai expressed within framework example global structure
might parse tree match deformable object model image assignment
values variables markov random field
define lightest derivation terms set statements set weighted
rules deriving statements statements special goal statement
case looking lightest derivation goal statement usually express
lightest derivation rule schemas implicitly represent large set
rules terms small number rules variables lightest derivation
formally equivalent search graphs nilsson
formulation natural applications interested
one goals construction global optimization
across many levels processing perceptual system described
used integrate multiple stages processing pipeline single global optimization solved efficiently
c

ai access foundation rights reserved

fifelzenszwalb mcallester

dynamic programming fundamental technique designing efficient inference good examples viterbi hidden markov rabiner
chart parsing methods stochastic context free grammars charniak
described used speed solution normally
solved dynamic programming demonstrate specific
goal estimate boundary convex object cluttered image second set
experiments used salient curves images
describe model salient curves compositional rule enforces
long range shape constraints leads large solved
classical dynamic programming methods
consider related dijkstras shortest paths dsp
dijkstra search hart nilsson raphael dsp
used shortest path cyclic graph use priority queue define order
nodes expanded worst case running time log n n
number nodes graph number edges dsp
expansion node v involves generating nodes u edge v u
difference two methods uses heuristic function avoid
expanding non promising nodes
knuth gave generalization dsp used solve lightest derivation
cyclic rules knuth call knuths lightest derivation
kld analogy dijkstras kld uses priority queue define order
statements expanded expansion statement v involves generating
conclusions derived single step v statements already
expanded long rule bounded number antecedents kld worst
case running time log n n number statements
number rules nilssons ao used solve
lightest derivation although ao use heuristic function true
generalization use priority queue handles acyclic rules
require n time even applied shortest path particular ao
variants use backward chaining technique starts goal repeatedly
refines subgoals forward chaining
klein manning described parsing similar kld
use heuristic function one contributions generalization
arbitrary lightest derivation call lightest derivation
ld method forward chaining uses priority queue control order
statements expanded handles cyclic rules worst case running time
log n rule small number antecedents ld
seen true generalization lightest derivation lightest derivation
comes shortest path ld identical
course running times seen practice often well predicted worst case
analysis specially true large defined implicitly
example use dynamic programming solve shortest path acyclic
graph time better log n bound dsp implicit
extensions handle cyclic rules jimenez torras
ao backward chaining terms inference rules defining lightest derivation



fithe generalized architecture

graphs dsp much efficient since expands nodes best first order
searching shortest path source goal dsp expand nodes v
v w v length shortest path source v w
length shortest path source goal case monotone
admissible heuristic function h v possible obtain similar bound searching
implicit graphs expand nodes v v h v w
running time kld ld expressed similar way solving
lightest derivation kld expand statements v v w v
weight lightest derivation v w weight lightest derivation
goal statement furthermore ld expand statements v v h v w
heuristic function h v gives estimate additional weight necessary
deriving goal statement derivation v heuristic values used ld
analogous distance node goal graph search notion
used note heuristic values significantly different ones
used ao case ao heuristic function h v would estimate weight
lightest derivation v
important difference ld ao ld computes derivations
bottom fashion ao uses top method advantages depending type solved example classical
computer vision involves grouping pixels long smooth curves formulate
terms finding smooth curves pairs pixels far apart
image n pixels n pairs straight forward implementation
top would start considering n possibilities bottomup would start n pairs nearby pixels case expect
bottom grouping method would efficient top method
classical ao requires set rules acyclic jimenez torras
extended method handle cyclic rules another top
handle cyclic rules described bonet geffner hansen zilberstein
described search optimal solutions
cyclic described handle cyclic rules
require optimal solutions acyclic note ao handle rules
non superior weight functions defined section kld requires superior weight
functions ld replaces requirement requirement heuristic function
well known method defining heuristics consider abstract relaxed
search example consider solving rubiks cube small
number moves suppose ignore edge center pieces solve corners
example abstraction number moves necessary put
corners good configuration lower bound number moves necessary solve
original fewer corner configurations full configurations
makes easier solve abstract general shortest paths
goal abstract used define admissible monotone heuristic
function solving original
abstractions used define heuristic functions ld
lightest derivation notion shortest path goal replaced
notion lightest context context statement v derivation


fifelzenszwalb mcallester

goal hole filled derivation v computation lightest
abstract contexts lightest derivation
abstractions related relaxations defined pearl abstractions often lead small solved search relaxations lead
still large state space may simple enough solved closed
form definition abstractions use lightest derivation includes
relaxations special case
another contribution work hierarchical search method call ha ld
effectively use hierarchy abstractions solve lightest derivation
novel even case classical search shortest paths ha ld searches lightest derivations contexts every level abstraction
simultaneously specifically level abstraction set statements
rules search lightest derivations contexts level controlled
single priority queue understand running time ha ld let w weight
lightest derivation goal original abstracted statement v
abstraction hierarchy let v weight lightest derivation v level
abstraction let h v weight lightest context abstraction v defined
one level v hierarchy let k total number statements
hierarchy v h v w hal expands k statements solving
original factor two comes fact computes
derivations contexts level abstraction
previous use abstractions solving search include methods pattern databases culberson schaeffer korf korf felner
hierarchical ha hida holte perez zimmer macdonald holte
grajkowski tanner coarse fine dynamic programming cfdp raphael
pattern databases made possible compute solutions impressively large
search methods construct lookup table shortest paths node
goal abstract states practice limited tables remain
fixed different instances relatively small tables heuristic must
recomputed instance example rubiks cube precompute
number moves necessary solve every corner configuration table used
define heuristic function solving full configuration rubiks cube
ha hida use hierarchy abstractions avoid searching nodes
level hierarchy hand directed graphs methods may still
expand abstract nodes arbitrarily large heuristic values clear
generalize ha hida lightest derivation rules
one antecedent finally cfdp related ao repeatedly solves ever
refined dynamic programming leads worst case running time
n discuss relationships ha ld hierarchical
methods detail section
note search related previously used solve
number classical state space search includes
traveling salesman zhang korf edelkamp multiple
sequence alignment korf zhang thayer hohwald combinatorial
graphs felner parsing context free grammars klein manning


fithe generalized architecture

work bulitko sturtevant lu yau uses hierarchy state space abstractions real time search
pipeline
major artificial intelligence integration multiple processing stages
form complete perceptual system call pipeline general
concatenation systems stage feeds information next vision
example might edge detector feeding information boundary finding system
turn feeds information object recognition system
computational constraints need build modules clean interfaces
pipelines often make hard decisions module boundaries example edge detector
typically constructs boolean array indicates weather edge detected
image location general recognition presence edge
certain location depend context around people often see edges places
image gradient small higher cognitive level clear actually
object boundary location speech recognition systems try address
returning n best lists may may contain actual utterance would
speech recognition system able take high level information account
avoid hard decision exactly strings output n best list
processing pipeline specified describing stages terms rules
constructing structures structures produced previous stage vision system
one stage could rules grouping edges smooth curves next stage could
rules grouping smooth curves objects case construct single
lightest derivation representing entire system moreover hierarchical set
abstractions applied entire pipeline ha ld compute lightest
derivations complete scene interpretation derived one level abstraction guides
processing stages concrete level provides mechanism enables coarse
high level processing guide low level computation believe important
property implementing efficient perceptual pipelines avoid making hard decisions
processing stages
note formulation complete computer vision system lightest derivation related work geman potter chi tu chen yuille
zhu jin geman papers image understanding posed
parsing goal explain image terms set objects
formed possibly recursive composition generic parts tu et al use
data driven mcmc compute optimal parses geman et al jin
geman use bottom building compositions greedy fashion
neither methods guaranteed compute optimal scene interpretation
hope ha ld provide principled computational technique solving large
parsing defined compositional
overview
begin formally defining lightest derivation section section
discusses dynamic programming relationship lightest derivation


fifelzenszwalb mcallester

graphs section describe knuths lightest derivation
section describe ld prove correctness section shows abstractions
used define mechanically constructed heuristic functions ld describe
ha ld section discuss use solving pipeline section section discusses relationship ha ld hierarchical search methods
sections present experimental conclude section

lightest derivation
let set statements r set inference rules following form
w


wn
c g w wn
antecedents ai conclusion c statements weights wi
non negative real valued variables g non negative real valued weight function
rule antecedents function g simply non negative real value throughout
use g c denote inference rule type
derivation c finite tree rooted rule g c n children
th child derivation ai leaves tree rules antecedents
every derivation weight value obtained recursive application
functions g along derivation tree figure illustrates derivation tree
intuitively rule g c says derive antecedents ai
weights wi derive conclusion c weight g w wn
interested compute lightest derivation special goal statement
discussed assume weight functions g associated lightest derivation non decreasing variable
fundamental property ensuring lightest derivations optimal substructure property case lightest derivations constructed lightest derivations
facilitate runtime analysis assume every rule small
number antecedents use n denote number statements lightest derivation
denotes number rules interested
n large implicitly defined compact way
small number rules variables examples assume
n since statements conclusion rule clearly
derivable ignored
dynamic programming
say set rules acyclic ordering statements
rule conclusion c antecedents statements come c
ordering dynamic programming used solve lightest derivation


fithe generalized architecture




c

derivation


derivation


derivation


figure derivation c tree rules rooted rule r conclusion c
children root derivations antecedents r leafs tree
rules antecedents

functions g rule non decreasing set rules acyclic case
lightest derivations computed sequentially terms acyclic ordering
th step lightest derivation th statement obtained minimizing rules
used derive statement method takes time compute
lightest derivation statement
note cyclic rules sometimes possible compute lightest derivations
taking multiple passes statements note authors would refer
dijkstras kld dynamic programming method
use term referring compute lightest derivations fixed
order independent solutions computed along way includes recursive
implementations use memoization
examples
rules computing shortest paths single source weighted graph shown
figure assume given weighted graph g v e wxy
non negative weight edge x e distinguished start node first
rule states path weight zero start node second set rules
state path node x extend path edge x
obtain appropriately weighted path node rule type
edge graph lightest derivation path x corresponds shortest path
x note general graphs rules cyclic figure illustrates graph


fifelzenszwalb mcallester



path
x e
path x w
path w wxy
figure rules computing shortest paths graph

b

c

path w

path c w

path b w wdb

path b w wcb

path w

path e w

path w wsd

path c w wec



e

path w
path
path e w wse

path

figure graph two highlighted paths b corresponding derivations
rules figure

two different derivations path b rules described corresponds
two different paths b
rules chart parsing shown figure assume given weighted
context free grammar chomsky normal form charniak e weighted set
productions form x x z x z nonterminal symbols
terminal symbol input string given sequence terminals sn


fithe generalized architecture

production x si

phrase x w x si
production x z j k n
phrase j w
phrase z j k w
phrase x k w w w x z
figure rules parsing context free grammar
first set rules state grammar contains production x si
phrase type x generating th entry input weight w x si second
set rules state grammar contains production x z phrase
type j phrase type z j k appropriately
weighted phrase type x k let start symbol grammar
goal parsing lightest derivation phrase n rules acyclic
phrases composed together form longer phrases
graphs
lightest derivation closely related graphs let r set
statements rules defining lightest derivation convert
graph representation build graph disjunction node
statement conjunction node rule r edge statement rule deriving statement edge rule antecedents
leaves graph rules antecedents derivations
statement rules r represented solutions rooted statement
corresponding graph conversely possible represent
graph search lightest derivation case view node
graph statement build appropriate set rules r

knuths lightest derivation
knuth described generalization dijkstras shortest paths call
knuths lightest derivation kld knuths used solve large class
lightest derivation allows rules cyclic requires
weight functions associated rule non decreasing superior specifically
require following two properties weight function g rule
non decreasing
superior

wi wi g w wi wn g w wi wn
g w wn wi


fifelzenszwalb mcallester

example
g x xn x xn
g x xn max x xn
non decreasing superior functions
knuths computes lightest derivations non decreasing weight order since
interested lightest derivation special goal statement often stop
computing lightest derivation every statement
weight assignment expression form b w b statement
w non negative real value say weight assignment b w derivable
derivation b weight w set rules r statement b weight
w write r b w rules r used derive b w let b r
infimum set weights derivable b
b r inf w r b w
given set rules r statement goal interested computing derivation
goal weight goal r
define bottom logic programming language easily express
wish discuss throughout rest defined
set rules priorities encode priority rule writing along
line separating antecedents conclusion follows
w


wn
p w wn
c g w wn
call rule form prioritized rule execution set prioritized rules
p defined procedure figure procedure keeps track set
priority queue q weight assignments form b w initially empty q
contains weight assignments defined rules antecedents priorities given
rules iteratively remove lowest priority assignment b w q b
already assigned weight assignment ignored otherwise add
assignment expand every assignment derivable b w
assignments already rule p added q priority specified
rule procedure stops queue empty
executing set prioritized rules set weight assignments moreover
procedure implicitly keep track derivations remembering assignments
used derive item inserted queue
lemma execution finite set prioritized rules p derives every statement
derivable rules p
proof rule causes one item inserted queue thus eventually q
empty terminates q empty every statement derivable


fithe generalized architecture

procedure run p

initialize q assignments defined rules antecedents priorities
q empty

remove lowest priority element b w q

b assigned weight

b w

insert assignments derivable b w assignments
rule p q priority specified rule
return
figure running set prioritized rules
single rule antecedents weight already weight implies
every derivable statement weight
ready define knuths lightest derivation
easily described terms prioritized rules
definition knuths lightest derivation let r finite set non decreasing
superior rules define set prioritized rules k r setting priority rule
r weight conclusion kld given execution k r
running k r b w added w b r
means assignments represent lightest derivations
assignments inserted non decreasing weight order stop
soon insert weight assignment goal expand statements b
b r goal r statements b b r goal r
properties follow general described next section
implementation
figure implemented run log n time n
refer size defined prioritized rules p
practice set prioritized rules p often specified implicitly terms small
number rules variables case executing p closely related
work logical described mcallester
main difficulty devising efficient implementation procedure figure
step step need weight assignments combined
b w derive weight assignments logical work shows
set inference rules variables transformed set rules
every rule two antecedents particularly simple form moreover
transformation increase number rules much rules
transformed execution implemented efficiently hashtable represent
heap represent q indexing tables allow us perform step quickly


fifelzenszwalb mcallester

consider second set rules parsing figure represented
single rule variables moreover rule two antecedents executing
parsing rules keep track table mapping value j statements phrase j
weight table quickly statements weight
combined statement form phrase z j k similarly keep
track table mapping value j statements phrase z j k weight
second table lets us quickly statements combined statement
form phrase j refer reader mcallester details

lightest derivation
lightest derivation ld generalization search lightest
derivation subsumes parsing similar kld
use heuristic function speed computation consider lightest derivation
rules r goal statement goal knuths expand statement b
b r goal r heuristic function ld avoid expanding
statements light derivations part light derivation goal
let r set rules statements h heuristic function assigning
weight statement h b estimate additional weight required
derive goal derivation b note case shortest path
weight exactly distance node goal value b r h b provides
figure merit statement b lightest derivation expands
statements order figure merit
say heuristic function monotone every rule g c r
derivable weight assignments ai wi
wi h ai g w wn h c



definition agrees standard notion monotone heuristic function rules
come shortest path h monotone h goal
h admissible appropriate notion admissibility correctness
ld however required h monotone h goal finite
case monotonicity implies heuristic value every statement c appears
derivation goal finite assume h c finite every statement h c
finite ignore c every rule derives c
definition lightest derivation let r finite set non decreasing rules h
monotone heuristic function r define set prioritized rules r setting
priority rule r weight conclusion plus heuristic value
g w wn h c ld given execution r
execution r correctly computes lightest derivations
expands statements order figure merit values
theorem execution r b w w b r
proof proof induction size statement trivial
suppose statement true right removed b wb q


fithe generalized architecture

added fact b wb q implies weight assignment derivable
thus wb b r
suppose derivation b weight wb wb consider moment right
removed b wb q added let g c
rule antecedents ai weight conclusion c
let wc g r r induction hypothesis weight ai
ai r thus c wc q priority wc h c let wc weight assigns
c since g non decreasing know wc wc since h monotone wc h c wb h b
follows monotonicity condition along path c b
note wc h c wb h b turn implies b wb weight
assignment q minimum priority
theorem execution r statements expanded order figure
merit value b r h b
proof first minimum priority q decrease throughout
execution suppose b w element q minimum priority
removing b w q decrease minimum priority suppose add
b w insert assignments derivable b w q since h monotone
priority every assignment derivable b w least priority b w
weight assignment b w expanded removed q added
last theorem w b r definition r weight assignment
queued priority b r h b since removed b w q must
minimum priority queue minimum priority decrease time
must expand statements order figure merit value
accurate heuristic functions ld much efficient kld
consider situation perfect heuristic function suppose h b
exactly additional weight required derive goal derivation b
figure merit b r h b equals weight lightest derivation goal uses
b case ld derive goal expanding statements part
lightest derivation goal
correctness kld follows correctness ld set non decreasing
superior rules consider trivial heuristic function h b fact
rules superior imply heuristic monotone theorems imply
knuths correctly computes lightest derivations expands statements
order lightest derivable weights

heuristics derived abstractions
consider case additive rules rules weight conclusion
sum weights antecedents plus non negative value v called weight
rule denote rule v c weight derivation
additive rules sum weights rules appear derivation tree
context statement b finite tree rules add derivation
b tree get derivation goal intuitively context b derivation goal
hole filled derivation b see figure


fifelzenszwalb mcallester



goal




context c

c

context

derivation


derivation


derivation


figure derivation goal defines contexts statements appear derivation tree note context c together rule c
derivations define context

additive rules context weight sum weights rules
let r set additive rules statements b define context b r
weight lightest context b value b r context b r
weight lightest derivation goal uses b
contexts derived rules r together context rules c r defined
follows first goal empty context weight zero captured rule
antecedents context goal rule v c r put n rules
c r rules capture notion context c derivations aj j
define context ai
context c ai ai v context ai
figure illustrates context c together derivations rule
c define context


fithe generalized architecture

say heuristic function h admissible h b context b r admissible
heuristic functions never estimate weight deriving goal derivation
particular statement heuristic function perfect h b context b r
obtain admissible monotone heuristic functions abstractions
abstractions
let r lightest derivation statements rules r abstraction
r given r map abs every rule
v c r rule abs abs v abs c r v v
abstraction used define monotone admissible heuristic
function original
usually think abs defining coarsening mapping several statements
abstract statement example parser abs might map lexicalized
nonterminal n phouse nonlexicalized nonterminal n p case abstraction
defines smaller abstract statements abstractions often defined
mechanical way starting map abs set abstract statements
project rules r abs get set abstract
rules typically several rules r map abstract rule need
keep one copy abstract rule weight lower bound weight
concrete rules mapping
every derivation r maps abstract derivation abs c r
c r let goal abstract abs goal every context r
maps abstract context see context abs c r context c r
means lightest abstract context weights form admissible heuristic function
h c context abs c r
heuristic function monotone
consider rule v c r let ai wi weight assignments derivable
r case rule abs abs v abs c r v v
abs ai wi derivable r wi wi definition contexts
abstract
x
context abs ai r v
wj context abs c r
j

since v v wj wj
context abs ai r v

x

wj context abs c r

j

plugging heuristic function h adding wi sides
x
wi h ai v
wj h c
j

exactly monotonicity condition equation additive rule


fifelzenszwalb mcallester

abstract defined r relatively small efficiently compute
lightest context weights every statement dynamic programming kld
store weights pattern database lookup table serve heuristic
function solving concrete ld heuristic may able stop
ld exploring lot non promising structures exactly
used culberson schaeffer korf solving large search
section pattern databases used
general setting lightest derivations experiments section demonstrate
technique specific application

hierarchical lightest derivation
main disadvantage pattern databases precompute context
weights every abstract statement often take lot time space
define hierarchical ha ld searches lightest derivations contexts
entire abstraction hierarchy simultaneously often solve
concrete without fully computing context weights level abstraction
level abstraction behavior ha ld similar behavior ld
abstraction derived heuristic function hierarchical queues
derivations statement c priority depends lightest abstract context
c abstract contexts computed advance instead abstract contexts
computed time computing derivations abstract context
c derivations c stalled captured addition context abs c
antecedent rule derives c
define abstraction hierarchy levels sequence lightest derivation additive rules k rk k single abstraction
function abs k abstraction function maps k onto k require k rk abstraction k rk defined previous section
v c rk exists rule abs abs v abs c
rk v v hierarchical computes lightest derivations statements
k contexts k define heuristic values extend abs maps
abstract set statements containing single element since abs
onto k k number statements decrease go
abstraction hierarchy denote abs k abstraction function k obtained
composing abs k times
interested computing lightest derivation goal statement goal let
goal k abs k goal goal level abstraction hierarchical
defined set prioritized rules h figure rules labeled compute derivations
statements one level abstraction context weights level define
priorities rules labeled base compute contexts one level abstraction
derivation weights level define priorities rules labeled start
start start inference handling abstract level
execution h starts computing derivation context start
start continues deriving statements rules
lightest derivation goal found derives context goal


fithe generalized architecture



start


start


context

base

goal k w
w
context goal k



context abs c wc
w


wn
v w wn wc
c v w wn



context c wc
w


wn
v wc w wn
context ai v wc w wn wi

figure prioritized rules h defining ha ld base rules defined k
rules defined rule v c rk
k

base rule starts computing contexts statements rules
general ha ld interleaves computation derivations contexts level
abstraction since execution h uses single priority queue
note computation happens given level abstraction lightest derivation goal found level means structure
abstraction hierarchy defined dynamically example cfdp
could define set statements level abstraction refining statements
appear lightest derivation goal level assume static
abstraction hierarchy
statement c k k use c denote weight
lightest derivation c rk context c denotes weight lightest
context c rk abstract level define context


fifelzenszwalb mcallester

ha ld correctly computes lightest derivations lightest contexts
every level abstraction moreover order derivations contexts
expanded controlled heuristic function defined follows c k k
define heuristic value c contexts level heuristic value
context c derivations level
h c context abs c
h context c c
abstract level define h h context let generalized statement
element k k expression form context c
c k define intrinsic priority follows
p h
c k p context c weight lightest derivation goal k
uses c p c lower bound weight
sections cannot used directly correctness
ha ld rules figure generate heuristic values time
generate derivations depend heuristic values intuitively must
execution prioritized rules h heuristic value available
appropriate point time next lemma shows rules h satisfy monotonicity
property respect intrinsic priority generalized statements theorem proves
correctness hierarchical
lemma monotonicity rule hierarchical
weight antecedent weight conclusion
priority rule h
b h p
proof rules start start follows fact rules
antecedents h h context
consider rule labeled base w goal k see note zero
priority rule w h context goal k b note p goal k
goal k equals priority rule
consider rule labeled wc context abs c wi ai
part note priority rule wc h c wc part b consider
first antecedent rule h context abs c abs c c
p context abs c wc h context abs c wc consider antecedent
ai abs ai p ai wi wc abs ai
h ai context abs ai wc wi implies p ai wi h ai wc
finally consider rule labeled wc context c wj aj
j part note priority rule wi h context ai wi
ppart
b consider first antecedent rule h context c c v j wj
see p context c wc h c wi consider antecedent aj
abs aj h aj p aj wj wi abs aj
h aj wi wj hence p aj wj h aj wi


fithe generalized architecture

theorem execution h maintains following invariants
w w
w q priority w h
p p q
p q denotes smallest priority q
proof initial state empty q contains
context priority initial state invariant true since empty
invariant follows definition h h context invariant follows
fact p q p let q denote state
immediately prior iteration loop figure suppose
invariants true let q denote state iteration
first prove invariant let w element removed q
iteration soundness rules w w clearly
invariant holds w invariant implies p q w h h
invariant know contains case
invariant q follows invariant q invariant part
monotonicity lemma
finally consider invariant q proof reverse induction
abstraction level say level k k form context c
c k reverse induction base case considers level initially
inserts context queue priority p q
must contain context hence invariant holds q
level
assume invariant holds q levels greater k
consider level k first consider statements c k since rules rk additive
every statement c derivable rk lightest derivation derivation weight
c follows correctness knuths moreover additive
rules subtrees lightest derivations lightest derivations structural
induction lightest derivation conclusion c p c p q
c c consider lightest derivation rk conclusion c
p c p q final rule derivation v c corresponds rule
add antecedent context abs c part b monotonicity lemma
antecedents rule intrinsic priority less p q induction
hypothesis lightest derivations ai ai since invariant holds
statements levels greater k context abs c context abs c
implies point rule used derive c c priority p c
p c p q hence item must removed queue therefore
must contain c w w invariant w c
consider form context c c k see c rk
additive thus every statement derivable c rk lightest derivation subtrees
lightest derivations lightest derivations prove structural induction


fifelzenszwalb mcallester

lightest derivation conclusion context c p context c
p q context c context c suppose last rule form
context c ai ai v context ai
rule corresponds rule add antecedent ai part b
monotonicity lemma antecedents rule intrinsic priority less
p q invariant statements k induction hypothesis lightest
derivations c rk antecedents rule lightest weight
point context ai context ai derived priority p ai
p ai p q implies item removed queue invariant
context ai context ai
suppose last rule context goal k rule corresponds base rule add goal k antecedent note p goal k
goal k p context goal k hence p goal k p q invariant statements
k goal k goal k point base rule used queue
context goal k context goal k priority p context goal k previous cases
p context goal k p q implies context goal k context goal k
last theorem implies generalized statements expanded order
intrinsic priority let k number statements c entire abstraction hierarchy
p c p goal goal every statement c p c p context c
conclude ha ld expands k generalized statements computing
lightest derivation goal
example
consider execution ha ld specific example example illustrates
ha ld interleaves computation structures different levels abstraction
consider following abstraction hierarchy levels
x xn yn z zn goal x z goal




x
xi

























xi yj ij goal
x goal
r

r








z



z

x


x













z goal
zi goal
abs xi x abs yi abs zi z abs goal goal
initially q context priority
comes queue gets put nothing else happens
context comes queue gets put statements
abstract context causes rules come rules r
antecedents fire putting x q priority


fithe generalized architecture

x come queue get put causing two
rules fire putting goal priority z priority queue

context x
q goal priority z priority
point goal comes queue goes base rule fires
putting context goal queue priority
context goal comes queue base case contexts two
rules use context goal x put context x
context q priority
context x comes queue gets put abstract
context xi rules put xi q priority
context comes queue goes previous step
rules put yi q priority

context x goal
context goal context x context
q xi yi priority n z priority
next x come queue go causes
rule put goal queue priority
goal comes queue goes stop since
derivation concrete goal
note ha ld terminates fully computing abstract derivations contexts
particular z q z never expanded moreover context z even
queue keep running would eventually derive context z
would allow zi derived

perception pipeline
figure shows hypothetical run hierarchical processing pipeline
vision system system weighted statements edges used derive
weighted statements contours provide input later stages ultimately resulting
statements recognized objects
well known subjective presence edges particular image location
depend context given image patch appears interpreted
perception pipeline stating higher level processes later pipeline
influence low level interpretations kind influence happens naturally lightest


fifelzenszwalb mcallester



edges

contours

recognition



edges

contours

recognition



edges

contours

recognition

figure vision system several levels processing forward arrows represent
normal flow information one stage processing next backward
arrows represent computation contexts downward arrows represent
influence contexts

derivation example lightest derivation complete scene analysis might
require presence edge locally apparent implementing whole
system single lightest derivation avoid need make hard decisions
stages pipeline
influence late pipeline stages guiding earlier stages pronounced use
ha ld compute lightest derivations case influence apparent
structure optimal solution flow information across different
stages processing ha ld complete interpretation derived one level abstraction
guides processing stages concrete level structures derived late stages
pipeline guide earlier stages abstract context weights allows early
processing stages concentrate computational efforts constructing structures
likely part globally optimal solution
emphasized use admissible heuristics note architecture including ha ld used inadmissible heuristic functions course
would break optimality guarantees inadmissible heuristics important
admissible heuristics tend force first stages processing pipeline generate
many derivations derivations composed weights increase causes
large number derivations generated first stages processing
first derivation reaches end pipeline inadmissible heuristics produce behavior
similar beam search derivations generated first stage pipeline flow
whole pipeline quickly natural way construct inadmissible heuristics
simply scale admissible heuristic ones obtained abstractions
possible construct hierarchical inadmissible heuristics obtained
one level abstraction used guide search level

hierarchical methods
section compare ha ld hierarchical search methods


fithe generalized architecture

coarse fine dynamic programming
ha ld related coarse fine dynamic programming cfdp method described
raphael understand relationship consider finding shortest
path trellis graph one shown figure k columns
n nodes every node one column connected constant number nodes
next column standard dynamic programming used shortest path
kn time cfdp ha ld often shortest path much faster
hand worst case behavior different describe
cfdp taking significantly time ha ld
cfdp works coarsening graph grouping nodes column
small number supernodes illustrated figure b weight edge
two supernodes b minimum weight nodes b b
starts dynamic programming shortest path p
coarse graph shown bold figure b supernodes along p
partitioned define finer graph shown figure c procedure repeated
eventually shortest path p go supernodes size one corresponding
path original graph point know p must shortest path
original graph best case optimal path iteration
refinement optimal path previous iteration would log n
shortest paths computations fairly coarse graphs hand worst
case cfdp take n iterations refine whole graph many iterations
involve finding shortest paths large graphs case cfdp takes kn time
much worst standard dynamic programming
suppose use ha ld shortest path graph
one figure build abstraction hierarchy log n levels
supernode level contains nodes one column original graph coarse
graph figure b represents highest level abstraction hierarchy note
ha ld consider small number log n predefined graphs cfdp end
considering much larger number n graphs best case scenario ha ld
expand nodes shortest path level
hierarchy worst case ha ld compute lightest path context every
node hierarchy context node v path v th
abstraction level graph kn nodes edges ha ld spend
kn log kn time computing paths contexts level summing levels
get kn log kn time total much worst kn time
taken standard dynamic programming
hierarchical heuristic search
hierarchical method related ha hida described
holte et al holte et al methods restricted shortest paths
use hierarchy abstractions heuristic function defined
level abstraction shortest paths goal level main
idea run ida compute shortest path computing heuristic values ondemand let abs map node abstraction let g goal node concrete


fifelzenszwalb mcallester













b



c

figure original dynamic programming graph b coarse graph shortest path
shown bold c refinement coarse graph along shortest path

graph whenever heuristic value concrete node v needed call
recursively shortest path abs v abs g recursive call uses heuristic
values defined abstraction computed deeper recursive calls
clear generalize ha hida lightest derivation
rules multiple antecedents another disadvantage methods
potentially stall case directed graphs example suppose
ha hida expand node two successors x x close goal
far point need heuristic value x might
spend long time computing shortest path abs abs g hand
ha ld would wait shortest path fully computed intuitively ha ld
would compute shortest paths abs x abs abs g simultaneously soon
shortest path abs x abs g found start exploring path x
g independent long would take compute path abs abs g


fithe generalized architecture

r
r

r

r
r
r
r

r

figure convex set specified hypothesis r r

convex object detection
consider application ha ld detecting convex objects
images pose formulation similar one described raphael
optimal convex object around point found solving shortest
path compare ha ld search methods including cfdp
pattern databases indicate ha ld performs better
methods wide range inputs
let x reference point inside convex object represent object boundary
polar coordinates respect coordinate system centered x case
object described periodic function r specifying distance x object
boundary function angle specify r finite number angles
n assume boundary straight line segment sample points
assume object contained ball radius r around x r
integer thus object parametrized r rn ri r example
n angles shown figure
every hypothesis r rn specifies convex object hypothesis describes
convex set exactly object boundary turns left sample point ri
increases let c ri ri ri boolean function indicating three sequential
values r define boundary locally convex hypothesis r rn
convex locally convex
throughout section assume reference point x fixed advance
goal optimal convex object around given reference point practice
reference locations found variety methods hough transform
parametrization convex objects similar identical one used raphael



fifelzenszwalb mcallester

let ri ri image data cost measuring evidence boundary segment
ri ri consider finding convex object
sum data costs along whole boundary minimal look
convex hypothesis minimizing following energy function
e r rn

n

x

ri ri



data costs precomputed specified lookup table n r entries
experiments use data cost integral image gradient along
boundary segment another would use data term described
raphael cost depends contrast inside outside
object measured within pie slice defined
optimal convex object found standard dynamic programming techniques let b r r ri ri cost optimal partial convex object starting
r r ending ri ri keep track last two boundary points
enforce convexity constraint extend partial objects keep track
first two boundary points enforce rn r convexity constraint r
compute b recursive formula
b r r r r r r
b r r ri ri min b r r ri ri ri ri
ri

minimization choices ri c ri ri ri true
cost optimal object given minimum value b n r r rn r
c rn r r true optimal object found tracing back typical dynamic programming main dynamic
programming table n r entries takes r time compute entry
overall runs n r time quite slow
optimal convex objects defined terms lightest derivation
let convex r r ri ri denote partial convex object starting r r
ending ri ri corresponds entry dynamic programming table
described define set statements
convex b c n b c r goal
optimal convex object corresponds lightest derivations goal rules
figure first set rules specify cost partial object r r
second set rules specify object ending ri ri extended
choice ri boundary locally convex ri last set rules specify
complete convex object partial object r rn rn r
boundary locally convex r
construct abstraction hierarchy define l nested partitions radius space
r ranges integers abstract statement instead specifying integer
value r specify range r contained simplify notation


fithe generalized architecture

r r r

convex r r r r r r
r r ri ri ri r c ri ri ri true
convex r r ri ri w
convex r r ri ri w ri ri
r r rn r c rn r r true
convex n r r rn r w
goal w
figure rules finding optimal convex object
assume r power two k th partition p k contains r k ranges
k consecutive integers j th range p k given j k j k
statements abstraction hierarchy
k convex b c n b c p k goal k
k l range p contains single integer let f map range
p k range p k containing statements level k l define
abstraction function
abs convex b c convex f f b f c f
abs goal k goal k
abstract rules use bounds data costs boundary segments si
si si si ranges p k
dk si si

min
ri ri
ri si
ri si

since range p k union two ranges p k one entry dk computed
quickly constant time dk computed bounds levels computed n r time total need abstract versions convexity constraints
si si si p k let c k si si si true exist integers ri ri ri
si si si respectively c ri ri ri true value c k
defined closed form evaluated quickly simple geometry
rules abstraction hierarchy almost identical rules figure
rules level k obtained original rules simply replacing instance
r p k c c k dk


fifelzenszwalb mcallester

standard dp
cfdp
ha ld
pattern database
pattern database

seconds
seconds
seconds
seconds
seconds

table running time comparison example figure
experimental
figure shows example image set reference locations selected manually
optimal convex object found around reference point reference
locations used n r parametrize object table compares
running time different optimization implemented
line shows time took solve contained example image
particular search standard dp uses dynamic programming
solution outlined cfdp method raphael
modified representation convex objects hierarchical uses
abstraction hierarchy described pattern databases used dynamic
programming compute pattern database particular level abstraction
used database provide heuristic values note described
pattern database depends input running times listed include time
took compute pattern database case
see cfdp ha ld pattern databases much efficient
standard dynamic programming use abstractions ha ld
slightly faster methods example note running time
varies output every method
globally optimum objects
quantitative evaluation different search created large set
varying difficulty size follows given value r generated square
images width height r image circle radius less r near
center pixels image corrupted independent gaussian noise
difficulty controlled standard deviation noise figure
shows example images optimal convex object found around centers
graph figure shows running time seconds different search
function noise level size fixed r
sample point indicates average running time random inputs graph
shows running times point circles reliably detected
compared ha ld cfdp pattern databases pd pd pd
pd refer pattern database defined respectively since
pattern database needs recomputed input trade amount
time spent computing database accuracy heuristic provides
see easy better use smaller database defined higher level
abstraction harder worth spending time computing bigger
database ha ld outperforms methods every situation captured


fithe generalized architecture



b
figure reference locations b optimal convex objects



fifelzenszwalb mcallester

figure random images circles optimal convex object around center
one n r noise level images

figure shows running time different methods function
size r fixed noise level sample point
indicates average running time taken random inputs see running
time pattern database grows quickly size increases
computing database fixed level abstraction takes n r time
hand running time cfdp ha ld grows much slower
cfdp performed essentially well ha ld experiment graph figure
shows ha ld performs better difficulty increases

finding salient curves images
classical computer vision involves finding salient curves images intuitively
goal long smooth curves go along paths high image gradient
standard way pose define saliency score search curves
optimizing score methods use score defined simple combination local
terms example score usually depends curvature image gradient
point curve type score often optimized efficiently dynamic
programming shortest paths montanari shashua ullman
basri alter williams jacobs
consider compositional model finding salient curves important
aspect model capture global shape constraints particular looks
curves almost straight something done local constraints
alone local constraints enforce small curvature point curve


fithe generalized architecture

figure running time different search function noise level
input sample point indicates average running time taken
random inputs case n r see text discussion

enough prevent curves turning twisting around long distances
finding salient curve image compositional model defined
solved dynamic programming slow practical
use shortest paths applicable compositional nature
model instead use ld heuristic function derived abstraction
pattern database
let c curve endpoints b c curve endpoints b c
two curves composed form curve c c define weight
composition sum weights c c plus shape cost depends
geometric arrangement points b c figure illustrates idea shape
costs use note c c long arrangement endpoints reflect
non local geometric properties general consider composing c c angle
formed ab bc least lengths c c approximately equal
constraints reduce total number compositions play important role
abstract defined
besides compositional rule say b nearby locations
short curve endpoints b forms base case creating longer curves


fifelzenszwalb mcallester

figure running time different search function size r
sample point indicates average running time taken random
inputs case n see text discussion

b




c

figure curve endpoints c formed composing curves endpoints
b b c assume cost composition
proportional sin cost scale invariant encourages curves
relatively straight

assume short curves straight weight depends image
data along line segment b use data term seg b zero
image gradient along pixels ab perpendicular ab higher otherwise
figure gives formal definition two rules model constants k
k specify minimum maximum length base case curves l constant


fithe generalized architecture

pixels b c angle ab bc least l
curve b w
curve b c w
curve c w w shape b c
pixels b k b k

curve b seg b
figure rules finding almost straight curves pair endpoints l
k k constants shape b c function measuring cost
composition

controlling maximum depth derivations derivation curve b encodes curve
b value seen approximate measure arclength derivation
curve b full binary tree depth encodes curve length
k k let k k allow curves length
rules figure define good measure saliency
prefer short curves long ones define saliency curve
terms weight minus arclength salient curves light long let
positive constant consider finding lightest derivation goal
curve b w
goal w
n n image n statements form curve c moreover
c far apart n choices midpoint b defining two curves
composed lightest derivation curve c makes dynamic programming
solution lightest derivation impractical tried kld even
small images runs memory minutes describe
abstraction used define heuristic function ld
consider hierarchical set partitions image boxes th partition
defined tiling image boxes pixels partitions form pyramid
boxes different sizes level box level union boxes level
boxes level pixels let box containing
th level pyramid define
abs curve b curve b


fifelzenszwalb mcallester


b


b

c


c

figure abstraction maps curve statement statement curves
boxes j curve b gets coarsened curve c j since
light curves almost straight j usually implies b c

figure illustrates map selects pyramid level abstract statement intuitively abs defines adaptive coarsening criteria b far
curve b must long turn implies map b boxes
coarse partition image creates abstract small number
statements without losing much information
define abstract need define set abstract rules recall
every concrete rule r need corresponding abstract rule r weight
r weight r small number rules antecedents
figure concrete rule seg b curve b define corresponding abstract
rule seg b abs curve b compositional rules figure lead abstract
rules composing curves boxes
curve b curve b c v curve c
b c boxes th pyramid level c boxes
level containing c respectively weight v shape b c
b c arbitrary pixels b c respectively compute value
v bounding orientations line segments ab bc boxes


fithe generalized architecture

pixels running time seconds

pixels running time seconds

pixels running time seconds
figure salient curve different images running time sum
time spent computing pattern database time spent solving
concrete



fifelzenszwalb mcallester

figure example salient curve goes locations essentially
local evidence curve locations

abstract defined relatively small even large images
use pattern database outlined section input image use
kld compute lightest context weights every abstract statement use
weights heuristic values solving concrete ld figure illustrates
obtained method seems abstract
able capture short curves extended salient curve took
one minute salient curve images figure lists
dimensions image running time case
note rely initial binary edge detection stage instead
base case rules allow salient curves go pixel even local
evidence boundary particular location figure shows example
happens case small part horse back blends background
consider local properties alone
curve finding described section would difficult formulate
without ld general notion heuristics derived abstractions lightest
derivation however framework introduced becomes
relatively easy specify
future plan compose rules computing salient curves rules
computing complex structures basic idea pyramid boxes defining
abstract applicable variety computer vision


fithe generalized architecture

conclusion
although presented preliminary last two sections view
main contribution providing general architecture perceptual inference
dijkstras shortest paths search fundamental
many applications knuth noted generalization dijkstras general
defined set recursive rules given similar generalizations search heuristics derived abstractions described
method solving lightest derivation hierarchy abstractions
finally outlined generalizations construction
processing pipelines perceptual inference

acknowledgments
material upon work supported national science foundation
grant

references
basri r alter extracting salient curves images analysis
saliency network ieee conference computer vision pattern recognition
bonet b geffner h better ao proceedings
national conference artificial intelligence
bulitko v sturtevant n lu j yau state abstraction real time heuristic
search technical report university alberta department computer science
charniak e statistical language learning mit press
culberson j schaeffer j pattern databases computational intelligence

dijkstra e note two connection graphs numerical mathematics
edelkamp symbolic pattern databases heuristic search panning international conference ai scheduling
felner finding optimal solutions graph partitioning heuristic
search annals mathematics artificial intelligence
geman potter chi z composition systems quarterly applied
mathematics
hansen e zilberstein lao heuristic search finds solutions
loops artificial intelligence
hart p nilsson n raphael b formal basis heuristic determination
minimal cost paths ieee transactions systems science cybernetics

holte r grajkowski j tanner b hierarchical heuristic search revisited
symposium abstraction reformulation approximation


fifelzenszwalb mcallester

holte r perez zimmer r macdonald hierarchical searching abstraction hierarchies efficiently proceedings national conference artificial
intelligence
jimenez p torras c efficient searching implicit
graphs cycles artificial intelligence
jin geman context hierarchy probabilistic image model
ieee conference computer vision pattern recognition
klein manning c parsing fast exact viterbi parse selection proceedings hlt naacl
knuth generalization dijkstras information processing letters

korf r finding optimal solutions rubiks cube pattern databases
proceedings national conference artificial intelligence
korf r felner disjoint pattern database heuristics artificial intelligence

korf r zhang w thayer hohwald h frontier search journal
acm
mcallester complexity analysis static analyses journal acm

montanari u optimal detection curves noisy pictures communications
acm
nilsson n principles artificial intelligence morgan kaufmann
pearl j heuristics intelligent search strategies computer solving
addison wesley
rabiner l tutorial hidden markov selected applications speech
recognition proceedings ieee
raphael c coarse fine dynamic programming ieee transactions pattern
analysis machine intelligence
shashua ullman structural saliency detection globally salient
structures locally connected network ieee international conference
computer vision
tu z chen x yuille zhu image parsing unifying segmentation
detection recognition international journal computer vision

williams l jacobs local parallel computation stochastic completion
fields ieee conference computer vision pattern recognition
zhang w korf r study complexity transitions asymmetric traveling
salesman artificial intelligence




