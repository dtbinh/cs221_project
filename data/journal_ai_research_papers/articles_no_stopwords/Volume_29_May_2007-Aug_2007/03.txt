Journal Artificial Intelligence Research 29 (2007) 421-489

Submitted 8/06; published 8/07

Algebraic Graphical Model Decision
Uncertainties, Feasibilities, Utilities
Cedric Pralet

cedric.pralet@onera.fr

ONERA Toulouse, France
2 av. Edouard Belin, 31400 Toulouse

Gerard Verfaillie

gerard.verfaillie@onera.fr

ONERA Toulouse, France
2 av. Edouard Belin, 31400 Toulouse

Thomas Schiex

thomas.schiex@toulouse.inra.fr

INRA Toulouse, France
Chemin de Borde Rouge, 31320 Castanet-Tolosan

Abstract
Numerous formalisms dedicated algorithms designed last decades
model solve decision making problems. formalisms, constraint networks, express simple decision problems, others designed take account uncertainties, unfeasible decisions, utilities. Even single formalism, several
variants often proposed model different types uncertainty (probability, possibility...) utility (additive not). article, introduce algebraic graphical model
encompasses large number formalisms: (1) first adapt previous structures
Friedman, Chu Halpern representing uncertainty, utility, expected utility
order deal generic forms sequential decision making; (2) structures,
introduce composite graphical models express information via variables linked
local functions, thanks conditional independence; (3) graphical models,
finally define simple class queries represent various scenarios terms
observabilities controllabilities. natural decision-tree semantics queries
completed equivalent operational semantics, induces generic algorithms.
proposed framework, called Plausibility-Feasibility-Utility (PFU) framework,
provides better understanding links existing formalisms,
covers yet unpublished frameworks (such possibilistic influence diagrams) unifies
formalisms quantified boolean formulas influence diagrams. backtrack
variable elimination generic algorithms first step towards unified algorithms.

1. Introduction
last decades, numerous formalisms developed express solve decision
making problems. problems, agent must make decisions consisting either
choosing actions ways fulfill (as action planning, task scheduling, resource
allocation), choosing explanations observed phenomena (as diagnosis situation
assessment). choices may depend various parameters:
1. uncertainty measures, call plausibilities, may describe beliefs state
environment;
2. preconditions may satisfied decision feasible;
c
2007
AI Access Foundation. rights reserved.

fiPralet, Verfaillie, & Schiex

3. possible states environment decisions generally
value decision makers point view. Utilities expressed model costs,
gains, risks, satisfaction degrees, hard requirements, generally, preferences;
4. time involved, decision processes may sequential environment may
partially observable. means may several decision steps,
values variables may observed two steps, chess
player plays turn observe move opponent playing
again;
5. may adversarial collaborative decision makers, controlling
set decisions. Hence, multi-agent aspect yield partial controllabilities.
Given plausibilities defined states environment, feasibility constraints decisions, utilities defined decisions states environment, given possible multiple decision steps, objective provide decision
maker optimal decision rules decision variables controls, depending
environment agents. concise, class problems denoted
class sequential decision problems plausibilities, feasibilities, utilities.
Various formalisms designed cope problems class, sometimes
degenerated form (covering subset features general problem):
formalisms developed boolean satisfiability framework: satisfiability problem (SAT), quantified boolean formulas, stochastic SAT (Littman, Majercik, & Pitassi,
2001), extended stochastic SAT (Littman et al., 2001);
formalisms developed close constraint satisfaction framework: constraint
satisfaction problems (CSPs, Mackworth, 1977), valued/semiring CSPs (Bistarelli,
Montanari, Rossi, Schiex, Verfaillie, & Fargier, 1999) (covering classical, fuzzy, additive, lexicographic, probabilistic CSPs), mixed CSPs probabilistic mixed CSPs
(Fargier, Lang, & Schiex, 1996), quantified CSPs (Bordeaux & Monfroy, 2002),
stochastic CSPs (Walsh, 2002);
formalisms developed represent uncertainty extended represent decision
problems uncertainty: Bayesian networks (Pearl, 1988), Markov random fields
(Chellappa & Jain, 1993) (also known Gibbs networks), chain graphs (Frydenberg, 1990), hybrid mixed networks (Dechter & Larkin, 2001; Dechter & Mateescu,
2004), influence diagrams (Howard & Matheson, 1984), unconstrained (Jensen & Vomlelova, 2002), asymmetric (Smith, Holtzman, & Matheson, 1993; Nielsen & Jensen,
2003), sequential (Jensen, Nielsen, & Shenoy, 2004) influence diagrams, valuation
networks (Shenoy, 1992), asymmetric (Shenoy, 2000) sequential (Demirer &
Shenoy, 2001) valuation networks;
formalisms developed classical planning framework, STRIPS planning (Fikes & Nilsson, 1971; Ghallab, Nau, & Traverso, 2004), conformant planning (Goldman & Boddy, 1996), probabilistic planning (Kushmerick, Hanks, &
Weld, 1995);

422

fiThe PFU Framework

formalisms Markov decision processes (MDPs), probabilistic, possibilistic,
using Spohns epistemic beliefs (Spohn, 1990; Wilson, 1995; Giang & Shenoy,
2000), factored not, possibly partially observable (Puterman, 1994; Monahan, 1982;
Sabbadin, 1999; Boutilier, Dean, & Hanks, 1999; Boutilier, Dearden, & Goldszmidt,
2000).
Many formalisms present interesting similarities:
include variables modeling state environment (environment variables)
decisions (decision variables);
use sets functions which, depending formalism considered, model
plausibilities, feasibilities, utilities;
use operators either combine local information (such aggregate probabilities independence hypothesis, + aggregate gains costs), project
global information (such + compute marginal probability, min max
compute optimal decision).
Even meaning variables, functions, combination projection operators
may specific formalism, seen graphical models sense
exploit, implicitly explicitly, hypergraph local functions variables.
article shows possible build generic algebraic framework subsuming many
formalisms reducing decision making problems sequence so-called variable
eliminations aggregation local functions.
generic framework able provide:
better understanding existing formalisms: generic framework obvious
theoretical pedagogical interest, since bring light similarities differences formalisms covered help people different communities
communicate common basis;
increased expressive power : generic framework may able capture problems
cannot directly modeled existing formalism. increased expressiveness reachable capturing essential algebraic properties existing
frameworks;
generic algorithms: ultimately, besides generic framework, possible
define generic algorithms capable solving problems defined framework.
objective fits growing effort identify common algorithmic approaches
developed solving different AI problems. may facilitate crossfertilization allowing subsumed framework reuse algorithmic ideas defined
another one.
1.0.1 Article Outline
introduction notations notions, article starts showing,
catalog existing formalisms decision making, generic algebraic framework
423

fiPralet, Verfaillie, & Schiex

informally identified. generic framework, called Plausibility-Feasibility-Utility
(PFU) framework, formally introduced three steps: (1) algebraic structures capturing plausibilities, feasibilities, utilities introduced (Section 4), (2) algebraic structures exploited build generic form graphical model (Section 5),
(3) problems graphical models captured notion queries (Section 6).
framework analyzed Section 7 generic algorithms defined Section 8.
table recapitulating main notations used available Appendix proofs
propositions theorems appear Appendix B. short version framework
described article already published (Pralet, Verfaillie, & Schiex, 2006c).

2. Background Notations Definitions
essential objects used article variables, domains, local functions (called
scoped functions).
Definition 1. domain values variable x denoted dom(x) every
dom(x), (x, a) denotes assignment value x. extension, set
variables S, denote
Q dom(S) Cartesian product domains variables
S, i.e. dom(S) = xS dom(x). element dom(S) called assignment S.1
A1 , A2 assignments disjoint subsets S1 , S2 , A1 .A2 , called concatenation
A1 A2 , assignment S1 S2 variables S1 assigned A1
variables S2 assigned A2 . assignment set variables S,
projection onto 0 assignment 0 variables assigned
value A.
Definition 2. (Scoped function) scoped function pair (S, ) set
variables function mapping elements dom(S) given set E. following,
often consider implicit denote scoped function (S, ) alone.
set variables called scope denoted sc(). assignment
superset sc() A0 projection onto sc(), define (A) (A) = (A0 ).
example, scoped function mapping assignments sc() elements
boolean lattice B = {t, f } analogous constraint describing subset dom(sc())
authorized tuples constraint networks.
this, general notion graphical model defined:
Definition 3. (Graphical model) graphical model pair (V, ) V = {x1 , . . . , xn }
finite set variables = {1 , . . . , } finite set scoped functions whose
scopes included V .
terminology graphical models used simply set scoped functions
represented hypergraph contains one hyperedge per function scope.
see, hypergraph captures form independence (see Section 5) induces
parameters time space complexity algorithms (see Section 8).
definition graphical models generalizes usual one used statistics, defining graphical
1. assignment = {x1 , . . . , xk } actually set variable-value pairs {(x1 , a1 ), . . . , (xk , ak )};
assume variables implicit using tuple values (a1 , . . . , ak ) dom(S).

424

fiThe PFU Framework

model (directed not) graph nodes represent random variables
structure captures probabilistic independence relations.
Local scoped functions graphical model give space-tractable definition
global function defined aggregation. example, Bayesian network (Pearl,
1988) global probability distribution Px,y,z x, y, z may defined product
(using operator ) set scoped functions {Px , Py|x , Pz|y }. Local scoped functions
facilitate projection information expressed graphical model onto
smaller scope. example, order compute
P marginal
P probability distribution Py,z
previous network, computeP x Px,y,z = ( x Px Py|x ) Pz|y avoid
taking Pz|y account. operator
used project information onto smaller
scope eliminating variable x. Operators used combine scoped functions called
combination operators, operators used project information onto smaller scopes
called elimination operators.
Definition 4. (Combination) Let 1 , 2 scoped functions E1 E2 respectively. Let
: E1 E2 E binary operator. combination 1 2 , denoted 1 2 ,
scoped function E scope sc(1 )sc(2 ) defined (1 2 )(A) = 1 (A)2 (A)
assignments sc(1 ) sc(2 ). called combination operator 1
2 .
rest article, combination operators denoted .
Definition 5. (Elimination) Let scoped function E. Let op E
associative, commutative, identity element . elimination variable
x op scoped function whose scope sc() {x} whose value
assignment scope (opx )(A) = opadom(x) (A.(x, a)). context, op called
elimination operator x. elimination set variables = {x1 , . . . , xk }
function scope sc() defined opS (A) = opA0 dom(S) (A.A0 ).
P
Hence, computing x (Px Py|x Pz|x ), scoped functions aggregated using
combination operator = information projected eliminating x using
elimination operator +. article, denotes elimination operators.
cases, elimination set variables operator op scoped
function performed subset dom(S) containing assignments
satisfy property denoted boolean scoped function F . Then, must compute
every dom(sc() S) value opA0 dom(S),F (A0 )=t (A.A0 ). simplicity
homogeneity, order always use elimination dom(S), equivalently
truncate elements dom(S) satisfy F mapped special
value (denoted ) defined new identity op.
Definition 6. (Truncation operator) unfeasible value new special element
supposed outside domain E every elimination operator op : E E E.
explicitly extend every elimination operator op : E E E E {} taking
convention op(, e) = op(e, ) = e e E {}.
Let {t, f } boolean lattice. boolean b e E, define b ? e
equal e b = otherwise. ? called truncation operator.

425

fiPralet, Verfaillie, & Schiex

Given boolean scoped function F , ? make possible write quantities
opA0 dom(S),F (A0 )=t elimination opS (F ? ).
order solve decision problems, one usually wants compute functions mapping
available information decision. notion decision rules used formalize
this:
Definition 7. (Decision rule, policy) decision rule variable x given set variables
0 function : dom(S 0 ) dom(x) mapping assignment 0 value dom(x).
extension, decision rule set variables given set variables 0 function
: dom(S 0 ) dom(S). set decision rules called policy.
Definition 8. (Optimal decision rule) Consider totally -ordered set E, scoped function
dom(sc()) E, set variables sc(). decision rule : dom(sc()
S) dom(S). said optimal iff, (A, A0 ) dom(sc() S) dom(S),
(A.(A)) (A.A0 ) (resp. (A.(A)) (A.A0 )). decision rule always exists
dom(sc()) finite.
words, optimal decision rules examples decision rules given argmin
argmax (in article, consider optimality decision rules always given
min max totally ordered set).
Definition 9. (Directed Acyclic Graph (DAG)) directed graph G DAG contains
directed cycle. variables used vertices, paG (x) denotes set parents
variable x G.
Last, [1, n] denote set integers 1 n.

3. Examples Graphical Models Generic Framework
present different AI formalisms expressing solving decision problems.
simple case, single decision maximizes utility sought. introduction
plausibilities (uncertainties), unfeasible actions (feasibilities), sequential decision
(several decision steps observations decision steps) appears
sophisticated frameworks. goal section show formalisms
viewed graphical models specific elimination combination operators
used.
3.1 Examples Graphical Models
examples used cover various AI formalisms, briefly described. wider
accurate review existing graphical models could provided (Pralet, 2006).
3.1.1 Constraint Networks
Constraint networks (CNs, Mackworth, 1977), often called constraint satisfaction problems
(CSPs), graphical models (V, ) scoped functions constraints mapping
assignments onto {t, f }. usual query CN determine existence

426

fiThe PFU Framework

assignment V satisfies constraints. setting f t, decision problem
answered computing:


max .
(1)
V



quantity equals true, optimal decision rule V defines solution.
query answered performing eliminations (using max) combination scoped
functions (using ). Replacing hard constraints soft constraints (boolean scoped
functions replaced cost functions) replacing abstract operator equal
+, min, , . . . leads queries valued totally ordered semiring CN (Bistarelli
et al., 1999).
3.1.2 Bayesian Networks
Bayesian networks (BNs, Pearl, 1988) model problems plausibilities expressed
probabilities. BN graphical model (V, ) set local conditional
probability distributions: = {Px | paG (x) , x V }, G DAG vertices V .
BN represents joint probability distribution P
QV variables combination
local conditional probability distributions (PV = xV Px | paG (x) ), combination
local constraints CN defines global constraint variables. One possible query
BN compute marginal probability distribution variable V :
!
X
X

Py =
PV =
Px | paG (x) .
(2)
V {y}

V {y}

xV

Equation 2 corresponds variable eliminations (with +) product scoped functions.
queries BNs MAP (Maximum Posteriori hypothesis), eliminations
max performed.
3.1.3 Quantified Boolean Formulas Quantified CNs
Quantified boolean formulas (QBFs) quantified CNs (Bordeaux & Monfroy, 2002)
model sequential decision problems. Let x1 , x2 , x3 boolean variables. QBF using
so-called prenex conjunctive normal form looks (with f t):
x1 x2 x3 ((x1 x3 ) (x2 x3 )) = max min max((x1 x3 ) (x2 x3 )).
x1

x2

x3

(3)

Thus, query value x1 values x2 , exists value
x3 clauses x1 x3 x2 x3 satisfied? answered
Equation 3 using sequence eliminations (max x1 , min x2 , max x3 )
conjunction clauses. quantified CN, clauses replaced constraints.
3.1.4 Stochastic CNs
stochastic CN (Walsh, 2002) model sequential decision problems probabilities
plausibilities hard requirements utilities, provided decisions influence
environment (the so-called contingency assumption). stochastic CN, two types
427

fiPralet, Verfaillie, & Schiex

variables defined: decision variables di environment (stochastic) variables sj .
global probability distribution environment variables expressed combination
local probability distributions. environment variables mutually independent,
local probability distributions simply unary probability distributions Psj . Finally,
stochastic CN defines set constraints {C1 , . . . , Cm } mapping tuples values onto
{0, 1} (instead {t, f }). allows constraints multiplied probabilities.
Consider situation first two decisions d1 d2 made, environment
variable s1 observed, decisions d3 d4 made, environment variable s2
remains unobserved. possible query stochastic CN compute decision rules
d1 , d2 , d3 , d4 maximize expected constraint satisfaction, Equation 4:
Q

X
X
C
(4)
(Ps1 Ps2 )
max
max
i[1,m] .
d1 ,d2

s1

d3 ,d4

s2

answer query defined Equation 4 determined sequence eliminations
(max decision variables, + environment ones) combination scoped
functions (probabilities combined using , constraints combined using , since
expressed onto {0, 1} instead {t, f }, probabilities combined constraints
using ).
3.1.5 Influence Diagrams
Influence diagrams (Howard & Matheson, 1984) model sequential decision problems
probabilities plausibilities together gains costs utilities.
seen extension BNs including notions decision utility. precisely,
influence diagram composite graphical model defined three sets variables
organized DAG G: (1) set chance variables; S, conditional
probability distribution Ps | paG (s) given parents G specified; (2) set
decision variables; D, paG (d) set variables observed decision
made; (3) set = {u1 , . . . , um } utility variables, associated
utility function Ui = UpaG (ui ) scope paG (ui ). Utility variables P
must leaves
DAG, utility functions define global additive utility UG = i[1,m] Ui .
usual problem associated influence diagram compute decision rules
maximizing global expected utility. modify example used stochastic CNs
replacing Ps1 Ps1 | d2 , Ps2 Ps2 | d1 ,d3 , constraints C1 , . . . , Cm additive
utility functions U1 , . . . , Um , optimal policy obtained computing optimal
decision rules d1 , d2 , d3 , d4 Equation 5:

X
X
P
max
max
Ps1 | d2 Ps2 | d1 ,d3
U
.
(5)

i[1,m]
d1 ,d2

s1

d3 ,d4

s2

Again, answer query defined Equation 5 computed sequence
eliminations (alternating max- sum-eliminations) combination scoped functions (plausibilities combined using , utilities combined using +, plausibilities utilities
combined using ).

428

fiThe PFU Framework

3.1.6 Valuation Networks
Valuation networks (Shenoy, 1992) model sequential decision problems plausibilities, feasibilities, utilities, plausibilities combined using utilities
additive. valuation network composed several sets nodes valuations: (1)
set decision nodes, (2) set chance nodes, (3) set F indicator valuations,
specify unfeasible assignments decision chance variables, (4) set P probability valuations, multiplicative factors joint probability distribution
chance variables, (5) set
P U utility valuations, representing additive factors
joint utility function UG =
Ui U Ui . Arcs nodes used define
order decisions made chance variables observed. order
d1 d2 s1 d3 d4 s2 , shown optimal decision rules d1 , d2 , d3 , d4
defined Equation 6:





X
X

X
?
max
max
Pi
Ui .
(6)
d1 ,d2

s1

d3 ,d4

F

s2

Pi P

Ui U

Local feasibility constraints combined using , combined scoped functions using truncation operator ? (cf. Definition 6).
3.1.7 Finite Horizon Markov Decision Processes
Finite horizon Markov Decision Processes (MDPs, Puterman, 1994; Monahan, 1982; Sabbadin, 1999; Boutilier et al., 1999, 2000) model sequential decision problems plausibilities utilities horizon time-steps. every time-step t, variable st
represents state environment variable dt represents decision made
observing st . factored MDPs, several state variables may used time-step.
probabilistic finite horizon MDP, plausibilities environment described
local probability distributions Pst+1 | st ,dt state st+1 given st dt . utilities
states decisions local additive rewards Rst ,dt , boolean functions Fdt | st
express whether decision dt feasible state st . optimal policy initial state
s1 computed following equation (which bit unusual defining optimal
policies MDP, equivalent usual form):
!
!


max u max . . . u max
d1

s2

d2

sT

dT


t[1,T ]

Fdt |st

?

p Pst+1 |st ,dt
t[1,T [

pu

u Rst ,dt
t[1,T ]

.

(7)

Plausibilities combined using p = , utilities combined using u = +, plausibilities
utilities combined using pu = , decision variables eliminated using max,
environment variables eliminated using u = +. truncation operator ? enables
elimination operators ignore unfeasible decisions.
pessimistic possibilistic finite horizon MDP (Sabbadin, 1999), probability distributions Pst+1 | st ,dt replaced possibility distributions st+1 | st ,dt , rewards Rst ,dt
replaced preferences st ,dt , operators used u = p = u = min
pu : (p, u) max(1 p, u).

429

fiPralet, Verfaillie, & Schiex

3.2 Towards Generic Framework
previous section shows usual queries various existing formalisms reduced
sequence variable eliminations combination scoped functions.
observation led definition algebraic MDPs (Perny, Spanjaard, & Weng,
2005) definition valuation algebras (Shenoy, 1991; Kolhas, 2003), generic
algebraic framework eliminations performed combination scoped
functions. However, valuation algebras use one combination operator, whereas several
combination operators may needed manipulate different types scoped functions (as
previously shown). Moreover, valuation algebras deal one type elimination,
whereas several elimination operators may required handling different types
variables. valuation networks (Shenoy, 2000), plausibilities necessarily represented
probabilities, eliminations min cannot performed. Essentially, powerful
framework needed.
order simple yet general enough cover queries defined Equations 1
7, generic form consider is:
!



!
Sov



F

?

p
Pi P

Pi

pu

u
Ui U

Ui

.

(8)

(1) used combine local feasibilities, p used combine plausibilities, u
used combine utilities, pu used combine plausibilities utilities,
truncation operator ? used ignore unfeasible decisions without deal
elimination operations restricted domains;2 (2) F , P , U (possibly empty) sets
local feasibility, plausibility, utility functions respectively; (3) Sov operatorvariable(s) sequence, indicating eliminate variables. Sov involves min max
eliminate decision variables operator u eliminate environment variables.
Equation 8 still informal. define formally, provide clear
semantics, need define three key elements:
1. must capture essential properties combination operators p , u , pu
used respectively combine plausibilities, utilities, plausibilities utilities.
must characterize elimination operators u p used project information coming utilities plausibilities. operators define
algebraic structure PFU (Plausibility-Feasibility-Utility) framework.
2. algebraic structure, must define generic form graphical model, involving set variables sets scoped functions expressing plausibilities, feasibilities,
utilities (sets P , F , U ). Together, define PFU network. factored
form offered graphical models must analyzed order understand
applied concisely represent global functions (using
notion conditional independence).
2. Equation 8, plausibilities combined using operator p utilities combined
using operator u ; denote models composite graphical models include
different types scoped functions (plausibilities, feasibilities, utilities). Beyond this, Equation 8
allows heterogeneous information among type scoped functions. example, order
manipulate probabilities possibilities, use p defined probability-possibility pairs
(p1 , 1 ) p (p2 , 2 ) = (p1 p2 , min(1 , 2 )).

430

fiThe PFU Framework

3. Finally, must define queries PFU networks capturing interesting decision problems. Equation 8 shows, queries defined sequence Sov operatorvariable(s) pairs, applied combination scoped functions network.
fact answer queries represents meaningful values decision theory point view proved relating approach.
3.3 Summary
informally shown several queries various formalisms dealing plausibilities and/or feasibilities and/or utilities reduce sequences variable eliminations
applied combinations scoped functions, using various operators. intuitively
covered Equation 8.
three key elements (an algebraic structure, PFU network, sequence variable eliminations) needed formally define give sense equation introduced
Sections 4, 5, 6.

4. PFU Algebraic Structures
first element PFU framework algebraic structure specifying information provided plausibilities, feasibilities, utilities combined synthesized.
algebraic structure obtained adapting previous structures defined Friedman,
Chu, Halpern (Friedman & Halpern, 1995; Halpern, 2001; Chu & Halpern, 2003a)
representing uncertainties expected utilities.
4.1 Definitions
Definition 10. (E, ~) commutative monoid iff E set ~ binary operator
E associative (x ~ (y ~ z) = (x ~ y) ~ z), commutative (x ~ = ~ x),
identity 1E E (x ~ 1E = 1E ~ x = x).
Definition 11. (E, , ) commutative semiring iff
(E, ) commutative monoid, identity denoted 0E ,
(E, ) commutative monoid, identity denoted 1E ,
0E annihilator (x 0E = 0E ),
distributes (x (y z) = (x y) (x z)).
Definition 12. Let (Ea , , ) commutative semiring. Then, (Eb , b , ab ) semimodule (Ea , , ) iff
(Eb , b ) commutative monoid, identity denoted 0Eb ,
ab : Ea Eb Eb satisfies
ab distributes b (a ab (b1 b b2 ) = (a ab b1 ) b (a ab b2 )),
ab distributes ((a1 a2 ) ab b = (a1 ab b) b (a2 ab b)),
431

fiPralet, Verfaillie, & Schiex

linearity property: a1 ab (a2 ab b) = (a1 a2 ) ab b,
b Eb , 0Ea ab b = 0Eb 1Ea ab b = b.
Definition 13. Let E set partial order . operator ~ E monotonic
iff (x y) (x ~ z ~ z) x, y, z E.
4.2 Plausibility Structure
Various forms plausibilities exist (Halpern, 2003). usual one probabilities.
shown previously, example Equation 2, probabilities aggregated using p =
combination operator, projected using p = + elimination operator.
plausibilities expressed possibility degrees [0, 1]. Possibilities
eliminated using p = max usually combined using p = min. interesting case
appears possibility degrees booleans describing states environment
completely possible impossible. Plausibilities combined using p =
eliminated using p = .
Another example Spohns epistemic beliefs, known -rankings (kappa rankings, Spohn, 1990; Wilson, 1995; Giang & Shenoy, 2000). case, plausibilities
elements N {+} called surprise degrees, 0 associated non-surprising situations,
+ associated completely surprising (impossible) situations, generally
surprise degree k viewed probability k infinitesimal . Surprise degrees
combined using p = + eliminated using p = min.
capture various plausibility modeling frameworks, start FriedmanHalperns work plausibility measures (Friedman & Halpern, 1995; Halpern, 2001). Weydert (1994) Darwiche-Ginsberg (1992) developed similar approaches.
Friedman-Halperns structure Assume want express plausibilities assignments set variables S. subset dom(S) called event. Friedman
Halpern (1995) define plausibilities elements set Ep called plausibility domain.
Ep equipped partial order p two special elements 0p 1p satisfying
0p p p p 1p p Ep . function P l : 2dom(S) Ep plausibility measure
iff satisfies P l() = 0p , P l(dom(S)) = 1p , (W1 W2 ) (P l(W1 ) p P l(W2 )).
means 0p associated impossibility, 1p associated highest plausibility
degree, plausibility degree set least high plausibility degree
subsets.
Among plausibility measures, focus so-called algebraic conditional plausibility
measures, use abstract functions p p analogous +
probabilities. measures satisfy properties decomposability: disjoint
events W1 , W2 , P l(W1 W2 ) = P l(W1 ) p P l(W2 ). associative commutative,
follows p associative commutative representations disjoint events, i.e.
(a p b) p c = p (b p c) p b = b p exist pairwise disjoint sets
W1 , W2 , W3 P l(W1 ) = a, P l(W2 ) = b, P l(W3 ) = c. details available
Friedman-Halperns references (Friedman & Halpern, 1995; Halpern, 2001).
Restriction Friedman-Halperns structure important aspect FriedmanHalperns work algebraic properties p p hold domains

432

fiThe PFU Framework

definition p p . Although sufficient express manipulate plausibilities, algorithmically restrictive. Indeed, consider Bayesian network involving two
Px1 constant
boolean variables {x1 , x2 } define Px1 ,x2 Px1 Px2 | x1 . Assume
P
Px2 | x1 ((x2 , t)) must
factor 0 = 0.5. order evaluate Px2 ((x2 , t)), quantity x1 0 P
computed. so, simpler factor compute 0 x1 Px2 | x1 ((x2 , t)).
Px2 | x1 ((x2 , t).(x1 , t)) = 0.6 Px2 | x1 ((x2 , t).(x1 , f )) = 0.8, answer 0.5(0.6+0.8) =
0.7. Performing 0.6 + 0.8 requires applying addition outside range usual probabilities, p b defined + b 1, since two probabilities whose sum exceeds
1 cannot associated disjoint events.
take issues account, adapt Friedman-Halperns Ep , p , p p
p become closed Ep Friedman-Halperns axioms hold closed
structure. closure performed, obtain plausibility structure.
Definition 14. plausibility structure tuple (Ep , p , p )
(Ep , p , p ) commutative semiring (identities p p denoted 0p
1p respectively),
Ep equipped partial order p 0p = min(Ep ) p
p monotonic respect p .
Elements Ep called plausibility degrees
Note 1p necessarily maximal element Ep . probabilities, FriedmanHalperns structure would ([0, 1], +0 , ), +0 b = + b + b 1
undefined otherwise. order get closed operators, take (Ep , p , p ) = (R+ , +, )
therefore 1p = 1 maximal element Ep . cases, Friedman-Halperns
structure already closed. case -rankings (where (Ep , p , p ) = (N
{+}, min, +)) possibilities (where (Ep , p , p ) typically ([0, 1], max, min),
although choices ([0, 1], max, ) possible).
Given two plausibility structures (Ep , p , p ) (Ep0 , 0p , 0p ), define E = Ep Ep0 ,
(p1 , p01 ) (p2 , p02 ) = (p1 p p2 , p01 0p p02 ) (p1 , p01 ) (p2 , p02 ) = (p1 p p2 , p01 0p p02 ),
(E, , ) plausibility structure too. allows us deal different kinds plausibilities (such probabilities possibilities) families probability distributions.
4.2.1 Plausibility Measures Plausibility Distributions
Let us consider plausibility measure (Friedman & Halpern, 1995; Halpern, 2001) P l :
2dom(S) Ep set variables S. Assume P l(W1 W2 ) = P l(W1 ) p P l(W2 )
disjoint sets W1 , W2 2dom(S) , case Friedman-Halperns algebraic
plausibility measures. assumption entails P l(W ) = p AW P l({A}) W
2dom(S) . holds even W = since 0p identity p . Hence, defining
P l({A}) complete assignments suffices describe P l. Moreover,
case, three conditions defining plausibility measures (P l(dom(S)) = 1p , P l() = 0p ,
(W1 W2 ) (P l(W1 ) p P l(W2 ))) equivalent p Adom(S) P l({A}) = 1p ,
using monotonicity p third condition. means deal
plausibility distributions instead plausibility measures:

433

fiPralet, Verfaillie, & Schiex

Definition 15. plausibility distribution function PS : dom(S) Ep
p Adom(S) PS (A) = 1p .
normalization condition imposed plausibility distributions simply generalization convention probabilities sum 1. captures fact
disjunction assignments 1p plausibility degree.
Proposition 1. plausibility distribution PS extended give plausibility distribution PS 0 every 0 S, defined PS 0 = p SS 0 PS .
4.3 Feasibility Structure
Feasibilities define whether decision possible not, therefore expressed
booleans {t, f }. set equipped total order bool satisfying f bool t.
Boolean scoped functions expressing feasibilities combined using operator ,
since assignment decision variables feasible iff feasibility functions agree
assignment feasible.
Given scoped function expressing feasibilities, compute whether assignment set variables feasible according computing sc(Fi )S (A), since
feasible according iff one extensions sc(Fi ) feasible. means
projection feasibility functions onto smaller scope uses elimination operator .
result, feasibilities expressed using feasibility structure Sf = ({t, f }, , ).
Sf commutative semiring, plausibility structure. Therefore,
plausibility notions properties apply feasibility. may therefore speak feasibility distributions, normalization condition FS = imposed feasibility
distribution FS means least one decision must feasible.
4.4 Utility Structure
Utilities express preferences take various forms. Typically, utilities combined
+. utilities model priorities combined using min. Also, utilities
represent hard requirements goals achieved properties satisfied,
modeled booleans combined using . generally, utility degrees defined
elements set Eu equipped partial order u . Smaller utility degrees associated
less preferred events. Utility degrees combined using operator u
assumed associative commutative. guarantees combined utilities
depend way combination performed. assume u admits
identity 1u Eu , representing indifference. ensures existence default utility
degree utility scoped functions. properties captured
following notion utility structure.
Definition 16. (Eu , u ) utility structure iff commutative monoid Eu
equipped partial order u . Elements Eu called utility degrees.
Eu may minimum element u representing unacceptable events
annihilator u (the combination event unacceptable one must
unacceptable too). u usually monotonic. properties necessary
establish forthcoming results.
434

fiThe PFU Framework

distinction plausibilities, feasibilities, utilities important
justified using algebraic arguments. Since p u may different operators (for
example, p = u = + usual probabilities additive utilities), must
distinguish plausibilities utilities. necessary distinguish feasibilities
utilities plausibilities. Indeed, imagine simple card game involving two players P1
P2 , three cards: jack J, queen Q, king K. P1 must first play one
card x {J, Q, K}, P2 must play card {J, Q, K}, last P1 must play card
z {J, Q, K}. rule forbids play card consecutively (feasibility functions Fxy :
x 6= Fyz : 6= z). goal P1 two cards x z value strictly
better P2 card y. setting J < Q < K, requirement corresponds two utility
functions Uxy : x > Uyz : z > y. order compute optimal decisions presence
unfeasibilities, must restrict optimizations (eliminations decision variables max
min) feasible values: instead maxx miny maxz (Uxy Uyz ), must compute:



max
min
max
(Uxy (a, b) Uyz (b, c))
,
adom(x)

bdom(y),Fxy (a,b)=t

cdom(z),Fyz (b,c)=t

which, setting f t, logically equivalent


max min Fxy max (Fyz (Uxy Uyz )) .
x



z

latter quantity, feasibility functions concerning P2 play (y) taken account
using logical connective , P2 unfeasible decisions ignored set
scenarios considered. Feasibility functions concerning P1 last move (z) taken account using , P1 consider scenarios achieves forbidden move.
Therefore, feasibility functions cannot handled simply using combination
operator utility functions: need dissociate unfeasible decision
makers (unfeasibility absolute) unacceptable required one decision
maker (utility relative), i.e. decision maker wants decision maker
do.
general level, example Uxy Uyz soft requirements
know exactly advance controls variable, logical connectives
cannot used anymore. order ignore unfeasible values decision variables elimination, use truncation operator ? introduced Definition 6. order eliminate
variable x scoped function ignoring unfeasibilities indicated feasibility
function , simply perform elimination x (Fi ? ) instead . maps
unfeasibilities value , ignored elimination operators (see Definition 6).
example above, Uxy Uyz additive gains costs, would compute


max min Fxy ? max (Fyz ? (Uxy + Uyz )) .
x



z

4.5 Combining Plausibilities Utilities via Expected Utility
define expected utilities, plausibilities utilities must combined. Consider
situation utility ui obtained plausibility pi [1, N ],

435

fiPralet, Verfaillie, & Schiex

p1 p . . . p pN = 1p . L = ((p1 , u1 ), . . . , (pN , uN )) classically called lottery (von Neumann & Morgenstern, 1944). speak expected utility, implicitly speak
expected utility EU (L) lottery L.
standard way combine plausibilities utilities use
P probabilistic expected utility (von Neumann & Morgenstern, 1944) defining EU (L) i[1,N ] (pi ui ):
aggregates plausibilities utilities using combination operator pu = projects
aggregated information using elimination operator u = +. However, alternative
definitions exist:
plausibilities possibilities, EU (L) = mini[1,N ] max(1 pi , ui )
possibilistic pessimistic expected utility (Dubois & Prade, 1995) (i.e. u = min
pu : (p, u) max(1p, u)) EU (L) = maxi[1,N ] min(pi , ui ) possibilistic
optimistic expected utility (Dubois & Prade, 1995) (i.e. u = max pu = min).
plausibilities -rankings utilities positive integers (Giang & Shenoy,
2000), EU (L) = mini[1,N ] (pi + ui ) (i.e. u = min pu = +).
generalize definitions EU (L), start Chu-Halperns work generalized expected utility (Chu & Halpern, 2003a, 2003b).
Chu-Halperns structure Generalized expected utility defined expectation domain, tuple (Ep , Eu , Eu0 , u , pu ) that: (1) Ep set plausibility degrees
Eu set utility degrees; (2) pu : Ep Eu Eu0 combines plausibilities
utilities satisfies 1p pu u = u; (3) u : Eu0 Eu0 Eu0 commutative associative
operator aggregate information combined using pu .
decision problem additive, i.e. when, plausibility degrees p1 , p2 associated disjoint events, (p1 p p2 ) pu u = (p1 pu u) u (p2 pu u), generic definition
expected utility lottery is:
EU (L) = u (pi pu ui ).
i[1,N ]

Classical expectation domains satisfy additional properties u monotonic 0p pu u = 0u , 0u identity u .
Restriction Chu-Halperns structure sequential decision making
use pu : Ep Eu Eu0 u : Eu0 Eu0 Eu0 compute expected utilities
first decision step, need introduce operators 0pu : Ep Eu0 Eu00
0u : Eu00 Eu00 Eu00 compute expected utilities second decision step. end,
decision steps, must define operators pu operators u . order
avoid definition algebraic structure would depend number decision
steps, take Eu = Eu0 work one operator pu : Ep Eu Eu one
operator u : Eu Eu Eu .
plausibilities, sake future algorithms, restrict Chu-Halperns
expectation domains (Ep , Eu , Eu , u , pu ) u pu become closed generalize
properties initial u pu . However, closure sufficient deal
sequential decision making, Chu-Halperns expected utility designed one-step
decision processes only. introduce three additional axioms u pu :
436

fiThe PFU Framework

first axiom similar standard axiom lotteries (von Neumann & Morgenstern, 1944) defining compound lotteries. states lottery L2 involves
utility u plausibility p2 , one utilities lottery L1 expected
utility L2 plausibility p1 , utility u obtained
plausibility p1 p p2 . gives axiom p1 pu (p2 pu u) = (p1 p p2 ) pu u.
require pu distributes u . justify point, assume
lottery L = ((p1 , u1 ), (p2 , u2 )) obtained plausibility p. Two different versions
contribution L global utility degree derived: first p pu
((p1 pu u1 ) u (p2 pu u2 )), second, uses compound lotteries, ((p p
p1 ) pu u1 ) u ((p p p2 ) pu u2 ). want two quantities equal
p, p1 , p2 , u1 , u2 . shown equivalent simpler property p pu (u1 u
u2 ) = (p pu u1 ) u (p pu u2 ), i.e. pu distributes u .
Finally, assume pu right monotonic (i.e. (u1 u u2 ) (p pu u1 u
p pu u2 )). means agent prefers (strictly not) event ev2
another event ev1 , events plausibility degree p,
contribution ev2 global expected utility degree must lesser
contribution ev1 .
axioms define notion expected utility structure.
Definition 17. Let (Ep , p , p ) plausibility structure let (Eu , u ) utility
structure. (Ep , Eu , u , pu ) expected utility structure iff
(Eu , u , pu ) semimodule (Ep , p , p ) (cf. Definition 12),
u monotonic u pu right monotonic u ((u1 u u2 ) (ppu u1 u
p pu u2 )).
Many structures considered literature instances expected utility structures,
shown Proposition 2. results presented remaining article hold
usual expected utility structures, generally structures satisfying
axioms specified Definitions 14, 16, 17.
Proposition 2. structures Table 1 expected utility structures.
possible define complex expected utility structures existing ones.
example, two expected utility structures (Ep , Eu , u , pu ) (Ep0 , Eu0 , 0u , 0pu ),
possible build compound expected utility structure (Ep Ep0 , Eu Eu0 , 00u , 00pu ).
used deal simultaneously probabilistic possibilistic expected utility
generally deal tuples expected utilities.
business dinner example flesh definitions, consider following
toy example, referred sequel. correspond concrete
real-life problem, used simplicity. Peter invites John Mary (a divorced
couple) business dinner order convince invest company. Peter
knows John present end dinner, invest $10K. holds
Mary $50K. Peter knows John Mary come together (one
437

fiPralet, Verfaillie, & Schiex

1
2
3
4
5
6
7
8
9

Ep
R+
R+
[0, 1]
[0, 1]
N {}
{t, f }
{t, f }
{t, f }
{t, f }

p





bool
bool
bool
bool

p
+
+
max
max
min





p


min
min
+





0p , 1p
0, 1
0, 1
0, 1
0, 1
, 0
f,
f,
f,
f,

Eu
R {}
R+
[0, 1]
[0, 1]
N {}
{t, f }
{t, f }
{t, f }
{t, f }

u





bool
bool
bool
bool

u
+

min
min
+





u
+
+
max
min
min





pu
0u , 1u

0, 0

0, 1
min
0, 1
max(1p, u) 1, 1
+
, 0

f,

t,

f, f

t, f

Table 1: Expected utility structures for: 1. probabilistic expected utility additive
utilities (allows probabilistic expected utility cost gain computed); 2. probabilistic expected utility multiplicative utilities, called
probabilistic expected satisfaction (allows probability satisfaction
constraints computed); 3. possibilistic optimistic expected utility; 4. possibilistic pessimistic expected utility; 5. qualitative utility -rankings
positive utilities; 6. boolean optimistic expected utility conjunctive utilities (allows one know whether exists possible world
goals set goals G satisfied); bool denotes order booleans
f bool t; 7. boolean pessimistic expected utility conjunctive utilities
(allows one know whether possible worlds, goals set goals G
satisfied); 8. boolean optimistic expected utility disjunctive utilities (allows
one know whether exists possible world least one goal
set goals G satisfied); 9. boolean pessimistic expected utility disjunctive
utilities (allows one know whether possible worlds, least one goal
set goals G satisfied).

baby-sit child), least one come, case John
comes Mary occurs probability 0.6. menu, Peter order
fish meat main course, white red wine. However, restaurant
serve fish red wine together. John white wine Mary
meat. menu suit them, leave dinner. John comes, Peter
want leave dinner best friend.
Example. dinner problem uses expected utility structure representing probabilistic
expected additive utility (row 1 Table 1): plausibility structure (R+ , +, ), u = +,
pu = , utilities additive gains ((Eu , u ) = (R {}, +), convention
u + () = ).
4.6 Relation Existing Structures
compare algebraic structures defined existing ones (Friedman & Halpern,
1995; Halpern, 2001; Chu & Halpern, 2003a), observe that:

438

fiThe PFU Framework

structures defined less general Friedman-Chu-Halperns, since additional axioms introduced. example, plausibility structures able
model belief functions (Shafer, 1976), decomposable, whereas
possible using Friedman-Halperns plausibility measures (however, authors
aware existing schemes decision theory using belief functions). Moreover, one-step decision processes, Chu-Halperns generalized expected utility
general, since assumes pu : Ep Eu Eu0 whereas consider
pu : Ep Eu Eu .
Conversely, structures defined deal multi-step decision processes
whereas Chu-Halperns generalized expected utility designed one-shot decision
processes. Beyond this, axioms, use closed operators, essentially motivated operational reasons. use less expressive structure
sake future algorithms (cf. Section 8).
set Ep plausibility degrees set Eu utility degrees defined, plausibilities utilities must cardinal. Purely ordinal approaches CP-nets (Boutilier,
Brafman, Domshlak, Hoos, & Poole, 2004), which, Bayesian networks, exploit notion conditional independence express network purely ordinal preference relations,
covered.
pu takes values Eu , implicitly assumed plausibilities utilities
commensurable: works Fargier Perny (1999), describing purely ordinal approach,
qualitative preferences plausibilities necessarily commensurable,
captured either. Also, works Giang Shenoy (2005), satisfy required
associativity, commutativity, identity, annihilator, distributivity properties,
covered implicitly use pu : Ep Eu Eu0 Eu 6= Eu0 (even
expected utility EU (L) = (p1 pu u1 ) u (p2 pu u2 ) lottery L = ((p1 , u1 ), (p2 , u2 )) stays
Eu ).
Furthermore, axioms entail distributional plausibilities covered (the
plausibility set variable assignments determined plausibilities covered
complete assignment): Dempster-Shafer belief functions (Shafer, 1976) Choquet expected
utility (Schmeidler, 1989) encompassed. Finally, one partial order u Eu
defined, assumed decision makers share preferences utilities.
4.7 Summary
section, introduced expected utility structures, first element
PFU framework. specify plausibilities combined projected (using
p p respectively), utilities combined (using u ), plausibilities
utilities aggregated define generalized expected utility (using u pu ).
structure chosen inspired Friedman-Chu-Halperns plausibility measures generalized expected utility. main differences lie addition axioms deal
multi-step decision processes use extended domains closed operators,
motivated operational reasons.

439

fiPralet, Verfaillie, & Schiex

5. Plausibility-Feasibility-Utility Networks
second element PFU framework network scoped functions Pi , ,
Ui (cf. Equation 8) set variables V . network defines compact structured representation state environment, decisions, global
plausibilities, feasibilities, utilities hold them.
rest article, plausibility function denotes scoped function Ep (the
set plausibility degrees), feasibility function scoped function {t, f } (the set
feasibility degrees), utility function, scoped function Eu (the set utility
degrees).
5.1 Variables
structured representations, decisions represented using decision variables,
directly controlled agent, state environment represented environment variables, directly controlled agent. notion agent used
restricted cooperative adversarial decision makers (if uncertainty
way decision maker behaves, decisions controls modeled
environment variables). use VD denote set decision variables denote
set environment variables. VD form partition V .
Example. dinner problem modeled using six variables: bpJ bpM (value
f ), representing Johns Marys presence beginning, epJ epM (value
f ), representing presence end, mc (value f ish meat), representing main
course choice, w (value white red), representing wine choice. Thus,
VD = {mc, w} = {bpJ , bpM , epJ , epM }.
5.2 Decomposing Plausibilities Feasibilities Local Functions
Using combined local functions represent global one raises considerations:
local functions obtained global one, conversely,
local functions directly used, implicit assumptions global function
made. show questions boil notion conditional
independence. following definitions propositions, (Ep , p , p ) corresponds
plausibility structure.
5.2.1 Preliminaries: Generalization Bayesian Networks Results
Assume want express global plausibility distribution PS (cf. Definition 15)
combination local plausibility functions Pi . work Bayesian networks (Pearl,
1988) shown, factorization joint distribution essentially related notion conditional independence. introduce conditional independence, first define
conditional plausibility distributions.
Definition 18. plausibility distribution PS said conditionable iff
exists set functions denoted PS1 | S2 (one function pair S1 , S2 disjoint subsets
S) S1 , S2 , S3 disjoint subsets S,

440

fiThe PFU Framework

(a) assignments S2 PS2 (A) 6= 0p , PS1 | S2 (A) plausibility distribution S1 ,3
(b) PS1 | = PS1 ,
(c) p S1 PS1 ,S2 | S3 = PS2 | S3 ,
(d) PS1 ,S2 | S3 = PS1 | S2 ,S3 p PS2 | S3 ,
(e) (PS1 ,S2 ,S3 = PS1 | S3 p PS2 | S3 p PS3 ) (PS1 ,S2 | S3 = PS1 | S3 p PS2 | S3 ).
PS1 | S2 called conditional plausibility distribution S1 given S2 .
Condition (a) means conditional plausibility distributions must normalized.
Condition (b) means information given empty set variables
change plausibilities states environment. Condition (c) means
conditional plausibility distributions consistent marginalization point view.
Condition (d) analog so-called chain rule probabilities. Condition (e)
kind weak division axiom.4
Proposition 3 gives simple conditions plausibility structure, satisfied usual
frameworks, suffice plausibility distributions conditionable.
Definition 19. plausibility structure (Ep , p , p ) called conditionable plausibility
structure iff satisfies axioms:
p1 p p2 p2 6= 0p , max{p Ep | p1 = p p p2 } exists p 1p ,
p1 p p2 , exists unique p Ep p1 = p p p2 ,
p1 p p2 , exists unique p Ep p2 = p p p1 .
Proposition 3. (Ep , p , p ) conditionable plausibility structure, plausibility distributions conditionable: suffices define PS1 | S2 PS1 | S2 (A) = max{p
Ep | PS1 ,S2 (A) = p p PS2 (A)} dom(S1 S2 ) satisfying PS2 (A) 6= 0p .
systematic definition conditional plausibility distributions given Proposition 3
fits usual definitions conditional distributions, are, probabilities,
PS1 | S2 (A) = PS1 ,S2 (A)/PS2 (A), -rankings, PS1 | S2 (A) = PS1 ,S2 (A) PS2 (A),
possibility degrees combined using min, PS1 | S2 (A) = PS1 ,S2 (A) PS1 ,S2 (A) <
PS2 (A), 1 otherwise. following, every conditioning statement PS1 | S2 conditionable plausibility structures refer canonical notion conditioning given
Proposition 3. Conditional independence defined.
3. avoid specifying properties PS1 | S2 hold assignments S1 S2 satisfying PS2 (A) 6=
0p , use expressions PS1 | S2 = denote dom(S1 S2 ), (PS2 (A) 6= 0p )
(PS1 | S2 (A) = (A)).
4. Compared Friedman Halperns conditional plausibility measures (Friedman & Halpern, 1995;
Halpern, 2001), (c) analog axiom (Alg1), (d) analog axiom (Alg2), (e) analog
axiom (Alg4), axiom (Alg3) corresponds distributivity p p .

441

fiPralet, Verfaillie, & Schiex

Definition 20. Let (Ep , p , p ) conditionable plausibility structure. Let PS
plausibility distribution S1 , S2 , S3 disjoint subsets S. S1 said
conditionally independent S2 given S3 , denoted I(S1 , S2 | S3 ), iff PS1 ,S2 | S3 = PS1 | S3 p
PS2 | S3 .
means S1 conditionally independent S2 given S3 , iff problem
split one part depending S1 S3 , another part depending S2 S3 .5
definition satisfies usual properties conditional independence, Proposition 4
shows.
Proposition 4. I(., . | .) satisfies semigraphoid axioms:
1. symmetry: I(S1 , S2 | S3 ) I(S2 , S1 | S3 ),
2. decomposition: I(S1 , S2 S3 | S4 ) I(S1 , S2 | S4 ),
3. weak union: I(S1 , S2 S3 | S4 ) I(S1 , S2 | S3 S4 ),
4. contraction: (I(S1 , S2 | S4 ) I(S1 , S3 | S2 S4 )) I(S1 , S2 S3 | S4 ).
Proposition 4 makes possible use Bayesian network techniques express information compact way. Bayesian networks, DAG variables used represent
conditional independences variables (Pearl, 1988). cases, image
processing statistical physics, natural express conditional independences
sets variables. probabilities used, situations modeled using chain graphs (Frydenberg, 1990). chain graph, DAG defined DAG
variables, DAG sets variables, called components. Conditional probability distributions Px | paG (x) variables replaced conditional probability distributions Pc | paG (c)
components, Pc | paG (c) expressed factored form c1 c2 . . .ckc . Markov
random fields (Chellappa & Jain, 1993) correspond case Q
unique
component equal V , factored form PV looks 1/Z jJ eHj
(Gibbs distribution).
formally introduce DAGs sets variables, called DAGs components,
use factor plausibility distributions.
Definition 21. DAG G said DAG components set variables iff
vertices G form partition S. C(G) denotes set components G.
c C(G), paG (c) denotes set variables included parents c G, ndG (c)
denotes set variables included non-descendant components c G.
Definition 22. Let (Ep , p , p ) conditionable plausibility structure. Let PS
plausibility distribution let G DAG components S. G said
compatible PS iff I(c, ndG (c) paG (c) | paG (c)) c C(G) (c conditionally
independent non-descendants given parents).
5. Definition 20 differs Halperns, S1 conditionally independent (CI) S2 given S3 iff
PS1 | S2 ,S3 = PS1 | S3 PS2 | S1 ,S3 = PS2 | S3 . Halpern (2001) called definition adopt noninteractivity (NI) showed NI weaker CI. implies NI satisfied often
may lead factorizations. Halpern gave simple axiom (axiom (Alg4)) CI
NI equivalent. Though axiom holds many usual frameworks, hold possibility
degrees combined using min, case covered PFU algebraic structure.

442

fiThe PFU Framework

Theorem 1. (Conditional independence factorization) Let (Ep , p , p ) conditionable plausibility structure let G DAG components S.
(a) G compatible plausibility distribution PS S, PS = p cC(G) Pc | paG (c) .
(b) If, c C(G), function c,paG (c) c,paG (c) (A) plausibility
distribution c assignments paG (c), = p cC(G) c,paG (c)
plausibility distribution G compatible.
Theorem 1 links conditional independence factorization. Theorem 1(a) generalization usual result Bayesian networks (Pearl, 1988) says DAG
variables
compatible probability distribution PS , PS factored
Q
PS = xS Px | paG (x) . Theorem 1(b) generalization standard result Bayesian
networks (Pearl, 1988) says that, given DAG G variables
Q S, conditional
probabilities Px | paG (x) defined variable x S, xS Px | paG (x) defines
probability distribution G compatible. results generalizations
since hold arbitrary plausibility distributions (and probability distributions
only). Results similar spirit provided Halpern (2001), gives conditions
plausibility measure represented Bayesian network.
Theorem 1(a) entails that, order factor global plausibility distribution PS ,
suffices define DAG components compatible it, i.e. express conditional
independences. define DAG, following systematic procedure used.
initial DAG components empty DAG G. C(G) = {c1 , . . . , ck1 }
partition S, do:
1. Let Sk = c1 . . . ck1 ; choose subset ck set Sk variables already
considered.
2. Add ck component G find minimal subset pak Sk I(ck , Sk
pak | pak ). Add edges directed components containing least one variable
pak ck , paG (ck ) = (c{c1 ,...,ck1 })/(cpak 6=) c.
resulting DAG components guaranteed compatible PS , implies, using Theorem 1(a), local functions Pi representing PS simply defined
functions set {Pc | paG (c) , c C(G)}. practice, reasonable notion
causes effects, networks smaller somehow easier build
obtained using following two heuristics order choose ck step
procedure above:
(R1) Consider causes effects: dinner problem, suggests putting epJ
ck causes bpJ w Sk .
(R2) Gather component variables correlated even variables Sk
assigned : bpJ bpM correlated (R1) apply. Indeed, cannot
say bpJ causal influence bpM , bpM causal influence bpJ ,
since Mary John chooses first (s)he baby-sits specified.
even assume bpJ bpM correlated via unmodeled common cause,

443

fiPralet, Verfaillie, & Schiex

coin toss determines baby-sitter. Hence, bpJ bpM put
component c = {bpJ , bpM }.6
say (R1) (R2) build DAG respecting causality. must seen
possible mechanisms help identifying conditional independences using notions
causes effects.
previous results extending Bayesian networks results plausibility distributions
apply feasibilities. Indeed, feasibility structure Sf = ({t, f }, , ) particular
case conditionable plausibility structure, since satisfies axioms Definition 19.
may therefore speak conditional feasibility distribution. set decision
variables, construction DAG compatible feasibility distribution FS leads
factorization FS = cC(G) Fc | paG (c) .
5.2.2 Taking Differenty Types Variables Account
material defined previous subsection enables us factor one plausibility distribution PVE defined set environment variables one feasibility distribution
FVD defined set VD decision variables. However, dealing one plausibility
distribution one feasibility distribution VD sufficient.
Indeed, plausibilities, decision variables influence environment (for example,
health state patient depends treatment chosen doctor). Rather
expressing one plausibility distribution , want express family plausibility distributions , one assignment VD . make clear, define
controlled plausibility distributions.
Definition 23. plausibility distribution controlled VD (or controlled
plausibility distribution), denoted PVE || VD , function dom(VE VD ) Ep ,
assignments AD VD , PVE || VD (AD ) plausibility distribution .
feasibilities, goes way around: values environment variables
constrain possible decisions (for example, unmanned aerial vehicle flying
cannot take off). Thus, want express family feasibility distributions VD ,
one assignment . words, want express controlled feasibility
distribution FVD || .
order directly reuse Theorem 1 controlled distributions, introduce notion
completion controlled distribution. allows us extend distribution
full set variables V assigning plausibility (resp. feasibility) degree
every assignment VD (resp. ), work one plausibility (resp. feasibility)
distribution.
6. Components {bpJ , bpM } could broken assuming example bpM causally influences
bpJ , i.e. Mary chooses baby-sits first. (and prefer to) keep component
{bpJ , bpM } because, general, breaking components increase scopes functions involved.
example, assume want model plausibilities variables representing colors pixels
N N image, color pixel probabilistically depends colors 4 neighbors
only. component approach, results Markov random fields (Chellappa & Jain, 1993) show
local functions obtained scopes size 5 only, whereas component-breaking mechanism,
size largest scope linear N .

444

fiThe PFU Framework

Proposition 5. Let (Ep , p , p ) conditionable plausibility structure. Then,
n N , exists unique p0 p i{1,...,n} p0 = 1p .
Definition 24. Let (Ep , p , p ) conditionable plausibility structure let PVE || VD
controlled plausibility distribution. Then, completion PVE || VD function denoted
PVE ,VD , defined PVE ,VD = PVE || VD p p0 , p0 unique element Ep
p i[1,|dom(VD )|] p0 = 1p (the cardinality set denoted ||).
words, PVE ,VD defined PVE || VD assigning plausibility degree
p0 assignments VD . case probability theory, corresponds saying
assignments VD equiprobable. definition completion controlled
plausibility distribution could made flexible: instead defining uniform plausibility distribution VD , could define plausibility distribution assignment
VD 0p plausibility degree. arbitrarily choose uniform distribution,
goal introduce prior plausibilities decision variables, sake
factorization.
Proposition 6. Let PVE ,VD completion controlled plausibility distribution PVE || VD .
Then, PVE ,VD plausibility distribution VD PVE | VD = PVE || VD .
result, use PVE | VD denote PVE || VD (and equivalent). Similarly,
possible complete controlled feasibility distribution FVD || .
5.2.3 First Factorization
Proposition 7 below, entailed Theorem 1(a), shows obtain first factorization
PVE | VD FVD | .
Definition 25. DAG G typed DAG components VD iff vertices
G form partition VD element partition subset either
VD . vertex G called component. set components contained
(environment components) denoted CE (G) set components contained VD
(decision components) denoted CD (G).
Proposition 7. Let G typed DAG components VD . Let Gp partial
graph G induced arcs G incident environment components. Let Gf
partial graph G induced arcs G incident decision components. Gp
compatible completion PVE || VD (cf. Definition 22) Gf compatible
completion FVD || ,
PVE | VD =

p Pc | paG (c)
cCE (G)

FVD | =


cCD (G)

Fc | paG (c) .

allows us specify local Pi functions: suffices express Pc | paG (c)
Fc | paG (c) express PVE | VD FVD | compact way. fact, could
defined two DAGs, one factorization PVE | VD factorization
FVD | , two DAGs actually always merged soon make
(undemanding) assumption impossible, given x VD , x
influences constrains possible decision values x. assumption ensures
union two DAGs create cycles. use one DAG simplicity.
445

fiPralet, Verfaillie, & Schiex

Example. Consider dinner problem illustrate first factorization step. One way
obtain G use causality-based reasoning described Theorem 1. start
empty DAG. epJ epM effects bpJ , bpM , w, mc,
considered first component c1 . bpJ chosen variable add c1 ,
cannot say bpJ necessarily effect another variable. previously explained,
bpJ cause bpM , effect bpM , bpJ may correlated bpM via
unmodeled cause. result, get c1 = {bpJ , bpM } first component. Obviously, c1
parents DAG first added component.
Then, epJ epM effects w mc, consider epJ epM
second component c2 . Since w necessarily effect mc, add w c2 .
dinner problem specifies ordering fish red wine simultaneously feasible,
know whether wine chosen main course, i.e. w
cause effect mc. result, take c2 = {mc, w}. menu choice
independent present beginning, c2 parent temporary DAG.
epJ direct effect bpJ w (John leaves dinner white wine
chosen), add epJ c3 . Moreover, epJ correlated epM c1 c2 =
{bpJ , bpM , mc, w} assigned. Therefore, take c3 = {epJ }. Given epJ depends
bpJ w, c3 gets {bpJ , bpM } {mc, w} parents. Finally, c4 = {epM }, epM
depends bpM mc (Mary leaves meat chosen) independent epJ given bpM
mc, I({epM }, {epJ , bpJ , w} | {bpM , mc}). entails {epM } added
DAG {bpJ , bpM } {mc, w} parents. Therefore, get CD (G) = {{mc, w}}
set decision components CE (G) = {{bpJ , bpM }, {epJ }, {epM }} set
environment components. DAG components shown Figure 1a.
Using Proposition 7, know joint probability distribution factors PVE | VD =
PbpJ ,bpM PepJ | bpJ ,bpM ,mc,w PepM | bpJ ,bpM ,mc,w joint feasibility distribution
factored FVD | = Fmc,w .
5.2.4 Factorization Steps
Proposition 7 provides us decomposition PVE | VD FVD | based conditional independence relation I(., . | .) Definition 20. may possible perform
factorization steps factoring Pc | paG (c) set local plausibility functions Pi
factoring Fc | paG (c) set local feasibility functions .
cases, expressing factors Pc | paG (c) Fc | paG (c) quite natural. example, p = , variables environment component c = {xi,j | i, j [1, n]}
without parents represent pixel colors, want model Pc two
adjacent pixels different colors, natural define set binary differ
ence constraints xi,j ,xk,l

factor
P

P
=




c
c
x
,x
i[1,n1]
j[1,n]
i,j
i+1,j

i[1,n] j[1,n1] xi,j ,xi,j+1 . decomposition cannot obtained based
conditional independence relation I(., . | .) Definition 20.
settings, Markov random fields (Chellappa & Jain, 1993), systematic
techniques exist obtain factorizations. Bayesian network community
offers systematic techniques: hybrid networks (Dechter & Larkin, 2001),
extract deterministic information contained conditional probability
distributions. precisely, conditional probability distribution Px | paG (x)
446

fiThe PFU Framework

expressed Px | paG (x) = Px | paG (x) , 0-1 function defined

0 Px | paG (x) (A) = 0
(A) =
. factorization Px | paG (x) Px | paG (x)
1 otherwise
computationally relevant constraint propagation techniques 0-1 functions
used solve hybrid networks.
may use another weaker definition conditional independence: valuation-based
systems (Shenoy, 1994), S1 S2 said conditionally independent given S3
regard function S1 ,S2 ,S3 function factors two scoped functions
scopes S1 S3 S2 S3 . definition used first factorization
step destroys normalization conditions may useful
computational point view.
additional factorization steps interest decreasing size
scopes functions involved adding redundant information problem
computationally useful.
every environment component c, Pi F act(c) stands Pi factor
Pc | paG (c) , second factorization gives us
Pc | paG (c) =

p
Pi .
Pi F act(c)

p c Pc | paG (c) = 1p , Pi functions F act(c) satisfy normalization condition
p c (p Pi F act(c) Pi ) = 1p . scopes sc(Pi ) contained sc(Pc | paG (c) ) = c paG (c).
every decision component c, F act(c) stands factor Fc | paG (c) ,
second factorization gives us
Fc | paG (c) =



F act(c)

.

Given c Fc | paG (c) = t, functions F act(c) satisfy normalization condition
c F act(c) = t. Moreover, sc(Fi ) c paG (c).
factorizations, decrease scopes functions involved, could
exploited. Indeed, scoped function Pi internal local
structure, instance Pi noisy-OR gate (Pearl, 1988) Bayesian network,
presence context-specific independence (Boutilier, Friedman, Goldszmidt, & Koller,
1996). internal local structures made explicit representing functions
tools Algebraic Decision Diagrams (Bahar, Frohm, Gaona, Hachtel, Macii, Pardo,
& Somenzi, 1993). rest article, make assumption way
scoped function represented.
Example. PbpJ ,bpM expressed terms first plausibility function P1 specifying probability John Mary present beginning. P1 defined
P1 ((bpJ , t).(bpM , f )) = 0.6, P1 ((bpJ , f ).(bpM , t)) = 0.4, P1 ((bpJ , t).(bpM , t)) =
P1 ((bpJ , f ).(bpM , f )) = 0. add redundant deterministic information second plausibility function P2 defined constraint bpJ 6= bpM (P2 (A) = 1 constraint
satisfied, 0 otherwise). get PbpJ ,bpM = P1 p P2 F act({bpJ , bpM }) = {P1 , P2 }.

447

fiPralet, Verfaillie, & Schiex

PepJ | bpJ ,bpM ,mc,w specified combination two plausibility functions P3
P4 . P3 expresses John absent beginning, absent end: P3
hard constraint (bpJ = f ) (epJ = f ) (P3 (A) = 1 constraint satisfied, 0 otherwise).
Then, P4 : (bpJ = t) ((epJ = t) (w 6= white)) hard constraint specifying
John leaves iff white wine chosen. Hence, PepJ | bpJ ,bpM ,mc,w = P3 p P4
F act({epJ }) = {P3 , P4 }. Similarly, PepM | bpJ ,bpM ,mc,w = P5 p P6 , P5 , P6 defined
constraints, F act({epM }) = {P5 , P6 }.
feasibilities, Fmc,w specified feasibility function F1 expressing ordering fish red wine allowed: F1 : ((mc = f ish)(w = red)) F act({mc, w})
= {F1 }. association local functions components appears Figure 1a.
5.3 Local Utilities
Local utilities defined states environment (as utility
health state patient), decisions (as utility decision buying
car not), states environment decisions (as utility
result horse race bet race).7
order specify local utilities, one standard approach, used CSPs influence
diagrams, directly define set U local utility functions, modeling preferences
hard requirements, decision environment variables. set implicitly defines
global utility UV = uUi U Ui variables. factored form obtained
global joint utility, one may rely, u = +, work Fishburn (1982)
Bacchus-Grove (1995), introduced notion conditional independence utilities.
normalization condition imposed local utilities.
Example. dinner problem, three local utility functions defined. binary
utility function U1 expresses Peter want John leave dinner: U1
hard constraint (bpJ = t) (epJ = t) (U1 (A) = 0 constraint satisfied,
otherwise). Two unary utility functions U2 U3 epJ epM respectively express
gains expected presences end: U2 ((epJ , t)) = 10 U2 ((epJ , f )) = 0 (John
invests $10K present end), U3 ((epM , t)) = 50 U3 ((epM , f )) = 0
(Mary invests $50K present end). U2 U3 viewed soft constraints.
local functions represented graphical model Figure 1b.
5.4 Formal Definition PFU Networks
formally define Plausibility-Feasibility-Utility networks. definition justified previous construction process, holds even plausibility structure
conditionable.
7. influence diagrams, special nodes called value nodes introduced represent outcome
decisions, one utility function associated value nodes (the utility outcome).
PFU framework, directly represent utility functions scoped functions hold
parents value nodes. explicitly express utility functions scoped functions,
plausibility feasibility functions. words, utility functions directly utilities outcome
decision environment variables assignments.

448

fiThe PFU Framework

P4
mc, w

bpJ , bpM
P1 , P 2

F1

w
F1

epJ

epM

P3 , P 4

P5 , P 6

(a)

mc

bpJ

P3

environment

P2 U1

P1

bpM

P5

decision

epJ
U2

plausibility
function

epM

P6

feasibility
function

U3

utility
function

(b)

Figure 1: (a) DAG components (b) Network scoped functions.
Definition 26. Plausibility-Feasibility-Utility network expected utility structure
tuple N = (V, G, P, F, U ) following conditions hold:
V = {x1 , x2 , . . .} finite set finite domain variables. V partitioned VD
(decision variables) (environment variables);
G typed DAG components VD (cf. Definition 25);
P = {P1 , P2 , . . .} finite set plausibility functions. Pi P associated unique component c CE (G) sc(Pi ) c paG (c). set
Pi P associated component c CE (G) denoted F act(c) must satisfy
p (p Pi F act(c) Pi ) = 1p ;
c

F = {F1 , F2 , . . .} finite set feasibility functions. function associated
unique component c CD (G) sc(Fi ) c paG (c). set
F associated
component c CD (G) denoted F act(c) must satisfy
F act(c) = t;
c

U = {U1 , U2 , . . .} finite set utility functions.
5.5 PFU Networks Global Functions
seen obtain PFU network expressing global controlled plausibility
distribution PVE || VD , global controlled feasibility distribution FVD || , global utility
UV . Conversely, let N = (V, G, P, F, U ) PFU network, i.e. set variables, typed
DAG components, sets scoped functions.
global function = p Pi P Pi controlled plausibility distribution given
VD . Moreover, Theorem 1(b), plausibility structure conditionable
Gp partial DAG G induced arcs incident environment components,
Gp compatible completion ;
global function = F controlled feasibility distribution VD given
. Moreover, Theorem 1(b), Gf partial DAG G induced arcs
G incident decision components, Gf compatible completion ;
449

fiPralet, Verfaillie, & Schiex

= uUi U Ui necessarily global utility.
therefore denote PVE || VD , FVD || , UV .
5.6 Back Existing Frameworks
Let us consider formalisms described Section 3 again.
CSP (hard soft) easily represented PFU network N = (V, G, , , U ):
variables V decision variables, G reduced single decision component
containing variables, constraints represented utility functions. Using
feasibility functions represent constraints, would impossible represent inconsistent networks normalization conditions feasibilities. SAT
modeled similarly; difference constraints replaced clauses.
PFU network used represent local functions quantified
boolean formula quantified CSP. differences CSPs SAT appear
consider queries network (see Section 6).
Bayesian network modeled N = (V, G, P, , ): variables V
environment variables, G DAG BN, P = {Px | paG (x) , x V }.
feasibility utility function. chain graph modeled N = (V, G, P, , ),
G DAG components chain graph P set factors
Pc | paG (c) .
stochastic CSP represented PFU network N = (V, G, P, , U ), V
partitioned VD , set decision variables, , set stochastic
variables, G DAG depends relations stochastic variables,
P set probability distributions stochastic variables, U set
constraints.
influence diagram modeled N = (V, G, P, , U ) VD contains
decision variables, contains chance variables, G DAG influence
diagram without utility nodes arcs random variables (i.e.
keep so-called influence arcs), P = {Px | paG (x) , x }.
feasibilities, one utility function Ui defined per utility variable u, scope
Ui paG (u). represent valuation networks, set F feasibility functions
added. Note business dinner example could modeled using standard influence diagram, since influence diagrams cannot model feasibilities
(suitable extensions exist however, Shenoy, 2000).
finite horizon probabilistic MDP modeled N = (V, G, P, F, U ).
time-steps, VD = {dt , [1, ]}{s1 } = {st , [2, ]};8 G DAG
components (a) component contains one variable, (b) unique parent
decision component {dt } {st }, (c) parents environment component
{st+1 } {st } {dt }; P = {Pst+1 |st ,dt , [1, 1]}, F = {Fdt | st , [1, ]},
U = {Rst ,dt , [1, ]}. Modeling finite horizon possibilistic MDP similar.
8. plausibility distribution initial state s1 , s1 viewed environment
variable. corresponds special case decision variables model problem parameters.

450

fiThe PFU Framework

5.7 Summary
section, introduced second element PFU framework: network
variables linked local plausibility, feasibility, utility functions, DAG capturing
normalization conditions. factorization global plausibilities, feasibilities, utilities
scoped functions linked conditional independence.

6. Queries PFU Network
query correspond reasoning task information expressed PFU network.
decision variables involved PFU network considered, answering query may
provide decision rules. Examples informal queries dinner problem
1. best menu choice Peter know present beginning?
2. best menu choice Peter knows present beginning?
3. maximize expected investment restaurant chooses main
course first Peter pessimistic choice, present
beginning observed, last Peter chooses wine?
Dissociating PFU networks queries consistent trend influence diagram community relax so-called information links, Unconstrained Influence
Diagrams (Jensen & Vomlelova, 2002) Limited Memory Influence Diagrams (Lauritzen
& Nilsson, 2001): explains intuition queries change local relations
variables.
section, define simple class queries PFU networks. assume
sequence decisions must performed, order decisions
observations made known. make no-forgetting assumption, is,
making decision, agent aware previous decisions observations.
on, set utility degrees Eu assumed totally ordered. total order
assumption, holds standard frameworks, implies always
exists optimal decision rule. See Subsection 6.7 discussion extend
results partial order.
Two definitions answer query given, first based decision trees,
second operational. equivalence two definitions
established.
6.1 Query Definition
order formulate reasoning tasks PFU network, use sequence Sov operatorvariable(s) pairs. sequence captures different aspects query:
Partial observabilities: Sov specifies order decisions made environment variables observed. x appears left VD (for example
Sov = . . . (u , {x}) . . . (max, {y}) . . .), means value x known (observed) value chosen. Conversely, Sov = . . . (max, {y}) . . . (u , {x}) . . .
x observed choosing y.
451

fiPralet, Verfaillie, & Schiex

Optimistic/pessimistic attitude concerning decision makers: (max, {y}) inserted
elimination sequence decision maker optimistic behavior
agent controlling decision variable (i.e. agent cooperative),
(min, {y}) one pessimistic (i.e. agent controlling adversary).
operator used environment variables always u , model expected
utilities sought.9
Parameters decision making problem: set variables involved
Sov kind parameters. absence indicates want obtain optimal
expected utilities and/or optimal policies assignment S. useful
order evaluate several scenarios simultaneously.
Example. sequence corresponding informal query: maximize
expected investment restaurant chooses main course first Peter pessimistic
choice, present beginning dinner observed, last
Peter chooses wine knowing present end?
Sov = (min, {mc}).(u , {bpJ , bpM }).(max, {w}).(u , {epJ , epM }).
models fact that: (1) Peter pessimistic main course (min mc),
chosen without observing variable (no variable left mc Sov); (2) Peter
makes best choice wine (max w) main course chosen
knowing present beginning (w appears right mc, bpJ , bpM
Sov), knowing present end (w appears left epJ , epM ).
Specifically, bpJ bpM partially observable, whereas epJ epM unobservable.
query becomes Peter observes present beginning
dinner chooses wine knowing present end?,
sequence use Sov = (u , {bpJ , bpM }).(max, {w}).(u , {epJ , epM }). case,
variable mc appear sequence anymore, means mc parameter
answer value parameter sought.
Definition 27. query PFU network pair Q = (N , Sov) N PFU
network Sov = (op1 , S1 )(op2 , S2 ) (opk , Sk ) sequence operator-set variables
pairs
(1) Si disjoint;
(2) either Si VD opi = min max, Si opi = u ;
(3) variables involved Si , called free variables, decision variables;
(4) variables x, different types (one decision variable,
environment variable), directed path component contains
x component contains DAG PFU network N , x
appear right Sov, i.e. either x appears left y, x free
variable.
9. decision made nature plausibility distribution decision,
decision viewed environment variable.

452

fiThe PFU Framework

Condition (1) ensures variable eliminated once. Condition (2) means
optimal decisions sought decision variables (either maximized decision
maker controls decision variable cooperative, minimized adversarial),
whereas expected utilities sought environment variables. Condition (3) means
variables eliminated Sov act problem parameters viewed
decision variables. Condition (4) means x different types x
ancestor y, x assigned y. ensures causality respected variables different types: example, (N , (u , {bpJ , bpM , epJ , epM }).(max, {mc, w})),
violates condition (4), violates causality since menu cannot chosen knowing
present end.
Variables appearing Sov called quantified variables, analogy quantified
boolean formulas. set free variables denoted Vf r . Notice definition
queries prevent environment variable quantified min max,
may u = min u = max. Note straightforward
every PFU network N , exists least one query N without free variables.
[1, k], define
set l(Si ) variables appearing Vf r left Si Sov l(Si ) =
Vf r (j[1,i1] Sj );
set r(Si ) variables appearing right Si Sov r(Si ) = j[i+1,k] Sj .
6.2 Semantic Answer Query
subsection, assume plausibility structure conditionable (cf. Definition 19). controlled plausibility distribution PVE || VD = p Pi P Pi completed (cf. Definition 24) give plausibility distribution PVE ,VD VD . Similarly,
controlled feasibility distribution FVD || = F completed give feasibility distribution FVE ,VD VD . use global utility UV = uUi U Ui
defined PFU network.
Imagine want answer query Q = (N , Sov), N network
dinner problem Sov = (min, {mc}).(u , {bpJ , bpM }).(max, {w}).(u , {epJ , epM }).
answer query, use decision tree. First, restaurant chooses
worst possible main course, taking account feasibility distribution mc. Here,
Fmc ((mc, meat)) = Fmc,w ((mc, meat).(w, white)) Fmc,w ((mc, meat).(w, red)) = =
t. Similarly, Fmc ((mc, f ish)) = t. choices feasible. Then, A1 denotes
assignment mc, uncertainty present beginning given main
course choice described probability distribution PbpJ ,bpM | mc (A1 ). possible
assignment A2 {bpJ , bpM }, i.e. A2 PbpJ ,bpM | mc (A1 .A2 ) 6= 0p , Peter
chooses best wine taking account feasibility Fw | mc,bpJ ,bpM (A1 .A2 ):
restaurant chooses meat, Peter chooses optimal value red white,
restaurant chooses fish, Peter choose white wine only. Then, feasible
assignment A3 w, uncertainty regarding presence John Mary end
dinner given PepJ ,epM | bpJ ,bpM ,mc,w (A1 .A2 .A3 ).
Note conditional probabilities used decision tree directly
defined network. must computed global distributions.
computation challenge large problems.
453

fiPralet, Verfaillie, & Schiex

utility UV (A1 .A2 .A3 .A4 ) associated possible complete assignment
A1 .A2 .A3 .A4 variables. possible assignment A1 .A2 .A3 {bpJ , bpM , mc, w},
last stage, i.e. one epJ epM assigned,
P seen lottery (von
Neumann & Morgenstern, 1944) whose expected utility A4 dom({epJ ,epM }) p(A4 )u(A4 ),
p(A4 ) = PepJ ,epM | bpJ ,bpM ,mc,w (A1 .A2 .A3 .A4 ) u(A4 ) = UV (A1 .A2 .A3 .A4 ).
expected utility becomes reward scenario {bpM , bpJ , mc, w} described
A1 .A2 .A3 . provides us criterion choosing optimal value w. step
bpJ bpM assigned seen lottery, provides us
criterion choosing worst value mc. computation associated previously
described process is:
min
A1 dom(mc),Fmc (A1 )=t
X
PbpJ ,bpM | mc (A1 .A2 )
(
A2 dom({bpJ ,bpM }),PbpJ ,bpM

(

max
X

| mc (A1 .A2 )6=0

A3 dom(w),Fw | mc,bpJ ,bpM (A1 .A2 .A3 )=t

(

PepJ ,epM | bpJ ,bpM ,mc,w (A1 .A2 .A3 .A4 )

A4 dom({epJ , epM })
PepJ ,epM | bpJ ,bpM ,mc,w (A1 .A2 .A3 .A4 ) 6= 0

UV (A1 .A2 .A3 .A4 )))).

Decision rules decision variables (argmin argmax) recorded
computation. formulation represents decision process decision tree
internal level corresponds variable assignments. Arcs associated
assignment set decision variables weighted feasibility decision given
previous assignments. Arcs associated assignment environment variables
weighted plausibility degree assignment given previous assignments. Leaf
nodes correspond utilities complete assignments, node collects values
children compute value.
6.2.1 Formalization Decision Tree Procedure
order formalize decision tree procedure, technical results first introduced
Proposition 8. results definitions preceding skipped first
reading.
Definition 28. Let PS1 | S2 conditional plausibility distribution S1 given S2 let
dom(S2 ). function PS1 | S2 (A) said well-defined iff PS2 (A) 6= 0p . Similarly,
FS1 | S2 conditional feasibility distribution S1 given S2 , then, dom(S2 ),
FS1 | S2 (A) said well-defined iff FS2 (A) = t.
Next, conditioning defined directly controlled plausibility distributions
dom(VD ), PVE || VD (A) plausibility distribution :
Definition 29. Assume plausibility structure used conditionable. Let PVE || VD
controlled plausibility distribution S, 0 two disjoint subsets . define
conditional controlled plausibility distributions by: dom(S 0 VD )
PS 0 || VD (A) 6= 0p , PS | 0 || VD (A) = max{p Ep | PS,S 0 || VD (A) = p p PS 0 || VD (A)},
canonical definition conditioning given Proposition 3. Given controlled feasi454

fiThe PFU Framework

bility distribution FVD || , definition conditional controlled feasibility distributions
FS | 0 || S, 0 disjoint subsets VD similar.
Proposition 8. Assume plausibility structure used conditionable. Let Q =
(N , Sov) query Sov = (op1 , S1 ) (op2 , S2 ) (opk , Sk ). Let Vf r denote set
free variables Q.
(1) Si PSi | l(Si ) (A) well-defined, exists least one A0 dom(Si )
satisfying PSi | l(Si ) (A.A0 ) 6= 0p .
(2) Si VD FSi | l(Si ) (A) well-defined, exists least one A0 dom(Si )
satisfying FSi | l(Si ) (A.A0 ) = t.
(3) 6= Si leftmost set environment variables appearing Sov, then,
dom(l(Si )), PSi | l(Si ) (A) well-defined.
(4) i, j [1, k], < j, Si , Sj , r(Si ) l(Sj ) VD (Sj first set environment variables appearing right Si Sov), (A, A0 ) dom(l(Si ))dom(Si ),
PSi | l(Si ) (A) well-defined, PSi | l(Si ) (A.A0 ) 6= 0p , then, A00 extending A.A0
l(Sj ), PSj | l(Sj ) (A00 ) well-defined.
(5) i, j [1, k], < j, Si VD , Sj VD , r(Si ) l(Sj ) (Sj first set
decision variables appearing right Si Sov), (A, A0 ) dom(l(Si ))dom(Si ),
FSi | l(Si ) (A) well-defined, FSi | l(Si ) (A.A0 ) = t, then, A00 extending A.A0
l(Sj ), FSj | l(Sj ) (A00 ) well-defined.
(6) [1, k] Si , PSi | l(Si ) = PSi | l(Si )VE ||VD .
(7) [1, k] Si VD , FSi | l(Si ) = FSi | l(Si )VD ||VE .
technical results Proposition 8 ensure that, following semantic answer
query (see Definition 30),
quantities PS | l(S) (A.A0 ) FS | l(S) (A.A0 ) used defined (thanks items 3
5 Proposition 8);
eliminations restricted domains defined restricted domains
used never empty (items 1 2 Proposition 8);
conditional distributions used coincide conditioning defined directly
controlled plausibility feasibility distributions PVE || VD FVD || (items 6
7 Proposition 8). useful guarantees PS | l(S) (A.A0 )
FS | l(S) (A.A0 ), priori require notion completion written,
actually independent notion completion, arbitrarily added
basic information expressed PFU network. use PS | l(S) FS | l(S) instead
conditional controlled distributions PS | l(S)VE || VD FS | l(S)VD || notation
convenience explicitly represent PS | l(S)VE || VD FS | l(S)VD ||
depend assignment VD l(S) l(S) respectively.

455

fiPralet, Verfaillie, & Schiex

Definition 30. semantic answer Sem-Ans(Q) query Q = (N , Sov) function
set Vf r free variables Q defined by10

FVf r (A) = f
Sem-Ans(Q)(A) =
Qs(N , Sov, A) otherwise,
Qs inductively defined by:
(1)

Qs(N , , A) = UV (A)

(2)

Qs(N , (op, S) . Sov, A) =


min
Qs (N , Sov, A.A0 )

0

dom(S)



FS|l(S) (A.A0 ) =




Qs (N , Sov, A.A0 )
max











A0 dom(S)
FS|l(S) (A.A0 ) =

u

A0 dom(S)
PS|l(S) (A.A0 ) 6= 0p

(S VD ) (op = min),
(S VD ) (op = max),


PS|l(S) (A.A0 ) pu Qs (N , Sov, A.A0 )

(S ).

words, step involving decision variables (first two cases) corresponds
optimization step among feasible choices, step involving environment variables
(third case) corresponds lottery (von Neumann & Morgenstern, 1944)
rewards Qs (N , Sov, A.A0 ), plausibility attributed reward
PS | l(S) (A.A0 ) (the formula looking ui (pi pu ui ) expected utility lottery).
set decision variables eliminated, decision rule recorded, using
argmax (resp. argmin) max (resp. min) performed.
Example. maximum investment Peter expect, associated decision(s) make chooses menu without knowing attend? answer
question, use query bpJ , bpM , epJ , epM eliminated
mc w represent fact values known menu chosen.
query is:
(N , (max, {mc, w}).(u , {bpJ , bpM , epJ , epM })).
answer $6K, (mc, meat).(w, red) decision. Peter knows comes,
query becomes
(N , (u , {bpJ , bpM }).(max, {mc, w}).(u , {epJ , epM })).
optimal values mc w depend bpJ bpM . answer $26K
$20K gain observability present. decision rule {mc, w}
(mc, meat).(w, red) John present Mary not, (mc, f ish).(w, white) otherwise.
Consider query introduced beginning Section 6.1:
(N , (min, {mc}).(u , {bpJ , bpM }).(max, {w}).(u , {epJ , epM })).
answer : worst main course case, even Peter chooses wine, situation unacceptable. order compute expected utility menu choice,
use query mc w free variables:
10. unfeasible value, cf. Definition 6.

456

fiThe PFU Framework

(N , (u , {bpJ , bpM , epJ , epM })).
answer function {mc, w}. examples show queries capture various
situations terms partial observabilities, optimistic/pessimistic attitude, parameters
decision process.
6.3 Operational Answer Query
quantities PS | l(S) (A.A0 ) FS | l(S) (A.A0 ) involved definition semantic answer query directly available local functions expensive
compute. instance,
probabilities, PS | l(S) (A.A0 ) equals PS,l(S) (A.A0 )/Pl(S) (A).
P
0 00
Computing PS,l(S) (A.A0 ) =
A00 dom(V (Sl(S))) PVE ,VD (A.A .A ) typically requires time
exponential |V (S l(S))|. Moreover, quantities must computed node
decision tree. Fortunately, exists alternative definition answer
query, directly expressed using PFU instance, i.e. expressed local
plausibility, feasibility, utility functions.
Definition 31. operational answer Op-Ans(Q) query Q = (N , Sov) function
free variables Q: assignment free variables, (Op-Ans(Q))(A)
defined inductively follows:
(Op-Ans(Q))(A) = Qo (N , Sov, A)
Qo(N , (op, S) . Sov, A) = opA0 dom(S) Qo (N , Sov, A.A0 )
!



!
Qo(N , , A) =
? p Pi pu u Ui
(A).
F

Pi P

(9)
(10)

Ui U

Equation 10, problem variables assigned, answer query
combination plausibility degree, feasibility degree, utility degree
corresponding complete assignment. Equation 9, variables assigned
(op, S) leftmost operator-variable(s) pair Sov, answer query obtained
eliminating using op elimination operator. Again, optimal decision rules
decision variables recorded needed, using argmin argmax. Equivalently,
considering sequence operator-variable(s) pairs sequence variable eliminations,
Op-Ans(Q) written:
!
!



Op-Ans(Q) = Sov
? p Pi pu u Ui
.
F

Pi P

Ui U

shows Op-Ans(Q) actually corresponds generic form Equation 8.
6.4 Equivalence Theorem
Theorem 2 proves semantic definition Sem-Ans(Q) gives semantic foundations
computed operational definition Op-Ans(Q).
Theorem 2. plausibility structure conditionable, then, queries Q PFU
network, Sem-Ans(Q) = Op-Ans(Q) optimal policies decisions
Sem-Ans(Q) Op-Ans(Q).
457

fiPralet, Verfaillie, & Schiex

words, Theorem 2 shows possible perform computations
completely generic algebraic framework, providing result computations
decision-theoretic foundations. Due equivalence theorem, Op-Ans(Q) denoted
simply Ans(Q) following. Note operational definition applies even
non-conditionable plausibility structure. Giving decision-theoretic-based semantics
Op-Ans plausibility structure conditionable open issue.
6.5 Bounded Queries
may interesting relax problem computing exact answer query.
Assume leftmost operator-variable(s) pair sequence Sov (max, {x}),
x decision variable. decision maker point view, computing decision rules
providing expected utility greater given threshold may sufficient.
case E-MAJSAT problem, defined Given boolean formula set
variables V = VD , exist assignment VD formula
satisfied least half assignments ? Extending generic PFU framework
answer queries done Definitions 32 33, introduce bounded queries.
Definition 32. bounded query B-Q triple (N , Sov, ), (N , Sov) query
Eu ( threshold).
Definition 33. answer Ans(B-Q) bounded query B-Q = (N , Sov, ) boolean
function free variables unbounded query Q = (N , Sov). every assignment
free variables,

Ans(Q)(A) u
(Ans(B-Q))(A) =
f otherwise.
threshold may used prune search space resolution, computing answer bounded query easier computing answer unbounded
one.
6.6 Back Existing Frameworks
Let us consider frameworks Section 3. Solving CSP (Equation 1) totally
ordered soft CSP corresponds query Q = (N , (max, V )), N PFU network
corresponding CSP V set variables CSP. Computing probability
distribution variable Bayesian network (Equation 2) modeled N corresponds
Q = (N , (+, V {y}). examples mono-operator queries, involving one
type elimination operator.
Consider multi-operator queries. search optimal policy stochastic
CSP associated Equation 4 captured query Q = (N , (max, {d1 , d2 })
.(+, {s1 }).(max, {d3 , d4 }).(+, {s2 })). query influence diagrams Equation 5
query valuation networks Equation 6 captured way.
finite horizon MDP time-steps (Equation 7), query looks Q =
(N , (max, {d1 }).(u , {s2 }).(max, {d2 }) . . . (u , {sT }).(max, {dT })), u = + probabilistic MDPs u = min pessimistic possibilistic MDPs. initial state s1

458

fiThe PFU Framework

free variable. quantified CSP quantified boolean formula, elimination operators
min max used represent .
formally, show:
Theorem 3. Queries bounded queries used express solve following
list problems:
1. SAT framework: SAT, MAJSAT, E-MAJSAT, quantified boolean formula, stochastic
SAT (SSAT) extended-SSAT (Littman et al., 2001).
2. CSP (or CN) framework:
Check consistency CSP (Mackworth, 1977); find solution CSP; count
number solutions CSP.
Find solution valued CSP (Bistarelli et al., 1999).
Solve quantified CSP (Bordeaux & Monfroy, 2002).
Find conditional decision unconditional decision mixed CSP
probabilistic mixed CSP (Fargier et al., 1996).
Find optimal policy stochastic CSP policy value greater
threshold; solve stochastic COP (Constraint Optimization Problem) (Walsh,
2002).
3. Integer Linear Programming (Schrijver, 1998) finite domain variables.
4. Search solution plan length k classical planning problem (STRIPS
planning, Fikes & Nilsson, 1971; Ghallab et al., 2004).
5. Answer classical queries Bayesian networks (Pearl, 1988), Markov random fields
(Chellappa & Jain, 1993), chain graphs (Frydenberg, 1990), plausibilities
expressed probabilities, possibilities, -rankings:
Compute plausibility distributions.
MAP (Maximum Posteriori hypothesis) MPE (Most Probable Explanation).
Compute plausibility evidence.
CPE task hybrid networks (Dechter & Larkin, 2001) (CPE means CNF Probability Evaluation, CNF formula Conjunctive Normal Form).
6. Solve influence diagram (Howard & Matheson, 1984).
7. finite horizon, solve probabilistic MDP, possibilistic MDP, MDP based
-rankings, completely partially observable (POMDP), factored (Puterman,
1994; Monahan, 1982; Sabbadin, 1999; Boutilier et al., 1999, 2000).

459

fiPralet, Verfaillie, & Schiex

6.7 Towards Complex Queries
Queries made complex relaxing assumptions:
definition queries, order u Eu assumed total. Extending
results partial order possible (Eu , u ) defines lattice (partially ordered
set closed least upper greatest lower bounds) pu distributes
least upper bound lub greatest lower bound glb (i.e. p pu lub(u1 , u2 ) =
lub(p pu u1 , p pu u2 ) p pu glb(u1 , u2 ) = glb(p pu u1 , p pu u2 )). allows
semiring CSPs (Bistarelli et al., 1999) captured framework. believe
extensions partial orders utilities allow algebraic MDPs (Perny
et al., 2005) captured.
try relax no-forgetting assumption, limited memory influence
diagrams (LIMIDs, Lauritzen & Nilsson, 2001), show relevant
decision processes involving multiple decision makers memory constraints
policy recording. cases, optimal decisions become nondeterministic
(decisions choose x = 0 probability p x = 1 probability 1p).
order decisions made environment variables observed
total completely determined query. One may wish compute
optimal policy decisions, optimal order perform
decisions, without exactly knowing steps agents make decisions
steps observations made. Work influence diagrams unordered
decisions (Jensen & Vomlelova, 2002) good starting point try extend
work direction.
possible relax assumption variables finite domain,
nontrivial, since transforming u = + integrals straightforward,
performing min- max-eliminations continuous domains requires guarantee
existence supremum.
6.8 Summary
Section 6, last element PFU framework, class queries PFU networks,
introduced. decision-tree based definition answer query provided.
first main result section Theorem 2, gives theoretical foundations
another equivalent operational definition, reducing answer query sequence
eliminations combination scoped functions. latter best adapted future
algorithms, directly handles local functions defined PFU network.
second important result Theorem 3, shows many standard queries PFU
queries. Overall, PFU framework captured Definitions 14, 16, 17 algebraic
structures, Definition 26 PFU networks, Definitions 27 31 queries.

7. Gains Costs
better understanding Theorem 3 shows many existing frameworks instances
PFU framework. unification, similarities differences ex460

fiThe PFU Framework

isting formalisms analyzed. instance, comparing VCSPs optimistic
version finite horizon possibilistic MDPs operational definition answer
query, appears finite horizon optimistic possibilistic MDP (partially observable
not) fuzzy CSP: indeed represented query Q whose operational
answer looks maxV (min ), V set variables set scoped
functions. Techniques available solving fuzzy CSPs used solve finite
horizon optimistic possibilistic MDPs.
complexity theory point view, studying time space complexity
answering queries form Equation 8 lead upper bounds complexity
several frameworks simultaneously. One may try characterize properties
lead given theoretical complexity.
Increased expressive power expressive power PFU networks result
number features: (1) flexibility plausibility/utility model; (2) flexibility
possible networks; (3) flexibility queries terms situation modeling. enables
queries PFU networks cover generic finite horizon sequential decision problems
plausibilities, feasibilities, utilities, cooperative adversarial decision makers, partial observabilities, possible parameters decision process modeled free
variables.
none frameworks indicated Theorem 3 presents flexibility, every
subsumed formalism X indicated Theorem 3, possible find problem
represented PFUs directly X. specifically, compared influence
diagrams (Howard & Matheson, 1984; Jensen & Vomlelova, 2002; Smith et al., 1993; Nielsen
& Jensen, 2003; Jensen et al., 2004) valuation networks (VNs, Shenoy, 1992, 2000;
Demirer & Shenoy, 2001), PFUs deal probabilistic expected additive
utility allow us perform eliminations min model presence adversarial
agents. Thus, quantified boolean formulas cannot represented influence diagrams
VNs, covered PFU networks (see Theorem 3). Moreover, PFU networks use
DAG captures normalization conditions plausibilities feasibilities, whereas
VNs, information lost. Compared sequential influence diagrams (Jensen
et al., 2004) sequential VNs (Demirer & Shenoy, 2001), PFUs express so-called
asymmetric decision problems (problems variables may even need
considered decision process) adding dummy values variables.
Actually, simple problems expressed PFUs cannot apparently
directly expressed frameworks. simple instance feasibilities normalization
conditions + hard requirements captured subsumed frameworks.
example, using CSP model would result loss information provided
normalization conditions feasibilities. occurs influence diagrams sequential decision processes based possibilistic expected utility, could
called possibilistic influence diagrams. Similarly stochastic CSPs without contingency
assumption.
cost greater flexibility increased expressive power PFU framework
cannot described simply straightforwardly as, example, constraint networks.
Generic algorithms Section 8 shows generic algorithms built answer
queries PFU networks. previously said, building generic algorithms facilitate
461

fiPralet, Verfaillie, & Schiex

cross-fertilization sense subsumed formalisms directly benefit
techniques developed another subsumed formalism. fits growing
effort generalize resolution methods used different AI problems. example, soft
constraint propagation drastically improves resolution valued CSPs; integrating
tool generic algorithm PFUs could improve resolution influence diagrams.
Using abstract operators may enable us identify algorithmically interesting properties,
infer necessary sufficient conditions particular algorithm usable.
However, one could argue techniques highly specific one formalism
one type problem, that, case, dedicated approaches certainly outperform
generic algorithm. solution characterize actual properties used
dedicated approach, order generalize much possible. Moreover, even
specialized schemes usually improve generic ones, exist cases general
tools efficient specialized algorithms, shown use SAT solvers
solving STRIPS planning problems (Sang, Beame, & Kautz, 2005).

8. Algorithms
ability design generic algorithms one motivations building PFU
framework, choices justified algorithmic considerations. present generic
algorithms answer arbitrary PFU queries.
8.1 Generic Tree Search Algorithm
operational definition answer query Q actually defines naive exponential
time algorithm compute Ans(Q) using tree-exploration procedure, variable
ordering given Sov, collects elementary plausibilities, feasibilities, utilities.
precisely, assignment free variables Q, tree explored.
node tree corresponds partial assignment variables. value leaf
provided combination scoped functions PFU network, applied
complete assignment defined path root leaf. Depending
operator used, value internal node computed performing min, max,
u operation values children. root node returns (Ans(Q))(A).
corresponding pseudo-code given Figure 2. query (N , Sov), first call
TreeSearchAnswerQ(N , Sov). returns function free variables.
assume every operator returns result constant time, time
complexity algorithm O(m n ln(d) dn ), stands maximum domain
size, n stands number variables PFU network, stands number
scoped functions.11
space complexity polynomial (it shown linear entry data
size). Hence, computing answer bounded query PSPACE. Moreover, given
satisfiability QBF PSPACE-complete problem expressed
bounded query (cf. Theorem 3), follows computing answer bounded query
PSPACE-hard. PSPACE PSPACE-hard, decision problem consists
11. factor n ln(d) corresponds upper bound time needed get (A) scoped function
represented table (of size dn ).

462

fiThe PFU Framework

TreeSearchAnswerQ((V, G, P, F, U ), Sov)
begin
foreach dom(Vf r ) (A) AnswerQ((V, G, P, F, U ), Sov, A)
return
end
AnswerQ((V, G, P, F, U ), Sov, A)
begin
Sov = return ((Fi F ) ? (p Pi P Pi ) pu (uUi U Ui ))(A)
else
(op, S).Sov 0 Sov
choose x
= {x} Sov Sov 0 else Sov (op, {x}).Sov 0
dom dom(x)
res
dom 6=
choose dom
dom dom {a}
res op (res, AnswerQ((V, G, P, F, U ), Sov, A.(x, a)))
return res
end

Figure 2:
generic tree search algorithm answering query Q
((V, G, P, F, U ), Sov)

=

answering bounded query PSPACE-complete. result surprising, gives
idea level expressiveness reached PFU framework.
work needed identify subclasses queries lower complexity, although many
already known.
8.2 Generic Variable Elimination Algorithm
Quite naturally, generic variable elimination algorithm (Bertele & Brioschi, 1972; Shenoy,
1991; Dechter, 1999; Kolhas, 2003) defined answer queries PFU network.
8.2.1 First Naive Scheme
first naive variable elimination algorithm given Figure 3. eliminates variables
right left sequence Sov query, whereas tree search
procedure, variables assigned left right. right-to-left processing
entails algorithm naturally returns function free variables query.
first call VarElimAnswerQ((V, G, P, F, U ), Sov).
version presented Figure 3 actually naive variable elimination scheme
time space complexities O(m n ln(d) dn ) O(m dn ) respectively: begins
combining scoped functions eliminating variables, whereas interest
variable elimination algorithm primarily use factorization local functions.

463

fiPralet, Verfaillie, & Schiex

VarElimAnswerQ((V, G, P, F, U ), Sov)
begin
0 ((Fi F ) ? (p Pi P Pi ) pu (uUi U Ui ))
Sov 6=
Sov 0 .(op, S) Sov
choose x
= {x} Sov Sov 0 else Sov Sov 0 .(op, {x})
0 opx 0
return 0
end

Figure 3: first generic variable elimination algorithm answering query Q =
((V, G, P, F, U ), Sov)
8.2.2 Improving Basic Scheme
algorithm Figure 3 works unique global function defined combination
plausibility, feasibility, utility functions (first line), whereas factorization
available. improve scheme, properties algebraic structure used.
sequel, denote +x (resp. x ) scoped function (resp.
have) x scope. Moreover, extend every combination operator E {}
setting e = e = (combining anything something unfeasible unfeasible
too).12
First, order use factorization plausibilities feasibilities, use
properties below, come right monotonicity pu , distributivity pu
u , definition
truncation operator ?:
x
min
(P
pu U ) = P x pu (minx U )
x




maxx (P x pu U ) = P x pu (maxx U )



ux (P x pu U ) = P x pu (ux U )
minx (F x ? U ) = F x ? (minx U )





maxx (F x ? U ) = F x ? (maxx U )


ux (F x ? U ) = F x ? (ux U ) .
express variable x eliminated, necessary consider plausibility
functions feasibility functions x scope.
However, necessary add axioms expected utility structure, since
general case, expression ux (P +x pu (U x u U +x )) cannot decomposed. give two axioms, Ax1 Ax2, sufficient additional condition
exploit factorization utility functions.
(Ax1) (Ep , p ) = (Eu , u ), p = u , p = u = pu
(Ax2) u = u Eu (but Eu {}).
12. operator op used combination operator scoped functions elimination operator variables. case, extension op used combination operator creates
operator op0 op0 (e, ) = , whereas extension op used elimination operator
creates operator op00 op00 (e, ) = e. op0 op00 coincide E differ E {}.

464

fiThe PFU Framework

Among cases Table 1, rows 2, 3, 5, 6 satisfy Ax1, whereas rows 1, 4, 7, 8 satisfy
Ax2. Ax1 andAx2 enable us write:
minx (F +x ? (U x u U +x )) = U x u (minx F +x ? U +x )
maxx (F +x ? (U x u U +x )) = U x u (maxx F +x ? U +x ) .

x

U u (ux P +x pu U +x ) Ax1
+x
x
+x
u P pu (U u U ) =
((p x P +x ) pu U x ) u (ux P +x pu U +x ) Ax2.
x
Hence, eliminating variable x, necessary consider utility functions
x scope.
present algorithm Ax1 satisfied. Ax2 holds, working plausibility/utility pairs (p, u) allows Ax1 recovered: used, example, solve
influence diagrams (Ndilikilikesha, 1994). Ax1 satisfied, actually one
set E = Ep = Eu , one order =p =u , one combination operator = p = u = pu ,
one elimination operator = p = u . Rather express feasibilities {t, f },
express {1E , } mapping onto 1E f onto : preserves value
answer query, since f ? u = u ? u = 1E u.
improved variable elimination algorithm shown Figure 4. answer query
Q = ((V, G, P, F, U ), Sov), first call Ax1-VarElimAnswerQ(P F U, Sov).
returns set scoped functions whose -combination equals Ans(Q). time,
factorization available PFU network exploited, since eliminating variable x,
scoped functions involving x considered.
Ax1-VarElimAnswerQ(, Sov)
begin
Sov = return
else
Sov 0 .(op, S) Sov
choose x
= {x} Sov Sov 0 else Sov Sov 0 .(op, {x})
+x { | x sc()}
0 opx +x
( +x ) {0 }
return Ax1-VarElimAnswerQ(, Sov)
end

Figure 4: Variable elimination algorithm Ax1 holds (: set scoped functions)
Ax1 holds, algorithm actually standard variable elimination algorithm
commutative semiring. classical variable elimination algorithms, time complexity
algorithm O(m n ln(d) dw+1 ), w tree-width (Bodlaender, 1997;
Dechter & Fattah, 2001) network scoped functions, constrained elimination
order imposed Sov. Yet, space complexity exponential tree-width.
8.3 Approaches
Starting generic tree-search algorithm Section 8.1, bound computations local
consistencies (Mackworth, 1977; Cooper & Schiex, 2004; Larrosa & Schiex., 2003)

465

fiPralet, Verfaillie, & Schiex

integrated order prune search space. Local consistencies improve quality
bounds thanks use smaller local functions. Techniques coming quantified
boolean formulas game algorithms (such -algorithm) considered
efficiently manage bounds min max operators alternate. Caching strategies
exploiting problem structure (Darwiche, 2001; Jegou & Terrioux, 2003) obvious
candidates improve basic tree search scheme. Additional axioms Ax1 Ax2
useful direction. Heuristics choice variable assign pair
(op, S) encountered, well heuristics value choices, may speed
search.
another direction, approximate algorithms using sampling local search could
considered: sampling eliminations + (+, u ) performed, local
search eliminations min max performed.

9. Conclusion
last decades, AI witnessed design study numerous formalisms
reasoning decision problems. article, built generic framework
model sequential decision making plausibilities, feasibilities, utilities.
framework covers many existing approaches, including hard, valued, quantified, mixed,
stochastic CSPs, Bayesian networks, finite horizon probabilistic possibilistic MDPs,
influence diagrams. result algebraic framework built upon decision-theoretic
foundations: PFU framework. two facets PFU framework explicit
Theorem 2, states operational definition answer query equivalent
decision tree-based semantics. result design accounts
expressiveness computational aspects.
Compared related works (Shenoy, 1991; Dechter, 1999; Kolhas, 2003), PFU framework framework directly deals different types variables (decision
environment variables), different types local functions (plausibilities, feasibilities,
utilities), different types combination elimination operators.
algorithmic point view, generic algorithms based tree search variable
elimination described. prove PFU framework abstraction. next step explore ways improving algorithms, generalize
techniques used formalisms subsumed PFU framework. Along line,
generic approach query optimization lead definition original architectures
answering queries, called multi-operator cluster trees multi-operator cluster DAGs.
applied QBFs structures compatible Ax1 (Pralet, Schiex, &
Verfaillie, 2006a), well influence diagrams structures satisfying Ax2 (Pralet,
Schiex, & Verfaillie, 2006b).

Acknowledgments
would thank Jean-Loup Farges, Jerome Lang, Regis Sabbadin, three
anonymous reviewers useful comments previous versions article. work

466

fiThe PFU Framework

described article initiated first author LAAS-CNRS INRA
Toulouse. partially conducted within EU Integrated Project COGNIRON (The
Cognitive Companion) funded European Commission Division FP6-IST Future
Emerging Technologies Contract FP6-002020.

Appendix A. Notations
See Table 2.
Symbol

p
u

p
u
pu
p
u
?



VD
dom(x)
dom(S)
G
paG (x)
ndG (x)
CE (G)
CD (G)
Pi

Ui
F act(c)
sc()
PS
PS1 | S2
FS
FS1 | S2

eaning
Elimination operator
Elimination operator plausibilities
Elimination operator utilities
Combination operator
Combination operator plausibilities
Combination operator utilities
Combination operator plausibilities utilities
Partial order plausibilities
Partial order utilities
Truncation operator
Unfeasible value

Environment variables
Decision variables
Domain
values variable x
Q
dom(x)
xS
Directed Acyclic Graph (DAG)
Parents x DAG G
Non-descendant x DAG G
Set environment components G
Set decision components G
Plausibility function
Feasibility function
Utility function
Pi factors associated component c
Scope local function
Plausibility distribution
Conditional plausibility distribution S1 given S2
Feasibility distribution
Conditional feasibility distribution S1 given S2

Sov
Sequence operator-variable(s) pairs
Sem-Ans(Q) Semantic answer query Q (decision trees)
Op-Ans(Q) Operational answer query Q
Ans(Q)
Answer query Q

Table 2: Notation.
467

fiPralet, Verfaillie, & Schiex

Appendix B. Proofs
Proposition 1 plausibility distribution PS extended give plausibility distribution PS 0 every 0 S, defined PS 0 = p SS 0 PS .
Proof. Given p associative commutative, p 0 PS 0 = p 0 (p SS 0 PS ) = p PS = 1p .
Thus, PS 0 : dom(S 0 ) Ep plausibility distribution 0 .

Proposition 2 structures presented Table 1 expected utility structures.
Proof. sufficient verify required axioms successively.

Proposition 3 (Ep , p , p ) conditionable plausibility structure, plausibility distributions conditionable: suffices define PS1 | S2 PS1 | S2 (A) = max{p
Ep | PS1 ,S2 (A) = p p PS2 (A)} dom(S1 S2 ) satisfying PS2 (A) 6= 0p .
Proof. Let PS plausibility distribution S. S1 , S2 disjoint subsets
dom(S1 S2 ) satisfying PS2 (A) 6= 0p , let us define PS1 | S2 (A) = max{p Ep | PS1 ,S2 (A) =
p p PS2 (A)}. must show PS1 | S2 functions satisfy axioms a, b, c, d, e Definition 18.
(a) definition PS1 | S2 distributivity p p , write
PS2 = p S1 PS1 ,S2 = p S1 (PS1 | S2 p PS2 ) = (p S1 PS1 | S2 ) p PS2 .
PS2 p PS2 , infer p S1 PS1 | S2 p 1p . Let A2 assignment S2 satisfying
PS2 (A2 ) 6= 0p . Assume hypothesis (H): p S1 PS1 | S2 (A2 ) p 1p holds.
Then, A1 dom(S1 ), PS1 ,S2 (A1 .A2 ) p PS2 (A2 ), since PS1 ,S2 (A1 .A2 ) = PS2 (A2 ),
PS1 | S2 (A1 .A2 ) = 1p , implies p S1 PS1 | S2 (A2 ) p 1p monotonicity
p . Moreover, (H) implies exists unique p Ep satisfying (p S1 PS1 | S2 (A2 )) p
p = 1p . Combining equation PS2 (A2 ) gives PS2 (A2 ) p PS2 (A2 ) p p = PS2 (A2 ), i.e.
PS2 (A2 ) p (1p p p) = PS2 (A2 ). implies 1p p p p 1p . Given 1p p p p 1p
(by monotonicity p ), obtain 1p p p = 1p . analyze two cases.
p p 1p , exists unique p0 satisfying p0 p p = 1p . (p S1 PS1 | S2 (A2 )) p p
= 1p 1p p p = 1p , entails p S1 PS1 | S2 (A2 ) = 1p , contradicts (H).
p = 1p , 1p p 1p = 1p . entails p idempotent. Let dom0 subset
dom(S1 ) p A1 dom0 PS1 ,S2 (A1 .A2 ) = PS2 (A2 ). Let A01 dom0 .
write:
(
PS1 ,S2 (A01 .A2 ) p (p A1 dom0 {A0 } PS1 ,S2 (A1 .A2 )) = PS2 (A2 )
1
.
PS1 ,S2 (A01 .A2 ) p (p A1 dom0 PS1 ,S2 (A1 .A2 )) = PS2 (A2 ) (as p idempotent)
PS1 ,S2 (A01 .A2 ) p PS2 (A2 ), exists unique p00 Ep PS1 ,S2 (A01 .A2 )p
p00 = PS2 (A2 ). Therefore, p A1 dom0 PS1 ,S2 (A1 .A2 ) = p A1 dom0 {A0 } PS1 ,S2 (A1 .A2 ),
1
gives p A1 dom0 {A0 } PS1 ,S2 (A1 .A2 ) = PS2 (A2 ).
1

assumption p A1 dom0 PS1 ,S2 (A1 .A2 ) = PS2 (A2 ) holds dom0 = dom(S1 ). Recursively applying previous mechanism removing one assignment dom0
iteration leads p A1 dom0 PS1 ,S2 (A1 .A2 ) = PS2 (A2 ) |dom0 | = 1, i.e. leads
PS1 ,S2 (A001 .A2 ) = PS2 (A2 ) dom0 = {A001 }. result, obtain contradiction.
cases, contradiction (H) obtained, p S1 PS1 | S2 (A2 ) = 1p .
(b) PS1 = PS1 | p P = PS1 | p (p PS ) = PS1 | p 1p = PS1 | .
468

fiThe PFU Framework

(d) Let dom(S1 S2 S3 ) satisfying PS2 ,S3 (A) 6= 0p . Then, PS1 ,S2 | S3 (A) = PS1 | S2 ,S3 (A) p
PS2 | S3 (A) holds, because:
PS1 ,S2 ,S3 (A) p PS3 (A), then, exists unique p Ep PS1 ,S2 ,S3 (A) =
pp PS3 (A). PS1 ,S2 ,S3 (A) = PS1 ,S2 | S3 (A)p PS3 (A) (by definition PS1 ,S2 | S3 )
PS1 ,S2 ,S3 (A) = PS1 | S2 ,S3 (A)p PS2 | S3 (A)p PS3 (A) (by definition PS1 | S2 ,S3
PS2 | S3 ), implies PS1 ,S2 | S3 (A) = PS1 | S2 ,S3 (A) p PS2 | S3 (A).
Otherwise, PS1 ,S2 ,S3 (A) = PS3 (A). implies 1p p PS1 ,S2 | S3 (A) and,
PS1 ,S2 | S3 (A) p 1p , PS1 ,S2 | S3 (A) = 1p . Similarly, entails PS2 | S3 (A) = 1p
PS1 | S2 ,S3 (A) = 1p (the monotonicity p implies PS1 ,S2 ,S3 (A) = PS2 ,S3 (A) =
PS3 (A)). 1p = 1p p 1p , get PS1 ,S2 | S3 (A) = PS1 | S2 ,S3 (A) p PS2 | S3 (A).
(c)

p S1 PS1 ,S2 | S3

= p S1 (PS1 | S2 ,S3 p PS2 | S3 ) (using (d))
= (p S1 PS1 | S2 ,S3 ) p PS2 | S3 (because p distributes p )
= PS2 | S3 (using (a))

(e) Assume PS1 ,S2 ,S3 = PS1 | S3 p PS2 | S3 p PS3 . Let dom(S1 S2 S3 )
PS3 (A) 6= 0p . Then, PS1 ,S2 | S3 (A) = PS1 | S3 (A) p PS2 | S3 (A) holds, because:
PS1 ,S2 ,S3 (A) p PS3 (A), exists unique p Ep PS1 ,S2 ,S3 (A) =
p p PS3 (A), therefore PS1 ,S2 | S3 (A) = PS1 | S3 (A) p PS2 | S3 (A).
Otherwise, write PS1 | S3 (A) = PS2 | S3 (A) = PS1 ,S2 | S3 (A) = 1p using reasoning similar (d), therefore PS1 ,S2 | S3 (A) = PS1 | S3 (A) p PS2 | S3 (A).

Proposition 4 I(., . | .) satisfies semigraphoid axioms:
1. symmetry: I(S1 , S2 | S3 ) I(S2 , S1 | S3 ),
2. decomposition: I(S1 , S2 S3 | S4 ) I(S1 , S2 | S4 ),
3. weak union: I(S1 , S2 S3 | S4 ) I(S1 , S2 | S3 S4 ),
4. contraction: (I(S1 , S2 | S4 ) I(S1 , S3 | S2 S4 )) I(S1 , S2 S3 | S4 ).
Proof.
1. Symmetry axiom: directly satisfied commutativity p .
2. Decomposition axiom: assume I(S1 , S2 S3 | S4 ) holds.
PS1 ,S2 | S4 = p S3 PS1 ,S2 ,S3 | S4
= p S3 (PS1 | S4 p PS2 ,S3 | S4 ) (since I(S1 , S2 S3 | S4 ))
= PS1 | S4 p (p S3 PS2 ,S3 | S4 ) (by distributivity p p )
= PS1 | S4 p PS2 | S4 .
Thus, I(S1 , S2 | S4 ) holds.
3. Weak union axiom: assume I(S1 , S2 S3 | S4 ) holds. decomposition axiom entails
I(S1 , S3 | S4 ) satisfied.
PS1 ,S2 ,S3 ,S4 = PS1 ,S2 ,S3 | S4 p PS4 (chain rule)
= PS1 | S4 p PS2 ,S3 | S4 p PS4 (since I(S1 , S2 S3 | S4 ))
= PS1 | S4 p PS3 | S4 p PS4 p PS2 | S3 ,S4 (chain rule)
= PS1 ,S3 | S4 p PS4 p PS2 | S3 ,S4 (since I(S1 , S3 | S4 ))
= PS1 | S3 ,S4 p PS2 | S3 ,S4 p PS3 ,S4 (chain rule).
axiom (e) Definition 18, infer PS1 ,S2 | S3 ,S4 = PS1 | S3 ,S4 p PS2 | S3 ,S4 , i.e.
I(S1 , S2 | S3 S4 ) holds.
469

fiPralet, Verfaillie, & Schiex

4. Contraction axiom: assume I(S1 , S2 | S4 ) I(S1 , S3 | S2 S4 ) hold.
PS1 ,S2 ,S3 | S4 = PS1 ,S3 | S2 ,S4 p PS2 | S4 (chain rule)
= PS1 | S2 ,S4 p PS3 | S2 ,S4 p PS2 | S4 (since I(S1 , S3 | S2 S4 ))
= PS1 ,S2 | S4 p PS3 | S2 ,S4 (chain rule)
= PS1 | S4 p PS2 | S4 p PS3 | S2 ,S4 (since I(S1 , S2 | S4 ))
= PS1 | S4 p PS2 ,S3 | S4 (chain rule).
Thus, I(S1 , S2 S3 | S4 ) holds.

Theorem 1 (Conditional independence factorization) Let (Ep , p , p ) conditionable plausibility structure let G DAG components S.
(a) G compatible plausibility distribution PS S, PS = p cC(G) Pc | paG (c) .
(b) If, c C(G), function c,paG (c) c,paG (c) (A) plausibility
distribution c assignments paG (c), = p cC(G) c,paG (c)
plausibility distribution G compatible.
Proof.
(a) First, |C(G)| = 1, G contains unique component c1 . Then, p cC(G) Pc | paG (c) = Pc1 | =
Pc1 : proposition holds |C(G)| = 1.
Assume proposition holds DAGs n components. Let G DAG
components compatible plausibility distribution PS |C(G)| = n + 1.
Let c0 component labeling leaf G. G compatible PS , write
I(c0 , ndG (c0 ) paG (c0 ) | paG (c0 )). c0 leaf, ndG (c0 ) = c0 , consequently
I(c0 , (S c0 ) paG (c0 ) | paG (c0 )). means PSpaG (c0 ) | paG (c0 ) = Pc0 | paG (c0 ) p
P(Sc0 )paG (c0 ) | paG (c0 ) . Combining side equation PpaG (c0 ) gives
PS = Pc0 | paG (c0 ) p PSc0 .
0
Let G DAG obtained G deleting node labeled c0 . every
component c C(G0 ), paG0 (c) = paG (c) (since deleted component c0 leaf). Moreover
ndG0 (c) equals either ndG (c) ndG (c)c0 (again, since deleted component c0 leaf).
first case (ndG0 (c) = ndG (c)), property I(c, ndG (c) paG (c) | paG (c)) directly implies
I(c, ndG0 (c)paG0 (c) | paG0 (c)). second case (ndG0 (c) = ndG (c)c0 ), decomposition
axiom allows us write I(c, ndG0 (c) paG0 (c) | paG0 (c)) I(c, ndG (c) paG (c) | paG (c)).
Consequently, G0 DAG compatible PSc0 . |C(G0 )| = n, induction hypothesis
gives PSc0 = p cC(G0 ) Pc | paG (c) , implies PS = p cC(G) Pc | paG (c) , desired.
(b) Assume every component c, c,paG (c) (A) plausibility distribution c
assignments paG (c). |C(G)| = 1, C(G) = {c1 }. Then, = c1 plausibility
distribution c1 . Moreover, | = 1p , write c1 | = c1 | p | , i.e.
I(c1 , | ). Therefore, G compatible c1 : proposition holds |C(G)| = 1.
Assume proposition holds DAGs n components. Consider DAG G
n + 1 components. first show plausibility distribution S, i.e.
p (p cC(G) c,paG (c) ) = 1p . Let c0 leaf component G. c0 leaf, unique
scoped function whose scope contains variable c0 c0 ,paG (c0 ) . distributivity
p p , implies
p c0 (p cC(G) c,paG (c) ) = (p c0 c0 ,paG (c0 ) ) p (p cC(G){c0 } c,paG (c) ).
Given c0 ,paG (c0 ) (A) plausibility distribution c0 assignments paG (c0 ),
470

fiThe PFU Framework

p c0 c0 ,paG (c0 ) = 1p . Consequently,
p c0 (p cC(G) c,paG (c) ) = p cC(G){c0 } c,paG (c) .
Applying induction hypothesis DAG n components obtained G deleting c0 , infer p Sc0 (p cC(G){c0 } c,paG (c) ) = 1p . allows us write
p Sc0 (p c0 (p cC(G) c,paG (c) )) = 1p , i.e. p = 1p : plausibility distribution
S. remains prove G DAG components compatible . Let c C(G).
must show I(c, ndG (c) paG (c) | paG (c)) holds. two cases:
1. c = c0 , must prove
c0 ,ndG (c0 )paG (c0 ) | paG (c0 ) = c0 | paG (c0 ) p ndG (c0 )paG (c0 ) | paG (c0 ) .
First, note
c0 ,paG (c0 ) = p S(c0 paG (c0 )) (p cC(G) c,paG (c) )
= (p S(c0 paG (c0 )) (p cC(G){c0 } c,paG (c) )) p c0 ,paG (c0 )
(because p distributes p sc(c0 ,paG (c0 ) ) c0 paG (c0 )
= (p SpaG (c0 ) (p cC(G) c,paG (c) )) p c0 ,paG (c0 )
(because p distributes p c0 c0 ,paG (c0 ) = 1p )
= paG (c0 ) p c0 ,paG (c0 ) .
this, possible write:
ndG (c0 )paG (c0 ) | paG (c0 ) p c0 | paG (c0 ) p paG (c0 )
= ndG (c0 )paG (c0 ) | paG (c0 ) p c0 ,paG (c0 )
= ndG (c0 )paG (c0 ) | paG (c0 ) p paG (c0 ) p c0 ,paG (c0 )
= ndG (c0 ) p c0 ,paG (c0 )
= S{c0 } p c0 ,paG (c0 ) (because c0 leaf G)
=

(p cC(G){c0 } c,paG (c) ) p c0 ,paG (c0 )

= p cC(G) c,paG (c)
= .
Using axiom (e) Definition 18, entails ndG (c0 )paG (c0 ) | paG (c0 ) p c0 | paG (c0 ) =
SpaG (c0 ) | paG (c0 ) , i.e., = c0 ndG (c0 ), I(c0 , ndG (c0 ) paG (c0 ) | paG (c0 )).
2. Otherwise, c 6= c0 . Let G0 DAG obtained G deleting c0 . G0 contains
n components: induction hypothesis, I(c, ndG0 (c) paG0 (c) | paG0 (c)). c0
leaf G, c0
/ paG (c), implies paG0 (c) = paG (c). Thus,
I(c, ndG0 (c) paG (c) | paG (c)).
(i) ndG0 (c) = ndG (c), immediate I(c, ndG (c) paG (c) | paG (c)).
(ii) Otherwise, ndG0 (c) 6= ndG (c). c0 leaf G, equivalent say
ndG (c) = ndG0 (c) c0 . means c ancestor c0 , fortiori c
/ paG (c0 ). following, four semigraphoid axioms used
prove required result. decomposition axiom, I(c0 , ndG (c0 )
paG (c0 ) | paG (c0 )), (c ndG0 (c)) ndG (c0 ) (because ndG (c0 ) =
c0 ), follows I(c0 , (c ndG0 (c)) paG (c0 ) | paG (c0 )), or, words,
c paG (c0 ) = , I(c0 , c (ndG0 (c) paG (c0 )) | paG (c0 )). Using weak
union axiom leads I(c0 , c | (ndG0 (c) paG (c0 )) paG (c0 )) and, using symmetry axiom, I(c, c0 | (ndG0 (c) paG (c0 )) paG (c0 )). shown previously,
I(c, ndG0 (c)paG (c) | paG (c)). Together I(c, c0 | (ndG0 (c)paG (c0 ))paG (c0 )),
contraction axiom implies I(c, (ndG0 (c) paG (c)) c0 | paG (c)). c0
/
paG (c) ndG (c) = ndG0 (c) c0 , means I(c, ndG (c) paG (c)) | paG (c)).
471

fiPralet, Verfaillie, & Schiex

proved G compatible . Consequently, proposition holds
n + 1 components G, ends proof induction.

Proposition 5 Let (Ep , p , p ) conditionable plausibility structure. Then,
n N , exists unique p0 p i[1,n] p0 = 1p .
Proof. Let n N . p i[1,n] 1p = 1p , p0 = 1p satisfies required property. Moreover,
case, distributivity p p implies p Ep , p i[1,n] p = p. Therefore,
p i[1,n] p = 1p , p = 1p , shows p0 unique.
Otherwise, p i[1,n] 1p 6= 1p . case, 1p p p i[1,n] 1p monotonicity p ,
write 1p p p i[1,n] 1p . second item Definition 19 implies exists unique
p0 Ep 1p = p0 p (p i[1,n] 1p ), i.e. 1p = p i[1,n] p0 .

Proposition 6
Let PVE ,VD completion controlled plausibility distribution
PVE || VD . Then, PVE ,VD plausibility distribution VD PVE | VD = PVE || VD .
Proof. PVE ,VD = PVE || VD p p0 , p0 element Ep p i[1,|dom(VD )|] p0 = 1p .
p VD PVE ,VD = p VD (PVE || VD p p0 ) = p VD ((p PVE || VD ) p p0 ) = p VD p0 =
p i[1,|dom(VD )|] p0 = 1p . proves PVE ,VD plausibility distribution VD .
PVE ,VD = PVE || VD p p0 PVE ,VD = PVE | VD p PVD , write PVE || VD p p0 =
PVE | VD p PVD . Moreover, PVD = p PVE ,VD = p (PVE || VD p p0 ) = p0 . Thus, PVE || VD p
p0 = PVE | VD p p0 . Summing equation |dom(VD )| times p gives PVE || VD = PVE | VD .

Proposition 7 Let G typed DAG components VD . Let Gp partial
graph G induced arcs G incident environment components. Let Gf
partial graph G induced arcs G incident decision components. Gp
compatible completion PVE || VD (cf. Definition 22) Gf compatible
completion FVD || ,
PVE | VD =

p Pc | paG (c)
cCE (G)

FVD | =


cCD (G)

Fc | paG (c) .

Proof. result proved PVE | VD (the proof FVD | similar). completion
PVE || VD looks PVE ,VD = PVE || VD p p0 . Gp compatible completion, Theorem 1a
entails PVE ,VD = p cC(Gp ) Pc | paGp (c) . decision components roots Gp , infer,
successively eliminating environment components, PVD = p PVE ,VD = p cCD (Gp ) Pc .

hand, PVD = p PVE || VD p p0 = p0 . proves p cCD (Gp ) Pc =
p0 . Therefore, PVE ,VD = PVE | VD p p0 = (p cCE (Gp ) Pc | paGp (c) ) p p0 . Summing equation
|dom(VD )| times p gives PVE | VD = p cCE (Gp ) Pc | paGp (c) . CE (Gp ) = CE (G) paGp (c) =
paG (c) every c CE (G), entails PVE | VD = p cCE (G) Pc | paG (c) .

Proposition 8 Assume plausibility structure used conditionable. Let Q =
(N , Sov) query Sov = (op1 , S1 ) (op2 , S2 ) (opk , Sk ). Let Vf r denote set
free variables Q.
472

fiThe PFU Framework

(1) Si PSi | l(Si ) (A) well-defined, exists least one A0 dom(Si )
satisfying PSi | l(Si ) (A.A0 ) 6= 0p .
(2) Si VD FSi | l(Si ) (A) well-defined, exists least one A0 dom(Si )
satisfying FSi | l(Si ) (A.A0 ) = t.
(3) 6= Si leftmost set environment variables appearing Sov, then,
dom(l(Si )), PSi | l(Si ) (A) well-defined.
(4) i, j [1, k], < j, Si , Sj , r(Si )l(Sj ) VD (Sj first set environment variables appearing right Si Sov), (A, A0 ) dom(l(Si )) dom(Si ),
PSi | l(Si ) (A) well-defined, PSi | l(Si ) (A.A0 ) 6= 0p , then, A00 extending A.A0
l(Sj ), PSj | l(Sj ) (A00 ) well-defined.
(5) i, j [1, k], < j, Si VD , Sj VD , r(Si ) l(Sj ) (Sj first set
decision variables appearing right Si Sov), (A, A0 ) dom(l(Si ))dom(Si ),
FSi | l(Si ) (A) well-defined, FSi | l(Si ) (A.A0 ) = t, then, A00 extending A.A0
l(Sj ), FSj | l(Sj ) (A00 ) well-defined.
(6) [1, k] Si , PSi | l(Si ) = PSi | l(Si )VE ||VD .
(7) [1, k] Si VD , FSi | l(Si ) = FSi | l(Si )VD ||VE .
Proof. denote p0 element Ep completion PVE || VD equals PVE || VD p0 .
Note p0 6= 0p , since must satisfy p i[1,|dom(VD )|] p0 = 1p .
Lemma 1. Let (Ep , p , p ) conditionable plausibility structure. Then, (p1 p p2 = 0p )
((p1 = 0p ) (p2 = 0p )).
Proof. First, p1 = 0p p2 = 0p , p1 p p2 = 0p . Conversely, assume p1 p p2 = 0p . Then,
p1 p 0p , conditionability plausibility structure together p1 p 0p = 0p entails
p2 = 0p . Similarly, p2 p 0p , p1 = 0p . Therefore (p1 p p2 = 0p ) ((p1 = 0p )(p2 = 0p )).
Lemma 2. Assume plausibility structure conditionable. Let S1 , S2 disjoint subsets
. Then, PS1 | S2 || VD = PS1 | S2 ,VD .
Proof. Note PS1 ,S2 | VD = PS1 | S2 ,VD p PS2 | VD . Moreover, write PS1 ,S2 | VD =
PS1 ,S2 || VD = PS1 | S2 || VD p PS2 || VD = PS1 | S2 || VD p PS2 | VD . Let assignment V .
PS1 ,S2 | VD (A) p PS2 | VD (A), conditionability plausibility structure entails
PS1 | S2 ,VD (A) = PS1 | S2 || VD (A). Otherwise, PS1 ,S2 | VD (A) = PS2 | VD (A), entails
PS1 ,S2 || VD (A) = PS2 || VD (A). case, PS1 | S2 ,VD (A) = PS1 | S2 || VD (A) = 1p . Therefore,
PS1 | S2 ,VD = PS1 | S2 || VD .
(1) Assume Si PSi | l(Si ) (A) well-defined. Then, PSi | l(Si ) (A) plausibility
distribution Si . Hence, p A0 dom(Si ) PSi | l(Si ) (A.A0 ) = 1p , implies exists
least one A0 dom(Si ) PSi | l(Si ) (A.A0 ) 6= 0p .
(2) Proof similar point (2).
(3) Assume 6= . Let Si leftmost set environment variables appearing Sov
let dom(l(Si )). Since l(Si ) = , write Pl(Si ) (A) = p V l(Si ) PVE ,VD (A) =
p VD l(Si ) (p PVE ,VD (A)) = p VD l(Si ) p0 6= 0p . Therefore, PSi | l(Si ) (A) well-defined.

473

fiPralet, Verfaillie, & Schiex

(6) Let lE (Si ) = l(Si ) lD (Si ) = l(Si ) VD . set variables S, denote dG (S)
set variables V descendant DAG G least one variable S.
First, PSi ,lE (Si ) || VD = p (Si lE (Si )) PVE || VD = p (Si lE (Si )) (p Pj P Pj ). definition query, variables dG (VD lD (Si )) belong Si lE (Si ) (the environment
variables descendants as-yet-unassigned decision variables assigned yet).
Thus, PSi ,lE (Si ) || VD = p (Si lE (Si )dG (VD lD (Si ))) (p Pj F act(c),c*VE dG (VD lD (Si )) Pj ).
last equality obtained successively eliminating environment components included
dG (VD lD (Si )) (using normalization conditions). scope plausibility function
Pj F act(c) included cpaG (c), equality entails PSi ,lE (Si ) || VD depend
assignment VD lD (Si ). Morever, PlE (Si ) || VD = Si PSi ,lE (Si ) || VD depend
assignment VD lD (Si ) too. PSi | lE (Si ) || VD = max{p Ep | PSi ,lE (Si ) || VD =
p p PlE (Si ) || VD }, entails PSi | lE (Si ) || VD depend assignment
VD lD (Si ). denoted PSi | lE (Si ) || lD (Si ) .
show PSi | l(Si ) = PSi | lE (Si ) || lD (Si ) . First, note
PSi ,l(Si ) = p VD lD (Si ) PSi ,lE (Si ),VD = p VD lD (Si ) (PSi | lE (Si ),VD p PlE (Si ),VD )
= p VD lD (Si ) (PSi | lE (Si ) || VD p PlE (Si ),VD ) (using Lemma 2)
= PSi | lE (Si ) || VD p (p VD lD (Si ) PlE (Si ),VD )
(since PSi | lE (Si ) || VD depend assignment VD lD (Si ))
= PSi | lE (Si ) || VD p Pl(Si ) .
Let assignment V .
PSi ,l(Si ) (A) p Pl(Si ) (A), conditionability plausibility structure directly
entails PSi | l(Si ) (A) = PSi | lE (Si ) || VD (A).
Otherwise, PSi ,l(Si ) (A) = Pl(Si ) (A). case, PSi | l(Si ) (A) = 1p . Next, V
l(Si ) = (VD lD (Si )) (VE lE (Si )), observe Pl(Si ) = p V l(Si ) (PVE || VD p p0 ) =
p VD lD (Si ) (PlE (Si ) || VD p p0 ). Similarly, PSi ,l(Si ) = p V (Si l(Si )) (PVE || VD p
p0 ) = p VD lD (Si ) (PSi ,lE (Si ) || VD p p0 ). PSi ,l(Si ) (A) = Pl(Si ) (A), infer
p VD lD (Si ) (PlE (Si ) || VD (A) p p0 ) = p VD lD (Si ) (PSi ,lE (Si ) || VD (A) p p0 ). neither
PlE (Si ) || VD PSi ,lE (Si ) || VD depends assignment VD lD (Si ), entails
PlE (Si ) || VD (A) p (p VD lD (Si ) p0 ) = PSi ,lE (Si ) || VD (A) p (p VD lD (Si ) p0 ). Summing
equation |dom(lD (Si ))| times gives PSi ,lE (Si ) || VD (A) = PlE (Si ) || VD (A), thus
PSi | lE (Si ) || VD (A) = 1p = PSi | l(Si ) (A).
(7) Proof similar point (6).
(4) Let i, j [1, k] < j, Si , Sj , r(Si ) l(Sj ) VD (Sj first set
environment variables appearing right Si Sov). Let (A, A0 ) dom(l(Si ))dom(Si )
PSi | l(Si ) (A) well-defined (i.e. Pl(Si ) (A) 6= 0p ) PSi | l(Si ) (A.A0 ) 6= 0p . Let A00
extension A.A0 l(Sj ). must show PSj | l(Sj ) (A00 ) well-defined, i.e.
Pl(Sj ) (A00 ) 6= 0p . PSi | l(Si ) (A.A0 ) 6= 0p Pl(Si ) (A) 6= 0p , Lemma 1 implies
PSi ,l(Si ) (A.A0 ) 6= 0p . Similarly proof point (6), possible show Pl(Sj )
depend assignment l(Sj ) (Si l(Si )). Therefore, every A00 extending A.A0
l(Sj ), p l(Sj )(Si l(Si )) Pl(Sj ) (A00 ) 6= 0p , implies Pl(Sj ) (A00 ) 6= 0p .
(5) Proof similar point (4), except plausibilities replaced feasibilities decision
variables replaced environment variables.

474

fiThe PFU Framework

Theorem 2 plausibility structure conditionable, then, queries Q PFU
network, Sem-Ans(Q) = Op-Ans(Q) optimal policies decisions
Sem-Ans(Q) Op-Ans(Q).
Proof. Let Af r assignment set free variables Vf r FVf r (Af r ) = f .
semantic definition gives (Sem-Ans(Q))(Af r ) = . Given FVf r (Af r ) = V Vf r FVE ,VD (Af r ) =
V Vf r FVD || (Af r ) = V Vf r (Fi F (Af r )) (since completion FVD || gives FVD || =
FVD ,VE ), infer every complete assignment A00 extending Af r , F (A00 ) = f
(Fi F (A00 )) ? (p Pi P Pi (A00 )) pu (uUi U Ui (A00 )) = . min(, ) = max(, ) = u =
, entails (Op-Ans(Q))(Af r ) = too.
analyze case FVf r (Af r ) = t. use A00 denote complete assignment
must considered semantic definition. Using properties:
p pu min(u1 , u2 ) = min(p pu u1 , p pu u2 ) (right monotonicity pu ),
p pu max(u1 , u2 ) = max(p pu u1 , p pu u2 ) (right monotonicity pu ),
p pu (u1 u u2 ) = (p pu u1 ) u (p pu u2 ) (distributivity pu u ),
p1 pu (p2 pu u) = (p1 p p2 ) pu u,
move PSi | l(Si ) (A.A0 ) get, starting semantic definition,
(p i[1,k],Si PSi | l(Si ) )(A00 ) pu UV (A00 )
right elimination operators.
prove quantity equals PVE | VD (A00 ) pu UV (A00 ). Let rightmost set
quantified environment variables. chain rule enables us write PVE | VD = PS | lE (S),VD p
PlE (S) | VD , lE (S) = l(S) . Moreover, using Lemma 2 Proposition 8(6),
write PS | lE (S),VD = PS | lE (S) || VD = PS | l(S) . Therefore, PVE | VD = PS | l(S) p PlE (S) | VD . Recursively applying mechanism leads to: PVE | VD = p i[1,k],Si PSi | l(Si ) . Therefore, obtain
PVE | VD (A00 ) pu UV (A00 ) right elimination operators.
semantic definition query meaning simplified bit, thanks Lemma 1.
lemma implies conditions PS | l(S) (A.A0 ) 6= 0p , used Pl(S) (A) 6= 0p ,
equivalent PS,l(S) (A.A0 ) 6= 0p , since PS,l(S) (A.A0 ) = PS | l(S) (A.A0 ) p Pl(S) (A). result,
operators uA0 dom(S),PS | l(S) (A.A0 )6=0p replaced uA0 dom(S),PS,l(S) (A.A0 )6=0p . Similarly,
eliminations minA0 dom(S),FS | l(S) (A.A0 )=t , conditions FS | l(S) (A.A0 ) = replaced
FS,l(S) (A.A0 ) = t. holds eliminations maxadom(xi ),FS | l(S) (A.A0 )=t .
start operational definition show reformulated above.
operational definition applies sequence variable eliminations global function (Fi F ) ?
(p Pi P Pi ) pu (Ui U Ui ), equals FVD | ? PVE | VD pu UV . Let leftmost
set quantified decision variables. Let assignment l(S). Assume quantified min. Let A0 dom(S) FS,l(S) (A.A0 ) = f . inferred
complete assignment A00 extending A.A0 , FVE ,VD (A00 ) = f , consequently FVD | (A00 ) =
f . implies FVD | (A00 ) ? PVE | VD (A00 ) pu UV (A00 ) = . Given min(, ) =
max(, ) = u = , obtain Qo(N , Sov, A.A0 ) = . min(d, ) = d, entails
minA0 dom(S) Qo(N , Sov, A.A0 ) = minA0 dom(S){A0 } Qo(N , Sov, A.A0 ). Thus, minA0 dom(S)
replaced minA0 dom(S),FS,l(S) (A.A0 )=t (as FVf r (A) = t, exists least one assignment
A0 dom(S) FS,l(S) (A.A0 ) = t). result holds quantified max. Applying mechanism set quantified decision variables left right Sov,
obtain minA0 dom(S) maxA0 dom(S) replaced minA0 dom(S),FS,l(S) (A.A0 )=t
maxA0 dom(S),FS,l(S) (A.A0 )=t respectively. Moreover, shown every complete assignment A00 considered corresponding transformed operational definition, FVD | (A00 ) = t.
thus possible replace FVD | (A00 ) ? PVE | VD (A00 ) pu UV (A00 ) PVE | VD (A00 ) pu UV (A00 ).

475

fiPralet, Verfaillie, & Schiex

transform uA0 dom(S) Qo(N , Sov, A.A0 ) looks expression
semantic definition. Let leftmost set quantified environment variables. Let assignment l(S). Let A0 dom(S) PS,l(S) (A.A0 ) = 0p . Then, complete assignments
A00 extending A.A0 , PVE | VD (A00 ) = 0p , thus PVE | VD (A00 ) pu UV (A00 ) = 0u . min(0u , 0u ) =
max(0u , 0u ) = 0u u 0u = 0u , obtain Qo(N , Sov, A.A0 ) = 0u . u 0u = d, computing uA0 dom(S) Qo(N , Sov, A.A0 ) equivalent computing uA0 dom(S){A0 } Qo(N , Sov, A.A0 ).
Thus, uA0 dom(S) replaced uA0 dom(S),PS,l(S) (A.A0 )6=0p (as Pl(S) (A) 6= 0p , exists
least one assignment A0 dom(S) satisfying PS,l(S) (A.A0 ) 6= 0p ). Applying mechanism,
considering set quantified environment variables left right Sov, get
uA0 dom(S),PS,l(S) (A,A0 )6=0p instead uA0 dom(S) .
Consequently, found function Sem-Ans(Q) = Op-Ans(Q) = .
Moreover, optimal policies decisions Sem-Ans(Q) optimal policies decisions
. Indeed, transformation rules used preserve set optimal policies. holds
Op-Ans(Q) . entails Sem-Ans(Q) = Op-Ans(Q), optimal policies
Sem-Ans(Q) Op-Ans(Q).

Theorem 3 Queries bounded queries used express solve following
nonexhaustive list problems:
1. SAT framework: SAT, MAJSAT, E-MAJSAT, quantified boolean formula, stochastic
SAT (SSAT) extended-SSAT (Littman et al., 2001).
2. CSP (or CN) framework:
Check consistency CSP (Mackworth, 1977); find solution CSP; count
number solutions CSP.
Find solution valued CSP (Bistarelli et al., 1999).
Solve quantified CSP (Bordeaux & Monfroy, 2002).
Find conditional decision unconditional decision mixed CSP
probabilistic mixed CSP (Fargier et al., 1996).
Find optimal policy stochastic CSP policy value greater
threshold; solve stochastic COP (Constraint Optimization Problem) (Walsh,
2002).
3. Integer Linear Programming (Schrijver, 1998) finite domain variables.
4. Search solution plan length k classical planning problem (STRIPS
planning, Fikes & Nilsson, 1971; Ghallab et al., 2004).
5. Answer classical queries Bayesian networks (Pearl, 1988), Markov random fields
(Chellappa & Jain, 1993), chain graphs (Frydenberg, 1990), plausibilities
expressed probabilities, possibilities, -rankings:
Compute plausibility distributions.
MAP (Maximum Posteriori hypothesis) MPE (Most Probable Explanation).
476

fiThe PFU Framework

Compute plausibility evidence.
CPE task hybrid networks (Dechter & Larkin, 2001) (CPE means CNF Probability Evaluation, CNF formula Conjunctive Normal Form).
6. Solve influence diagram (Howard & Matheson, 1984).
7. finite horizon, solve probabilistic MDP, possibilistic MDP, MDP based
-rankings, completely partially observable (POMDP), factored (Puterman,
1994; Monahan, 1982; Sabbadin, 1999; Boutilier et al., 1999, 2000).
Proof.
Lemma 3. Let (Ep , Eu , u , pu ) expected utility structure Eu totally ordered
u . Let S1 ,S2 local function Eu whose scope S1 S2 .
max
u
S1 ,S2 ((A).A)
:dom(S2 )dom(S1 ) Adom(S2 )

= u max S1 ,S2 .
S2

S1

Moreover, : dom(S2 ) dom(S1 ) satisfies (maxS1 S1 ,S2 )(A) = S1 ,S2 ((A).A)
dom(S2 ) iff max:dom(S2 )dom(S1 ) uAdom(S2 ) S1 ,S2 ((A).A) = uAdom(S2 ) S1 ,S2 ((A).A).
words, two sides equality set optimal policies S1 .
Proof. Let 0 : dom(S2 ) dom(S1 ) function
max:dom(S2 )dom(S1 ) uAdom(S2 ) S1 ,S2 ((A).A) = uAdom(S2 ) S1 ,S2 (0 (A).A).
Given that, dom(S2 ), S1 ,S2 (0 (A).A) u maxA0 dom(S1 ) S1 ,S2 (A0 .A), monotonicity
u entails uAdom(S2 ) S1 ,S2 (0 (A).A) u uAdom(S2 ) maxA0 dom(S1 ) S1 ,S2 (A0 .A). Thus,
max:dom(S2 )dom(S1 ) uAdom(S2 ) S1 ,S2 ((A).A) u uS2 maxS1 S1 ,S2 .
hand, let 0 : dom(S2 ) dom(S1 ) function dom(S2 ),
(maxS1 S1 ,S2 )(A) = S1 ,S2 (0 (A).A). Then,
uS2 maxS1 S1 ,S2 =
u
S1 ,S2 (0 (A).A) u
max
u
S1 ,S2 ((A).A).
:dom(S2 )dom(S1 ) Adom(S2 )

Adom(S2 )

antisymmetry u implies required equality. equality set optimal policies
S1 directly implied equality.
give proof theorem, uses cases previous lemma.
1. (CSP based problems, Mackworth, 1977 )
Let us consider CSP set variables V set constraints {C1 , . . . , Cm }.
(a) (Consistency solution finding) Consistency checked using query Q =
(N , (max, V )), N = (V, G, , , U ) (all variables V decision variables, G
reduced unique decision component containing variables, U = {C1 , . . . , Cm }),
expected utility structure boolean optimistic expected conjunctive
utility (row 6 Table 1). Computing Ans(Q) = maxV (C1 . . . Cm ) equivalent
checking consistency, Ans(Q) = iff exists assignment V satisfying
C1 . . .Cm , i.e. iff CSP consistent. order get solution Ans(Q) = t,
suffices record optimal decision rule V . Integer Linear Programming (Schrijver,
1998) finite domain variables formulated CSP.
(b) (Counting number solutions) expected utility structure considered
task probabilistic expected satisfaction (row 2 Table 1). PFU network
N = (V, G, P, , U ), variables V environment variables, G DAG
unique component c0 = V , P = {1/0 }, 0 constant factor equal |dom(V )|

477

fiPralet, Verfaillie, & Schiex

F act(c0 ) = {0 }, U = {C1 , . . . , Cm }. Implicitly, 1/0 specifies
complete assignments equiprobable. enables normalization
condition
P
c CE (G), p c p Pi F act(c) Pi = 1p satisfied, since V (1/|dom(V )|) = 1.
query consider Q = (N , (+, V )). P
hard check satisfies
conditions imposed queries Ans(Q) = V (1/0 (C1 . . . Cm )) gives
percentage solutions CSP. 0 Ans(Q) gives number solutions.
2. (Solving Valued CSP (VCSP), Bistarelli et al., 1999 )
order model problem, difficulty lies definition expected utility
structure. VCSP, triple (E, ~, ) called valuation structure introduced. satisfies
properties (E, ~) commutative semigroup, total order E, E
minimum element denoted >. expected utility structure consider following
one: (Ep , p , p ) = ({t, f }, , ), (Eu , u ) = (E, ~), expected utility structure
(Ep , Eu , u , pu ), u = min pu defined f alse pu u = > true pu u =
u (it hard check structure expected utility structure). Next,
PFU network N = (V, G, , , U ), V set variables VCSP, G
DAG one decision component containing variables, U contains soft
constraints. query Q = (min, V ) enables us find minimum violation degree
soft constraints. solution VCSP optimal (argmin) decision rule V .
3. (Problems SAT framework, Littman et al., 2001 )
SAT framework, queries conjunctive normal form boolean formula set
variables V = {x1 , . . . , xn } asked. Let us first prove extended SSAT formula evaluated PFU query. extended SSAT formula defined
triple (, , q) boolean formula conjunctive normal form, threshold
[0, 1], q = (q1 x1 ) . . . (qn xn ) sequence quantifier/variable pairs (the quantifiers
, , R; meaning R appears below). take f t, value quantification sequence q, val(, q), defined recursively by: (i) val(, ) = 1
t, 0 otherwise; (ii) val(,
(x) q 0 ) = maxx val(, q 0 ); (iii) val(, (x) q 0 ) = minx val(, q 0 );
P
(iv) val(, (Rx) q 0 ) =
0.5
val(, q 0 ). Intuitively, last case means R quantifies
x
boolean variables taking equiprobable values. extended SSAT formula (, , q) iff
val(, q) . denotes set variables quantified R, equivalent definition
val(, q) is: (i) val(, ) = 0.5|S| t, 0 otherwise; (ii) val(,
(x) q 0 ) = maxx val(, q 0 );
P
0
0
0
(iii) val(, (x) q ) = minx val(, q ); (iv) val(, (Rx) q ) = x val(, q 0 ). second definition proves val(, q) computed PFU query defined by: (a) expected
utility structure: probabilistic expected satisfaction (row 2 Table 1); (b) PFU network:
N = (V, G, P, , U ), V set variables formula (the decision variables
variables quantified ), G DAG without arcs, one decision component per
decision variable unique environment component containing variables quantified
R, P = {0 }, 0 constant factor equal 0.5|VE | , U set clauses ; (c)
query: Q = (N , Sov), Sov obtained q replacing , , R max, min,
+ respectively. Then, Ans(Q) = val(, q), implies value extended SSAT
formula (, , q) value bounded query (N , Sov, ).
SSAT particular case extended-SSAT therefore covered. SAT, MAJSAT, EMAJSAT, QBF particular cases extended SSAT. result, instances
PFU bounded queries. precisely, SAT corresponds bounded query form
Q = (N , (max, V ), 1); MAJSAT (given boolean formula set variables V ,
satisfied least half assignments V ) corresponds bounded query
form (N , (+, V ), 0.5); E-MAJSAT (given boolean formula V = VD ,
exist assignment VD formula satisfied least half assignments
?) corresponds bounded query form (N , (max, VD ).(+, ), 0.5); QBF

478

fiThe PFU Framework

corresponds bounded query max existentially quantified variables min
universally quantified variables alternate.
4. (Solving Quantified CSP (QCSP), Bordeaux & Monfroy, 2002 )
QCSP represents formula form Q1 x1 . . . Qn xn (C1 . . . Cm ), Qi
quantifier ( ) Ci constraint. value QCSP defined recursively
follows: value QCSP without variables (i.e. containing t, f , connectives)
given definition connectives. QCSP x qcsp iff either qcsp((x, t)) =
qcsp((x, f )) = t. Assuming f t, gives x qcsp iff maxx qcsp = t. QCSP x qcsp
iff qcsp((x, t)) = qcsp((x, f )) = t. Equivalently, x qcsp iff minx qcsp = t.
implies value QCSP actually given formula op(Q1 )x1 . . . op(Qn )xn (C1
. . . Cm ), op() = max op() = min. corresponds answer query
(N , (op(Q1 ), x1 ). . . . .(op(Qn ), xn )), N = (V, G, , , U ) (V set variables
QCSP, G DAG one decision component containing variables, U
set constraints), expected utility structure boolean optimistic expected
conjunctive utility (row 6 Table 1).
5. (Solving mixed CSP probabilistic mixed CSP, Fargier et al., 1996 )
probabilistic mixed CSP defined (i) set variables partitioned set W
contingent variables set X decision variables; assignment AW W called
world assignment AX X called decision; (ii) set C = {C1 , . . . , Cm }
constraints involving least one decision variable; (iii) probability distribution PW
worlds; possible world AW (i.e. PW (AW ) > 0) covered decision AX iff
assignment AW .AX satisfies constraints C.
one hand, decision must made without knowing world, task find
optimal non-conditional decision, i.e. find assignment AX decision variables

covered AX . probability equal
P maximizes probability world
P
P
(A
)
=
(P
W
W C1 . . . Cm ). result, optimal
AW | (C1 ...Cm )(AX ,AW )=1 W
W
non-conditionalPdecision found recording optimal decision rule X
formula maxX W (PW C1 . . . Cm ). previous formula actually specifies
solve problem PFUs. algebraic structure probabilistic expected additive
utility (row 2 Table 1), PFU network N = (V, G, P, , U ), VD = X, = W ,
G DAG without arc, one decision component X set environment components
depends PW specified, P set multiplicative factors define PW ,
finally U = {C1 , . . . , Cm }. query Q = (N , (max, X).(+, W )).
hand, world known decision made, task look
optimal conditional decision, i.e. look decision rule 0 : dom(W ) dom(X)
maximizes probability
P world covered. words, goal compute
max:dom(W )dom(X) AW dom(W ) | (C1 ...Cm )(AW .(AW ))=1 PW (AW ) =
P
max:dom(W )dom(X) AW dom(W ) (PW C1 . . . Cm ) (AW .(AW )). Due Lemma 3,
P
equals W maxX (PW C1 . . . Cm ), 0 found recording optimal
decision rule X. proves query Q = (N , (+, W ).(max, X)) enables us compute
optimal conditional decision.
Mixed CSPs, PW replaced set K constraints defining possible worlds.
goal look decision, either conditional non-conditional, maximizes
number covered worlds. task equivalent, ignoring normalizing constant, find
decision maximizes percentage covered worlds. solved using set
plausibility functions P = K {N0 }, N0 normalizing constant ensuring
normalization condition plausibilities holds. N0 number possible worlds,
actually need computed, since constant factor interested
optimal decisions.
479

fiPralet, Verfaillie, & Schiex

6. (Stochastic CSP (SCSP) stochastic COP (SCOP), Walsh, 2002 )
Formally, SCSP tuple (V, S, P, C, ), V list variables (each variable x
finite domain dom(x)), set stochastic variables V , P = {Ps | S}
set probability distributions (in advanced version SCSPs, probabilities
may defined Bayesian network; subsumption result still valid case),
C = {C1 , . . . , Cm } set constraints, threshold [0, 1].
SCSP-policy tree internal nodes labeled variables. root labeled
first variable V , parents leaves labeled last variable
V . Nodes labeled decision variable one child, whereas nodes labeled
stochastic variable |dom(s)| children. Leaf nodes labeled 1 complete
assignment define satisfies theQconstraints C, 0 otherwise. leaf
node associated probability sS Ps (AS ), stands assignment
implicitly defined path root leaf. satisfaction SCSP-policy
sum values leaves weighted probabilities. SCSP satisfiable
iff exists SCSP-policy satisfaction least . optimal satisfaction
SCSP maximum satisfaction SCSP-policies.
subsumption proof, first consider problem looking optimal satisfaction SCSP. SCSP-policy, decision variable x take one value per
assignment set preds (x) stochastic variables precede x list variables V . Instead described tree, SCSP-policy viewed set
x
functions
=
(x)) dom(x)), x V S}, value val() =
P
Q { : dom(pred
Q
(
P

C
x (AS ))). goal maximize previAS dom(S)
sS
Ci C )(AS .(
xV

ous quantity among sets . Let last decision variable V , let set
local functions : dom(preds (y))
defining
rule y.
X dom(y)X
Ya decision
(
val() = max
Ps
Ci )(AS .( x (AS ))).
max








dom(preds (y)) Spreds (y) sS

Ci C

xV


equals:

P Lemma 3,
Pprevious quantity
Q
Q
max
P

preds (y))
Spreds (y)
sS
Ci C Ci . recursive application mechanism shows answer Ans(Q) query Q described equal optimal
satisfaction SCSP:
expected utility structure: row 2 Table 1 (probabilistic expected satisfaction);
PFU network: N = (V 0 , G, P, , U ), V 0 set variables SCSP; =
VD = V 0 S; G DAG without arcs, one component per variable; P =
{Ps | S}; F act({s}) = {Ps }; U set constraints SCSP;
query: Q =(N , Sov), Sov=t(V ) (V thelist variables SCSP), t(V )
(+, {x}).t(V 00 ) x
.
recursively defined t() = t(x.V 00 ) =
(max, {x}).t(V 00 ) otherwise
optimal SCSP-policy recorded evaluation Ans(Q). satisfiability
SCSP answered bounded query (N , Sov, ). Again, corresponding
SCSP-policy obtained recording optimal decision rules.
Stochastic Constraint Optimization Problem (SCOP), constraints C additive
soft constraints. subsumption proof similar.
7. (Classical planning problems (STRIPS planning), Fikes & Nilsson, 1971; Ghallab et al., 2004 )
order search plan length lesser k, simply model classical planning problem CSP. transformation already available literature (Ghallab
et al., 2004). However, model classical planning problem directly
PFU framework. precisely, state one step described set boolean

480

fiThe PFU Framework

environment variables. step, unique decision variable whose set values corresponds actions available. Plausibility functions deterministic link
variables step variables step + 1 (these functions simply specify positive
negative effects actions). initial state represented plausibility function
links variables first step. Feasibility functions define preconditions action
feasible. link variables step decision variable step. Utility
functions boolean functions describe goal states. hold variables
step k. order search plan length lesser k, sequence elimination
max-elimination variables. expected utility structure used boolean optimistic
expected disjunctive utility.
8. (Influence diagrams, Howard & Matheson, 1984 )
start definition influence diagrams Section 3. decision variable
d, associate decision rule : dom(paG (d)) dom(d). influence diagram policy
(ID-policy) set = { | D} decision rules (one decision variable).
value val() ID-policy X
given
Yby probabilistic
X expectation utility:
((
Ps | paG (s) ) (
Ui ))(AS .( (AS ))).
val() =
dom(S)

Ui U

sS

dD

solve influence diagram, must compute maximum value previous quantity find associated optimal ID-policy. Using Lemma 3 DAG structure,
possible show, using ideas SCSP subsumption proof, optimal
expected utility given answer query Q (associated optimal decision rules
recorded evaluation Ans(Q)):
expected utility structure: row 1 Table 1 (probabilistic expected additive utility);
PFU network: N = (V, G0 , P, , U ); V set variables influence diagram,
G0 DAG obtained DAG influence diagram removing utility
nodes arcs decision nodes; G0 , one component per variable; P =
{Ps | paG (s) , } F act({s}) = {Ps | paG (s) }; U set utility functions
associated utility nodes.
PFU query: Q = (N , Sov), Sov obtained DAG influence diagram
follows. Initially, Sov = . DAG influence diagram, decisions totally
ordered. Let first decision variable DAG G influence diagram (i.e.
decision variable parent decision variable). Then, repeatedly update Sov
Sov Sov.(+, paG (d)).(max, {d}) delete variables paG (d) G
decision variable remains. Then, perform Sov Sov.(+, S), set
chance variables deleted G.
9. (Finite horizon MDPs, Puterman, 1994; Monahan, 1982; Sabbadin, 1999; Boutilier et al.,
1999, 2000 ) order prove encoding PFU framework given Sections 5.6
6.6 actually enables us solve time-steps probabilistic MDP, start reminding
algorithm used compute optimal MDP-policy. Usually, decision rule dT
chosen computing VsT = maxdT RsT ,dT . VsT optimal reward obtained
state sT . AtP
time-step [1, [, decision rule di chosen computing Vsi =
maxdi (Rsi ,di + si+1 Psi+1 | si ,di Vsi+1 ). Last, optimal expected value reward,
depends initial state s1 , Vs1 .
Let us prove induction
forP
[1,
P
Q 1],
P
Vs1 = maxd1 s2 . . .maxdi si+1 (( k[1,i] Psk+1 | sk ,dk ) (( k[1,i] Rsk ,dk ) + Vsi+1 )).
proposition holds = 1, since P

Vs1 = maxd1 (R
P s1 ,d1 + s2 Ps2 | s1 ,d1 Vs2)
P
= maxd1 s2 (Ps2 | s1 ,d1 (Rs1 ,d1 + Vs2 )) (since s2 Ps2 | s1 ,d1 = 1).
481

fiPralet, Verfaillie, & Schiex

Moreover, proposition
holds P
stepQi 1 (with > 1), P
P
Vs1 = maxd1 s2 . . . maxdi1 si (( k[1,i1] Psk+1 | sk ,dk ) (( k[1,i1] Rsk ,dk ) + Vsi )).
Given
P
P
P
( k[1,i1] Rsk ,dk ) + Vsi = ( k[1,i1] Rsk ,dk ) + maxdi (Rsi ,di + si+1 Psi+1 | si ,di Vsi+1 )
P
P
= maxdi (( k[1,i] Rsk ,dk ) + si+1 Psi+1 | si ,di Vsi+1 )
P
P
= maxdi si+1 Psi+1 | si ,di (( k[1,i] Rsk ,dk ) + Vsi+1 )
P
(the last equality holds since si+1 Psi+1 | si ,di = 1), inferred
Q
P
( k[1,i1] Psk+1 | sk ,dk ) (( k[1,i1] Rsk ,dk ) + Vsi )
P
Q
P
= maxdi si+1 (( k[1,i] Psk+1 | sk ,dk ) (( k[1,i] Rsk ,dk ) + Vsi+1 )),
proves proposition holds step i. proves holds step ,
therefore Vs1 = Ans(Q). Furthermore, step proof preserves set optimal
decision rules, optimal MDP-policy recorded evaluation Ans(Q).
study case partially observable finite horizon MDPs (finite horizon POMDPs).
POMDP, add time step > 1 conditional probability distribution Pot | st
making observation ot time step given state st . value st remains unobserved. assume probability distribution Ps1 initial state available.
subsumption proof case difficult. consider approach POMDPs
consists finding optimal policy tree. approach equivalent compute,
decision variable dt , decision rule dt depending observations made
far, i.e. function dt : dom({o2 , . . . , ot }) dom(dt ). set functions denoted
dt . set = {d1 , . . . , dT } called POMDP-policy. value POMDP-policy
recursively defined follows. First, value reward last decision step,
depends assignment AsT sT observations O2T made
beginning, V ()sT ,o2 ,...,ot (AsT .O2T ) = RsT ,dT (AsT , dT (O2T )). time step i,
obtained reward depends actual state Asi observations O2i made far.
expression is:
V ()si ,o2 ,...,o
P
Pi (Asi .O2i )
= (Rsi ,di + si+1 Psi+1 | si ,di oi+1 Poi+1 | si+1 V ()si+1 ,o1 ,...,oi+1 )(A)
= Asi .di (O2i ).O2i equation recursive formula used define
value policy tree POMDP (Kaelbling, Littman, & Cassandra,P
1998) equivalent. Finally, expected reward POMDP-policy V () = s1 Ps1 V ()s1 .
Solving finite horizon POMDP consists computing optimal expected reward among
POMDP-policies (i.e. computing V = maxd1 ,...,dT V ({d1 , . . . , dT })), well
associated optimal decision rules.
Using proof induction observable MDP case, first possible prove
problem steps, P
P
V = maxd1 ,...,dT o2 ,...,oT s1 ,...,sT V
Q
Q
P
V = (Ps1 i[1,T [ Psi+1 | si ,di i[1,T [ Poi+1 | si+1 ) ( i[1,T ] Rsi ,di ).
this, recursive use Lemma
3 enables
P
P us infer
Pthat
P
V = maxd1 o2 maxd2 o3 maxd3 . . . oT maxdT s1 ,...,sT V .
proves query defined enables us compute V well optimal policy:
algebraic structure: probabilistic expected additive utility (row 1 Table 1);
PFU network: N = (V, G, P, , U ); V equals {si | [1, ]} {oi | [2, ]} {di |
[1, ]}, VD = {di | [1, ]}; G DAG one variable per component;
decision component parents, environment component {oi }
{si } parent, component {si+1 } {si } {di } parents; P = {Ps1 }
{Psi+1 | si ,di , [1, 1]} {Poi | si | [2, ]}; F act({s1 }) = {Ps1 }, F act({si+1 }) =
{Psi+1 | si ,di }, F act({oi }) = {Poi | si }; last, U = {Rsi ,di | [1, ]};
482

fiThe PFU Framework

PFU query: based DAG, necessary condition query defined
decision di must appear left variables {sk | k [i + 1, ]} {ok | k
[i + 1, ]}; query considered Q = (N , Sov),
Sov = (max, d1 ).(+, o2 ).(max, d2 ). . . . .(+, oT ).(max, dT ).(+, {s1 , . . . , sT }).
proofs finite horizon (PO)MDPs based possibilities -rankings similar.
subsumption factored MDPs, first argue every factored MDP
represented usual MDP, therefore PFU query PFU network. Even
sufficient argument, define better representation factored MDPs
PFU framework: corresponds representation variables describing states
directly used together local plausibility functions rewards, modeled
scoped functions (defined decision trees, binary decision diagrams. . . ).
10. (Queries Bayesian networks, Pearl, 1988, Markov random fields, Chellappa & Jain, 1993,
chain graphs, Frydenberg, 1990 )
suffices consider chain graphs, since Bayesian networks Markov random fields
particular cases chain graphs. subsumption proofs provided general case
plausibility distributions defined totally ordered conditionable plausibility structure.
(a) (MAP, MPE, probability evidence) MPE (Most Probable Explanation)
computation probability evidence particular cases MAP (Maximum
Posteriori hypothesis), suffices prove MAP subsumed. probabilistic
MAP problem consists finding, given probability distribution PV , Maximum
Posteriori explanation assignment subset V observed (also
called evidence). formally, let denote set variables explanation
sought let e denote observed assignment O. MAP problem consists
finding assignment maxAdom(D) PD | (A.e) = PD | (A .e).
PD | = PD,O /PO , write:
maxAdom(D) PD | (A.e) = (maxAdom(D) P
PD,O (A.e))/PO (e)
= (maxAdom(D) A0 dom(V (DO)) PV (A.e.A0 ))/PO (e)
P
Thus, computing maxD V (DO) PV (e) sufficient (the difference lies normalizing constant). result generalized totally ordered conditionable
plausibility structures.
Indeed, p monotonic, maxAdom(D) PD,O (A.e) = (maxAdom(D) PD | (A.e)) p
PO (e). maxAdom(D) PD,O (A.e) p PO (e), exists unique p Ep
maxAdom(D) PD,O (A.e) = p p PO (e). gives us p = maxAdom(D) PD | (A.e).
Otherwise, maxAdom(D) PD,O (A.e) = PO (e), infer exists
dom(D) PD,O (A .e) = PO (e), therefore PD | (A .e) = 1p . Thus,
maxAdom(D) PD | (A.e) = 1p too. shows determining maxAdom(D) PD,O (A.e)
gives maxAdom(D) PD | (A.e).
Moreover, argmax{PD,O (A0 .e), A0 dom(D)}, max{p Ep | PD,O (A .e) =
p p PO (e)} p max{p Ep | PD,O (A.e) = p p PO (e)} dom(D). Therefore, optimal assignment maxD PD,O (e) optimal assignment
maxD PD | (e). result, MAP problem reduced computation
maxD PD,O (e) = maxD p V (DO) PV (e) = maxD p V (PV p )
scoped function scope (e0 ) = 1p e0 = e, 0p otherwise. define PFU query whose answer Ans(Q) = maxD p V (PV p ):
plausibility structure (Ep , p , p ), utility structure (Eu , u ) = (Ep , p ),
expected utility structure (Ep , Eu , u , pu ) = (Ep , Ep , p , p );
PFU network: difficulty definition PFU network lies fact
normalization conditions components must satisfied. idea
483

fiPralet, Verfaillie, & Schiex

components variable involved modified. PFU
network N = (V, G, P, , U ); V set variables chain graph; VD =
= V D; G DAG components obtained DAG G0 chain
graph splitting every component c variable involved:
component c transformed |c| components containing one variable;
|c| components become parents child components c; component
{x0 } included one |c| components, x0 D, {x0 } decision
component; otherwise, {x0 } environment component, create plausibility function Pi , equal constant p0 (x0 ) p i[1,|dom(x0 )|] p0 (x0 ) = 1p ,
F act({x0 }) = {p0 (x0 )}; P contains first constants defined above,
second factors expressing Pc | paG0 (c) chain graph components
c satisfying c (D O) = ; last, U contains factors expressing Pc | paG0 (c)
chain graph components c c (D O) 6= , constant factor
p1 (x0 ) satisfying p1 (x0 ) p p0 (x0 ) = 1p component {x0 } created
splitting process described above, hard constraints representing ;
PFU network, local normalization conditions satisfied, combination
local functions equals PV p ;
PFU query: query simply Q = (N , (max, D).(u , V D)).
optimal decision rule recorded computation Ans(Q).
(b) (Plausibility distribution computation task ) Given plausibility distribution PV expressed combination plausibility functions chain graphs, goal
compute plausibility distribution PS set V . basic formula PS =
p V PV proves query defined actually computes PS . query shows
usefulness free variables.
plausibility structure (Ep , p , p ), utility structure (Eu , u ) = (Ep , p ),
expected utility structure (Ep , Eu , u , pu ) = (Ep , Ep , p , p );
PFU network: N = (V, G, P, , U ), = V S, VD = S, DAG
G sets P , U obtained similarly MAP case;
PFU query: Q = (N , (u , V S))
11. (Hybrid networks, Dechter & Larkin, 2001 )
hybrid network triple (G, P, F ), G DAG set variables V partitioned
R D, P set probability distributions expressing Pr | paG (r) r R,
F set functions fpaG (d) (variables deterministic,
sense value completely determined assignment parents).
general task hybrid networks task belief assessment conditioned formula conjunctive normal form. consists computing probability distribution
variable x given complex evidence (complex may involve several variables).
P Ignoring normalizing constant, requires compute, assignments (x, a)
x, Adom(V {x}) | (A.(x,a))=t PV (A.(x, a)). C = {C1 , . . . , Cm } denotes set clauses
P
Q
Q
Q
, equals ( V {x} ( rR Pr | paG (r) ) ( dD fpaG (d) ) ( Ci C Ci ))((x, a)).
query corresponding computation uses probabilistic expected satisfaction structure (row 2 Table 1), PFU network N = (V, G, P, , U ), = V , VD = {x},
P = {Pr | paG (r) | r R {x}} {fpaG (d) | {x}}, either U = C {Px | paG (x) }
x R U = C {fpaG (x) } x D. query Q = (N , (+, V {x})).

484

fiThe PFU Framework

References
Bacchus, F., & Grove, A. (1995). Graphical Models Preference Utility. Proc.
11th International Conference Uncertainty Artificial Intelligence (UAI-95),
pp. 310, Montreal, Canada.
Bahar, R., Frohm, E., Gaona, C., Hachtel, G., Macii, E., Pardo, A., & Somenzi, F. (1993).
Algebraic Decision Diagrams Applications. IEEE /ACM International
Conference CAD, pp. 188191, Santa Clara, California, USA. IEEE Computer
Society Press.
Bertele, U., & Brioschi, F. (1972). Nonserial Dynamic Programming. Academic Press.
Bistarelli, S., Montanari, U., Rossi, F., Schiex, T., Verfaillie, G., & Fargier, H. (1999).
Semiring-Based CSPs Valued CSPs: Frameworks, Properties Comparison.
Constraints, 4 (3), 199240.
Bodlaender, H. (1997). Treewidth: Algorithmic techniques results. Proc.
22nd International Symposium Mathematical Foundations Computer Science
(MFCS-97).
Bordeaux, L., & Monfroy, E. (2002). Beyond NP: Arc-consistency Quantified Constraints. Proc. 8th International Conference Principles Practice
Constraint Programming (CP-02), Ithaca, New York, USA.
Boutilier, C., Brafman, R., Domshlak, C., Hoos, H., & Poole, D. (2004). CP-nets: Tool
Representing Reasoning Conditional Ceteris Paribus Preference Statements.
Journal Artificial Intelligence Research, 21, 135191.
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-Theoretic Planning: Structural Assumptions Computational Leverage. Journal Artificial Intelligence Research,
11, 194.
Boutilier, C., Dearden, R., & Goldszmidt, M. (2000). Stochastic Dynamic Programming
Factored Representations. Artificial Intelligence, 121 (1-2), 49107.
Boutilier, C., Friedman, N., Goldszmidt, M., & Koller, D. (1996). Context-Specific Independence Bayesian Networks. Proc. 12th International Conference
Uncertainty Artificial Intelligence (UAI-96), pp. 115123, Portland, Oregon, USA.
Chellappa, R., & Jain, A. (1993). Markov Random Fields: Theory Applications. Academic Press.
Chu, F., & Halpern, J. (2003a). Great Expectations. Part I: Customizability
Generalized Expected Utility. Proc. 18th International Joint Conference
Artificial Intelligence (IJCAI-03), Acapulco, Mexico.
Chu, F., & Halpern, J. (2003b). Great Expectations. Part II: Generalized Expected Utility
Universal Decision Rule. Proc. 18th International Joint Conference
Artificial Intelligence (IJCAI-03), pp. 291296, Acapulco, Mexico.
Cooper, M., & Schiex, T. (2004). Arc Consistency Soft Constraints. Artificial Intelligence, 154 (1-2), 199227.
Darwiche, A. (2001). Recursive Conditioning. Artificial Intelligence, 126 (1-2), 541.
485

fiPralet, Verfaillie, & Schiex

Darwiche, A., & Ginsberg, M. (1992). Symbolic Generalization Probability Theory.
Proc. 10th National Conference Artificial Intelligence (AAAI-92), pp.
622627, San Jose, CA, USA.
Dechter, R. (1999). Bucket Elimination: Unifying Framework Reasoning. Artificial
Intelligence, 113 (1-2), 4185.
Dechter, R., & Fattah, Y. E. (2001). Topological Parameters Time-Space Tradeoff.
Artificial Intelligence, 125 (1-2), 93118.
Dechter, R., & Larkin, D. (2001). Hybrid Processing Beliefs Constraints. Proc.
17th International Conference Uncertainty Artificial Intelligence (UAI-01),
pp. 112119, Seattle, WA, USA.
Dechter, R., & Mateescu, R. (2004). Mixtures Deterministic-Probabilistic Networks
AND/OR Search Space. Proc. 20th International Conference
Uncertainty Artificial Intelligence (UAI-04), Banff, Canada.
Demirer, R., & Shenoy, P. (2001). Sequential Valuation Networks: New Graphical Technique Asymmetric Decision Problems. Proc. 6th European Conference
Symbolic Quantitavive Approaches Reasoning Uncertainty (ECSQARU01), pp. 252265, London, UK.
Dubois, D., & Prade, H. (1995). Possibility Theory Basis Qualitative Decision
Theory. Proc. 14th International Joint Conference Artificial Intelligence
(IJCAI-95), pp. 19251930, Montreal, Canada.
Fargier, H., Lang, J., & Schiex, T. (1996). Mixed Constraint Satisfaction: Framework
Decision Problems Incomplete Knowledge. Proc. 13th National
Conference Artificial Intelligence (AAAI-96), pp. 175180, Portland, OR, USA.
Fargier, H., & Perny, P. (1999). Qualitative Models Decision Uncertainty without
Commensurability Assumption. Proc. 15th International Conference
Uncertainty Artificial Intelligence (UAI-99), pp. 188195, Stockholm, Sweden.
Fikes, R., & Nilsson, N. (1971). STRIPS: New Approach Application Theorem
Proving. Artificial Intelligence, 2 (3-4), 189208.
Fishburn, P. (1982). Foundations Expected Utility. D. Reidel Publishing Company,
Dordrecht.
Friedman, N., & Halpern, J. (1995). Plausibility Measures: Users Guide. Proc.
11th International Conference Uncertainty Artificial Intelligence (UAI-95), pp.
175184, Montreal, Canada.
Frydenberg, M. (1990). Chain Graph Markov Property. Scandinavian Journal
Statistics, 17, 333353.
Ghallab, M., Nau, D., & Traverso, P. (2004). Automated Planning: Theory Practice.
Morgan Kaufmann.
Giang, P., & Shenoy, P. (2000). Qualitative Linear Utility Theory Spohns Theory
Epistemic Beliefs. Proc. 16th International Conference Uncertainty
Artificial Intelligence (UAI-00), pp. 220229, Stanford, California, USA.

486

fiThe PFU Framework

Giang, P., & Shenoy, P. (2005). Two Axiomatic Approaches Decision Making Using
Possibility Theory. European Journal Operational Research, 162 (2), 450467.
Goldman, R., & Boddy, M. (1996). Expressive Planning Explicit Knowledge. Proc.
3rd International Conference Artificial Intelligence Planning Systems (AIPS96), pp. 110117, Edinburgh, Scotland.
Halpern, J. (2001). Conditional Plausibility Measures Bayesian Networks. Journal
Artificial Intelligence Research, 14, 359389.
Halpern, J. (2003). Reasoning Uncertainty. MIT Press.
Howard, R., & Matheson, J. (1984). Influence Diagrams. Readings Principles
Applications Decision Analysis, pp. 721762. Strategic Decisions Group, Menlo
Park, CA, USA.
Jegou, P., & Terrioux, C. (2003). Hybrid Backtracking bounded Tree-decomposition
Constraint Networks. Artificial Intelligence, 146 (1), 4375.
Jensen, F., Nielsen, T., & Shenoy, P. (2004). Sequential Influence Diagrams: Unified
Asymmetry Framework. Proceedings Second European Workshop Probabilistic Graphical Models (PGM-04), pp. 121128, Leiden, Netherlands.
Jensen, F., & Vomlelova, M. (2002). Unconstrained Influence Diagrams. Proc.
18th International Conference Uncertainty Artificial Intelligence (UAI-02), pp.
234241, Seattle, WA, USA.
Kaelbling, L., Littman, M., & Cassandra, A. (1998). Planning Acting Partially
Observable Stochastic Domains. Artificial Intelligence, 101 (1-2), 99134.
Kolhas, J. (2003). Information Algebras: Generic Structures Inference. Springer.
Kushmerick, N., Hanks, S., & Weld, D. (1995). Algorithm Probabilistic Planning.
Artificial Intelligence, 76 (1-2), 239286.
Larrosa, J., & Schiex., T. (2003). quest best form local consistency
weighted csp. Proc. 18th International Joint Conference Artificial Intelligence (IJCAI-03), Acapulco, Mexico.
Lauritzen, S., & Nilsson, D. (2001). Representing Solving Decision Problems
Limited Information. Management Science, 47 (9), 12351251.
Littman, M., Majercik, S., & Pitassi, T. (2001). Stochastic Boolean Satisfiability. Journal
Automated Reasoning, 27 (3), 251296.
Mackworth, A. (1977). Consistency Networks Relations. Artificial Intelligence, 8 (1),
99118.
Monahan, G. (1982). Survey Partially Observable Markov Decision Processes: Theory,
Models, Algorithms. Management Science, 28 (1), 116.
Ndilikilikesha, P. (1994). Potential Influence Diagrams. International Journal Approximated Reasoning, 10, 251285.
Nielsen, T., & Jensen, F. (2003). Representing solving asymmetric decision problems.
International Journal Information Technology Decision Making, 2, 217263.

487

fiPralet, Verfaillie, & Schiex

Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems: Networks Plausible Inference. Morgan Kaufmann.
Perny, P., Spanjaard, O., & Weng, P. (2005). Algebraic Markov Decision Processes.
Proc. 19th International Joint Conference Artificial Intelligence (IJCAI05), Edinburgh, Scotland.
Pralet, C. (2006). Generic Algebraic Framework Representing Solving Sequential Decision Making Problems Uncertainties, Feasibilities, Utilities. Ph.D.
thesis, Ecole Nationale Superieure de lAeronautique et de lEspace, Toulouse, France.
Pralet, C., Schiex, T., & Verfaillie, G. (2006a). Decomposition Multi-Operator Queries
Semiring-based Graphical Models. Proc. 12th International Conference
Principles Practice Constraint Programming (CP-06), pp. 437452, Nantes,
France.
Pralet, C., Schiex, T., & Verfaillie, G. (2006b). Influence Diagrams Multioperator
Cluster DAGs. Proc. 22nd International Conference Uncertainty
Artificial Intelligence (UAI-06), Cambridge, MA, USA.
Pralet, C., Verfaillie, G., & Schiex, T. (2006c). Decision Uncertainties, Feasibilities,
Utilities: Towards Unified Algebraic Framework. Proc. 17th European
Conference Artificial Intelligence (ECAI-06), pp. 427431, Riva del Garda, Italy.
Puterman, M. (1994). Markov Decision Processes, Discrete Stochastic Dynamic Programming. John Wiley & Sons.
Sabbadin, R. (1999). Possibilistic Model Qualitative Sequential Decision Problems
Uncertainty Partially Observable Environments. Proc. 15th International Conference Uncertainty Artificial Intelligence (UAI-99), pp. 567574,
Stockholm, Sweden.
Sang, T., Beame, P., & Kautz, H. (2005). Solving Bayesian Networks Weighted Model
Counting. Proc. 20th National Conference Artificial Intelligence (AAAI05), pp. 475482, Pittsburgh, PA, USA.
Schmeidler, D. (1989). Subjective Probability Expected Utility without Additivity.
Econometrica, 57 (3), 571587.
Schrijver, A. (1998). Theory Linear Integer Programming. John Wiley Sons.
Shafer, G. (1976). Mathematical Theory Evidence. Princeton University Press.
Shenoy, P. (1991). Valuation-based Systems Discrete Optimization. Uncertainty
Artificial Intelligence, 6, 385400.
Shenoy, P. (1992). Valuation-based Systems Bayesian Decision Analysis. Operations
Research, 40 (3), 463484.
Shenoy, P. (1994). Conditional Independence Valuation-Based Systems. International
Journal Approximated Reasoning, 10 (3), 203234.
Shenoy, P. (2000). Valuation Network Representation Solution Asymmetric Decision
Problems. European Journal Operational Research, 121, 579608.

488

fiThe PFU Framework

Smith, J., Holtzman, S., & Matheson, J. (1993). Structuring Conditional Relationships
Influence Diagrams. Operations Research, 41, 280297.
Spohn, W. (1990). General Non-Probabilistic Theory Inductive Reasoning. Proc.
6th International Conference Uncertainty Artificial Intelligence (UAI-90),
pp. 149158, Cambridge, MA, USA.
von Neumann, J., & Morgenstern, O. (1944). Theory Games Economic Behaviour.
Princeton University Press.
Walsh, T. (2002). Stochastic Constraint Programming. Proc. 15th European
Conference Artificial Intelligence (ECAI-02), pp. 111115, Lyon, France.
Weydert, E. (1994). General Belief Measures. Proc. 10th International Conference
Uncertainty Artificial Intelligence (UAI-94), pp. 575582.
Wilson, N. (1995). Order Magnitude Calculus. Proc. 11th International
Conference Uncertainty Artificial Intelligence (UAI-95), pp. 548555, Montreal,
Canada.

489


