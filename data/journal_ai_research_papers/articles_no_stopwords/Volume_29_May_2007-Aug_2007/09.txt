Journal Artificial Intelligence Research 29 (2007) 309-352

Submitted 6/06; published 7/07

Learning Symbolic Models Stochastic Domains
Hanna M. Pasula
Luke S. Zettlemoyer
Leslie Pack Kaelbling

pasula@csail.mit.edu
lsz@csail.mit.edu
lpk@csail.mit.edu

MIT CSAIL
Cambridge, 02139

Abstract
article, work towards goal developing agents learn act
complex worlds. develop probabilistic, relational planning rule representation
compactly models noisy, nondeterministic action effects, show rules
effectively learned. experiments simple planning domains 3D simulated
blocks world realistic physics, demonstrate learning algorithm allows
agents effectively model world dynamics.

1. Introduction
One goals artificial intelligence build systems act complex environments effectively humans do: perform everyday human tasks, making breakfast
unpacking putting away contents office. Many tasks involve manipulating objects. pile things up, put objects boxes drawers, arrange
shelves. requires understanding world works: depending
objects pile arranged made of, pile sometimes slips falls
over; pulling drawer usually opens it, sometimes drawer sticks; moving box
typically break items inside it.
Building agents perform common tasks challenging problem. work,
approach problem developing rule-based representation agents use
model, learn, effects acting environment. Learning allows agents
adapt new environments without requiring humans hand-craft models, something
humans notoriously bad at, especially numeric parametrization required.
representation use probabilistic relational, includes additional logical
concepts. present supervised learning algorithm uses representation language
build model action effects given set example action executions. optimizing
tradeoff maximizing likelihood examples minimizing complexity
current hypothesis, algorithm effectively selects relational model structure,
set model parameters, language new relational concepts together provide
compact, yet highly accurate description action effects.
agent hopes act real world must integrated system perceives
environment, understands, commands motors effect changes it. Unfortunately,
current state art reasoning, planning, learning, perception, locomotion,
manipulation far removed human-level abilities, cannot yet contemplate
c
2007
AI Access Foundation. rights reserved.

fiPasula, Zettlemoyer, & Pack Kaelbling

Figure 1: three-dimensional blocks world simulation. world consists table, several cubes
roughly uniform density varying size, robotic gripper moved
simulated motors.

working actual domain interest. Instead, choose work domains
almost ridiculously simplified proxies.1
One popular proxy, used since beginning work AI planning (Fikes &
Nilsson, 1971) world stacking blocks. typically formalized version
logic, using predicates on(a, b) clear (a) describe relationships blocks
one another. Blocks always neatly stacked; dont fall jumbles.
article, present work context slightly less ridiculous version blocks
world, one constructed using three-dimensional rigid-body dynamics simulator (ODE,
2004). example world configuration shown Figure 1. simulated blocks
world, blocks vary size colour; piles always tidy, may sometimes fall
over; gripper works medium-sized blocks, unreliable even there.
approach capable enabling effective behavior domain must handle noisy,
nondeterministic nature, nontrivial dynamics, able handle
domains similar characteristics.
One strategy formulating approach learn models worlds dynamics
use planning different courses action based goals may change
time. Another strategy assume fixed goal reward function, learn
policy optimizes reward function. worlds complexity imagining,
would impossible establish, advance, appropriate reaction every possible
situation; addition, expect agent overall control architecture
hierarchical, individual level hierarchy changing goals.
reasons, learn model world dynamics, use make plans
achieve goals hand.
begin paper describing assumptions underlie modeling decisions.
describe syntax semantics modeling language give algorithm
1. reasonable alternative approach, advocated Brooks (1991), working real world,
natural complexity, solving problems almost ridiculously simplified proxies
problems interest.

310

fiLearning Symbolic Models Stochastic World Dynamics

learning models language. validate models, introduce simple planning
algorithm provide empirical results demonstrating utility learned models
showing plan them. Finally, survey relevant previous work,
draw conclusions.

2. Structured Stochastic Worlds
agent introduced novel world must find best possible explanation
worlds dynamics within space possible models represent, defined
agents representation language. ideal language would able compactly model
every action effect agent might encounter, others. extra modeling capacity
wasted complicate learning, since agent consider larger space
possible models, likely overfit experience. Choosing good representation
language provides strong bias algorithm learn models language.
languages used describe deterministic planning models are,
least surface, first order; is, abstract particular identities objects, describing effects actions terms properties relations among
objects. (They accomplish letting action take arguments, representing
arguments variables.) representational capacity crucial reasons compactness generalization: usually grossly inefficient describe behavior
individual objects.
Much original work probabilistic planning uses formalism Markov decision processes, represents states world individually atomically (Puterman, 1999). recently, propositional (factored) representations dynamics
employed (Boyen & Koller, 1998; Guestrin, Koller, Parr, & Venkataraman, 2003),
first-order representations developed, including probabilistic rules (Blum & Langford, 1999), equivalence classes (Draper, Hanks, & Weld, 1994), situation calculus
approach Boutilier, Reiter, Price (2001). representations make easy
articulate take direct advantage two useful assumptions world dynamics:
frame assumption, states that, agent takes action world, anything
explicitly changed stays same, outcome assumption, states
action affects world small number distinct ways, possible effect causes
set changes world happen together single outcome.
take point departure probabilistic first-order representations world
dynamics. representations traditionally applied domains logistics
planning traditional, abstract blocks world, idealized symbolic abstractions
underlying domain. goal learn models realistic worlds, requires
us adapt modeling language accommodate additional uncertainty complexity.
by:
Allowing rules refer objects mentioned argument list action.
Relaxing frame assumption: allowing unmodeled noise changes world.
Extending language: allowing complex forms quantification construction new concepts.
311

fiPasula, Zettlemoyer, & Pack Kaelbling

Action parameterization traditional representations action dynamics, objects
whose properties may changed result action must named argument
list action. Instead, define actions parameters describe
objects free parameters action: example, block picked up,
object currently held block placed. However, actions change
properties objects, ones parameter list, models
way determining objects affected. paper,
introduce use deictic references identify objects. Deictic references (Agre
& Chapman, 1987) identify objects relative agent action performed.
example, refer objects thing block picked up,
currently held object, table block accidentally falls onto. use deictic
references mechanism adding new logical variables models, much
way Benson (1996).
Modeling noise complex domains, actions affect world variety ways.
must learn model circumstances reasonable effects,
behavior unusual situations. complicates dynamics, makes
learning difficult. Also, actions executed physical world,
guaranteed small number simple effects, result may violate
outcomes assumption. blocks world happen, example, stack
knocked over. develop simple noise mechanism allows us partially model
action effects, ignoring ones rare complicated model explicitly.
Language extension traditional symbolic domains, rules constructed using
predefined set observable predicates. However, sometimes useful define additional
predicates whose truth values computed based predefined ones.
found essential modeling certain advanced planning domains (Edelkamp &
Hoffman, 2004).
traditional blocks worlds, example, usual set predicates contains on, clear
inhand. working realistic, noisy blocks world, found
predicates would sufficient allow agent learn accurate model.
example, would difficult state putting block tall stack likely cause
stack topple without concept stack height, state attempting
pick block clear usually picks block top stack without
way describing block top stack.
could simply add additional predicates seem useful perceptual
language, hand-engineering appropriate language every time tackle new problem
difficult, time consuming, error prone. State-of-the-art planning representations
PDDL (Edelkamp & Hoffman, 2004) use concept language define new predicates
concepts terms previous, simpler ones. paper, show concepts
learned, much predicates invented ILP (Khan, Muggleton, & Parson, 1998).
see, traditional blocks world predicates, including inhand clear,
well useful concepts height, easily defined terms given simple
concept language (Yoon, Fern, & Givan, 2002).
312

fiLearning Symbolic Models Stochastic World Dynamics

3. State Action Representation
goal learn model state transition dynamics world. so, need
able represent set possible states world set possible
actions agent take. represent components using subset
relatively standard first-order logic equality. representation states actions
ground inference, learning, planning.
begin defining primitive language includes set constants C, set
predicates , set functions . three types functions : traditional
functions, range objects; discrete-valued functions, range predefined
discrete set values; integer-valued functions, range finite subset
integers. primitives observed directly world. (In work,
assume environment completely observable; is, agent able
perceive unambiguous correct description current state.2 ) constants
C assumed intrinsic meaning, viewed meaningless markers
assigned perceptual system, described detail below.
3.1 State Representation
States describe possible different configurations properties relations
objects. state describes particular configuration values
objects world, individual objects denoted using constants.
limit number objects world configuration, though current
formalism mechanism creation deletion objects result
world dynamics.
Formally, state descriptions conjunctive sentences form:
^

^

tG(C,m())

^

()(t)

^

(t) = ,

tG(C,m())

m(x) arity predicate function x, C set c1 . . . cn constants, G(x, a)
set length lists elements x, () indicates predicates may
optionally negated, indicates functions assigned value
range. manner, states list truth values possible groundings
predicates functions terms. sentence gives complete specification,
vocabulary , properties interrelations |C| objects present
world. (Note predicate function arguments always constants, never
terms made using function symbols, descriptions always finite given finite
language.)
rest section, describe two approaches denoting objects using
constants C, illustrate example conjunctive state sentences.
3.1.1 Intrinsic Constants
first approach state descriptions refers objects using intrinsic constants.
intrinsic constant associated particular object consistently used denote
2. strong, ultimately indefensible assumption; one highest priorities future
work extend case environment partially observable.

313

fiPasula, Zettlemoyer, & Pack Kaelbling

object. constants useful perceptual system unique way
identify perceived objects independent attributes relations one another.
example, internet software agent might access universal identifiers
distinguish objects perceives.
example, let us consider representing states simple blocks world, using
language contains predicates on, clear, inhand, inhand-nil, block, table,
integer-valued function height. objects world include blocks BLOCK-A BLOCK-B,
table TABLE, gripper. Blocks blocks table. block
nothing clear. gripper hold one block empty. sentence
inhand-nil on(BLOCK-A, BLOCK-B) on(BLOCK-B, TABLE) on(BLOCK-B, BLOCK-A)
on(BLOCK-A, TABLE) on(TABLE, BLOCK-A) on(TABLE, BLOCK-B) on(TABLE, TABLE)
on(BLOCK-A, BLOCK-A) on(BLOCK-B, BLOCK-B) table(TABLE) table(BLOCK-A)
(1)
table(BLOCK-B) block(BLOCK-A) block(BLOCK-B) block(TABLE) clear(BLOCK-A)
clear(BLOCK-B) clear(TABLE) inhand(BLOCK-A) inhand(BLOCK-B)
inhand(TABLE) height(BLOCK-A) = 2 height(BLOCK-B) = 1 height(TABLE) = 0
represents blocks world gripper holds nothing two blocks single
stack table. Block BLOCK-A top stack, BLOCK-B BLOCK-A
table TABLE.
encoding, sentence contains meaningful information objects
identities, used learning world dynamics.
3.1.2 Skolem Constants
Alternatively, states denote objects using skolem constants. Skolem constants
arbitrary identifiers associated objects world inherent
meaning beyond used state description.3 constants useful
perceptual system way assigning meaningful identifiers objects
observes. example, consider robot might build state description room
finds in. assume robot observe objects present,
properties, relationships other. However, naming objects,
reason choose particular name specific object. Instead, creates
arbitrary identifiers, skolem constants, uses build state.
Using skolem constants, rewrite Sentence 1 as:
inhand-nil on(c001, c002) on(c002, c004) on(c002, c001)
on(c001, c004) on(c004, c001) on(c004, c002) on(c004, c004)
on(c001, c001) on(c002, c002) table(c004) table(c001)
table(c002) block(c001) block(c002) block(c004) clear(c001)
clear(c002) clear(c004) inhand(c001) inhand(c002)
inhand(c004) height(c001) = 2 height(c002) = 1 height(c004) = 0
Here, perceptual system describes table two blocks using arbitrary
constants c004, c001, c002.
3. Skolem constants interpreted skolemizations existential variables.

314

fiLearning Symbolic Models Stochastic World Dynamics

perspective, states world isomorphic interpretations
logical language, since might many interpretations satisfy particular statespecification sentence; interpretations permutation
objects constants refer to. occurs objects distinguishable based
properties relations objects.
techniques develop paper generally applicable representing
learning dynamics worlds intrinsic constants skolem constants.
highlight cases true presented. see
use skolem constants perceptually plausible forces us
create new learning algorithms abstract object identity aggressively previous
work improve quality learned models.
3.2 Action Representation
Actions represented positive literals whose predicates drawn special set, ,
whose terms drawn set constants C associated world
action executed.
example, simulated blocks world, contains pickup/1, action picking
blocks, puton/1, action putting blocks. action literal pickup(BLOCK-A)
could represent action gripper attempts pickup block BLOCK-A
state represented Sentence 1.

4. World Dynamics Representation
learning probabilistic transition dynamics world, viewed
conditional probability distribution Pr(s0 |s, a), s, s0 A. represent
dynamics rules constructed basic logic described Section 3, using
logical variables abstract identities particular objects world.
section, begin describing traditional representation deterministic world
dynamics. Next, present probabilistic case. Finally, extend ways
mentioned Section 2: permitting rules refer objects mentioned
action description, adding noise, extending language allow
construction new concepts.
dynamic rule action z form
x.(x) z(x) 0 (x) ,
meaning that, vector terms x context holds
current time step, taking action z(x) cause formula 0 hold terms
next step. action z(x) must contain every xi x. constrain 0
conjunctions literals constructed primitive predicates terms xi x,
functions applied terms set equal value range. addition,
allowed contain literals constructed integer-valued functions term related
integer range greater-than less-than predicates.
say rule covers state action exists substitution
mapping variables x C (note may fewer variables x constants
315

fiPasula, Zettlemoyer, & Pack Kaelbling

C) |= ((x)) = z((x)). is, substitution constants
variables that, applied context (x), grounds entailed
state and, applied rule action z(x), makes equal action a.
Now, given rule covers a, say subsequent state s0 ?
First, rule directly specifies 0 ((x)) holds next step. may
incomplete specification state; use frame assumption fill rest:
s0 = 0 ((x))

^

^

l(s, , (x))

{{(t):tG(C,m())}pos(0 ((x)))}



^

^

l(s, , (x)) ,

{{(t):tG(C,m())}funct(0 ((x)))}

l(s, y, t) stands literal predicate function argument
list t, pos(0 ) set literals 0 negations ignored, funct(0 ) set
ground functions 0 extracted equality assignments. say
every literal would needed make complete description state
included 0 ((x)) retrieved, associated truth value equality assignment,
s.
general set rules action, require contexts
mutually exclusive, given state-action pair covered one rule;
covered none, assume nothing changes. 4 example, consider
small set rules picking blocks,
pickup(X, Y) : inhand-nil, on(X, Y), block(Y), height(Y) < 10
inhand(X), on(X, Y), clear(Y),
pickup(X, Y) : inhand-nil, on(X, Y), table(Y)
inhand(X), on(X, Y).
top line rule shows action followed context; next line describes
effects, outcome. According two rules, executing pickup(X, Y) changes
world hand empty X Y. exact set changes depends
whether table, block height nine less.
4.1 Probabilistic Rules
deterministic dynamics rules described allow generalization objects
exploitation frame assumption, well suited use highly
stochastic domains. order apply domains extend
describe probability distribution resulting states, Pr(s0 |s, a). Probabilistic STRIPS
operators (Blum & Langford, 1999) model agents actions affect world around
describing actions alter properties relationships objects
world. rule specifies small number simple action outcomessets changes
occur tandem.
4. Without restriction, would need define method choosing possibly conflicting predictions different covering rules. simplest way would involve picking one
rules, perhaps specific one, one confident of. (Rule confidence scores
would estimated.)

316

fiLearning Symbolic Models Stochastic World Dynamics

see probabilistic rules form


p1

01 (x)
x.(x) z(x) . . . . . .

p
0
n n (x)

,

p1 . . . pn positive numbers summing 1, representing probability distribution, 01 . . . 0n formulas describing subsequent state, s0 .
Given state action a, compute coverage deterministic
case. Now, however, given covering substitution (x), probabilistic rules longer predict
unique successor state. Instead, 01 . . . 0n used construct new state,
single 0 deterministic case. n possible subsequent
states, s0i , occur associated probability pi .
probability rule r assigns moving state state s0 action
taken, Pr(s0 |s, a, r), calculated as:

P (s0 |s, a, r) =

X

P (s0 , 0i |s, a, r)

0i r

=

X

P (s0 |0i , s, a, r)P (0i |s, a, r)

(2)

0i r

P (0i |s, a, r) pi , outcome distribution P (s0 |0i , s, a, r) deterministic
distribution assigns mass relevant s0 . P (s0 |0i , s, a, r) = 1.0, is,
s0 state would constructed given rule outcome, say
outcome 0i covers s0 .
general, possible, representation, subsequent state s0 covered
one rules outcomes. case, probability s0 occurring
sum probabilities relevant outcomes. Consider rule painting blocks:

paint(X) : inhand(X), block(X)
(



.8 : painted(X), wet
.2 : change.

rule used model transition caused action paint(a) initial
state contains wet painted(a), one possible successor state: one
change occurs, wet painted(a) remain true. outcomes describe
one successor state, must sum probabilities recover states total
probability.
set rules specifies complete conditional probability distribution Pr(s0 |s, a)
following way: current state action covered exactly one rule,
distribution subsequent states prescribed rule. not, s0 predicted
probability 1.0.
317

fiPasula, Zettlemoyer, & Pack Kaelbling

example, probabilistic set rules picking blocks might look follows:
pickup(X, Y) : inhand-nil, on(X, Y), block(Y), height(Y) < 10
(



.7 : inhand(X), on(X, Y), clear(Y)
.3 : change

pickup(X, Y) : inhand-nil, on(X, Y), table(Y)
(



.8 : inhand(X), on(X, Y)
.2 : change

top line rule still shows action followed context; bracket surrounds
outcomes distribution. outcomes before,
small chance occur.
4.2 Deictic Reference
standard relational representations action dynamics, variable denoting object
whose properties may changed result action must named argument
list action. result awkwardness even deterministic situations. example, abstract action picking block must take two arguments. pickup(X, Y),
X block picked block picked up.
relationship encoded added condition on(X, Y) rules context. condition restrict applicability rule; exists guarantee bound
appropriate object. restriction adopted means that, given
grounding action, variables rule bound, necessary
search substitutions would allow rule cover state. However, complicate planning because, many cases, ground instances operator considered,
even though eventually rejected due violations preconditions.
example, would reject instances violating on(X, Y) relation context.
complex domains, requirement even awkward: depending
circumstances, taking action may affect different, varied sets objects. blocks worlds
block may several others, pickup action may affect properties
blocks. model without additional mechanism referring objects,
might increase, even vary, number arguments pickup takes.
handle gracefully, extend rule formalism include deictic references
objects. rule may augmented list, D, deictic references. deictic
reference consists variable Vi restriction set literals define
Vi respect variables x action Vj j < i.
restrictions supposed pick single, unique object: notif pick
several, nonethe rule fails apply. So, handle pickup action described above,
action would single argument, pickup(X), rule would contain deictic
variable V constraint on(X, V).
use rules deictic references, must extend procedure computing rule
coverage ensure deictic references resolved. deictic variables
may bound simply starting bindings x working sequentially
deictic variables, using restrictions determine unique bindings. point
318

fiLearning Symbolic Models Stochastic World Dynamics

binding deictic variable unique, fails refer, rule fails cover
stateaction pair.
formulation means extra variables need included action specification, reduces number operator instances, yet, requirement
unique designation, substitution still quickly discovered testing coverage.
So, example, denote red block table V2 (assuming
one table one block) would use following deictic references:
V1 : table(V1 )
V2 : color (V2 ) = red block (V2 ) on(V2 , V1 ) .
several, no, tables world, then, rule semantics, first
reference would fail: similarly, second reference would fail number red blocks
unique table represented V1 one.
give action-oriented example, denoting block top block
touched, touch(Z) action, would use following deictic reference:
V1 : on(V1 , Z) block (V1 ) .
set deictic probabilistic rules picking blocks might look follows:
pickup(X) :

n

: inhand(Y), Z : table(Z)



empty context
(



.9 : inhand-nil, inhand(Y), on(Y, Z)
.1 : change

pickup(X) :

n

: block(Y), on(X, Y)



inhand-nil, height(Y) < 10
(



.7 : inhand(X), on(X, Y), clear(Y)
.3 : change

pickup(X) :

n

: table(Y), on(X, Y)



inhand-nil
(
.8 : inhand(X), on(X, Y)

.2 : change
top line rule shows action followed deictic variables,
variable annotated restriction. next line context, outcomes
distribution follow. first rule applies situations something
gripper, states probability 0.9 action cause gripped
object fall table, nothing change otherwise. second rule applies
situations object picked another block, states
probability success 0.7. third rule applies situations object
picked table describes slightly higher success probability, 0.8. Note
different objects affected, depending state world.
319

fiPasula, Zettlemoyer, & Pack Kaelbling

4.3 Adding Noise
Probability models type seen thus far, ones small set possible
outcomes, sufficiently flexible handle noise real world. may
large number possible outcomes highly unlikely, reasonably hard model:
example, configurations may result tall stack blocks topples.
would inappropriate model outcomes impossible, dont space
inclination model individual outcome.
So, allow rule representation account results noise. definition, noise able represent outcomes whose probability havent quantified.
Thus, allowing noise, lose precision true probability distribution
next states.
handle noise, must change rules two ways. First, rule
additional noise outcome 0noise , associated probability P (0noise |s, a, r); now,
set outcome probabilities must sum 1.0 include P (0noise |s, a, r) well
P (01 |s, a, r) . . . P (0n |s, a, r). However, 0noise associated list literals,
since declining model detail happens world cases.
Second, create additional default rule, empty context two outcomes: empty outcome (which, combination frame assumption, models
situations nothing changes), and, again, noise outcome (modeling situations). rule allows noise occur situations specific rule applies;
probability assigned noise outcome default rule specifies kind background
noise level.
Since explicitly modeling effects noise, longer calculate
transition probability Pr(s0 |s, a, r) using Equation 2: lack required distribution
P (s0 |0i , s, a, r) noise outcome. Instead, substitute worst case constant bound
pmin P (s0 |0noise , s, a, r). allows us bound transition probability
P (s0 |s, a, r) = pmin P (0noise |s, a, r) +

X

P (s0 |0i , s, a, r)P (0i |s, a, r)

0i r

P (s0 |s, a, r).

(3)

Intuitively, pmin assigns small amount probability mass every possible next state
Note take value higher true minimum: approximation.
However, ensure probability model remains well-defined, pmin times number
possible states exceed 1.0.
way, create partial model allows us ignore unlikely overly complex
state transitions still learning acting effectively. 5
Since rules include noise deictic references, call Noisy Deictic Rules
(NDRs). rather stochastic world, set NDRs picking blocks might
s0 .

5. P (s0 |0noise , s, a, r) could modeled using well-defined probability distribution describing noise
world, would give us full distribution next states. premise
might difficult specify distributionin domain, would ensure
distribution assign probability worlds impossible, worlds blocks
floating midair. long events unlikely enough would want consider
planning, reasonable model directly.

320

fiLearning Symbolic Models Stochastic World Dynamics

look follows:
pickup(X) :

n

: inhand(Y), Z : table(Z)



empty context


.6 : inhand-nil, inhand(Y), on(Y, Z)
.1 : change


.3 : noise
pickup(X) :

n

: block(Y), on(X, Y)



inhand-nil, height(Y) < 10




.7 : inhand(X), on(X, Y), clear(Y)

.1 : change


.2 : noise
n

pickup(X) :

: table(Y), on(X, Y)



inhand-nil


.8 : inhand(X), on(X, Y)
.1 : change


.1 : noise
default (rule:
.6 : change

.4 : noise
format rules before, Section 4.2, except rule
includes explicit noise outcome. first three rules similar old versions,
difference model noise. final rule default rule: states
that, rule applies, probability observing change 0.4.
Together rules provide complete example type rule set learn
Section 5.1. However, written fixed modeling language functions
predicates. next section describes concepts used extend language.
4.4 Concept Definitions
addition observed primitive predicates, often useful background
knowledge defines additional predicates whose truth values computed based
observations. found essential modeling certain planning
domains (Edelkamp & Hoffman, 2004).
background knowledge consists definitions additional concept predicates
functions. work, express concept definitions using concept language
includes conjunction, existential quantification, universal quantification, transitive closure, counting. Quantification used defining concepts inhand(X) :=
block(X) Y.on(X, ). Transitive closure included language via Kleene star
operator defines concepts above(X, ) := (X, ). Finally, counting included using special quantifier # returns number objects formula
true. useful defining integer-valued functions height(X) := #Y.above(X, ).
321

fiPasula, Zettlemoyer, & Pack Kaelbling

defined, concepts enable us simplify context deictic variable definitions, well restrict ways cannot described using simple conjunctions.
Note, however, need track concept values outcomes, since
always computed primitives. Therefore, rule contexts use language
enriched concepts; outcomes contain primitives.
example, deictic noisy rule attempting pick block X, side
side background knowledge necessary primitive predicates
table.

pickup(X) :




: topstack(Y, X),


Z : on(Y, Z),

: table(T)




inhand-nil, height(Y) < 9


.80 : on(Y, Z)


.10 : on(Y, Z), on(Y, T)


.05 : change



.05 : noise

clear(X)

:= Y.on(Y, X)

inhand(X) := block(X) Y.on(X, Y)
inhand-nil := Y.inhand(Y)
above(X, Y)

:= (X, Y)

(4)

topstack(X, Y) := clear(X) above(X, Y)
height(X) := #Y.above(X, Y)

rule complicated example rules given thus far: deals
situation block picked up, X, middle stack. deictic variable
identifies (unique) block top stack, deictic variable Zthe object
, deictic variable Tthe table. might expected, gripper succeeds
lifting high probability.
concept definitions include clear(X), defined exists object
X; inhand(X), defined X block object; inhand-nil, defined
exists object hand; above(X, Y), defined transitive
closure on(X, Y); topstack(X, Y), defined X Y, clear; height(X),
defined number objects X using chain ons. explained
above, concepts used context deictic variable definitions,
outcomes track primitive predicates; fact, appears outcomes, since
value table predicates never changes.
4.5 Action Models
combine set concept definitions set rules define action model.
best action models represent rule set using NDRs, but, comparison purposes,
experiments involve rule sets use simpler representations, without
noise deictic references. Moreover, rule sets differ whether allowed
contain constants. rules presented far contained none, neither
context outcomes. reasonable setup states contain skolem
constants, constants inherent meaning names assigned
general repeated. However, states intrinsic constants, perfectly
acceptable include constants action models. all, constants used
uniquely identify objects world.
develop learning algorithm next section, assume general
constants allowed action model, show simple restrictions within
322

fiLearning Symbolic Models Stochastic World Dynamics

algorithm ensure learned models contain any. show,
Section 7, learning action models restricted free constants provides
useful bias improve generalization training small data sets.

5. Learning Action Models
defined rule action models, describe may constructed
using learning algorithm attempts return action model best explains set
example actions results. formally, algorithm takes training set E,
example (s, a, s0 ) triple, searches action model maximizes
likelihood action effects seen E, subject penalty complexity.
Finding involves two distinct problems: defining set concept predicates,
constructing rule set R using language contains predicates together
directly observable primitive predicates. section, first discuss second problem,
rule set learning, assuming fixed set predicates provided learner. Then,
present simple algorithm discovers new, useful concept predicates.
5.1 Learning Rule Sets
problem learning rule sets is, general, NP-hard (Zettlemoyer, Pasula, & Kaelbling,
2003). Here, address problem using greedy search. structure search
hierarchically identifying two self-contained subproblems: outcome learning,
subproblem general rule set search, parameter estimation, subproblem
outcome learning. Thus, overall algorithm involves three levels greedy search:
outermost level, LearnRules, searches space rule sets, often
constructing new rules, altering existing ones; middle level, InduceOutcomes which,
given incomplete rule consisting context, action, set deictic references, fills
rest rule; innermost level, LearnParameters, takes slightly
complete rule, lacking distribution outcomes, finds distribution
optimizes likelihood examples covered rule. present three
levels starting inside out, subroutine described one
depends it. Since three subroutines attempt maximize scoring metric,
begin introducing metric.
5.1.1 Scoring Metric
greedy search algorithm must judge parts search space desirable.
Here, done help scoring metric rule sets,
S(R) =

X

log(P (s0 |s, a, r(s,a) ))

(s,a,s0 )E

X

P EN (r)

(5)

rR

r(s,a) rule governing transition occurring performed s,
scaling parameter, P EN (r) complexity penalty applied rule r. Thus, S(R)
favors rule sets maximize likelihood bound data penalizes rule sets
overly complex.
Ideally, P would likelihood example. However, rules noise outcomes
cannot assign exact likelihood so, case, use lower bound defined Equa323

fiPasula, Zettlemoyer, & Pack Kaelbling

tion 3 instead. P EN (r) defined simply total number literals r. chose
penalty simplicity, performed worse penalty
term tested informal experiments. scaling parameter set 0.5 experiments, could set using cross-validation hold-out dataset
principled technique. metric puts pressure model explain examples using
non-noise outcomes, increases P , opposing pressure complexity, via
P EN (r).
assume state-action pair (s, a) covered one rule (which,
finite set examples, enforced simply ensuring examples stateaction pair covered one rule) rewrite metric terms rules rather
examples, give
S(R) =

X

X

rR

(s,a,s0 )Er

log(P (s0 |s, a, r)) P EN (r)

(6)

Er set examples covered r. Thus, rules contribution S(R)
calculated independently others.
5.1.2 Learning Parameters
first algorithms described section, LearnParameters, takes incomplete
rule r consisting action, set deictic references, context, set outcomes,
learns distribution P maximizes rs score examples Er covered it.
Since procedure allowed alter number literals rule, therefore
cannot affect complexity penalty term, optimal distribution simply one
maximizes log likelihood Er . case rules noise outcomes
log(P (s0 |s, a, r))

X

L =

(s,a,s0 )Er



=



log pmin P (0noise |s, a, r) +

X

X

P (s0 |0i , s, a, r)P (0i |s, a, r) .

(7)

0i r

(s,a,s0 )Er

non-noise outcome, P (s0 |0i , s, a, r) one 0i covers (s, a, s0 ) zero otherwise.
(In case rules without noise outcomes, sum slightly simpler,
pmin P (0noise |s, a, r) term missing.)
every example covered unique outcome, Ls maximum expressed
closed form. Let set examples covered outcome 0 E0 . add
Lagrange multiplier enforce constraint P (0i |s, a, r) distributions must sum
1.0, get


L =

X
(s,a,s0 )Er

=

X
E0

log


X

P (s0 |0i , s, a, r)P (0i |s, a, r) + (

X

0i r

0i

|E0 | log P (0i |s, a, r) + (


X

P (0i |s, a, r) 1.0) .

0i

324

P (0i |s, a, r) 1.0)

fiLearning Symbolic Models Stochastic World Dynamics

Then, partial derivative L respect P (0i |s, a, r) |E0 |/P (0i |s, a, r)
= |E|, P (0i |s, a, r) = |E0i |/|E|. Thus, parameters estimated
calculating percentage examples outcome covers.
However, seen Section 4.1, possible example covered
one outcome; indeed, noise outcome, covers examples,
always case. situation, sum examples cannot rewritten
simple sum terms representing different outcomes containing single
relevant probability: probabilities overlapping outcomes remain tied together,
general closed-form solution exists, estimating maximum-likelihood parameters
nonlinear programming problem. Fortunately, instance well-studied problem
maximizing concave function (the log likelihood presented Equation 7) probability simplex. Several gradient ascent algorithms known problem (Bertsekas,
1999); since function concave, guaranteed converge global maximum.
LearnParameters uses conditional gradient method, works by, iteration,
moving along parameter axis maximal partial derivative. step-sizes
chosen using Armijo rule (with parameters = 1.0, = 0.1, = 0.01.)
search converges improvement L small, less 106 . chose
algorithm easy implement converged quickly experiments
tried. However, problems found method converges slowly, one
many nonlinear optimization methods, constrained Newtons method,
could directly applied.
5.1.3 Inducing Outcomes
Given LearnParameters, algorithm learning distribution outcomes,
consider problem taking incomplete rule r consisting context, action,
perhaps set deictic references, finding optimal way fill rest
rulethat is, set outcomes {01 . . . 0n } associated distribution P
maximize score
S(r) =

X

log(P (s0 |s, a, r)) P ENo (r),

(s,a,s0 )Er

Er set examples covered r, P ENo (r) total number literals
outcomes r. (S(r) factor scoring metric Equation 6 due
rule r, without aspects P EN (r) fixed purposes subroutine:
number literals context.)
general, outcome induction NP-hard (Zettlemoyer, Pasula, & Kaelbling, 2003). InduceOutcomes uses greedy search restricted subset possible outcome sets:
proper training examples, outcome set proper every outcome
covers least one training example. Two operators, described below, move
space immediate moves improve rule score. set
outcomes considers, InduceOutcomes calls LearnParameters supply best P can.
initial set outcomes created by, example, writing set
atoms changed truth values result action, creating outcome
describe every set changes observed way.
325

fiPasula, Zettlemoyer, & Pack Kaelbling

E1
E2
E3
E4

01
02
03
04

= t(c1), h(c2) h(c1), h(c2)
= h(c1), t(c2) h(c1), h(c2)
= h(c1), h(c2) t(c1), t(c2)
= h(c1), h(c2) h(c1), h(c2)
(a)

= {h(c1)}
= {h(c2)}
= {t(c1), t(c2)}
= {no change}
(b)

Figure 2: (a) Possible training data learning set outcomes. (b) initial set
outcomes would created data (a) picking smallest
outcome describes change.

example, consider coins domain. coins world contains n coins,
showing either heads tails. action flip-coupled, takes arguments,
flips coins, half time heads, otherwise tails. set training
data learning outcomes two coins might look part (a) Figure 2 h(C)
stands heads(C), t(C) stands heads(C), s0 part (s, a, s0 ) example
= flip-coupled. suppose suggested rule flip-coupled
context deictic references. Given data, initial set outcomes four
entries part (b) Figure 2.
rule contained variables, either abstract action arguments deictic
references, InduceOutcomes would introduce variables appropriate places
outcome set. variable introduction achieved applying inverse action
substitution examples set changes computing initial set outcomes. 6
So, given deictic reference C : red(C) always found refer c1, red
coin, example set outcomes would contain C wherever currently contains c1.
Finally, disallow use constants rules, variables become way
outcomes refer objects whose properties changed. Then, changes containing
constant referred variable cannot expressed, corresponding
example covered noise outcome.
Outcome Search Operators

InduceOutcomes uses two search operators. first add operator, picks
pair non-contradictory outcomes set creates new outcome
conjunction. example, might pick 01 02 combine them, adding new
outcome 05 = {h(c1), h(c2)} set. second remove operator drops
outcome set. Outcomes dropped overlapping
outcomes every example cover, otherwise outcome set would remain proper.
(Of course, outcome set contains noise outcome, every outcome
dropped, since examples covered noise outcome.) Whenever operator
adds removes outcome, LearnParameters called find optimal distribution
6. Thus, InduceOutcomes introduces variables aggressively wherever possible, based intuition
corresponding objects would better described constant, become apparent
training example.

326

fiLearning Symbolic Models Stochastic World Dynamics

new outcome set, used calculate maximum log likelihood
data respect new outcome set.
Sometimes, LearnParameters return zero probabilities outcomes.
outcomes removed outcome set, since contribute nothing
likelihood, add complexity. optimization improves efficiency
search.
outcomes Figure 2, 04 dropped since covers E4 ,
covered 01 02 . new outcome created conjoining
existing ones 05 = {h(c1), h(c2)}, covers E1 , E2 , E3 . Thus, 05 added,
01 02 dropped. Adding 05 dropping 01 , 02 , 04 creates outcome
set {03 , 05 }, optimal set outcomes training examples Figure 2.
Notice outcome always equal union sets literals change
training examples covers. fact ensures every proper outcome made
merging outcomes initial outcome set. InduceOutcomes can, theory, find
set outcomes.
5.1.4 Learning Rules
know fill incomplete rules, describe LearnRules, outermost
level learning algorithm, takes set examples E fixed language
primitive derived predicates, performs greedy search space rule
sets. precisely, searches space proper rule sets, rule set R
defined proper respect data set E includes one rule
applicable every example e E change occurs, include
rules applicable examples.
search proceeds described pseudocode Figure 3. starts rule set
contains default rule. every step, takes current rule set applies
search operators obtain set new rule sets. selects rule set R
maximizes scoring metric S(R) defined Equation 5. Ties S(R) broken
randomly.
begin explaining search initialized, go describe
operators used, finish working simple example shows LearnRules
action.
Rule Set Search Initialization

LearnRules initialized proper rule set. paper, always initialize
set noisy default rule. treats action effects training set
noise; search progresses, search operators introduce rules explain action
effects explicitly. chose initial starting point simplicity, worked
well informal experiments. Another strategy would start specific rule
set, describing detail examples. bottom-up methods advantage
data-driven, help search reach good parts search space
easily. However, show, several search operators used algorithm
presented guided training examples, algorithm already
desirable property. Moreover, bottom-up method bad complexity properties
327

fiPasula, Zettlemoyer, & Pack Kaelbling

LearnRuleSet(E)
Inputs:
Training examples E
Computation:
Initialize rule set R contain default rule
better rules sets found
search operator
Create new rule sets O, RO = O(R, E)
rule set R0 RO
score improves (S(R0 ) > S(R))
Update new best rule set, R = R0
Output:
final rule set R

Figure 3: LearnRuleSet pseudocode. algorithm performs greedy search space
rule sets. step set search operators propose set new rule sets.
highest scoring rule set selected used next iteration.

situations large data set described using relatively simple set rules,
case interested in.
Rule Set Search Operators

rule set search, LearnRules repeatedly finds applies operator
increase score current rule set most.
search operators work creating new rule set rules (usually
altering existing rule) integrating new rules rule set way
ensures rule set remains proper. Rule creation involves picking action z,
set deictic references D, context , calling InduceOutcomes
learning algorithm complete rule filling 0i pi s. (If new rule covers
examples, attempt abandoned, since adding rule cannot help scoring
metric.) Integration rule set involves adding new rules, removing
old rules cover examples. increase number examples
covered default rule.
5.1.5 Search Operators
search operator takes input rule set R set training examples E,
creates set new rule sets RO evaluated greedy search loop. eleven
search operators. first describe complex operator, ExplainExamples, followed
simple one, DropRules. Then, present remaining nine operators,
share common computational framework outlined Figure 4.
Together, operators provide many different ways moving space
possible rule sets. algorithm adapted learn different types rule sets (for
example, without constants) restricting set search operators used.
328

fiLearning Symbolic Models Stochastic World Dynamics

OperatorTemplate(R, E)
Inputs:
Rule set R
Training examples E
Computation:
Repeatedly select rule r R
Create copy input rule set R0 = R
Create new set rules, N , making changes r
new rule r0 N covers examples
Estimate new outcomes r0 InduceOutcomes
Add r0 R0 remove rules R0 cover
examples r0 covers
Recompute set examples default rule R0
covers parameters default rule
Add R0 return rule sets RO
Output:
set rules sets, RO

Figure 4: OperatorTemplate Pseudocode. algorithm basic framework used
six different search operators. operator repeatedly selects rule, uses make n
new rules, integrates rules original rule set create new rule set.

ExplainExamples takes input training set E rule set R creates new,
alternative rule sets contain additional rules modeling training examples
covered default rule R. Figure 5 shows pseudocode
algorithm, considers training example E covered default
rule R, executes three-step procedure. first step builds large
specific rule r0 describes example; second step attempts trim rule,
generalize maximize score, still ensuring covers E;
third step creates new rule set R0 copying R integrating new
rule r0 new rule set.
illustration, let us consider steps 1 2 ExplainExamples might
applied training example (s, a, s0 ) = ({on(a, t), on(b, a)}, pickup(b), {on(a, t)}),
background knowledge defined Rule 4 Section 4.4 constants
allowed.
Step 1 builds rule r. creates new variable X represent object b
action; then, action substitution becomes = {X b}, action r
set pickup(X). context r set conjunction inhand-nil, inhand(X),
clear(X), height(X) = 2, on(X, X), above(X, X), topstack(X, X). Then, Step
1.2, ExplainExamples attempts create deictic references name constants
whose properties changed example, already action substitution. case, changed literal on(b, a), b substitution,
C = {a}; new deictic variable created restricted, extended
329

fiPasula, Zettlemoyer, & Pack Kaelbling

ExplainExamples(R, E)
Inputs:
rule set R
training set E
Computation:
example (s, a, s0 ) E covered default rule R
Step 1: Create new rule r
Step 1.1: Create action context r
Create new variables represent arguments
Use create new action substitution
Set rs action 1 (a)
Set rs context conjunction boolean equality literals
formed using variables available functions predicates
(primitive derived) entailed
Step 1.2: Create deictic references r
Collect set constants C whose properties changed s0 ,

c C
Create new variable v extend map v c
Create , conjunction literals containing v formed using
available variables, functions, predicates, entailed
Create deictic reference variable v restriction 1 ()
uniquely refers c s, add r
Step 1.3: Complete rule
Call InduceOutcomes create rules outcomes.
Step 2: Trim literals r
Create rule set R0 containing r default rule
Greedily trim literals r, ensuring r still covers (s, a, s0 ) filling
outcomes using InduceOutcomes R0 score stops improving
Step 3: Create new rule set containing r
Create new rule set R0 = R
Add r R0 remove rules R0 cover examples r covers
Recompute set examples default rule R0 covers parameters
default rule
Add R0 return rule sets RO
Output:
set rule sets, RO

Figure 5: ExplainExamples Pseudocode. algorithm attempts augment rule set new
rules covering examples currently handled default rule.

330

fiLearning Symbolic Models Stochastic World Dynamics

{X b, a}. Finally, Step 1.3, outcome set created. Assuming
examples context applies, nine ten end X lifted,
rest falling onto table, resulting rule r0 looks follows:


inhand(Y ), clear(Y), on(X, Y), table(Y)

pickup(X) : : above(X, Y), topstack(X, Y), above(Y, Y)


topstack(Y, Y), on(Y, Y), height(Y) = 1
inhand-nil, inhand(X), clear(X), table(X), height(X) = 2, on(X, X),
above(X,
X), topstack(X, X)

0.9 : on(X, Y)

0.1 : noise

(The falls table outcome modeled noise, since absence constants
rule way referring table.)
Step 2, ExplainExamples trims rule remove literals always true
training examples, on(X, X), table()s, redundant ones,
inhand(), clear(Y), perhaps one heights, give


pickup(X) : : on(X, Y)
inhand-nil,
clear(X), height(X) = 2

0.9 : on(X, Y)

0.1 : noise

rules context describes starting example concisely. Explain Examples
consider dropping remaining literals, thereby generalizing rule
applies examples different starting states. However, generalizations
necessarily improve score. smaller contexts, might end
creating outcomes describe new examples, penalty term
guaranteed improve. change likelihood term depend whether
new examples higher likelihood new rule default rule,
whether old examples higher likelihood old distribution
new one. Quite frequently, need cover new examples
give new rule distribution closer random before, usually
lead decrease likelihood large overcome improvement
penalty, given likelihood-penalty trade-off.
Let us assume that, case, predicate dropped without worsening
likelihood. rule integrated rule set is.
DropRules cycles rules current rule set, removes one
turn set. returns set rule sets, one missing different rule.
remaining operators create new rule sets input rule set R repeatedly
choosing rule r R making changes create one new rules. new
rules integrated R, ExplainExamples, create new rule set R0 .
Figure 4 shows general pseudocode done. operators vary
way select rules changes make them. variations described
331

fiPasula, Zettlemoyer, & Pack Kaelbling

operator below. (Note operators, deal deictic
references constants, applicable action model representation allows
features.)
DropLits selects every rule r R n times, n number literals
context r; words, selects r literal context.
creates new rule r0 removing literal rs context; N Figure 4
simply set containing r0 .
So, example pickup rule created ExplainExamples would selected three times,
inhand-nil, clear(X), one height(X) = 2, would create
three new rules (each different literal missing), three singleton N sets,
three candidate new rule sets R0 . Since newly-created r0 generalizations r,
certain cover rs examples, r removed
R0 s.
changes suggested DropLits therefore exactly suggested trimming search ExplainExamples, one crucial difference:
DropLits attempts integrate new rule full rule set, instead making quick comparison default rule Step 2 ExplainExamples.
ExplainExamples used trimming search relatively cheap, local
heuristic allowing decide rule size, DropLits uses search globally
space rule sets, comparing contributions various conflicting rules.
DropRefs operator used deictic references permitted. selects
rule r R deictic reference r. creates new rule r0
removing deictic reference r; N is, again, set containing r0 .
applying operator, pickup rule would selected once, reference
describing , one new rule set would returned: one containing rule
without .
GeneralizeEquality selects rule r R twice equality literal context
create two new rules: one equality replaced , one
replaced . rule integrated rule set R,
resulting two R0 returned. Again, generalized rules certain cover
rs examples, R0 contain r.
context pickup rule contains one equality literal, height(X) = 1. GeneralizeEquality attempt replace literal height(X) 1 height(X) 1.
domain containing two blocks, would likely yield interesting
generalizations.
ChangeRanges selects rule r R n times equality inequality literal
context, n total number values range literal.
time selects r creates new rule r0 replacing numeric value chosen
(in)equality another possible value range. Note quite possible
new rules cover examples, abandoned.
remaining rules integrated new copies rule set usual.
332

fiLearning Symbolic Models Stochastic World Dynamics

Thus, f () ranges [1 . . . n], ChangeRange would, applied rule containing inequality f () < i, construct rule sets replaced
integers [1 . . . n].
pickup rule contains one equality literal, height(X) = 1. two-block domain
(s, a, s0 ) example drawn, height() take values 0, 1, 2,
rule will, again, selected thrice, new rules created containing
new equalities. Since rule constrains X something, new rule containing
height(X) = 0 never cover examples certainly abandoned.
SplitOnLits selects rule r R n times, n number literals
absent rules context deictic references. (The set absent literals
obtained applying available functions predicatesboth primitive
derivedto terms present rule, removing literals already present
rule resulting set.) constructs set new rules. case
predicate inequality literals, creates one rule positive version
literal inserted context, one negative version.
case equality literals, constructs rule every possible value equality could
take. either case, rules cover examples dropped. remaining
rules corresponding one literal placed N , integrated
rule set simultaneously.
Note newly created rules will, them, cover examples
start covered original rule others, examples
split them.
list literals may added pickup rule consists inhand(X),
inhand(Y), table(X), table(Y), clear(Y), on(Y, X), on(Y, Y), on(X, X), height(Y) =?,
possible applications topstack. literals make
interesting examples: adding context create rules either
cover examples all, abandoned, cover set
examples original rule, rejected likelihood
worse penalty. However, illustrate process, attempting add
height(Y) =? predicate result creation three new rules height(Y) = n
context, one n [0, 1, 2]. rules would added rule set
once.
AddLits selects rule r R 2n times, n number predicate-based
literals absent rules context deictic references, 2 reflects fact literal may considered positive negative form.
constructs new rule literal inserting literal earliest place
rule variables well-defined: literal contains deictic
variables, context, otherwise restriction last
deictic variable mentioned literal. (So, V1 V2 deictic variables V1
appears first, on(V1 , V2 ) would inserted restriction V2 .) resulting
rule integrated rule set.
list literals may added pickup rule much SplitOnLits,
without height(Y) =?. Again, process lead anything interesting
333

fiPasula, Zettlemoyer, & Pack Kaelbling

example, reason. illustration, inhand(Y) would
chosen twice, inhand(Y), added context case. Since
context already contains inhand-nil, adding inhand(Y) redundant, adding
inhand(Y) produce contradiction, neither rule seriously considered.
AddRefs operator used deictic references permitted. selects
rule r R n times, n number literals constructed
using available predicates, variables r, new variable v. case,
creates new deictic reference v, using current literal define restriction,
adds deictic reference antecendent r construct new rule,
integrated rule set.
Supposing V new variable, list literals would constructed
pickup rule consists inhand(V), clear(V), on(V, X), on(X, V), table(V), on(V, Y),
on(Y, V), on(V, V), possible applications topstack (which
mirror on.) used create deictic references V : table(V).
(A useful reference here, allows rule describe falls table outcomes
explicitly; operator likely accepted point search.)
RaiseConstants operator used constants permitted. selects
rule r R n times, n number constants among arguments rs
action. constant c, constructs new rule creating new variable
replacing every occurrence c it. integrates new rule rule
set.
SplitVariables operator used constants permitted. selects
rule r R n times, n number variables among arguments rs
action. variable v, goes examples covered rule r
collects constants v binds to. Then, creates rule constants
replacing every occurrence v constant. rules corresponding one
variable v combined set N integrated old rule set together.
found operators consistently used learning.
set operators heuristic, complete sense every rule set
constructed initial rule setalthough, course, guarantee
scoring metric lead greedy search global maximum.
LearnRuless search strategy one large drawback; set learned rules
guaranteed proper training set testing data. New test examples
could covered one rule. happens, employ alternative
rule-selection semantics, return default rule model situation. way,
essentially saying dont know happen. However,
significant problem; problematic test examples always added future training
set used learn better models. Given sufficiently large training set, failures
rare.

334

fiLearning Symbolic Models Stochastic World Dynamics

e1:

B2

B2
B0

puton(B1)

B2
B0

puton(B1)

B0
B1

B1

e3:

r1 :

r2 :

puton(X)
:
(
)
: inhand(Y)
: table(T)
empty
context

0.33 : on(Y, T)
0.33 : on(Y, X)


0.34 : noise

r3 :

puton(X)
:
n

: inhand(Y)
clear(X)
n
1.0 : on(Y, X)

B1

B1

e2:

B0

puton(X)
:
(
)
: inhand(Y)
Z : on(Z, X)
empty
( context
0.5 : on(Y, Z)

0.5 : noise

B2

B2
puton(B1)

B0 B1

B2
B0 B1

Figure 6: Three training examples three blocks world. example paired initial
rule ExplainExamples might create model it. example, agent trying
put block B2 onto block B1.

Example Rule Set Learning

example, consider LearnRuleSet might learn set rules model three
training examples Figure 6, given settings complexity penalty noise bound
later used experiments: = 0.5 pmin = 0.0000001. pmin low
three-block domain, since 25 different states, use consistency.
initialization, rule set contains default rule; changes occur
examples modeled noise. Since examples include change, default rule
noise probability 1.0. describe path greedy search takes.
first round search ExplainExamples operator suggests adding new
rules describe examples. general, ExplainExamples tries construct rules
compact, cover many examples, assign relatively high probability
covered example. (The latter means noise outcomes avoided whenever
possible.) One reasonable set rules suggested shown right-hand side
Figure 6. Notice r3 deterministic, high-probability relatively compact:
e3 unique initial state, ExplainExamples take advantage this. Meanwhile,
335

fiPasula, Zettlemoyer, & Pack Kaelbling

e1 e2 starting state, rules explaining must cover
others examples. Thus, noise outcomes unavoidable rules, since lack
necessary deictic references. (Deictic variables created describe objects
whose state changes example explained.)
Now, consider adding one rules. guarantee
constitute improvement, since high complexity penalty would make rule
look bad, high pmin would make default rule look good. determine
best move is, algorithm compares scores rule sets containing
proposed rules score initial rule set containing default rule. Let us
calculate scores example, starting rule set consisting rule r1 ,
covers e1 e2 , default rule rd , covers remaining example,
therefore noise probability 1.0. use Equation 5, let rules
complexity number literals body: so, case r1 , three. get:
S(r1 , rd ) =

X

log(P (s0 |s, a, r(s,a) ))

(s,a,s0 )E

X

P EN (r)

rR

= log(0.5 + 0.5 pmin ) + log(0.5 pmin ) + log(pmin ) P EN (r1 ) P EN (rd )
= log(0.50000005) + log(0.00000005) + log(0.0000001) 0.5 3 0.5 0
= 0.301 7.301 7 1.5
= 16.101
So, rule set containing r1 score 16.101. Similar calculations show
rule sets containing r2 r3 scores 10.443 15.5 respectively. Since
initial rule set score 21, new rule sets improvements, one
containing r2 best, picked greedy search. new rule set now:


puton(X) : : inhand(Y), : table(T)
empty context

0.33 : on(Y, T)
0.33 : on(Y, X)


0.34 : noise
default
rule:
1.0 : change

0.0 : noise

Notice training examples covered non-default rule. situation,
default rule cover examples probability assigned noise
outcome.
next step, search decide altering existing rule, introducing another rule describe example currently covered default rule. Since
default rule covers examples, altering single rule rule set option.
operators likely score highly get rid noise outcome,
rule means referring block X e1 .
appropriate operator therefore AddRefs, introduce new deictic reference describing block. course, increases size rule, complexity,
336

fiLearning Symbolic Models Stochastic World Dynamics

addition means rule longer applies e3 , leaving example
handled default rule. However, new rule set raises probabilities
examples enough compensate increase complexity, ends
score 10.102, clear improvement 10.443. highest score
obtainable step, algorithm alters rule set get:
puton(X) :



: inhand(Y), : table(T), Z : on(Z, X)



empty context

0.5 : on(Y, Z)

0.5 : on(Y, T)
default
rule:
0.0 : change

1.0 : noise

default rule covers e3 , ExplainExamples something work again.
Adding r3 get rid noise, yield much improved score 4.602. Again,
biggest improvement made, rule set becomes:
puton(X) :



: inhand(Y), : table(T), Z : on(Z, X)



empty context

0.5 : on(Y, Z)

0.5 : on(Y, T)


puton(X) : : inhand(Y)
clear(X)

1.0 : on(Y, X)
default
rule:
1.0 : change

0.0 : noise

Note rule could added earlier e3 covered
first rule added, r2 , specialized. Thus, adding r3 rule set containing r2
would knocked r2 out, caused examples e1 e2 explained noise
default rule, would reduced overall score. (It is, however, possible rule
knock another yet improve score: requires complicated set
examples.)
Learning continues search. Attempts apply rule-altering operators
current rules either make bigger without changing likelihood, lead
creation noise outcomes. Dropping either rule add noise probability
default rule lower score. Since extra examples explained,
operator improve score, search stops rule set. seems
reasonable rule set domain: one rule covers happens try puton
clear block, one describes try puton block another block it.
Ideally, would first rule generalize blocks something them,
instead on, notice would need examples containing higher stacks.
337

fiPasula, Zettlemoyer, & Pack Kaelbling

5.1.6 Different Versions Algorithm
making small variations LearnRuleSet algorithm, learn different types
rule sets. important evaluating algorithm.
explore effects constants rules, evaluate three different versions
rule learning: propositional, relational, deictic. propositional rule learning, ExplainExamples creates initial trimmed rules constants never introduces variables.
None search operators introduce variables used. Thus, learned rules
guaranteed propositionalthey cannot generalize across identities specific
objects. relational rule learning, variables allowed rule action arguments
search operators allowed introduce deictic references. ExplainExamples creates
rules constants name objects, long constants already
variable action argument list mapped them. Finally, deictic rule learning,
constants allowed. see deictic learning provides strong bias
improve generalization.
demonstrate addition noise deictic references result better
rules, learn action models enhancements. Again,
done changing algorithm minor ways. disallow noise, set rule noise
probability zero, means must constrain outcome sets contain
outcome every example change observed; rules cannot express
changes abandoned. disallow deictic references, disable operators
introduce them, ExplainExamples create empty deictic reference set.
5.2 Learning Concepts
contexts deictic references NDRs make use concept predicates functions well primitive ones. concepts specified hand, learned using
rather simple algorithm, LearnConcepts, uses LearnRuleSet subprocedure
testing concept usefulness. algorithm works constructing increasingly complex concepts, running LearnRuleSet checking concepts appear learned
rules. first set created applying operators Figure 7 literals built
original language. Subsequent sets concepts constructed using literals
proved useful latest run; concepts tried before, always
true always false across examples, discarded. search ends none
new concepts prove useful.
example, consider predicate topstack simple blocks world, could
discovered follows. first round learning, literal on(X1 , X2 ) used define
new predicate n(Y1 , Y2 ) := (Y1 , Y2 ), true Y1 stacked Y2 .
Assuming new predicate appears learned rules, used second
round learning, define, among others, m(Z1 , Z2 ) := n(Z1 , Z2 ) clear(Z1 ). ensuring
Z1 clear, predicate true Z1 highest block stack
containing Z2 . notion topstack used determining happen
gripper tries pick Z2 . descends above, likely grasp
block top stack instead.
Since concept language quite rich, overfitting (e.g., learning concepts
used identify individual examples) serious problem. handle
338

fiLearning Symbolic Models Stochastic World Dynamics

p(X) n := QY.p(Y )
p(X1 , X2 ) n(Y2 ) := QY1 .p(Y1 , Y2 )
p(X1 , X2 ) n(Y1 ) := QY2 .p(Y1 , Y2 )
p(X1 , X2 ) n(Y1 , Y2 ) := p (Y1 , Y2 )
p(X1 , X2 ) n(Y1 , Y2 ) := p+ (Y1 , Y2 )
p1 (X1 ), p2 (X2 ) n(Y1 ) := p1 (Y1 ) p2 (Y1 )
p1 (X1 ), p2 (X2 , X3 ) n(Y1 , Y2 ) := p1 (Y1 ) p2 (Y1 , Y2 )
p1 (X1 ), p2 (X2 , X3 ) n(Y1 , Y2 ) := p1 (Y1 ) p2 (Y2 , Y1 )
p1 (X1 , X2 ), p2 (X3 , X4 ) n(Y1 , Y2 ) := p1 (Y1 , Y2 ) p2 (Y1 , Y2 )
p1 (X1 , X2 ), p2 (X3 , X4 ) n(Y1 , Y2 ) := p1 (Y1 , Y2 ) p2 (Y2 , Y1 )
p1 (X1 , X2 ), p2 (X3 , X4 ) n(Y1 , Y2 ) := p1 (Y1 , Y2 ) p2 (Y1 , Y1 )
p1 (X1 , X2 ), p2 (X3 , X4 ) n(Y1 , Y2 ) := p1 (Y1 , Y2 ) p2 (Y2 , Y2 )
f (X) = c n() := #Y.f (Y ) = c
f (X) c n() := #Y.f (Y ) c
f (X) c n() := #Y.f (Y ) c
Figure 7: Operators used invent new predicate n. operator takes input one
literals, listed left. ps represent old predicates; f represents old function;
Q refer ; c numerical constant. operator takes literal
returns concept definition. operators applied literals used
rules rule set create new predicates.

expected way: introducing penalty term, 0 c(R), create new scoring metric
0 (R) = S(R) 0 c(R)
c(R) number distinct concepts used rule set R 0 scaling
parameter. new metric 0 used LearnRuleSet; avoids overfitting favoring
rule sets use fewer derived predicates. (Note fact 0 cannot factored
rule, was, matter, since factoring used InduceOutcomes
LearnParameters, neither change number concepts used relevant
rule: outcomes contain primitive predicates.)
5.3 Discussion
rule set learning challenge addressed section complicated need learn
structure rules, numeric parameters associated outcome distributions,
definitions derived predicates modeling language. LearnConcepts
339

fiPasula, Zettlemoyer, & Pack Kaelbling

algorithm conceptually simple, performs simultaneous learning effectively,
see experiments Section 7.2.
large number possible search operators might cause concern overall
computational complexity LearnRuleSet algorithm. Although algorithm expensive, set search operators designed control complexity attempting
keep number rules current set small possible.
step search, number new rule sets considered depends
current set rules. ExplainExamples operator creates new rule sets,
number examples covered default rule. Since search starts rule set
containing default rule, initially equal number training examples.
However, ExplainExamples designed introduce rules cover many examples,
practice grows small quickly. operators create O(rm) new rule sets,
r number rules current set depends specific operator.
example, could number literals dropped context
rule DropLits operator. Although large, r stays small practice
search starts default rule complexity penalty favors small rule
sets.
ensure score increases search step, algorithm guaranteed converge (usually local) optimum. not, however, guarantee
quickly get there. practice, found algorithm converged quickly
test domains. LearnRuleSet algorithm never took 50 steps
LearnConcepts outer loop never cycled 5 times. entire algorithm never took
six hours run single processor, although significant effort made
cache intermediate computations final implementation.
spite this, realize that, scale complex domains, approach
eventually become prohibitively expensive. plan handle problem developing new algorithms learn concepts, rules, rule parameters online manner,
directed search operators. However, leave complex approach
future work.

6. Planning
experiments Section 7.2 involve learning models complex actions
true models dynamics, level relational rules, available evaluation.
Instead, learned models evaluated planning executing actions.
many possible ways plan. work, explore MDP planning.
MDP (Puterman, 1999) 4-tuple (S, A, T, R). set possible states,
set possible actions, distribution encodes transition dynamics
world, (s0 |s, a). Finally, R reward signal maps every state real value.
policy plan (possibly stochastic) mapping states actions. expected
amount reward achieved executing starting called value
P

defined V (s) = E[
i=0 R(si )|], si states reached
time i. discount factor 0 < 1 favors immediate rewards. goal
MDP planning find policy achieve reward time.
340

fiLearning Symbolic Models Stochastic World Dynamics

optimal policy found solving set Bellman equations,
s, V (s) = R(s) +

X

(s0 |s, (a))V (s0 ).

(8)

s0

application, action set state set defined world
modeling. rule set R defines transition model reward function R defined
hand.
planning large domains, difficult solve Bellman
equations exactly. approximation, implemented simple planner based
sparse sampling algorithm (Kearns, Mansour, & Ng, 2002). Given state s, creates tree
states (of predefined depth branching factor) sampling forward using transition
model, computes value node using Bellman equation, selects action
highest value.
adapt algorithm handle noisy outcomes, predict next state,
estimating value unknown next state fraction value staying
state: i.e., sample forward stayed state scale
value obtain. scaling factor 0.75, depth branching factor
four.
scaling method guess value unknown next state might
be; noisy rules partial models, way compute value explicitly.
future, would explore methods learn associate values noise
outcomes. example, value outcome tower blocks falls
different goal build tall stack blocks goal put
blocks table.
algorithm solve hard combinatorial planning problems, allow
us choose actions maximize relatively simple reward functions. see
next section, enough distinguish good models poor ones. Moreover,
development first-order planning techniques active field research (AIPS, 2006).

7. Evaluation
section, demonstrate rule learning algorithm robust variety lownoise domains, show works intrinsically noisy simulated blocks world
domain. begin describing test domains, report series experiments.
7.1 Domains
experiments performed involve learning rules domains briefly
described following sections.
7.1.1 Slippery Gripper
slippery gripper domain, inspired work Draper et al. (1994), abstract,
symbolic blocks world simulated robotic arm, used move blocks
around table, nozzle, used paint blocks. Painting block
might cause gripper become wet, makes likely fail
manipulate blocks successfully; fortunately, wet gripper dried.
341

fiPasula, Zettlemoyer, & Pack Kaelbling

pickup(X, ) : on(X, ), clear(X),
inhand-nil, block(X), block(Y ), wet,



pickup(X, ) : on(X, ), clear(X),
inhand-nil, block(X), block(Y ), wet




inhand(X), clear(X), inhand-nil,

.7 :
on(X, ), clear(Y )


.2 : on(X, TABLE), on(X, )
.1 : change

inhand(X), clear(X), inhand-nil,

.33 :
on(X, ), clear(Y )


.33 : on(X, TABLE), on(X, )
.34 : change

pickup(X, ) : on(X, ), clear(X),
inhand-nil, block(X), table(Y ), wet
pickup(X, ) : on(X, ), clear(X),
inhand-nil, block(X), table(Y ), wet

(


(


inhand(X), clear(X), inhand-nil,
on(X, )
.5 : change

.5 :

inhand(X), clear(X), inhand-nil,
on(X, )
.2 : change

.8 :


inhand-nil, clear(Y), inhand(X),


.7 :


on(X, Y), clear(X)

puton(X, Y) : clear(Y), inhand(X),
on(X, TABLE), clear(X), inhand-nil,

block(Y)

.2 : inhand(X)


.1 : change

(
puton(X, TABLE) : inhand(X)

(

.6 : painted(X)
.1 : painted(X), wet
.3 : change



.9 : wet
.1 : change

paint(X) : block(X)

dry : context

on(X, TABLE), clear(X), inhand-nil,
inhand(X)
.2 : change
.8 :

Figure 8: Eight relational planning rules model slippery gripper domain.

Figure 8 shows set rules model domain. Individual states represent
world objects intrinsic constants experimental data generated sampling
rules. Section 7.2.1, explore learning algorithms Section 5 compare
number training examples scaled single complex world.
7.1.2 Trucks Drivers
Trucks drivers logistics domain, adapted 2002 AIPS international planning
competition (AIPS, 2002). four types constants: trucks, drivers, locations,
objects. Trucks, drivers objects locations. locations
connected paths links. Drivers board trucks, exit trucks, drive trucks
locations linked. Drivers walk, without truck, locations
connected paths. Finally, objects loaded unloaded trucks.
set rules shown Figure 9. actions simple rules succeed
fail change world. However, walk action interesting twist. drivers
try walk one location another, succeed time,
342

fiLearning Symbolic Models Stochastic World Dynamics

load(O, T, L) :

at(T, L), at(O, L)



.9 : at(O, L), in(O, )
.1 : change

unload(O, T, L) :

in(O, ), at(T, L)



.9 : at(O, L), in(O, )
.1 : change

board(D, T, L) :

at(T, L), at(D, L), empty(T )



.9 : at(D, L), driving(D, ), empty(T )
.1 : change

disembark(D, T, L) :

at(T, L), driving(D, )



.9 : driving(D, ), at(D, L), empty(T )
.1 : change

drive(T, F L, L, D) :

driving(D, ), at(T, F L), link(F L, L)



.9 : at(T, L), at(T, F L)
.1 : change


.9 : at(D, L), at(D, F L)

walk(D, F L, L) :

at(D, F L), path(F L, L)
.1 : pick X s.t. path(F L, X)
at(D, X), at(D, F L)

Figure 9: Six rules encode world dynamics trucks drivers domain.
time arrive randomly chosen location connected path
origin location.
representation presented cannot encode action efficiently. best rule
set rule origin location, outcomes every location origin
linked to. Extending representation allow actions walk represented
single rule interesting area future work.
slippery gripper domain, individual states represent world objects intrinsic
constants experimental data generated sampling rules. trucks
drivers dynamics difficult learn but, see Section 7.2.1, learned
enough training data.
7.1.3 Simulated Blocks World
validate rule extensions paper, Section 7.2 presents experiments rigid
body, simulated physics blocks world. section describes logical interface
simulated world. description extra complexities inherent learning dynamics
world presented Section 1.
define interface symbolic representation use describe
action dynamics physical domain simulated blocks world. perceptual
system produces states contain skolem constants. logical language includes
binary predicate on(X, Y), defined X exerts downward force obtained
querying internal state simulator, unary typing predicates table block.
actuation system translates actions sequences motor commands simulator.
Actions always execute, regardless state world. define two actions;
parameters allow agent specify objects intends manipulate.
pickup(X) action centers gripper X, lowers hits something, grasps,
raises gripper. Analogously, puton(X) action centers gripper X, lowers
encounters pressure, opens it, raises it.
343

fiPasula, Zettlemoyer, & Pack Kaelbling

using simulator sidestepping difficult pixels-to-predicates problem
occurs whenever agent map domain observations internal representation.
Primitive predicates defined terms internal state simulation simpler
cleaner observations real world would be. make domain completely
observable: prerequisite learning planning algorithms. Choosing set
predicates observe important. make rule learning problem easy
hard, difficulty making choice magnified richer settings. limited
language described balances extremes providing on, would difficult
derive means, providing predicates inhand clear,
learned.
7.2 Experiments
section describes two sets experiments. First, compare learning deictic,
relational, propositional rules slippery gripper trucks drivers data.
domains modeled planning rules, contain intrinsic constants, noisy,
thus allow us explore effect deictic references constants rules directly.
Then, describe set experiments learns rules model data simulated
blocks world. data inherently noisy contains skolem constants. result,
focus evaluating full algorithm performing ablation studies demonstrate
deictic references, noise outcomes, concepts required effective learning.
experiments use examples, (s, a, s0 ) E, generated randomly constructing
state s, randomly picking arguments action a, executing action
state generate s0 . distribution used construct biased guarantee that,
approximately half examples, chance change state. method
data generation designed ensure learning algorithms always data
representative entire model learn. Thus, experiments
ignore problems agent would face generate data exploring world.
7.2.1 Learning Rule Sets Noise
know model used generate data, evaluate model respect
set similarly generated test examples E calculating average variational distance
true model P estimate P ,
V D(P, P ) =

1 X
|P (E) P (E)| .
|E| EE

Variational distance suitable measure clearly favors similar distributions,
yet well-defined zero probability event observed. (As happen
non-noisy rule learned sparse data many outcomes
should.)
comparisons performed four actions. first two, paint pickup,
slippery gripper domain, second two, drive walk,
trucks drivers domain. action presents different challenges learning. Paint
simple action one outcome lead successor state (as
described Section 4.1). Pickup complex action must represented
344

fiLearning Symbolic Models Stochastic World Dynamics

Paint Action
0.3
0.25

Pickup Action
0.35

Propositional
Relational
Deictic

Variational Distance

Variational Distance

0.35

0.2
0.15
0.1
0.05
0

0.3
0.25
0.2
0.15
0.1
0.05
0

100 200 300 400 500 600 700 800 9001000
Training set size

100 200 300 400 500 600 700 800 9001000
Training set size

Walk Action
0.3
0.25

Drive Action
0.2

Propositional
Relational
Deictic

Variational Distance

Variational Distance

0.35

Propositional
Relational
Deictic

0.2
0.15
0.1
0.05
0

0.15

Propositional
Relational
Deictic

0.1
0.05
0

100 200 300 400 500 600 700 800 900
Training set size

100 200 300 400 500 600 700 800 900
Training set size

Figure 10: Variational distance function number training examples propositional, relational, deictic rules. results averaged ten trials
experiment. test set size 400 examples.

one planning rule. Drive simple action four arguments. Finally, walk
complicated action uses path connectivity world noise model lost
pedestrians. slippery gripper actions performed world four blocks.
trucks driver actions performed world two trucks, two drivers, two
objects, four locations.
compare three versions algorithm: deictic, includes full rules language allow constants; relational, allows variables constants
deictic references; propositional, constants variables. Figure 10
shows results. relational learning consistently outperforms propositional learning;
implies variable abstractions useful. cases except walk action,
deictic learner outperforms relational learner. result implies forcing
rules contain variables preventing overfitting learning better models.
results walk action interesting. Here, deictic learner cannot actually
represent optimal rule; requires noise model complex. deictic learner
quickly learns best rule can, relational propositional learners eventually
345

fiPasula, Zettlemoyer, & Pack Kaelbling

Learning Simulated Blocksworld
18

learned concepts
hand-engineered concepts
without noise outcomes
restricted language

Total Reward

16
14
12
10
8
6
200

300

400

500
600
700
Training set size

800

900

1000

Figure 11: performance various action model variants function number training
examples. data points averaged five planning trials three
rule sets learned different training data sets. comparison, average reward
performing actions 9.2, reward obtained human directed
gripper averaged 16.2.

learn better rule sets use constants accurately model walkers
moving random locations.
experiments, see variable abstraction helps learn less data,
deictic rules, abstract aggressively, perform best, long
represent model learned. next section, consider deictic
rules, since working domain simulated perception
access objects identities names using skolem constants.
7.2.2 Learning Blocks World Simulator
final experiment demonstrates noise outcomes complicated concepts
necessary learn good action models blocks world simulator.
true model known, evaluate learned model using plan
estimating average reward gets. reward function used simulated blocks
world average height blocks world, breadth depth
search sampling planner four. learning, set 0.5 pmin
0.0000001.
tested four action model variants, varying training set size; results
shown Figure 11. curve labeled learned concepts represents full algorithm
presented paper. performance approaches obtained human expert,
comparable algorithm labeled hand-engineered concepts
346

fiLearning Symbolic Models Stochastic World Dynamics

concept learning, was, instead, provided hand-coded versions concepts
clear, inhand, inhand-nil, above, topstack, height. concept learner discovered
these, well useful predicates, e.g., p(X, Y) := clear(Y) on(Y, X),
call onclear. could action models outperformed hand-engineered ones
slightly small training sets. domains less well-studied blocks world, might
less obvious useful concepts are; concept-discovery technique presented
prove helpful.
remaining two model variants obtained rewards comparable reward
nothing all. (The planner attempt act experiments,
poor job.) one variant, used full set predefined concepts rules
could noise outcomes. requirement explain every action effect led
significant overfitting decrease performance. rule set given
traditional blocks world language, include above, topstack, height,
allowed learn rules noise outcomes. tried full-language variant noise
outcomes allowed, deictic references not: resulting rule sets contained
noisy rules, planner attempt act all. poor performance
ablated versions representation shows three extensions
essential modeling simulated blocks world domain.
human agent commanding gripper solve problem received average
total reward 16.2, theoretical maximum due unexpected action
outcomes. Thus, ND rules performing near-human levels, suggesting
representation reasonable one problem. suggests planning
approximations learning bounds limiting performance. Traditional rules,
face challenge modeling transitions seen data, much larger
hypothesis space consider learning; surprising generalize poorly
consistently out-performed NDRs.
Informally, report NDR algorithms execute significantly faster
traditional ones. one standard desktop PC, learning NDRs takes minutes learning
traditional rules take hours. noisy deictic action models generally
compact traditional ones (they contain fewer rules fewer outcomes) planning
much faster well.
get better feel types rules learned, two interesting rules produced
full algorithm.

pickup(X) :

: onclear(X, Y), Z : on(Y, Z),
: table(T)



inhand-nil, size(X) < 2

.80 : on(Y, Z)
.10 : on(X, Y)


.10 : on(X, Y), on(Y, T), on(Y, Z)

rule applies empty gripper asked pick small block X sits
top another block Y. gripper grabs high probability.

347

fiPasula, Zettlemoyer, & Pack Kaelbling


puton(X) :

: topstack(Y, X), Z : inhand(Z),
: table(T)

size(Y) < 2


.62 :

.12 :

.04 :



.22 :



on(Z, Y)
on(Z, T)
on(Z, T), on(Y, T), on(Y, X)
noise

rule applies gripper asked put contents, Z, block X
inside stack topped small block Y. placing things small block chancy,
reasonable probability Z fall table, small probability
follow.

8. Discussion
paper, developed probabilistic action model representation rich enough
used learn models planning physically simulated blocks world.
first step towards defining representations algorithms enable learning
complex worlds.
8.1 Related Work
problem learning deterministic action models well studied. work
area (Shen & Simon, 1989; Gil, 1993, 1994; Wang, 1995) focused incrementally
learning planning operators interacting simulated worlds. However, work
assumes learned models completely deterministic.
Oates Cohen (1996) earliest work learning probabilistic planning operators. rules factored apply parallel. However, representation
strictly propositional, allows rule contain single outcome.
previous work, developed algorithms learning probabilistic relational planning operators (Pasula, Zettlemoyer, & Kaelbling, 2004). Unfortunately, neither probabilistic
algorithms robust enough learn complex, noisy environments simulated
blocks world.
One previous system comes close goal TRAIL learner (Benson, 1996).
TRAIL learns extended version Horn clauses noisy environments applying inductive logic programming (ILP) learning techniques robust noise. TRAIL
introduced deictic references name objects based functional relationships
arguments actions. deictic references, exists-unique quantification
semantics, generalization Bensons original work. Moreover, TRAIL models continuous actions real-valued fluents, allows represent complex
models date, including knowledge required pilot realistic flight simulator. However, rules TRAIL learns limited probabilistic representation
represent possible transition distributions. TRAIL include mechanisms
learning new predicates.
348

fiLearning Symbolic Models Stochastic World Dynamics

work action model learning used different versions greedy search
rule structure learning, closely related inspired learning version
spaces Mitchell (1982) later ILP work (Lavrac & Dzeroski, 1994). paper,
explore, first time, new way moving space rule sets
using noise rule initial rule set. found approach works well
practice, avoiding need hand-selected initial rule set allowing algorithm
learn significantly complex environments.
far know, work learning action models explored learning concepts.
ILP literature, recent work (Assche, Vens, Blockeel, & Dzeroski, 2004) shown
adding concept learning decision tree learning algorithms improves classification
performance.
Outside action learning, exists much related research learning probabilistic
models relational logical structure. complete discussion beyond scope
paper, present highlights. work learns representations
relational extension Bayesian networks. comprehensive example, see work Getoor
(2001). work extends research ILP incorporating probabilistic dependencies.
example, see wide range techniques presented Kersting (2006). Additionally,
recent work learning Markov logic networks (Richardson & Domingos, 2006; Kok
& Domingos, 2005), log-linear models features defined first-order
logical formulae. action models action model learning algorithms paper
designed represent action effects, special case general approaches listed
above. discussed Section 2, tailoring representation match
model learnt, simplify learning.
Finally, let us consider work related NDR action model representation.
relevant approach PPDDL, representation language probabilistic planning operators
problem domains (Younes & Littman, 2004). NDR representation partially
inspired PPDDL operators includes restrictions make easier learn extensions, noise outcomes, required effectively model simulated blocks
world. future, algorithms paper could extended learn full PPDDL
rules. Also, PPDDL planning algorithms (for examples, see papers recent planning
competitions) could adapted improve simple planning presented Section 6.
general sense, NDRs related probabilistic relational representations
designed model dependencies across time. examples, see work relational
dynamic Bayesian networks (Sanghai, Domingos, & Weld, 2005), specialization PRMs, logical hidden Markov models (Kersting, Raedt, & Raiko, 2006),
come ILP research tradition. approaches make different set modeling
assumptions closely tied planning representations NDR models
extend.
8.2 Future Ongoing Work
remains much done context learning probabilistic planning rules.
First all, likely work applied additional domains (such
realistic robotic applications dialogue systems) representation need
adapted, search operators adjusted accordingly. possible changes mentioned
349

fiPasula, Zettlemoyer, & Pack Kaelbling

article include allowing rules apply parallel, different rules could apply
different aspects state, extending outcomes include quantifiers, actions
walk, trucks drivers domain Section 7.1.2, could described using
single rule. significant change intend pursue expanding approach
handle partial observability, possibly incorporating techniques work
deterministic learning (Amir, 2005). hope make changes make
using rules easier, associating values noise outcomes help planner
decide whether avoided.
second research direction involves development new algorithms learn probabilistic operators incremental, online manner, similar learning setup
deterministic case (Shen & Simon, 1989; Gil, 1994; Wang, 1995). potential
scale approach larger domains, make applicable even situations
difficult obtain set training examples contains reasonable sampling worlds
likely relevant agent. line work require development
techniques effectively exploring world learning model, much done
reinforcement learning. longer term, would online algorithms learn
operators concept predicates, useful primitive predicates motor
actions.

Acknowledgments
material based upon work supported part Defense Advanced Research
Projects Agency (DARPA), Department Interior, NBC, Acquisition
Services Division, Contract No. NBCHD030010; part DARPA Grant No.
HR0011-04-1-0012 .

References
Agre, P., & Chapman, D. (1987). Pengi: implementation theory activity.
Proceedings Sixth National Conference Artificial Intelligence (AAAI).
AIPS (2002). International planning competition. http://www.dur.ac.uk/d.p.long/competition.html.
AIPS (2006). International planning competition. http://www.ldc.usb.ve/bonet/ipc5/.
Amir, E. (2005). Learning partially observable deterministic action models. Proceedings
Nineteenth International Joint Conference Artificial Intelligence (IJCAI).
Assche, A. V., Vens, C., Blockeel, H., & Dzeroski, S. (2004). random forest approach
relational learning. Proceedings ICML Workshop Statistical Relational
Learning Connections Fields.
Benson, S. (1996). Learning Action Models Reactive Autonomous Agents. Ph.D. thesis,
Stanford University.
Bertsekas, D. P. (1999). Nonlinear Programming. Athena Scientific.
Blum, A., & Langford, J. (1999). Probabilistic planning graphplan framework.
Proceedings Fifth European Conference Planning (ECP).
350

fiLearning Symbolic Models Stochastic World Dynamics

Boutilier, C., Reiter, R., & Price, B. (2001). Symbolic dynamic programming first-order
MDPs. Proceedings Seventeenth International Joint Conference Artificial
Intelligence (IJCAI).
Boyen, X., & Koller, D. (1998). Tractable inference complex stochastic processes.
Proceedings Fourteenth Annual Conference Uncertainty AI (UAI).
Brooks, R. A. (1991). Intelligence without representation. Artificial Intelligence, 47.
Draper, D., Hanks, S., & Weld, D. (1994). Probabilistic planning information gathering
contingent execution. Proceedings Second International conference
AI Planning Systems (AIPS).
Edelkamp, S., & Hoffman, J. (2004). PDDL2.2: language classical part 4th
international planning competition. Technical Report 195, Albert-Ludwigs-Universitat,
Freiburg, Germany.
Fikes, R. E., & Nilsson, N. J. (1971). STRIPS: new approach application theorem
proving problem solving. Artificial Intelligence, 2(2).
Getoor, L. (2001). Learning Statistical Models Relational Data. Ph.D. thesis, Stanford.
Gil, Y. (1993). Efficient domain-independent experimentation. Proceedings Tenth
International Conference Machine Learning (ICML).
Gil, Y. (1994). Learning experimentation: Incremental refinement incomplete planning domains. Proceedings Eleventh International Conference Machine
Learning (ICML).
Guestrin, C., Koller, D., Parr, R., & Venkataraman, S. (2003). Efficient solution algorithms
factored MDPs. Journal Artificial Intelligence Research (JAIR), 19.
Kearns, M., Mansour, Y., & Ng, A. (2002). sparse sampling algorithm near-optimal
planning large Markov decision processes. Machine Learning (ML), 49(2).
Kersting, K. (2006). Inductive Logic Programming Approach Statistical Relational
Learning. IOS Press.
Kersting, K., Raedt, L. D., & Raiko, T. (2006). Logical hidden markov models. Journal
Artificial Intelligence Research (JAIR), 25.
Khan, K., Muggleton, S., & Parson, R. (1998). Repeat learning using predicate invention.
International Workshop Inductive Logic Programming (ILP).
Kok, S., & Domingos, P. (2005). Learning structure markov logic networks. Proceedings Twenty Second International Conference Machine Learning (ICML).
Lavrac, N., & Dzeroski, S. (1994). Inductive Logic Programming Techniques Applications. Ellis Horwood.
Mitchell, T. M. (1982). Generalization search. Artificial Intelligence, 18(2).
Oates, T., & Cohen, P. R. (1996). Searching planning operators context-dependent
probabilistic effects. Proceedings Thirteenth National Conference
Artificial Intelligence (AAAI).
ODE (2004). Open dynamics engine toolkit.. http://opende.sourceforge.net.
351

fiPasula, Zettlemoyer, & Pack Kaelbling

Pasula, H., Zettlemoyer, L., & Kaelbling, L. (2004). Learning probabilistic relational planning rules. Proceedings Fourteenth International Conference Automated
Planning Scheduling (ICAPS).
Puterman, M. L. (1999). Markov Decision Processes. John Wiley Sons, New York.
Richardson, M., & Domingos, P. (2006). Markov logic networks. Machine Learning (ML),
62.
Sanghai, S., Domingos, P., & Weld, D. (2005). Relational dynamic bayesian networks.
Journal Artificial Intelligence Research (JAIR), 24.
Shen, W.-M., & Simon, H. A. (1989). Rule creation rule learning environmental exploration. Proceedings Eleventh International Joint Conference
Artificial Intelligence (IJCAI).
Wang, X. (1995). Learning observation practice: incremental approach planning operator acquisition. Proceedings Twelfth International Conference
Machine Learning (ICML).
Yoon, S., Fern, A., & Givan, R. (2002). Inductive policy selection first-order Markov
decision processes. Proceedings Eighteenth Conference Uncertainty
Artificial Intelligence (UAI).
Younes, H. L. S., & Littman, M. L. (2004). PPDDL1.0: extension PDDL expressing
planning domains probabilistic effects. School Computer Science, Carnegie
Mellon University, Technical Report CMU-CS-04-167.
Zettlemoyer, L., Pasula, H., & Kaelbling, L. (2003). Learning probabilistic relational planning rules. MIT Tech Report.

352


