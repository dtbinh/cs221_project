Journal Artificial Intelligence Research 29 (2007) 153-190

Submitted 10/06; published 06/07

Generalized A* Architecture
Pedro F. Felzenszwalb

pff@cs.uchicago.edu

Department Computer Science
University Chicago
Chicago, IL 60637

David McAllester

mcallester@tti-c.org

Toyota Technological Institute Chicago
Chicago, IL 60637

Abstract
consider problem computing lightest derivation global structure using
set weighted rules. large variety inference problems AI formulated
framework. generalize A* search heuristics derived abstractions
broad class lightest derivation problems. describe new algorithm searches
lightest derivations using hierarchy abstractions. generalization A* gives
new algorithm searching AND/OR graphs bottom-up fashion.
discuss algorithms described provide general architecture addressing pipeline problem problem passing information back forth
various stages processing perceptual system. consider examples computer vision natural language processing. apply hierarchical search algorithm
problem estimating boundaries convex objects grayscale images compare
search methods. second set experiments demonstrate use new
compositional model finding salient curves images.

1. Introduction
consider class problems defined set weighted rules composing structures
larger structures. goal problems find lightest (least cost) derivation
global structure derivable given rules. large variety classical inference
problems AI expressed within framework. example global structure
might parse tree, match deformable object model image, assignment
values variables Markov random field.
define lightest derivation problem terms set statements, set weighted
rules deriving statements using statements special goal statement.
case looking lightest derivation goal statement. usually express
lightest derivation problem using rule schemas implicitly represent large set
rules terms small number rules variables. Lightest derivation problems
formally equivalent search AND/OR graphs (Nilsson, 1980), find
formulation natural applications interested in.
One goals research construction algorithms global optimization
across many levels processing perceptual system. described algorithms
used integrate multiple stages processing pipeline single global optimization problem solved efficiently.
c
2007
AI Access Foundation. rights reserved.

fiFelzenszwalb & McAllester

Dynamic programming fundamental technique designing efficient inference algorithms. Good examples Viterbi algorithm hidden Markov models (Rabiner,
1989) chart parsing methods stochastic context free grammars (Charniak, 1996).
algorithms described used speed solution problems normally
solved using dynamic programming. demonstrate specific problem,
goal estimate boundary convex object cluttered image. second set
experiments show algorithms used find salient curves images.
describe new model salient curves based compositional rule enforces
long range shape constraints. leads problem large solved using
classical dynamic programming methods.
algorithms consider related Dijkstras shortest paths algorithm (DSP)
(Dijkstra, 1959) A* search (Hart, Nilsson, & Raphael, 1968). DSP A*
used find shortest path cyclic graph. use priority queue define order
nodes expanded worst case running time O(M log N ) N
number nodes graph number edges. DSP A*
expansion node v involves generating nodes u edge v u.
difference two methods A* uses heuristic function avoid
expanding non-promising nodes.
Knuth gave generalization DSP used solve lightest derivation
problem cyclic rules (Knuth, 1977). call Knuths lightest derivation algorithm
(KLD). analogy Dijkstras algorithm, KLD uses priority queue define order
statements expanded. expansion statement v involves generating
conclusions derived single step using v statements already
expanded. long rule bounded number antecedents KLD worst
case running time O(M log N ) N number statements problem
number rules. Nilssons AO* algorithm (1980) used solve
lightest derivation problems. Although AO* use heuristic function, true
generalization A* use priority queue, handles acyclic rules,
require O(M N ) time even applied shortest path problem.1 particular, AO*
variants use backward chaining technique starts goal repeatedly
refines subgoals, A* forward chaining algorithm.2
Klein Manning (2003) described A* parsing algorithm similar KLD
use heuristic function. One contributions generalization algorithm
arbitrary lightest derivation problems. call algorithm A* lightest derivation
(A*LD). method forward chaining, uses priority queue control order
statements expanded, handles cyclic rules worst case running time
O(M log N ) problems rule small number antecedents. A*LD
seen true generalization A* lightest derivation problems. lightest derivation
problem comes shortest path problem A*LD identical A*.
course running times seen practice often well predicted worst case
analysis. specially true problems large defined implicitly.
example, use dynamic programming solve shortest path problem acyclic
graph O(M ) time. better O(M log N ) bound DSP, implicit
1. extensions handle cyclic rules (Jimenez & Torras, 2000).
2. AO* backward chaining terms inference rules defining lightest derivation problem.

154

fiThe Generalized A* Architecture

graphs DSP much efficient since expands nodes best-first order.
searching shortest path source goal, DSP expand nodes v
d(v) w . d(v) length shortest path source v, w
length shortest path source goal. case A* monotone
admissible heuristic function, h(v), possible obtain similar bound searching
implicit graphs. A* expand nodes v d(v) + h(v) w .
running time KLD A*LD expressed similar way. solving
lightest derivation problem, KLD expand statements v d(v) w . d(v)
weight lightest derivation v, w weight lightest derivation
goal statement. Furthermore, A*LD expand statements v d(v) + h(v) w .
heuristic function, h(v), gives estimate additional weight necessary
deriving goal statement using derivation v. heuristic values used A*LD
analogous distance node goal graph search problem (the notion
used A*). note heuristic values significantly different ones
used AO*. case AO* heuristic function, h(v), would estimate weight
lightest derivation v.
important difference A*LD AO* A*LD computes derivations
bottom-up fashion, AO* uses top-down approach. method advantages, depending type problem solved. example, classical problem
computer vision involves grouping pixels long smooth curves. formulate
problem terms finding smooth curves pairs pixels far apart.
image n pixels (n2 ) pairs. straight forward implementation
top-down algorithm would start considering (n2 ) possibilities. bottomup algorithm would start O(n) pairs nearby pixels. case expect
bottom-up grouping method would efficient top-down method.
classical AO* algorithm requires set rules acyclic. Jimenez Torras
(2000) extended method handle cyclic rules. Another top-down algorithm
handle cyclic rules described Bonet Geffner (2005). Hansen Zilberstein (2001)
described search algorithm problems optimal solutions
cyclic. algorithms described paper handle problems cyclic rules
require optimal solutions acyclic. note AO* handle rules
non-superior weight functions (as defined Section 3) KLD requires superior weight
functions. A*LD replaces requirement requirement heuristic function.
well known method defining heuristics A* consider abstract relaxed
search problem. example, consider problem solving Rubiks cube small
number moves. Suppose ignore edge center pieces solve corners.
example problem abstraction. number moves necessary put
corners good configuration lower bound number moves necessary solve
original problem. fewer corner configurations full configurations
makes easier solve abstract problem. general, shortest paths
goal abstract problem used define admissible monotone heuristic
function solving original problem A*.
show abstractions used define heuristic functions A*LD.
lightest derivation problem notion shortest path goal replaced
notion lightest context, context statement v derivation
155

fiFelzenszwalb & McAllester

goal hole filled derivation v. computation lightest
abstract contexts lightest derivation problem.
Abstractions related problem relaxations defined Pearl (1984). abstractions often lead small problems solved search, relaxations lead
problems still large state space may simple enough solved closed
form. definition abstractions use lightest derivation problems includes
relaxations special case.
Another contribution work hierarchical search method call HA*LD.
algorithm effectively use hierarchy abstractions solve lightest derivation
problem. algorithm novel even case classical search (shortest paths) problem. HA*LD searches lightest derivations contexts every level abstraction
simultaneously. specifically, level abstraction set statements
rules. search lightest derivations contexts level controlled
single priority queue. understand running time HA*LD, let w weight
lightest derivation goal original (not abstracted) problem. statement v
abstraction hierarchy let d(v) weight lightest derivation v level
abstraction. Let h(v) weight lightest context abstraction v (defined
one level v hierarchy). Let K total number statements
hierarchy d(v) + h(v) w . HAL*D expands 2K statements solving
original problem. factor two comes fact algorithm computes
derivations contexts level abstraction.
Previous algorithms use abstractions solving search problems include methods based pattern databases (Culberson & Schaeffer, 1998; Korf, 1997; Korf & Felner,
2002), Hierarchical A* (HA*, HIDA*) (Holte, Perez, Zimmer, & MacDonald, 1996; Holte,
Grajkowski, & Tanner, 2005) coarse-to-fine dynamic programming (CFDP) (Raphael,
2001). Pattern databases made possible compute solutions impressively large
search problems. methods construct lookup table shortest paths node
goal abstract states. practice approach limited tables remain
fixed different problem instances, relatively small tables heuristic must
recomputed instance. example, Rubiks cube precompute
number moves necessary solve every corner configuration. table used
define heuristic function solving full configuration Rubiks cube.
HA* HIDA* use hierarchy abstractions avoid searching nodes
level hierarchy. hand, directed graphs methods may still
expand abstract nodes arbitrarily large heuristic values. clear
generalize HA* HIDA* lightest derivation problems rules
one antecedent. Finally, CFDP related AO* repeatedly solves ever
refined problems using dynamic programming. leads worst case running time
O(N ). discuss relationships HA*LD hierarchical
methods detail Section 8.
note A* search related algorithms previously used solve
number problems classical state space search problems. includes
traveling salesman problem (Zhang & Korf, 1996), planning (Edelkamp, 2002), multiple
sequence alignment (Korf, Zhang, Thayer, & Hohwald, 2005), combinatorial problems
graphs (Felner, 2005) parsing using context-free-grammars (Klein & Manning, 2003).
156

fiThe Generalized A* Architecture

work Bulitko, Sturtevant, Lu, Yau (2006) uses hierarchy state-space abstractions real-time search.
1.1 Pipeline Problem
major problem artificial intelligence integration multiple processing stages
form complete perceptual system. call pipeline problem. general
concatenation systems stage feeds information next. vision,
example, might edge detector feeding information boundary finding system,
turn feeds information object recognition system.
computational constraints need build modules clean interfaces
pipelines often make hard decisions module boundaries. example, edge detector
typically constructs Boolean array indicates weather edge detected
image location. general recognition presence edge
certain location depend context around it. People often see edges places
image gradient small if, higher cognitive level, clear actually
object boundary location. Speech recognition systems try address problem
returning n-best lists, may may contain actual utterance. would
speech recognition system able take high-level information account
avoid hard decision exactly strings output n-best list.
processing pipeline specified describing stages terms rules
constructing structures using structures produced previous stage. vision system
one stage could rules grouping edges smooth curves next stage could
rules grouping smooth curves objects. case construct single
lightest derivation problem representing entire system. Moreover, hierarchical set
abstractions applied entire pipeline. using HA*LD compute lightest
derivations complete scene interpretation derived one level abstraction guides
processing stages concrete level. provides mechanism enables coarse
high-level processing guide low-level computation. believe important
property implementing efficient perceptual pipelines avoid making hard decisions
processing stages.
note formulation complete computer vision system lightest derivation problem related work Geman, Potter, Chi (2002), Tu, Chen, Yuille,
Zhu (2005) Jin Geman (2006). papers image understanding posed
parsing problem, goal explain image terms set objects
formed (possibly recursive) composition generic parts. Tu et al. (2005) use
data driven MCMC compute optimal parses Geman et al. (2002) Jin
Geman (2006) use bottom-up algorithm building compositions greedy fashion.
Neither methods guaranteed compute optimal scene interpretation.
hope HA*LD provide principled computational technique solving large
parsing problems defined compositional models.
1.2 Overview
begin formally defining lightest derivation problems Section 2. section
discusses dynamic programming relationship lightest derivation problems
157

fiFelzenszwalb & McAllester

AND/OR graphs. Section 3 describe Knuths lightest derivation algorithm.
Section 4 describe A*LD prove correctness. Section 5 shows abstractions
used define mechanically constructed heuristic functions A*LD. describe
HA*LD Section 6 discuss use solving pipeline problem Section 7. Section 8 discusses relationship HA*LD hierarchical search methods.
Sections 9 10 present experimental results. conclude Section 11.

2. Lightest Derivation Problems
Let set statements R set inference rules following form,
A1 = w1
..
.
= wn
C = g(w1 , . . . , wn )
antecedents Ai conclusion C statements , weights wi
non-negative real valued variables g non-negative real valued weight function.
rule antecedents function g simply non-negative real value. Throughout
paper use A1 , . . . , g C denote inference rule type.
derivation C finite tree rooted rule A1 , . . . , g C n children,
i-th child derivation Ai . leaves tree rules antecedents.
Every derivation weight value obtained recursive application
functions g along derivation tree. Figure 1 illustrates derivation tree.
Intuitively rule A1 , . . . , g C says derive antecedents Ai
weights wi derive conclusion C weight g(w1 , . . . , wn ). problem
interested compute lightest derivation special goal statement.
algorithms discussed paper assume weight functions g associated lightest derivation problem non-decreasing variable.
fundamental property ensuring lightest derivations optimal substructure property. case lightest derivations constructed lightest derivations.
facilitate runtime analysis algorithms assume every rule small
number antecedents. use N denote number statements lightest derivation
problem, denotes number rules. problems interested
N large problem implicitly defined compact way,
using small number rules variables examples below. assume
N since statements conclusion rule clearly
derivable ignored.
2.1 Dynamic Programming
say set rules acyclic ordering statements
rule conclusion C antecedents statements come C
ordering. Dynamic programming used solve lightest derivation problem
158

fiThe Generalized A* Architecture

A1
A2
A3
C

Derivation
A1

Derivation
A2

Derivation
A3

Figure 1: derivation C tree rules rooted rule r conclusion C.
children root derivations antecedents r. leafs tree
rules antecedents.

functions g rule non-decreasing set rules acyclic. case
lightest derivations computed sequentially terms acyclic ordering O.
i-th step lightest derivation i-th statement obtained minimizing rules
used derive statement. method takes O(M ) time compute
lightest derivation statement .
note cyclic rules sometimes possible compute lightest derivations
taking multiple passes statements. note authors would refer
Dijkstras algorithm (and KLD) dynamic programming method. paper
use term referring algorithms compute lightest derivations fixed
order independent solutions computed along way (this includes recursive
implementations use memoization).
2.2 Examples
Rules computing shortest paths single source weighted graph shown
Figure 2. assume given weighted graph G = (V, E), wxy
non-negative weight edge (x, y) E distinguished start node. first
rule states path weight zero start node s. second set rules
state path node x extend path edge x
obtain appropriately weighted path node y. rule type
edge graph. lightest derivation path(x) corresponds shortest path
x. Note general graphs rules cyclic. Figure 3 illustrates graph
159

fiFelzenszwalb & McAllester

(1)

path(s) = 0
(2) (x, y) E,
path(x) = w
path(y) = w + wxy
Figure 2: Rules computing shortest paths graph.

b

c

path(d) = w

path(c) = w

path(b) = w + wdb

path(b) = w + wcb

path(s) = w

path(e) = w

path(d) = w + wsd

path(c) = w + wec



e

path(s) = w
path(s) = 0
path(e) = w + wse

path(s) = 0

Figure 3: graph two highlighted paths b corresponding derivations
using rules Figure 2.

two different derivations path(b) using rules described. corresponds
two different paths b.
Rules chart parsing shown Figure 4. assume given weighted
context free grammar Chomsky normal form (Charniak, 1996), i.e., weighted set
productions form X X Z X, Z nonterminal symbols
terminal symbol. input string given sequence terminals (s1 , . . . , sn ).
160

fiThe Generalized A* Architecture

(1) production X si ,

phrase(X, i, + 1) = w(X si )
(2) production X Z 1 < j < k n + 1,
phrase(Y, i, j) = w1
phrase(Z, j, k) = w2
phrase(X, i, k) = w1 + w2 + w(X Z)
Figure 4: Rules parsing context free grammar.
first set rules state grammar contains production X si
phrase type X generating i-th entry input weight w(X si ). second
set rules state grammar contains production X Z phrase
type j phrase type Z j k an, appropriately
weighted, phrase type X k. Let start symbol grammar.
goal parsing find lightest derivation phrase(S, 1, n + 1). rules acyclic
phrases composed together form longer phrases.
2.3 AND/OR Graphs
Lightest derivation problems closely related AND/OR graphs. Let R set
statements rules defining lightest derivation problem. convert problem
AND/OR graph representation build graph disjunction node
statement conjunction node rule R. edge statement rule deriving statement, edge rule antecedents.
leaves AND/OR graph rules antecedents. derivations
statement using rules R represented solutions rooted statement
corresponding AND/OR graph. Conversely, possible represent AND/OR
graph search problem lightest derivation problem. case view node
graph statement build appropriate set rules R.

3. Knuths Lightest Derivation
Knuth (1977) described generalization Dijkstras shortest paths algorithm call
Knuths lightest derivation (KLD). Knuths algorithm used solve large class
lightest derivation problems. algorithm allows rules cyclic requires
weight functions associated rule non-decreasing superior. Specifically
require following two properties weight function g rule,
non-decreasing:
superior:

wi0 wi g(w1 , . . . , wi0 , . . . , wn ) g(w1 , . . . , wi , . . . , wn )
g(w1 , . . . , wn ) wi
161

fiFelzenszwalb & McAllester

example,
g(x1 , . . . , xn ) = x1 + + xn
g(x1 , . . . , xn ) = max(x1 , . . . , xn )
non-decreasing superior functions.
Knuths algorithm computes lightest derivations non-decreasing weight order. Since
interested lightest derivation special goal statement often stop
algorithm computing lightest derivation every statement.
weight assignment expression form (B = w) B statement
w non-negative real value. say weight assignment (B = w) derivable
derivation B weight w. set rules R, statement B, weight
w write R ` (B = w) rules R used derive (B = w). Let `(B, R)
infimum set weights derivable B,
`(B, R) = inf{w : R ` (B = w)}.
Given set rules R statement goal interested computing derivation
goal weight `(goal , R).
define bottom-up logic programming language easily express
algorithms wish discuss throughout rest paper. algorithm defined
set rules priorities. encode priority rule writing along
line separating antecedents conclusion follows,
A1 = w1
..
.
= wn
p(w1 , . . . , wn )
C = g(w1 , . . . , wn )
call rule form prioritized rule. execution set prioritized rules
P defined procedure Figure 5. procedure keeps track set
priority queue Q weight assignments form (B = w). Initially empty Q
contains weight assignments defined rules antecedents priorities given
rules. iteratively remove lowest priority assignment (B = w) Q. B
already assigned weight new assignment ignored. Otherwise add
new assignment expand every assignment derivable (B = w)
assignments already using rule P added Q priority specified
rule. procedure stops queue empty.
result executing set prioritized rules set weight assignments. Moreover,
procedure implicitly keep track derivations remembering assignments
used derive item inserted queue.
Lemma 1. execution finite set prioritized rules P derives every statement
derivable rules P .
Proof. rule causes one item inserted queue. Thus eventually Q
empty algorithm terminates. Q empty every statement derivable
162

fiThe Generalized A* Architecture

Procedure Run(P )
1.
2. Initialize Q assignments defined rules antecedents priorities
3. Q empty
4.
Remove lowest priority element (B = w) Q
5.
B assigned weight
6.
{(B = w)}
7.
Insert assignments derivable (B = w) assignments using
rule P Q priority specified rule
8. return
Figure 5: Running set prioritized rules.
single rule using antecedents weight already weight S. implies
every derivable statement weight S.
ready define Knuths lightest derivation algorithm. algorithm
easily described terms prioritized rules.
Definition 1 (Knuths lightest derivation). Let R finite set non-decreasing
superior rules. Define set prioritized rules K(R) setting priority rule
R weight conclusion. KLD given execution K(R).
show running K(R), (B = w) added w = `(B, R).
means assignments represent lightest derivations. show
assignments inserted non-decreasing weight order. stop algorithm
soon insert weight assignment goal expand statements B
`(B, R) < `(goal , R) statements B `(B, R) = `(goal , R).
properties follow general result described next section.
3.1 Implementation
algorithm Figure 5 implemented run O(M log N ) time, N
refer size problem defined prioritized rules P .
practice set prioritized rules P often specified implicitly, terms small
number rules variables. case problem executing P closely related
work logical algorithms described McAllester (2002).
main difficulty devising efficient implementation procedure Figure 5
step 7. step need find weight assignments combined
(B = w) derive new weight assignments. logical algorithms work shows
set inference rules variables transformed new set rules,
every rule two antecedents particularly simple form. Moreover,
transformation increase number rules much. rules
transformed execution implemented efficiently using hashtable represent
S, heap represent Q indexing tables allow us perform step 7 quickly.
163

fiFelzenszwalb & McAllester

Consider second set rules parsing Figure 4. represented
single rule variables. Moreover rule two antecedents. executing
parsing rules keep track table mapping value j statements phrase(Y, i, j)
weight S. Using table quickly find statements weight
combined statement form phrase(Z, j, k). Similarly keep
track table mapping value j statements phrase(Z, j, k) weight
S. second table lets us quickly find statements combined statement
form phrase(Y, i, j). refer reader (McAllester, 2002) details.

4. A* Lightest Derivation
A* lightest derivation algorithm (A*LD) generalization A* search lightest
derivation problems subsumes A* parsing. algorithm similar KLD
use heuristic function speed computation. Consider lightest derivation problem
rules R goal statement goal . Knuths algorithm expand statement B
`(B, R) < `(goal , R). using heuristic function A*LD avoid expanding
statements light derivations part light derivation goal .
Let R set rules statements , h heuristic function assigning
weight statement. h(B) estimate additional weight required
derive goal using derivation B. note case shortest path problem
weight exactly distance node goal. value `(B, R) + h(B) provides
figure merit statement B. A* lightest derivation algorithm expands
statements order figure merit.
say heuristic function monotone every rule A1 , . . . , g C R
derivable weight assignments (Ai = wi ) have,
wi + h(Ai ) g(w1 , . . . , wn ) + h(C).

(1)

definition agrees standard notion monotone heuristic function rules
come shortest path problem. show h monotone h(goal ) = 0
h admissible appropriate notion admissibility. correctness
A*LD, however, required h monotone h(goal ) finite.
case monotonicity implies heuristic value every statement C appears
derivation goal finite. assume h(C) finite every statement. h(C)
finite ignore C every rule derives C.
Definition 2 (A* lightest derivation). Let R finite set non-decreasing rules h
monotone heuristic function R. Define set prioritized rules A(R) setting
priority rule R weight conclusion plus heuristic value,
g(w1 , . . . , wn ) + h(C). A*LD given execution A(R).
show execution A(R) correctly computes lightest derivations
expands statements order figure merit values.
Theorem 2. execution A(R), (B = w) w = `(B, R).
Proof. proof induction size S. statement trivial = .
Suppose statement true right algorithm removed (B = wb ) Q
164

fiThe Generalized A* Architecture

added S. fact (B = wb ) Q implies weight assignment derivable
thus wb `(B, R).
Suppose derivation B weight wb0 < wb . Consider moment right
algorithm removed (B = wb ) Q added S. Let A1 , . . . , g C
rule antecedents Ai weight conclusion C not.
Let wc = g(`(A1 , R), . . . , `(An , R)). induction hypothesis weight Ai
`(Ai , R). Thus (C = wc ) Q priority wc + h(C). Let wc0 weight assigns
C. Since g non-decreasing know wc wc0 . Since h monotone wc0 +h(C) wb0 +h(B).
follows using monotonicity condition along path C B .
note wc + h(C) < wb + h(B) turn implies (B = wb ) weight
assignment Q minimum priority.
Theorem 3. execution A(R) statements expanded order figure
merit value `(B, R) + h(B).
Proof. First show minimum priority Q decrease throughout
execution algorithm. Suppose (B = w) element Q minimum priority.
Removing (B = w) Q decrease minimum priority. suppose add
(B = w) insert assignments derivable (B = w) Q. Since h monotone
priority every assignment derivable (B = w) least priority (B = w).
weight assignment (B = w) expanded removed Q added S.
last theorem w = `(B, R) definition A(R) weight assignment
queued priority `(B, R) + h(B). Since removed (B = w) Q must
minimum priority queue. minimum priority decrease time
must expand statements order figure merit value.
accurate heuristic functions A*LD much efficient KLD.
Consider situation perfect heuristic function. is, suppose h(B)
exactly additional weight required derive goal using derivation B.
figure merit `(B, R) + h(B) equals weight lightest derivation goal uses
B. case A*LD derive goal expanding statements part
lightest derivation goal .
correctness KLD follows correctness A*LD. set non-decreasing
superior rules consider trivial heuristic function h(B) = 0. fact
rules superior imply heuristic monotone. theorems imply
Knuths algorithm correctly computes lightest derivations expands statements
order lightest derivable weights.

5. Heuristics Derived Abstractions
consider case additive rules rules weight conclusion
sum weights antecedents plus non-negative value v called weight
rule. denote rule A1 , . . . , v C. weight derivation using
additive rules sum weights rules appear derivation tree.
context statement B finite tree rules add derivation
B tree get derivation goal . Intuitively context B derivation goal
hole filled derivation B (see Figure 6).
165

fiFelzenszwalb & McAllester

...
...
goal

A1
A2
A3
context C

C

context A3

Derivation
A1

Derivation
A2

Derivation
A3

Figure 6: derivation goal defines contexts statements appear derivation tree. Note context C together rule A1 , A2 , A3 C
derivations A1 A2 define context A3 .

additive rules, context weight sum weights rules it.
Let R set additive rules statements . B define `(context(B), R)
weight lightest context B. value `(B, R) + `(context(B), R)
weight lightest derivation goal uses B.
Contexts derived using rules R together context rules c(R) defined
follows. First, goal empty context weight zero. captured rule
antecedents 0 context(goal ). rule A1 , . . . , v C R put n rules
c(R). rules capture notion context C derivations Aj j 6=
define context Ai ,
context(C), A1 , . . . , Ai1 , Ai+1 , . . . , v context(Ai ).
Figure 6 illustrates context C together derivations A1 A2 rule
A1 , A2 , A3 C define context A3 .
166

fiThe Generalized A* Architecture

say heuristic function h admissible h(B) `(context(B), R). Admissible
heuristic functions never over-estimate weight deriving goal using derivation
particular statement. heuristic function perfect h(B) = `(context(B), R).
show obtain admissible monotone heuristic functions abstractions.
5.1 Abstractions
Let (, R) lightest derivation problem statements rules R. abstraction
(, R) given problem (0 , R0 ) map abs : 0 , every rule
A1 , . . . , v C R rule abs(A1 ), . . . , abs(An ) v0 abs(C) R0 v 0 v.
show abstraction used define monotone admissible heuristic
function original problem.
usually think abs defining coarsening mapping several statements
abstract statement. example, parser abs might map lexicalized
nonterminal N Phouse nonlexicalized nonterminal N P . case abstraction
defines smaller problem abstract statements. Abstractions often defined
mechanical way starting map abs set abstract statements
0 . project rules R 0 using abs get set abstract
rules. Typically several rules R map abstract rule. need
keep one copy abstract rule, weight lower bound weight
concrete rules mapping it.
Every derivation (, R) maps abstract derivation `(abs(C), R0 )
`(C, R). let goal abstract problem abs(goal ) every context (, R)
maps abstract context see `(context(abs(C)), R0 ) `(context(C), R).
means lightest abstract context weights form admissible heuristic function,
h(C) = `(context(abs(C)), R0 ).
show heuristic function monotone.
Consider rule A1 , . . . , v C R let (Ai = wi ) weight assignments derivable
using R. case rule abs(A1 ), . . . , abs(An ) v0 abs(C) R0 v 0 v
(abs(Ai ) = wi0 ) derivable using R0 wi0 wi . definition contexts (in
abstract problem) have,
X
`(context(abs(Ai )), R0 ) v 0 +
wj0 + `(context(abs(C)), R0 ).
j6=i

Since v 0 v wj0 wj have,
`(context(abs(Ai )), R0 ) v +

X

wj + `(context(abs(C)), R0 ).

j6=i

Plugging heuristic function h adding wi sides,
X
wi + h(Ai ) v +
wj + h(C),
j

exactly monotonicity condition equation (1) additive rule.
167

fiFelzenszwalb & McAllester

abstract problem defined (0 , R0 ) relatively small efficiently compute
lightest context weights every statement 0 using dynamic programming KLD.
store weights pattern database (a lookup table) serve heuristic
function solving concrete problem using A*LD. heuristic may able stop
A*LD exploring lot non-promising structures. exactly approach
used Culberson Schaeffer (1998) Korf (1997) solving large search
problems. results section show pattern databases used
general setting lightest derivations problems. experiments Section 10 demonstrate
technique specific application.

6. Hierarchical A* Lightest Derivation
main disadvantage using pattern databases precompute context
weights every abstract statement. often take lot time space.
define hierarchical algorithm, HA*LD, searches lightest derivations contexts
entire abstraction hierarchy simultaneously. algorithm often solve
concrete problem without fully computing context weights level abstraction.
level abstraction behavior HA*LD similar behavior A*LD
using abstraction-derived heuristic function. hierarchical algorithm queues
derivations statement C priority depends lightest abstract context
C. abstract contexts computed advance. Instead, abstract contexts
computed time computing derivations. abstract context
C, derivations C stalled. captured addition context(abs(C))
antecedent rule derives C.
define abstraction hierarchy levels sequence lightest derivation problems additive rules (k , Rk ) 0 k 1 single abstraction
function abs. 0 k < 1 abstraction function maps k onto k+1 . require (k+1 , Rk+1 ) abstraction (k , Rk ) defined previous section:
A1 , . . . , v C Rk exists rule abs(A1 ), . . . , abs(An ) v0 abs(C)
Rk+1 v 0 v. hierarchical algorithm computes lightest derivations statements
k using contexts k+1 define heuristic values. extend abs maps
m1 abstract set statements containing single element . Since abs
onto |k | |k+1 |. is, number statements decrease go
abstraction hierarchy. denote abs k abstraction function 0 k obtained
composing abs k times.
interested computing lightest derivation goal statement goal 0 . Let
goal k = abs k (goal ) goal level abstraction. hierarchical algorithm
defined set prioritized rules H Figure 7. Rules labeled compute derivations
statements one level abstraction using context weights level define
priorities. Rules labeled BASE compute contexts one level abstraction
using derivation weights level define priorities. rules labeled START1
START2 start inference handling abstract level.
execution H starts computing derivation context START1
START2. continues deriving statements m1 using rules.
lightest derivation goal m1 found algorithm derives context goal m1
168

fiThe Generalized A* Architecture

0

START1:
=0

START2:

0
context() = 0

BASE:

goal k = w
w
context(goal k ) = 0

UP:

context(abs(C)) = wc
A1 = w1
..
.
= wn
v + w1 + + wn + wc
C = v + w1 + + wn

DOWN:

context(C) = wc
A1 = w1
..
.
= wn
v + wc + w1 + + wn
context(Ai ) = v + wc + w1 + + wn wi

Figure 7: Prioritized rules H defining HA*LD. BASE rules defined 0 k 1.
rules defined rule A1 , . . . , v C Rk
0 k 1.

BASE rule starts computing contexts statements m1 using rules.
general HA*LD interleaves computation derivations contexts level
abstraction since execution H uses single priority queue.
Note computation happens given level abstraction lightest derivation goal found level above. means structure
abstraction hierarchy defined dynamically. example, CFDP algorithm,
could define set statements level abstraction refining statements
appear lightest derivation goal level above. assume static
abstraction hierarchy.
statement C k 0 k 1 use `(C) denote weight
lightest derivation C using Rk `(context(C)) denotes weight lightest
context C using Rk . abstract level define `() = `(context()) = 0.
169

fiFelzenszwalb & McAllester

show HA*LD correctly computes lightest derivations lightest contexts
every level abstraction. Moreover, order derivations contexts
expanded controlled heuristic function defined follows. C k 0 k
1 define heuristic value C using contexts level heuristic value
context(C) using derivations level,
h(C) = `(context(abs(C))),
h(context(C)) = `(C).
abstract level define h() = h(context()) = 0. Let generalized statement
either element k 0 k expression form context(C)
C k . define intrinsic priority follows,
p() = `() + h().
C k , p(context(C)) weight lightest derivation goal k
uses C, p(C) lower bound weight.
results Sections 4 5 cannot used directly show correctness
HA*LD. rules Figure 7 generate heuristic values time
generate derivations depend heuristic values. Intuitively must show
execution prioritized rules H, heuristic value available
appropriate point time. next lemma shows rules H satisfy monotonicity
property respect intrinsic priority generalized statements. Theorem 5 proves
correctness hierarchical algorithm.
Lemma 4 (Monotonicity). rule 1 , . . . , hierarchical algorithm,
weight antecedent `(i ) weight conclusion
(a) priority rule + h().
(b) + h() p(i ).
Proof. rules START1 START2 result follows fact rules
antecedents h() = h(context()) = 0.
Consider rule labeled BASE w = `(goal k ). see (a) note always zero
priority rule w = h(context(goal k )). (b) note p(goal k ) =
`(goal k ) equals priority rule.
consider rule labeled wc = `(context(abs(C))) wi = `(Ai ) i.
part (a) note priority rule + wc h(C) = wc . part (b) consider
first antecedent rule. h(context(abs(C))) = `(abs(C)) `(C) ,
p(context(abs(C))) = wc + h(context(abs(C))) + wc . consider antecedent
Ai . abs(Ai ) = p(Ai ) = wi + wc . abs(Ai ) 6= show
h(Ai ) = `(context(abs(Ai ))) wc + wi . implies p(Ai ) = wi + h(Ai ) + wc .
Finally consider rule labeled wc = `(context(C)) wj = `(Aj )
j. part (a) note priority rule + wi h(context(Ai )) = wi .
Ppart
(b) consider first antecedent rule. h(context(C)) = `(C) v + j wj
see p(context(C)) = wc + h(C) + wi . consider antecedent Aj .
abs(Aj ) = h(Aj ) = 0 p(Aj ) = wj + wi . abs(Aj ) 6= show
h(Aj ) + wi wj . Hence p(Aj ) = wj + h(Aj ) + wi .
170

fiThe Generalized A* Architecture

Theorem 5. execution H maintains following invariants.
1. ( = w) w = `().
2. ( = w) Q priority w + h().
3. p() < p(Q) ( = `())
p(Q) denotes smallest priority Q.
Proof. initial state algorithm empty Q contains ( = 0)
(context() = 0) priority 0. initial state invariant 1 true since empty;
invariant 2 follows definition h() h(context()); invariant 3 follows
fact p(Q) = 0 p() 0 . Let Q denote state
algorithm immediately prior iteration loop Figure 5 suppose
invariants true. Let 0 Q0 denote state algorithm iteration.
first prove invariant 1 0 . Let ( = w) element removed Q
iteration. soundness rules w `(). w = `() clearly
invariant 1 holds 0 . w > `() invariant 2 implies p(Q) = w +h() > `()+h()
invariant 3 know contains ( = `()). case 0 = S.
Invariant 2 Q0 follows invariant 2 Q, invariant 1 0 , part (a)
monotonicity lemma.
Finally, consider invariant 3 0 Q0 . proof reverse induction
abstraction level . say level k k form context(C)
C k . reverse induction, base case considers level m. Initially
algorithm inserts ( = 0) (context() = 0) queue priority 0. p(Q0 ) > 0
0 must contain ( = 0) (context() = 0). Hence invariant 3 holds 0 Q0
level m.
assume invariant 3 holds 0 Q0 levels greater k
consider level k. first consider statements C k . Since rules Rk additive,
every statement C derivable Rk lightest derivation (a derivation weight
`(C)). follows correctness Knuths algorithm. Moreover, additive
rules, subtrees lightest derivations lightest derivations. show structural
induction lightest derivation conclusion C p(C) < p(Q0 )
(C = `(C)) 0 . Consider lightest derivation Rk conclusion C
p(C) < p(Q0 ). final rule derivation A1 , . . . , v C corresponds rule
add antecedent context(abs(C)). part (b) monotonicity lemma
antecedents rule intrinsic priority less p(Q0 ). induction
hypothesis lightest derivations (Ai = `(Ai )) 0 . Since invariant 3 holds
statements levels greater k (context(abs(C)) = `(context(abs(C)))) 0 .
implies point rule used derive (C = `(C)) priority p(C).
p(C) < p(Q0 ) hence item must removed queue. Therefore
0 must contain (C = w) w and, invariant 1, w = `(C).
consider form context(C) C k . see c(Rk )
additive thus every statement derivable c(Rk ) lightest derivation subtrees
lightest derivations lightest derivations themselves. prove structural induction
171

fiFelzenszwalb & McAllester

lightest derivation conclusion context(C) p(context(C)) <
p(Q0 ) (context(C) = `(context(C))) 0 . Suppose last rule form,
context(C), A1 , . . . , Ai1 , Ai+1 , . . . , v context(Ai ).
rule corresponds rule add antecedent Ai . part (b)
monotonicity lemma antecedents rule intrinsic priority less
p(Q0 ). invariant 3 statements k induction hypothesis lightest
derivations using c(Rk ), antecedents rule lightest weight
0 . point (context(Ai ) = `(context(Ai ))) derived priority p(Ai ).
p(Ai ) < p(Q0 ) implies item removed queue and, invariant 1,
(context(Ai ) = `(context(Ai ))) 0 .
suppose last (and only) rule 0 context(goal k ). rule corresponds BASE rule add goal k antecedent. Note p(goal k ) =
`(goal k ) = p(context(goal k )) hence p(goal k ) < p(Q0 ). invariant 3 statements
k (goal k = `(goal k )) 0 point BASE rule used queue
(context(goal k ) = `(context(goal k ))) priority p(context(goal k )). previous cases
p(context(goal k )) < p(Q0 ) implies (context(goal k ) = `(context(goal k ))) 0 .
last theorem implies generalized statements expanded order
intrinsic priority. Let K number statements C entire abstraction hierarchy
p(C) p(goal ) = `(goal ). every statement C p(C) p(context(C)).
conclude HA*LD expands 2K generalized statements computing
lightest derivation goal .
6.1 Example
consider execution HA*LD specific example. example illustrates
HA*LD interleaves computation structures different levels abstraction.
Consider following abstraction hierarchy 2 levels.
0 = {X1 , . . . , Xn , Y1 , . . . , Yn , Z1 , . . . , Zn , goal 0 }, 1 = {X, Y, Z, goal 1 },




1 X,
Xi ,

















Y,


,


1

Xi , Yj ij goal 0 ,
X, 1 goal 1 ,
, R1 =
,
R0 =








Z,
,


Z
,
X,


X




5







5
Z 1 goal 1 ,
Zi goal 0 ,
abs(Xi ) = X, abs(Yi ) = , abs(Zi ) = Z abs(goal0 ) = goal1 .
1. Initially = Q = {( = 0) (context() = 0) priority 0}.
2. ( = 0) comes queue gets put nothing else happens.
3. (context() = 0) comes queue gets put S. statements 1
abstract context S. causes rules come rules R1
antecedents fire, putting (X = 1) (Y = 1) Q priority 1.
172

fiThe Generalized A* Architecture

4. (X = 1) (Y = 1) come queue get put S, causing two
rules fire, putting (goal 1 = 3) priority 3 (Z = 7) priority 7 queue.
5. have,
= {( = 0), (context() = 0), (X = 1), (Y = 1)}
Q = {(goal 1 = 3) priority 3, (Z = 7) priority 7}
6. point (goal 1 = 3) comes queue goes S. BASE rule fires
putting (context(goal 1 ) = 0) queue priority 3.
7. (context(goal 1 ) = 0) comes queue. base case contexts 1 . Two
rules use (context(goal 1 ) = 0), (X = 1) (Y = 1) put (context(X) = 2)
(context(Y ) = 2) Q priority 3.
8. (context(X) = 2) comes queue gets put S. abstract
context Xi 0 , rules put (Xi = i) Q priority + 2.
9. (context(Y ) = 2) comes queue goes S. previous step
rules put (Yi = i) Q priority + 2.
10. have,
= {( = 0), (context() = 0), (X = 1), (Y = 1), (goal 1 = 3),
(context(goal 1 ) = 0), (context(X) = 2), (context(Y ) = 2)}
Q = {(Xi = i) (Yi = i) priority + 2 1 n, (Z = 7) priority 7}
11. Next (X1 = 1) (Y1 = 1) come queue go S. causes
rule put (goal 0 = 3) queue priority 3.
12. (goal 0 = 3) comes queue goes S. algorithm stop since
derivation concrete goal.
Note HA*LD terminates fully computing abstract derivations contexts.
particular (Z = 7) Q Z never expanded. Moreover context(Z) even
queue. keep running algorithm would eventually derive context(Z),
would allow Zi derived.

7. Perception Pipeline
Figure 8 shows hypothetical run hierarchical algorithm processing pipeline
vision system. system weighted statements edges used derive
weighted statements contours provide input later stages ultimately resulting
statements recognized objects.
well known subjective presence edges particular image location
depend context given image patch appears. interpreted
perception pipeline stating higher level processes later pipeline
influence low-level interpretations. kind influence happens naturally lightest
173

fiFelzenszwalb & McAllester

m1

Edges

Contours

Recognition

1

Edges

Contours

Recognition

0

Edges

Contours

Recognition

Figure 8: vision system several levels processing. Forward arrows represent
normal flow information one stage processing next. Backward
arrows represent computation contexts. Downward arrows represent
influence contexts.

derivation problem. example, lightest derivation complete scene analysis might
require presence edge locally apparent. implementing whole
system single lightest derivation problem avoid need make hard decisions
stages pipeline.
influence late pipeline stages guiding earlier stages pronounced use
HA*LD compute lightest derivations. case influence apparent
structure optimal solution flow information across different
stages processing. HA*LD complete interpretation derived one level abstraction
guides processing stages concrete level. Structures derived late stages
pipeline guide earlier stages abstract context weights. allows early
processing stages concentrate computational efforts constructing structures
likely part globally optimal solution.
emphasized use admissible heuristics, note A* architecture, including HA*LD, used inadmissible heuristic functions (of course
would break optimality guarantees). Inadmissible heuristics important
admissible heuristics tend force first stages processing pipeline generate
many derivations. derivations composed weights increase causes
large number derivations generated first stages processing
first derivation reaches end pipeline. Inadmissible heuristics produce behavior
similar beam search derivations generated first stage pipeline flow
whole pipeline quickly. natural way construct inadmissible heuristics
simply scale-up admissible heuristic ones obtained abstractions.
possible construct hierarchical algorithm inadmissible heuristics obtained
one level abstraction used guide search level below.

8. Hierarchical Methods
section compare HA*LD hierarchical search methods.
174

fiThe Generalized A* Architecture

8.1 Coarse-to-Fine Dynamic Programming
HA*LD related coarse-to-fine dynamic programming (CFDP) method described
Raphael (2001). understand relationship consider problem finding shortest
path trellis graph one shown Figure 9(a). k columns
n nodes every node one column connected constant number nodes
next column. Standard dynamic programming used find shortest path
O(kn) time. CFDP HA*LD often find shortest path much faster.
hand worst case behavior algorithms different describe
below, CFDP taking significantly time HA*LD.
CFDP algorithm works coarsening graph, grouping nodes column
small number supernodes illustrated Figure 9(b). weight edge
two supernodes B minimum weight nodes b B.
algorithm starts using dynamic programming find shortest path P
coarse graph, shown bold Figure 9(b). supernodes along P
partitioned define finer graph shown Figure 9(c) procedure repeated.
Eventually shortest path P go supernodes size one, corresponding
path original graph. point know P must shortest path
original graph. best case optimal path iteration
refinement optimal path previous iteration. would result O(log n)
shortest paths computations, fairly coarse graphs. hand, worst
case CFDP take (n) iterations refine whole graph, many iterations
involve finding shortest paths large graphs. case CFDP takes (kn2 ) time
much worst standard dynamic programming approach.
suppose use HA*LD find shortest path graph
one Figure 9(a). build abstraction hierarchy O(log n) levels
supernode level contains 2i nodes one column original graph. coarse
graph Figure 9(b) represents highest level abstraction hierarchy. Note
HA*LD consider small number, O(log n), predefined graphs CFDP end
considering much larger number, (n), graphs. best case scenario HA*LD
expand nodes shortest path level
hierarchy. worst case HA*LD compute lightest path context every
node hierarchy (here context node v path v t). i-th
abstraction level graph O(kn/2i ) nodes edges. HA*LD spend
O(kn log(kn)/2i ) time computing paths contexts level i. Summing levels
get O(kn log(kn)) time total, much worst O(kn) time
taken standard dynamic programming approach.
8.2 Hierarchical Heuristic Search
hierarchical method related HA* HIDA* algorithms described
Holte et al. (1996) Holte et al. (2005). methods restricted shortest paths
problems use hierarchy abstractions. heuristic function defined
level abstraction using shortest paths goal level above. main
idea run A* IDA* compute shortest path computing heuristic values ondemand. Let abs map node abstraction let g goal node concrete
175

fiFelzenszwalb & McAllester





(a)







(b)



(c)

Figure 9: (a) Original dynamic programming graph. (b) Coarse graph shortest path
shown bold. (c) Refinement coarse graph along shortest path.

graph. Whenever heuristic value concrete node v needed call algorithm
recursively find shortest path abs(v) abs(g). recursive call uses heuristic
values defined abstraction, computed deeper recursive calls.
clear generalize HA* HIDA* lightest derivation problems
rules multiple antecedents. Another disadvantage methods
potentially stall case directed graphs. example, suppose using
HA* HIDA* expand node two successors x y, x close goal
far. point need heuristic value x y, might
spend long time computing shortest path abs(y) abs(g). hand,
HA*LD would wait shortest path fully computed. Intuitively HA*LD
would compute shortest paths abs(x) abs(y) abs(g) simultaneously. soon
shortest path abs(x) abs(g) found start exploring path x
g, independent long would take compute path abs(y) abs(g).
176

fiThe Generalized A* Architecture

r2
r1

r3

r0
r4
r7
r5

r6

Figure 10: convex set specified hypothesis (r0 , . . . , r7 ).

9. Convex Object Detection
consider application HA*LD problem detecting convex objects
images. pose problem using formulation similar one described Raphael
(2001), optimal convex object around point found solving shortest
path problem. compare HA*LD search methods, including CFDP A*
pattern databases. results indicate HA*LD performs better
methods wide range inputs.
Let x reference point inside convex object. represent object boundary
using polar coordinates respect coordinate system centered x. case
object described periodic function r() specifying distance x object
boundary function angle . specify r() finite number angles
(0 , . . . , N 1 ) assume boundary straight line segment sample points.
assume object contained ball radius R around x r()
integer. Thus object parametrized (r0 , . . . , rN 1 ) ri [0, R 1]. example
N = 8 angles shown Figure 10.
every hypothesis (r0 , . . . , rN 1 ) specifies convex object. hypothesis describes
convex set exactly object boundary turns left sample point (i , ri )
increases. Let C(ri1 , ri , ri+1 ) Boolean function indicating three sequential
values r() define boundary locally convex i. hypothesis (r0 , . . . , rN 1 )
convex locally convex i.3
Throughout section assume reference point x fixed advance.
goal find optimal convex object around given reference point. practice
reference locations found using variety methods Hough transform.
3. parametrization convex objects similar identical one used Raphael (2001).

177

fiFelzenszwalb & McAllester

Let D(i, ri , ri+1 ) image data cost measuring evidence boundary segment
(i , ri ) (i+1 , ri+1 ). consider problem finding convex object
sum data costs along whole boundary minimal. is, look
convex hypothesis minimizing following energy function,
E(r0 , . . . , rN 1 ) =

N
1
X

D(i, ri , ri+1 ).

i=0

data costs precomputed specified lookup table O(N R2 ) entries.
experiments use data cost based integral image gradient along
boundary segment. Another approach would use data term described
Raphael (2001) cost depends contrast inside outside
object measured within pie-slice defined i+1 .
optimal convex object found using standard dynamic programming techniques. Let B(i, r0 , r1 , ri1 , ri ) cost optimal partial convex object starting
r0 r1 ending ri1 ri . keep track last two boundary points
enforce convexity constraint extend partial objects. keep track
first two boundary points enforce rN = r0 convexity constraint r0 .
compute B using recursive formula,
B(1, r0 , r1 , r0 , r1 ) = D(0, r0 , r1 ),
B(i + 1, r0 , r1 , ri , ri+1 ) = min B(i, r0 , r1 , ri1 , ri ) + D(i, ri , ri+1 ),
ri1

minimization choices ri1 C(ri1 , ri , ri+1 ) = true.
cost optimal object given minimum value B(N, r0 , r1 , rN 1 , r0 )
C(rN 1 , r0 , r1 ) = true. optimal object found tracing-back typical dynamic programming algorithms. main problem approach dynamic
programming table O(N R4 ) entries takes O(R) time compute entry.
overall algorithm runs O(N R5 ) time quite slow.
show optimal convex objects defined terms lightest derivation
problem. Let convex (i, r0 , r1 , ri1 , ri ) denote partial convex object starting r0 r1
ending ri1 ri . corresponds entry dynamic programming table
described above. Define set statements,
= {convex (i, a, b, c, d) | [1, N ], a, b, c, [0, R 1]} {goal }.
optimal convex object corresponds lightest derivations goal using rules
Figure 11. first set rules specify cost partial object r0 r1 .
second set rules specify object ending ri1 ri extended
choice ri+1 boundary locally convex ri . last set rules specify
complete convex object partial object r0 rN rN = r0
boundary locally convex r0 .
construct abstraction hierarchy define L nested partitions radius space
[0, R 1] ranges integers. abstract statement instead specifying integer
value r() specify range r() contained. simplify notation
178

fiThe Generalized A* Architecture

(1) r0 , r1 [0, R 1],

convex (1, r0 , r1 , r0 , r1 ) = D(0, r0 , r1 )
(2) r0 , r1 , ri1 , ri , ri+1 [0, R 1] C(ri1 , ri , ri+1 ) = true,
convex (i, r0 , r1 , ri1 , ri ) = w
convex (i + 1, r0 , r1 , ri , ri+1 ) = w + D(i, ri , ri+1 )
(3) r0 , r1 , rN 1 [0, R 1] C(rN 1 , r0 , r1 ) = true,
convex (N, r0 , r1 , rN 1 , r0 ) = w
goal = w
Figure 11: Rules finding optimal convex object.
assume R power two. k-th partition P k contains R/2k ranges,
2k consecutive integers. j-th range P k given [j 2k , (j + 1) 2k 1].
statements abstraction hierarchy are,
k = {convex (i, a, b, c, d) | [1, N ], a, b, c, P k } {goal k },
k [0, L 1]. range P 0 contains single integer 0 = . Let f map range
P k range P k+1 containing it. statements level k < L 1 define
abstraction function,
abs(convex (i, a, b, c, d)) = convex (i, f (a), f (b), f (c), f (d)),
abs(goal k ) = goal k+1 .
abstract rules use bounds data costs boundary segments (i , si )
(i+1 , si+1 ) si si+1 ranges P k ,
Dk (i, si , si+1 ) =

min
D(i, ri , ri+1 ).
ri si
ri+1 si+1

Since range P k union two ranges P k1 one entry Dk computed
quickly (in constant time) Dk1 computed. bounds levels computed O(N R2 ) time total. need abstract versions convexity constraints.
si1 , si , si+1 P k , let C k (si1 , si , si+1 ) = true exist integers ri1 , ri ri+1
si1 , si si+1 respectively C(ri1 , ri , ri+1 ) = true. value C k
defined closed form evaluated quickly using simple geometry.
rules abstraction hierarchy almost identical rules Figure 11.
rules level k obtained original rules simply replacing instance
[0, R 1] P k , C C k Dk .
179

fiFelzenszwalb & McAllester

Standard DP
CFDP
HA*LD
A* pattern database 2
A* pattern database 3

6718.6 seconds
13.5 seconds
8.6 seconds
14.3 seconds
29.7 seconds

Table 1: Running time comparison example Figure 12.
9.1 Experimental Results
Figure 12 shows example image set reference locations selected manually
optimal convex object found around reference point. 14 reference
locations used N = 30 R = 60 parametrize object. Table 1 compares
running time different optimization algorithms implemented problem.
line shows time took solve 14 problems contained example image using
particular search algorithm. standard DP algorithm uses dynamic programming
solution outlined above. CFDP method based algorithm Raphael (2001)
modified representation convex objects. hierarchical A* algorithm uses
abstraction hierarchy described here. A* pattern databases used dynamic
programming compute pattern database particular level abstraction,
used database provide heuristic values A*. Note problem described
pattern database depends input. running times listed include time
took compute pattern database case.
see CFDP, HA*LD A* pattern databases much efficient
standard dynamic programming algorithm use abstractions. HA*LD
slightly faster methods example. Note running time
varies algorithm algorithm output every method find
globally optimum objects.
quantitative evaluation different search algorithms created large set
problems varying difficulty size follows. given value R generated square
images width height 2 R + 1. image circle radius less R near
center pixels image corrupted independent Gaussian noise.
difficulty problem controlled standard deviation, , noise. Figure 13
shows example images optimal convex object found around centers.
graph Figure 14 shows running time (in seconds) different search
algorithms function noise level problem size fixed R = 100.
sample point indicates average running time 200 random inputs. graph
shows running times point circles reliably detected.
compared HA*LD CFDP A* using pattern databases (PD2 PD3). PD2
PD3 refer A* pattern database defined 2 3 respectively. Since
pattern database needs recomputed input trade-off amount
time spent computing database accuracy heuristic provides.
see easy problems better use smaller database (defined higher level
abstraction) harder problems worth spending time computing bigger
database. HA*LD outperforms methods every situation captured here.
180

fiThe Generalized A* Architecture

(a)

(b)
Figure 12: (a) Reference locations. (b) Optimal convex objects.

181

fiFelzenszwalb & McAllester

Figure 13: Random images circles optimal convex object around center
one (with N = 20 R = 100). noise level images = 50.

Figure 15 shows running time different methods function problem
size R, problems fixed noise level = 100. sample point
indicates average running time taken 200 random inputs. see running
time pattern database approach grows quickly problem size increases.
computing database fixed level abstraction takes O(N R5 ) time.
hand running time CFDP HA*LD grows much slower.
CFDP performed essentially well HA*LD experiment, graph Figure 14
shows HA*LD performs better difficulty problem increases.

10. Finding Salient Curves Images
classical problem computer vision involves finding salient curves images. Intuitively
goal find long smooth curves go along paths high image gradient.
standard way pose problem define saliency score search curves
optimizing score. methods use score defined simple combination local
terms. example, score usually depends curvature image gradient
point curve. type score often optimized efficiently using dynamic
programming shortest paths algorithms (Montanari, 1971; Shashua & Ullman, 1988;
Basri & Alter, 1996; Williams & Jacobs, 1996).
consider new compositional model finding salient curves. important
aspect model capture global shape constraints. particular, looks
curves almost straight, something done using local constraints
alone. Local constraints enforce small curvature point curve,
182

fiThe Generalized A* Architecture

Figure 14: Running time different search algorithms function noise level
input. sample point indicates average running time taken 200
random inputs. case N = 20 R = 100. See text discussion.

enough prevent curves turning twisting around long distances.
problem finding salient curve image compositional model defined
solved using dynamic programming, approach slow practical
use. Shortest paths algorithms applicable compositional nature
model. Instead use A*LD heuristic function derived abstraction
(a pattern database).
Let C1 curve endpoints b C2 curve endpoints b c.
two curves composed form curve C c. define weight
composition sum weights C1 C2 plus shape cost depends
geometric arrangement points (a, b, c). Figure 16 illustrates idea shape
costs use. Note C1 C2 long, arrangement endpoints reflect
non-local geometric properties. general consider composing C1 C2 angle
formed ab bc least /2 lengths C1 C2 approximately equal.
constraints reduce total number compositions play important role
abstract problem defined below.
Besides compositional rule say b nearby locations,
short curve endpoints b. forms base case creating longer curves.
183

fiFelzenszwalb & McAllester

Figure 15: Running time different search algorithms function problem size R.
sample point indicates average running time taken 200 random
inputs. case N = 20 = 100. See text discussion.

b




c

Figure 16: curve endpoints (a, c) formed composing curves endpoints
(a, b) (b, c). assume /2. cost composition
proportional sin2 (t). cost scale invariant encourages curves
relatively straight.

assume short curves straight, weight depends image
data along line segment b. use data term, seg(a, b), zero
image gradient along pixels ab perpendicular ab, higher otherwise.
Figure 17 gives formal definition two rules model. constants k1
k2 specify minimum maximum length base case curves, L constant
184

fiThe Generalized A* Architecture

(1) pixels a, b, c angle ab bc least /2 0 L,
curve(a, b, i) = w1
curve(b, c, i) = w2
curve(a, c, + 1) = w1 + w2 + shape(a, b, c)
(2) pixels a, b k1 ||a b|| k2 ,

curve(a, b, 0) = seg(a, b)
Figure 17: Rules finding almost straight curves pair endpoints. L,
k1 k2 constants, shape(a, b, c) function measuring cost
composition.

controlling maximum depth derivations. derivation curve(a, b, i) encodes curve
b. value seen approximate measure arclength. derivation
curve(a, b, i) full binary tree depth encodes curve length
2i k1 2i k2 . let k2 = 2k1 allow curves length.
rules Figure 17 define good measure saliency
always prefer short curves long ones. define saliency curve
terms weight minus arclength, salient curves light long. Let
positive constant. consider finding lightest derivation goal using,
curve(a, b, i) = w
goal = w 2i
n n image (n4 ) statements form curve(a, c, i). Moreover,
c far apart (n) choices midpoint b defining two curves
composed lightest derivation curve(a, c, i). makes dynamic programming
solution lightest derivation problem impractical. tried using KLD even
small images algorithm runs memory minutes. describe
abstraction used define heuristic function A*LD.
Consider hierarchical set partitions image boxes. i-th partition
defined tiling image boxes 2i 2i pixels. partitions form pyramid
boxes different sizes level. box level union 4 boxes level
it, boxes level 0 pixels themselves. Let (a) box containing
i-th level pyramid. define
abs(curve(a, b, i)) = curve(fi (a), (b), i).
185

fiFelzenszwalb & McAllester


B


b

c


C

Figure 18: abstraction maps curve statement statement curves
boxes. > j curve(a, b, i) gets coarsened curve(c, d, j). Since
light curves almost straight, > j usually implies ||a b|| > ||c d||.

Figure 18 illustrates map selects pyramid level abstract statement. Intuitively abs defines adaptive coarsening criteria. b far other,
curve b must long, turn implies map b boxes
coarse partition image. creates abstract problem small number
statements without losing much information.
define abstract problem need define set abstract rules. Recall
every concrete rule r need corresponding abstract rule r0 weight
r0 weight r. small number rules antecedents
Figure 17. concrete rule seg(a,b) curve(a, b, 0) define corresponding abstract
rule, seg(a,b) abs(curve(a, b, 0)). compositional rules Figure 17 lead abstract
rules composing curves boxes,
curve(A, B, i), curve(B, C, i) v curve(A0 , C 0 , + 1),
A, B C boxes i-th pyramid level A0 C 0 boxes
level + 1 containing C respectively. weight v shape(a, b, c)
a, b c arbitrary pixels A, B C respectively. compute value
v bounding orientations line segments ab bc boxes.
186

fiThe Generalized A* Architecture

146 137 pixels. Running time: 50 seconds (38 + 12).

122 179 pixels. Running time: 65 seconds (43 + 22).

226 150 pixels. Running time: 73 seconds (61 + 12).
Figure 19: salient curve different images. running time sum
time spent computing pattern database time spent solving
concrete problem.

187

fiFelzenszwalb & McAllester

Figure 20: example salient curve goes locations essentially
local evidence curve locations.

abstract problem defined relatively small even large images,
use pattern database approach outlined Section 5.1. input image use
KLD compute lightest context weights every abstract statement. use
weights heuristic values solving concrete problem A*LD. Figure 19 illustrates
results obtained using method. seems abstract problem
able capture short curves extended salient curve. took
one minute find salient curve images. Figure 19 lists
dimensions image running time case.
Note algorithm rely initial binary edge detection stage. Instead
base case rules allow salient curves go pixel, even local
evidence boundary particular location. Figure 20 shows example
happens. case small part horse back blends background
consider local properties alone.
curve finding algorithm described section would difficult formulate
without A*LD general notion heuristics derived abstractions lightest
derivation problems. However, using framework introduced paper becomes
relatively easy specify algorithm.
future plan compose rules computing salient curves rules
computing complex structures. basic idea using pyramid boxes defining
abstract problem applicable variety problems computer vision.
188

fiThe Generalized A* Architecture

11. Conclusion
Although presented preliminary results last two sections, view
main contribution paper providing general architecture perceptual inference.
Dijkstras shortest paths algorithm A* search fundamental algorithms
many applications. Knuth noted generalization Dijkstras algorithm general
problems defined set recursive rules. paper given similar generalizations A* search heuristics derived abstractions. described
new method solving lightest derivation problems using hierarchy abstractions.
Finally, outlined approach using generalizations construction
processing pipelines perceptual inference.

Acknowledgments
material based upon work supported National Science Foundation
Grant No. 0535174 0534820.

References
Basri, R., & Alter, T. (1996). Extracting salient curves images: analysis
saliency network. IEEE Conference Computer Vision Pattern Recognition.
Bonet, B., & Geffner, H. (2005). algorithm better AO*. Proceedings
National Conference Artificial Intelligence.
Bulitko, V., Sturtevant, N., Lu, J., & Yau, T. (2006). State abstraction real-time heuristic
search. Technical Report, University Alberta, Department Computer Science.
Charniak, E. (1996). Statistical Language Learning. MIT Press.
Culberson, J., & Schaeffer, J. (1998). Pattern databases. Computational Intelligence, 14 (3),
318334.
Dijkstra, E. (1959). note two problems connection graphs. Numerical Mathematics, 1, 269271.
Edelkamp, S. (2002). Symbolic pattern databases heuristic search panning. International Conference AI Planning Scheduling.
Felner, A. (2005). Finding optimal solutions graph partitioning problem heuristic
search. Annals Mathematics Artificial Intelligence, 45 (3-4), 293322.
Geman, S., Potter, D., & Chi, Z. (2002). Composition systems. Quarterly Applied
Mathematics, 707736.
Hansen, E., & Zilberstein, S. (2001). LAO*: heuristic search algorithm finds solutions
loops. Artificial Intelligence, 129, 3562.
Hart, P., Nilsson, N., & Raphael, B. (1968). formal basis heuristic determination
minimal cost paths. IEEE Transactions Systems Science Cybernetics, 4 (2),
100107.
Holte, R., Grajkowski, J., & Tanner, B. (2005). Hierarchical heuristic search revisited.
Symposium Abstraction, Reformulation Approximation.
189

fiFelzenszwalb & McAllester

Holte, R., Perez, M., Zimmer, R., & MacDonald, A. (1996). Hierarchical A*: Searching abstraction hierarchies efficiently. Proceedings National Conference Artificial
Intelligence.
Jimenez, P., & Torras, C. (2000). efficient algorithm searching implicit AND/OR
graphs cycles. Artificial Intelligence, 124, 130.
Jin, Y., & Geman, S. (2006). Context hierarchy probabilistic image model.
IEEE Conference Computer Vision Pattern Recognition.
Klein, D., & Manning, C. (2003). A* parsing: Fast exact viterbi parse selection. Proceedings HLT-NAACL.
Knuth, D. (1977). generalization Dijkstras algorithm. Information Processing Letters,
6 (1), 15.
Korf, R. (1997). Finding optimal solutions Rubiks cube using pattern databases.
Proceedings National Conference Artificial Intelligence.
Korf, R., & Felner, A. (2002). Disjoint pattern database heuristics. Artificial Intelligence,
134, 922.
Korf, R., Zhang, W., Thayer, I., & Hohwald, H. (2005). Frontier search. Journal
ACM, 52 (5), 715748.
McAllester, D. (2002). complexity analysis static analyses. Journal ACM,
49 (4), 512537.
Montanari, U. (1971). optimal detection curves noisy pictures. Communications
ACM, 14 (5).
Nilsson, N. (1980). Principles Artificial Intelligence. Morgan Kaufmann.
Pearl, J. (1984). Heuristics: intelligent search strategies computer problem solving.
Addison-Wesley.
Rabiner, L. (1989). tutorial hidden Markov models selected applications speech
recognition. Proceedings IEEE, 77 (2), 257286.
Raphael, C. (2001). Coarse-to-fine dynamic programming. IEEE Transactions Pattern
Analysis Machine Intelligence, 23 (12), 13791390.
Shashua, A., & Ullman, S. (1988). Structural saliency: detection globally salient
structures using locally connected network. IEEE International Conference
Computer Vision.
Tu, Z., Chen, X., Yuille, A., & Zhu, S. (2005). Image parsing: Unifying segmentation,
detection, recognition. International Journal Computer Vision, 63 (2), 113
140.
Williams, L., & Jacobs, D. (1996). Local parallel computation stochastic completion
fields. IEEE Conference Computer Vision Pattern Recognition.
Zhang, W., & Korf, R. (1996). study complexity transitions asymmetric traveling
salesman problem. Artificial Intelligence, 81, 12.

190


