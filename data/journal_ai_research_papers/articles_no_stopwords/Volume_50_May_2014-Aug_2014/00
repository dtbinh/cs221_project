journal artificial intelligence

submitted published

topic dissimilarity sensitivity
translation rule selection
min zhang

minzhang suda edu cn

provincial key laboratory computer information processing technology
soochow university suzhou china

xinyan xiao

xiaoxinyan ict ac cn

iip key lab institute computing technology
chinese academy sciences china

deyi xiong

dyxiong suda edu cn

provincial key laboratory computer information processing technology
soochow university suzhou china

qun liu

liuqun ict ac cn

cngl school computing dublin city university ireland
iip key lab institute computing technology
chinese academy sciences china

abstract
translation rule selection task selecting appropriate translation rules ambiguous
source language segment translation ambiguities pervasive statistical machine translation introduce two topic translation rule selection incorporates global
topic information translation disambiguation associate synchronous translation rule
source target side topic distributions topic distributions propose topic
dissimilarity model select desirable less dissimilar rules imposing penalties rules
large value dissimilarity topic distributions given documents order encourage use non topic specific translation rules present topic sensitivity model
balance translation rule selection generic rules topic specific rules furthermore
project target side topic distributions onto source side topic model space benefit
topic information source target language integrate proposed topic dissimilarity sensitivity model hierarchical phrase machine translation synchronous
translation rule selection experiments topic translation rule selection model
substantially improve translation quality

introduction
translation rules bilingual segments establish translation equivalences source
target language widely used statistical machine translation smt representations ranging word pairs bilingual phrases synchronous rules word phraseand syntax smt respectively normally large number translation rules learnt
bilingual training data single source segment occurs different contexts
example xiong zhang li observe chinese verb translated
segment defined string terminals nonterminals

c

ai access foundation rights reserved

fiz hang x iao x iong l iu

different translation rules average therefore select appropriate translation
rule ambiguous source segment crucial issue smt
traditionally appropriateness translation rule measured multiple probabilities
estimated word aligned data bidirectional translation probabilities koehn och
marcu probabilities fail capture local global contexts highly ambiguous
source segments sufficient select correct translation rules segments therefore approaches proposed capture rich contexts sentence level help
select proper translation rules phrase carpuat wu syntax smt chan ng
chiang liu lin liu liu lin studies local
features surrounding words syntactic information helpful translation rule
selection
beyond contextual features sentence level conjecture translation rules
related high level global information topic hofmann blei ng jordan
information document level order visualize relatedness translation
rules document topics four hierarchical phrase translation rules topic
distributions figure figure observe
first translation rules divided two categories terms topic distributions
topic sensitive rules e topic specific rules topic insensitive rules e non topic specific generic rules former rules e g translation rule b figure
much higher distribution probabilities specific topics topics
latter rules e g translation rule c figure even distribution topics
second topic information used disambiguate ambiguous source segments figure
translation rule b c source segment however topic distributions
quite different rule b distributes topic international relations
highest probability suggests rule b much related topic
topics contrast rule c even distribution topics therefore document
international relations rule b appropriate rule c source
x
segment



two observations suggest different translation rules different topic distributions
document level topic information used benefit translation rule selection
article propose framework translation rule selection exactly capitalizes
document level topic information proposed topic translation rule selection framework
associates translation rule topic distribution rule topic distribution source
target side source document annotated corresponding topic distribution
document topic distribution dissimilarity document topic distribution rule topic
distribution calculated used help select translation rules related documents
terms topics particular
given document translated use topic dissimilarity model calculate dissimilarity translation rule document topic distributions
translation system penalize candidate translations high dissimilarities
section explains system penalizes candidate translations high dissimilarities



fit opic issimilarity



ensitivity odels















u operational capability





































c

x













b





give x

x

x




x












grants x











held talks x x

figure four synchronous rules topic distributions sub graph shows rule
topic distribution x axis shows topic index axis topic probability notably rule b rule c shares source chinese string
different topic distributions due different english translations

dissimilarity topic insensitive translation rule given source document
computed topic dissimilarity model often high documents normally
topic sensitive dont want penalize generic topic insensitive rules therefore
propose topic sensitivity model rewards topic insensitive rules
complement topic dissimilarity model
associate translation rule rule topic distribution source target side order calculate dissimilarity target side rule topic distributions
translation rules source side document topic distributions given documents
decoding project target side rule topic distributions translation rules onto space
source side document topic model one many mapping
use hierarchical phrase smt system chiang validate effectiveness
topic translation rule selection experiments chinese english translation
tasks section method outperforms baseline hierarchial phrase system
b leu points large scale training data
use topic dissimilarity sensitivity improve smt first presented
previous xiao xiong zhang liu lin article provide
detailed comparison related work formulations two well integration



fiz hang x iao x iong l iu

procedure importantly carry large scale experiments bilingual monolingual training data incorporate detailed analysis output topic dissimilarity
sensitivity document translation hypothesis level
rest article organized follows section introduces related work section
provides background knowledge statistical machine translation topic modeling section
elaborates topic translation rule selection framework including topic dissimilarity
topic sensitivity model section discusses estimate rule topic document topic distributions project target side rule topic distributions onto source side topic space
one many mapping fashion section presents integration topic translation rule
selection hierarchical phrase smt section describes series experiments
verify effectiveness section provides detailed analysis output
section gives suggestions bilingual topic modeling perspective
machine translation finally conclude section future directions

related work
topic dissimilarity sensitivity translation rule selection related three
categories work smt translation rule selection topic smt document level
translation section introduce related approaches three categories highlight
differences method previous work
translation rule selection
mentioned translation rule selection important task smt several approaches proposed recently carpuat wu explore word phrase sense
disambiguation wsd psd translation rule selection phrase smt carpuat wu
b wsd psd system integrate sentence level local collocation features experiments multi word psd improve phrase selection following wsd line
chan et al integrate wsd system hierarchical phrase smt lexical selection
selection short phrases length wsd system adopts sentence level
features local collocations surrounding words
different lexical phrasal selection wsd psd et al propose maximum entropy maxent model context dependent synchronous rule selection hierarchical phrase smt local context features phrase boundary words part speech
information incorporated model liu et al extends selection method
et al integrate similar maxent rule selection model tree string syntax
smt system liu liu lin model uses syntactic information source parse
trees features
significant difference topic rule selection framework previous approaches translation rule selection use global topic information help select translation rules ambiguous source segments rather sentence level local context features
topic smt
topic modeling hofmann blei et al popular technique discovering underlying
topic structures documents recent years witnessed topic explored


fit opic issimilarity



ensitivity odels

smt zhao xing tam lane schultz proposed topicspecific lexicon translation adaptation improve translation quality focus
word level translations first estimate word translation probabilities conditioned topics
adapt lexical translation probabilities phrases topic conditioned probabilities since
modern smt systems use synchronous rules bilingual phrases translate sentences believe
reasonable incorporate topic phrase synchronous rule selection
lexical selection
gong zhang zhou adopt topic model filter phrase pairs consistent source documents terms topics assign topic document
translated similarly phrase pair assigned one topic phrase pair
discarded topic mismatches document topic differences work twofold
first calculate dissimilarities translation rules documents topic distributions instead comparing best topics assigned translation rules documents
second integrate topic information smt soft constraint manner via topic
explore topic information hard constraint fashion discarding translation rules
unmatched topics
topic used domain adaptation translation language smt
foster kuhn describe mixture model smt adaptation divide
training corpus different domains used train domain specific translation
model decoding combine general domain translation model specific domain
translation model selected according text distances calculated topic model
tam et al ruiz federico use bilingual topic model project latent topic
distributions across languages bilingual topic model apply source side topic
weights onto target side topic model adapt target side n gram language model
document level machine translation
since incorporate document topic information smt work related documentlevel machine translation tiedemann integrates cache language translation built recently translated sentences smt gong zhang zhou
extend cache introducing two additional caches static cache stores
phrases extracted documents training data similar document question
topic cache target language topic words xiao zhu yao zhang try solve
translation consistency issue document level translation introducing hard constraint
ambiguous source words required consistently translated frequent translation options ture oard resnik soften consistency constraint integrating three
counting features decoder studies normally focus surface structure capture inter sentence dependencies document level machine translation explore topic
structure document document translation

preliminaries
establish section background knowledge statistical machine translation
topic modeling although introduction short sufficient understanding



fiz hang x iao x iong l iu

sub
pi
logp ei f
p
logp f ei
p
logplex ei f
p
logplex f ei
p e
logp ei e ei
pi
log ei f
e


descriptions
direct translation probabilities
inverse translation probabilities
direct lexical translation probabilities
inverse lexical translation probabilities
language model
reordering model
word count
rule count

table widely used sub statistical machine translation number
translation rules used generate target sentence e given source sentence
f ei f target source side translation rule ri

topic dissimilarity sensitivity try bridge gap topic modeling
statistical machine translation
statistical machine translation
given source sentence f smt systems best translation e among possible translations follows
hp




exp

h
f
e


hp

e argmax p



e
e exp
hm f e



x

hm f e
argmax exp
e

argmax
e






x



hm f e



hm f e feature function defined source sentence f corresponding
transla
hp
p


tion e weight feature function since normalization e exp
hm f e

constant possible translations e need calculate decoding
weighted model equation log linear model feature functions hm f e
referred sub components log linear model table
widely used feature functions smt easily factored
translation rules facilitates application dynamic programming decoding
proposed topic dissimilarity sensitivity easily factorized
section
notation used want emphasize sub model component log linear model otherwise
call language model reordering model



fit opic issimilarity



ensitivity odels

log linear model smt sub trained separately combined
assumption independent associated weights tuned
minimum error rate training mert och margin infused relaxed
mira chiang marton resnik note normalization factor equation
calculated training directly optimize
log linear model smt towards translation quality measure bleu feature weights
optimized towards criteria maximum mutual information mmi necessarily
optimal respect translation quality och
integrate proposed two log linear model hierarchical phrasebased smt system section order validate effectiveness two provide
details hierarchical phrase smt chiang section translation rules
hierarchial phrase smt synchronous context free grammar rules denoted
follows
x h

x undifferentiated nonterminal strings terminals nonterminals
source target side respectively denotes one one mapping nonterminals
nonterminals rules automatically extracted word aligned bilingual
training data addition rules two special rules introduced hierarchical
phrase smt
hx x
hs x x



two rules used serially concatenate nonterminal xs monotonic manner form
initial symbol start symbol grammar hierarchical phrase smt
log linear model hierarchical phrase smt formulated follows

x
log r lm logplm e wp e rp

w exp
rd

derivation defined set triples r j denotes application
translation rule spans words j source side number translation rules
probability translation rule r defined
r p p plex plex



lexical translation probabilities plex plex estimate probabilities
words translate words word word fashion koehn et al
topic modeling
topic modeling used discover topics occur collection documents latent
dirichlet allocation lda blei et al probabilistic latent semantic analysis plsa
order simplify decoder implementation two nonterminals allowed hierarchical translation
rules



fiz hang x iao x iong l iu

hofmann topic lda widely used topic model exploit
mine topics translation rule selection
lda views document mixture topics probability distribution words particularly lda works generative process follows
document dj sample document topic distribution per document topic distribution j dirichlet distribution dir j dir
word wj nj words document dj
sample topic assignment zj multinomial j
sample word wj multinomial zj zj per topic word distribution topic zj drawn dir
generally speaking lda contains two groups parameters first group parameters
characterizes document topic distributions j record distribution document
topics second group parameters used topic word distributions k represent
topic distribution words
given document collection observed words w wj goal lda inference
compute values two sets parameters well latent topic assignments
z zj inference complicated due latent topic assignments z efficient inference
proposed address collapsed gibbs sampling griffiths
steyvers two sets parameters integrated lda model
latent topic assignments z sampled p z w obtain values z
estimate recovering posterior distributions given z w section
use two sets estimated parameters topic assignments words calculate
parameters

topic dissimilarity sensitivity
section elaborate topic translation rule selection including topic
dissimilarity model topic sensitivity model
topic dissimilarity model
sentences translated accordance topics zhao xing tam
et al take translation rule b figure example source side rule
b occurs document international relations hope encourage application rule
b rather rule c achieved calculating dissimilarity probability
distributions translation rule document topics
order calculate topic dissimilarity translation rule selection associate
source target side translation rule rule topic distribution p z r
placeholder source side f target side e r source target side translation
rule r z corresponding topic r therefore translation rule two rule topic
distributions p zf rf source side p ze target side


fit opic issimilarity



ensitivity odels

supposing k topics two distributions represented k dimension vector k th component p z k r denotes probability topic k given r sourceand target side rule topic distributions separately estimated training data estimation
method described section discuss reason estimate
separate manner
analogously represent topic information document translated documenttopic distribution p z k dimension vector k th dimension p z k
topic proportion topic k document different rule topic distribution
document topic distribution directly inferred shelf lda tool
defined rule topic document topic distributions measure dissimilarity translation rule document decide whether rule suitable document
translation traditionally similarity two probability distributions calculated information measurements jensen shannon divergence lin hellinger distance blei
lafferty
adopt hellinger distance hd measure topic dissimilarity symmetric widely used comparing two probability distributions blei lafferty given
rule topic distribution p z r document topic distribution p z hd computed
follows
k p

x
p
p z k p z k r

hd p z p z r
k

let derivation defined section let p z r represent corresponding rule topic
distributions rules topic dissimilarity model dsim p z p z r derivation
defined hd equation follows
x
dsim p z p z r
hd p z p z r

rd

obviously larger hellinger distance candidate translation yielded derivation
document larger dissimilarity topic dissimilarity model
defined aim select translation rules similar document translated
terms topics
topic sensitivity model
introduce topic sensitivity model lets revisit figure easily
probability rule c distributes evenly topics indicates insensitive topics
therefore applied topics contrast distributions three rules
peak topics generally speaking topic insensitive rule fairly flat distribution
topics topic sensitive rule sharp distribution topics
document typically focuses topics sharp distribution topics
words documents normally topic sensitive since distribution topic insensitive
rule fairly flat dissimilarity topic insensitive rule topic sensitive document
low therefore system proposed topic dissimilarity model punish
topic insensitive rules


fiz hang x iao x iong l iu

however topic insensitive rules may preferable topic sensitive rules neither
similar given documents document topic love rule b c
figure dissimilar document rule b relates international relations topic
rule c topic insensitive nevertheless since rule c occurs frequently across
topics prefer rule c rule b translate document love
address issue topic dissimilarity model propose topic sensitivity
model model employs entropy metric measure topic sensitivity rule
follows
k
x
p z k r log p z k r

h p z r
k

according equation topic insensitive rule normally large entropy topicsensitive rule smaller entropy
given derivation rule topic distributions p z r rules topic sensitivity
model defined follows
x
h p z r

sen p z r
rd

incorporating topic sensitivity model topic dissimilarity model enable smt
system balance selection topic sensitive topic insensitive rules given rules approximately equal values topic dissimilarity prefer topic insensitive rules

estimation
unlike document topic distributions directly learned lda tools need estimate
rule topic distributions translation rules want exploit topic information
source target language separately train two monolingual topic source
target side learn correspondences two topic via word alignments
bilingual training data
particularly adopt two rule topic distributions translation rule source side
rule topic distribution p zf rf target side rule topic distribution p ze
defined section two rule topic distributions estimated trained
topic way section notably source language documents available
decoding order compute dissimilarity target side rule topic distribution
translation rule source side document topic distribution given document need
project target side rule topic distribution translation rule onto space source side
topic model section
establish alternative approaches estimation rule topic distributions via
multilingual topic mimno wallach naradowsky smith mccallum boyd graber
blei bilingual topic infer word word alignments document pairs
zhao xing former multilingual topic require documents
different languages comparable terms content similarity contrast latter bilingual
topic require documents parallel e translations capture
word alignments





fit opic issimilarity

z

n



ensitivity odels

z

w

n



z

w
z

topic
correspondence

n

z



k

k

target

source



w

nl

n



word
alignment

n



w




w

z

z

e

j

w


l



l

k



f

j





k

k

b



target

source



b

c

figure graphical model representations bilingual topic model b polylingual topic
model mimno et al c bilingual topic model zhao xing
number parallel sentence pairs document word alignment
source target sentence simplicity display hmm transitions
among word alignments subfigure shows build topic correspondences source target language source target topics separately learned
shown

biggest difference method multilingual bilingual topic
use per tuple topic distribution documents tuple define
tuple set documents different languages per tuple topic distribution similar
per document topic distribution difference per tuple topic
distribution shared documents tuple
topic assignments words languages naturally connected since sampled
topic distribution contrast assume document source target
side sampled document specific distribution topics topic correspondences source target document learned projection via word alignments visualize
difference figure
yet another difference topic specific lexicon translation model
zhao xing use bilingual topics improve smt word level
instead rule level since synchronous rule rarely factorized individual words
believe reasonable incorporate topic model directly rule level rather
word level section empirically compare model topic specific lexicon
translation model
tam et al construct two monolingual topic parallel source target
documents build topic correspondences source target documents enforcing one one topic mapping constraint project target side topics onto space
source side topic model one many fashion section compare two different
methods building topic correspondences



fiz hang x iao x iong l iu

rule topic distribution estimation
estimate rule topic distributions word aligned bilingual training corpus document
boundaries explicitly given source target side rule topic distributions estimated
way therefore simplicity describe estimation source side rule topic
distribution p zf rf translation rule section
estimation rule topic distributions analogous traditional estimation rule translation probabilities chiang addition word aligned corpus input rule topic
distribution estimation contains source side document topic distributions inferred lda tool
first extract translation rules bilingual training data traditional way
source side translation rule rf extracted source language document df documenttopic distribution p zf df obtain instance rf p zf df fraction count
instance described chiang way collect set instances
rf p zf df different document topic distributions translation rule
instances calculate probability p zf k rf rf topic k follows
p
p zf k df

p zf k rf pk ii
p

k
ii p zf k df

equation obtain two rule topic distributions p zf rf p ze
rule source target side document topic distributions p zf df p ze de respectively
target side rule topic distribution projection

described previous section estimate target side rule topic distributions however directly use equation calculate dissimilarity target side
rule topic distribution p ze translation rule source side document topic distribution
p zf df source language document translated order measure dissimilarity need project target side topics onto source side topic space projection takes
following two steps
first calculate correspondence probability p zf ze pair target side topic
ze source side topic zf inferred two separately trained monolingual
topic respectively
second project target side rule topic distribution translation rule onto sourceside topic space correspondence probabilities learned first step
first step estimate topic topic correspondence probabilities co occurrence
counts topic assignments source target words word aligned corpus topic assignments source target words inferred two monolingual topic topic
assignments characterize sentence pair f e zf ze zf ze two vectors
containing topic assignments words source target sentence f e respectively
set word alignment links j source target sentence particularly link
j represents source side position aligns target side position j


fit opic issimilarity



ensitivity odels

notations calculate co occurrence count source side topic kf
target side topic ke follows
x x
zfi kf zej ke

zf ze j

zfi zej topic assignments words ej respectively x kronecker
function x otherwise
compute topic topic correspondence probability p zf kf ze ke
normalizing co occurrence count follows
p
p
zf ze
j zfi kf zej ke
p
p

p zf kf ze ke
zf ze
j zej ke

overall first step obtain topic topic correspondence matrix mke kf
item mi j represents probability p zf ze j
second step given correspondence matrix mke kf project target side ruletopic distribution p ze source side topic space multiplication follows
p ze p ze mke kf



way get second distribution translation rule source side topic space
call projected target side topic distribution p ze
word alignment noises may introduced equation turn may flatten
sharpness projected topic distributions calculated equation order decrease
flattening effects word alignment noises take following action practice
topic topic correspondence probability p zf kf ze ke calculated via word alignments

k predefined number topics set normalize
less k
correspondence probabilities target side topic ke
obviously projection method allows one target side topic ze align multiple source side
topics different one one correspondence used tam et al investigate correspondence matrix mke kf obtained training data topic
correspondence source target language necessarily one one typically
correspondence probability p zf kf ze ke target side topic mainly distributes two
three source side topics table shows example target side topic three mainly
aligned source side topics

integration
incorporate topic dissimilarity sensitivity model two features hierarchical
phrase system chiang log linear discriminative framework och ney
dissimilarity values positive hellinger distances positive weight
dissimilarity feature tuned mert negative therefore log linear model favor
candidate translations lower values dissimilarity feature less dissimilar
words translation rules similar document translated terms topics
selected


fiz hang x iao x iong l iu

e topic
enterprises
rural
state
agricultural
market
reform
production
peasants
owned
enterprise
p zf ze

agricultural
rural
peasant
u reform
finance
social
safety
n adjust
policy
income

f topic

enterprise
market
ik state
company
k finance
bank
investment
n manage
u reform
e operation

f topic

u develop
l economic
e technology
china
e technique
industry
structure
innovation
accelerate
u reform

f topic







table example topic topic correspondence last line shows correspondence
probability column shows topic represented top topical words first
column target side topic remaining three columns source side topics

one possible side effect integration dissimilarity feature system
favour translations generated fewer translation rules generated translation
rules translation rules higher dissimilarity see equation
say topic dissimilarity feature acts translation rule count penalty derivations
fortunately however use translation rule count feature see last row table
normally favours translations yielded derivation large number translation rules
feature balance mentioned side effect topic dissimilarity feature
translation rule associated source side rule topic distribution projected
target side rule topic distribution decoding add four features follows
dsim p zf p zf rf dsimsrc topic dissimilarity feature source side rule topic
distributions
dsim p zf p ze dsimtrg topic dissimilarity feature projected targetside rule topic distributions
sen p zf rf sensrc topic sensitivity feature source side rule topic distributions
sen p ze sentrg topic sensitivity feature projected target side rule topic
distributions
source side projected target side rule topic distributions translation rules
calculated decoding described last section decoding first infer
topic distribution p zf given document source language translation rule
adopted derivation scores four features updated correspondingly according
equation obviously computational cost features rather small
since glue rule rules unknown words extracted training data set values four
features rules zero



fit opic issimilarity



ensitivity odels

topic specific lexicon translation zhao xing tam et al first
calculate topic specific translation probabilities normalizing entire lexicon translation table
adapt lexical weights translation rules correspondingly decoding makes
decoder run slower therefore comparing previous topic specific lexicon translation methods method provides efficient way incorporating topic smt

experiments
section conducted two groups experiments validate effectiveness topicbased translation rule selection framework first group experiments use medium scale
bilingual data train smt system topic purpose group experiments
quickly answer following questions
topic dissimilarity model able improve translation rule selection terms b leu
furthermore source side target side rule topic distributions complementary

helpful introduce topic sensitivity model distinguish topic insensitive topicsensitive rules
topic method better previous topic specific lexicon translation method zhao
xing terms b leu decoding speed
confirm efficacy topic dissimilarity sensitivity model mediumscale training data conducted second group experiments large scale training data
investigate following questions
one many target side rule topic projection method better previous methods
proposed zhao xing tam et al
effects types rules phrase rules rules
non terminals
else achieve use monolingual data train topic
setup
carried experiments nist chinese english translation used nist evaluation set mt development set sets mt mt test sets
numbers documents mt mt mt respectively case insensitive
nist b leu papineni roukos ward zhu used measure translation performance
used minimum error rate training och optimize feature weights
medium scale experiments used fbis corpus bilingual training data
contains documents k sentence pairs chinese words english
words large scale experiments bilingual training data consists ldc e ldc ldc ldc ldc hong kong hansards laws news



fiz hang x iao x iong l iu

selected corpora contain documents sentences average document sentences
obtained symmetric word alignments training data first running giza och ney
directions applying refinement rule grow diag final koehn et al
hierarchical phrase translation rules extracted word aligned training data
used srilm toolkit stolcke train language xinhua portion
gigaword corpus contains english words trained gram language model
medium scale experiments gram language model large scale experiments
order train two monolingual topic source target side bilingual
training data used open source lda tool gibbslda gibsslda implementation
lda gibbs sampling parameter estimation inference source targetside topic separately estimated chinese english part bilingual
training data set number topic k source target side topic
used default setting tool training inference decoding inferred
document topic distribution document dev test sets translation
trained source side topic model note topic inference dev test sets performed
parameters two topic estimated training data
case insensitive bleu used evaluation metric performed statistical
significance bleu differences paired bootstrap sampling koehn order
alleviate impact instability mert ran tuning process three times
large scale experiments presented average bleu scores three runs following
suggestion clark dyer lavie smith
medium scale experiments
section conducted medium scale experiments investigate effectiveness two
topic translation rule selection
e ffect



opic issimilarity odel

quickly investigated effectiveness topic dissimilarity sensitivity model
medium scale training data shown table table observe
use topic dissimilarity model source side projected target side ruletopic distributions dsimsrc dsimtrg table see descriptions section
obtain absolute improvement b leu points baseline
combine two topic dissimilarity features together achieve improvement b leu points dsimsrc
two observations topic dissimilarity model able improve translation quality
terms b leu
http gibbslda sourceforge net
determine k testing preliminary experiments k produces
slightly better performance values order improve stability topic estimation run
tool multiple times use best model respect log likelihood



fit opic issimilarity

system
baseline
topiclex
dsimsrc
dsimtrg
dsimsrc dsimtrg
dsim sen

mt









ensitivity odels

mt







avg







speed







table topic dissimilarity sensitivity model terms b leu speed words
per second comparing traditional hierarchical system baseline system topic specific lexicon translation model topiclex dsimsrc dsimtrg topic dissimilarity features source side projected target side ruletopic distributions respectively dsim sen activates two dissimilarity features
two sensitivity features described section avg denotes average b leu
scores two test sets scores bold significantly better baseline p
speed denotes number words translated per second
rule type
phrase
monotone
reordering


count





src sen





trg sen





table percentages topic sensitive rules listed rule types according entropies
source side src target side trg rule topic distributions phrase rules fully
lexicalized monotone reordering rules contain nonterminals

order gain insights topic dissimilarity model helpful translation rule
selection investigate many rules topic sensitive described section
use entropy measure whether translation rule topic sensitive rule topic distribution entropy translation rule calculated equation smaller certain
threshold rule topic sensitive since documents often focus topics use average entropy document topic distributions training documents threshold compare
entropies source side target side rule topic distributions threshold findings
shown table translation rules topic sensitive rules compare entropies
source side rule topic distributions threshold compare entropies targetside rule topic distributions threshold topic sensitive rules account
strongly suggest rules occur documents specific topics topic information
used improve translation rule selection
e ffect



opic ensitivity odel

see table still translation rules generic sensitive topics rules widely used documents mentioned


fiz hang x iao x iong l iu

topic dissimilarity model punishes rules documents normally topic specific
therefore introduce topic sensitivity model complement topic dissimilarity model experiment model last line table obtain improvement
b leu points incorporating topic sensitivity model indicates necessary
distinguish topic insensitive topic sensitive rules
c omparison



opic pecific l exicon ranslation odel

compared topic topic specific lexicon translation model proposed
zhao xing introduce framework combine hidden markov model hmm
lda topic model smt shown figure framework bilingual sentence
pair single topic assignment sampled document pair topic distribution
words target language e g english sampled given sentence pair topic assignment
monolingual per topic word distribution word alignments words
source language sampled first order markov process topic specific translation
lexicon respectively
zhao xing integrate topic specific word word translation lexicons estimated
bilingual topic model described topic specific lexicon translation model
formulated follows
p wf df p wf df p df
x

p wf z k p z k p z k df



k

model probability candidate translation source word wf source document df calculated marginalizing topics corresponding topic specific translation
lexicons simplify estimation p wf z k directly computing probabilities
word aligned corpus associated target side topic assignments inferred
target side topic model despite simplification improvement implementation comparable improvement obtained zhao xing given document need
adapt lexical translation weights rules adapted lexicon translation model integrated
feature log linear discriminative framework
comparison table topic specific lexicon translation model
better baseline b leu points however topic method combination
topic dissimilarity sensitivity outperforms baseline b leu points
compare two methods terms decoding speed words second baseline translates words per second system topic specific lexicon translation
model translates words one second overhead topic specific lexicon translation model mainly comes adaptation lexical weights takes time
adaptation contrast method speed words per second sentence
average three times faster topic specific lexicon translation method
large scale experiments
section investigated deeper second group experiments
large scale training data


fit opic issimilarity

e ffect





ensitivity odels

ne p rojection

discussed section need project target side topics onto source side topic space
calculate dissimilarity target side rule topic distribution source side
document topic distribution propose one many projection method issue order
investigate effectiveness method conducted experiments large scale training
data compare following methods
one one mapping enforce one one mapping source side target side
topics similar method tam et al achieve aligning target side
topic corresponding source side topic largest correspondence probability
calculated section
marginalization word alignments following zhao xing first obtain
topics target side lda retrieve topics source language
marginalization word alignments follows
x
p wf k
p wf p z k



combination source target language documents concatenate target document aligned source document one document run lda tool
combined documents train one topic model mixed language words decoding
use trained topic model infer topics source documents
order compare one many projection method three methods described
add target side topic dissimilarity feature dsimtrg log linear translation model
experiment reported table clearly four methods achieve improvements
baseline however one many projection method performs better three
methods particular
method outperforms one one topic mapping method indicates sourceside target side topics exactly match one one correspondence manner
reason marginalization method performs worse among four methods may
topic model trained target documents
surprisingly combination method performs quite well shows lda model
hidden topics even mixed language documents

e ffect
rules



opic rule election f ramework



ypes



conducted experiments investigate effect topic
types rules selection particularly divide translation rules hierarchical phrase smt
three types phrase rules contain terminals bilingual phrase
pairs used phrase system monotone rules contain non terminals produce


fiz hang x iao x iong l iu

system
baseline
one one
marginalization
combination
one many

mt






mt






avg






table effect one many topic projection method methods marginalization marginalization word alignments combination combination source
target language documents
system
baseline
phrase rule
monotone rule
reordering rule


mt






mt






avg






table effect topic rule selection three types rules phrase rules
fully lexicalized monotone reordering rules contain nonterminals

monotone translations finally reordering rules contain non terminals change
order translations define monotone reordering rules according chiang et al

study impact topic translation rule type activate
four features described section rules type topic dissimilarity
sensitivity features two types translation rules deactivated
table shows experiment table observe
topic achieve highest improvement b leu points baseline phrase rules among three types translation rules reasonable phrase
rules consist topical words
obtain improvements b leu points baseline monotone
reordering rules respectively shows able help select
appropriate translation rules non terminals
activate topic dissimilarity sensitivity translation rules
still achieve additional improvement b leu points total outperform
baseline absolute improvement b leu points
e ffect



ore onolingual data

comparing table table topic dissimilarity sensitivity
trained medium scale data k documents collectively achieve improvement


fit opic issimilarity



ensitivity odels

system
baseline
dsimsrc sensrc dsimtrg sentrg
dsimsrc sensrc dsimtrg sentrg
dsimsrc sensrc dsimtrg sentrg
dsimsrc sensrc dsimtrg sentrg

mt






mt






avg






table effect monolingual data train topic features bold
topic dissimilarity sensitivity model lda topic model trained
combination source target part large scale bilingual data corresponding
monolingual corpus

b leu points baseline two trained large scale data k documents obtain improvement b leu points suggests performance gains
may obtained data parallel bilingual data document boundaries provided easily accessible try collect monolingual data source target language
interest study whether gain improvements monolingual data
train topic
used chinese monolingual corpus documents collected chinese sohu
weblog collected chinese corpus contains k documents chinese
words used english monolingual corpus documents collected
english blog authorship corpus schler koppel argamon pennebaker english
monolingual corpus consists k documents english words combined
chinese corpus source part large scale bilingual data train source side lda topic
model st english monolingual corpus combined target part large scale
bilingual data train target side lda topic model
used two topic st infer topics test sets topic information source target part large scale bilingual training data inferred st
used estimate source side rule topic distributions projected target side rule topic distributions way obtain topic dissimilarity sensitivity model
source target side
experiment shown table unfortunately obtain improvements training topic larger data combination chinese monolingual
corpus source part bilingual training data instead performance drops
use topic model st build source side dissimilarity sensitivity features
adopt topic model build target side dissimilarity sensitivity
features
one reason lower performance larger topic model training data may
use topics topics may improve larger corpora order
investigate conducted experiments topics trained sourceside topic model combination source part large scale bilingual data sohu
weblog data topic model built source side topic dissimilarity model
http blog sohu com



fiz hang x iao x iong l iu

system
baseline
k
k
k
k

mt






mt






avg






table experiment different number topics k source side topic dissimilarity model dsimsrc integrated smt system
test set
mt
mt

monosrc



bisrc



monobisrc



table hellinger distances mt test sets chinese monolingual corpus
monosrc source part bilingual training data bisrc well combination monobisrc terms average document topic distributions

integrated smt system experiment shown table table
topics able improve model corpora
yet another reason may additional monolingual corpus similar test sets
terms topic distributions order examine hypothesis inferred document topic
distributions documents test sets chinese monolingual corpus source part
bilingual corpus topic model st average document topic distributions
obtain four average document topic distributions mt mt chinese monolingual
corpus source part bilingual corpus respectively average topic distributions approximated corpus topic distributions four corpora calculate
hellinger distances corpus topic distributions test sets chinese
monolingual corpus source part bilingual training data shown table
table clearly additional monolingual corpus much less similar
test sets comparing bilingual training corpus hellinger distance test
set mt monobisrc corpus almost twice large bilingual training data
vs topic model trained enlarged corpus make topic
select translation rules similar documents test sets terms topic
distributions suggests select additional monolingual data similar
test sets want obtain improvements
conducted group experiments empirically examine hypothesis
translating web domain test set similar additional weblog corpus terms
topics used web portion nist mt set development set web
portion nist mt test set displayed table
additional monolingual data improve performance time suggests
select monolingual corpus similar test sets learn topics topic
dissimilarity sensitivity


fit opic issimilarity



ensitivity odels

system
baseline
dsimsrc sensrc dsimtrg sentrg
dsimsrc sensrc dsimtrg sentrg

mt web




table translating web domain test set topic trained
data augmented monolingual weblog corpus features bold topicbased dissimilarity sensitivity model lda topic model trained
combination source target part large scale bilingual data corresponding
monolingual corpus mt web web portion nist mt test set

analysis
section study details topic translation rule selection
looking differences make target documents individual translation hypotheses
differences help us gain insights presented improve translation
quality analysis baseline system system enhanced proposed
topic four features section activated trained large scale bilingual
data described section notational convenience hereafter refer baseline
system base system enhanced topic dissimilarity sensitivity
topsel
differences target documents
order measure impact topic target documents calculate
hellinger distances target documents generated base topsel system reference documents generated human terms topics inferred target side lda topic
model according following steps target side lda topic model trained target
part large scale bilingual data described section
use target side lda topic model infer document topic distribution document
reference translations called reference distribution
use target side lda topic model infer document topic distribution target document generated base system called base distribution
similarly obtain
topsel system

topsel

distribution target document generated

calculate dissimilarity base reference distribution well topsel reference distribution according equation dissimilarities first averaged documents averaged four reference translations
table shows calculated dissimilarities according equation smaller
hellinger distance two items similar average hellinger distance
topsel reference documents distance base reference


fiz hang x iao x iong l iu

system
base
topsel

mt



mt



avg



table dissimilarities measured hellinger distance reference documents target
documents generated base topsel system terms topics according
equation

similar
less similar
p

mt




table number target documents generated
erence documents base

mt



topsel

less similar ref

documents therefore target documents generated topsel system
mt mt similar documents reference translations
baseline system calculate number target documents generated topsel
less similar reference documents base average hellinger
distances numbers shown table according sign test numbers
topsel statistically significantly better baseline system terms similarity
translations generated two systems human generated translations
differences translation hypotheses
look deeper translation hypotheses understand select translation
rules table shows three translation examples compare baseline system
enhanced topic order conduct quantitative comparison calculate
dissimilarity values measured hellinger distance underlined phrases table
topic dissimilarity model dissimilarity values computed projected
target side rule topic distributions underlined phrases source side document topic
distributions corresponding documents phrase used values shown
table
two tables easily observe system topic dissimilarity
model prefers target phrases smaller hellinger distances documents
occur terms topic distributions contrast baseline able use document level
topic information translation rule selection figure shows topic distributions
source side document topsel phrase allow base phrase permit eg
major topics source side document topic topsel phrase allow mainly
distributes different topics including topic base phrase permit
mainly different topics include topic
distribution probability topics larger



fit opic issimilarity

source

ensitivity odels



described northern limit line unlawful
referred northern limit line legitimate
reference pointed northern limit line legitimate
source
base
would permit love others accepted people
topsel
allow love love others accepted people
reference would someone allow person loves accept peoples
love time
source
base
present internet entitled statutory right leave
topsel
present internet enjoy statutory right leave
reference present internet enjoy rights
base

eg



topsel

n gc

eg

p k n

eg

table translation examples nist mt test sets comparing baseline
system enhanced topic underlined words highlight
difference enhanced baseline
phrase
unlawful
legitimate
permit
allow
entitled
enjoy

hd







table dissimilarity values measured hellinger distance underlined phrases table projected target side rule topic distributions corresponding
source side document topic distributions documents calculated topic dissimilarity model

discussion bilingual topic modeling
although topic widely adopted monolingual text analysis bilingual multilingual
topic less explored especially tailored multilingual tasks machine
translation section try provide suggestions bilingual topic modeling
perspective statistical machine translation well practice integration topic smt suggestions listed follows future
directions
investigation topic divergences across different languages cross language divergences
pervasive become one big challenges machine translation dorr
language level divergences hint divergences topic concept level may exist
across languages may explain one many topic projection target side


fiz hang x iao x iong l iu

figure topic distributions source side document
base phrase permit shown eg table

topsel

phrase allow b

source side better one one mapping although mimno et al
studied topic divergences wikipedia articles believe deeper wider
investigation topic divergence needed shed light build
better bilingual topic
adding linguistic assumptions topic modeling practices smt integrating linguistic knowledge machine translation normally generates better translations
chiang et al believe adding linguistic assumptions beyond bag ofwords improve topic modeling flexible topic modeling framework allows us
integrate rich linguistic knowledge form features definitely facilitate
application topic natural language processing
joint modeling topic induction synchronous grammar induction synchronous grammar induction machine translation task automatically learning translation rules
bilingual data blunsom cohn dyer osborne xiao xiong bayesian
approaches successfully used topic modeling synchronous grammar induction joint modeling interesting direction benefit grammar
adaptation one domain another domain machine translation

conclusions
article presented topic translation rule selection framework incorporates topic information source target language translation rule disambiguation particularly use topic dissimilarity model select appropriate translation rules
documents according similarities translation rules documents adopt


fit opic issimilarity



ensitivity odels

topic sensitivity model complement topic dissimilarity model order balance translation
rule selection topic sensitive topic insensitive rules order calculate dissimilarities source target side topic distributions project topic distributions target
side onto source side topic model space efficient way
integrated topic rule selection hierarchical phrase smt
system experiments medium large scale training data
topic dissimilarity sensitivity model able substantially improve translation
quality terms b leu improve translation rule selection types rules e
phrase monotone reordering rules
method better previous topic specific lexicon translation method translation quality decoding speed
proposed one many projection method outperforms methods
one one mapping marginalization via word alignments
want use additional monolingual corpus train topic first investigate whether monolingual corpus similar test data terms topic
distributions
topic provide global document level information machine translation
future would use topic address document level machine translation issues
coherence cohesion barzilay lee hardmeier nivre tiedemann
want integrate topic linguistically syntax machine translation
syntactic translation rule selection liu et al

acknowledgments
work sponsored national natural science foundation china projects
qun lius work partially supported science foundation ireland
grant ce part cngl dublin city university would thank
three anonymous reviewers insightful comments corresponding author article
deyi xiong

references
barzilay r lee l catching drift probabilistic content applications
generation summarization susan dumais roukos eds hlt naacl
main proceedings pp boston massachusetts usa association computational linguistics
blei lafferty j correlated topic model science aas
blei ng jordan latent dirichlet allocation jmlr



fiz hang x iao x iong l iu

blunsom p cohn dyer c osborne gibbs sampler phrasal synchronous
grammar induction proceedings joint conference th annual meeting
acl th international joint conference natural language processing
afnlp pp suntec singapore association computational linguistics
boyd graber j blei multilingual topic unaligned text proceedings
twenty fifth conference uncertainty artificial intelligence uai pp
arlington virginia united states auai press
carpuat wu phrase sense disambiguation outperforms word sense disambiguation statistical machine translation proceedings th conference
theoretical methodological issues machine translation pp
carpuat wu b improving statistical machine translation word sense disambiguation proceedings joint conference empirical methods natural
language processing computational natural language learning emnlp conll pp
prague czech republic association computational linguistics
chan ng h chiang word sense disambiguation improves statistical machine translation proceedings th annual meeting association computational linguistics pp prague czech republic association computational
linguistics
chiang hierarchical phrase model statistical machine translation proc
acl
chiang hierarchical phrase translation computational linguistics

chiang marton resnik p online large margin training syntactic structural
translation features proceedings conference empirical methods natural language processing pp honolulu hawaii association computational
linguistics
clark j h dyer c lavie smith n better hypothesis testing statistical
machine translation controlling optimizer instability proceedings th annual
meeting association computational linguistics human language technologies
pp portland oregon usa
dorr b j machine translation divergences formal description proposed solution
computational linguistics
foster g kuhn r mixture model adaptation smt proc second workshop
statistical machine translation pp prague czech republic
gong z zhang zhou g cache document level statistical machine translation proc emnlp
gong z zhang zhou g statistical machine translation lda proc
iucs p



griffiths l steyvers finding scientific topics proceedings national
academy sciences suppl



fit opic issimilarity



ensitivity odels

hardmeier c nivre j tiedemann j document wide decoding phrase statistical machine translation proceedings joint conference empirical
methods natural language processing computational natural language learning
pp jeju island korea association computational linguistics
z liu q lin improving statistical machine translation lexicalized rule
selection proceedings nd international conference computational linguistics
coling pp manchester uk coling organizing committee
hofmann probabilistic latent semantic analysis proc uai pp
koehn p statistical significance tests machine translation evaluation proceedings
emnlp pp barcelona spain
koehn p och f j marcu statistical phrase translation proc hlt naacl

lin j divergence measures shannon entropy ieee trans inf theor

liu q z liu lin maximum entropy rule selection model syntaxbased statistical machine translation proceedings conference empirical methods natural language processing pp honolulu hawaii association
computational linguistics
liu liu q lin tree string alignment template statistical machine translation proc acl
mimno wallach h naradowsky j smith mccallum polylingual
topic proc emnlp
och f j ney h discriminative training maximum entropy statistical
machine translation proc acl
och f j minimum error rate training statistical machine translation proc acl
och f j ney h systematic comparison statistical alignment
computational linguistics
papineni k roukos ward zhu w j bleu method automatic evaluation
machine translation proc acl
ruiz n federico topic adaptation lecture translation bilingual latent
semantic proceedings sixth workshop statistical machine translation
schler j koppel argamon pennebaker j w effects age gender
blogging aaai spring symposium computational approaches analyzing weblogs pp

stolcke srilm extensible language modeling toolkit proc icslp
tam c lane r schultz bilingual lsa adaptation statistical machine
translation machine translation
tiedemann j context adaptation statistical machine translation exponentially decaying cache proceedings workshop domain adaptation



fiz hang x iao x iong l iu

natural language processing pp uppsala sweden association computational
linguistics
ture f oard w resnik p encouraging consistent translation choices proceedings conference north american chapter association computational linguistics human language technologies pp montreal canada association computational linguistics
xiao zhu j yao zhang h document level consistency verification machine
translation proceedings mt summit xiii pp xiamen china
xiao x xiong max margin synchronous grammar induction machine translation proceedings conference empirical methods natural language
processing pp seattle washington usa association computational linguistics
xiao x xiong zhang liu q lin topic similarity model hierarchical phrase translation proceedings th annual meeting association
computational linguistics long papers pp jeju island korea
association computational linguistics
xiong zhang li h modeling translation predicate argument structure
smt proceedings th annual meeting association computational
linguistics long papers pp jeju island korea association computational linguistics
zhao b xing e p hm bitam bilingual topic exploration word alignment
translation proc nips
zhao b xing e p bitam bilingual topic admixture word alignment
proc acl




