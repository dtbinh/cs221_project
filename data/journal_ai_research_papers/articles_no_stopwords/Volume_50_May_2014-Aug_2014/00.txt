Journal Artificial Intelligence Research 50 (2014) 1-30

Submitted 12/13; published 05/14

Topic-Based Dissimilarity Sensitivity Models
Translation Rule Selection
Min Zhang

MINZHANG @ SUDA . EDU . CN

Provincial Key Laboratory Computer Information Processing Technology,
Soochow University, Suzhou, China

Xinyan Xiao

XIAOXINYAN @ ICT. AC . CN

IIP Key Lab, Institute Computing Technology,
Chinese Academy Sciences, China

Deyi Xiong

DYXIONG @ SUDA . EDU . CN

Provincial Key Laboratory Computer Information Processing Technology,
Soochow University, Suzhou, China

Qun Liu

LIUQUN @ ICT. AC . CN

CNGL, School Computing, Dublin City University, Ireland
IIP Key Lab, Institute Computing Technology,
Chinese Academy Sciences, China

Abstract
Translation rule selection task selecting appropriate translation rules ambiguous
source-language segment. translation ambiguities pervasive statistical machine translation, introduce two topic-based models translation rule selection incorporates global
topic information translation disambiguation. associate synchronous translation rule
source- target-side topic distributions.With topic distributions, propose topic
dissimilarity model select desirable (less dissimilar) rules imposing penalties rules
large value dissimilarity topic distributions given documents. order encourage use non-topic specific translation rules, present topic sensitivity model
balance translation rule selection generic rules topic-specific rules. Furthermore,
project target-side topic distributions onto source-side topic model space benefit
topic information source target language. integrate proposed topic dissimilarity sensitivity model hierarchical phrase-based machine translation synchronous
translation rule selection. Experiments show topic-based translation rule selection model
substantially improve translation quality.

1. Introduction
Translation rules bilingual segments1 establish translation equivalences source
target language. widely used statistical machine translation (SMT) various representations ranging word pairs bilingual phrases synchronous rules word-, phraseand syntax-based SMT respectively. Normally, large number translation rules learnt
bilingual training data single source segment occurs different contexts.
example, Xiong, Zhang, Li (2012) observe Chinese verb translated
1. segment defined string terminals and/or nonterminals.

c
2014
AI Access Foundation. rights reserved.

fiZ HANG , X IAO , X IONG , & L IU

140 different translation rules average. Therefore select appropriate translation
rule ambiguous source segment crucial issue SMT.
Traditionally appropriateness translation rule measured multiple probabilities
estimated word-aligned data, bidirectional translation probabilities (Koehn, Och, &
Marcu, 2003). probabilities fail capture local global contexts highly ambiguous
source segments, sufficient select correct translation rules segments. Therefore various approaches proposed capture rich contexts sentence level help
select proper translation rules phrase- (Carpuat & Wu, 2007a) syntax-based SMT (Chan, Ng,
& Chiang, 2007; He, Liu, & Lin, 2008; Liu, He, Liu, & Lin, 2008). studies show local
features, surrounding words, syntactic information on, helpful translation rule
selection.
Beyond contextual features sentence level, conjecture translation rules
related high-level global information, topic (Hofmann, 1999; Blei, Ng, & Jordan,
2003) information document level. order visualize relatedness translation
rules document topics, show four hierarchical phrase-based translation rules topic
distributions Figure 1. figure, observe
First, translation rules divided two categories terms topic distributions:
topic-sensitive rules (i.e., topic-specific rules) topic-insensitive rules (i.e., non-topic specific generic rules). former rules, e.g., translation rule (a), (b) (d) Figure
1, much higher distribution probabilities specific topics topics.
latter rules, e.g., translation rule (c) Figure 1, even distribution topics.
Second, topic information used disambiguate ambiguous source segments. Figure
1, translation rule (b) (c) source segment. However topic distributions
quite different. Rule (b) distributes topic international relations
highest probability, suggests rule (b) much related topic
topics. contrast, rule (c) even distribution topics. Therefore document
international relations, rule (b) appropriate rule (c) source
X1 .
segment



two observations suggest different translation rules different topic distributions
document-level topic information used benefit translation rule selection.
article, propose framework translation rule selection exactly capitalizes
document-level topic information. proposed topic-based translation rule selection framework
associates translation rule topic distribution (rule-topic distribution) source
target side. source document annotated corresponding topic distribution
(document-topic distribution). Dissimilarity document-topic distribution rule-topic
distribution calculated used help select translation rules related documents
terms topics. particular,
Given document translated, use topic dissimilarity model calculate dissimilarity translation rule document based topic distributions.
translation system penalize candidate translations high dissimilarities.2
2. Section 6 explains system penalizes candidate translations high dissimilarities.

2

fiT OPIC -BASED ISSIMILARITY



ENSITIVITY ODELS

0.6

0.6

0.4

0.4

0.2

0.2

0

U operational capability

1

(a)

5

10

15

20

25

0
30

1

0.6

0.6

0.4

0.4

0.2

0.2

0
1

5

(c)

X
10

1

0
15

20

25

5

(b)

30

1

give X1

(d) X1

X
10

15

1 ! X
5

20

25

30

25

30

grants X1

1

10

2

15

20

held talks X1 X2

Figure 1: Four synchronous rules topic distributions. sub-graph shows rule
topic distribution, X-axis shows topic index Y-axis topic probability. Notably, rule (b) rule (c) shares source Chinese string,
different topic distributions due different English translations.

dissimilarity topic-insensitive translation rule given source document
computed topic dissimilarity model often high documents normally
topic-sensitive. dont want penalize generic topic-insensitive rules. Therefore
propose topic sensitivity model rewards topic-insensitive rules
complement topic dissimilarity model.
associate translation rule rule-topic distribution source target side. order calculate dissimilarity target-side rule-topic distributions
translation rules source-side document-topic distributions given documents
decoding, project target-side rule-topic distributions translation rules onto space
source-side document topic model one-to-many mapping.
use hierarchical phrase-based SMT system (Chiang, 2007) validate effectiveness
topic-based models translation rule selection. Experiments Chinese-English translation
tasks (Section 7) show method outperforms baseline hierarchial phrase-based system
+1.2 B LEU points large-scale training data.
use topic-based dissimilarity sensitivity models improve SMT first presented
previous paper (Xiao, Xiong, Zhang, Liu, & Lin, 2012). article, provide
detailed comparison related work formulations two models well integration

3

fiZ HANG , X IAO , X IONG , & L IU

procedure. importantly, carry large-scale experiments bilingual monolingual training data incorporate detailed analysis output topic-based dissimilarity
sensitivity models document translation hypothesis level.
rest article organized follows. Section 2 introduces related work. Section 3
provides background knowledge statistical machine translation topic modeling. Section 4
elaborates topic-based translation rule selection framework, including topic dissimilarity
topic sensitivity model. Section 5 discusses estimate rule-topic document-topic distributions project target-side rule-topic distributions onto source-side topic space
one-to-many mapping fashion. Section 6 presents integration topic-based translation rule
selection models hierarchical phrase-based SMT. Section 7 describes series experiments
verify effectiveness approach. Section 8 provides detailed analysis output
models. Section 9 gives suggestions bilingual topic modeling perspective
machine translation. Finally, conclude Section 10 future directions.

2. Related Work
topic-based dissimilarity sensitivity models translation rule selection related three
categories work SMT: translation rule selection, topic models SMT document-level
translation. section, introduce related approaches three categories highlight
differences method previous work.
2.1 Translation Rule Selection
mentioned before, translation rule selection important task SMT. Several approaches proposed recently. Carpuat Wu explore word phrase sense
disambiguation (WSD PSD) translation rule selection phrase-based SMT (Carpuat & Wu,
2007a, 2007b). WSD PSD system integrate sentence-level local collocation features. Experiments show multi-word PSD improve phrase selection. following WSD line,
Chan et al. (2007) integrate WSD system hierarchical phrase-based SMT lexical selection
selection short phrases length 1 2. WSD system adopts sentence-level
features local collocations, surrounding words on.
Different lexical phrasal selection using WSD/PSD, et al. (2008) propose maximum entropy (MaxEnt) based model context-dependent synchronous rule selection hierarchical phrase-based SMT. Local context features phrase boundary words part-of-speech
information incorporated model. Liu et al. (2008) extends selection method
et al. integrate similar MaxEnt-based rule selection model tree-to-string syntax-based
SMT system (Liu, Liu, & Lin, 2006). model uses syntactic information source parse
trees features.
significant difference topic-based rule selection framework previous approaches translation rule selection use global topic information help select translation rules ambiguous source segments rather sentence-level local context features.
2.2 Topic Models SMT
Topic modeling (Hofmann, 1999; Blei et al., 2003) popular technique discovering underlying
topic structures documents. Recent years witnessed topic models explored
4

fiT OPIC -BASED ISSIMILARITY



ENSITIVITY ODELS

SMT. Zhao Xing (2006, 2007) Tam, Lane, Schultz (2007) proposed topicspecific lexicon translation adaptation models improve translation quality. models focus
word-level translations. first estimate word translation probabilities conditioned topics,
adapt lexical translation probabilities phrases topic-conditioned probabilities. Since
modern SMT systems use synchronous rules bilingual phrases translate sentences, believe
reasonable incorporate topic models phrase synchronous rule selection
lexical selection.
Gong, Zhang, Zhou (2010) adopt topic model filter phrase pairs consistent source documents terms topics. assign topic document
translated. Similarly, phrase pair assigned one topic. phrase pair
discarded topic mismatches document topic. differences work twofold.
First, calculate dissimilarities translation rules documents based topic distributions instead comparing best topics assigned translation rules documents.
Second, integrate topic information SMT soft-constraint manner via topic-based
models. explore topic information hard-constraint fashion discarding translation rules
unmatched topics.
Topic models used domain adaptation translation language models SMT.
Foster Kuhn (2007) describe mixture model approach SMT adaptation. divide
training corpus different domains, used train domain-specific translation
model. decoding, combine general domain translation model specific domain
translation model selected according various text distances calculated topic model.
Tam et al. (2007) Ruiz Federico (2011) use bilingual topic model project latent topic
distributions across languages. Based bilingual topic model, apply source-side topic
weights onto target-side topic model adapt target-side n-gram language model.
2.3 Document-Level Machine Translation
Since incorporate document topic information SMT, work related documentlevel machine translation. Tiedemann (2010) integrates cache-based language translation models built recently translated sentences SMT. Gong, Zhang, Zhou (2011)
extend cache-based approach introducing two additional caches: static cache stores
phrases extracted documents training data similar document question
topic cache target language topic words. Xiao, Zhu, Yao, Zhang (2011) try solve
translation consistency issue document-level translation introducing hard constraint
ambiguous source words required consistently translated frequent translation options. Ture, Oard, Resnik (2012) soften consistency constraint integrating three
counting features decoder. studies normally focus surface structure capture inter-sentence dependencies document-level machine translation explore topic
structure document document translation.

3. Preliminaries
establish section background knowledge statistical machine translation
topic modeling. Although introduction short, sufficient understanding

5

fiZ HANG , X IAO , X IONG , & L IU

Sub-models
PI
logP (ei |f )
P1I
logP (f |ei )
P1I
logPlex (ei |f )
P1I
logPlex (f |ei )
P1|e|
logP (ei |e1 ...ei1 )
PI1
1 log(ei , f )
|e|


Descriptions
direct translation probabilities
inverse translation probabilities
direct lexical translation probabilities
inverse lexical translation probabilities
language model
reordering model
word count
rule count

Table 1: widely-used sub-models statistical machine translation. number
translation rules used generate target sentence e given source sentence
f . ei f target source side translation rule ri .

topic-based dissimilarity sensitivity models try bridge gap topic modeling
statistical machine translation.
3.1 Statistical Machine Translation
Given source sentence f , SMT systems find best translation e among possible translations follows.
hP




exp

h
(f,
e)

1
hP

e = argmax P



e
e exp
1 hm (f, e )
#)
(
"
X
(1)
hm (f, e)
= argmax exp
e

= argmax
e

m=1

(


X

)

hm (f, e)

m=1

hm (f, e) feature function defined source sentence f corresponding
transla-i
hP
P


tion e, weight feature function. Since normalization e exp
1 hm (f, e )

constant possible translations e , need calculate decoding.
weighted model equation (1) log-linear model. feature functions hm (f, e)
referred sub-models3 components log-linear model. Table 1,
show widely-used feature functions SMT. easily factored
translation rules, facilitates application dynamic programming decoding.
show proposed topic-based dissimilarity sensitivity models easily factorized
Section 4.
3. notation used want emphasize sub-model component log-linear model. Otherwise
call models, language model, reordering model on.

6

fiT OPIC -BASED ISSIMILARITY



ENSITIVITY ODELS

log-linear model SMT, sub-models trained separately combined
assumption independent other. associated weights tuned
using minimum error rate training (MERT) (Och, 2003) Margin Infused Relaxed Algorithm
(MIRA) (Chiang, Marton, & Resnik, 2008). Note normalization factor equation (1)
calculated training algorithms. algorithms directly optimize
log-linear model SMT towards translation quality measure BLEU. Feature weights
optimized towards criteria Maximum Mutual Information (MMI) necessarily
optimal respect translation quality (Och, 2003).
integrate proposed two models log-linear model hierarchical phrasebased SMT system (Section 6) order validate effectiveness two models, provide
details hierarchical phrase-based SMT (Chiang, 2005) section. Translation rules
hierarchial phrase-based SMT synchronous context-free grammar rules, denoted
follows.
X h, ,
(2)
X undifferentiated nonterminal, strings terminals nonterminals4
source target side respectively, denotes one-to-one mapping nonterminals
nonterminals . rules automatically extracted word-aligned bilingual
training data. addition rules, two special rules introduced hierarchical
phrase-based SMT.
hX1 , X1
hS0 X1 , S0 X1

(3)

two rules used serially concatenate nonterminal Xs monotonic manner form
initial symbol S, start symbol grammar hierarchical phrase-based SMT.
log-linear model hierarchical phrase-based SMT formulated follows.
!
X
log(t(r)) + lm logPlm (e) + wp |e| + rp
(4)
w(D) = exp
rD

derivation defined set triples (r, i, j), denotes application
translation rule spans words j source side. number translation rules
D. probability translation rule r defined
t(r) = P (|)1 P (|)2 Plex (|)3 Plex (|)4

(5)

lexical translation probabilities Plex (|) Plex (|) estimate probabilities
words translate words word-by-word fashion (Koehn et al., 2003).
3.2 Topic Modeling
Topic modeling used discover topics occur collection documents. Latent
Dirichlet Allocation (LDA) (Blei et al., 2003) Probabilistic Latent Semantic Analysis (PLSA)
4. order simplify decoder implementation, two nonterminals allowed hierarchical translation
rules.

7

fiZ HANG , X IAO , X IONG , & L IU

(Hofmann, 1999) topic models. LDA widely used topic model, exploit
mine topics translation rule selection.
LDA views document mixture various topics, probability distribution words. particularly, LDA works generative process follows.
document Dj , sample document-topic distribution (per-document topic distribution) j Dirichlet distribution Dir(): j Dir();
word wj,i Nj words document Dj ,
Sample topic assignment zj,i Multinomial(j );
Sample word wj,i Multinomial(zj,i ) zj,i per-topic word distribution topic zj,i drawn Dir().
Generally speaking, LDA contains two groups parameters. first group parameters
characterizes document-topic distributions (j ), record distribution document
topics. second group parameters used topic-word distributions (k ), represent
topic distribution words.
Given document collection observed words w = {wj,i }, goal LDA inference
compute values two sets parameters well latent topic assignments
z = {zj,i }. inference complicated due latent topic assignments z. efficient inference
algorithm proposed address problem Collapsed Gibbs Sampling (Griffiths
& Steyvers, 2004), two sets parameters integrated LDA model,
latent topic assignments z sampled P (z|w). obtain values z,
estimate recovering posterior distributions given z w. Section 4,
use two sets estimated parameters topic assignments words calculate
parameters models.

4. Topic-based Dissimilarity Sensitivity Models
section, elaborate topic-based models translation rule selection, including topic
dissimilarity model topic sensitivity model.
4.1 Topic Dissimilarity Model
Sentences translated accordance topics (Zhao & Xing, 2006, 2007; Tam
et al., 2007). Take translation rule (b) Figure 1 example. source side rule
(b) occurs document international relations, hope encourage application rule
(b) rather rule (c). achieved calculating dissimilarity probability
distributions translation rule document topics.
order calculate topic dissimilarity translation rule selection, associate
source target side translation rule rule-topic distribution P (z |r ),
placeholder source side f target side e, r source target side translation
rule r, z corresponding topic r . Therefore translation rule two rule-topic
distributions: P (zf |rf ) source side P (ze |re ) target side.
8

fiT OPIC -BASED ISSIMILARITY



ENSITIVITY ODELS

Supposing K topics, two distributions represented K-dimension vector. k-th component P (z = k|r ) denotes probability topic k given r . sourceand target-side rule-topic distributions separately estimated training data. estimation
method described Section 5, discuss reason estimate
separate manner.
Analogously, represent topic information document translated documenttopic distribution P (z|d), K-dimension vector. k-th dimension P (z = k|d)
topic proportion topic k document d. Different rule-topic distribution,
document-topic distribution directly inferred off-the-shelf LDA tool.
Based defined rule-topic document-topic distributions, measure dissimilarity translation rule document decide whether rule suitable document
translation. Traditionally, similarity two probability distributions calculated information measurements Jensen-Shannon divergence (Lin, 2006) Hellinger distance (Blei &
Lafferty, 2007).
adopt Hellinger distance (HD) measure topic dissimilarity, symmetric widely used comparing two probability distributions (Blei & Lafferty, 2007). Given
rule-topic distribution P (z |r ) document-topic distribution P (z|d), HD computed
follows.
K p
2
X
p
P (z = k|d) P (z = k|r )
(6)
HD(P (z|d), P (z |r )) =
k=1

Let derivation defined Section 3.1. Let P(z|r) represent corresponding rule-topic
distributions rules D. topic dissimilarity model Dsim(P (z|d), P(z|r)) derivation
defined HD equation (6) follows
X
Dsim(P (z|d), P(z|r)) =
HD(P (z|d), P (z |r ))
(7)
rD

Obviously, larger Hellinger distance candidate translation yielded derivation
document, larger dissimilarity them. topic dissimilarity model
defined above, aim select translation rules similar document translated
terms topics.
4.2 Topic Sensitivity Model
introduce topic sensitivity model, lets revisit Figure 1. easily find
probability rule (c) distributes evenly topics. indicates insensitive topics,
therefore applied topics. contrast, distributions three rules
peak topics. Generally speaking, topic-insensitive rule fairly flat distribution
topics, topic-sensitive rule sharp distribution topics.
document typically focuses topics, sharp distribution topics.
words, documents normally topic-sensitive. Since distribution topic-insensitive
rule fairly flat, dissimilarity topic-insensitive rule topic-sensitive document
low. Therefore, system proposed topic dissimilarity model punish
topic-insensitive rules.
9

fiZ HANG , X IAO , X IONG , & L IU

However, topic-insensitive rules may preferable topic-sensitive rules neither
similar given documents. document topic love, rule (b) (c)
Figure 1 dissimilar document rule (b) relates international relations topic
rule (c) topic-insensitive. Nevertheless, since rule (c) occurs frequently across various
topics, prefer rule (c) rule (b) translate document love.
address issue topic dissimilarity model, propose topic sensitivity
model. model employs entropy based metric measure topic sensitivity rule
follows
K
X
P (z = k|r ) log(P (z = k|r ))
(8)
H(P (z |r )) =
k=1

According equation, topic-insensitive rule normally large entropy topicsensitive rule smaller entropy.
Given derivation rule-topic distributions P(z|r) rules D, topic sensitivity
model defined follows.
X
H(P (z |r ))
(9)
Sen(P(z|r)) =
rD

Incorporating topic sensitivity model topic dissimilarity model, enable SMT
system balance selection topic-sensitive topic-insensitive rules. Given rules approximately equal values topic dissimilarity, prefer topic-insensitive rules.

5. Estimation
Unlike document-topic distributions directly learned LDA tools, need estimate
rule-topic distributions translation rules. want exploit topic information
source target language, separately train two monolingual topic models source
target side, learn correspondences two topic models via word alignments
bilingual training data.
Particularly, adopt two rule-topic distributions translation rule: 1) source-side
rule-topic distribution P (zf |rf ) 2) target-side rule-topic distribution P (ze |re ),
defined Section 4.1. two rule-topic distributions estimated using trained
topic models way (Section 5.1). Notably, source-language documents available
decoding. order compute dissimilarity target-side rule-topic distribution
translation rule source-side document-topic distribution given document need
project target-side rule-topic distribution translation rule onto space source-side
topic model (Section 5.2).
establish alternative approaches estimation rule-topic distributions via
multilingual topic models (Mimno, Wallach, Naradowsky, Smith, & McCallum, 2009; Boyd-Graber
& Blei, 2009) bilingual topic models infer word-to-word alignments document pairs
(Zhao & Xing, 2006, 2007). former multilingual topic models require documents
different languages comparable terms content similarity. contrast, latter bilingual
topic models require documents parallel, i.e., translations other, capture
word alignments.



10

fiT OPIC -BASED ISSIMILARITY

Z

N



ENSITIVITY ODELS

Z

W

N



Z

W
Z

topic
correspondence

N1

Z



k

k

target

source

(a)

W

NL

N



word
alignment

N



W




W

Z

Z

e

J

W
1

L

1

L

k



f

J





k

k

B



target

source

(a*)

(b)

(c)

Figure 2: Graphical model representations (a) bilingual topic model, (b) polylingual topic
model Mimno et al. (2009), (c) bilingual topic model Zhao Xing (2007)
number parallel sentence pairs document, word alignment
source target sentence. simplicity, display HMM transitions
among word alignments a. Subfigure (a*) shows build topic correspondences source target language source target topics separately learned
shown (a).

biggest difference method multilingual/bilingual topic models
use per-tuple topic distribution documents tuple. define
tuple set documents different languages. per-tuple topic distribution similar
per-document topic distribution. difference per-tuple topic
distribution shared documents tuple.
Topic assignments words languages naturally connected since sampled
topic distribution. contrast, assume document source/target
side sampled document-specific distribution topics. Topic correspondences source target document learned projection via word alignments. visualize
difference Figure 2.
Yet another difference models topic-specific lexicon translation model
Zhao Xing (2007) use bilingual topics improve SMT word level
instead rule level. Since synchronous rule rarely factorized individual words,
believe reasonable incorporate topic model directly rule level rather
word level. Section 7.2.3, empirically compare model topic-specific lexicon
translation model.
Tam et al. (2007) construct two monolingual topic models parallel source target
documents. build topic correspondences source target documents enforcing one-to-one topic mapping constraint. project target-side topics onto space
source-side topic model one-to-many fashion. Section 7.3.1, compare two different
methods building topic correspondences.

11

fiZ HANG , X IAO , X IONG , & L IU

5.1 Rule-Topic Distribution Estimation
estimate rule-topic distributions word-aligned bilingual training corpus document
boundaries explicitly given. source- target-side rule-topic distributions estimated
way. Therefore, simplicity, describe estimation source-side rule-topic
distribution P (zf |rf ) translation rule section.
estimation rule-topic distributions analogous traditional estimation rule translation probabilities (Chiang, 2007). addition word-aligned corpus, input rule-topic
distribution estimation contains source-side document-topic distributions inferred LDA tool.
first extract translation rules bilingual training data traditional way.
source side translation rule rf extracted source-language document df documenttopic distribution P (zf |df ), obtain instance (rf , P (zf |df ), ), fraction count
instance described Chiang (2007). way, collect set instances
= {(rf , P (zf |df ), )} different document-topic distributions translation rule. Using
instances, calculate probability P (zf = k|rf ) rf topic k follows:
P
P (zf = k|df )
(10)
P (zf = k|rf ) = PK II
P

k =1
II P (zf = k |df )

Based equation, obtain two rule-topic distributions P (zf |rf ) P (ze |re )
rule using source- target-side document-topic distributions P (zf |df ) P (ze |de ) respectively.
5.2 Target-Side Rule-Topic Distribution Projection

described previous section, estimate target-side rule-topic distributions. However, directly use equation (6) calculate dissimilarity target-side
rule-topic distribution P (ze |re ) translation rule source-side document-topic distribution
P (zf |df ) source-language document translated. order measure dissimilarity, need project target-side topics onto source-side topic space. projection takes
following two steps.
First, calculate correspondence probability p(zf |ze ) pair target-side topic
ze source-side topic zf , inferred two separately trained monolingual
topic models respectively.
Second, project target-side rule-topic distribution translation rule onto sourceside topic space using correspondence probabilities learned first step.
first step, estimate topic-to-topic correspondence probabilities using co-occurrence
counts topic assignments source target words word-aligned corpus. topic assignments source/target words inferred two monolingual topic models. topic
assignments, characterize sentence pair (f, e) (zf , ze , a), zf ze two vectors
containing topic assignments words source target sentence f e respectively,
set word alignment links {(i, j)} source target sentence. Particularly, link
(i, j) represents source-side position aligns target-side position j.
12

fiT OPIC -BASED ISSIMILARITY



ENSITIVITY ODELS

notations, calculate co-occurrence count source-side topic kf
target-side topic ke follows.
X X
(zfi , kf ) (zej , ke )
(11)
(zf ,ze ,a) (i,j)a

zfi zej topic assignments words ej respectively, (x, y) Kronecker
function, 1 x = 0 otherwise.
compute topic-to-topic correspondence probability P (zf = kf |ze = ke )
normalizing co-occurrence count follows.
P
P
(zf ,ze ,a)
(i,j)a (zfi , kf ) (zej , ke )
P
P
(12)
P (zf = kf |ze = ke ) =
(zf ,ze ,a)
(i,j)a (zej , ke )

Overall, first step, obtain topic-to-topic correspondence matrix MKe Kf ,
item Mi,j represents probability P (zf = i|ze = j).
second step, given correspondence matrix MKe Kf , project target-side ruletopic distribution P (ze |re ) source-side topic space multiplication follows.
(P (ze |re )) = P (ze |re ) MKe Kf

(13)

way, get second distribution translation rule source-side topic space,
call projected target-side topic distribution (P (ze |re )).
Word alignment noises may introduced equation (11), turn may flatten
sharpness projected topic distributions calculated equation (13). order decrease
flattening effects word alignment noises, take following action practice:
topic-to-topic correspondence probability P (zf = kf |ze = ke ) calculated via word alignments
1
K predefined number topics, set 0 re-normalize
less K
correspondence probabilities target-side topic ke .
Obviously, projection method allows one target-side topic ze align multiple source-side
topics. different one-to-one correspondence used Tam et al. (2007). investigate correspondence matrix MKe Kf obtained training data. find topic
correspondence source target language necessarily one-to-one. Typically,
correspondence probability P (zf = kf |ze = ke ) target-side topic mainly distributes two
three source-side topics. Table 2 shows example target-side topic three mainly
aligned source-side topics.

6. Integration
incorporate topic dissimilarity sensitivity model two new features hierarchical
phrase-based system (Chiang, 2007) log-linear discriminative framework (Och & Ney,
2002). dissimilarity values positive Hellinger distances positive. weight
dissimilarity feature tuned MERT negative. Therefore log-linear model favor
candidate translations lower values dissimilarity feature (less dissimilar).
words, translation rules similar document translated terms topics
selected.
13

fiZ HANG , X IAO , X IONG , & L IU

e-topic
enterprises
rural
state
agricultural
market
reform
production
peasants
owned
enterprise
P (zf |ze )

(agricultural)
~(rural)
(peasant)
U(reform)
(finance)
(social)
(safety)
N(adjust)
(policy)
\(income)

f-topic 1

(enterprise)
|(market)
Ik(state)
i(company)
7K(finance)
1(bank)
(investment)
+n(manage)
U(reform)
E(operation)

f-topic 2

u(develop)
L(economic)
E(technology )
I(China)
E(technique)
(industry)
((structure)
M#(innovation)
\(accelerate)
U(reform)

f-topic 3

0.38

0.28

0.16

Table 2: example topic-to-topic correspondence. last line shows correspondence
probability. column shows topic represented top-10 topical words. first
column target-side topic, remaining three columns source-side topics.

One possible side-effect integration dissimilarity feature system
favour translations generated fewer translation rules generated translation
rules translation rules result higher dissimilarity (see equation (7)).
say, topic-based dissimilarity feature acts translation rule count penalty derivations.
Fortunately, however, use translation rule count feature (see last row Table 1)
normally favours translations yielded derivation large number translation rules.
feature balance mentioned side-effect topic-based dissimilarity feature.
translation rule associated source-side rule-topic distribution projected
target-side rule-topic distribution decoding, add four features follows.5
Dsim(P (zf |d), P(zf |rf )) (or DsimSrc): Topic dissimilarity feature source-side rule-topic
distributions.
Dsim(P (zf |d), (P(ze |re ))) (or DsimTrg): Topic dissimilarity feature projected targetside rule-topic distributions.
Sen(P(zf |rf )) (or SenSrc): Topic sensitivity feature source-side rule-topic distributions.
Sen(T (P(ze |re )) (or SenTrg): Topic sensitivity feature projected target-side rule-topic
distributions.
source-side projected target-side rule-topic distributions translation rules
calculated decoding described last section. decoding, first infer
topic distribution P (zf |d) given document source language. translation rule
adopted derivation, scores four features updated correspondingly according
equation (7) (9). Obviously, computational cost features rather small.
5. Since glue rule rules unknown words extracted training data, set values four
features rules zero.

14

fiT OPIC -BASED ISSIMILARITY



ENSITIVITY ODELS

topic-specific lexicon translation models (Zhao & Xing, 2007; Tam et al., 2007), first
calculate topic-specific translation probabilities normalizing entire lexicon translation table
adapt lexical weights translation rules correspondingly decoding. makes
decoder run slower. Therefore, comparing previous topic-specific lexicon translation methods, method provides efficient way incorporating topic models SMT.

7. Experiments
section, conducted two groups experiments validate effectiveness topicbased translation rule selection framework. first group experiments, use medium-scale
bilingual data train SMT system topic models. purpose group experiments
quickly answer following questions:
topic dissimilarity model able improve translation rule selection terms B LEU?
Furthermore, source-side target-side rule-topic distributions complementary
other?
helpful introduce topic sensitivity model distinguish topic-insensitive topicsensitive rules?
topic-based method better previous topic-specific lexicon translation method (Zhao
& Xing, 2007) terms B LEU decoding speed?
confirm efficacy topic-based dissimilarity sensitivity model mediumscale training data, conducted second group experiments large-scale training data
investigate following questions:
one-to-many target-side rule-topic projection method better previous methods
proposed Zhao Xing (2007) Tam et al. (2007)?
effects models various types rules, phrase rules rules
non-terminals?
else achieve use monolingual data train topic models?
7.1 Setup
carried experiments NIST Chinese-to-English translation. used NIST evaluation set 2005 (MT05) development set, sets MT06/MT08 test sets.
numbers documents MT05, MT06, MT08 100, 79, 109 respectively. Case-insensitive
NIST B LEU (Papineni, Roukos, Ward, & Zhu, 2002) used measure translation performance.
used minimum error rate training (Och, 2003) optimize feature weights.
medium-scale experiments, used FBIS corpus bilingual training data,
contains 10,947 documents, 239K sentence pairs 6.9M Chinese words 9.14M English
words. large-scale experiments, bilingual training data consists LDC2003E14, LDC2004T07, LDC2005T06, LDC2005T10 LDC2004T08 (Hong Kong Hansards/Laws/News).

15

fiZ HANG , X IAO , X IONG , & L IU

selected corpora contain 103,236 documents 2.80M sentences. average, document 28.4 sentences.
obtained symmetric word alignments training data first running GIZA++ (Och & Ney,
2003) directions applying refinement rule grow-diag-final-and (Koehn et al.,
2003). hierarchical phrase translation rules extracted word-aligned training data.
used SRILM toolkit (Stolcke, 2002) train language models Xinhua portion
GIGAWORD corpus, contains 238M English words. trained 4-gram language model
medium-scale experiments 5-gram language model large-scale experiments.
order train two monolingual topic models source target side bilingual
training data, used open source LDA tool GibbsLDA++.6 GibssLDA++ implementation
LDA using gibbs sampling parameter estimation inference. source- targetside topic models separately estimated Chinese English part bilingual
training data. set number topic K = 30 source- target-side topic models,
used default setting tool training inference.7 decoding, inferred
document-topic distribution document dev/test sets translation using
trained source-side topic model. Note topic inference dev/test sets performed
parameters two topic models estimated training data.
case-insensitive BLEU-4 used evaluation metric. performed statistical
significance BLEU differences using paired bootstrap re-sampling (Koehn, 2004). order
alleviate impact instability MERT, ran tuning process three times
large scale experiments presented average BLEU scores three runs following
suggestion Clark, Dyer, Lavie, Smith (2011)
7.2 Medium-Scale Experiments
section, conducted medium-scale experiments investigate effectiveness two
topic-based models translation rule selection.
7.2.1 E FFECT



OPIC ISSIMILARITY ODEL

quickly investigated effectiveness topic dissimilarity sensitivity model using
medium-scale training data. Results shown Table 3. table, observe
use topic dissimilarity model source-side projected target-side ruletopic distributions (DsimSrc/DsimTrg table, see descriptions Section 5),
obtain absolute improvement 0.48/0.38 B LEU points baseline.
combine two topic dissimilarity features together, achieve improvement 0.16 B LEU points DsimSrc.
two observations show topic dissimilarity model able improve translation quality
terms B LEU.
6. http://gibbslda.sourceforge.net/
7. determine K testing {15, 30, 50, 100, 200} preliminary experiments. find K = 30 produces
slightly better performance values. order improve stability topic estimation, run
tool multiple times use best model respect log-likelihood.

16

fiT OPIC -BASED ISSIMILARITY

System
Baseline
TopicLex
DsimSrc
DsimTrg
DsimSrc+DsimTrg
Dsim+Sen

MT06
30.20
30.65
30.41
30.51
30.73
30.95



ENSITIVITY ODELS

MT08
21.93
22.29
22.69
22.39
22.69
22.92

Avg
26.07
26.47
26.55
26.45
26.71
26.94

Speed
12.6
3.3
11.5
11.7
11.2
10.2

Table 3: Results topic dissimilarity sensitivity model terms B LEU speed (words
per second), comparing traditional hierarchical system (Baseline) system topic-specific lexicon translation model (TopicLex). DsimSrc DsimTrg topic dissimilarity features source-side projected target-side ruletopic distributions respectively. Dsim+Sen activates two dissimilarity features
two sensitivity features described Section 6. Avg denotes average B LEU
scores two test sets. Scores bold significantly better Baseline (p < 0.01).
Speed denotes number words translated per second.
Rule Type
Phrase
Monotone
Reordering


Count
3.9M
19.2M
5.7M
28.8M

Src-Sen(%)
83.4
85.3
85.9
85.1

Trg-Sen(%)
84.4
86.1
86.8
86.0

Table 4: Percentages topic-sensitive rules listed rule types according entropies
source-side (Src) target-side (Trg) rule-topic distributions. Phrase rules fully
lexicalized, monotone reordering rules contain nonterminals.

order gain insights topic dissimilarity model helpful translation rule
selection, investigate many rules topic-sensitive. described Section 4.2,
use entropy measure whether translation rule topic-sensitive based rule-topic distribution. entropy translation rule calculated equation (8) smaller certain
threshold, rule topic-sensitive. Since documents often focus topics, use average entropy document-topic distributions training documents threshold. compare
entropies source-side target-side rule-topic distributions threshold. findings
shown Table 4. 85.5% translation rules topic-sensitive rules compare entropies
source-side rule-topic distributions threshold. compare entropies targetside rule-topic distributions threshold, topic-sensitive rules account 86%.
strongly suggest rules occur documents specific topics topic information
used improve translation rule selection.
7.2.2 E FFECT



OPIC ENSITIVITY ODEL

see Table 4, still 15% translation rules generic, sensitive topics. rules widely used documents. mentioned before,
17

fiZ HANG , X IAO , X IONG , & L IU

topic dissimilarity model always punishes rules documents normally topic-specific.
therefore introduce topic sensitivity model complement topic dissimilarity model. experiment result model show last line Table 3. obtain improvement
0.23 B LEU points incorporating topic sensitivity model. indicates necessary
distinguish topic-insensitive topic-sensitive rules.
7.2.3 C OMPARISON



OPIC -S PECIFIC L EXICON RANSLATION ODEL

compared topic models topic-specific lexicon translation model proposed
Zhao Xing (2007). introduce framework combine Hidden Markov Model (HMM)
LDA topic model SMT, shown Figure 2. framework, bilingual sentence
pair single topic assignment sampled document-pair topic distribution .
words target language (e.g., English) sampled given sentence-pair topic assignment
monolingual per-topic word distribution . that, word alignments words
source language sampled first-order Markov process topic-specific translation
lexicon respectively.
Zhao Xing integrate topic-specific word-to-word translation lexicons estimated
bilingual topic model described topic-specific lexicon translation model,
formulated follows.
P (we |wf , df ) P (wf |we , df )P (we |df )
X
=
P (wf |we , z = k)P (we |z = k)P (z = k|df )

(14)

k

model, probability candidate translation source word wf source document df calculated marginalizing topics corresponding topic-specific translation
lexicons. simplify estimation p(wf |we , z = k) directly computing probabilities
word-aligned corpus associated target-side topic assignments inferred
target-side topic model. Despite simplification, improvement implementation comparable improvement obtained Zhao Xing (2007). Given new document, need
adapt lexical translation weights rules. adapted lexicon translation model integrated
new feature log-linear discriminative framework.
show comparison results Table 3. topic-specific lexicon translation model
better baseline 0.4 B LEU points. However, topic-based method (the combination
topic dissimilarity sensitivity models) outperforms baseline 0.87 B LEU points.
compare two methods terms decoding speed (words/second). baseline translates 12.6 words per second, system topic-specific lexicon translation
model translates 3.3 words one second. overhead topic-specific lexicon translation model mainly comes adaptation lexical weights. takes 72.8% time
adaptation. contrast, method speed 10.2 words per second sentence
average, three times faster topic-specific lexicon translation method.
7.3 Large-Scale Experiments
section, investigated deeper models second group experiments
large-scale training data.
18

fiT OPIC -BASED ISSIMILARITY

7.3.1 E FFECT





ENSITIVITY ODELS

NE - -M P ROJECTION

discussed Section 5.2, need project target-side topics onto source-side topic space
calculate dissimilarity target-side rule-topic distribution source-side
document-topic distribution. propose one-to-many projection method issue. order
investigate effectiveness method, conducted experiments large-scale training
data compare following 3 methods.
One-to-One Mapping enforce one-to-one mapping source-side target-side
topics, similar method Tam et al. (2007). achieve aligning target-side
topic corresponding source-side topic largest correspondence probability
calculated Section 5.2.
Marginalization Word Alignments Following Zhao Xing (2007), first obtain
topics target side using LDA retrieve topics source language
marginalization word alignments follows.
X
P (wf |k) =
P (wf |we )P (we |z = k)
(15)


Combination source target language documents concatenate target document aligned source document one document. run LDA tool
combined documents train one topic model mixed-language words. decoding,
use trained topic model infer topics source documents.
order compare one-to-many projection method three methods described above,
add target-side topic dissimilarity feature (DsimTrg) log-linear translation model.
experiment results reported Table 5. Clearly, four methods achieve improvements
baseline. However, one-to-many projection method performs better three
methods. particular,
method outperforms one-to-one topic mapping method, indicates sourceside target-side topics exactly match one-to-one correspondence manner.
reason marginalization method performs worse among four methods may
topic model trained target documents.
Surprisingly, combination method performs quite well. shows LDA model
find hidden topics even mixed-language documents.

7.3.2 E FFECT
RULES



OPIC -BASED RULE ELECTION F RAMEWORK



VARIOUS YPES



conducted experiments investigate effect topic-based models various
types rules selection. Particularly, divide translation rules hierarchical phrase-based SMT
three types: 1) phrase rules, contain terminals bilingual phrase
pairs used phrase-based system; 2) monotone rules, contain non-terminals produce
19

fiZ HANG , X IAO , X IONG , & L IU

System
Baseline
One-to-One
Marginalization
Combination
One-to-Many

MT06
31.77
32.15
32.23
32.17
32.44

MT08
24.89
25.32
24.99
25.56
25.54

Avg
28.33
28.73
28.61
28.86
28.99

Table 5: Effect one-to-many topic projection method methods. Marginalization: Marginalization Word Alignments; Combination: Combination source
target language documents.
System
Baseline
Phrase rule
Monotone rule
Reordering rule


MT06
31.77
32.43
32.24
31.82
32.77

MT08
24.89
25.53
25.62
25.15
26.29

Avg
28.33
28.98
28.93
28.48
29.53

Table 6: Effect topic-based rule selection models three types rules. Phrase rules
fully lexicalized, monotone reordering rules contain nonterminals.

monotone translations; finally 3) reordering rules, contain non-terminals change
order translations. define monotone reordering rules according Chiang et al.
(2008).
study impact topic-based models translation rule type A, activate
four features described Section 6 rules type A. Topic dissimilarity
sensitivity features two types translation rules deactivated.
Table 6 shows experiment results. table, observe
topic-based models achieve highest improvement 0.65 B LEU points baseline phrase rules among three types translation rules. reasonable phrase
rules consist topical words.
obtain improvements 0.6 0.15 B LEU points baseline monotone
reordering rules respectively. shows models able help select
appropriate translation rules non-terminals.
activate topic dissimilarity sensitivity models translation rules,
still achieve additional improvement 0.55 B LEU points. total, models outperform
baseline absolute improvement 1.2 B LEU points.
7.3.3 E FFECT



ORE ONOLINGUAL DATA

Comparing Table 6 Table 3, find topic-based dissimilarity sensitivity models
trained medium-scale data (about 10K documents) collectively achieve improvement 0.87
20

fiT OPIC -BASED ISSIMILARITY



ENSITIVITY ODELS

System
Baseline
DsimSrc + SenSrc + DsimTrg + SenTrg
DsimSrc + SenSrc + DsimTrg + SenTrg
DsimSrc + SenSrc + DsimTrg + SenTrg
DsimSrc + SenSrc + DsimTrg + SenTrg

MT06
31.77
32.77
32.70
32.37
32.61

MT08
24.89
26.29
25.91
25.80
25.66

Avg
28.33
29.53
29.31
29.09
29.13

Table 7: Effect using monolingual data train topic models. features bold
topic-based dissimilarity/sensitivity model LDA topic model trained using
combination source/target part large-scale bilingual data corresponding
monolingual corpus.

B LEU points baseline two models trained large-scale data (about 100K documents) obtain improvement 1.2 B LEU points. suggests performance gains
may obtained data. parallel bilingual data document boundaries provided easily accessible, try collect monolingual data source or/and target language.
interest study whether gain improvements using monolingual data
train topic models.
used Chinese monolingual corpus documents collected Chinese Sohu
weblog 2009.8 collected Chinese corpus contains 500K documents 273.8M Chinese
words. used English monolingual corpus documents collected
English Blog Authorship corpus (Schler, Koppel, Argamon, & Pennebaker, 2006). English
monolingual corpus consists 371K documents 98M English words. combined new
Chinese corpus source part large-scale bilingual data train source-side LDA topic
model ST . English monolingual corpus combined target part large-scale
bilingual data train target-side LDA topic model .
used two topic models ST infer topics test sets. Topic information source target part large-scale bilingual training data inferred ST
used estimate source-side rule-topic distributions projected target-side rule-topic distributions. way, obtain new topic-based dissimilarity sensitivity model
source/target side.
Experiment results shown Table 7. Unfortunately, obtain improvements training topic models larger data, combination Chinese monolingual
corpus source part bilingual training data. Instead, performance drops 29.53
29.31 use topic model ST build source-side dissimilarity sensitivity features
29.09 adopt topic model build target-side dissimilarity sensitivity
features.
One reason lower performance larger topic model training data may
use 30 topics. Using topics may improve models larger corpora. order
investigate this, conducted new experiments topics 30. trained sourceside topic model using combination source part large-scale bilingual data Sohu
weblog data. Based topic model, built source-side topic dissimilarity model
8. http://blog.sohu.com/.

21

fiZ HANG , X IAO , X IONG , & L IU

System
Baseline
K = 30
K = 50
K = 100
K = 200

MT06
31.77
32.32
31.96
32.26
32.16

MT08
24.89
25.41
25.73
25.53
25.28

Avg
28.33
28.86
28.85
28.90
28.72

Table 8: Experiment results different number topics (K). source-side topic dissimilarity model (DsimSrc) integrated SMT system.
Test Set
MT06
MT08

MonoSrc
0.359
0.232

BiSrc
0.238
0.136

MonoBiSrc
0.297
0.261

Table 9: Hellinger distances MT06/08 test sets Chinese monolingual corpus
(MonoSrc) source part bilingual training data (BiSrc) well combination (MonoBiSrc) terms average document-topic distributions.

integrated SMT system. Experiment results shown Table 8. table,
find using topics able improve model corpora.
Yet another reason may additional monolingual corpus similar test sets
terms topic distributions. order examine hypothesis, inferred document-topic
distributions documents test sets, Chinese monolingual corpus source part
bilingual corpus using topic model ST . average document-topic distributions
obtain four average document-topic distributions MT06, MT08, Chinese monolingual
corpus source part bilingual corpus respectively. average topic distributions approximated corpus-topic distributions four corpora. calculate
Hellinger distances corpus-topic distributions test sets Chinese
monolingual corpus source part bilingual training data, shown Table 9.
table, clearly find additional monolingual corpus much less similar
test sets comparing bilingual training corpus. Hellinger distance test
set MT08 MonoBiSrc corpus almost twice large bilingual training data
(0.261 vs. 0.136). topic model trained enlarged corpus make topic-based
models select translation rules similar documents test sets terms topic
distributions. suggests select additional monolingual data similar
test sets want obtain improvements.
conducted new group experiments empirically examine hypothesis
translating web-domain test set similar additional weblog corpus terms
topics. used web portion NIST MT06 set new development set web
portion NIST MT08 new test set. Results displayed Table 10, show
additional monolingual data improve performance time. suggests
select monolingual corpus similar test sets learn topics topic-based
dissimilarity sensitivity models.
22

fiT OPIC -BASED ISSIMILARITY



ENSITIVITY ODELS

System
Baseline
DsimSrc + SenSrc + DsimTrg + SenTrg
DsimSrc + SenSrc + DsimTrg + SenTrg

MT08-web
20.45
21.42
21.77

Table 10: Results translating web-domain test set topic-based models trained
data augmented monolingual weblog corpus. features bold topicbased dissimilarity/sensitivity model LDA topic model trained using
combination source/target part large-scale bilingual data corresponding
monolingual corpus. MT08-web web portion NIST MT08 test set.

8. Analysis
section, study details topic-based models translation rule selection
looking differences make target documents individual translation hypotheses.
differences help us gain insights presented models improve translation
quality. analysis, baseline system system enhanced proposed
topic-based models (all four features Section 6 activated) trained large-scale bilingual
data described Section 7.1. notational convenience, hereafter refer baseline
system BASE system enhanced topic-based dissimilarity sensitivity models
TOPSEL.
8.1 Differences Target Documents
order measure impact topic-based models target documents, calculate
Hellinger distances target documents generated BASE / TOPSEL system reference documents generated human terms topics inferred target-side LDA topic
model according following 4 steps. target-side LDA topic model trained target
part large-scale bilingual data described Section 7.1.
Use target-side LDA topic model infer document-topic distribution document
reference translations (called reference distribution).
Use target-side LDA topic model infer document-topic distribution target document generated BASE system (called BASE distribution).
Similarly, obtain
TOPSEL system.

TOPSEL

distribution target document generated

Calculate dissimilarity BASE reference distribution well TOPSEL reference distribution according equation (6). dissimilarities first averaged documents averaged four reference translations.
Table 11 shows calculated dissimilarities. According equation (6), smaller
Hellinger distance two items, similar are. average Hellinger distance
TOPSEL reference documents 0.123 distance BASE reference
23

fiZ HANG , X IAO , X IONG , & L IU

System
BASE
TOPSEL

MT06
0.119
0.116

MT08
0.137
0.129

Avg
0.128
0.123

Table 11: Dissimilarities (measured Hellinger distance) reference documents target
documents generated BASE TOPSEL system terms topics according
equation (6).

similar (+)
Less similar (-)
p<

MT06
49
30
0.05

Table 12: number target documents generated
erence documents BASE.

MT08
72
37
0.01
TOPSEL

more/less similar ref-

documents 0.128. Therefore, target documents generated TOPSEL system
MT06 MT08 similar documents reference translations
baseline system. calculate number target documents generated TOPSEL
more/less similar reference documents BASE based average Hellinger
distances. numbers shown Table 12. According sign test using numbers,
TOPSEL statistically significantly better baseline system terms similarity
translations generated two systems human-generated translations.
8.2 Differences Translation Hypotheses
look deeper translation hypotheses understand models select translation
rules. Table 13 shows three translation examples compare baseline system
enhanced topic-based models. order conduct quantitative comparison, calculate
dissimilarity values (measured Hellinger distance) underlined phrases Table 13 using
topic-based dissimilarity model. dissimilarity values computed projected
target-side rule-topic distributions underlined phrases source-side document-topic
distributions corresponding documents phrase used. values shown
Table 14.
two tables, easily observe system topic-based dissimilarity
model prefers target phrases smaller Hellinger distances documents
occur terms topic distributions. contrast, baseline able use document-level
topic information translation rule selection. Figure 3 shows topic distributions
source-side document, TOPSEL phrase allow BASE phrase permit Eg. 2.
major topics source-side document topic 12 36. TOPSEL phrase allow mainly
distributes 12 different topics9 including topic 12 36 BASE phrase permit
mainly 10 different topics include topic 12.
9. distribution probability topics larger 0.03.

24

fiT OPIC -BASED ISSIMILARITY

Source

ENSITIVITY ODELS

7 8 { "

described northern limit line unlawful .
referred northern limit line legitimate .
Reference pointed northern limit line legitimate .
Source
BASE
would permit love others accepted people .
TOPSEL
allow love love others accepted people
Reference would someone allow person loves accept peoples
love time
Source
BASE
present , internet entitled statutory right leave
TOPSEL
present internet enjoy statutory right leave
Reference present internet enjoy rights
BASE

Eg. 1



TOPSEL

#N gC < O<

Eg. 2

8 p k { N |

Eg. 3

Table 13: Translation examples NIST MT06/08 test sets, comparing baseline
system enhanced topic-based models. underlined words highlight
difference enhanced models baseline.
Phrase
unlawful
legitimate
permit
allow
entitled
enjoy

HD
3.08
2.27
3.75
3.47
3.45
3.24

Table 14: Dissimilarity values (measured Hellinger distance) underlined phrases Table 13 projected target-side rule-topic distributions corresponding
source-side document-topic distributions documents calculated topic-based dissimilarity model.

9. Discussion Bilingual Topic Modeling
Although topic models widely adopted monolingual text analysis, bilingual multilingual
topic models less explored, especially tailored multilingual tasks machine
translation. section try provide suggestions bilingual topic modeling
perspective statistical machine translation well practice integration topic models SMT. suggestions listed follows, future
directions.
Investigation Topic divergences across different languages Cross-language divergences
pervasive become one big challenges machine translation (Dorr, 1994).
language-level divergences hint divergences topic concept level may exist
across languages. may explain one-to-many topic projection target side
25

fiZ HANG , X IAO , X IONG , & L IU

Figure 3: Topic distributions source-side document (a),
BASE phrase permit shown Eg. 2 Table 13.

TOPSEL

phrase allow (b)

source side better one-to-one mapping. Although Mimno et al. (2009)
studied topic divergences using Wikipedia articles, believe deeper wider
investigation topic divergence needed shed new light build
better bilingual topic models.
Adding linguistic assumptions topic modeling Practices SMT show integrating linguistic knowledge machine translation normally generates better translations
(Chiang et al., 2008). believe adding linguistic assumptions beyond bag-ofwords improve topic modeling. flexible topic modeling framework allows us
integrate rich linguistic knowledge form features definitely facilitate
application topic models natural language processing.
Joint modeling topic induction synchronous grammar induction Synchronous grammar induction machine translation task automatically learning translation rules
bilingual data (Blunsom, Cohn, Dyer, & Osborne, 2009; Xiao & Xiong, 2013). Bayesian
approaches successfully used topic modeling synchronous grammar induction, joint modeling interesting direction, benefit grammar
adaptation one domain another domain machine translation.

10. Conclusions
article presented topic-based translation rule selection framework incorporates topic information source target language translation rule disambiguation. Particularly, use topic dissimilarity model select appropriate translation rules
documents according similarities translation rules documents. adopt
26

fiT OPIC -BASED ISSIMILARITY



ENSITIVITY ODELS

topic sensitivity model complement topic dissimilarity model order balance translation
rule selection topic-sensitive topic-insensitive rules. order calculate dissimilarities source- target-side topic distributions, project topic distributions target
side onto source-side topic model space new efficient way.
integrated topic-based rule selection models hierarchical phrase-based SMT
system. Experiments medium/large-scale training data show
topic dissimilarity sensitivity model able substantially improve translation
quality terms B LEU improve translation rule selection various types rules (i.e.,
phrase/monotone/reordering rules).
method better previous topic-specific lexicon translation method translation quality decoding speed.
proposed one-to-many projection method outperforms various methods
one-to-one mapping, marginalization via word alignments on.
want use additional monolingual corpus train topic models, first investigate whether new monolingual corpus similar test data terms topic
distributions.
Topic models provide global document-level information machine translation.
future, would use topic models address document-level machine translation issues,
coherence cohesion (Barzilay & Lee, 2004; Hardmeier, Nivre, & Tiedemann, 2012).
want integrate topic-based models linguistically syntax-based machine translation
syntactic translation rule selection (Liu et al., 2006).

Acknowledgments
work sponsored National Natural Science Foundation China projects
61373095 61333018. Qun Lius work partially supported Science Foundation Ireland
(Grant No. 07/CE/I1142) part CNGL Dublin City University. would thank
three anonymous reviewers insightful comments. corresponding author article
Deyi Xiong.

References
Barzilay, R., & Lee, L. (2004). Catching drift: Probabilistic content models, applications
generation summarization. Susan Dumais, D. M., & Roukos, S. (Eds.), HLT-NAACL
2004: Main Proceedings, pp. 113120, Boston, Massachusetts, USA. Association Computational Linguistics.
Blei, D. M., & Lafferty, J. D. (2007). correlated topic model science. AAS, 1(1), 1735.
Blei, D. M., Ng, A., & Jordan, M. (2003). Latent dirichlet allocation. JMLR, 3, 9931022.

27

fiZ HANG , X IAO , X IONG , & L IU

Blunsom, P., Cohn, T., Dyer, C., & Osborne, M. (2009). gibbs sampler phrasal synchronous
grammar induction. Proceedings Joint Conference 47th Annual Meeting
ACL 4th International Joint Conference Natural Language Processing
AFNLP, pp. 782790, Suntec, Singapore. Association Computational Linguistics.
Boyd-Graber, J., & Blei, D. M. (2009). Multilingual topic models unaligned text. Proceedings
Twenty-Fifth Conference Uncertainty Artificial Intelligence, UAI 09, pp. 7582,
Arlington, Virginia, United States. AUAI Press.
Carpuat, M., & Wu, D. (2007a). phrase sense disambiguation outperforms word sense disambiguation statistical machine translation. Proceedings 11th Conference
Theoretical Methodological Issues Machine Translation, pp. 4352.
Carpuat, M., & Wu, D. (2007b). Improving statistical machine translation using word sense disambiguation. Proceedings 2007 Joint Conference Empirical Methods Natural
Language Processing Computational Natural Language Learning (EMNLP-CoNLL), pp.
6172, Prague, Czech Republic. Association Computational Linguistics.
Chan, Y. S., Ng, H. T., & Chiang, D. (2007). Word sense disambiguation improves statistical machine translation. Proceedings 45th Annual Meeting Association Computational Linguistics, pp. 3340, Prague, Czech Republic. Association Computational
Linguistics.
Chiang, D. (2005). hierarchical phrase-based model statistical machine translation. Proc.
ACL 2005.
Chiang, D. (2007). Hierarchical phrase-based translation. Computational Linguistics, 33(2), 201
228.
Chiang, D., Marton, Y., & Resnik, P. (2008). Online large-margin training syntactic structural
translation features. Proceedings 2008 Conference Empirical Methods Natural Language Processing, pp. 224233, Honolulu, Hawaii. Association Computational
Linguistics.
Clark, J. H., Dyer, C., Lavie, A., & Smith, N. A. (2011). Better hypothesis testing statistical
machine translation: Controlling optimizer instability. Proceedings 49th Annual
Meeting Association Computational Linguistics: Human Language Technologies,
pp. 176181, Portland, Oregon, USA.
Dorr, B. J. (1994). Machine translation divergences: formal description proposed solution.
Computational Linguistics, 20(4), 597633.
Foster, G., & Kuhn, R. (2007). Mixture-model adaptation SMT. Proc. Second Workshop
Statistical Machine Translation, pp. 128135, Prague, Czech Republic.
Gong, Z., Zhang, M., & Zhou, G. (2011). Cache-based document-level statistical machine translation. Proc. EMNLP 2011.
Gong, Z., Zhang, Y., & Zhou, G. (2010). Statistical machine translation based LDA. Proc.
IUCS 2010, p. 286 290.



Griffiths, T. L., & Steyvers, M. (2004). Finding scientific topics. Proceedings National
Academy Sciences, 101(Suppl. 1), 52285235.

28

fiT OPIC -BASED ISSIMILARITY



ENSITIVITY ODELS

Hardmeier, C., Nivre, J., & Tiedemann, J. (2012). Document-wide decoding phrase-based statistical machine translation. Proceedings 2012 Joint Conference Empirical
Methods Natural Language Processing Computational Natural Language Learning,
pp. 11791190, Jeju Island, Korea. Association Computational Linguistics.
He, Z., Liu, Q., & Lin, S. (2008). Improving statistical machine translation using lexicalized rule
selection. Proceedings 22nd International Conference Computational Linguistics
(Coling 2008), pp. 321328, Manchester, UK. Coling 2008 Organizing Committee.
Hofmann, T. (1999). Probabilistic latent semantic analysis. Proc. UAI 1999, pp. 289296.
Koehn, P. (2004). Statistical significance tests machine translation evaluation. Proceedings
EMNLP 2004, pp. 388395, Barcelona, Spain.
Koehn, P., Och, F. J., & Marcu, D. (2003). Statistical phrase-based translation. Proc. HLT-NAACL
2003.
Lin, J. (2006). Divergence measures based Shannon entropy. IEEE Trans. Inf. Theor., 37(1),
145151.
Liu, Q., He, Z., Liu, Y., & Lin, S. (2008). Maximum entropy based rule selection model syntaxbased statistical machine translation. Proceedings 2008 Conference Empirical Methods Natural Language Processing, pp. 8997, Honolulu, Hawaii. Association
Computational Linguistics.
Liu, Y., Liu, Q., & Lin, S. (2006). Tree-to-string alignment template statistical machine translation. Proc. ACL 2006.
Mimno, D., Wallach, H. M., Naradowsky, J., Smith, D. A., & McCallum, A. (2009). Polylingual
topic models. Proc. EMNLP 2009.
Och, F. J., & Ney, H. (2002). Discriminative training maximum entropy models statistical
machine translation. Proc. ACL 2002.
Och, F. J. (2003). Minimum error rate training statistical machine translation. Proc. ACL 2003.
Och, F. J., & Ney, H. (2003). systematic comparison various statistical alignment models.
Computational Linguistics, 29(1), 1951.
Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2002). BLEU: method automatic evaluation
machine translation. Proc. ACL 2002.
Ruiz, N., & Federico, M. (2011). Topic adaptation lecture translation bilingual latent
semantic models. Proceedings Sixth Workshop Statistical Machine Translation.
Schler, J., Koppel, M., Argamon, S., & Pennebaker, J. W. (2006). Effects age gender
blogging. AAAI Spring Symposium: Computational Approaches Analyzing Weblogs, pp.
199205.
Stolcke, A. (2002). SRILM extensible language modeling toolkit. Proc. ICSLP 2002.
Tam, Y.-C., Lane, I. R., & Schultz, T. (2007). Bilingual LSA-based adaptation statistical machine
translation. Machine Translation, 21(4), 187207.
Tiedemann, J. (2010). Context adaptation statistical machine translation using models exponentially decaying cache. Proceedings 2010 Workshop Domain Adaptation

29

fiZ HANG , X IAO , X IONG , & L IU

Natural Language Processing, pp. 815, Uppsala, Sweden. Association Computational
Linguistics.
Ture, F., Oard, D. W., & Resnik, P. (2012). Encouraging consistent translation choices. Proceedings 2012 Conference North American Chapter Association Computational Linguistics: Human Language Technologies, pp. 417426, Montreal, Canada. Association Computational Linguistics.
Xiao, T., Zhu, J., Yao, S., & Zhang, H. (2011). Document-level consistency verification machine
translation. Proceedings 2011 MT summit XIII, pp. 131138, Xiamen, China.
Xiao, X., & Xiong, D. (2013). Max-margin synchronous grammar induction machine translation. Proceedings 2013 Conference Empirical Methods Natural Language
Processing, pp. 255264, Seattle, Washington, USA. Association Computational Linguistics.
Xiao, X., Xiong, D., Zhang, M., Liu, Q., & Lin, S. (2012). topic similarity model hierarchical phrase-based translation. Proceedings 50th Annual Meeting Association
Computational Linguistics (Volume 1: Long Papers), pp. 750758, Jeju Island, Korea.
Association Computational Linguistics.
Xiong, D., Zhang, M., & Li, H. (2012). Modeling translation predicate-argument structure
SMT. Proceedings 50th Annual Meeting Association Computational
Linguistics (Volume 1: Long Papers), pp. 902911, Jeju Island, Korea. Association Computational Linguistics.
Zhao, B., & Xing, E. P. (2007). HM-BiTAM: Bilingual topic exploration, word alignment,
translation. Proc. NIPS 2007.
Zhao, B., & Xing, E. P. (2006). BiTAM: Bilingual topic admixture models word alignment.
Proc. ACL 2006.

30


