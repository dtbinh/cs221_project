journal artificial intelligence

submitted published

decision theoretic model assistance
alan fern

afern eecs oregonstate edu

school eecs oregon state university corvallis usa

sriraam natarajan

natarasr indiana edu

soic indiana university bloomington usa

kshitij judah

judahk eecs oregonstate edu

school eecs oregon state university corvallis usa

prasad tadepalli

tadepall eecs oregonstate edu

school eecs oregon state university corvallis usa

abstract
growing interest intelligent assistants variety applications sorting
email helping people disabilities daily chores formulate
intelligent assistance decision theoretic framework present theoretical
empirical first introduce class pomdps called hidden goal mdps hgmdps
formalizes interactively assisting agent whose goal hidden whose
actions observable spite restricted nature optimal action selection
hgmdps pspace complete even deterministic dynamics introduce
restricted model called helper action mdps hamdps sufficient modeling many
real world classes hamdps efficient possible
interestingly general hamdps simple myopic policy achieves near
optimal regret compared oracle assistant knows agents goal introduce
sophisticated versions policy general case hgmdps combine
novel quickly learning agent assisted evaluate
two game computer environments human subjects perform tasks real world
domain providing assistance folder navigation computer desktop environment
three domains framework assistant substantially reduces
user effort modest computation

introduction
personalized ai systems interactively assist human users received significant attention recent years yorke smith saadati myers morley lieberman myers berry
blythe conleyn gervasio mcguinness morley pfeffer pollack tambe however
overarching formal framework interactive assistance captures different systems
provides theoretical foundation largely missing address lacuna introducing general framework decision theoretic assistance analyzing complexity
different assumptons proposing different heuristic solutions evaluating effectiveness
consider model assistant observes goal oriented agent must select assistive
actions order best help agent achieve goals real applications requires
assistant able handle uncertainty environment agent reason varying
c

ai access foundation rights reserved

fif ern natarajan j udah tadepalli

action costs handle unforeseen situations adapt agent time consider
decision theoretic model partially observable markov decision processes pomdps
naturally handles features providing formal basis designing intelligent assistants
first contribution work formulate selecting assistive actions
class partially observable markov decision processes pomdps called hidden goal mdps
hgmdps jointly application environment along agents policy
hidden goals key feature explicitly reasons environment
agent provides potential flexibility assisting ways unforeseen developer
situations encountered thus developer need design hand coded assistive
policy preconceived application scenario instead framework burden
developer provide model application domain agent alternatively
mechanism learning one experience framework uses
attempt compute situation whether assistance could beneficial
assistive action select
second contribution work analyze properties formulation despite
restricted nature hgmdps complexity determining hgmdp finite horizon
policy given value pspace complete even deterministic environments motivates
restricted model called helper action mdp hamdp assistant executes helper
action step agent obliged accept helper action helpful goal
receives reward bonus cost reduction otherwise agent continue
preferred action without reward penalty assistant classes
complete pspace np class hamdps
deterministic agents polynomial time minimizing expected worstcase regret relative oracle assistant knows goal agent
optimal worst case regret characterized graph theoretic property called tree rank
corresponding goals policy tree computed linear time
principle given hgmdp one could apply pomdp solver order arrive optimal
assistant policy unfortunately relatively poor scalability pomdp solvers often force us
utilize approximate heuristic solutions particularly true assistant continually
learning updated agent environment sequence
accurate hgmdps needs solved third contribution work set
myopic action selection mecahnisms approximate optimal policy hamdps
analyze myopic heuristic regret upper bounded entropy
goal distribution hamdps furthermore give variant policy able
achieve worst case expected regret logarithmic number goals without
prior knowledge goal distribution describe two approaches
combination explicit goal estimation myopic heuristics bounded search
generally applicable hgmdps
order useful hgmdp must incorporate reasonably accurate model agent assisted fourth contribution work describe novel
model bootstrapping mechanism quickly learning agent policy important
usability assistant early lifetime main idea assume agent
close rational decision theoretic sense motivates defining prior agent policies
places higher probability policies closer optimal prior combination



fia ecision heoretic odel ssistance

bayesian updates allows agent model learned quickly rationality assumption
approximately satisfied
final contribution work evaluate framework three domains first
consider two game computer environments human subjects domains
assistants resulting framework substantially reduce amount work
performed human subjects consider realistic domain folder navigator
bao herlocker dietterich task tracer project domain user navigates
directory structure searching particular location open save file unknown
assistant job assistant predict users destination folder take actions
provide short cuts reach generic assistant framework compares
favorably hand coded solution bao et al
remainder organized follows next section introduce formal setup terms hgmdps followed analysis computational complexity
guarantees myopic heuristics case hamdps next present approximate solution general hgmdps goal estimation online action selection finally
give empirical evaluation three domains conclude discussion
related future work

hidden goal markov decision processes
throughout refer entity attempting assist agent
assisting entity assistant consider episodic setting beginning
episode agent begins world state selects goal finite set possible
goals goal set example might contain possible dishes agent might interested
cooking possible destination folders agent may possibly navigate importantly assistant fully observe world state agents actions cannot observe
goal agent model interaction agent assistant sequential
agent assistant alternate turns taking single action per turn possibly noop episode
ends agents assistants action leads goal state immediate reward
accumulated action episode total reward episode equal
sum rewards obtained episode note available actions agent
assistant need may varying rewards since assistant agent
share objective rewards viewed perspective agent objective
assistant behave way maximizes expected total reward episode
formally model interaction via hidden goal markov decision processes
hgmdps hgmdp mdp goal user observed rest
environment completely observed hgmdp tuple hs g r ig
set states g finite set possible agent goals set agent actions
set assistant actions typically include noop action allows assistant
decide provide assistance particular decision epoch transition function
g probability transition state taking action
agents goal g r reward function maps g real values
consider strictly alternating turn model simplicity however straightforward use model
capture interactions strictly alternating e g allowing assistant agent take multiple actions
row



fif ern natarajan j udah tadepalli

agents policy maps g distributions need optimal sense
ig initial state goal distribution
assistant policy hgmdp defines distribution actions given sequence
preceding observations e sequence state action pairs important assistant policy depends history rather current state since entire history potentially
provide evidence goal agent may necessary selecting appropriate
action must define objective function used evaluate value particular assistant policy consider finite horizon episodic setting hgmdp
episode begins drawing initial state goal g ig process alternates
agent assistant executing actions including noops environment
horizon terminal state reached agent assumed select actions according
many domains terminal goal state reached within horizon though general goals
arbitrary impact reward function reward episode equal sum
rewards actions executed agent assistant episode objective
assistant reason hgmdp observed state action history order select
actions maximize expected worst case total reward episode
proceeding worth reviewing assumptions formulation potential implications
partial observability definition hgmdp similar definition
partially observable markov decision process pomdp fact hgmdp special
case pomdp unobserved part state space consists single component corresponds goal user simplicity assumed
world state fully observable choice fundamental framework one
imagine relatively straightforward extensions techniques model environment
partially observable mdp pomdp world states fully observable
section shed light hardness hgmdps describe specialized
heuristic solutions performance guarantees
agent policy assumed agent modeled memoryless reactive
policy gives distribution actions conditioned current world state
goal assumption fundamental framework one extend
include complex user example include hierarchical goal
structures extension explored previously natarajan tadepalli fern

sequential interaction assumed simplicity interaction model
assistant agent involves interleaved sequential actions rather parallel actions
example precludes assistant taking actions parallel agent
parallel assistive actions useful many cases many domains sequential
actions norm especially motivated domains intelligent desktop
assistants help store retrieve files filter spam sort email etc smart homes
open doors switch appliances many opportunities assistance
domains sequential variety many cases tasks appear require parallel
activity often formulated set threads thread sequential hence



fia ecision heoretic odel ssistance

formulated separate assistant extending framework handle general parallel
assistance interesting future direction
goal dependent transitions dependence reward policy goal allows
model capture agents desires behavior goal dependence
goal less intuitive many cases dependence used
model dynamics environment however allow goal dependence
generality modeling example convenient model basic communication
actions agent changing aspects state actions often
goal dependent
two main obstacles solving intelligent assistance framework
first many scenarios initially hgmdp directly disposal since lack
accurate information agent policy goal distribution ig often due
fact assistant deployed variety initially unknown agents rather assistant environment given possible set goals described
section difficulty learn approximate hgmdp estimating agent
policy goal distribution ig furthermore describe bootstrapping mechanism learning approximations quickly second obstacle solving hgmdp
generally high computational complexity solving hgmdps deal issue section
considers approximate techniques efficiently solving hgmdps
mentioned possible provide assistance simpler domain specific
engineered solutions particular domains consider later could solved several less
expensive solutions require machinery hgmdps fact one domains
compare model existing supervised learning method goal work
provide domain independent framework potentially encapsulate several assistant
systems hope gives rise robust understanding methodology building
systems much less human effort future

theoretical analysis
section analyze hardness solving hgmdps despite
special case pomdps hard motivates model called helper action
mdp hamdp restricted amenable approximate solutions
introduce myopic heuristic solve hamdps analyze performance
analyze special cases hamdps permit efficient solutions
complexity hidden goal mdps
given knowledge agents goal g hgmdp assistants reduces solving
mdp assistant actions mdp transition function captures state change due
assistant action ensuing state change due agent action selected according
given g likewise reward function transition captures reward due assistant action
ensuing agent action conditioned g optimal policy mdp corresponds
optimal assistant policy g however since real assistant often uncertainty
agents goal unlikely optimal performance achieved



fif ern natarajan j udah tadepalli

view hgmdp collection g mdps share state space
assistant placed one mdps beginning episode cannot observe
one mdp fixing goal component hgmdp definition one
goals collection easily modeled restricted type partially observable mdp
pomdp state space g component completely observable g component unobservable changes beginning episode according ig
remains constant throughout episode furthermore pomdp transition provides observations agent action gives direct evidence unchanging g component
perspective hgmdps appear significant restriction general pomdps however
first shows despite restriction worst case complexity reduced even
deterministic dynamics
given hgmdp horizon reward target r short term reward
maximization asks whether exists history dependent assistant policy achieves
expected finite horizon reward least r general pomdps pspacecomplete papadimitriou tsitsiklis mundhenk pomdps deterministic
dynamics np complete littman however following
theorem short term reward maximization hgmdps deterministic dynamics pspacecomplete
proof membership pspace follows fact hgmdp polynomially encoded pomdp policy existence pspace pspace hardness
reduce pspace complete tqbf truth quantified boolean formula
existence history dependent assistant policy expected reward r
let quantified boolean formula form x x x xn c x xn
cm x xn ci disjunctive clause us goal gi clause
agent chooses goal uniformly randomly set goals formed hides
assistant states consist pairs form v v current value
goal clause next variable set actions assistant set existentially
quantified variables agent simulates setting universally quantified variables choosing
actions set equal probability episode terminates variables
set assistant gets reward value clause end reward
otherwise
note assistant get useful informtion goal termination
episode satisfiable assistant policy leads reward
goals choices agent actions hence expected value goal
distribution least one goals satisfied setting universal
quantifiers leading expected value hence tqbf reduces deciding
hgmdp policy expected reward
shows pomdp encoded hgmdp deterministic dynamics stochastic dynamics pomdp captured via stochastic agent policy
hgmdp however hgmdps resulting pspace hardness reduction quite pathological compared likely arise realistic assistant domains importantly
agents actions provide practically information agents goal end episode
late exploit knowledge suggests search restricted classes
hgmdps allow efficient solutions performance guarantees


fia ecision heoretic odel ssistance

helper action mdps
motivation helper action mdps hamdps place restrictions agent assistant avoid following three complexities arise general hgmdps agent
behave arbitrarily poorly left unassisted agent actions may provide significant evidence goal agent free effectively ignore assistants help
exploit assistive action even would beneficial assistant
actions possibility negatively impacting agent compared assistant
hamdps address first issue assuming agent competent approximately
maximizing reward without assistant second third issues addressed assuming agent detect exploit helpful actions assistant actions
never hurt agent even unhelpful
informally hamdp provides assistant helper action agents actions whenever helper action h executed directly corresponding agent action
agent receives bonus reward however agent accept helper action h
taking hence receive bonus action agent considers good
achieving goal without assistant thus primary objective assistant hamdp
maximize number helper actions get accepted agent
simple model captures much essence assistive domains assistant
actions cause minimal harm agent able detect accept good assistance
arises
hamdp hgmdp hs g r ig following constraints
agent assistant actions sets h hn
ai corresponding helper action hi
state space w w w set world states states w
encode current world state previous assistant action
reward function r assistant actions agent actions reward zero
unless agent selects action ai state hi gives reward
agent receives bonus whenever takes action directly corresponding helper
action
assistant acts states w taking hi deterministically
transitions hi
agent acts states resulting states according transition function depend hi e hi g ai g ai
transition function
finally agent policy let g function returns set actions p g
distribution actions view g set actions agent
note assumed bonus rewards simplicity easily extended
non uniform positive bonus rewards particular main concerning bounding regret
myopic policy analogous simply includes constant factor equal maximum possible reward bonus
exceptions theorems currently analogous non uniform reward
bonuses



fif ern natarajan j udah tadepalli

considers acceptable equally good state goal g agent policy selects
ai helper action hi whenever ai acceptable hi g ai whenever
ai g otherwise agent draws action p g
hamdp primary impact assistant action influence reward following
agent action notice hamdps rewards inherent underlying
environment rather rewards bonuses received whenever agent accepts helper
action could defined model include environmental reward addition
helper bonuses unnecessarily complicates model following hardness shows
instead assume inherent environmental reward already captured agent policy
via g considered contain actions approximately optimize reward
example hamdp model captures doorman domain desktop domain experiments doorman domain helper actions correspond opening doors
agent reduces cost navigating one room another desktop domain helper actions correspond offering shortcuts users destination folders importantly
opening incorrect door offering incorrect shortcut increase physical cost
agent assistant key property hamdps
despite apparent simplification hamdps hgmdps turns somewhat
surprisingly worst case computational complexity reduced
theorem short term reward maximization hamdps pspace complete
proof membership pspace follows easily since hamdps specialization hgmdps
proof pspace hardness identical theorem except instead
agents actions stochastic environment universal quantifiers agent accepts
actions last one sets variable suggested assistant
assistants actions environment chooses value universally quantified variable equal
probability last action accepted agent goal clause evaluates otherwise
history dependent policy whose expected reward greater equal number
existential variables quantified boolean formula satisfiable
unlike case hgmdps stochastic dynamics essential pspace hardness
shown later section despite negative following sections demonstrate utility hamdp restriction giving performance guarantees simple policies improved
complexity far analogous general hgmdps
myopic heuristic analysis
hamdps closely related sequential prediction framework littlestone
framework round learner shown instance predicts binary label
prediction incorrect mistake made learner given true label realizable
setting true labels determined hypothesis target class optimal prediction
minimizes upper bound number mistakes possible hypotheses
view helper action hamdp prediction user action maximizing bonus
reward hamdp equivalent minimizing number mistakes sequential prediction
unlike sequential prediction predictions actions need binary sequential
prediction sequence states arbitrarily chosen assume generated


fia ecision heoretic odel ssistance

markov process spite differences sequential prediction
adapted hamdps albeit different terminology however derive
first principles consistency
given assistant policy regret particular episode extra reward oracle
assistant knowledge goal would achieve hamdps oracle assistant
achieve reward equal finite horizon select helper action
accepted agent thus regret execution hamdp equal
number helper actions accepted agent call mispredictions
know optimizing regret pspace hard thus focus bounding
expected worst case regret assistant introduce first myopic heuristic
able achieve regret bounds logarithmic number goals
coarsened posterior heuristic intuitively myopic heuristic select action
highest probability accepted respect coarsened version posterior
distribution goals myopic policy state given history h consistent goal
set c h set goals non zero probability respect history h
straightforward maintain c h observation observations include world state
agents actions myopic policy defined
h arg max ig c h g


g g g set goals agent considers
acceptable action state expression ig c h g viewed probability
mass g coarsened goal posterior assigns goals outside c h probability
zero otherwise weighs proportional prior
theorem hamdp expected regret coarsened posterior heuristic bounded
entropy goal distribution h ig
proof main idea proof misprediction myopic policy e
selected helper action accepted agent uncertainty goal reduced
constant factor allow us bound total number mispredictions trajectory
consider misprediction step coarsened posterior heuristic selects helper action hi
state given history h agent accept action instead selects ai
definition myopic policy know ig c h g ai ig c h g since
otherwise assistant would chosen hi fact argue ig c h
ig c h h history misprediction probability mass
ig consistent goal set misprediction less half consistent goal set
misprediction consider two cases ig c h g ai
ig c h ig c h g ai ig c h first case immediately get
ig c h g ig c h combining fact c h c h
g get desired ig c h ig c h second case note
c h c h g g ai
c h c h g ai
combining assumption second case immediately implies ig c h
ig c h


fif ern natarajan j udah tadepalli

shows misprediction made histories h h ig c h
ig c h implies episode n mispredictions resulting history hn
ig c hn n consider arbitrary episode true goal g know
ig g lower bound ig c hn implies ig g n equivalently n
log ig g thus episode goal g maximum number mistakes bounded
log ig g fact get thepexpected number mispredictions episode
respect ig bounded g ig g log ig g h ig completes
proof
since h ig log g implies hamdps expected regret myopic
policy logarithmic number goals furthermore uncertainty
goal decreases decreasing h ig regret bound improves get regret ig
puts mass single goal turns logarithmic bound asymptotically tight
worst case
theorem exists hamdp assistant policy expected regret least
log g
proof consider deterministic hamdp environment structured binary tree
depth log g leaf corresponds one g goals considering uniform
goal distribution easy verify node tree equal chance true
goal left right sub tree episode thus policy chance
committing misprediction step episode since episode length log g
expected regret episode policy log g
resolving gap myopic policy bound regret lower bound open
pproximate g oal istributions
instead true underlying
suppose assistant uses approximate goal distribution ig
goal distribution ig computing myopic policy assistant selects actions
c h g refer myopic policy relative
maximize ig
g
instead bounded terms kullbackleibler kl diverextra regret ig
g
zero
gence kullback leibler distributions kl ig k ig
g
equals ig

theorem hamdp goal distribution ig expected regret myopic policy
bounded h kl k
respect distribution ig
g
g
g
proof proof similar theorem except since myopic policy respect
rather derive episode maximum number mispredictions n
ig
g



fia ecision heoretic odel ssistance

g fact average number mispredictions given
bounded log ig

p

g

p

g

p

g





ig g log
ig g



ig g log
log ig g log ig g
ig g
x
ig g
ig g log ig g
ig g log

ig g
g

h ig kl ig k ig


note random variable x distribution p finite domain size n
kl p k u log n h p u uniform distribution thus consequence
theorem myopic policy respect uniform goal distribution expected regret
bounded log g hamdp showing logarithmic regret achieved without
knowledge ig strengthened hold worst case regret
theorem hamdp worst case hence expected regret myopic policy
respect uniform goal distribution bounded log g
proof proof theorem shows number mispredictions episode bounded
case g shows worst case regret bound log g
log ig
g
immediately implies expected regret bound uniform myopic policy bounded
log g
deterministic agent policies
consider several special cases hamdps first restrict agents policy
deterministic goal e g single action state goal pair g
theorem myopic policy achieves optimal expected reward hamdps deterministic agent policies
proof given appendix sometimes desirable minimize worst possible regret
compared oracle assistant knows agents goal captured
graph theoretic notion tree rank generalizes rank decision trees ehrenfeucht
haussler
definition rank rooted tree rank root node node leaf node
rank node else node least two distinct children c c equal highest ranks
among children rank node rank c otherwise rank node rank highest
ranked child
optimal trajectory tree ott hamdp deterministic environments tree
nodes represent states hamdp reached prefixes optimal action sequences
different goals starting initial state node tree represents state set
multiple initial states build ott initial state rank would maximum
ranks trees



fif ern natarajan j udah tadepalli

goals optimal path initial state since agent policy deterministic
one trajectory per goal tree hence size optimal trajectory tree
bounded number goals times maximum length trajectory
size state space deterministic domains following lemma follows induction
depth optimal trajectory tree proof appendix
lemma minimum worst case regret policy hamdp deterministic environments deterministic agent policies equal tree rank optimal trajectory tree
leads following
theorem agent policy deterministic minimizing maximum regret
hamdps deterministic environments p
proof first construct optimal trajectory tree compute rank linear time
simultaneously computing optimal minimax policy recursive definition tree rank
follows lemma
bounded branching factor policies
assumption deterministic agent policy may restrictive many domains
consider agent policies may constant number possible actions g
state goal pair defined
definition branching factor hamdp largest number possible actions g
agent state goal assistants action
doorman domain section branching factor since two optimal
actions reach goal state
theorem minimizing worst case regret finite horizon hamdps deterministic environments constant branching factor k np complete
proof appendix minimizing expected regret bounded
k np hard conjecture np question remains open

solving practical hgmdps
although hamdps offer theoretically elegant framework requirements practical assistant
systems easily satisfed assumptions section consider general
solving hgmdps offer practical heuristic solutions inspired
theoretical analysis
principle could use general purpose pomdp solver solve hgmdps
pomdp solvers point methods search methods become efficient years still inefficient used interactive setting
parameters pomdp continually updated moreover analysis previous
section suggests simple myopic heuristics knowledge goal distribution
optimal policies given goals appear promising yield respectable performance reason
adopt bayesian goal estimation followed heuristic action selection
evaluate three different domains first give overview solution
describe components detail


fia ecision heoretic odel ssistance

overview
section assume given hgmdp delegating learning
section let ot ot observation sequence observed assistant
beginning current trajectory time observation tuple world state
previously selected action assistant agent given ot goal compute
assistant action whose value close optimal
motivate useful consider special characteristics hgmdp
importantly belief state corresponds distribution agents goal since agent
assumed goal directed observed agent actions provide substantial evidence
goal might might fact even assistant nothing agents goals
often rapidly revealed analyzing relevance agents initial actions possible
goals cases suggests state goal estimation hgmdp may
solved quite effectively observing agents actions relate possible goals
rather requiring assistant select actions explicitly purpose information gathering
agents goals words cases expect purely nearly myopic
action selection strategies avoid reasoning information gathering effective
reasoning information gathering one key complexities involved solving pomdps
compared mdps leverage intuitive properties hgmdp gain tractability
limiting completely avoiding reasoning course shown pspace hardness
goals rapidly revealed non myopic reasoning essential
note cases assistant pure information gathering actions
disposal e g asking agent question consider actions experiments
believe actions handled naturally framework incorporating
small amount look ahead search
motivation assistant architecture depicted figure alternates
goal estimation action selection follows
observing agents next action update goal distribution hgmdp
model
updated distribution evaluate effectiveness assistant actions including
noop building sparse sampling look ahead tree bounded depth perhaps depth
one leaves evaluated via myopic heuristic
key element architecture computation myopic heuristics top
heuristic optionally obtain non myopic behavior via search building look ahead sparsesampling tree experiments search improve performance small margin
significant computational cost note idea utilizing myopic heuristics select
actions pomdps see example cassandra geffner bonet
similar methods used previously success applications computer bridge
ginsberg main contribution particularly well
suited setting evaluate efficiently computable heuristics specifically designed
solving hgmdps describe goal estimation action selection operations
detail



fif ern natarajan j udah tadepalli

goal estimation

p g

action selection

assistant
ot



wt
environment

user
ut

figure depiction assistant architecture agent hidden goal selects actions
ut cause environment change world state wt typically moving closer
goal assistant upper rectangle able observe world state along
observations generated environment setting contain user agent
actions along world state assistant divided two components first
goal estimation component computes posterior agent goals p g given observations second action selection component uses goal distribution compute
best assistive action via combination bounded search myopic heuristic
computation best action might noop cases none assistive
actions higher utility user



fia ecision heoretic odel ssistance

goal estimation
given hgmdp agent policy initial goal distribution ig objective maintain
posterior goal distribution p g ot gives probability agent goal g
conditioned observation sequence ot note since assumed assistant cannot
affect agents goal observations related agents actions relevant posterior
given agent policy straightforward incrementally update posterior p g ot upon
agents actions
beginning episode initialize goal distribution p g ig timestep
episode ot involve agent action leave distribution unchanged otherwise agent selects action state update posterior according p g ot
z p g ot g z normalizing constant distribution adjusted place weight goals likely cause agent execute action
accuracy goal estimation relies well policy learned assistant reflects
true agent policy described section use model bootstrapping
estimating update estimate end episode provided agent close
optimal experimental domains lead rapid goal estimation even early
lifetime assistant
assumed simplicity actions agent directly observable
domains natural assume state world observable rather
actual action identities cases observing agent transitioning
use mdp transition function marginalize possible agent actions yielding update
p g ot z p g ot

x

g

aa

action selection
given hgmdp distribution goals p g ot address
selecting assistive action mechanisms utilize combination bounded look ahead search
myopic heuristic computations increasing amount look ahead search actions
returned closer optimal cost computation fortunately many hgmdps
useful assistant actions computed relatively little search first describe several
myopic heuristics used greedy action selection combination search
next review utilize sparse sampling obtain non myopic action selection
action election h euristics
explain action selection procedure introduce idea assistant mdp relative
goal g denote g mdp g identical except
change initial goal distribution p g g goal fixed g
episode since hidden component state space goal fixing goal
g makes state fully observable yielding mdp episode g evolves drawing
initial world state selecting assistant actions goal g achieved note
state transition assistant action state successive state transitions first
due assistant action due ensuing agent action selected
agent policy goal g optimal policy g gives optimal assistive action assuming


fif ern natarajan j udah tadepalli

agent acting achieve goal g denote q function g qg
expected cost executing action state following optimal policy
consider second heuristic action selection accounts non uniform rewards true goal posterior unlike coarsened posterior heuristic introduced section
simply expected q value action assistant mdps called qmdp
method cassandra heuristic value assistant action state given observations
ot
x
qg p g ot
h ot
g

intuitively h ot measures utility taking action assumption
goal ambiguity resolved one step thus heuristic value information gathering
utility action rather heuristic favor assistant actions make progress toward goals
high posterior probability goal posterior highly ambiguous often lead
assistant prefer noop least hurt progress toward goal note
heuristic well others used evaluate utility state rather
state action pair maximizing actions maxa h ot
primary computational complexity computing h solve assistant mdps
goal order obtain q functions technically since transition functions assistant
mdps depend approximate agent policy must solve mdp updating
estimate end episode see section policy learning however incremental dynamic programming methods prioritized sweeping moore atkeson
alleviate much computational cost particular deploying assistant solve
mdp offline default agent policy given boltzmann bootstrapping distribution describe section deployment prioritized sweeping used incrementally
update q values learned refinements make
practical exactly solve assistant mdps may resort approximations consider two approximations experiments one replace users policy
used computing assistant mdp fixed default user policy eliminating need
compute assistant mdp every step denote approximation hd another approximation uses simulation technique policy rollout bertsekas tsitsiklis approximate
qg expression h done first simulating effect taking action
state estimate expected cost agent achieve g resulting
state approximate qg assuming assistant select single
initial action followed agent actions formally let cn g function simulates n trajectories achieving goal state averaging trajectory costs

h ot except replace qg expectation
p heuristic h r identical

c g combine heuristics fixed default
user policy policy rollouts denote hd r
parse ampling
heuristics somewhat myopic sense take account
potentially persistent ambiguity agents goal consider use information
gathering actions resolve ambiguity cases beneficial consider nonmyopic reasoning one combine heuristics shallow search belief space



fia ecision heoretic odel ssistance

assistant mdp purpose utilize depth bounded sparse sampling trees kearns
mansour ng compute approximation q function given belief state
st ot denoted qd st ot given particular belief state assistant select
action maximizes qd note convenience represent belief state pair
current state st observation history ot lossless representation belief state since
posterior goal distribution computed exactly ot goal hidden
portion pomdp state
base case q st ot equal one myopic heuristics described
increasing depth looking ahead state transitions evaluating one
heuristics looking ahead possible track potential changes belief state
taking certain actions determine whether changes belief would beneficial
respect providing better assistance sparse sampling look ahead approximately
computing
qd e r g v




v max q





g random variable distributed according goal posterior p g
random variable represents belief state taking action belief state
particular world state arrived simply observation sequence extended
observation obtained state transition first term expectation
represents immediate reward assistant action goal g
sparse sampling approximates expectation averaging set b samples successor belief states sparse sampling pseudo code presented table given input belief
state assistant action heuristic h depth bound sampling width b returns approximation qd first depth bound equal zero heuristic
value returned otherwise b samples observations resulting taking action belief state
generated observations form oi ai si state
resulting taking action state ai ensuing agent action selected goal
drawn goal posterior si taking action ai state observation oi
corresponds belief state si oi oi simply concatenation oi
code recursively computes value belief states maximizing qd
actions averages
b become large sparse sampling produce arbitrarily close approximation
true q function belief state mdp computational complexity sparse sampling linear
b exponential thus depth must kept small real time operation

learning hgmdps
section tackle learning hgmdp interacting environment assist agent assume set goals g known agent primary
role learning acquire agents policy goal distribution assumption natural
situations assistant applied many times environment possibly
different agents example desktop environment environment mdp corresponds
description desktop functionalities remains fixed across users one


fif ern natarajan j udah tadepalli

given heuristic function h belief state action depth bound sampling width b
return approximation qd value belief state
return h
sample set b observations ob resulting taking action
belief state follows
sample environment mdp transition function
b sample goal gi p gi
c sample agent action ai agent policy gi
oi ai si si sample environment mdp transition
function ai
oi gi ai si compute vi maxa qd si oi
p
return qd b r gi vi

table pseudo code sparse sampling hgmdp
provided description mdp typically straightforward learn model
primary cost longer warming period assistant
relaxing assumption provided set possible goals problematic
current framework saw section solution methods depend knowing set
goals clear learn observations since goals unlike states
actions directly observable assistant extending framework assistant
automatically infer set possible user goals allow user define goals
interesting future direction note however often possible designer enumerate
set user goals deployment perhaps complete allows useful assistance
provided
maximum likelihood estimates
straightforward estimate goal distribution g agent policy simply observing
agents actions possibly assisted compute empirical estimates relevant
quantities done storing goal achieved end episode along
set world state action pairs observed agent episode estimate ig
observed frequency goal usually laplace correction avoid
extreme values probabilities likewise estimate g simply frequency
action taken agent state goal g limit
maximum likelihood estimates converge correct values true hgmdp practice
convergence slow slow convergence lead poor performance early stages
assistants lifetime alleviate propose bootstrapping
learning agent policy


fia ecision heoretic odel ssistance

model bootstrapping
leverage environment mdp model order bootstrap learning agent policy
particular assume agent near optimal sense particular goal
world state likely select actions close optimal unrealistic
many application domains might benefit intelligent assistants particular
many tasks conceptually simple humans yet quite tedious e g navigating
directory structure computer desktop performing optimally tasks difficult
humans
given near rationality assumption initialize estimate agents policy
prior biased toward optimal agent actions consider environment mdp
assistant actions removed solve q function q g mdp
techniques q function gives expected cost executing agent action world state
acting optimally achieve goal g agent actions world without assistant
rational agent would select actions maximize q function state goal
furthermore close rational agent would prefer actions achieve higher q values highly
suboptimal actions first define boltzmann distribution used define
prior

g
exp k q g

z g
z g normalizing constant k temperature constant larger values
k skews distribution heavily toward optimal actions given definition prior
distribution w g taken dirichlet parameters
ai g parameter controls strength prior intuitively
thought number pseudo actions represented prior representing
number pseudo actions involved agent action ai since dirichlet conjugate
multinomial distribution form g easy update posterior
g observation one take mode mean posterior point
estimate agent policy used define hgmdp
experiments found prior provides good initial proxy actual agent
policy allowing assistant immediately useful updating posterior tunes
assistant better peculiarities given agent example many cases
multiple optimal actions posterior come reflect systematic bias among equally
good actions agent computationally main obstacle computing
q function needs done given application domain since environment
mdp constant dynamic programming accomplished polynomial time
number states goals practical number alternatives exist including
use factored mdp boutilier et al approximate solution methods boutilier
et al guestrin et al developing domain specific solutions
finally work utilize uninformative prior goal distribution interesting
future direction would bootstrap goal distribution estimate observations
population agents



fif ern natarajan j udah tadepalli

experimental
section present conducting user studies simulations three domains
two game environments folder predictor domain intelligent desktop assistant
user studies two game domains episode users assistants actions
recorded user studies performed human subjects graduate students cs
department oregon state university single session ratio cost achieving
goal assistants help optimal cost without assistant calculated averaged
multiple trials user present similar simulations well third
domain folder predictor domain simulated user used one heuristics
generate top recommended folders user present number clicks required
average user reach desired folder two three domains namely doorman
domain folder predictor domain fall category hamdps since assistive
actions merely viewed helper actions agent ignore kitchen domain
hand needs slightly general formulation since agent assistant
strictly alternate assistants actions cannot ignored agent
doorman domain
doorman domain agent set possible goals collect wood food
gold grid cells blocked cell four doors agent open
door move next cell see figure door closes one time step time
one door open goal assistant help user reach goal faster opening
correct doors
state tuple hs di stands agents cell door open
total number states squares possibilities door actions agent
open door move directions pickup whatever cell
total actions assistant open doors perform noop actions agents
assistants actions strictly alternate domain satisfying definition hamdps
reward cost user open door reward assistants
action trial ends agent picks desired object note included
noop action assistant domain action never selected since cost opening
wrong door noop potential benefit selecting noop
experiment evaluated two heuristics one fixed user policy default
policy hgmdp creation hd avoiding need repeated computation hgmdp
every step second use policy rollout calculate q values hr
trial system chooses goal one two heuristics random user shown
goal tries achieve starting center square every users action
assistant opens door nothing user may pass door open different door
user achieves goal trial ends one begins assistant uses
users trajectory update agents policy
user studies doorman domain presented tabe first two
rows give cumulative user study actions selected greedily according hr
hd respectively rather reporting negtive rewards table shows total number



fia ecision heoretic odel ssistance

figure doorman domain agents goal fetch resource grid cells separated
doors must opened passing

actions trials across users without assistant n total number actions
assistant u average percentage savings u n trials users
seen methods reduce number actions note
assistant selects among four doors random would reduce number actions
comparison omniscient assistant knows users goal reduces number
actions first door opened user
experiments count users first action number actions reduces
observed hr appears slight edge hd one possible reason
hd solve mdp updating user policy hr
updated user policy thus rollout reasoning accurate model user
heuristic
hr
hd
hr
b
b
b
b

total
actions
n








user
actions
u








fractional savings
u n








time
per
action secs








table experiments doorman domain first two rows table present
user studies rest table presents simulation

gives pessimistic estimate usefulness assistant assuming optimal user measure utility
normalized optimal utility without aid assistant
note first action requirement easily aviodable simply equivalent switch indicate
user ready move grid replace requirement explicitly adding button
interface start episode



fif ern natarajan j udah tadepalli

another interesting observation individual differences among users
users prefer fixed path goal regardless assistants actions users
flexible survey conducted end experiment learned one
features users liked system tolerant choice suboptimal paths
data reveals system able reduce costs approximately even
users chose suboptimal trajectories
conducted experiments sparse sampling non zero depths considered
depths sampling widths b b leaves sparse
sampling tree evaluated hr simply applies rollout user policy hence sparse
sampling b would correspond heuristic hr experiments
conduct user studies due high cost effort required humans studies
simulated human users choosing actions according policies learned observed
actions previous user study presented last rows table note
absolute numbers actions user studies simulations comparable
different numbers trajectories human users tested fewer trajectories
minimize fatigue see sparse sampling increased average run time last column
order magnitude able produce reduction average cost user
surprising hindsight simulated experiments sparse sampling able sample
exact user policy e sampling learned policy used
simulations suggest small amount non myopic reasoning
positive benefit substantial computation cost note however bulk benefit
realized assistant obtained without reasoning showing myopic heuristics
well suited domain
kitchen domain
kitchen domain goals agent cook dishes shelves
ingredients dish recipe represented partially ordered plan ingredients
fetched order mixed heated shelves doors
must opened fetching ingredients one door open time
different recipes state consists location ingredients
bowl shelf table mixing state temperature state ingredient bowl
door open state includes action history preserve ordering
plans recipes users actions open doors fetch ingredients pour
bowl mix heat bake contents bowl replace ingredient back shelf
assistant perform user actions except pouring ingredients replacing ingredient
back shelf restricted assistant pouring ingredients irreversible action reward non pour actions experiments conducted human subjects
computer science graduate students unlike doorman domain allowed
assistant take multiple consecutive actions turn switches user assistant
executes noop action
domain large state space hence possible update user policy
every trajectory hence two heuristics compare use default user policy
second heuristic addition uses policy rollout compare actions words compare
hd hd r user studies shown top part table doorman



fia ecision heoretic odel ssistance

figure kitchen domain user prepare dishes described recipes
right assistants actions shown bottom frame

domain total number agent actions without assistant percentage reduction due assistant presented number user actions summed users
cumulative presented observed hd r performs better hd
observed experiments hd r technique aggressive choosing non noop
actions hd would wait goal distribution highly skewed toward particular
goal
heuristic
hd r
hd
hd r
b
b
b
b

total
actions
n








user
actions
u








fractional savings
u n








time
per
action secs








table experiments kitchen domain first two rows table present
user studies last rows present simulation

compared use sparse sampling heuristic simulated user trajectories
domain well see last rows table absolute numbers actions
user studies comparable simuations due different numbers trajectories
case since sparse sampling considers larger number trajectories methods
policies learned sometimes better learned heuristics although took
time execute however significant difference solution quality
rollouts sparse sampling simulations showing myopic heuristics performing


fif ern natarajan j udah tadepalli

well sparse sampling much less computation sparse sampling higher depths requires
order magnitude computation time compared rollout
folder predictor
section present evaluation framework real world domain part
task tracer project dragunov dietterich johnsrude mclaughlin li herlocker
researchers developed file location system called folder predictor bao et al idea
behind folder predictor learning users file access patterns assistant
help user file accesses predicting folder file accessed
saved
setting goal folder predictor minimize number clicks user
predictor would choose top three folders would minimize cost append
ui shown ovals figure user taken first recommended folder
users target folder first recommended folder user would reach folder zero clicks
reach second third recommended folder one click user choose one
recommendations navigate windows folder hierarchy recommendations
relevant

figure folder predictor bao et al
bao et al considered supervised learning implemented costsensitive predictions cost number clicks user bao
et al take account response user
predictions instance user chooses ignore recommended folders navigates
folder hierarchy make predictions due fact model
one time prediction consider user responses considers
restricted set previously accessed folders ancestors possible destinations
precludes handling possibility user accessing folder
decision theoretic model naturally handles case predictions changing recommendations response user actions first step used data collected
user interface used model make predictions use users response predic

fia ecision heoretic odel ssistance

tions make predictions handle possibility folder consider
folders folder hierarchies prediction used mixture density obtain
probability distribution folders
p f p f pl f
p probability according bao et als pl uniform probability distribution set folders ratio number times previously accessed
folder accessed total number folder accesses
idea behind density function early stages task user
accessing folders later stages user access folders particular task
hierarchy hence number folder accesses increases value increases would
eventually converge hence resulting distribution would converge p data set
consists collection requests open file open save file saveas ordered time
request contains information type request open saveas current task
destination folder etc data set consists total open saveas requests folder
hierarchy consists folders
state space consists parts current folder user accessing three
recommendations two unordered would correspond state space size

action user choose recommended folder select
different folder action assistant corresponds choosing top folders
action space size
reward case negative number user
clicks domain assistant users actions strictly alternate assistant revises
predictions every user action prior distribution initialized rewards computed
model developed bao et al
applied decision theoretic model data set request assistant would
make prediction hd r heuristic uses default user policy rollout
method user simulated user would accept recommendation shortens
path goal otherwise would act according optimal policy user
considered close optimal unrealistic real world compare
used model developed bao et al data set present table
restricted folder set
folders

one time prediction



repredictions



table experiments folder predictor domain numbers indicate average number clicks required agent reach correct folder entry
top left hand cell performance current task tracer one
bottom right hand cell performance decision theoretic assistant

table shows average cost folder navigation different cases bao et als original
modified include mixture distributions model without
mixture distributions seen model use mixture distributions
least user cost navigation hence effective bao et al shown


fif ern natarajan j udah tadepalli

performs significantly better windows default prediction average
clicks per folder navigation improvement attributed two modifications
mentioned earlier first use predictions model natural decisiontheoretic framework model makes one time prediction hence cannot make use
users response recommendations secondly considering folders hierarchy
prediction including possibility user accessing folder found useful
observed modifications yields lower cost original
combining two changes significantly effective

discussion related work
work inspired growing interest success building useful software assistants
yorke smith et al lieberman myers et al effort focused
building desktop assistants help tasks calendar scheduling refanidis alexiadis yorke smith email filtering cohen carvalho mitchell line diagnostics skaanning jensen kjaerulff travel ambite barish knoblock
muslea oh minton tasks typically requires designing software system
around specialized technologies example email filtering typically posed
supervised learning cohen et al travel combines information gathering
search constraint propagation ambite et al printer diagnostics formulated
bayesian network inference skaanning et al approaches focus socially assistive robots setting robot designed aid human agents achieving goals johnson
cuijpers juol torta simonov frisiello bazzani yan weber wermter et al unfortunately plethora systems approaches lacks overarching conceptual framework
makes difficult build others work argue decision theoretic
provides common framework allows design systems respond
novel situations flexible manner reducing need pre programmed behaviors formulate general version assistantship involves inferring users goals taking
actions minimize expected costs
earlier work learning apprentice systems focused learning users observation mahadevan mitchell mostow steinberg tadepalli mitchell caruana freitag
j mcdermott zabowski work closely related learning demonstration
programming demonstration johnson konidaris kuindersma grupen barto
atkeson schaal cypher lau wolfman domingos weld emphasis
systems provide interface computer system unobtrusively observe
human user task learn human acts user teacher
performance system measured quickly system learns imitate user
e supervised learning setting note imitation assistance two different things
general expect secretaries learn us typically expected replace
us setting assistants goal reduce expected cost users solving
user assistant capable exactly set actions assistants actions cost
nothing compared users makes sense assistant try completely replace
human even case assistantship framework different learning demonstration
still requires assistant infer users goal actions trying achieve
moreover assistant might learn solve goal reasoning action set



fia ecision heoretic odel ssistance

rather shown examples user general however action
set user assistant may different supervised learning appropriate
example case folder predictor system needs decide set folders
present user user needs decide choose awkward
impossible formulate supervised learning programming demonstration
taking decision theoretic view helps us assistantship principled
manner taking account uncertainty users goals costs taking different
actions assistant chooses action whose expected cost lowest framework naturally
prevents assistant taking actions noop assistive action
expected reduce overall cost user rather learning user behave
framework assistant learns users policy similar secretary learns
habits boss much imitate help effective way work
assumed user mdp small enough solved exactly given users goals
assumption may valid makes sense cases learn user
behave natural treat case users actions provide exploratory
guidance system clouse utgoff driessens gives opportunity
system imitate user knows nothing better improve upon users policy

personal assistant systems pomdp however
systems formulated domain specific pomdps solved offline instance
coach system helped people suffering dementia giving appropriate prompts
needed daily activities boger poupart hoey boutilier fernie mihailidis
use plan graph keep track users progress estimate users responsiveness
determine best prompting strategy distinct difference
single fixed goal washing hands hidden variable user responsiveness
low high rather formulation goal random variable hidden
assistant since state action space significantly smaller states
folder predictor domain possible solve pomdp exactly given need
solve pomdp every user action becomes prohibitively expensive yet another
difference length trajectory goal small case hence plan graph
would suffice capture user policy model restrict plan graph instead
solve user mdp bootstrap policy mentioned learning user policy
future direction work even though start initial estimate user policy
update every goal achieved considered online learning user policy
reasonably good prior note combination two frameworks one modeling
users responsiveness modeling users goal would useful assistant
infers agent goals relevant hidden properties user responsiveness
electric elves assistant takes many mundane responsibilities human
agent including rescheduling meeting appear user likely miss
domain specific pomdp formulated solved offline variety techniques one
since system monitors users short regular intervals radical changes belief
states usually possible pruned search space varakantham maheswaran
tambe neither exact approximate pomdp solvers feasible online setting
pomdp changing learn user must repeatedly solved
costly run boger et al complex implement baseline e g electric


fif ern natarajan j udah tadepalli

elves varakantham et al experiments demonstrate simple methods onestep look ahead followed rollouts would work well many domains pomdps
solved online distinct related work doshi gmytrasiewicz authors introduce
setting interactive pomdps agent agents beliefs clearly
general complex ordinary pomdps model simpler assumes
agent oblivious presence beliefs assistant simplified model suffices
many domains relaxing assumption without sacrificing tractability would interesting
several dialogue systems proposed many decisiontheoretic principles walker singh litman kearns walker instance
njfun system designed mdp provide assistance user interacting user
providing answer users questions uses automatic speech recognizer asr
interpret human dialogues uses dialogue policy choose best action response
goals user could set standard queries locations restaurants wineries
shopping centers etc state space would dialogue states e current state
dialogue user assistant greeting choice state etc observations
interpretations dialogues human asr njfun system usefully
modeled hgmdp goal assistant infer users query given observations provide appropriate response initial assistant policy learned training
data manner similar dialogue policy njfun system
work related line plan recognition naturally extended include
hierarchies hierarchical versions hmms bui venkatesh west pcfgs
pynadath wellman blaylock allen describe statistical goal recognition
uses maximum likelihood estimates goal schemas parameters blaylock allen
approaches notion cost reward incorporating plan recognition
decision theoretic context obtain natural notion optimal assistance namely maximizing
expected utility
substantial area user modeling horvitz et al took bayesian
model whether user needs assistance actions attributes provided
assistance needed spreadsheet application horvitz et al hui boutilier used
similar idea assistance text editing hui boutilier use dbns handcoded
parameters infer type user compute expected utility assisting user
would interesting explore use ideas plan recognition charniak goldman
gal reddy shieber rubin grosz chu song kautz levinson system
take account users intentions attitudes computing optimal policy
assistant
recently methods proposed solving pomdps called point methods pineau gordon thrun porta vlassis spaan poupart kurniawati hsu
lee shani pineau kaplow example method point value
iteration pbvi pineau et al porta et al takes set belief points b input
maintains set pomdp vectors iteration iteration produces set vectors optimal belief point respect vectors previous iteration
approximation made pbvi compared value iteration guarantee
set vectors optimal entire belief space omitting vectors pbvi
maintains constant run time per iteration application efficient point methods



fia ecision heoretic odel ssistance

pbvi decision theoretic assistance evaluation performance compared
policy rollout sparse sampling methods remains promising direction

summary future work
introduced decision theoretic framework assistant systems described hgmdp
appropriate model selecting assistive actions computational complexity hgmdps
motivated definition simpler model called hamdp allows efficient myopic heurstics
tractable special cases
described approximate solution iteratively estimating agents
goal selecting actions myopic heuristics evaluation human subjects two
game domains significantly help user demonstrated
real world folder predictor decision theoretic framework effective state
art techniques folder prediction
one future direction consider complex domains assistant able series activities parallel agent another possible direction assume hierarchical goal
structure user goal estimation context recently assistantship model
extended hierarchical relational settings natarajan et al including parameterized
task hierarchies conditional relational influences prior knowledge assistant prior
knowledge would relax assumption user mdp solved tractably knowledge
compiled underlying dynamic bayesian network bayesian network inference used infer distribution users goals given sequence atomic actions
parameters users policy estimated observing users actions
framework naturally extended case environment partially observable agent assistant requires recognizing actions taken gather
information e g opening fridge decide make available incorporating sophisticated user modeling includes users forgetting goals paying attention
important detail changing intentions would extremely important building
practical systems assistive technology useful assistant quickly learn
tasks expert users transfer knowledge novice users training

acknowledgements
material upon work supported defense advanced projects agency
darpa department interior nbc acquisition services division contract nbchd opinions findings conclusions recommendations expressed
material authors necessarily reflect views darpa alan
fern prasad tadepalli gratefully acknowledge following grants nsf iis onr
n sriraam natarajan thanks army office grant number w nf young investigator program

appendix proof theorem
according theory pomdps optimal action pomdp maximizes sum
immediate expected reward value resulting belief state assistant kaelbling



fif ern natarajan j udah tadepalli

littman cassandra agent policy deterministic initial goal distribution
ig history agent actions states h fully captures belief state agent let
v ig h represent value current belief state value function belief state
given following bellman equation h stands history assistants
action hi agents action aj
v ig h max e r hi g aj v ig h
hi

h



since one agents action g agent action aj subsequent state
value depend hi hence best helper action h assistant given
h ig h arg max e r hi g g
hi
x
arg max
ig g ai g
hi

gc h

arg max ig c h g ai
hi

c h set goals consistent current history h g ai set goals
ai good state ai g indicator function ai g
note h exactly myopic policy

appendix b proof lemma
worst case regret pair g hamdp given following bellman equation
assuming g set possible goals agent current state regret g
terminal state goals g satisfied otherwise regret g mini maxj
regret si gi regret sj gj si gi sj gj children g
outer min due assistant picking helper action goal gi maximize
reward inner max due agent accepting picking different goal
minimize reward proof induction node trajectory tree represents state
set goals g state optimal path
basis g leaf node terminal state goals g satisfied hence
rank equals reward
inductive step suppose induction true children g consider two
cases
case unique child g representing g highest regret among
children inductive hypothesis rank g regret g assistant chooses
helper action corresponds g agent choose actions yield lower
regret worst case choosing helper action would increase regret since
agent could choose add regret regret g regret g
rank g rank g
case least two children g g g highest rank
among children inductive hypothesis rank g rank g regret g
regret g agent increase regret choosing goal g
assistant chooses g vice versa hence regret g regret g rank g


fia ecision heoretic odel ssistance

rank g
hence cases shown regret g rank g

appendix c proof theorem
first np build tree representation history dependent policy
initial state every node tree represeted triple g state
g set goals good path index helper action chosen
policy node root node corresponds possible initial state initial goal
set ig children node tree represent possible successor nodes sj j gj reached
agents response hi whether accepting hi executing ai executing actions
children resulting ai called accepted latter called rejected note
multiple children action dynamics function agents
goal
guess policy tree check maximum regret e maximum number
rejected children path root leaf within bounds verify optimal
policy tree polynomial size note number leaf nodes upper bounded g
maxg n g n g number leaf nodes generated goal g estimate n g
start root navigate downwards node contains g goal set
accepted child contains g child reached g
misprediction k children reached hence number nodes reached g
grows geometrically number mispredictions theorem since
log g mispredictions path n g k log g k logk g log k g log k hence
total number leaf nodes tree bounded g log k total number nodes
tree bounded g log k number steps horizon since
polynomial parameters np
np hardness reduce sat given consider literal clause
ci propositional formula possible goal rest proof identical
theorem except variables set assistant since universal quantifiers
agent rejects setting last variable clause clause evaluates
worst regret goal iff sat satisfying assignment

references
ambite j l barish g knoblock c muslea oh j minton getting
interactive agent execution optimizing travel proceedings
fourteenth conference innovative applications artificial intelligence pp

atkeson c g schaal learning tasks single demonstration proceedings
ieee international conference robotics automation pp
bao x herlocker j l dietterich g fewer clicks less frustration reducing
cost reaching right folder proceedings eleventh international conference
intelligent user interfaces pp
bertsekas p tsitsiklis j n neuro dynamic programming athena scientific



fif ern natarajan j udah tadepalli

blaylock n allen j f statistical goal parameter recognition proceedings
fourteenth international conference automated scheduling pp
boger j poupart p hoey j boutilier c fernie g mihailidis decisiontheoretic task assistance persons dementia proceedings nineteenth international joint conference artificial intelligence pp
boutilier c dean hanks decision theoretic structural assumptions
computational leverage journal artificial intelligence
bui h venkatesh west g policy recognition abstract hidden markov
journal artificial intelligence
cassandra r exact approximate partially observable markov decision processes ph thesis brown university
charniak e goldman r plan recognition stories life corr abs
chu song kautz h levinson r start thing
interactive activity recognition prompting proceedings twenty fifth aaai
conference workshop artificial intelligence smarter living pp
clouse j utgoff p e teaching method reinforcement learning proceedings
ninth international workshop machine learning pp
cohen w w carvalho v r mitchell learning classify email speech acts
proceedings conference empirical methods natural language processing pp

cypher watch programming demonstration mit press
doshi p gmytrasiewicz p particle filtering interactive pomdps
proceedings workshop modeling agents observations pp
dragunov n dietterich g johnsrude k mclaughlin li l herlocker j l
tasktracer desktop environment support multi tasking knowledge workers proceedings tenth international conference intelligent user interfaces pp
driessens k adding guidance relational reinforcement learning third freiburgleuven workshop machine learning
ehrenfeucht haussler learning decision trees random examples information computation
gal reddy shieber rubin grosz b plan recognition exploratory
domains artificial intelligence
geffner h bonet b solving large pomdps real time dynamic programming
proceedings aaai fall symposium pompds
ginsberg l gib steps toward expert level bridge playing program proceedings sixteenth international joint conference artificial intelligence pp
guestrin c koller parr r venkataraman efficient solution
factored mdps journal artificial intelligence



fia ecision heoretic odel ssistance

horvitz e breese j heckerman hovel rommelse k lumiere project
bayesian user modeling inferring goals needs software users proceedings
fourteenth conference uncertainty artificial intelligence pp
hui b boutilier c whos asking help bayesian intelligent assistance proceedings eleventh international conference intelligent user interfaces
pp
johnson cuijpers r juol j torta e simonov frisiello bazzani yan w
weber c wermter et al socially assistive robots comprehensive
extending independent living international journal social robotics
johnson inverse optimal control deterministic continuous time nonlinear systems
ph thesis university illinois urbana champaign
kaelbling l littman cassandra acting partially bservable
stochastic domains artificial intelligence
kearns j mansour ng sparse sampling near optimal
large markov decision processes proceedings sixteenth international
joint conference artificial intelligence pp
konidaris g kuindersma grupen r barto robot learning demonstration
constructing skill trees international journal robotics
kullback leibler r information sufficiency annals mathematical
statistics
kurniawati h hsu lee w sarsop efficient point pomdp
approximating optimally reachable belief spaces proceedings robotics science
systems iv
lau wolfman domingos p weld programming demonstration
version space algebra machine learning
lieberman h user interface goals ai opportunities ai magazine
littlestone n learning quickly irrelevant attributes abound linear threshold
machine learning
littman l sequential decision making ph thesis brown university
mahadevan mitchell mostow j steinberg l tadepalli p apprenticebased knowledge acquisition artificial intelligence
mitchell caruana r freitag j mcdermott zabowski experience
learning personal assistant communications acm
moore w atkeson c g prioritized sweeping reinforcement learning less
data less time machine learning
mundhenk complexity partially observable markov decision
processes ph thesis friedrich schiller universitdt



fif ern natarajan j udah tadepalli

myers k berry p blythe j conleyn k gervasio mcguinness morley pfeffer
pollack tambe intelligent personal assistant task time
management ai magazine vol pp
natarajan tadepalli p fern relational hierarchical model decision theoretic
assistance proceedings seventeenth annual international conference inductive
logic programming pp
papadimitriou c tsitsiklis j complexity markov decision processes mathematics operations
pineau j gordon g thrun point value iteration anytime
pomdps proceedings eighteenth international joint conference artificial
intelligence pp
porta j vlassis n spaan poupart p point value iteration continuous
pomdps journal machine learning
pynadath v wellman p probabilistic state dependent grammars plan recognition proceedings sixteenth conference uncertainty artificial intelligence
pp
refanidis alexiadis yorke smith n beyond calendar mashups intelligent calendaring proceedings twenty first international conference automated
scheduling system demonstrations
shani g pineau j kaplow r survey point pomdp solvers autonomous
agents multi agent systems
singh p litman j kearns j walker optimizing dialogue management reinforcement learning experiments njfun system journal artificial
intelligence
skaanning c jensen f v kjaerulff u printer troubleshooting bayesian networks proceedings thirteenth international conference industrial engineering applications artificial intelligence expert systems pp
varakantham p maheswaran r tambe exploiting belief bounds practical
pomdps personal assistant agents proceedings fourth internation conference
autonomous agents multiagent systems pp
walker application reinforcement learning dialogue strategy selection
spoken dialogue system email journal artificial intelligence
yorke smith n saadati myers k morley design proactive personal
agent task management international journal artificial intelligence tools





