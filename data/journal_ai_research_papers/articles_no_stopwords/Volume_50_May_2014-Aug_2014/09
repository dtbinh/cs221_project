journal artificial intelligence

submitted published

hc search learning framework search
structured prediction
janardhan rao doppa

doppa eecs oregonstate edu

school eecs oregon state university
corvallis usa

alan fern

afern eecs oregonstate edu

school eecs oregon state university
corvallis usa

prasad tadepalli

tadepall eecs oregonstate edu

school eecs oregon state university
corvallis usa

abstract
structured prediction learning function maps structured inputs
structured outputs prototypical examples structured prediction include part ofspeech tagging semantic segmentation images inspired recent successes
search structured prediction introduce framework structured prediction
called hc search given structured input framework uses search procedure guided
learned heuristic h uncover high quality candidate outputs employs
separate learned cost function c select final prediction among outputs
overall loss prediction architecture decomposes loss due h leading
high quality outputs loss due c selecting best among generated
outputs guided decomposition minimize overall loss greedy stage wise
manner first training h quickly uncover high quality outputs via imitation learning
training c correctly rank outputs generated via h according true
losses importantly training procedure sensitive particular loss function
interest time bound allowed predictions experiments several benchmark
domains significantly outperforms several state art methods

introduction
consider structured prediction predictor must produce
structured output given structured input example part speech pos tagging structured input sequence words structured output corresponds
pos tags words image scene labeling another example structured
input image structured output semantic labeling image regions
structured prediction tasks arise several domains ranging natural
language processing e g named entity recognition coreference resolution semantic
parsing computer vision e g multi object tracking activity recognition videos
speech e g text speech mapping speech recognition compuational biology
e g protein secondary structure prediction gene prediction
viewed traditional classification set possible classes structured
prediction exponential size input thus producing
c

ai access foundation rights reserved

fidoppa fern tadepalli

output combinatorial nature introduces non trivial choice selecting
computational framework producing outputs importantly framework needs
balance two conflicting criteria must flexible enough allow complex
accurate structured predictors learned must support inference outputs
within computational time constraints application one core
challenges structured prediction achieve balance criteria
standard structured prediction learn cost function c x
scoring potential structured output given structured input x given cost
function input x output computation involves solving called argmin
minimum cost output given input
arg minyy x c x



example approaches conditional random fields crfs lafferty mccallum
pereira max margin markov networks taskar guestrin koller
structured svms tsochantaridis hofmann joachims altun represent cost
function linear model template features x unfortunately exactly
solving argmin often intractable efficient solutions exist limited
cases dependency structure among features forms tree cases one
forced simplify features allow tractable inference detrimental
prediction accuracy alternatively heuristic optimization method used
loopy belief propagation variational inference methods shown
success practice difficult characterize solutions predict
likely work well
inspired recent successes output space search approaches doppa fern
tadepalli wick rohanimanesh bellare culotta mccallum place
restrictions form cost function methods learn use cost
function conduct search space complete outputs via search procedure
e g greedy search return least cost output uncovered search
prediction search procedure needs able efficiently evaluate cost
function specific input output pairs generally straightforward even
corresponding argmin intractable thus methods free increase
complexity cost function without considering impact inference complexity
approaches achieved state art performance number
benchmark primary contribution highlight fundamental
deficiency share particular prior work uses single cost function serve
dual roles guiding search toward good outputs scoring generated
outputs order select best one serving dual roles often means cost
function needs make unclear tradeoffs increasing difficulty learning indeed
traditional ai search literature roles typically served different functions
mainly heuristic function guiding search cost evaluation function often part
definition selecting final output
study framework structured prediction called hc search
closely follows traditional search literature key idea learn distinct functions
roles heuristic function h guide search generate set
high quality candidate outputs cost function c score outputs generated


fihc search learning framework search structured prediction

heuristic h given structured input predictions made h guide
search strategy e g greedy search beam search time bound generate set
candidate outputs returning generated output least cost according c
move hc search might appear relatively small significant
implications terms theory practice first regret hc search
decomposed loss due h leading high quality outputs
loss due c selecting best among generated outputs decomposition
helps us target training minimize losses individually greedy stagewise manner second performance approaches single
function arbitrarily bad compared hc search worst case
finally practice hc search performs significantly better single
cost function search state art approaches structured prediction
effectiveness hc search particular depends critically
quality search space complete outputs used quality
defined expected depth target outputs zero loss outputs located
ability learn heuristic function effectively guiding search generate highquality candidate outputs accuracy learned cost function selecting
best output among candidate outputs generated heuristic function work
assume availability efficient search space complete outputs provide
effective training regime learning heuristic function cost function within
hc search framework

summary contributions
main contributions work follows introduce hc search framework two different functions learned serve purposes search heuristic
cost function search literature analyze representational power
computational complexity learning within hc search framework identify
novel decomposition overall regret hc search terms generation
loss loss due heuristic generating high quality candidate outputs selection
loss loss due cost function selecting best among generated outputs
guided decomposition propose stage wise learning heuristic
cost functions imitation learning empirically evaluate hc search
number benchmarks comparing state art methods analyzing different dimensions framework
remainder proceeds follows section introduce
setup give high level overview framework analyze complexity hc search
learning describe approaches heuristic cost function learning
section section presents experimental followed engineering methodology applying framework section finally sections
discuss related work future directions


fidoppa fern tadepalli

hc search framework
section first state formal setup describe specifics
search spaces search strategies investigate work next give
high level overview hc search framework along learning objective
setup
structured prediction specifies space structured inputs x space structured outputs non negative loss function l x l x
loss associated labeling particular input x output true output provided training set input output pairs x drawn
unknown target distribution goal return function predictor structured inputs outputs whose predicted outputs low expected loss respect
distribution since learning heuristic cost functions
input output pairs standard structured prediction assume availability
feature function x n computes n dimensional feature vector
pair importantly employ two different feature functions h c heuristic
cost function noting serving two different roles heuristic making
local decisions guide search towards high quality outputs cost function
making global decisions scoring candidate outputs generated heuristic
framework
search spaces search strategies
overview basic search concepts context search framework
search spaces
search space complete outputs assume
given every state search space complete outputs consists input output
pair x representing possibility predicting output structured input
x search space defined terms two functions initial state function
x returns initial state input x successor function
search state x x returns set next states x x yk
share input x parent example sequence labeling
part speech tagging x sequence words corresponding part speech
pos labels successors x might correspond ways changing one
output labels called flipbit space figure provides illustration
flipbit search space handwriting recognition task
search space quality effectiveness hc search framework depends
quality search space used quality search space turn
understood terms expected amount search needed uncover correct output
search procedures time required target output grow
function depth target thus one way quantify expected amount
search independently specific search strategy considering expected depth
target outputs particular given input output pair x target depth


fihc search learning framework search structured prediction

figure example flipbit search space handwriting recognition
search state consists complete input output pair complete output
every state differs parent exactly one label highlighted
state corresponds one true output smallest depth
equal number errors initial state



fidoppa fern tadepalli

defined minimum depth state corresponding target
output example flipbit space shown figure clearly according
definition expected target depth flipbit space equal expected number
errors output corresponding initial state
variety search spaces flipbit space limited discrepancy search
lds space doppa et al defined hand designed proposal
distributions wick et al used past work applies
space focus lds space experiment shown
effectively uncover high quality outputs relatively shallow search depths doppa et al

lds space defined terms recurrent classifier h uses next input
token e g word output tokens small preceding window e g pos labels
predict next output token initial state lds space consists input
x paired output recurrent classifier h x one recurrent
classifiers recurrent classifier makes mistake effects get propagated
stream tokens lds space designed prevent error propagation
immediately correcting mistakes made continuing recurrent classifier
since know mistakes made correct possible
corrections called discrepancies considered hence successors state x
lds space consist running recurrent classifier changing exactly
one label e introducing single discrepancy somewhere current output
sequence preserving previously introduced discrepancies previous work
lds space shown effective uncovering high quality outputs relatively
shallow search depths one would expect good recurrent classifier doppa et al
appendix contains details examples lds space employ
work
search strategies
recall hc search framework role search procedure uncover highquality outputs consider uninformed informed search strategies however
uninformed search procedures depth bounded breadth first search practical
high quality outputs exist small depths even feasible
good choice dont use search time bound intelligent way
make predictions structured prediction informed search strategies
take heuristic functions account greedy search best first search better
choice noting effectiveness depends quality search heuristic h prior
work doppa et al wick et al shown greedy search hill climbing
heuristic value works quite well number structured prediction tasks
used effective search space thus work focus empirical work
hc search framework greedy search though applies widely
hc search
parameterized search space complete outputs e g lds
space heuristic search strategy e g greedy search learned heuristic function


fihc search learning framework search structured prediction

figure high level overview hc search framework given structured input x
search space definition first instantiate search space complete
outputs search node space consists complete input output pair
next run search procedure e g greedy search guided heuristic
function h time bound highlighted nodes correspond search
trajectory traversed search procedure case greedy search
scores nodes correspond cost values different heuristic scores shown figure return least cost output
uncovered search prediction input x

h x learned cost function c x given input x
prediction time bound hc search makes predictions follows traverses search
space starting x search procedure guided heuristic function h
time bound exceeded cost function c applied return least cost
output generated search prediction input x figure gives
high level overview hc search framework
formally let yh x set candidate outputs generated heuristic h
given input x output returned hc search least cost output
set according c e
arg minyyh x c x


fidoppa fern tadepalli

figure example illustrates c search suffer arbitrarily large loss compared hc search

expected loss hc search e h c given heuristic h c
defined
e h c e x l x

goal learn heuristic function h corresponding cost function c minimize
expected loss respective spaces h c e
h c arg min h c hc e h c



contrast framework existing approaches output space search doppa et al
wick et al use single function say c serve dual purpose heuristic
cost function raises question whether hc search uses two different functions strictly powerful terms achievable losses following
proposition shows expected loss hc search arbitrarily smaller
restricting single function c
proposition let h c functions function space
learning minc e c c min h c e h c moreover exist learning
minc e c c arbitrarily larger e worse min h c e h c
proof first part proposition follows fact first minimization
subset choices considered second
see second part consider single training instance search
space shown figure search procedure greedy search guided
h hc search c one function used l n n represents
true loss feature vector node n respectively cost heuristic functions
linear functions n node corresponds lowest loss output greedy search
must follow trajectory highlighted nodes order reach output first consider
hc search highlighted path followed heuristic h needs satisfy
following constraints h h h h weights wh


fihc search learning framework search structured prediction

heuristic satisfies constraints given heuristic function order return node
final output cost function must satisfy following constraints c c
c c c c c c weights wc solve
thus see hc search achieve zero loss
consider case single function c used heuristic cost
function order generate loss zero function c must satisfy combined
set constraints placed heuristic cost function however
verified set weights satisfies c c c c
hence single function c space achieve loss zero
scaling losses constant factors make loss suffered arbitrarily high
thus see potential representational advantages following hcsearch framework follows consider implications added expressiveness
terms worst case time complexity learning
learning complexity
consider feasibility efficient optimal learning simplest setting greedy
search linear heuristic cost functions represented weight vectors wh
wc respectively particular consider hc search consistency
input training set structured examples must decide whether
exists wh wc hc search greedy search achieve zero loss
training set first note shown np hard appealing
learning beam search xu fern yoon particular
imply trivial cases simply determining whether linear
heuristic wh uncovers zero loss search node np hard since hc search
return zero loss outputs heuristic able uncover see
hard
prove stronger provides insight hc search framework particular even easy learn heuristic uncovers
zero loss outputs consistency still hard shows worst case
hardness learning simply hardness discovering
good outputs rather additionally complicated potential interaction
h c intuitively learning h worst case ambiguity
many small loss outputs generate
able effective c return best one formalized following
theorem whose proof appendix
theorem hc search consistency greedy search linear heuristic
cost functions np hard even restrict possible
heuristic functions uncover zero loss output

learning
complexity suggests general learning optimal h c pair
impractical due potential interdependence section develop greedy


fidoppa fern tadepalli

stage wise learning first learns h corresponding c
motivated observing decomposition expected loss components due h
c first describe decomposition staged learning
motivates next describe approaches learning heuristic cost functions
loss decomposition staged learning
heuristic h cost function c expected loss e h c decomposed
two parts generation loss h due h generating high quality outputs
selection loss c h additional loss conditional h due c selecting
best loss output
best loss output generated heuristic formally let yh
set yh x e

yh
arg minyyh x l x

express decomposition follows


e h c e x l x yh
e x l x l x yh


z


z

h



c h

note given labeled data straightforward estimate generation
selection loss useful diagnosing hc search framework example one
observes system high generation loss little payoff working
improve cost function empirical evaluation illustrate
decomposition useful understanding learning
addition useful diagnosis decomposition motivates learning targets minimizing errors separately particular optimize
overall error hc search greedy stage wise manner first train
heuristic h order optimize generation loss component h train cost
function c optimize selection loss c h conditioned h
h arg minhh h
c arg mincc c h
note greedy sense h learned without considering
proof theorem hinges coupling
implications learning c
found practice learning h independently c effective strategy
follows first describe generic heuristic function learning
applicable wide range search spaces search strategies explain
cost function learning
heuristic function learning
generally learning heuristic viewed reinforcement learning rl heuristic viewed policy guiding search actions rewards


fihc search learning framework search structured prediction

received uncovering high quality outputs zhang dietterich fact
explored structured prediction case greedy search wick
rohanimanesh singh mccallum shown effective given carefully
designed reward function action space viable general purpose
rl quite sensitive parameters specific definition reward
function actions make designing effective learner quite challenging indeed recent work jiang teichert daume iii eisner shown generic rl
struggle structured prediction even significant effort
put forth designer hence work follow imitation
learning makes stronger assumptions nevertheless effective
easy apply across variety
heuristic function learning via exact imitation
input training examples search space definition l loss function
rank search procedure max search time bound
output h heuristic function
initialize set ranking examples r
training example x

x initial state search tree

set open nodes internal memory search procedure

search step max

select state expand nt select l mt

expand every state nt successor function ct expand nt

prune states update internal memory state search procedure
mt prune l mt ct nt

generate ranking examples rt imitate search step

add ranking examples rt r r r rt aggregation training data

end
end
h rank learner r learn heuristic function ranking examples
return learned heuristic function h
heuristic learning observation many structured
prediction quickly generate high quality outputs guiding
search procedure true loss function l heuristic obviously
done training data know suggests formulating heuristic
learning framework imitation learning attempting learn heuristic
mimics search decisions made true loss function training examples
learned heuristic need approximate true loss function uniformly output
space need make distinctions important guiding search
main assumptions made true loss function provide effective
heuristic guidance search procedure worth imitating
learn imitate search decisions sufficiently well
imitation learning similar prior work learning single cost functions
output space search doppa et al however key distinction learning


fidoppa fern tadepalli

focused making distinctions necessary uncovering good outputs purpose
heuristic hence requires different formulation prior work order
avoid need approximate loss function arbitrarily closely restrict
rank search strategies search strategy called rank makes
search decisions comparing relative values search nodes ranks assigned
heuristic rather sensitive absolute values heuristic common
search procedures greedy search beam search best first search fall
category
imitating search behavior
given search space complete outputs rank search procedure
search time bound learning procedure generates imitation training data
training example x follows run search procedure time bound
input x heuristic equal true loss function e h x l x
search process observe pairwise ranking decisions made
oracle heuristic record sufficient see replicating search
state x smaller loss x ranking example generated
form constraint h x h x ties broken fixed arbitrator
aggregate set ranking examples collected training examples given
learning learn weights heuristic function
learn function h hypothesis space h consistent
ranking examples learned heuristic guaranteed replicate oracle guided
search training data given assumptions base learning
e g pac generic imitation learning used give generalization guarantees
performance search examples khardon fern yoon givan
syed schapire ross bagnell experiments simple
described performs extremely well
describes heuristic function learning via exact imitation
search guided loss function applicable wide range search spaces search
procedures loss functions learning takes input x
set training examples structured prediction e g handwriting recognition
search space complete outputs e g lds space initial
state function successor function l task loss function defined
complete outputs e g hamming loss rank search procedure e g greedy
search max search time bound e g number search steps
algorithmic description assumes search procedure
described terms three steps executed repeatedly open list search
nodes selection expansion pruning execution search procedure
selects one open nodes internal memory expansion step
heuristic value expands selected nodes generate candidate set step
retains subset open nodes expansion internal memory
prunes away remaining ones step heuristic value example
lds space employed work implemented arbitrator breaks ties
position discrepancy prefers earlier discrepancies



fihc search learning framework search structured prediction

greedy search maintains best node best first beam search retains best b
nodes fixed beam width b pure best first search pruning
loops training example collects set ranking constraints specifically example x search procedure run time bound
max true loss function l heuristic steps search step
set pairwise ranking examples generated sufficient allowing search step
imitated step described detail constraints
aggregated across search steps training examples given rank learning
e g perceptron svm rank learn weights heuristic function
step
important step heuristic function learning generation
ranking examples imitate step search procedure step follows
give generic description sufficient pairwise decisions imitate search
illustrate greedy search simple example
sufficient pairwise decisions
noted need collect learn imitate sufficient pairwise
decisions encountered search say set constraints sufficient
structured training example x heuristic function consistent
constraints causes search follow trajectory open lists encountered
search precise specification constraints depends actual search procedure
used rank search procedures sufficient constraints
categorized two types
selection constraints ensure search node internal memory
state expanded next search step ranked better
nodes
pruning constraints ensure internal memory state set search nodes
search procedure preserved every search step specifically
constraints involve ranking every search node internal memory state better
lower h value pruned
illustrate constraints concretely greedy search noting similar
formulations rank search procedures straightforward see doppa fern
tadepalli beam search formulation
constraints greedy search
basic rank search procedure given input x traverses
search space selecting next state successor current state looks best
according heuristic function h particular si search state step greedy
search selects si arg minss si h x greedy search internal
memory state search procedure step consists best open unexpanded
node si


fidoppa fern tadepalli

figure example search tree illustrates greedy search loss function
node represents complete input output pair evaluated loss
function highlighted nodes correspond trajectory greedy search
guided loss function

let x yi correspond input output pair associated state si since greedy
search maintains single open node si internal memory every search step
selection constraints let ci candidate set expanding state si
e ci si let si best node candidate set ci evaluated
loss function e si arg minsci l greedy search prunes nodes
candidate set si pruning constraints need ensure si ranked better
nodes ci therefore include one ranking constraint every
node x ci x yi h x yi h x
illustrate ranking constraints example figure shows
example search tree depth two associated losses every search node
highlighted nodes correspond trajectory greedy search loss function
learner imitate first search step h h h h pruning
constraints similarly h h h h form pruning constraints
second search step therefore aggregate set constraints needed imitate greedy
search behavior shown figure
h h h h h h h h
cost function learning
given learned heuristic h want learn cost function correctly ranks
potential outputs generated search procedure guided h formally let yh x
set candidate outputs generated search procedure guided heuristic h
given input x lbest loss best output among outputs evaluated
true loss function l e lbest minyyh x l x exact learning scenario
goal parameters cost function c every training example


fihc search learning framework search structured prediction

x loss minimum cost output equals lbest e l x lbest
arg minyyh x c x practice exact learning isnt possible goal
cost function average loss training data predicted output
cost function minimized
cost function learning via cross validation
input training examples search space definition l loss function
search procedure max search time bound
output c cost function
divide training set k folds dk
learn k different heuristics h hk
k

ti j dj training data heuristic hi

hi learn heuristic ti l max heuristic learning via
end
generate ranking examples cost function training
intialize set ranking examples r
k

training example x di

generate outputs running search procedure heuristic hi time
bound max yhi x generate outputs x hi max

compute set best loss outputs ybest yhi x l x lbest
lbest minyyh x l x


pair outputs ybest ybest yhi x ybest

add ranking example c x ybest c x r

end

end
end
train cost function ranking examples
c rank learner r
return learned cost function c
formulate cost function training instance rank learning
agarwal roth specifically want best loss outputs yh x
ranked better non best loss outputs according cost function
bi partite ranking let ybest set best loss outputs yh x e
ybest yh x l x lbest generate one ranking example every pair
outputs ybest ybest yh x ybest requiring c x ybest c x search
procedure able generate target output e lbest similar
standard learning crfs svm struct much simpler rank learning
cost function needs rank correct output incorrect outputs
generated search set best loss outputs ybest large bi partite
ranking may highly constrained cases one could relax
attempting learn cost function ranks least one output ybest higher
non best loss outputs easily implemented online learning


fidoppa fern tadepalli

framework follows error e best cost output according current
weights
ybest weights updated ensure best cost output ybest ybest
according current weights ranked better outputs yh x ybest
important note theory practice distribution outputs
generated learned heuristic h testing data may slightly different
one training data thus train c training examples used train h c
necessarily optimized test distribution mitigate effect train
cost function via cross validation see training cost function
data used train heuristic training methodology commonly
used ranking style collins among others
describes cost function training via cross validation
four main steps first divide training data k folds
second learn k different heuristics heuristic hi learned data
folds excluding ith fold steps third generate ranking examples
cost function learning described heuristic hi data
trained steps finally give aggregate set ranking examples r rank
learner e g perceptron svm rank learn cost function c step
rank learner
section describe specifics rank learner used learn
heuristic cost functions aggregate sets ranking examples produced
use shelf rank learning e g perceptron
svm rank base learner train heuristic function set ranking
examples r specific implementation employed online passive aggressive
pa crammer dekel keshet shalev shwartz singer base
learner training conducted iterations experiments
pa online large margin makes several passes training
examples r updates weights whenever encounters ranking error recall
ranking example form h x h x heuristic training c x
c x cost function training x structured input target output
potential outputs x l x l x let
difference losses two outputs involved ranking example
experimented pa variants use margin scaling margin scaled slack
scaling errors weighted tsochantaridis joachims hofmann altun since
margin scaling performed slightly better slack scaling report pa
variant employs margin scaling give full details margin scaling
update
let wt current weights linear ranking function
ranking error
cycling training data e wt x wt x
weights wt corrects error obtained following equation
wt wt x x


fihc search learning framework search structured prediction

learning rate given
wt x wt x

k x x k





specific update previously used cost sensitive multiclass classification
crammer et al see equation structured output keshet
shalev shwartz singer chazan see equation

experiments
section empirically investigate hc search compare
state art structured prediction
datasets
evaluate following four structured prediction including
three benchmark sequence labeling image labeling
handwriting recognition hw input sequence binary segmented
handwritten letters output corresponding character sequence z
dataset contains roughly examples divided folds taskar et al
consider two different variants task work hal daume
iii langford marcu hw small version use one fold training
remaining folds testing vice versa hw large
nettalk stress text speech mapping task
assign one stress labels letter word training
words test words standard dataset use sliding window size
observational features
nettalk phoneme similar nettalk stress except task
assign one phoneme labels letter word
scene labeling data set contains images outdoor scenes vogel
schiele image divided patches placing regular grid size
entire image patch takes one semantic labels sky
water grass trunks foliage field rocks flowers sand simple appearance features
including color texture position used represent patch training
performed images remaining images used testing
experimental setup
hc search experiments use limited discrepancy space lds exactly
described work doppa et al search space structured outputs
prior work hc search shown greedy search works quite well structured prediction tasks particularly lds space doppa et al hence
consider greedy search experiments would point experiments shown beam search best first search produce similar


fidoppa fern tadepalli

training testing set search time bound search steps domains
except scene labeling much larger search space uses
found values larger produce noticeable improvement
extremely small values performance tends worse increases quickly
made larger full spectrum time bounds later
domains learn linear heuristic cost functions second order features unless otherwise noted case feature vector measures features neighboring label pairs
triples along features structured input measure error hamming
loss unless otherwise noted
comparison state art
compare hc search structured prediction including crfs lafferty et al svm struct tsochantaridis et al
searn hal daume iii et al cascades weiss taskar c search
identical hc search except uses single function output space search
doppa et al performance recurrent simple
recurrent classifier trained exactly work doppa et al top section
table shows error rates different scene labeling
possible run crfs svm struct cascades due complicated grid structure
outputs hence table report best published crfs
svm struct searn cascades trained implementation weiss provided authors used sequence labeling hamming loss
would point cascades differ appear
work doppa fern tadepalli obtained updated version
cascades training code across benchmarks see hc search comparable significantly better state art including c search uses single
function heuristic function cost function scene labeling
domain significant improving error rate
hc search state art across learning
separate heuristic cost functions significantly improve output space search
higher order features
one advantages compared many frameworks structured prediction ability use expressive feature spaces without paying huge computational
price bottom part table shows third order features compared
second order hc search c search cascades note practical
run methods third order features due substantial increase inference
time overall error hc search higher order features slightly improved compared
second order features across benchmarks still better error rates
c search cascades third order features exception cascades
hw large fact hc search second order features still outperforming
third order methods three five domains
personal communication author



fihc search learning framework search structured prediction


hw small

hw large

datasets
stress phoneme

scene labeling

hc search
c search
crf
svm struct
recurrent
searn
cascades

comparison state art



































hc search
c search
cascades

b third order features

















table error rates different structured prediction
loss decomposition analysis
examine hc search c search terms loss decomposition see equation generation loss h selection loss c h quantities
easily measured hc search c search keeping track best loss output
generated search guided heuristic cost function c search
across testing examples table shows giving overall error hc
decomposition across benchmarks hc search c search
first see generation loss h similar c search hc search across
benchmarks exception scene labeling hc search generates slightly better
outputs shows least lds search space difference performance
c search hc search cannot explained c search generating lower quality
outputs rather difference two methods reflected difference
selection loss c h meaning c search effective ranking outputs
generated search compared hc search clearly shows advantage
separating roles c h understandable light training mechanism
c search cost function trained satisfy constraints related
generation loss selection loss turns many generation
loss constraints hypothesize biases c search toward low generation loss
expense selection loss
methods selection loss c h contributes significantly overall error compared h shows approaches
able uncover high quality outputs unable correctly rank generated
outputs according losses suggests first avenue improving
hc search would improve cost function learning component e g
non linear cost functions


fidoppa fern tadepalli

ablation study
futher demonstrate two separate functions heuristic cost function
hc search lead accurate predictions compared single function
c search perform ablation experiments study take learned
heuristic function h cost function c hc search framwork use one
make predictions example hh search corresponds configuration
use function h heuristic cost function similarly cc search corresponds
configuration use function c heuristic cost function
table b shows ablation experiments make several interesting observations first overall error hc search significantly
better hh search cc search second selection loss hh search
increases compared hc search understandable h trained
score candidate outputs generated search third generation
loss cc search increases compared hc search behavior significant
increases compared scene labeling task provide
evidence importance separating training heuristic cost
functions
hw small
c h
h

stress
c h
h

hc




hc search vs c search
























b ablation study






















c heuristic function training via dagger























datasets
error

hc

hc search
c search










hh search
cc search










hc search
c search







hc

hw large
c h
h

hc

phoneme
c h
h

hc

scene
c h
h

oracle heuristic
lc search
oracle h























table hc search error decomposition heuristic cost function
heuristic training via dagger
heuristic learning follows simplest imitation learning exact
imitation learner attempts exactly imitate observed expert trajectories
imitate search oracle heuristic experiments exact
imitation performs quite well known exact imitation certain deficiencies
general particular functions trained via exact imitation prone error propagation kaariainen ross bagnell errors made test time change
distribution decisions encountered future compared training distribution
address sophisticated imitation learning developed state art dagger ross gordon bagnell


fihc search learning framework search structured prediction

consider whether dagger improve heuristic learning turn overall
accuarcy
dagger iterative iteration adds imitation data aggregated data set first iteration follows exact imitation data
collected observing expert trajectory number iteration
imitation function heuristic learned current data successive iterations
generate trajectories following mixture expert suggestions case ranking decisions suggestions recently learned imitation function decision point
along trajectory added aggregate data set labeling expert decision
way later iterations allow dagger learn states visited possibly erroneous learned functions correct mistakes expert input ross et al
iterations dagger learned policy without mixing
expert policy performs well across diverse domains therefore use
dagger experiments experiments run iterations dagger
noting noticable improvement observed iterations
table c shows hc search c search obtained training dagger hc search generation loss h improved slightly sequence labeling
little room improvement dagger leads significant improvement generation loss challenging scene labeling
see overall error hc search scene labeling reduces due improvement
generation loss showing cost function able leverage better outputs produced
heuristic similarly overall error c search improved dagger across
board see significant improvements handwiriting scene labeling
domains interesting note unlike hc search improvement c search
mostly due improvement selection loss c h except scene labeling task
due improvement generation loss selection loss
improving heuristic learning able improve overall
performance clear whether improvement perhaps due future
advances imitation learning would yet lead overall improvement
may possible improve generation loss clear cost function
able exploit improvments help evaluate ran experiment
gave hc search true loss function use heuristic oracle heuristic e
h x l x training cost function testing provides
assessment much better might able could improve heuristic
learning table label lc search oracle h
oracle heuristic h negligible might expect smaller observed
hc search shows may possible improve heuristic learning
via better imitation
see oracle overall error hc better
hc search hw small scene labeling tasks selection error c h got slightly
worse indicates cost function learner able leverage varying degrees better outputs produced oracle heuristic suggests improving
heuristic learner order reduce generation loss could viable way
reducing overall loss hc search even without altering current cost learner however saw much less room improve heuristic learner


fidoppa fern tadepalli

data sets hence potential gains less directly trying improve cost
learner
training different time bounds

train

trained hc search different time bounds e number greedy search steps
see overall loss generation loss selection loss vary increase training
time bound general time bound increases generation loss monotonically
decrease since strictly outputs encountered hand difficulty
cost function learning increase time bound grows since must learn distinguish larger set candidate outputs thus degree overall
error decreases grows time bound depends combination much
generation loss decreases whether cost function learner able accurately
distinguish improved outputs
figure shows performance hc search full spectrum time bounds
qualitatively see generation loss due heuristic decreases remarkably
fast benchmarks improves little initial decrease see
cost function learner achieves relatively stable selection loss short time though
increase bit time cases combined effect see overall
error hc improves quickly increase time bound improvement tends
small beyond certain time bound cases e g phoneme prediction
scene labeling performance tends get slightly worse large time bounds
happens increase selection loss counteracted decreased generation
loss
loss function
hamming
vc

test
hamming
vc





table training non hamming loss functions

training non hamming loss functions
one advantages hc search compared many approaches structured
prediction sensitive loss function used training trained hcsearch different loss functions handwriting domain verify true
practice used hamming loss uniform misclassification cost characters
vowel consonant vc loss different misclassification costs vowels consonants
experiment vc loss used misclassification costs vowels
consonants respectively training done folds remaining folds used
testing table shows training testing two loss functions
report cumulative loss testing examples see testing
loss function training loss function gives slightly better performance
training different loss function shows hc search learning


fihc search learning framework search structured prediction

figure hc search training different time bounds training time
bound e greedy search steps x axis error axis
three curves graph corresponding overall loss hc generation loss h
selection loss c h



fidoppa fern tadepalli

sensitive loss function however may hold generally much
depends structure loss function ability cost function
capture loss
discussion efficiency hc search
hc search framework basic computational elements include generating candidate
states given state computing heuristic function features via h cost function
features via c candidate states computing heuristic cost scores via
learned heuristic cost function pair h c computational time generating
candidate states depends employed search space initial
state function successor function example generation candidates
efficient flipbit space compared lds space involves running
recurrent classifier every action specified successor function therefore
efficiency overall depends size candidate set
greatly improved generating fewer candidate states e g via pruning parallelizing
computation done preliminary work direction introducing sparse
versions lds flipbit search spaces pruning actions recurrent
classifier scores specified prunining parameter k simple pruning strategy
resulted fold speedup little loss accuracy across several benchmark
doppa et al however work needs done learning pruning
rules improve efficiency hc search

engineering methodology applying hc search
section describe engineering methodology applying hc search framework high level methodology involves selecting effective
time bounded search architecture search space search procedure search time bound
leveraging loss decomposition terms generation selection loss training
debugging heuristic cost functions describe steps detail
selection time bounded search architecture
time bounded search architecture instantiated selecting search space search
strategy search time bound mentioned effectiveness hc search
depends critically quality search space e search depth target
outputs found employed fact prior work empirically demonstrated performance gap search architectures flipbit space lds
space grows difference target depths increase doppa et al
therefore important select design high quality search space
hand
exists greedy predictor structured prediction one could leverage define appropriate variant lds space fortunately greedy
predictors several natural language processing computer vision relational
networks preferences example transition parsers dependency parsing nivre goldberg elhadad greedy classifiers co reference


fihc search learning framework search structured prediction

resolution chang samdani roth stoyanov eisner event extraction
li ji huang sequential labelers boundary detection objects images
payet todorovic iterative classifiers collective inference relational networks
sen namata bilgic getoor gallagher eliassi rad doppa yu tadepalli
getoor classifier chains multi label prediction read pfahringer holmes
frank greedy planners preferences xu fern yoon
general designing high quality search spaces key topic
work needs done direction learning search operators macro actions
transformation rules transformation learning tbl brill optimize
search space one many possibilities sometimes structure help
designing effective search spaces example multi label prediction
outputs binary vectors small number active labels highly sparse
simple flipbit space initialized null vector effective doppa yu
fern tadepalli b
picking search space need select appropriate search procedure
search time bound effectiveness search architecture measured performing
oracle search true loss function used heuristic cost function training
data one could perform oracle search search different search procedures e g
greedy beam search different time bounds select search procedure
effective see benefit beam search considered
expect change harder non hamming loss functions e g
b cubed score co reference resolution search space redundant
fix search time bound value performance search architecture
stagnates otherwise one allow slack search procedure recover
errors experiments found size structured output
reasonable value time bound figure provides justification choice
training debugging
training procedure involves learning heuristic h cost function c optimize
performance selected time bounded search architecture training data
following staged learning one could start learning heuristic via exact
imitation oracle search learned heuristic h evaluated
measuring generation loss hl search configuration performance hlsearch configuration acceptable respect performance search
move cost function learning part otherwise try improve heuristic
employing sophisticated imitation learning e g dagger enriching
feature function h employing powerful rank learner similarly learning
cost function c conditioned learned heuristic measure selection loss
selection loss high try improve cost function adding
expressive features c employing powerful rank learner

comparison related work
described earlier majority structured prediction work focused use
exact inference computing outputs tractable approximate inference


fidoppa fern tadepalli

techniques loopy belief propagation relaxation methods learning focused tuning cost function parameters order optimize
objective functions differ among learning lafferty et al taskar
et al tsochantaridis et al mcallester hazan keshet
approximate cost function learning approaches employ inference routine
training example piece wise training sutton mccallum decomposed
learning samdani roth special case pseudo max training sontag meshi
jaakkola globerson fall category training approaches
efficient still need inference make predictions testing
cases one could employ constrained conditional ccm framework
chang ratinov roth declarative global constraints make predictions learned cost function ccm framework relies integer linear
programming ilp inference method roth tau yih recent work attempted integrate approximate inference cost function learning principled
manner meshi sontag jaakkola globerson stoyanov ropson eisner
hazan urtasun domke researchers worked higher order
features crfs context sequence labeling pattern sparsity assumption
ye lee chieu wu qian jiang zhang huang wu however
approaches applicable graphical sparsity assumption
hold
alternative addressing inference complexity cascade training felzenszwalb mcallester weiss taskar weiss sapp taskar
efficient inference achieved performing multiple runs inference coarse level
fine level abstraction approaches shown good success place
restrictions form cost functions facilitate cascading another potential drawback cascades approaches ignore loss
function e g assuming hamming loss require loss function
decomposable way supports loss augmented inference sensitive
loss function makes minimal assumptions requiring
blackbox evaluate potential output
classifier structured prediction avoid directly solving argmin assuming structured outputs generated making series discrete
decisions attempts learn recurrent classifier given input
x iteratively applied order generate series decisions producing target
output simple training methods e g dietterich hild bakiri shown
good success positive theoretical guarantees syed schapire
ross bagnell however recurrent classifiers prone error propagation
kaariainen ross bagnell recent work e g searn hal daume iii
et al smile ross bagnell dagger ross et al attempts
address issue sophisticated training techniques shown state theart structured prediction however approaches use classifiers produce
structured outputs single sequence greedy decisions unfortunately many
decisions difficult predict greedy classifier crucial
good performance contrast leverages recurrent classifiers define good


fihc search learning framework search structured prediction

quality search spaces complete outputs allows decision making comparing
multiple complete outputs choosing best
non greedy methods learn scoring function search space
partial structured outputs daumeiii marcu daume iii xu fern yoon
b huang fayong guo yu huang mi zhao methods
perform online training differ way search errors defined
weights updated errors occur unfortunately training scoring function
difficult hard evaluate states partial outputs theoretical
guarantees learned scoring function e g convergence generalization
rely strong assumptions xu et al b
work closely related output space search approaches doppa et al
wick et al use single cost function serve search heuristic
score candidate outputs serving dual roles often means cost
function needs make unclear tradeoffs increasing difficulty learning hcsearch overcomes deficiency learning two different functions heuristic
function guide search generate high quality candidate outputs cost function
rank candidate outputs additionally error decomposition hc search terms
heuristic error cost function error allows human designers learning system
diagnose failures take corrective measures
related ranking collins uses generative
model propose k best list outputs ranked separate ranking
function contrast rather restricting generative model producing potential
outputs leverages generic search efficient search spaces guided
learned heuristic function minimal representational restrictions employs
learned cost function rank candidate outputs recent work generating multiple
diverse solutions probabilistic framework considered another way producing
candidate outputs representative set approaches line work diverse mbest batra yadollahpour guzman rivera shakhnarovich best modes park
ramanan chen kolmogorov zhu metaxas lampert determinantal
point processes kulesza taskar
general area speedup learning studied search community
related work fern cost function typically known
objective learn control knowledge e heuristic function directing search
low cost terminal node search space example stage boyan
moore learns evaluation function states improve performance
search value state corresponds performance local search
starting state zhang dietterich use reinforcement learning rl
methods learn heuristics job shop scheduling goal minimizing duration
schedule unlike combinatorial optimization
cost function given structured prediction therefore hc search
learns cost function score structured outputs along heuristic
function guide search towards low cost outputs


fidoppa fern tadepalli

summary future work
introduced hc search framework structured prediction whose principal feature
separation cost function search heuristic showed framework
yields significantly superior performance state art allows informative error analysis diagnostics
investigation showed main source error existing output space approaches including hc search inability cost function correctly rank candidate outputs produced output generation process analysis
suggests learning powerful cost functions e g regression trees mohan chen
weinberger eye towards anytime performance grubb bagnell
xu weinberger chapelle would productive suggested
room improve overall performance better heuristic learning thus another
direction pursue heuristic function learning speed process generating
high quality outputs fern
future work includes applying framework challenging natural language processing e g co reference resolution dependency parsing semantic
parsing computer vision e g object detection biological images lam doppa hu
todorovic dietterich reft daly multi object tracking complex sports
videos chen fern todorovic effectiveness hc search depends
quality search space therefore work needs done learning
optimize search spaces leveraging structure similarly studying pruning
techniques improve efficiency learning inference another useful
direction
acknowledgements
authors would thank anonymous reviewers jason eisner associate
editor comments feedback first author would thank tom
dietterich encouragement support throughout work work supported part nsf grants iis iis part defense advanced
projects agency darpa air force laboratory afrl
contract fa opinions findings conclusions recommendations expressed material author necessarily reflect
views nsf darpa air force laboratory afrl
us government preliminary version article published aaai doppa
et al

appendix limited discrepancy search lds space
limited discrepancy search lds space doppa et al defined terms
learned recurrent classifier h thus start describing recurrent classifier
explain key idea behind lds space simplicity explain main ideas
sequence labeling handwriting recognition task noting generalize
non sequence labeling full details see doppa et al


fihc search learning framework search structured prediction

figure illustration recurrent classifier handwriting recognition classifier predicts labels left right order makes labeling decision
position greedily character image predicted label
previous position shown dotted box particular example
classifier makes mistake first position error propagates
positions leading bad output

recurrent classifier
sequence labeling recurrent classifier produces label position
sequence input position predicted labels previous positions
dietterich et al learned classifier accurate number incorrect
labeling decisions relatively small however even small number errors
propagate cause poor outputs
figure illustrates recurrent classifier handwriting recognition example
classifier predicts labels left right order makes labeling decision
position greedily character image predicted label previous
position shown dotted box particular example classifier makes
mistake first position error propagates leading bad output
errors
limited discrepancy search lds
lds originally introduced context solving heuristic search
harvey ginsberg key idea behind lds realize classifier
prediction corrected small number critical errors much better output


fidoppa fern tadepalli



b

figure illustration limited discrepancy search lds handwriting recognition
given discrepancy set generate unique output
running recurrent classifier changes lds one
discrepancy introduce discrepancy first position label shown
red run classifier able correct two subsequent labels b
lds two discrepancies introduce additional discrepancy fifth
position label c shown red run classifier recover target
output struct

would produced lds conducts shallow search space possible corrections
hope finding output better original
given classifier h sequence length discrepancy pair l
index sequence position l label generally different
prediction classifier position set discrepancies
generate unique output h x running classifier changes
discrepancies viewed overriding prediction h particular positions
possibly correcting errors introducing errors one extreme empty
get original output produced greedy classifier see figure
extreme specifies label position output influenced h
completely specified discrepancy set figure illustrates lds
handwriting example introduce discrepancy first position label
shown red run classifier able correct two subsequent labels see
figure introduce additional discrepancy fifth position label c shown
red run classifier recover target output struct see figure b


fihc search learning framework search structured prediction

figure example limited discrepancy search lds space handwriting recognition
highlighted state corresponds one true output
smallest depth

practice h reasonably accurate primarily interested small
discrepancy sets relative length sequence know
corrections made thus lds conducts search discrepancy
sets usually small large sets

lds space
given recurrent classifier h define corresponding limited discrepancy search space
complete outputs follows state search space represented x
x input sequence discrepancy set view state x equivalent
input output state x h x initial state function simply returns x
corresponds original output recurrent classifier successor function
state x returns set states form x
additional discrepancy way path lds search space starts
output generated recurrent classifier traverses sequence outputs
differ original number discrepancies given reasonably accurate h
expect high quality outputs generated relatively shallow depths
search space hence generated quickly


fidoppa fern tadepalli

figure illustrates limited discrepancy search space state consists
input x discrepancy set output produced running classifier
specified discrepancy set e h x root node empty discrepancy set nodes
level one contain discrepancy sets size one highlighted state corresponds
smallest depth state containing target output

appendix b hardness proof hc search consistency
theorem hc search consistency greedy search linear heuristic
cost functions np hard even restrict possible
heuristic functions uncover zero loss output
proof reduce minimum disagreement linear binary classifiers
proven np complete work hoffgen simon horn
one statement given input set n p dimensional vectors
x xn positive integer k decide whether
p dimensional real valued weight vector w w xi k
vectors
first sketch high level idea proof given instance minimum disagreement construct hc search consistency single structured
training example search space corresponding training example designed
single node n loss zero nodes loss
linear heuristic functions greedy search paths terminate n
generating set nodes outputs path search space designed
possible path initial node n corresponds selecting k fewer
vectors denote traversing path set nodes
generated hence must scored c say n includes feature vectors corresponding
along negation feature vectors define
n assigned zero vector cost node weight vector
order achieve zero loss given path consideration must weight
vector wc wc x x n construction equivalent
wc x x possible found solution minimum
disagreement since k remaining details construct
space setting heuristic weights generate paths corresponding
possible way paths end n completeness describe
construction
search node space n tuple n
k one node types set x x
viewed indexing example xi effectively codes many instances
selected mistakes hence put finally encodes type
search node following meanings become clear
construction decision positive selection negative selection x positive
instance x negative instance search space constructed example xi
may clear example allow riding discrepancies provide opportunity recover search errors



fihc search learning framework search structured prediction

figure example search space x x x k greedy paths
terminate zero loss node n path selects one instance
include mistake set

considered order choice made whether count mistake put
choice made decision nodes form
indicating decision made example already
examples selected decision node k two children
respectively correspond selecting xi mistake set
later features assigned nodes allow heuristic make
selection desired
selection node single node child particular positive selection node
positive instance node x child negative selection nodes
negative instance node x child instance node
effectively implements process putting xi become clear
feature vectors described arriving positive negative instance
node consideration xi complete must move decision next
example xi thus positive instance node x single child decision node


fidoppa fern tadepalli

negative instance node single child decision node
noting number mistakes incremented negative nodes
final details search space structure ensure k mistakes
allowed force search paths terminate n particular decision
node k know mistakes allowed hence
decisions allowed thus node form path n
goes positive instance nodes x n x reflects none
xi xn figure shows example search space construction
given search space polynomial size since k n one verify
set k fewer instances path root n goes
negative instance nodes instances positive instance nodes
instances possible path goes positive negative
instance node instance k negative nodes thus direct
correspondence paths mistake sets
describe assign features node way allows
heuristic function select path effectively construct set node
u feature vector u x b component x p dimensional feature vector
correspond one xi component n dimensional vector
si implement selection instances finally b binary value
equal non instance nodes positive negative instance nodes
mapping nodes feature vectors follows decision node
zeros except b positive selection node zeros except si
b negative selection nodes similar except si positive instance
node x feature vector xi negative instance nodes x
feature vector xi finally feature vector n zeros
key idea note heuristic function effectively select positive
negative selection node setting weight si positive negative respectively
particular set negative selection nodes visited hence negative instance nodes
correspond first k fewer negative weight values component feature
vector thus heuristic select set negative nodes wants go
k path three types nodes encountered
cost function must rank first control nodes decision selection nodes
b next positive instance nodes feature
vector xi k negative instance nodes feature vectors xi
cost function easily rank n higher control nodes setting weight
b negative heuristic weights x component allows n
ranked highest solution original minimum disagreement
solution disagreement easy see
solution hc search consistency selecting heuristic spans
proper set

references
agarwal roth learnability bipartite ranking functions proceedings
international conference learning theory colt pp


fihc search learning framework search structured prediction

batra yadollahpour p guzman rivera shakhnarovich g diverse mbest solutions markov random fields proceedings european conference
computer vision eccv pp
boyan j moore w learning evaluation functions improve optimization local search journal machine learning jmlr
brill e transformation error driven learning natural language processing case study part speech tagging computational linguistics

chang k w samdani r roth constrained latent variable model
coreference resolution proceedings conference empirical methods
natural language processing emnlp pp
chang w ratinov l roth structured learning constrained
conditional machine learning journal mlj
chen c kolmogorov v zhu metaxas lampert c h computing
probable modes graphical model proceedings international
conference artificial intelligence statistics aistats
chen fern todorovic multi object tracking via constrained sequential labeling appear proceedings ieee conference computer vision
pattern recognition cvpr
collins discriminative reranking natural language parsing proceedings
international conference machine learning icml pp
collins ranking named entity extraction boosting
voted perceptron acl
crammer k dekel keshet j shalev shwartz singer online passiveaggressive journal machine learning jmlr
daume iii h practical structured learning techniques natural language
processing ph thesis university southern california los angeles ca
daumeiii h marcu learning search optimization approximate large
margin methods structured prediction icml
dietterich g hild h bakiri g comparison id backpropagation
english text speech mapping machine learning journal mlj
domke j structured learning via logistic regression proceedings advances
neural information processing systems nips pp
doppa j r fern tadepalli p output space search structured prediction proceedings international conference machine learning icml
doppa j r fern tadepalli p hc search learning heuristics cost
functions structured prediction proceedings aaai conference artificial
intelligence aaai
doppa j r fern tadepalli p structured prediction via output space
search journal machine learning jmlr


fidoppa fern tadepalli

doppa j r yu j c fern tadepalli p b hc search multi label
prediction empirical study appear proceedings aaai conference
artificial intelligence aaai
doppa j r yu j tadepalli p getoor l chance constrained programs
link prediction proceedings nips workshop analyzing networks
learning graphs
doppa j r yu j tadepalli p getoor l learning link
prediction chance constraints proceedings european conference
machine learning ecml pp
felzenszwalb p f mcallester generalized architecture journal
artificial intelligence jair
fern speedup learning encyclopedia machine learning pp
fern yoon w givan r approximate policy iteration policy
language bias solving relational markov decision processes journal artificial
intelligence jair
goldberg elhadad efficient easy first non directional
dependency parsing proceedings human language technologies conference
north american chapter association computational linguistic hltnaacl pp
grubb bagnell speedboost anytime prediction uniform nearoptimality journal machine learning proceedings track
hal daume iii langford j marcu search structured prediction
machine learning journal mlj
harvey w ginsberg l limited discrepancy search proceedings
international joint conference artificial intelligence ijcai pp
hazan urtasun r efficient learning structured predictors general
graphical corr abs
hoffgen k u simon h u horn k v robust trainability single
neurons journal computer system sciences
huang l fayong guo structured perceptron inexact search
proceedings human language technology conference north american
chapter association computational linguistics hlt naacl pp

jiang j teichert daume iii h eisner j learned prioritization
trading accuracy speed proceedings advances neural information
processing nips
kaariainen lower bounds reductions atomic learning workshop
keshet j shalev shwartz singer chazan phoneme alignment
discriminative learning proceedings annual conference international
speech communication association interspeech pp


fihc search learning framework search structured prediction

khardon r learning take actions machine learning journal mlj

kulesza taskar b determinantal point processes machine learning
foundations trends machine learning
lafferty j mccallum pereira f conditional random fields probabilistic
segmenting labeling sequence data proceedings international
conference machine learning icml pp
lam doppa j r hu x todorovic dietterich reft daly
learning detect basal tubules nematocysts sem images iccv workshop
computer vision accelerated biosciences cvab ieee
li q ji h huang l joint event extraction via structured prediction
global features proceedings st annual meeting association
computational linguistics acl pp
mcallester hazan keshet j direct loss minimization structured
prediction proceedings advances neural information processing systems
nips pp
meshi sontag jaakkola globerson learning efficiently
approximate inference via dual losses proceedings international conference
machine learning icml pp
mohan chen z weinberger k q web search ranking initialized
gradient boosted regression trees journal machine learning proceedings track
nivre j deterministic incremental dependency parsing computational linguistics
park ramanan n best maximal decoders part proccedings
ieee international conference computer vision iccv pp
payet n todorovic sledge sequential labeling image edges
boundary detection international journal computer vision ijcv

qian x jiang x zhang q huang x wu l sparse higher order conditional random fields improved sequence labeling proceedings international
conference machine learning icml
read j pfahringer b holmes g frank e classifier chains multi label
classification machine learning
ross bagnell efficient reductions imitation learning journal
machine learning proceedings track
ross gordon g j bagnell reduction imitation learning
structured prediction regret online learning journal machine learning
proceedings track


fidoppa fern tadepalli

roth tau yih w integer linear programming inference conditional
random fields proceedings international conference machine learning
icml pp
samdani r roth efficient decomposed learning structured prediction
proceedings international conference machine learning icml
sen p namata g bilgic getoor l gallagher b eliassi rad collective classification network data ai magazine
sontag meshi jaakkola globerson data means less inference
pseudo max structured learning proceedings advances neural
information processing systems nips pp
stoyanov v eisner j easy first coreference resolution proceedings
international conference computational linguistics coling pp
stoyanov v ropson eisner j empirical risk minimization graphical
model parameters given approximate inference decoding model structure
proceedings international conference artificial intelligence statistics
aistats pp
sutton c mccallum piecewise training structured prediction
machine learning journal mlj
syed u schapire r reduction apprenticeship learning classification proceedings advances neural information processing systems nips
pp
taskar b guestrin c koller max margin markov networks proceedings
advances neural information processing systems nips
tsochantaridis hofmann joachims altun support vector machine learning interdependent structured output spaces proceedings
international conference machine learning icml
tsochantaridis joachims hofmann altun large margin methods
structured interdependent output variables journal machine learning
jmlr
vogel j schiele b semantic modeling natural scenes content
image retrieval international journal computer vision ijcv
weiss structured prediction cascades code http code google com p
structured cascades
weiss sapp b taskar b sidestepping intractable inference structured
ensemble cascades proceedings advances neural information processing
systems nips pp
weiss taskar b structured prediction cascades journal machine
learning proceedings track
wick l rohanimanesh k bellare k culotta mccallum samplerank training factor graphs atomic gradients proceedings international
conference machine learning icml


fihc search learning framework search structured prediction

wick l rohanimanesh k singh mccallum training factor graphs
reinforcement learning efficient map inference proceedings advances
neural information processing systems nips pp
xu fern yoon learning linear ranking functions beam search
application journal machine learning

xu fern yoon w b learning linear ranking functions beam
search application journal machine learning jmlr

xu fern yoon w iterative learning weighted rule sets
greedy search proceedings international conference automated
systems icaps pp
xu z weinberger k chapelle greedy miser learning test time
budgets proceedings international conference machine learning icml
ye n lee w chieu h l wu conditional random fields
high order features sequence labeling proceedings advances neural
information processing systems nips pp
yu h huang l mi h zhao k max violation perceptron forced
decoding scalable mt training proceedings empirical methods natural
language processing emnlp pp
zhang w dietterich g reinforcement learning job shop
scheduling proceedings international joint conference artificial intelligence
ijcai pp




