Journal Artificial Intelligence Research 50 (2014)

Submitted 10/13; published 05/14

Decision-Theoretic Model Assistance
Alan Fern

AFERN @ EECS . OREGONSTATE . EDU

School EECS, Oregon State University, Corvallis, USA

Sriraam Natarajan

NATARASR @ INDIANA . EDU

SoIC, Indiana University, Bloomington, USA

Kshitij Judah

JUDAHK @ EECS . OREGONSTATE . EDU

School EECS, Oregon State University, Corvallis, USA

Prasad Tadepalli

TADEPALL @ EECS . OREGONSTATE . EDU

School EECS, Oregon State University, Corvallis, USA

Abstract
growing interest intelligent assistants variety applications sorting
email helping people disabilities daily chores. paper, formulate
problem intelligent assistance decision-theoretic framework, present theoretical
empirical results. first introduce class POMDPs called hidden-goal MDPs (HGMDPs),
formalizes problem interactively assisting agent whose goal hidden whose
actions observable. spite restricted nature, show optimal action selection
HGMDPs PSPACE-complete even deterministic dynamics. introduce
restricted model called helper action MDPs (HAMDPs), sufficient modeling many
real-world problems. show classes HAMDPs efficient algorithms possible.
interestingly, general HAMDPs show simple myopic policy achieves near
optimal regret, compared oracle assistant knows agents goal. introduce
sophisticated versions policy general case HGMDPs combine
novel approach quickly learning agent assisted. evaluate approach
two game-like computer environments human subjects perform tasks, real-world
domain providing assistance folder navigation computer desktop environment.
results show three domains framework results assistant substantially reduces
user effort modest computation.

1. Introduction
Personalized AI systems interactively assist human users received significant attention recent years (Yorke-Smith, Saadati, Myers, & Morley, 2012; Lieberman, 2009; Myers, Berry,
Blythe, Conleyn, Gervasio, McGuinness, Morley, Pfeffer, Pollack, & Tambe, 2007). However,
overarching formal framework interactive assistance captures different systems
provides theoretical foundation largely missing. paper address lacuna introducing general framework decision-theoretic assistance, analyzing problem complexity
different assumptons, proposing different heuristic solutions, evaluating effectiveness.
consider model assistant observes goal-oriented agent must select assistive
actions order best help agent achieve goals. real applications, requires
assistant able handle uncertainty environment agent, reason varying
c
2014
AI Access Foundation. rights reserved.

fiF ERN , NATARAJAN , J UDAH , & TADEPALLI

action costs, handle unforeseen situations, adapt agent time. consider
decision-theoretic model, based partially observable Markov decision processes (POMDPs),
naturally handles features, providing formal basis designing intelligent assistants.
first contribution work formulate problem selecting assistive actions
class partially observable Markov decision processes (POMDPs) called Hidden Goal MDPs
(HGMDPs), jointly models application environment along agents policy
hidden goals. key feature approach explicitly reasons environment
agent, provides potential flexibility assisting ways unforeseen developer
new situations encountered. Thus, developer need design hand-coded assistive
policy preconceived application scenario. Instead, using framework, burden
developer provide model application domain agent, alternatively
mechanism learning one models experience. framework uses
models attempt compute, situation, whether assistance could beneficial
assistive action select.
second contribution work analyze properties formulation. Despite
restricted nature HGMDPs, complexity determining HGMDP finite-horizon
policy given value PSPACE-complete even deterministic environments. motivates
restricted model called Helper Action MDP (HAMDP), assistant executes helper
action step. agent obliged accept helper action helpful goal
receives reward bonus (or cost reduction) so. Otherwise, agent continue
preferred action without reward penalty assistant. show classes
problem complete PSPACE NP. show class HAMDPs
deterministic agents polynomial time algorithms minimizing expected worstcase regret relative oracle assistant knows goal agent. Further, show
optimal worst case regret characterized graph theoretic property called tree rank
corresponding all-goals policy tree computed linear time.
principle, given HGMDP, one could apply POMDP solver order arrive optimal
assistant policy. Unfortunately, relatively poor scalability POMDP solvers often force us
utilize approximate/heuristic solutions. particularly true assistant continually
learning updated models agent and/or environment, results sequence
accurate HGMDPs, needs solved. third contribution work set
myopic action selection mecahnisms approximate optimal policy. HAMDPs,
analyze myopic heuristic show regret upper bounded entropy
goal distribution HAMDPs. Furthermore give variant policy able
achieve worst-case expected regret logarithmic number goals without
prior knowledge goal distribution. describe two approaches based
combination explicit goal estimation, myopic heuristics, bounded search
generally applicable HGMDPs.
order approach useful, HGMDP must incorporate reasonably accurate model agent assisted. fourth contribution work describe novel
model-based bootstrapping mechanism quickly learning agent policy, important
usability assistant early lifetime. main idea assume agent
close rational decision-theoretic sense, motivates defining prior agent policies
places higher probability policies closer optimal. prior combination

72

fiA ECISION -T HEORETIC ODEL SSISTANCE

Bayesian updates allows agent model learned quickly rationality assumption
approximately satisfied.
final contribution work evaluate framework three domains. First
consider two game-like computer environments human subjects. results domains
show assistants resulting framework substantially reduce amount work
performed human subjects. consider realistic domain, folder navigator
(Bao, Herlocker, & Dietterich, 2006) Task Tracer project. domain, user navigates
directory structure searching particular location open save file, unknown
assistant. job assistant predict users destination folder take actions
provide short cuts reach it. results show generic assistant framework compares
favorably hand-coded solution Bao et al.
remainder paper organized follows. next section, introduce formal problem setup terms HGMDPs, followed analysis computational complexity
guarantees myopic heuristics case HAMDPs. Next, present approximate solution approach general HGMDPs based goal estimation online action selection. Finally
give empirical evaluation approach three domains conclude discussion
related future work.

2. Hidden Goal Markov Decision Processes
Throughout paper refer entity attempting assist agent
assisting entity assistant. consider episodic problem setting beginning
episode agent begins world state selects goal finite set possible
goals. goal set, example, might contain possible dishes agent might interested
cooking, possible destination folders agent may possibly navigate to. Importantly, assistant fully observe world state agents actions, cannot observe
goal agent. model interaction agent assistant sequential
agent assistant alternate turns, taking single action per turn (possibly noop).1 episode
ends either agents assistants action leads goal state. immediate reward
accumulated action episode. total reward episode equal
sum rewards obtained episode. Note available actions agent
assistant need may varying rewards. Since assistant agent
share objective, rewards viewed perspective agent. objective
assistant behave way maximizes expected total reward episode.
formally, model interaction via Hidden Goal Markov Decision Processes
(HGMDPs). HGMDP MDP goal user observed rest
environment completely observed. HGMDP tuple hS, G, A, A0 , T, R, , , IG
set states, G finite set possible agent goals, set agent actions,
A0 set assistant actions. Typically A0 include noop action, allows assistant
decide provide assistance particular decision epoch. transition function
(s, g, a, s0 ) probability transition state s0 taking action A0
agents goal g. R reward function maps G (A A0 ) real values.
1. consider strictly alternating turn model simplicity. However, straightforward use model
capture interactions strictly alternating, e.g. allowing assistant agent take multiple actions
row.

73

fiF ERN , NATARAJAN , J UDAH , & TADEPALLI

agents policy maps G distributions need optimal sense.
(IG ) initial state (goal) distribution.
assistant policy HGMDP defines distribution actions given sequence
preceding observations, i.e., sequence state action pairs. important assistant policy depends history rather current state since entire history potentially
provide evidence goal agent, may necessary selecting appropriate
action. must define objective function used evaluate value particular assistant policy. consider finite-horizon episodic problem setting, HGMDP
episode begins drawing initial state goal g IG . process alternates
agent assistant executing actions (including noops) environment
horizon terminal state reached. agent assumed select actions according .
many domains, terminal goal state reached within horizon, though general, goals
arbitrary impact reward function. reward episode equal sum
rewards actions executed agent assistant episode. objective
assistant reason HGMDP observed state-action history order select
actions maximize expected (or worst-case) total reward episode.
proceeding worth reviewing assumptions formulation potential implications.
Partial Observability: definition HGMDP similar definition
Partially Observable Markov Decision Process (POMDP). fact, HGMDP special
case POMDP unobserved part state space consists single component corresponds goal user. simplicity assumed
world state fully observable. choice fundamental framework one
imagine relatively straightforward extensions techniques model environment
partially observable MDP (POMDP) world states fully observable.
Section 3, shed light hardness HGMDPs describe specialized
heuristic solutions performance guarantees.
Agent Policy: assumed agent modeled memoryless/reactive
policy gives distribution actions conditioned current world state
goal. assumption fundamental framework one extend
include complex models user, example, include hierarchical goal
structures. extension explored previously (Natarajan, Tadepalli, & Fern,
2007).
Sequential Interaction: assumed simplicity interaction model
assistant agent involves interleaved, sequential actions rather parallel actions.
This, example, precludes assistant taking actions parallel agent.
parallel assistive actions useful many cases, many domains sequential
actions norm. especially motivated domains intelligent desktop
assistants help store retrieve files, filter spam, sort email, etc., smart homes
open doors, switch appliances, on. Many opportunities assistance
domains sequential variety. many cases, tasks appear require parallel
activity often formulated set threads thread sequential hence

74

fiA ECISION -T HEORETIC ODEL SSISTANCE

formulated separate assistant. Extending framework handle general parallel
assistance interesting future direction.
Goal-Dependent Transitions: dependence reward policy goal allows
model capture agents desires behavior goal. dependence
goal less intuitive many cases dependence used
model dynamics environment. However, allow goal dependence
generality modeling. example, convenient model basic communication
actions agent changing aspects state, result actions often
goal dependent.
two main obstacles solving problem intelligent assistance framework.
First, many scenarios, initially HGMDP directly disposal since lack
accurate information agent policy and/or goal distribution IG . often due
fact assistant deployed variety initially unknown agents. Rather, assistant find environment given possible set goals. described
Section 5, approach difficulty learn approximate HGMDP estimating agent
policy goal distribution IG . Furthermore, describe bootstrapping mechanism learning approximations quickly. second obstacle solving HGMDP
generally high computational complexity solving HGMDPs. deal issue Section 4
considers various approximate techniques efficiently solving HGMDPs.
mentioned possible provide assistance using simpler domain-specific
engineered solutions. particular, domains consider later could solved using several less
expensive solutions require machinery HGMDPs. fact, one domains,
compare model existing supervised learning method. goal work
provide domain-independent framework potentially encapsulate several assistant
systems hope gives rise robust understanding methodology building
systems much less human effort future.

3. Theoretical Analysis
section, analyze hardness solving HGMDPs, show despite
special case POMDPs, hard. motivates new model called Helper-Action
MDP (HAMDP) restricted amenable approximate solutions.
introduce myopic heuristic solve HAMDPs analyze performance.
analyze special cases HAMDPs permit efficient solutions.
3.1 Complexity Results Hidden Goal MDPs
Given knowledge agents goal g HGMDP, assistants problem reduces solving
MDP assistant actions. MDP transition function captures state change due
assistant action ensuing state change due agent action selected according
given g. Likewise reward function transition captures reward due assistant action
ensuing agent action conditioned g. optimal policy MDP corresponds
optimal assistant policy g. However, since real assistant often uncertainty
agents goal, unlikely optimal performance achieved.

75

fiF ERN , NATARAJAN , J UDAH , & TADEPALLI

view HGMDP collection |G| MDPs share state space,
assistant placed one MDPs beginning episode, cannot observe
one. MDP result fixing goal component HGMDP definition one
goals. collection easily modeled restricted type partially observable MDP
(POMDP) state space G. component completely observable, G component unobservable changes beginning episode (according IG )
remains constant throughout episode. Furthermore, POMDP transition provides observations agent action, gives direct evidence unchanging G component.
perspective HGMDPs appear significant restriction general POMDPs. However,
first result shows despite restriction worst-case complexity reduced even
deterministic dynamics.
Given HGMDP , horizon = O(|M |), reward target r , short-term reward
maximization problem asks whether exists history-dependent assistant policy achieves
expected finite horizon reward least r . general POMDPs problem PSPACEcomplete (Papadimitriou & Tsitsiklis, 1987; Mundhenk, 2001), POMDPs deterministic
dynamics, NP-complete (Littman, 1996). However, following result.
Theorem 1. Short-term reward maximization HGMDPs deterministic dynamics PSPACEcomplete.
Proof. Membership PSPACE follows fact HGMDP polynomially encoded POMDP policy existence PSPACE. show PSPACE-hardness,
reduce PSPACE-complete problem TQBF (truth quantified Boolean formula) problem
existence history-dependent assistant policy expected reward r.
Let quantified Boolean formula form x1 x2 x3 . . . xn {C1 (x1 , . . . , xn ) . . .
Cm (x1 , . . . , xn )}, Ci disjunctive clause. us, goal gi clause,
agent chooses goal uniformly randomly set goals formed hides
assistant. states consist pairs form (v, i), v {0, 1} current value
goal clause, next variable set. actions assistant set existentially
quantified variables. agent simulates setting universally quantified variables choosing
actions set {0, 1} equal probability. episode terminates variables
set, assistant gets reward 1 value clause 1 end reward 0
otherwise.
Note assistant get useful informtion goal termination
episode. satisfiable, assistant policy leads reward 1
goals choices agent actions, hence expected value 1 goal
distribution. not, least one goals satisfied setting universal
quantifiers, leading expected value < 1. Hence TQBF problem reduces deciding
HGMDP policy expected reward 1.
result shows POMDP encoded HGMDP deterministic dynamics, stochastic dynamics POMDP captured via stochastic agent policy
HGMDP. However, HGMDPs resulting PSPACE-hardness reduction quite pathological compared likely arise realistic assistant domains. importantly,
agents actions provide practically information agents goal end episode,
late exploit knowledge. suggests search restricted classes
HGMDPs allow efficient solutions performance guarantees.
76

fiA ECISION -T HEORETIC ODEL SSISTANCE

3.2 Helper Action MDPs
motivation Helper Action MDPs (HAMDPs) place restrictions agent assistant avoid following three complexities arise general HGMDPs: 1) agent
behave arbitrarily poorly left unassisted agent actions may provide significant evidence goal; 2) agent free effectively ignore assistants help
exploit results assistive action, even would beneficial; 3) assistant
actions possibility negatively impacting agent compared assistant.
HAMDPs address first issue assuming agent competent (approximately)
maximizing reward without assistant. second third issues addressed assuming agent always detect exploit helpful actions assistant actions
never hurt agent even unhelpful.
Informally, HAMDP provides assistant helper action agents actions. Whenever helper action h executed directly corresponding agent action a,
agent receives bonus reward 1. However, agent accept helper action h
(by taking a) hence receive bonus, action agent considers good
achieving goal without assistant. Thus, primary objective assistant HAMDP
maximize number helper actions get accepted agent.2
simple, model captures much essence assistive domains assistant
actions cause minimal harm agent able detect accept good assistance
arises.
HAMDP HGMDP hS, G, A, A0 , T, R, , , IG following constraints:
agent assistant actions sets = {a1 , . . . , } A0 = {h1 , . . . , hn },
ai corresponding helper action hi .
state space = W (W A0 ), W set world states. States W A0
encode current world state previous assistant action.
reward function R 0 assistant actions. agent actions reward zero
unless agent selects action ai state (s, hi ) gives reward 1. is,
agent receives bonus 1 whenever takes action directly corresponding helper
action.
assistant always acts states W , taking hi deterministically
transitions (s, hi ).
agent always acts states A0 , resulting states according transition function depend hi , i.e. ((s, hi ), g, ai , s0 ) = 0 (s, g, ai , s0 )
transition function 0 .
Finally, agent policy, let (s, g) function returns set actions P (s, g)
distribution actions. view (s, g) set actions agent
2. Note assumed bonus rewards 1 simplicity. results easily extended
non-uniform positive bonus rewards. particular main result concerning bounding regret
myopic policy, analogous result simply includes constant factor equal maximum possible reward bonus.
exceptions Theorems 7 8, currently analogous results non-uniform reward
bonuses.

77

fiF ERN , NATARAJAN , J UDAH , & TADEPALLI

considers acceptable (or equally good) state goal g. agent policy always selects
ai helper action hi whenever ai acceptable. is, ((s, hi ), g) = ai whenever
ai (s, g). Otherwise agent draws action P (s, g).
HAMDP, primary impact assistant action influence reward following
agent action. notice HAMDPs rewards inherent underlying
environment. Rather, rewards bonuses received whenever agent accepts helper
action. could defined model include environmental reward addition
helper bonuses, unnecessarily complicates model (as following hardness result shows).
Instead, assume inherent environmental reward already captured agent policy
via (s, g), considered contain actions approximately optimize reward.
example, HAMDP model captures doorman domain, desktop domain experiments. doorman domain, helper actions correspond opening doors
agent, reduces cost navigating one room another. desktop domain, helper actions correspond offering shortcuts users destination folders. Importantly
opening incorrect door offering incorrect shortcut increase (physical) cost
agent assistant all, key property HAMDPs.
Despite apparent simplification HAMDPs HGMDPs, turns somewhat
surprisingly worst case computational complexity reduced.
Theorem 2. Short-term reward maximization HAMDPs PSPACE-complete.
Proof. Membership PSPACE follows easily since HAMDPs specialization HGMDPs.
proof PSPACE-hardness identical Theorem 1 except here, instead
agents actions, stochastic environment models universal quantifiers. agent accepts
actions last one sets variable suggested assistant.
assistants actions, environment chooses value universally quantified variable equal
probability. last action accepted agent goal clause evaluates 1, otherwise not.
history-dependent policy whose expected reward greater equal number
existential variables quantified Boolean formula satisfiable.
Unlike case HGMDPs, stochastic dynamics essential PSPACE-hardness
shown later section. Despite negative result, following sections demonstrate utility HAMDP restriction giving performance guarantees simple policies improved
complexity results. far, analogous results general HGMDPs.
3.3 Myopic Heuristic Analysis
HAMDPs closely related sequential prediction framework Littlestone (1988).
framework, round learner shown new instance predicts binary label.
prediction incorrect, mistake made, learner given true label. realizable
setting, true labels determined hypothesis target class. optimal prediction
algorithm minimizes upper bound number mistakes possible hypotheses.
view helper action HAMDP prediction user action. Maximizing bonus
reward HAMDP equivalent minimizing number mistakes sequential prediction.
Unlike sequential prediction, predictions (actions) need binary. sequential
prediction sequence states arbitrarily chosen, assume generated
78

fiA ECISION -T HEORETIC ODEL SSISTANCE

Markov process. spite differences, results sequential prediction
adapted HAMDPs albeit using different terminology. However, derive results
first principles consistency.
Given assistant policy 0 , regret particular episode extra reward oracle
assistant knowledge goal would achieve 0 . HAMDPs oracle assistant
always achieve reward equal finite horizon m, always select helper action
accepted agent. Thus, regret execution 0 HAMDP equal
number helper actions accepted agent, call mispredictions.
know optimizing regret PSPACE-hard thus focus bounding
expected worst-case regret assistant. introduce first myopic heuristic
show able achieve regret bounds logarithmic number goals.
Coarsened Posterior Heuristic. Intuitively, myopic heuristic select action
highest probability accepted respect coarsened version posterior
distribution goals. myopic policy state given history H based consistent goal
set C(H), set goals non-zero probability respect history H.
straightforward maintain C(H) observation (observations include world state
agents actions). myopic policy defined as:
(s, H) = arg max IG (C(H) G(s, a))


G(s, a) = {g | (s, g)} set goals agent considers
acceptable action state s. expression IG (C(H) G(s, a)) viewed probability
mass G(s, a) coarsened goal posterior assigns goals outside C(H) probability
zero otherwise weighs proportional prior.
Theorem 3. HAMDP expected regret coarsened posterior heuristic bounded
entropy goal distribution H(IG ).
Proof. main idea proof show misprediction myopic policy (i.e.
selected helper action accepted agent) uncertainty goal reduced
constant factor, allow us bound total number mispredictions trajectory.
Consider misprediction step coarsened posterior heuristic selects helper action hi
state given history H, agent accept action instead selects 6= ai .
definition myopic policy know IG (C(H) G(s, ai )) IG (C(H) G(s, )), since
otherwise assistant would chosen hi . fact argue IG (C(H 0 ))
IG (C(H))/2 H 0 history misprediction. is, probability mass
IG consistent goal set misprediction less half consistent goal set
misprediction. show consider two cases: 1) IG (C(H) G(s, ai )) <
IG (C(H))/2, 2) IG (C(H) G(s, ai )) IG (C(H))/2. first case, immediately get
IG (C(H) G(s, )) < IG (C(H))/2. Combining fact C(H 0 ) C(H)
G(s, ) get desired result IG (C(H 0 )) IG (C(H))/2. second case, note
C(H 0 ) C(H) (G(s, ) G(s, ai ))
C(H) (C(H) G(s, ai ))
Combining assumption second case immediately implies IG (C(H 0 ))
IG (C(H))/2.
79

fiF ERN , NATARAJAN , J UDAH , & TADEPALLI

shows misprediction made histories H H 0 IG (C(H 0 ))
IG (C(H))/2. implies episode, n mispredictions resulting history Hn ,
IG (C(Hn )) 2n . consider arbitrary episode true goal g. know
IG (g) lower bound IG (C(Hn )), implies IG (g) 2n equivalently n
log(IG (g)). Thus episode goal g maximum number mistakes bounded
log(IG (g)). Using fact get thePexpected number mispredictions episode
respect IG bounded g IG (g) log(IG (g)) = H(IG ), completes
proof.
Since H(IG ) log(|G|), result implies HAMDPs expected regret myopic
policy logarithmic number goals. Furthermore, uncertainty
goal decreases (decreasing H(IG )) regret bound improves get regret 0 IG
puts mass single goal. turns logarithmic bound asymptotically tight
worst case.
Theorem 4. exists HAMDP assistant policy expected regret least
log(|G|)/2.
Proof. Consider deterministic HAMDP environment structured binary tree
depth log(|G|), leaf corresponds one |G| goals. considering uniform
goal distribution easy verify node tree equal chance true
goal left right sub-tree episode. Thus, policy 0.5 chance
committing misprediction step episode. Since episode length log(|G|),
expected regret episode policy log(|G|)/2.
Resolving gap myopic policy bound regret lower bound open problem.
3.3.1 PPROXIMATE G OAL ISTRIBUTIONS .
0 instead true underlying
Suppose assistant uses approximate goal distribution IG
goal distribution IG computing myopic policy. is, assistant selects actions
0 (C(H) G(s, a)), refer myopic policy relative 0 .
maximize IG
G
0 instead bounded terms KullbackLeibler (KL) diverextra regret using IG
G
0 ), zero 0
gence (Kullback & Leibler, 1951) distributions KL(IG k IG
G
equals IG .

Theorem 5. HAMDP goal distribution IG , expected regret myopic policy
0 bounded H(I ) + KL(I k 0 ).
respect distribution IG
G
G
G
Proof. proof similar Theorem 3, except since myopic policy respect
0 rather , derive that, episode, maximum number mispredictions n
IG
G

80

fiA ECISION -T HEORETIC ODEL SSISTANCE

0 (g)). Using fact, average number mispredictions given by:
bounded log(IG

P

g

P

g

P

g

=

1
)=
IG (g) log( 0
IG (g)


1
IG (g) log( 0
) + log(IG (g)) log(IG (g)) =
IG (g)
X
IG (g)
IG (g) log(IG (g))
IG (g) log( 0
)
IG (g)
g
0
H(IG ) + KL(IG k IG
).

Note random variable X distribution P finite domain size N ,
KL(P k U ) = log(N ) H(P ), U uniform distribution. Thus, consequence
Theorem 5 myopic policy respect uniform goal distribution expected regret
bounded log(|G|) HAMDP, showing logarithmic regret achieved without
knowledge IG . strengthened hold worst case regret.
Theorem 6. HAMDP, worst case hence expected regret myopic policy
respect uniform goal distribution bounded log(|G|).
Proof. proof Theorem 5 shows number mispredictions episode bounded
0 ). case 0 = 1/|G| shows worst case regret bound log(|G|).
log(IG
G
immediately implies expected regret bound uniform myopic policy bounded
log(|G|).
3.4 Deterministic Agent Policies
consider several special cases HAMDPs. First, restrict agents policy
deterministic goal, i.e. (s, g) single action state-goal pair (s, g).
Theorem 7. myopic policy achieves optimal expected reward HAMDPs deterministic agent policies.
proof given Appendix. sometimes desirable minimize worst possible regret
compared oracle assistant knows agents goal. show below, captured
graph-theoretic notion tree rank generalizes rank decision trees (Ehrenfeucht &
Haussler, 1989).
Definition 1. rank rooted tree rank root node. node leaf node
rank(node) = 0, else node least two distinct children c1 c2 equal highest ranks
among children, rank(node) = 1+ rank(c1 ). Otherwise rank(node) = rank highest
ranked child.
optimal trajectory tree (OTT) HAMDP deterministic environments tree
nodes represent states HAMDP reached prefixes optimal action sequences
different goals starting initial state.3 node tree represents state set
3. multiple initial states, build OTT initial state. rank would maximum
ranks trees.

81

fiF ERN , NATARAJAN , J UDAH , & TADEPALLI

goals optimal path initial state. Since agent policy deterministic,
one trajectory per goal tree. Hence size optimal trajectory tree
bounded number goals times maximum length trajectory,
size state space deterministic domains. following Lemma follows induction
depth optimal trajectory tree. proof Appendix.
Lemma 1. minimum worst-case regret policy HAMDP deterministic environments deterministic agent policies equal tree rank optimal trajectory tree.
leads following.
Theorem 8. agent policy deterministic, problem minimizing maximum regret
HAMDPs deterministic environments P.
Proof. first construct optimal trajectory tree. compute rank linear time
simultaneously computing optimal minimax policy using recursive definition tree rank.
result follows Lemma 1.
3.5 Bounded Branching Factor Policies
assumption deterministic agent policy may restrictive many domains.
consider agent policies may constant number possible actions (s, g)
state-goal pair defined below.
Definition 2. branching factor HAMDP largest number possible actions (s, g)
agent state goal assistants action.
Doorman domain Section 6.1 branching factor 2 since two optimal
actions reach goal state.
Theorem 9. Minimizing worst-case regret finite horizon HAMDPS deterministic environments constant branching factor k NP-complete.
proof appendix. show minimizing expected regret bounded
k NP-hard. conjecture problem NP, question remains open.

4. Solving Practical HGMDPs
Although HAMDPs offer theoretically elegant framework, requirements practical assistant
systems easily satisfed assumptions. section, consider general
problem solving HGMDPs offer practical heuristic solutions inspired
theoretical analysis.
principle could use general purpose POMDP solver solve HGMDPs.
POMDP solvers based point-based methods search-based methods become efficient years, still inefficient used interactive setting,
parameters POMDP continually updated. Moreover, analysis previous
section suggests, simple myopic heuristics based knowledge goal distribution
optimal policies given goals appear promising yield respectable performance. reason,
adopt approach based Bayesian goal estimation followed heuristic action selection,
evaluate three different domains. Below, first give overview solution algorithm
describe components detail.
82

fiA ECISION -T HEORETIC ODEL SSISTANCE

4.1 Overview
section, assume given HGMDP , delegating problem learning
Section 5. Let Ot = o1 , ..., ot observation sequence observed assistant
beginning current trajectory time t. observation tuple world state
previously selected action (by either assistant agent). Given Ot goal compute
assistant action whose value (close to) optimal.
motivate approach, useful consider special characteristics HGMDP.
importantly, belief state corresponds distribution agents goal. Since agent
assumed goal directed, observed agent actions provide substantial evidence
goal might might be. fact, even assistant nothing, agents goals
often rapidly revealed analyzing relevance agents initial actions possible
goals. cases, suggests state/goal estimation problem HGMDP may
solved quite effectively observing agents actions relate various possible goals,
rather requiring assistant select actions explicitly purpose information gathering
agents goals. words, cases, expect purely (or nearly) myopic
action selection strategies, avoid reasoning information gathering, effective.
Reasoning information gathering one key complexities involved solving POMDPs
compared MDPs. leverage intuitive properties HGMDP gain tractability
limiting completely avoiding reasoning. course, shown PSPACE-hardness
results, goals always rapidly revealed non-myopic reasoning essential.
note cases, assistant pure information-gathering actions
disposal, e.g. asking agent question. consider actions experiments,
believe actions handled naturally framework incorporating
small amount look-ahead search.
motivation, assistant architecture, depicted Figure 1, alternates
goal estimation action selection follows:
1. observing agents next action, update goal distribution based HGMDP
model.
2. Based updated distribution evaluate effectiveness assistant actions (including
noop) building sparse-sampling look-ahead tree bounded depth (perhaps depth
one), leaves evaluated via myopic heuristic.
key element architecture computation myopic heuristics. top
heuristic, optionally obtain non-myopic behavior via search building look-ahead sparsesampling tree. experiments show search improve performance small margin
significant computational cost. note idea utilizing myopic heuristics select
actions POMDPs new, see example (Cassandra, 1998; Geffner & Bonet, 1998),
similar methods used previously success applications computer bridge
(Ginsberg, 1999). main contribution show approach particularly well
suited setting evaluate efficiently computable heuristics specifically designed
solving HGMDPs. describe goal estimation action selection operations
detail.

83

fiF ERN , NATARAJAN , J UDAH , & TADEPALLI

Goal Estimation

P(G)

Action Selection

Assistant
Ot



Wt
Environment

User
Ut

Figure 1: Depiction assistant architecture. agent hidden goal selects actions
Ut cause environment change world state Wt , typically moving closer
goal. assistant (upper rectangle) able observe world state along
observations generated environment, setting contain user/agent
actions along world state. assistant divided two components. First,
goal estimation component computes posterior agent goals P (G) given observations. Second, action selection component uses goal distribution compute
best assistive action via combination bounded search myopic heuristic
computation. best action might noop cases none assistive
actions higher utility user.

84

fiA ECISION -T HEORETIC ODEL SSISTANCE

4.2 Goal Estimation
Given HGMDP agent policy initial goal distribution IG , objective maintain
posterior goal distribution P (g|Ot ), gives probability agent goal g
conditioned observation sequence Ot . Note since assumed assistant cannot
affect agents goal, observations related agents actions relevant posterior.
Given agent policy , straightforward incrementally update posterior P (g|Ot ) upon
agents actions.
beginning episode initialize goal distribution P (g|O0 ) IG . timestep
episode, Ot involve agent action, leave distribution unchanged. Otherwise, agent selects action state s, update posterior according P (g|Ot ) =
(1/Z) P (g|Ot1 ) (a|s, g), Z normalizing constant. is, distribution adjusted place weight goals likely cause agent execute action s.
accuracy goal estimation relies well policy learned assistant reflects
true agent policy. described Section 5.2, use model-based bootstrapping approach
estimating update estimate end episode. Provided agent close
optimal, experimental domains, approach lead rapid goal estimation, even early
lifetime assistant.
assumed simplicity actions agent directly observable.
domains, natural assume state world observable, rather
actual action identities. cases, observing agent transitioning s0
use MDP transition function marginalize possible agent actions yielding update,
P (g|Ot ) = (1/Z) P (g|Ot1 )

X

(a|s, g)T (s, a, s0 ).

aA

4.3 Action Selection
Given HGMDP distribution goals P (g|Ot ), address problem
selecting assistive action. mechanisms utilize combination bounded look-ahead search
myopic heuristic computations. increasing amount look-ahead search actions
returned closer optimal cost computation. Fortunately, many HGMDPs,
useful assistant actions computed relatively little search. first describe several
myopic heuristics used either greedy action selection combination search.
Next, review utilize sparse sampling obtain non-myopic action selection.
4.3.1 ACTION ELECTION H EURISTICS
explain action selection procedure, introduce idea assistant MDP relative
goal g , denote (g). MDP (g) identical except
change initial goal distribution P (G0 = g) = 1. is, goal always fixed g
episode. Since hidden component state space goal, fixing goal
(g) makes state fully observable, yielding MDP. episode (g) evolves drawing
initial world state selecting assistant actions goal g achieved. Note
state transition assistant action a0 state result successive state transitions, first
due assistant action due ensuing agent action, selected based
agent policy goal g. optimal policy (g) gives optimal assistive action assuming
85

fiF ERN , NATARAJAN , J UDAH , & TADEPALLI

agent acting achieve goal g. denote Q-function (g) Qg (s, a),
expected cost executing action state following optimal policy.
consider second heuristic action selection, accounts non-uniform rewards true goal posterior, unlike coarsened posterior heuristic introduced Section 3.3.
simply expected Q-value action assistant MDPs, called QMDP
method Cassandra (1998). heuristic value assistant action state given observations
Ot
X
Qg (s, a) P (g|Ot ).
H(s, a, Ot ) =
g

Intuitively H(s, a, Ot ) measures utility taking action assumption
goal ambiguity resolved one step. Thus, heuristic value information-gathering
utility action. Rather, heuristic favor assistant actions make progress toward goals
high posterior probability. goal posterior highly ambiguous often lead
assistant prefer noop, least hurt progress toward goal. Note
heuristic, well others below, used evaluate utility state s, rather
state-action pair, maximizing actions maxa H(s, a, Ot ).
primary computational complexity computing H solve assistant MDPs
goal order obtain Q-functions. Technically, since transition functions assistant
MDPs depend approximate agent policy , must re-solve MDP updating
estimate end episode (see Section 5.2 policy learning). However, using incremental dynamic programming methods prioritized sweeping (Moore & Atkeson, 1993)
alleviate much computational cost. particular, deploying assistant solve
MDP offline based default agent policy given Boltzmann bootstrapping distribution describe Section 5.2. deployment, prioritized sweeping used incrementally
update Q-values based learned refinements make .
practical exactly solve assistant MDPs, may resort various approximations. consider two approximations experiments. One replace users policy
used computing assistant MDP fixed default user policy, eliminating need
compute assistant MDP every step. denote approximation Hd . Another approximation uses simulation technique policy rollout (Bertsekas & Tsitsiklis, 1996) approximate
Qg (s, a) expression H. done first simulating effect taking action
state using estimate expected cost agent achieve g resulting
state. is, approximate Qg (s, a) assuming assistant select single
initial action followed agent actions. formally, let Cn (, s, g) function simulates n trajectories achieving goal state averaging trajectory costs.

H(s, a, Ot ) except replace Qg (s, a) expectation
P heuristic H0r identical
0
s0 (s, a, ) C(, , g). combine heuristics, using fixed default
user policy policy rollouts, denote Hd,r .
4.3.2 PARSE AMPLING
heuristics somewhat myopic sense take account
potentially persistent ambiguity agents goal consider use information
gathering actions resolve ambiguity. cases beneficial consider nonmyopic reasoning, one combine heuristics shallow search belief space

86

fiA ECISION -T HEORETIC ODEL SSISTANCE

assistant MDP. purpose utilize depth bounded sparse sampling trees (Kearns,
Mansour, & Ng, 1999) compute approximation Q-function given belief state
(st , Ot ), denoted Qd (st , a, Ot ). Given particular belief state, assistant select
action maximizes Qd . Note convenience represent belief state pair
current state st observation history Ot . lossless representation belief state since
posterior goal distribution computed exactly Ot goal hidden
portion POMDP state.
base case Q0 (st , a, Ot ) equal one myopic heuristics described above.
Increasing depth result looking ahead state transitions evaluating one
heuristics. looking ahead possible track potential changes belief state
taking certain actions determine whether changes belief would beneficial
respect providing better assistance. Sparse sampling look-ahead approximately
computing:
Qd (s, a, O) = E[R(s, g, a) + V d1 (s0 , O0 )]




V (s, O) = max Q (s, a, O)


(1)
(2)

g random variable distributed according goal posterior P (g|O) (s0 , O0 )
random variable represents belief state taking action belief state (s, O).
particular, s0 world state arrived O0 simply observation sequence extended
observation obtained state transition. first term expectation
represents immediate reward assistant action goal g.
Sparse sampling approximates expectation averaging set b samples successor belief states. sparse-sampling pseudo-code presented Table 4.3.2. Given input belief
state (s, O), assistant action a, heuristic H, depth bound d, sampling width b algorithm returns (an approximation of) Qd (s, a, O). First, depth bound equal zero heuristic
value returned. Otherwise b samples observations resulting taking action belief state
(s, O) generated. observations form oi = (s0i , ai , si ), s0i state
resulting taking action state s, ai ensuing agent action selected s0i based goal
drawn goal posterior, si result taking action ai state s0i . observation oi
corresponds new belief state (si , [O; oi ]) [O; oi ] simply concatenation oi O.
code recursively computes value belief states maximizing Qd
actions averages results.
b become large, sparse sampling produce arbitrarily close approximation
true Q-function belief state MDP. computational complexity sparse sampling linear
b exponential d. Thus depth must kept small real-time operation.

5. Learning HGMDPs
section, tackle problem learning HGMDP interacting environment assist agent. assume set goals G known agent. primary
role learning acquire agents policy goal distribution. assumption natural
situations assistant applied many times environment, possibly
different agents. example, desktop environment, environment MDP corresponds
description various desktop functionalities, remains fixed across users. one
87

fiF ERN , NATARAJAN , J UDAH , & TADEPALLI

Given: heuristic function H, belief state (s, O), action a, depth bound d, sampling width b
Return: approximation Qd (s, a, O) value belief state (s, O)
1. = 0 return H(s, a, O)
2. Sample set b observations {o1 , . . . , ob } resulting taking action
belief state (s, O) follows:
(a) Sample s0i environment MDP transition function (s, a, )
(b) Sample goal gi P (gi |O)
(c) Sample agent action ai agent policy (|s0i , gi )
(d) oi = (s0i , ai , si ), si sample environment MDP transition
function (s0i , ai , )
3. oi = (s0i , gi , ai , si ) compute Vi = maxa0 Qd1 (si , a0 , [O; oi ])
P
4. Return Qd (s, a, O) = 1b R(s, gi , a) + Vi

Table 1: Pseudo-code Sparse Sampling HGMDP
provided description MDP typically straightforward learn model
primary cost longer warming period assistant.
Relaxing assumption provided set possible goals problematic
current framework. saw Section 4, solution methods depend knowing set
goals clear learn observations, since goals, unlike states
actions, directly observable assistant. Extending framework assistant
automatically infer set possible user goals, allow user define goals,
interesting future direction. note, however, often possible designer enumerate
set user goals deployment perhaps complete, allows useful assistance
provided.
5.1 Maximum Likelihood Estimates
straightforward estimate goal distribution G0 agent policy simply observing
agents actions, possibly assisted, compute empirical estimates relevant
quantities. done storing goal achieved end episode along
set world state-action pairs observed agent episode. estimate IG
based observed frequency goal (usually Laplace correction avoid
extreme values probabilities). Likewise, estimate (a|s, g) simply frequency
action taken agent state goal g. limit
maximum likelihood estimates converge correct values true HGMDP, practice
convergence slow. slow convergence lead poor performance early stages
assistants lifetime. alleviate problem propose approach bootstrapping
learning agent policy .
88

fiA ECISION -T HEORETIC ODEL SSISTANCE

5.2 Model-Based Bootstrapping
leverage environment MDP model order bootstrap learning agent policy.
particular, assume agent near optimal sense that, particular goal
world state, likely select actions close optimal. unrealistic
many application domains might benefit intelligent assistants. particular,
many tasks, conceptually simple humans, yet quite tedious, e.g., navigating
directory structure computer desktop. Performing optimally tasks difficult
humans.
Given near rationality assumption, initialize estimate agents policy
prior biased toward optimal agent actions. consider environment MDP
assistant actions removed solve Q-function Q(a, s, g) using MDP planning
techniques. Q-function gives expected cost executing agent action world state
acting optimally achieve goal g using agent actions. world without assistant,
rational agent would always select actions maximize Q-function state goal.
Furthermore, close-to-rational agent would prefer actions achieve higher Q-values highly
suboptimal actions. first define Boltzmann distribution, used define
prior,
1
(a|s, g) =
exp(K Q(a, s, g))
(3)
Z(s, g)
Z(s, g) normalizing constant, K temperature constant. Using larger values
K skews distribution heavily toward optimal actions. Given definition, prior
distribution (|w, g) taken Dirichlet parameters (1 , . . . , |A| ), =
0 (ai |s, g). 0 parameter controls strength prior. Intuitively 0
thought number pseudo-actions represented prior, representing
number pseudo-actions involved agent action ai . Since Dirichlet conjugate
multinomial distribution, form (|s, g), easy update posterior
(|s, g) observation. One take mode mean posterior point
estimate agent policy used define HGMDP.
experiments, found prior provides good initial proxy actual agent
policy, allowing assistant immediately useful. updating posterior tunes
assistant better peculiarities given agent. example, many cases
multiple optimal actions posterior come reflect systematic bias among equally
good actions agent has. Computationally main obstacle approach computing
Q-function, needs done given application domain since environment
MDP constant. Using dynamic programming accomplished polynomial time
number states goals. practical, number alternatives exist including
use factored MDP algorithms (Boutilier et al., 1999), approximate solution methods (Boutilier
et al., 1999; Guestrin et al., 2003), developing domain specific solutions.
Finally, work, utilize uninformative prior goal distribution. interesting
future direction would bootstrap goal distribution estimate based observations
population agents.

89

fiF ERN , NATARAJAN , J UDAH , & TADEPALLI

6. Experimental Results
section, present results conducting user studies simulations three domains:
two game-like environments folder predictor domain intelligent desktop assistant.
user studies two game-like domains, episode, users assistants actions
recorded. user studies performed using 12 human subjects (graduate students CS
department Oregon State University) single session. ratio cost achieving
goal assistants help optimal cost without assistant calculated averaged
multiple trials user. present similar results simulations well. third
domain folder predictor domain, simulated user used one heuristics
generate top 3 recommended folders user. present number clicks required
average user reach desired folder. Two three domains, namely, doorman
domain folder predictor domain, fall category HAMDPs since assistive
actions merely viewed helper actions agent ignore. kitchen domain
hand needs slightly general formulation since agent assistant
strictly alternate, assistants actions cannot ignored agent.
6.1 Doorman Domain
doorman domain, agent set possible goals collect wood, food
gold. grid cells blocked. cell four doors agent open
door move next cell (see Figure 2). door closes one time-step time
one door open. goal assistant help user reach goal faster opening
correct doors.
state tuple hs, di, stands agents cell door open.
total number states 245 (49 squares 5 possibilities door). actions agent
open door move 4 directions pickup whatever cell,
total 9 actions. assistant open doors perform noop (5 actions. agents
assistants actions strictly alternate domain, satisfying definition HAMDPs.
reward 1 (or cost 1) user open door reward assistants
action. trial ends agent picks desired object. Note included
noop action assistant, domain action never selected, since cost opening
wrong door noop same, potential benefit selecting noop.
experiment, evaluated two heuristics: one fixed user policy default
policy HGMDP creation (Hd ) avoiding need repeated computation HGMDP
every step second use policy rollout calculate Q-values (Hr ).
trial, system chooses goal one two heuristics random. user shown
goal tries achieve it, always starting center square. every users action,
assistant opens door nothing. user may pass door open different door.
user achieves goal, trial ends, new one begins. assistant uses
users trajectory update agents policy.
results user studies doorman domain presented Tabe 2. first two
rows give cumulative results user study actions selected greedily according Hr
Hd respectively. Rather reporting negtive rewards, table shows total number

90

fiA ECISION -T HEORETIC ODEL SSISTANCE

Figure 2: Doorman Domain. agents goal fetch resource. grid cells separated
doors must opened passing through.

actions trials across users without assistant N, total number actions
assistant U, average percentage savings (1-(U/N)) trials users.4
seen, methods reduce number actions 50%. Note
assistant selects among four doors random would reduce number actions
25% comparison. omniscient assistant knows users goal reduces number
actions 78%. 100% first door always opened user.
experiments, count users first action, number actions reduces 65%.5
observed Hr appears slight edge Hd . One possible reason
using Hd , re-solve MDP updating user policy, Hr always
using updated user policy. Thus, rollout reasoning accurate model user.
Heuristic
Hr
Hd
Hr
= 2, b = 1
= 2, b = 2
= 3, b = 1
= 3, b = 2

Total
Actions
N
750
882
1550
1337
1304
1167
1113

User
Actions
U
339
435
751
570
521
467
422

Fractional Savings
1 (U/N )
0.55 0.055
0.51 0.05
0.543 0.17
0.588 0.17
0.597 0.17
0.6 0.15
0.623 0.15

Time
per
action (in secs
0.0562
0.0021
0.031
0.097
0.35
0.384
2.61

Table 2: Results experiments Doorman Domain. first two rows table present
results user studies rest table presents results simulation.

4. gives pessimistic estimate usefulness assistant assuming optimal user measure utility
normalized optimal utility without aid assistant.
5. Note first action requirement easily aviodable. simply equivalent switch indicate
user ready move grid. replace requirement explicitly adding button
interface start new episode.

91

fiF ERN , NATARAJAN , J UDAH , & TADEPALLI

Another interesting observation individual differences among users.
users always prefer fixed path goal regardless assistants actions. users
flexible. survey conducted end experiment, learned one
features users liked system tolerant choice suboptimal paths.
data reveals system able reduce costs approximately 50% even
users chose suboptimal trajectories.
conducted experiments using sparse sampling non-zero depths. considered
depths = 1 = 2 using sampling widths b = 1 b = 2. leaves sparse
sampling tree evaluated using Hr simply applies rollout user policy. Hence sparse
sampling = 0 b = 1, would correspond heuristic Hr . experiments,
conduct user studies, due high cost effort required humans studies,
simulated human users choosing actions according policies learned observed
actions previous user study. results presented last 5 rows Table 2. Note
absolute numbers actions user studies simulations comparable
based different numbers trajectories. human users tested fewer trajectories
minimize fatigue. see sparse sampling increased average run time (last column)
order magnitude, able produce reduction average cost user. result
surprising hindsight, simulated experiments, sparse sampling able sample
exact user policy (i.e. sampling learned policy, used
simulations). results suggest small amount non-myopic reasoning
positive benefit substantial computation cost. Note, however, bulk benefit
realized assistant obtained without reasoning, showing myopic heuristics
well-suited domain.
6.2 Kitchen Domain
kitchen domain, goals agent cook various dishes. 2 shelves
3 ingredients each. dish recipe, represented partially ordered plan. ingredients
fetched order, mixed heated. shelves doors
must opened fetching ingredients one door open time.
8 different recipes. state consists location ingredients
(bowl/shelf/table), mixing state temperature state ingredient (if bowl)
door open. state includes action history preserve ordering
plans recipes. users actions are: open doors, fetch ingredients, pour
bowl, mix, heat bake contents bowl, replace ingredient back shelf.
assistant perform user actions except pouring ingredients replacing ingredient
back shelf. restricted assistant pouring ingredients irreversible action. reward non-pour actions -1. Experiments conducted 12 human subjects
computer science graduate students. Unlike doorman domain, allowed
assistant take multiple consecutive actions. turn switches user assistant
executes noop action.
domain large state space hence possible update user policy
every trajectory. Hence, two heuristics compare use default user policy.
second heuristic addition uses policy rollout compare actions. words, compare
Hd Hd,r . results user studies shown top part Table 3. doorman

92

fiA ECISION -T HEORETIC ODEL SSISTANCE

Figure 3: kitchen domain. user prepare dishes described recipes
right. assistants actions shown bottom frame.

domain, total number agent actions without assistant, percentage reduction due assistant presented. number user actions summed 12 users
cumulative results presented. observed Hd,r performs better Hd .
observed experiments Hd,r technique aggressive choosing non-noop
actions Hd , would wait goal distribution highly skewed toward particular
goal.
Heuristic
Hd,r
Hd
Hd,r
= 2, b = 1
= 2, b = 2
= 3, b = 1
= 3, b = 2

Total
Actions
N
3188
3175
6498
6532
6477
6536
6585

User
Actions
U
1175
1458
2332
2427
2293
2458
2408

Fractional Savings
1 (U/N )
0.6361 0.15
0.5371 0.10
0.6379 0.14
0.6277 0.14
0.646 0.14
0.6263 0.15
0.645 0.14

Time
per
action (secs)
0.013
0.013
0.013
0.054
0.190
0.170
0.995

Table 3: Results experiments Kitchen Domain. first two rows table present
results user studies last 5 rows present results simulation.

compared use sparse sampling heuristic simulated user trajectories
domain well (see last 5 rows Table 3). Again, absolute numbers actions
user studies comparable simuations due different numbers trajectories
case. Since sparse sampling considers larger number trajectories methods,
policies learned sometimes better learned heuristics, although took
time execute. However, significant difference solution quality
rollouts sparse sampling simulations, showing myopic heuristics performing
93

fiF ERN , NATARAJAN , J UDAH , & TADEPALLI

well sparse sampling much less computation. Sparse sampling higher depths requires
order magnitude computation time compared rollout.
6.3 Folder Predictor
section, present evaluation framework real-world domain. part
Task Tracer project (Dragunov, Dietterich, Johnsrude, McLaughlin, Li, & Herlocker, 2005),
researchers developed file location system called folder predictor (Bao et al., 2006). idea
behind folder predictor learning users file access patterns, assistant
help user file accesses predicting folder file accessed
saved.
setting, goal folder predictor minimize number clicks user.
predictor would choose top three folders would minimize cost append
UI (shown ovals Figure 4). Also, user taken first recommended folder.
users target folder first recommended folder, user would reach folder zero clicks
reach second third recommended folder one click. user either choose one
recommendations navigate windows folder hierarchy recommendations
relevant.

Figure 4: Folder predictor (Bao et al., 2006).
Bao et al. considered problem supervised learning problem implemented costsensitive algorithm predictions cost number clicks user (Bao
et al., 2006). But, algorithm take account response user
predictions. instance, user chooses ignore recommended folders navigates
folder hierarchy, make re-predictions. due fact model
one-time prediction consider user responses. Also, algorithm considers
restricted set previously accessed folders ancestors possible destinations.
precludes handling possibility user accessing new folder.
decision-theoretic model naturally handles case re-predictions changing recommendations response user actions. first step, used data collected
user interface used model make predictions. use users response predic94

fiA ECISION -T HEORETIC ODEL SSISTANCE

tions make predictions. Also, handle possibility new folder, consider
folders folder hierarchies prediction. used mixture density obtain
probability distribution folders.
P (f ) = 0 P0 (f ) + (1 0 )Pl (f )
P0 probability according Bao et.als algorithm (2006), Pl uniform probability distribution set folders 0 ratio number times previously accessed
folder accessed total number folder accesses.
idea behind using density function early stages task, user
accessing new folders later stages user access folders particular task
hierarchy. Hence number folder accesses increases value 0 increases would
eventually converge 1, hence resulting distribution would converge P0 . data set
consists collection requests open file (Open) save file (saveAs), ordered time.
request contains information as, type request (open saveAs), current task,
destination folder, etc. data set consists total 810 open/saveAs requests. folder
hierarchy consists 226 folders.
state space consists 4 parts: current folder user accessing three
recommendations two unordered. would correspond state space size
226 225 224
2 . action user either choose recommended folder select
different folder. action assistant corresponds choosing top 3 folders
action space size 225 224
2 . reward case negative number user
clicks. domain, assistant users actions strictly alternate assistant revises
predictions every user action. prior distribution initialized using rewards computed
model developed Bao et al. (2006).
applied decision theoretic model data set. request, assistant would
make prediction using Hd,r heuristic (which uses default user policy rollout
method) user simulated. user would accept recommendation shortens
path goal, otherwise would act according optimal policy. user
considered close optimal, unrealistic real world. compare results,
used model developed Bao et al. data set present results Table 4.
Restricted folder set
Folders

One-time Prediction
1.3724
1.319

Repredictions
1.34
1.2344

Table 4: Results experiments folder predictor domain. numbers indicate average number clicks required agent reach his/her correct folder. entry
top left hand cell performance current Task Tracer, one
bottom right hand cell performance decision-theoretic assistant.

table shows average cost folder navigation 4 different cases: Bao et.als original
algorithm, algorithm modified include mixture distributions model without
mixture distributions. seen model use mixture distributions
least user cost navigation hence effective. Bao et. al shown
95

fiF ERN , NATARAJAN , J UDAH , & TADEPALLI

algorithm performs significantly better windows default prediction average
2.6 clicks per folder navigation. improvement attributed two modifications
mentioned earlier. First, use re-predictions model natural decisiontheoretic framework model makes one-time prediction hence cannot make use
users response recommendations. Secondly, considering folders hierarchy
prediction including possibility user accessing new folder found useful.
observed either modifications yields lower cost original algorithm,
combining two changes significantly effective.

7. Discussion Related Work
work inspired growing interest success building useful software assistants
(Yorke-Smith et al., 2012; Lieberman, 2009; Myers et al., 2007). effort focused
building desktop assistants help tasks calendar scheduling (Refanidis, Alexiadis, & Yorke-Smith, 2011), email filtering (Cohen, Carvalho, & Mitchell, 2004), on-line diagnostics (Skaanning, Jensen, & Kjaerulff, 2000), travel planning (Ambite, Barish, Knoblock,
Muslea, Oh, & Minton, 2002). tasks typically requires designing software system
around specialized technologies algorithms. example, email filtering typically posed
supervised learning problem (Cohen et al., 2004), travel planning combines information gathering
search constraint propagation (Ambite et al., 2002), printer diagnostics formulated
Bayesian network inference (Skaanning et al., 2000). approaches focus socially assistive robots setting robot designed aid human agents achieving goals (Johnson,
Cuijpers, Juol, Torta, Simonov, Frisiello, Bazzani, Yan, Weber, Wermter, et al., 2013). Unfortunately plethora systems approaches lacks overarching conceptual framework,
makes difficult build others work. paper, argue decision-theoretic
approach provides common framework allows design systems respond
novel situations flexible manner reducing need pre-programmed behaviors. formulate general version assistantship problem involves inferring users goals taking
actions minimize expected costs.
Earlier work learning apprentice systems focused learning users observation (Mahadevan, Mitchell, Mostow, Steinberg, & Tadepalli, 1993; Mitchell, Caruana, Freitag,
J.McDermott, & Zabowski, 1994). work closely related learning demonstration
programming demonstration (Johnson, 2014; Konidaris, Kuindersma, Grupen, & Barto, 2012;
Atkeson & Schaal, 1997; Cypher, 1993; Lau, Wolfman, Domingos, & Weld, 2003). emphasis
systems provide interface computer system unobtrusively observe
human user task learn itself. human acts user teacher.
performance system measured quickly system learns imitate user,
i.e., supervised learning setting. Note imitation assistance two different things
general. expect secretaries learn us, typically expected replace
us. setting, assistants goal reduce expected cost users problem solving.
user assistant capable exactly set actions, assistants actions cost
nothing compared users, makes sense assistant try completely replace
human. Even case, assistantship framework different learning demonstration
still requires assistant infer users goal actions trying achieve
it. Moreover, assistant might learn solve goal reasoning action set

96

fiA ECISION -T HEORETIC ODEL SSISTANCE

rather shown examples user. general, however, action
set user assistant may different, supervised learning appropriate.
example, case folder predictor. system needs decide set folders
present user, user needs decide choose. awkward
impossible formulate problem supervised learning programming demonstration.
Taking decision-theoretic view helps us approach assistantship problem principled
manner taking account uncertainty users goals costs taking different
actions. assistant chooses action whose expected cost lowest. framework naturally
prevents assistant taking actions (other noop) assistive action
expected reduce overall cost user. Rather learning user behave,
framework assistant learns users policy. similar secretary learns
habits boss, much imitate her, help effective way. work
assumed user MDP small enough solved exactly given users goals.
assumption may always valid, makes sense cases learn user
behave. natural treat case users actions provide exploratory
guidance system (Clouse & Utgoff, 1992; Driessens, 2002). gives opportunity
system imitate user knows nothing better improve upon users policy
can.
personal assistant systems based POMDP models. However,
systems formulated domain-specific POMDPs solved offline. instance,
COACH system helped people suffering dementia giving appropriate prompts
needed daily activities (Boger, Poupart, Hoey, Boutilier, Fernie, & Mihailidis, 2005).
use plan graph keep track users progress estimate users responsiveness
determine best prompting strategy. distinct difference approach
single fixed goal washing hands, hidden variable user responsiveness
either low high. Rather, formulation goal random variable hidden
assistant. Since state-action space significantly smaller (1280 2 108 states
folder predictor domain), possible solve POMDP exactly. Given need
re-solve POMDP every user action, becomes prohibitively expensive. Yet another
difference length trajectory goal small case hence plan graph
would suffice capture user policy. model, restrict plan graph instead
solve user MDP bootstrap policy. mentioned learning user policy
future direction. work, even though start initial estimate user policy,
update every goal achieved. considered online learning user policy
reasonably good prior. note combination two frameworks (one modeling
users responsiveness modeling users goal) would useful, assistant
infers agent goals relevant hidden properties user, responsiveness.
Electric Elves, assistant takes many mundane responsibilities human
agent including rescheduling meeting appear user likely miss it.
domain-specific POMDP formulated solved offline using variety techniques. one
approach, since system monitors users short regular intervals, radical changes belief
states usually possible pruned search space (Varakantham, Maheswaran,
& Tambe, 2005). Neither exact approximate POMDP solvers feasible online setting,
POMDP changing learn user, must repeatedly solved.
either costly run (Boger et al., 2005), complex implement baseline, e.g., Electric
97

fiF ERN , NATARAJAN , J UDAH , & TADEPALLI

Elves (Varakantham et al., 2005). experiments demonstrate simple methods onestep look-ahead followed rollouts would work well many domains POMDPs
solved online. distinct related work (Doshi & Gmytrasiewicz, 2004), authors introduce
setting interactive POMDPs, agent models agents beliefs. Clearly,
general complex ordinary POMDPs. model simpler assumes
agent oblivious presence beliefs assistant. simplified model suffices
many domains, relaxing assumption without sacrificing tractability would interesting.
several dialogue systems proposed many based decisiontheoretic principles (Walker, 2000; Singh, Litman, Kearns, & Walker, 2002). instance,
NJFun system designed MDP provide assistance user interacting user
providing answer users questions. uses automatic speech recognizer (ASR)
interpret human dialogues uses dialogue policy choose best action (the response).
goals user could set standard queries locations restaurants, wineries,
shopping centers etc. state space would dialogue states, i.e., current state
dialogue user assistant (such greeting, choice state etc). observations
interpretations dialogues human ASR. NJFun system usefully
modeled HGMDP, goal assistant infer users query given observations provide appropriate response. initial assistant policy learned training
data, manner similar dialogue policy NJFun system.
work related on-line plan recognition naturally extended include
hierarchies hierarchical versions HMMs (Bui, Venkatesh, & West, 2002) PCFGs
(Pynadath & Wellman, 2000). Blaylock Allen describe statistical approach goal recognition
uses maximum likelihood estimates goal schemas parameters (Blaylock & Allen, 2004).
approaches notion cost reward. incorporating plan recognition
decision-theoretic context, obtain natural notion optimal assistance, namely maximizing
expected utility.
substantial research area user modeling. Horvitz et al. took Bayesian
approach model whether user needs assistance based actions attributes provided
assistance needed spreadsheet application (Horvitz et al., 1998). Hui Boutilier used
similar idea assistance text editing (Hui & Boutilier, 2006). use DBNs handcoded
parameters infer type user compute expected utility assisting user.
would interesting explore use ideas plan recognition (Charniak & Goldman, 2013;
Gal, Reddy, Shieber, Rubin, & Grosz, 2012; Chu, Song, Kautz, & Levinson, 2011) system
take account users intentions attitudes computing optimal policy
assistant.
Recently, methods proposed solving POMDPs called point based methods (Pineau, Gordon, & Thrun, 2003; Porta, Vlassis, Spaan, & Poupart, 2006; Kurniawati, Hsu,
& Lee, 2008; Shani, Pineau, & Kaplow, 2013). example method point based value
iteration (PBVI) (Pineau et al., 2003; Porta et al., 2006) takes set belief points B input
maintains set POMDP -vectors iteration. iteration produces new set vectors optimal belief point respect -vectors previous iteration.
approximation made PBVI compared value iteration guarantee
set -vectors optimal entire belief space. omitting -vectors, PBVI
maintains constant run time per iteration. Application efficient point based methods

98

fiA ECISION -T HEORETIC ODEL SSISTANCE

PBVI decision-theoretic assistance problem evaluation performance compared
policy rollout sparse sampling methods remains promising research direction.

8. Summary Future Work
introduced decision-theoretic framework assistant systems described HGMDP
appropriate model selecting assistive actions. computational complexity HGMDPs
motivated definition simpler model called HAMDP, allows efficient myopic heurstics
tractable special cases.
described approximate solution approach based iteratively estimating agents
goal selecting actions using myopic heuristics. evaluation using human subjects two
game-like domains show approach significantly help user. demonstrated
real world folder predictor decision-theoretic framework effective state
art techniques folder prediction.
One future direction consider complex domains assistant able series activities parallel agent. Another possible direction assume hierarchical goal
structure user goal estimation context. Recently, assistantship model
extended hierarchical relational settings (Natarajan et al., 2007) including parameterized
task hierarchies conditional relational influences prior knowledge assistant. prior
knowledge would relax assumption user MDP solved tractably. knowledge
compiled underlying Dynamic Bayesian network, Bayesian network inference algorithms used infer distribution users goals given sequence atomic actions.
parameters users policy estimated observing users actions.
framework naturally extended case environment partially observable agent and/or assistant. requires recognizing actions taken gather
information, e.g., opening fridge decide make based available. Incorporating sophisticated user modeling includes users forgetting goals, paying attention
important detail, and/or changing intentions would extremely important building
practical systems. assistive technology useful assistant quickly learn
new tasks expert users transfer knowledge novice users training.

Acknowledgements
material based upon work supported Defense Advanced Research Projects Agency
(DARPA), Department Interior, NBC, Acquisition Services Division, Contract No. NBCHD030010. opinions, findings, conclusions recommendations expressed
material authors necessarily reflect views DARPA. Alan
Fern Prasad Tadepalli gratefully acknowledge following grants: NSF IIS-0964705 ONR
N00014-11-1-0106. Sriraam Natarajan thanks Army Research Office grant number W911NF-13-10432 Young Investigator Program.

Appendix A. Proof Theorem 7
According theory POMDPs, optimal action POMDP maximizes sum
immediate expected reward value resulting belief state (of assistant) (Kaelbling,

99

fiF ERN , NATARAJAN , J UDAH , & TADEPALLI

Littman, & Cassandra, 1998). agent policy deterministic, initial goal distribution
IG history agent actions states H fully captures belief state agent. Let
V (IG , H) represent value current belief state. value function belief state
given following Bellman equation, H 0 stands history assistants
action hi agents action aj .
V (IG , H) = max E(R((s, hi ), g, aj )) + V (IG , H 0 )
hi

H0

(4)

Since one agents action (s, g), agent action aj , subsequent state s0
value depend hi . Hence best helper action h assistant given by:
h (IG , H) = arg max E(R((s, hi ), g, (s, g)))
hi
X
= arg max
IG (g)I(ai (s, g))
hi

gC(H)

= arg max IG (C(H) G(s, ai ))
hi

C(H) set goals consistent current history H, G(s, ai ) set goals
ai good state s. I(ai (s, g)) indicator function = 1 ai (s, g).
Note h exactly myopic policy. 2

Appendix B. Proof Lemma 1
worst-case regret pair (s, G) HAMDP given following Bellman equation.
assuming G set possible goals agent current state s. Regret(s, G) = 0
terminal state goals G satisfied s. Otherwise, Regret(s, G) = mini maxj6=i
{Regret(si , Gi ), 1 + Regret(sj , Gj ))} (si , Gi ) (sj , Gj ) Children((s, G)).
outer min due assistant picking helper action goal Gi maximize
reward inner max due agent either accepting it, picking different goal
minimize reward. proof induction. node trajectory tree represents state
set goals G state optimal path.
Basis: (s, G) leaf node, either terminal state goals G satisfied s. Hence
rank equals reward 0.
Inductive step: Suppose induction true children (s, G). consider two
cases.
Case 1. unique child (s, G) representing (s1 , G1 ) highest regret among
children. inductive hypothesis, rank((s1 , G1 )) = regret(s1 , G1 ). assistant chooses
helper action corresponds (s1 , G1 ), agent choose actions yield lower
regret worst case. Choosing helper action would increase regret, since
agent could choose a1 add 1 regret. have, regret(s, G)= regret(s1 , G1 ) =
rank((s1 , G1 )) = rank((s, G)).
Case 2. least two children (s1 , G1 ) (s2 , G2 ) (s, G) highest rank
among children. inductive hypothesis, rank((s1 , G1 )) = rank((s2 , G2 )) = regret(s1 , G1 )
= regret(s2 , G2 ). agent increase regret 1 choosing goal G2
assistant chooses G1 vice versa. Hence, regret(s, G)= 1+regret(s1 , G1 ) = 1+rank((s1 , G1 )) =
100

fiA ECISION -T HEORETIC ODEL SSISTANCE

rank((s, G)).
Hence cases, shown regret(s, G) rank((s, G)). 2

Appendix C. Proof Theorem 9
first show problem NP. build tree representation history-dependent policy
initial state. Every node tree represeted triple (s, i, G), state,
G set goals good path, index helper action chosen
policy node. root node corresponds possible initial state initial goal
set IG . children node tree represent possible successor nodes (sj , j, Gj ) reached
agents response hi , whether accepting hi executing ai executing actions.
children resulting ai called accepted, latter called rejected. Note
multiple children result action dynamics function agents
goal.
guess policy tree check maximum regret, i.e. maximum number
rejected children path root leaf, within bounds. verify optimal
policy tree polynomial size note number leaf nodes upper bounded |G|
maxg N (g), N (g) number leaf nodes generated goal g. estimate N (g),
start root navigate downwards. node contains g goal set,
accepted child contains g, child reached g. not,
misprediction k children reached. Hence, number nodes reached g
grows geometrically number mispredictions. Theorem 6, since
log |G| mispredictions path, N (g) k log2 |G| = k logk |G| log2 k = |G|log2 k . Hence
total number leaf nodes tree bounded |G|1+log k , total number nodes
tree bounded m|G|1+log k , number steps horizon. Since
polynomial problem parameters, problem NP.
show NP-hardness, reduce 3-SAT given problem. consider 3-literal clause
Ci propositional formula possible goal. rest proof identical
Theorem 1 except variables set assistant since universal quantifiers.
agent rejects setting last variable clause clause evaluates 0.
worst regret goal 0 iff 3-SAT problem satisfying assignment. 2

References
Ambite, J. L., Barish, G., Knoblock, C. A., Muslea, M., Oh, J., & Minton, S. (2002). Getting
there: Interactive planning agent execution optimizing travel. Proceedings
Fourteenth Conference Innovative Applications Artificial Intelligence, pp. 862
869.
Atkeson, C. G., & Schaal, S. (1997). Learning tasks single demonstration. Proceedings
IEEE International Conference Robotics Automation, pp. 17061712.
Bao, X., Herlocker, J. L., & Dietterich, T. G. (2006). Fewer clicks less frustration: reducing
cost reaching right folder. Proceedings Eleventh International Conference
Intelligent User Interfaces, pp. 178185.
Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientific.

101

fiF ERN , NATARAJAN , J UDAH , & TADEPALLI

Blaylock, N., & Allen, J. F. (2004). Statistical goal parameter recognition. Proceedings
Fourteenth International Conference Automated Planning Scheduling, pp. 297305.
Boger, J., Poupart, P., Hoey, J., Boutilier, C., Fernie, G., & Mihailidis, A. (2005). decisiontheoretic approach task assistance persons dementia. Proceedings Nineteenth International Joint Conference Artificial Intelligence, pp. 12931299.
Boutilier, C., Dean, T., & Hanks, S. (1999). Decision-theoretic planning: Structural assumptions
computational leverage. Journal Artificial Intelligence Research, 11, 194.
Bui, H., Venkatesh, S., & West, G. (2002). Policy recognition abstract hidden markov models.
Journal Artificial Intelligence Research, 17, 451499.
Cassandra, A. R. (1998). Exact approximate algorithms partially observable Markov decision processes. Ph.D. thesis, Brown University.
Charniak, E., & Goldman, R. (2013). Plan recognition stories life. CoRR, abs/1304.1497.
Chu, Y., Song, Y., Kautz, H., & Levinson, R. (2011). start thing
do? interactive activity recognition prompting. Proceedings Twenty-Fifth AAAI
Conference Workshop Artificial Intelligence Smarter Living, pp. 1521.
Clouse, J. A., & Utgoff, P. E. (1992). teaching method reinforcement learning. Proceedings
Ninth International Workshop Machine Learning, pp. 92110.
Cohen, W. W., Carvalho, V. R., & Mitchell, T. M. (2004). Learning classify email speech acts.
Proceedings Conference Empirical Methods Natural Language Processing, pp.
309316.
Cypher, A. (1993). Watch Do: Programming Demonstration. MIT Press.
Doshi, P., & Gmytrasiewicz, P. (2004). particle filtering algorithm interactive POMDPs.
Proceedings Workshop Modeling Agents Observations, pp. 8793.
Dragunov, A. N., Dietterich, T. G., Johnsrude, K., McLaughlin, M., Li, L., & Herlocker, J. L. (2005).
Tasktracer: desktop environment support multi-tasking knowledge workers. Proceedings Tenth International Conference Intelligent User Interfaces, pp. 7582.
Driessens, K. (2002). Adding guidance relational reinforcement learning. Third FreiburgLeuven Workshop Machine Learning.
Ehrenfeucht, A., & Haussler, D. (1989). Learning decision trees random examples. Information Computation, 82(3), 231246.
Gal, Y., Reddy, S., Shieber, S., Rubin, A., & Grosz, B. (2012). Plan recognition exploratory
domains. Artificial Intelligence, 176(1), 22702290.
Geffner, H., & Bonet, B. (1998). Solving large POMDPs using real time dynamic programming.
Proceedings AAAI Fall Symposium POMPDs.
Ginsberg, M. L. (1999). GIB: Steps Toward Expert-Level Bridge-Playing Program. Proceedings Sixteenth International Joint Conference Artificial Intelligence, pp. 584589.
Guestrin, C., Koller, D., Parr, R., & Venkataraman, S. (2003). Efficient solution algorithms
factored MDPs. Journal Artificial Intelligence Research, 19, 399468.

102

fiA ECISION -T HEORETIC ODEL SSISTANCE

Horvitz, E., Breese, J., Heckerman, D., Hovel, D., & Rommelse, K. (1998). lumiere project:
Bayesian user modeling inferring goals needs software users. Proceedings
Fourteenth Conference Uncertainty Artificial Intelligence, pp. 256265.
Hui, B., & Boutilier, C. (2006). Whos asking help?: Bayesian approach intelligent assistance. Proceedings Eleventh International Conference Intelligent User Interfaces,
pp. 186193.
Johnson, D., Cuijpers, R., Juol, J., Torta, E., Simonov, M., Frisiello, A., Bazzani, M., Yan, W.,
Weber, C., Wermter, S., et al. (2013). Socially assistive robots: comprehensive approach
extending independent living. International Journal Social Robotics, 6(2), 195211.
Johnson, M. (2014). Inverse optimal control deterministic continuous-time nonlinear systems.
Ph.D. thesis, University Illinois Urbana-Champaign.
Kaelbling, L., Littman, M., & Cassandra, A. (1998). Planning acting partially bservable
stochastic domains. Artificial Intelligence, 101(1-2), 99134.
Kearns, M. J., Mansour, Y., & Ng, A. Y. (1999). sparse sampling algorithm near-optimal
planning large markov decision processes. Proceedings Sixteenth International
Joint Conference Artificial Intelligence, pp. 13241331.
Konidaris, G., Kuindersma, S., Grupen, R., & Barto, A. (2012). Robot learning demonstration
constructing skill trees. International Journal Robotics Research, 31(3), 360375.
Kullback, S., & Leibler, R. (1951). information sufficiency. Annals Mathematical
Statistics, 22(1), 7986.
Kurniawati, H., Hsu, D., & Lee, W. (2008). Sarsop: Efficient point-based POMDP planning
approximating optimally reachable belief spaces. Proceedings Robotics: Science
Systems IV.
Lau, T., Wolfman, S., Domingos, P., & Weld, D. (2003). Programming demonstration using
version space algebra. Machine Learning, 53(1-2), 111156.
Lieberman, H. (2009). User interface goals, AI opportunities. AI Magazine, 30(3), 1622.
Littlestone, N. (1988). Learning quickly irrelevant attributes abound: new linear-threshold
algorithm. Machine Learning, 2(4), 285318.
Littman, M. L. . (1996). Algorithms Sequential Decision Making. Ph.D. thesis, Brown University.
Mahadevan, S., Mitchell, T. M., Mostow, J., Steinberg, L. I., & Tadepalli, P. (1993). apprenticebased approach knowledge acquisition.. Artificial Intelligence, 64(1), 152.
Mitchell, T. M., Caruana, R., Freitag, D., J.McDermott, & Zabowski, D. (1994). Experience
learning personal assistant. Communications ACM, 37(7), 8091.
Moore, A. W., & Atkeson, C. G. (1993). Prioritized sweeping: Reinforcement learning less
data less time. Machine Learning, 13, 103130.
Mundhenk, M. (2001). complexity planning partially-observable Markov Decision
Processes. Ph.D. thesis, Friedrich-Schiller-Universitdt.

103

fiF ERN , NATARAJAN , J UDAH , & TADEPALLI

Myers, K., Berry, P., Blythe, J., Conleyn, K., Gervasio, M., McGuinness, D., Morley, D., Pfeffer,
A., Pollack, M., & Tambe, M. (2007). intelligent personal assistant task time
management. AI Magazine, Vol. 28, pp. 4761.
Natarajan, S., Tadepalli, P., & Fern, A. (2007). relational hierarchical model decision-theoretic
assistance. Proceedings Seventeenth Annual International Conference Inductive
Logic Programming, pp. 175190.
Papadimitriou, C., & Tsitsiklis, J. (1987). complexity Markov Decision Processes. Mathematics Operations Research, 12(3), 441450.
Pineau, J., Gordon, G., & Thrun, S. (2003). Point-based value iteration: anytime algorithm
POMDPs. Proceedings Eighteenth International Joint Conference Artificial
Intelligence, pp. 1025 1030.
Porta, J., Vlassis, N., Spaan, M., & Poupart, P. (2006). Point-based value iteration continuous
POMDPs. Journal Machine Learning Research, 7, 23292367.
Pynadath, D. V., & Wellman, M. P. (2000). Probabilistic state-dependent grammars plan recognition. Proceedings Sixteenth Conference Uncertainty Artificial Intelligence,
pp. 507514.
Refanidis, I., Alexiadis, A., & Yorke-Smith, N. (2011). Beyond calendar mashups: Intelligent calendaring. Proceedings Twenty-First International Conference Automated Planning
Scheduling System Demonstrations.
Shani, G., Pineau, J., & Kaplow, R. (2013). survey point-based POMDP solvers. Autonomous
Agents Multi-Agent Systems, 27(1), 151.
Singh, S. P., Litman, D. J., Kearns, M. J., & Walker, M. A. (2002). Optimizing dialogue management reinforcement learning: Experiments njfun system.. Journal Artificial
Intelligence Research, 16, 105133.
Skaanning, C., Jensen, F. V., & Kjaerulff, U. (2000). Printer troubleshooting using bayesian networks. Proceedings Thirteenth International Conference Industrial Engineering Applications Artificial Intelligence Expert Systems, pp. 367379.
Varakantham, P., Maheswaran, R. T., & Tambe, M. (2005). Exploiting belief bounds: practical
POMDPs personal assistant agents. Proceedings Fourth Internation Conference
Autonomous Agents Multiagent Systems, pp. 978985.
Walker, M. A. (2000). application reinforcement learning dialogue strategy selection
spoken dialogue system email. Journal Artificial Intelligence Research, 12, 387416.
Yorke-Smith, N., Saadati, S., Myers, K., & Morley, D. (2012). design proactive personal
agent task management. International Journal Artificial Intelligence Tools, 21(1), 90
119.

104


