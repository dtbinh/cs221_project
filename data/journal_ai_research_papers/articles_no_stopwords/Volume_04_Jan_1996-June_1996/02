Journal Artificial Intelligence Research 4 (1996) 37{59

Submitted 9/95; published 2/96

Logarithmic-Time Updates Queries
Probabilistic Networks
Arthur L. Delcher

Computer Science Department, Loyola College Maryland
Baltimore, MD 21210

Adam J. Grove

delcher@cs.loyola.edu

grove@research.nj.nec.com

NEC Research Institute
Princeton, NJ 08540

Simon Kasif

kasif@cs.jhu.edu

Judea Pearl

pearl@lanai.cs.ucla.edu

Department Computer Science, Johns Hopkins University
Baltimore, MD 21218
Department Computer Science, University California
Los Angeles, CA 90095

Abstract

Traditional databases commonly support ecient query update procedures
operate time sublinear size database. goal paper
take first step toward dynamic reasoning probabilistic databases comparable
eciency. propose dynamic data structure supports ecient algorithms
updating querying singly connected Bayesian networks. conventional algorithm,
new evidence absorbed time O(1) queries processed time O(N ), N
size network. propose algorithm which, preprocessing phase,
allows us answer queries time O(log N ) expense O(log N ) time per evidence
absorption. usefulness sub-linear processing time manifests applications
requiring (near) real-time response large probabilistic databases. brie discuss
potential application dynamic probabilistic reasoning computational biology.

1. Introduction
Probabilistic (Bayesian) networks increasingly popular modeling technique
used successfully numerous applications intelligent systems real-time planning navigation, model-based diagnosis, information retrieval, classification, Bayesian
forecasting, natural language processing, computer vision, medical informatics computational biology. Probabilistic networks allow user describe environment using
\probabilistic database" consists large number random variables, corresponding important parameter environment. random variables could
fact hidden may correspond unknown parameters (causes) uence
observable variables. Probabilistic networks quite general store information
probability failure particular component computer system, prob c 1996 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiDelcher, Grove, Kasif & Pearl

ability page computer cache requested near future, probability
document relevant particular query, probability amino-acid
subsequence protein chain folding alpha-helix conformation.
applications mind include networks dynamically maintained
keep track probabilistic model changing system. instance, consider task
automated detection power-plant failures. might repeat cycle consists
following sequence operations: First perform sensing operations. operations
cause updates performed specific variables probabilistic database. Based
evidence estimate (query) probability failure certain sites. precisely,
query probability distribution random variables measure probability
failure sites based evidence. Since plant requires constant monitoring,
must repeat cycle sense/evaluate frequent basis.
conventional (non-probabilistic) database tracking plant's state would
appropriate here, possible directly observe whether failure
occur. hand, probabilistic \database" based Bayesian network
useful operations|update query|can performed quickly.
real-time near real-time often necessary, question extremely
fast reasoning probabilistic networks important.
Traditional (non-probabilistic) databases support ecient query update procedures
often operate time sublinear size database (e.g., using binary search). goal paper take step toward systems perform
dynamic probabilistic reasoning (such probability event given set
observations) time sublinear size probabilistic network. Typically,
sublinear performance complex networks attained using parallelism. paper
relies preprocessing.
Specifically, describe new algorithms performing queries updates belief
networks form trees (causal trees, polytrees join trees). define two natural
database operations probabilistic networks.
1.

Update-Node

: Perform sensory input, modify evidence leaf node (single
variable) network absorb evidence network.

2.

Query-Node

: Obtain marginal probability distribution values
arbitrary node (single variable) network.

standard algorithms introduced Pearl (1988) perform Query-Node operation O(1) time although evidence absorption, i.e., Update-Node operation, takes
O(N ) time N size network. Alternatively, one assume
Update-Node operation takes O(1) time (by simply recording change) QueryNode operation takes O(N ) time (evaluating entire network).
paper describe approach perform queries updates O(log N )
time. significant systems since improve ability system
respond change encountered O(N ) time O(log N ). approach
based preprocessing network using form node absorption carefully structured
way create hierarchy abstractions network. Previous uses node absorption
techniques reported Peot Shachter (1991).
38

fiQueries & Updates Probabilistic Networks

note measuring complexity terms size network, N ,
overlook important factors. Suppose variable network domain
size k less. many purposes, k considered constant. Nevertheless,
algorithms consider slowdown power k, become
significant practice unless N large. Thus careful state slowdown
exists.
Section 2 considers case causal trees, i.e., singly connected networks
node one parent. standard algorithm (see Pearl, 1988) must use O(k2 N )
time either updates retrieval, although one operations done
O(1) time. discuss brie Section 2.1, straightforward variant
algorithm takes O(k2D) time queries updates, height
tree.
present algorithm takes O(k3 log N ) time updates O(k2 log N )
time queries causal tree. course represent tremendous speedup,
especially large networks. algorithm begins polynomial-time preprocessing
step (linear size network), constructing another data structure (which
probabilistic tree) supports fast queries updates. techniques use
motivated earlier algorithms dynamic arithmetic trees, involve \caching" sucient intermediate computations update phase querying relatively
easy. note, however, substantial interesting differences
algorithm probabilistic networks arithmetic trees. particular,
apparent later, computation probabilistic trees requires bottom-up top-down
processing, whereas arithmetic trees need former. Perhaps even interesting relevant probabilistic operations different algebraic structure
arithmetic operations (for instance, lack distributivity).
Bayesian trees many applications literature including classification.
instance, one popular methods classification Bayes classifier
makes independence assumption features used perform classification
(Duda & Hart, 1973; Rachlin, Kasif, Salzberg, & Aha, 1994). Probabilistic trees
used computer vision (Hel-Or & Werman, 1992; Chelberg, 1990), signal processing
(Wilsky, 1993), game playing (Delcher & Kasif, 1992), statistical mechanics (Berger
& Ye, 1990). Nevertheless, causal trees fairly limited modeling purposes. However
similar structures, called join trees, arise course one standard algorithms
computing arbitrary Bayesian networks (see Lauritzen Spiegelhalter, 1988). Thus
algorithm join trees potential relevance many networks trees.
join trees special structure, allow optimization basic
causal-tree algorithm. elaborate Section 5.
Section 6 consider case arbitrary polytrees. give O(log N ) algorithm updates queries, involves transforming polytree join tree,
using results Sections 2 5. join tree polytree particularly
simple form, giving algorithm updates take O(kp+3 log N ) time queries
O(kp+2 log N ), p maximum number parents node. Although
constant appears large, must noted original polytree takes O(kp+1 N ) space
merely represent, conditional probability tables given explicit matrices.
39

fiDelcher, Grove, Kasif & Pearl


U

,
@

MV jU ,,
,

,


,
V


@



@ X jU
@
@
R
@


X

,
@

jX ,




,

,



@ Z jX
@
R
@


Z


Figure 1: segment causal tree.
Finally, discuss specific modelling application computational biology probabilistic models used describe, analyze predict functional behavior biological sequences protein chains DNA sequences (see Delcher, Kasif, Goldberg,
Hsu, 1993 references). Much information computational biology databases
noisy. However, number successful attempts build probabilistic models
made. case, use probabilistic tree depth 300 consists 600 nodes
matrices conditional probabilities 2 2. tree used model dependence
protein's secondary structure chemical structure. detailed description
problem experimental results given Delcher et al. (1993). problem
obtain effective speed-up factor 10 perform update compared
standard algorithm. Clearly, getting order magnitude improvement response
time probabilistic real-time system could tremendous importance future use
systems.

2. Causal Trees

probabilistic causal tree directed tree node represents discrete random
variable X , directed edge annotated matrix conditional probabilities
jX (associated edge X ! ). is, x possible value X; Y;
(x; )th component jX Pr(Y = jX = x). tree represents joint
probability distribution product space variables; detailed definitions
discussion see Pearl (1988). Brie y, idea consider product, nodes,
conditional probability node given parents. example, Figure 1
implied distribution is:
Pr(U = u; V = v; X = x; = y; Z = z ) =
Pr(U = u) Pr(V = v jU = u) Pr(X = xjU = u) Pr(Y = jX = x) Pr(Z = z jX = x):
Given particular values u; v; x; y; z; conditional probabilities read
appropriate matrices . One advantage product representation
40

fiQueries & Updates Probabilistic Networks

concise. example, need four matrices unconditional probability U ,
size square largest variable's domain size. contrast,
general distribution N variables requires exponential (in N ) representation.
course, every distribution represented causal tree. turns
product decomposition implied tree corresponds particular pattern
conditional independencies often hold (if perhaps approximately) real
applications. Intuitively speaking, Figure 1 implied independencies
conditional probability U given V , X , Z depends values V
X ; probability given U , V , X , Z depends X . Independencies
sort arise many reasons, instance causal modeling interactions
variables. refer reader Pearl (1988) details related modeling
independence assumptions using graphs.
following, make several assumptions significantly simplify presentation, sacrifice generality. First, assume variable ranges
same, constant, number values k.1 follows marginal probability distribution
variable viewed k-dimensional vector, conditional probability
matrix jX square k k matrix. common case binary random
variables (k = 2); distribution values (TRUE, FALSE) (p; 1 , p)
probability p.
next assumption tree binary, complete, node 0
2 children. tree converted form, doubling number
nodes. instance, suppose node p children c1 ; c2; c3 original tree.
create another \copy" p, p0, rearrange tree two children p
c1 p0, two children p0 c2 c3. constrain p0 always
value p simply choosing identity matrix conditional probability table
p p0 . distribution represented new tree effectively
original. Similarly, always add \dummy" leaf nodes necessary ensure
node two children. explained introduction, interested processes
certain variables' values observed, upon wish condition. final
assumption observed evidence nodes leaves tree. Again,
possible \copy" nodes add dummy nodes, restrictive.
product distribution alluded corresponds distribution variables
prior observations. practice, interested conditional distribution,
simply result conditioning observed evidence (which, earlier
assumption, corresponds seeing values leaf nodes). Thus, non-leaf node
X interested conditional marginal probability X , i.e., k-dimensional
vector:
Bel (X ) = Pr(X j evidence values):
main algorithmic problem compute Bel (X ) (non-evidence) node X
tree given current evidence. well known probability vector Bel (X )
computed linear time (in size tree) popular algorithm based
1. assumption nonrestrictive add \dummy" values variable's range,
given conditional probability 0. Nevertheless, may computational advantage
allowing different variable domain sizes. changes required permit dicult, since
complicate presentation somewhat omit them.

41

fiDelcher, Grove, Kasif & Pearl

following equation:
Bel (X ) = Pr(X j evidence) = (X ) (X )
normalizing constant, (X ) probability evidence subtree
node X given X , (X ) probability X given evidence rest
tree. interpret equation, note X = (x1; x2; : : :; xk ) (Y = y1 ; y2 ; : : :; yk )
two vectors define operation component-wise product (pairwise
dyadic product vectors):
X = (x1y1; x2y2; : : :; xkyk ):
usefulness (X ) (X ) derives fact computed recursively, follows:
1. X root node, (X ) prior probability X .
2. X leaf node, (X ) vector 1 ith position (where ith value
observed) 0 elsewhere. value X observed, (X )
vector consisting 1's.2
3. Otherwise, if, shown Figure 1, children node X Z , sibling
V parent U , have:
(X ) = (MY jX (Y )) (MZjX (Z ))


(X ) = MX jU (U ) (MV jU (V ))


presentation technique follows Pearl (1988). However, use
somewhat different notation don't describe messages sent parents successors, rather discuss direct relations among vectors terms simple
algebraic equations. take advantage algebraic properties equations
development.
easy see equations evaluated time proportional
size network. formal proof given Pearl (1988).
Theorem 1: belief distribution every variable (that is, marginal probability
distribution variable, given evidence) causal tree evaluated
O(k2N ) time N size tree. (The factor k2 due multiplication
matrix vector must performed node.)
theorem shows possible perform evidence absorption O(N ) time,
queries constant time (i.e., retrieving previously computed values lookup
table). next sections show perform queries updates
worst-case O(log N ) time. Intuitively, recompute marginal distributions
update, rather make small number changes, sucient, however,
compute value variable logarithmic delay.
2. set 1 components corresponding possible values|this especially useful
observed variable part joint-tree clique (Section 5). general, (X ) thought
likelihood vector X given observations X .

42

fiQueries & Updates Probabilistic Networks

2.1 Simple Preprocessing Approach

obtain intuition new approach begin simple observation.
Consider causal tree depth D. node X tree initially compute
(X ) vector. vectors left uncomputed. Given update node , calculate
revised (X ) vectors nodes X ancestors tree. clearly
done time proportional depth tree, i.e., O(D). rest information
tree remains unchanged. consider Query-Node operation node V
tree. obviously already accurate (V ) vector every node tree
including V . However, order compute (V ) vector need compute
(Y ) vectors nodes V tree multiply appropriate
vectors kept current. means compute accurate (V ) vector
need perform O(D) work well. Thus, approach don't perform complete
update every (X ) (X ) vector tree.
Lemma 2: Update-Node Query-Node operations causal tree performed O(k2 D) time depth tree.
implies tree balanced, operations done O(log N )
time. However, important applications trees balanced (e.g., models
temporal sequences, Delcher et al., 1993). obvious question therefore is: Given causal
tree produce equivalent balanced tree 0? answer question
appears dicult, possible use sophisticated approach produce data
structure (which causal tree) process queries updates O(log N ) time.
approach described subsequent sections.

2.2 Dynamic Data Structure Causal Trees

data structure allow ecient incremental processing probabilistic tree =
T0 sequence trees, T0; T1; T2; : : :; Ti; : : :; Tlog N . Ti+1 contracted
version Ti, whose nodes subset Ti . particular, Ti+1 contain
half many leaves predecessor.
defer details contraction process next section. However, one key
idea maintain consistency, sense Bel (X ); (X ); (X ) given
values trees X appears. choose conditional probability
matrices contracted trees (i.e., trees T0 ) ensure this.
Recall equations form

(X ) = (MY jX (Y )) (MZjX (Z ))


(X ) = MX jU (U ) (MV jU (V ))


Z children X , X right child U , V X 's sibling (Figure 1).
However, equations convenient form following notational
conventions helpful. First, let Ai (x) (resp., Bi (x)) denote conditional
probability matrix X X 's left (resp., right) child tree Ti. Note
identity children differ tree tree, X 's original children
might removed contraction process. One advantage new notation
43

fiDelcher, Grove, Kasif & Pearl

uj

Ti

, @
,
@
,
@

vj

e

xj

Rake

, @
,
@

p p zjp p

(e; x)

)

uj

Ti+1

, @
,
@
,
@

vj

p p zjp p

Figure 2: effect operation Rake (e; x). e must leaf, z may may
leaf.
explicit dependence identity children suppressed. Next, suppose X 's
parent Ti u. let Ci (x) denote either Ai (u) Bi (u), Di(x) denote either
Bi (u) Ai(u) , depending whether X right left child, respectively, U .
necessary keep careful track correspondences, simply note
equations become:3
(x) = Ai(x) (y) Bi (x) (z)
(x) = Di(x) ((u) Ci(x) (v))
next section describe preprocessing step creates dynamic data
structure.


2.3

Rake



Operation

basic operation used contract tree Rake removes leaf
parent tree. effect operation tree shown Figure 2.
define algebraic effect operation equations associated tree.
Recall want define conditional probability matrices raked tree
distribution remaining variables unchanged. achieve substituting
equations (x) (x) equations (u), (z ), (v ). following,
important note (u), (z ) (v ) unaffected rake operation.
following, let Diagff denote diagonal matrix whose diagonal entries
components vector ff. derive algebraic effect rake operation follows:
(u) = Ai (u) (v) Bi (u) (x)
= Ai (u) (v ) Bi (u) (Ai (x) (e) Bi (x) (z ))
= Ai (u) (v ) Bi (u) DiagA (x)(e) Bi (x) (z )


= Ai (u) (v ) Bi (u) DiagA (x)(e) Bi (x) (z )
= Ai+1 (u) (v ) Bi+1 (u) (z )
Ai+1 (u) = Ai (u) Bi+1 (u) = Bi (u) DiagA (x)(e) Bi (x). (Of course, case
leaf raked right child generates analogous equations.) Thus, defining






3. Throughout, assume lower precedence matrix multiplication (indicated ).

44

fiQueries & Updates Probabilistic Networks

Ai+1(u) Bi+1 (u) way, ensure values raked tree identical

corresponding values original tree. yet enough, must
check values similarly preserved. two values could possibly change
(z ) (v ), check both. former, must

(z) = Di(z) ((x) Ci(z) (e))
= Di+1 (z ) ( (u) Ci+1(z ) (v )) :
substituting (x) algebraic manipulation, see assured
Ci+1(z) = Ci(x) Di+1(z) = Di(z) DiagC (z)(e) Di(x). However recall that, definition, Ci+1 (z ) = Ai+1 (u) Ci (x) = Ai (u), Ci+1 (z ) = Ci(x) follows. Furthermore,
Di+1(z) = Bi+1(u)
= (Bi (u) DiagA (x)(e) Bi (x))
= Bi (x) DiagA (x)(e) Bi (u)
= Di(z ) DiagC (z)(e) Di (x)
















required.
(v ) necessary verify

(v) = Di(v) ((u) Ci(v) (x))
= Di+1 (v ) ( (u) Ci+1 (v ) (z )) :
substituting (x), shown true Di+1(v ) = Di (v ) = Ai (u) =
Ai+1(u) Ci+1(v) = Ci(v) DiagA (x)(e) Bi(x) = Bi+1(u). identities follow






definition, done.
Beginning given tree = T0, successive tree constructed performing
sequence rakes, rake away half remaining evidence nodes.
specifically, let Contract operation apply Rake operation every
leaf causal tree, left-to-right order, excluding leftmost rightmost
leaf. Let fTig set causal trees constructed Ti+1 causal tree generated
Ti single application Contract. following result proved using easy
inductive argument:

Theorem 3: Let T0 causal tree size N . number leaves Ti+1 equal

half leaves Ti (not counting two extreme leaves) starting T0,
O(log N ) applications Contract, produce three-node tree: root,
leftmost leaf rightmost leaf.
observations process:
1. complexity Contract linear size tree. Additionally, log N applications Contract reduce set tree equations single equation involving
root O(N ) total time.
2. total space store sets equations associated fTi g0ilog N
twice space required store equations T0.
45

fiDelcher, Grove, Kasif & Pearl

3. equation Ti+1 store equations describe relationship
conditional probability matrices Ti+1 matrices Ti . Notice
that, even though Ti+1 produced Ti series rake operations, matrix
Ti+1 depends directly matrices present Ti. would case
attempted simultaneously rake adjacent children.
regard equations part Ti+1. So, formally speaking fTig causal trees
augmented auxiliary equations. contracted trees describes
probability distribution subset first set variables consistent
original distribution.
note ideas behind Rake operation originally developed Miller
Reif (1985) context parallel computation bottom-up arithmetic expression
trees (Kosaraju & Delcher, 1988; Karp & Ramachandran, 1990). contrast, using
context incremental update query operations sequential computing.
similar data structure independently proposed Frederickson (1993)
context dynamic arithmetic expression trees, different approach incremental
computing arithmetic trees developed Cohen Tamassia (1991).
important interesting differences arithmetic expression-tree case
own. arithmetic expressions computation done bottom-up. However, probabilistic networks -messages must passed top-down. Furthermore, arithmetic expressions
two algebraic operations allowed, typically require distributivity one
operation other, analogous property hold us. respects approach substantial generalization previous work, remaining
conceptually simple practical.

3. Example: Chain

obtain intuition algorithms, sketch generate utilize
Ti; 0 log N equations perform -value queries updates O(log N )
time N = 2L + 1 node chain length L. Consider chain length 4 Figure 3,
trees generated repeated application Contract chain.
equations correspond contracted trees figure follows (ignoring trivial equations). Recall Ai (xj ) matrix associated left edge
random variable xj Ti.

(x1)
(x2)
(x3)
(x4)

=
=
=
=

A0(x1) (e1) B0(x1) (x2)
A0(x2) (e2) B0(x2) (x3)
A0(x3) (e3) B0(x3) (x4)
A0(x4) (e4) B0(x4) (e5)

9
>
>
>
>
>
>
=
>
>
>
B0 (x1) DiagA0 (x2)(e2) B0(x2) >
>
>
B0 (x3) DiagA0 (x4)(e4) B0(x4) ;

(x1) = A1(x1) (e1) B1(x1) (x3)
(x3) = A1(x3) (e3) B1(x3) (e5)


B1(x1) =
B1(x3) =

9
>
>
>
=
>
>
>
;

46

T0

T1

fiQueries & Updates Probabilistic Networks

T0 :

xm
1
em
1
?

T1 :

xm
1
em
1
?

T2 :

xm
1

xm - xm
3 - xm
4 - e5m

- 2

em
2

em
3

?

em
4

?

?

xm - em
5

- 3

em
3
?

em

- 5

em
1
?

Figure 3: simple chain example.

(x1) = A2(x1) (e1) B2(x1) (e5)

9
>
>
=
>
>
;


T2
B2(x1) = B1 (x1) DiagA (x )(e ) B1(x3)
listed matrices because, example, constant.
consider query operation x2 . Rather performing standard computation
find level x2 \raked". Since occurred level 0, obtain
equation
(x2) = A0(x2) (e2) B0(x2) (x3)
Thus must compute (x3), find x3 \raked". happened
level 1. However, level equation associated x3 is:
(x3) = A1(x3) (e3) B1(x3) (e5)
means need follow chain. general chain N nodes
answer query node chain evaluating log N equations instead N
equations.
consider update e4 . Since e4 raked immediately, first modify
equation
B1(x3) = B0(x3) DiagA (x )(e ) B0(x4)
first level e4 occurs right-hand side. Since B1 (x3) affected
change e4 , subsequently modify equation
B2(x1) = B1(x1) DiagA (x )(e ) B1(x3)
1

47

3

3

0

4

4

1

3

3

fiDelcher, Grove, Kasif & Pearl

second level. general, clearly need update log N equations; i.e., one
per level. generalize example describe general algorithms queries
updates causal trees.

3.1 Performing Queries Updates Eciently

section shall show utilize contracted trees Ti; 0 log N
perform queries updates O(log N ) time general causal trees. shall show
logarithmic amount work necessary sucient compute enough information
data structure update query value.

3.2 Queries

compute (x) node x following. first locate ind (x),
defined highest level x appears Ti . equation (x)
form:
(x) = Ai(x) (y) Bi(x) (z)
z left right children, respectively, x Ti.
Since x appear Ti+1 , raked level equations, implies
one child (we assume z ) leaf. therefore need compute (y ),
done recursively. instead raked leaf, would compute (z ) recursively.
either case O(1) operations done addition one recursive call,
value higher level equations. Since O(log N ) levels, operations
matrix vector multiplications, procedure takes O(k2 log N ) time. function
-Query (x) given Figure 4.

3.3 Updates

describe update operations modify enough information data
structure allow us query vectors vectors eciently. importantly
reader note update operation try maintain correct
values. sucient ensure that, x, matrices Ai(x) Bi (x) (and
thus Ci (x) Di(x)) always date.
update value evidence node, simply changing value
leaf e. level equations, value (e) appear twice:
-equation e's parent -equation e's sibling Ti. e
disappears, say level i, value incorporated one constant matrices Ai+1 (u)
Bi+1 (u) u grandparent e Ti . constant matrix turn affects
exactly one constant matrix next higher level, on. Since effect
level computed O(k3 ) time (due matrix multiplication) O(log N )
levels equations, update accomplished O(k3 log N ) time. constant k3
actually pessimistic, faster matrix multiplication algorithms exist.
update procedure given Figure 5. Update initially called Update((E ) =
e; i) E leaf, level raked, e new evidence.
operation start sequence O(log N ) calls function -Update (X = Term; i)
change propagate log N equations.
48

fiQueries & Updates Probabilistic Networks

FUNCTION -Query (x)
look equation associated (x) Tind (x).
Case 1: x leaf. equation form: (x) = e e known.
case return e.
Case 2: equation associated (x) form

(x) = Ai(x) (y) Bi (x) (z)
z leaf therefore (z ) known. case return
Ai(X ) -Query (y) Bi (X ) (z)
case leaf analogous.
Figure 4: Function compute value node.

3.4 Queries

relatively easy use similar recursive procedure perform (x) queries. Unfortunately, approach yields O(log2N )-time algorithm simply use recursion
calculate terms calculate terms using earlier procedure.
O(log N ) recursive calls calculate values, defined equation
involves term taking O(log N ) time compute.
achieve O(log N ) time, shall instead implement (x) queries defining procedure Calc (x; i) returns triple vectors hP; L; Ri P = (x), L = (y )
R = (z ) z left right children, respectively, x Ti.
compute (x) node x following. Let = ind (x). equation
(x) Ti form:

(x) = Di(x) ((u) Ci(x) (v))
u parent x Ti v sibling. call procedure Calc (u; + 1)
return triple h (u); (v); (x)i, immediately compute (x)
using equation.
Procedure Calc (x; i) implemented following fashion.
Case 1: Ti 3-node tree x root, children x leaves, hence
values known, (x) given sequence prior probabilities x.
Case 2: x appear Ti+1 , one x's children leaf, say e raked
level i. Let z child. call Calc (u; + 1), u parent
x Ti, receive back h(u); (z); (v)i h(u); (v); (z)i according whether x
49

fiDelcher, Grove, Kasif & Pearl

FUNCTION -Update (Term = Value; i)
1. Find (at one) equation Ti , defining Ai Bi , Term
appears right-hand side; let Term0 matrix defined equation
(i.e., left-hand side).
2. Update Term0; let Value new value.
3. Call -Update (Term0 = Value; + 1) recursively.
Figure 5: update procedure.
left right child u Ti (and v u's child). compute (x)
(u) (v ), (e) (z ), return necessary triple.
Specifically,

(x) =

(

Di(x) ((u) Ai+1 (u) (v))
Di(x) ((u) Bi+1 (u) (v))

choice depends whether x right left child, respectively, u Ti.
Case 3: x appear Ti+1, call Calc (x; + 1). returns correct
value (x). child z x Ti remains child x Ti+1 , returns
correct value (z ). z child x occur Ti+1 , must
case z raked level one z 's children, say e, leaf let
child q . situation Calc (x; + 1) returned value (q )
compute

(z) = Ai(z) (e) Bi (z) (q)
return value.
three cases, constant amount work done addition single recursive
call uses equations higher level. Since O(log N ) levels equations,
requiring matrix vector multiplication, total work done O(k2 log N ).

4. Extended Example
section illustrate application algorithms specific example. Consider
sequence contracted trees shown Figure 6. Corresponding trees
50

fiQueries & Updates Probabilistic Networks

xl
1

T0 :







xl

#
#
#
#

2

xl
4

xl
6






e1

xl
8






e2

e4



e8











xl
2




xl
5

xl
7

xl
3






Z
Z

c
c
c
c

, @
@
@

,
,

xl
1

T1 :

Z
Z
Z

e6






xl
4



e9






xl
6

e7






e5

e1

e3

T2:

xl
1






e1

e3

T3: xl
1






xl
4

e5

e9

e5

Figure 6: Example tree contraction.

51






e1

e9

e7

e9

fiDelcher, Grove, Kasif & Pearl

equations following:
T0 :
(x1) = A0(x1 ) (x2 ) B0 (x1 ) (x3 )
..
.





(x2) = D0 (x2) ((x1) C0(x2) (x3 ))
..
.





T1 :
(x1) = A1(x1 ) (x2 ) B1 (x1 ) (e9 )
..
.











(x2) = D1 (x2) ((x1) C1(x2) (e9 ))
..
.





T2 :
(x1) = A2(x1 ) (x4 ) B2 (x1 ) (e9 )
..
.







(x4) = D2 (x4) ((x1) C2(x4) (e9 ))
..
.









T3 :
(x1) = A3(x1 ) (e1 ) B3 (x1) (e9 )






consider, instance, effect update e2 . Since raked immediately,
new value (e2) incorporated in:
B1 (x6 ) = B0 (x6 ) DiagA0 (x8 ) (e2) B0 (x8 )
subsequent Rake operations know A2(x4 ) depends B1 (x6), A3 (x1)
depends A2 (x4), must update values follows:
A2 (x4 ) = A1 (x4) DiagB1 (x6 ) (e3 ) A1 (x6)
A3 (x1 ) = A2 (x1) DiagB2 (x4 ) (e5 ) A2 (x4)


















Finally, consider query x7 . Since x7 raked together e5 T0 , follow
steps outlined generate following calls: Calc (x7; 0), Calc (x4; 1),
Calc (x4; 2), Calc (x1; 3). provides us (x7). case, (x7)
particularly easy compute since x7 's children leaf nodes. simply
compute (x7) (x7) normalize, giving us conditional marginal distribution
Bel (x7) required.

5. Join Trees

Perhaps best-known technique computing arbitrary (i.e., singly-connected)
Bayesian networks uses idea join trees (junction trees) (Lauritzen & Spiegelhalter,
1988). many ways join tree thought causal tree, albeit one somewhat
special structure. Thus algorithm previous section applied. However,
structure join tree permits optimization, describe section.
becomes especially relevant next section, use join-tree technique
show O(log N ) updates queries done arbitrary polytrees. review
join-trees utility extremely brief quite incomplete; clear expositions
see, instance, Spiegelhalter et al. (1993) Pearl (1988).
Given Bayesian network, first step towards constructing join-tree moralize
network: insert edges every pair parents common node, treat
52

fiQueries & Updates Probabilistic Networks

edges graph undirected (Spiegelhalter et al., 1993). resulting undirected
graph called moral graph. interested undirected graphs chordal :
every cycle length 4 contain chord (i.e., edge two nodes
non-adjacent cycle). moral graph chordal, necessary add
edges make so; various techniques triangulation stage known (for instance,
see Spiegelhalter et al., 1993).
p probability distribution represented Bayesian network G = (V; E ),
= (V; F ) result moralizing triangulating G, then:
1. jV j cliques,4 say C1; : : :; CjV j.
2. cliques ordered > 1 j (i) <

Ci \ Cj(i) = Ci \ (C1 [ C2 [ : : : [ Ci,1:)
tree formed treating cliques nodes, connecting node Ci
\parent" Cj (i), called join tree.
3. p =





p(CijCj(i))

4. p(CijCj (i)) = p(CijCj (i) \ Ci )
2 3, see direct edges away \parent" cliques,
resulting directed tree fact Bayesian causal tree represent original
distribution p. true matter form original graph. course,
price cliques may large, domain size (the number possible values
clique node) exponential size. technique guaranteed
ecient.
use Rake technique Section 2 directed join tree without
modification. However, property 4 shows conditional probability matrices
join tree special structure. use gain eciency.
following, let k domain size variables G usual. Let n maximum
size cliques join tree; without loss generality assume cliques
size (because add \dummy" variables). Thus domain size
clique K = kn . Finally, let c maximum intersection size clique parent
(i.e., jCj (i) \ Cij) L = kc .
standard algorithm, would represent p(CijCj (i)) K K matrix, MC jC .
However, p(Ci jCj (i) \ Ci) represented smaller L K matrix, MC jC \C .
property 4 above, MC jC identical MC jC \C , except many rows repeated.
Thus K L matrix J








j (i)



j (i)





j (i)

MC jC = J MC jC


j (i)

j (i)

j (i)

\Ci :

(J actually simple matrix whose entries 0 1, exactly one 1 per row; however
use fact.)
4. clique maximal completely-connected subgraph.

53

fiDelcher, Grove, Kasif & Pearl

claim that, case join trees, following true. First, matrices

Ai Bi used Rake algorithm stored factored form, product
two matrices dimension K L L K respectively. So, instance, factor Ai
Ali Ari . never need explicitly compute, store, full matrices.
seen, claim true = 0 matrices factor way. proof
> 1 uses inductive argument, illustrate below. second claim that,

matrices stored factored form, matrix multiplications used
Rake algorithm one following types: 1) L K matrix times K L matrix,
2) L K matrix times K K diagonal matrix, 3) L L matrix times L K
matrix, 4) L K matrix times vector.
prove claims consider, instance, equation defining Bi+1 terms lowerlevel matrices. Section 2, Bi+1 (u) = Bi (u) DiagA (x)(e) Bi (x): But, assumption,
is:
(Bil (u) Bir (u)) Diag(A (x)A (x))(e) (Bil (x) Bil (x));
which, using associativity, clearly equivalent
h

Bil (u) ((Bir (u) DiagA (x)(A (x)(e))) Bil(x)) Bil (x) :
However, every multiplication expression one forms stated earlier. Identifying
Bil+1 (u) Bil(u) Bir+1 (u) bracketed part expression proves case,
course case rake left child (so Ai+1 (u) updated) analogous.
Thus, even using straightforward technique matrix multiplication, cost
updating Bi+1 O(KL2) = O(kn+2c ). contrasts O(K 3) factor
matrices, may represent worthwhile speedup c small. Note overall time
update using scheme O(kn+2c log N ). Queries, involve matrix
vector multiplication, require O(kn+c log N ) time.
many join trees difference N log N unimportant,
clique domain size K often enormous dominates complexity. Indeed, K L
may large cannot represent required matrices explicitly. course,
cases technique little offer. cases benefits
worthwhile. important general class so, immediate
reason presenting technique join trees, case polytrees.


l


r


l


r


6. Polytrees

polytree singly connected Bayesian network; drop assumption Section 2
node one parent. Polytrees offer much exibility causal
trees, yet well-known process update query O(N ) time,
causal trees. reason polytrees extremely popular class networks.
suspect possible present O(log N ) algorithm updates queries
polytrees, direct extension ideas Section 2. Instead propose different
technique, involves converting polytree join tree using ideas
preceding section. basis simple observation join tree
polytree already chordal. Thus (as show detail below) little lost considering
join tree instead original polytree. specific property polytrees
require following. omit proof well-known proposition.
54

fiQueries & Updates Probabilistic Networks

Proposition 4: moral graph polytree P = (V; E ) chordal,
set maximal cliques ffv g [ parents (v ) : v 2 V g.
Let p maximum number parents node. proposition, every
maximal clique join tree p +1 variables, domain size node
join tree K = kp+1 . may large, recall conditional probability
matrix original polytree, variable p parents, K entries anyway since
must give conditional distribution every combination node's parents. Thus K
really measure size polytree itself.
follows proposition perform query update
polytrees time O(K 3 log N ), simply using algorithm Section 2 directed
join tree. But, noted Section 5, better. Recall savings depend
c, maximum size intersection node parent join tree.
However, join tree formed polytree, two cliques share
single node. follows immediately Proposition 4, two cliques
one node common must either two nodes share one parent,
else node one parents share yet another parent. Neither
consistent network polytree. Thus complexity bounds Section 5,
put c = 1. follows process updates O(Kk2c log N ) = O(kp+3 log N )
time queries O(kp+2 log N ).

7. Application: Towards Automated Site-Specific Muta-Genesis

experiment commonly performed biology laboratories procedure
particular site protein changed (i.e., single amino-acid mutated)
tested see whether protein settles different conformation. many cases,
overwhelming probability protein change secondary structure outside
mutated region. process often called muta-genesis. Delcher et al. (1993) developed
probabilistic model protein structure basically long chain. length
chain varies 300{500 nodes. nodes network either protein-structure
nodes (PS-nodes) evidence nodes (E-nodes). PS-node network discrete
random variable Xi assumes values corresponding descriptors secondary sequence
structure: helix, sheet coil. PS-node model associates evidence node
corresponds occurrence particular subsequence amino acids particular
location protein.
model, protein-structure nodes finite strings alphabet fh; e ; c g.
example string hhhhhh string six residues ff-helical conformation,
eecc string two residues -sheet conformation followed two residues folded
coil. Evidence nodes nodes contain information particular region
protein. Thus, main idea represent physical statistical rules form
probabilistic network.
first set experiments converged following model that, clearly
biologically naive, seems match prediction accuracy many existing approaches
neural networks. network looks set PS-nodes connected chain.
node connect single evidence node. experiments PS-nodes strings
length two three alphabet fh; e ; c g evidence nodes strings
55

fiDelcher, Grove, Kasif & Pearl


cc

?
GS


ch
?
SA


hh
?







Figure 7: Example causal tree model using pairs, showing protein segment GSAT
corresponding secondary structure cchh.
length set amino acids. following example clarifies representation.
Assume string amino acids GSAT. model string network comprised
three evidence nodes GS, SA, three PS-nodes. network shown Figure 7.
correct prediction assign values cc, ch, hh PS-nodes shown
figure.
probabilistic model, test robustness protein
whether small changes protein affect structure certain critical sites
protein. experiments, probabilistic network performs \simulated evolution"
protein, namely simulator repeatedly mutates region chain tests
whether designated sites protein coiled helix predicted
remain conformation. main goal experiment test stable bonds far
away mutated location affected. previous results (Delcher et al., 1993)
support current thesis biology community, namely local distant changes
rarely affect structure.
algorithms presented previous sections paper perfectly suited
type application predicted generate factor 10 improvement
eciency current brute-force implementation presented Delcher et al. (1993)
change propagated throughout network.

8. Summary
paper proposed several new algorithms yield substantial improvement
performance probabilistic networks form causal trees. updating procedures
absorb sucient information tree query procedure compute
correct probability distribution node given current evidence. addition,
procedures execute time O(log N ), N size network. algorithms
expected generate orders-of-magnitude speed-ups causal trees contain long
paths (not necessarily chains) matrices conditional probabilities
relatively small. currently experimenting approach singly connected
networks (polytrees). likely dicult generalize techniques general
networks. Since known general problem inference probabilistic networks
NP -hard (Cooper, 1990), obviously possible obtain polynomial-time incremental
56

fiQueries & Updates Probabilistic Networks

solutions type discussed paper general probabilistic networks.
natural open question extending approach developed paper dynamic
operations probabilistic networks addition deletion nodes modifying
matrices conditional probabilities (as result learning).
would interesting investigate practical logarithmic-time parallel algorithms probabilistic networks realistic parallel models computation. One
main goals massively parallel AI research produce networks perform real-time
inference large knowledge-bases eciently (i.e., time proportional depth
network rather size network) exploiting massive parallelism. Jerry
Feldman pioneered philosophy context neural architectures (see Stanfill
Waltz, 1986, Shastri, 1993, Feldman Ballard, 1982). achieve type performance neural network framework, typically postulate parallel hardware
associates processor node network typically ignores communication requirements. careful mapping parallel architectures one indeed achieve ecient
parallel execution specific classes inference operations (see Mani Shastri, 1994,
Kasif, 1990, Kasif Delcher, 1992). techniques outlined paper presented
alternative architecture supports fast (sub-linear time) response capability
sequential machines based preprocessing. However, approach obviously limited
applications number updates queries time constant. One would
naturally hope develop parallel computers support real-time probabilistic reasoning
general networks.

Acknowledgements
Simon Kasif's research Johns Hopkins University sponsored part National
Science foundation Grants No. IRI-9116843, IRI-9223591 IRI-9220960.

References

Berger, T., & Ye, Z. (1990). Entropic aspects random fields trees. IEEE Trans.
Information Theory, 36 (5), 1006{1018.
Chelberg, D. M. (1990). Uncertainty interpretation range imagery. Proc. Intern.
Conference Computer Vision, pp. 654{657.
Cohen, R. F., & Tamassia, R. (1991). Dynamic trees applications. Proceedings
2nd ACM-SIAM Symposium Discrete Algorithms, pp. 52{61.
Cooper, G. (1990). computational complexity probabilistic inference using bayes
belief networks. Artificial Intelligence, 42, 393{405.
Delcher, A., & Kasif, S. (1992). Improved decision making game trees: Recovering
pathology. Proceedings 1992 National Conference Artificial Intelligence.
Delcher, A. L., Kasif, S., Goldberg, H. R., & Hsu, B. (1993). Probabilistic prediction protein secondary structure using causal networks. Proceedings 1993 International
Conference Intelligent Systems Computational Biology, pp. 316{321.
57

fiDelcher, Grove, Kasif & Pearl

Duda, R., & Hart, P. (1973). Pattern Classification Scene Analysis. Wiley, New York.
Feldman, J. A., & Ballard, D. (1982). Connectionist models properties. Cognitive
Science, 6, 205{254.
Frederickson, G. N. (1993). data structure dynamically maintaining rooted trees.
Proc. 4th Annual Symposium Discrete Algorithms, pp. 175{184.
Hel-Or, Y., & Werman, M. (1992). Absolute orientation uncertain data: unified
approach. Proc. Intern. Conference Computer Vision Pattern Recognition,
pp. 77{82.
Karp, R. M., & Ramachandran, V. (1990). Parallel algorithms shared-memory machines.
Van Leeuwen, J. (Ed.), Handbook Theoretical Computer Science, pp. 869{941.
North-Holland.
Kasif, S. (1990). parallel complexity discrete relaxation constraint networks.
Artificial Intelligence, 45, 275{286.
Kasif, S., & Delcher, A. (1994). Analysis local consistency parallel constraint networks.
Artificial Intelligence, 69.
Kosaraju, S. R., & Delcher, A. L. (1988). Optimal parallel evaluation tree-structured
computations raking. Reif, J. H. (Ed.), VLSI Algorithms Architectures:
Proceedings 1988 Aegean Workshop Computing, pp. 101{110. Springer Verlag.
LNCS 319.
Lauritzen, S., & Spiegelhalter, D. (1988). Local computations probabilities graphical
structures applications expert systems. J. Royal Statistical Soc. Ser. B,
50, 157{224.
Mani, D., & Shastri, L. (1994). Massively parallel reasoning large knowledge
bases. Tech. rep., Intern. Computer Science Institute.
Miller, G. L., & Reif, J. (1985). Parallel tree contraction application. Proceedings
26th IEEE Symposium Foundations Computer Science, pp. 478{489.
Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems. Morgan Kaufmann.
Peot, M. A., & Shachter, R. D. (1991). Fusion propagation multiple observations
belief networks. Artificial Intelligence, 48, 299{318.
Rachlin, J., Kasif, S., Salzberg, S., & Aha, D. (1994). Towards better understanding
memory-based bayesian classifiers. Proceedings Eleventh International
Conference Machine Learning, pp. 242{250 New Brunswick, NJ.
Shastri, L. (1993). computational model tractable reasoning: Taking inspiration
cognition. Proceeding 1993 Intern. Joint Conference Artificial Intelligence.
AAAI.
58

fiQueries & Updates Probabilistic Networks

Spiegelhalter, D., Dawid, A., Lauritzen, S., & Cowell, R. (1993). Bayesian analysis expert
systems. Statistical Science, 8 (3), 219{283.
Stanfill, C., & Waltz, D. (1986). Toward memory-based reasoning. Communications
ACM, 29 (12), 1213{1228.
Wilsky, A. (1993). Multiscale representation markov random fields. IEEE Trans. Signal
Processing, 41, 3377{3395.

59


