journal artificial intelligence

submitted published

logarithmic time updates queries
probabilistic networks
arthur l delcher

computer science department loyola college maryland
baltimore md

adam j grove

delcher cs loyola edu

grove nj nec com

nec institute
princeton nj

simon kasif

kasif cs jhu edu

judea pearl

pearl lanai cs ucla edu

department computer science johns hopkins university
baltimore md
department computer science university california
los angeles ca

abstract

traditional databases commonly support ecient query update procedures
operate time sublinear size database goal
take first step toward dynamic reasoning probabilistic databases comparable
eciency propose dynamic data structure supports ecient
updating querying singly connected bayesian networks conventional
evidence absorbed time queries processed time n n
size network propose preprocessing phase
allows us answer queries time log n expense log n time per evidence
absorption usefulness sub linear processing time manifests applications
requiring near real time response large probabilistic databases brie discuss
potential application dynamic probabilistic reasoning computational biology

introduction
probabilistic bayesian networks increasingly popular modeling technique
used successfully numerous applications intelligent systems real time navigation model diagnosis information retrieval classification bayesian
forecasting natural language processing computer vision medical informatics computational biology probabilistic networks allow user describe environment
probabilistic database consists large number random variables corresponding important parameter environment random variables could
fact hidden may correspond unknown parameters causes uence
observable variables probabilistic networks quite general store information
probability failure particular component computer system prob c ai access foundation morgan kaufmann publishers rights reserved

fidelcher grove kasif pearl

ability page computer cache requested near future probability
document relevant particular query probability amino acid
subsequence protein chain folding alpha helix conformation
applications mind include networks dynamically maintained
keep track probabilistic model changing system instance consider task
automated detection power plant failures might repeat cycle consists
following sequence operations first perform sensing operations operations
cause updates performed specific variables probabilistic database
evidence estimate query probability failure certain sites precisely
query probability distribution random variables measure probability
failure sites evidence since plant requires constant monitoring
must repeat cycle sense evaluate frequent basis
conventional non probabilistic database tracking plant state would
appropriate possible directly observe whether failure
occur hand probabilistic database bayesian network
useful operations update query performed quickly
real time near real time often necessary question extremely
fast reasoning probabilistic networks important
traditional non probabilistic databases support ecient query update procedures
often operate time sublinear size database e g binary search goal take step toward systems perform
dynamic probabilistic reasoning probability event given set
observations time sublinear size probabilistic network typically
sublinear performance complex networks attained parallelism
relies preprocessing
specifically describe performing queries updates belief
networks form trees causal trees polytrees join trees define two natural
database operations probabilistic networks


update node

perform sensory input modify evidence leaf node single
variable network absorb evidence network



query node

obtain marginal probability distribution values
arbitrary node single variable network

standard introduced pearl perform query node operation time although evidence absorption e update node operation takes
n time n size network alternatively one assume
update node operation takes time simply recording change querynode operation takes n time evaluating entire network
describe perform queries updates log n
time significant systems since improve ability system
respond change encountered n time log n
preprocessing network form node absorption carefully structured
way create hierarchy abstractions network previous uses node absorption
techniques reported peot shachter


fiqueries updates probabilistic networks

note measuring complexity terms size network n
overlook important factors suppose variable network domain
size k less many purposes k considered constant nevertheless
consider slowdown power k become
significant practice unless n large thus careful state slowdown
exists
section considers case causal trees e singly connected networks
node one parent standard see pearl must use k n
time updates retrieval although one operations done
time discuss brie section straightforward variant
takes k time queries updates height
tree
present takes k log n time updates k log n
time queries causal tree course represent tremendous speedup
especially large networks begins polynomial time preprocessing
step linear size network constructing another data structure
probabilistic tree supports fast queries updates techniques use
motivated earlier dynamic arithmetic trees involve caching sucient intermediate computations update phase querying relatively
easy note however substantial interesting differences
probabilistic networks arithmetic trees particular
apparent later computation probabilistic trees requires bottom top
processing whereas arithmetic trees need former perhaps even interesting relevant probabilistic operations different algebraic structure
arithmetic operations instance lack distributivity
bayesian trees many applications literature including classification
instance one popular methods classification bayes classifier
makes independence assumption features used perform classification
duda hart rachlin kasif salzberg aha probabilistic trees
used computer vision hel werman chelberg signal processing
wilsky game playing delcher kasif statistical mechanics berger
ye nevertheless causal trees fairly limited modeling purposes however
similar structures called join trees arise course one standard
computing arbitrary bayesian networks see lauritzen spiegelhalter thus
join trees potential relevance many networks trees
join trees special structure allow optimization basic
causal tree elaborate section
section consider case arbitrary polytrees give log n updates queries involves transforming polytree join tree
sections join tree polytree particularly
simple form giving updates take kp log n time queries
kp log n p maximum number parents node although
constant appears large must noted original polytree takes kp n space
merely represent conditional probability tables given explicit matrices


fidelcher grove kasif pearl


u




mv ju






v






x ju


r



x




jx










z jx

r



z


figure segment causal tree
finally discuss specific modelling application computational biology probabilistic used describe analyze predict functional behavior biological sequences protein chains dna sequences see delcher kasif goldberg
hsu references much information computational biology databases
noisy however number successful attempts build probabilistic
made case use probabilistic tree depth consists nodes
matrices conditional probabilities tree used model dependence
protein secondary structure chemical structure detailed description
experimental given delcher et al
obtain effective speed factor perform update compared
standard clearly getting order magnitude improvement response
time probabilistic real time system could tremendous importance future use
systems

causal trees

probabilistic causal tree directed tree node represents discrete random
variable x directed edge annotated matrix conditional probabilities
jx associated edge x x possible value x
x th component jx pr jx x tree represents joint
probability distribution product space variables detailed definitions
discussion see pearl brie idea consider product nodes
conditional probability node given parents example figure
implied distribution
pr u u v v x x z z
pr u u pr v v ju u pr x xju u pr jx x pr z z jx x
given particular values u v x z conditional probabilities read
appropriate matrices one advantage product representation


fiqueries updates probabilistic networks

concise example need four matrices unconditional probability u
size square largest variable domain size contrast
general distribution n variables requires exponential n representation
course every distribution represented causal tree turns
product decomposition implied tree corresponds particular pattern
conditional independencies often hold perhaps approximately real
applications intuitively speaking figure implied independencies
conditional probability u given v x z depends values v
x probability given u v x z depends x independencies
sort arise many reasons instance causal modeling interactions
variables refer reader pearl details related modeling
independence assumptions graphs
following make several assumptions significantly simplify presentation sacrifice generality first assume variable ranges
constant number values k follows marginal probability distribution
variable viewed k dimensional vector conditional probability
matrix jx square k k matrix common case binary random
variables k distribution values true false p p
probability p
next assumption tree binary complete node
children tree converted form doubling number
nodes instance suppose node p children c c c original tree
create another copy p p rearrange tree two children p
c p two children p c c constrain p
value p simply choosing identity matrix conditional probability table
p p distribution represented tree effectively
original similarly add dummy leaf nodes necessary ensure
node two children explained introduction interested processes
certain variables values observed upon wish condition final
assumption observed evidence nodes leaves tree
possible copy nodes add dummy nodes restrictive
product distribution alluded corresponds distribution variables
prior observations practice interested conditional distribution
simply conditioning observed evidence earlier
assumption corresponds seeing values leaf nodes thus non leaf node
x interested conditional marginal probability x e k dimensional
vector
bel x pr x j evidence values
main algorithmic compute bel x non evidence node x
tree given current evidence well known probability vector bel x
computed linear time size tree popular
assumption nonrestrictive add dummy values variable range
given conditional probability nevertheless may computational advantage
allowing different variable domain sizes changes required permit dicult since
complicate presentation somewhat omit



fidelcher grove kasif pearl

following equation
bel x pr x j evidence x x
normalizing constant x probability evidence subtree
node x given x x probability x given evidence rest
tree interpret equation note x x x xk yk
two vectors define operation component wise product pairwise
dyadic product vectors
x x x xkyk
usefulness x x derives fact computed recursively follows
x root node x prior probability x
x leaf node x vector ith position ith value
observed elsewhere value x observed x
vector consisting
otherwise shown figure children node x z sibling
v parent u
x jx mzjx z


x mx ju u mv ju v


presentation technique follows pearl however use
somewhat different notation describe messages sent parents successors rather discuss direct relations among vectors terms simple
algebraic equations take advantage algebraic properties equations
development
easy see equations evaluated time proportional
size network formal proof given pearl
theorem belief distribution every variable marginal probability
distribution variable given evidence causal tree evaluated
k n time n size tree factor k due multiplication
matrix vector must performed node
theorem shows possible perform evidence absorption n time
queries constant time e retrieving previously computed values lookup
table next sections perform queries updates
worst case log n time intuitively recompute marginal distributions
update rather make small number changes sucient however
compute value variable logarithmic delay
set components corresponding possible values especially useful
observed variable part joint tree clique section general x thought
likelihood vector x given observations x



fiqueries updates probabilistic networks

simple preprocessing

obtain intuition begin simple observation
consider causal tree depth node x tree initially compute
x vector vectors left uncomputed given update node calculate
revised x vectors nodes x ancestors tree clearly
done time proportional depth tree e rest information
tree remains unchanged consider query node operation node v
tree obviously already accurate v vector every node tree
including v however order compute v vector need compute
vectors nodes v tree multiply appropriate
vectors kept current means compute accurate v vector
need perform work well thus perform complete
update every x x vector tree
lemma update node query node operations causal tree performed k time depth tree
implies tree balanced operations done log n
time however important applications trees balanced e g
temporal sequences delcher et al obvious question therefore given causal
tree produce equivalent balanced tree answer question
appears dicult possible use sophisticated produce data
structure causal tree process queries updates log n time
described subsequent sections

dynamic data structure causal trees

data structure allow ecient incremental processing probabilistic tree
sequence trees ti tlog n ti contracted
version ti whose nodes subset ti particular ti contain
half many leaves predecessor
defer details contraction process next section however one key
idea maintain consistency sense bel x x x given
values trees x appears choose conditional probability
matrices contracted trees e trees ensure
recall equations form

x jx mzjx z


x mx ju u mv ju v


z children x x right child u v x sibling figure
however equations convenient form following notational
conventions helpful first let ai x resp bi x denote conditional
probability matrix x x left resp right child tree ti note
identity children differ tree tree x original children
might removed contraction process one advantage notation


fidelcher grove kasif pearl

uj

ti







vj

e

xj

rake





p p zjp p

e x



uj

ti







vj

p p zjp p

figure effect operation rake e x e must leaf z may may
leaf
explicit dependence identity children suppressed next suppose x
parent ti u let ci x denote ai u bi u di x denote
bi u ai u depending whether x right left child respectively u
necessary keep careful track correspondences simply note
equations become
x ai x bi x z
x di x u ci x v
next section describe preprocessing step creates dynamic data
structure




rake



operation

basic operation used contract tree rake removes leaf
parent tree effect operation tree shown figure
define algebraic effect operation equations associated tree
recall want define conditional probability matrices raked tree
distribution remaining variables unchanged achieve substituting
equations x x equations u z v following
important note u z v unaffected rake operation
following let diagff denote diagonal matrix whose diagonal entries
components vector derive algebraic effect rake operation follows
u ai u v bi u x
ai u v bi u ai x e bi x z
ai u v bi u diaga x e bi x z


ai u v bi u diaga x e bi x z
ai u v bi u z
ai u ai u bi u bi u diaga x e bi x course case
leaf raked right child generates analogous equations thus defining






throughout assume lower precedence matrix multiplication indicated



fiqueries updates probabilistic networks

ai u bi u way ensure values raked tree identical

corresponding values original tree yet enough must
check values similarly preserved two values could possibly change
z v check former must

z di z x ci z e
di z u ci z v
substituting x algebraic manipulation see assured
ci z ci x di z di z diagc z e di x however recall definition ci z ai u ci x ai u ci z ci x follows furthermore
di z bi u
bi u diaga x e bi x
bi x diaga x e bi u
di z diagc z e di x
















required
v necessary verify

v di v u ci v x
di v u ci v z
substituting x shown true di v di v ai u
ai u ci v ci v diaga x e bi x bi u identities follow






definition done
beginning given tree successive tree constructed performing
sequence rakes rake away half remaining evidence nodes
specifically let contract operation apply rake operation every
leaf causal tree left right order excluding leftmost rightmost
leaf let ftig set causal trees constructed ti causal tree generated
ti single application contract following proved easy
inductive argument

theorem let causal tree size n number leaves ti equal

half leaves ti counting two extreme leaves starting
log n applications contract produce three node tree root
leftmost leaf rightmost leaf
observations process
complexity contract linear size tree additionally log n applications contract reduce set tree equations single equation involving
root n total time
total space store sets equations associated fti g ilog n
twice space required store equations


fidelcher grove kasif pearl

equation ti store equations describe relationship
conditional probability matrices ti matrices ti notice
even though ti produced ti series rake operations matrix
ti depends directly matrices present ti would case
attempted simultaneously rake adjacent children
regard equations part ti formally speaking ftig causal trees
augmented auxiliary equations contracted trees describes
probability distribution subset first set variables consistent
original distribution
note ideas behind rake operation originally developed miller
reif context parallel computation bottom arithmetic expression
trees kosaraju delcher karp ramachandran contrast
context incremental update query operations sequential computing
similar data structure independently proposed frederickson
context dynamic arithmetic expression trees different incremental
computing arithmetic trees developed cohen tamassia
important interesting differences arithmetic expression tree case
arithmetic expressions computation done bottom however probabilistic networks messages must passed top furthermore arithmetic expressions
two algebraic operations allowed typically require distributivity one
operation analogous property hold us respects substantial generalization previous work remaining
conceptually simple practical

example chain

obtain intuition sketch generate utilize
ti log n equations perform value queries updates log n
time n l node chain length l consider chain length figure
trees generated repeated application contract chain
equations correspond contracted trees figure follows ignoring trivial equations recall ai xj matrix associated left edge
random variable xj ti

x
x
x
x






x e b x x
x e b x x
x e b x x
x e b x e












b x diaga x e b x


b x diaga x e b x

x x e b x x
x x e b x e


b x
b x

















fiqueries updates probabilistic networks



xm

em





xm

em





xm


xm xm
xm
e



em


em




em






xm em




em



em



em



figure simple chain example

x x e b x e











b x b x diaga x e b x
listed matrices example constant
consider query operation x rather performing standard computation
level x raked since occurred level obtain
equation
x x e b x x
thus must compute x x raked happened
level however level equation associated x
x x e b x e
means need follow chain general chain n nodes
answer query node chain evaluating log n equations instead n
equations
consider update e since e raked immediately first modify
equation
b x b x diaga x e b x
first level e occurs right hand side since b x affected
change e subsequently modify equation
b x b x diaga x e b x




















fidelcher grove kasif pearl

second level general clearly need update log n equations e one
per level generalize example describe general queries
updates causal trees

performing queries updates eciently

section shall utilize contracted trees ti log n
perform queries updates log n time general causal trees shall
logarithmic amount work necessary sucient compute enough information
data structure update query value

queries

compute x node x following first locate ind x
defined highest level x appears ti equation x
form
x ai x bi x z
z left right children respectively x ti
since x appear ti raked level equations implies
one child assume z leaf therefore need compute
done recursively instead raked leaf would compute z recursively
case operations done addition one recursive call
value higher level equations since log n levels operations
matrix vector multiplications procedure takes k log n time function
query x given figure

updates

describe update operations modify enough information data
structure allow us query vectors vectors eciently importantly
reader note update operation try maintain correct
values sucient ensure x matrices ai x bi x
thus ci x di x date
update value evidence node simply changing value
leaf e level equations value e appear twice
equation e parent equation e sibling ti e
disappears say level value incorporated one constant matrices ai u
bi u u grandparent e ti constant matrix turn affects
exactly one constant matrix next higher level since effect
level computed k time due matrix multiplication log n
levels equations update accomplished k log n time constant k
actually pessimistic faster matrix multiplication exist
update procedure given figure update initially called update e
e e leaf level raked e evidence
operation start sequence log n calls function update x term
change propagate log n equations


fiqueries updates probabilistic networks

function query x
look equation associated x tind x
case x leaf equation form x e e known
case return e
case equation associated x form

x ai x bi x z
z leaf therefore z known case return
ai x query bi x z
case leaf analogous
figure function compute value node

queries

relatively easy use similar recursive procedure perform x queries unfortunately yields log n time simply use recursion
calculate terms calculate terms earlier procedure
log n recursive calls calculate values defined equation
involves term taking log n time compute
achieve log n time shall instead implement x queries defining procedure calc x returns triple vectors hp l ri p x l
r z z left right children respectively x ti
compute x node x following let ind x equation
x ti form

x di x u ci x v
u parent x ti v sibling call procedure calc u
return triple h u v x immediately compute x
equation
procedure calc x implemented following fashion
case ti node tree x root children x leaves hence
values known x given sequence prior probabilities x
case x appear ti one x children leaf say e raked
level let z child call calc u u parent
x ti receive back h u z v h u v z according whether x


fidelcher grove kasif pearl

function update term value
one equation ti defining ai bi term
appears right hand side let term matrix defined equation
e left hand side
update term let value value
call update term value recursively
figure update procedure
left right child u ti v u child compute x
u v e z return necessary triple
specifically

x



di x u ai u v
di x u bi u v

choice depends whether x right left child respectively u ti
case x appear ti call calc x returns correct
value x child z x ti remains child x ti returns
correct value z z child x occur ti must
case z raked level one z children say e leaf let
child q situation calc x returned value q
compute

z ai z e bi z q
return value
three cases constant amount work done addition single recursive
call uses equations higher level since log n levels equations
requiring matrix vector multiplication total work done k log n

extended example
section illustrate application specific example consider
sequence contracted trees shown figure corresponding trees


fiqueries updates probabilistic networks

xl










xl








xl


xl







e

xl







e

e



e











xl





xl


xl


xl







z
z

c
c
c
c








xl




z
z
z

e






xl




e






xl


e






e

e

e



xl







e

e

xl







xl


e

e

e

figure example tree contraction








e

e

e

e

fidelcher grove kasif pearl

equations following

x x x b x x







x x x c x x








x x x b x e













x x x c x e








x x x b x e









x x x c x e












x x e b x e






consider instance effect update e since raked immediately
value e incorporated
b x b x diaga x e b x
subsequent rake operations know x depends b x x
depends x must update values follows
x x diagb x e x
x x diagb x e x


















finally consider query x since x raked together e follow
steps outlined generate following calls calc x calc x
calc x calc x provides us x case x
particularly easy compute since x children leaf nodes simply
compute x x normalize giving us conditional marginal distribution
bel x required

join trees

perhaps best known technique computing arbitrary e singly connected
bayesian networks uses idea join trees junction trees lauritzen spiegelhalter
many ways join tree thought causal tree albeit one somewhat
special structure thus previous section applied however
structure join tree permits optimization describe section
becomes especially relevant next section use join tree technique
log n updates queries done arbitrary polytrees review
join trees utility extremely brief quite incomplete clear expositions
see instance spiegelhalter et al pearl
given bayesian network first step towards constructing join tree moralize
network insert edges every pair parents common node treat


fiqueries updates probabilistic networks

edges graph undirected spiegelhalter et al resulting undirected
graph called moral graph interested undirected graphs chordal
every cycle length contain chord e edge two nodes
non adjacent cycle moral graph chordal necessary add
edges make techniques triangulation stage known instance
see spiegelhalter et al
p probability distribution represented bayesian network g v e
v f moralizing triangulating g
jv j cliques say c cjv j
cliques ordered j

ci cj ci c c ci
tree formed treating cliques nodes connecting node ci
parent cj called join tree
p





p cijcj

p cijcj p cijcj ci
see direct edges away parent cliques
resulting directed tree fact bayesian causal tree represent original
distribution p true matter form original graph course
price cliques may large domain size number possible values
clique node exponential size technique guaranteed
ecient
use rake technique section directed join tree without
modification however property shows conditional probability matrices
join tree special structure use gain eciency
following let k domain size variables g usual let n maximum
size cliques join tree without loss generality assume cliques
size add dummy variables thus domain size
clique k kn finally let c maximum intersection size clique parent
e jcj cij l kc
standard would represent p cijcj k k matrix mc jc
however p ci jcj ci represented smaller l k matrix mc jc c
property mc jc identical mc jc c except many rows repeated
thus k l matrix j








j



j





j

mc jc j mc jc


j

j

j

ci

j actually simple matrix whose entries exactly one per row however
use fact
clique maximal completely connected subgraph



fidelcher grove kasif pearl

claim case join trees following true first matrices

ai bi used rake stored factored form product
two matrices dimension k l l k respectively instance factor ai
ali ari never need explicitly compute store full matrices
seen claim true matrices factor way proof
uses inductive argument illustrate second claim

matrices stored factored form matrix multiplications used
rake one following types l k matrix times k l matrix
l k matrix times k k diagonal matrix l l matrix times l k
matrix l k matrix times vector
prove claims consider instance equation defining bi terms lowerlevel matrices section bi u bi u diaga x e bi x assumption

bil u bir u diag x x e bil x bil x
associativity clearly equivalent
h

bil u bir u diaga x x e bil x bil x
however every multiplication expression one forms stated earlier identifying
bil u bil u bir u bracketed part expression proves case
course case rake left child ai u updated analogous
thus even straightforward technique matrix multiplication cost
updating bi kl kn c contrasts k factor
matrices may represent worthwhile speedup c small note overall time
update scheme kn c log n queries involve matrix
vector multiplication require kn c log n time
many join trees difference n log n unimportant
clique domain size k often enormous dominates complexity indeed k l
may large cannot represent required matrices explicitly course
cases technique little offer cases benefits
worthwhile important general class immediate
reason presenting technique join trees case polytrees


l


r


l


r


polytrees

polytree singly connected bayesian network drop assumption section
node one parent polytrees offer much exibility causal
trees yet well known process update query n time
causal trees reason polytrees extremely popular class networks
suspect possible present log n updates queries
polytrees direct extension ideas section instead propose different
technique involves converting polytree join tree ideas
preceding section basis simple observation join tree
polytree already chordal thus detail little lost considering
join tree instead original polytree specific property polytrees
require following omit proof well known proposition


fiqueries updates probabilistic networks

proposition moral graph polytree p v e chordal
set maximal cliques ffv g parents v v v g
let p maximum number parents node proposition every
maximal clique join tree p variables domain size node
join tree k kp may large recall conditional probability
matrix original polytree variable p parents k entries anyway since
must give conditional distribution every combination node parents thus k
really measure size polytree
follows proposition perform query update
polytrees time k log n simply section directed
join tree noted section better recall savings depend
c maximum size intersection node parent join tree
however join tree formed polytree two cliques share
single node follows immediately proposition two cliques
one node common must two nodes share one parent
else node one parents share yet another parent neither
consistent network polytree thus complexity bounds section
put c follows process updates kk c log n kp log n
time queries kp log n

application towards automated site specific muta genesis

experiment commonly performed biology laboratories procedure
particular site protein changed e single amino acid mutated
tested see whether protein settles different conformation many cases
overwhelming probability protein change secondary structure outside
mutated region process often called muta genesis delcher et al developed
probabilistic model protein structure basically long chain length
chain varies nodes nodes network protein structure
nodes ps nodes evidence nodes e nodes ps node network discrete
random variable xi assumes values corresponding descriptors secondary sequence
structure helix sheet coil ps node model associates evidence node
corresponds occurrence particular subsequence amino acids particular
location protein
model protein structure nodes finite strings alphabet fh e c g
example string hhhhhh string six residues helical conformation
eecc string two residues sheet conformation followed two residues folded
coil evidence nodes nodes contain information particular region
protein thus main idea represent physical statistical rules form
probabilistic network
first set experiments converged following model clearly
biologically naive seems match prediction accuracy many existing approaches
neural networks network looks set ps nodes connected chain
node connect single evidence node experiments ps nodes strings
length two three alphabet fh e c g evidence nodes strings


fidelcher grove kasif pearl


cc


gs


ch

sa


hh








figure example causal tree model pairs showing protein segment gsat
corresponding secondary structure cchh
length set amino acids following example clarifies representation
assume string amino acids gsat model string network comprised
three evidence nodes gs sa three ps nodes network shown figure
correct prediction assign values cc ch hh ps nodes shown
figure
probabilistic model test robustness protein
whether small changes protein affect structure certain critical sites
protein experiments probabilistic network performs simulated evolution
protein namely simulator repeatedly mutates region chain tests
whether designated sites protein coiled helix predicted
remain conformation main goal experiment test stable bonds far
away mutated location affected previous delcher et al
support current thesis biology community namely local distant changes
rarely affect structure
presented previous sections perfectly suited
type application predicted generate factor improvement
eciency current brute force implementation presented delcher et al
change propagated throughout network

summary
proposed several yield substantial improvement
performance probabilistic networks form causal trees updating procedures
absorb sucient information tree query procedure compute
correct probability distribution node given current evidence addition
procedures execute time log n n size network
expected generate orders magnitude speed ups causal trees contain long
paths necessarily chains matrices conditional probabilities
relatively small currently experimenting singly connected
networks polytrees likely dicult generalize techniques general
networks since known general inference probabilistic networks
np hard cooper obviously possible obtain polynomial time incremental


fiqueries updates probabilistic networks

solutions type discussed general probabilistic networks
natural open question extending developed dynamic
operations probabilistic networks addition deletion nodes modifying
matrices conditional probabilities learning
would interesting investigate practical logarithmic time parallel probabilistic networks realistic parallel computation one
main goals massively parallel ai produce networks perform real time
inference large knowledge bases eciently e time proportional depth
network rather size network exploiting massive parallelism jerry
feldman pioneered philosophy context neural architectures see stanfill
waltz shastri feldman ballard achieve type performance neural network framework typically postulate parallel hardware
associates processor node network typically ignores communication requirements careful mapping parallel architectures one indeed achieve ecient
parallel execution specific classes inference operations see mani shastri
kasif kasif delcher techniques outlined presented
alternative architecture supports fast sub linear time response capability
sequential machines preprocessing however obviously limited
applications number updates queries time constant one would
naturally hope develop parallel computers support real time probabilistic reasoning
general networks

acknowledgements
simon kasif johns hopkins university sponsored part national
science foundation grants iri iri iri

references

berger ye z entropic aspects random fields trees ieee trans
information theory
chelberg uncertainty interpretation range imagery proc intern
conference computer vision pp
cohen r f tamassia r dynamic trees applications proceedings
nd acm siam symposium discrete pp
cooper g computational complexity probabilistic inference bayes
belief networks artificial intelligence
delcher kasif improved decision making game trees recovering
pathology proceedings national conference artificial intelligence
delcher l kasif goldberg h r hsu b probabilistic prediction protein secondary structure causal networks proceedings international
conference intelligent systems computational biology pp


fidelcher grove kasif pearl

duda r hart p pattern classification scene analysis wiley york
feldman j ballard connectionist properties cognitive
science
frederickson g n data structure dynamically maintaining rooted trees
proc th annual symposium discrete pp
hel werman absolute orientation uncertain data unified
proc intern conference computer vision pattern recognition
pp
karp r ramachandran v parallel shared memory machines
van leeuwen j ed handbook theoretical computer science pp
north holland
kasif parallel complexity discrete relaxation constraint networks
artificial intelligence
kasif delcher analysis local consistency parallel constraint networks
artificial intelligence
kosaraju r delcher l optimal parallel evaluation tree structured
computations raking reif j h ed vlsi architectures
proceedings aegean workshop computing pp springer verlag
lncs
lauritzen spiegelhalter local computations probabilities graphical
structures applications expert systems j royal statistical soc ser b

mani shastri l massively parallel reasoning large knowledge
bases tech rep intern computer science institute
miller g l reif j parallel tree contraction application proceedings
th ieee symposium foundations computer science pp
pearl j probabilistic reasoning intelligent systems morgan kaufmann
peot shachter r fusion propagation multiple observations
belief networks artificial intelligence
rachlin j kasif salzberg aha towards better understanding
memory bayesian classifiers proceedings eleventh international
conference machine learning pp brunswick nj
shastri l computational model tractable reasoning taking inspiration
cognition proceeding intern joint conference artificial intelligence
aaai


fiqueries updates probabilistic networks

spiegelhalter dawid lauritzen cowell r bayesian analysis expert
systems statistical science
stanfill c waltz toward memory reasoning communications
acm
wilsky multiscale representation markov random fields ieee trans signal
processing




