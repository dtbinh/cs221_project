journal artificial intelligence

submitted published

partially controlled multi agent systems
ronen brafman

brafman cs ubc ca

computer science department
university british columbia
vancouver b c canada v l z

moshe tennenholtz

moshet ie technion ac il

industrial engineering management
technion israel institute technology
haifa israel

abstract

motivated control theoretic distinction controllable uncontrollable
events distinguish two types agents within multi agent system controllable
agents directly controlled system designer uncontrollable agents
designer direct control refer systems partially
controlled multi agent systems investigate one might uence behavior
uncontrolled agents appropriate design controlled agents particular
wish understand naturally described terms methods
applied uence uncontrollable agents effectiveness methods
whether similar methods work across different domains game theoretic framework
studies design partially controlled multi agent systems two contexts
one context uncontrollable agents expected utility maximizers
reinforcement learners suggest different techniques controlling agents
behavior domain assess success examine relationship

introduction
control agents central topic two engineering fields artificial intelligence ai discrete events systems des ramadge wonham one
particular area fields concerned multi agent environments
examples include work distributed ai bond gasser work decentralized
supervisory control lin wonham fields developed
techniques incorporated particular assumptions hence
natural techniques assumptions used one field may adopted
may lead insights field
difference ai work multi agent systems work decentralized discrete
event systems distinguishes controllable uncontrollable events controllable
events events directly controlled system designer uncontrollable events directly controlled system designer translating terminology context multi agent systems introduce distinction two
types agents controllable agents directly controlled system designer
uncontrollable agents designer direct control leads
c ai access foundation morgan kaufmann publishers rights reserved

fibrafman tennenholtz

naturally concept partially controlled multi agent system pcmas
following design challenge ensuring agents system behave appropriately
adequate design controllable agents believe many
naturally formulated instances pcmas design goal characterize important
instances design examine tools used solve
assess effectiveness generality tools
distinguishes partially controlled multi agent systems ai context similar des structural assumptions make uncontrolled agents
involved unlike typical des concerned physical processes devices ai particularly interested self motivated agents two concrete examples
rational agents e expected utility maximizers learning agents e g reinforcement learners indeed examples constitute two central self motivated
agents game theory decision theory referred educative evolutive
e g see gilboa matsui special nature uncontrollable agents
special structure uncontrollable events induce differentiates pcmas
corresponding des literature difference raises questions
suggests perspective design multi agent systems particular calls
techniques designing controllable agents exploiting structural assumptions
uence behavior uncontrollable agents lead system desired
behavior
order understand issues study two stated solved
adopting perspective pcmas design
interest large community goal uence
behavior agents control exert uence indirectly
choosing suitable behaviors agents direct control one case
attempt uence behavior rational agents case try
uence learning agents
first study concerned enforcement social laws number
agents designed different designers work within shared environment beneficial
impose certain constraints behavior overall system function
better example shoham tennenholtz imposing certain trac
laws considerably simplify task motion robot still
enabling ecient motions indeed see later conventions heart
many coordination techniques multi agent systems yet without suitable mechanisms
rational agents may incentive follow conventions
certain cases use perspective partially controlled multi agent systems
structural assumption rationality enforce conventions
second study involves two agent system consisting teacher student
teacher knowledgeable agent student agent learning
behave domain goal utilize teacher control
improve behavior student controlled us hence
instance partially controlled multi agent systems structural assumption
uncontrolled agent employs particular learning
studies presented suggest techniques achieving satisfactory system
behavior design controllable agents relevant techniques


fion partially controlled multi agent systems

experimentally assessed beyond formulation solution two interesting multi agent system design suggests general perspective certain
design although feel still premature draw general conclusion
potential general theory pcmas design certain concepts punishment
reward suggest central area
organized follows section describe enforcing
social behavior multi agent systems section describe standard game theoretic
model suggest mechanism threats punishments general
tool class issues pertain design threats punishments
discussed section section introduces second case study pcmas design
embedded teaching reinforcement learners context teacher learner
embedded shared environment teacher serving controller whose
aim direct learner desired behavior formal model
introduced section section derive optimal teaching policies
certain assumptions viewing teaching markov decision process effectiveness
different teaching policies studied experimentally section finally section
examine relationship methods used two domains
possibility general methodology designing partially controlled multi agent systems
conclude section summary discussion related work

enforcement social behavior
section introduce enforcement social laws multi agent
context proposed solution falls naturally pcmas design perspective
take explain motivate particular social law enforcement
solution sections formalize investigate
framework general game theoretic model
use following scenario illustrate
hired design working environment artificial
agents part job involves designing number agents use
maintain warehouse agents designed different designers
warehouse obtain equipment make sure different agents
designed different designers operate eciently environment
choose introduce number social laws constraints behavior
agents help agents coordinate activities domain
rules include number trac laws regulating motion domain
well law specifies every tool used agent must
returned designated storage area robots programmed follow
laws expect others laws quite successful allow ecient activity warehouse designer arrives
pressed corporate bosses deliver better performance decides
exploit rules designs agent locally maximize performance
regardless social laws


fibrafman tennenholtz

multi participant environments one agent might
dynamic goals interested finding ways agents coexist
achieving goals several approaches coordination agent activity discussed
distributed systems dai literature examples protocols reaching
consensus dwork moses rational deals negotiations zlotkin rosenschein
kraus wilkenfeld rosenschein genesereth organizational structures durfee lesser corkill fox malone social laws moses
tennenholtz shoham tennenholtz minsky briggs cook
methods behavior agent predetermined prescribed
certain stage example content deal reached outcome
negotiation process completed social law instituted work
relies assumption agents follow prescribed behaviors e g obey
law stick agreement assumption central success
methods however makes agents follow rules vulnerable rational agent
performs local maximization payoff exploiting knowledge others follow
rules example designer may program robot return tools
saving time required thus causing agents fail tasks
despite somewhat futuristic avor although instances shared environments
beginning appear cyberspace scenario useful illustrating vulnerability
popular coordination mechanism appearing multi agent literature
within ai e g see bond gasser assume agents involved
fully rational aside note case actually need attribute much
intelligence agents sucient assume designers
design way maximizes utility disregarding utility
agents
order handle need modify existing design paradigms
adopting perspective partially controlled multi agent systems obtain one possible
handle requires making following basic assumption
original designer scenario controls number reliable agents
basic idea reliable agents designed punish agents
deviate desirable social standard punishment mechanism hardwired unchangeable common knowledge agents controlled
original designer aware punishment possibility punishment
mechanism well designed deviations social standard become irrational
deviation actually occur punishment actually executed hence
making agents bit sophisticated prevent temptation breaking
social laws
suggested solution adopt perspective partially controlled multi agent systems agents controllable others uncontrollable assumed
adopt basic model expected utility maximization punishment mechanism
part control strategy used uence behavior uncontrolled
agents
ease exposition assume reliable agents follow designer instructions assume
non malicious failures crash failures possible



fion partially controlled multi agent systems

dynamic game theoretic model

section introduce basic game theoretic model use study
enforcement social behavior solution later sections
model used study embedded teaching wish emphasize model
use common model representing emergent behavior population
e g huberman hogg kandori mailath rob altenberg feldman
gilboa matsui weidlich haag kinderman snell

definition k person game g defined k dimensional matrix size n

nk nm number possible actions strategies th agent entries
vectors length k real numbers called payoff vectors joint strategy
tuple ik j k case ij nj
intuitively dimension matrix represents possible actions one k
players game following convention used game theory often use term
strategy place action since dimensions matrix n nk th
agent ni possible strategies choose j th component vector residing
ik cell e mi ik represents feedback player j receives
players joint strategy ik agent strategy im
k use term joint strategy refer combined choice
strategies agents

definition n k g iterative game consists set n agents given k person
game g game g played repetitively unbounded number times iteration
random k tuple agents play instance game members k tuple
selected uniform distribution set agents

every iteration n k g game represents local interaction k agents agents
play particular iteration game must choose strategy use
interaction agent use different strategies different interactions outcome
iteration represented payoff vector corresponding agents joint strategy
intuitively payoff tells us good outcome joint behavior point
view agent many situations represented n k g game example
trac aspect multi agent system represented n k g game
time number agents meet intersection encounter instance
game agents choose number strategies e g move ahead yield
payoff function gives utility set strategies example time
two agents meet agents choose move ahead collision occurs payoffs
low
definition joint strategy game g called ecient sum players payoffs
maximal
use term emergent behavior classical mathematical economics interpretation
evolution behavior repetitive local interactions usually pairs agents
agent may change strategy following interactions feedback received previous
interactions



fibrafman tennenholtz

hence eciency one global criterion judging goodness outcomes
system perspective unlike single payoffs describe single agent perspective

definition let fixed joint strategy given game g payoff pi player
instance g joint strategy played pi pi say
punishment w r pi pi otherwise say benefit w r
pi pi
hence punishment benefit w r joint strategy measure gain benefit
loss punishment agent somehow change joint behavior agents

current discussion punishment benefit respect chosen
ecient solution
designers multi agent system would prefer ecient possible
cases entails behavior sense unstable individual agents
may locally prefer behave differently thus agents may need constrained behave
way locally sub optimal refer constraints exclude
possible behaviors social laws
due symmetry system assumption agents
rational utility additive e utility two outcomes sum
utilities clear agent expected payoff higher one obtained
strategies giving ecient solution thus clear case ecient
solution fair sense agents get least could law
existed solution provide better expected payoff
however good intentions designer creating environment beneficial
participating agents may backfire social law provides information behavior
agents conforming information agents respective designers
use increase expected payoff
example assume playing n g game g prisoner dilemma
represented strategic form following matrix
agent
agent






ecient solution game obtained players play strategy assume
solution chosen original designer followed agents
control
designer agent function environment social law
obeyed may tempted program agent conform chosen law instead
program agent play strategy maximizes expected outcome strategy
addition payoffs utilities across agents dangerous practice however particular model
shown system joint strategies ecient maximizes agent expected
cumulative rewards



fion partially controlled multi agent systems

agent obtain payoff playing one good agents
thus even though social law accepted order guarantee payoff
agent good agents obtain payoff playing non conforming
agents note designer exploits information strategies good players
dictated social law agents controlled designer uncontcolable
agents behavior dictated original designer
agents conforming social law referred malicious agents order
prevent temptation exploit social law introduce number punishing
agents designed initial designer play irrationally detect behavior
conforming social law attempting minimize payoff malicious agents
knowledge future participants punishment policy would deter deviations eliminate need carrying hence punishing behavior used
threat aimed deterring agents violating social law threat part
control strategy adopted controllble agents order uence behavior
unconrollable agents notice control strategy relies structural assumption
unconrollable agents expected utility maximizers
define minimized malicious payoff minimal expected payoff malicious players guaranteed punishing agents punishment exists
minimized malicious payoff lower expected payoff obtained playing according
social law strategy guarantees malicious agents expected payoff lower
one obtained playing according social law called punishing strategy
throughout section following section make natural assumption
expected payoff malicious agents playing greater
one obtained ecient solution
example continued example punishment would simply play strategy
may cause payoff punishing agent decrease would
guarantee malicious agent obtains payoff better playing punishing
agent many non malicious agents punishing malicious agents expected payoff
would decrease become smaller payoff guaranteed social law strategy
would punishing strategy

design punishments

previous section described general model multi agent interaction showed
perspecive partially controlled multi agent systems leads one possible solution
enforcing social behavior setting via idea threats
punishments proceed examine issue punishment design
assume p agents designer controls ability
observe instances game occur informed outcome
games c additional agents conform law play strategies
entailed chosen ecient solution malicious agents bound
law
assumptions may treated similarly



fibrafman tennenholtz

would answer questions game offer ability punish
minimized malicious payoff optimal ratio p c
difference different social laws
example continued consider example observed
cause expected maximal loss malicious agents occurs
punishing agents play strategy gain malicious agent makes
playing agent following social law order punishing
strategy effective must case expected payoff malicious agent
greater expected payoff obtained following social law order
achieve must ensure ratio punishing conforming agents
malicious agent sucient encounters punishing agents case assuming
deviators meet expected benefit recalling agent equally
likely meet agent need pc make incentive deviate negative
implementing punishment requires complex behavior agents
must able detect deviations well switch punishing strategy
whole behavior viewed complex social law calls
complex agents carry makes programming task harder
clearly would minimize number complex agents keeping
benefit malicious behavior negative major question ratio
benefit deviation prospective punishment
seen example larger punishment smaller number
sophisticated punishing agents needed therefore would
strategies minimize malicious agent payoff order require
additional definitions

definition two person game g zero sum game every joint strategy
players sum players payoffs

hence zero sum game win win situations larger payoff one
agent smaller payoff agent convention payoff matrix two
person zero sum game mention payoffs player

definition let g two person game let pig payoff player g
f g strategies played player respectively projected

game gp following two person zero sum game strategies players
g payoff matrix p gp p g define transposed game g g
game g roles players change

projected game first agent payoff equals negated value second agent
payoff original game thus game ects desire lower payoffs
second player original game
give general two person game g number strategies
make use following standard game theoretic definition


fion partially controlled multi agent systems

definition given game g joint strategy players nash equilibrium

g whenever player takes action different action payoff given
players play higher payoff given everybody plays
strategy nash equilibrium game agent obtain better payoff
unilaterally changing behavior agents play according
nash equilibrium central notion theory non cooperative games luce
raiffa owen fudenberg tirole notion well studied
understood reducing concepts basic concept may quite useful
design perspective particular nash equilibrium exists finite games
payoffs prescribed nash equilibria given zero sum game uniquely defined


theorem given n g iterative game minimized malicious payoff achieved

playing strategy player prescribed nash equilibrium projected game gp
playing player g strategy player prescribed nash equilibrium
projected game g p playing player g

proof assume punishing agent plays role player player adopts

strategy prescribed nash equilibrium player get better payoff
one guaranteed since deviation player improve situation
definition nash equilibrium hand player cause harm
harm obtained playing strategy see assume player uses
arbitrary strategy player adopts strategy prescribed outcome
player higher one guaranteed playing nash equilibrium
definition nash equilibrium addition due fact
zero sum game implies outcome player lower one
guaranteed player would play according case punishing agent
player treated similarly

example continued continuing prisoner dilemma example gp would
agent
agent




nash equilibrium attained playing strategies yielding example
g p gp therefore punishing strategies strategy case

corollary let n g iterative game p punishing agents let v v

payoffs nash equilibria gp gpt respectively case uniquely
defined let b b maximal payoffs player obtain g g respectively
notice cases strategies prescribed original game determined strategies
player nash equilibria projected games



fibrafman tennenholtz

assuming player obeying social law let e e payoffs player
respectively g players play according ecient solution prescribed
social law finally assume expected benefit two malicious agents meet
necessary sucient condition existence punishing strategy
n p b b p v v e e
n
n

proof expected payoff obtained malicious agent encountering law

abiding agent b b expected payoff encountering punishing agent v v
order test conditions existence punishing strategy would need
consider best case scenario point view malicious agent case
non punishing agents law abiding agents order obtain expected utility
malicious agent make average quantities taking account
proportion law abiding punishing agents population gives us
expected utility malicious agent n n p b b np v v definition
punishing strategy exists expected utility lower expected
utility guaranteed social law since expected utility guaranteed
social law e e get desired
value punishment v v independent ecient solution
chosen e e identical ecient solutions definition however b b depends
choice ecient solution number solutions exist minimizing
b b important consideration design social law affects incentive
cheat








example let look slightly different version prisoner dilemma game
matrix

agent
agent






ecient solutions given joint strategies
case b b gained playing strategy instead case
b b
clearly incentive deviate social law prescribing strategies
social law prescribing
summarize preceding discussion suggests designing number punishing agents
whose behavior punishment mode prescribed theorem case n g games
ensuring sucient number agents take away incentive deviate
social laws hence given malicious agents rational follow social
norm consequently need utilize punishment mechanism
observed different social laws leading solutions equally ecient different
properties comes punishment design consequently assumption
would minimize number punishing agents guaranteeing ecient


fion partially controlled multi agent systems

solution participants choose ecient solution minimizes value
b b

embedded teaching
section move second study pcmas design
uncontrollable agent reinforcement learner choice arbitrary rational
agents reinforcement learners two major types agents studied mathematical
economics decision theory game theory types agents discussed
work dai concerned self motivated agents e g zlotkin rosenschein
kraus wilkenfeld yanco stein sen sekaran hale
agent ability function environment greatly affected knowledge
environment special cases design agents sucient knowledge
performing task gold general agents must acquire information line
order optimize performance e must learn one possible
improving performance learning employing teacher example
lin uses teaching example improve performance agents supplying
examples task achieved tan work
viewed form teaching agents share experiences methods nontrivial form communication perception required strive model broad notion
teaching encompasses behavior improve learning agent performance
wish conduct general study partially controlled multi agent systems
uncontrollable agent runs learning time want
model clearly delineate limits teacher e controlling agent ability
uence student
propose teaching maintains situated spirit much
reinforcement learning sutton watkins kaelbling call
embedded teaching embedded teacher simply knowledgeable controlled agent
situated student shared environment goal lead student
adopt specific behavior however teacher ability teach restricted
nature environment share repertoire actions limited
may lack full control outcome actions example consider
two mobile robots without means direct communication robot familiar
surroundings robot situation robot help robot reach
goal certain actions blocking robot headed wrong
direction however robot may limited control outcome
interaction uncertainty behavior robot control uncertainty
nevertheless robot specific structure learner obeying learning scheme
attempt control indirectly choice actions robot
differentiate teacher student use female pronouns former male pronouns
latter
general fact agent controllable imply perfectly control outcome
actions choice hence robot may controllable sense running program
supplied us yet move forward command may desired outcome



fibrafman tennenholtz

follows goal understand embedded teacher help student
adopt particular behavior address number theoretical questions relating
experimentally explore techniques teaching two types
reinforcement learners

basic teaching setting
consider teacher student repeatedly engage joint activity
student prior knowledge pertaining activity teacher understands
dynamics model teacher goal lead student adopt particular
behavior interactions example teacher student meet occasionally
road teacher wants teach student drive right side perhaps
teacher student share resource cpu time goal teach
judicious use resource model encounters g iterative games
capture idea teacher knowledgeable student assume
knows structure game e knows payoff function
recognizes actions taken play hand student know
payoff function although perceive payoff receives make
simplifying assumptions teacher student two actions
choose outcome depends choice actions furthermore
excluding study section ignore cost teaching hence omit
teacher payoff description provides basic setting take
first step towards understanding teaching
teaching model concisely modeled matrix teacher actions
designated ii student actions designated numbers
entry corresponds joint action represents student payoff
joint action played suppose matrix figure
wish teach student use action stage assume student
receives better payoff following action learn play
see situations teaching trivial assume first row dominates second row e c b case student naturally prefer
take action teaching challenging although might useful speeding
learning process example c b matrix b figure teacher
make advantage action noticeable student playing action

suppose one c b holds case teaching still easy
use basic teaching strategy call preemption preemption teacher
chooses action makes action look better action example
situation described matrix c figure teacher choose action
case could made inherent value teaching may appropriate forum
airing views
fact idea consider basic embedded teaching setting already challenging later see basic setting closely related fundamental issue non cooperative
games



fion partially controlled multi agent systems

ii

ii

b



c


ii



c


b

ii







ii



e

figure game matrices b c e teacher possible actions ii
student possible actions
next assume c greater b matrix figure
regardless action teacher chooses student receives higher payoff
playing action since minf g maxf g therefore matter teacher
student learn prefer action teaching hopeless situation
types interactions isomorphic case c b
matrix e figure still challenging situation teacher action
dominates action therefore preemption cannot work
teaching strategy exists complex choosing action
since seems challenging teaching situation devote attention
teaching reinforcement learner choose action class games
turns situation quite important game theory multi agent
interaction projection famous game prisoner dilemma discussed
previous sections general represent prisoner dilemma
following game matrix
teacher
student coop defect
student coop
coop b c commonly coop
defect c b
defect c c

teacher
defect
c c


c b actions prisoner dilemma called cooperate coop
defect identify coop actions defect actions ii
prisoner dilemma captures essence many important social economic situations
particular encapsulates notion cooperation thus motivated enormous discussion among game theorists mathematical economists overview see eatwell
milgate newman prisoner dilemma whatever choice one player
second player maximize payoff playing defect thus seems rational
player defect however players defect payoffs much worse
cooperate


fibrafman tennenholtz

example suppose two agents given moving object
agent perform task alone take amount time energy
value however together effort make valued get
following instance prisoner dilemma
agent
agent move
rest
move

rest
experimental part study teacher task teach student
cooperate prisoner dilemma measure success teaching strategy
looking cooperation rate induces students period time
percentage student actions coop experimental presented
involving prisoner dilemma respect following matrix
teacher
student coop defect
coop
defect
observed qualitatively similar instantiations prisoner
dilemma although precise cooperation rate varies

optimal teaching policies

previous section concentrated modeling teaching context instance
partially controlled multi agent system determining particular
interesting section start exploring question teacher
teach first define optimal policy define markov decision
processes mdp bellman certain assumptions teaching
viewed mdp allow us tap vast knowledge accumulated
solving particular use well known methods value
iteration bellman optimal teaching policy
start defining optimal teaching policy teaching policy function
returns action iteration possibly may depend complete history
past joint actions right definition optimal policy teacher
motivation may vary however teacher objective maximize
number iterations student action good coop prisoner
dilemma teacher know precise number iterations playing
slightly prefers earlier success later success
formalized follows let u value teacher places student
action let teacher policy assume induces probability distribution
pr k set possible student actions time k define value strategy


x
val kek u
k



fion partially controlled multi agent systems

ek u expected value u

ek u

x pr



k u

student set actions teacher goal strategy
maximizes val discounted expected value student actions example
case prisoner dilemma could
fcoop defectg u coop u defect
next define mdps mdp decision maker continually moving
different states point time observes current state receives payoff
depends state chooses action action current state
determine perhaps stochastically next state goal maximize function
payoffs formally mdp four tuple hs p ri state space
decision maker set possible actions p probability
transition states given decision maker action r reward
function notice given initial state policy decision maker p
induces probability distribution ps k ps k probability
kth state obtained current state
optimal policy mdp policy maximizes state
discounted sum expected values payoffs received future states starting
e

x
x
k ps k r
k




although may immediately obvious single policy maximizing discounted sums
starting state exists well known ways finding policy
experiments use method value iteration bellman
suppose student set possible states set actions
teacher set actions moreover suppose following
properties satisfied
student state function old state current joint action
denoted
student action stochastic function current state probability
choosing state
teacher knows student state natural way happen
teacher knows student initial state function outcome game
uses simulate agent
notice assumptions teaching policy function
know student next action function next state know
student next state function current state current action teacher
current action hence next action function current state action
well teacher current action however know student current action
function current state hence student next action function
current state teacher current action implies knowledge
teacher needs optimally choose current action student current state


fibrafman tennenholtz

additional information redundant cannot improve success generally
repeat line reasoning indefinitely future see teacher
policy function student state function
possible see makings following mdp
given observation three assumptions see indeed teacher
policy induces probability distribution set possible student actions time k
implies definition val makes sense
define teacher mdp tmdp h p u

x

p def







j defined j otherwise probability transition
sum probabilities student actions induce
transition reward function expected value u

x

u def




u

theorem optimal teaching policy given optimal policy tmdp
proof definition optimal policy tmdp policy
maximizes



x
x
k p


k



k u




x
x
k p

k

however equal






k




x


u


x
x x p
k

k









k u

know ps k probability state student time
k given teacher uses current state hence

x p







k

probability action taken student time k given initial
current state upon examination see identical val
optimal policy used teaching teacher possess sucient information determine current state student even case
allows us calculate upper bound success val teaching policy
number property learning measures degree uence
agent given student


fion partially controlled multi agent systems

experimental study

section describe experimental study embedded teaching first define
learning schemes considered describe set obtained computer
simulations

learning schemes

experiment two types students one uses reinforcement learning
viewed q learning one state uses q learning choosing
parameters students tried emulate choices made reinforcement learning
literature
first student call blind q learner bql perceive rewards
cannot see teacher acted remember past actions keeps
one value action example q coop q defect case prisoner
dilemma update rule following performed action received reward
r
qnew qold r
parameter learning rate fixed unless stated otherwise experiments wish emphasize although bql bit less sophisticated real
reinforcement learners discussed ai literature defined popular powerful type learning rule much discussed used literature
narendra thathachar
second student q learner ql observe teacher actions
number possible states ql maintains q value state action pair
states encode recent experiences e past joint actions update rule
qnew qold r v
r reward received upon performing state state student
following performance called discount factor unless
otherwise noted v current estimate value best policy
defined maxa q q values initially set zero
student update rule tells us q values change experiences must specify q values determine behavior ql
bql students choose actions boltzmann distribution distribution
associates probability ps performance action state p
bql
exp q
q
def
p

ql

p




bql
ps def
p exp exp

q

exp q
called temperature usually one starts high value
makes action choice random inducing exploration part student
slowly reduced making q values play greater role student choice action
use following schedule n n schedule
characteristic properties fast initial decay slow later decay experiment
fixed temperature






fibrafman tennenholtz

approximately optimal policy


iterations
iterations
iterations
iterations



fraction coops

fraction coops



two q learners





iterations
iterations
iterations
iterations


















temperature














temperature





figure fraction coops function temperature approximately optimal
policy left teaching identical q learner right curve
corresponds coop rate fixed number iterations approx
optimal policy curves iterations nearly identical

blind q learners

motivated discussion section concentrate section
following section teaching context prisoner dilemma section
discuss another type teaching setting section describes experimental
bql examined policy approximates optimal policy two teaching
methods rely student model
optimal policy

first bqls fit student model section state space use
set possible assignment q values continuous subspace
discretize order able compute policy obtaining state space
approximately states next notice transitions stochastic function
current state current q values teacher action see notice q value
updates function current q value payoff payoff function
teacher student actions student actions stochastic function
current q value left side figure see success teaching policy
generated dynamic programming solve optimization curve
represents fraction coops function temperature fixed number
iterations values means experiments
two q learners

ran experiments two identical bqls viewed teaching
another q learner shown right side figure temperatures
optimal strategy performs better q learning teaching strategy fact
temperatures less success rate approaches beneficial later
add temperature decay however see inherent limit ability


fion partially controlled multi agent systems

tit tat


iterations
iterations
iterations
iterations



fraction coops

fraction coops



tit tat





iterations
iterations
iterations
iterations


















temperature














temperature





figure fraction coops function temperature teaching strategy
tft left tft right
affect behavior higher temperatures interesting phenomenon phase
transition observed around qualitative explanation phenomenon
high temperature adds randomness student choice action makes
probabilities p less extreme consequently ability predict student behavior
lessens probability choosing good action however randomness
serves lower success rate initially guarantees level effective cooperation
temperature increases finally notice although
coop coop seems best joint action pair agents two interacting q learners
never learn play joint strategy consistently although coops
low temperatures
teaching without model

teacher precise model student cannot use techniques
section derive optimal policy assume teacher
observe student current state e knows student q values
therefore explore two teaching methods exploit knowledge game
fact student bql
methods motivated basic strategy countering student move
basic idea try counter good actions student action lead
high payoff counter bad actions action give low payoff
ideally would play coop student plays coop defect
student plays defect course know action student choose
try predict past actions
assume q values change little one iteration
student likely action next game action took
recent game therefore saw student play coop previous turn play
coop similarly teacher follow defect student defect


fibrafman tennenholtz

fraction coops time

fraction coops





approximately optimal
q learning
tit tat
tit tat






iterations





figure fraction coops function time bql temperature decay
scheme section teaching strategies shown approximately optimal strategy q learning tft tft
part strategy called tit tat tft short well known eatwell et al
experiments successful teaching bql see figure
experimented variant tft call tft strategy
teacher plays defect observing two consecutive defects part student
motivated observation certain situations better let student
enjoy free lunch match defect coop make coop look bad
may cause q value coop low unlikely
try two consecutive defects indicate probability student playing
defect next quite high shown figure indicate strategy worked
better tft ranges temperature better q learning however
general tft tft gave disappointing
finally figure shows performance four teaching strategies discussed
far incorporate temperature decay see optimal policy
successful explained teaching easier student predictable
case temperature lower temperature decay student spends
time relatively low temperature behaves similarly case fixed
low temperature initial high temperature phase could altered behavior
observe effects

teaching q learners

unlike bql q learners ql number possible states encode joint actions
previous games played ql memory one four possible states corresponding
four possible joint actions prisoner dilemma ql memory
states encoding sequence joint actions
complex learning architectures structure brings certain
one possible may structure teaching resistant

sense use identical q learner implies model student tft
tft make use model



fion partially controlled multi agent systems

tit tat

two q learners

fraction coops

fraction coops






iterations
iterations
iterations
iterations



iterations
iterations
iterations
iterations


















temperature














temperature





figure curve shows fraction coops ql function temperature
fixed number iterations tft used teach left identical q learner used teach right values means experiments
real threat added computational complexity mentioned approximate
optimal teaching policy bql compute space approximately
discretized states representing state bql requires two numbers one
q value representing state ql states requires numbers
one q value state action pair one encoding current state size
corresponding discretized state space teacher markov decision process grows
exponentially simplest case memory one student four states
would states since solving states took hours
sun sparcstation able approximate optimal teaching policies
even simplest ql
lost structure may mean complexity means
properties exploit reach surprisingly good exploiting structure
q learners moreover teaching method introduced previous
section however ql method takes meaning suggests familiar
notions reward punishment interestingly one may recall punishment
major tool enforcement social behavior
choosing actions qls care immediate rewards
current action effect future rewards makes suitable reward
punishment scheme idea following suppose ql something bad defect
case although cannot reliably counter move move lower
reward punish later choosing action gives negative
payoff matter student plays achieve following student defect
defect teacher immediate reward obtained ql playing defect
may high learn associate subsequent punishment defect action
thus may locally beneficial perform defect may able make
long term rewards defect less desirable similarly follow student coop
reward form coop teacher since guarantees positive payoff
student


fibrafman tennenholtz

fraction coops time

fraction coops



tit tat
q learning









iterations





figure fraction coops ql function time temperature decay tft
q learning teaching strategies
suggests tit tat notice bqls tft cannot understood
reward punishment strategy bqls care immediate outcome
action value associate action weighted average immediate
payoffs generated playing action
figure see success rates tft function temperature well
rates q learning teaching strategy latter case teacher identical
student apparent tft extremely successful especially higher temperatures
interestingly behavior quite different two qls indeed examine
behavior two qls see lesser extent phase change noticed
bqls still exists obtain completely different behavior tft used coop levels
increase temperature reaching almost hence see tft works
better student q learner exhibits certain level experimentation indeed
examine success teaching strategies low temperature see
q learning performs better tft explains behavior tft ql
temperature decay introduced described figure figure ql seems
effective tft probably fact experiment
student temperature quite low time
experiments ql remembers last joint action experimented
ql memory performance worse explained follows
ql memory one fully observable markov decision process
teacher plays tft tft deterministic function previous joint action
know q learning converges optimal policy conditions watkins
dayan adding memory effectively adds irrelevant attributes turn
causes slower learning rate examined whether tft would successful
agents memory two shown success rate
considerably lower tft although better two qls
tft performed well teaching strategy explained motivation
want produce quantitative explanation one used predict
success vary parameters payoff matrix


fion partially controlled multi agent systems

coop rates function dif

fraction coops














dif







figure coop rates function dif b c c b means
experiments iterations student memory
let student payoff matrix matrix figure let p probability
student plays coop let q p probability student plays
defect probabilities function student q values see description
section let us assume probabilities p q change considerably
one iteration next seems especially justified learning rate small
given information student expected reward playing coop
tft teacher current action student previous action assume
teacher play coop probability p thus student expected payoff
playing coop p q b since q learners care discounted future reward
current reward happens next important since assumed
student cooperated teacher cooperate next iteration still
assume p probability student cooperate next student expected
payoff next step p q c ignore higher order terms expected reward
playing coop becomes p q b p q c expected reward defect thus
p c q p b q therefore tft succeed teaching strategy

p q b p q c p c q p b q
since initially p q behavior stage p q approximately equal determine whether tft succeeds attempt predict
success tft whether
dif b c c b
test hypothesis ran tft number matrices q learners different
discount factors figure fraction coops iterations
function dif teacher tft temperature decay see
dif reasonable predictor success almost rates
rates however successful


fibrafman tennenholtz

teaching design tool

section identified class games challenging teach previous
sections mostly devoted exploring teaching strategies games
student q learner one assumptions made teacher trying
optimize function student behavior care
order achieve optimal behavior however often teacher would maximize
function depends behavior student behavior
case even simple games discussed section pose challenge
section examine basic coordination block pushing
objective teaching teaching essential obtaining good
aim section demonstrate point hence value understanding
embedded teaching teaching strategy achieves much
better performance naive teaching strategy leads behavior much better
two reinforcement learners
consider two agents must push block far possible along given path
course time units time unit agent push block along
path gently saving energy hard spending much energy block
move iteration c x h x h units desired direction h c
constants x number agents push hard iteration agents
paid according distance block pushed naturally agents wish work
little possible paid much possible payoff iteration
function cost pushing payment received assume agent prefers
block pushed hard least one agents guaranteeing reasonable
payment agent prefers agent one pushing hard
denote two actions gentle hard get related game described
follows
hard gentle
hard
gentle

notice game falls category games teaching easy
teacher cares student learn push hard simply push
gently however teacher actually trying maximize distance
block moved teaching strategy may optimal notice
instances hard push naive teaching strategy mentioned yield
instances hard push order increase number need
complex teaching strategy
use bql consider following strategy
teacher push gently k iterations start push hard see
right selection k obtain desired behavior student push hard
time total number hard push instances improve dramatically
figure x coordinate corresponds parameter k coordinate
corresponds number hard push instances occur iterations
obtained average trials


fion partially controlled multi agent systems
















figure teaching push hard number hard push instances student
iterations function number iterations teacher
push hard avg trials
see figure eciency system non monotonic
threshold k behavior obtain appropriate selection k much better
would obtained naive teaching strategy interesting
note existence sharp phase transition performance neighborhood
optimal k finally mention agents reinforcement learners
get instances push hard much worse obtained
knowledgeable agent utilizes knowledge uence behavior
agent

towards general theory
two case studies presented raise natural question whether general
domain independent techniques pcmas design exist whether learned
tools case studies believe still premature say whether
general theory pcmas design emerge requires much additional work indeed
given considerable differences exist two domains explored
given large range multi agent systems agents envisioned
doubt existence common low level techniques pcmas design even within
class rational agents investigated agents differ considerably
physical computational memory capabilities decision making
e g expected utility maximization maximization worst case outcomes minimization
regret similarly social law enforcement take different forms
example malicious agents could cooperate among however
abstract view taken certain important unifying concepts appear namely punishment
reward
punishment reward abstract descriptions two types high level feedbacks
controllable agents provide uncontrollable agents although punishment
reward take different form meaning two domains cases uncon

fibrafman tennenholtz

trollable agents seem care controllable agent reaction action
see cases controllable agents uence uncontrollable agents
perception worthiness actions precise manner controllable
agents affect perception differs cases utilizes inherent aspect
uncertainty uncontrollable agent world model case rational agents despite
perfect knowledge dynamics world uncertainty remains regarding
outcome non malicious agents actions fixing certain course action
controllable agents uence malicious agents perception outcome
actions case learning agent one affect perception student
action affecting basic world model hence seems high level pcmas design two stages first analyze factors uence uncontrollable
agent perception actions next analyze ability control factors
retrospect implicit study social law enforcement
used projected game agent perception action
changed used indirect mechanism threats enforce perception desired
study embedded teaching started analysis different games
possibility affecting agent perception action games next tried
provide perception case bql students controllable teacher
complete control elements determine student perception
random nature student action yet try somehow affect
case q learners direct control available factors determining
student perception yet teacher could control aspects perception
found sucient
one might ask representative studies general pcmas domains
therefore relevant insight may provide chosen two domains
belief represent key aspects types agents studied ai
ai study dynamic agents act improve state agents likely
use information revise assessment state world much learning
agents need make decisions current information much
expected utility maximizers studied hence typical multi agent systems studied
ai include agents exhibit one properties
punishment rewards provide conceptual basis designing controllable agents mdps supply natural model many domains particular mdps
suitable uncertainty exists stemming agents choices
nature showed section least principle use established techniques
obtain strategies controllable agents phrased markov
decision process mdp perspective cases would require sophisticates tools number important challenges must met first assumptions
agent state fully observable environment state fully observable
unrealistic many domains assumptions invalid obtain partially
observable markov decision process pomdp sondik unfortunately although
pomdps used principle obtain ideal policy agents current techniques solving pomdps limited small hence practice one
resort heuristic punishment reward strategies section


fion partially controlled multi agent systems

one controlling agent poses natural challenge generalizing tools techniques
mdps distributed decision making processes

summary related work
introduces distinction controllable uncontrollable agents
concept partially controlled multi agent systems provides two multi agent
system design naturally fall framework pcmas design suggests concrete
techniques uencing behavior uncontrollable agents domains
work contributes ai introducing exploring promising perspective
system design contributes des considering two types structural
assumptions agents corresponding rational learning agents
application enforcement social behavior introduces
tool design multi agent systems punishment threats used notion
investigated part explicit design paradigm punishment deterrence threats
studied political science dixit nalebuff schelling yet
difference line work related game theoretic consider case
dynamic multi agent system concentrate punishment design issues
question minimizing number reliable agents needed control system unlike
much work multi agent systems assume agents rational agents
law abiding rather assumed designer control agents
deviations social laws uncontrolled agents need rational
notice behavior controllable agents may considered irrational cases
however eventually lead desired behavior agents approaches
negotiations viewed incorporating threats particular rosenschein
genesereth consider mechanism making deals among rational agents agents
asked offer joint strategy followed agents declare move
would take agreement joint strategy latter move viewed
threat describing implications refusing agent suggested joint strategy
example prisoner dilemma setting agent may propose joint cooperation
threaten defecting otherwise work first part could viewed
examining threat could credible effective particular context
iterative multi agent interactions
part study proposed embedded teaching situated teaching paradigm
suitable modeling wide range teaching instances modeled teacher
student players iterated two player game concentrated particular
iterative game showed challenging game type model
dynamics teacher student interaction made explicit clearly delineated
limits placed teacher ability uence student showed
detailed model student optimal teaching policies theoretically generated
viewing teaching markov decision process performance
optimal teaching policy serves bound agent ability uence student
examined ability teach two types reinforcement learners particular
showed optimal policy cannot used use tft teaching method
case q learners policy successful consequently proposed model


fibrafman tennenholtz

explains success finally showed even games teaching
challenging nevertheless quite useful moreover objective
simply teaching student even simpler domains present non trivial
choices future hope examine learning architectures see whether
lessons learned domain generalized whether use methods
accelerate learning domains
number authors discussed reinforcement learning multi agent systems
yanco stein examine evolution communication among cooperative reinforcement learners sen et al use q learning induce cooperation two
block pushing robots matraic parker consider use reinforcement
learning physical robots consider features real robots discussed
shoham tennenholtz examine evolution conventions
society reinforcement learners kittock investigates effects societal structure multi agent learning littman develops reinforcement learning techniques
agents whose goals opposed tan examines benefit sharing information
among reinforcement learners finally whitehead shown n reinforcement
learners observe everything decrease learning time factor
n however work concerned teaching question
much uence one agent another lin explicitly concerned
teaching way accelerating learning enhanced q learners uses experience replay supplies students examples task achieved remarked
earlier teaching different since teachers embedded
student domain within game theory extensive body work tries
understand evolution cooperation iterated prisoner dilemma
good playing strategies eatwell et al work players
knowledge teaching issue
last least work important links work conditioning especially
operant conditioning psychology mackintosh conditioning experiments
experimenter tries induce changes subjects arranging certain relationships
hold environment explicitly operant conditioning reinforcing
subjects actions framework controlled agent plays similar role
experimenter work uses control theoretic related
applying two basic ai contexts
main drawback case studies simple domains conducted typical initial exploration future work try
remove limiting assumptions incorporate example
embedded teaching context assumed uncertainty outcome joint action similarly model multi agent interaction section
symmetric assuming agents play k roles game equally
likely play role etc another assumption made malicious agents
loners acting opposed team agents perhaps importantly
future work identify additional domains naturally described terms
pcmas formalize general methodology solving pcmas design


fion partially controlled multi agent systems

acknowledgements
grateful yoav shoham members nobotics group stanford
input anonymous referees productive comments suggestions
especially grateful james kittock comments help improving
presentation supported fund promotion
technion nsf grant iri afosr grant af f

references

altenberg l feldman w selection generalized transmission
evolution modifier genes reduction principle genetics
bellman r dynamic programming princeton university press
bond h gasser l readings distributed artificial intelligence ablex
publishing corporation
briggs w cook flexible social laws proc th international joint
conference artificial intelligence pp
dixit k nalebuff b j thinking strategically competitive edge
business politics everyday life norton york
durfee e h lesser v r corkill coherent cooperation among communicating solvers ieee transactions computers
dwork c moses knowledge common knowledge byzantine environment crash failures information computation
eatwell j milgate newman p eds palgrave game theory
w w norton company inc
fox organizational view distributed systems ieee trans sys man
cyber
fudenberg tirole j game theory mit press
gilboa matsui social stability equilibrium econometrica

gold complexity automaton identificaion given data information
control
huberman b hogg behavior computational ecologies
huberman b ed ecology computation elsevier science
kaelbling l learning embedded systems ph thesis stanford university


fibrafman tennenholtz

kandori mailath g rob r learning mutation long equilibria
games mimeo university pennsylvania
kinderman r snell l markov random fields applications
american mathematical society
kittock j e impact locality authority emergent conventions
proceedings twelfth national conference artificial intelligence aaai
pp
kraus wilkenfeld j function time cooperative negotiations
proc aaai pp
lin f wonham w decentralized control coordination discrete event
systems proceedings th ieee conf decision control pp
lin l self improving reactive agents reinforcement learning
teaching machine learning
littman markov games framework multi agent reinforcement learning
proc th int conf mach learn
luce r raiffa h games decisions introduction critical survey
john wiley sons
mackintosh n conditioning associative learning oxford university press
malone w modeling coordination organizations markets management
science
mataric j reward functions accelerating learning proceedings
th international conference machine learning pp
minsky n imposition protocols open distributed systems ieee
transactions software engineering
moses tennenholtz artificial social systems computers artificial
intelligence
narendra k thathachar l learning automata introduction
prentice hall
owen g game theory nd ed academic press
parker l e learning cooperative robot teams proceedings ijcai
workshop dynamically interacting robots
ramadge p wonham w control discrete event systems proceedings
ieee
rosenschein j genesereth r deals among rational agents proc
th international joint conference artificial intelligence pp


fion partially controlled multi agent systems

schelling strategy con ict harvard university press
sen sekaran hale j learning coordinate without sharing information
proc aaai pp
shoham tennenholtz emergent conventions multi agent systems
initial experimental observations proc rd international conference principles knowledge representation reasoning pp
shoham tennenholtz social laws artificial agent societies line
design artificial inteligence
sondik e j optimal control partially observable markov processes
infinite horizon discounted costs operations
sutton r learning predict method temporal differences mach lear

tan multi agent reinforcement learning independent vs cooperative agents
proceedings th international conference machine learning
watkins c learning delayed rewards ph thesis cambridge university
watkins c dayan p q learning machine learning
weidlich w haag g concepts quantitative sociology
dynamics interacting populations springer verlag
whitehead complexity analysis cooperative mechanisms reinforcement
learning proceedings aaai pp
yanco h stein l adaptive communication protocol cooperating
mobile robots animal animats proceedings second international
conference simulation adaptive behavior pp
zlotkin g rosenschein j domain theory task oriented negotiation
proc th international joint conference artificial intelligence pp




