Journal Artificial Intelligence Research 4 (1996) 477-507

Submitted 9/95; published 6/96

Partially Controlled Multi-Agent Systems
Ronen I. Brafman

brafman@cs.ubc.ca

Computer Science Department
University British Columbia
Vancouver, B.C., Canada V6L 1Z4

Moshe Tennenholtz

moshet@ie.technion.ac.il

Industrial Engineering Management
Technion - Israel Institute Technology
Haifa 32000, Israel

Abstract

Motivated control theoretic distinction controllable uncontrollable
events, distinguish two types agents within multi-agent system: controllable
agents , directly controlled system's designer, uncontrollable agents ,
designer's direct control. refer systems partially
controlled multi-agent systems, investigate one might uence behavior
uncontrolled agents appropriate design controlled agents. particular,
wish understand problems naturally described terms, methods
applied uence uncontrollable agents, effectiveness methods,
whether similar methods work across different domains. Using game-theoretic framework,
paper studies design partially controlled multi-agent systems two contexts:
one context, uncontrollable agents expected utility maximizers,
reinforcement learners. suggest different techniques controlling agents'
behavior domain, assess success, examine relationship.

1. Introduction
control agents central research topic two engineering fields: Artificial Intelligence (AI) Discrete Events Systems (DES) (Ramadge & Wonham, 1989). One
particular area fields concerned multi-agent environments;
examples include work distributed AI (Bond & Gasser, 1988), work decentralized
supervisory control (Lin & Wonham, 1988). fields developed
techniques incorporated particular assumptions models. Hence,
natural techniques assumptions used one field may adopted
may lead new insights field.
difference AI work multi-agent systems, work decentralized discrete
event systems distinguishes controllable uncontrollable events. Controllable
events events directly controlled system's designer, uncontrollable events directly controlled system's designer. Translating terminology context multi-agent systems, introduce distinction two
types agents: controllable agents , directly controlled system's designer,
uncontrollable agents , designer's direct control. leads
c 1996 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiBrafman & Tennenholtz

naturally concept partially controlled multi-agent system (PCMAS)
following design challenge: ensuring agents system behave appropriately
adequate design controllable agents. believe many problems
naturally formulated instances PCMAS design. goal characterize important
instances design problem, examine tools used solve it,
assess effectiveness generality tools.
distinguishes partially controlled multi-agent systems AI context similar models DES structural assumptions make uncontrolled agents
involved. Unlike typical DES models concerned physical processes devices, AI particularly interested self-motivated agents, two concrete examples
rational agents, i.e., expected utility maximizers, learning agents, e.g., reinforcement learners. Indeed, examples constitute two central models self-motivated
agents game theory decision theory, referred educative evolutive models
(e.g., see Gilboa & Matsui, 1991). special nature uncontrollable agents
special structure uncontrollable events induce differentiates PCMAS
corresponding models DES literature. difference raises new questions
suggests new perspective design multi-agent systems. particular, calls
techniques designing controllable agents that, exploiting structural assumptions,
uence behavior uncontrollable agents lead system desired
behavior.
order understand issues, study two problems stated solved
adopting perspective PCMAS design; problems
interest large community. problems goal uence
behavior agents control. exert uence indirectly
choosing suitable behaviors agents direct control. one case,
attempt uence behavior rational agents, case, try
uence learning agents.
first study concerned enforcement social laws. number
agents designed different designers work within shared environment, beneficial
impose certain constraints behavior, that, overall, system function
better. example, Shoham Tennenholtz (1995) show imposing certain \trac
laws," considerably simplify task motion planning robot, still
enabling ecient motions. Indeed, see later, conventions heart
many coordination techniques multi-agent systems. Yet, without suitable mechanisms,
rational agents may incentive follow conventions. show how,
certain cases, use perspective partially controlled multi-agent systems
structural assumption rationality enforce conventions.
second study involves two-agent system consisting teacher student.
teacher knowledgeable agent, student agent learning
behave domain. goal utilize teacher (which control)
improve behavior student (which controlled us). Hence,
instance partially controlled multi-agent systems structural assumption
uncontrolled agent employs particular learning algorithm.
studies presented paper suggest techniques achieving satisfactory system
behavior design controllable agents, relevant, techniques
478

fiOn Partially Controlled Multi-Agent Systems

experimentally assessed. Beyond formulation solution two interesting problems multi-agent system design, paper suggests general perspective certain
design problems. Although feel still premature draw general conclusion
potential general theory PCMAS design, certain concepts, punishment
reward, suggest central area.
paper organized follows: Section 2, describe problem enforcing
social behavior multi-agent systems. Section 3 describe standard game-theoretic
model problem suggest mechanism threats punishments general
tool class problems. Issues pertain design threats punishments
discussed Section 4. Section 5 introduces second case study PCMAS design:
embedded teaching reinforcement learners. context, teacher learner
embedded shared environment teacher serving controller whose
aim direct learner desired behavior. formal model problem
introduced Section 6. Section 7, show derive optimal teaching policies (under
certain assumptions) viewing teaching Markov decision process. effectiveness
different teaching policies studied experimentally Section 8. Finally, Section 9,
examine relationship methods used two domains
possibility general methodology designing partially controlled multi-agent systems.
conclude Section 10, summary discussion related work.

2. Enforcement Social Behavior
section introduce problem enforcement social laws multi-agent
context. proposed solution falls naturally PCMAS design perspective
take. Here, explain motivate particular problem social law enforcement
approach solution. Sections 3 4 formalize investigate approach
framework general game-theoretic model.
use following scenario illustrate problem:
hired design new working environment artificial
agents. Part job involves designing number agents use
maintain warehouse. agents, designed different designers,
using warehouse obtain equipment. make sure different agents
designed different designers operate eciently environment,
choose introduce number social laws, is, constraints behavior
agents, help agents coordinate activities domain.
rules include number `trac laws', regulating motion domain,
well law specifies every tool used agent must
returned designated storage area. robots programmed follow
laws, expect others so. laws quite successful, allow ecient activity warehouse, new designer arrives.
Pressed corporate bosses deliver better performance, decides
exploit rules. designs agent locally maximize performance,
regardless social laws. do?
479

fiBrafman & Tennenholtz

multi-participant environments, one above, agent might
dynamic goals, interested finding ways agents coexist
achieving goals. Several approaches coordination agent activity discussed
distributed systems DAI literature. examples are: protocols reaching
consensus (Dwork & Moses, 1990), rational deals negotiations (Zlotkin & Rosenschein,
1993; Kraus & Wilkenfeld, 1991; Rosenschein & Genesereth, 1985), organizational structures (Durfee, Lesser, & Corkill, 1987; Fox, 1981; Malone, 1987), social laws (Moses
& Tennenholtz, 1995; Shoham & Tennenholtz, 1995; Minsky, 1991; Briggs & Cook, 1995).
methods, behavior agent predetermined prescribed
certain stage, example, content deal reached, outcome
negotiation process completed, social law instituted. work
relies assumption agents follow prescribed behaviors, e.g., obey
law stick agreement. assumption central success
methods. However, makes agents follow rules vulnerable rational agent
performs local maximization payoff, exploiting knowledge others follow
rules. example, new designer may program robot return tools,
saving time required so, thus causing agents fail tasks.
Despite somewhat futuristic avor (although instances shared environments
beginning appear cyberspace), scenario useful illustrating vulnerability
popular coordination mechanism appearing multi-agent literature
within AI (e.g., see Bond & Gasser, 1988) assume agents involved
fully rational. aside, note that, case, actually need attribute much
intelligence agents themselves, sucient assume designers
design way maximizes utility, disregarding utility
agents.
order handle problem need modify existing design paradigms.
adopting perspective partially controlled multi-agent systems, obtain one possible
handle problem, requires making following basic assumption:
original designer, scenario, controls number reliable agents.1
basic idea reliable agents designed punish agents
deviate desirable social standard. punishment mechanism `hardwired' (unchangeable) common-knowledge. agents controlled
original designer aware punishment possibility. punishment
mechanism well designed, deviations social standard become irrational.
result, deviation actually occur punishment actually executed! Hence,
making agents bit sophisticated, prevent temptation breaking
social laws.
suggested solution adopt perspective partially controlled multi-agent systems. agents controllable, others uncontrollable assumed
adopt basic model expected utility maximization. punishment mechanism
(part of) control strategy used uence behavior uncontrolled
agents.
1. ease exposition, assume reliable agents follow designer's instructions; assume
non-malicious failures, crash failures, possible.

480

fiOn Partially Controlled Multi-Agent Systems

3. Dynamic Game Theoretic Model

section introduce basic game-theoretic model, use study
problem enforcement social behavior solution. Later on, Sections 5{8,
model used study embedded teaching. wish emphasize model
use common model representing emergent behavior population2
(e.g., Huberman & Hogg, 1988; Kandori, Mailath, & Rob, 1991; Altenberg & Feldman,
1987; Gilboa & Matsui, 1991; Weidlich & Haag, 1983; Kinderman & Snell, 1980).

Definition 1 k-person game g defined k-dimensional matrix size n1

nk , nm number possible actions (or strategies) m'th agent. entries
vectors length k real numbers, called payoff vectors. joint strategy
tuple (i1; i2; : : :; ik ), 1 j k, case 1 ij nj .
Intuitively, dimension matrix represents possible actions one k
players game. Following convention used game theory, often use term
strategy place action . Since dimensions matrix n1 nk , i'th
agent ni possible strategies choose from. j 'th component vector residing
(i1; i2; : : :; ik ) cell (i.e., Mi1 ;i2 ;:::;ik ) represents feedback player j receives
players' joint strategy (i1; i2; : : :; ik ), is, agent m's strategy im
1 k. Here, use term joint strategy refer combined choice
strategies agents.

Definition 2 n-k-g iterative game consists set n agents given k person
game g . game g played repetitively unbounded number times. iteration,
random k-tuple agents play instance game, members k-tuple
selected uniform distribution set agents.

Every iteration n-k-g game represents local interaction k agents. agents
play particular iteration game must choose strategy use
interaction; agent use different strategies different interactions. outcome
iteration represented payoff vector corresponding agents' joint strategy.
Intuitively, payoff tells us good outcome joint behavior point
view agent. Many situations represented n-k-g game, example,
\trac" aspect multi-agent system represented n-k-g game,
time number agents meet intersection. encounter instance
game agents choose number strategies, e.g., move ahead, yield.
payoff function gives utility set strategies. example, time
two agents meet agents choose move ahead, collision occurs payoffs
low.
Definition 3 joint strategy game g called ecient sum players' payoffs
maximal.
2. paper use term emergent behavior classical mathematical-economics interpretation:
evolution behavior based repetitive local interactions (usually pairs of) agents,
agent may change strategy following interactions based feedback received previous
interactions.

481

fiBrafman & Tennenholtz

Hence, eciency one global criterion judging \goodness" outcomes
system's perspective, unlike single payoffs describe single agent's perspective.3

Definition 4 Let fixed joint strategy given game g, payoff pi(s) player
i; instance g joint strategy s0 played, pi (s) pi (s0 ) say
i's punishment w.r.t. pi (s) , pi (s0 ), otherwise say benefit w.r.t.
pi(s0) , pi (s).
Hence, punishment benefit w.r.t. joint strategy measure gain (benefit)
loss (punishment) agent somehow change joint behavior agents
s0 .
current discussion punishment benefit always respect chosen
ecient solution.
designers multi-agent system, would prefer ecient possible.
cases entails behavior sense unstable, is, individual agents
may locally prefer behave differently. Thus, agents may need constrained behave
way locally sub-optimal. refer constraints exclude
possible behaviors social laws .
Due symmetry system assumption agents
rational utility additive (i.e., utility two outcomes sum
utilities), clear agent's expected payoff higher one obtained
using strategies giving ecient solution. Thus, clear case ecient
solution fair, sense agents get least could law
existed, solution provide better expected payoff.
However, good intentions designer creating environment beneficial
participating agents, may backfire. social law provides information behavior
agents conforming it, information agents (or respective designers)
use increase expected payoff.
Example 1 Assume playing n-2-g game g prisoner's dilemma,
represented strategic form following matrix.
agent 2
agent 1
1
2
1
(2,2) (-10,10)
2
(10,-10) (-5,-5)
ecient solution game obtained players play strategy 1. Assume
solution chosen original designer, followed agents
control.
designer new agent function environment social law
obeyed may tempted program agent conform chosen law. Instead,
program agent play strategy maximizes expected outcome, strategy
3. Addition payoffs utilities across agents dangerous practice. However, particular model,
shown system joint-strategies always ecient maximizes agent's expected
cumulative rewards.

482

fiOn Partially Controlled Multi-Agent Systems

#2. new agent obtain payoff 10 playing one `good' agents.
Thus, even though social law accepted order guarantee payoff 2
agent, `good' agents obtain payoff -10 playing non-conforming
agents. Note new designer exploits information strategies `good' players,
dictated social law. agents controlled new designer uncontcolable
agents; behavior dictated original designer.
Agents conforming social law referred malicious agents . order
prevent temptation exploit social law, introduce number punishing
agents , designed initial designer, play `irrationally' detect behavior
conforming social law, attempting minimize payoff malicious agents.
knowledge future participants punishment policy would deter deviations eliminate need carrying out. Hence, punishing behavior used
threat aimed deterring agents violating social law. threat (part of)
control strategy adopted controllble agents order uence behavior
unconrollable agents. Notice control strategy relies structural assumption
unconrollable agents expected utility maximizers.
define minimized malicious payoff minimal expected payoff malicious players guaranteed punishing agents. punishment exists ,
minimized malicious payoff lower expected payoff obtained playing according
social law. strategy guarantees malicious agents expected payoff lower
one obtained playing according social law called punishing strategy .
Throughout section following section make natural assumption
expected payoff malicious agents playing greater
one obtained ecient solution4 .
Example 1 (continued) Example 1, punishment would simply play strategy
2 on. may cause payoff punishing agent decrease, would
guarantee malicious agent obtains payoff better -5 playing punishing
agent. many non-malicious agents punishing, malicious agents' expected payoff
would decrease become smaller payoff guaranteed social law. Strategy
2 would punishing strategy.

4. Design Punishments

previous section described general model multi-agent interaction showed
perspecive partially controlled multi-agent systems leads one possible solution
problem enforcing social behavior setting, via idea threats
punishments. proceed examine issue punishment design.
assume p agents designer controls either ability
observe instances game occur, informed outcome
games. c additional agents conform law (that is, play strategies
entailed chosen ecient solution), malicious agents, bound
law.
4. assumptions may treated similarly.

483

fiBrafman & Tennenholtz

would answer questions as: game offer ability punish?
minimized malicious payoff? optimal ratio p; c; m?
difference different social laws?
Example 1 (continued) Consider Example 1 again. observed
cause expected maximal loss malicious agents 7 (= 2 , (,5)). occurs
punishing agents play strategy 2. gain malicious agent makes
playing agent following social law 8 (= 10 , 2). order punishing
strategy effective, must case expected payoff malicious agent
greater expected payoff obtained following social law. order
achieve this, must ensure ratio punishing/conforming agents
malicious agent sucient encounters punishing agents. case, assuming
2 deviators meet expected benefit 0 recalling agent equally
likely meet agent, need pc > 87 make incentive deviate negative.
Implementing punishment approach requires complex behavior. agents
must able detect deviations well switch new punishing strategy.
whole behavior viewed new, complex, social law. calls
complex agents carry out, makes programming task harder.
Clearly, would minimize number complex agents, keeping
benefit malicious behavior negative. Here, major question ratio
benefit deviation prospective punishment.
seen example, larger punishment, smaller number
sophisticated punishing agents needed. Therefore, would find
strategies minimize malicious agent's payoff. order require
additional definitions.

Definition 5 two person game g zero-sum game every joint strategy
players, sum players' payoffs 0.

Hence, zero-sum game, win/win situations, larger payoff one
agent, smaller payoff agent. convention, payoff matrix two
person zero-sum game mention payoffs player 1.

Definition 6 Let g two person game. Let Pig (s,t) payoff player g (where
2 f1; 2g) strategies played player 1 2 respectively. projected

game, gp , following two person zero-sum game: strategies players
g , payoff matrix P gp (s; t) = ,P2g (s,t). Define transposed game g , g ,
game g roles players change.

projected game, first agent's payoff equals negated value second agent's
payoff original game. Thus, game ects desire lower payoffs
second player original game.
give general result two-person game, g (with number strategies).
make use following standard game-theoretic definition:
484

fiOn Partially Controlled Multi-Agent Systems

Definition 7 Given game g, joint strategy players Nash equilibrium

g whenever player takes action different action , payoff given
players play higher payoff given everybody plays .
is, strategy Nash equilibrium game agent obtain better payoff
unilaterally changing behavior agents play according .
Nash-equilibrium central notion theory non-cooperative games (Luce &
Raiffa, 1957; Owen, 1982; Fudenberg & Tirole, 1991). result, notion well studied
understood, reducing new concepts basic concept may quite useful
design perspective. particular, Nash-equilibrium always exists finite games,
payoffs prescribed Nash-equilibria given zero-sum game uniquely defined.
show:

Theorem 1 Given n-2-g iterative game, minimized malicious payoff achieved

playing strategy player 1 prescribed Nash equilibrium projected game gp,
playing player 1 (in g ), strategy player 1 prescribed Nash equilibrium
projected game (g )p, playing player 2 (in g ).5

Proof: Assume punishing agent plays role player 1. player 1 adopts

strategy prescribed Nash-equilibrium player 2 get better payoff
one guaranteed since deviation player 2 improve situation (by
definition Nash-equilibrium). hand, player 1 cause harm
harm obtained playing strategy . see this, assume player 1 uses
arbitrary strategy s, player 2 adopts strategy prescribed . outcome
player 1 higher one guaranteed playing Nash-equilibrium
(by definition Nash-equilibrium). addition, due fact
zero-sum game implies outcome player 2 lower one
guaranteed player 1 would play according . case punishing agent
player 2 treated similarly.

Example 1 (continued) Continuing prisoner's dilemma example, gp would
agent 2
agent 1 1 2
1
-2 -10
2
10 5
Nash equilibrium attained playing strategies yielding 5. example,
(g )p = gp. Therefore, punishing strategies strategy # 2 case.

Corollary 1 Let n-2-g iterative game, p punishing agents. Let v v'

payoffs Nash equilibria gp gpT respectively (which, case, uniquely
defined). Let b,b' maximal payoffs player 1 obtain g g respectively,
5. Notice that, cases, strategies prescribed original game determined strategies
player 1 Nash-Equilibria projected games.

485

fiBrafman & Tennenholtz

assuming player 2 obeying social law. Let e e' payoffs player 1 2,
respectively, g , players play according ecient solution prescribed
social law. Finally, assume expected benefit two malicious agents meet
0. necessary sucient condition existence punishing strategy
(n,1,p) (b + b0) , p (v + v 0 ) < (e + e0 ).
n,1
n,1

Proof: expected payoff obtained malicious agent encountering law-

abiding agent b+2b , expected payoff encountering punishing agent ,(v2+v ) .
order test conditions existence punishing strategy would need
consider best case scenario point view malicious agent; case
non-punishing agents law-abiding agents. order obtain expected utility
malicious agent make average quantities taking account
proportion law-abiding punishing agents population. gives us
expected utility malicious agent (2(n,n1,,1)p) (b + b0) , 2(np,1) (v + v 0). definition,
punishing strategy exists expected utility lower expected
utility guaranteed social law. Since expected utility guaranteed
social law e+2e , get desired result.
value punishment, (v+2v ) above, independent ecient solution
chosen, e + e0 identical ecient solutions, definition. However, b + b0 depends
choice ecient solution. number solutions exist, minimizing
b + b0 important consideration design social law, affects incentive
`cheat'.
0

0

0

0

Example 2 Let's look slightly different version prisoner's dilemma. game
matrix

agent 2
agent 1
1
2
1
(0,0) (-10,10)
2
(10,-10) (-5,-5)
3 ecient solutions, given joint strategies (1,1), (1,2), (2,1).
case (1,1) b+b'=20 (gained playing strategy 2 instead 1). case
(2,1) (1,2) b+b'=5.
Clearly, incentive deviate social law prescribing strategies (1,1)
social law prescribing (2,1) (1,2).
summarize, preceding discussion suggests designing number punishing agents,
whose behavior punishment mode prescribed Theorem 1 case n-2-g games.
ensuring sucient number agents take away incentive deviate
social laws. Hence, given malicious agents rational, follow social
norm, consequently, need utilize punishment mechanism.
observed different social laws leading solutions equally ecient different
properties comes punishment design. Consequently, assumption
would minimize number punishing agents guaranteeing ecient
486

fiOn Partially Controlled Multi-Agent Systems

solution participants, choose ecient solution minimizes value
b + b0.

5. Embedded Teaching
section move second study PCMAS design problem; now,
uncontrollable agent reinforcement learner. choice arbitrary; rational
agents reinforcement learners two major types agents studied mathematical
economics, decision theory, game theory. types agents discussed
work DAI concerned self-motivated agents (e.g., Zlotkin & Rosenschein,
1993; Kraus & Wilkenfeld, 1991; Yanco & Stein, 1993; Sen, Sekaran, & Hale, 1994).
agent's ability function environment greatly affected knowledge
environment. special cases, design agents sucient knowledge
performing task (Gold, 1978), but, general, agents must acquire information on-line
order optimize performance, i.e., must learn. One possible approach
improving performance learning algorithms employing teacher. example,
Lin (1992) uses teaching example improve performance agents, supplying
examples show task achieved. Tan's work (1993)
viewed form teaching agents share experiences. methods nontrivial form communication perception required. strive model broad notion
teaching encompasses behavior improve learning agent's performance.
is, wish conduct general study partially controlled multi-agent systems
uncontrollable agent runs learning algorithm. time, want
model clearly delineate limits teacher's (i.e., controlling agent's) ability
uence student.
Here, propose teaching approach maintains situated \spirit" much
reinforcement learning (Sutton, 1988; Watkins, 1989; Kaelbling, 1990), call
embedded teaching . embedded teacher simply \knowledgeable" controlled agent
situated student shared environment. Her6 goal lead student
adopt specific behavior. However, teacher's ability teach restricted
nature environment share: repertoire actions limited,
may lack full control outcome actions. example, consider
two mobile robots without means direct communication. Robot 1 familiar
surroundings, Robot 2 not. situation, Robot 1 help Robot 2 reach
goal certain actions, blocking Robot 2 headed wrong
direction. However, Robot 1 may limited control outcome
interaction uncertainty behavior Robot 2 control uncertainty.
Nevertheless, Robot 2 specific structure, learner obeying learning scheme,
attempt control indirectly choice actions Robot 1.7
6. differentiate teacher student, use female pronouns former male pronouns
latter.
7. general, fact agent controllable imply perfectly control outcome
actions, choice. Hence, robot may controllable sense, running program
supplied us, yet move-forward command may always desired outcome.

487

fiBrafman & Tennenholtz

follows, goal understand embedded teacher help student
adopt particular behavior. address number theoretical questions relating
problem, experimentally explore techniques teaching two types
reinforcement learners.

6. Basic Teaching Setting
consider teacher student repeatedly engage joint activity.
student prior knowledge pertaining activity, teacher understands
dynamics. model, teacher's goal lead student adopt particular
behavior interactions. example, teacher student meet occasionally
road teacher wants teach student drive right side. perhaps,
teacher student share resource, CPU time, goal teach
judicious use resource. model encounters 2-2-g iterative games.
capture idea teacher knowledgeable student, assume
knows structure game, i.e., knows payoff function,
recognizes actions taken play. hand, student know
payoff function, although perceive payoff receives. paper, make
simplifying assumptions teacher student two actions
choose outcome depends choice actions. Furthermore,
excluding study Section 8.4, ignore cost teaching, hence, omit
teacher payoff description.8 provides basic setting take
first step towards understanding teaching problem.9
teaching model concisely modeled 2 2 matrix. teacher's actions
designated II , student's actions designated numbers 1
2. entry corresponds joint action represents student's payoff
joint action played. suppose matrix Figure 1,
wish teach student use action 1. stage, assume student
always receives better payoff following action 1 learn play it.
see situations teaching trivial. Assume first row dominates second row, i.e., > c b > d. case, student naturally prefer
take action 1, teaching challenging, although might useful speeding
learning process. example, , c > b , d, matrix B Figure 1, teacher
make advantage action 1 noticeable student always playing action
I.
suppose one > c b > holds. case, teaching still easy.
use basic teaching strategy, call preemption . preemption teacher
chooses action makes action 1 look better action 2. example,
situation described matrix C Figure 1, teacher always choose action .
8. case could made inherent value teaching, may appropriate forum
airing views.
9. fact, idea consider basic embedded teaching setting already challenging. later see, basic setting closely related fundamental issue non-cooperative
games.

488

fiOn Partially Controlled Multi-Agent Systems

II

II

1 b

1 6 5

2 c
(A)

II

1 5 1
2 2 6
(C)

2 1 2
(B)

II

1 3 -2
2 5 6
(D)



II

1 5 -10
2 10 -5
(E)

Figure 1: Game matrices A, B, C, D, E. teacher's possible actions II ,
student's possible actions 1 2.
Next, assume c greater b, matrix Figure 1.
Regardless action teacher chooses, student receives higher payoff
playing action 2 (since minf5; 6g > maxf3; ,2g). Therefore, matter teacher
does, student learn prefer action 2. Teaching hopeless situation.
types interactions isomorphic case c > > > b,
matrix E Figure 1. still challenging situation teacher action 2
dominates action 1 (because 10 > 5 ,5 > ,10). Therefore, preemption cannot work.
teaching strategy exists, complex always choosing action.
Since seems challenging teaching situation, devote attention
teaching reinforcement learner choose action 1 class games.
turns situation quite important game-theory multi-agent
interaction. projection famous game, prisoner's dilemma, discussed
previous sections. general, represent prisoner's dilemma using
following game matrix:
teacher
student Coop Defect
student Coop
Coop (a,a) (b,c) commonly Coop (a,a)
Defect (c,b) (d,d)
Defect (c,-c)

teacher
Defect
(-c,c)
(d,d)

c > > > b. actions prisoner's dilemma called Cooperate (Coop)
Defect; identify Coop actions 1 , Defect actions 2 II .
prisoner's dilemma captures essence many important social economic situations;
particular, encapsulates notion cooperation. thus motivated enormous discussion among game-theorists mathematical economists (for overview, see Eatwell,
Milgate, & Newman, 1989). prisoner's dilemma, whatever choice one player,
second player maximize payoff playing Defect. thus seems \rational"
player defect. However, players defect, payoffs much worse
cooperate.
489

fiBrafman & Tennenholtz

example, suppose two agents given $10 moving object.
agent perform task alone, take amount time energy
value $20. However, together, effort make valued $5. get
following instance prisoner's dilemma:
Agent 1
Agent 2 Move
Rest
Move
(5,5) (-10,10)
Rest (10,-10) (0,0)
experimental part study, teacher's task teach student
cooperate prisoner's dilemma. measure success teaching strategy
looking cooperation rate induces students period time, is,
percentage student's actions Coop. experimental results presented
paper involving prisoner's dilemma respect following matrix:
Teacher
Student Coop Defect
Coop (10,10) (-13,13)
Defect (13,-13) (-6,-6)
observed qualitatively similar results instantiations prisoner's
dilemma, although precise cooperation rate varies.

7. Optimal Teaching Policies

previous section concentrated modeling teaching context instance
partially controlled multi-agent system, determining particular problems
interesting. section start exploring question teacher
teach. First, define optimal policy is. Then, define Markov decision
processes (MDP) (Bellman, 1962), show certain assumptions teaching
viewed MDP. allow us tap vast knowledge accumulated
solving problems. particular, use well known methods, value
iteration (Bellman, 1962), find optimal teaching policy.
start defining optimal teaching policy. teaching policy function
returns action iteration; possibly, may depend complete history
past joint actions. \right" definition optimal policy, teacher's
motivation may vary. However, paper, teacher's objective maximize
number iterations student's action \good", Coop prisoner's
dilemma. teacher know precise number iterations playing,
slightly prefers earlier success later success.
formalized follows: Let u(a) value teacher places student's
action, a, let teacher's policy, assume induces probability distribution
Pr;k set possible student actions time k. define value strategy

1
X
val( ) = kEk (u)
k=0

490

fiOn Partially Controlled Multi-Agent Systems

Ek (u) expected value u:

Ek (u) =

X Pr

a2As

;k (a) u(a)

Here, student's set actions. teacher's goal find strategy
maximizes val(), discounted expected value student's actions. example,
case prisoner's dilemma, could
= fCoop,Defectg u(Coop) = 1 u(Defect) = 0.
Next, define MDPs. MDP, decision maker continually moving
different states. point time observes current state, receives payoff
(which depends state), chooses action. action current state
determine (perhaps stochastically) next state. goal maximize function
payoffs. Formally, MDP four-tuple hS; A; P; ri, state-space,
decision-maker's set possible actions, P : ! [0; 1] probability
transition states given decision-maker's action, r : ! < reward
function. Notice given initial state 2 , policy decision maker , P
induces probability distribution Ps;;k , Ps;;k (s0 ) probability
kth state obtained s0 current state s.
0-optimal policy MDP policy maximizes state
discounted sum expected values payoffs received future states, starting
s, i.e.,
1
X
X
0k( Ps;;k (s0 ) r(s0))
k=0

2S
0

Although may immediately obvious, single policy maximizing discounted sums
starting state exists, well-known ways finding policy.
experiments use method based value-iteration (Bellman, 1962).
suppose student set possible states, set actions
, teacher's set actions . Moreover, suppose following
properties satisfied:
(1) student's new state function old state current joint-action,
denoted : ! ;
(2) student's action stochastic function current state, probability
choosing state (s; a);
(3) teacher knows student's state. (The natural way happen
teacher knows student's initial state, function , outcome game,
uses simulate agent.)
Notice assumptions teaching policy function :
know student's next action function next state. know
student's next state function current state, current action, teacher's
current action. Hence, next action function current state action,
well teacher's current action. However, know student's current action
function current state. Hence, student's next action function
current state teacher's current action. implies knowledge
teacher needs optimally choose current action student's current state,
491

fiBrafman & Tennenholtz

additional information redundant cannot improve success. generally,
repeat line reasoning indefinitely future, see teacher's
policy function student's state: function .
possible see makings following MDP.
Given observation three assumptions, see that, indeed, teacher's
policy induces probability distribution set possible student actions time k.
implies definition val makes sense here.
Define teacher's MDP TMDP= h; At; P; U i,

X

P (s; s0; at) def
=

2As

(s; as) ; (s;as;at)
0

(i;j defined 1 = j , 0 otherwise). is, probability transition
s0 sum probabilities student's actions induce
transition. reward function expected value u:

X

U (s) def
=

as2As

(s; as) u(as)

Theorem 2 optimal teaching policy given 0 optimal policy TMDP.
Proof: definition, 0 optimal policy TMDP policy 2
maximizes
is,

1
X
X
k( P
0

k=0

2

s;;k (s0 ) U (s0 ))

0

1
X
X
k( P

k=0

However, equal

()

0

0
s;;k (s ) (

2
0

X
as2As

(s0 ; as) u(as)))

1
X
X X (s0; ) P
k

k=0

0



as2As 2
0

0
s;;k (s ) u(as )

know Ps;;k (s0 ) probability s0 state student time
k, given teacher uses current state s. Hence,

X (s0; ) P

2
0



0
s;;k (s )

probability action taken student time k given initial
(current) state s. Upon examination, see (*) identical val( ).
optimal policy used teaching, teacher possess sucient information determine current state student. even case,
allows us calculate upper bound success val( ) teaching policy .
number property learning algorithm, measures degree uence
agent given student.
492

fiOn Partially Controlled Multi-Agent Systems

8. Experimental Study

section describe experimental study embedded teaching. First, define
learning schemes considered, then, describe set results obtained using computer
simulations.

8.1 Learning Schemes

experiment two types students: One uses reinforcement learning algorithm
viewed Q-learning one state, uses Q-learning. choosing
parameters students tried emulate choices made reinforcement learning
literature.
first student, call Blind Q-learner (BQL), perceive rewards,
cannot see teacher acted remember past actions. keeps
one value action, example, q (Coop) q (Defect) case prisoner's
dilemma. update rule following: performed action received reward
R
qnew (a) = (1 , ff) qold(a) + R
parameter ff, learning-rate, fixed (unless stated otherwise) 0:1 experiments. wish emphasize although BQL bit less sophisticated \real"
reinforcement learners discussed AI literature (which defined below), popular powerful type learning rule, much discussed used literature
(Narendra & Thathachar, 1989).
second student Q-learner (QL). observe teacher's actions
number possible states. QL maintains Q-value state-action pair.
states encode recent experiences, i.e., past joint actions. update rule is:
qnew (s; a) = (1 , ff) qold (s; a) + (R + V (s0))
R reward received upon performing state s; s0 state student
following performance s; called discount factor, 0:9, unless
otherwise noted; V (s0) current estimate value best policy s0 ,
defined maxa2As q (s0 ; a). Q-values initially set zero.
student's update rule tells us Q-values change result new experiences. must specify Q-values determine behavior. QL
BQL students choose actions based Boltzmann distribution. distribution
associates probability Ps (a) performance action state (P (a)
BQL).
exp(q (a)=T )
q(s; a)=T )
def
P
(
QL
)
P
(

)
=
(BQL)
Ps(a) def
= P exp(exp(
0
q (s; )=T )
2A
2A exp(q (a0)=T )
called temperature . Usually, one starts high value ,
makes action choice random, inducing exploration part student.
slowly reduced, making Q-values play greater role student's choice action.
use following schedule: (0) = 75 (n +1) = (n) 0:9+0:05. schedule
characteristic properties fast initial decay slow later decay. experiment
fixed temperature.
0

0

493

fiBrafman & Tennenholtz

Approximately Optimal Policy
1

10000 iterations
5000 iterations
1000 iterations
100 iterations

0.8

Fraction Coops

Fraction Coops

1

Two Q-learners

0.6
0.4
0.2

10000 iterations
5000 iterations
1000 iterations
100 iterations

0.8
0.6
0.4
0.2

0

0
0

1

2

3

4 5 6 7
Temperature

8

9 10

0

1

2

3

4 5 6 7
Temperature

8

9 10

Figure 2: Fraction Coops function temperature approximately optimal
policy (left) \teaching" using identical Q-learner (right). curve
corresponds Coop rate fixed number iterations. approx.
optimal policy curves 1000, 5000 10000 iterations nearly identical.

8.2 Blind Q-Learners

Motivated discussion Section 6 concentrate section
following section teaching context prisoner's dilemma. Section 8.4
discuss another type teaching setting. section describes experimental results
BQL. examined policy approximates optimal policy, two teaching
methods rely student model.
8.2.1 Optimal Policy

First show BQLs fit student model Section 7. state space, use
set possible assignment Q-values. continuous subspace <2,
discretize (in order able compute policy), obtaining state space
approximately 40,000 states. Next, notice transitions stochastic function
current state (current Q-values) teacher's action. see notice Q-value
updates function current Q-value payoff; payoff function
teacher's student's actions; student's actions stochastic function
current Q-value. left side Figure 2 see success teaching using policy
generated using dynamic programming solve optimization problem. curve
represents fraction Coops function temperature fixed number
iterations. values means 100 experiments.
8.2.2 Two Q-Learners

ran experiments two identical BQLs. viewed \teaching" using
another Q-learner. results shown right side Figure 2. temperatures
optimal strategy performs better Q-learning \teaching" strategy. fact
temperatures 1.0 less success rate approaches 1 beneficial later
add temperature decay. However, see inherent limit ability
494

fiOn Partially Controlled Multi-Agent Systems

Tit-For-Tat
1

10000 iterations
5000 iterations
1000 iterations
100 iterations

0.8

Fraction Coops

Fraction Coops

1

2-Tit-For-Tat

0.6
0.4
0.2

10000 iterations
5000 iterations
1000 iterations
100 iterations

0.8
0.6
0.4
0.2

0

0
0

1

2

3

4 5 6 7
Temperature

8

9 10

0

1

2

3

4 5 6 7
Temperature

8

9 10

Figure 3: Fraction Coops function temperature teaching strategy based
TFT (left) 2TFT (right).
affect behavior higher temperatures. interesting phenomenon phase
transition observed around = 2:5. qualitative explanation phenomenon
high temperature adds randomness student's choice action, makes
probabilities P (a) less extreme. Consequently, ability predict student's behavior
lessens, probability choosing good action. However, randomness
serves lower success rate initially, guarantees level effective cooperation,
approach 0.5 temperature increases. Finally, notice although
(Coop,Coop) seems best joint-action pair agents, two interacting Q-learners
never learn play joint strategy consistently, although approach 80% Coops
low temperatures.
8.2.3 Teaching Without Model

teacher precise model student, cannot use techniques
Section 7 derive optimal policy; models, assume teacher
\observe" student's current state (i.e. knows student's Q-values).
therefore explore two teaching methods exploit knowledge game
fact student BQL.
methods motivated basic strategy countering student's move.
basic idea try counter good actions student action lead
high payoff, counter bad actions action give low payoff.
Ideally, would play Coop student plays Coop, Defect
student plays Defect. course, don't know action student choose,
try predict past actions.
assume Q-values change little one iteration other,
student's likely action next game action took
recent game. Therefore, saw student play Coop previous turn, play
Coop . Similarly, teacher follow Defect student Defect
495

fiBrafman & Tennenholtz

Fraction Coops time

Fraction Coops

1
0.9
0.8
0.7
approximately optimal
Q-learning
Tit-For-Tat
2-Tit-For-Tat

0.6
0.5
2000

4000 6000
Iterations

8000

10000

Figure 4: Fraction Coops function time BQL using temperature decay
scheme Section 8.1. Teaching strategies shown: approximately optimal strategy, Q-learning, TFT, 2TFT.
part. strategy, called Tit-For-Tat (TFT short), well known (Eatwell et al., 1989).
experiments show successful teaching BQL (see Figure 3).
experimented variant TFT, call 2TFT. strategy
teacher plays Defect observing two consecutive Defects part student.
motivated observation certain situations better let student
enjoy free lunch (that is, match Defect Coop) make Coop look bad
him, may cause Q-value Coop low unlikely
try again. Two consecutive Defects indicate probability student playing
Defect next quite high. results, shown Figure 3, indicate strategy worked
better TFT, ranges temperature, better Q-learning. However,
general, TFT 2TFT gave disappointing results.10
Finally, Figure 4 shows performance four teaching strategies discussed
far incorporate temperature decay. see optimal policy
successful. explained before, teaching easier student predictable,
case temperature lower. temperature decay student spends
time relatively low temperature behaves similarly case fixed,
low temperature. initial high-temperature phase could altered behavior,
observe effects.

8.3 Teaching Q-Learners

Unlike BQL, Q-learners (QL) number possible states encode joint actions
previous games played. QL memory one four possible states, corresponding
four possible joint actions prisoner's dilemma; QL memory
states, encoding sequence joint actions.
complex learning architectures structure, brings certain
problems. One possible problem may structure \teaching-resistant."

10. sense use identical Q-learner implies model student, TFT
2TFT make use model.

496

fiOn Partially Controlled Multi-Agent Systems

Tit-For-Tat

Two Q-learners
1
Fraction Coops

Fraction Coops

1
0.8
0.6
0.4

10000 iterations
5000 iterations
1000 iterations
100 iterations

0.2

10000 iterations
5000 iterations
1000 iterations
100 iterations

0.8
0.6
0.4
0.2

0

0
0

1

2

3

4 5 6 7
Temperature

8

9 10

0

1

2

3

4 5 6 7
Temperature

8

9 10

Figure 5: curve shows fraction Coops QL function temperature
fixed number iterations TFT used teach (left) identical Q-learner used teach (right). Values means 100 experiments.
real threat added computational complexity. mentioned, approximate
optimal teaching policy BQL compute space approximately 40,000
discretized states. representing state BQL requires two numbers, one
Q-value, representing state QL states requires 2m + 1 numbers:
one Q-value state/action pair, one encoding current state. size
corresponding discretized state-space teacher's Markov decision process grows
exponentially m. simplest case memory one (a student four states)
would 1018 states. Since solving problem 40,000 states took 12 hours
sun sparcstation-10, able approximate optimal teaching policies
even simplest QL.
lost. structure may mean complexity, means
properties exploit. reach surprisingly good results exploiting structure
Q-learners. Moreover, using teaching method introduced previous
section. However, QL method takes new meaning suggests familiar
notions reward punishment. Interestingly, one may recall punishment
major tool approach enforcement social behavior.
choosing actions, QLs \care" immediate rewards,
current action's effect future rewards. makes suitable reward
punishment scheme. idea following: suppose QL something \bad" (Defect
case). Although cannot reliably counter move move lower
reward, punish later choosing action always gives negative
payoff, matter student plays. achieve following student's Defect
Defect teacher. immediate reward obtained QL playing Defect
may high, learn associate subsequent punishment Defect action.
Thus, may locally beneficial perform Defect, may able make
long-term rewards Defect less desirable. Similarly, follow student's Coop
reward form Coop teacher, since guarantees positive payoff
student.
497

fiBrafman & Tennenholtz

Fraction Coops time

Fraction Coops

1

Tit-For-Tat
Q-learning

0.9
0.8
0.7
0.6
0.5
2000

4000 6000
Iterations

8000

10000

Figure 6: Fraction Coops QL function time temperature decay TFT
Q-learning teaching strategies.
suggests using Tit-For-Tat again. Notice BQLs, TFT cannot understood
reward/punishment strategy BQLs care immediate outcome
action; value associate action weighted average immediate
payoffs generated playing action.
Figure 5 see success rates TFT function temperature, well
rates Q-learning teaching strategy. latter case, teacher identical
student. apparent TFT extremely successful, especially higher temperatures.
Interestingly, behavior quite different two QLs. Indeed, examine
behavior two QLs, see that, lesser extent, phase change noticed
BQLs still exists. obtain completely different behavior TFT used: Coop levels
increase temperature, reaching almost 100% 3.0. Hence, see TFT works
better student Q-learner exhibits certain level experimentation. Indeed,
examine success teaching strategies low temperature, see
Q-learning performs better TFT. explains behavior TFT QL
temperature decay introduced, described Figure 6. figure, QL seems
effective TFT. probably result fact experiment
student's temperature quite low time.
experiments QL remembers last joint action. experimented
QL memory performance worse. explained follows.
QL memory one more, problem fully observable Markov decision process
teacher plays TFT, TFT deterministic function previous joint action.
know Q-learning converges optimal policy conditions (Watkins &
Dayan, 1992). Adding memory effectively adds irrelevant attributes, which, turn,
causes slower learning rate. examined whether 2TFT would successful
agents memory two. results shown here, success rate
considerably lower TFT, although better two QLs.
TFT performed well teaching strategy, explained motivation using it.
want produce quantitative explanation, one used predict
success vary various parameters, payoff matrix.
498

fiOn Partially Controlled Multi-Agent Systems

Coop rates function DIF

Fraction Coops

1
0.8
0.6
0.4
0.2
0
-20 -10

0

10

20 30
DIF

40

50

60

Figure 7: Coop rates function DIF = + b + (a + c) , (c + + (b + d)). means
100 experiments, 10000 iterations each. Student's memory 1.
Let student's payoff matrix matrix Figure 1; let p probability
student plays Coop, let q = 1 , p probability student plays
Defect. probabilities function student's Q-values (see description
Section 8.1). Let us assume probabilities p q change considerably
one iteration next. seems especially justified learning rate, ff, small.
Given information, student's expected reward playing Coop?
TFT, teacher's current action student's previous action, assume
teacher play Coop probability p. Thus, student's expected payoff
playing Coop (p + q b). Since Q-learners care discounted future reward
(not current reward), happens next important. Since assumed
student cooperated, teacher cooperate next iteration, still
assume p probability student cooperate next, student's expected
payoff next step (p + q c). ignore higher order terms expected reward
playing Coop becomes: p + q b + (p + q c): expected reward Defect thus:
p c + q + (p b + q d): Therefore, TFT succeed teaching strategy when:

p + q b + (p + q c) > p c + q + (p b + q d):
Since initially p = q = 0:5, behavior stage p q approximately equal determine whether TFT succeeds, attempt predict
success TFT based whether:
DIF = + b + (a + c) , [(c + + (b + d))] 0
test hypothesis ran TFT number matrices using Q-learners different
discount factors. results Figure 7 show fraction Coops 10000 iterations
function DIF teacher using TFT, temperature decay. see
DIF reasonable predictor success. 0, almost rates
20%, 8 rates 65%. However, 0 8 successful.
499

fiBrafman & Tennenholtz

8.4 Teaching Design Tool

Section 6 identified class games challenging teach, previous
sections mostly devoted exploring teaching strategies games
student Q-learner. One assumptions made teacher trying
optimize function student's behavior care
order achieve optimal behavior. However, often teacher would maximize
function depends behavior student's behavior.
case, even simple games discussed Section 6 pose challenge.
section, examine basic coordination problem, block pushing,
objective teaching, teaching essential obtaining good results.
aim section demonstrate point, hence value understanding
embedded teaching. results show teaching strategy achieves much
better performance naive teaching strategy leads behavior much better
two reinforcement learners.
Consider two agents must push block far possible along given path
course 10,000 time units. time unit agent push block along
path, either gently (saving energy) hard (spending much energy). block
move iteration c x h + (2 , x) h units desired direction, h; c > 0
constants x number agents push hard. iteration, agents
paid according distance block pushed. Naturally, agents wish work
little possible paid much possible, payoff iteration
function cost pushing payment received. assume agent prefers
block pushed hard least one agents (guaranteeing reasonable
payment), agent prefers agent one pushing hard.
denote two actions gentle hard, get related game described
follows:
hard gentle
hard (3,3) (2,6)
gentle (6,2) (1,1)

Notice game falls category games teaching easy.
teacher cares student learn push hard, simply push
gently. However, teacher actually trying maximize distance
block moved, teaching strategy may optimal. Notice
20000 instances hard push; naive teaching strategy mentioned yield
10000 instances hard push. order increase number, need
complex teaching strategy.
results use BQL = 0:001. Consider following strategy
teacher: push gently K iterations, start push hard. see,
right selection K , obtain desired behavior. student push hard
time, total number hard push instances improve dramatically.
Figure 8, x coordinate corresponds parameter K , coordinate
corresponds number hard push instances occur 10000 iterations.
results obtained average results 50 trials.
500

fiOn Partially Controlled Multi-Agent Systems

17000
16000
15000
14000
13000
12000
11000
2000

4000

6000

8000

Figure 8: Teaching push hard: number hard push instances student 10000
iterations function number iterations teacher
push hard (avg. 50 trials).
see Figure 8, eciency system non-monotonic
threshold K . behavior obtain appropriate selection K much better
would obtained naive teaching strategy. interesting
note existence sharp phase transition performance neighborhood
optimal K . Finally, mention agents reinforcement learners,
get 7618 instances \push hard", much worse obtained
knowledgeable agent utilizes knowledge uence behavior
agent.

9. Towards General Theory
two case studies presented paper raise natural question whether general,
domain independent techniques PCMAS design exist, whether learned
tools case studies. believe still premature say whether
general theory PCMAS design emerge; requires much additional work. Indeed,
given considerable differences exist two domains explored
paper, given large range multi-agent systems agents envisioned,
doubt existence common low-level techniques PCMAS design. Even within
class rational agents investigated, agents differ considerably
physical, computational, memory capabilities, approach decision making
(e.g., expected utility maximization, maximization worst-case outcomes, minimization
regret). Similarly, problem social-law enforcement take different forms,
example, malicious agents could cooperate among other. However,
abstract view taken, certain important unifying concepts appear, namely, punishment
reward.
Punishment reward abstract descriptions two types high-level feedbacks
controllable agents provide uncontrollable agents. Although punishment
reward take different form meaning two domains, cases, uncon501

fiBrafman & Tennenholtz

trollable agents seem \care" controllable agent's reaction action.
see cases, controllable agents uence uncontrollable agents'
perception worthiness actions. precise manner controllable
agents affect perception differs, cases utilizes inherent aspect
uncertainty uncontrollable agent's world model. case rational agents, despite
perfect knowledge dynamics world, uncertainty remains regarding
outcome non-malicious agents' actions. fixing certain course action
controllable agents, uence malicious agents' perception outcome
actions. case learning agent, one affect perception student's
action affecting basic world model. Hence, seems high-level approach PCMAS design two stages: First, analyze factors uence uncontrollable
agent's perception actions. Next, analyze ability control factors.
retrospect, implicit approach. study social-law enforcement,
used projected game find agent's perception action
changed used indirect mechanism threats enforce perception desired.
study embedded teaching, started analysis different games
possibility affecting agent's perception action games. Next, tried
provide perception. case BQL students, controllable teacher
complete control elements determine student's perception
random nature student's action. Yet, try somehow affect them.
case Q-learners, direct control available factors determining
student's perception. Yet, teacher could control aspects perception,
found sucient.
One might ask representative studies general PCMAS domains,
therefore, relevant insight may provide. chosen two domains
belief represent key aspects types agents studied AI.
AI, study dynamic agents act improve state. agents likely
use information revise assessment state world, much learning
agents, need make decisions based current information, much
expected utility maximizers studied. Hence, typical multi-agent systems studied
AI include agents exhibit one properties.
punishment rewards provide conceptual basis designing controllable agents, MDPs supply natural model many domains. particular, MDPs
suitable uncertainty exists, stemming either agents' choices
nature. showed Section 7, least principle, use established techniques
obtain strategies controllable agents problem phrased Markov
decision process. Using MDP perspective cases would require sophisticates tools number important challenges must met first: (1) assumptions
agent's state fully observable environment's state fully observable
unrealistic many domains. assumptions invalid, obtain partially
observable Markov decision process (POMDP) (Sondik, 1978). Unfortunately, although
POMDPs used principle obtain ideal policy agents, current techniques solving POMDPs limited small problems. Hence, practice one
resort heuristic punishment reward strategies. (2) Section 7
502

fiOn Partially Controlled Multi-Agent Systems

one controlling agent. poses natural challenge generalizing tools techniques
MDPs distributed decision making processes.

10. Summary Related Work
paper introduces distinction controllable uncontrollable agents
concept partially controlled multi-agent systems. provides two problems multi-agent
system design naturally fall framework PCMAS design suggests concrete
techniques uencing behavior uncontrollable agents domains.
work contributes AI research introducing exploring promising perspective
system design contributes DES research considering two types structural
assumptions agents, corresponding rational learning agents.
application approach enforcement social behavior introduces new
tool design multi-agent systems, punishment threats. used notion
investigated part explicit design paradigm. Punishment, deterrence, threats
studied political science (Dixit & Nalebuff, 1991; Schelling, 1980); yet,
difference line work (and related game-theoretic models), consider case
dynamic multi-agent system concentrate punishment design issues,
question minimizing number reliable agents needed control system. Unlike
much work multi-agent systems, assume agents rational agents
law-abiding. Rather, assumed designer control agents
deviations social laws uncontrolled agents need rational.
Notice behavior controllable agents may considered irrational cases;
however, eventually lead desired behavior agents. approaches
negotiations viewed incorporating threats. particular, Rosenschein
Genesereth (1985) consider mechanism making deals among rational agents, agents
asked offer joint strategy followed agents declare move
would take agreement joint strategy. latter move viewed
threat describing implications refusing agent's suggested joint strategy.
example, prisoner's dilemma setting agent may propose joint cooperation
threaten defecting otherwise. work first part paper could viewed
examining threat could credible effective particular context
iterative multi-agent interactions.
part study, proposed embedded teaching situated teaching paradigm
suitable modeling wide range teaching instances. modeled teacher
student players iterated two-player game. concentrated particular
iterative game, showed challenging game type. model,
dynamics teacher-student interaction made explicit, clearly delineated
limits placed teacher's ability uence student. showed
detailed model student, optimal teaching policies theoretically generated
viewing teaching problem Markov decision process. performance
optimal teaching policy serves bound agent's ability uence student.
examined ability teach two types reinforcement learners. particular,
showed optimal policy cannot used, use TFT teaching method.
case Q-learners policy successful. Consequently, proposed model
503

fiBrafman & Tennenholtz

explains success. Finally, showed even games teaching
challenging, nevertheless quite useful. Moreover, objective
simply teaching student, even simpler domains present non-trivial
choices. future hope examine learning architectures see whether
lessons learned domain generalized, whether use methods
accelerate learning domains.
number authors discussed reinforcement learning multi-agent systems.
Yanco Stein (1993) examine evolution communication among cooperative reinforcement learners. Sen et al. (1994) use Q-learning induce cooperation two
block pushing robots. Matraic (1995) Parker (1993) consider use reinforcement
learning physical robots. consider features real robots, discussed
paper. Shoham Tennenholtz (1992) examine evolution conventions
society reinforcement learners. Kittock (1994) investigates effects societal structure multi-agent learning. Littman (1994) develops reinforcement learning techniques
agents whose goals opposed, Tan (1993) examines benefit sharing information
among reinforcement learners. Finally, Whitehead (1991) shown n reinforcement
learners observe everything decrease learning time factor
n. However, work concerned teaching, question
much uence one agent another. Lin (1992) explicitly concerned
teaching way accelerating learning enhanced Q-learners. uses experience replay supplies students examples task achieved. remarked
earlier, teaching approach different ours, since teachers embedded
student's domain. Within game theory extensive body work tries
understand evolution cooperation iterated prisoner's dilemma find
good playing strategies (Eatwell et al., 1989). work players
knowledge, teaching issue.
Last least, work important links work conditioning especially
operant conditioning psychology (Mackintosh, 1983). conditioning experiments
experimenter tries induce changes subjects arranging certain relationships
hold environment, explicitly (in operant conditioning) reinforcing
subjects' actions. framework controlled agent plays similar role
experimenter. work uses control-theoretic approach related problem,
applying two basic AI contexts.
main drawback case studies simple domains conducted. typical initial exploration new problems, future work try
remove limiting assumptions models incorporate. example,
embedded teaching context, assumed uncertainty outcome joint action. Similarly, model multi-agent interaction Section 3
symmetric, assuming agents play k roles game, equally
likely play role, etc. Another assumption made malicious agents
\loners" acting own, opposed team agents. Perhaps importantly,
future work identify additional domains naturally described terms
PCMAS formalize general methodology solving PCMAS design problems.
504

fiOn Partially Controlled Multi-Agent Systems

Acknowledgements
grateful Yoav Shoham members Nobotics group Stanford
input, anonymous referees productive comments suggestions.
especially grateful James Kittock comments help improving
presentation paper. research supported fund promotion
research Technion, NSF grant IRI-9220645, AFOSR grant AF F49620-941-0090.

References

Altenberg, L., & Feldman, M. W. (1987). Selection, generalized transmission,
evolution modifier genes. i. reduction principle. Genetics, 559{572.
Bellman, R. (1962). Dynamic Programming. Princeton University Press.
Bond, A. H., & Gasser, L. (1988). Readings Distributed Artificial Intelligence. Ablex
Publishing Corporation.
Briggs, W., & Cook, D. (1995). Flexible Social Laws. Proc. 14th International Joint
Conference Artificial Intelligence, pp. 688{693.
Dixit, A. K., & Nalebuff, B. J. (1991). Thinking strategically : competitive edge
business, politics, everyday life. Norton, New York.
Durfee, E. H., Lesser, V. R., & Corkill, D. D. (1987). Coherent Cooperation Among Communicating Problem Solvers. IEEE Transactions Computers, 36, 1275{1291.
Dwork, C., & Moses, Y. (1990). Knowledge Common Knowledge Byzantine Environment: Crash Failures. Information Computation, 88 (2), 156{186.
Eatwell, J., Milgate, M., & Newman, P. (Eds.). (1989). New Palgrave: Game Theory.
W.W.Norton & Company, Inc.
Fox, M. S. (1981). organizational view distributed systems. IEEE Trans. Sys., Man.,
Cyber., 11, 70{80.
Fudenberg, D., & Tirole, J. (1991). Game Theory. MIT Press.
Gilboa, I., & Matsui, A. (1991). Social stability equilibrium. Econometrica, 59 (3),
859{867.
Gold, M. (1978). Complexity Automaton Identificaion Given Data. Information
Control, 37, 302{320.
Huberman, B. A., & Hogg, T. (1988). Behavior Computational Ecologies.
Huberman, B. A. (Ed.), Ecology Computation. Elsevier Science.
Kaelbling, L. (1990). Learning embedded systems. Ph.D. thesis, Stanford University.
505

fiBrafman & Tennenholtz

Kandori, M., Mailath, G., & Rob, R. (1991). Learning, Mutation Long Equilibria
Games. Mimeo. University Pennsylvania, 1991.
Kinderman, R., & Snell, S. L. (1980). Markov Random Fields Applications.
American Mathematical Society.
Kittock, J. E. (1994). impact locality authority emergent conventions.
Proceedings Twelfth National Conference Artificial Intelligence (AAAI '94),
pp. 420{425.
Kraus, S., & Wilkenfeld, J. (1991). Function Time Cooperative Negotiations.
Proc. AAAI-91, pp. 179{184.
Lin, F., & Wonham, W. (1988). Decentralized control coordination discrete-event
systems. Proceedings 27th IEEE Conf. Decision Control, pp. 1125{1130.
Lin, L. (1992). Self-improving reactive agents based reinforcement learning, planning,
teaching. Machine Learning, 8 (3{4).
Littman, M. (1994). Markov games framework multi-agent reinforcement learning.
Proc. 11th Int. Conf. Mach. Learn.
Luce, R. D., & Raiffa, H. (1957). Games Decisions- Introduction Critical Survey.
John Wiley Sons.
Mackintosh, N. (1983). Conditioning Associative Learning. Oxford University Press.
Malone, T. W. (1987). Modeling Coordination Organizations Markets. Management
Science, 33 (10), 1317{1332.
Mataric, M. J. (1995). Reward Functions Accelerating Learning. Proceedings
11th international conference Machine Learning, pp. 181{189.
Minsky, N. (1991). imposition protocols open distributed systems. IEEE
Transactions Software Engineering, 17 (2), 183{195.
Moses, Y., & Tennenholtz, M. (1995). Artificial social systems. Computers Artificial
Intelligence, 14 (6), 533{562.
Narendra, K., & Thathachar, M. A. L. (1989). Learning Automata: Introduction.
Prentice Hall.
Owen, G. (1982). Game Theory (2nd Ed.). Academic Press.
Parker, L. E. (1993). Learning Cooperative Robot Teams. Proceedings IJCAI-93
Workshop Dynamically Interacting Robots.
Ramadge, P., & Wonham, W. (1989). Control Discrete Event Systems. Proceedings
IEEE, 77 (1), 81{98.
Rosenschein, J. S., & Genesereth, M. R. (1985). Deals Among Rational Agents. Proc.
9th International Joint Conference Artificial Intelligence, pp. 91{99.
506

fiOn Partially Controlled Multi-Agent Systems

Schelling, T. (1980). Strategy Con ict. Harvard University Press.
Sen, S., Sekaran, M., & Hale, J. (1994). Learning coordinate without sharing information.
Proc. AAAI-94, pp. 426{431.
Shoham, Y., & Tennenholtz, M. (1992). Emergent Conventions Multi-Agent Systems:
initial experimental results observations. Proc. 3rd International Conference Principles Knowledge Representation Reasoning, pp. 225{231.
Shoham, Y., & Tennenholtz, M. (1995). Social Laws Artificial Agent Societies: Off-line
Design. Artificial Inteligence, 73.
Sondik, E. J. (1978). optimal control partially observable markov processes
infinite horizon: Discounted costs. Operations Research, 26 (2).
Sutton, R. (1988). Learning predict method temporal differences. Mach. Lear.,
3 (1), 9{44.
Tan, M. (1993). Multi-Agent Reinforcement Learning: Independent vs. Cooperative Agents.
Proceedings 10th International Conference Machine Learning.
Watkins, C. (1989). Learning Delayed Rewards. Ph.D. thesis, Cambridge University.
Watkins, C., & Dayan, P. (1992). Q-learning. Machine Learning, 8 (3/4), 279{292.
Weidlich, W., & Haag, G. (1983). Concepts Models Quantitative Sociology;
Dynamics Interacting Populations. Springer-Verlag.
Whitehead, S. (1991). complexity analysis cooperative mechanisms reinforcement
learning. Proceedings AAAI-91, pp. 607{613.
Yanco, H., & Stein, L. (1993). Adaptive Communication Protocol Cooperating
Mobile Robots. Animal Animats: Proceedings Second International
Conference Simulation Adaptive Behavior, pp. 478{485.
Zlotkin, G., & Rosenschein, J. S. (1993). Domain Theory Task Oriented Negotiation.
Proc. 13th International Joint Conference Artificial Intelligence, pp. 416{422.

507


