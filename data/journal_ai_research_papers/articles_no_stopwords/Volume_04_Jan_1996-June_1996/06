Journal Artificial Intelligence Research 4 (1996) 397-417

Submitted 12/95; published 6/96

Experimental Evidence Utility
Occam's Razor
Geoffrey I. Webb

webb@deakin.edu.au

School Computing Mathematics
Deakin University
Geelong, Vic, 3217, Australia.

Abstract

paper presents new experimental evidence utility Occam's razor.
systematic procedure presented post-processing decision trees produced C4.5.
procedure derived rejecting Occam's razor instead attending assumption similar objects likely belong class. increases decision
tree's complexity without altering performance tree training data
inferred. resulting complex decision trees demonstrated have,
average, variety common learning tasks, higher predictive accuracy less
complex original decision trees. result raises considerable doubt utility
Occam's razor commonly applied modern machine learning.

1. Introduction

fourteenth century William Occam stated \plurality assumed without necessity". principle since become known Occam's razor. Occam's razor
originally intended basis determining one's ontology. However, modern times
widely reinterpreted adopted epistemological principle|a means
selecting alternative theories well ontologies. Modern reinterpretations
Occam's razor widely employed classification learning. However, utility
principle subject widespread theoretical experimental attack. paper
adds debate providing experimental evidence utility
modern interpretation Occam's razor. evidence takes form systematic procedure adding non-redundant complexity classifiers manner demonstrated
frequently improve predictive accuracy.
modern interpretation Occam's razor characterized \of two hypotheses H H0 , explain E, simpler preferred" (Good, 1977).
However, specify aspect theory measured simplicity.
Syntactic, semantic, epistemological pragmatic simplicity alternative criteria
employed Bunge (1963). practice, common use Occam's razor
machine learning seeks minimize surface syntactic complexity. interpretation
paper addresses.
assumed Occam's razor usually applied expectation
application will, general, lead particular form advantage. widely
accepted articulation precisely Occam's razor applied advantages
expected application classification learning. However, literature
contain two statements seem capture least one widely adopted approach

c 1996 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiWebb

principle. Blumer, Ehrenfeucht, Haussler, Warmuth (1987) suggest wield
Occam's razor adopt goal discovering \the simplest hypothesis consistent
sample data" expectation simplest hypothesis \perform well
observations taken source". Quinlan (1986) states
\Given choice two decision trees, correct
training set, seems sensible prefer simpler one grounds
likely capture structure inherent problem. simpler tree would
therefore expected classify correctly objects outside training set."
statements would necessarily accepted proponents Occam's
razor, capture form Occam's razor paper seeks address|a learning
bias toward classifiers minimize surface syntactic complexity expectation
maximizing predictive accuracy.
statements Occam's razor restrict classifiers
correctly classify objects training set. Many modern machine learning systems
incorporate learning biases tolerate small levels misclassification training data
(Clark & Niblett, 1989; Michalski, 1984; Quinlan, 1986, 1990, example). context,
extending scope definition beyond decision trees classifiers general,
seems reasonable modify Quinlan's (1986) statement (above)
Given choice two plausible classifiers perform identically
training set, simpler classifier expected classify correctly objects
outside training set.
referred Occam thesis.
concept identical performance training set could defined many different
ways. might tempting opt definition requires identical error rates
two classifiers applied training set. less strict interpretation might allow two
classifiers differing error rates long difference within statistical
confidence limit. However, maximize applicability results, paper adopt
strict interpretation identical performance|that every object training
set, classifiers provide classification o.
noted Occam thesis claiming two classifiers
equal empirical support least complex always greater predictive accuracy
previously unseen objects. Rather, claiming frequently less
complex higher predictive accuracy.
paper first examines arguments Occam thesis.
presents new empirical evidence thesis. evidence acquired using
learning algorithm post-processes decision trees learnt C4.5. post-processor
developed rejecting Occam thesis instead attending assumption
similarity predictive class. post-processor systematically adds complexity
decision trees without altering performance training data. demonstrated
lead increase predictive accuracy previously unseen objects range
`real-world' learning tasks. evidence taken incompatible Occam thesis.
398

fiFurther Experimental Evidence Utility Occam's Razor

2. Previous Theoretical Experimental Work
provide context new evidence Occam thesis, worth brie
examining previous relevant theoretical experimental work. relevant, outline
provided reasons contribution may failed persuade
side debate.

2.1 Law Conservation Generalization Performance

conservation law generalization performance (Schaffer, 1994) proves learning
bias outperform bias space possible learning tasks1. follows
Occam's razor valuable learning bias, subset
possible learning tasks. might argued set `real-world' learning tasks
subset.
paper predicated accepting proposition set `real-world' learning
tasks distinguished set possible learning tasks respects render
conservation law inapplicable. Rao, Gordon, Spears (1995) argue case
learning tasks universe uniformly distributed across space
possible learning tasks.
so? One argument support proposition follows.
`Real-world' learning tasks defined people use machine learning systems.
end, task constructors sought ensure independent variables
(class attributes) related dependent variables (other attributes) ways
captured within space classifiers made available learning system.
Actual machine learning tasks drawn randomly space possible learning
tasks. human involvement formulation problems ensures this.
simple thought experiment support proposition, consider learning task
class attribute generated random number generator way
relates attributes. majority machine learning researchers would
slightest disconcerted systems failed perform well trained data.
example, consider learning task class attribute simple count
number missing attribute values object. Assume learning task
submitted system, C4.5 (Quinlan, 1993), develops classifiers
mechanism testing classification whether attribute value missing. Again,
majority machine learning researchers would unconcerned systems failed
perform well circumstances. Machine learning simply unsuited tasks.
knowledgeable user would apply machine learning data, least
expectation obtaining useful classifier therefrom.
paper explores applicability Occam thesis `real-world' learning tasks.

2.2 Theoretical Objections Occam Thesis

machine learning systems explicitly implicitly employ Occam's razor. addition
almost universal use machine learning, principle Occam's razor widely
1. law proved discrete valued learning tasks, reason believe
apply continuous valued tasks

399

fiWebb

accepted general scientific practice. persisted, despite Occam's razor
subjected extensive philosophical, theoretical empirical attack, suggests
attacks found persuasive.
philosophical front, summarize Bunge (1963), complexity theory
(classifier) depends entirely upon language encoded. claim
acceptability theory depends upon language happens expressed
appears indefensible. Further, obvious theoretical relationship syntactic complexity quality theory, possibility world
intrinsically simple use Occam's razor enables discovery intrinsic
simplicity. However, even world intrinsically simple, reason
simplicity correspond syntactic simplicity arbitrary language.
merely state less complex explanation preferable specify
criterion preferable. implicit assumption underlying much machine learning research appears that, things equal, less complex classifiers be,
general, accurate (Blumer et al., 1987; Quinlan, 1986). Occam thesis
paper seeks discredit.
straight-forward interpretation, syntactic measure used predict
expected accuracy appears absurd. two classifiers identical meaning (such
20AGE40 POS 20AGE30 30AGE40 POS)
possible accuracies differ, matter greatly complexities differ.
simple example highlights apparent dominance semantics syntax
determination predictive accuracy.

2.3 Previous Experimental Evidence Occam Thesis
empirical front, number recent experimental results appeared con ict
Occam thesis. Murphy Pazzani (1994) demonstrated number artificial classification learning tasks, simplest consistent decision trees lower predictive
accuracy slightly complex consistent trees. experimentation, however,
showed results dependent upon complexity target concept.
bias toward simplicity performed well target concept best described
simple classifier bias toward complexity performed well target concept
best described complex classifier (Murphy, 1995). addition, simplest classifiers
obtained better average (over consistent classifiers) predictive accuracy
data augmented irrelevant attributes attributes strongly correlated target
concept, required classification.
Webb (1994) presented results suggest wide range learning tasks
UCI repository learning tasks (Murphy & Aha, 1993), relative generality
classifiers better predictor classification performance relative surface
syntactic complexity. However, could argued results demonstrate
strategy selecting simplest pair theories lead maximization
predictive accuracy, demonstrate selecting simplest available
theories would fail maximize predictive accuracy.
Schaffer (1992, 1993) shown pruning techniques reduce complexity
decreasing resubstitution accuracy sometimes increase predictive accuracy sometimes
400

fiFurther Experimental Evidence Utility Occam's Razor

decrease predictive accuracy inferred decision trees. However, proponent Occam
thesis could explain results terms positive effect application Occam's
razor (the reduction complexity) counter-balanced negative effect
reduction empirical support (resubstitution accuracy).
Holte, Acker, Porter (1989) shown specializing small disjuncts (rules
low empirical support) exclude areas instance space occupied training
objects frequently decreases error rate unseen objects covered disjuncts.
specialization involves increasing complexity, might viewed contrary
Occam thesis. However, research shows total error rates classifiers
disjuncts embedded increases disjuncts specialized.
proponent Occam thesis could thus dismiss relevance former results
arguing thesis applies complete classifiers elements
classifiers.

2.4 Theoretical Experimental Support Occam Thesis
theoretical experimental objections Occam thesis exists
body apparent theoretical empirical support.
Several attempts made provide theoretical support Occam thesis
machine learning context (Blumer et al., 1987; Pearl, 1978; Fayyad & Irani, 1990).
However, proofs apply equally systematic learning bias favors small
subset hypothesis space. Indeed, argued equally support
preference classifiers high complexity (Schaffer, 1993; Berkman & Sandholm, 1995).
Holte (1993) compared learning simple classification rules use sophisticated learner complex decision trees. found that, number tasks UCI
repository machine learning datasets (Murphy & Aha, 1993), simple rules achieved
accuracies within percentage points complex trees. could considered
supportive Occam thesis. However, case simple rules outperform
complex decision trees. demonstrated exist yet another
learning bias consistently outperformed studied.
final argument might considered support Occam thesis
majority machine learning systems employ form Occam's razor appear perform well practice. However, demonstrated even better
performance would obtained Occam's razor abandoned.

3. New Experimental Evidence Occam Thesis
theoretical experimental objections Occam thesis appear
greatly diminished machine learning community's use Occam's razor. paper
seeks support objections Occam thesis robust general experimental
counter-evidence. end presents systematic procedure increasing complexity inferred decision trees without modifying performance training data.
procedure takes form post-processor decision trees produced C4.5 (Quinlan, 1993). application procedure range learning tasks UCI
repository learning tasks (Murphy & Aha, 1993) demonstrated result, average,
401

fiWebb

increased predictive accuracy inferred decision trees applied previously
unseen data.

3.1 Theoretical Basis Decision Tree Post-processor
similarity assumption common assumption machine learning|that objects
similar high probability belonging class (Rendell & Seshu, 1990).
techniques described rely upon assumption theoretical justification
rather upon Occam thesis.
Starting similarity assumption, machine learning viewed inference
suitable similarity metric learning task. decision tree viewed
partitioning instance space. partition, represented leaf, contains
objects similar relevant respects thus expected belong
class.
raises issue similarity measured. Instance-based learning methods (Aha, Kibler, & Albert, 1991) tend map instance space onto ndimensional geometric space employ geometric distance measures within
space measure similarity. approach problematic number grounds.
First, assumes underlying metrics different attributes commensurable.
possible determine priori whether difference five years age signifies
greater lesser difference similarity difference one inch height? Second,
assumes possible provide priori definitions similarity respect
single attribute. one really make universal prescription value 16 always
similar value 2 value 64? never case
relevant similarity metric based log2 surface value, case 16 would
similar 64 2?
wish employ induction learn classifiers expressed particular language
would appear forced assume language question manner
captures relevant aspect similarity. potential leaf decision tree presents
plausible similarity metric (all objects fall within leaf similar respect).
Empirical evaluation (the performance leaf training set) used
infer relevance similarity metric induction task hand. leaf l covers
large number objects class c classes, provides evidence
similarity respect tests define l predictive c.
Figure 1 illustrates simple instance space partition C4.5 (Quinlan, 1993)
imposes thereon. Note C4.5 forms nodes continuous attributes, B ,
consist test cut value x. test takes form x. respect
Figure 1 one cut, value 5 attribute A.
C4.5 infers relevant similarity metric relates attribute only. partition
(shown dashed line) placed value 5 attribute A. However, one
accept Occam thesis, accept similarity assumption, reason
believe area instance space B > 5 5 (lightly shaded
Figure 1) belong class + (as determined C4.5) rather class {.
C4.5 uses Occam thesis justify termination partitioning instance
space soon decision tree accounts adequately training set. consequence,
402

fiFurther Experimental Evidence Utility Occam's Razor

..........
.......... { {
..........
.......... { {
.......... {
+
{
+
{
+
1 2 3 4 5 6 7 8 9 10

Figure 1: simple instance space
10
9
8
7
6
B5
4
3
2
1

large areas instance space occupied objects training set may
left within partitions similarity assumption provides little support.
example, respect Figure 1, could argued relevant similarity metric
respect region 5 B > 5 similarity respect B . Within
entire instance space, objects values B > 5 belong class {. five
objects. contrast, three objects values 5 provide
evidence objects area instance space belong class +.
tests represents plausible similarity metric basis available evidence. Thus,
object within region similar plausible respect three positive five
negative objects. objects similar relevant respects high probability
belonging class, information available plausible
object similar three positive five negative objects, would appear
probable object negative positive.
disagreement C4.5 similarity assumption case contrasts
with, example, area instance space 5 B < 1. region,
similarity assumption suggests C4.5's partition appropriate plausible
similarity metrics indicate object region similar positive objects
only2 .
post-processor developed research analyses decision trees produced C4.5
order identify regions|those occupied objects training set
evidence (in terms similarity assumption) favoring relabeling
2. provide example implausible similarity metric, consider similarity metric defined
root node, everything similar. plausible great level
dissimilarity classes respect metric. relevant similarity metric,
distribution training examples representative distribution objects domain
whole, similarity assumption would violated, similar objects would probability
0.58 belonging class. probability calculated follows. probabilities
object + { 0.3 0.7 respectively. object + probability belonging
class another object similar 0.3. object { probability
belonging class another object similar 0.7. Thus, probability
object belonging class another similar object 0:3 0:3 + 0:7 0:7 = 0:58. numbers
involved simple example are, course, small reach conclusion high level
confidence|the example intended illustrative only.

403

fiWebb

different class assigned C4.5. regions identified, new branches
added decision tree, creating new partitionings instance space. trees
must provide identical performance respect training set regions
instance space occupied objects training set affected.
dicult see plausible metric complexity could interpret addition
branches increasing complexity tree.
end result post-processor adds complexity decision tree without
altering tree applies training data. Occam thesis predicts will,
general, lower predictive accuracy similarity assumption predicts will,
general, increase predictive accuracy. seen, latter prediction consistent
experimental evidence former not.

3.2 Post-processor

process could applied continuous discrete attributes,
current implementation addresses continuous attributes.
post-processor operates examining leaf l tree turn. l,
attribute considered turn. a, possible thresholds
region instance space occupied objects l explored. First, minimum
(min) maximum (max) determined values possible objects
reach l. l lies branch split threshold
split provides upper limit (max) values l. lies > branch,
threshold provides lower limit (min). node lie branch,
max = 1. node lie > branch, min = ,1. objects
training set values within range min::max considered
following operations.
value observed training set attribute within allowable range
outside actual range values objects l, evidence evaluated
support reclassifying region threshold. level support
given threshold evaluated using Laplacian accuracy estimate (Niblett & Bratko, 1986).
leaf relates binary classification (an object belongs class question
not), binary form Laplace used. threshold attribute leaf l,
evidence support labeling partition class n maximum value
ancestor node x l formula
P +1
+2
number objects x min < t; P number
objects belong class n.
evidence support labeling partition threshold calculated identically
exception objects < max instead considered.
maximum evidence new labeling exceeds evidence current labeling
region, new branch added appropriate threshold creating new leaf node
labeled appropriate class.
addition evidence favor current labeling gathered above, evidence support current labeling region calculated using Laplace accuracy
404

fiFurther Experimental Evidence Utility Occam's Razor

estimate considering objects leaf, number objects leaf
P number objects belong class node labeled.
approach ensures new partitions define true regions. is,
attribute value v possible partition v unless possible
objects domain values greater v objects values less
equal v reach node partitioned (even though objects
training set fall within new partition). particular, ensures new cuts
simple duplications existing cuts ancestors current node. Thus, every
modification adds non-redundant complexity tree.
algorithm presented Figure 2. implemented modification
C4.5 release 6, called C4.5X. source code modifications available
on-line appendix paper.
C4.5X, multiple sets values equally satisfy specified constraints
maximize Laplace function, values na nb deeper tree selected
closer root and, single node, preference values aa ab depends
upon order attributes definition data preference values va
vb dependent upon data order. selection strategies side effect
implementation system. reason believe experimental results
would differ general strategies used select competing constraints.
default, C4.5 develops two decision trees time run, unpruned
pruned (simplified) decision tree. C4.5X produces post-processed versions
trees.

3.3 Evaluation
evaluate post-processor applied datasets containing continuous attributes
UCI machine learning repository (Murphy & Aha, 1993) held (due
previous machine learning experimentation) local repository Deakin University.
datasets believed broadly representative repository
whole. experimentation eleven data sets, two additional data sets, sick
euthyroid discordant results, retrieved UCI repository added
study order investigate specific issues, discussed below.
resulting thirteen datasets described Table 1. second column contains
number attributes object described. Next proportion
continuous. fourth column indicates proportion attribute values
data missing (unknown). fifth column indicates number objects
data set contains. sixth column indicates proportion belong
class represented objects within data set. final column indicates
number classes data set describes. Note glass type dataset uses
Float/Not Float/Other three class classification rather commonly used six
class classification.
data set divided training evaluation sets 100 times. training
set consisted 80% data, randomly selected. evaluation set consisted
remaining 20% data. C4.5 C4.5X applied resulting 1300
(13 data sets 100 trials) training evaluation set pairs.
405

fiWebb

Let cases(n) denote set training examples reach node n.
Let value(a; x) denote value attribute training example x.
Let pos(X; c) denote number objects class c set training examples X.
Let Laplace(X; c) = ( +2)+1 X set training examples, jX j number training
examples c class.
Let upperlim(n; a) denote minimum value cut attribute ancestor node n
n lies branch. cut, upperlim(n; a) = 1. determines
upper bound values may reach n.
Let lowerlim(n; a) denote maximum value cut attribute ancestor node n
n lies > branch. cut, lowerlim(n; a) = ,1. determines
lower bound values may reach n.
post-process leaf l dominated class c
1. Find values
n : n ancestor l
: continuous attribute
v : 9x : x 2 cases(n ) & v = value(a ; x) & v min(v : 9y : 2 cases(l) & v =
value(a ; y)) & v > lowerlim(l; )
c : c class
maximize L = Laplace(fx : x 2 cases(n ) & value(a ; x) v & value(a ; x) >
lowerlim(l; )g; c ).
2. Find values
n : n ancestor l
: continuous attribute
v : 9x : x 2 cases(n ) & v = value(a ; x) & v > max(v : 9y : 2 cases(l) & v =
value(a ; y)) & v upperlim(l; )
c : c class
maximize L = Laplace(fx : x 2 cases(n ) & value(a ; x) > v & value(a ; x)
upperlim(l; )g; c ).
3. L > Laplace(cases(l); c) & L L
(a) c 6= c
i. replace l node n test v .
ii. set branch n lead new leaf class c .
iii. set > branch n lead l.
else L > Laplace(cases(l); c)
(b) c 6= c
i. replace l node n test v .
ii. set > branch n lead new leaf class c .
iii. set branch n lead l.
pos X;c
jX j

































b

b

b

b



b

b

b

b







b

b

b



b

b

b

b

b

b

b

b



b

b



b









b

b

b

b

b

Figure 2: C4.5X post-processing algorithm
406

fiFurther Experimental Evidence Utility Occam's Razor

Table 1: UCI data sets used experimentation
%
%
No. contin%
No. common No.
Name
Attrs. uous missing objects class classes
breast cancer Wisconsin
9
100
<1
699
66
2
Cleveland heart disease
13
46
<1
303
54
2
credit rating
15
40
1
690
56
2
discordant results
29
24
6
3772
98
2
echocardiogram
6
83
3
74
68
2
glass type
9
100
0
214
40
3
hepatitis
19
32
6
155
79
2
Hungarian heart disease
13
46
20
295
64
2
hypothyroid
29
24
6
3772
92
4
iris
4
100
0
150
33
3
new thyroid
5
100
0
215
70
3
Pima indians diabetes
8
100
0
768
65
2
sick euthyroid
29
24
6
3772
94
2
Table 2 summarizes percentage predictive accuracy obtained unpruned decision trees generated C4.5 C4.5X. presents mean (x) standard
deviation (s) set 100 trials respect data set C4.5
C4.5X along results two-tailed matched pairs t-test comparing means.
twelve thirteen data sets C4.5X obtained higher mean accuracy C4.5.
remaining data set, hypothyroid, C4.5 obtained higher mean predictive accuracy
C4.5CS (albeit small margin|measured two decimal places respective mean accuracies 99.51 99.46, respectively). nine data sets advantage toward
C4.5X statistically significant 0.05 level (p 0:05), although advantage
respect discordant results data small apparent measured one
decimal place (measured two decimal places values 98.58 98.62 respectively).
advantage toward C4.5 hypothyroid data statistically significant
0.05 level. differences mean predictive accuracy Hungarian heart disease,
new thyroid sick euthyroid data sets significant 0.05 level.
Table 3 uses format Table 2 summarize predictive accuracy obtained
pruned decision trees generated C4.5 C4.5X. twelve data
sets C4.5X obtained higher mean predictive accuracy C4.5. remaining data
set, hypothyroid, C4.5 obtained higher mean predictive accuracy, although
magnitude difference small apparent level precision
displayed (measured two decimal places mean accuracies 99.51 99.46).
six data sets advantage toward C4.5X statistically significant 0.05
level, although difference apparent precision two decimal places
discordant results data (99.81 99.82, respectively). advantage toward C4.5
hypothyroid data statistically significant 0.05 level. differences
407

fiWebb

Table 2: Percentage predictive accuracy unpruned decision trees.
Name
breast cancer Wisconsin
Cleveland heart disease
credit rating
discordant results
echocardiogram
glass type
hepatitis
Hungarian heart disease
hypothyroid
iris
new thyroid
Pima indians diabetes
sick euthyroid

C4.5

x

94.1
72.8
82.2
98.6
72.0
74.0
79.6
77.0
99.5
95.4
89.9
70.2
98.7



1.8
5.0
3.4
0.5
9.8
7.0
7.1
5.3
0.2
3.4
4.2
3.5
0.5

C4.5X

x





p

94.4 1.7 {3.2 0.002
74.4 4.8 {6.1 0.000
83.0 3.3 {7.6 0.000
98.6 0.5 {5.4 0.000
73.5 10.2 {2.8 0.007
75.3 7.2 {4.2 0.000
80.8 6.9 {3.3 0.001
77.4 5.2 {1.8 0.082
99.5 0.2 4.4 0.000
95.7 3.5 {2.2 0.028
90.1 4.3 {1.0 0.302
71.3 3.6 {8.1 0.000
98.7 0.5 {0.0 0.963

Table 3: Percentage accuracy pruned decision trees.
Name
breast cancer Wisconsin
Cleveland heart disease
credit rating
discordant results
echocardiogram
glass type
hepatitis
Hungarian heart disease
hypothyroid
iris
new thyroid
Pima indians diabetes
sick euthyroid

C4.5

x

95.1
74.1
84.1
98.8
74.2
74.4
79.9
79.2
99.5
95.4
89.6
72.2
98.7



1.7
5.3
3.2
0.4
9.3
6.9
6.2
4.9
0.2
3.6
4.2
3.5
0.4

C4.5X

x

95.2
74.8
84.6
98.8
75.1
75.4
80.7
79.4
99.5
95.7
89.8
72.8
98.7



1.7
5.3
3.2
0.4
9.8
6.9
6.2
4.8
0.2
3.7
4.2
3.5
0.4



p

{2.0 0.051
{3.7 0.000
{5.3 0.000
{2.6 0.010
{1.6 0.1180
{3.3 0.001
{3.0 0.003
{1.0 0.310
5.4 0.000
{1.6 0.109
{0.8 0.451
{5.9 0.000
{0.7 0.480

breast cancer Wisconsin, echocardiogram, Hungarian heart disease, iris, new thyroid
sick euthyroid statistically significant 0.05 level.
completing experimentation initial eleven data sets, results
hypothyroid data stood stark contrast ten. raised
possibility might distinguishing features hypothyroid data
408

fiFurther Experimental Evidence Utility Occam's Razor

accounted difference performance. Table 1 indicates data set clearly
distinguishable ten initial data sets following six respects|

attributes;
containing greater proportion discrete attributes (which directly addressed
C4.5X);






containing objects;
greater proportion objects belong common class;
classes;
producing decision trees extremely high predictive accuracy without post-processing.

explore issues discordant results sick euthyroid data sets retrieved
UCI repository added study. data sets identical
hypothyroid data set exception different class attribute. three
data sets contain objects, described attributes. addition
discordant results sick euthyroid data little illuminate issue however.
three data sets changes accuracy small magnitude. hypothyroid
significant advantage C4.5. sick euthyroid significant advantage
either system. discordant results data significant advantage C4.5X.
question whether distinguishing feature hypothyroid data
explains observed results remains unanswered. investigation issue lies
beyond scope current paper remains interesting direction future research.
results suggest C4.5X's post-processing frequently increases predictive
accuracy type data found UCI repository. (Of twenty-six
comparisons, significant increase fifteen significant decrease
two. sign test reveals rate success significant 0.05 level,
p = 0:001.)
Tables 4 5 summarize number nodes decision trees developed. Table 4
addresses unpruned decision trees Table 5 addresses pruned decision trees. postprocessing modification replaces single leaf split two leaves. one
modification performed per leaf original tree. data sets postprocessed decision trees significantly complex original decision trees.
cases post-processing increased mean number nodes decision trees
approximately 50%. demonstrates post-processing causing substantial
change.

4. Discussion

primary objective research discredit Occam thesis.
end uses post-processor disregards Occam thesis instead theoretically
founded upon similarity assumption. Experimentation post-processor
409

fiWebb

Table 4: Number nodes unpruned decision trees.
C4.5
C4.5X
Name
x

x


p
breast cancer Wisconsin 38.1 6.0 64.0 10.3 {51.5 0.000
Cleveland heart disease 66.7 7.1 100.2 11.3 {61.9 0.000
credit rating
117.6 18.1 177.9 28.4 {44.2 0.000
discordant results
64.0 10.6 85.2 16.2 {33.3 0.000
echocardiogram
15.4 4.1 22.1 6.3 {26.1 0.000
glass type
43.0 5.2 69.7 8.4 {57.2 0.000
hepatitis
24.5 4.2 34.8 6.0 {49.1 0.000
Hungarian heart disease 62.1 7.5 94.8 13.0 {50.1 0.000
hypothyroid
29.4 4.4 47.5 7.1 {57.8 0.000
iris
9.0 1.9 16.0 4.0 {31.5 0.000
new thyroid
14.7 2.4 23.4 3.8 {41.5 0.000
Pima indians diabetes 164.8 10.8 238.8 16.3 {108.9 0.000
sick euthyroid
71.7 6.6 111.4 12.1 {65.8 0.000
Table 5: Number nodes pruned decision trees.
C4.5

C4.5X

Name
x

x
breast cancer Wisconsin 19.2 5.0 33.1
Cleveland heart disease 44.6 8.3 68.3
credit rating
51.2 14.8 78.4
discordant results
24.9 5.6 32.5
echocardiogram
10.4 3.0 14.8
glass type
36.6 5.5 61.0
hepatitis
13.7 4.8 19.8
Hungarian heart disease 26.8 11.4 41.2
hypothyroid
23.6 2.9 37.1
iris
8.2 1.9 14.8
new thyroid
14.1 2.7 22.5
Pima indians diabetes 112.0 16.4 163.9
sick euthyroid
46.5 5.8 72.6



8.6
12.8
24.2
8.8
4.8
9.5
6.6
17.3
5.6
3.9
4.3
24.0
8.7



{34.9
{43.6
{25.8
{21.1
{21.0
{48.5
{30.7
{22.1
{46.7
{30.3
{36.9
{62.5
{76.7

p

0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000

demonstrated possible develop systematic procedures that, range `realworld' learning tasks increase predictive accuracy inferred decision trees result
changes substantially increase complexity without altering performance
upon training data.
is, general, dicult attack Occam thesis due absence widely
agreed formulation thereof. However, far apparent Occam thesis might
410

fiFurther Experimental Evidence Utility Occam's Razor

..........
. . . .{. . . . . . { {
..........
.......... { {
.......... {
+
{
+
{
+
1 2 3 4 5 6 7 8 9 10

Figure 3: Modified simple instance space
10
9
8
7
6
B5
4
3
2
1

recast accommodate experimental results provide practical learning
bias.

4.1 Directions Future Research
implications research reach beyond relevance Occam's razor. postprocessor appears practical utility increasing quality inferred decision trees.
However, objective research improve predictive accuracy rather
discredit Occam thesis, post-processor would modified number ways.
first modification would enable addition multiple partitions single
leaf original tree. C4.5X selects single modification
maximum support. design decision originated desire minimize likelihood
performing modifications decrease accuracy. principle, however, would
appear desirable select modifications strong support,
could inserted tree order level supporting evidence.
Even greater increases accuracy might expected one removed constraint
post-processing alter performance decision tree respect
training set. case, new partitions may well found employ objects
regions instance space provide evidence support adding partitions
correct misclassifications small numbers objects leaf node original tree.
similarity assumption would provide strong evidence repartitioning.
situation would occur, example, respect learning problem illustrated
Figure 1, additional object class { attribute values A=2 B=9.
illustrated Figure 3. case C4.5 would still create indicated partitions.
However, C4.5X would unable relabel area containing additional object due
constraint alter performance original decision tree respect
training set. Thus addition object prevents C4.5X relabeling shaded
region even though, basis similarity assumption, improves evidence
support relabeling.
extended post-processor would encourage following model inductive inference decision trees. role C4.5 (or similar system) would identify clusters
411

fiWebb

objects within instance space grouped single leaf node.
second stage would analyze regions instance space lie outside clusters
order allocate classes regions. Current decision tree learners, motivated
Occam thesis, ignore second stage, leaving regions outside identified clusters
associated whatever classes assigned by-product cluster
identification process.

4.2 Related Research
number researchers developed learning systems viewed considering
evidence neighboring regions instance space order derive classifications
within regions instance space occupied examples training
set. Ting (1994) explicitly, examining training set directly explore
neighborhood object classified. system uses instance based learning
classification within nodes decision tree low empirical support (small disjuncts).
number systems viewed considering evidence neighboring
regions classification. systems learn apply multiple classifiers (Ali,
Brunk, & Pazzani, 1994; Nock & Gascuel, 1995; Oliver & Hand, 1995). context,
point within region instance space occupied training objects
likely covered multiple leaves rules. these, leaf rule greatest
empirical support used classification.
C4.5X uses two distinct criteria evaluating potential splits. standard C4.5 stage
tree induction employs information measure select splits. post-processor uses
Laplace accuracy estimate. Similar uses dual criteria investigated elsewhere.
Quinlan (1991) employs Laplace accuracy estimate considering neighboring regions
instance space estimate accuracy small disjuncts. Lubinsky (1995) Brodley
(1995) employ resubstitution accuracy select splits near leaves induction
decision trees.
adding split leaf, C4.5X specializing respect class leaf
(and generalizing respect class new leaf). Holte et al. (1989) explored
number techniques specializing small disjuncts. C4.5X differs leaves
candidates specialization, low empirical support. differs
manner selects specialization perform considering evidence
support alternative splits rather strength evidence support
individual potential conditions current disjunct.

4.3 Bias Versus Variance
Breiman, Friedman, Olshen, Stone (1984) provide analysis complexity induction terms trade-off bias variance. classifier partitions instance
space regions. regions large, degree fit accurate partitioning instance space poor, increasing error rates. effect called bias.
regions small, probability individual regions labeled
wrong class increased. effect, called variance, increases error rates. According
analysis, due variance, fine partitioning instance space tends increase
412

fiFurther Experimental Evidence Utility Occam's Razor

error rate while, due bias, coarse partitioning tends increase error
rate.
Increasing complexity decision tree creates finer partitionings instance
space. analysis used argue addition undue complexity
decision trees ground increase variance hence error rate.
However, success C4.5X decreasing error rate demonstrates
successfully managing bias/variance trade-off introduces complexity
decision tree. using evidence neighboring regions instance space, C4.5X
successful increasing error rate resulting variance lower rate
decreases error rate resulting bias. success C4.5X demonstrates
adding undue complexity C4.5's decision trees.

4.4 Minimum Encoding Length Induction

Minimum encoding length approaches perform induction seeking theory enables
compact encoding theory available data. Two key approaches
developed, Minimum Message Length (MML) (Wallace & Boulton, 1968)
Minimum Description Length (MDL) (Rissanen, 1983). approaches admit probabilistic interpretations. Given prior probabilities theories data, minimization
MML encoding closely approximates maximization posterior probability (Wallace & Freeman, 1987). MDL code length defines upper bound \unconditional
likelihood" (Rissanen, 1987).
two approaches differ MDL employs universal prior, Rissanen (1983)
explicitly justifies terms Occam's razor, MML allows specification distinct
appropriate priors induction task. However, practice, default prior usually
employed MML, one appears derive justification Occam's razor.
Neither MDL MML default prior would add complexity decision tree
justified solely basis evidence neighboring regions
instance space. evidence study presented herein appears support
potential desirability so. casts doubt upon utility universal
prior employed MDL default prior usually employed MML, least
respect use maximizing predictive accuracy.
noted, however, probabilistic interpretation minimum
encoding length techniques indicates encoding length minimization represents maximization posterior probability unconditional likelihood. Maximization
factors necessarily directly linked maximizing predictive accuracy.

4.5 Appropriate Application Grafting Pruning

important note although paper calls question value learning
biases penalize complexity, way provide support learning biases
encourage complexity sake. C4.5X grafts new nodes onto decision tree
empirical support so.
results way argue appropriate use decision tree pruning.
generate pruned trees, C4.5 removes branches statistical estimates upper
bounds error rates indicate increase branch removed.
413

fiWebb

could argued C4.5 reduces complexity empirical support
so. interesting note eight thirteen data sets examined, C4.5X's
post-processing pruned trees resulted higher average predictive accuracy
post-processing unpruned trees. results suggest pruning grafting
play valuable role applied appropriately.

5. Conclusion
paper presents systematic procedure adding complexity inferred decision trees
without altering performance training data. procedure demonstrated lead increases predictive accuracy range learning tasks applied
pruned unpruned trees inferred C4.5. one thirteen learning
tasks examined procedure lead statistically significant loss accuracy
case magnitude difference mean accuracy extremely small.
face it, provides strong experimental evidence Occam thesis.
post-processing technique developed rejecting Occam thesis instead attending similarity assumption|that similar objects high probability
belonging class.
procedure developed constrained need ensure revised decision
tree performed identically original decision tree respect training data.
constraint arose desire obtain experimental evidence Occam
thesis. possible constraint removed, basic techniques outlined
paper could result even greater improvements predictive accuracy reported
herein.
research considered one version Occam's razor favors minimization
syntactic complexity expectation tend increase predictive accuracy.
interpretations Occam's razor possible, one minimize
semantic complexity. others (Bunge, 1963) provided philosophical objections
formulations Occam's razor, paper sought investigate them.
version Occam's razor examined research used widely machine
learning apparent success. objections principle substantiated research raise question, apparent success
awed? Webb (1994) suggests apparent success principle due
manner syntactic complexity usually associated relevant qualities
inferred classifiers generality prior probability. thesis accepted one
key challenges facing machine learning understand deeper qualities
employ understanding place machine learning sounder theoretical footing.
paper offers small contribution direction demonstrating minimization
surface syntactic complexity not, itself, general maximize predictive accuracy
inferred classifiers.
nonetheless important realize that, thrust paper notwithstanding,
Occam's razor often useful learning bias employ. frequently good pragmatic reasons preferring simple hypothesis. simple hypothesis
general easier understand, communicate employ. preference simple
414

fiFurther Experimental Evidence Utility Occam's Razor

hypotheses cannot justified terms expected predictive accuracy may justified
pragmatic grounds.

Acknowledgements
research supported Australian Research Council. grateful
Charlie Clelland, David Dowe, Doug Newlands, Ross Quinlan anonymous reviewers
extremely valuable comments paper benefited greatly.

References

Aha, D. W., Kibler, D., & Albert, M. K. (1991). Instance-based learning algorithms.
Machine Learning, 6, 37{66.
Ali, K., Brunk, C., & Pazzani, M. (1994). learning multiple descriptions concept.
Proceedings Tools Artificial Intelligence New Orleans, LA.
Berkman, N. C., & Sandholm, T. W. (1995). minimized decision tree:
re-examination. Technical report 95-20, University Massachusetts Amherst,
Computer Science Department, Amherst, Mass.
Blumer, A., Ehrenfeucht, A., Haussler, D., & Warmuth, M. K. (1987). Occam's Razor.
Information Processing Letters, 24, 377{380.
Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. (1984). Classification
Regression Trees. Wadsworth International, Belmont, Ca.
Brodley, C. E. (1995). Automatic selection split criterion tree growing based
node selection. Proceedings Twelth International Conference Machine
Learning, pp. 73{80 Taho City, Ca. Morgan Kaufmann.
Bunge, M. (1963). Myth Simplicity. Prentice-Hall, Englewood Cliffs, NJ.
Clark, P., & Niblett, T. (1989). CN2 induction algorithm. Machine Learning, 3,
261{284.
Fayyad, U. M., & Irani, K. B. (1990). minimized decision tree?
AAAI-90: Proceedings Eighth National Conference Artificial Intelligence, pp.
749{754 Boston, Ma.
Good, I. J. (1977). Explicativity: mathematical theory explanation statistical
applications. Proceedings Royal Society London Series A, 354, 303{330.
Holte, R. C. (1993). simple classification rules perform well commonly used
datasets. Machine Learning, 11 (1), 63{90.
Holte, R. C., Acker, L. E., & Porter, B. W. (1989). Concept learning problem
small disjuncts. Proceedings Eleventh International Joint Conference
Artificial Intelligence, pp. 813{818 Detroit. Morgan Kaufmann.
415

fiWebb

Lubinsky, D. J. (1995). Increasing performance consistency classification trees
using accuracy criterion leaves. Proceedings Twelth International
Conference Machine Learning, pp. 371{377 Taho City, Ca. Morgan Kaufmann.
Michalski, R. S. (1984). theory methodology inductive learning. Michalski,
R. S., Carbonell, J. G., & Mitchell, T. M. (Eds.), Machine Learning: Artificial
Intelligence Approach, pp. 83{129. Springer-Verlag, Berlin.
Murphy, P. M. (1995). empirical analysis benefit decision tree size biases
function concept distribution. Tech. rep. 95-29, Department Information
Computer Science, University California, Irvine.
Murphy, P. M., & Aha, D. W. (1993). UCI repository machine learning databases.
[Machine-readable data repository]. University California, Department Information Computer Science, Irvine, CA.
Murphy, P. M., & Pazzani, M. J. (1994). Exploring decision forest: empirical investigation Occam's razor decision tree induction. Journal Artificial Intelligence
Research, 1, 257{275.
Niblett, T., & Bratko, I. (1986). Learning decision rules noisy domains. Bramer,
M. A. (Ed.), Research Development Expert Systems III, pp. 25{34. Cambridge
University Press, Cambridge.
Nock, R., & Gascuel, O. (1995). learning decision committees. Proceedings
Twelth International Conference Machine Learning, pp. 413{420 Taho City, Ca.
Morgan Kaufmann.
Oliver, J. J., & Hand, D. J. (1995). pruning averaging decision trees. Proceedings
Twelth International Conference Machine Learning, pp. 430{437 Taho City,
Ca. Morgan Kaufmann.
Pearl, J. (1978). connection complexity credibility inferred
models. International Journal General Systems, 4, 255{264.
Quinlan, J. R. (1986). Induction decision trees. Machine Learning, 1, 81{106.
Quinlan, J. R. (1990). Learning logical definitions relations. Machine Learning, 5,
239{266.
Quinlan, J. R. (1991). Improved estimates accuracy small disjuncts. Machine
Learning, 6, 93{98.
Quinlan, J. R. (1993). C4.5: Programs Machine Learning. Morgan Kaufmann, Los
Altos.
Rao, R. B., Gordon, D., & Spears, W. (1995). every generalization action really
equal opposite reaction? Analysis conservation law generalization performance. Proceedings Twelth International Conference Machine
Learning, pp. 471{479 Taho City, Ca. Morgan Kaufmann.
416

fiFurther Experimental Evidence Utility Occam's Razor

Rendell, L., & Seshu, R. (1990). Learning hard concepts constructive induction:
Framework rationale. Computational Intelligence, 6, 247{270.
Rissanen, J. (1983). universal prior integers estimation minimum description
length. Annals Statistics, 11, 416{431.
Rissanen, J. (1987). Stochastic complexity. Journal Royal Statistical Society Series
B, 49 (3), 223{239.
Schaffer, C. (1992). Sparse data effect overfitting avoidance decision tree
induction. AAAI-92: Proceedings Tenth National Conference Artificial
Intelligence, pp. 147{152 San Jose, CA. AAAI Press.
Schaffer, C. (1993). Overfitting avoidance bias. Machine Learning, 10, 153{178.
Schaffer, C. (1994). conservation law generalization performance. Proceedings
1994 International Conference Machine Learning San Mateo, Ca. Morgan
Kaufmann.
Ting, K. M. (1994). problem small disjuncts: remedy decision trees.
Proceedings Tenth Canadian Conference Artificial Intelligence, pp. 63{70.
Morgan Kaufmann,.
Wallace, C. S., & Boulton, D. M. (1968). information measure classification. Computer Journal, 11, 185{194.
Wallace, C. S., & Freeman, P. R. (1987). Estimation inference compact coding.
Journal Royal Statistical Society Series B, 49 (3), 240{265.
Webb, G. I. (1994). Generality significant complexity: Toward alternatives
Occam's razor. Zhang, C., Debenham, J., & Lukose, D. (Eds.), AI'94 { Proceedings Seventh Australian Joint Conference Artificial Intelligence, pp. 60{67
Armidale. World Scientific.

417


