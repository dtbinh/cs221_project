journal artificial intelligence

submitted published

experimental evidence utility
occam razor
geoffrey webb

webb deakin edu au

school computing mathematics
deakin university
geelong vic australia

abstract

presents experimental evidence utility occam razor
systematic procedure presented post processing decision trees produced c
procedure derived rejecting occam razor instead attending assumption similar objects likely belong class increases decision
tree complexity without altering performance tree training data
inferred resulting complex decision trees demonstrated
average variety common learning tasks higher predictive accuracy less
complex original decision trees raises considerable doubt utility
occam razor commonly applied modern machine learning

introduction

fourteenth century william occam stated plurality assumed without necessity principle since become known occam razor occam razor
originally intended basis determining one ontology however modern times
widely reinterpreted adopted epistemological principle means
selecting alternative theories well ontologies modern reinterpretations
occam razor widely employed classification learning however utility
principle subject widespread theoretical experimental attack
adds debate providing experimental evidence utility
modern interpretation occam razor evidence takes form systematic procedure adding non redundant complexity classifiers manner demonstrated
frequently improve predictive accuracy
modern interpretation occam razor characterized two hypotheses h h explain e simpler preferred good
however specify aspect theory measured simplicity
syntactic semantic epistemological pragmatic simplicity alternative criteria
employed bunge practice common use occam razor
machine learning seeks minimize surface syntactic complexity interpretation
addresses
assumed occam razor usually applied expectation
application general lead particular form advantage widely
accepted articulation precisely occam razor applied advantages
expected application classification learning however literature
contain two statements seem capture least one widely adopted

c ai access foundation morgan kaufmann publishers rights reserved

fiwebb

principle blumer ehrenfeucht haussler warmuth suggest wield
occam razor adopt goal discovering simplest hypothesis consistent
sample data expectation simplest hypothesis perform well
observations taken source quinlan states
given choice two decision trees correct
training set seems sensible prefer simpler one grounds
likely capture structure inherent simpler tree would
therefore expected classify correctly objects outside training set
statements would necessarily accepted proponents occam
razor capture form occam razor seeks address learning
bias toward classifiers minimize surface syntactic complexity expectation
maximizing predictive accuracy
statements occam razor restrict classifiers
correctly classify objects training set many modern machine learning systems
incorporate learning biases tolerate small levels misclassification training data
clark niblett michalski quinlan example context
extending scope definition beyond decision trees classifiers general
seems reasonable modify quinlan statement
given choice two plausible classifiers perform identically
training set simpler classifier expected classify correctly objects
outside training set
referred occam thesis
concept identical performance training set could defined many different
ways might tempting opt definition requires identical error rates
two classifiers applied training set less strict interpretation might allow two
classifiers differing error rates long difference within statistical
confidence limit however maximize applicability adopt
strict interpretation identical performance every object training
set classifiers provide classification
noted occam thesis claiming two classifiers
equal empirical support least complex greater predictive accuracy
previously unseen objects rather claiming frequently less
complex higher predictive accuracy
first examines arguments occam thesis
presents empirical evidence thesis evidence acquired
learning post processes decision trees learnt c post processor
developed rejecting occam thesis instead attending assumption
similarity predictive class post processor systematically adds complexity
decision trees without altering performance training data demonstrated
lead increase predictive accuracy previously unseen objects range
real world learning tasks evidence taken incompatible occam thesis


fifurther experimental evidence utility occam razor

previous theoretical experimental work
provide context evidence occam thesis worth brie
examining previous relevant theoretical experimental work relevant outline
provided reasons contribution may failed persuade
side debate

law conservation generalization performance

conservation law generalization performance schaffer proves learning
bias outperform bias space possible learning tasks follows
occam razor valuable learning bias subset
possible learning tasks might argued set real world learning tasks
subset
predicated accepting proposition set real world learning
tasks distinguished set possible learning tasks respects render
conservation law inapplicable rao gordon spears argue case
learning tasks universe uniformly distributed across space
possible learning tasks
one argument support proposition follows
real world learning tasks defined people use machine learning systems
end task constructors sought ensure independent variables
class attributes related dependent variables attributes ways
captured within space classifiers made available learning system
actual machine learning tasks drawn randomly space possible learning
tasks human involvement formulation ensures
simple thought experiment support proposition consider learning task
class attribute generated random number generator way
relates attributes majority machine learning researchers would
slightest disconcerted systems failed perform well trained data
example consider learning task class attribute simple count
number missing attribute values object assume learning task
submitted system c quinlan develops classifiers
mechanism testing classification whether attribute value missing
majority machine learning researchers would unconcerned systems failed
perform well circumstances machine learning simply unsuited tasks
knowledgeable user would apply machine learning data least
expectation obtaining useful classifier therefrom
explores applicability occam thesis real world learning tasks

theoretical objections occam thesis

machine learning systems explicitly implicitly employ occam razor addition
almost universal use machine learning principle occam razor widely
law proved discrete valued learning tasks reason believe
apply continuous valued tasks



fiwebb

accepted general scientific practice persisted despite occam razor
subjected extensive philosophical theoretical empirical attack suggests
attacks found persuasive
philosophical front summarize bunge complexity theory
classifier depends entirely upon language encoded claim
acceptability theory depends upon language happens expressed
appears indefensible obvious theoretical relationship syntactic complexity quality theory possibility world
intrinsically simple use occam razor enables discovery intrinsic
simplicity however even world intrinsically simple reason
simplicity correspond syntactic simplicity arbitrary language
merely state less complex explanation preferable specify
criterion preferable implicit assumption underlying much machine learning appears things equal less complex classifiers
general accurate blumer et al quinlan occam thesis
seeks discredit
straight forward interpretation syntactic measure used predict
expected accuracy appears absurd two classifiers identical meaning
age pos age age pos
possible accuracies differ matter greatly complexities differ
simple example highlights apparent dominance semantics syntax
determination predictive accuracy

previous experimental evidence occam thesis
empirical front number recent experimental appeared con ict
occam thesis murphy pazzani demonstrated number artificial classification learning tasks simplest consistent decision trees lower predictive
accuracy slightly complex consistent trees experimentation however
showed dependent upon complexity target concept
bias toward simplicity performed well target concept best described
simple classifier bias toward complexity performed well target concept
best described complex classifier murphy addition simplest classifiers
obtained better average consistent classifiers predictive accuracy
data augmented irrelevant attributes attributes strongly correlated target
concept required classification
webb presented suggest wide range learning tasks
uci repository learning tasks murphy aha relative generality
classifiers better predictor classification performance relative surface
syntactic complexity however could argued demonstrate
strategy selecting simplest pair theories lead maximization
predictive accuracy demonstrate selecting simplest available
theories would fail maximize predictive accuracy
schaffer shown pruning techniques reduce complexity
decreasing resubstitution accuracy sometimes increase predictive accuracy sometimes


fifurther experimental evidence utility occam razor

decrease predictive accuracy inferred decision trees however proponent occam
thesis could explain terms positive effect application occam
razor reduction complexity counter balanced negative effect
reduction empirical support resubstitution accuracy
holte acker porter shown specializing small disjuncts rules
low empirical support exclude areas instance space occupied training
objects frequently decreases error rate unseen objects covered disjuncts
specialization involves increasing complexity might viewed contrary
occam thesis however shows total error rates classifiers
disjuncts embedded increases disjuncts specialized
proponent occam thesis could thus dismiss relevance former
arguing thesis applies complete classifiers elements
classifiers

theoretical experimental support occam thesis
theoretical experimental objections occam thesis exists
body apparent theoretical empirical support
several attempts made provide theoretical support occam thesis
machine learning context blumer et al pearl fayyad irani
however proofs apply equally systematic learning bias favors small
subset hypothesis space indeed argued equally support
preference classifiers high complexity schaffer berkman sandholm
holte compared learning simple classification rules use sophisticated learner complex decision trees found number tasks uci
repository machine learning datasets murphy aha simple rules achieved
accuracies within percentage points complex trees could considered
supportive occam thesis however case simple rules outperform
complex decision trees demonstrated exist yet another
learning bias consistently outperformed studied
final argument might considered support occam thesis
majority machine learning systems employ form occam razor appear perform well practice however demonstrated even better
performance would obtained occam razor abandoned

experimental evidence occam thesis
theoretical experimental objections occam thesis appear
greatly diminished machine learning community use occam razor
seeks support objections occam thesis robust general experimental
counter evidence end presents systematic procedure increasing complexity inferred decision trees without modifying performance training data
procedure takes form post processor decision trees produced c quinlan application procedure range learning tasks uci
repository learning tasks murphy aha demonstrated average


fiwebb

increased predictive accuracy inferred decision trees applied previously
unseen data

theoretical basis decision tree post processor
similarity assumption common assumption machine learning objects
similar high probability belonging class rendell seshu
techniques described rely upon assumption theoretical justification
rather upon occam thesis
starting similarity assumption machine learning viewed inference
suitable similarity metric learning task decision tree viewed
partitioning instance space partition represented leaf contains
objects similar relevant respects thus expected belong
class
raises issue similarity measured instance learning methods aha kibler albert tend map instance space onto ndimensional geometric space employ geometric distance measures within
space measure similarity problematic number grounds
first assumes underlying metrics different attributes commensurable
possible determine priori whether difference five years age signifies
greater lesser difference similarity difference one inch height second
assumes possible provide priori definitions similarity respect
single attribute one really make universal prescription value
similar value value never case
relevant similarity metric log surface value case would
similar
wish employ induction learn classifiers expressed particular language
would appear forced assume language question manner
captures relevant aspect similarity potential leaf decision tree presents
plausible similarity metric objects fall within leaf similar respect
empirical evaluation performance leaf training set used
infer relevance similarity metric induction task hand leaf l covers
large number objects class c classes provides evidence
similarity respect tests define l predictive c
figure illustrates simple instance space partition c quinlan
imposes thereon note c forms nodes continuous attributes b
consist test cut value x test takes form x respect
figure one cut value attribute
c infers relevant similarity metric relates attribute partition
shown dashed line placed value attribute however one
accept occam thesis accept similarity assumption reason
believe area instance space b lightly shaded
figure belong class determined c rather class
c uses occam thesis justify termination partitioning instance
space soon decision tree accounts adequately training set consequence


fifurther experimental evidence utility occam razor













figure simple instance space





b





large areas instance space occupied objects training set may
left within partitions similarity assumption provides little support
example respect figure could argued relevant similarity metric
respect region b similarity respect b within
entire instance space objects values b belong class five
objects contrast three objects values provide
evidence objects area instance space belong class
tests represents plausible similarity metric basis available evidence thus
object within region similar plausible respect three positive five
negative objects objects similar relevant respects high probability
belonging class information available plausible
object similar three positive five negative objects would appear
probable object negative positive
disagreement c similarity assumption case contrasts
example area instance space b region
similarity assumption suggests c partition appropriate plausible
similarity metrics indicate object region similar positive objects

post processor developed analyses decision trees produced c
order identify regions occupied objects training set
evidence terms similarity assumption favoring relabeling
provide example implausible similarity metric consider similarity metric defined
root node everything similar plausible great level
dissimilarity classes respect metric relevant similarity metric
distribution training examples representative distribution objects domain
whole similarity assumption would violated similar objects would probability
belonging class probability calculated follows probabilities
object respectively object probability belonging
class another object similar object probability
belonging class another object similar thus probability
object belonging class another similar object numbers
involved simple example course small reach conclusion high level
confidence example intended illustrative



fiwebb

different class assigned c regions identified branches
added decision tree creating partitionings instance space trees
must provide identical performance respect training set regions
instance space occupied objects training set affected
dicult see plausible metric complexity could interpret addition
branches increasing complexity tree
end post processor adds complexity decision tree without
altering tree applies training data occam thesis predicts
general lower predictive accuracy similarity assumption predicts
general increase predictive accuracy seen latter prediction consistent
experimental evidence former

post processor

process could applied continuous discrete attributes
current implementation addresses continuous attributes
post processor operates examining leaf l tree turn l
attribute considered turn possible thresholds
region instance space occupied objects l explored first minimum
min maximum max determined values possible objects
reach l l lies branch split threshold
split provides upper limit max values l lies branch
threshold provides lower limit min node lie branch
max node lie branch min objects
training set values within range min max considered
following operations
value observed training set attribute within allowable range
outside actual range values objects l evidence evaluated
support reclassifying region threshold level support
given threshold evaluated laplacian accuracy estimate niblett bratko
leaf relates binary classification object belongs class question
binary form laplace used threshold attribute leaf l
evidence support labeling partition class n maximum value
ancestor node x l formula
p

number objects x min p number
objects belong class n
evidence support labeling partition threshold calculated identically
exception objects max instead considered
maximum evidence labeling exceeds evidence current labeling
region branch added appropriate threshold creating leaf node
labeled appropriate class
addition evidence favor current labeling gathered evidence support current labeling region calculated laplace accuracy


fifurther experimental evidence utility occam razor

estimate considering objects leaf number objects leaf
p number objects belong class node labeled
ensures partitions define true regions
attribute value v possible partition v unless possible
objects domain values greater v objects values less
equal v reach node partitioned even though objects
training set fall within partition particular ensures cuts
simple duplications existing cuts ancestors current node thus every
modification adds non redundant complexity tree
presented figure implemented modification
c release called c x source code modifications available
line appendix
c x multiple sets values equally satisfy specified constraints
maximize laplace function values na nb deeper tree selected
closer root single node preference values aa ab depends
upon order attributes definition data preference values va
vb dependent upon data order selection strategies side effect
implementation system reason believe experimental
would differ general strategies used select competing constraints
default c develops two decision trees time run unpruned
pruned simplified decision tree c x produces post processed versions
trees

evaluation
evaluate post processor applied datasets containing continuous attributes
uci machine learning repository murphy aha held due
previous machine learning experimentation local repository deakin university
datasets believed broadly representative repository
whole experimentation eleven data sets two additional data sets sick
euthyroid discordant retrieved uci repository added
study order investigate specific issues discussed
resulting thirteen datasets described table second column contains
number attributes object described next proportion
continuous fourth column indicates proportion attribute values
data missing unknown fifth column indicates number objects
data set contains sixth column indicates proportion belong
class represented objects within data set final column indicates
number classes data set describes note glass type dataset uses
float float three class classification rather commonly used six
class classification
data set divided training evaluation sets times training
set consisted data randomly selected evaluation set consisted
remaining data c c x applied resulting
data sets trials training evaluation set pairs


fiwebb

let cases n denote set training examples reach node n
let value x denote value attribute training example x
let pos x c denote number objects class c set training examples x
let laplace x c x set training examples jx j number training
examples c class
let upperlim n denote minimum value cut attribute ancestor node n
n lies branch cut upperlim n determines
upper bound values may reach n
let lowerlim n denote maximum value cut attribute ancestor node n
n lies branch cut lowerlim n determines
lower bound values may reach n
post process leaf l dominated class c
values
n n ancestor l
continuous attribute
v x x cases n v value x v min v cases l v
value v lowerlim l
c c class
maximize l laplace fx x cases n value x v value x
lowerlim l g c
values
n n ancestor l
continuous attribute
v x x cases n v value x v max v cases l v
value v upperlim l
c c class
maximize l laplace fx x cases n value x v value x
upperlim l g c
l laplace cases l c l l
c c
replace l node n test v
ii set branch n lead leaf class c
iii set branch n lead l
else l laplace cases l c
b c c
replace l node n test v
ii set branch n lead leaf class c
iii set branch n lead l
pos x c
jx j

































b

b

b

b



b

b

b

b







b

b

b



b

b

b

b

b

b

b

b



b

b



b









b

b

b

b

b

figure c x post processing


fifurther experimental evidence utility occam razor

table uci data sets used experimentation


contin
common
name
attrs uous missing objects class classes
breast cancer wisconsin






cleveland heart disease






credit rating






discordant






echocardiogram






glass type






hepatitis






hungarian heart disease






hypothyroid






iris






thyroid






pima indians diabetes






sick euthyroid






table summarizes percentage predictive accuracy obtained unpruned decision trees generated c c x presents mean x standard
deviation set trials respect data set c
c x along two tailed matched pairs test comparing means
twelve thirteen data sets c x obtained higher mean accuracy c
remaining data set hypothyroid c obtained higher mean predictive accuracy
c cs albeit small margin measured two decimal places respective mean accuracies respectively nine data sets advantage toward
c x statistically significant level p although advantage
respect discordant data small apparent measured one
decimal place measured two decimal places values respectively
advantage toward c hypothyroid data statistically significant
level differences mean predictive accuracy hungarian heart disease
thyroid sick euthyroid data sets significant level
table uses format table summarize predictive accuracy obtained
pruned decision trees generated c c x twelve data
sets c x obtained higher mean predictive accuracy c remaining data
set hypothyroid c obtained higher mean predictive accuracy although
magnitude difference small apparent level precision
displayed measured two decimal places mean accuracies
six data sets advantage toward c x statistically significant
level although difference apparent precision two decimal places
discordant data respectively advantage toward c
hypothyroid data statistically significant level differences


fiwebb

table percentage predictive accuracy unpruned decision trees
name
breast cancer wisconsin
cleveland heart disease
credit rating
discordant
echocardiogram
glass type
hepatitis
hungarian heart disease
hypothyroid
iris
thyroid
pima indians diabetes
sick euthyroid

c

x































c x

x





p















table percentage accuracy pruned decision trees
name
breast cancer wisconsin
cleveland heart disease
credit rating
discordant
echocardiogram
glass type
hepatitis
hungarian heart disease
hypothyroid
iris
thyroid
pima indians diabetes
sick euthyroid

c

x































c x

x

































p















breast cancer wisconsin echocardiogram hungarian heart disease iris thyroid
sick euthyroid statistically significant level
completing experimentation initial eleven data sets
hypothyroid data stood stark contrast ten raised
possibility might distinguishing features hypothyroid data


fifurther experimental evidence utility occam razor

accounted difference performance table indicates data set clearly
distinguishable ten initial data sets following six respects

attributes
containing greater proportion discrete attributes directly addressed
c x






containing objects
greater proportion objects belong common class
classes
producing decision trees extremely high predictive accuracy without post processing

explore issues discordant sick euthyroid data sets retrieved
uci repository added study data sets identical
hypothyroid data set exception different class attribute three
data sets contain objects described attributes addition
discordant sick euthyroid data little illuminate issue however
three data sets changes accuracy small magnitude hypothyroid
significant advantage c sick euthyroid significant advantage
system discordant data significant advantage c x
question whether distinguishing feature hypothyroid data
explains observed remains unanswered investigation issue lies
beyond scope current remains interesting direction future
suggest c x post processing frequently increases predictive
accuracy type data found uci repository twenty six
comparisons significant increase fifteen significant decrease
two sign test reveals rate success significant level
p
tables summarize number nodes decision trees developed table
addresses unpruned decision trees table addresses pruned decision trees postprocessing modification replaces single leaf split two leaves one
modification performed per leaf original tree data sets postprocessed decision trees significantly complex original decision trees
cases post processing increased mean number nodes decision trees
approximately demonstrates post processing causing substantial
change

discussion

primary objective discredit occam thesis
end uses post processor disregards occam thesis instead theoretically
founded upon similarity assumption experimentation post processor


fiwebb

table number nodes unpruned decision trees
c
c x
name
x

x


p
breast cancer wisconsin
cleveland heart disease
credit rating

discordant

echocardiogram

glass type

hepatitis

hungarian heart disease
hypothyroid

iris

thyroid

pima indians diabetes
sick euthyroid

table number nodes pruned decision trees
c

c x

name
x

x
breast cancer wisconsin
cleveland heart disease
credit rating

discordant

echocardiogram

glass type

hepatitis

hungarian heart disease
hypothyroid

iris

thyroid

pima indians diabetes
sick euthyroid


































p















demonstrated possible develop systematic procedures range realworld learning tasks increase predictive accuracy inferred decision trees
changes substantially increase complexity without altering performance
upon training data
general dicult attack occam thesis due absence widely
agreed formulation thereof however far apparent occam thesis might


fifurther experimental evidence utility occam razor













figure modified simple instance space





b





recast accommodate experimental provide practical learning
bias

directions future
implications reach beyond relevance occam razor postprocessor appears practical utility increasing quality inferred decision trees
however objective improve predictive accuracy rather
discredit occam thesis post processor would modified number ways
first modification would enable addition multiple partitions single
leaf original tree c x selects single modification
maximum support design decision originated desire minimize likelihood
performing modifications decrease accuracy principle however would
appear desirable select modifications strong support
could inserted tree order level supporting evidence
even greater increases accuracy might expected one removed constraint
post processing alter performance decision tree respect
training set case partitions may well found employ objects
regions instance space provide evidence support adding partitions
correct misclassifications small numbers objects leaf node original tree
similarity assumption would provide strong evidence repartitioning
situation would occur example respect learning illustrated
figure additional object class attribute values b
illustrated figure case c would still create indicated partitions
however c x would unable relabel area containing additional object due
constraint alter performance original decision tree respect
training set thus addition object prevents c x relabeling shaded
region even though basis similarity assumption improves evidence
support relabeling
extended post processor would encourage following model inductive inference decision trees role c similar system would identify clusters


fiwebb

objects within instance space grouped single leaf node
second stage would analyze regions instance space lie outside clusters
order allocate classes regions current decision tree learners motivated
occam thesis ignore second stage leaving regions outside identified clusters
associated whatever classes assigned product cluster
identification process

related
number researchers developed learning systems viewed considering
evidence neighboring regions instance space order derive classifications
within regions instance space occupied examples training
set ting explicitly examining training set directly explore
neighborhood object classified system uses instance learning
classification within nodes decision tree low empirical support small disjuncts
number systems viewed considering evidence neighboring
regions classification systems learn apply multiple classifiers ali
brunk pazzani nock gascuel oliver hand context
point within region instance space occupied training objects
likely covered multiple leaves rules leaf rule greatest
empirical support used classification
c x uses two distinct criteria evaluating potential splits standard c stage
tree induction employs information measure select splits post processor uses
laplace accuracy estimate similar uses dual criteria investigated elsewhere
quinlan employs laplace accuracy estimate considering neighboring regions
instance space estimate accuracy small disjuncts lubinsky brodley
employ resubstitution accuracy select splits near leaves induction
decision trees
adding split leaf c x specializing respect class leaf
generalizing respect class leaf holte et al explored
number techniques specializing small disjuncts c x differs leaves
candidates specialization low empirical support differs
manner selects specialization perform considering evidence
support alternative splits rather strength evidence support
individual potential conditions current disjunct

bias versus variance
breiman friedman olshen stone provide analysis complexity induction terms trade bias variance classifier partitions instance
space regions regions large degree fit accurate partitioning instance space poor increasing error rates effect called bias
regions small probability individual regions labeled
wrong class increased effect called variance increases error rates according
analysis due variance fine partitioning instance space tends increase


fifurther experimental evidence utility occam razor

error rate due bias coarse partitioning tends increase error
rate
increasing complexity decision tree creates finer partitionings instance
space analysis used argue addition undue complexity
decision trees ground increase variance hence error rate
however success c x decreasing error rate demonstrates
successfully managing bias variance trade introduces complexity
decision tree evidence neighboring regions instance space c x
successful increasing error rate resulting variance lower rate
decreases error rate resulting bias success c x demonstrates
adding undue complexity c decision trees

minimum encoding length induction

minimum encoding length approaches perform induction seeking theory enables
compact encoding theory available data two key approaches
developed minimum message length mml wallace boulton
minimum description length mdl rissanen approaches admit probabilistic interpretations given prior probabilities theories data minimization
mml encoding closely approximates maximization posterior probability wallace freeman mdl code length defines upper bound unconditional
likelihood rissanen
two approaches differ mdl employs universal prior rissanen
explicitly justifies terms occam razor mml allows specification distinct
appropriate priors induction task however practice default prior usually
employed mml one appears derive justification occam razor
neither mdl mml default prior would add complexity decision tree
justified solely basis evidence neighboring regions
instance space evidence study presented herein appears support
potential desirability casts doubt upon utility universal
prior employed mdl default prior usually employed mml least
respect use maximizing predictive accuracy
noted however probabilistic interpretation minimum
encoding length techniques indicates encoding length minimization represents maximization posterior probability unconditional likelihood maximization
factors necessarily directly linked maximizing predictive accuracy

appropriate application grafting pruning

important note although calls question value learning
biases penalize complexity way provide support learning biases
encourage complexity sake c x grafts nodes onto decision tree
empirical support
way argue appropriate use decision tree pruning
generate pruned trees c removes branches statistical estimates upper
bounds error rates indicate increase branch removed


fiwebb

could argued c reduces complexity empirical support
interesting note eight thirteen data sets examined c x
post processing pruned trees resulted higher average predictive accuracy
post processing unpruned trees suggest pruning grafting
play valuable role applied appropriately

conclusion
presents systematic procedure adding complexity inferred decision trees
without altering performance training data procedure demonstrated lead increases predictive accuracy range learning tasks applied
pruned unpruned trees inferred c one thirteen learning
tasks examined procedure lead statistically significant loss accuracy
case magnitude difference mean accuracy extremely small
face provides strong experimental evidence occam thesis
post processing technique developed rejecting occam thesis instead attending similarity assumption similar objects high probability
belonging class
procedure developed constrained need ensure revised decision
tree performed identically original decision tree respect training data
constraint arose desire obtain experimental evidence occam
thesis possible constraint removed basic techniques outlined
could even greater improvements predictive accuracy reported
herein
considered one version occam razor favors minimization
syntactic complexity expectation tend increase predictive accuracy
interpretations occam razor possible one minimize
semantic complexity others bunge provided philosophical objections
formulations occam razor sought investigate
version occam razor examined used widely machine
learning apparent success objections principle substantiated raise question apparent success
awed webb suggests apparent success principle due
manner syntactic complexity usually associated relevant qualities
inferred classifiers generality prior probability thesis accepted one
key challenges facing machine learning understand deeper qualities
employ understanding place machine learning sounder theoretical footing
offers small contribution direction demonstrating minimization
surface syntactic complexity general maximize predictive accuracy
inferred classifiers
nonetheless important realize thrust notwithstanding
occam razor often useful learning bias employ frequently good pragmatic reasons preferring simple hypothesis simple hypothesis
general easier understand communicate employ preference simple


fifurther experimental evidence utility occam razor

hypotheses cannot justified terms expected predictive accuracy may justified
pragmatic grounds

acknowledgements
supported australian council grateful
charlie clelland david dowe doug newlands ross quinlan anonymous reviewers
extremely valuable comments benefited greatly

references

aha w kibler albert k instance learning
machine learning
ali k brunk c pazzani learning multiple descriptions concept
proceedings tools artificial intelligence orleans la
berkman n c sandholm w minimized decision tree
examination technical report university massachusetts amherst
computer science department amherst mass
blumer ehrenfeucht haussler warmuth k occam razor
information processing letters
breiman l friedman j h olshen r stone c j classification
regression trees wadsworth international belmont ca
brodley c e automatic selection split criterion tree growing
node selection proceedings twelth international conference machine
learning pp taho city ca morgan kaufmann
bunge myth simplicity prentice hall englewood cliffs nj
clark p niblett cn induction machine learning

fayyad u irani k b minimized decision tree
aaai proceedings eighth national conference artificial intelligence pp
boston
good j explicativity mathematical theory explanation statistical
applications proceedings royal society london series
holte r c simple classification rules perform well commonly used
datasets machine learning
holte r c acker l e porter b w concept learning
small disjuncts proceedings eleventh international joint conference
artificial intelligence pp detroit morgan kaufmann


fiwebb

lubinsky j increasing performance consistency classification trees
accuracy criterion leaves proceedings twelth international
conference machine learning pp taho city ca morgan kaufmann
michalski r theory methodology inductive learning michalski
r carbonell j g mitchell eds machine learning artificial
intelligence pp springer verlag berlin
murphy p empirical analysis benefit decision tree size biases
function concept distribution tech rep department information
computer science university california irvine
murphy p aha w uci repository machine learning databases
machine readable data repository university california department information computer science irvine ca
murphy p pazzani j exploring decision forest empirical investigation occam razor decision tree induction journal artificial intelligence

niblett bratko learning decision rules noisy domains bramer
ed development expert systems iii pp cambridge
university press cambridge
nock r gascuel learning decision committees proceedings
twelth international conference machine learning pp taho city ca
morgan kaufmann
oliver j j hand j pruning averaging decision trees proceedings
twelth international conference machine learning pp taho city
ca morgan kaufmann
pearl j connection complexity credibility inferred
international journal general systems
quinlan j r induction decision trees machine learning
quinlan j r learning logical definitions relations machine learning

quinlan j r improved estimates accuracy small disjuncts machine
learning
quinlan j r c programs machine learning morgan kaufmann los
altos
rao r b gordon spears w every generalization action really
equal opposite reaction analysis conservation law generalization performance proceedings twelth international conference machine
learning pp taho city ca morgan kaufmann


fifurther experimental evidence utility occam razor

rendell l seshu r learning hard concepts constructive induction
framework rationale computational intelligence
rissanen j universal prior integers estimation minimum description
length annals statistics
rissanen j stochastic complexity journal royal statistical society series
b
schaffer c sparse data effect overfitting avoidance decision tree
induction aaai proceedings tenth national conference artificial
intelligence pp san jose ca aaai press
schaffer c overfitting avoidance bias machine learning
schaffer c conservation law generalization performance proceedings
international conference machine learning san mateo ca morgan
kaufmann
ting k small disjuncts remedy decision trees
proceedings tenth canadian conference artificial intelligence pp
morgan kaufmann
wallace c boulton information measure classification computer journal
wallace c freeman p r estimation inference compact coding
journal royal statistical society series b
webb g generality significant complexity toward alternatives
occam razor zhang c debenham j lukose eds ai proceedings seventh australian joint conference artificial intelligence pp
armidale world scientific




