journal artificial intelligence

submitted published

committee sample selection
probabilistic classifiers
shlomo argamon engelson

department computer science
jerusalem college technology machon lev
p b
jerusalem israel

argamon mail jct ac il

ido dagan

department mathematics computer science
bar ilan university
ramat gan israel

dagan cs biu ac il

abstract
many real world learning tasks expensive acquire sucient number labeled
examples training investigates methods reducing annotation cost
sample selection training learning program examines many
unlabeled examples selects labeling informative
stage avoids redundantly labeling examples contribute little information
work follows previous query committee extends
committee paradigm context probabilistic classification describe
family empirical methods committee sample selection probabilistic classification evaluate informativeness example measuring degree
disagreement several model variants variants committee drawn
randomly probability distribution conditioned training set labeled far
method applied real world natural language processing task stochastic part speech tagging variants method achieve significant
reduction annotation cost although computational eciency differs particular
simplest variant two member committee parameters tune gives excellent
sample selection yields significant reduction size
model used tagger

introduction
supervised concept learning build classifiers concept given
set labeled examples many real world concept learning tasks however acquiring
labeled training examples expensive hence objective develop automated
methods reduce training cost within framework active learning
learner control choice examples labeled used
training
c ai access foundation morgan kaufmann publishers rights reserved

fiargamon dagan

two main types active learning first uses membership queries
learner constructs examples asks teacher label angluin mackay
b plutowski white provides proven computational
advantages angluin applicable since possible
construct meaningful informative unlabeled examples training diculty may
overcome large set unlabeled training data available case second
type active learning sample selection often applied learner examines many
unlabeled examples selects informative ones learning seung opper
sompolinsky freund seung shamir tishby cohn atlas ladner
lewis catlett lewis gale
address sample selection training probabilistic
classifier classification framework performed probability model
given input example assigns score possible classification selects
highest score
follows theoretical work sample selection query committee
qbc paradigm seung et al freund et al propose novel empirical
scheme applying qbc paradigm probabilistic classification allowing label
noise addressed original qbc framework see section
committee selection scheme learner receives stream unlabeled examples
input decides whether ask label end
learner constructs committee two classifiers statistics
current training set committee member classifies candidate example
learner measures degree disagreement among committee members example
selected labeling depending degree disagreement according selection
protocol
previous work dagan engelson engelson dagan b presented
particular selection protocol probabilistic concepts extends previous
work mainly generalizing selection scheme comparing variety different
selection protocols preliminary version appeared engelson dagan

application natural language processing
much early work sample selection theoretical nature
tested toy however motivated complex real world
area statistical natural language text processing work addresses
task part speech tagging core task statistical natural language processing
nlp work sample selection natural language tasks mainly focused
text categorization works lewis catlett liere
tadepalli mccallum nigam
statistical nlp probabilistic classifiers often used select preferred analysis
linguistic structure text syntactic structure black jelinek lafferty
magerman mercer roukos word categories church word senses gale


ficommittee sample selection probabilistic classifiers

church yarowsky parameters classification model estimated
training corpus collection text
common case supervised training learner uses corpus
sentence manually annotated correct analysis manual annotation typically
expensive consequence large annotated corpora exist mainly english language covering genres text situation makes dicult apply
supervised learning methods languages english adapt systems different genres text furthermore infeasible many cases develop supervised
methods require annotations different currently available
cases manual annotation avoided altogether self organized methods shown part speech tagging english kupiec even
kupiec tagger though manual somewhat unprincipled biasing initial model
necessary achieve satisfactory convergence elworthy merialdo
investigated effect self converging estimation part speech tagging
found initial manual training needed generally supervised
training provided better fact fully unsupervised methods
applicable many nlp tasks perhaps even part speech tagging
languages sample selection appropriate way reduce cost annotating corpora
easy obtain large volumes raw text smaller subsets selected
annotation
applied committee selection learning hidden markov hmms
part speech tagging english sentences part speech tagging task labeling
word sentence appropriate part speech example labeling
occurrence word hand noun verb task non trivial since determining
word part speech depends linguistic context hmms used extensively
task e g church merialdo cases trained corpora
manually annotated correct part speech word
experiments part speech tagging described section committeebased selection substantially faster learning rates enabling learner achieve
given level accuracy far fewer training examples sequential training
text

background
objective sample selection select examples informative future might determine informativeness example one
derive explicit measure expected amount information gained
example cohn ghahramani jordan mackay b
example mackay b assesses informativeness example neural network
learning task expected decrease overall variance model prediction
training example explicit measures appealing since attempt
give precise characterization information content example membership querying explicit formulation information content sometimes enables finding


fiargamon dagan

informative examples analytically saving cost searching example space
use explicit methods may limited however since explicit measures generally
model specific b complex often requiring approximations practical
c depend accuracy current hypothesis given step
alternative measuring informativeness example explicitly measure
implicitly quantifying amount uncertainty classification example
given current training data informativeness example evaluated respect
derived training data stage learning one use
single model training data seen far taken lewis
gale training binary classifier select training examples
whose classification probability closest e examples current
best model uncertain
order better evaluate classification uncertainty respect entire space
possible one may instead measure classification disagreement among sample
set possible committee entire model space enables measuring
degree training entails single best classification example
hand referring single model measures degree model
certain classification example classifier sucient training predicting ips coin heads probability predict heads hence
make mistakes time however although classifier quite uncertain
correctness classification additional training improve accuracy
two main approaches generating committee order evaluate example
uncertainty version space random sampling version
space pursued cohn et al seeks choose committee members
border space allowed training data version space
mitchell thus chosen committee far
possible consistent training data ensures
disagree example whenever training example would restrict version space
version space dicult apply since finding edge
version space non trivial general furthermore directly
applicable case probabilistic classification almost
possible though equally probable given training alternative random sampling exemplified query committee seung et al freund
et al inspired sampled randomly
set possible according probability given
training data work applies random sampling probabilistic classifiers
computing approximation posterior model distribution given training data
generating committee members distribution mccallum nigam
use similar sample selection text categorization naive bayes classifier primary difference skew example selection density weighted
sampling documents similar many documents training
set selected labeling higher probability



ficommittee sample selection probabilistic classifiers

matan presents two methods random sampling first method
trains committee members different subsets training data second method
neural network matan generates committee members backpropagation training different initial weights networks reach different local minima
similar taken liere tadepalli applied committee
selection text categorization winnow learning littlestone
learns linear classifiers represented model space set classifiers model set classifier model set learns independently labeled
examples initialized different initial hypothesis thus point
set gives selection possible hypotheses given training data labeling decisions
performed two chosen random model set
disagree document class document label requested
space updated

query committee
mentioned follows theoretical work sample selection query
committee qbc paradigm seung et al freund et al method
proposed learning binary non probabilistic concepts cases exists prior
probability distribution measure concept class qbc selects informative training
examples stream unlabeled examples example selected learner
queries teacher correct label adds training set examples
selected training restrict set consistent concepts e set concepts
label training examples correctly version space
simple version qbc analyzed freund et al see
summary freund uses following selection
draw unlabeled input example random probability distribution example space
select random two hypotheses according prior probability distribution concept class restricted set consistent concepts
select example training two hypotheses disagree classification
freund et al prove assumptions achieves exponential
reduction number labeled examples required achieve desired classification
accuracy compared random selection training examples speedup achieved
tends select examples split version space two parts
similar size one parts eliminated version space example
correct label added training set

selection probabilistic classifiers
address sample selection training probabilistic classifier classification framework performed probabilistic model given input


fiargamon dagan

example assigns probability probability score possible classification
selects best classification probabilistic classifiers fall within framework
addressed theoretical qbc work training probabilistic classifier involves estimating values model parameters determine probability estimate possible
classification example expect cases optimal classifier
assign highest probability correct class guaranteed occur
accordingly notion consistent hypothesis generally applicable probabilistic
classifiers thus posterior distribution classifiers given training data cannot
defined restriction prior set consistent hypotheses rather within
bayesian framework posterior distribution defined statistics training
set assigning higher probability classifiers likely given statistics
discuss desired properties examples selected training generally speaking training example contributes data several statistics turn
determine estimates several parameter values informative example therefore
one whose contribution statistics leads useful improvement parameter estimates assuming existence optimal classification model given concept
maximum likelihood model identify three properties parameters
acquiring additional statistics beneficial
current estimate parameter uncertain due insucient statistics
training set uncertain estimate likely far true value
parameter cause incorrect classification additional statistics would bring
estimate closer true value
classification sensitive changes current estimate parameter otherwise acquiring additional statistics unlikely affect classification therefore
beneficial
parameter takes part calculating class probabilities large proportion
examples parameters relevant classifying examples determined probability distribution input examples low utility future
estimation
committee selection scheme describe tends select
examples affect parameters three properties property addressed
randomly picking parameter values committee members posterior distribution
parameter estimates given current statistics statistics parameter
insucient variance posterior distribution estimates large hence
large differences values parameter picked different committee
members note property addressed uncertainty classification
judged relative single model e g lewis gale
captures uncertainty respect given parameter values sense property
model uncertainty choice values first place use
single model criticized cohn et al
property addressed selecting examples committee members highly disagree classification thus tends acquire statistics uncertainty


ficommittee sample selection probabilistic classifiers

parameter estimates entails uncertainty actual classification analogous splitting
version space qbc finally property addressed independently examining
input examples drawn input distribution way implicitly
model expected utility statistics classifying future examples

outline
following section defines basic concepts notation use rest
section presents general selection scheme along variant selection
next two sections demonstrate effectiveness sample selection
scheme section presents artificial colorful coin ipper providing
simple illustration operation proposed system section presents
task stochastic part speech tagging demonstrating usefulness committeebased sample selection real world

definitions
concern minimize number labeled examples needed
learn classifier accurately classifies input examples e classes c c c
known set possible classes learning stream unlabeled examples
supplied free examples drawn unknown probability distribution
cost however learning obtain true label given example
objective reduce cost much possible still learning accurate
classifier
address specific case probabilistic classifiers classification done
basis score function fm c e assigns score possible class
input example classifier assigns input example class highest score
fm determined probabilistic model many applications fm conditional
probability function pm cje specifying probability class given example
alternatively score functions denote likelihood class may used
odds ratio particular type model used classification determines
specific form score function features example
probabilistic model thus score function fm defined set parameters fffi g giving probabilities possible events example model
part speech tagging contains parameters probability particular word
verb noun training values parameters estimated
set statistics extracted training set labeled examples particular model
denoted faig ai specific value corresponding ffi

committee sample selection
section describes apply committee evaluating classification uncertainty input example learning evaluates


fiargamon dagan

example giving committee containing several versions copies classifier consistent training data seen far greater agreement
committee members classification example greater certainty
classification training data entails specific classification high
certainty probabilistic sense versions classifier consistent
data produce classification example selected labeling therefore
committee members disagree appropriate classification

generating committee
generate committee k members randomly choose k according
posterior distribution p js possible given current training statistics
sampling performed depends form distribution turn
depends form model thus implementing committee selection
particular appropriate sampling procedure must devised illustration
committee generation rest section describes sampling process
consisting independent binomial parameters multinomial parameter groups
consider first model containing single binomial parameter probability
success estimated value statistics model given n
number trials x number successes trials
given n x best model parameter value estimated several
estimation methods example maximum likelihood estimate mle nx
giving model fff nx g generating committee however
interested best model rather sampling distribution given
statistics example need sample posterior density estimates
namely p ajs binomial case density beta distribution johnson
sampling distribution yields set estimates scattered around nx assuming
uniform prior variance estimates gets smaller n gets larger
estimate participates different member committee thus statistics
estimating parameter closer estimates used different
committee
consider model consisting single group interdependent parameters defining multinomial case posterior dirichlet distribution johnson
committee members generated sampling joint distribution giving values
model parameters
consisting set independent binomials multinomials sampling
p js amounts sampling parameters independently
complex dependencies among parameters sampling may dicult practice
though may possible make enough independence assumptions make sampling
feasible
sampling posterior generates committee members whose parameter estimates differ
low training counts tend agree high
counts classification example relies parameters whose estimates com

ficommittee sample selection probabilistic classifiers

unlabeled input example e
draw randomly p js statistics acquired
previously labeled examples
classify e model giving classifications c c
c c select e annotation
e selected get correct label update accordingly
figure two member sequential selection
mittee members differ differences affect classification example would
selected learning leads selecting examples contribute statistics
currently unreliable estimates effect classification thus address
properties discussed section

selection
within committee paradigm exist different methods selecting informative examples previous sample selection used sequential selection
seung et al freund et al dagan engelson batch selection lewis
catlett lewis gale present general sequential batch committee selection cases assume
selection applied small amount labeled initial training supplied order
initialize training statistics
two member sequential selection

sequential selection examines unlabeled examples supplied one one
estimates expected information gain examples determined suciently
informative selected training simply choose committee size two
posterior distribution select example two
disagree classification gives parameter free two member sequential selection
shown figure basic parameters
general sequential selection

general selection

larger number k committee members order evaluate example informativeness precisely

refined example selection criteria


fiargamon dagan

unlabeled input example e
draw k fmi g randomly p js possibly temperature
classify e model mi giving classifications fci g
measure disagreement e fci g
decide whether select e annotation value
e
e selected get correct label update accordingly
figure general sequential selection

tuning frequency selection replacing p js distribution
different variance effect adjusting variability among committee
members chosen many cases eg hmms described section
implemented parameter called temperature used multiplier
variance posterior parameter distribution

gives general sequential selection shown figure
easy see two member sequential selection special case general sequential selection order instantiate general larger committees need
fix general measure e disagreement step decision method selecting
examples according disagreement step
measure disagreement entropy distribution classifications voted
committee members vote entropy natural measure quantifying
uniformity classes assigned example different committee members
normalize entropy bound maximum possible value log min k jcj
giving value denoting number committee members assigning
class c input example e v c e normalized vote entropy
x v c e v c e

e
log min k jc j c k log k
normalized vote entropy value one committee members disagree
value zero agree taking intermediate values cases partial agreement
consider two alternatives selection criterion step simplest
thresholded selection example selected annotation normalized vote
entropy exceeds threshold another alternative randomized selection
example selected annotation ip coin biased according
vote entropy higher vote entropy corresponding higher probability selection
mccallum nigam suggested alternative measure kl divergence mean
pereira tishby lee clear whether measure advantage simpler
entropy function



ficommittee sample selection probabilistic classifiers

batch b n examples
example e b
draw k randomly p js
b classify e model giving classifications fcig
c measure disagreement e e fcig
select annotation examples highest e
update statistics selected examples
figure batch selection
use simple model selection probability linear function normalized vote
entropy p e gd e calling g entropy gain
batch selection

alternative sequential selection batch selection rather evaluating examples
individually informativeness large batch n examples examined
best selected annotation batch selection given figure
procedure repeated sequentially successive batches n examples returning
start corpus end n equal size corpus batch selection
selects globally best examples corpus stage lewis catlett
batch selection certain theoretical drawbacks freund et al particularly
consider distribution input examples however shown mccallum
nigam distribution input examples modeled taken
account selection combining disagreement measure
measure example density produces good batch selection work
discussed detail section separate diculty batch selection
computational disadvantage must look large number examples
selecting batch size decreased batch selection behaves similarly
sequential selection

example colorful coin flipper
illustrative example learning task define colorful coin ipper ccf
machine contains infinite number coins colors machine chooses
coins ip one one color coin fixed unknown probability
chosen coin ipped comes heads probability determined solely
color ips coin machine tells learner color coin chosen
selection method used dagan engelson randomized sequential selection
linear selection probability model parameters k g



fiargamon dagan

ip order know outcome ip however learner must pay machine
training learner may choose colors coins whose outcomes examine
objective selective sampling choose minimize training cost number
ips examined required attain given prediction accuracy ip outcomes
case ccf example e coin ip characterized color
class c heads tails note require ips given color
class therefore best hope classify according
likely class color
ccf define model whose parameters heads probabilities
coins particular color ccf three colors one possible model would
fr g b g giving probabilities heads red green blue
coins respectively coin given color classified heads score given
directly appropriate model parameter tails otherwise

implementation sample selection
training model ccf amounts counting proportion heads color
providing estimates heads probabilities complete training every coin ip training
sequence examined added counts sample selection seek label
count training ips colors additional counts likely improve
model accuracy useful colors train training
examples far seen whose current probability estimates near
cf section
recall sample selection build committee sampling p js
case ccf model parameters ffi heads probabilities different
colors independent sampling p js amounts sampling independently
parameters
form posterior distribution p ffi ai js given beta distribution found technically easier use normal approximation found
satisfactory practice let ni number coin ips color seen far ni
number ips came heads approximate p ffi ai js truncated normal distribution restricted estimated mean nn variance
n approximation made easy incorporate temperature parameter section used multiplier variance estimate thus
actually approximate p ffi aijs truncated normal distribution mean
variance sampling distribution done given press
flannery teukolsky vetterling sampling normal distribution










vote entropy
ccf useful illustrate importance determining classification uncertainty
vote entropy committee rather entropy
class distribution given single model discussed section consider ccf


ficommittee sample selection probabilistic classifiers

model





red
heads
heads
heads
heads

blue
tail
tail
heads
heads


green
heads
tail
tail
tail

color e acde
red

blue

green
b

figure committee ccf b resultant vote entropy color
ccf colors

ccf colors


ptm
committee sampling
complete training

ptm
committee sampling
complete training





selected training

selected training



















desired accuracy
















desired accuracy







b

figure ccf random ccfs different coin colors
averaged different ccfs comparing complete training two
member sample selection figures amount training required
desired classification accuracy colors b colors

three coin colors red blue green suppose member committee figure
generated committee estimate color vote entropy e well
average class distribution entropies given individual
acde given figure b
compare entropies red blue example see entropies
expected class probability distribution quite high since estimated
class probabilities near however consider vote entropies
assigned classes blue maximal entropy since range possible straddles
class boundary red minimal entropy since range possible
straddle class boundary quite certain optimal classification
red heads see green higher vote entropy red although
average class distribution entropy lower shows importance vote entropy
selection


fiargamon dagan

ccf frequency selection

color ccf
color ccf




selection frequency





















selected training









figure frequency selection vs amount selected training ccfs
colors averaged different ccfs


simulated sample selection simple ccf model order illustrate
properties following generated random ccfs fixed number coins
randomly generating occurrence probabilities heads probabilities coin color
generated learning curves complete training input examples two
member sample selection coin ips initial training complete training
sample selection run coin ip sequences accuracy measured
computing expected accuracy assuming infinite test set mle model
generated selected training figures accuracy theoretical
perfectly trained model ptm knows parameters perfectly
figure summarizes average comparison runs complete vs sample
selection ccfs coins figures b compare amount
selected training required reach given desired accuracy see cases
soon sample selection starts operating eciency higher complete training
gap increases size greater accuracy desired figure examine
cumulative frequency selection ratio number selected examples
total number examples seen learning progresses see exponential decrease
frequency selection expected case qbc non probabilistic
analyzed seung et al freund et al

application stochastic part speech tagging
applied committee selection real world task learning hidden markov
hmms part speech tagging english sentences part speech tagging
task labeling word sentence appropriate part speech
example labeling occurrence word hand noun verb task nontrivial since determining word part speech depends linguistic context hmms


ficommittee sample selection probabilistic classifiers

used extensively task e g church merialdo cases
trained corpora manually annotated correct part speech
word

hmms part speech tagging
first order hidden markov model hmm probabilistic finite state string generator
rabiner defined set states q fqi g set output symbols set
transition probabilities p qi qj possible transition states qi qj set
output probabilities p ajq state q output symbol distinguished
start state q probability string generated hmm
given

n
x

p qi qi p aijqi
q qn qn

sum paths hmm joint probability path traversed
output given string contrast ordinary markov hmm
known sequence states generated given string hence term hidden
hmms used widely speech language processing particular hmm
used provide classification model sequence elements need classify
element sequence encode possible class state hmm training
hmm amounts estimating values transition output probabilities
given sequence classification assume generated hmm
compute likely state sequence string viterbi viterbi

hmm used part speech tagging words encoding possible partof speech tag noun verb adjective etc hmm state output probabilities
p wjt give probability producing word w language conditioned
current tag transition probabilities p give probability generating
word tag given previous word tag constitutes weak
syntactic model language model often termed tag bigram model
given input word sequence w w wn seek likely tag sequence
tn

arg maxt p jw arg maxt pp w
w
arg maxt p w
alternative classification scheme compute likely state individual element
instead likely state sequence forward backward rabiner
called baum welch baum address alternative
computationally expensive typically used part speech tagging possible
however apply committee selection method type classification
noted practical implementations part speech tagging often employ tag trigram
model probability tag depends last two tags rather last one
committee selection method apply bigram model easily applied
trigram case



fiargamon dagan

since p w constant thus seek maximizes

p w

n



p ti ti p wijti

technical convenience use bayes theorem replace p wijti term term
p jw p w
noting p wi effect maximization tag sequences
p
therefore omitted following church parameters part speech model
tag probabilities p ti transition probabilities p ti ti lexical probabilities
p tjw
supervised training tagger performed tagged corpus text collection
manually labeled correct part speech word maximum likelihood estimates mles parameters easily computed word tag counts
corpus example mle p fraction tag occurrences corpus tag whereas p tjw ratio count word w
labeled tag total count w committee selection scheme
counts used compute posterior distributions parameter estimates
discussed section
next describe application committee selection scheme hmm
classification framework first discuss sample posterior distributions hmm parameters p ti tj p tjw given training statistics
discuss question define example training hmm deals
principle infinite strings substrings make decisions labeling finally
describe measure amount disagreement committee members








posterior distributions multinomial parameters
section consider select committee members posterior parameter distributions p ffi aijs hmm assuming uniform prior first note
parameters hmm define set multinomial probability distributions multinomial corresponds conditioning event values given corresponding
set conditioned events example transition probability parameter p ti tj
conditioning event ti conditioned event tj
let fui g denote set possible values given multinomial variable e g
possible tags given word let fni g denote set statistics extracted
training set ni number times value ui appears training
p
set denote total number appearances multinomial variable n ni
parameters whose distributions wish estimate ffi p ui
maximum likelihood estimate multinomial distribution parameters
ffi nn practice estimator usually smoothed way compensate
data sparseness smoothing typically reduces estimates values positive


sample model space tag probability parameters since amount data tag
frequencies large enough make mles quite definite



ficommittee sample selection probabilistic classifiers

counts gives small positive estimates values zero count simplicity
first describe approximation p ffi aijs unsmoothed estimator
posterior p ffi ai js dirichlet distribution johnson ease
implementation used generalization normal approximation described
section binomial parameters assume first multinomial collection
independent binomials corresponds single value ui multinomial
separately apply constraint parameters binomials sum
binomial sample approximate distribution possibly
temperature generate particular multinomial distribution renormalize
sampled parameters sum
sample smoothed estimator first note estimator smoothed
model interpolating uniform


si nni
smoothing parameter controlling amount smoothing experiments number possible values given multinomial
sample truncated normal approximation section
smoothed estimate e mean si variance n normalization
multinomial applied
finally generate random hmm given statistics note parameters
p ti tj p tjw independent thus independently choose values
hmm parameters multinomial distribution

examples hmm training
typically concept learning formulated set training
examples independent training hmm however
state output pair dependent previous state presented principle
single infinite input string training order perform sample selection
must divide infinite string short finite strings
part speech tagging may solved considering sentence
individual example generally break text point tagging
unambiguous particular common lexicon specifies partsof speech possible word e parameters p tjw positive
bigram tagging use unambiguous words one possible part speech
example boundaries similar natural breakpoints occur hmm applications
example speech recognition consider different utterances separately
cases hmm learning natural breakpoints occur heuristic
applied preferring break almost unambiguous points input
implementation smooth mle interpolation uniform probability distribution
following merialdo adaptation p ffi ai smoothed version estimator given

j



fiargamon dagan

quantifying disagreement
recall selection decide whether select example
much committee members disagree labeling discussed section
suggest use vote entropy measuring classification disagreement committee members idea supported fact found empirically
average normalized vote entropy words tagger training classified correctly whereas average entropy incorrectly classified words
demonstrates vote entropy useful measure classification uncertainty
likelihood error training data
bigram tagging example consists sequence several words implementation measured vote entropy separately word sequence use
average vote entropy sequence measurement disagreement
example use average entropy rather entropy entire sequence
number committee members small respect total number
possible tag sequences


present applying committee sample selection bigram part ofspeech tagging comparing complete training examples corpus evaluation performed university pennsylvania tagged corpus acl dci
cd rom ease implementation used complete closed lexicon contains
words corpus approximately word occurrences corpus
ambiguous lexicon one possible part speech
committee selection initialized first words
corpus examined following examples corpus possible
labeling training set consisted first million words corpus sentence
ordering randomized compensate inhomogeneity corpus composition test set
separate portion corpus consisting words starting first

compared amount training required different selection methods achieve
given tagging accuracy test set amount training tagging
accuracy measured ambiguous words
labeling efficiency
use lexicon provided brill part speech tagger brill actual application
complete lexicon would available complete lexicon valid evaluation
complete training committee selection comparative
work tagging measured accuracy words ambiguous ones complete
training system words gave us accuracy ambiguous words
corresponds accuracy words test set comparable published
bigram tagging



ficommittee sample selection probabilistic classifiers


batch selection n
thresholded selection th
randomized selection g
two member selection
complete training



selected training

















accuracy









figure labeled training versus classification accuracy batch random thresholded runs k

figure presents comparison several selection methods reported parameter settings best found selection method manual tuning
figure shows advantage sample selection gives regard annotation cost
example complete training requires annotated examples containing ambiguous
words achieve accuracy selection methods require
ambiguous words achieve accuracy first approximation
methods considered give similar thus seems refined choice
selection method crucial achieving large reductions annotation cost
computational efficiency

figure plots classification accuracy versus number words examined instead
selected complete training clearly ecient terms learns
examples examined selective methods similar though two member selection seems
require somewhat fewer examples examination methods furthermore
since two committee members used method computationally ecient
evaluating examined example
model size

ability committee selection focus informative parts
training corpus analyzed figure examined number lexical bigram


fiargamon dagan


batch selection n
thresholded selection th
randomized selection g
two member selection
complete training



examined training

















accuracy









figure examined training labeled unlabeled versus classification accuracy
batch random thresholded runs k




two member selection
complete training



two member selection
complete training





bigram model size

lexical model size



























accuracy






















accuracy









b

figure numbers frequency counts plotted axis versus classification accuracy
x axis lexical counts freq w b bigram counts freq



ficommittee sample selection probabilistic classifiers


two member selection
batch selection n
batch selection n
batch selection n
batch selection n



accuracy














examined training





figure evaluating batch selection classification accuracy versus number
words examined corpus different batch sizes
counts stored e non zero training two member selection
complete training graphs committee selection achieves
accuracy complete training fewer lexical bigram counts achieve
accuracy two member selection requires lexical counts bigram counts
compared lexical counts bigram counts complete training
implies many counts data needed correct tagging since smoothing
estimates probabilities equally well committee selection ignores counts
focusing efforts parameters improve model performance behavior
additional practical advantage reducing size model significantly
average count lower model constructed selective training fully trained
model suggesting selection method tends avoid examples increase
counts already known parameters
batch selection

investigated properties batch selection varying batch size
examples fixing number examples selected batch found
terms number labeled examples required attain given accuracy selection
different batch sizes performed similarly means increased batch size
mentioned tagging phase smooth mle estimates interpolation uniform
probability distribution following merialdo



fiargamon dagan

seem improve effectiveness selection hand see
decrease performance increased batch size might expected due
poorer modeling input distribution noted section may indicate
even batch size selecting examples seen small enough
let us model input distribution reasonable accuracy however similarity
performance different batch sizes sequential selection
hold respect amount unlabeled training used figure shows accuracy
attained function amount unlabeled training used see quite clearly
expected larger batch sizes required examining far larger number unlabeled
training examples order obtain accuracy

discussion
committee selection monte carlo technique
view committee selection monte carlo method estimating probability distribution classes assigned example possible given
training data proportion votes among committee members class c example e sample estimate probability model chosen randomly
posterior model distribution assigning c e proportion votes
c given e v kc e monte carlo estimate
z

p cje tm cje p js dm


ranges possible vectors parameter values model space
p js posterior probability density model given statistics tm cje
c highest probability class e e c arg maxc pm ci je
pm cje class probability distribution e given model otherwise vote
entropy defined section thus approximation entropy p
entropy direct measure uncertainty example classification possible
note measure entropy final classes assigned example possible
e tm class probabilities given single model e pm
illustrated ccf example section measuring entropy pm say looking
expected probability would properly address properties
discussed section


batch selection
property discussed section states parameters affect examples
low overall utility atypical examples useful learning sequential
selection property addressed independently examining input examples
drawn input distribution way implicitly model distribution model
parameters used classifying input examples modeling however inherent
basic form batch selection lead less effective freund et al



ficommittee sample selection probabilistic classifiers

diculty batch selection addressed directly mccallum nigam
describe version batch selection called pool sampling differs
basic batch selection scheme presented section two ways first quantify disagreement committee members kl divergence mean pereira
et al rather vote entropy significantly disagreement measure
combined explicit density measure density weighted sampling documents similar many documents training set probably
selected labeling intended address property section authors
found empirically text classification naive bayes density weighted poolbased selection method kl divergence mean improved learning eciency
complete training found sequential selection vote entropy worse
complete training
hypothesize due high degree sparseness example space
text documents leads large proportion examples atypical even
though documents similar given atypical document rare many different atypical
documents occur since case sequential variant may tend select many
atypical documents labeling would degrade learner performance skewing
statistics remedied adding density weighting sequential selection
future may yield ecient sequential selection works
well highly sparse domains

conclusions
labeling large training sets supervised classification often costly process especially
complicated domain areas natural language processing presented
reducing cost significantly committee sample selection
reduces redundant annotation examples contribute little information
method applicable whenever possible estimate posterior distribution
model space given training data shown apply training hidden
markov demonstrated effectiveness complex task part speech
tagging implicit modeling uncertainty makes selection system generally applicable
relatively simple implement practical settings method may applied
semi interactive process system selects several examples annotation
time updates statistics receiving labels user
committee sampling method addresses three factors relate
informativeness training example model parameters affects factors
statistical significance parameter estimate parameter effect
classification probability parameter used classification
future use committee uncertainty classification relative
entire model space sequential selection implicitly distribution
examples
experimental study variants selection method suggests several practical
conclusions first found simplest version committee method


fiargamon dagan

two member committee yields reduction annotation cost comparable
multi member committee two member version simpler implement
parameters tune computationally ecient second generalized
selection scheme giving several alternatives optimizing method specific task
bigram tagging comparative evaluation different variants method showed
similar large reductions annotation cost suggesting robustness committeebased third sequential selection implicitly expected utility
example relative example distribution worked general better batch
selection recent improving batch selection modeling explicitly typicality
examples suggest comparison two approaches discussed previous
section finally studied effect sample selection size trained model
showing significant reduction model size selectively trained
future propose investigate applicability effectiveness committeebased sample selection additional probabilistic classification tasks furthermore
generality obtained implicitly modeling information gain suggests variants
committee sampling non probabilistic contexts explicit modeling
information gain may impossible contexts committee members might generated randomly varying decisions made learning

acknowledgments
discussions yoav freund yishai mansour wray buntine greatly enhanced
work first author bar ilan university work performed
supported fulbright foundation part work

references
angluin learning regular sets queries counterexamples information
computation
angluin queries concept learning machine learning
baum l e inequality associated maximization technique statistical
estimation probabilistic functions markov process inequalities
black e jelinek f lafferty j magerman mercer r roukos towards
history grammars richer probabilistic parsing proc
annual meeting acl pp
brill e simple rule part speech tagger proc acl conference
applied natural language processing
church k w stochastic parts program noun phrase parser unrestricted
text proc acl conference applied natural language processing
cohn atlas l ladner r improving generalization active learning
machine learning


ficommittee sample selection probabilistic classifiers

cohn ghahramani z jordan active learning statistical
tesauro g touretzky alspector j eds advances neural
information processing vol morgan kaufmann san mateo ca
dagan engelson committee sampling training probabilistic
classifiers proceedings international conference machine learning
elworthy baum welch estimation improve taggers proc acl
conference applied natural language processing pp
engelson dagan minimizing manual annotation cost supervised learning corpora proceedings th annual meeting association
computational linguistics
engelson dagan b sample selection natural language learning
wermter riloff e scheler g eds symbolic connectionist statistical
approaches learning natural language processing springer verlag
freund seung h shamir e tishby n selective sampling
query committee machine learning
freund sifting informative examples random source working notes
workshop relevance aaai fall symposium series pp
gale w church k yarowsky method disambiguating word senses
large corpus computers humanities
johnson n l continuous univariate distributions john wiley sons
york
johnson n l continuous multivariate distributions john wiley sons
york
kupiec j robust part speech tagging hidden makov model computer
speech language
lewis catlett j heterogeneous uncertainty sampling supervised
learning proceedings international conference machine learning
lewis gale w sequential training text classifiers
proceedings acm sigir conference
liere r tadepalli p active learning committees text categorization
proceedings national conference artificial intelligence
littlestone n learning quickly irrelevant features abound linearthreshold machine learning
mackay j c evidence framework applied classification networks
neural computation


fiargamon dagan

mackay j c b information objective functions active data selection
neural computation
matan site learning tech rep logic stanford university
mccallum k nigam k employing em pool active learning
text classification proceedings international conference machine
learning
merialdo b tagging text probabilistic model proc int l conf
acoustics speech signal processing
merialdo b tagging text probabilistic model computational linguistics

mitchell generalization search artificial intelligence
pereira f tishby n lee l distributional clustering english words
proceedings annual meeting association computational linguistics
acl
plutowski white h selecting concise training sets clean data ieee
trans neural networks
press w h flannery b p teukolsky vetterling w numerical
recipes c cambridge university press
rabiner l r tutorial hidden markov selected applications
speech recognition proc ieee
seung h opper sompolinsky h query committee proceedings
acm workshop computational learning theory
viterbi j error bounds convolutional codes asymptotically optimal
decoding ieee trans informat theory




