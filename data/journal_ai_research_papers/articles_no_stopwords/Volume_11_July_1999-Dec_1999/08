Journal Artificial Intelligence Research 11 (1999) 335{360

Submitted 8/98; published 11/99

Committee-Based Sample Selection
Probabilistic Classifiers
Shlomo Argamon-Engelson

Department Computer Science
Jerusalem College Technology, Machon Lev
P.O.B. 16031
Jerusalem 91160, Israel

argamon@mail.jct.ac.il

Ido Dagan

Department Mathematics Computer Science
Bar-Ilan University
52900 Ramat Gan, Israel

dagan@cs.biu.ac.il

Abstract
many real-world learning tasks expensive acquire sucient number labeled
examples training. paper investigates methods reducing annotation cost
sample selection. approach, training learning program examines many
unlabeled examples selects labeling informative
stage. avoids redundantly labeling examples contribute little new information.
work follows previous research Query Committee, extends
committee-based paradigm context probabilistic classification. describe
family empirical methods committee-based sample selection probabilistic classification models, evaluate informativeness example measuring degree
disagreement several model variants. variants (the committee) drawn
randomly probability distribution conditioned training set labeled far.
method applied real-world natural language processing task stochastic part-of-speech tagging. find variants method achieve significant
reduction annotation cost, although computational eciency differs. particular,
simplest variant, two member committee parameters tune, gives excellent
results. show sample selection yields significant reduction size
model used tagger.

1. Introduction
Algorithms supervised concept learning build classifiers concept based given
set labeled examples. many real-world concept learning tasks, however, acquiring
labeled training examples expensive. Hence, objective develop automated
methods reduce training cost within framework active learning,
learner control choice examples labeled used
training.
c 1999 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiArgamon & Dagan

two main types active learning. first uses membership queries,
learner constructs examples asks teacher label (Angluin, 1988; MacKay,
1992b; Plutowski & White, 1993). approach provides proven computational
advantages (Angluin, 1987), always applicable since always possible
construct meaningful informative unlabeled examples training. diculty may
overcome large set unlabeled training data available. case second
type active learning, sample selection, often applied: learner examines many
unlabeled examples, selects informative ones learning (Seung, Opper,
& Sompolinsky, 1992; Freund, Seung, Shamir, & Tishby, 1997; Cohn, Atlas, & Ladner,
1994; Lewis & Catlett, 1994; Lewis & Gale, 1994).
paper, address problem sample selection training probabilistic
classifier. Classification framework performed probability-based model which,
given input example, assigns score possible classification selects
highest score.
research follows theoretical work sample selection Query Committee
(QBC) paradigm (Seung et al., 1992; Freund et al., 1997). propose novel empirical
scheme applying QBC paradigm probabilistic classification models (allowing label
noise), addressed original QBC framework (see Section 2.2).
committee-based selection scheme, learner receives stream unlabeled examples
input decides whether ask label not. end,
learner constructs `committee' (two more) classifiers based statistics
current training set. committee member classifies candidate example,
learner measures degree disagreement among committee members. example
selected labeling depending degree disagreement, according selection
protocol.
previous work (Dagan & Engelson, 1995; Engelson & Dagan, 1996b) presented
particular selection protocol probabilistic concepts. paper extends previous
work mainly generalizing selection scheme comparing variety different
selection protocols (a preliminary version appeared Engelson & Dagan, 1996a).

1.1 Application Natural Language Processing
Much early work sample selection either theoretical nature,
tested toy problems. We, however, motivated complex, real-world problems
area statistical natural language text processing. work addresses
task part-of-speech tagging, core task statistical natural language processing
(NLP). work sample selection natural language tasks mainly focused
text categorization problems, works Lewis Catlett (1994), Liere
Tadepalli (1997), McCallum Nigam (1998).
statistical NLP, probabilistic classifiers often used select preferred analysis
linguistic structure text, syntactic structure (Black, Jelinek, Lafferty,
Magerman, Mercer, & Roukos, 1993), word categories (Church, 1988), word senses (Gale,
336

fiCommittee-Based Sample Selection Probabilistic Classifiers

Church, & Yarowsky, 1993). parameters classification model estimated
training corpus (a collection text).
common case supervised training, learner uses corpus
sentence manually annotated correct analysis. Manual annotation typically
expensive. consequence, large annotated corpora exist, mainly English language, covering genres text. situation makes dicult apply
supervised learning methods languages English, adapt systems different genres text. Furthermore, infeasible many cases develop new supervised
methods require annotations different currently available.
cases, manual annotation avoided altogether, using self-organized methods, shown part-of-speech tagging English Kupiec (1992). Even
Kupiec's tagger, though, manual (and somewhat unprincipled) biasing initial model
necessary achieve satisfactory convergence. Elworthy (1994) Merialdo (1991)
investigated effect self-converging re-estimation part-of-speech tagging
found initial manual training needed. generally, supervised
training provided, better results. fact, fully unsupervised methods
applicable many NLP tasks, perhaps even part-of-speech tagging
languages. Sample selection appropriate way reduce cost annotating corpora,
easy obtain large volumes raw text smaller subsets selected
annotation.
applied committee-based selection learning Hidden Markov Models (HMMs)
part-of-speech tagging English sentences. Part-of-speech tagging task labeling
word sentence appropriate part speech (for example, labeling
occurrence word `hand' noun verb). task non-trivial since determining
word's part speech depends linguistic context. HMMs used extensively
task (e.g., Church, 1988; Merialdo, 1991), cases trained corpora
manually annotated correct part speech word.
experiments part-of-speech tagging, described Section 6.5, show using committeebased selection results substantially faster learning rates, enabling learner achieve
given level accuracy using far fewer training examples sequential training using
text.

2. Background
objective sample selection select examples informative future. might determine informativeness example? One
approach derive explicit measure expected amount information gained
using example (Cohn, Ghahramani, & Jordan, 1995; MacKay, 1992b, 1992a).
example, MacKay (1992b) assesses informativeness example, neural network
learning task, expected decrease overall variance model's prediction,
training example. Explicit measures appealing, since attempt
give precise characterization information content example. Also, membership querying, explicit formulation information content sometimes enables finding
337

fiArgamon & Dagan

informative examples analytically, saving cost searching example space.
use explicit methods may limited, however, since explicit measures generally
(a) model-specific, (b) complex, often requiring various approximations practical,
(c) depend accuracy current hypothesis given step.
alternative measuring informativeness example explicitly measure
implicitly, quantifying amount uncertainty classification example
given current training data. informativeness example evaluated respect
models derived training data stage learning. One approach use
single model based training data seen far. approach taken Lewis
Gale (1994), training binary classifier. select training examples
whose classification probability closest 0.5, i.e, examples current
best model uncertain.
order better evaluate classification uncertainty respect entire space
possible models, one may instead measure classification disagreement among sample
set possible models (a committee). Using entire model space enables measuring
degree training entails single (best) classification example.
hand, referring single model measures degree model
certain classification. example, classifier sucient training predicting ips coin heads probability 0.55 always predict heads, hence
make mistakes 45% time. However, although classifier quite uncertain
correctness classification, additional training improve accuracy.
two main approaches generating committee order evaluate example
uncertainty: version space approach random sampling approach. version
space approach, pursued Cohn et al. (1994) seeks choose committee members
border space models allowed training data (the version space,
Mitchell, 1982). Thus models chosen committee far
possible consistent training data. ensures models
disagree example whenever training example would restrict version space.
version space approach dicult apply since finding models edge
version space non-trivial general. Furthermore, approach directly
applicable case probabilistic classification models, almost models
possible, though equally probable, given training. alternative random sampling, exemplified Query Committee algorithm (Seung et al., 1992; Freund
et al., 1997), inspired paper. approach, models sampled randomly
set possible models, according probability models given
training data. work applies random sampling approach probabilistic classifiers
computing approximation posterior model distribution given training data,
generating committee members distribution. McCallum Nigam (1998)
use similar approach sample selection text categorization using naive Bayes classifier. primary difference skew example selection using density-weighted
sampling, documents similar many documents training
set selected labeling higher probability.

338

fiCommittee-Based Sample Selection Probabilistic Classifiers

Matan (1995) presents two methods random sampling. first method,
trains committee members different subsets training data. second method,
neural network models, Matan generates committee members backpropagation training using different initial weights networks reach different local minima.
similar approach taken Liere Tadepalli (1997), applied committee-based
selection approach text categorization using Winnow learning algorithm (Littlestone,
1988) learns linear classifiers. represented model space set classifiers (the model set). classifier model set learns independently labeled
examples, initialized different initial hypothesis (thus point
set gives selection possible hypotheses given training data). Labeling decisions
performed based two models chosen random model set. models
disagree document's class, document's label requested, models
space updated.

2.1 Query Committee
mentioned above, paper follows theoretical work sample selection Query
Committee (QBC) paradigm (Seung et al., 1992; Freund et al., 1997). method
proposed learning binary (non-probabilistic) concepts cases exists prior
probability distribution measure concept class. QBC selects `informative' training
examples stream unlabeled examples. example selected learner
queries teacher correct label adds training set. examples
selected training, restrict set consistent concepts, i.e, set concepts
label training examples correctly (the version space).
simple version QBC, analyzed Freund et al. (1997) (see
summary Freund, 1994), uses following selection algorithm:
1. Draw unlabeled input example random probability distribution example space.
2. Select random two hypotheses according prior probability distribution concept class, restricted set consistent concepts.
3. Select example training two hypotheses disagree classification.
Freund et al. prove that, assumptions, algorithm achieves exponential
reduction number labeled examples required achieve desired classification
accuracy, compared random selection training examples. speedup achieved
algorithm tends select examples split version space two parts
similar size. One parts eliminated version space example
correct label added training set.

2.2 Selection Probabilistic Classifiers
address problem sample selection training probabilistic classifier. Classification framework performed probabilistic model which, given input
339

fiArgamon & Dagan

example, assigns probability (or probability-based score) possible classification
selects best classification. Probabilistic classifiers fall within framework
addressed theoretical QBC work. Training probabilistic classifier involves estimating values model parameters determine probability estimate possible
classification example. expect cases optimal classifier
assign highest probability correct class, guaranteed always occur.
Accordingly, notion consistent hypothesis generally applicable probabilistic
classifiers. Thus, posterior distribution classifiers given training data cannot
defined restriction prior set consistent hypotheses. Rather, within
Bayesian framework, posterior distribution defined statistics training
set, assigning higher probability classifiers likely given statistics.
discuss desired properties examples selected training. Generally speaking, training example contributes data several statistics, turn
determine estimates several parameter values. informative example therefore
one whose contribution statistics leads useful improvement parameter estimates. Assuming existence optimal classification model given concept
(such maximum likelihood model), identify three properties parameters
acquiring additional statistics beneficial:
1. current estimate parameter uncertain due insucient statistics
training set. uncertain estimate likely far true value
parameter cause incorrect classification. Additional statistics would bring
estimate closer true value.
2. Classification sensitive changes current estimate parameter. Otherwise, acquiring additional statistics unlikely affect classification therefore
beneficial.
3. parameter takes part calculating class probabilities large proportion
examples. Parameters relevant classifying examples, determined probability distribution input examples, low utility future
estimation.
committee-based selection scheme, describe below, tends select
examples affect parameters three properties. Property 1 addressed
randomly picking parameter values committee members posterior distribution
parameter estimates (given current statistics). statistics parameter
insucient variance posterior distribution estimates large, hence
large differences values parameter picked different committee
members. Note property 1 addressed uncertainty classification
judged relative single model (as in, e.g., Lewis & Gale, 1994). approach
captures uncertainty respect given parameter values, sense property 2,
model uncertainty choice values first place (the use
single model criticized Cohn et al., 1994).
Property 2 addressed selecting examples committee members highly disagree classification. Thus, algorithm tends acquire statistics uncertainty
340

fiCommittee-Based Sample Selection Probabilistic Classifiers

parameter estimates entails uncertainty actual classification (this analogous splitting
version space QBC). Finally, property 3 addressed independently examining
input examples drawn input distribution. way, implicitly
model expected utility statistics classifying future examples.

2.3 Paper Outline
following section defines basic concepts notation use rest
paper. Section 4 presents general selection scheme along variant selection
algorithms. next two sections demonstrate effectiveness sample selection
scheme. Section 5 presents results artificial \colorful coin ipper" problem, providing
simple illustration operation proposed system. Section 6 presents results
task stochastic part-of-speech tagging, demonstrating usefulness committeebased sample selection real world.

3. Definitions
concern paper minimize number labeled examples needed
learn classifier accurately classifies input examples e classes c 2 C , C
known set possible classes. learning, stream unlabeled examples
supplied free, examples drawn unknown probability distribution.
cost, however, learning algorithm obtain true label given example.
objective reduce cost much possible, still learning accurate
classifier.
address specific case probabilistic classifiers, classification done
basis score function, FM (c; e), assigns score possible class
input example. classifier assigns input example class highest score.
FM determined probabilistic model . many applications, FM conditional
probability function, PM (cje), specifying probability class given example.
Alternatively, score functions denote likelihood class may used
(such odds ratio). particular type model used classification determines
specific form score, function features example.
probabilistic model , thus score function FM , defined set parameters, fffi g, giving probabilities various possible events. example, model
part-of-speech tagging contains parameters probability particular word
verb noun. training, values parameters estimated
set statistics, , extracted training set labeled examples. particular model
denoted = faig, ai specific value corresponding ffi .

4. Committee-Based Sample Selection
section describes algorithms apply committee-based approach evaluating classification uncertainty input example. learning algorithm evaluates
341

fiArgamon & Dagan

example giving committee containing several versions, copies, classifier, `consistent' training data seen far. greater agreement
committee members classification example, greater certainty
classification. training data entails specific classification high
certainty, (in probabilistic sense) versions classifier consistent
data produce classification. example selected labeling, therefore,
committee members disagree appropriate classification.

4.1 Generating Committee
generate committee k members, randomly choose k models according
posterior distribution P (M jS ) possible models given current training statistics.
sampling performed depends form distribution, turn
depends form model. Thus implementing committee-based selection
particular problem, appropriate sampling procedure must devised. illustration
committee generation, rest section describes sampling process models
consisting independent binomial parameters multinomial parameter groups.
Consider first model containing single binomial parameter (the probability
success), estimated value a. statistics model given N ,
number trials, x, number successes trials.
Given N x, `best' model parameter value estimated several
estimation methods. example, maximum likelihood estimate (MLE) = Nx ,
giving model = fff = Nx g. generating committee models, however,
interested `best' model, rather sampling distribution models given
statistics. example, need sample posterior density estimates
ff, namely p(ff = ajS ). binomial case, density beta distribution (Johnson,
1970). Sampling distribution yields set estimates scattered around Nx (assuming
uniform prior), variance estimates gets smaller N gets larger.
estimate participates different member committee. Thus, statistics
estimating parameter, closer estimates used different models
committee.
consider model consisting single group interdependent parameters defining multinomial. case, posterior Dirichlet distribution (Johnson, 1972).
Committee members generated sampling joint distribution, giving values
model parameters.
models consisting set independent binomials multinomials, sampling
P (M jS ) amounts sampling parameters independently. models
complex dependencies among parameters sampling may dicult. practice,
though, may possible make enough independence assumptions make sampling
feasible.
Sampling posterior generates committee members whose parameter estimates differ
based low training counts tend agree based high
counts. classification example relies parameters whose estimates com342

fiCommittee-Based Sample Selection Probabilistic Classifiers

unlabeled input example e:
1. Draw 2 models randomly P (M jS ), statistics acquired
previously labeled examples;
2. Classify e model, giving classifications c1 c2;
3. c1 6= c2, select e annotation;
4. e selected, get correct label update accordingly.
Figure 1: two member sequential selection algorithm.
mittee members differ, differences affect classification, example would
selected learning. leads selecting examples contribute statistics
currently unreliable estimates effect classification. Thus address
Properties 1 2 discussed Section 2.2.

4.2 Selection Algorithms
Within committee-based paradigm exist different methods selecting informative examples. Previous research sample selection used either sequential selection
(Seung et al., 1992; Freund et al., 1997; Dagan & Engelson, 1995), batch selection (Lewis
& Catlett, 1994; Lewis & Gale, 1994). present general algorithms sequential batch committee-based selection. cases, assume
selection algorithm applied small amount labeled initial training supplied, order
initialize training statistics.
4.2.1 Two Member Sequential Selection

Sequential selection examines unlabeled examples supplied, one one,
estimates expected information gain. examples determined suciently
informative selected training. simply, choose committee size two
posterior distribution models, select example two models
disagree classification. gives parameter-free, two member sequential selection
algorithm, shown Figure 1. basic algorithm parameters.
4.2.2 General Sequential Selection

general selection algorithm results from:

Using larger number k committee members, order evaluate example informativeness precisely,

refined example selection criteria,
343

fiArgamon & Dagan

unlabeled input example e:
1. Draw k models fMi g randomly P (M jS ) (possibly using temperature t);
2. Classify e model Mi giving classifications fci g;
3. Measure disagreement D(e) based fci g;
4. Decide whether select e annotation, based value
D(e);
5. e selected, get correct label update accordingly.
Figure 2: general sequential selection algorithm.

Tuning frequency selection replacing P (M jS ) distribution
different variance. effect adjusting variability among committee
members chosen. many cases (eg., HMMs, described Section 6 below)
implemented parameter (called temperature), used multiplier
variance posterior parameter distribution.

gives general sequential selection algorithm, shown Figure 2.
easy see two member sequential selection special case general sequential selection. order instantiate general algorithm larger committees, need
fix general measure D(e) disagreement (step 3), decision method selecting
examples according disagreement (step 4).
measure disagreement entropy distribution classifications `voted for'
committee members. vote entropy natural measure quantifying
uniformity classes assigned example different committee members1 .
normalize entropy bound maximum possible value (log min(k; jcj)),
giving value 0 1. Denoting number committee members assigning
class c input example e V (c; e), normalized vote entropy is:
X V (c; e) V (c; e)
1
D(e) = ,
log min(k; jC j) c k log k
Normalized vote entropy value one committee members disagree,
value zero agree, taking intermediate values cases partial agreement.
consider two alternatives selection criterion (step 4). simplest
thresholded selection, example selected annotation normalized vote
entropy exceeds threshold . Another alternative randomized selection,
example selected annotation based ip coin biased according
vote entropy|a higher vote entropy corresponding higher probability selection.
1. McCallum Nigam (1998) suggested alternative measure, KL-divergence mean
(Pereira, Tishby, & Lee, 1993). clear whether measure advantage simpler
entropy function.

344

fiCommittee-Based Sample Selection Probabilistic Classifiers

batch B N examples:
1. example e B :
(a) Draw k models randomly P (M jS );
(b) Classify e model, giving classifications fcig;
(c) Measure disagreement D(e) e based fcig;
2. Select annotation examples highest D(e);
3. Update statistics selected examples.
Figure 3: batch selection algorithm.
use simple model selection probability linear function normalized vote
entropy: P (e) = gD(e), calling g entropy gain2 .
4.2.3 Batch Selection

alternative sequential selection batch selection. Rather evaluating examples
individually informativeness large batch N examples examined,
best selected annotation. batch selection algorithm given Figure 3.
procedure repeated sequentially successive batches N examples, returning
start corpus end. N equal size corpus, batch selection
selects globally best examples corpus stage (as Lewis & Catlett, 1994).
Batch selection certain theoretical drawbacks (Freund et al., 1997), particularly
consider distribution input examples. However, shown McCallum
Nigam (1998), distribution input examples modeled taken
account selection. combining disagreement measure
measure example density, produces good results batch selection (this work
discussed detail Section 7.2). separate diculty batch selection
computational disadvantage must look large number examples
selecting any. batch size decreased, batch selection behaves similarly
sequential selection.

5. Example: Colorful Coin Flipper
illustrative example learning task, define colorful coin- ipper (CCF)
machine contains infinite number coins various colors. machine chooses
coins ip, one one, color coin fixed (unknown) probability
chosen. coin ipped, comes heads probability determined solely
color. ips coin, machine tells learner color coin chosen
2. selection method used (Dagan & Engelson, 1995) randomized sequential selection using
linear selection probability model, parameters k, g.

345

fiArgamon & Dagan

ip. order know outcome ip, however, learner must pay machine.
training, learner may choose colors coins whose outcomes examine.
objective selective sampling choose minimize training cost (number
ips examined) required attain given prediction accuracy ip outcomes.
case CCF, example e coin ip, characterized color,
class c either heads tails. Note require ips given color always
class. Therefore best hope classify according
likely class color.
CCF, define model whose parameters heads probabilities
coins particular color. So, CCF three colors, one possible model would
= fr = 0:8; g = 0:66; b = 0:2g, giving probabilities heads red, green, blue
coins, respectively. coin given color classified `heads' score (given
directly appropriate model parameter) > 12 , `tails' otherwise.

5.1 Implementation Sample Selection
Training model CCF amounts counting proportion heads color,
providing estimates heads probabilities. complete training every coin ip training
sequence examined added counts. sample selection seek label
count training ips colors additional counts likely improve
model's accuracy. Useful colors train either training
examples far seen, whose current probability estimates near 0.5
(cf. Section 2.2).
Recall sample selection build committee sampling models P (M jS ).
case CCF, model parameters ffi (the heads probabilities different
colors) independent, sampling P (M jS ) amounts sampling independently
parameters.
form posterior distribution P (ffi = ai jS ) given beta distribution, found technically easier use normal approximation, found
satisfactory practice. Let Ni number coin ips color seen far, ni
number ips came heads. approximate P (ffi = ai jS ) truncated normal distribution (restricted [0,1]), estimated mean = Nn variance
i2 = (1N, ). approximation made easy incorporate `temperature' parameter (as Section 4.2.2), used multiplier variance estimate i2 . Thus,
actually approximate P (ffi = aijS ) truncated normal distribution mean
variance i2 t. Sampling distribution done using algorithm given Press,
Flannery, Teukolsky, Vetterling (1988) sampling normal distribution.










5.2 Vote Entropy
CCF useful illustrate importance determining classification uncertainty
using vote entropy committee models rather using entropy
class distribution given single model (as discussed Section 2). Consider CCF
346

fiCommittee-Based Sample Selection Probabilistic Classifiers

Model
0
1
2
3

Red
0.55 (heads)
0.55 (heads)
0.60 (heads)
0.60 (heads)

Blue
0.45 (tail)
0.45 (tail)
0.55 (heads)
0.55 (heads)
(a)

Green
0.48 (heads)
0.75 (tail)
0.85 (tail)
0.95 (tail)

Color D(e) ACDE
Red
0.0 0.98
Blue
1.0 0.99
Green 0.81 0.68
(b)

Figure 4: (a) committee CCF models. (b) resultant vote entropy color.
CCF results 50 colors

CCF results 100 colors

200
PTM
Committee-Based Sampling
Complete Training

PTM
Committee-Based Sampling
Complete Training

200

150

Selected training

Selected training

150

100

100

50
50

0
0.5

0
0.55

0.6

0.65
Desired accuracy

0.7

0.75

0.8

0.5

(a)

0.55

0.6

0.65
Desired accuracy

0.7

0.75

0.8

(b)

Figure 5: CCF results random CCFs 50 100 different coin colors. Results
averaged 4 different CCFs, comparing complete training two
member sample selection. figures show amount training required
desired classification accuracy: (a) 50 colors, (b) 100 colors.

three coin colors, red, blue, green. Suppose 4-member committee Figure 4(a)
generated. committee, estimate color vote entropy D(e), well
average class distribution entropies given individual models
(ACDE), given Figure 4(b).
compare entropies red blue, example, see entropies
expected class probability distribution quite high (since estimated
class probabilities near 0.5). However, consider vote entropies (over
assigned classes), blue maximal entropy, since range possible models straddles
class boundary (0.5), red minimal entropy, since range possible models
straddle class boundary. is, quite certain optimal classification
red \heads". see green higher vote entropy red, although
average class distribution entropy lower. shows importance using vote entropy
selection.
347

fiArgamon & Dagan

CCF frequency selection
1
50 color CCF
100 color CCF

0.9
0.8

Selection frequency

0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0

20

40

60

80

100
120
Selected training

140

160

180

200

Figure 6: Frequency selection vs. amount selected training CCFs 50 100
colors, averaged 4 different CCFs.

5.3 Results
simulated sample selection simple CCF model order illustrate
properties. following, generated random CCFs fixed number coins
randomly generating occurrence probabilities heads probabilities coin color.
generated learning curves complete training, input examples, two
member sample selection, using 50 coin- ips initial training. complete training
sample selection run coin- ip sequences. Accuracy measured
computing expected accuracy (assuming infinite test set) MLE model
generated selected training. figures show accuracy theoretical
perfectly trained model (PTM) knows parameters perfectly.
Figure 5 summarizes average results 4 comparison runs complete vs. sample
selection CCFs 50 100 coins. Figures 5(a) (b), compare amount
selected training required reach given desired accuracy. see cases
soon sample selection starts operating, eciency higher complete training,
gap increases size greater accuracy desired. Figure 6, examine
cumulative frequency selection (ratio number selected examples
total number examples seen) learning progresses. see exponential decrease
frequency selection, expected case QBC non-probabilistic models
(analyzed Seung et al., 1992; Freund et al., 1997).

6. Application: Stochastic Part-Of-Speech Tagging
applied committee-based selection real-world task learning Hidden Markov
Models (HMMs) part-of-speech tagging English sentences. Part-of-speech tagging
task labeling word sentence appropriate part speech (for
example, labeling occurrence word `hand' noun verb). task nontrivial since determining word's part speech depends linguistic context. HMMs
348

fiCommittee-Based Sample Selection Probabilistic Classifiers

used extensively task (e.g., Church, 1988; Merialdo, 1991), cases
trained corpora manually annotated correct part speech
word.

6.1 HMMs Part-Of-Speech Tagging
first-order Hidden Markov Model (HMM) probabilistic finite-state string generator
(Rabiner, 1989), defined set states Q = fqi g, set output symbols , set
transition probabilities P (qi !qj ) possible transition states qi qj , set
output probabilities P (ajq) state q output symbol 2 , distinguished
start state q0 . probability string = a1a2 generated HMM
given
!
n
X

P (qi,1 !qi )P (aijqi ) ;
q1 qn 2Qn i=1

sum, paths HMM, joint probability path traversed
output given string. contrast ordinary Markov Models, HMM
known sequence states generated given string (hence term `hidden').
HMMs used widely speech language processing. particular, HMM
used provide classification model sequence elements: need classify
element sequence, encode possible class state HMM. Training
HMM amounts estimating values transition output probabilities.
Then, given sequence classification, assume generated HMM
compute likely state sequence string, using Viterbi algorithm3 (Viterbi,
1967).
HMM used part-of-speech tagging words encoding possible partof-speech tag, (noun, verb, adjective, etc.), HMM state. output probabilities,
P (wjt), give probability producing word w language conditioned
current tag t. transition probabilities, P (t1!t2), give probability generating
word tag t2 given previous word's tag t1 . constitutes weak
syntactic model language. model often termed tag-bigram model4 .
Given input word sequence W = w1 wn , seek likely tag sequence
= t1 tn :
)
arg maxT P (T jW ) = arg maxT PP(T;W
(W )
= arg maxT P (T; W )
3. alternative classification scheme compute likely state individual element
(instead likely state sequence) Forward-Backward algorithm (Rabiner, 1989) (also
called Baum-Welch algorithm Baum, 1972). address alternative,
computationally expensive typically used part-of-speech tagging. possible,
however, apply committee-based selection method type classification.
4. noted practical implementations part-of-speech tagging often employ tag-trigram
model, probability tag depends last two tags rather last one.
committee-based selection method apply bigram model easily applied
trigram case.

349

fiArgamon & Dagan

since P (W ) constant. Thus seek maximizes

P (T; W ) =

n

i=1

P (ti,1!ti )P (wijti )

technical convenience, use Bayes' theorem replace P (wijti ) term term
P (t jw )P (w )
, noting P (wi) effect maximization tag sequences
P (t )
therefore omitted (following Church, 1988). parameters part-of-speech model,
then, are: tag probabilities P (ti), transition probabilities P (ti,1!ti ), lexical probabilities
P (tjw).
Supervised training tagger performed using tagged corpus (text collection),
manually labeled correct part-of-speech word. Maximum likelihood estimates (MLEs) parameters easily computed word tag counts
corpus. example, MLE P (t) fraction tag occurrences corpus tag t, whereas P (tjw) ratio count word w
labeled tag total count w. committee-based selection scheme,
counts used compute posterior distributions parameter estimates,
discussed Section 6.2.
next describe application committee-based selection scheme HMM
classification framework. First discuss sample posterior distributions HMM parameters P (ti !tj ) P (tjw), given training statistics.5
discuss question define example training|an HMM deals (in
principle) infinite strings; substrings make decisions labeling? Finally,
describe measure amount disagreement committee members.








6.2 Posterior Distributions Multinomial Parameters
section, consider select committee members based posterior parameter distributions P (ffi = aijS ) HMM, assuming uniform prior. First note
parameters HMM define set multinomial probability distributions. multinomial corresponds conditioning event values given corresponding
set conditioned events. example, transition probability parameter P (ti !tj )
conditioning event ti conditioned event tj .
Let fui g denote set possible values given multinomial variable (e.g.,
possible tags given word), let = fni g denote set statistics extracted
training set, ni number times value ui appears training
P
set. denote total number appearances multinomial variable N = ni .
parameters whose distributions wish estimate ffi = P (ui ).
maximum likelihood estimate multinomial's distribution parameters,
ffi , ff^i = nN . practice, estimator usually smoothed way compensate
data sparseness. smoothing typically reduces estimates values positive


5. sample model space tag probability parameters, since amount data tag
frequencies large enough make MLEs quite definite.

350

fiCommittee-Based Sample Selection Probabilistic Classifiers

counts gives small positive estimates values zero count. simplicity,
first describe approximation P (ffi = aijS ) unsmoothed estimator6 .
posterior P (ffi = ai jS ) Dirichlet distribution (Johnson, 1972); ease
implementation, used generalization normal approximation described
(Section 5.1) binomial parameters. assume first multinomial collection
independent binomials, corresponds single value ui multinomial;
separately apply constraint parameters binomials sum
1. binomial, sample approximate distribution (possibly
temperature t). Then, generate particular multinomial distribution, renormalize
sampled parameters sum 1.
sample smoothed estimator, first note estimator smoothed
model (interpolating uniform)

;
ff^Si = (1(1,,))Nni++
1 smoothing parameter controlling amount smoothing (in experiments = 0:05), number possible values given multinomial.
sample truncated normal approximation (as Section 5)
smoothed estimate, i.e, mean = ff^Si variance 2 = (1N,) . Normalization
multinomial applied above.
Finally, generate random HMM given statistics , note parameters
P (ti !tj ) P (tjw) independent other. thus independently choose values
HMM's parameters multinomial distribution.

6.3 Examples HMM Training
Typically, concept learning problems formulated set training
examples independent other. training HMM, however,
state/output pair dependent previous state, presented (in principle)
single infinite input string training. order perform sample selection,
must divide infinite string (short) finite strings.
part-of-speech tagging, problem may solved considering sentence
individual example. generally, break text point tagging
unambiguous. particular, common lexicon specifies partsof-speech possible word (i.e, parameters P (tjw) positive).
bigram tagging, use unambiguous words (those one possible part speech)
example boundaries. Similar natural breakpoints occur HMM applications;
example, speech recognition consider different utterances separately.
cases HMM learning, natural breakpoints occur, heuristic
applied, preferring break `almost unambiguous' points input.
6. implementation smooth MLE interpolation uniform probability distribution,
following Merialdo (1991). Adaptation P (ffi = ai ) smoothed version estimator given
below.
j

351

fiArgamon & Dagan

6.4 Quantifying Disagreement
Recall selection algorithms decide whether select example based
much committee members disagree labeling. discussed Section 4.2.2,
suggest use vote entropy measuring classification disagreement committee members. idea supported fact found empirically
average normalized vote entropy words tagger (after training) classified correctly 0.25, whereas average entropy incorrectly classified words
0.66. demonstrates vote entropy useful measure classification uncertainty
(likelihood error) based training data.
bigram tagging, example consists sequence several words. implementation, measured vote entropy separately word sequence, use
average vote entropy sequence measurement disagreement
example. use average entropy rather entropy entire sequence,
number committee members small respect total number
possible tag sequences.

6.5 Results
present results applying committee-based sample selection bigram part-ofspeech tagging, comparing complete training examples corpus. Evaluation performed using University Pennsylvania tagged corpus ACL/DCI
CD-ROM I. ease implementation, used complete (closed) lexicon contains
words corpus.7 Approximately 63% word occurrences corpus
ambiguous lexicon (have one possible part-of-speech).
committee-based selection algorithm initialized using first 1,000 words
corpus, examined following examples corpus possible
labeling. training set consisted first million words corpus, sentence
ordering randomized compensate inhomogeneity corpus composition. test set
separate portion corpus consisting 20,000 words, starting first
1,000,000.
compared amount training required different selection methods achieve
given tagging accuracy test set, amount training tagging
accuracy measured ambiguous words.8
6.5.1 Labeling Efficiency
7. use lexicon provided Brill's part-of-speech tagger (Brill, 1992). actual application
complete lexicon would available, results using complete lexicon valid, evaluation
complete training committee-based selection comparative.
8. work tagging measured accuracy words, ambiguous ones. Complete
training system 1,000,000 words gave us accuracy 93.5% ambiguous words,
corresponds accuracy 95.9% words test set, comparable published results
bigram tagging.

352

fiCommittee-Based Sample Selection Probabilistic Classifiers

40000
Batch selection (m=5; N=100)
Thresholded selection (th=0.3)
Randomized selection (g=0.5)
Two member selection
Complete training

35000

Selected training

30000
25000
20000
15000
10000
5000
0
0.85

0.86

0.87

0.88

0.89
Accuracy

0.9

0.91

0.92

0.93

Figure 7: Labeled training versus classification accuracy. batch, random, thresholded runs, k = 5 = 50.

Figure 7 presents comparison results several selection methods. reported parameter settings best found selection method manual tuning.
Figure 7 shows advantage sample selection gives regard annotation cost.
example, complete training requires annotated examples containing 98,000 ambiguous
words achieve 92.6% accuracy, selection methods require 18,000{25,000
ambiguous words achieve accuracy. find that, first approximation,
methods considered give similar results. Thus, seems refined choice
selection method crucial achieving large reductions annotation cost.
6.5.2 Computational Efficiency

Figure 8 plots classification accuracy versus number words examined, instead
selected. Complete training clearly ecient terms, learns
examples examined. selective methods similar, though two member selection seems
require somewhat fewer examples examination methods. Furthermore,
since two committee members used method computationally ecient
evaluating examined example.
6.5.3 Model Size

ability committee-based selection focus informative parts
training corpus analyzed Figure 9. examined number lexical bigram
353

fiArgamon & Dagan

400000
Batch selection (m=5; N=100)
Thresholded selection (th=0.3)
Randomized selection (g=0.5)
Two member selection
Complete training

350000

Examined training

300000
250000
200000
150000
100000
50000
0
0.85

0.86

0.87

0.88

0.89
Accuracy

0.9

0.91

0.92

0.93

Figure 8: Examined training (both labeled unlabeled) versus classification accuracy.
batch, random, thresholded runs, k = 5 = 50.

20000

1600
Two member selection
Complete training

18000

Two member selection
Complete training

1400

14000

Bigram model size

Lexical model size

16000

12000
10000
8000
6000

1200
1000
800
600

4000
400

2000
0
0.85

0.86

0.87

0.88

0.89
Accuracy

0.9

0.91

0.92

200
0.85

0.93

(a)

0.86

0.87

0.88

0.89
0.9
Accuracy

0.91

0.92

0.93

0.94

(b)

Figure 9: Numbers frequency counts > 0, plotted (y-axis) versus classification accuracy
(x-axis). (a) Lexical counts (freq(t; w)) (b) Bigram counts (freq(t1 !t2 )).

354

fiCommittee-Based Sample Selection Probabilistic Classifiers

1
Two member selection
Batch selection (m=5; N=50)
Batch selection (m=5; N=100)
Batch selection (m=5; N=500)
Batch selection (m=5; N=1000)

0.98

Accuracy

0.96
0.94
0.92
0.9
0.88
0.86
0

100000

200000

300000 400000
Examined training

500000

600000

Figure 10: Evaluating batch selection, = 5. Classification accuracy versus number
words examined corpus different batch sizes.
counts stored (i.e, non-zero) training, using two member selection
algorithm complete training. graphs show, committee-based selection achieves
accuracy complete training fewer lexical bigram counts. achieve
92% accuracy, two member selection requires 6200 lexical counts 750 bigram counts,
compared 15,800 lexical counts 1100 bigram counts complete training.
implies many counts data needed correct tagging, since smoothing
estimates probabilities equally well.9 Committee-based selection ignores counts,
focusing efforts parameters improve model's performance. behavior
additional practical advantage reducing size model significantly. Also,
average count lower model constructed selective training fully trained
model, suggesting selection method tends avoid using examples increase
counts already known parameters.
6.5.4 Batch Selection

investigated properties batch selection, varying batch size 50 1000
examples, fixing number examples selected batch 5. found
terms number labeled examples required attain given accuracy, selection
different batch sizes performed similarly. means increased batch size
9. mentioned above, tagging phase smooth MLE estimates interpolation uniform
probability distribution, following Merialdo (1994).

355

fiArgamon & Dagan

seem improve effectiveness selection. hand, see
decrease performance increased batch size, might expected due
poorer modeling input distribution (as noted Section 2.2). may indicate
even batch size 1000 (selecting 1/200 examples seen) small enough
let us model input distribution reasonable accuracy. However, similarity
performance different batch sizes sequential selection
hold respect amount unlabeled training used. Figure 10 shows accuracy
attained function amount unlabeled training used. see quite clearly that,
expected, using larger batch sizes required examining far larger number unlabeled
training examples order obtain accuracy.

7. Discussion
7.1 Committee-Based Selection Monte-Carlo Technique
view committee-based selection Monte-Carlo method estimating probability distribution classes assigned example possible models, given
training data. proportion votes among committee members class c example e sample-based estimate probability, model chosen randomly
posterior model distribution, assigning c e. is, proportion votes
c given e, V (kc;e) , Monte-Carlo estimate
Z

P (cje; ) = TM (cje)P (M jS )dM


ranges possible models (vectors parameter values) model space M,
P (M jS ) posterior probability density model given statistics , TM (cje) = 1
c highest probability class e based (i.e, c = arg maxc PM (ci je),
PM (cje) class probability distribution e given model ), 0 otherwise. Vote
entropy, defined Section 4.2.2, thus approximation entropy P .
entropy direct measure uncertainty example classification possible models.
Note measure entropy final classes assigned example possible
models (i.e, TM ), class probabilities given single model (i.e, PM ),
illustrated CCF example Section 5.2. Measuring entropy PM (say, looking
expected probability models) would properly address properties 1 2
discussed Section 2.2.


7.2 Batch Selection
Property 3 discussed Section 2.2 states parameters affect examples
low overall utility, atypical examples useful learning. sequential
selection, property addressed independently examining input examples
drawn input distribution. way, implicitly model distribution model
parameters used classifying input examples. modeling, however, inherent
basic form batch selection, lead less effective (Freund et al.,
1997).
356

fiCommittee-Based Sample Selection Probabilistic Classifiers

diculty batch selection addressed directly McCallum Nigam (1998),
describe version batch selection (called pool-based sampling), differs
basic batch selection scheme presented Section 4.2.3 two ways. First, quantify disagreement committee members KL-divergence mean (Pereira
et al., 1993), rather vote entropy. significantly, disagreement measure
combined explicit density measure density-weighted sampling, documents similar many documents training set probably
selected labeling. intended address property 3 Section 2.2. authors
found empirically text classification using naive Bayes, density-weighted poolbased selection method using KL-divergence mean improved learning eciency
complete training. found sequential selection using vote entropy worse
complete training problem.
hypothesize due high degree sparseness example space
(text documents), leads large proportion examples atypical (even
though documents similar given atypical document rare, many different atypical
documents occur.) Since case, sequential variant may tend select many
atypical documents labeling, would degrade learner performance skewing
statistics. problem remedied adding density-weighting sequential selection
future research. may yield ecient sequential selection algorithm works
well highly sparse domains.

8. Conclusions
Labeling large training sets supervised classification often costly process, especially
complicated domain areas natural language processing. presented
approach reducing cost significantly using committee-based sample selection,
reduces redundant annotation examples contribute little new information.
method applicable whenever possible estimate posterior distribution
model space given training data. shown apply training Hidden
Markov Models, demonstrated effectiveness complex task part speech
tagging. Implicit modeling uncertainty makes selection system generally applicable
relatively simple implement. practical settings, method may applied
semi-interactive process, system selects several new examples annotation
time updates statistics receiving labels user.
committee-based sampling method addresses three factors relate
informativeness training example model parameters affects. factors
are: (1) statistical significance parameter's estimate, (2) parameter's effect
classification, (3) probability parameter used classification
future. use committee models uncertainty classification relative
entire model space, sequential selection implicitly models distribution
examples.
experimental study variants selection method suggests several practical
conclusions. First, found simplest version committee-based method,
357

fiArgamon & Dagan

using two-member committee, yields reduction annotation cost comparable
multi-member committee. two-member version simpler implement,
parameters tune computationally ecient. Second, generalized
selection scheme giving several alternatives optimizing method specific task.
bigram tagging, comparative evaluation different variants method showed
similar large reductions annotation cost, suggesting robustness committeebased approach. Third, sequential selection, implicitly models expected utility
example relative example distribution, worked general better batch
selection. Recent results improving batch selection modeling explicitly `typicality'
examples suggest comparison two approaches (as discussed previous
section). Finally, studied effect sample selection size trained model,
showing significant reduction model size selectively trained models.
future research propose investigate applicability effectiveness committeebased sample selection additional probabilistic classification tasks. Furthermore,
generality obtained implicitly modeling information gain suggests using variants
committee-based sampling non-probabilistic contexts, explicit modeling
information gain may impossible. contexts, committee members might generated randomly varying decisions made learning algorithm.

Acknowledgments
Discussions Yoav Freund, Yishai Mansour, Wray Buntine greatly enhanced
work. first author Bar-Ilan University work performed,
supported Fulbright Foundation part work.

References
Angluin, D. (1987). Learning regular sets queries counterexamples. Information
Computation, 75 (2), 87{106.
Angluin, D. (1988). Queries concept learning. Machine Learning, 2, 319{342.
Baum, L. E. (1972). inequality associated maximization technique statistical
estimation probabilistic functions markov process. Inequalities, 3:1-8.
Black, E., Jelinek, F., Lafferty, J., Magerman, D., Mercer, R., & Roukos, S. (1993). Towards
history-based grammars: using richer models probabilistic parsing. Proc.
Annual Meeting ACL, pp. 31{37.
Brill, E. (1992). simple rule-based part speech tagger. Proc. ACL Conference
Applied Natural Language Processing.
Church, K. W. (1988). stochastic parts program noun phrase parser unrestricted
text. Proc. ACL Conference Applied Natural Language Processing.
Cohn, D., Atlas, L., & Ladner, R. (1994). Improving generalization active learning.
Machine Learning, 15, 201{221.
358

fiCommittee-Based Sample Selection Probabilistic Classifiers

Cohn, D. A., Ghahramani, Z., & Jordan, M. I. (1995). Active learning statistical
models. Tesauro, G., Touretzky, D., & Alspector, J. (Eds.), Advances Neural
Information Processing, Vol. 7. Morgan Kaufmann, San Mateo, CA.
Dagan, I., & Engelson, S. (1995). Committee-based sampling training probabilistic
classifiers. Proceedings International Conference Machine Learning.
Elworthy, D. (1994). Baum-Welch re-estimation improve taggers?. Proc. ACL
Conference Applied Natural Language Processing, pp. 53{58.
Engelson, S., & Dagan, I. (1996a). Minimizing manual annotation cost supervised learning corpora. Proceedings 34th Annual Meeting Association
Computational Linguistics.
Engelson, S., & Dagan, I. (1996b). Sample selection natural language learning.
Wermter, S., Riloff, E., & Scheler, G. (Eds.), Symbolic, Connectionist, Statistical
Approaches Learning Natural Language Processing. Springer-Verlag.
Freund, Y., Seung, H. S., Shamir, E., & Tishby, N. (1997). Selective sampling using
Query Committee algorithm. Machine Learning, 28, 133{168.
Freund, Y. (1994). Sifting informative examples random source. Working Notes
Workshop Relevance, AAAI Fall Symposium Series, pp. 85{89.
Gale, W., Church, K., & Yarowsky, D. (1993). method disambiguating word senses
large corpus. Computers Humanities, 26, 415{439.
Johnson, N. L. (1970). Continuous Univariate Distributions { 2. John Wiley & Sons, New
York.
Johnson, N. L. (1972). Continuous Multivariate Distributions. John Wiley & Sons, New
York.
Kupiec, J. (1992). Robust part-of-speech tagging using hidden makov model. Computer
Speech Language, 6, 225{242.
Lewis, D. D., & Catlett, J. (1994). Heterogeneous uncertainty sampling supervised
learning. Proceedings International Conference Machine Learning.
Lewis, D. D., & Gale, W. A. (1994). sequential algorithm training text classifiers.
Proceedings ACM SIGIR Conference.
Liere, R., & Tadepalli, P. (1997). Active learning committees text categorization.
Proceedings National Conference Artificial Intelligence.
Littlestone, N. (1988). Learning quickly irrelevant features abound: new linearthreshold algorithm. Machine Learning, 2.
MacKay, D. J. C. (1992a). evidence framework applied classification networks.
Neural Computation, 4.
359

fiArgamon & Dagan

MacKay, D. J. C. (1992b). Information-based objective functions active data selection.
Neural Computation, 4.
Matan, O. (1995). On-site learning. Tech. rep. LOGIC-95-4, Stanford University.
McCallum, A. K., & Nigam, K. (1998). Employing EM pool-based active learning
text classification. Proceedings International Conference Machine
Learning.
Merialdo, B. (1991). Tagging text probabilistic model. Proc. Int'l Conf.
Acoustics, Speech, Signal Processing.
Merialdo, B. (1994). Tagging text probabilistic model. Computational Linguistics,
20 (2), 155{172.
Mitchell, T. (1982). Generalization search. Artificial Intelligence, 18.
Pereira, F., Tishby, N., & Lee, L. (1993). Distributional clustering english words.
Proceedings Annual Meeting Association Computational Linguistics
(ACL).
Plutowski, M., & White, H. (1993). Selecting concise training sets clean data. IEEE
Trans. Neural Networks, 4 (2).
Press, W. H., Flannery, B. P., Teukolsky, S. A., & Vetterling, W. T. (1988). Numerical
Recipes C. Cambridge University Press.
Rabiner, L. R. (1989). tutorial Hidden Markov Models selected applications
speech recognition. Proc. IEEE, 77 (2).
Seung, H. S., Opper, M., & Sompolinsky, H. (1992). Query committee. Proceedings
ACM Workshop Computational Learning Theory.
Viterbi, A. J. (1967). Error bounds convolutional codes asymptotically optimal
decoding algorithm. IEEE Trans. Informat. Theory, T-13.

360


