journal artificial intelligence

submitted published

evolutionary reinforcement learning
david e moriarty

moriarty isi edu

university southern california information sciences institute
admiralty way marina del rey ca

alan c schultz

navy center applied artificial intelligence
naval laboratory washington dc

schultz aic nrl navy mil

john j grefenstette

institute biosciences bioinformatics biotechnology
george mason university manassas va

gref ib gmu edu

abstract

two distinct approaches solving reinforcement learning namely
searching value function space searching policy space temporal difference methods evolutionary well known examples approaches kaelbling
littman moore recently provided informative survey temporal difference methods article focuses application evolutionary reinforcement
learning emphasizing alternative policy representations credit assignment methods specific genetic operators strengths weaknesses evolutionary
reinforcement learning presented along survey representative
applications

introduction
kaelbling littman moore recently sutton barto provide informative surveys field reinforcement learning rl characterize two
classes methods reinforcement learning methods search space value functions methods search space policies former class exemplified
temporal difference td method latter evolutionary ea
kaelbling et al focus entirely first set methods provide
excellent account state art td learning article intended round
picture addressing evolutionary methods solving reinforcement learning

kaelbling et al clearly illustrate reinforcement learning presents challenging array
diculties process scaling realistic tasks including associated
large state spaces partially observable states rarely occurring states nonstationary environments point best remains open question
sensible pursue parallel lines alternative methods beyond
scope article address whether better general search value function
space policy space hope highlight strengths evolutionary
reinforcement learning reader advised view
c


ai access foundation morgan kaufmann publishers rights reserved

fimoriarty schultz grefenstette

article ea vs td discussion cases two methods provide complementary
strengths hybrid approaches advisable fact survey implemented systems
illustrates many ea reinforcement learning systems include elements tdlearning well
next section spells reinforcement learning order provide
specific anchor later discussion section presents particular td method section outlines call evolutionary reinforcement learning
earl provides simple example particular earl system following three
sections focus features distinguish eas rl eas general function optimization including alternative policy representations credit assignment methods
rl specific genetic operators sections highlight strengths weaknesses
ea section brie surveys successful applications ea systems
challenging rl tasks final section summarizes presentation points
directions

reinforcement learning

reinforcement learning methods share goal solve sequential decision tasks
trial error interactions environment barto sutton watkins
grefenstette ramsey schultz sequential decision task agent interacts
dynamic system selecting actions affect state transitions optimize
reward function formally given time step agent perceives state
st selects action system responds giving agent possibly zero
numerical reward r st changing state st st state transition may
determined solely current state agent action may involve stochastic
processes
agent goal learn policy maps states actions
optimal policy defined many ways typically defined policy
produces greatest cumulative reward states

argmax
v




v cumulative reward received state policy
many ways compute v one uses discount rate discount rewards
time sum computed infinite horizon

v




x
ir









rt reward received time step alternatively v could computed
summing rewards finite horizon h

v st

xh r






agent state descriptions usually identified values returned
sensors provide description agent current state state


fievolutionary reinforcement learning

world often sensors give agent complete state information thus
state partially observable
besides reinforcement learning intelligent agents designed paradigms
notably supervised learning brie note major differences
among approaches general methods require explicit model
state transition function given model search
possible action choices action sequence guide agent
initial state goal state since operate model
environment backtrack undo state transitions enter undesirable states
contrast rl intended apply situations suciently tractable action
model exist consequently agent rl paradigm must actively explore
environment order observe effects actions unlike rl agents
cannot normally undo state transitions course cases may possible
build action model experience sutton enabling
experience accumulates however rl focuses behavior agent
insucient knowledge perform
agents trained supervised learning supervised learning agent
presented examples state action pairs along indication action
correct incorrect goal supervised learning induce general policy
training examples thus supervised learning requires oracle supply
correctly labeled examples contrast rl require prior knowledge correct
incorrect decisions rl applied situations rewards sparse
example rewards may associated certain states cases may
impossible associate label correct incorrect particular decisions without
reference agent subsequent decisions making supervised learning infeasible
summary rl provides exible design intelligent agents situations supervised learning impractical rl applied
significant domain knowledge unavailable costly obtain
example common rl task robot control designers autonomous robots often
lack sucient knowledge intended operational environment use
supervised learning regime design control policy robot case
goal rl would enable robot generate effective decision policies explores
environment
figure shows simple sequential decision task used example later
task agent grid world move state state
selecting among two actions right r sensor agent returns
identity current state agent starts state receives reward
indicated upon visiting state task continues agent moves grid
world e g taking action state goal learn policy returns
highest cumulative rewards example policy sequences
actions r r r r starting state gives optimal score


fimoriarty schultz grefenstette



b

c



e





























































figure simple grid world sequential decision task agent starts state
receives row column current box sensory input agent moves
one box another selecting two moves right
agent score increased payoff indicated box goal
policy maximizes cumulative score

policy space vs value function space
given reinforcement learning described previous section
address main topic optimal policy consider two main approaches
one involves search policy space involves search value function space
policy space search methods maintain explicit representations policies modify
variety search operators many search methods considered
including dynamic programming value iteration simulated annealing evolutionary
focuses evolutionary specialized
reinforcement learning task
contrast value function methods maintain explicit representation
policy instead attempt learn value function v returns expected
cumulative reward optimal policy state focus value
function approaches rl design learn value functions
experience common learning value functions temporal difference td method described next section

temporal difference reinforcement learning
stated introduction comprehensive comparison value function search
direct policy space search beyond scope nevertheless useful
point key conceptual differences typical value function methods typical
evolutionary searching policy space common learning
value function v rl temporal difference td method sutton


fievolutionary reinforcement learning

td learning uses observations prediction differences consecutive
states update value predictions example two consecutive states j return
payoff prediction values respectively difference suggests payoff
state may overestimated reduced agree predictions
state j updates value function v achieved following update rule

v st v st v st v st rt

represents learning rate rt immediate reward thus difference
predictions v st v st consecutive states used measure prediction error
consider chain value predictions v v sn consecutive state transitions
last prediction v sn containing non zero reward environment

many iterations sequence update rule adjust values state
agree successors eventually reward received v sn
words single reward propagated backwards chain value predictions
net accurate value function used predict expected reward
state system
mentioned earlier
goal
td methods learn value function




optimal policy v given v optimal action computed
following equation

argmax
v




course already stated rl state transition function unknown
agent without knowledge way evaluating alternative
value function used compute called q function q watkins
watkins dayan q function value function represents
expected value taking action state acting optimally thereafter

q r v


r represents immediate reward received state given q function
actions optimal policy directly computed following equation

argmax
q




q st q st max
q st q st r st




table shows q function grid world figure table
representation q function associates cumulative future payoffs state action
pair system letter number pairs top represent state given row
column figure r represent actions right respectively
td method adjusts q values decision selecting next action
agent considers effect action examining expected value state
transition caused action
q function learned following td update equation




fimoriarty schultz grefenstette

b b b b b c c c c c e e e e e
r


table q function simple grid world value associated state action
pair
essentially equation updates q st current reward predicted
reward future actions selected optimally watkins dayan proved
updates performed fashion every q value explicitly represented
estimates asymptotically converge correct values reinforcement learning
system thus use q values select optimal action state qlearning widely known implementation temporal difference learning
use qualitative comparisons evolutionary approaches later sections

evolutionary reinforcement learning earl
policy space rl searches policies optimize appropriate objective
function many search might used survey focuses evolutionary
begin brief overview simple ea rl followed detailed
discussion features characterize general class eas rl

design considerations evolutionary

evolutionary eas global search techniques derived darwin theory
evolution natural selection ea iteratively updates population potential
solutions often encoded structures called chromosomes iteration
called generation ea evaluates solutions generates offspring fitness
solution task environment substructures genes solutions
modified genetic operators mutation recombination idea
structures associated good solutions mutated combined form
even better solutions subsequent generations canonical evolutionary
shown figure wide variety eas developed including genetic
holland goldberg evolutionary programming fogel owens
walsh genetic programming koza evolutionary strategies rechenberg

eas general purpose search methods applied variety domains
including numerical function optimization combinatorial optimization adaptive control
adaptive testing machine learning one reason widespread success eas
relatively requirements application namely
appropriate mapping search space space chromosomes
appropriate fitness function


fievolutionary reinforcement learning

procedure ea
begin

initialize p
evaluate structures p
termination condition satisfied
begin

select p p
alter structures p
evaluate structures p
end
end
figure pseudo code evolutionary
example case parameter optimization common represent list
parameters vector real numbers bit string encodes parameters
representations standard genetic operators mutation
cut splice crossover applied straightforward manner produce genetic
variations required see figure user must still decide rather large number
control parameters ea including population size mutation rates recombination
rates parent selection rules extensive literature studies suggest
eas relatively robust wide range control parameter settings grefenstette
schaffer caruana eshelman das thus many eas
applied relatively straightforward manner
however many applications eas need specialized domain grefenstette critical design choice facing user representation mapping search space knowledge structures
phenotype space space chromosomes genotype space many studies
shown effectiveness eas sensitive choice representations
sucient example choose arbitrary mapping search space space
chromosomes apply standard genetic operators hope best makes
good mapping subject continuing general consensus candidate solutions share important phenotypic similarities must exhibit similar forms
building blocks represented chromosomes holland follows
user ea must carefully consider natural way represent elements
search space chromosomes moreover often necessary design appropriate
mutation recombination operators specific chosen representation
end design process representation genetic operators selected
ea comprise form search bias similar biases machine learning meth

fimoriarty schultz grefenstette

parent



b

c



e

f

g

parent



b

c



e

f

g

offspring



b

c



e

f

g

offspring



b

c



e

f

g

figure genetic operators fixed position representation two offspring generated crossing selected parents operation shown called one point
crossover first offspring inherits initial segment one parent
final segment parent second offspring inherits pattern
genes opposite parents crossover point position chosen
random second offspring incurred mutation shaded gene
ods given proper bias ea quickly identify useful building blocks within
population converge promising areas search space
case rl user needs make two major design decisions first
space policies represented chromosomes ea second fitness
population elements assessed answers questions depend user
chooses bias ea next section presents simple earl adopts
straightforward set design decisions example meant provide baseline
comparison elaborate designs

simple earl

remainder shows many ways use eas search space
rl policies section provides concrete example simple earl call
earl pseudo code shown figure system provides ea counterpart
simple table td system described section
straightforward way represent policy ea use single chromosome per policy single gene associated observed state earl
gene value allele biological terminology represents action value associated
corresponding state shown figure table shows part earl population
policies sample grid world number policies population
usually order
fitness policy population must ect expected accumulated fitness
agent uses given policy fixed constraints fitness
individual policy evaluated world deterministic sample grid world
ways exploit specific knowledge eas include use heuristics initialize
population hybridization specific search see grefenstette
discussions methods



fievolutionary reinforcement learning

procedure earl
begin

initialize population policies p
evaluate policies p
termination condition satisfied
begin

select high payoff policies p policies p
update policies p
evaluate policies p
end
end
figure pseudo code evolutionary reinforcement learning system
policy












sn


figure table policy representation observed state gene indicates
preferred action state representation standard genetic
operators mutation crossover applied
fitness policy evaluated single trial starts agent
initial state terminates agent reaches terminal state e g falls grid
grid world non deterministic worlds fitness policy usually averaged
sample trials options include measuring total payoff achieved
agent fixed number steps measuring number steps required achieve
fixed level payoff
fitness policies population determined population
generated according steps usual ea figure first parents selected
reproduction typical selection method probabilistically select individuals
relative fitness
pi
pr pi pnfitness
j fitness pj



pi represents individual n total number individuals selection
rule expected number offspring given policy proportional policy
fitness example policy average fitness might single offspring whereas


fimoriarty schultz grefenstette

policy









r

r


r















r




r
r
r
r
r

b
r
r




b
r
r
r
r
r

b
r
r

r
r

b
r
r
r
r


b
r
r
r
r
r

c



r
r

c
r





c

r

r
r

c

r
r
r
r

c
r


r



r
r
r

r





r



r
r
r
r
r


r
r
r

r


r
r
r
r


e






e
r
r
r
r
r

e
r





e






e fitness
r
r

r


table ea population five decision policies sample grid world simple
policy representation specifies action state world fitness
corresponds payoffs accumulated policy grid
world
policy twice average fitness would two offspring offspring formed
cloning selected parents policies generated applying standard
genetic operators crossover mutation clones shown figure process
generating populations strategies continue indefinitely terminated
fixed number generations acceptable level performance achieved
simple rl grid world earl may provide adequate later sections point ways even earl exhibits
strengths complementary td methods rl however case td
methods earl methods extended handle many challenges inherent
realistic rl following sections survey extensions organized around three specific biases distinguish eas reinforcement learning earl
generic eas policy representations fitness credit assignment rlspecific genetic operators

policy representations earl

perhaps critical feature distinguishes classes eas one another
representation used example eas function optimization use simple string
vector representation whereas eas combinatorial optimization use distinctive representations permutations trees graph structures likewise eas rl use
distinctive set representations policies range potential policy representations unlimited representations used earl systems date
largely categorized along two discrete dimensions first policies may represented condition action rules neural networks second policies may represented
single chromosome representation may distributed one
populations

single chromosome representation policies
rule policies

rl practical interest number observable states large
simple table representation earl impractical large scale state
many parent selection rules explored grefenstette b



fievolutionary reinforcement learning

policy

c ai

c ai

c ai



c ik aik

figure rule policy representation gene represents condition action rule
maps set states action general rules independent
position along chromosome con ict resolution mechanisms may
needed conditions rules allowed intersect
w
w k
policy

w

w

w



wk




wk
wj

figure simple parameter representation weights neural network fitness
policy payoff agent uses corresponding neural net
decision policy
spaces reasonable represent policy set condition action rules
condition expresses predicate matches set states shown figure early
examples representation include systems ls smith ls schaffer
grefenstette followed later samuel grefenstette et al
neural net representation policies

td rl systems earl systems often employ neural net representations
function approximators simplest case see figure neural network
agent decision policy represented sequence real valued connection weights
straightforward ea parameter optimization used optimize weights
neural network belew mcinerney schraudolph whitley dominic das
anderson yamauchi beer representation thus requires least
modification standard ea turn distributed representations policies
earl systems

distributed representation policies

previous section outlined earl approaches treat agent decision policy
single genetic structure evolves time section addresses earl approaches
decompose decision policy smaller components approaches two
potential advantages first allow evolution work detailed level task
e g specific subtasks presumably evolving solution restricted subtask


fimoriarty schultz grefenstette

sensors

message list

rewards

classifiers

decision

evolutionary


figure holland learning classifier system
easier evolving monolithic policy complex task second decomposition permits
user exploit background knowledge user might base decomposition
subtasks prior analysis overall performance task example might known
certain subtasks mutually exclusive therefore learned independently
user might decompose complex task subtasks certain components
explicitly programmed components learned
terms knowledge representation earl alternative single chromosome
representation distribute policy several population elements assigning
fitness individual elements policy evolutionary selection pressure
brought bear detailed aspects learning task fitness
function individual subpolicies individual rules even individual neurons general
analogous classic td methods take extreme
learning statistics concerning state action pair case single chromosome
representations partition distributed earl representations rule
neural net classes
distributed rule policies

well known example distributed rule earl learning classifier systems lcs model holland reitman holland wilson
lcs uses evolutionary evolve rules called classifiers
map sensory input appropriate action figure outlines holland lcs framework
holland sensory input received posted message list left
hand side classifier matches message message list right hand side posted
message list messages may subsequently trigger classifiers post
messages invoke decision lcs traditional forward chaining model
rule systems
lcs chromosome represents single decision rule entire population
represents agent policy general classifiers map set observed states set
messages may interpreted internal state changes actions example


fievolutionary reinforcement learning

condition
action strength

r













table lcs population grid world care symbol allows
generality conditions example first rule says turn right column
strength rule used con ict resolution parent selection
genetic

lcs
lcs

lcs

environment

figure two level hierarchical alecsys system lcs learns specific behavior
interactions among rule sets pre programmed
learning agent grid world figure two sensors one column
one row population lcs might appear shown table
first classifier matches state column recommends action r classifier
statistic called strength estimates utility rule strength statistics
used con ict resolution one action recommended
fitness genetic genetic operators applied highly fit classifiers
generate rules generally population size e number rules policy
kept constant thus classifiers compete space policy
another way earl systems distribute representation policies partition
policy separate modules module updated ea dorigo
colombetti describe architecture called alecsys complex reinforcement learning task decomposed subtasks learned via separate
lcs shown figure provide method called behavior analysis training
bat manage incremental training agents distributed lcs architecture
single chromosome representation extended partitioning policy across multiple co evolving populations example cooperative co evolution
model potter agent policy formed combining chromosomes several independently evolving populations chromosome represents set rules
figure rules address subset performance task example
separate populations might evolve policies different components complex task


fimoriarty schultz grefenstette

ea
ea

domain
model
collaboration

fitness

evolutionary


population

representative

merge

representative

individual

evaluated

representative

ea

ea n

representative

figure cooperative coevolutionary architecture perspective ith ea instance ea contributes representative merged others
representatives form collaboration policy agent fitness
representative ects average fitness collaborations

might address mutually exclusive sets observed states fitness chromosome
computed overall fitness agents employ chromosome part
combined chromosomes combined chromosomes represent decision policy
called collaboration figure
distributed network policies

distributed earl systems neural net representations designed
potter de jong separate populations neurons evolve evaluation
neuron fitness collaboration neurons selected population
sane moriarty miikkulainen two separate populations maintained
evolved population neurons population network blueprints motivation sane comes priori knowledge individual neurons fundamental
building blocks neural networks sane explicitly decomposes neural network search
several parallel searches effective single neurons neuron level evolution provides evaluation recombination neural network building blocks
population blueprints search effective combinations building blocks figure
gives overview interaction two populations
individual blueprint population consists set pointers individuals
neuron population generation neural networks constructed
combining hidden neurons specified blueprint blueprint receives fitness
according well corresponding network performs task neuron receives
fitness according well top networks participates perform
task aggressive genetic selection recombination strategy used quickly build
propagate highly fit structures neuron blueprint populations


fievolutionary reinforcement learning

network blueprint population

neuron population

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

figure overview two populations sane member neuron population specifies series connections connection labels weights
made within neural network member network blueprint population specifies series pointers specific neurons used build
neural network

fitness credit assignment earl

evolutionary driven concept natural selection population
elements higher fitness leave offspring later generations thus uencing
direction search favor high performance regions search space concept
fitness central ea section discuss features fitness model
common across earl systems specifically focus ways fitness
function ects distinctive structure rl

agent model

first common features earl fitness fitness computed
respect rl agent however policy represented ea must
converted decision policy agent operating rl environment agent
assumed observe description current state select next action consulting
current policy collect whatever reward provided environment earl
systems td systems agent generally assumed perform little additional
computation selecting next action neither limits agent
strict stimulus response behavior usually assumed agent perform
extensive reasoning acting assumption ects fact
rl tasks involve sort control activity agent must respond dynamic
environment within limited time frame


fimoriarty schultz grefenstette

policy level credit assignment

shown previous section meaning fitness earl systems may vary depending population elements represent single chromosome representation
fitness associated entire policies distributed representation fitness may associated individual decision rules case fitness ects accumulated
rewards received agent course interaction environment
specified rl model fitness may ect effort expended amount delay
worthwhile considering different approaches credit assignment td
ea methods reinforcement learning payoffs may sparse
associated certain states consequently payoff may ect quality
extended sequence decisions rather individual decision example robot
may receive reward movement places goal position within room
robot reward however depends many previous movements leading
point dicult credit assignment therefore exists apportion
rewards sequence decisions individual decisions
general ea td methods address credit assignment different ways td approaches credit reward signal explicitly propagated
decision made agent many iterations payoffs distributed across
sequence decisions appropriately discounted reward value associated
individual state decision pair
simple earl systems earl rewards associated sequences
decisions distributed individual decisions credit assignment
individual decision made implicitly since policies prescribe poor individual decisions
fewer offspring future generations selecting poor policies evolution
automatically selects poor individual decisions building blocks consisting
particular state action pairs highly correlated good policies propagated
population replacing state action pairs associated poorer policies
figure illustrates differences credit assignment td earl
grid world figure q learning td method explicitly assigns credit blame
individual state action pair passing back immediate reward estimated payoff
state thus error term becomes associated action performed
agent ea explicitly propagate credit action rather
associates overall fitness entire policy credit assigned implicitly
fitness evaluations entire sequences decisions consequently ea tend select
policies generate first third sequences achieve lower fitness
scores ea thus implicitly selects action state b example
present bad sequences present good sequences

subpolicy credit assignment

besides implicit credit assignment performed building blocks earl systems
addressed credit assignment directly shown section
individuals earl system might represent entire policies components
policy e g component rule sets individual decision rules individual neurons
distributed representation earls fitness explicitly assigned individual components


fievolutionary reinforcement learning

td explicit credit assignment
max q b

r

b

max q b

r

max q



max q b



r

b

b

b

c



c

r

b

b r

c

c



c



r

b

b

c









b r

c





max q c

max q c

b

max q c

b r

c

c

max q b

b

fitness

max q c

b

max q c

b r

max q b

r

max q b

b

max q b

b

max q



max q b

ea implicit credit assignment

max q

c

figure explicit vs implicit credit assignment q learning td method assigns credit
state action pair immediate reward predicted future
rewards ea method assigns credit implicitly associating fitness values
entire sequences decisions
cases policy represented explicit components different fitness functions
associated different evolving populations allowing implementer shape
overall policy evolving subpolicies specific subtasks dorigo colombetti
potter de jong grefenstette ambitious goal allow system
manage number co evolving species well form interactions potter
exciting still early stage
example lcs model classifier decision rule strength
updated td method called bucket brigade holland
bucket brigade strength classifier used bid classifiers
right post messages bids subtracted winning classifiers passed back
classifiers posted enabling message previous step classifier strengths
thus reinforced classifier posts message triggers another classifier
classifier invokes decision lcs receives strength reinforcement directly
environment bucket brigade bid passing mechanism clearly bears strong
relation method temporal differences sutton bucket brigade updates
given classifier strength strength classifiers fire direct
activation td methods differ slightly respect assign credit
strictly temporal succession take account causal relations steps
remains unclear appropriate distributing credit
even single chromosome representations td methods adopted
earl systems samuel gene decision rule maintains quantity called
strength used resolve con ict one rule matches agent current
sensor readings payoff obtained thereby terminating trial strengths


fimoriarty schultz grefenstette

rules fired trial updated grefenstette addition resolving
con icts rule strength plays role triggering mutation operations described
next section

rl specific genetic operators

creation special genetic operators provides another avenue imposing rlspecific bias eas specialized operators earl systems first appeared holland
called triggered operators responsible creating classifiers
learning agent found classifier existing population matched
agent current sensor readings case high strength rule explicitly generalized
cover set sensor readings similar rule creation operator included
early versions samuel grefenstette et al later versions samuel included
number mutation operators created altered rules agent early
experiences example samuel specialization mutation operator triggered
low strength general rule fires episode high payoff
case rule conditions reduced generality closely match agent sensor
readings example agent sensor readings range bearing
original rule
range bearing set turn strength

rule would
range bearing set turn strength

since episode triggering operator resulted high payoff one might suspect
original rule generalized specific version might lead
better strength rule initialized payoff received
triggering episode considered lamarckian operator agent
experience causing genetic change passed later offspring
samuel uses rl specific crossover operator recombine policies particular
crossover samuel attempts cluster decision rules assigning offspring
example suppose traces previous evaluations parent strategies follows ri j denotes j th decision rule policy
trace parent
episode


r r r r high payoff
r r r
low payoff
jean baptiste lamarck developed evolutionary theory stressed inheritance acquired characteristics particular acquired characteristics well adapted surrounding environment
course lamarck theory superseded darwin emphasis two stage adaptation undirected
variation followed selection generally failed substantiate lamarckian mechanisms
biological systems gould



fievolutionary reinforcement learning



trace parent


r r
r r r


one possible offspring would

low payoff
high payoff

fr r r r r r r r r g
motivation rules fire sequence achieve high payoff
treated group recombination order increase likelihood offspring
policy inherit better behavior patterns parents rules
fire successful episodes e g r randomly assigned one two offspring
form crossover lamarckian since triggered experiences
agent directly related structure rl since groups
components policies according temporal association among decision rules

strengths earl

ea represents interesting alternative solving rl offering
several potential advantages scaling realistic applications particular earl
systems developed address dicult challenges rl including
large state spaces
incomplete state information
non stationary environments
section focuses ways earl address challenges

scaling large state spaces

many early papers rl literature analyze eciency alternative learning methods
toy similar grid world shown figure studies useful
academic exercises number observed states realistic applications rl likely
preclude requires explicit storage manipulation statistics
associated observable state action pair two ways earl policy
representations help address large state spaces generalization selectivity
policy generalization

earl policy representations specify policy level abstraction higher
explicit mapping observed states actions case rule representations
rule language allows conditions match sets states thus greatly reducing storage


fimoriarty schultz grefenstette

b b b b b c c c c c e e e e e
r
l

table approximated value function population table table displays average fitness policies select state action pair ects
estimated impact action overall fitness given tiny population
size example estimates particularly accurate note question
marks states actions converged since policies select alternative action population statistics impact actions
fitness different simple td methods statistics actions
maintained

required specify policy noted however generality rules
within policy may vary considerably level rules specify action
single observed state way completely general rules recommend action
regardless current state likewise neural net representations mapping function
stored implicitly weights connections neural net case
generalized policy representation facilitates search good policies grouping together
states action required
policy selectivity

earl systems selective representations policies ea learns mappings observed states recommended actions usually eliminating explicit information
concerning less desirable actions knowledge bad decisions explicitly preserved
since policies make decisions selected evolutionary
eventually eliminated population advantage selective representations attention focused profitable actions reducing space requirements
policies
consider example simple earl operating grid world population evolves policies normally converge best actions specific state
selective pressure achieve high fitness levels example population shown
table converged alleles actions states b b e e
converged state action pairs highly correlated fitness example policies
converged action r state b taking action r state b achieves much higher
expected return action vs table policies select action
state b achieve lower fitness scores selected simple earl snapshot population table provides implicit estimate corresponding td value
function table distribution biased toward profitable state actions
pairs


fievolutionary reinforcement learning


l


l

red

r

blue

r


green

l

blue

l



r
r



figure environment incomplete state information circles represent
states world colors represent agent sensory input agent
equally likely start red state green state

dealing incomplete state information

clearly favorable condition reinforcement learning occurs agent
observe true state dynamic system interacts complete state
information available td methods make ecient use available feedback associating
reward directly individual decisions real world situations however agent
sensors likely provide partial view may fail disambiguate many
states consequently agent often unable completely distinguish current
state termed perceptual aliasing hidden state
case limited sensory information may useful associate rewards
larger blocks decisions consider situation figure agent must
act without complete state information circles represent specific states world
colors represent sensor information agent receives within state square
nodes represent goal states corresponding reward shown inside state
agent choice two actions l r assume state transitions
deterministic agent equally likely start state
red green sensor readings
example two different states return sensor reading blue
agent unable distinguish moreover actions blue
state return different rewards q function applied treats sensor
reading blue one observable state rewards action averaged
blue states thus q blue l q blue r converge respectively
since reward q blue r higher alternatives observable states red
green agent policy q learning choose enter observable state blue
time final decision policy q learning shown table table
shows optimal policy respect agent limited view world


fimoriarty schultz grefenstette

value function policy optimal policy
r
r
l
r
r
l
expected reward



red
green
blue

table policy expected reward returned converged q function compared
optimal policy given sensory information
words policy ects optimal choices agent cannot distinguish two blue
states
associating values individual observable states simple td methods
vulnerable hidden state example ambiguous state information
misleads td method mistakenly combines rewards two different states
system confounding information multiple states td cannot recognize
advantages might associated specific actions specific states example
action l top blue state achieves high reward
contrast since ea methods associate credit entire policies rely
net decision sequences sensor information may
ambiguous example evolutionary exploits disparity rewards
different blue states evolves policies enter good blue state avoid
bad one agent remains unable distinguish two blue states evolutionary implicitly distinguishes among ambiguous states rewarding policies
avoid bad states
example ea method expected evolve optimal policy current
example given existing ambiguous state information policies choose action
sequence r l starting red state achieve highest levels fitness
therefore selected reproduction ea agents policies
placed green state select action l receive lowest fitness score since
subsequent action l blue sensors returns negative reward thus many
policies achieve high fitness started red state selected
choose l green state course many generations policies must
choose action r green state maximize fitness ensure survival
confirmed hypotheses empirical tests q learner single step updates
table representation converged values table every run
evolutionary consistently converged population optimal policy
figure shows average percentage optimal policy population function
time averaged independent runs
thus even simple ea methods earl appear robust presence
hidden states simple td methods however refined sensor information could
still helpful previous example although ea policies achieve better average
reward td policy evolved policy remains unable procure
used binary tournament selection policy population crossover probability
mutation rate



fievolutionary reinforcement learning



percentage optimal





















generation











figure optimal policy distribution hidden state evolutionary
graph plots percentage optimal policies population
averaged runs
rewards two blue states rewards could realized however
agent could separate two blue states thus method generates additional
features disambiguate states presents important asset ea methods kaelbling
et al describe several promising solutions hidden state
additional features agent previous decisions observations automatically
generated included agent sensory information chrisman lin mitchell
mccallum ring methods effective disambiguating
states td methods initial studies required determine
extent similar methods resolve significant hidden state information realistic
applications would useful develop ways use methods augment sensory
data available ea methods well

non stationary environments

agent environment changes time rl becomes even dicult
since optimal policy becomes moving target classic trade exploration
exploitation becomes even pronounced techniques encouraging exploration
td rl include adding exploration bonus estimated value state action
pairs ects long since agent tried action sutton
building statistical model agent uncertainty dayan sejnowski
simple modifications standard evolutionary offer ability track nonstationary environments thus provide promising rl dicult
cases
fact evolutionary search competition within population policies
suggest immediate benefits tracking non stationary environments extent
population maintains diverse set policies changes environment bias


fimoriarty schultz grefenstette

selective pressure favor policies fit current environment
long environment changes slowly respect time required evaluate
population policies population able track changing fitness landscape
without alteration empirical studies maintaining
diversity within population may require higher mutation rate usually
adopted stationary environments cobb grefenstette
addition special mechanisms explored order make eas responsive rapidly changing environments example grefenstette suggests
maintaining random search within restricted portion population random
population elements analogous immigrants populations uncorrelated
fitness landscapes maintaining source diversity permits ea respond rapidly
large sudden changes fitness landscape keeping randomized portion
population less population impact search eciency
stationary environments minimized general easily applied
earl systems
useful developed ensure diversity evolving popultions include fitness sharing goldberg richardson crowding de jong
local mating collins jefferson goldberg fitness sharing model example similar individuals forced share large portion single fitness value
shared solution point sharing decreases fitness similar individuals causes
evolution select individuals overpopulated niches
earl methods employ distributed policy representations achieve diversity automatically well suited adaptation dynamic environments distributed
representation individual represents partial solution complete solutions
built combining individuals individual solve task
evolutionary search several complementary individuals together
solve task evolutionary pressures therefore present prevent convergence
population moriarty miikkulainen showed inherent diversity specialization sane allow adapt much quickly changes environment
standard convergent evolutionary
finally learning system detect changes environment even direct
response possible anytime learning model grefenstette ramsey
earl system maintains case base policies indexed values environmental
detectors corresponding environment given policy evolved
environmental change detected population policies partially reinitialized
previously learned policies selected basis similarity previously
encountered environment current environment environment
changes cyclic population immediately seeded policies
effect last occurrence current environment population
policies protected kinds errors detecting environmental
changes example even spurious environmental change mistakenly detected
learning unduly affected since part current population policies
replaced previously learned policies zhou explored similar
lcs


fievolutionary reinforcement learning

summary earl systems respond non stationary environments techniques generic evolutionary techniques specifically designed rl mind

limitations earl
although ea rl promising growing list successful applications outlined following section number challenges remain

online learning
distinguish two broad approaches reinforcement learning online learning
oine learning online learning agent learns directly experiences
operational environment example robot might learn navigate warehouse
actually moving physical environment two earl
situation first likely require large number experiences order
evaluate large population policies depending quickly agent performs tasks
environmental feedback may take unacceptable amount time
run hundreds generations ea evaluates hundreds thousands policies
second may dangerous expensive permit agent perform actions
actual operational environment might cause harm environment yet
likely least policies ea generates bad policies
objections apply td methods well example theoretical
prove optimality q learning require every state visited infinitely often
obviously impossible practice likewise td methods may explore
undesirable states acceptable value function found
td earl practical considerations point toward use oine learning
rl system performs exploration simulation environment
simulation provide number advantages earl including ability
perform parallel evaluations policies population simultaneously grefenstette


rare states
memory record observed states rewards differs greatly ea td
methods temporal difference methods normally maintain statistics concerning every stateaction pair states revisited reinforcement combined previous
value information thus supplements previous information information content agent reinforcement model increases exploration manner td
methods sustain knowledge good bad state action pairs
pointed previously ea methods normally maintain information good
policies policy components knowledge bad decisions explicitly preserved since
policies make decisions selected evolutionary
eventually eliminated population example refer table
shows implicit statistics population table note question


fimoriarty schultz grefenstette

marks states actions converged since policies population select
alternative action ea statistics impact actions fitness
reduction information content within evolving population disadvantage respect states rarely visited evolutionary value
genes real impact fitness individual tends drift random
values since mutations tend accumulate genes state rarely encountered
mutations may freely accumulate gene describes best action state
even evolutionary learns correct action rare state
information may eventually lost due mutations contrast since table td
methods permanently record information state action pairs may
robust learning agent encounter rare state course td method
uses function approximator neural network value function
suffer memory loss concerning rare states since many updates frequently
occurring states dominate updates rare states

proofs optimality

one attractive features td methods q learning proof
optimality watkins dayan however practical importance
limited since assumptions underlying proof e g hidden states state visited
infinitely often satisfied realistic applications current theory evolutionary
provide similar level optimality proofs restricted classes search spaces
vose wright however general theoretical tools available
applied realistic rl case ultimate convergence optimal policy
may less important practice eciently finding reasonable approximation
pragmatic may ask ecient alternative rl
terms number reinforcements received developing policy within
tolerance level optimal policy model probably approximately correct
pac learning valiant performance learner measured many
learning experiences e g samples supervised learning required converging
correct hypothesis within specified error bounds although developed initially
supervised learning pac extended recently td methods
fiechter general ea methods ros analytic methods
still early stage development along lines may one day
provide useful tools understanding theoretical practical advantages alternative
approaches rl time experimental studies provide valuable evidence
utility

examples earl methods

finally take look significant examples earl
rl rather attempt exhaustive survey selected four earl
systems representative diverse policies representations outlined section
samuel represents class single chromosome rule earl systems alecsys
example distributed rule earl method genitor single chromosome
neural net system sane distributed neural net system brief survey


fievolutionary reinforcement learning

provide starting point interested investigating evolutionary
reinforcement learning



samuel
samuel grefenstette et al earl system combines darwinian lamarckian evolution aspects temporal difference reinforcement learning samuel

used learn behaviors navigation collision avoidance tracking herding robots autonomous vehicles
samuel uses single chromosome rule representation policies
member population policy represented rule set gene rule
maps state world actions performed example rule might
range bearing set turn strength

use high level language rules offers several advantages low level binary
pattern languages typically adopted genetic learning systems first makes easier
incorporate existing knowledge whether acquired experts symbolic learning programs second easier transfer knowledge learned human operators samuel
includes mechanisms allow coevolution multiple behaviors simultaneously
addition usual genetic operators crossover mutation samuel uses traditional machine learning techniques form lamarckian operators samuel keeps
record recent experiences allow operators generalization specialization
covering deletion make informed changes individual genes rules
experiences
samuel used successfully many reinforcement learning applications
brie describe three examples learning complex behaviors real robots
applications samuel learning performed simulation ecting fact
initial phases learning controlling real system expensive
dangerous learned behaviors tested line system
schultz grefenstette schultz schultz grefenstette samuel
used learn collision avoidance local navigation behaviors nomad mobile
robot sensors available learning task five sonars five infrared sensors
range bearing goal current speed vehicle samuel
learned mapping sensors controllable actions turning rate
translation rate wheels samuel took human written rule set could reach
goal within limited time without hitting obstacle percent time
generations able obtain percent success rate
schultz grefenstette robot learned herd second robot pasture task learning system used range bearing second robot
heading second robot range bearing goal input sensors
system learned mapping sensors turning rate steering rate
experiments success measured percentage times robot could
maneuver second robot goal within limited amount time second robot
implemented random walk plus behavior made avoid nearby obstacles
first robot learned exploit achieve goal moving second robot goal


fimoriarty schultz grefenstette

samuel given initial human designed rule set performance percent
generations able move second robot goal percent
time
grefenstette samuel ea system combined case learning
address adaptation called anytime learning grefenstette
ramsey learning agent interacts external environment
internal simulation anytime learning involves two continuously running
interacting modules execution module learning module execution
module controls agent interaction environment includes monitor
dynamically modifies internal simulation model observations actual agent
environment learning module continuously tests strategies agent
simulation model genetic evolve improved strategies
updates knowledge base used execution module best available
whenever simulation model modified due observed change agent
environment genetic restarted modified model learning system
operates indefinitely execution system uses learning become
available work samuel shows ea method particularly well suited
anytime learning previously learned strategies treated cases indexed
set conditions learned situation encountered
nearest neighbor used similar previously learned cases
nearest neighbors used initialize genetic population policies case
grefenstette reports experiments mobile robot learns track another
robot dynamically adapts policies anytime learning encounters series
partial system failures blurs line online oine learning
since online system updated whenever oine learning system develops
improved policy fact oine learning system even executed board
operating mobile robot



alecsys

described previously alecsys dorigo colombetti distributed rule
ea supports design autonomous systems called behavioral engineering tasks performed complex autonomous systems
decomposed individual behaviors learned via learning classifier systems module shown figure decomposition performed human designer
fitness function associated lcs carefully designed ect role
associated component behavior within overall autonomous system furthermore
interactions among modules preprogrammed example designer may
decide robot learn goal except threatening predator
near case robot evade predator overall architecture
set behaviors set evasion behavior higher priority
goal seeking behavior individual lcs modules evolve decision rules
optimally performing subtasks
alecsys used develop behavioral rules number behaviors
autonomous robots including complex behavior groups chase feed escape


fievolutionary reinforcement learning

dorigo colombetti implemented tested
simulated robots real robots exploits human design earl
methods optimize system performance method shows much promise scaling
realistic tasks



genitor

genitor whitley kauth whitley aggressive general purpose genetic

shown effective specialized use reinforcement learning
whitley et al demonstrated genitor eciently evolve decision
policies represented neural networks limited reinforcement domain
genitor relies solely evolutionary adjust weights neural
networks solving rl member population genitor represents
neural network sequence connection weights weights concatenated realvalued chromosome along gene represents crossover probability crossover
gene determines whether network mutated randomly perturbed whether
crossover operation recombination another network performed crossover
gene modified passed offspring offspring performance compared
parent offspring outperforms parent crossover probability decreased
otherwise increased whitley et al refer technique adaptive mutation
tends increase mutation rate populations converge essentially method
promotes diversity within population encourage continual exploration solution
space
genitor uses called steady state genetic parents
selected genetic operators applied individual evaluated
contrasts generational gas entire population evaluated replaced
generation steady state ga policy evaluated retains
fitness value indefinitely since policies lower fitness likely
replaced possible fitness noisy evaluation function may
undesirable uence direction search case pole balancing rl
application fitness value depends length time policy maintain
good balance given randomly chosen initial state fitness therefore random
variable depends initial state authors believe noise fitness
function little negative impact learning good policies perhaps
dicult poor networks obtain good fitness good networks
many copies population survive occasional bad fitness evaluation
interesting general issue earl needs analysis
genitor adopts specific modification rl applications first representation uses real valued chromosome rather bit string representation weights
consequently genitor recombines policies weight definitions thus reducing potentially random disruption neural network weights might crossover
operations occurred middle weight definition second modification
high mutation rate helps maintain diversity promote rapid exploration
policy space finally genitor uses unusually small populations order discourage
different competing neural network species forming within population whit

fimoriarty schultz grefenstette

ley et al argue speciation leads competing conventions produces poor
offspring two dissimilar networks recombined
whitley et al compare genitor adaptive heuristic critic anderson
ahc uses td method reinforcement learning several different
versions common pole balancing benchmark task genitor found comparable ahc learning rate generalization one interesting difference
whitley et al found genitor consistent ahc solving
pole balancing failure signals occurs wider pole bounds make
much harder ahc preponderance failures appears cause states
overpredict failure contrast ea method appears effective finding policies
obtain better overall performance even success uncommon difference seems
ea tends ignore cases pole cannot balanced concentrate successful cases serves another example advantages associated
search policy space overall policy performance rather paying
much attention value associated individual states



sane

sane symbiotic adaptive neuro evolution system designed ecient method
building artificial neural networks rl domains possible generate
training data normal supervised learning moriarty miikkulainen
sane system uses evolutionary form hidden layer connections
weights neural network neural network forms direct mapping sensors
actions provides effective generalization state space sane method
credit assignment ea allows apply many
reinforcement sparse covers sequence decisions described previously sane
uses distributed representation policies
sane offers two important advantages reinforcement learning normally
present implementations neuro evolution first maintains diverse populations
unlike canonical function optimization ea converge population single solution sane forms solutions unconverged population several different types
neurons necessary build effective neural network inherent evolutionary
pressure develop neurons perform different functions thus maintain several different types individuals within population diversity allows recombination operators
crossover continue generate neural structures even prolonged evolution
feature helps ensure solution space explored eciently throughout
learning process sane therefore resilient suboptimal convergence
adaptive changes domain
second feature sane explicitly decomposes search complete solutions search partial solutions instead searching complete neural networks
solutions smaller good neurons evolved combined form effective full solution neural network words sane effectively
performs reduction search space neural networks
sane shown effective several different large scale one
sane evolved neural networks direct focus minimax game tree search moriarty


fievolutionary reinforcement learning

miikkulainen selecting moves evaluated given game
situation sane guides search away misinformation search tree towards
effective moves sane tested game tree search othello
evaluation function former world champion program bill lee mahajan
tested full width minimax search sane significantly improved play bill
examining subset board positions
second application sane used learn obstacle avoidance behaviors
robot arm moriarty miikkulainen b approaches learning robot arm
control learn hand eye coordination supervised training methods examples
correct behavior explicitly given unfortunately domains obstacles
arm must make several intermediate joint rotations reaching target generating
training examples extremely dicult reinforcement learning however
require examples correct behavior learn intermediate movements
general reinforcements sane implemented form neuro control networks capable
maneuvering oscar robot arm among obstacles reach random target locations
given camera visual infrared sensory input neural networks learned
effectively combine target reaching obstacle avoidance strategies
related examples evolutionary methods learning neural net control
systems robotics reader see cliff harvey husbands husbands
harvey cliff yamauchi beer

summary
article began suggesting two distinct approaches solving reinforcement learning
one search value function space one search policy space td
earl examples two complementary approaches approaches assume
limited knowledge underlying system learn experimenting different policies reinforcement alter policies neither requires precise
mathematical model domain may learn direct interactions
operational environment
unlike td methods earl methods generally base fitness overall performance
policy sense ea methods pay less attention individual decisions td
methods first glance appears make less ecient use
information may fact provide robust path toward learning good policies especially
situations sensors inadequate observe true state world
useful view path toward practical rl systems choice ea
td methods tried highlight strengths evolutionary
shown earl td complementary approaches
means mutually exclusive cited examples successful earl systems
samuel alecsys explicitly incorporate td elements multilevel credit assignment methods likely many practical applications depend
kinds multi strategy approaches machine learning
listed number areas need work particularly theoretical side rl would highly desirable better tools predicting
amount experience needed learning agent reaching specified level per

fimoriarty schultz grefenstette

formance existing proofs optimality q learning ea extremely
limited practical use predicting well perform realistic preliminary shown tools pac analysis applied
ea td methods much effort needed direction
many serious challenges remain scaling reinforcement learning methods realistic applications pointing shared goals concerns two complementary
approaches hope motivate collaboration progress field

references

anderson c w learning control inverted pendulum neural networks
ieee control systems magazine
barto g sutton r watkins c j c h learning sequential
decision making gabriel moore j w eds learning computational
neuroscience mit press cambridge
belew r k mcinerney j schraudolph n n evolving networks
genetic connectionist learning farmer j langton c
rasmussen taylor c eds artificial life ii reading addison wesley
chrisman l reinforcement learning perceptual aliasing perceptual
distinctions proceedings tenth national conference artificial
intelligence pp san jose ca
cliff harvey husbands p explorations evolutionary robotics adaptive
behavior
cobb h g grefenstette j j genetic tracking changing environments proc fifth international conference genetic pp
collins r j jefferson r selection massively parallel genetic
proceedings fourth international conference genetic pp
san mateo ca morgan kaufmann
dayan p sejnowski j exploration bonuses dual control machine
learning
de jong k analysis behavior class genetic adaptive systems
ph thesis university michigan ann arbor mi
dorigo colombetti robot shaping experiment behavioral engineering mit press cambridge
fiechter c n ecient reinforcement learning proceedings seventh
annual acm conference computational learning theory pp association
computing machinery
fogel l j owens j walsh j artificial intelligence simulated
evolution wiley publishing york


fievolutionary reinforcement learning

goldberg e genetic search optimization machine learning addison wesley reading
goldberg e richardson j genetic sharing multimodal
function optimization proceedings second international conference genetic pp san mateo ca morgan kaufmann
grefenstette j j optimization control parameters genetic ieee
transactions systems man cybernetics smc
grefenstette j j incorporating specific knowledge genetic
davis l ed genetic simulated annealing pp san mateo
ca morgan kaufmann
grefenstette j j credit assignment rule discovery system genetic
machine learning
grefenstette j j genetic changing environments manner r
manderick b eds parallel solving nature pp
grefenstette j j robot learning parallel genetic networked
computers proceedings summer computer simulation conference
scsc pp
grefenstette j j genetic learning adaptation autonomous robots robotics
manufacturing recent trends applications pp
asme press york
grefenstette j j proportional selection sampling handbook
evolutionary computation chap c iop publishing oxford university press
grefenstette j j b rank selection handbook evolutionary computation chap c iop publishing oxford university press
grefenstette j j ramsey c l anytime learning proc
ninth international conference machine learning pp san mateo ca
morgan kaufmann
grefenstette j j ramsey c l schultz c learning sequential decision
rules simulation competition machine learning
holland j h adaptation natural artificial systems introductory
analysis applications biology control artificial intelligence university
michigan press ann arbor mi
holland j h escaping brittleness possibilities general purpose learning
applied parallel rule systems machine learning artificial
intelligence vol morgan kaufmann los altos ca


fimoriarty schultz grefenstette

holland j h genetic classifier systems foundations future
directions proceedings second international conference genetic pp hillsdale jersey
holland j h reitman j cognitive systems adaptive
pattern directed inference systems academic press york
husbands p harvey cliff circle round state space attractors
evolved sighted robots robot autonomous systems
kaelbling l p littman l moore w reinforcement learning survey
journal artificial intelligence
koza j r genetic programming programming computers means
natural selection mit press cambridge
lee k f mahajan development world class othello program
artificial intelligence
lin l j mitchell memory approaches reinforcement learning nonmarkovian domains tech rep cmu cs carnegie mellon university school
computer science
mccallum k reinforcement learning selective perception hidden
state ph thesis university rochester
moriarty e miikkulainen r evolving neural networks focus minimax
search proceedings twelfth national conference artificial intelligence
aaai pp seattle wa mit press
moriarty e miikkulainen r ecient reinforcement learning
symbiotic evolution machine learning
moriarty e miikkulainen r b evolving obstacle avoidance behavior
robot arm animals animats proceedings fourth international
conference simulation adaptive behavior sab pp cape cod

moriarty e miikkulainen r forming neural networks ecient
adaptive co evolution evolutionary computation
potter design analysis computational model cooperative
coevolution ph thesis george mason university
potter de jong k evolving neural networks collaborative
species proceedings summer computer simulation conference ottawa
canada
potter de jong k grefenstette j coevolutionary
learning sequential decision rules eshelman l ed proceedings sixth
international conference genetic pittsburgh pa


fievolutionary reinforcement learning

rechenberg cybernetic solution path experimental library
translation royal aircraft establishment farnborough hants aug
ring b continual learning reinforcement environments ph thesis
university texas austin
ros j p probably approximately correct pac learning analysis handbook
evolutionary computation chap b iop publishing oxford university press
schaffer j caruana r eshelman l j das r study control
parameters affecting online performance genetic function optimization proceedings third international conference genetic
pp morgan kaufmann
schaffer j grefenstette j j multi objective learning via genetic
proceedings ninth international joint conference artificial intelligence
pp morgan kaufmann
schultz c learning robot behaviors genetic intelligent
automation soft computing trends development applications
pp tsi press albuquerque
schultz c grefenstette j j genetic learn behaviors
autonomous vehicles proceedings aiaa guidance navigation control
conference hilton head sc
schultz c grefenstette j j robo shepherd learning complex robotic behaviors robotics manufacturing recent trends applications
pp asme press york
smith f flexible learning solving heuristics adaptive search
proceedings eighth international joint conference artificial intelligence
pp morgan kaufmann
sutton r integrated architectures learning reacting
approximate dynamic programming machine learning proceedings seventh
international conference pp
sutton r learning predict methods temporal differences machine
learning
sutton r barto reinforcement learning introduction mit press
cambridge
valiant l g theory learnable communications acm

vose wright h simple genetic linear fitness evolutionary computation


fimoriarty schultz grefenstette

watkins c j c h learning delayed rewards ph thesis university
cambridge england
watkins c j c h dayan p q learning machine learning
whitley genitor selective pressure proceedings
third international conference genetic pp san mateo ca
morgan kaufman
whitley kauth j genitor different genetic proceedings
rocky mountain conference artificial intelligence pp denver co
whitley dominic das r anderson c w genetic reinforcement
learning neurocontrol machine learning
wilson w zcs zeroth level classifier system evolutionary computation

yamauchi b beer r sequential behavior learning evolved
dynamical neural networks adaptive behavior
zhou h csm computational model cumulative learning machine learning





