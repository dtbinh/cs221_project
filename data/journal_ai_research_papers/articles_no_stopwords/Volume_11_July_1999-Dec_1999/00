journal artificial intelligence

submitted published

decision theoretic structural assumptions
computational leverage
craig boutilier

cebly cs ubc ca

department computer science university british columbia
vancouver bc v z canada

thomas dean

tld cs brown edu

department computer science brown university
box providence ri usa

steve hanks

hanks cs washington edu

department computer science engineering university washington
seattle wa usa

abstract

uncertainty central study automated sequential
decision making addressed researchers many different fields including
ai decision analysis operations control theory economics
assumptions perspectives adopted areas often differ substantial ways
many interest researchers fields modeled markov
decision processes mdps analyzed techniques decision theory
presents overview synthesis mdp related methods showing
provide unifying framework modeling many classes studied
ai describes structural properties mdps exhibited particular classes exploited construction optimal approximately
optimal policies plans commonly possess structure reward
value functions used describe performance criteria functions used describe
state transitions observations relationships among features used describe
states actions rewards observations
specialized representations employing representations achieve
computational leverage exploiting forms structure certain ai techniques
particular use structured intensional representations
viewed way surveys several types representations classical
decision theoretic exploit representations number different ways ease computational burden constructing
policies plans focuses primarily abstraction aggregation decomposition techniques ai style representations

introduction

decision theoretic notions represent domain uncertainty plan quality
recently drawn considerable attention artificial intelligence ai decision theoretic
dtp attractive extension classical ai paradigm
allows one model actions uncertain effects decision maker
see example recent texts dean allen aloimonos dean wellman russell
norvig reported hanks russell wellman

c ai access foundation morgan kaufmann publishers rights reserved

fiboutilier dean hanks

incomplete information world factors resource consumption lead
solutions varying quality may absolute well defined goal
state roughly aim dtp form courses action plans policies
high expected utility rather plans guaranteed achieve certain goals
ai viewed particular solving sequential decision
type connections dtp used fields
decision analysis economics operations become apparent
conceptual level sequential decision viewed instances markov
decision processes mdps use mdp framework make connections
explicit
much recent dtp explicitly adopted mdp framework underlying model barto bradtke singh boutilier dearden boutilier dearden
goldszmidt dean kaelbling kirman nicholson koenig simmons
koenig tash russell allowing adaptation existing solving mdps e g field applied
however work departed traditional definition
ai community one goal make explicit
connection two lines work
adopting mdp framework model posing solving
illuminated number interesting connections among techniques solving decision
drawing work ai reasoning uncertainty decision analysis
one interesting insights emerge body work many
dtp exhibit considerable structure thus solved special purpose
methods recognize exploit structure particular use feature
representations describe typical practice ai often highlights
special structure allows exploited computationally little effort
two general impediments widespread acceptance mdps within
ai general model first absence explanations mdp model
make connections current explicit conceptual
computational level may due large part fact mdps
developed studied primarily dominant concerns naturally rather
different one aim make connections clear provide brief
description mdps conceptual model emphasizes connection
ai explore relationship mdp solution ai
particular emphasize ai
viewed special cases mdps classical designed
exploit characteristics associated cases
second impediment skepticism among ai researchers regarding computational
adequacy mdps model techniques scale solve
reasonable size one diculty solution techniques mdps tendency rely
explicit state formulations problematic ai
since state spaces grow exponentially number features state space size
dimensionality somewhat lesser concern decision analysis
fields operations researcher decision analyst often hand craft model ignores
certain features deemed irrelevant define features summarize


fidecision theoretic structural assumptions

wide class states ai emphasis automatic solution
posed users lack expertise decision analyst thus assuming well crafted
compact state space often appropriate
specialized representations ai
solving used design ecient mdp solution techniques particular
ai methods assume certain structure state space actions
operators specification goal success criteria representations
designed make structure explicit exploit
structure solve effectively demonstrate process identifying
structure making explicit exploiting algorithmically brought bear
solution mdps
several objectives first provides overview dtp mdps
suitable readers familiar traditional ai methods makes connections
work second describes types structure exploited
ai representations methods facilitate computationally effective mdps
suitable introduction ai methods familiar classical
presentation mdps finally surveys recent work use mdps ai
suggests directions regard therefore interest
researchers dtp

general definition
roughly speaking class consider involving systems whose
dynamics modeled stochastic processes actions decision maker
referred agent uence system behavior system current
state choice action jointly determine probability distribution system
possible next states agent prefers certain system states e g goal states
others therefore must determine course action called plan policy
likely lead target states possibly avoiding undesirable states
along way agent may know system state exactly making decision
act however may rely incomplete noisy sensors forced
base choice action probabilistic estimate state
help illustrate types interested consider following
example imagine robot agent designed help someone user
oce environment see figure three activities might undertake picking
user mail getting coffee tidying user lab robot move
location location perform actions tend achieve certain target
states e g bringing coffee user demand maintaining minimal level tidiness
lab
might associate certain level uncertainty effects robot actions
e g tries move adjacent location might succeed time fail
move time robot might incomplete access
true state system sensors might supply incomplete information
cannot tell whether mail available pickup mail room incorrect


fiboutilier dean hanks

office

hallway

lab
mailroom

coffee

figure decision theoretic
information even mail room sensors occasionally fail detect presence
mail
finally performance robot might measured ways actions
guarantee goal achieved maximize objective function defined
possible effects actions achieve goal state sucient probability avoiding disastrous states near certainty stipulation optimal
acceptable behavior important part specification
types captured general framework include classical goal oriented deterministic complete knowledge extensions
conditional probabilistic well general
formulations
discussion point assumed extensional representation system
states one state explicitly named ai intensional representations common intensional representation one states sets
states described sets multi valued features choice appropriate set
features important part design features might include
current location robot presence absence mail performance
metric typically expressed intensionally figure serves reference example use throughout lists basic features used describe
states system actions available robot exogenous events
might occur together intuitive description features actions events
remainder organized follows section present mdp
framework abstract introducing basic concepts terminology noting relationship abstract model classical ai section surveys common solution techniques dynamic programming general
mdp search points relationship assumptions solution techniques section turns
representations showing ways structured representations commonly
used ai used represent mdps compactly well section surveys


fidecision theoretic structural assumptions

features
location

denoted
description
loc etc location robot five possible locations mailroom coffee room
c user oce hallway h laboratory l
tidiness
etc
degree lab tidiness five possible values messiest
tidiest
mail present

mail user mail box true false
robot mail
rhm rhm robot mail possession
coffee request
cr cr
outstanding unfulfilled request coffee user
robot coffee rhc rhc robot coffee possession
actions
denoted
description
move clockwise
clk
move adjacent location clockwise direction
counterclockwise cclk
move adjacent location counterclockwise direction
tidy lab
tidy
robot lab degree tidiness increased
pickup mail
pum
robot mailroom mail present robot
takes mail rhm becomes true becomes false
get coffee
getc
robot coffee room gets coffee rhc becomes true
deliver mail
delm
robot oce mail hands mail user
rhm becomes false
deliver coffee
delc
robot oce coffee hands coffee
user rhc cr become false
events
denoted
description
mail arrival
arrm
mail arrives causing become true
request coffee
reqc
user issues coffee request causing cr become true
untidy lab
mess
lab becomes messier one degree less tidy

figure elements robot domain
recent work abstraction aggregation decomposition methods
shows connection traditional ai methods goal regression last
section demonstrates representational computational methods ai
used solution general mdps section points additional ways
type computational leverage might developed future

markov decision processes basic formulation
section introduce mdp framework make explicit relationship
model classical ai interested controlling stochastic
dynamical system system point time one number distinct
states system state changes time response events action
particular kind event instigated agent order change system state
assume agent control actions taken though
effects taking action might perfectly predictable contrast exogenous events
agent control occurrence may partially predictable
abstract view agent consistent ai view agent
autonomous decision maker control view policy determined ahead
time programmed device executed without deliberation


fiboutilier dean hanks

states state transitions

define state description system particular point time one
defines states vary particular applications notions natural
others however common assume state captures information relevant
agent decision making process assume finite state space fs sn g
possible system states cases agent complete information
current state uncertainty incomplete information captured
probability distribution states
discrete time stochastic dynamical system consists state space probability
distributions governing possible state transitions next state system depends
past states distributions constitute model system evolves time
response actions exogenous events ecting fact effects actions
events may perfectly predictable even prevailing state known
although generally concerned agent chooses appropriate course
action remainder section assume agent course action
fixed concentrating predicting system state occurrence
predetermined sequence actions discuss action selection next
section
assume system evolves stages occurrence event marks
transition one stage next stage since events define changes stage
since events often necessarily cause state transitions often equate stage
transitions state transitions course possible event occur leave
system state
system progression stages roughly analogous passage time
two identical assume action possibly op taken
stage every action takes unit time complete thus speak loosely
stages correspond units time refer interchangeably set stages
set time points
model uncertainty regarding system state stage random
variable takes values assumption forward causality requires
variable depend directly value future variable k k roughly
requires model system past history directly determines
distribution current states whereas knowledge future states uence
estimate current state indirectly providing evidence current state
may lead future states figure shows graphical perspective
discrete time stochastic dynamical system nodes random variables denoting
state particular time arcs indicate direct probabilistic dependence
states previous states describe system completely must supply
conditional distributions pr js times
states thought descriptions system modeled question arises much detail system captured state description
discussion applies cases state space countably infinite see
puterman discussion infinite continuous state
deal topics considerable literature community
continuous time markov decision processes puterman



fidecision theoretic structural assumptions









b







c





































figure general stochastic process markov chain b stationary markov
chain c
detail implies information system turn often translates better
predictions future behavior course detail implies larger set
increase computational cost decision making
commonly assumed state contains enough information predict next
state words information history system relevant predicting
future captured explicitly state formally assumption markov
assumption says knowledge present state renders information past
irrelevant making predictions future
pr js pr js
markovian represented graphically structure figure b
ecting fact present state sucient predict future state evolution
finally common assume effects event depend prevailing
state stage time event occurs distribution predicting
next state regardless stage model said stationary
represented schematically two stages figure c case
single conditional distribution required generally restrict attention
discrete time finite state stochastic dynamical systems markov property commonly called markov chains furthermore discussion restricted stationary
chains
complete model must provide probability distribution initial states
ecting probability state stage distribution repre worth mentioning markov property applies particular model system
indeed non markovian model system finite order e whose dynamics depend
k previous states k converted equivalent though larger markov model
control theory called conversion state form luenberger
course statement model detail saying state carries enough information
make stage irrelevant predicting transitions



fiboutilier dean hanks






































figure state transition diagram
sented real valued row vector size n js j one entry state denote
vector p use p denote ith entry probability starting state
si
represent stage nonstationary markov chain transition matrices
size n n matrix p captures transition probabilities governing
system moves stage stage matrix consists probabilities ptij
ptij pr sj js si process stationary transition matrix
stages one matrix whose entries denoted pij suce given
initial
states p probability distribution states n stages
q pdistribution

n
stationary markov process represented state transition diagram
figure nodes correspond particular states stage represented
explicitly arcs denote possible transitions non zero probability labeled
transition probabilities pij pr sj js si arc node node
j labeled pij pij size diagram least n
n depending number arcs useful representation transition
graph relatively sparse example states immediate transitions
neighbors

example illustrate notions imagine robot figure executing

policy moving counterclockwise repeatedly restrict attention two
variables location loc presence mail giving state space size
suppose robot moves adjacent location probability
addition mail arrive mailroom probability time independent robot location causing variable become true
becomes true robot cannot move state false since action
moving uence presence mail state transition diagram
example illustrated figure transition matrix shown

structure markov chain occasionally interest us subset

c closed pij c j c proper closed set proper
subset c enjoys property sometimes refer proper closed sets recurrent
classes states closed set consists single state state called
absorbing state agent enters closed set absorbing state remains
important note nodes represent random variables earlier figures



fidecision theoretic structural assumptions






om






om



hm

lm











cm

mm

mm





hm
























lm





cm



























































































































figure state transition diagram transition matrix moving robot
forever probability example figure set states
holds forms recurrent class absorbing states example
program robot stay put whenever state hm loc would
absorbing state altered chain finally say state transient
belong recurrent class figure state holds transient eventually
probability agent leaves state never returns since way
remove mail arrives

actions
markov chains used describe evolution stochastic system
capture fact agent choose perform actions alter state
system key element mdps set actions available decision maker
action performed particular state state changes stochastically response
action assume agent takes action stage process
system changes state accordingly
stage process state agent available set actions
ats called feasible set stage describe effects ats must
supply state transition distribution pr js actions states
stages unlike case markov chain terms pr js
true conditional distributions rather family distributions parameterized
since probability part model retain notation however
suggestive nature
often assume feasible set actions stages states
case set actions fa ak g executed time
contrasts ai practice assigning preconditions actions defining
states meaningfully executed model takes view
action executed attempted state action effect
executed state execution leads disastrous effects noted
action transition matrix action preconditions often computational convenience
rather representational necessity make process ecient
identifying states planner even consider selecting action
preconditions represented mdps relaxing assumption set


fiboutilier dean hanks









































































































































om





lm














cm

mm

mm











hm








hm



lm

om



cm



figure transition matrix clk induced transition diagram two action
policy
feasible actions states illustrate concepts however
sometimes assume actions preconditions
restrict attention stationary processes case means
effects action depends state stage transition
matrices thus take form pkij pr sj js si ak capturing probability
system moves state sj ak executed state si stationary
action fully described single n n transition matrix p k important note
transition matrix action includes direct effects executing
action effects exogenous events might occur stage

example example figure extended agent two available

actions moving clockwise moving counterclockwise transition matrix
cclk assumption mail arrives probability shown figure
matrix clk appears left figure suppose agent fixes behavior
moves clockwise locations c counterclockwise locations h
l address agent might come know location
actually implement behavior defines markov chain illustrated
transition diagram right figure

exogenous events

exogenous events events stochastically cause state transitions much
actions beyond control decision maker might correspond
evolution natural process action another agent notice effect
action cclk figure combines effects robot action
exogenous event mail arrival state transition probabilities incorporate motion
robot causing change location possible change mail status due
mail arrival purposes decision making precisely combined effect
possible assess effects actions exogenous events separately combine
single transition matrix certain cases boutilier puterman discuss later
section



fidecision theoretic structural assumptions

important predicting distribution possible states resulting
action taken call actions implicit event since effects
exogenous event folded transition probabilities associated action
however often natural view transitions comprised two separate
events effect state generally often think transitions
determined effects agent chosen action certain exogenous
events beyond agent control may occur certain probability
effects actions decomposed fashion call action model
explicit event model
specifying transition function action zero exogenous events
generally easy actions events interact complex ways instance consider
specifying effect action pum pickup mail state mail present
possibility simultaneous mail arrival e unit discrete
time event arrm occurs robot obtain newly arrived mail
mail remain mailbox intuitively depends whether mail arrived
pickup completed albeit within time quantum state transition
case viewed composition two transitions precise description
composition depends ordering agent action exogenous event
mail arrives first transition might state mail
waiting state mail waiting robot holding mail
pickup action completed first transition would e pum
effect mail arrives remains box
picture complicated actions events truly occur simultaneously
interval case resulting transition need composition
individual transitions example robot lifts side table glass
water situated water spill similarly exogenous event causes side
raised action event occur simultaneously qualitatively
different water spilled thus interleaving semantics described
appropriate
complications modeling exogenous events combination
actions events approached many ways depending modeling assumptions one willing make generally specify three types information first
provide transition probabilities actions events assumption
occur isolation standard transition matrices transition matrix
figure decomposed two matrices shown figure one clk one
arrm second exogenous event must specify probability occurrence
since vary state generally require vector length n indicating
probability occurrence state occurrence vector arrm would

fact individual matrices deterministic artifact example general
actions events represented genuinely stochastic matrices



fiboutilier dean hanks
























































































































































action clk


































































































































event arrm

figure transition matrices action exogenous event explicit event
model
assume illustration mail arrives none present final
requirement combination function describes compose transitions
action subset event transitions indicated complex
sometimes almost unrelated individual action event transitions however
certain assumptions combination functions specified reasonably concisely
one way modeling composition transitions assume interleaving semantics type alluded case one needs specify probability
action events take place occur specific order instance one might
assume event occurs time within discrete time unit according
continuous distribution e g exponential distribution given rate information probability particular ordering transitions given certain events
occur computed resulting distribution possible next states
example probability composed transitions
would given probabilities mail arrived first last respectively
certain cases probability ordering needed illustrate another
combination function assume action occurs exogenous events
furthermore assume events commutative initial state pair
events e e distribution applying event sequence e e
identical obtained sequence e e b occurrence probabilities
intermediate states identical intuitively set events domain arrm reqc
mess property conditions combined transition distribution
action computed considering probability subset events
applying subset order distribution associated
generally construct implicit event model components
explicit event model thus natural specification converted form usually
used mdp solution two assumptions instance
form implicit event transition matrix pr si sj action given matrix
pcra si sj assumes event occurrences matrices pre si sj events
e occurrence vector pre si event e effective transition matrix
probability different events may correlated possibly particular states case
necessary specify occurrence probabilities subsets events treat event occurrence
probabilities independent ease exposition



fidecision theoretic structural assumptions

event e defined follows
pcre si sj pre si pre si sj



pre si j
j

equation captures event transition probabilities probability event occurrence factored let e e denote diagonal matrices entries ekk pre sk
pre sk pr
c e si sj e pre e assumptions
ekk
implicit event matrix pr si sj action given pr pcre pcren pra
ordering n possible events
naturally different procedures constructing implicit event matrices required
given different assumptions action event interaction whether implicit constructed specified directly without explicit mention exogenous events
assume unless stated otherwise action transition matrices take account
effects exogenous events well thus represent agent best information
happen takes particular action

observations

although effects action depend aspect prevailing state
choice action depend agent observe current state
remember prior observations model agent observational sensing
capabilities introducing finite set observations fo oh g agent receives
observation set stage prior choosing action stage
model observation random variable ot whose value taken
probability particular ot generated depend

state system
action taken
state system taking action effects
exogenous events realized action taken
let pr ot oh js si ak sj probability agent observes

oh stage given performs ak state si ends state sj actions

assume observational distributions stationary independent stage

phi j k pr ohjsi ak sj denote quantity view probabilistic dependencies

among state action observation variables graph time indexed variables
shown nodes one variable directly probabilistically dependent another
edge latter former see figure
model allows wide variety assumptions agent sensing capabilities
one extreme fully observable mdps fomdps agent knows exactly
state stage model case letting setting
pr oh jsi ak sj




iff oh sj
otherwise

fiboutilier dean hanks

















figure graph showing dependency relationships among states actions observations different times
example means robot knows exact location whether
mail waiting mailbox even mailroom mail arrives
agent thus receives perfect feedback actions effects
exogenous events noisy effectors complete noise free instantaneous
sensors recent ai adopts mdp framework explicitly assumes full
observability
extreme might consider non observable systems nomdps
agent receives information system state execution model
case letting fog observation reported stage revealing
information state pr sj jsi ak pr sj jsi ak open loop
systems agent receives useful feedback actions agent
noisy effectors sensors case agent chooses actions according plan
consisting sequence actions executed unconditionally effect agent relying
predictive model determine good action choices execution time
traditionally ai work implicitly made assumption non observability
often coupled omniscience assumption agent knows initial state
certainty predict effects actions perfectly precisely predict occurrence exogenous events effects circumstances agent
predict exact outcome plan thus obviating need observation
agent build straight line plan sequence actions performed without
feedback good plan whose execution might depend information gathered execution time
two extremes special cases general observation model described
allows agent receive incomplete noisy information system state
e partially observable mdps pomdps example robot might able
determine location exactly might able determine whether mail arrives
unless mailroom furthermore mail sensors might occasionally report
inaccurately leading incorrect belief whether mail waiting

example suppose robot checkmail action change system

state generates observation uenced presence mail provided


fidecision theoretic structural assumptions

loc
loc
loc
loc

pr obs mail pr obs nomail









figure observation probabilities checking mailbox
robot mailroom time action performed robot
mailroom sensor reports mail noisy checkmail sensor
described probability distribution one shown figure
view error probabilities probability false positives false
negatives

system trajectories observable histories

use terms trajectory history interchangeably describe system behavior
course solving episode perhaps initial segment thereof
complete system history sequence states actions observations generated
stage time point interest finite infinite length complete
histories represented possibly infinite sequence tuples form

hhs hs hs ot ii
define two alternative notions history contain less complete information
arbitrary stage define observable history sequence

hho hot ii
observation initial state observable history stage comprises
information available agent history chooses action stage
third type trajectory system trajectory sequence

hhs hs
describing system behavior objective terms independent agent particular
view system
evaluating agent performance generally interested system
trajectory agent policy must defined terms observable history since
agent access system trajectory except fully observable case
two equivalent

reward value

facing decision maker select action performed stage
decision making decision basis observable history
agent still needs way judge quality course action done defining


fiboutilier dean hanks













c



r



figure decision process rewards action costs
value function v function mapping set system histories hs reals
v hs r agent prefers system history h h case v h v h
thus agent judges behavior good bad depending effect
underlying system trajectory generally agent cannot predict certainty
system trajectory occur best generate probability distribution
possible trajectories caused actions case computes expected value
candidate course action chooses policy maximizes quantity
system dynamics specifying value function arbitrary trajectories
cumbersome unintuitive therefore important identify structure
value function lead parsimonious representation
two assumptions value functions commonly made mdp literature
time separability additivity time separable value function defined terms
primitive functions applied component states actions reward
function r r associates reward state costs assigned
taking actions defining cost function c r associates cost
performing action state rewards added value function costs
subtracted
value function time separable simple combination rewards costs
accrued stage simple combination means value taken function
costs rewards stage costs rewards depend stage
function combines must independent stage commonly linear
combination product value function additive combination function
sum reward cost function values accrued history stages addition
rewards action costs system time separable value viewed graphically
shown figure
assumption time separability restrictive example might
certain goals involving temporal deadlines workplace tidy soon possible
tomorrow morning maintenance allow mail sit mailroom
technically set histories interest depends horizon chosen described
term reward somewhat misnomer reward could negative case
penalty might better word likewise costs positive punitive negative beneficial thus admit great exibility defining value functions
see luenberger precise definition time separability



fidecision theoretic structural assumptions

undelivered minutes require value functions non separable
given current representation state note however separability
markov property property particular representation could add additional
information state example clock time interval time
time tidiness achieved length time mail sits mail room
robot picks additional information could reestablish time separable value function expense increase number
states ad hoc cumbersome action representation

horizons success criteria

order evaluate particular course action need specify long
many stages executed known horizon finite horizon
agent performance evaluated fixed finite number stages
commonly aim maximize total expected reward associated course
action therefore define finite horizon value length history h bellman


v h

tx



fr st c st g r st

infinite horizon hand requires agent performance
evaluated infinite trajectory case total reward may unbounded
meaning policy could arbitrarily good bad executed long enough
case may necessary adopt different means evaluating trajectory
common introduce discount factor ensuring rewards costs accrued
later stages counted less accrued earlier stages value function
expected total discounted reward defined follows bellman howard


x
v h r st c st


fixed discount rate formulation particularly simple
elegant way ensure bounded measure value infinite horizon though
important verify discounting fact appropriate economic justifications often
provided discounted reward earned sooner worth one earned
later provided reward somehow invested discounting suitable
modeling process terminates probability point time e g
robot break case discounted correspond expected total
reward finite uncertain horizon reasons discounting sometimes used
finite horizon well
another technique dealing infinite horizon evaluate trajectory
average reward accrued per stage gain gain history defined
n
x
g h lim fr st c st g
n n

see bacchus boutilier grove however systematic handling certain
types history dependent reward functions



fiboutilier dean hanks

refinements criterion proposed puterman
sometimes ensures total reward infinite trajectory
bounded thus expected total reward criterion well defined consider case
common ai planners agent task bring system goal state
positive reward received goal reached actions incur non negative
cost goal reached system enters absorbing state
rewards costs accrued long goal reached certainty
situation formulated infinite horizon total reward bounded
desired trajectory bertsekas puterman general
cannot formulated fixed finite horizon unless priori bound
number steps needed reach goal established sometimes
called indefinite horizon practical point view agent continue
execute actions finite number stages exact number cannot determined
ahead time

solution criteria

complete definition need specify constitutes
solution see split explicit mdp formulations
work ai community classical mdp generally stated
optimization given value function horizon evaluation metric e g
expected total reward expected total discounted reward expected average reward per stage
agent seeks behavioral policy maximizes objective function
work ai often seeks satisficing solutions literature
generally taken plan satisfies goal equally preferred
plan satisfies goal plan satisfies goal preferable
plan probabilistic framework might seek plan satisfies
goal maximum probability optimization lead situations
optimal plan infinite length system state fully observable
satisficing alternative kushmerick hanks weld seek plan satisfies
goal probability exceeding given threshold

example extend running example demonstrate infinite horizon fully
observable discounted reward situation begin adding one dimension
state description boolean variable rhm robot mail giving us
system states provide agent two additional actions pum
pickup mail delm deliver mail described figure reward
agent way mail delivery encouraged associate reward
state rhm false states
actions cost agent gets total reward six stage system
trajectory
hloc rhmi stay hloc rhmi pum hloc rhmi
clk hloc h rhmi clk hloc rhmi delm hloc rhmi

though see haddawy hanks williamson hanks restatement
optimization



fidecision theoretic structural assumptions

assign action cost action except stay cost
total reward becomes use discount rate discount future
rewards costs initial segment infinite horizon history would contribute
total
value trajectory subsequently extended furthermore establish
bound total expected value trajectory best case subsequent
stages yield reward expected total discounted reward bounded


x



similar effect behavior achieved penalizing states e negative
rewards rhm true

policies

mentioned policies courses action plans informally point
provide precise definition decision facing agent viewed
generally deciding action perform given current observable history
define policy mapping set observable histories ho actions
ho intuitively agent executes action

hho hot ot
stage performed actions made observations ot
earlier stages made observation ot current stage
policy induces distribution pr hj set system histories hs probability distribution depends initial distribution p define expected value
policy
x
ev
v h pr hj
h hs

would agent adopt policy maximizes expected value
satisficing context acceptably high expected value
general form policy depending arbitrary observation history
lead complicated policies policy construction special cases
however assumptions observability structure value function
optimal policies much simpler form
case fully observable mdp time separable value function optimal
action stage computed information current state
stage restrict policies simpler form without
danger acting suboptimally due fact full observability allows state
observed completely markov assumption renders prior history irrelevant
non observable case observational history contains vacuous observations
agent must choose actions knowledge previous actions
stage however since incorporates previous actions takes form


fiboutilier dean hanks

form policy corresponds linear unconditional sequence actions ha
straight line plan ai nomenclature

model summary assumptions computational
complexity

concludes exposition mdp model uncertainty generality allows us capture wide variety classes currently
studied literature section review basic components model
describe commonly studied dtp literature respect model
summarize known complexity section describe specialized computational techniques used solve classes
model summary assumptions

mdp model consists following components
state space finite countable set states generally make markov
assumption requires state convey information necessary predict
effects actions events independent information
system history
set actions action ak represented transition matrix size
js jjs j representing probability pkij performing action ak state si move
system state sj assume throughout action model stationary
meaning transition probabilities vary time transition matrix
action generally assumed account exogenous events might
occur stage action executed
set observation variables set messages sent agent
action performed provide execution time information current
system state action ak pair states si sj pkij
associate distribution possible observations pkm
ij denotes probability
obtaining observation om given action ak taken si resulted
transition state sj
value function v value function maps state history real number
v h v h case agent considers history h least good
h state history records progression states system assumes along
actions performed assumptions time separability additivity
common v particular generally use reward function r cost function
c defining value
horizon number stages state histories
evaluated v
many ai literature produce partially ordered sequence actions plans
however involve conditional nondeterministic execution rather represent fact
linear sequence consistent partial order solve thus partially ordered
plan concise representation particular set straight line plans



fidecision theoretic structural assumptions

optimality criterion provides criterion evaluating potential solutions


common

use general framework classify commonly studied
decision making literature case note modeling assumptions define class

decision sciences tradition
fully observable markov decision processes fomdps ex

tremely large body studying fomdps present basic algorithmic techniques detail next section commonly used formulation fomdps assumes full observability stationarity uses optimality
criterion maximization expected total reward finite horizon maximization expected total discounted reward infinite horizon minimization
expected cost goal state
fomdps introduced bellman studied depth
fields decision analysis including seminal work howard recent texts fomdps include bertsekas puterman average reward optimality received attention literature blackwell howard
puterman ai literature discounted total reward
popular well barto et al dearden boutilier dean kaelbling kirman nicholson koenig though average reward criterion
proposed suitable modeling ai boutilier
puterman mahadevan schwartz

partially observable markov decision processes pomdps pomdps

closer fomdps general model decision processes described
pomdps generally studied assumption stationarity optimality criteria identical fomdps though average reward criterion
widely considered discuss pomdp viewed
fomdp state space consisting set probability distributions
probability distributions represent states belief agent observe
state belief system although exact knowledge
system state
pomdps widely studied control theory astrom lovejoy b smallwood sondik sondik drawn increasing
attention ai circles cassandra kaelbling littman hauskrecht
littman parr russell simmons koenig thrun fox burgard zhang liu uence diagrams howard matheson
shachter popular model decision making ai fact
structured representational method pomdps see section

ai tradition


fiboutilier dean hanks

classical deterministic classical ai model assumes

deterministic actions action ak taken state si one successor sj
important assumptions non observability value determined
reaching goal state plan leads goal state preferred
often preference shorter plans represented
discount factor encourage faster goal achievement assigning
cost actions reward associated transitions goal states
absorbing action costs typically ignored except noted
classical usually assumed initial state known certainty
contrasts general specification mdps assume
knowledge even distributional information initial state policies
defined applicable matter state distribution states one finds
oneself action choices defined every possible state history knowledge
initial state determinism allow optimal straight line plans constructed
loss value associated non observability unpredictable exogenous
events uncertain action effects cannot modeled consistently assumptions adopted
overview early classical variety approaches
adopted see allen hendler tate well yang recent text

optimal deterministic separate body work retains classical

assumptions complete information determinism tries recast
optimization relaxes implicit assumption achieve goal
costs time methods use sort representations
applied satisficing
haddawy hanks present multi attribute utility model planners
keeps explicit information initial state goals allows preferences stated partial satisfaction goals well cost
resources consumed satisfying model allows expression preferences phenomena temporal deadlines maintenance intervals
dicult capture time separable additive value function williamson
see williamson hanks implements model extending classical solve resulting optimization haddawy
suwandi implement model complete decision theoretic framework
model refinement differs somewhat generative
model discussed model set possible plans pre stored
abstraction hierarchy solver job hierarchy
optimal choice concrete actions particular
perez carbonell work incorporates cost information classical
framework maintains split classical satisficing planner
additional cost information provided utility model cost information
used learn search control rules allow classical planner generate low cost
goal satisfying plans


fidecision theoretic structural assumptions

conditional deterministic classical assumption

omniscience relaxed somewhat allowing state aspects
world unknown agent thus situation certain
system one particular set states know one unknown
truth values included initial state specification taking actions
cause proposition become unknown well
actions provide agent information plan executed conditional planners introduce idea actions providing runtime information
prevailing state distinguishing action makes proposition p true
action tell agent whether p true action executed
action causal informational effects simultaneously changing
world reporting value one propositions second sort
information useful time except allows steps plan
executed conditionally depending runtime information provided prior
information producing steps value actions lies fact different
courses action may appropriate different conditions informational
effects allow runtime selection actions observations produced much
general pomdp model
examples conditional planners classical framework include early work
warren recent cnlp peot smith cassandra pryor
collins plynth goldman boddy uwl etzioni hanks
weld draper lesh williamson systems

probabilistic without feedback direct probabilistic extension

classical stated follows kushmerick et al
take input probability distribution initial states b stochastic actions
explicit implicit transition matrices c set goal states probability
success threshold objective produce plan reaches goal state
probability least given initial state distribution provision made
execution time observation thus straight line plans form policy
possible restricted case infinite horizon nomdp one
actions incur cost goal states offer positive reward absorbing
special case objective satisficing policy rather
optimal one

probabilistic feedback draper et al proposed

extension probabilistic actions provide feedback
exactly observation model described section
posed building plan leaves system goal state sucient
probability plan longer simple sequence actions contain conditionals loops whose execution depends observations generated sensing
actions restricted case general pomdp absorbing goal states cost free actions used objective policy
conditional plan leaves system goal state sucient probability


fiboutilier dean hanks

comparing frameworks task oriented versus process oriented

useful point pause contrast types considered classical literature typically studied within mdp framework although
ai literature emphasized goal pursuit one shot view
solving cases viewing infinite horizon decision
satisfying formulation consider running example involving oce
robot simply possible model responding coffee requests mail
arrival keeping lab tidy strict goal satisfaction capturing
possible nuances intuitively optimal behavior
primary diculty explicit persistent goal states exist
simply require robot attain state lab tidy mail awaits
unfilled coffee requests exist successful plan could anticipate possible system behavior
goal state reached possible occurrence exogenous events goal
achievement requires robot bias methods achieving goals way
best suits expected course subsequent events instance coffee requests
likely point time unmet requests highly penalized robot
situate coffee room order satisfy anticipated future request quickly
realistic decision scenarios involve task oriented process oriented behavior
formulations take account provide satisfying
wider range situations
complexity policy construction

defined several different ways different
set assumptions state space system dynamics actions deterministic
stochastic observability full partial none value function time separable goal
goal rewards action costs partially satisfiable goals temporal deadlines
horizon finite infinite indefinite optimality criterion optimal satisficing solutions set assumptions puts corresponding particular complexity
class defines worst case time space bounds representation
solving summarize known complexity
classes defined
fully observable markov decision processes fully observable mdps fomdps
time separable additive value functions solved time polynomial size
state space number actions size inputs common solving fomdps value iteration policy iteration
described next section finite horizon discounted infinite horizon
require polynomial amount computation per iteration js j jaj js j jaj js j
respectively converge polynomial number iterations factor
discounted case hand shown p complete
papadimitriou tsitsiklis means ecient parallel solution
unlikely space required store policy n stage finite horizon
precisely maximum number bits required represent transition probabilities
costs
see littman dean kaelbling summary complexity



fidecision theoretic structural assumptions

js jn interesting classes infinite horizon specifically involving discounted time separable additive reward optimal policy
shown stationary policy stored js j space
bear mind worst case bounds many cases better time bounds
compact representations found sections explore ways represent
solve eciently
partially observable markov decision processes pomdps notorious
computational diculty mentioned pomdp viewed fomdp
infinite state space consisting probability distributions distribution
representing agent state belief point time astrom smallwood
sondik finding optimal policy pomdp objective
maximizing expected total reward expected total discounted reward finite
horizon shown exponentially hard js j papadimitriou
tsitsiklis finding policy maximizes approximately
maximizes expected discounted total reward infinite horizon shown
undecidable madani condon hanks
even restricted cases pomdp computationally dicult worst
case littman considers special case boolean rewards determining whether
infinite horizon policy nonzero total reward given rewards associated states non negative shows exptime complete
transitions stochastic pspace hard transitions deterministic
deterministic recall classical defined quite
differently mdp agent ability observe state
perfect predictive powers knowing initial state effects actions
certainty addition rewards come reaching goal state plan
achieves goal suces
typically defined terms set p boolean features
propositions complete assignment truth values features describes exactly one state
partial assignment truth values describes set states set propositions p
induces state space size jpj thus space required represent
feature representation exponentially smaller required
representation see section
ability represent compactly dramatic impact worstcase complexity bylander shows deterministic without
observation pspace complete roughly speaking means worst
time increase exponentially p size solution plan
grow exponentially size hold even action
space severely restricted example np complete even
cases action restricted one precondition feature one postcondition
feature conditional optimal pspace complete well
inputs generally compact generally exponentially
terms complexity fomdp pomdp phrased
probabilistic probabilistic goal oriented pomdps
typically search solution space probability distributions states


fiboutilier dean hanks

formulas describe states even simplest probabilistic one
admits observability undecidable worst madani et al intuition
even though set states finite set distributions states
worst agent may search infinite number plans able
determine whether solution exists guaranteed
solution plan eventually one exists cannot guaranteed terminate finite time
solution plan conditional probabilistic generalization
non observable probabilistic thus undecidable well
interesting note connection conditional probabilistic
pomdps actions observations two equivalent expressive
power reward structure conditional probabilistic quite
restrictive goal states positive rewards states reward goal states
absorbing since cannot put priori bound length solution plan
conditional probabilistic must viewed infinite horizon
objective maximize total expected undiscounted reward note however since goal
states absorbing guarantee total expected reward non negative
bounded even infinite horizon technically means conditional probabilistic restricted case infinite horizon positive bounded
puterman section therefore conclude solving
arbitrary infinite horizon undiscounted positive bounded pomdp undecidable
commonly studied infinite horizon pomdp criterion maximizing expected discounted total reward finding optimal near optimal solutions
undecidable noted
conclusion

end section noting independent describe worst case behavior effect indicate badly made
perform arbitrarily unfortunate instance interesting question
whether build representations techniques typically perform
well instances typically arise practice concern leads us examine
characteristics eye toward exploiting restrictions placed
states actions observability value function optimality criterion
begin algorithmic techniques focus value function particularly
take advantage time separability goal orientation following section
explore complementary techniques building compact representations

solution dynamic programming search
section review standard solving described
terms unstructured representations noted analysis
fully observable markov decision processes fomdps far widely
studied general class stochastic sequential decision begin
describing techniques solving fomdps focusing techniques exploit structure
value function time separability additivity


fidecision theoretic structural assumptions

dynamic programming approaches

suppose given fully observable mdp time separable additive value function
words given state space action space transition matrix pr js
action reward function r cost function c start
finding policy maximizes expected total reward fixed finite horizon
suppose given policy action performed agent
state stages remaining act bellman shows
expected value policy state computed set stage go
value functions vt define v r define

vt r c

x




fpr j vt g



definition value function makes dependence initial state clear
say policy optimal vt vt policies
optimal stage go value function denoted vt simply value function
optimal horizon policy bellman principle optimality bellman forms basis
stochastic dynamic programming used solve mdps establishing
following relationship optimal value function tth stage optimal value
function previous stage

vt r max
fc


x



pr ja vt g



value iteration

equation forms basis value iteration finite horizon
value iteration begins value function v r uses equation compute
sequence value functions longer time intervals horizon action
maximizes right hand side equation chosen policy element
resulting policy optimal stage fully observable mdp indeed
shorter horizon
important note policy describes done every stage
every state system even agent cannot reach certain states given system
initial configuration available actions return point

example consider simplified version robot example four

state variables cr rhc rhm movement locations ignored
four actions getc pum delc delm actions getc pum make rhc
rhm respectively true certainty action delm rhm holds makes
rhm false probability delc makes cr rhc false
probability leaving state unchanged probability reward
associated cr reward associated reward state
sum rewards objective satisfied state figure shows
optimal stage stage stage value functions states along

recall fomdps aspects history relevant



fiboutilier dean hanks

state
hm rhm cr rhci
hm rhm cr rhci
hm rhm cr rhci
hm rhm cr rhci
hm cr rhci
hm cr rhci
hm rhm cri
hm rhm cri
hm cri

v











v















pum
delm delm
delc delc
delm delm
delc delc
getc
delm delm
pum

v

figure finite horizon optimal value policy
optimal choice action state stage pairing values state
missing variables hold instantiations variables note v
simply r state
illustrate application equation first consider calculation v
robot choice delivering coffee delivering mail expected value
option one stage remaining given
ev delc v v
ev delm
v

thus delm v value maximizing choice notice
robot one action perform aim lesser objective due
risk failure inherent delivering coffee two stages remaining
state robot deliver mail certainty move one
stage go attempt deliver coffee delc
illustrate effects fixed finite horizon policy choice note
pum two stages remaining choice getting mail coffee
robot get mail subsequent delivery last stage guaranteed
succeed whereas subsequent coffee delivery may fail however compute
see
ev getc v
ev pum v
three stages go robot instead retrieve coffee
coffee two chances successful delivery expected value course
action greater guaranteed mail delivery note three stages
allow sucient time try achieve objectives fact
larger reward associated coffee delivery ensures greater number
stages remaining robot focus first coffee retrieval delivery
attempt mail retrieval delivery coffee delivery successfully completed
often faced tasks fixed finite horizon example
may want robot perform tasks keeping lab tidy picking mail whenever


fidecision theoretic structural assumptions

arrives responding coffee requests fixed time horizon associated
tasks performed need arises best
modeled infinite horizon
consider building policy maximizes discounted sum
expected rewards infinite horizon howard showed
exists optimal stationary policy intuitively case
matter stage process still infinite number stages remaining
optimal action state independent stage therefore restrict
attention policies choose action state regardless stage
process restriction policy size jsj regardless
number stages policy executed policy form
contrast optimal policies finite horizon generally nonstationary
illustrated example
howard shows value policy satisfies following recurrence

v r fc

x



optimal value function v satisfies

v r max
fc


pr j v g

x



pr ja v g




value fixed policy evaluated method successive approximation almost identical procedure described equation begin
arbitrary assignment values v define

vt r c

x




fpr j vt g



sequence functions vt converges linearly true value function v
one alter value iteration slightly builds optimal policies
infinite horizon discounted starts value function v
assigns arbitrary value given value estimate vt state vt
calculated

vt r max
fc


x



pr ja vt g



sequence functions vt converges linearly optimal value function v
finite number iterations n choice maximizing action forms
optimal policy vn approximates value
far commonly studied literature though argued boutilier
puterman mahadevan schwartz often best modeled
average reward per stage optimality criterion discussion average reward optimality
many variants refinements see puterman
number iterations n stopping criterion generally involves measuring difference vt vt discussion stopping criteria convergence see
puterman



fiboutilier dean hanks

policy iteration

howard policy iteration alternative value iteration infinitehorizon rather iteratively improving estimated value function instead
modifies policies directly begins arbitrary policy iterates computing

iteration comprises two steps policy evaluation policy improvement
policy evaluation compute value function v
current policy
policy improvement action maximizes

qi r c

x



pr ja v



qi v let otherwise
iterates states step evaluates current
policy solving n n linear system represented equation one equation
computationally expensive however converges
optimal policy least linearly certain conditions converges superlinearly
quadratically puterman practice policy iteration tends converge many
fewer iterations value iteration policy iteration thus spends computational
time individual stage fewer stages need computed
modified policy iteration puterman shin provides middle ground
policy iteration value iteration structure exactly
policy iteration alternating evaluation improvement phases key insight
one need evaluate policy exactly order improve therefore evaluation
phase involves usually small number iterations successive approximation e
setting v vt small equation tuning value
used iteration modified policy iteration work extremely well practice
puterman value iteration policy iteration special cases modified
policy iteration corresponding setting respectively
number variants value policy iteration proposed
instance asynchronous versions require value function
constructed policy improved state lockstep case value iteration
infinite horizon long state updated suciently often convergence
assured similar guarantees provided asynchronous forms policy iteration variants important tools understanding online approaches
solving mdps bertsekas tsitsiklis nice discussion asynchronous dynamic
programming see bertsekas bertsekas tsitsiklis
q function defined equation called use q learning watkins dayan
gives value performing action state assuming value function v accurately ects
future value
see littman et al discussion complexity



fidecision theoretic structural assumptions

undiscounted infinite horizon

diculty finding optimal solutions infinite horizon total reward
grow without limit time thus definition must provide way
ensure value metric bounded arbitrarily long horizons use expected
total discounted reward optimality criterion offers particularly elegant way
guarantee bound since infinite sum discounted rewards finite however although
discounting appropriate certain classes e g economic
system may terminate point certain probability many realistic
ai domains dicult justify counting future rewards less present rewards
discounted reward criterion appropriate
variety ways bound total reward undiscounted
cases structured reward bounded
example goal reward collected actions incur cost
case total reward bounded legitimately posed
terms maximizing total expected undiscounted reward many cases e g goal
reached certainty
cases discounting inappropriate total reward unbounded different
success criteria employed example instead posed one
wish maximize expected average reward per stage gain computational
techniques constructing gain optimal policies similar dynamic programming
described generally complicated convergence rate
tends quite sensitive communicating structure periodicity mdp
refinements gain optimality studied example bias optimality
used distinguish two gain optimal polices giving preference policy whose total
reward initial segment policy execution larger
complicated discounted variants standard
policy value iteration see puterman details
dynamic programming pomdps

dynamic programming techniques applied partially observable settings well
smallwood sondik main diculty building policies situations
state fully observable since past observations provide information
system current state decisions must information gleaned
past optimal policy depend observations agent made since
beginning execution history dependent policies grow size exponential
length horizon history dependence precludes dynamic programming
observable history summarized adequately probability distribution
astrom policies computed function distributions belief
states
key observation sondik smallwood sondik sondik
one views pomdp time separable value function taking state space
set probability distributions one obtains fully observable mdp
solved dynamic programming computational


fiboutilier dean hanks

state space fomdp n dimensional continuous space special
techniques must used solve smallwood sondik sondik
explore techniques note currently practical
small cassandra et al cassandra littman zhang
littman lovejoy b number approximation methods developed
lovejoy white iii scherer ai brafman hauskrecht
parr russell zhang liu used increase range solvable
even techniques presently limited practical value
pomdps play key role reinforcement learning well natural state
space consisting agent observations provides incomplete information underlying system state see e g mccallum

ai state search
noted section classical ai formulated
infinite horizon mdp therefore solved value iteration
recall two assumptions classical specialize general mdp model namely
determinism actions use goal states instead general reward function
third assumption want construct optimal course action starting
known initial state counterpart fomdp model presented
since policy dictates optimal action state stage plan
see interest online within ai led revised formulations
fomdps take initial current states account
though defined classical earlier non observable process
nomdp solved fully observable let g set goal states
sinit initial state applying value iteration type equivalent
determining reachability goal states system states instance
make goal states absorbing assign reward transitions g
g g others set states vk exactly
set states lead goal state particular vk sinit
successful plan constructed extracting actions k stage finite horizon
policy produced value iteration determinism assumption means agent
predict state perfectly every stage execution fact cannot observe
state unimportant
assumptions commonly made classical exploited computationally value iteration first terminate process first iteration k
vk sinit interested plans begin sinit acting
optimally every possible start state second terminate value iteration js j
iterations vjs j sinit point searched every possible
state guarantee solution plan exists therefore view classical finite horizon decision horizon js j use value iteration
accurately n dimensional simplex n dimensional space
specifically vk indicates probability one reaches goal region optimal
policy g stochastic settings deterministic case discussed value must




fidecision theoretic structural assumptions

equivalent floyd warshall minimum cost path
weighted graph floyd
search

value iteration theory used classical take advantage
fact goal initial states known particular computes value
policy assignment states stages wasteful since optimal
actions computed states cannot reached sinit cannot possibly
lead state g g problematic js j large since iteration
value iteration requires js jjaj computations reason dynamic programming
approaches used extensively ai
restricted form value function especially fact initial goal states
given makes advantageous view graph search unlike
general fomdps generally known priori states desirable
respect long term value well defined set target states classical
makes search appropriate taken
ai
one way formulate graph search make node graph
correspond state initial state goal states identified
search proceed forward backward graph directions
simultaneously
forward search initial state root search tree node chosen
tree fringe set leaf nodes feasible actions applied
action application extends plan one step one stage generates unique
successor state leaf node tree node pruned state
defines already tree search ends state identified member
goal set case solution plan extracted tree branches
pruned case solution plan exists forward search attempts build
plan beginning end adding actions end current sequence actions
forward search never considers states cannot reached sinit
backward search viewed several different ways could arbitrarily select
g g root search tree expand search tree fringe
selecting state fringe adding tree states action would
cause system enter chosen state general action give rise
one predecessor vertex even actions deterministic state pruned
appears search tree already search terminates initial state added
tree solution plan extracted tree search similar
dynamic programming finding shortest path graph
difference backward search considers states depth k search
tree reach chosen goal state within k steps dynamic programming
contrast visit every state every stage search
one diculty backward described commitment
particular goal state course assumption relaxed could
simultaneously search paths goal states adding first level search


fiboutilier dean hanks

tree vertex reach g g see section goal regression
viewed least implicitly
generally thought regression backward techniques effective
practice progression forward methods reasoning branching factor
forward graph number actions feasibly applied given
state substantially larger branching factor reverse graph
number operators could bring system given state especially true
goal sets represented small set propositional literals section two
approaches mutually exclusive however one mix forward backward expansions underlying graph terminate forward path backward
path meet
important thing observe restrict attention relevant reachable states forward search states
reached sinit ever considered provide benefit dynamic programming
methods states reachable since unreachable states cannot play role constructing successful plan backward approaches similarly states lying path
goal region g considered significant advantages dynamic
programming fraction state space connected goal region
contrast dynamic programming methods exception asynchronous methods must examine entire state space every iteration course ability ignore
parts state space comes stringent definition relevant states
g positive reward states matter except extent move agent
closer goal choice action states unreachable sinit interest
state search techniques use knowledge specific initial state specific
goal set constrain search process forward search exploit knowledge
goal set backward search exploit knowledge initial state graphplan
blum furst viewed method integrates
propagation forward reachability constraints backward goal informed search
describe section furthermore work partial order pop
viewed slightly different form search described
section discuss feature intensional representations mdps

decision trees real time dynamic programming

state search techniques limited deterministic goal oriented domains knowledge initial state exploited general mdps well forming basis
decision tree search assume given finite horizon fomdp
horizon initial state sinit decision tree rooted sinit constructed much
way search tree deterministic french action
applicable sinit forms level tree states positive probability actions occur applied sinit placed level arc
see bacchus et al recent work makes case progression good
search control bonet et al argue progression deterministic useful
integrating execution



fidecision theoretic structural assumptions

init
v max v v

p


p




p






v
p






v p v p v










v

v

figure initial stages decision tree evaluating action choices sinit
value action expected value successor states value
state maximum values successor actions indicated
dashed arrows selected nodes
labeled probability pr ja sinit relating level actions applicable
states level tree grown depth point
branch tree path consisting positive probability length trajectory rooted
sinit see figure
relevant part optimal stage value function optimal policy easily
computed tree say value node tree labeled
action expected value successor states tree probabilities labeling
arcs value node tree labeled state sum r
maximum value successor actions rollback procedure whereby value
leaves tree first computed values successively higher levels tree
determined preceding values fact form value iteration value
state level precisely vt maximizing actions form optimal
finite horizon policy form value iteration directed stage go values
computed states reachable sinit within steps infinite horizon
solved analogous fashion one determine priori depth
required e number iterations value iteration needed ensure convergence
optimal policy
unfortunately branching factor stochastic generally much greater
deterministic troublesome still fact one must
construct entire decision tree sure proper values computed hence
optimal policy constructed stands contrast classical search
attention focused single branch goal state reached path
constructed determines satisfactory plan worst case behavior may
require searching whole tree decision tree evaluation especially problematic
states level given value r



fiboutilier dean hanks

entire tree must generated general ensure optimal behavior furthermore
infinite horizon pose diculty determining suciently deep tree
one way around diculty use real time search korf particular
real time dynamic programming rtdp proposed barto et al
way approximately solving large mdps online fashion one interleave search
execution approximately optimal policy form rtdp similar decisiontree evaluation follows imagine agent finds particular state sinit
build partial search tree depth perhaps uniformly perhaps
branches expanded deeply others partial tree construction may halted due
time pressure due assessment agent expansion tree may
fruitful decision act must made rollback procedure applied
partial possibly unevenly expanded decision tree
reward values used evaluate leaves tree may offer
inaccurate picture value nodes higher tree heuristic information
used estimate long term value states labeling leaves value iteration
deeper tree accurate estimated value root generally speaking
fixed heuristic see section structured representations mdps
provide means construct heuristics dearden boutilier specifically
admissible heuristics upper lower bounds true values leaf nodes
tree methods branch bound search used
key advantage integrating search execution actual outcome
action taken used prune tree branches rooted unrealized
outcomes subtree rooted realized state expanded make
next action choice hansen zilberstein viewed
variant methods stationary policies e state action mappings
extracted search process
rtdp formulated barto et al generally form online asynchronous value iteration specifically values rolled backed cached used
improved heuristic estimates value function states question technique investigated bonet et al dearden boutilier koenig
simmons closely tied korf lrta value
updates need proceed strictly decision tree determine states key
requirement rtdp simply actual state sinit one states whose value
updated decision action iteration
second way avoid computational diculties arise large search
spaces use sampling methods methods sample space possible trajectories use sampled information provide estimates values specific courses
action quite common reinforcement learning sutton barto
simulation often used generate experience value function
learned present context kearns mansour ng kearns mansour
ng investigated search methods infinite horizon mdps successor
states specific action randomly sampled according transition distribution
thus rather expand successor states sampled states searched though
method exponential effective horizon mixing rate mdp
required expand actions number states expanded less required


fidecision theoretic structural assumptions

full search even underlying transition graph sparse able provide polynomial bounds ignoring action branching horizon effects number
trajectories need sampled order generate approximately optimal behavior
high probability

summary

seen dynamic programming methods state search methods
used fully observable non observable mdps forward search methods interpretable directed forms value iteration dynamic programming
generally require explicit enumeration state space iteration search
techniques enumerate reachable states branching factor may require
sucient depth search tree search methods enumerate individual states multiple
times whereas considered per stage dynamic programming overcoming diculty search requires use cycle checking multiple path checking
methods
note search techniques applied partially observable well
one way search space belief states dynamic programming applied belief space mdp see section specifically belief
states play role system states stochastic effects actions belief states
induced specific observation probabilities since observation distinct fixed
effect belief state type pursued bonet geffner
koenig simmons

factored representations

point discussion mdps used explicit extensional representation
set states actions states enumerated directly many cases
advantageous representational computational point view talk
properties states sets states set possible initial states set
states action executed generally convenient
compact describe sets states certain properties features enumerate
explicitly representations descriptions objects substitute objects
called intensional technique choice ai systems
intensional representation systems often built defining set
features sucient describe state dynamic system interest
example figure state described set six features robot location
lab tidiness whether mail present whether robot mail whether
pending coffee request whether robot coffee first
second features take one five values last four take one two
values true false assignment values six features completely defines state
state space thus comprises possible combinations feature values jsj
feature factor typically assigned unique symbolic name indicated
second column figure fundamental tradeoff extensional intensional
representations becomes clear example extensional representation coffee
example views space possible states single variable takes possible


fiboutilier dean hanks

values whereas intensional factored representation views state cross product
six variables takes substantially fewer values generally state space
grows exponentially number features required describe system
fact state system described set features allows one
adopt factored representations actions rewards components mdp
factored action representation instance one generally describes effect action
specific state features rather entire states often provides considerable representational economy instance strips action representation fikes nilsson
state transitions induced actions represented implicitly describing
effects actions features change value action executed
factored representations compact individual actions affect relatively
features effects exhibit certain regularities similar remarks apply
representation reward functions observation regularities
make factored representations suitable many often exploited
decision making
factored representations long used classical ai similar
representations adopted recent use mdp within ai
section section focus economy representation afforded exploiting
structure inherent many domains following section section
describe structure made explicit factored representations
exploited computationally plan policy construction

factored state spaces markov chains

begin examining structured states systems whose state described
finite set state variables whose values change time simplify illustration
potential space savings assume state variables boolean
variables size state space jsj n large specifying
representing dynamics explicitly state transition diagrams n n matrices
impractical furthermore representing reward function n vector specifying
observational probabilities similarly infeasible section define class
dynamics represented space many cases begin
considering represent markov chains compactly consider incorporating
actions observations rewards
let state variable x take finite number values let
x stand
set possible values assume
x finite though much follows
applied countable state action spaces well say state space
specified one state variable variable denoted general model taking
values state space factored one state variable state
possible assignment values variables letting xi represent ith state
variable state space cross product value spaces individual state

variables

xi denotes state process stage
let xit random variable representing value ith state variable stage
variables often called uents ai literature mccarthy hayes classical
atomic propositions used describe domain



fidecision theoretic structural assumptions

bayesian network pearl representational framework compactly representing probability distribution factored form although networks typically used represent atemporal domains apply techniques
represent markov chains encoding chain transition probabilities network
structure dean kanazawa
formally bayes net directed acyclic graph vertices correspond random
variables edge two variables indicates direct probabilistic dependency
network constructed ects implicit independencies among
variables network must quantified specifying probability variable
vertex conditioned possible values immediate parents graph addition
network must include marginal distribution unconditional probability
vertex parents quantification captured associating conditional
probability table cpt variable network together independence
assumptions defined graph quantification defines unique joint distribution
variables network probability event space
computed exploit independencies represented within graphical
structure refer pearl details
figures c page special cases bayes nets called temporal bayesian
networks networks vertices graph represent system state different
time points arcs represent dependencies across time points temporal networks
vertex parent temporal predecessor conditional distributions transition
probability distributions marginal distributions distributions initial states
networks figure ect extensional representation scheme states
explicitly enumerated techniques building performing inference probabilistic temporal networks designed especially application factored representations
figure illustrates two stage temporal bayes net tbn describing state transition
probabilities associated markov chain induced fixed policy executing
action cclk repeatedly moving counterclockwise tbn set variables
partitioned corresponding state variables given time stage
corresponding state variables time directed arcs indicate probabilistic dependencies variables markov chain diachronic arcs directed
time variables time variables synchronic arcs directed
variables time figure contains diachronic arcs synchronic arcs
discussed later section
given state time network induces unique distribution states
quantification network describes state particular variable changes
function certain state variables lack direct arc generally directed
path synchronic arcs among variables variable xt another
variable yt means knowledge xt irrelevant prediction immediate
one stage evolution variable markov process
figure shows compact representation best circumstances
many potential links one stage next omitted graphical
representation makes explicit fact policy e action cclk affect
state variable loc exogenous events arrm reqc mess affect


fiboutilier dean hanks

loc

loc





cr

cr

p loc
loc l c h


l

c

h
p cr
cr

f

rhc

f



rhc

rhm

rhm





time

time

p rhc
rhc f

f


figure factored tbn markov chain induced moving counterclockwise
selected cpts shown
variables cr tidy respectively furthermore dynamics loc
variables described knowledge state parent
variables instance distribution loc depends value loc
previous stage e g loct loct probability loct
probability similarly cr become true probability due reqc
event true cannot become false simple policy rhc remains
true false certainty true false previous stage finally
effects relevant variables independent instantiation variables
time distribution next states computed multiplying conditional
probabilities relevant variables
ability omit arcs graph locality independence action
effects strong effect number parameters must supplied complete
model although full transition matrix cclk would size
transition model figure requires parameters
example shows tbns exploit independence represent markov chains
compactly example extreme effectively relationship
variables chain viewed product six independently evolving processes
cpts brevity
fact exploit fact probabilities sum one leave one entry unspecified per row
cpt explicit transition matrix case tbn requires explicit parameters
transition matrix requires entries generally ignore fact comparing
sizes representations



fidecision theoretic structural assumptions

loc

loc





cr

cr

rhc

rhc

rhm

rhm





time

time

loc rhc


l

c



h


f
l
f
c
f

f
h
f

p loc
l c h









p cr

loc rhc cr f

p rhc
loc rhc
f


f


l
f
l

c
f
c
etc
etc





l
l
l
l



f
f


f
f
etc


f

f

f

f









etc

figure tbn markov chain induced moving counterclockwise delivering coffee



fiboutilier dean hanks

general subprocesses interact still exhibit certain independencies
regularities exploited tbn representation consider two distinct
markov chains exhibit different types dependencies
figure illustrates tbn representing markov chain induced following
policy robot consistently moves counterclockwise unless oce
coffee case delivers coffee user notice different variables
dependent instance predicting value rhc requires knowing values
loc rhc cpt rhc shows robot retains coffee stage
certainty stage locations except executes delc
thus losing coffee variable loc depends value rhc location
change figure one exception robot oce coffee
location remains since robot move executes delc effect
variable cr explained follows robot oce delivers coffee
possession fulfill outstanding coffee request however chance cr
remaining true conditions indicates chance spilling coffee
even though dependencies e additional diachronic arcs tbn
still requires parameters distribution resulting states determined multiplying conditional distributions individual variables even though
variables related state known variables time loct
rhct etc independent words
pr loct crt rhct rhmt js

pr loct js pr js pr crt js pr rhct js pr rhmt js pr js
figure illustrates tbn representing markov chain induced policy
assume act moving counterclockwise slightly
different effect suppose robot moves hallway
adjacent location chance spilling coffee possession
fragment cpt rhc figure illustrates possibility furthermore
robot carrying mail whenever loses coffee whether accidentally intentionally
via delc action chance lose mail notice effects
policy variables rhc rhm correlated one cannot accurately predict
probability rhmt without determining probability rhct correlation
modeled synchronic arc rhc rhm slice network
independence variables given hold tbns synchronic
arcs determining probability resulting state requires simple probabilistic
reasoning example application chain rule example write
pr rhct rhmt js pr rhmt jrhct pr rhct js
joint distribution variables given computed equation term replacing pr rhct js pr rhmt js two
variables correlated remaining variables independent
refer tbns synchronic arcs one figure simple tbns
general tbns allow synchronic well diachronic arcs figure


fidecision theoretic structural assumptions

loc

loc





cr

cr

rhc

rhc

rhm

rhm





time

time

pr rhc
loc rhc f


f


h
f
h

c
f
c
etc
etc
pr rhmt
rhc rhc rhmt f





f




f


f
f




f

f

f


f
f

f
f
f

figure tbn markov chain induced moving counterclockwise delivering coffee correlated effects

factored action representations

extended markov chains account different actions must extend
tbn representation account fact state transitions uenced
agent choice action discuss variety techniques specifying transition
matrices exploit factored state representation produce representations
natural compact explicit transition matrices
implicit event

begin implicit event model section effects actions
exogenous events combined single transition matrix consider explicitevent section saw previous section value
policy iteration require use transition ect ultimate transition
probabilities including effects exogenous events
one way model dynamics fully observable mdp represent action
separate tbn tbn shown figure seen representation
action cclk since policy inducing markov chain example consists
repeated application action alone network fragment figure
illustrates interesting aspects tbn delc action including effects
exogenous events robot satisfies outstanding coffee request delivers
coffee oce coffee chance spillage shown
conditional probability table cr effect rhc explained follows


fiboutilier dean hanks

loc

rhc

cr
time

loc

rhc

cr
time



pr rhc
loc rhc f



f
l

f
l

c
f
c
etc
etc
pr cr
loc rhc cr f



f



f

f
f



l
f

l

f
l
f
f
l
etc
etc





cr



rhc

loc

else

f

f




cr



cr



f

f





b




cr



rhc

f

loc
else

f
f



cr





c

figure factored tbn action delc structured cpt representations b c
robot loses coffee user spillage delivers oce attempts
delivery elsewhere chance random passerby take coffee
robot
case markov chains effects actions different variables
correlated case must introduce synchronic arcs correlations
thought ramifications baker finger lin reiter
structured cpts

conditional probability table cpt node cr figure rows one
assignment parents however cpt contains number regularities
intuitively ects fact coffee request met successfully e
variable becomes false time delc executed robot coffee
right location user oce otherwise cr remains true true
becomes true probability words three distinct cases
considered corresponding three rules governing stochastic effect delc
cr represented compactly decision tree representation
else branches summarize groups cases involving multivalued variables
loc shown figure b compactly still decision graph
figure c tree graph representations cpts interior nodes labeled
parent variables edges values variables leaves terminals distributions
child variable values
decision tree decision graph representations used represent actions fully
observable mdps boutilier et al hoey st aubin hu boutilier
child boolean label leaves probability variable true
probability variable false one minus value



fidecision theoretic structural assumptions

described detail poole boutilier goldszmidt intuitively trees
graphs embody rule structure present family conditional distributions
represented cpt settings consider often yield considerable representational compactness rule representations used directly poole
context decision processes often compact trees poole
b generically refer representations type tbns structured cpts
probabilistic strips operators

tbn representation viewed oriented toward describing effects actions
distinct variables cpt variable expresses stochastically changes
persists perhaps function state certain variables however
long noted ai reasoning action
actions change state limited ways affect relatively small number
variables one diculty variable oriented representations tbns one
must explicitly assert variables unaffected specific action persist value e g
see cpt rhc figure instance infamous frame
mccarthy hayes
another form representation actions might called outcome oriented representation one explicitly describes possible outcomes action possible joint
effects variables idea underlying strips representation
classical fikes nilsson
classical strips operator described precondition set effects
former identifies set states action executed latter
describes input state changes taking action probabilistic
strips operator pso hanks hanks mcdermott kushmerick et al
extends strips representation two ways first allows actions different
effects depending context second recognizes effects actions
known certainty
formally pso consists set mutually exclusive exhaustive logical formulae
called contexts stochastic effect associated context intuitively context discriminates situations action differing stochastic effects
stochastic effect set change sets simple list variable values
probability attached change set requirement probabilities sum
one semantics stochastic effect described follows stochastic
effect action applied state possible resulting states determined
change sets occurring corresponding probability resulting state associated change set constructed changing variable values state match
change set unmentioned variables persist value note since one
fact certain direct dependencies among variables bayes net rendered irrelevant
specific variable assignments studied generally guise context specific independence
boutilier friedman goldszmidt koller see geiger heckerman shimony
related notions
conditional nature effects feature deterministic extension strips known adl
pednault



fiboutilier dean hanks

rhc


f

loc


cr rhc
cr rhc
rhc
rhc

cr
cr

nil

else

rhc cr
rhc cr
rhc
rhc
cr
cr

nil




















figure pso representation delc action
loc


l

loc l
nil


loc c
nil


h

c



loc
nil


loc h
nil


loc rhc rhm
loc rhc
loc
rhc rhm
rhc
nil








figure pso representation simplified cclk action
context hold state transition distribution action state
easily determined
figure gives graphical depiction pso delc action shown tbn
figure three contexts rhc rhc loc rhc loc represented
decision tree leaf branch decision tree stochastic effect
set change sets associated probabilities determined corresponding context
example rhc loc holds action four possible effects robot loses
coffee may may satisfy coffee request due chance spillage
mail may may arrive notice outcome spelled completely
number outcomes two contexts rather large due possible exogenous
events discuss section
key difference psos tbns lies treatment persistence
variables unaffected action must given cpts tbn model
variables mentioned pso model e g compare variable loc
representations delc way psos said solve frame
since unaffected variables need mentioned action description
keep figure manageable ignore effect exogenous event mess variable
discussion frame tbns see boutilier goldszmidt



fidecision theoretic structural assumptions

arrm

mess

loc

loc

loc

loc

rhc

rhc

rhc

rhc

rhm

rhm

rhm

rhm

cr

cr

cr

cr

























figure simplified explicit event model delc
psos provide effective means representing actions correlated effects
recall description cclk action captured figure robot may
drop coffee moves hallway may drop mail drops
coffee tbn representation cclk one must rhct rhct
parents rhmt must model dependence rhm change value
variable rhc figure shows cclk action pso format simplicity ignore
occurrence exogenous events pso representation offer economical
representation correlated effects since possible outcomes moving
hallway spelled explicitly specifically possible simultaneous change values
variables question made clear
explicit event

explicit event represented tbns somewhat different form
discussion section form taken explicit event depends crucially one assumptions interplay effects action
exogenous events however certain assumptions even explicit event
rather concise
illustrate figure shows deliver coffee action represented tbn
exogenous events explicitly represented first slice network shows effects
action delc without presence exogenous events subsequent slices describe
effects events arrm mess use two events illustration notice
presence extra random variables representing occurrence events question
cpts nodes ect occurrence probabilities events


fiboutilier dean hanks

conditions directed arcs event variables state variables indicate
effects events probabilities depend state variables general
thus tbn represents occurrence vectors see section compact form
notice contrast event occurrence variables explicitly represent
action occurrence variable network since modeling effect
system given action taken
example ects assumptions described section namely events
occur action takes place event effects commutative
reason ordering events arrm mess network irrelevant
model system actually passes two intermediate though necessarily distinct
states goes stage stage use subscripts suggest
process course described earlier actions events combined
decomposable way complex combination functions modeled tbns
one example see boutilier puterman
equivalence representations

obvious question one might ask concerns extent certain representations
inherently concise others focus standard implicit event
describing domain features make different representations less
suitable
tbn pso representations oriented toward representing changes
values state variables induced action key distinction lies fact
tbns model uence variable separately pso model explicitly
represents complete outcomes simple tbn network synchronic arcs
used represent action cases correlations among action
effect different state variables worst case effect variable
differs state time variable must time variables parents
regularities exploited structured cpt representations
action requires specification n n parameters assuming boolean variables
compared n entries required explicit transition matrix number
parents variable bounded k need specify n k conditional
probabilities reduced cpts exhibit structure e g
represented concisely decision tree instance cpt captured
representation choice f k entries f polynomial function
number parents variable representation size n f k polynomial
number state variables often case instance actions one
stochastic effects variable requires number pre conditions hold
different effect comes play
pso representation may concise tbn action multiple
independent stochastic effects pso requires possible change list enumerated corresponding probability occurrence number changes grows
exponentially number variables affected action fact evident
sections discuss representations model choice action explicitly variable
network



fidecision theoretic structural assumptions

rhc


f

loc


rhc cr
rhc

nil


nil


else



rhc
nil




figure factored pso representation delc action
figure impact exogenous events affects number variables stochastically independently arise respect direct action effects
well consider action set unpainted parts spray painted part
successfully painted probability successes uncorrelated ignoring
complexity representing different conditions action could take place
simple tbn represent action parameters one success probability per
part contrast pso representation might require one list distinct change
lists associated probabilities thus pso representation exponentially
larger number affected variables simple tbn representation
fortunately certain variables affected deterministically cause
pso representation blow furthermore pso representations modified
exploit independence action effects different state variables boutilier
dearden dearden boutilier thus escaping combinatorial diculty
instance might represent delc action shown figure factored
form illustrated figure simplicity effect action
exogenous event arrm much tbn determine overall effect
combining change sets appropriate contexts multiplying corresponding
probabilities
simple tbns defined original set state variables sucient represent actions correlated action effects require presence synchronic arcs
worst case means time variables n parents
fact
p acyclicity condition assures worst case total number parents
nk k thus end specifying n entries required
explicit transition matrix however number parents whether occurring within
time slice bounded regularities cpts allow compact
representation tbns still profitably used
pso representations compare favorably tbns cases
action effects different variables correlated case psos provide
somewhat economical representation action effects primarily one
worry frame conditions main advantage psos one need enlist
aid probabilistic reasoning procedures determine transitions induced actions
correlated effects contrast explicit specification outcomes psos
type reasoning required determine joint effects action represented tbn
however section discusses certain transformations render simple tbns sucient
mdp



fiboutilier dean hanks

form synchronic arcs described section essentially correlated effects
compiled explicit outcomes psos
recent littman shown simple tbns psos
used represent action represented tbn without exponential blowup
representation size effected clever transformation
sets actions propositional variables introduced simple tbn
pso representation structure original tbn ected
incurring polynomial increase size input action
descriptions description policy though resulting policy consists actions
exist underlying domain extracting true policy dicult
noted however representation automatically constructed
general tbn specification unlikely could provided directly since
actions variables transformed physical meaning
original mdp
transformations eliminate synchronic constraints

discussion assumed variables propositions used tbn
pso action descriptions original state variables however certain transformations used ensure one represent action simple tbns
long one require original state variables used one transformation
simply clusters variables action correlated effect compound
variable takes values assignments clustered variables used
tbn removing need synchronic arcs course variable
domain size exponential number clustered variables
intuitions underlying psos used convert general tbn action descriptions simple tbn descriptions explicit events dictating precise outcome
action intuitively event occur k different forms corresponding
different change list induced action change list respect variables
question example convert action description cclk figure
explicit event model shown figure notice event takes values
corresponding possible effects correlated variables rhc rhm specifically denotes event robot escaping hallway successfully without losing
cargo b denotes event robot losing coffee c denotes event losing
coffee mail effect event space represents possible combined
effects obviating need synchronic arcs network
actions explicit nodes network

one diculty tbn pso action description action
represented separately offering opportunity exploit patterns across actions
instance fact location persists actions except moving clockwise counterclockwise means frame axiom duplicated tbn actions
case psos course addition ramifications correlated action
figure describes markov chain induced policy representation cclk easily
extracted



fidecision theoretic structural assumptions

loc

hall


b
c

event



rhc

rhm


loc

else

f


b b
c c

loc

f


b
c





rhc

rhc




rhm
time

rhm
time

loc






rhm

b


b


f



c


f



else

event

event

rhc

c


figure explicit event model removes correlations
effects duplicated across actions well instance coffee request occurs
probability robot ends oce correlation duplicated
across actions compelling example might one robot move
briefcase location one number ways capture fact
ramification contents briefcase move location briefcase
regardless action moves briefcase
circumvent diculty introduce choice action random variable network conditioning distribution state variable transitions
value variable unlike state variables event variables explicit event
generally require distribution action variable intent simply
model schematically conditional state transition distributions given particular
choice action choice action dictated decision maker
policy determined reason anticipating terminology used uence
diagrams see section call nodes decision nodes depict network diagrams boxes variable take value action available
agent
tbn explicit decision node shown figure restricted example
might imagine decision node take one two values clk cclk fact
issuance coffee request depends whether robot successfully moved
remained oce represented arc loct crt
rather repeated across multiple action networks furthermore noisy persistence
actions represented adding action pum however
undercuts advantage see try combine actions
one diculty straightforward use decision nodes standard
representation uence diagram literature adding candidate actions
cause explosion network dependency structure example consider two


fiboutilier dean hanks

act

loc

loc

cr

cr





time

time

figure uence diagram restricted process

act
x

x

x

x

x

act

x

else





















f


x



z

z
action

z

z

z

b action



f




f





z



f


z

c influence diagram

figure unwanted dependencies uence diagrams





cpt



f


fidecision theoretic structural assumptions

action networks shown figure b action makes true probability
x true effect otherwise makes true z true
combining actions single network obvious way produces uence
diagram shown figure c notice four parent nodes inheriting
union parents individual networks plus action node requiring
cpt entries actions together eight additional entries
action affect individual networks ect fact depends
x performed z performed fact lost
naively constructed uence diagram however structured cpts used
recapture independence compactness representation tree figure
captures distribution much concisely requiring eight entries structured
representation allows us concisely express persists actions
large domains expect variables generally unaffected substantial number
perhaps actions thus requiring representations uence diagrams
see boutilier goldszmidt deeper discussion issue relationship
frame
provide distributional information action choice hard
see tbn explicit decision node used represent markov chain
induced particular policy natural way specifically adding arcs state
variables time decision node value decision node e choice
action point dictated prevailing state

uence diagrams
uence diagrams howard matheson shachter extend bayesian networks
include special decision nodes represent action choices value nodes represent
effect action choice value function presence decision nodes means
action choice treated variable decision maker control value nodes treat
reward variable uenced usually deterministically certain state variables
uence diagrams typically associated schematic representation
stationary systems instead used tool decision analysts sequential
decision carefully handcrafted generic use uence diagrams
discussed tatman shachter event theory plan
construction associated uence diagrams choice possible actions
stage must explicitly encoded model uence diagrams therefore usually
used model finite horizon decision explicitly describing evolution
process stage terms state variables
section decision nodes take values specific actions though set
possible actions tailored particular stage addition analyst generally
include stage state variables thought relevant decision
subsequent stages value nodes key feature uence diagrams
discussed section usually single value node specified arcs indicating
generally randomized policy represented specifying distribution possible actions
conditioned state



fiboutilier dean hanks


rhm


rhm
rew

cr



etc




cr

etc




















figure representation reward function uence diagram
uence particular state decision variables often multiple stages overall
value function
uence diagrams typically used model partially observable arc
state variable decision node ects fact value state variable
available decision maker time action chosen words
variable value forms part observation made time prior action
selected time policy constructed refer variable
allows compact specification observation probabilities associated system
fact probability given observation depends directly certain variables
others mean far fewer model parameters required

factored reward representation

already noted common formulating mdp adopt
simplified value function assigning rewards states costs actions evaluating histories combining factors according simple function addition
simplification alone allows representation value function significantly
parsimonious one complex comparison complete histories even
representation requires explicit enumeration state action space however
motivating need compact representations parameters factored representations rewards action costs often obviate need enumerate state
action parameters explicitly
action effect particular variable reward associated state often
depends values certain features state example robot
domain associate rewards penalties undelivered mail unfulfilled coffee
requests untidiness lab reward penalty independent
variables individual rewards associated groups states differ
values relevant variables relationship rewards state variables
represented value nodes uence diagrams represented diamond figure
conditional reward table crt node table associates reward
every combination values parents graph table shown figure
locally exponential number relevant variables although figure shows
case stationary markovian reward function uence diagrams used represent


fidecision theoretic structural assumptions

nonstationary history dependent rewards often used represent value functions
finite horizon
although worst case crt take exponential space store many
cases reward function exhibits structure allowing represented compactly
decision trees graphs boutilier et al strips tables boutilier dearden
logical rules poole figure shows fragment one possible
decision tree representation reward function used running example
independence assumptions studied multiattribute utility theory keeney raiffa
provide yet another way reward functions represented compactly
assume component attributes reward function make independent contributions state total reward individual contributions combined functionally
instance might imagine penalizing states cr holds partial reward
penalizing situations undelivered mail rhm
penalizing untidiness e proportion untidy things
reward state determined simply adding individual penalties associated feature individual component rewards along combination
function constitute compact representation reward function tree fragment
figure ects additive independent structure described considerably
complex representation defines independent rewards individual
propositions separately use additive reward functions mdps considered
boutilier brafman geib meuleau hauskrecht kim peshkin kaelbling dean
boutilier singh cohn
another example structured rewards goal structure studied classical
goals generally specified single proposition set literals achieved
generally represented compactly haddawy hanks
explore generalizations goal oriented permit extensions partial goal
satisfaction yet still admit compact representations

factored policy value function representation

techniques studied far concerned input specification mdp
states actions reward function components solution policy optimal value function candidates compact structured representation
simplest case stationary policy fully observable policy
must associate action every state nominally requiring representation size
jsj exacerbated nonstationary policies pomdps example
policy finite horizon fomdp stages generates policy size jsj
finite horizon pomdp possible
p observable history length might require
different action choice many tk bk histories generated fixed
policy b maximum number possible observations one make following
action
fact policies require much space motivates need compact functional representations standard techniques tree structures discussed
methods dealing pomdps conversion fomdps belief space see section
complex still



fiboutilier dean hanks

cr
rhc

etc

loc


l c

loc


h

h

l

c



delc clk clk cclk cclk hrm clk getc
pum cclk

delm cclk

pum cclk

figure tree representation policy
actions reward functions used represent policies value functions well
focus stationary policies value functions fomdps logical
function representation may used example schoppers uses strips style
representation universal plans deterministic plan policies decision trees
used policies value functions boutilier et al chapman
kaelbling example policy robot domain specified decision tree
given figure policy dictates instance cr rhc true
robot deliver coffee user oce b move toward oce
oce unless c mail mailroom case
pickup mail way

summary

section discussed number compact factored representations components
mdp began discussing intensional state representations temporal bayesian
networks device representing system dynamics tree structured conditional
probability tables cpts probabilistic strips operators psos introduced
alternative transition matrices similar tree structures logical representations
introduced representing reward functions value functions policies
representations often used describe compactly
offer guarantee solved effectively next
section explore use factored representations avoid iterating
explicitly entire set states actions

abstraction aggregation decomposition methods

greatest challenge mdps basis dtp lies discovering computationally feasible methods construction optimal approximately optimal satisficing
policies course arbitrary decision intractable even producing satisficing
approximately optimal policies generally infeasible however previous sections
suggest many realistic application domains may exhibit considerable structure
furthermore structure modeled explicitly exploited typical
solved effectively instance structure type lead compact


fidecision theoretic structural assumptions

factored representations input data output policies often polynomial sized
respect number variables actions describing suggests
compact representations policy construction techniques developed exploit structure tractable many commonly occurring
instances
dynamic programming state search techniques described section exploit structure different kind value functions decomposed
state dependent reward functions state goal functions tackled dynamic
programming regression search respectively exploit structure
decomposable value functions prevent search explicitly possible
policies however polynomial size state space
curse dimensionality makes even infeasible practical
though compact representations aid specification large
clear large system specified compactly representation exploits
regularities found domain recent ai dtp stressed
regularities implicit compact representations speed process
techniques focus optimal approximately optimal policy construction
following subsection focus abstraction aggregation techniques especially manipulate factored representations roughly techniques allow
explicit implicit grouping states indistinguishable respect certain characteristics e g value optimal action choice refer set states grouped
manner aggregate abstract state sometimes cluster assume
set abstract states constitutes partition state space say every state
exactly one abstract state union abstract states comprises entire state
space grouping similar states abstract state treated single state thus
alleviating need perform computations state individually techniques
used approximation elements abstract state approximately
indistinguishable e g values states lie within small interval
look use decomposition techniques mdp
broken pieces solved independently solutions
pieced together used guide search global solution subprocesses whose
solutions interact minimally treated independent might expect approximately
optimal global solution furthermore structure requires solution
particular subproblem solutions subproblems ignored
altogether
related use reachability analysis restrict attention relevant regions
state space indeed reachability analysis communicating structure mdp
used form certain types decompositions specifically distinguish serial
decompositions parallel decompositions
serial decomposition viewed partitioning state space
blocks representing less independent subprocess solved
serial decomposition relationship blocks generally complicated
case abstraction aggregation partition resulting decomposition
might group states non disjoint sets cover entire state space consider
soft state aggregation see singh jaakkola jordan



fiboutilier dean hanks

states within particular block may behave quite differently respect say value
dynamics important consideration choosing decomposition possible
represent block compactly compute eciently consequences moving
one block another subproblems corresponding subprocesses
solved eciently
parallel decomposition somewhat closely related abstract mdp
mdp divided parallel sub mdps decision action causes
state change within sub mdp thus mdp cross product join
sub mdps contrast union serial decomposition brie discuss several
methods parallel mdp decomposition

abstraction aggregation
one way structure exploited policy construction relies notion
aggregation grouping states indistinguishable respect certain
characteristics example might group together states optimal
action value respect k stage go value function
aggregates constructed solution
ai emphasis generally placed particular form aggregation namely
abstraction methods states aggregated ignoring certain features
policy figure illustrates type abstraction states cr
rhc loc true grouped action selected
state intuitively three propositions hold features ignored
abstracted away e deemed irrelevant decision tree representation
policy value function partitions state space distinct cluster leaf
tree representations e g strips rules abstract state space similarly
precisely type abstraction used compact factored representations actions goals discussed section tbn shown figure
effect action delc variable cr given cpt crt however
stochastic effect state parent variables
value representation abstracts away variables combining states
distinct values irrelevant non parent variables intensional representations often
make easy decide features ignore certain stage solving
thus implicitly aggregate state space
least three dimensions along abstractions type compared first uniformity uniform abstraction one variables deemed
relevant irrelevant uniformly across state space nonuniform abstraction allows certain variables ignored certain conditions others
distinction illustrated schematically figure tabular representation cpt
viewed form uniform abstraction effect action variable
distinguished clusters states differ value parent variable
distinguished states agree parent variables disagree others
decision tree representation cpt embodies nonuniform abstraction
second dimension comparison accuracy states grouped together
basis certain characteristics abstraction called exact states within


fidecision theoretic structural assumptions

uniform
abc
abc

abc
abc

abc
abc

abc
abc

nonuniform

b




ab



abc

c

abc

exact





approximate



















adaptive

fixed

figure different forms state space abstraction
cluster agree characteristic non exact abstraction called approximate
illustrated schematically figure exact abstraction groups together states
agree value assigned value function approximate abstraction
allows states grouped together differ value extent states
differ often used measure quality approximate abstraction
third dimension adaptivity technically property abstraction
abstractions used particular adaptive abstraction
technique one abstraction change course computation
fixed abstraction scheme groups together states see figure
example one imagine abstraction representation value function
v k revising abstraction represent v k accurately
abstraction aggregation techniques studied literature
mdps bertsekas castanon develop adaptive aggregation opposed
abstraction technique proposed method operates state spaces however
therefore exploit implicit structure state space adaptive uniform
abstraction method proposed schweitzer et al solving stochastic queuing methods often referred aggregation disaggregation procedures
typically used accelerate calculation value function fixed policy valuefunction calculation requires computational effort least quadratic size state
space impractical large state spaces aggregation disaggregation procedures states first aggregated clusters system equations solved
series summations performed requiring effort cubic number
clusters next disaggregation step performed cluster requiring effort least
linear size cluster net total work least linear
total number states worst cubic size largest cluster
dtp generally assumed computations even linear size full
state space infeasible therefore important develop methods perform


fiboutilier dean hanks

work polynomial log size state space amenable
reductions without perhaps unacceptable sacrifice solution quality
following section review recent techniques dtp aimed achieving
reductions
goal regression classical

section introduced general technique regression backward search
state space solve classical involving deterministic actions performance criteria specified terms reaching goal satisfying state one
diculty search requires branch search tree lead particular
goal state commitment goal state may retracted backtracking
search process sequence actions lead particular goal state
initial state however goal usually specified set literals g representing set
states reaching state g equally suitable may therefore wasteful
restrict search finding plan reaches particular element g
goal regression abstraction technique avoids choosing particular goal state pursue regression planner works searching sequence actions
follows current set subgoals sg initialized g iteration action
selected achieves one current subgoals sgi without deleting
others whose preconditions con ict unachieved subgoals
subgoals achieved removed current subgoal set replaced formula
representing context achieve current subgoals forming sgi
process known regressing sgi process repeated one
two conditions holds current subgoal set satisfied initial state
case current sequence actions selected successful plan b action
applied case current sequence cannot extended successful plan
earlier action choice must reconsidered
example example consider simplified version robot example used section illustrate value iteration robot four actions
pum getc delc delm make deterministic obvious way
initial state sinit hcr rhc rhmi goal set g fcr g regressing g delm sg fcr rhmg regressing sg
delc sg frhc rhmg regressing sg pum
sg frhc g regressing sg getc sg fm g note
sinit sg sequence actions getc pum delc delm successfully reach
goal state
see implements form abstraction first note goal
provides initial partition state space dividing one set states
goal satisfied g second set g viewed partition
zero stage go value function g represents states whose value positive
g represents states whose value zero
every regression step thought revising partition
attempts satisfy current subgoal set sgi applying action uses


fidecision theoretic structural assumptions

getc

rhc







pum

rhc

delc

cr

delm

cr







rhm

rhm









goal







figure example goal regression
regression compute largest set states executing subgoals
satisfied particular state space repartitioned two abstract states sgi
sgi way abstraction mechanism implemented goal regression
considered adaptive viewed stage value function state
satisfying sgi reach goal state steps action sequence produced
sgi regression process stopped initial state member
abstract state sgi figure illustrates repartitioning state space
different regions sgi steps example
regression produces compact representation something value function
discussion deterministic goal dynamic programming section
analogy exact regions produced regression record property
goal reachability contingent particular choice action action sequence
standard dynamic programming methods implemented structured way
simply noticing number different regions produced ith iteration
considering actions regressed stage union
regressions form states positive values vi thus making representation
stage go value function exact notice iteration costly since
regression actions must attempted obviates need
backtracking ensure shortest plan found standard regression
provide guarantees without commitment particular search strategy e g breadthfirst use dynamic programming strips action descriptions forms basic
idea schoppers universal method schoppers
another general technique solving classical partial order pop chapman sacerdoti embodied popular snlp mcallester rosenblitt ucpop penberthy weld
main motivation least commitment comes realization
regression techniques incrementally building plan end beginning
temporal dimension thus iteration must commit inserting step last
plan
many cases determined particular step must appear somewhere
plan necessarily last step plan indeed many cases step
case however states sgi cannot reach goal region steps
case cannot specific sequence actions chosen far
type sometimes called nonlinear least commitment see weld
survey nice overview



fiboutilier dean hanks

consideration cannot appear last fact cannot recognized later choices
reveal inconsistency cases regression prematurely commit
incorrect ordering eventually backtrack choice example
suppose scenario robot hold one item time
coffee mail picking mail causes robot spill coffee possession
similarly grasping coffee makes drop mail plan generated regression would
longer valid first two actions delc delm inserted
plan action added achieve rhc rhm without making one false
search plan would backtrack ultimately would discovered
successful plan end two actions performed sequence
partial order proceed much regression choosing
actions achieve unachieved subgoals regression determine subgoals
leaving actions unordered whatever extent possible strictly speaking subgoal sets
regressed rather unachieved goal action precondition addressed separately
actions ordered relative one another one action threatens negate
desired effect another example might first place actions
delc delm plan leave unordered pum added plan
achieve requirement rhm delm ordered delm still unordered
respect delc getc finally added plan achieve rhc
action delc two threats arise first getc threatens desired effect rhm pum
resolved ordering getc pum delm assume former ordering
chosen second pum threatens desired effect rhc getc threat
resolved placing pum getc delc since first threat resolved
ordering getc pum latter ordering consistent one
plan getc delc pum delm backtracking required generate plan
actions initially unordered orderings introduced
discovery threats required
terms abstraction incomplete partially ordered plan threat free
perhaps certain open conditions unachieved preconditions subgoals
viewed much way partially completed regression plan state satisfying
open conditions reach goal state executing total ordering plan
actions consistent current set ordering constraints see kambhampati
framework unifies approaches solving classical plan generation
techniques relying regression studied extensively deterministic
setting recently applied probabilistic unobservable kushmerick
et al partially observable draper hanks weld b domains
part techniques assume goal performance criterion attempt
construct plans whose probability reaching goal state exceeds threshold
augment standard pop methods techniques evaluating plan probability
achieving goal techniques improving probability adding structure
plan next section consider use regression related techniques
solve mdps performance criteria general goals


fidecision theoretic structural assumptions

stochastic dynamic programming structured representations

key idea underlying propositional goal regression one need regress relevant propositions action extended stochastic dynamic programming
methods value iteration policy iteration used solve general mdps
however two key diculties overcome lack specific goal region
uncertainty associated action effects
instead viewing state space partitioned goal non goal clusters
consider grouping states according expected values ideally might want
group states according value respect optimal policy consider
somewhat less dicult task grouping states according value respect
fixed policy essentially task performed policy evaluation step
policy iteration insights used construct optimal policies
fixed policy want group states value policy
generalizing goal versus non goal distinction begin partition groups
states according immediate rewards analogue regression developed
stochastic case reason backward construct partition states
grouped according value respect one stage go value function
iterate manner kth iteration produce partition groups
states according k stage go value function
iteration perform work polynomial number abstract states
size mdp representation lucky total number abstract states
bounded logarithmic factor size state space implement
scheme effectively perform operations regression without ever enumerating
set states structured representations state transition
value policy functions play role
fomdps approaches type taken boutilier boutilier dearden boutilier et al boutilier dearden goldszmidt dietterich
flann hoey et al illustrate basic intuitions behind
describing value iteration discounted infinite horizon fomdps might work
assume mdp specified compact representation reward function
decision tree actions tbns
value iteration produce sequence value functions v v vn vk
representing utility optimal k stage policy aim produce compact
representation value function vn suitable n produce compact
representation optimal stationary policy given compact representation
reward function r clear constitutes compact representation v
usual think leaf tree cluster states identical utility
produce v compact form proceed two phases
branch tree v provides intensional description namely conjunction variable values labeling branch abstract state region comprising
states identical value respect initial value function v deterministic action perform regression step description determine
conditions perform would end cluster would
furthermore determine region state space containing states identical future value


fiboutilier dean hanks

x

x

x


x











z

z




time

z

time


figure example action
respect execution one stage go unfortunately nondeterministic
actions cannot handled quite way given state action might lead
several different regions v non zero probability however leaf tree
representing v e region v regress conjunction x describing
region action produce conditions x becomes true
false specified probability words instead regressing standard fashion determine conditions x becomes true produce set distinct
conditions x becomes true different probabilities piecing together
regions produced different labels description v construct
set regions state given region transitions action
particular part v identical probability hence b identical expected future
value boutilier et al view generalization propositional goal
regression suitable decision theoretic
example illustrate consider example action shown figure value
function v shown left figure order generate set regions
consisting states whose future value w r v identical proceed
two steps see figure first determine conditions fixed
probability making true hence fixed probability moving left
right subtree v conditions given tree representing cpt
node makes first portion tree representing v see step
figure notice tree leaves labeled probability making
true implicitly false
makes true know future value e value zero stages
go becomes false need know whether makes z true
ignore immediate reward cost distinctions within region produced description
recall value performing state given r c expected future value
simply focus abstract states whose elements identical future expected value differences
immediate reward cost added fact



fidecision theoretic structural assumptions



x



z







x










z

z


z


v

step






z

z


z


z

step

figure iteration decision theoretic regression step produces portion
tree dashed lines step produces portion dotted lines
determine whether future value probability z becomes
true given tree representing cpt node z step figure
conditions cpt conjoined conditions required predicting
probability grafting tree z tree given first step
grafting slightly different three leaves tree
full tree z attached leaf x b tree z simplified
attached leaf x f f removal redundant test variable
c notice need attach tree z leaf x f
since makes true probability conditions z relevant
determination v false
leaves newly formed tree pr pr z
joint distributions z effect variables independent semantics network tells us probability z true
zero stages go given conditions labeling appropriate branch
tree hold one stage go words tree uniquely determines
state one stage remaining probability making conditions
labeling branches v true computation expected future value obtained
performing one stage go placed leaves tree
taking expectation values leaves v
set regions produced way describes function qff qff
value associated performing state one stage go acting optimally
thereafter functions action pieced together e maxed see
section determine v course process repeated number times
produce vn suitable n well optimal policy respect vn
basic technique used number different ways dietterich flann
propose ideas similar restrict attention mdps goal regions


fiboutilier dean hanks

deterministic actions represented strips operators thus rendering true goalregression techniques directly applicable boutilier et al develop version
modified policy iteration produce tree structured policies value functions
boutilier dearden develop version value iteration described
extended deal correlations action effects e synchronic arcs
tbns boutilier abstraction schemes categorized nonuniform
exact adaptive
utility exact abstraction techniques tested real world date boutilier et al series abstract process
examples reported scheme shown useful especially larger
example one specific million states tree representation value function leaves indicating tremendous amount
regularity value function schemes exploit regularity solve
quickly example much less half time required modified policy iteration much lower memory demands however schemes involve
substantial overhead tree construction smaller little regularity
overhead repaid time savings simple vector matrix representations methods
faster though still generally provide substantial memory savings might
viewed best worst case behavior described boutilier et al
series linear examples e value functions represented
trees whose size linear number variables tree scheme solves
many orders magnitude faster classical state techniques contrast exponentially many distinct values tested e distinct
value state tree construction methods required construct complete
decision tree addition performing number expected value maximization
computations classical methods worst case tree construction overhead makes
run times slower standard modified policy iteration
hoey et al similar described uses algebraic decision
diagrams adds bahar frohm gaona hachtel macii pardo somenzi rather
trees adds simple generalization boolean decision diagrams bdds bryant
allow terminal nodes labeled real values instead boolean values
essentially add similar tree except
isomorphic subtrees shared lets adds provide compact representations
certain types value functions highly optimized add manipulation evaluation
software developed verification community applied solving mdps
initial provided hoey et al encouraging showing considerable savings
tree example add applied
million state example described revealed value function
distinct values cf tree leaves required produced add description
value function less internal nodes solved
seven minutes times faster earlier reported timing decision
trees though improvement due use optimized add software
packages similar obtain million states
dietterich flann describe work context reinforcement learning rather
method solving mdps directly



fidecision theoretic structural assumptions

solved four hours encouraging fact worst case
exponential examples overhead associated adds compared classical
vector methods much less trees factor compared
modified policy iteration state variables lessens become larger
tree methods yet applied real world
exact abstraction schemes clear examples resulting policies value functions may compact others set regions may get
large even reaching level individual states boutilier et al thus precluding
computational savings boutilier dearden develop approximation scheme
exploits tree structured nature value functions produced stage k
value function vk pruned produce smaller less accurate tree approximates vk specifically approximate value functions represented trees whose leaves
labeled upper lower bound value function region decisiontheoretic regression performed bounds certain subtrees value tree
pruned leaves subtree close value tree large
given computational constraints scheme nonuniform approximate adaptive
approximation scheme tailored provide roughly accurate value
function given maximum tree size smallest value function respect tree
size given minimum accuracy reported boutilier dearden
approximation small set examples including worst case examples
tree allows substantial reduction computational cost instance
variable worst case example small amount pruning introduced average error
reduced computation time factor aggressive pruning tends
increase error decrease computation time rapidly making appropriate tradeoffs
two dimensions still addressed method remains tested
evaluated realistic
structured representations solution applied
fomdps methods solving uence diagrams shachter exploit structure
natural way tatman shachter explore connection uence diagrams fomdps relationship uence diagram solution techniques
dynamic programming boutilier poole classic history independent
methods solving pomdps conversion fomdp belief states exploit types structured representations described however exploiting structured
representations pomdps remains explored depth
abstract plans

one diculties adaptive abstraction schemes suggested fact
different abstractions must constructed repeatedly incurring substantial computational overhead overhead compensated savings obtained policy
construction e g reducing number backups problematic
many cases savings dominated time space required generate
abstractions thus motivates development cheaper less accurate approximate
clustering schemes


fiboutilier dean hanks

another way reduce overhead adopt fixed abstraction scheme
one abstraction ever produced adopted classical hierarchical abstraction planners pioneered sacerdoti abstrips system sacerdoti similar form abstraction studied knoblock see
knoblock tenenberg yang work variables case propositional
ranked according criticality roughly important variables solution
abstraction constructed deleting
description set propositions low criticality solution abstract
plan achieves elements original goal deleted however
preconditions effects actions deleted accounted solution might solution original even abstract solution
used restrict search solution underlying concrete space often
hierarchies refined abstractions used propositions introduced
back domain stages
form abstraction uniform propositions deleted uniformly fixed since
abstract solution need solution might tempted view
approximate abstraction method however best think abstract
plan solution rather form heuristic information help solve
true quickly
intuitions underlying knoblock scheme applied dtp boutilier dearden variables ranked according degree uence reward
function subset important variables deemed relevant subset
determined variables uence relevant variables effects
actions determined easily strips tbn action descriptions
deemed relevant remaining variables deemed irrelevant
deleted description action reward descriptions
leaves abstract mdp smaller state space e fewer variables solved
standard methods recall state space reduction exponential number
variables removed view method uniform fixed approximate abstraction
scheme unlike output classical abstraction methods abstract policy produced
implemented value degree optimal abstract policy
true optimal policy differ value bounded priori abstraction fixed

example simple illustration suppose reward satisfying coffee requests

penalty satisfying substantially greater keeping
lab tidy delivering mail suppose time pressure requires agent focus
specific subset objectives order produce small abstract state space
case four reward laden variables see figure cr
judged important action descriptions used determine
variables directly indirectly affect probability achieving cr
cr rhc loc deemed relevant allowing rhm
ignored state space thus reduced size size addition several
action descriptions e g tidy become trivial deleted


fidecision theoretic structural assumptions

advantage abstractions easily computed incur little
overhead disadvantages uniform nature abstractions restrictive
relevant reward variables determined policy constructed
without knowledge agent ability control variables important
variables large impact reward agent
control may taken account less important variables agent actually
uence ignored however series abstractions used take
account objectives decreasing importance posteriori valuable objectives
dealt risk controllability taken account boutilier et al
policies generated abstract levels used seed value
policy iteration less abstract levels certain cases reducing time convergence
dearden boutilier suggested dearden boutilier
abstract value function used heuristic online search policies
improve abstract policy constructed discussed section thus error
approximate value function overcome extent search heuristic
function improved asynchronous updates
different use abstraction adopted drips planner haddawy suwandi
haddawy doan actions abstracted collapsing branches possible outcomes maintaining probabilistic intervals abstract disjunctive effects
actions combined decomposition hierarchy much hierarchical
task networks done evaluating abstract plans decomposition network producing ranges utility possible instantiations plans refining
plans possibly optimal use task networks means search
restricted finite horizon open loop plans action choice restricted possible refinements network task networks offer useful way encode priori heuristic
knowledge structure good plans
model minimization reduction methods

abstraction techniques defined recast terms minimizing stochastic
automaton providing unifying view different methods offering insights
abstraction process dean givan automata theory know
given finite state machine recognizing language l exists unique minimal
finite state machine recognizes l could might
exponentially smaller minimal machine called minimal
model language l captures every relevant aspect machines
said equivalent define similar notions equivalence mdps since
primarily concerned important equivalent mdps agree value
functions policies practical standpoint may necessary
minimal model reduced model suciently small still equivalent
apply idea model minimization model reduction follows
begin takes input implicit mdp model factored form
produces lucky explicit reduced model whose size within polynomial
factor size factored representation use favorite state
dynamic programming solve explicit model


fiboutilier dean hanks

think dynamic programming techniques rely structured representations discussed earlier operating reduced model without ever explicitly constructing
model cases building reduced model may appropriate
cases one might save considerable effort explicitly constructing parts
reduced model absolutely necessary
potential computational model minimization techniques sketched small minimal model may exist may hard
instead might look reduced model easier necessarily minimal could fail case might look model small enough useful
approximately equivalent original factored model careful
mean approximate intuitively two mdps approximately equivalent
corresponding optimal value functions within small factor one another
order practical mdp model reduction schemes operate directly implicit
factored representation original mdp lee yannakakis call online
model minimization online model minimization starts initial partition states
minimization iteratively refines partition splitting clusters smaller clusters
cluster split states cluster behave differently respect
transitions states clusters local property satisfied
clusters given partition model consisting aggregate states correspond
clusters partition equivalent original model addition
initial partition method splitting clusters satisfy certain properties
guaranteed minimal model case mdp reduction initial partition
groups together states reward nearly reward case
approximation methods
clusters partitions manipulated online model reduction methods represented intensionally formulas involving state variables instance formula
rhc loc represents set states robot coffee located
mail room operations performed clusters require conjoining complementing simplifying checking satisfiability worst case operations
intractable successful application methods depends critically
way represented illustrate basic idea simple
example

example figure depicts simple version running example single
action three boolean state variables corresponding rhc robot
coffee rhc cr outstanding request coffee cr
considering two location possibilities loc c robot coffee
room loc c whether outstanding coffee request depends
whether request previous stage whether robot
coffee room location depends location previous stage
reward depends whether outstanding coffee request

property required initial partition two states cluster partition
defining minimal model recall minimal model unique must cluster
initial partition



fidecision theoretic structural assumptions

st

st

cr

cr

pr cr
cr
cr
loc c
loc c




loc

loc

pr loc c

rhc

pr rhc
loc c
loc c
rhc
rhc




r


r cr
else

rhc

figure factored model illustrating model reduction techniques
cr loc c

cr
cr

cr
cr loc c


b

figure involving aggregate states model corresponding initial
partition b minimal model
initial partition shown figure defined terms immediate rewards
say states particular starting cluster behave respect
particular destination cluster probability ending destination
cluster states starting cluster property satisfied
starting cluster cr destination cluster cr figure split
cluster labeled cr obtain model figure b property satisfied
pairs clusters model figure b minimal model
lee yannakakis non deterministic finite state machines
extended givan dean handle classical strips givan dean
mdps dean givan basic step splitting cluster closely
related goal regression relationship explored givan dean variants
model reduction apply action space large represented
factored form dean givan kim example action specified
set parameters corresponding allocations several different
resources optimization exist computing approxi

fiboutilier dean hanks







r

g

c

g
p

b

b



e

b



b

c

figure reachability serial decomposition
mate dean givan leach ecient use
approximate givan leach dean

reachability analysis serial decomposition
reachability analysis

existence goal states exploited different settings instance deterministic classical regression viewed form directed dynamic
programming without uncertainty certain policy reaches goal state
dynamic programming backups need performed goal states
possible states regression therefore implicitly exploits certain reachability characteristics domain along special structure value function
reachability analysis applied much broadly forms basis types
decomposition decomposition solving mdp broken several
subprocesses solved independently roughly independently solutions
pieced together subprocesses whose solutions interact marginally treated
independent might expect good nonoptimal global solution furthermore
structure requires solution particular subproblem
needed solutions subproblems ignored need computed
instance regression analysis optimal action states cannot reach
goal region irrelevant solution classical ai shown
schematically figure regions b never explored backward
search state space states reach goal within search horizon
ever deemed relevant regions b may reachable start state
fact reach goal state means known irrelevant
system dynamics stochastic scheme form basis approximately
optimal solution method regions b ignored unlikely transition
regression goal region region r similar remarks progression forward
search start state apply illustrated figure b


fidecision theoretic structural assumptions

several schemes proposed ai literature exploiting reachability
constraints apart usual forward backward search approaches peot smith
introduce operator graph structure computed prior solving
caches reachability relationships among propositions graph consulted
process deciding actions insert plan resolve
threats
graphplan blum furst attempts blend considerations
forward backward reachability deterministic context one
diculties regression may regress goal region sequence
operators region cannot reached initial state
figure example states region r may reachable initial
state graphplan constructs variant operator graph called graph
certain forward reachability constraints posted regression implemented
usual current subgoal set violates forward reachability constraints
point subgoal set abandoned regression search backtracks
conceptually one might think graphplan constructing forward search tree
state space initial state root backward search
goal region backward tree course process state
instead constraints possible variable values hold simultaneously different
stages recorded regression used search backward
graph sense graphplan viewed constructing abstraction
forward reachable states distinguished unreachable states stage
distinction among abstract states quickly identify infeasible regression
paths note however graphplan approximates distinction overestimating
set reachable states overestimation opposed underestimation ensures
regression search space contains legitimate plans
reachability exploited solution general mdps dean
et al propose envelope method solving goal mdps approximately
assuming path generated quickly given start state goal region
mdp consisting states path perhaps neighboring states solved
deal transitions lead envelope heuristic method estimates value
states time permits set neighboring states expanded increasing
solution quality accurately evaluating quality alternative actions
ideas underlying graphplan applied general mdps
boutilier brafman geib construction graph generalized deal stochastic conditional action representation offered tbns given
initial state set initial states discovers reachability constraints
form graphplan instance two variable values x x
cannot obtain simultaneously action sequence starting
given initial state lead state values hold reachability
constraints discovered process used simplify action reward representation mdp refers reachable states case action
approximate abstraction techniques described section might used generate
heuristic information
general k ary constraints type considered boutilier et al



fiboutilier dean hanks

requires unreachable set values hold effectively deleted cases certain
variables discovered immutable given initial conditions
deleted leading much smaller mdps simplified representation retains original
propositional structure standard abstraction methods applied reachable
mdp suggested strong synergy exists abstraction reachability analysis together techniques reduce size effective mdp
solved much dramatically isolation reachability constraints used prune regression paths deterministic domains used
prune value function policy estimates generated decision theoretic regression
abstraction boutilier et al
reported boutilier et al limited single process
domain reachability analysis together abstraction provide substantial reductions size effective mdp must solved least domains
domain binary variables reachability considerations generally eliminated
order variables depending initial state arity binary
ternary constraints considered reducing state space size anywhere
incorporating abstraction reachable mdp provided considerably
reduction reducing mdp sizes ranging effectively zero states
latter case would occur discovered values variables impact reward
altered case every course action expected utility
mdp solved solved applying null actions zero cost
serial decomposition communicating structure

communicating reachability structure mdp provides way formalize different types decomposition classify mdp according markov
chains induced stationary policies admits fixed markov chain group
states maximal recurrent classes transient states described section
mdp recurrent policy induces markov chain single recurrent class
mdp unichain policy induces single recurrent class possibly transient states mdp communicating pair states policy
reach mdp weakly communicating exists closed set
states communicating plus possibly set states transient every policy
call mdps noncommunicating
notions crucial construction optimal average reward policies
exploited decomposition suppose mdp discovered consist
set recurrent classes c cn e matter policy adopted agent cannot
leave class enters class set transient states clear
optimal policy restricted class ci constructed without reference policy
decisions made states outside ci even values essentially ci
viewed independent subprocess
simple way view classes think agent adopting randomized policy action
adopted state positive probability classes induced markov chain correspond
classes mdp



fidecision theoretic structural assumptions

observation leads following suggestion optimal policy construction
solve subprocesses consisting recurrent classes mdps remove
states mdp forming reduced mdp consisting transient states
break reduced mdp recurrent classes solve independently
key effectively use value function original recurrent
states computed solving independent subproblems step take account
transitions recurrent classes reduced mdp figure c shows mdp
broken classes might constructed way original mdp classes c
e recurrent solved independently removed mdp class
recurrent reduced mdp course solved without reference classes
b rely value states transitions class e however
value function e available purpose used solve
consisted jdj states hand b solved finally
solved lin dean provide version type decomposition
employs factored representation factored representation allows dimensionality
reduction different state subspaces aggregating states differ values
irrelevant variables subspaces
key decomposition discovery recurrent classes mdp
puterman suggests adaptation fox landi fox landi
discovering structure markov chains n recall n jsj alleviate
diculties work explicit state representation boutilier
puterman propose variant works factored tbn
representation
one diculty form decomposition reliance strongly independent
subproblems e recurrent classes within mdp others explored exact approximate techniques work less restrictive assumptions one simple method
approximation construct approximately recurrent classes figure c might
imagine c e nearly independent sense transitions
low probability high cost treating independent might lead approximately optimal policies whose error bounded solutions c e interact
strongly enough solutions constructed completely independently
different solving decomposed taken
optimal value function e pointed calculate
optimal value function first thing note need know
value function states e value every state e reachable
state single step set states outside reachable single
step state inside referred states periphery values
states intersection e periphery summarize value exiting
ending e refer set states periphery block
kernel mdp different blocks interact one another
states kernel
ross varadarajan make related suggestion solving average reward
slight correction made suggested boutilier puterman



fiboutilier dean hanks

loc c

loc l

loc

loc

figure decomposition location

loc c

loc l
kernel

loc

loc

figure kernel decomposition depicting kernel states



fidecision theoretic structural assumptions

example spatial features often provide natural dimension along decom

pose domain running example location robot might used
decompose state space blocks states one block possible locations figure shows decomposition superimposed state transition
diagram mdp states kernel shaded might correspond
entrances exits locations star shaped topology induced kernel
decomposition used kushner chen dean lin illustrated
figure figure hallway location explicitly represented
simplification may reasonable hallway conduit moving
one room another case function hallway accounted
dynamics governing states kernel figures idealized given
full set features running example kernel would contain many
states

one technique computing optimal policy entire mdp involves repeatedly
solving mdps corresponding individual blocks techniques works follows
initially guess value every state kernel given current estimate
values kernel states solve component mdps solution produces
estimate states kernel adjust values states kernel
considering difference current estimates iterate
difference negligible
iterative method solving decomposed mdp special case lagrangian
method finding extrema function literature replete
methods linear nonlinear systems equations winston possible
formulate mdp linear program epenoux puterman dantzig
wolfe developed method decomposing system equations involving
large number variables set smaller systems equations interacting set
coupling variables variables shared two blocks dantzig wolfe
decomposition method original large system equations solved iteratively
solving smaller systems adjusting coupling variables iteration
adjustment required linear programming formulation mdp
values states encoded variables
kushner chen exploit fact mdps modeled linear programs
dantzig wolfe decomposition method solve mdps involving large number
states dean lin describe general framework solving decomposed mdps
pointing work kushner chen special case neither work addresses
issue decompositions come dean et al investigate methods
decomposing state space two blocks reachable k steps fewer
reachable k steps see discussion reachability set states
reachable k fewer steps used construct mdp basis policy
approximates optimal policy k increases size block states reachable
k steps increases ensuring better solution amount time required compute
ideally would aggregate kernel states value provide compact representation
remainder section however consider opportunities combining
aggregation decomposition methods



fiboutilier dean hanks

solution increases dean et al discuss methods solving mdps time critical
trading quality time
ignored issue obtain decompositions expedite calculations ideally component decomposition would yield simplification via
aggregation abstraction reducing dimensionality component thereby
avoiding explicit enumeration states lin presents methods exploiting
structure certain special cases communicating structure revealed
domain expert general however finding decomposition minimize effort
spent solving component mdps quite hard least hard finding smallest circuit consistent given input output behavior best hope
good heuristic methods unfortunately aware particularly useful
heuristics finding serial decompositions markov decision processes developing
heuristics clearly area investigation
related form decomposition development macro operators mdps
sutton macros long history classical solving fikes
hart nilsson korf recently generalized mdps
hauskrecht meuleau kaelbling dean boutilier parr parr russell
precup sutton singh stone veloso sutton thrun schwartz
work macro taken local policy region state
space block terminology given mdp comprising blocks
set macros defined block mdp solved selecting macro action
block global policy induced set macros picked close
optimal least best combination macros set available
sutton precup et al macros treated temporally abstract actions
defined macro treated single action
used policy value iteration along concrete actions hauskrecht et al
parr parr russell exploited hierarchical fashion
high level mdp consisting states lying boundaries blocks macros
actions chosen states issue macro generation
constructing set macros guaranteed provide exibility select close optimal
global behavior addressed hauskrecht et al parr relationship
serial decomposition techniques quite close thus discovering good
decompositions constructing good sets macros exploiting intensional representations
areas clearer compelling solutions required date work area
provided much computational utility solution mdps except cases
good hand crafted region decompositions macros provided little
work taken account factored nature many mdps reason
discuss detail however general notion serial decomposition continues
develop shows great promise

multiattribute reward parallel decomposition
another form decomposition parallel decomposition mdp broken
set sub mdps run parallel specifically stage global
decision process state subprocess affected instance figure action


fidecision theoretic structural assumptions



mdp



mdp



mdp

figure parallel decomposition

affects state subprocess intuitively action suitable execution

original mdp state reasonably good sub mdps
generally sub mdps form product join decomposition original
state space contrast union decompositions state space determined serial
decompositions state space formed taking cross product sub mdp state
spaces join certain states subprocesses cannot linked subprocesses
may identical action spaces figure may action space
global action choice factored choice subprocess latter
case sub mdps may completely independent case global mdp
solved exponentially faster challenging arises constraints
legal action combinations example actions subprocesses
require certain shared resources interactions global choice may arise
parallel mdp decomposition wish solve sub mdps use policies
value functions generated help construct optimal approximately optimal solution
original mdp highlighting need appropriate decompositions mdps
develop suitable merging techniques recent parallel decomposition methods
involved decomposing mdp subprocesses suitable distinct objectives since
reward functions often deal multiple objectives associated independent
reward whose rewards summed determine global reward often
natural way decompose mdps thus ideas multiattribute utility theory
seen play role solution mdps
boutilier et al decompose mdp specified tbns additive reward
function abstraction technique described section component
reward function abstraction used generate mdp referring variables
relevant component since certain state variables may present multiple
sub mdps e relevant one objective original state space join
subspaces thus decomposition tackled automatically merging tackled several
ways one involves sum value functions obtained solving sub mdps
heuristic estimate true value function heuristic used guide online
state search see section sub mdps interact heuristic
perfect leads backtrack free optimal action selection interact search
note existence factored mdp representation crucial abstraction method



fiboutilier dean hanks

required detect con icts note sub mdp identical sets actions
action space large branching factor search process may prohibitive
singh cohn deal parallel decomposition though assume
global mdp specified explicitly set parallel mdps thus generating decompositions
global mdp issue global mdp given cross product state
action spaces sub mdps reward functions summed however
constraints feasible action combinations couple solutions sub mdps
solve global mdp sum sub mdp value functions used upper bound
optimal global value function maximum global state
used lower bound bounds form basis action elimination procedure
value iteration solving global mdp unfortunately value iteration
run explicit state space global mdp since action space cross
product potential computational bottleneck value iteration well
meuleau et al use parallel decomposition approximate solution stochastic resource allocation large state action spaces much singh
cohn mdp specified terms number independent mdps
involving distinct objective whose action choices linked shared resource constraints value functions individual mdps constructed oine used
set online action selection procedures unlike many approximation procedures
discussed makes attempt construct policy explicitly
similar real time search rtdp respect construct value function
explicitly method applied large mdps state spaces size
actions spaces even larger solve roughly half
hour solutions produced approximate size precludes
exact solution good estimates solution quality hard derive however
method applied smaller nature whose exact solution
computed approximations high quality meuleau et al able
solve large mdps large factored state action spaces model
relies somewhat restrictive assumptions nature local value functions
ensure good solution quality however basic appears generalizable
offers great promise solving large factored mdps
singh cohn meuleau et al seen
rely least implicitly structured mdp representations involving almost independent
subprocesses seems likely approaches could take advantage automatic
mdp decomposition boutilier et al factored
representations explicitly play part

summary

seen number ways intensional representations exploited
solve mdps effectively without enumeration state space include techniques
abstraction mdps including relevance analysis goal regression
decision theoretic regression techniques relying reachability analysis serial decomposition methods parallel mdp decomposition exploiting multiattribute nature
singh cohn incorporate methods removing unreachable states value iteration



fidecision theoretic structural assumptions

reward functions many methods fortunate circumstances offer exponential reduction solution time space required represent policy value function
none come guarantees reductions except certain special cases
methods described provide approximate solutions often error bounds provided offer optimality guarantees general provide optimal
solutions suitable assumptions
one avenue explored detail relationship structured solution methods developed mdps described techniques used solving
bayesian networks since many discussed section rely structure inherent tbn representation mdp natural ask whether
embody intuitions underlie solution bayes nets thus
whether solution techniques bayes nets directly indirectly applied
mdps ways give rise similar discussed remains
open question point undoubtedly strong ties exist tatman shachter
explored connections uence diagrams mdps kjaerulff
investigated computational considerations involved applying join tree methods
reasoning tasks monitoring prediction temporal bayes nets abstraction methods discussed section interpreted form variable elimination
dechter zhang poole elimination variables occurs temporal order
good orderings within time slice must exploit tree graph structure
cpts approximation schemes variable elimination dechter poole
may related certain approximation methods developed mdps
independence decompositions mdps discussed section clearly viewed
exploiting independence relations made explicit unrolling tbn development connections bayes net inference doubt prove
useful enhancing understanding existing methods increasing range
applicability pointing

concluding remarks
search effective controlling automated agents long important history continue grow importance decisionmaking functionality automated work several disciplines among ai decision
analysis addressed carried different definitions different sets simplifying assumptions different viewpoints hence
different representations solving often assumptions seem made historical reasons reasons convenience
often dicult separate essential assumptions accidental important
clarify relationships among definitions crucial assumptions solution
techniques meaningful synthesis take place
analyzed approaches particular class sequential decision studied decision analysis ai literature
started general reasonably neutral statement couched convenience language markov decision processes demonstrated
disciplines define e assumptions make effect


fiboutilier dean hanks

assumptions worst case time complexity solving defined
assumptions regarding two main factors seem distinguish commonly studied
classes decision

observation sensing sensing tend fast cheap accurate laborious
costly noisy

incentive structure agent behavior evaluated ability perform
particular task ability control system interval time

moving beyond worst case analysis generally assumed although pathological cases inevitably dicult agent able solve typical easy
cases effectively agent needs able identify structure
exploit structure algorithmically
identified three ways structural regularities recognized represented
exploited computationally first structure induced domain level simplifying
assumptions full observability goal satisfaction time separable value functions
second structure exploited compact domain specific encodings states
actions rewards designer use techniques make structure explicit
decision making exploit structural regularities apply
particular hand third involves aggregation abstraction decomposition techniques whereby structural regularities discovered exploited
solving process developing framework one allows comparison
domains assumptions techniques drawn different disciplines
discover essential structure required specific representations
prove effective way insights techniques developed
certain within certain disciplines evaluated potentially applied
within disciplines
main focus work elucidation forms structure
decision exploited representationally computationally
part focused propositional structure commonly associated ai circles complete treatment would included
compact representations dynamics rewards policies value functions often
considered continuous real valued domains instance discussed linear
dynamics quadratic cost functions often used control theory caines
use neural network representations value functions frequently adopted within
reinforcement learning community bertsekas tsitsiklis tesauro
discussed partitioning continuous state spaces often addressed reinforcement
learning moore atkeson neither addressed relational quantificational structure used first order representations however even techniques
cast within framework described example use piecewise linear
value functions seen form abstraction different linear components
applied different regions clusters state space
bertsekas tsitsiklis provide depth treatment neural network linear function
approximators mdps reinforcement learning



fidecision theoretic structural assumptions

although certain cases indicated devise methods exploit several
types structure along lines limited extent
many representations described complementary
pose obstacles combination remains seen interact
techniques developed forms structure used continuous state
action spaces
analysis raises opportunities challenges understanding assumptions
techniques relationships designer decision making agents many
tools build effective solvers challenges lie development
additional tools integration existing ones

acknowledgments

many thanks careful comments referees thanks ron parr robert
st aubin comments earlier draft students taking cs
spring taught martha pollack university pittsburgh cpsc
winter university british columbia deserve thanks detailed
comments
boutilier supported nserc grant ogp nce irisii program project ic dean supported part national science foundation
presidential young investigator award iri air force advanced
projects agency department defense contract f c hanks supported part arpa rome labs grant f
part nsf grant iri

references

allen j hendler j tate eds readings morgan kaufmann
san mateo
astrom k j optimal control markov decision processes incomplete state
estimation j math anal appl
bacchus f boutilier c grove rewarding behaviors proceedings
thirteenth national conference artificial intelligence pp portland

bacchus f boutilier c grove structured solution methods nonmarkovian decision processes proceedings fourteenth national conference
artificial intelligence pp providence ri
bacchus f kabanza f temporal logic control search
forward chaining planner
proceedings third european
workshop ewsp assisi italy available via url
ftp logos uwaterloo ca pub tlplan tlplan ps z
bacchus f teh w making forward chaining relevant proceedings
fourth international conference ai systems pp pittsburgh pa


fiboutilier dean hanks

bahar r frohm e gaona c hachtel g macii e pardo somenzi
f algebraic decision diagrams applications international conference computer aided design pp ieee
baker b nonmonotonic reasoning framework situation calculus
artificial intelligence
barto g bradtke j singh p learning act real time dynamic
programming artificial intelligence
bellman r dynamic programming princeton university press princeton nj
bertsekas p castanon adaptive aggregation infinite horizon
dynamic programming ieee transactions automatic control
bertsekas p dynamic programming prentice hall englewood cliffs nj
bertsekas p tsitsiklis j n neuro dynamic programming athena belmont

blackwell discrete dynamic programming annals mathematical statistics

blum l furst l fast graph analysis proceedings
fourteenth international joint conference artificial intelligence pp
montreal canada
bonet b geffner h learning sorting decision trees pomdps
proceedings fifteenth international conference machine learning pp
madison wi
bonet b loerincs g geffner h robust fast action selection mechanism
proceedings fourteenth national conference artificial intelligence pp
providence ri
boutilier c correlated action effects decision theoretic regression proceedings thirteenth conference uncertainty artificial intelligence pp
providence ri
boutilier c brafman r geib c prioritized goal decomposition markov
decision processes toward synthesis classical decision theoretic
proceedings fifteenth international joint conference artificial intelligence
pp nagoya japan
boutilier c brafman r geib c structured reachability analysis markov
decision processes proceedings fourteenth conference uncertainty
artificial intelligence pp madison wi
boutilier c dearden r abstractions decision theoretic
time constraints proceedings twelfth national conference artificial
intelligence pp seattle wa


fidecision theoretic structural assumptions

boutilier c dearden r approximating value trees structured dynamic
programming proceedings thirteenth international conference machine
learning pp bari italy
boutilier c dearden r goldszmidt exploiting structure policy construction proceedings fourteenth international joint conference artificial
intelligence pp montreal canada
boutilier c dearden r goldszmidt stochastic dynamic programming
factored representations manuscript
boutilier c friedman n goldszmidt koller context specific independence bayesian networks proceedings twelfth conference uncertainty
artificial intelligence pp portland
boutilier c goldszmidt frame bayesian network action
representations proceedings eleventh biennial canadian conference
artificial intelligence pp toronto
boutilier c poole computing optimal policies partially observable
decision processes compact representations proceedings thirteenth
national conference artificial intelligence pp portland
boutilier c puterman l process oriented average reward optimality proceedings fourteenth international joint conference artificial
intelligence pp montreal canada
brafman r heuristic variable grid solution method pomdps proceedings fourteenth national conference artificial intelligence pp
providence ri
bryant r e graph boolean function manipulation ieee
transactions computers c
bylander computational complexity propositional strips
artificial intelligence
caines p e linear stochastic systems wiley york
cassandra r kaelbling l p littman l acting optimally partially
observable stochastic domains proceedings twelfth national conference
artificial intelligence pp seattle wa
cassandra r littman l zhang n l incremental pruning simple fast exact method pomdps proceedings thirteenth conference
uncertainty artificial intelligence pp providence ri
chapman conjunctive goals artificial intelligence


fiboutilier dean hanks

chapman kaelbling l p input generalization delayed reinforcement
learning performance comparisons proceedings twelfth
international joint conference artificial intelligence pp sydney australia
dantzig g wolfe p decomposition principle dynamic programs operations

dean allen j aloimonos artificial intelligence theory practice
benjamin cummings
dean givan r model minimization markov decision processes
proceedings fourteenth national conference artificial intelligence pp
providence ri aaai
dean givan r kim k e solving large state
action spaces proceedings fourth international conference ai
systems pp pittsburgh pa
dean givan r leach model reduction techniques computing approximately optimal solutions markov decision processes proceedings
thirteenth conference uncertainty artificial intelligence pp providence ri
dean kaelbling l kirman j nicholson deadlines
stochastic domains proceedings eleventh national conference artificial
intelligence pp
dean kaelbling l kirman j nicholson time constraints stochastic domains artificial intelligence
dean kanazawa k model reasoning persistence causation
computational intelligence
dean lin h decomposition techniques stochastic domains proceedings fourteenth international joint conference artificial
intelligence pp
dean wellman control morgan kaufmann san mateo
california
dearden r boutilier c integrating execution stochastic
domains proceedings tenth conference uncertainty artificial intelligence pp washington dc
dearden r boutilier c abstraction approximate decision theoretic artificial intelligence
dechter r bucket elimination unifying framework probabilistic inference
proceedings twelfth conference uncertainty artificial intelligence pp
portland


fidecision theoretic structural assumptions

dechter r mini buckets general scheme generating approximations
automated reasoning probabilistic inference proceedings fifteenth international joint conference artificial intelligence pp nagoya japan
epenoux f sur un probleme de production et de stockage dans l aleatoire
management science
dietterich g flann n explanation learning reinforcement
learning unified proceedings twelfth international conference
machine learning pp lake tahoe nv
draper hanks weld probabilistic model action leastcommitment information gathering proceedings tenth conference uncertainty artificial intelligence pp washington dc
draper hanks weld b probabilistic information gathering contingent execution proceedings second international conference
ai systems pp
etzioni hanks weld draper lesh n williamson
incomplete information proceedings third international conference principles knowledge representation reasoning pp
boston
fikes r hart p nilsson n learning executing generalized robot plans
artificial intelligence
fikes r nilsson n j strips application theorem
proving solving artificial intelligence
finger j exploiting constraints design synthesis ph thesis stanford university stanford
floyd r w shortest path communications acm

fox b l landi identifying ergodic subchains
transient states stochastic matrix communications acm
french decision theory halsted press york
geiger heckerman advances probabilistic reasoning proceedings
seventh conference uncertainty artificial intelligence pp los
angeles ca
givan r dean model minimization regression propositional strips
proceedings fifteenth international joint conference artificial
intelligence pp nagoya japan


fiboutilier dean hanks

givan r leach dean bounded parameter markov decision processes
proceedings fourth european conference ecp pp
toulouse france
goldman r p boddy representing uncertainty simple planners
proceedings fourth international conference principles knowledge
representation reasoning pp bonn germany
haddawy p doan abstracting probabilistic actions proceedings
tenth conference uncertainty artificial intelligence pp washington
dc
haddawy p hanks utility goal directed decision theoretic
planners computational intelligence
haddawy p suwandi decision theoretic refinement inheritence abstraction proceedings second international conference ai systems pp chicago il
hanks projecting plans uncertain worlds ph thesis yale university
department computer science ct
hanks mcdermott v modeling dynamic uncertain world symbolic
probabilistic reasoning change artificial intelligence
hanks russell wellman eds decision theoretic proceedings aaai spring symposium aaai press menlo park
hansen e zilberstein heuristic search cyclic graphs
proceedings fifteenth national conference artificial intelligence pp
madison wi
hauskrecht heuristic variable grid solution method pomdps proceedings fourteenth national conference artificial intelligence pp
providence ri
hauskrecht control stochastic domains imperfect information ph thesis massachusetts institute technology cambridge
hauskrecht meuleau n kaelbling l p dean boutilier c hierarchical
solution markov decision processes macro actions proceedings
fourteenth conference uncertainty artificial intelligence pp madison
wi
hoey j st aubin r hu boutilier c spudd stochastic
decision diagrams proceedings fifteenth conference uncertainty
artificial intelligence stockholm appear
howard r dynamic programming markov processes mit press cambridge massachusetts


fidecision theoretic structural assumptions

howard r matheson j e uence diagrams howard r matheson j e eds principles applications decision analysis strategic
decisions group menlo park ca
kambhampati refinement unifying framework plan synthesis
ai magazine summer
kearns mansour ng sparse sampling nearoptimal large markov decision processes proceedings sixteenth
international joint conference artificial intelligence stockholm appear
keeney r l raiffa h decisions multiple objectives preferences
value tradeoffs john wiley sons york
kjaerulff u computational scheme reasoning dynamic probabilistic networks proceedings eighth conference uncertainty ai pp
stanford
knoblock c generating abstraction hierarchies automated
reducing search kluwer boston
knoblock c tenenberg j yang q characterizing abstraction hierarchies proceedings ninth national conference artificial
intelligence pp anaheim ca
koenig optimal probabilistic decision theoretic markovian
decision theory sc thesis ucb csd university california berkeley
computer science department
koenig simmons r real time search nondeterministic domains
proceedings fourteenth international joint conference artificial intelligence
pp montreal canada
korf r macro operators weak method learning artificial intelligence

korf r e real time heuristic search artificial intelligence
kushmerick n hanks weld probabilistic
artificial intelligence
kushner h j chen c h decomposition systems governed markov
chains ieee transactions automatic control
lee yannakakis online minimization transition systems proceedings
th annual acm symposium theory computing pp victoria
bc
lin f reiter r state constraints revisited journal logic computation



fiboutilier dean hanks

lin h exploiting structure control ph thesis department
computer science brown university
lin h dean generating optimal policies high level plans conditional branches loops proceedings third european workshop
ewsp pp
littman l probabilistic propositional representations complexity proceedings fourteenth national conference artificial intelligence
pp providence ri
littman l dean l kaelbling l p complexity solving
markov decision proceedings eleventh conference uncertainty
artificial intelligence pp montreal canada
littman l sequential decision making ph thesis cs
brown university department computer science providence ri
lovejoy w computationally feasible bounds partially observed markov
decision processes operations
lovejoy w b survey algorithmic methods partially observed markov
decision processes annals operations
luenberger g introduction linear nonlinear programming addisonwesley reading massachusetts
luenberger g introduction dynamic systems theory applications wiley york
madani condon hanks undecidability probabilistic
infinite horizon partially observable markov decision proceedings
sixteenth national conference artificial intelligence orlando fl appear
mahadevan discount discount reinforcement learning case
study comparing r learning q learning proceedings eleventh international conference machine learning pp brunswick nj
mcallester rosenblitt systematic nonlinear proceedings
ninth national conference artificial intelligence pp anaheim ca
mccallum r instance utile distinctions reinforcement learning
hidden state proceedings twelfth international conference machine
learning pp lake tahoe nevada
mccarthy j hayes p j philosophical standpoint
artificial intelligence machine intelligence


fidecision theoretic structural assumptions

meuleau n hauskrecht kim k peshkin l kaelbling l dean boutilier c
solving large weakly coupled markov decision processes proceedings
fifteenth national conference artificial intelligence pp madison
wi
moore w atkeson c g parti game variable resolution
reinforcement learning multidimensional state spaces machine learning

papadimitriou c h tsitsiklis j n complexity markov chain decision
processes mathematics operations
parr r flexible decomposition weakly coupled markov decision
processes proceedings fourteenth conference uncertainty artificial
intelligence pp madison wi
parr r russell approximating optimal policies partially observable
stochastic domains proceedings fourteenth international joint conference
artificial intelligence pp montreal
parr r russell reinforcement learning hierarchies machines
jordan kearns solla eds advances neural information processing
systems pp mit press cambridge
pearl j probabilistic reasoning intelligent systems networks plausible
inference morgan kaufmann san mateo
pednault e adl exploring middle ground strips situation calculus proceedings first international conference principles
knowledge representation reasoning pp toronto canada
penberthy j weld ucpop sound complete partial order planner
adl proceedings third international conference principles knowledge
representation reasoning pp boston
peot smith conditional nonlinear proceedings first
international conference ai systems pp college park md
perez carbonell j g control knowledge improve plan quality
proceedings second international conference ai systems pp
chicago il
poole exploiting rule structure decision making within independent
choice logic proceedings eleventh conference uncertainty artificial
intelligence pp montreal canada
poole independent choice logic modelling multiple agents uncertainty artificial intelligence


fiboutilier dean hanks

poole b probabilistic partial evaluation exploiting rule structure probabilistic
inference proceedings fifteenth international joint conference artificial
intelligence pp nagoya japan
poole context specific approximation probabilistic inference proceedings
fourteenth conference uncertainty artificial intelligence pp
madison wi
precup sutton r singh theoretical reinforcement learning
temporally abstract behaviors proceedings tenth european conference
machine learning pp chemnitz germany
pryor l collins g cassandra contingencies technical
report northwestern university institute learning sciences
puterman l markov decision processes john wiley sons york
puterman l shin modified policy iteration discounted
markov decision management science
ross k w varadarajan r multichain markov decision processes
sample path constraint decomposition mathematics operations
russell norvig p artificial intelligence modern prentice hall
englewood cliffs nj
sacerdoti e hierarchy abstraction spaces artificial intelligence

sacerdoti e nonlinear nature plans proceedings fourth
international joint conference artificial intelligence pp
schoppers j universal plans reactive robots unpredictable environments
proceedings tenth international joint conference artificial intelligence
pp milan italy
schwartz reinforcement learning method maximizing undiscounted rewards proceedings tenth international conference machine learning
pp amherst
schweitzer p l puterman l kindle k w iterative aggregationdisaggregation procedures discounted semi markov reward processes operations

shachter r evaluating uence diagrams operations

shimony e role relevance explanation irrelevance statistical
independence international journal approximate reasoning


fidecision theoretic structural assumptions

simmons r koenig probabilistic robot navigation partially observable
environments proceedings fourteenth international joint conference
artificial intelligence pp montreal canada
singh p cohn dynamically merge markov decision processes
advances neural information processing systems pp mit press
cambridge
singh p jaakkola jordan reinforcement learning soft state
aggregation hanson j cowan j giles c l eds advances neural
information processing systems morgan kaufmann san mateo
smallwood r sondik e j optimal control partially observable
markov processes finite horizon operations
smith peot postponing threats partial order proceedings
eleventh national conference artificial intelligence pp washington dc
sondik e j optimal control partially observable markov processes
infinite horizon discounted costs operations
stone p veloso team partitioned opaque transition reinforcement learning
asada ed robocup robot soccer world cup ii springer verlag berlin
sutton r td modeling world mixture time scales
proceedings twelfth international conference machine learning pp
lake tahoe nv
sutton r barto g reinforcement learning introduction mit press
cambridge
tash j russell control strategies stochastic planner proceedings
twelfth national conference artificial intelligence pp seattle
wa
tatman j shachter r dynamic programming uence diagrams
ieee transactions systems man cybernetics
tesauro g j td gammon self teaching backgammon program achieves masterlevel play neural computation
thrun fox burgard w probabilistic concurrent mapping
localization mobile robots machine learning
thrun schwartz finding structure reinforcement learning tesauro
g touretzky leen eds advances neural information processing
systems cambridge mit press
warren generating conditional plans programs proceedings aisb
summer conference pp university edinburgh


fiboutilier dean hanks

watkins c j c h dayan p q learning machine learning
weld introduction least commitment ai magazine winter

white iii c c scherer w solutions procedures partially observed
markov decision processes operations
williamson value directed ph thesis
university washington department computer science engineering
williamson hanks optimal goal directed utility model
proceedings second international conference ai systems pp
chicago il
winston p h artificial intelligence third edition addison wesley reading
massachusetts
yang q intelligent decomposition abstraction
springer verlag
zhang n l liu w model approximation scheme partially
observable stochastic domains journal artificial intelligence
zhang n l poole exploiting causal independence bayesian network
inference journal artificial intelligence




