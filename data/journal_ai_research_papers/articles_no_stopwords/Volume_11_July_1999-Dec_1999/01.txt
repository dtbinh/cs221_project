Journal Artificial Intelligence Research 11 (1999) 95-130

Submitted 3/98; published 7/99

Semantic Similarity Taxonomy: Information-Based
Measure Application Problems Ambiguity
Natural Language
Philip Resnik

resnik@umiacs.umd.edu

Department Linguistics
Institute Advanced Computer Studies
University Maryland
College Park, MD 20742 USA

Abstract

article presents measure semantic similarity is-a taxonomy based
notion shared information content. Experimental evaluation benchmark
set human similarity judgments demonstrates measure performs better
traditional edge-counting approach. article presents algorithms take advantage taxonomic similarity resolving syntactic semantic ambiguity, along
experimental results demonstrating effectiveness.

1. Introduction
Evaluating semantic relatedness using network representations problem long
history artificial intelligence psychology, dating back spreading activation
approach Quillian (1968) Collins Loftus (1975). Semantic similarity represents
special case semantic relatedness: example, cars gasoline would seem
closely related than, say, cars bicycles, latter pair certainly similar.
Rada et al. (Rada, Mili, Bicknell, & Blettner, 1989) suggest assessment similarity
semantic networks fact thought involving taxonomic (is-a) links,
exclusion link types; view taken here, although admittedly
links part-of viewed attributes contribute similarity (cf.
Richardson, Smeaton, & Murphy, 1994; Sussna, 1993).
Although many measures similarity defined literature, seldom
accompanied independent characterization phenomenon measuring,
particularly measure proposed service computational application (e.g.,
similarity documents information retrieval, similarity cases case-based reasoning).
Rather, worth similarity measure utility given task. cognitive
domain, similarity treated property characterized human perception intuition,
much way notions \plausibility" \typicality." such, worth
similarity measure fidelity human behavior, measured predictions
human performance experimental tasks. latter view underlies work
article, although results presented comprise direct comparison human
performance practical application problems natural language processing.
natural, time-honored way evaluate semantic similarity taxonomy measure
distance nodes corresponding items compared | shorter

c 1999 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiResnik

path one node another, similar are. Given multiple paths, one
takes length shortest one (Lee, Kim, & Lee, 1993; Rada & Bicknell, 1989; Rada
et al., 1989).
widely acknowledged problem approach, however, relies
notion links taxonomy represent uniform distances. Unfortunately, uniform
link distance dicult define, much less control. real taxonomies, wide
variability \distance" covered single taxonomic link, particularly certain
sub-taxonomies (e.g., biological categories) much denser others. example,
WordNet (Miller, 1990; Fellbaum, 1998), widely used, broad-coverage semantic network
English, dicult find links cover intuitively narrow distance
(rabbit ears is-a television antenna) intuitively wide one (phytoplankton
is-a living thing). kinds examples found Collins COBUILD
Dictionary (Sinclair, ed., 1987), identifies superordinate terms many words (e.g.,
safety valve is-a valve seems much narrower knitting machine is-a machine).
first part article, describe alternative way evaluate semantic similarity taxonomy, based notion information content. edge-counting
method, conceptually quite simple. However, sensitive problem
varying link distances. addition, combining taxonomic structure empirical
probability estimates, provides way adapting static knowledge structure multiple contexts. Section 2 sets probabilistic framework defines measure
semantic similarity information-theoretic terms, Section 3 presents evaluation
similarity measure human similarity judgments, using simple edge-counting
method baseline.
second part article, Sections 4 5, describe two applications semantic
similarity problems ambiguity natural language. first concerns particular
case syntactic ambiguity involves coordination nominal compounds,
pernicious source structural ambiguity English. Consider phrase food
handling storage procedures: represent conjunction food handling storage
procedures, refer handling storage food? second application
concerns resolution word sense ambiguity | words running text,
large open problem (though cf. Wilks & Stevenson, 1996), groups related words
often discovered distributional analysis text corpora found dictionaries
thesauri. Finally, Section 6 discusses related work.

2. Similarity Information Content

Let C set concepts is-a taxonomy, permitting multiple inheritance. Intuitively,
one key similarity two concepts extent share information, indicated is-a taxonomy highly specific concept subsumes both.
edge-counting method captures indirectly, since minimal path is-a links two nodes long, means necessary go high taxonomy,
abstract concepts, order find least upper bound. example, WordNet, nickel
dime subsumed coin, whereas specific superclass nickel
credit card share medium exchange (see Figure 1). feature-based setting
(e.g., Tversky, 1977), would ected explicit shared features: nickels dimes
96

fiInformation-Based Semantic Similarity

MEDIUM EXCHANGE
MONEY
CASH

CREDIT

COIN
NICKEL

DIME

CREDIT CARD

Figure 1: Fragment WordNet taxonomy. Solid lines represent is-a links; dashed lines
indicate intervening nodes omitted save space.
small, round, metallic, on. features captured implicitly
taxonomy categorizing nickel dime subordinates coin.
associating probabilities concepts taxonomy, possible capture
idea edge-counting, avoiding unreliability edge distances. Let
taxonomy augmented function p : C ! [0; 1], c 2 C , p(c)
probability encountering instance concept c. implies p monotonically
nondecreasing one moves taxonomy: c1 is-a c2, p(c1 ) p(c2). Moreover,
taxonomy unique top node probability 1.
Following standard argumentation information theory (Ross, 1976), information content concept c quantified negative log likelihood, , log p(c).
Notice quantifying information content way makes intuitive sense setting: probability increases, informativeness decreases; abstract concept,
lower information content. Moreover, unique top concept, information
content 0.
quantitative characterization information provides new way measure semantic similarity. information two concepts share, similar are,
information shared two concepts indicated information content concepts
subsume taxonomy. Formally, define
max [, log p(c)] ;
sim(c1; c2) =
(1)
c 2 (c1 ; c2)

(c1; c2) set concepts subsume c1 c2. class achieves
maximum value Equation 1 termed informative subsumer; often
unique informative subsumer, although need true general case.
Taking maximum respect information content analogous taking first
intersection semantic network marker-passing shortest path respect edge
distance (cf. Quillian, 1968; Rada et al., 1989); generalization taking maximum
taking weighted average introduced Section 3.4.
Notice although similarity computed considering upper bounds two
concepts, information measure effect identifying minimal upper bounds, since
class less informative superordinates. example, Figure 1, coin, cash,
etc. members (nickel; dime), concept structurally minimal
97

fiResnik

PERSON
p=.2491
info=2.005

ADULT
p=.0208
info=5.584

FEMALE_PERSON
p=.0188
info=5.736

PROFESSIONAL
p=.0079
info=6.993

ACTOR1
p=.0027
info=8.522

INTELLECTUAL
p=.0113
info=6.471

DOCTOR2
p=.0005
info=10.84

NURSE2
p=.0001
info=13.17

HEALTH_PROFESSIONAL
p=.0022
info=8.844

DOCTOR1
p=.0018
info=9.093

GUARDIAN
p=.0058
info=7.434

LAWYER
p=.0007
info=10.39

NURSE1
p=.0001
info=12.94

Figure 2: Another fragment WordNet taxonomy
upper bound, coin, informative. make difference cases
multiple inheritance: two distinct ancestor nodes may minimal upper bounds,
measured using distance graph, two nodes might different
values information content. notice is-a taxonomies WordNet,
multiple sub-taxonomies unique top node, asserting zero similarity
concepts separate sub-taxonomies (e.g., liberty, aorta) equivalent unifying
sub-taxonomies creating virtual topmost concept.
practice, one often needs measure word similarity , rather concept similarity.
Using s(w) represent set concepts taxonomy senses word w,
define
wsim(w1; w2) = cmax
(2)
1; c2 [sim(c1; c2)] ;

c1 ranges s(w1) c2 ranges s(w2). consistent Rada et al.'s
(1989) treatment \disjunctive concepts" using edge-counting: define distance
two disjunctive sets concepts minimum path length element
first set element second. Here, word similarity judged taking
maximal information content concepts words could instance. take example, consider word similarity wsim(doctor, nurse) would
computed, using taxonomic information Figure 2. (Note noun senses
considered here.) Equation 2, must consider pairs concepts hc1; c2i,
c1 2 fdoctor1; doctor2g c2 2 fnurse1; nurse2g, pair must
compute semantic similarity sim(c1 ,c2) according Equation 1. Table 1 illustrates
computation.
98

fiInformation-Based Semantic Similarity

c1 (description)

c2 (description)

sim(c1 ,c2)
doctor1 (medical) nurse1 (medical) health professional
8.844
doctor1 (medical) nurse2 (nanny)
person
2.005
doctor2 (Ph.D.) nurse1 (medical)
person
2.005
doctor2 (Ph.D.) nurse2 (nanny)
person
2.005
subsumer

Table 1: Computation similarity doctor nurse
table shows, senses doctor considered senses
nurse, maximum value 8.844, via health professional informative
subsumer; is, therefore, value word similarity doctor nurse.1

3. Evaluation

section describes simple, direct method evaluating semantic similarity, using
human judgments basis comparison.

3.1 Implementation

work reported used WordNet's taxonomy concepts represented nouns (and
compound nominals) English.2 Frequencies concepts taxonomy estimated
using noun frequencies Brown Corpus American English (Francis & Kucera,
1982), large (1,000,000 word) collection text across genres ranging news articles
science fiction. noun occurred corpus counted occurrence
taxonomic class containing it.3 example, Figure 1, occurrence noun
dime would counted toward frequency dime, coin, cash, forth. Formally,
X
freq(c) =
count(n);
(3)
n2words(c)
words(c) set words subsumed concept c. Concept probabilities
computed simply relative frequency:
(4)
p^(c) = freq(c) ;

N

N total number nouns observed (excluding subsumed
WordNet class, course). Naturally frequency estimates Equation 3 would
1. taxonomy Figure 2 fragment WordNet version 1.6, showing real quantitative information
computed using method described below. \nanny" sense nurse (nursemaid, woman
custodian children) primarily British usage. example omits two senses doctor
WordNet: theologian Roman Catholic Church, game played children. WordNet
use node labels doctor1, created labels sake readability.
2. Concept used refers Miller et al. (1990) call synset, essentially node taxonomy.
experiment reported section used noun taxonomy WordNet version 1.4,
approximately 50,000 nodes.
3. Plural nouns counted instances singular forms.

99

fiResnik

improved taking account intended sense noun corpus |
example, instance crane bird machine, both. Sense-tagged
corpora generally available, however, frequency estimates done using
weaker generally applicable technique.
noted present method associating probabilities concepts
taxonomy based notion single random variable ranging concepts
| case, \credit" noun occurrence would distributed
concepts noun, counts normalized across entire taxonomy sum 1.
(That approach taken Resnik, 1993a, see Resnik, 1998b discussion.)
assigning taxonomic probabilities purposes measuring semantic similarity, present
model associates separate, binomially distributed random variable concept.4
is, perspective given concept c, observed noun either
instance concept, probabilities p(c) 1 , p(c), respectively. Unlike
model single multinomial variable ranging entire set concepts,
formulation assigns probability 1 top concept taxonomy, leading
desirable consequence information content zero.

3.2 Task

Although standard way evaluate computational measures semantic similarity, one reasonable way judge would seem agreement human similarity ratings.
assessed using computational similarity measure rate similarity
set word pairs, looking well ratings correlate human ratings
pairs.
experiment Miller Charles (1991) provided appropriate human subject data
task. study, 38 undergraduate subjects given 30 pairs nouns
chosen cover high, intermediate, low levels similarity (as determined using
previous study, Rubenstein & Goodenough, 1965), subjects asked
rate \similarity meaning" pair scale 0 (no similarity) 4 (perfect
synonymy). average rating pair thus represents good estimate similar
two words are, according human judgments.5
order get baseline comparison, replicated Miller Charles's experiment,
giving ten subjects 30 noun pairs. subjects computer science graduate
students postdoctoral researchers University Pennsylvania, instructions
exactly used Miller Charles, main difference
replication subjects completed questionnaire electronic mail (though
instructed complete whole task single uninterrupted sitting). Five subjects
received list word pairs random order, five received list
reverse order. correlation Miller Charles mean ratings mean
ratings replication .96, quite close .97 correlation Miller Charles
obtained results ratings determined earlier study.
4. similar spirit way probabilities used Bayesian network.
5. anonymous reviewer points human judgments task may uenced prototypicality, e.g., pair bird/robin would likely yield higher ratings bird/crane. Issues kind
brie touched Section 6, part ignored since prototypicality,
topical relatedness, captured is-a taxonomies.

100

fiInformation-Based Semantic Similarity

subject replication, computed well ratings correlated
Miller Charles ratings. average correlation 10 subjects
r = 0:88, standard deviation 0.08.6 value represents upper bound
one expect computational attempt perform task.
purposes evaluation, three computational similarity measures used.
first similarity measurement using information content proposed previous section. second variant edge-counting method, converting distance
similarity subtracting path length maximum possible path length:


wsimedge (w1; w2) = (2 max) , cmin
len(c1; c2)
;c
1

2



(5)

c1 ranges s(w1), c2 ranges s(w2), max maximum depth taxonomy, len(c1; c2) length shortest path c1 c2 . (Recall s(w)
denotes set concepts taxonomy represent senses word w.) senses
w1 w2 separate sub-taxonomies WordNet similarity taken
zero. Note correlation used evaluation metric, conversion
distance similarity viewed expository convenience, affect
results: although sign correlation coecient changes positive negative,
magnitude turns regardless whether minimum path
length subtracted (2 max).
third point comparison measure simply uses probability concept,
rather information content, define semantic similarity concepts
max [1 , p(c)]
simp(c)(c1; c2) =
(6)
c 2 (c1 ; c2)

corresponding measure word similarity:

h



wsimp(c)(w1; w2) = cmax
simp(c)(c1; c2) ;
;c
1

2

(7)

c1 ranges s(w1) c2 ranges s(w2) Equation 7. probability-based
similarity score included order assess extent similarity judgments might
sensitive frequency per se rather information content. Again, difference
maximizing 1 , p(c) minimizing p(c) turns affect magnitude
correlation. simply ensures value interpreted similarity value,
high values indicating similar words.

3.3 Results

Table 2 summarizes experimental results, giving correlation similarity
ratings mean ratings reported Miller Charles. Note that, owing noun
missing WordNet 1.4 taxonomy, possible obtain computational
similarity ratings 28 30 noun pairs; hence proper point comparison
human judgments correlation 30 items (r = :88), rather correlation
28 included pairs (r = :90). similarity ratings item given Table 3.
6. Inter-subject correlation replication, estimated using leaving-one-out resampling (Weiss & Kulikowski, 1991), r = :90; stdev = 0:07.

101

fiResnik

Similarity method
Correlation
Human judgments (replication) r = :9015
Information content
r = :7911
Probability
r = :6671
Edge-counting
r = :6645
Table 2: Summary experimental results.
Word Pair
car
gem
journey
boy
coast
asylum
magician
midday
furnace
food
bird
bird
tool
brother
crane
lad
journey
monk
food
coast
forest
monk
coast
lad
chord
glass
noon
rooster

automobile
jewel
voyage
lad
shore
madhouse
wizard
noon
stove
fruit
cock
crane
implement
monk
implement
brother
car
oracle
rooster
hill
graveyard
slave
forest
wizard
smile
magician
string
voyage

Miller Charles Replication
wsim wsimedge wsimp(c)
means
means
3.92
3.9 8.0411
30 0.9962
3.84
3.5 14.9286
30 1.0000
3.84
3.5 6.7537
29 0.9907
3.76
3.5 8.4240
29 0.9971
3.70
3.5 10.8076
29 0.9994
3.61
3.6 15.6656
29 1.0000
3.50
3.5 13.6656
30 0.9999
3.42
3.6 12.3925
30 0.9998
3.11
2.6 1.7135
23 0.6951
3.08
2.1 5.0076
27 0.9689
3.05
2.2 9.3139
29 0.9984
2.97
2.1 9.3139
27 0.9984
2.95
3.4 6.0787
29 0.9852
2.82
2.4 2.9683
24 0.8722
1.68
0.3 2.9683
24 0.8722
1.66
1.2 2.9355
26 0.8693
1.16
0.7 0.0000
0 0.0000
1.10
0.8 2.9683
24 0.8722
0.89
1.1 1.0105
18 0.5036
0.87
0.7 6.2344
26 0.9867
0.84
0.6 0.0000
0 0.0000
0.55
0.7 2.9683
27 0.8722
0.42
0.6 0.0000
0 0.0000
0.42
0.7 2.9683
26 0.8722
0.13
0.1 2.3544
20 0.8044
0.11
0.1 1.0105
22 0.5036
0.08
0.0 0.0000
0 0.0000
0.08
0.0 0.0000
0 0.0000
Table 3: Semantic similarity item.
102

fiInformation-Based Semantic Similarity

n1
tobacco
tobacco
tobacco

n2
wsim(n1 ,n2) subsumer
alcohol
7.63
drug
sugar
3.56 substance
horse
8.26 narcotic

Table 4: Similarity tobacco computed maximizing information content

3.4 Discussion

experimental results previous section suggest measuring semantic similarity
using information content provides results better traditional method
simply counting number intervening is-a links.
measure without problems, however. simple edge-counting,
measure sometimes produces spuriously high similarity measures words basis
inappropriate word senses. example, Table 4 shows word similarity several words
tobacco. Tobacco alcohol similar, drugs, tobacco sugar
less similar, though entirely dissimilar, since classified substances.
problem arises, however, similarity rating tobacco horse: word horse
used slang term heroin, result information-based similarity maximized,
path length minimized, two words categorized narcotics.
contrary intuition.
Cases probably relatively rare. However, example illustrates
general concern: measuring similarity words, really relationship among
word senses matters, similarity measure able take account.
absence reliable algorithm choosing appropriate word senses,
straightforward way information-based setting consider concepts
nouns belong rather taking single maximally informative class.
suggests defining measure weighted word similarity follows:
wsimff(w1; w2) =

X



ff(ci)[, log p(ci)];

(8)

fcig set concepts dominating w1 w2 sense either word,
weighting function concepts Pi ff(ci) = 1. measure similarity

takes information account previous one: rather relying
single concept maximum information content, allows class representing shared
properties contribute information content according value ff(ci). Intuitively,
values measure relevance. example, computing wsimff (tobacco,horse),
ci would range concepts tobacco horse instances, including
narcotic, drug, artifact, life form, etc. everyday context one might expect low
values ff(narcotic) ff(drug), context of, say, newspaper article
drug dealers, weights concepts might quite high. Although possible
include weighted word similarity comparison Section 3, since noun pairs
judged without context, Section 4 provides discussion weighting function
designed particular natural language processing task.
103

fiResnik

4. Using Taxonomic Similarity Resolving Syntactic Ambiguity

considered direct evaluation information-based semantic similarity measure,
turn application measure resolving syntactic ambiguity.

4.1 Clues Resolving Coordination Ambiguity

Syntactic ambiguity pervasive problem natural language. Church Patil
(1982) point out, class \every way ambiguous" syntactic constructions |
number analyses number binary trees terminal elements |
includes frequent constructions prepositional phrases, coordination, nominal
compounds. last several years, researchers natural language made great
deal progress using quantitative information text corpora provide needed
constraints. Progress broad-coverage prepositional phrase attachment ambiguity
particularly notable, dominant approach shifted structural
strategies quantitative analysis lexical relationships (Whittemore, Ferrara, & Brunner,
1990; Hindle & Rooth, 1993; Brill & Resnik, 1994; Ratnaparkhi & Roukos, 1994; Li & Abe,
1995; Collins & Brooks, 1995; Merlo, Crocker, & Berthouzoz, 1997). Noun compounds
received comparatively less attention (Kobayasi, Takunaga, & Tanaka, 1994; Lauer, 1994,
1995), problem coordination ambiguity (Agarwal & Boggess, 1992; Kurohashi
& Nagao, 1992).
section, investigate role semantic similarity resolving coordination
ambiguities involving nominal compounds. began noun phrase coordinations
form n1 n2 n3, admit two structural analyses, one n1 n2
two noun phrase heads conjoined (1a) one conjoined heads n1
n3 (1b).
(1) a. (bank warehouse) guard
b. (policeman) (park guard)
Identifying two head nouns conjoined necessary order arrive correct
interpretation phrase's content. example, analyzing (1b) according structure (1a) could lead machine translation system produce noun phrase describing
somebody guards policemen parks. Analyzing (1a) according structure (1b) could lead information retrieval system miss phrase looking
queries involving term bank guard.
Kurohashi Nagao (1992) point similarity form similarity meaning
important cues conjoinability. English, similarity form great extent
captured agreement number (singular vs. plural):
(2)

a. several business university groups
b. several businesses university groups
Similarity form candidate conjoined heads thus thought Boolean
variable: number agreement either satisfied candidate heads not.
Similarity meaning conjoined heads appears play important role:
(3)

a. television radio personality
104

fiInformation-Based Semantic Similarity

b. psychologist sex researcher
Clearly television radio similar television personality; correspondingly psychologist researcher. similarity meaning captured well semantic
similarity taxonomy, thus second variable consider evaluating coordination structure semantic similarity measured overlap information content
two head nouns.
addition, constructions considered here, appropriateness noun-noun
modification relevant:
(4)

a. mail securities fraud
b. corn peanut butter

One reason prefer conjoin mail securities mail fraud salient compound
nominal phrase. hand, corn butter familiar concept; compare
change perceived structure phrase corn peanut crops. order measure
appropriateness noun-noun modification, use quantitative measure selectional
fit called selectional association (Resnik, 1996), takes account lexical cooccurrence frequencies semantic class membership WordNet taxonomy. Brie y,
selectional association word w WordNet class c given
p(cjw) log p(cjw)
A(w; c) = D(p(C jw) k p(p(Cc) ))

(9)

D(p1 k p2) Kullback-Leibler distance (relative entropy) probability
distributions p1 p2. Intuitively, A(w, c) measuring extent class c
predicted word w; example, A(wool, clothing) would higher value than,
say, A(wool, person). selectional association A(w1; w2) two words defined
maximum A(w1; c) taken classes c w2 belongs. example,
A(wool, glove) would likely equal A(wool, clothing), compared to, say,
A(wool, sports equipment) | latter value corresponding sense glove
something used baseball boxing. (See Li & Abe, 1995, approach
selectional relationships modeled using conditional probability.) simple way treat
selectional association variable resolving coordination ambiguities prefer analyses include noun-noun modifications strong anities (e.g., bank modifier
guard) disprefer weak noun-noun relationships (e.g., corn modifier
butter). Thresholds defining \strong" \weak" parameters algorithm, defined
below.

4.2 Resolving Coordination Ambiguity: First Experiment

investigated roles sources evidence conducting straightforward disambiguation experiment using naturally occurring linguistic data. Two sets 100 noun
phrases form [NP n1 n2 n3] extracted parsed Wall Street Journal
(WSJ) corpus, found Penn Treebank (Marcus, Santorini, & Marcinkiewicz, 1993).
disambiguated hand, one set used development
105

fiResnik

Source evidence Conjoined Condition
Number agreement n1 n2 number(n1) = number(n2) number(n1) 6= number(n3)
n1 n3 number(n1) = number(n3) number(n1) 6= number(n2)
undecided otherwise
Semantic similarity n1 n2 wsim(n1,n2) > wsim(n1,n3)
n1 n3 wsim(n1,n3) > wsim(n1,n2)
undecided otherwise
Noun-noun
n1 n2 A(n1,n3) > A(n3,n1) >
modification
n1 n3 A(n1,n3) < A(n3,n1) <
undecided otherwise

Table 5: Rules number agreement, semantic similarity, noun-noun modification
resolving syntactic ambiguity noun phrases n1 n2 n3
testing.7 set simple transformations applied WSJ data, including mapping proper names token someone, expansion month abbreviations,
reduction nouns root forms.
Number agreement determined using simple analysis suxes combination
WordNet's lists root nouns irregular plurals.8 Semantic similarity determined using information-based measure Equation (2) | noun class probabilities
Equation (1) estimated using sample approximately 800,000 noun occurrences
Associated Press newswire stories.9 purpose determining semantic similarity,
nouns WordNet treated instances class hthingi. Appropriateness
noun-noun modification determined computing selectional association (Equation 9),
using co-occurrence frequencies taken sample approximately 15,000 noun-noun
compounds extracted WSJ corpus. (This sample include test data.)
selection modifier head selection head modifier
considered disambiguation algorithm. Table 5 provides details decision rule
source evidence used independently.10
addition, investigated several methods combining three sources information. included: (a) simple form \backing off" (specifically, given number
agreement, noun-noun modification, semantic similarity strategies order, use
choice given first strategy isn't undecided); (b) taking vote among
three strategies choosing majority; (c) classifying using results linear re7. Hand disambiguation necessary Penn Treebank encode NP-internal structure.
phrases disambiguated using full sentence occurred, plus previous
following sentence, context.
8. experiments section used WordNet version 1.2.
9. grateful Donald Hindle making data available.
10. Thresholds = 2:0 = 0:0 fixed manually based experience development set
evaluating test data.

106

fiInformation-Based Semantic Similarity

Strategy

Default
Number agreement
Noun-noun modification
Semantic similarity
Backing
Voting
Number agreement + default
Noun-noun modification + default
Semantic similarity + default
Backing + default
Voting + default
Regression
ID3 Tree

Coverage (%) Accuracy (%)

100.0
53.0
75.0
66.0
95.0
89.0
100.0
100.0
100.0
100.0
100.0
100.0
100.0

66.0
90.6
69.3
71.2
81.1
78.7
82.0
65.0
72.0
81.0
76.0
79.0
80.0

Table 6: Syntactic disambiguation items form n1 n2 n3
gression; (d) constructing decision tree classifier. latter two methods forms
supervised learning; experiment development set used training data.11
results shown Table 6. development set contained bias favor
conjoining n1 n2; therefore \default" strategy, always choosing bracketing,
used baseline comparison. default used resolving undecided cases
order make comparisons individual strategies 100% coverage. example,
\Number agreement + default" shows figures obtained number agreement
used make choice default selected choice undecided.
surprisingly, individual strategies perform reasonably well instances
classify, coverage poor; strategy based similarity form highly
accurate, arrives answer half time. However, heavy priori bias makes
difference | extent even though forms evidence
value, combination beats number-agreement-plus-default combination.
positive side, shows ambiguity resolved reasonably well using
simple algorithm: viewed terms many errors made, number agreement makes
possible cut baseline 34% error rate nearly half 18% incorrect analyses (a
44% reduction). negative side, results fail make strong case semantic
similarity add something useful.
taking issue, let us assess contributions individual strategies
results evidence combined, analyzing behavior unsupervised
evidence combination strategies. combining evidence voting, choice made
89 cases. number agreement strategy agreed majority vote 57 cases,
43 (75.4%) correct; noun-noun modification strategy agreed
majority 73 cases, 50 (68.5%) correct; semantic similarity strategy
11. calling \backing off" related spirit Katz's well known smoothing technique (Katz,
1987), \backing off" strategy used quantitative. retain double quotes order
highlight distinction.

107

fiResnik

agreed majority 58 cases, 43 (74.1%) correct. \backing off"
form evidence combination, number agreement makes choice 53 cases correct
48 (90.6%); then, remaining undecided, noun-noun modification makes choice
35 cases correct 24 (68.6%); then, still undecided, semantic similarity
makes choice 7 cases 5 correct (71.4%); remaining 5 cases
undecided.
analysis above-baseline performance semantic-similarity-plus-default
strategy show semantic similarity contain information correct answer:
agrees majority vote substantial portion time, selects correct
answers often one would expect default cases receives
\backing off." However, default correct two thirds time,
number agreement strategy correct nine ten times cases decide,
potential contribution semantic similarity remains suggestive rather conclusive.
second experiment, therefore, investigated dicult formulation problem
order obtain better assessment.

4.3 Resolving Coordination Ambiguity: Second Experiment

second experiment using data sources, investigated complex set
coordinations, looking noun phrases form n0 n1 n2 n3. syntactic
analyses phrases characterized top-level binary choice data
previous experiment, either conjoining heads n1 n2 (5) conjoining n1
n3 (6).12
(5) a. freshman ((business marketing) major)
b. (food (handling storage)) procedures
c. ((mail fraud) bribery) charges
(6)

a. Clorets (gum (breath mints))
b. (baby food) (puppy chow)

experiment, one set 89 items extracted Penn Treebank WSJ data
development, another set 89 items set aside testing. development
set showed significantly less bias data previous experiment, 53.9%
items conjoining n1 n2.
disambiguation strategies experiment refined version
used previous experiment, illustrated Table 7. Number agreement used
before. However, rather employing semantic similarity noun-noun modification
independent strategies | something clearly warranted given lackluster performance modification strategy | two combined measure weighted
semantic similarity defined Equation (8). Selectional association used basis
ff. particular, ff1;2(c) greater A(n0,c) A(n3,c), capturing fact
n1 n2 conjoined, combined phrase potentially stands head-modifier
relationship n0 modifier-head relationship n3 . Correspondingly, ff1;3(c)
greater A(n0,c) A(n2,c), capturing fact coordination n1
12. full 5-way classification problem structures (5) (6) investigated.

108

fiInformation-Based Semantic Similarity

Source evidence Conjoined Condition
Number agreement n1 n2 number(n1) = number(n2) number(n1) 6= number(n3)
n1 n3 number(n1) = number(n3) number(n1) 6= number(n2)
undecided otherwise
Weighted semantic n1 n2 wsimff1 2 (n1,n2) > wsimff1 3 (n1,n3)
similarity
n1 n3 wsimff1 3 (n1,n3) > wsimff1 2 (n1,n2)
undecided otherwise
;

;

;

;

Table 7: Rules number agreement weighted semantic similarity resolving syntactic ambiguity noun phrases n0 n1 n2 n3
Strategy

Default
Number agreement
Weighted semantic similarity
Backing

Coverage (%) Accuracy (%)

100.0
40.4
69.7
85.4

44.9
80.6
77.4
81.6

Table 8: Syntactic disambiguation items form n0 n1 n2 n3
n3 takes place context n2 modifying n3 n1 (or coordinated phrase
containing it) modified n0.
example, consider instance ambiguous phrase:
(7)

telecommunications products services units.

happens high-information-content connection exists product
sense \a quantity obtained multiplication" unit sense \a single undivided
whole." result, although neither senses relevant example, nouns n1
n3 would assigned high value (unweighted) semantic similarity chosen
incorrectly conjoined heads example. However, unweighted similarity computation misses important piece context: syntactic analysis conjoining product
unit (cf. examples 6a 6b), word telecommunications necessarily modifier
concept identified products. selectional association telecommunications products \multiplication" sense weak nonexistent. Weighting
selection association, therefore, provides way reduce impact spurious senses
similarity computation.
order combine sources evidence, used \backing off" (from number agreement
weighted semantic similarity) combine two individual strategies. baseline,
results evaluated simple default strategy always choosing group
common development set. results shown Table 8.
case, default strategy defined using development set misleading,
yielding worse chance accuracy. reason, strategy-plus-default figures
reported. However, even default choices made using bias found test set,
109

fiResnik

accuracy would 55.1%. contrast equivocal results first experiment,
experiment demonstrates clear contribution semantic similarity: employing
semantic similarity cases accurate number-agreement strategy
cannot apply, possible obtain equivalent even somewhat better accuracy
number agreement alone time doubling coverage.
Comparison previous algorithms unfortunately possible, since researchers
coordination ambiguity established common data set evaluation even
common characterization problem, contrast now-standard (v, n1, prep, n2)
contexts used work propositional phrase attachment. crucial caveat,
nonetheless interesting note results obtained broadly consistent
Kurohashi Nagao (1992), report accuracy results range 80-83%
100% coverage analyzing broad range conjunctive structures Japanese using
combination string matching, syntactic similarity, thesaurus-based similarity,
Agarwal Boggess (1992), use syntactic types structure, along partly
domain-dependent semantic labels, obtain accuracies similar range identifying
conjuncts English.

5. Using Taxonomic Similarity Word Sense Selection
section considers application semantic similarity measure resolving another
form ambiguity: selecting appropriate sense noun appears context
nouns related meaning.

5.1 Associating Word Senses Noun Groupings
Knowledge groups related words plays role many natural language applications.
examples, query expansion using related words well studied technique information
retrieval (e.g., Harman, 1992; Grefenstette, 1992), clusters similar words play
role smoothing stochastic language models speech recognition (Brown, Della Pietra,
deSouza, Lai, & Mercer, 1992), classes verbs share semantic structure form
basis approach interlingual machine translation (Dorr, 1997), clusterings
related words used characterizing subgroupings retrieved documents largescale Web searches (e.g., Digital Equipment Corporation, 1998). wide body
research use distributional methods measuring word similarity order
obtain groups related words (e.g., Bensch & Savitch, 1992; Brill, 1991; Brown et al., 1992;
Grefenstette, 1992, 1994; McKeown & Hatzivassiloglou, 1993; Pereira, Tishby, & Lee, 1993;
Schutze, 1993), thesauri WordNet another source word relationships (e.g.,
Voorhees, 1994).
Distributional techniques sometimes good job identifying groups related
words (see Resnik, 1998b, overview critical discussion), tasks
relevant relationships among words, among word senses. example, Brown
et al. (1992) illustrate notion distributionally derived, \semantically sticky" cluster
using automatically derived word group containing attorney, counsel, trial, court,
judge. Although semantic coherence cluster \pops out" human reader,
naive computational system defense word sense ambiguity: using cluster
110

fiInformation-Based Semantic Similarity

query expansion could result retrieving documents involving advice (one sense
counsel) royalty (as one sense court).13
Resnik (1998a) introduces algorithm uses taxonomically-defined semantic similarity order derive grouping relationships among word senses grouping relationships among words. Formally, problem stated follows. Consider set words
W = fw1; : : :; wng, word wi associated
set Si = fsi;1 ; : : :; si;mg posS
sible senses. Assume exists set W 0 Si , representing set word
senses ideal human judge would conclude belong group senses corresponding word grouping W . (It follows W 0 must contain least one representative
Si .) goal define membership function ' takes si;j , wi ,
W arguments computes value [0; 1], representing confidence
one state sense si;j belongs sense grouping W 0 . Note that, principle, nothing
precludes possibility multiple senses word included W 0.
example, consider group
attorney, counsel, trial, court, judge.
Restricting attention noun senses WordNet, every word attorney polysemous.
Treating word group W , good algorithm computing ' assign value
1 unique sense attorney, assign high value sense counsel

lawyer pleads cases court.
Similarly, assign high values senses trial
legal proceedings consisting judicial examination issues competent tribunal
determination person's innocence guilt due process law.
assign high values senses court
assembly conduct judicial business
room law court sits.
assign high value sense judge
public ocial authorized decide questions brought court justice.
assign low values ' various word senses words cluster
associated group lesser extent all. would include sense
counsel
direction advice decision course action;
similarly, low value ' assigned senses court
13. See Krovetz Kroft, 1992 Voorhees, 1993 experimentation discussion effects
word sense ambiguity information retrieval.

111

fiResnik

Algorithm (Resnik, 1998a). Given W = fw1 ; : : : ; w g, set nouns:
n

j = 1 n, < j
f

v = wsim(w , w )
c = informative subsumer w w
i;j



j

i;j



j

k = 1 num senses(w )
c ancestor sense
increment support[i, k] v


i;j

i;k

i;j

k = 1 num senses(w )
c ancestor sense
increment support[j, k'] v
0

j

j;k 0

i;j

i;j

increment normalization[i] v
increment normalization[j] v

i;j

g

i;j

= 1 n
k = 1 num senses(w )
f



(normalization[i] > 0.0)
' = support[i, k] / normalization[i]
else
' = 1 / num senses(w )
i;k

g

i;k



Figure 3: Disambiguation algorithm noun groupings
yard wholly partly surrounded walls buildings.
disambiguation algorithm noun groups given Figure 3. Intuitively, two
polysemous words similar, informative subsumer provides information
sense word relevant one. observation similar spirit
approaches word sense disambiguation based maximizing relatedness meaning (e.g.,
Lesk, 1986; Sussna, 1993). key idea behind algorithm consider nouns
word group pairwise. pair algorithm goes possible combinations
words' senses, assigns \credit" senses basis shared information content,
measured using information content informative subsumer.14
example, WordNet lists doctor meaning either medical doctor someone
holding Ph.D., lists nurse meaning either health professional nanny,
two words considered together, medical sense word obvious
human reader. effect finds parallel operation algorithm. Given
taxonomy Figure 2, consider case set W words contains w1 =
doctor, w2 = nurse, w3 = actor. first pairwise comparison, doctor nurse,
14. Figure 3, square bracket notation highlights fact support matrix normalization
array. Conceptually v c (triangular) matrices also; however, use subscripts rather
square brackets implementation time need implement since
values v c used discarded pass double loop.
i;j

i;j

112

fiInformation-Based Semantic Similarity

informative subsumer c1;2 = health professional, information
content v1;2 = 8.844. Therefore support doctor1 nurse1 incremented
8.844. Neither doctor2 nurse2 receives increment support based
comparison, since neither health professional ancestor. second pairwise
comparison, informative subsumer doctor actor c1;3 = person,
information content v1;3 = 2.005, increment amount
support doctor1, doctor2, actor1, person ancestor.
Similarly, third pairwise comparison, informative subsumer nurse
actor person, nurse1, nurse2, actor1 support incremented
2.005. end, therefore, doctor1 received support 8:884 + 2:005
possible 8:884 + 2:005 pairwise comparisons participated,
word sense ' = 1. contrast, doctor2 received support amount 2.005
possible 8:884 + 2:005 comparisons involved, value '
2:005 = 0:185.
doctor2 8:884+2
:005
Resnik (1998a) illustrates algorithm Figure 3 using word groupings variety
sources, including several sources distributional clustering cited above,
evaluates algorithm rigorously task associating WordNet senses
nouns Roget's thesaurus, based thesaurus category membership. average,
algorithm achieved approximately 89% performance human annotators performing
task.15 remainder section describe new application
algorithm, evaluate performance.

5.2 Linking WordNet using Bilingual Dictionary
Multilingual resources natural language processing dicult obtain, although
promising efforts underway projects EuroWordNet (Vossen, 1998).
many languages, however, large-scale resources unlikely available
near future, individual research efforts continue build scratch
adapt existing resources bilingual dictionaries (e.g., Klavans & Tzoukermann,
1995). section describe application algorithm Figure 3 English
definitions CETA Chinese-English dictionary (CETA, 1982). ultimate task,
undertaken context Chinese-English machine translation project,
associate Chinese vocabulary items nodes WordNet, much way
vocabulary Spanish, Dutch, Italian associated interlingual taxonomy nodes
derived American WordNet, EuroWordNet project; task similar
attempts relate dictionaries thesauri monolingually (e.g., see Section 5.3
Ji, Gong, & Huang, 1998). present study investigates extent semantic
similarity might useful partially automating process.
15. task performed independently two human judges. Treating Judge 1 benchmark
accuracies achieved Judge 2, algorithm, random selection respectively 65.7%, 58.6%,
34.8%; treating Judge 2 benchmark accuracies achieved Judge 1, algorithm,
random selection respectively 68.6%, 60.5%, 33.3%. relatively low accuracies human
judges demonstrate, disambiguation using WordNet's fine-grained senses quite bit dicult
disambiguation level homographs (Hearst, 1991; Cowie, Guthrie, & Guthrie, 1992). Resnik
Yarowsky (1997, 1999) discuss implications WordNet's fine-grainedness evaluation word
sense disambiguation, consider alternative evaluation methods.

113

fiResnik

example, consider following dictionary entries:
(a)
: 1. hliti brother-in-law (husband's elder brother) 2. hregi father 3.
hregi uncle (father's elder brother) 4. uncle (form address older
man)
: actress, player female roles.
(b)
order associate Chinese terms WordNet noun taxonomy,
important avoid associations inappropriate senses | example, word
, clearly associated father WordNet senses Church
entry (a),
Father, priest, God-the-Father, founding father.16
Although one traditional approach using dictionary entries compute word
overlap respect dictionary definitions (e.g., Lesk, 1986), English glosses
CETA dictionary generally short take advantage word overlap fashion.
However, many definitions useful property: possess multiple subdefinitions similar meaning, cases illustrated above. Although one
cannot always assume so, e.g.,
(c)
: 1. case (i.e., upper case lower case) 2. dial (of watch, etc.),
inspection dictionary confirms multiple definitions present tend
toward polysemy homonymy.
Based observation, conducted experiment assess extent
word sense disambiguation algorithm Figure 3 used identify relevant noun senses
WordNet Chinese words CETA dictionary, using English definitions
source similar nouns disambiguate. Nouns heading definitional noun phrases
extracted automatically via simple heuristic methods, randomly-selected sample
100 dictionary entries containing multiple definitions used test set. example,
noun groups associated definitions would
(a') uncle, brother-in-law, father
(b') actress, player.
WordNet's noun database used automatically identify compound nominals
possible. So, example, word defined \record player" would compound
record player rather player head noun record player compound
noun known WordNet.17
noted attempt made exclude dictionary entries (c)
creating test set. Since general way automatically identify alternative
definitions distinguished synonymy distinguished homonymy, entries
must faced disambiguation algorithm task.
Two independent judges recruited assistance annotating test set, one
native Chinese speaker, second Chinese language expert United States
government. judges independently annotated 100 test items. item,
16. Annotations within dictionary entries
ignored algorithm described section.
17. WordNet version 1.5 used experiment.

<lit>

114

(literary),

<reg>

(regional),

fiInformation-Based Semantic Similarity

WordNet definition, see 6 boxes: 1, 2, 3, 4, 5, is-a. definition:
think Chinese word meaning, select number corresponding
confidence choice, 1 lowest confidence 5 highest confidence.
Chinese word cannot meaning, specific meaning, select is-a.
example, Chinese word means \truck" WordNet definition \automotive vehicle:
self-propelled wheeled vehicle", would select option. (That is, makes sense say
Chinese word describes concept KIND automotive vehicle.) pick 1,
2, 3, 4, 5 confidence decision, 1 lowest confidence 5 highest
confidence.
neither cases apply WordNet definition, don't check anything
definition.

Figure 4: Instructions human judges selecting senses associated Chinese words
judge given Chinese word, full CETA dictionary definition (as examples
a{c), list WordNet sense descriptions associated sense head
noun associated noun group. example, list corresponding following
dictionary definition
(d)
: urgent message, urgent dispatch
would contain following WordNet sense descriptions, generated via head nouns
message dispatch:
message, content, subject matter, substance: communication
something
dispatch, expedition, expeditiousness, fastness: subconcept celerity, quickness, rapidity
dispatch, despatch, communique: ocial report (usually sent haste)
message: communication (usually brief) written spoken
signaled; \he sent three-word message"
dispatch, despatch, shipment: act sending something
dispatch, despatch: murder execution someone
item, judge first asked whether knew Chinese word meaning;
response negative, instructed proceed next item. items
known words, judges instructed Figure 4.
Although use is-a selection used analysis results,
important include provided judges way indicate
Chinese word could best classified WordNet noun taxonomy, without
assert translational equivalence Chinese concept close WordNet (English)
concept. So, example, judge could classify word
(the spring festival, lunar
new year, Chinese new year) belonging WordNet sense glossed
festival: day period time set aside feasting celebration,
115

fiResnik

sensible choice given \Chinese New Year" appear WordNet
concept. Annotating is-a relationship set important algorithm evaluated working groups head nouns, thereby potentially losing
information pointing specific concept reading. example, definition
: steel tube, steel pipe
(e)
would given algorithm group containing head nouns tube pipe.
test set annotated, evaluation done according two paradigms:
selection filtering. paradigms assume entry test set,
annotator correctly specified WordNet senses considered correct,
incorrect. algorithm tested set must identify, listed
sense, whether sense included item whether excluded.
example, WordNet sense corresponding \the murder execution someone"
would identified annotator incorrect (d), algorithm marking
\included" penalized.
selection paradigm, goal identify WordNet senses include.
therefore define precision paradigm
correctly included senses
Pselection = number
(10)
number included senses
recall
correctly included senses :
Rselection = number
(11)
number correct senses
correspond directly use precision recall information retrieval. Precision begins set senses included method, computes proportion
correct. Recall begins set senses included,
computes proportion method actually managed choose.
Since number potential WordNet senses item quite large, equally
valid alternative selection paradigm call filtering paradigm, according
goal identify WordNet senses exclude. One easily imagine
relevant paradigm | example, semi-automated setting one
wishes reduce burden user selecting among alternatives. filtering paradigm
one define filtering precision
correctly excluded senses
Pfiltering = number
number excluded senses

(12)

correctly excluded senses :
Rfiltering = number
number senses labeled incorrect

(13)

filtering recall

filtering paradigm, precision begins set senses method filtered
computes proportion correctly filtered out. recall filtering
begins set senses excluded (i.e. incorrect ones)
computes proportion method actually managed exclude.
116

fiInformation-Based Semantic Similarity

Sense Selection
Sense Filtering
Precision (%) Recall (%) Precision (%) Recall (%)
Random
29.5
31.2
88.0
87.1
Algorithm
36.9
69.9
93.8
79.3
Judge 2
54.8
55.6
91.9
91.7
Table 9: Evaluation using Judge 1 reference standard, considering items selected
confidence 3 above.
Judge 2
Algorithm
Random
Include Exclude Include Exclude Include Exclude
Judge 1 Include
40
32
58
25
26
57
Exclude
33
363
99
380
61
418
Table 10: Agreement disagreement Judge 1
Table 9 shows precision/recall figures using judgments Judge 1, native
Chinese speaker, reference standard, considering known items selected confidence 3 above.18 algorithm recorded 100 items known, confidence
values scaled linearly continuous values range [0,1] discrete values 1
5. table shows algorithm's results choice thresholded confidence 3,
Figure 5 shows recall precision vary confidence threshold changes.
lower bound comparison, algorithm implemented considered word
sense item, selecting sense probabilistically (with complete confidence)
way make average number senses per item close possible average
number senses per item reference standard (1.3 senses). Figures random
baseline average 10 runs. Table 10 illustrates choices underlying
figures; example, 26 senses random procedure chose include
included Judge 1.
fact Judge 2 low precision recall selection indicates
matching choices independent judge indeed dicult task. unsurprising, given previous experience problem selecting among WordNet's fine-grained
senses (Resnik, 1998a; Resnik & Yarowsky, 1997). results clearly show algorithm better baseline, indicate overgenerating senses,
hurts selection precision. terms filtering, algorithm chooses filter
sense tends reliably (filtering precision). However, propensity toward overgeneration ected below-baseline performance filtering recall; is, algorithm
choosing allow senses filtering out.
18. Judge 1, native speaker Chinese, identified 65 words known him; Judge 2 identified
69. on-line dictionary constructed large variety lexical resources, includes great
many uncommon words, archaic usages, regionalisms, like.

117

fiResnik

Filtering: Algorithm
Human
Random
Selection: Algorithm
Human
Random

Precision

1
0.8
0.6
0.4
0.2

0.2

0.4

0.6

0.8
Recall

1

Figure 5: Precision/recall curves using Judge 1 reference standard, varying confidence threshold
pattern results suggests best use algorithm present level
performance would filter lexical acquisition process human
loop, dividing candidate WordNet senses dictionary entries according higher
lower priority. Chinese-English dictionary entries serve appropriate input
algorithm (of approximately 37000 CETA dictionary), WordNet
sense selected algorithm confidence least equal 3
demoted lower priority group presentation alternatives, since algorithm's
choice exclude sense correct approximately 93% time. senses
selected algorithm necessarily included | human judge still
needed make selection, since selection precision low | algorithm tends
err side caution, correct senses found higher priority group
70% time.

5.3 Linking WordNet English Dictionary/Thesaurus

results WordNet sense selection using bilingual dictionary demonstrate
algorithm Figure 3 good job assigning low scores WordNet senses
filtered out, even probably trusted make categorical decisions. One
application proposed suitable, therefore, helping identify senses
filtered within semi-automated process lexical acquisition. describe closely
related, real-world application algorithm deployed: adding pointers
WordNet on-line dictionary/thesaurus Web.
context application Wordsmyth English Dictionary-Thesaurus (WEDT,
http://www.wordsmyth.net/), on-line educational dictionary aliated ARTFL
text database project (http://humanities.uchicago.edu/ARTFL/; Morrissey, 1993).
designed useful educational contexts, and, part design,
integrates thesaurus within structure dictionary. illustrated Figure 6,
118

fiInformation-Based Semantic Similarity

bar

SYL:
PRO:
POS:
DEF:
EXA:
EXA:
EXA:
SYN:
SIM:
DEF:
SYN:
SIM:
..
.

bar1
bar
noun
1. length solid material, usu. rectangular cylindrical:
bar soap;
candy bar;
iron bar.
rod (1), stick1 (1,2,3)
pole1 , shaft, stake1 , ingot, block, rail1 , railing,
crowbar, jimmy, lever
2. anything acts restraint hindrance.
block (10), hindrance (1), obstruction (1), impediment (1),
obstacle, barrier (1,3), stop (5)
barricade, blockade, deterrent, hurdle, curb, stumbling
block, snag, jam1 , shoal1 , reef1 , sandbar

Figure 6: Example Wordsmyth English Dictionary-Thesaurus (WEDT)
WEDT contains traditional dictionary information, part speech, pronunciation,
definitional information, many cases includes pointers synonyms (SYN)
similar words (SIM). Within on-line dictionary, thesaurus items hyperlinks
| example, stake1 link first WEDT entry stake | parenthetical
numbers refer specific definitions within entry.
thesaurus-like grouping similar words provides opportunity exploit
algorithm disambiguating noun groupings automatically linking WEDT entries
WordNet. value linking two resources comes compatability,
properties thesaurus dictionary, well complementarity: beyond alternative source definitional information lists synonyms,
WordNet provides ordering word senses frequency, estimates word familiarity, partof relationships, course overall taxonomic organization illustrated Figures 1
2. Figure 7 shows taxonomic information presented using WordNet Web
server (http://www.cogsci.princeton.edu/cgi-bin/webwn/).
collaboration WEDT ARTFL, taken noun entries
WEDT dictionary and, grouping similar words, added set experimental
hyperlinks WordNet entries WordNet Web server. Figure 8 shows experimental WordNet links (XWN) look WEDT user. Links WordNet senses,
pole1, appear together confidence level assigned sense disambiguation algorithm; senses confidence less threshold presented.19 XWN
hyperlink selected user, WordNet taxonomic information selected sense
appears parallel browser window, Figure 7.
window, user entry point capabilities WordNet
web server. example, one might choose look WordNet senses pole
19. current threshold, 0.1, chosen manually. may sub-optimal found works
well practice.

119

fiResnik

Sense 1
pole
(a long (usually round) rod wood metal plastic)
=> rod
(a long thin implement made metal wood)
=> implement
(a piece equipment tool used effect end)
=> instrumentality, instrumentation
(an artifact (or system artifacts)
instrumental accomplishing end)
=> artifact, artefact
(a man-made object)
=> object, physical object
(a physical (tangible visible) entity; ``it
full rackets, balls objects'')
=> entity, something
(anything existence (living nonliving))

Figure 7: WordNet entry (hypernyms) pole1
bar

SYL:
PRO:
POS:
DEF:
EXA:
EXA:
EXA:
SYN:
SIM:
XWN:
DEF:
SYN:
SIM:
XWN:

..
.

bar1
bar
noun
1. length solid material, usu. rectangular cylindrical:
bar soap;
candy bar;
iron bar.
rod (1), stick1 (1,2,3)
pole1 , shaft, stake1 , ingot, block, rail1 , railing,
crowbar, jimmy, lever
pole1 (0.82) ingot1 (1.00) block1 (0.16) rail1 (0.39)
railing1 (1.00) crowbar1 (1.00)
jimmy1 (1.00) lever1 (0.67) lever2(0.23) lever3(0.15)
2. anything acts restraint hindrance.
block (10), hindrance (1), obstruction (1), impediment (1),
obstacle, barrier (1,3), stop (5)
barricade, blockade, deterrent, hurdle, curb, stumbling
block, snag, jam1 , shoal1 , reef1 , sandbar
barricade1 (1.00) barricade2(1.00) blockade1 (0.25)
blockade2 (0.75) deterrent1 (1.00) hurdle1 (0.50)
hurdle2 (0.43) curb1 (0.56) curb2 (0.56)
curb3 (0.29) curb4 (0.44) stumbling block1 (1.00)
snag1 (1.00) jam1 (0.27) shoal1 (0.23) shoal2(0.91)
reef1 (1.00) sandbar1 (1.00)

Figure 8: Example WEDT experimental WordNet links
120

fiInformation-Based Semantic Similarity

1. pole { (a long (usually round) rod wood metal plastic)
2. Pole { (a native inhabitant Poland)
3. pole { (one two divergent mutually exclusive opinions; \they opposite poles" \they
poles apart")
4. perch, rod, pole { ((British) linear measure 16.5 feet)
5. perch, rod, pole { (a square rod land)
6. pole, celestial pole { (one two points intersection Earth's axis celestial sphere)
7. pole { (one two antipodal points Earth's axis rotation intersects Earth's surface)
8. terminal, pole { (a point electrical device (such battery) electric current enters
leaves)
9. pole { (a long fiberglass implement used pole vaulting)
10. pole, magnetic pole { (one two ends magnet magnetism seems concentrated)

Figure 9: List WordNet senses pole
noun, displayed Figure 9. Notice user WEDT simply gone directly
WordNet server look pole, full list 10 senses would appeared
indication potentially related WEDT dictionary entry
consideration. contrast, WEDT hyperlinks, introduced via sense selection
algorithm, filter majority irrelevant senses provide user measure
confidence selecting among remain.
Although formal evaluation WEDT/WordNet connection attempted,
results bilingual dictionary experiment suggest application word
sense disambiguation | filtering least relevant senses, leaving user
loop | task sense disambiguation algorithm well suited.
supported user feedback XWN feature WEDT, favorable
(Robert Parks, personal communication). site growing popularity,
current estimate 1000-1500 hits per day.

6. Related Work
extensive literature measuring similarity general, word similarity
particular; classic paper see Tversky (1977). Recent work information retrieval
computational linguistics emphasized distributional approach, words
represented vectors space features similarity measures defined
terms vectors; see Resnik (1998b) discussion, Lee (1997) good recent
example. Common traditional distributional approaches idea word
concept representations include explicit features, whether features specified
knowledge-based fashion (e.g., dog might features mammal, loyal) defined
terms distributional context (e.g., dog might features \observed within 5
words howl). representational assumption contrasts assumptions embodied
taxonomic representation, often is-a relation stands nondecomposed concepts. two inconsistent, course, since concepts taxonomy
121

fiResnik

sometimes decomposed explicit features, is-a relation, usually
interpreted, implies inheritance features whether explicit implicit.
respect, traditional approach counting edges viewed particularly simple
approximation similarity measure based counting feature differences,
assumption edge exists indicate difference least one feature.
Information-theoretic concepts techniques have, recent years, emerged
speech recognition community find wide application natural language processing; e.g.,
see Church Mercer (1993). information event fundamental notion
stochastic language modeling speech recognition, contribution correct
word prediction based conditional probability, p(wordjcontext), measured
information conveyed prediction, , log p(wordjcontext). forms basis
standard measures language model performance, cross entropy. Frequency
shared unshared features long factor computing similarity vector representations. inverse document frequency (idf) term weighting information
retrieval makes use logarithmic scaling, serves identify terms discriminate well among different documents, concept similar spirit idea
terms low information content (Salton, 1989).
Although counting edges is-a taxonomies seems something many people
tried, seem published descriptions attempts directly evaluate
effectiveness method. number researchers attempted make use
conceptual distance information retrieval. example, Rada et al. (1989, 1989) Lee
et al. (1993) report experiments using conceptual distance, implemented using edgecounting metric, basis ranking documents similarity query. Sussna
(1993) uses semantic relatedness measured WordNet word sense disambiguation,
defining measure distance weights different types links explicitly takes
depth taxonomy account.
Following original proposal measure semantic similarity taxonomy using
information content (Resnik, 1993b, 1993a), number related proposals explored. Leacock Chodorow (1994) define measure resembling information content,
using normalized path length two concepts compared rather
probability subsuming concept. Specifically, define
2
min
len(c1 ; c2) 3
c
;
c
(14)
wsimndist (w1; w2) = , log 4 1 (22 max) 5 :
(The notation Equation (5).) addition definition,
include several special cases, notably avoid infinite similarity c1
c2 exact synonyms thus path length 0. Leacock Chodorow
experimented measure information content measure described
context word sense disambiguation, found yield roughly similar results.
Implementing method testing task reported Section 3, found
actually outperformed information-based measure slightly data set; however,
follow-up experiment using different larger set noun pairs (100 items),
information-based measure performed significantly better (Table 11).
Analyzing differences two studies illuminating. follow-up experiment, used netnews archives gather highly frequent nouns within related topic areas
122

fiInformation-Based Semantic Similarity

Similarity method
Correlation
Information content
r = :6894
Leacock Chodorow r = :4320
Edge-counting
r = :4101
Table 11: Summary experimental results follow-up study.
(to ensure similar noun pairs occurred) selected noun pairings random (in
order avoid biasing follow-up study favor either algorithm). is, therefore,
predominance low-similarity noun pairs test data. Looking distribution
ratings noun pairs, given two measures, evident Leacock
Chodorow measure overestimating semantic similarity many predominantly
non-similar pairs. stands reason since measure identical whenever edge
distance identical, regardless whether pair high low taxonomy (e.g.,
distance plant animal distance white oak red
oak). contrast, information-based measure sensitive difference, better
avoiding spuriously high similarity values non-similar pairs. related note,
edge-counting measure used follow-up study variant computes path length
virtual top node, rather asserting zero similarity words path
connecting existing WordNet taxonomy, done previously. Using data
set follow-up study, information-based measure, r = :6894, significantly
better either edge-counting variants (r = :4101 r = :2777); going back
original Miller Charles data, virtual-top-node variant significantly better
assert-zero edge distance measure, correlation r = :7786 approaching
measure based information content. comparison follow-up
study original Miller Charles data illustrates quite clearly utility
similarity measure depend upon distribution items given task.
Lin (1997, 1998) recently proposed alternative information-theoretic similarity
measure, derived set basic assumptions similarity style reminiscent
way entropy/information formal definition derivable set
basic properties (Khinchin, 1957). Formally, Lin defines similarity taxonomy as:


log p( Ci)
simLin(c1; c2) = log2 p(
c1) + log p(c2)

(15)

Ci \maximally specific superclasses" ofT c1 c2 . Although
possibility multiple inheritance makes intersection Ci necessary principle, multiple inheritance fact rare WordNet practice one computes Equation (15)
separately common ancestor Ci , using p(Ci) numerator, takes
maximum (Dekang Lin, p.c.). multiplicative constant 2, therefore,
Lin's method determining similarity taxonomy essentially information-based
similarity measure Equation 1, normalized combined information content
two concepts assuming independence. Put another way, Lin's measure taking
123

fiResnik

Similarity method Correlation
Information content r = :7947
simWu&Palmer
r = :8027
simLin
r = :8339
Table 12: Summary Lin's results comparing alternative similarity measures
account commonalities differences items compared,
expressing information-theoretic terms.
Lin's measure theoretically well motivated elegantly derived. Moreover, Lin points
measure definition yield value simLin(x; x) regardless
identity x | unlike information content, criticized grounds
value self-similarity depends specific concept x is, two nonidentical items x rated similar third item z
(Richardson et al., 1994). cognitive perspective, however, similarity comparisons
involving self-similarity (\Robins similar robins"), well subclass relationships
(\Robins similar birds"), criticized psychologists anomalous (Medin, Goldstone, & Gentner, 1993). Moreover, experimental evidence human
judgments suggests identical objects judged equally similar, consistent
information-content measure proposed contrary Lin's measure. example, objects identical complex, twins, seem similar
objects identical simple, two instances simple geometric shape (Goldstone, 1999; Tversky, 1977). would appear, therefore, insofar
fidelity human judgments relevant, experimentation needed evaluate
competing predictions alternative similarity measures.
Wu Palmer (1994) propose similarity measure based edge distances,
related Lin's measure way takes account specific node dominating
c1 c2, characterizing commonalities, normalizing way accounts
differences. Revising Wu Palmer's notation slightly, measure is:
c3)
(16)
simWu&Palmer(c1; c2) = d(2c ) +d(d(
c2)
1
c3 maximally specific superclass c1 c2 , d(c3) depth, i.e. distance
root taxonomy, d(c1) d(c2) depths c1 c2 path
c3.
Lin (1998) repeats experiment Section 3 information content measure,
simLin, simWu&Palmer, reporting results appear Table 12. Lin uses sensetagged corpus estimate frequencies, smoothed probabilities rather simple relative frequency. results show somewhat higher correlation simLin
measures. experimentation needed order assess alternative measures,
particularly respect competing predictions variability performance
across data sets. seems clear, however, measures perform better
traditional edge-counting measure.
124

fiInformation-Based Semantic Similarity

7. Conclusions
article presented measure semantic similarity is-a taxonomy, based
notion information content. Experimental evaluation performed using large,
independently constructed corpus, independently constructed taxonomy, previously
existing new human subject data, results suggest measure performs
encouragingly well significantly better traditional edge-counting approach. Semantic similarity, measured using information content, shown useful
resolving cases two pervasive kinds linguistic ambiguity. resolving coordination
ambiguity, measure employed capture intuition similarity meaning
one indicator two words conjoined; suggestive results first experiment
bolstered unequivocal results second study, demonstrating significant improvements disambiguation strategy based syntactic agreement. resolving word
sense ambiguity, semantic similarity measure used assign confidence values
word senses nouns within thesaurus-like groupings. formal evaluation provided evidence technique produce useful results better suited semi-automated
sense filtering categorical sense selection. Application technique dictionary/thesaurus World Wide Web provides demonstration method action
real-world setting.

Acknowledgements
Sections 1-3 article comprise revised extended version Resnik (1995).
Section 4 describes previously presented algorithms data (Resnik, 1993b, 1993a), extended discussion analysis. Section 5 summarizes algorithm described
Resnik (1998a), extends previous results presenting new applications
algorithm, Section 5.2 containing formal evaluation new setting Section 5.3
giving real-world illustration approach put practice. Section 6
adds substantial discussion related work authors taken place since
information-based similarity measure originally proposed.
Parts research done University Pennsylvania partial
support IBM Graduate Fellowship grants ARO DAAL 03-89-C-0031, DARPA
N00014-90-J-1863, NSF IRI 90-16592, Ben Franklin 91S.3078C-1; parts research
done Sun Microsystems Laboratories Chelmsford, Massachusetts; parts
work supported University Maryland Department Defense
contract MDA90496C1250, DARPA/ITO Contract N66001-97-C-8540, Army Research
Laboratory contract DAAL03-91-C-0034 Battelle, research grant Sun
Microsystems Laboratories. author gratefully acknowledges comments three
anonymous JAIR reviewers helpful discussions John Kovarik, Claudia Leacock,
Dekang Lin, Johanna Moore, Mari Broman Olsen, Jin Tong, well comments
criticism received various presentations work.

References
Agarwal, R., & Boggess, L. (1992). simple useful approach conjunct identifica125

fiResnik

tion. Proceedings 30th Annual Meeting Association Computational
Linguistics, pp. 15{21. Association Computational Linguistics.
Bensch, P. A., & Savitch, W. J. (1992). occurrence-based model word categorization.
Presented 3rd Meeting Mathematics Language (MOL3).
Brill, E. (1991). Discovering lexical features language. Proceedings 29th
Annual Meeting Association Computational Linguistics, Berkeley, CA.
Brill, E., & Resnik, P. (1994). rule-based approach prepositional phrase attachment disambiguation. Proceedings 15th International Conference Computational
Linguistics (COLING-94).
Brown, P. F., Della Pietra, V. J., deSouza, P. V., Lai, J. C., & Mercer, R. L. (1992).
Class-based n-gram models natural language. Computational Linguistics, 18 (4),
467{480.
CETA (1982). Chinese Dictionaries: Extensive Bibliography Dictionaries Chinese
Languages. Chinese-English Translation Assistance Group, Greenwood
Publishing.
Church, K. W., & Mercer, R. (1993). Introduction special issue computational
linguistics using large corpora. Computational Linguistics, 19 (1), 1{24.
Church, K. W., & Patil, R. (1982). Coping syntactic ambiguity put
block box table. American Journal Computational Linguistics, 8 (3-4),
139{149.
Collins, A., & Loftus, E. (1975). spreading activation theory semantic processing.
Psychological Review, 82, 407{428.
Collins, M., & Brooks, J. (1995). Prepositional phrase attachment backed-off
model. Third Workshop Large Corpora. Association Computational
Linguistics. cmp-lg/9506021.
Cowie, J., Guthrie, J., & Guthrie, L. (1992). Lexical disambiguation using simulated annealing. Proceedings 14th International Conference Computational Linguistics
(COLING-92), pp. 359{365 Nantes, France.
Digital Equipment Corporation (1998).

AltaVista web page: Refine, cow9?..
http://altavista.digital.com/av/content/about_our_technology_cow9.htm.

Dorr, B. J. (1997). Large-Scale Dictionary Construction Foreign Language Tutoring
Interlingual Machine Translation. Machine Translation, 12 (4), 271{322.
Fellbaum, C. (Ed.). (1998). WordNet: Electronic Lexical Database. MIT Press.
Francis, W. N., & Kucera, H. (1982). Frequency Analysis English Usage: Lexicon
Grammar. Houghton Miin, Boston.
126

fiInformation-Based Semantic Similarity

Goldstone, R. L. (1999). Similarity. MIT Encyclopedia Cognitive Sciences. MIT
Press, Cambridge, MA.
Grefenstette, G. (1992). Use syntactic context produce term association lists text
retrieval. Proceedings Fifteenth Annual International ACM SIGIR Conference Research Development Information Retrieval, pp. 89{97.
Grefenstette, G. (1994). Explorations Automatic Thesaurus Discovery. Kluwer, Boston.
Harman, D. (1992). Relevance feedback revisited. Proceedings Fifteenth Annual
International ACM SIGIR Conference Research Development Information
Retrieval, pp. 1{10.
Hearst, M. (1991). Noun homograph disambiguation using local context large corpora.
Proceedings 7th Annual Conference University Waterloo Centre
New OED Text Research Oxford.
Hindle, D., & Rooth, M. (1993). Structural ambiguity lexical relations. Computational
Linguistics, 19 (1), 103{120.
Ji, D., Gong, J., & Huang, C. (1998). Combining Chinese thesaurus Chinese
dictionary. COLING-ACL '98, pp. 600{606. Universite de Montreal.
Katz, S. M. (1987). Estimation probabilities sparse data language model
component speech recognizer. IEEE Transactions Acoustics, Speech Signal
Processing, ASSP-35 (3), 400{401.
Khinchin, A. I. (1957). Mathematical Foundations Information Theory. New York: Dover
Publications. Translated R. A. Silverman M. D. Friedman.
Klavans, J. L., & Tzoukermann, E. (1995). Dictionaries Corpora: Combining Corpus
Machine-Readable Dictionary Data Building Bilingual Lexicons. Machine
Translation, 10, 185{218.
Kobayasi, Y., Takunaga, T., & Tanaka, H. (1994). Analysis Japanese compound nouns
using collocational information. Proceedings 15th International Conference
Computational Linguistics (COLING-94).
Krovetz, R., & Croft, W. B. (1992). Lexical ambiguity information retrieval. ACM
Transactions Information Systems, 10 (2), 115{141.
Kurohashi, S., & Nagao, M. (1992). Dynamic programming method analyzing conjunctive structures Japanese. Proceedings 14th International Conference
Computational Linguistics (COLING-92) Nantes, France.
Lauer, M. (1994). Conceptual association compound noun analysis. Proceedings
32nd Annual Meeting Association Computational Linguistics Las Cruces,
New Mexico. Student Session.
Lauer, M. (1995). Designing Statistical Language Learners: Experiments Noun Compounds. Ph.D. thesis, Macquarie University, Sydney, Australia.
127

fiResnik

Leacock, C., & Chodorow, M. (1994). Filling sparse training space word sense
identification. ms.
Lee, J. H., Kim, M. H., & Lee, Y. J. (1993). Information retrieval based conceptual
distance IS-A hierarchies. Journal Documentation, 49 (2), 188{207.
Lee, L. (1997). Similarity-based approaches natural language processing. Tech. rep.
TR-11-97, Harvard University. Doctoral dissertation. cmp-lg/9708011.
Lesk, M. (1986). Automatic sense disambiguation using machine readable dictionaries:
tell pine cone ice cream cone. Proceedings 1986 SIGDOC
Conference, pp. 24{26.
Li, H., & Abe, N. (1995). Generalizing case frames using thesaurus MDL principle.
Proceedings International Conference Recent Advances NLP Velingrad,
Bulgaria.
Lin, D. (1997). Using syntactic dependency local context resolve word sense ambiguity. Proceedings 35th Annual Meeting Association Computational
Linguistics 8th Conference European Chapter Association Computational Linguistics Madrid, Spain.
Lin, D. (1998). information-theoretic definition similarity. Proceedings
Fifteenth International Conference Machine Learning (ICML-98) Madison, Wisconsin.
Marcus, M. P., Santorini, B., & Marcinkiewicz, M. (1993). Building large annotated
corpus English: Penn Treebank. Computational Linguistics, 19, 313{330.
McKeown, K., & Hatzivassiloglou, V. (1993). Augmenting lexicons automatically: Clustering semantically related adjectives. Bates, M. (Ed.), ARPA Workshop Human
Language Technology. Morgan Kaufmann.
Medin, D., Goldstone, R., & Gentner, D. (1993). Respects similarity. Psychological
Review, 100 (2), 254{278.
Merlo, P., Crocker, M., & Berthouzoz, C. (1997). Attaching multiple prepositional phrases:
Generalized backed-off estimation. Proceedings Second Conference Empirical Methods Natural Language Processing (EMNLP-2). cmp-lg/9710005.
Miller, G. (1990). WordNet: on-line lexical database. International Journal Lexicography, 3 (4). (Special Issue).
Miller, G. A., & Charles, W. G. (1991). Contextual correlates semantic similarity. Language Cognitive Processes, 6 (1), 1{28.
Morrissey,
R.
(1993). Texts contexts: ARTFL database French studies. Profession 93,
27{33. http://humanities.uchicago.edu/homes/publications/romoart.html.
128

fiInformation-Based Semantic Similarity

Pereira, F., Tishby, N., & Lee, L. (1993). Distributional clustering English words. Proceedings 31st Annual Meeting Association Computational Linguistics
(ACL-93) Morristown, New Jersey. Association Computational Linguistics.
Quillian, M. R. (1968). Semantic memory. Minsky, M. (Ed.), Semantic Information
Processing. MIT Press, Cambridge, MA.
Rada, R., & Bicknell, E. (1989). Ranking documents thesaurus. JASIS, 40 (5),
304{310.
Rada, R., Mili, H., Bicknell, E., & Blettner, M. (1989). Development application
metric semantic nets. IEEE Transaction Systems, Man, Cybernetics,
19 (1), 17{30.
Ratnaparkhi, A., & Roukos, S. (1994). maximum entropy model prepositional phrase
attachment. Proceddings ARPA Workshop Human Language Technology
Plainsboro, NJ.
Resnik, P. (1993a). Selection Information: Class-Based Approach Lexical Relationships.
Ph.D.
thesis,
University

Pennsylvania.
(ftp://ftp.cis.upenn.edu/pub/ircs/tr/93-42.ps.Z).
Resnik, P. (1993b). Semantic classes syntactic ambiguity. Proceedings 1993
ARPA Human Language Technology Workshop. Morgan Kaufmann.
Resnik, P. (1995). Using information content evaluate semantic similarity taxonomy.
Proceedings 14th International Joint Conference Artificial Intelligence
(IJCAI-95). (cmp-lg/9511007).
Resnik, P. (1996). Selectional constraints: information-theoretic model computational realization. Cognition, 61, 127{159.
Resnik, P. (1998a). Disambiguating noun groupings respect Wordnet senses.
Armstrong, S., Church, K., Isabelle, P., Tzoukermann, E., & Yarowsky, D. (Eds.),
Natural Language Processing Using Large Corpora. Kluwer.
Resnik, P. (1998b). WordNet class-based probabilities. Fellbaum, C. (Ed.), WordNet:
Electronic Lexical Database. MIT Press.
Resnik, P., & Yarowsky, D. (1997). perspective word sense disambiguation methods
evaluation. ANLP Workshop Tagging Text Lexical Semantics
Washington, D.C.
Resnik, P., & Yarowsky, D. (1999). Distinguishing systems distinguishing senses: New
evaluation methods word sense disambiguation. Natural Language Engineering.
(to appear).
Richardson, R., Smeaton, A. F., & Murphy, J. (1994). Using WordNet knowledge base measuring semantic similarity words. Working paper CA1294, Dublin City University, School Computer Applications, Dublin, Ireland.
ftp://ftp.compapp.dcu.ie/pub/w-papers/1994/CA1294.ps.Z.
129

fiResnik

Ross, S. (1976). First Course Probability. Macmillan.
Rubenstein, H., & Goodenough, J. (1965). Contextual correlates synonymy. CACM,
8 (10), 627{633.
Salton, G. (1989). Automatic Text Processing. Addison-Wesley.
Schutze, H. (1993). Word space. Hanson, S. J., Cowan, J. D., & Giles, C. L. (Eds.), Advances Neural Information Processing Systems 5, pp. 895{902. Morgan Kaufmann
Publishers, San Mateo CA.
Sinclair (ed.), J. (1987). Collins COBUILD English Language Dictionary. Collins: London.
Sussna, M. (1993). Word sense disambiguation free-text indexing using massive semantic network. Proceedings Second International Conference Information
Knowledge Management (CIKM-93) Arlington, Virginia.
Tversky, A. (1977). Features similarity. Psychological Review, 84, 327{352.
Voorhees, E. M. (1993). Using WordNet disambiguate word senses text retrieval.
Korfhage, R., Rasmussen, E., & Willett, P. (Eds.), Proceedings Sixteenth Annual
International ACM SIGIR Conference Research Development Information
Retrieval, pp. 171{180.
Voorhees, E. M. (1994). Query expansion using lexical-semantic relations. 17th International Conference Research Development Information Retrieval (SIGIR
'94) Dublin, Ireland.
Vossen, P. (1998). Special issue EuroWordNet. Computers Humanities, 32 (2/3).
Weiss, S. M., & Kulikowski, C. A. (1991). Computer systems learn: classification
prediction methods statistics, neural nets, machine learning, expert systems.
Morgan Kaufmann, San Mateo, CA.
Whittemore, G., Ferrara, K., & Brunner, H. (1990). Empirical study predictive powers
simple attachment schemes post-modifier prepositional phrases. Proceedings
28th Annual Meeting Association Computational Linguistics, pp. 23{30.
Pittsburgh, Pennsylvania.
Wilks, Y., & Stevenson, M. (1996). grammar sense: word-sense tagging much
part-of-speech tagging?.. Technical Report CS-96-05, cmp-lg/9607028.
Wu, Z., & Palmer, M. (1994). Verb Semantics Lexical Selection. Proceedings
32nd Annual Meeting Association Computational Linguistics Las Cruces,
New Mexico.

130


