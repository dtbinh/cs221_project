Journal Artificial Intelligence Research 11 (1999) 1{94

Submitted 09/98; published 07/99

Decision-Theoretic Planning: Structural Assumptions
Computational Leverage
Craig Boutilier

cebly@cs.ubc.ca

Department Computer Science, University British Columbia
Vancouver, BC, V6T 1Z4, Canada

Thomas Dean

tld@cs.brown.edu

Department Computer Science, Brown University
Box 1910, Providence, RI, 02912, USA

Steve Hanks

hanks@cs.washington.edu

Department Computer Science Engineering, University Washington
Seattle, WA, 98195, USA

Abstract

Planning uncertainty central problem study automated sequential
decision making, addressed researchers many different fields, including
AI planning, decision analysis, operations research, control theory economics.
assumptions perspectives adopted areas often differ substantial ways,
many planning problems interest researchers fields modeled Markov
decision processes (MDPs) analyzed using techniques decision theory.
paper presents overview synthesis MDP-related methods, showing
provide unifying framework modeling many classes planning problems studied
AI. describes structural properties MDPs that, exhibited particular classes problems, exploited construction optimal approximately
optimal policies plans. Planning problems commonly possess structure reward
value functions used describe performance criteria, functions used describe
state transitions observations, relationships among features used describe
states, actions, rewards, observations.
Specialized representations, algorithms employing representations, achieve
computational leverage exploiting various forms structure. Certain AI techniques|
particular based use structured, intensional representations|can
viewed way. paper surveys several types representations classical
decision-theoretic planning problems, planning algorithms exploit representations number different ways ease computational burden constructing
policies plans. focuses primarily abstraction, aggregation decomposition techniques based AI-style representations.

1. Introduction

Planning using decision-theoretic notions represent domain uncertainty plan quality
recently drawn considerable attention artificial intelligence (AI).1 Decision-theoretic
planning (DTP) attractive extension classical AI planning paradigm
allows one model problems actions uncertain effects, decision maker
1. See, example, recent texts (Dean, Allen, & Aloimonos, 1995; Dean & Wellman, 1991; Russell &
Norvig, 1995) research reported (Hanks, Russell, & Wellman, 1994).

c 1999 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiBoutilier, Dean, & Hanks

incomplete information world, factors resource consumption lead
solutions varying quality, may absolute well-defined \goal"
state. Roughly, aim DTP form courses action (plans policies)
high expected utility rather plans guaranteed achieve certain goals.
AI planning viewed particular approach solving sequential decision problems
type, connections DTP models used fields research|such
decision analysis, economics operations research (OR)|become apparent.
conceptual level, sequential decision problems viewed instances Markov
decision processes (MDPs), use MDP framework make connections
explicit.
Much recent research DTP explicitly adopted MDP framework underlying model (Barto, Bradtke, & Singh, 1995; Boutilier & Dearden, 1994; Boutilier, Dearden,
& Goldszmidt, 1995; Dean, Kaelbling, Kirman, & Nicholson, 1993; Koenig, 1991; Simmons
& Koenig, 1995; Tash & Russell, 1994), allowing adaptation existing results algorithms solving MDPs (e.g., field OR) applied planning problems.
so, however, work departed traditional definition \planning
problem" AI planning community|one goal paper make explicit
connection two lines work.
Adopting MDP framework model posing solving planning problems
illuminated number interesting connections among techniques solving decision
problems, drawing work AI planning, reasoning uncertainty, decision analysis
OR. One interesting insights emerge body work many
DTP problems exhibit considerable structure, thus solved using special-purpose
methods recognize exploit structure. particular, use feature-based
representations describe problems, typical practice AI, often highlights
problem's special structure allows exploited computationally little effort.
two general impediments widespread acceptance MDPs within
AI general model planning. first absence explanations MDP model
make connections current planning research explicit, either conceptual
computational level. may due large part fact MDPs
developed studied primarily OR, dominant concerns are, naturally, rather
different. One aim paper make connections clear: provide brief
description MDPs conceptual model planning emphasizes connection
AI planning, explore relationship MDP solution algorithms AI
planning algorithms. particular, emphasize AI planning models
viewed special cases MDPs, classical planning algorithms designed
exploit problem characteristics associated cases.
second impediment skepticism among AI researchers regarding computational
adequacy MDPs planning model: techniques scale solve planning problems
reasonable size? One diculty solution techniques MDPs tendency rely
explicit, state-based problem formulations. problematic AI planning
since state spaces grow exponentially number problem features. State space size
dimensionality somewhat lesser concern decision analysis.
fields, operations researcher decision analyst often hand-craft model ignores
certain problem features deemed irrelevant, define features summarize
2

fiDecision-Theoretic Planning: Structural Assumptions

wide class problem states. AI, emphasis automatic solution problems
posed users lack expertise decision analyst. Thus, assuming well-crafted,
compact state space often appropriate.
paper show specialized representations algorithms AI planning
problem solving used design ecient MDP solution techniques. particular,
AI planning methods assume certain structure state space, actions (or
operators), specification goal success criteria. Representations
algorithms designed make problem structure explicit exploit
structure solve problems effectively. demonstrate process identifying
structure, making explicit, exploiting algorithmically brought bear
solution MDPs.
paper several objectives. First, provides overview DTP MDPs
suitable readers familiar traditional AI planning methods makes connections
work. Second, describes types structure exploited
AI representations methods facilitate computationally effective planning MDPs.
such, suitable introduction AI methods familiar classical
presentation MDPs. Finally, surveys recent work use MDPs AI
suggests directions research regard, therefore interest
researchers DTP.

1.1 General Problem Definition
Roughly speaking, class problems consider involving systems whose
dynamics modeled stochastic processes, actions decision maker,
referred agent , uence system's behavior. system's current
state choice action jointly determine probability distribution system's
possible next states. agent prefers certain system states (e.g., goal states)
others, therefore must determine course action|also called \plan" \policy"
paper|that likely lead target states, possibly avoiding undesirable states
along way. agent may know system's state exactly making decision
act, however|it may rely incomplete noisy sensors forced
base choice action probabilistic estimate state.
help illustrate types problems interested, consider following
example. Imagine robot agent designed help someone (the \user")
oce environment (see Figure 1). three activities might undertake: picking
user's mail, getting coffee, tidying user's research lab. robot move
location location perform various actions tend achieve certain target
states (e.g., bringing coffee user demand, maintaining minimal level tidiness
lab).
might associate certain level uncertainty effects robot's actions
(e.g., tries move adjacent location might succeed 90% time fail
move 10% time). robot might incomplete access
true state system sensors might supply incomplete information (it
cannot tell whether mail available pickup mail room) incorrect
3

fiBoutilier, Dean, & Hanks

Office

Hallway

Lab
MailRoom

Coffee

Figure 1: decision-theoretic planning problem
information (even mail room sensors occasionally fail detect presence
mail).
Finally, performance robot might measured various ways: actions
guarantee goal achieved? maximize objective function defined
possible effects actions? achieve goal state sucient probability avoiding \disastrous" states near certainty? stipulation optimal
acceptable behavior important part problem specification.
types problems captured using general framework include classical (goal-oriented, deterministic, complete knowledge) planning problems extensions
conditional probabilistic planning problems, well general
problem formulations.
discussion point assumed extensional representation system's
states|one state explicitly named. AI research, intensional representations common. intensional representation one states sets
states described using sets multi-valued features. choice appropriate set
features important part problem design. features might include
current location robot, presence absence mail, on. performance
metric typically expressed intensionally. Figure 2 serves reference example problem, use throughout paper. lists basic features used describe
states system, actions available robot exogenous events
might occur, together intuitive description features, actions events.
remainder paper organized follows. Section 2, present MDP
framework abstract, introducing basic concepts terminology noting relationship abstract model classical AI planning problem. Section 3 surveys common solution techniques|algorithms based dynamic programming general
MDP problems search algorithms planning problems|and points relationship problem assumptions solution techniques. Section 4 turns algorithms
representations, showing various ways structured representations commonly
used AI algorithms used represent MDPs compactly well. Section 5 surveys
4

fiDecision-Theoretic Planning: Structural Assumptions

Features
Location

Denoted
Description
Loc(M ), etc. Location robot. Five possible locations: mailroom (M), coffee room
(C), user's oce (O), hallway (H), laboratory (L)
Tidiness
(0), etc.
Degree lab tidiness. Five possible values: 0 (messiest) 4
(tidiest)
Mail present
M;
mail user's mail box? True (M ) False (M )
Robot mail
RHM; RHM robot mail possession?
Coffee request
CR; CR
outstanding (unfulfilled) request coffee user?
Robot coffee RHC; RHC robot coffee possession?
Actions
Denoted
Description
Move clockwise
Clk
Move adjacent location (clockwise direction)
Counterclockwise CClk
Move adjacent location (counterclockwise direction)
Tidy lab
Tidy
robot lab, degree tidiness increased 1
Pickup mail
PUM
robot mailroom mail present, robot
takes mail (RHM becomes true becomes false)
Get coffee
GetC
robot coffee room, gets coffee (RHC becomes true)
Deliver mail
DelM
robot oce mail, hands mail user
(RHM becomes false)
Deliver coffee
DelC
robot oce coffee, hands coffee
user (RHC CR become false)
Events
Denoted
Description
Mail arrival
ArrM
Mail arrives causing become true
Request coffee
ReqC
User issues coffee request causing CR become true
Untidy lab
Mess
lab becomes messier (one degree less tidy)

Figure 2: Elements robot domain.
recent work abstraction, aggregation problem decomposition methods,
shows connection traditional AI methods goal regression. last
section demonstrates representational computational methods AI planning
used solution general MDPs. Section 5 points additional ways
type computational leverage might developed future.

2. Markov Decision Processes: Basic Problem Formulation
section introduce MDP framework make explicit relationship
model classical AI planning models. interested controlling stochastic
dynamical system: system point time one number distinct
states, system's state changes time response events. action
particular kind event instigated agent order change system's state.
assume agent control actions taken when, though
effects taking action might perfectly predictable. contrast, exogenous events
agent's control, occurrence may partially predictable.
abstract view agent consistent \AI" view agent
autonomous decision maker \control" view policy determined ahead
time, programmed device, executed without deliberation.
5

fiBoutilier, Dean, & Hanks

2.1 States State Transitions

define state description system particular point time. one
defines states vary particular applications, notions natural
others. However, common assume state captures information relevant
agent's decision-making process. assume finite state space = fs1 ; : : : ; sN g
possible system states.2 cases agent complete information
current state; uncertainty incomplete information captured using
probability distribution states .
discrete-time stochastic dynamical system consists state space probability
distributions governing possible state transitions|how next state system depends
past states. distributions constitute model system evolves time
response actions exogenous events, ecting fact effects actions
events may perfectly predictable even prevailing state known.
Although generally concerned agent chooses appropriate course
action, remainder section assume agent's course action
fixed, concentrating problem predicting system's state occurrence
predetermined sequence actions. discuss action selection problem next
section.
assume system evolves stages, occurrence event marks
transition one stage next stage + 1. Since events define changes stage,
since events often (but necessarily) cause state transitions, often equate stage
transitions state transitions. course, possible event occur leave
system state.
system's progression stages roughly analogous passage time.
two identical assume action (possibly no-op) taken
stage, every action takes unit time complete. thus speak loosely
stages correspond units time, refer interchangeably set stages
set time points.3
model uncertainty regarding system's state stage random
variable takes values . assumption \forward causality" requires
variable depend directly value future variable k (k > t). Roughly,
requires model system past history \directly" determines
distribution current states, whereas knowledge future states uence
estimate current state indirectly providing evidence current state
may lead future states. Figure 3(a) shows graphical perspective
discrete-time, stochastic dynamical system. nodes random variables denoting
state particular time, arcs indicate direct probabilistic dependence
states previous states. describe system completely must supply
conditional distributions Pr(S jS 0 ; 1 ; t,1 ) times t.
States thought descriptions system modeled, question arises much detail system captured state description.
2. discussion paper applies cases state space countably infinite. See
(Puterman, 1994) discussion infinite continuous-state problems.
3. deal topics here, considerable literature community
continuous-time Markov decision processes (Puterman, 1994).

6

fiDecision-Theoretic Planning: Structural Assumptions

(a)







(b)







(c)

0

0

1

1



2



2



t-1

t-1











t-1



Figure 3: general stochastic process (a), Markov chain (b), stationary Markov
chain (c).
detail implies information system, turn often translates better
predictions future behavior. course, detail implies larger set ,
increase computational cost decision making.
commonly assumed state contains enough information predict next
state. words, information history system relevant predicting
future captured explicitly state itself. Formally, assumption, Markov
assumption, says knowledge present state renders information past
irrelevant making predictions future:
Pr(S t+1 jS ; t,1 ; : : : ; 0 ) = Pr(S t+1 jS )
Markovian models represented graphically using structure Figure 3(b),
ecting fact present state sucient predict future state evolution.4
Finally, common assume effects event depend prevailing
state, stage time event occurs.5 distribution predicting
next state regardless stage, model said stationary
represented schematically using two stages, Figure 3(c). case
single conditional distribution required. paper generally restrict attention
discrete-time, finite-state, stochastic dynamical systems Markov property, commonly called Markov chains. Furthermore, discussion restricted stationary
chains.
complete model must provide probability distribution initial states,
ecting probability state stage 0. distribution repre4. worth mentioning Markov property applies particular model system
itself. Indeed, non-Markovian model system (of finite order, i.e., whose dynamics depend
k previous states k) converted equivalent though larger Markov model.
control theory, called conversion state form (Luenberger, 1979).
5. course, statement model detail, saying state carries enough information
make stage irrelevant predicting transitions.

7

fiBoutilier, Dean, & Hanks

.3

6

.7

.9
.1

.5

1
.5

3

.1

4

1.0

.2

2

.8

5

.4

7
1.0

.5

Figure 4: state-transition diagram.
sented real-valued (row) vector size N = jS j (one entry state). denote
vector P 0 use p0i denote ith entry, is, probability starting state
si .
represent -stage nonstationary Markov chain transition matrices,
size N N , matrix P captures transition probabilities governing
system moves stage stage + 1. matrix consists probabilities ptij ,
ptij = Pr(S t+1 = sj jS = si ). process stationary, transition matrix
stages one matrix (whose entries denoted pij ) suce. Given
initial
states P 0 , probability distribution states n stages
Q0 Pdistribution
i.
i=n
stationary Markov process represented using state-transition diagram
Figure 4. nodes correspond particular states stage represented
explicitly. Arcs denote possible transitions (those non-zero probability) labeled
transition probabilities pij = Pr(S t+1 = sj jS = si ). arc node node
j labeled pij pij > 0.6 size diagram least O(N )
O(N 2 ), depending number arcs. useful representation transition
graph relatively sparse, example, states immediate transitions
neighbors.

Example 2.1 illustrate notions, imagine robot Figure 1 executing

policy moving counterclockwise repeatedly. restrict attention two
variables, location Loc presence mail , giving state space size 10.
suppose robot always moves adjacent location probability 1:0.
addition, mail arrive mailroom probability 0:2 time (independent robot's location), causing variable become true.
becomes true, robot cannot move state false, since action
moving uence presence mail. state-transition diagram
example illustrated Figure 5. transition matrix shown. 2

structure Markov chain occasionally interest us planning. subset

C closed pij = 0 2 C j 62 C . proper closed set proper
subset C enjoys property. sometimes refer proper closed sets recurrent
classes states. closed set consists single state, state called
absorbing state. agent enters closed set absorbing state, remains
6. important note nodes represent random variables earlier figures.

8

fiDecision-Theoretic Planning: Structural Assumptions

s1
s2

0.2

OM

0.8

0.8
0.2

OM

s7

HM

LM

0.8

1.0

1.0

s10
0.2

0.8
CM

MM

MM

0.2

s3

HM

s9 1.0

s4

0.8

s1
s2
s3
s4
s5
s6
s7
s8
s9
s10

s6

1.0

s5

LM

0.2

s8

CM

1.0

s1
0:0
0:0
0:0
0:0
0:8
0:0
0:0
0:0
0:0
0:0

s2
0:8
0 :0
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0 :0

s3
0:0
0:8
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0 :0

s4
0:0
0:0
0:8
0:0
0:0
0:0
0:0
0:0
0:0
0:0

s5
0:0
0:0
0:0
0:8
0:0
0:0
0:0
0:0
0:0
0:0

s6
0:0
0 :0
0:0
0 :0
0:2
0 :0
0 :0
0 :0
0 :0
1:0

s7
0:2
0:0
0:0
0:0
0:0
1:0
0:0
0:0
0:0
0:0

s8
0:0
0:2
0:0
0:0
0:0
0:0
1:0
0:0
0:0
0:0

s9
0:0
0:0
0:2
0:0
0:0
0:0
0:0
1:0
0:0
0:0

s10
0:0
0:0
0:0
0:2
0:0
0:0
0:0
0:0
1:0
0:0

Figure 5: state-transition diagram transition matrix moving robot.
forever probability 1. example (Figure 5), set states
holds forms recurrent class. absorbing states example,
program robot stay put whenever state hM; Loc(O)i, would
absorbing state altered chain. Finally, say state transient
belong recurrent class. Figure 5, state holds transient|eventually
(with probability 1), agent leaves state never returns, since way
remove mail arrives.

2.2 Actions
Markov chains used describe evolution stochastic system,
capture fact agent choose perform actions alter state
system. key element MDPs set actions available decision maker.
action performed particular state, state changes stochastically response
action. assume agent takes action stage process,
system changes state accordingly.
stage process state s, agent available set actions
Ats. called feasible set stage t. describe effects 2 Ats, must
supply state-transition distribution Pr(S t+1 jS = s; = a) actions a, states s,
stages t. Unlike case Markov chain, terms Pr(S t+1 jS = s; = a)
true conditional distributions, rather family distributions parameterized
, since probability part model. retain notation, however,
suggestive nature.
often assume feasible set actions stages states,
case set actions = fa1 ; : : : ; aK g executed time.
contrasts AI planning practice assigning preconditions actions defining
states meaningfully executed. model takes view
action executed (or \attempted") state. action effect
executed state, execution leads disastrous effects, noted
action's transition matrix. Action preconditions often computational convenience
rather representational necessity: make planning process ecient
identifying states planner even consider selecting action.
Preconditions represented MDPs relaxing assumption set
9

fiBoutilier, Dean, & Hanks

s1
s2
s3
s4
s5
s6
s7
s8
s9
s10

s1
0:0
0:0
0:0
0:0
0:8
0:0
0:0
0:0
0:0
0:0

s2
0:8
0:0
0:8
0:0
0:0
0:0
0:0
0:0
0:0
0:0

s3
0:0
0:8
0:0
0:8
0:0
0:0
0:0
0:0
0:0
0:0

s4
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0

s5
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0

s6
0:0
0:0
0:0
0:0
0:2
0:0
0:0
0:0
0:0
1:0

s7
0:2
0:0
0:2
0:0
0:0
1:0
0:0
1:0
0:0
0:0

s8
0:0
0:2
0:0
0:2
0:0
0:0
1:0
0:0
1:0
0:0

s9
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0

s10
0:2
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0

s1
0.8

0.2

OM

0.8

s2

LM

s3

s7

s10

1.0

s9

1.0
0.8

CM

MM

MM

s8

s4

0.2
0.2

1.0

s6
HM

s5

0.8
0.8

1.0

HM

0.2

LM

OM

0.2

CM

1.0

Figure 6: transition matrix Clk induced transition diagram two-action
policy.
feasible actions states. illustrate planning concepts below, however,
sometimes assume actions preconditions.
restrict attention stationary processes, case means
effects action depends state stage. transition
matrices thus take form pkij = Pr(S t+1 = sj jS = si ; = ak ), capturing probability
system moves state sj ak executed state si . stationary models
action fully described single N N transition matrix P k . important note
transition matrix action includes direct effects executing
action effects exogenous events might occur stage.7

Example 2.2 example Figure 5 extended agent two available

actions: moving clockwise moving counterclockwise. transition matrix
CClk (with assumption mail arrives probability 0:2) shown Figure 5.
matrix Clk appears left Figure 6. Suppose agent fixes behavior
moves clockwise locations C counterclockwise locations H ,
L (we address agent might come know location
actually implement behavior). defines Markov chain illustrated
transition diagram right Figure 6. 2

2.3 Exogenous Events

Exogenous events events stochastically cause state transitions, much
actions, beyond control decision maker. might correspond
evolution natural process action another agent. Notice effect
action CClk Figure 5 \combines" effects robot's action
exogenous event mail arrival: state-transition probabilities incorporate motion
robot (causing change location) possible change mail status due
mail arrival. purposes decision making, precisely combined effect
7. possible assess effects actions exogenous events separately, combine
single transition matrix certain cases (Boutilier & Puterman, 1995). discuss later
section.

10

fiDecision-Theoretic Planning: Structural Assumptions

important predicting distribution possible states resulting
action taken. call models actions implicit-event models, since effects
exogenous event folded transition probabilities associated action.
However, often natural view transitions comprised two separate
events, effect state. generally, often think transitions
determined effects agent's chosen action certain exogenous
events beyond agent's control, may occur certain probability.
effects actions decomposed fashion, call action model
explicit-event model.
Specifying transition function action zero exogenous events
generally easy, actions events interact complex ways. instance, consider
specifying effect action PUM (pickup mail) state mail present
possibility \simultaneous" mail arrival (i.e., \same unit" discrete
time). event ArrM occurs, robot obtain newly arrived mail,
mail remain mailbox? Intuitively, depends whether mail arrived
pickup completed (albeit within time quantum). state transition
case viewed composition two transitions precise description
composition depends ordering agent's action exogenous event.
mail arrives first, transition might ! s0 ! s00 , s0 state mail
waiting s00 state mail waiting robot holding mail;
pickup action completed first, transition would ! ! s0 (i.e., PUM
effect, mail arrives remains box).
picture complicated actions events truly occur simultaneously
interval|in case resulting transition need composition
individual transitions. example, robot lifts side table glass
water situated, water spill; similarly exogenous event causes side
raised. action event occur simultaneously, result qualitatively
different (the water spilled). Thus, \interleaving" semantics described
always appropriate.
complications, modeling exogenous events combination
actions events approached many ways, depending modeling assumptions one willing make. Generally, specify three types information. First,
provide transition probabilities actions events assumption
occur isolation|these standard transition matrices. transition matrix
Figure 5 decomposed two matrices shown Figure 7, one Clk one
ArrM.8 Second, exogenous event, must specify probability occurrence.
Since vary state, generally require vector length N indicating
probability occurrence state. occurrence vector ArrM would
[0:2 0:2 0:2 0:2 0:2 0:0 0:0 0:0 0:0 0:0]
8. fact individual matrices deterministic artifact example. general,
actions events represented using genuinely stochastic matrices.

11

fiBoutilier, Dean, & Hanks

s1
s2
s3
s4
s5
s6
s7
s8
s9
s10

s1

0:0
0:0
0:0
0:0
1:0
0:0
0:0
0:0
0:0
0:0

s2

1:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0

s3

0:0
1:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0

s4

0:0
0:0
1:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0

s5

0:0
0:0
0:0
1:0
0:0
0:0
0:0
0:0
0:0
0:0

s6

0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0
1:0

s7

0:0
0:0
0:0
0:0
0:0
1:0
0:0
0:0
0:0
0:0

s8

0:0
0:0
0:0
0:0
0:0
0:0
1:0
0:0
0:0
0:0

s9

0:0
0:0
0:0
0:0
0:0
0:0
0:0
1:0
0:0
0:0

s10
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0
1:0
0:0

s1
s2
s3
s4
s5
s6
s7
s8
s9
s10

Action Clk

s1

0 :0
0 :0
0 :0
0 :0
0 :0
0 :0
0 :0
0 :0
0 :0
0 :0

s2

0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0

s3

0 :0
0 :0
0 :0
0 :0
0 :0
0 :0
0 :0
0 :0
0 :0
0 :0

s4

0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0

s5

0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0
0:0

s6

1:0
0:0
0:0
0:0
0:0
1:0
0:0
0:0
0:0
0:0

s7

0:0
1:0
0:0
0:0
0:0
0:0
1:0
0:0
0:0
0:0

s8

0:0
0:0
1:0
0:0
0:0
0:0
0:0
1:0
0:0
0:0

s9

0:0
0:0
0:0
1:0
0:0
0:0
0:0
0:0
1:0
0:0

s10
0:0
0:0
0:0
0:0
1:0
0:0
0:0
0:0
0:0
1:0

Event ArrM

Figure 7: transition matrices action exogenous event explicit-event
model.
assume, illustration, mail arrives none present.9 final
requirement combination function describes \compose" transitions
action subset event transitions. indicated above, complex,
sometimes almost unrelated individual action event transitions. However,
certain assumptions combination functions specified reasonably concisely.
One way modeling composition transitions assume interleaving semantics type alluded above. case, one needs specify probability
action events take place occur specific order. instance, one might
assume event occurs time|within discrete time unit|according
continuous distribution (e.g., exponential distribution given rate). information, probability particular ordering transitions, given certain events
occur, computed, resulting distribution possible next states.
example above, probability (composed) transitions s1 ! s2 ! s3 s1 ! s1 ! s2
would given probabilities mail arrived first last, respectively.
certain cases, probability ordering needed. illustrate another
combination function, assume action always occurs exogenous events.
Furthermore, assume events commutative: (a) initial state pair
events e1 e2 , distribution results applying event sequence e1 e2
identical obtained sequence e2 e1 ; (b) occurrence probabilities
intermediate states identical. Intuitively, set events domain, ArrM, ReqC
Mess, property. conditions combined transition distribution
action computed considering probability subset events
applying subset order distribution associated a.
Generally, construct implicit-event model various components
explicit-event model; thus, \natural" specification converted form usually
used MDP solution algorithms. two assumptions above, instance,
form implicit event transition matrix Pr(si ; a; sj ) action a, given matrix
Pcra (si ; sj ) (which assumes event occurrences), matrices Pre (si ; sj ) events
e, occurrence vector Pre(si ) event e. effective transition matrix
9. probability different events may correlated (possibly particular states). case,
necessary specify occurrence probabilities subsets events. treat event occurrence
probabilities independent ease exposition.

12

fiDecision-Theoretic Planning: Structural Assumptions

event e defined follows:
Pcre (si ; sj ) = Pre (si )Pre (si ; sj ) +

(

1 , Pre (si ) : = j
0 : 6= j

equation captures event transition probabilities probability event occurrence factored in. let E; E 0 denote diagonal matrices entries Ekk = Pre (sk )
0 = 1 , Pre (sk ), Pr
c e(si; sj ) = E Pre +E 0. assumptions above,
Ekk
implicit-event matrix Pr(si ; a; sj ) action given Pr = Pcre1 Pcren Pra
ordering n possible events.
Naturally, different procedures constructing implicit-event matrices required
given different assumptions action event interaction. Whether implicit models constructed specified directly without explicit mention exogenous events,
always assume unless stated otherwise action transition matrices take account
effects exogenous events well, thus represent agent's best information
happen takes particular action.

2.4 Observations

Although effects action depend aspect prevailing state,
choice action depend agent observe current state
remember prior observations. model agent's observational sensing
capabilities introducing finite set observations = fo1 ; : : : ; oH g. agent receives
observation set stage prior choosing action stage.
model observation random variable Ot whose value taken O.
probability particular Ot generated depend on:

state system , 1
action taken , 1
state system taking action , 1 effects
exogenous events , 1 realized, action taken.
let Pr(Ot = oh jS t,1 = si ; At,1 = ak ; = sj ) probability agent observes

oh stage given performs ak state si ends state sj . actions,

assume observational distributions stationary (independent stage), using

phi;j;k = Pr(ohjsi; ak ; sj ) denote quantity. view probabilistic dependencies

among state, action observation variables graph time-indexed variables
shown nodes one variable directly probabilistically dependent another
edge latter former; see Figure 8.
model allows wide variety assumptions agent's sensing capabilities.
one extreme fully observable MDPs (FOMDPs), agent knows exactly
state stage t. model case letting = setting
Pr(oh jsi ; ak ; sj ) =
13

(

1 iff oh = sj
0 otherwise

fiBoutilier, Dean, & Hanks



t-1





t-1







Figure 8: Graph showing dependency relationships among states, actions observations different times.
example above, means robot always knows exact location whether
mail waiting mailbox, even mailroom mail arrives.
agent thus receives perfect feedback results actions effects
exogenous events|it noisy effectors complete, noise-free, \instantaneous"
sensors. recent AI research adopts MDP framework explicitly assumes full
observability.
extreme might consider non-observable systems (NOMDPs)
agent receives information system's state execution. model
case letting = fog. observation reported stage, revealing
information state, Pr(sj jsi ; ak ; o) = Pr(sj jsi ; ak ). open-loop
systems, agent receives useful feedback results actions: agent
noisy effectors sensors. case agent chooses actions according plan
consisting sequence actions executed unconditionally. effect, agent relying
predictive model determine good action choices execution time.
Traditionally, AI planning work implicitly made assumption non-observability,
often coupled omniscience assumption|that agent knows initial state
certainty, predict effects actions perfectly, precisely predict occurrence exogenous events effects. circumstances, agent
predict exact outcome plan, thus obviating need observation.
agent build straight-line plan|a sequence actions performed without
feedback|that good plan whose execution might depend information gathered execution time.
two extremes special cases general observation model described above,
allows agent receive incomplete noisy information system state
(i.e., partially observable MDPs, POMDPs). example, robot might able
determine location exactly, might able determine whether mail arrives
unless mailroom. Furthermore, \mail" sensors might occasionally report
inaccurately, leading incorrect belief whether mail waiting.

Example 2.3 Suppose robot \checkmail" action change system

state generates observation uenced presence mail, provided
14

fiDecision-Theoretic Planning: Structural Assumptions

Loc(M );
Loc(M );
Loc(M );
Loc(M );

Pr(Obs = mail) Pr(Obs = nomail)
0:92
0:08
0:05
0:95
0:00
1:00
0:00
1:00

Figure 9: Observation probabilities checking mailbox.
robot mailroom time action performed. robot
mailroom, sensor always reports \no mail." noisy \checkmail" sensor
described probability distribution one shown Figure 9.
view error probabilities probability \false positives" (0:05) \false
negatives" (0:08). 2

2.5 System Trajectories Observable Histories

use terms trajectory history interchangeably describe system's behavior
course problem-solving episode, perhaps initial segment thereof.
complete system history sequence states, actions, observations generated
stage 0 time point interest, finite infinite length. Complete
histories represented (possibly infinite) sequence tuples form

hhS 0 ; O0 ; A0 i; hS 1 ; O1 ; A1 i; : : : hS ; OT ; ii
define two alternative notions history contain less complete information.
arbitrary stage define observable history sequence

hhO0 ; A0 i; : : : ; hOt,1 ; At,1 ii
O0 observation initial state. observable history stage comprises
information available agent history chooses action stage t.
third type trajectory system trajectory, sequence

hhS 0 ; A0 i; : : : ; hS t,1 ; At,1 i;
describing system's behavior \objective" terms, independent agent's particular
view system.
evaluating agent's performance, generally interested system
trajectory. agent's policy must defined terms observable history, since
agent access system trajectory, except fully observable case,
two equivalent.

2.6 Reward Value

problem facing decision maker select action performed stage
decision problem, making decision basis observable history.
agent still needs way judge quality course action. done defining
15

fiBoutilier, Dean, & Hanks



t-1





t-1



C



R



Figure 10: Decision process rewards action costs.
value function V() function mapping set system histories HS reals;
is, V : HS ! R.10 agent prefers system history h h0 case V(h) > V(h0 ).
Thus, agent judges behavior good bad depending effect
underlying system trajectory. Generally, agent cannot predict certainty
system trajectory occur, best generate probability distribution
possible trajectories caused actions. case, computes expected value
candidate course action chooses policy maximizes quantity.
system dynamics, specifying value function arbitrary trajectories
cumbersome unintuitive. therefore important identify structure
value function lead parsimonious representation.
Two assumptions value functions commonly made MDP literature
time-separability additivity. time-separable value function defined terms
primitive functions applied component states actions. reward
function R : ! R associates reward state s. Costs assigned
taking actions defining cost function C : ! R associates cost
performing action state s. Rewards added value function, costs
subtracted.11
value function time-separable \simple combination" rewards costs
accrued stage. \Simple combination" means value taken function
costs rewards stage, costs rewards depend stage t,
function combines must independent stage, commonly linear
combination product.12 value function additive combination function
sum reward cost function values accrued history's stages. addition
rewards action costs system time-separable value viewed graphically
shown Figure 10.
assumption time-separability restrictive. example, might
certain goals involving temporal deadlines (have workplace tidy soon possible
9:00 tomorrow morning) maintenance (do allow mail sit mailroom
10. Technically, set histories interest depends horizon chosen, described below.
11. term \reward" somewhat misnomer reward could negative, case
\penalty" might better word. Likewise, \costs" either positive (punitive) negative (beneficial). Thus, admit great exibility defining value functions.
12. See (Luenberger, 1973) precise definition time-separability.

16

fiDecision-Theoretic Planning: Structural Assumptions

undelivered 10 minutes) require value functions non-separable
given current representation state. Note, however, separability|like
Markov property|is property particular representation. could add additional
information state example: clock time, interval time 9:00
time tidiness achieved, length time mail sits mail room
robot picks up, on. additional information could reestablish time-separable value function, expense increase number
states ad hoc cumbersome action representation.13

2.7 Horizons Success Criteria

order evaluate particular course action, need specify long (in
many stages) executed. known problem's horizon. finite-horizon
problems, agent's performance evaluated fixed, finite number stages .
Commonly, aim maximize total expected reward associated course
action; therefore define (finite-horizon) value length history h (Bellman,
1957):

V (h) =

TX
,1
t=0

fR(st ) , C (st; )g + R(sT )

infinite-horizon problem, hand, requires agent's performance
evaluated infinite trajectory. case total reward may unbounded,
meaning policy could arbitrarily good bad executed long enough.
case may necessary adopt different means evaluating trajectory.
common introduce discount factor, ensuring rewards costs accrued
later stages counted less accrued earlier stages. value function
expected total discounted reward problem defined follows (Bellman, 1957; Howard,
1960):
1
X
V (h) = (R(st) , C (st; ))
t=0

fixed discount rate (0 < 1). formulation particularly simple
elegant way ensure bounded measure value infinite horizon, though
important verify discounting fact appropriate. Economic justifications often
provided discounted models|a reward earned sooner worth one earned
later provided reward somehow invested. Discounting suitable
modeling process terminates probability 1 , point time (e.g.,
robot break down), case discounted models correspond expected total
reward finite uncertain horizon. reasons, discounting sometimes used
finite-horizon problems well.
Another technique dealing infinite-horizon problems evaluate trajectory
based average reward accrued per stage, gain. gain history defined
n
X
g(h) = lim 1 fR(st ) , C (st ; )g
n!1 n t=0

13. See (Bacchus, Boutilier, & Grove, 1996, 1997), however, systematic approach handling certain
types history-dependent reward functions.

17

fiBoutilier, Dean, & Hanks

Refinements criterion proposed (Puterman, 1994).
Sometimes problem ensures total reward infinite trajectory
bounded, thus expected total reward criterion well-defined. Consider case
common AI planners agent's task bring system goal state.
positive reward received goal reached, actions incur non-negative
cost, goal reached system enters absorbing state
rewards costs accrued. long goal reached certainty,
situation formulated infinite-horizon problem total reward bounded
desired trajectory (Bertsekas, 1987; Puterman, 1994). general, problems
cannot formulated (fixed) finite-horizon problems unless priori bound
number steps needed reach goal established. problems sometimes
called indefinite-horizon problems: practical point view, agent continue
execute actions finite number stages, exact number cannot determined
ahead time.

2.8 Solution Criteria

complete definition planning problem need specify constitutes
solution problem. see split explicit MDP formulations
work AI planning community. Classical MDP problems generally stated
optimization problems: given value function, horizon, evaluation metric (e.g.,
expected total reward, expected total discounted reward, expected average reward per stage)
agent seeks behavioral policy maximizes objective function.
Work AI often seeks satisficing solutions problems. planning literature,
generally taken plan satisfies goal equally preferred
plan satisfies goal, plan satisfies goal preferable
plan not.14 probabilistic framework, might seek plan satisfies
goal maximum probability (an optimization), lead situations
optimal plan infinite length system state fully observable.
satisficing alternative (Kushmerick, Hanks, & Weld, 1995) seek plan satisfies
goal probability exceeding given threshold.

Example 2.4 extend running example demonstrate infinite-horizon, fully
observable, discounted reward situation. begin adding one new dimension
state description, boolean variable RHM (does robot mail), giving us
system 20 states. provide agent two additional actions: PUM
(pickup mail) DelM (deliver mail) described Figure 2. reward
agent way mail delivery encouraged: associate reward
10 state RHM false 0 states.
actions cost, agent gets total reward 20 six-stage system
trajectory:
hLoc(M ); M; RHMi; Stay; hLoc(M ); M; RHMi; PUM; hLoc(M ); M; RHMi;
Clk; hLoc(H ); M; RHMi; Clk; hLoc(O); M; RHMi; DelM; hLoc(O); M; RHMi

14. Though see (Haddawy & Hanks, 1998; Williamson & Hanks, 1994) restatement planning
optimization problem.

18

fiDecision-Theoretic Planning: Structural Assumptions

assign action cost ,1 action except Stay (which 0 cost),
total reward becomes 16. use discount rate 0:9 discount future
rewards costs, initial segment infinite-horizon history would contribute
10 + :9(,1) + :81(,1) + :729(,1) + :6561(,1) + :59054(,1 + 10) = 12:2 total
value trajectory (as subsequently extended). Furthermore, establish
bound total expected value trajectory. best case, subsequent
stages yield reward 10, expected total discounted reward bounded

1
X
12:2 + :96 (10) + :97 (10) + : : : = 12:2 + 10 :96 0:9i < 66
i=0

similar effect behavior achieved penalizing states (i.e., negative
rewards) either RHM true. 2

2.9 Policies

mentioned policies (or courses action, plans) informally point,
provide precise definition. decision problem facing agent viewed
generally deciding action perform given current observable history.
define policy mapping set observable histories HO actions,
is, : HO ! A. Intuitively, agent executes action

= (hho0 ; a0 i; : : : ; hot,1 ; at,1 i; ot i)
stage performed actions a0 ; at,1 made observations o0 ; ot,1
earlier stages, made observation ot current stage.
policy induces distribution Pr(hj) set system histories HS ; probability distribution depends initial distribution P 0 . define expected value
policy be:
X
EV() =
V(h) Pr(hj)
h2HS

would agent adopt policy either maximizes expected value or,
satisficing context, acceptably high expected value.
general form policy, depending arbitrary observation history,
lead complicated policies policy-construction algorithms. special cases,
however, assumptions observability structure value function result
optimal policies much simpler form.
case fully observable MDP time-separable value function, optimal
action stage computed using information current state
stage: is, restrict policies simpler form : ! without
danger acting suboptimally. due fact full observability allows state
observed completely, Markov assumption renders prior history irrelevant.
non-observable case, observational history contains vacuous observations
agent must choose actions using knowledge previous actions
stage; however, since incorporates previous actions, takes form : ! A.
19

fiBoutilier, Dean, & Hanks

form policy corresponds linear, unconditional sequence actions ha1 ; a2 ; : : : ; i,
straight-line plan AI nomenclature.15

2.10 Model Summary: Assumptions, Problems, Computational
Complexity

concludes exposition MDP model planning uncertainty. generality allows us capture wide variety problem classes currently
studied literature. section review basic components model,
describe problems commonly studied DTP literature respect model,
summarize known complexity results each. Section 3, describe specialized computational techniques used solve problems problem classes.
2.10.1 Model Summary Assumptions

MDP model consists following components:
state space , finite countable set states. generally make Markov
assumption, requires state convey information necessary predict
effects actions events independent information
system history.
set actions A. action ak represented transition matrix size
jS jjS j representing probability pkij performing action ak state si move
system state sj . assume throughout action model stationary,
meaning transition probabilities vary time. transition matrix
action generally assumed account exogenous events might
occur stage action executed.
set observation variables O. set \messages" sent agent
action performed, provide execution-time information current
system state. action ak pair states si , sj , pkij > 0,
associate distribution possible observations: pkm
ij denotes probability
obtaining observation om given action ak taken si resulted
transition state sj .
value function V . value function maps state history real number
V (h1 ) V (h2 ) case agent considers history h1 least good
h2 . state history records progression states system assumes along
actions performed. Assumptions time-separability additivity
common V . particular, generally use reward function R cost function
C defining value.
horizon . number stages state histories
evaluated using V .
15. Many algorithms AI literature produce partially ordered sequence actions. plans
not, however, involve conditional nondeterministic execution. Rather, represent fact
linear sequence consistent partial order solve problem. Thus, partially ordered
plan concise representation particular set straight-line plans.

20

fiDecision-Theoretic Planning: Structural Assumptions

optimality criterion. provides criterion evaluating potential solutions
planning problems.

2.10.2 Common Planning Problems

use general framework classify various problems commonly studied
planning decision-making literature. case below, note modeling assumptions define problem class.

Planning Problems OR/Decision Sciences Tradition
Fully Observable Markov Decision Processes (FOMDPs) | ex-

tremely large body research studying FOMDPs, present basic algorithmic techniques detail next section. commonly used formulation FOMDPs assumes full observability stationarity, uses optimality
criterion maximization expected total reward finite horizon, maximization expected total discounted reward infinite horizon, minimization
expected cost goal state.
FOMDPs introduced Bellman (1957) studied depth
fields decision analysis OR, including seminal work Howard (1960). Recent texts FOMDPs include (Bertsekas, 1987) (Puterman, 1994). Average reward optimality received attention literature (Blackwell, 1962; Howard,
1960; Puterman, 1994). AI literature, discounted total reward models
popular well (Barto et al., 1995; Dearden & Boutilier, 1997; Dean, Kaelbling, Kirman, & Nicholson, 1995; Koenig, 1991), though average-reward criterion
proposed suitable modeling AI planning problems (Boutilier &
Puterman, 1995; Mahadevan, 1994; Schwartz, 1993).

Partially Observable Markov Decision Processes (POMDPs) | POMDPs

closer FOMDPs general model decision processes described.
POMDPs generally studied assumption stationarity optimality criteria identical FOMDPs, though average-reward criterion
widely considered. discuss below, POMDP viewed
FOMDP state space consisting set probability distributions
. probability distributions represent states belief: agent \observe"
state belief system although exact knowledge
system state itself.
POMDPs widely studied control theory (Astrom, 1965; Lovejoy, 1991b; Smallwood & Sondik, 1973; Sondik, 1978), drawn increasing
attention AI circles (Cassandra, Kaelbling, & Littman, 1994; Hauskrecht, 1998;
Littman, 1996; Parr & Russell, 1995; Simmons & Koenig, 1995; Thrun, Fox, & Burgard, 1998; Zhang & Liu, 1997). uence diagrams (Howard & Matheson, 1984;
Shachter, 1986) popular model decision making AI are, fact,
structured representational method POMDPs (see Section 4.3).

Planning Problems AI Tradition
21

fiBoutilier, Dean, & Hanks

Classical Deterministic Planning | classical AI planning model assumes

deterministic actions: action ak taken state si one successor sj .
important assumptions non-observability value determined
reaching goal state: plan leads goal state preferred
not. Often preference shorter plans; represented
using discount factor \encourage" faster goal achievement assigning
cost actions. Reward associated transitions goal states,
absorbing. Action costs typically ignored, except noted above.
classical models usually assumed initial state known certainty.
contrasts general specification MDPs above, assume
knowledge even distributional information initial state. Policies
defined applicable matter state (or distribution states) one finds
oneself in|action choices defined every possible state history. Knowledge
initial state determinism allow optimal straight-line plans constructed,
loss value associated non-observability, unpredictable exogenous
events uncertain action effects cannot modeled consistently assumptions adopted.
overview early classical planning research variety approaches
adopted, see (Allen, Hendler, & Tate, 1990) well Yang's (1998) recent text.

Optimal Deterministic Planning | separate body work retains classical

assumptions complete information determinism, tries recast planning
problem optimization relaxes implicit assumption \achieve goal
costs." time, methods use sort representations
algorithms applied satisficing planning.
Haddawy Hanks (1998) present multi-attribute utility model planners
keeps explicit information initial state goals, allows preferences stated partial satisfaction goals well cost
resources consumed satisfying them. model allows expression preferences phenomena temporal deadlines maintenance intervals
dicult capture using time-separable additive value function. Williamson (1996)
(see Williamson & Hanks, 1994). implements model extending classical planning algorithm solve resulting optimization problem. Haddawy
Suwandi (1994) implement model complete decision-theoretic framework.
model planning, refinement planning, differs somewhat generative
model discussed paper. model set possible plans pre-stored
abstraction hierarchy, problem solver's job find hierarchy
optimal choice concrete actions particular problem.
Perez Carbonell's (1994) work incorporates cost information classical
planning framework, maintains split classical satisficing planner
additional cost information provided utility model. cost information
used learn search-control rules allow classical planner generate low-cost
goal-satisfying plans.
22

fiDecision-Theoretic Planning: Structural Assumptions

Conditional Deterministic Planning | classical planning assumption

omniscience relaxed somewhat allowing state aspects
world unknown. agent thus situation certain
system one particular set states, know one. Unknown
truth values included initial state specification, taking actions
cause proposition become unknown well.
Actions provide agent information plan executed: conditional planners introduce idea actions providing runtime information
prevailing state, distinguishing action makes proposition P true
action tell agent whether P true action executed.
action causal informational effects, simultaneously changing
world reporting value one propositions. second sort
information useful planning time except allows steps plan
executed conditionally, depending runtime information provided prior
information-producing steps. value actions lies fact different
courses action may appropriate different conditions|these informational
effects allow runtime selection actions based observations produced, much
general POMDP model.
Examples conditional planners classical framework include early work
Warren (1976) recent CNLP (Peot & Smith, 1992), Cassandra (Pryor
& Collins, 1993), Plynth (Goldman & Boddy, 1994), UWL (Etzioni, Hanks,
Weld, Draper, Lesh, & Williamson, 1992) systems.

Probabilistic Planning Without Feedback | direct probabilistic extension

classical planning problem stated follows (Kushmerick et al., 1995):
take input (a) probability distribution initial states, (b) stochastic actions
(explicit implicit transition matrices), (c) set goal states, (d) probability
success threshold . objective produce plan reaches goal state
probability least , given initial state distribution. provision made
execution-time observation, thus straight-line plans form policy
possible. restricted case infinite-horizon NOMDP problem, one
actions incur cost goal states offer positive reward absorbing.
special case objective find satisficing policy rather
optimal one.

Probabilistic Planning Feedback | Draper et al. (1994a) proposed

extension probabilistic planning problem actions provide feedback,
using exactly observation model described Section 2.4. Again, problem
posed building plan leaves system goal state sucient
probability. plan longer simple sequence actions|it contain conditionals loops whose execution depends observations generated sensing
actions. problem restricted case general POMDP problem: absorbing goal states cost-free actions used, objective find policy
(conditional plan) leaves system goal state sucient probability.
23

fiBoutilier, Dean, & Hanks

Comparing Frameworks: Task-oriented Versus Process-oriented Problems

useful point pause contrast types problems considered classical planning literature typically studied within MDP framework. Although
problems AI planning literature emphasized goal-pursuit \one-shot" view
problem solving, cases viewing problem infinite-horizon decision problem
results satisfying formulation. Consider running example involving oce
robot. simply possible model problem responding coffee requests, mail
arrival keeping lab tidy strict goal-satisfaction problem capturing
possible nuances intuitively optimal behavior.
primary diculty explicit persistent goal states exist.
simply require robot attain state lab tidy, mail awaits,
unfilled coffee requests exist, \successful" plan could anticipate possible system behavior
goal state reached. possible occurrence exogenous events goal
achievement requires robot bias methods achieving goals way
best suits expected course subsequent events. instance, coffee requests
likely point time unmet requests highly penalized, robot
situate coffee room order satisfy anticipated future request quickly.
realistic decision scenarios involve task-oriented process-oriented behavior,
problem formulations take account provide satisfying models
wider range situations.
2.10.3 Complexity Policy Construction

defined planning problem several different ways, different
set assumptions state space, system dynamics actions (deterministic
stochastic), observability (full, partial, none), value function (time-separable, goal only,
goal rewards action costs, partially satisfiable goals temporal deadlines), planning
horizon (finite, infinite, indefinite), optimality criterion (optimal satisficing solutions). set assumptions puts corresponding problem particular complexity
class, defines worst-case time space bounds representation algorithm
solving problem. summarize known complexity results
problem classes defined above.
Fully Observable Markov Decision Processes Fully observable MDPs (FOMDPs)
time-separable, additive value functions solved time polynomial size
state space, number actions, size inputs.16 common algorithms solving FOMDPs value iteration policy iteration,
described next section. finite-horizon discounted infinite-horizon problems
require polynomial amount computation per iteration|O(jS j2 jAj) O(jS j2 jAj+jS j3 ),
respectively|and converge polynomial number iterations (with factor 1,1
discounted case). hand, problems shown P-complete
(Papadimitriou & Tsitsiklis, 1987), means ecient parallel solution algorithm
unlikely.17 space required store policy n-stage finite-horizon problem
16. precisely, maximum number bits required represent transition probabilities
costs.
17. See (Littman, Dean, & Kaelbling, 1995) summary complexity results.

24

fiDecision-Theoretic Planning: Structural Assumptions

O(jS jn). interesting classes infinite-horizon problems, specifically involving discounted models time-separable additive reward, optimal policy
shown stationary, policy stored O(jS j) space.
Bear mind worst-case bounds. many cases, better time bounds
compact representations found. Sections 4 5 explore ways represent
solve problems eciently.
Partially Observable Markov Decision Processes POMDPs notorious
computational diculty. mentioned above, POMDP viewed FOMDP
infinite state space consisting probability distributions , distribution
representing agent's state belief point time (Astrom, 1965; Smallwood &
Sondik, 1973). problem finding optimal policy POMDP objective
maximizing expected total reward expected total discounted reward finite
horizon shown exponentially hard jS j (Papadimitriou
& Tsitsiklis, 1987). problem finding policy maximizes approximately
maximizes expected discounted total reward infinite horizon shown
undecidable (Madani, Condon, & Hanks, 1999).
Even restricted cases POMDP problem computationally dicult worst
case. Littman (1996) considers special case boolean rewards: determining whether
infinite-horizon policy nonzero total reward given rewards associated states non-negative. shows problem EXPTIME-complete
transitions stochastic, PSPACE-hard transitions deterministic.
Deterministic Planning Recall classical planning problem defined quite
differently MDP problems above: agent ability observe state
perfect predictive powers, knowing initial state effects actions
certainty. addition, rewards come reaching goal state, plan
achieves goal suces.
Planning problems typically defined terms set P boolean features
propositions: complete assignment truth values features describes exactly one state,
partial assignment truth values describes set states. set propositions P
induces state space size 2jPj. Thus, space required represent planning problem
using feature-based representation exponentially smaller required
representation problem (see Section 4).
ability represent planning problems compactly dramatic impact worstcase complexity. Bylander (1994) shows deterministic planning problem without
observation PSPACE-complete. Roughly speaking, means worst planning
time increase exponentially P A, further, size solution plan
grow exponentially problem size. results hold even action
space severely restricted. example, planning problem NP-complete even
cases action restricted one precondition feature one postcondition
feature. Conditional optimal planning PSPACE-complete well. results
inputs generally compact (generally exponentially so)
terms complexity FOMDP POMDP problems phrased.
Probabilistic Planning probabilistic goal-oriented planning, POMDPs,
typically search solution space probability distributions states (or
25

fiBoutilier, Dean, & Hanks

formulas describe states). Even simplest problem probabilistic planning|one
admits observability|is undecidable worst (Madani et al., 1999). intuition
even though set states finite, set distributions states not,
worst agent may search infinite number plans able
determine whether solution exists. algorithm guaranteed find
solution plan eventually one exists, cannot guaranteed terminate finite time
solution plan. Conditional probabilistic planning generalization
non-observable probabilistic planning problem, thus undecidable well.
interesting note connection conditional probabilistic planning
POMDPs. actions observations two problems equivalent expressive
power, reward structure conditional probabilistic planning problem quite
restrictive: goal states positive rewards, states reward, goal states
absorbing. Since cannot put priori bound length solution plan,
conditional probabilistic planning must viewed infinite-horizon problem
objective maximize total expected undiscounted reward. Note, however, since goal
states absorbing, guarantee total expected reward non-negative
bounded, even infinite horizon. Technically means conditional probabilistic planning problem restricted case infinite-horizon positive-bounded problem
(Puterman, 1994, Section 7.2). therefore conclude problem solving
arbitrary infinite-horizon undiscounted positive-bounded POMDP undecidable.
commonly studied problem infinite-horizon POMDP criterion maximizing expected discounted total reward; finding optimal near-optimal solutions
problem undecidable, noted above.
2.10.4 Conclusion

end section noting results algorithm-independent describe worst-case behavior. effect, indicate badly algorithm made
perform \arbitrarily unfortunate" problem instance. interesting question
whether build representations, techniques, algorithms typically perform
well problem instances typically arise practice. concern leads us examine
problem characteristics eye toward exploiting restrictions placed
states actions, observability, value function optimality criterion.
begin algorithmic techniques focus value function|particularly
take advantage time-separability goal orientation. following section
explore complementary techniques building compact problem representations.

3. Solution Algorithms: Dynamic Programming Search
section review standard algorithms solving problems described
terms \unstructured" \ at" problem representations. noted analysis
above, fully observable Markov decision processes (FOMDPs) far widely
studied models general class stochastic sequential decision problems. begin
describing techniques solving FOMDPs, focusing techniques exploit structure
value function time-separability additivity.
26

fiDecision-Theoretic Planning: Structural Assumptions

3.1 Dynamic Programming Approaches

Suppose given fully-observable MDP time-separable, additive value function.
words, given state space , action space A, transition matrix Pr(s0 js; a)
action a, reward function R, cost function C . start problem
finding policy maximizes expected total reward fixed, finite-horizon .
Suppose given policy (s; t) action performed agent
state stages remaining act (for 0 ).18 Bellman (1957) shows
expected value policy state computed using set t-stage-to-go
value functions Vt . define V0 (s) R(s), define:

Vt (s) = R(s) + C ((s; t)) +

X

2S
0

fPr(s0j(s; t); s)Vt,1 (s0)g

(1)

definition value function makes its0 dependence initial state clear.
say policy optimal VT (s) VT (s) policies 0 2 .
optimal -stage-to-go value function, denoted VT , simply value function
optimal -horizon policy. Bellman's principle optimality (Bellman, 1957) forms basis
stochastic dynamic programming algorithms used solve MDPs, establishing
following relationship optimal value function tth stage optimal value
function previous stage:

Vt (s) = R(s) + max
fC (a) +
a2A

X

s0 2S

Pr(s0 ja; s)Vt,1 (s0 )g

(2)

3.1.1 Value Iteration

Equation 2 forms basis value iteration algorithm finite-horizon problems.
Value iteration begins value function V0 = R, uses Equation 2 compute
sequence value functions longer time intervals, horizon . action
maximizes right-hand side Equation 2 chosen policy element (s; t).
resulting policy optimal -stage, fully observable MDP, indeed
shorter horizon < .
important note policy describes done every stage
every state system, even agent cannot reach certain states given system's
initial configuration available actions. return point below.

Example 3.1 Consider simplified version robot example, four

state variables , CR, RHC, RHM (movement various locations ignored),
four actions GetC, PUM, DelC, DelM. Actions GetC PUM make RHC
RHM, respectively, true certainty. Action DelM, RHM holds, makes
RHM false probability 1.0; DelC makes CR RHC false
probability 0.3, leaving state unchanged probability 0.7. reward 3
associated CR reward 1 associated . reward state
sum rewards objective satisfied state. Figure 11 shows
optimal 0-stage, 1-stage 2-stage value functions various states, along

18. Recall FOMDPs aspects history relevant.

27

fiBoutilier, Dean, & Hanks

State
s0 = hM; RHM; CR; RHCi
s1 = hM; RHM; CR; RHCi
s2 = hM; RHM; CR; RHCi
s3 = hM; RHM; CR; RHCi
s4 = hM; CR; RHCi
s5 = hM; CR; RHCi
s6 = hM; RHM; CRi
s7 = hM; RHM; CRi
s8 = hM; CRi

V0

0
0
0
0
1
1
3
3
4

V1

0
1
0.9
1
2.9
2
7
6
8

(1)

(2)

1 PUM
DelM 2 DelM
DelC 2.43 DelC
DelM 2.9 DelM
DelC 5.43 DelC
3.9 GetC
DelM 11 DelM
10 PUM
12
V2

Figure 11: Finite-horizon optimal value policy.
optimal choice action state-stage pairing (the values \state"
missing variables hold instantiations variables). Note V0 (s)
simply R(s) state s.
illustrate application Equation 2, first consider calculation V1 (s3 ).
robot choice delivering coffee delivering mail, expected value
option, one stage remaining, given by:
EV1 (s3; DelC) = 0:3V0(s6) + 0:7V0 (s3) = 0:9
EV1(s3; DelM) =
1:0V0 (s4 )
= 1:0
Thus (s3 ; 1) = DelM V1 (s3 ) value maximizing choice. Notice
robot one action perform aim \lesser" objective due
risk failure inherent delivering coffee. two stages remaining
state, robot deliver mail, certainty move s4 one
stage go, attempt deliver coffee ( (s4 ; 1) = DelC).
illustrate effects fixed finite horizon policy choice, note
(s0; 2) = PUM. two stages remaining choice getting mail coffee,
robot get mail subsequent delivery (in last stage) guaranteed
succeed, whereas subsequent coffee delivery may fail. However, compute
(s0; 3), see:
EV3(s0 ; GetC) = 1:0V2(s2 ) = 2:43
EV3 (s0; PUM) = 1:0V2(s1 ) = 2:0
three stages go, robot instead retrieve coffee s0 .
coffee, two chances successful delivery. expected value course
action greater (guaranteed) mail delivery. Note three stages
allow sucient time try achieve objectives s0 . fact,
larger reward associated coffee delivery ensures greater number
stages remaining, robot focus first coffee retrieval delivery,
attempt mail retrieval delivery coffee delivery successfully completed. 2
Often faced tasks fixed finite horizon. example,
may want robot perform tasks keeping lab tidy, picking mail whenever
28

fiDecision-Theoretic Planning: Structural Assumptions

arrives, responding coffee requests, on. fixed time horizon associated
tasks|they performed need arises. problems best
modeled infinite-horizon problems.
consider problem building policy maximizes discounted sum
expected rewards infinite horizon.19 Howard (1960) showed always
exists optimal stationary policy problems. Intuitively, case
matter stage process in, still infinite number stages remaining;
optimal action state independent stage. therefore restrict
attention policies choose action state regardless stage
process. restriction, policy size jSj regardless
number stages policy executed|the policy form : ! A.
contrast, optimal policies finite-horizon problems generally nonstationary,
illustrated Example 3.1.
Howard shows value policy satisfies following recurrence:

V (s) = R(s) + fC ((s)) +

X

s0 2S

optimal value function V satisfies:

V (s) = R(s) + max
fC (a) +
a2A

Pr(s0 j(s); s)V (s0 )g

X
2S
0

Pr(s0 ja; s)V (s0 )g

(3)
(4)

value fixed policy evaluated using method successive approximation, almost identical procedure described Equation 1 above. begin
arbitrary assignment values V0 (s), define:

Vt (s) = R(s) + C ((s; t)) +

X

2S
0

fPr(s0j(s; t); s)Vt,1 (s0)g

(5)

sequence functions Vt converges linearly true value function V .
One alter value-iteration algorithm slightly builds optimal policies
infinite-horizon discounted problems. algorithm starts value function V0
assigns arbitrary value 2 . Given value estimate Vt (s) state s, Vt+1 (s)
calculated as:

Vt+1 (s) = R(s) + max
fC (a) +
a2A

X

s0 2S

Pr(s0 ja; s) Vt (s0 )g

(6)

sequence functions Vt converges linearly optimal value function V (s).
finite number iterations n, choice maximizing action forms
optimal policy Vn approximates value.20
19. far commonly studied problem literature, though argued (Boutilier &
Puterman, 1995; Mahadevan, 1994; Schwartz, 1993) problems often best modeled using
average reward per stage optimality criterion. discussion average reward optimality
many variants refinements, see (Puterman, 1994).
20. number iterations n based stopping criterion generally involves measuring difference Vt Vt+1 . discussion stopping criteria convergence algorithm, see
(Puterman, 1994).

29

fiBoutilier, Dean, & Hanks

3.1.2 Policy Iteration

Howard's (1960) policy-iteration algorithm alternative value iteration infinitehorizon problems. Rather iteratively improving estimated value function, instead
modifies policies directly. begins arbitrary policy 0 , iterates, computing
i+1 i.
iteration algorithm comprises two steps, policy evaluation policy improvement:
1. (Policy evaluation) 2 , compute value function V (s) based
current policy .
2. (Policy improvement) 2 , find action maximizes

Qi+1(a; s) = R(s) + C (a) +

X

s0 2S

Pr(s0 ja; s) V (s0 )

(7)

Qi+1 (a ; s) > V (s), let i+1 = ; otherwise i+1 (s) = (s).21
algorithm iterates i+1 (s) = (s) states s. Step 1 evaluates current
policy solving N N linear system represented Equation 3 (one equation
2 ), computationally expensive. However, algorithm converges
optimal policy least linearly certain conditions converges superlinearly
quadratically (Puterman, 1994). practice, policy iteration tends converge many
fewer iterations value iteration. Policy iteration thus spends computational
time individual stage, result fewer stages need computed.22
Modified policy iteration (Puterman & Shin, 1978) provides middle ground
policy iteration value iteration. structure algorithm exactly
policy iteration, alternating evaluation improvement phases. key insight
one need evaluate policy exactly order improve it. Therefore, evaluation
phase involves (usually small) number iterations successive approximation (i.e.,
setting V = Vt small t, using Equation 6). tuning value
used iteration, modified policy iteration work extremely well practice
(Puterman, 1994). value iteration policy iteration special cases modified
policy iteration, corresponding setting = 0 = 1, respectively.
number variants value policy iteration proposed.
instance, asynchronous versions algorithms require value function
constructed, policy improved, state lockstep. case value iteration
infinite-horizon problems, long state updated suciently often, convergence
assured. Similar guarantees provided asynchronous forms policy iteration. variants important tools understanding various online approaches
solving MDPs (Bertsekas & Tsitsiklis, 1996). nice discussion asynchronous dynamic
programming, see (Bertsekas, 1987; Bertsekas & Tsitsiklis, 1996).
21. Q-function defined Equation 7, called use Q-learning (Watkins & Dayan,
1992), gives value performing action state assuming value function V accurately ects
future value.
22. See (Littman et al., 1995) discussion complexity algorithm.

30

fiDecision-Theoretic Planning: Structural Assumptions

3.1.3 Undiscounted Infinite-Horizon Problems

diculty finding optimal solutions infinite-horizon problems total reward
grow without limit time. Thus, problem definition must provide way
ensure value metric bounded arbitrarily long horizons. use expected
total discounted reward optimality criterion offers particularly elegant way
guarantee bound, since infinite sum discounted rewards finite. However, although
discounting appropriate certain classes problems (e.g., economic problems,
system may terminate point certain probability), many realistic
AI domains dicult justify counting future rewards less present rewards,
discounted-reward criterion appropriate.
variety ways bound total reward undiscounted problems.
cases problem structured reward bounded. planning problems,
example, goal reward collected once, actions incur cost.
case total reward bounded problem legitimately posed
terms maximizing total expected undiscounted reward many cases (e.g., goal
reached certainty).
cases discounting inappropriate total reward unbounded, different
success criteria employed. example, problem instead posed one
wish maximize expected average reward per stage, gain. Computational
techniques constructing gain-optimal policies similar dynamic-programming
algorithms described above, generally complicated, convergence rate
tends quite sensitive communicating structure periodicity MDP.
Refinements gain optimality studied. example, bias optimality
used distinguish two gain-optimal polices giving preference policy whose total
reward initial segment policy execution larger. Again, algorithms
complicated discounted problems, variants standard
policy value iteration. See (Puterman, 1994) details.
3.1.4 Dynamic Programming POMDPs

Dynamic programming techniques applied partially observable settings well
(Smallwood & Sondik, 1973). main diculty building policies situations
state fully observable that, since past observations provide information
system's current state, decisions must based information gleaned
past. result, optimal policy depend observations agent made since
beginning execution. history-dependent policies grow size exponential
length horizon. history-dependence precludes dynamic programming,
observable history summarized adequately probability distribution
(Astrom, 1965), policies computed function distributions, belief
states.
key observation Sondik (Smallwood & Sondik, 1973; Sondik, 1978)
one views POMDP time-separable value function taking state space
set probability distributions , one obtains fully observable MDP
solved dynamic programming. (computational) problem approach
31

fiBoutilier, Dean, & Hanks

state space FOMDP N -dimensional continuous space,23 special
techniques must used solve (Smallwood & Sondik, 1973; Sondik, 1978).
explore techniques here, note currently practical
small problems (Cassandra et al., 1994; Cassandra, Littman, & Zhang, 1997;
Littman, 1996; Lovejoy, 1991b). number approximation methods, developed
(Lovejoy, 1991a; White III & Scherer, 1989) AI (Brafman, 1997; Hauskrecht, 1997;
Parr & Russell, 1995; Zhang & Liu, 1997), used increase range solvable
problems, even techniques presently limited practical value.
POMDPs play key role reinforcement learning well, \natural state
space" consisting agent observations provides incomplete information underlying system state (see, e.g., McCallum, 1995).

3.2 AI Planning State-Based Search
noted Section 2.7 classical AI planning problem formulated
infinite-horizon MDP therefore solved using algorithm value iteration.
Recall two assumptions classical planning specialize general MDP model, namely
determinism actions use goal states instead general reward function.
third assumption|that want construct optimal course action starting
known initial state|does counterpart FOMDP model presented above,
since policy dictates optimal action state stage plan.
see below, interest online algorithms within AI led revised formulations
FOMDPs take initial current states account.
Though defined classical planning problem earlier non-observable process
(NOMDP), solved fully observable. let G set goal states
sinit initial state. Applying value iteration type problem equivalent
determining reachability goal states system states. instance,
make goal states absorbing, assign reward 1 transitions 2 , G
g 2 G 0 others, set states Vk (s) > 0 exactly
set states lead goal state.24 particular, Vk (sinit ) > 0,
successful plan constructed extracting actions k-stage (finite-horizon)
policy produced value iteration. determinism assumption means agent
predict state perfectly every stage execution; fact cannot observe
state unimportant.
assumptions commonly made classical planning exploited computationally value iteration. First, terminate process first iteration k
Vk (sinit) > 0, interested plans begin sinit, acting
optimally every possible start state. Second, terminate value iteration jS j
iterations: VjS j(sinit ) = 0 point, algorithm searched every possible
state guarantee solution plan exists. Therefore, view classical planning finite-horizon decision problem horizon jS j. use value iteration
23. accurately, N -dimensional simplex, (N , 1)-dimensional space.
24. Specifically, Vk (s) indicates probability one reaches goal region optimal
policy 2 , G stochastic settings. deterministic case discussed, value must
1 0.

32

fiDecision-Theoretic Planning: Structural Assumptions

equivalent using Floyd-Warshall algorithm find minimum-cost path
weighted graph (Floyd, 1962).
3.2.1 Planning Search

value iteration can, theory, used classical planning, take advantage
fact goal initial states known. particular, computes value
policy assignment states stages. wasteful since optimal
actions computed states cannot reached sinit cannot possibly
lead state g 2 G. problematic jS j large, since iteration
value iteration requires O(jS jjAj) computations. reason dynamic programming
approaches used extensively AI planning.
restricted form value function, especially fact initial goal states
given, makes advantageous view planning graph-search problem. Unlike
general FOMDPs, generally known priori states desirable
respect (long-term) value, well-defined set target states classical planning
problem makes search-based algorithms appropriate. approach taken
AI planning algorithms.
One way formulate problem graph search make node graph
correspond state . initial state goal states identified,
search proceed either forward backward graph, directions
simultaneously.
forward search, initial state root search tree. node chosen
tree's fringe (the set leaf nodes), feasible actions applied.
action application extends plan one step (or one stage) generates unique new
successor state, new leaf node tree. node pruned state
defines already tree. search ends state identified member
goal set (in case solution plan extracted tree), branches
pruned (in case solution plan exists). Forward search attempts build
plan beginning end, adding actions end current sequence actions.
Forward search never considers states cannot reached sinit .
Backward search viewed several different ways. could arbitrarily select
g 2 G root search tree, expand search tree fringe
selecting state fringe adding tree states action would
cause system enter chosen state. general, action give rise
one predecessor vertex, even actions deterministic. state pruned
appears search tree already. search terminates initial state added
tree, solution plan extracted tree. search similar
dynamic-programming-based algorithms finding shortest path graph.
difference backward search considers states depth k search
tree reach chosen goal state within k steps. Dynamic programming algorithms,
contrast, visit every state every stage search.
One diculty backward approach described commitment
particular goal state. course, assumption relaxed, algorithm could
\simultaneously" search paths goal states adding first level search
33

fiBoutilier, Dean, & Hanks

tree vertex reach g 2 G. see Section 5 goal regression
viewed this, least implicitly.
generally thought regression (or backward) techniques effective
practice progression (or forward) methods. reasoning branching factor
forward graph, number actions feasibly applied given
state, substantially larger branching factor reverse graph,
number operators could bring system given state.25 especially true
goal sets represented small set propositional literals (Section 5). two
approaches mutually exclusive, however: one mix forward backward expansions underlying problem graph terminate forward path backward
path meet.
important thing observe algorithms restrict attention relevant reachable states. forward search, states
reached sinit ever considered: provide benefit dynamic programming
methods states reachable, since unreachable states cannot play role constructing successful plan. backward approaches, similarly, states lying path
goal region G considered, significant advantages dynamic
programming fraction state space connected goal region.
contrast, dynamic programming methods (with exception asynchronous methods) must examine entire state space every iteration. course, ability ignore
parts state space comes planning's stringent definition relevant: states
G positive reward, states matter except extent move agent
closer goal, choice action states unreachable sinit interest.
state-based search techniques use knowledge specific initial state specific
goal set constrain search process, forward search exploit knowledge
goal set, backward search exploit knowledge initial state. GraphPlan
algorithm (Blum & Furst, 1995) viewed planning method integrates
propagation forward reachability constraints backward goal-informed search.
describe approach Section 5. Furthermore, work partial order planning (POP)
viewed slightly different approach form search. described
Section 5, discuss feature-based intensional representations MDPs
planning problems.
3.2.2 Decision Trees Real-time Dynamic Programming

State-based search techniques limited deterministic, goal-oriented domains. Knowledge initial state exploited general MDPs well, forming basis
decision tree search algorithms. Assume given finite-horizon FOMDP
horizon initial state sinit . decision tree rooted sinit constructed much
way search tree deterministic planning problem (French, 1986). action
applicable sinit forms level 1 tree. states s0 result positive probability actions occur applied sinit placed level 2, arc
25. See Bacchus et al. (1995, 1998) recent work makes case progression good
search control, Bonet et al. (1997) argue progression deterministic planning useful
integrating planning execution.

34

fiDecision-Theoretic Planning: Structural Assumptions

init
V = max(V1 , V2 )
a1
p1

a2
p2

s1
a1

p3

s2
a2

a1

V1
p4

s3
a2

a1

V2 = p V 3 + p V4
3
4

s4
a2

a1

a2

V3

V4

Figure 12: initial stages decision tree evaluating action choices sinit .
value action expected value successor states, value
state maximum values successor actions (as indicated
dashed arrows selected nodes).
labeled probability Pr(s0 ja; sinit ) relating s0 a. Level 3 actions applicable
states level 2, on, tree grown depth 2T , point
branch tree path consisting positive-probability length-T trajectory rooted
sinit (see Figure 12).
relevant part optimal -stage value function optimal policy easily
computed using tree. say value node tree labeled
action expected value successor states tree (using probabilities labeling
arcs), value node tree labeled state sum R(s)
maximum value successor actions.26 rollback procedure, whereby value
leaves tree first computed values successively higher levels tree
determined using preceding values, is, fact, form value iteration. value
state level 2t precisely VT,t (s) maximizing actions form optimal
finite-horizon policy. form value iteration directed: (T , t)-stage-to-go values
computed states reachable sinit within steps. Infinite-horizon
problems solved analogous fashion one determine priori depth
required (i.e., number iterations value iteration needed) ensure convergence
optimal policy.
Unfortunately, branching factor stochastic problems generally much greater
deterministic problems. troublesome still fact one must
construct entire decision tree sure proper values computed, hence
optimal policy constructed. stands contrast classical planning search,
attention focused single branch: goal state reached, path
constructed determines satisfactory plan. worst-case behavior planning may
require searching whole tree, decision-tree evaluation especially problematic
26. States level 2T given value R(s).

35

fiBoutilier, Dean, & Hanks

entire tree must generated general ensure optimal behavior. Furthermore,
infinite-horizon problems pose diculty determining suciently deep tree.
One way around diculty use real time search (Korf, 1990). particular,
real-time dynamic programming, RTDP, proposed (Barto et al., 1995)
way approximately solving large MDPs online fashion. One interleave search
execution approximately optimal policy using form RTDP similar decisiontree evaluation follows. Imagine agent finds particular state sinit .
build partial search tree depth, perhaps uniformly perhaps
branches expanded deeply others. Partial tree construction may halted due
time pressure due assessment agent expansion tree may
fruitful. decision act must made, rollback procedure applied
partial, possibly unevenly expanded decision tree.
Reward values used evaluate leaves tree, may offer
inaccurate picture value nodes higher tree. Heuristic information
used estimate long-term value states labeling leaves. value iteration,
deeper tree, accurate estimated value root (generally speaking)
fixed heuristic. see Section 5 structured representations MDPs
provide means construct heuristics (Dearden & Boutilier, 1994, 1997). Specifically,
admissible heuristics upper lower bounds true values leaf nodes
tree, methods A* branch-and-bound search used.
key advantage integrating search execution actual outcome
action taken used prune tree branches rooted unrealized
outcomes. subtree rooted realized state expanded make
next action choice. algorithm Hansen Zilberstein (1998) viewed
variant methods stationary policies (i.e., state-action mappings)
extracted search process.
RTDP formulated Barto et al. (1995) generally form online, asynchronous value iteration. Specifically, values \rolled backed" cached used
improved heuristic estimates value function states question. technique investigated (Bonet et al., 1997; Dearden & Boutilier, 1994, 1997; Koenig
& Simmons, 1995), closely tied Korf's (1990) LRTA* algorithm. value
updates need proceed strictly using decision tree determine states; key
requirement RTDP simply actual state sinit one states whose value
updated decision-action iteration.
second way avoid computational diculties arise large search
spaces use sampling methods. methods sample space possible trajectories use sampled information provide estimates values specific courses
action. approach quite common reinforcement learning (Sutton & Barto, 1998),
simulation models often used generate experience value function
learned. present context, Kearns, Mansour Ng (Kearns, Mansour, &
Ng, 1999) investigated search methods infinite-horizon MDPs successor
states specific action randomly sampled according transition distribution.
Thus, rather expand successor states, sampled states searched. Though
method exponential \effective" horizon (or mixing rate) MDP
required expand actions, number states expanded less required
36

fiDecision-Theoretic Planning: Structural Assumptions

full search, even underlying transition graph sparse. able provide polynomial bounds (ignoring action branching horizon effects) number
trajectories need sampled order generate approximately optimal behavior
high probability.

3.3 Summary

seen dynamic programming methods state-based search methods
used fully observable non-observable MDPs, forward search methods interpretable \directed" forms value iteration. Dynamic programming algorithms
generally require explicit enumeration state space iteration, search
techniques enumerate reachable states; branching factor may require that,
sucient depth search tree, search methods enumerate individual states multiple
times, whereas considered per stage dynamic programming. Overcoming diculty search requires use cycle-checking multiple-path-checking
methods.
note search techniques applied partially observable problems well.
One way search space belief states (just dynamic programming applied belief space MDP|see Section 2.10.2). Specifically, belief
states play role system states stochastic effects actions belief states
induced specific observation probabilities, since observation distinct, fixed
effect belief state. type approach pursued (Bonet & Geffner,
1998; Koenig & Simmons, 1995).

4. Factored Representations

point discussion MDPs used explicit extensional representation
set states (and actions) states enumerated directly. many cases
advantageous, representational computational point view, talk
properties states sets states: set possible initial states, set
states action executed, on. generally convenient
compact describe sets states based certain properties features enumerate
explicitly. Representations descriptions objects substitute objects
called intensional technique choice AI systems.
intensional representation planning systems often built defining set
features sucient describe state dynamic system interest.
example Figure 2, state described set six features: robot's location,
lab's tidiness, whether mail present, whether robot mail, whether
pending coffee request, whether robot coffee. first
second features take one five values, last four take one two
values (true false). assignment values six features completely defines state;
state space thus comprises possible combinations feature values, jSj = 400.
feature, factor, typically assigned unique symbolic name, indicated
second column Figure 2. fundamental tradeoff extensional intensional
representations becomes clear example. extensional representation coffee
example views space possible states single variable takes 400 possible
37

fiBoutilier, Dean, & Hanks

values, whereas intensional factored representation views state cross product
six variables, takes substantially fewer values. Generally, state space
grows exponentially number features required describe system.
fact state system described using set features allows one
adopt factored representations actions, rewards components MDP.
factored action representation, instance, one generally describes effect action
specific state features rather entire states. often provides considerable representational economy. instance, Strips action representation (Fikes & Nilsson,
1971), state transitions induced actions represented implicitly describing
effects actions features change value action executed.
Factored representations compact individual actions affect relatively
features, effects exhibit certain regularities. Similar remarks apply
representation reward functions, observation models, on. regularities
make factored representations suitable many planning problems often exploited
planning decision-making algorithms.
factored representations long used classical AI planning, similar
representations adopted recent use MDP models within AI.
section (Section 4), focus economy representation afforded exploiting
structure inherent many planning domains. following section (Section 5),
describe structure|when made explicit factored representations|can
exploited computationally plan policy construction.

4.1 Factored State Spaces Markov Chains

begin examining structured states, systems whose state described using
finite set state variables whose values change time.27 simplify illustration
potential space savings, assume state variables boolean.
variables, size state space jSj = N = 2M . large , specifying
representing dynamics explicitly using state-transition diagrams N N matrices
impractical. Furthermore, representing reward function N -vector, specifying
observational probabilities, similarly infeasible. Section 4.2, define class
problems dynamics represented O(M ) space many cases. begin
considering represent Markov chains compactly consider incorporating
actions, observations rewards.
let state variable X take finite number values let
X stand
set possible values. assume
X finite, though much follows
applied countable state action spaces well. say state space
specified using one state variable (this variable denoted general model, taking
values ). state space factored one state variable. state
possible assignment values variables. Letting Xi represent ith state
variable, state space cross product value spaces individual state

variables; is, =
i=1
Xi . denotes state process stage t,
let Xit random variable representing value ith state variable stage t.
27. variables often called uents AI literature (McCarthy & Hayes, 1969). classical
planning, atomic propositions used describe domain.

38

fiDecision-Theoretic Planning: Structural Assumptions

Bayesian network (Pearl, 1988) representational framework compactly representing probability distribution factored form. Although networks typically used represent atemporal problem domains, apply techniques
represent Markov chains, encoding chain's transition probabilities network
structure (Dean & Kanazawa, 1989).
Formally, Bayes net directed acyclic graph vertices correspond random
variables edge two variables indicates direct probabilistic dependency
them. network constructed ects implicit independencies among
variables. network must quantified specifying probability variable
(vertex) conditioned possible values immediate parents graph. addition,
network must include marginal distribution: unconditional probability
vertex parents. quantification captured associating conditional
probability table (CPT) variable network. Together independence
assumptions defined graph, quantification defines unique joint distribution
variables network. probability event space
computed using algorithms exploit independencies represented within graphical
structure. refer Pearl (1988) details.
Figures 3(a)-(c) (page 7) special cases Bayes nets called \temporal" Bayesian
networks. networks, vertices graph represent system's state different
time points arcs represent dependencies across time points. temporal networks,
vertex's parent temporal predecessor, conditional distributions transition
probability distributions, marginal distributions distributions initial states.
networks Figure 3 ect extensional representation scheme states
explicitly enumerated, techniques building performing inference probabilistic temporal networks designed especially application factored representations.
Figure 13 illustrates two-stage temporal Bayes net (2TBN) describing state-transition
probabilities associated Markov chain induced fixed policy executing
action CClk (repeatedly moving counterclockwise). 2TBN, set variables
partitioned corresponding state variables given time (or stage)
corresponding state variables time + 1. Directed arcs indicate probabilistic dependencies variables Markov chain. Diachronic arcs directed
time variables time + 1 variables, synchronic arcs directed
variables time + 1. Figure 13 contains diachronic arcs; synchronic arcs
discussed later section.
Given state time t, network induces unique distribution states +1.
quantification network describes state particular variable changes
function certain state variables. lack direct arc (or generally directed
path synchronic arcs among + 1 variables) variable Xt another
variable Yt+1 means knowledge Xt irrelevant prediction (immediate,
one-stage) evolution variable Markov process.
Figure 13 shows compact representation best circumstances,
many potential links one stage next omitted. graphical
representation makes explicit fact policy (i.e., action CClk) affect
state variable Loc, exogenous events ArrM, ReqC, Mess affect
39

fiBoutilier, Dean, & Hanks

Loc

Loc





CR

CR

P(Loc t+1 )
Loc L C H
0.1 0.9 0 0 0
0 0.1 0.9 0 0
L
0 0 0.1 0.9 0
C
0 0 0 0.1 0.9
H 0.9 0 0 0 0.1
P(CR t+1)
CR

f

RHC

f
1.0 0
0.2 0.8

RHC

RHM

RHM





Time

Time t+1

P(RHC t+1)
RHC f
1.0 0
f
0 1.0

Figure 13: factored 2TBN Markov chain induced moving counterclockwise
(with selected CPTs shown).
variables , CR, Tidy, respectively.28 Furthermore, dynamics Loc (and
variables) described using knowledge state parent
variables; instance, distribution Loc +1 depends value Loc
previous stage (e.g., Loct = O, Loct+1 = probability 0:9 Loct+1 =
probability 0:1). Similarly, CR become true probability 0:2 (due ReqC
event), true, cannot become false (under simple policy); RHC remains
true (or false) certainty true (or false) previous stage. Finally,
effects relevant variables independent. instantiation variables
time t, distribution next states computed multiplying conditional
probabilities relevant + 1 variables.
ability omit arcs graph based locality independence action
effects strong effect number parameters must supplied complete
model. Although full transition matrix CClk would size 4002 = 160000,
transition model Figure 13 requires 66 parameters.29
example shows 2TBNs exploit independence represent Markov chains
compactly, example extreme effectively relationship
variables|the chain viewed product six independently evolving processes.
28. show CPTs brevity.
29. fact, exploit fact probabilities sum one leave one entry unspecified per row
CPT explicit transition matrix. case, 2TBN requires 48 explicit parameters,
transition matrix requires 400 300 = 159; 600 entries. generally ignore fact comparing
sizes representations.

40

fiDecision-Theoretic Planning: Structural Assumptions

Loc

Loc





CR

CR

RHC

RHC

RHM

RHM





Time

Time t+1

Loc RHC


L

C



H


f
L
f
C
f

f
H
f

P(Loc t+1 )
L C H
1.0 0 0 0 0
0 0.1 0.9 0 0
0 0 0.1 0.9 0
0 0 0 0.1 0.9
0.9 0 0 0 0.1
0.1 0.9 0 0 0
0 0.1 0.9 0 0
0 0 0.1 0.9 0
0 0 0 0.1 0.9
P(CR t+1)
0.9 0 0 0 0.1
Loc RHC CR f

P(RHC t+1)
Loc RHC
f
0.0 1.0

f 0.0 1.0

1.0 0.0
L
f 0.0 1.0
L
1.0 0.0
C
f 0.0 1.0
C
etc.
etc.





L
L
L
L



f
f


f
f
etc.


f

f

f

f

.05 .95
0.2 0.8
1.0 0.0
0.2 0.8
1.0 0.0
0.2 0.8
1.0 0.0
0.2 0.8
etc.

Figure 14: 2TBN Markov chain induced moving counterclockwise delivering coffee.

41

fiBoutilier, Dean, & Hanks

general, \subprocesses" interact, still exhibit certain independencies
regularities exploited 2TBN representation. consider two distinct
Markov chains exhibit different types dependencies.
Figure 14 illustrates 2TBN representing Markov chain induced following
policy: robot consistently moves counterclockwise unless oce
coffee, case delivers coffee user. Notice different variables
dependent: instance, predicting value RHC + 1 requires knowing values
Loc RHC t. CPT RHC shows robot retains coffee stage + 1
certainty, stage t, locations except (where executes DelC,
thus losing coffee). variable Loc depends value RHC. location
change Figure 13 one exception: robot oce coffee,
location remains (since robot move, executes DelC). effect
variable CR explained follows: robot oce delivers coffee
possession, fulfill outstanding coffee request. However, 0:05 chance CR
remaining true conditions indicates 5% chance spilling coffee.
Even though dependencies (i.e., additional diachronic arcs) 2TBN,
still requires 118 parameters. Again, distribution resulting states determined multiplying conditional distributions individual variables. Even though
variables \related," state known, variables time + 1 (Loct+1 ,
RHCt+1 , etc.) independent. words,
Pr(Loct+1 ; t+1 ; CRt+1 ; RHCt+1 ; RHMt+1 ; t+1 jS ) =
t)
Pr(Loct+1 jS ) Pr(T t+1 jS ) Pr(CRt+1 jS ) Pr(RHCt+1 jS ) Pr(RHMt+1 jS ) Pr(M t+1 jS(8)
Figure 15 illustrates 2TBN representing Markov chain induced policy
above, assume act moving counterclockwise slightly
different effect. suppose that, robot moves hallway
adjacent location, 0:3 chance spilling coffee possession:
fragment CPT RHC Figure 15 illustrates possibility. Furthermore,
robot carrying mail whenever loses coffee (whether \accidentally" \intentionally"
via DelC action), 0:5 chance lose mail. Notice effects
policy variables RHC RHM correlated: one cannot accurately predict
probability RHMt+1 without determining probability RHCt+1 . correlation
modeled synchronic arc RHC RHM + 1 slice network.
independence +1 variables given hold 2TBNs synchronic
arcs. Determining probability resulting state requires simple probabilistic
reasoning, example, application chain rule. example, write
Pr(RHCt+1 ; RHMt+1 jS ) = Pr(RHMt+1 jRHCt+1 ; ) Pr(RHCt+1 jS )
joint distribution + 1 variables given computed Equation 8 above, term replacing Pr(RHCt+1 jS ) Pr(RHMt+1 jS )|while two
variables correlated, remaining variables independent.
refer 2TBNs synchronic arcs, one Figure 14, simple 2TBNs.
General 2TBNs allow synchronic well diachronic arcs, Figure 15.
42

fiDecision-Theoretic Planning: Structural Assumptions

Loc

Loc





CR

CR

RHC

RHC

RHM

RHM





Time

Time t+1

Pr(RHC t+1 )
Loc RHC f
1.0 1.0

f 0.0 1.0

0.7 0.3
H
f 0.0 1.0
H
1.0 0.0
C
f 0.0 1.0
C
etc.
etc.
Pr(RHMt+1 )
RHC RHC t+1 RHMt f
1.0 0.0



0.0 1.0
f


0.5 0.5

f

0.0 1.0
f
f

1.0 0.0


f
0.0 1.0
f

f
1.0 0.0

f
f
0.0 1.0
f
f
f

Figure 15: 2TBN Markov chain induced moving counterclockwise delivering coffee correlated effects.

4.2 Factored Action Representations

extended Markov chains account different actions, must extend
2TBN representation account fact state transitions uenced
agent's choice action. discuss variety techniques specifying transition
matrices exploit factored state representation produce representations
natural compact explicit transition matrices.
4.2.1 Implicit-Event Models

begin implicit-event model Section 2.3 effects actions
exogenous events combined single transition matrix. consider explicitevent models Section 4.2.4. saw previous section, algorithms value
policy iteration require use transition models ect ultimate transition
probabilities, including effects exogenous events.
One way model dynamics fully observable MDP represent action
separate 2TBN. 2TBN shown Figure 13 seen representation
action CClk (since policy inducing Markov chain example consists
repeated application action alone). network fragment Figure 16(a)
illustrates interesting aspects 2TBN DelC action including effects
exogenous events. above, robot satisfies outstanding coffee request delivers
coffee oce coffee (with 0:05 chance spillage), shown
conditional probability table CR. effect RHC explained follows:
43

fiBoutilier, Dean, & Hanks

Loc

RHC

CR
Time

Loc

RHC

CR
Time t+1

(a)

Pr(RHC t+1 )
Loc RHC f

0.0 1.0

f 0.0 1.0
L
0.3 0.7
f 0.0 1.0
L
0.3 0.7
C
f 0.0 1.0
C
etc.
etc.
Pr(CR t+1 )
Loc RHC CR f
.05 .95


f 0.2 0.8


1.0 0.0
f

f 0.2 0.8
f

1.0 0.0

L
f 0.2 0.8

L
1.0 0.0
f
L
f 0.2 0.8
f
L
etc.
etc.





CR

0.05

RHC

Loc

else

f

f




CR

0.2 1.0

CR

1.0

f

f

0.2

0.2

(b)




CR

0.05

RHC

f

Loc
else

f
f

0.2

CR



1.0

(c)

Figure 16: factored 2TBN action DelC (a) structured CPT representations (b,c).
robot loses coffee (to user spillage) delivers oce; attempts
delivery elsewhere, 0:7 chance random passerby take coffee
robot.
case Markov chains, effects actions different variables
correlated, case must introduce synchronic arcs. correlations
thought ramifications (Baker, 1991; Finger, 1986; Lin & Reiter, 1994).
4.2.2 Structured CPTs

conditional probability table (CPT) node CR Figure 16(a) 20 rows, one
assignment parents. However, CPT contains number regularities.
Intuitively, ects fact coffee request met successfully (i.e.,
variable becomes false) 95% time DelC executed, robot coffee
right location (the user's oce). Otherwise, CR remains true true
becomes true probability 0:2 not. words, three distinct cases
considered, corresponding three \rules" governing (stochastic) effect DelC
CR. represented compactly using decision tree representation
(with \else" branches summarize groups cases involving multivalued variables
Loc) shown Figure 16(b), compactly still using decision graph
(Figure 16(c)). tree- graph-based representations CPTs, interior nodes labeled
parent variables, edges values variables, leaves terminals distributions
child variable's values.30
Decision-tree decision-graph representations used represent actions fully
observable MDPs (Boutilier et al., 1995; Hoey, St-Aubin, Hu, & Boutilier, 1999)
30. child boolean, label leaves probability variable true (the
probability variable false one minus value).

44

fiDecision-Theoretic Planning: Structural Assumptions

described detail (Poole, 1995; Boutilier & Goldszmidt, 1996).31 Intuitively, trees
graphs embody rule-like structure present family conditional distributions
represented CPT, settings consider often yield considerable representational compactness. Rule-based representations used directly Poole (1995,
1997a) context decision processes often compact trees (Poole,
1997b). generically refer representations type 2TBNs structured CPTs.
4.2.3 Probabilistic STRIPS Operators

2TBN representation viewed oriented toward describing effects actions
distinct variables. CPT variable expresses (stochastically) changes
(or persists), perhaps function state certain variables. However,
long noted AI research planning reasoning action
actions change state limited ways; is, affect relatively small number
variables. One diculty variable-oriented representations 2TBNs one
must explicitly assert variables unaffected specific action persist value (e.g.,
see CPT RHC Figure 13)|this instance infamous frame problem
(McCarthy & Hayes, 1969).
Another form representation actions might called outcome-oriented representation: one explicitly describes possible outcomes action possible joint
effects variables. idea underlying Strips representation
classical planning (Fikes & Nilsson, 1971).
classical Strips operator described precondition set effects.
former identifies set states action executed, latter
describes input state changes result taking action. probabilistic
Strips operator (PSO) (Hanks, 1990; Hanks & McDermott, 1994; Kushmerick et al., 1995)
extends Strips representation two ways. First, allows actions different
effects depending context, second, recognizes effects actions
always known certainty.32
Formally, PSO consists set mutually exclusive exhaustive logical formulae,
called contexts, stochastic effect associated context. Intuitively, context discriminates situations action differing stochastic effects.
stochastic effect set change sets|a simple list variable values|with
probability attached change set, requirement probabilities sum
one. semantics stochastic effect described follows: stochastic
effect action applied state s, possible resulting states determined
change sets, occurring corresponding probability; resulting state associated change set constructed changing variable values state match
change set, unmentioned variables persist value. Note since one
31. fact certain direct dependencies among variables Bayes net rendered irrelevant
specific variable assignments studied generally guise context-specific independence
(Boutilier, Friedman, Goldszmidt, & Koller, 1996); see (Geiger & Heckerman, 1991; Shimony, 1993)
related notions.
32. conditional nature effects feature deterministic extension Strips known ADL
(Pednault, 1989).

45

fiBoutilier, Dean, & Hanks

RHC


f

Loc


-CR -RHC +M
-CR -RHC
-RHC +M
-RHC

+CR +M
+CR
+M
nil

else

-RHC +CR +M
-RHC +CR
-RHC +M
-RHC
+CR +M
+CR
+M
nil

0.19
0.76
0.01
0.04

0.04
0.16
0.16
0.64

0.028
0.112
0.112
0.448
0.012
0.048
0.048
0.192

Figure 17: PSO representation DelC action.
Loc


L

+Loc(L) 0.9
nil
0.1

+Loc(C) 0.9
nil
0.1

H

C



+Loc(M) 0.9
nil
0.1

+Loc(H) 0.9
nil
0.1

+Loc(O) -RHC -RHM
+Loc(O) -RHC
+Loc(O)
-RHC -RHM
-RHC
nil

0.135
0.135
0.63
0.015
0.015
0.07

Figure 18: PSO representation simplified CClk action.
context hold state s, transition distribution action state
easily determined.
Figure 17 gives graphical depiction PSO DelC action (shown 2TBN
Figure 16). three contexts :RHC, RHC ^ Loc(O) RHC ^:Loc(O) represented
using decision tree. leaf branch decision tree stochastic effect
(set change sets associated probabilities) determined corresponding context.
example, RHC ^ Loc(O) holds, action four possible effects: robot loses
coffee; may may satisfy coffee request (due 0:05 chance spillage);
mail may may arrive. Notice outcome spelled completely.
number outcomes two contexts rather large due possible exogenous
events (we discuss Section 4.2.4).33
key difference PSOs 2TBNs lies treatment persistence.
variables unaffected action must given CPTs 2TBN model,
variables mentioned PSO model (e.g., compare variable Loc
representations DelC). way, PSOs said \solve" frame problem,
since unaffected variables need mentioned action's description.34
33. keep Figure 17 manageable, ignore effect exogenous event Mess variable .
34. discussion frame problem 2TBNs, see (Boutilier & Goldszmidt, 1996).

46

fiDecision-Theoretic Planning: Structural Assumptions

ArrM

Mess

Loc

Loc

Loc

Loc

RHC

RHC

RHC

RHC

RHM

RHM

RHM

RHM

CR

CR

CR

CR



















t+ 1

t+ 2

t+1

Figure 19: simplified explicit-event model DelC.
PSOs provide effective means representing actions correlated effects.
Recall description CClk action captured Figure 15, robot may
drop coffee moves hallway, may drop mail drops
coffee. 2TBN representation CClk, one must RHCt RHCt+1
parents RHMt+1 : must model dependence RHM change value
variable RHC. Figure 18 shows CClk action PSO format (for simplicity, ignore
occurrence exogenous events). PSO representation offer economical
representation correlated effects since possible outcomes moving
hallway spelled explicitly. Specifically, (possible) simultaneous change values
variables question made clear.
4.2.4 Explicit-Event Models

Explicit-event models represented using 2TBNs somewhat different form.
discussion Section 2.3, form taken explicit-event models depends crucially one's assumptions interplay effects action
exogenous events. However, certain assumptions even explicit-event models
rather concise.
illustrate, Figure 19 shows deliver-coffee action represented 2TBN
exogenous events explicitly represented. first \slice" network shows effects
action DelC without presence exogenous events. subsequent slices describe
effects events ArrM Mess (we use two events illustration). Notice
presence extra random variables representing occurrence events question.
CPTs nodes ect occurrence probabilities events various
47

fiBoutilier, Dean, & Hanks

conditions, directed arcs event variables state variables indicate
effects events. probabilities depend state variables general;
thus, 2TBN represents occurrence vectors (see Section 2.3) compact form.
notice that, contrast event occurrence variables, explicitly represent
action occurrence variable network, since modeling effect
system given action taken.35
example ects assumptions described Section 2.3, namely, events
occur action takes place event effects commutative,
reason ordering events ArrM Mess network irrelevant.
model, system actually passes two intermediate though necessarily distinct
states goes stage stage + 1; use subscripts "1 "2 suggest
process. course, described earlier, actions events combined
decomposable way; complex combination functions modeled using 2TBNs
(for one example, see Boutilier & Puterman, 1995).
4.2.5 Equivalence Representations

obvious question one might ask concerns extent certain representations
inherently concise others. focus standard implicit-event models,
describing domain features make different representations less
suitable.
2TBN PSO representations oriented toward representing changes
values state variables induced action; key distinction lies fact
2TBNs model uence variable separately, PSO model explicitly
represents complete outcomes. simple 2TBN|a network synchronic arcs|can
used represent action cases correlations among action's
effect different state variables. worst case, effect variable
differs state, time + 1 variable must time variables parents.
regularities exploited structured CPT representations,
action requires specification O(n2n ) parameters (assuming boolean variables),
compared 22n entries required explicit transition matrix. number
parents variable bounded k, need specify n2k conditional
probabilities. reduced CPTs exhibit structure (e.g.,
represented concisely decision tree). instance, CPT captured
representation choice f (k) entries, f polynomial function
number parents variable, representation size, O(n f (k)), polynomial
number state variables. often case, instance, actions one
(stochastic) effects variable requires number (pre-) conditions hold;
not, different effect comes play.
PSO representation may concise 2TBN action multiple
independent stochastic effects. PSO requires possible change list enumerated corresponding probability occurrence. number changes grows
exponentially number variables affected action. fact evident
35. Sections 4.2.7 4.3 discuss representations model choice action explicitly variable
network.

48

fiDecision-Theoretic Planning: Structural Assumptions

RHC


f

Loc


-RHC -CR
-RHC

nil

+M 0.2
nil 0.8
1.0

else
0.95
0.05

-RHC
nil

0.7
0.3

Figure 20: \factored" PSO representation DelC action.
Figure 17, impact exogenous events affects number variables stochastically independently. problem arise respect \direct" action effects,
well. Consider action set 10 unpainted parts spray painted; part
successfully painted probability 0:9, successes uncorrelated. Ignoring
complexity representing different conditions action could take place,
simple 2TBN represent action 10 parameters (one success probability per
part). contrast, PSO representation might require one list 210 distinct change
lists associated probabilities. Thus, PSO representation exponentially
larger (in number affected variables) simple 2TBN representation.
Fortunately, certain variables affected deterministically, cause
PSO representation blow up. Furthermore, PSO representations modified
exploit independence action's effects different state variables (Boutilier &
Dearden, 1994; Dearden & Boutilier, 1997), thus escaping combinatorial diculty.
instance, might represent DelC action shown Figure 17 \factored
form" illustrated Figure 20 (for simplicity, show effect action
exogenous event ArrM). Much 2TBN, determine overall effect
combining change sets (in appropriate contexts) multiplying corresponding
probabilities.
Simple 2TBNs defined original set state variables sucient represent actions.36 Correlated action effects require presence synchronic arcs.
worst case, means time + 1 variables 2n , 1 parents.
fact,
P acyclicity condition assures worst case, total number parents
nk=1 2k , 1; thus, end specifying O(22n ) entries, required
explicit transition matrix. However, number parents (whether occurring within
time slice + 1) bounded, regularities CPTs allow compact
representation, 2TBNs still profitably used.
PSO representations compare favorably 2TBNs cases
action's effects different variables correlated. case, PSOs provide
somewhat economical representation action effects, primarily one needn't
worry frame conditions. main advantage PSOs one need enlist
aid probabilistic reasoning procedures determine transitions induced actions
correlated effects. Contrast explicit specification outcomes PSOs
type reasoning required determine joint effects action represented 2TBN
36. However, Section 4.2.6 discusses certain problem transformations render simple 2TBNs sucient
MDP.

49

fiBoutilier, Dean, & Hanks

form synchronic arcs, described Section 4.1. Essentially, correlated effects
\compiled" explicit outcomes PSOs.
Recent results Littman (1997) shown simple 2TBNs PSOs
used represent action represented 2TBN without exponential blowup
representation size. effected clever problem transformation new
sets actions propositional variables introduced (using either simple 2TBN
PSO representation). structure original 2TBN ected new planning
problem, incurring polynomial increase size input action
descriptions description policy. Though resulting policy consists actions
exist underlying domain, extracting true policy dicult.
noted, however, representation automatically constructed
general 2TBN specification, unlikely could provided directly, since
actions variables transformed problem \physical" meaning
original MDP.
4.2.6 Transformations Eliminate Synchronic Constraints

discussion assumed variables propositions used 2TBN
PSO action descriptions original state variables. However, certain problem transformations used ensure one represent action using simple 2TBNs,
long one require original state variables used. One transformation
simply clusters variables action correlated effect. new compound
variable|which takes values assignments clustered variables|can used
2TBN, removing need synchronic arcs. course, variable
domain size exponential number clustered variables.
intuitions underlying PSOs used convert general 2TBN action descriptions simple 2TBN descriptions explicit \events" dictating precise outcome
action. Intuitively, event occur k different forms, corresponding
different change list induced action (or change list respect variables
question). example, convert \action" description CClk Figure 15
explicit-event model shown Figure 21.37 Notice \event" takes values
corresponding possible effects correlated variables RHC RHM. Specifically, denotes event robot escaping hallway successfully without losing
cargo, b denotes event robot losing coffee, c denotes event losing
coffee mail. effect, event space represents possible \combined"
effects, obviating need synchronic arcs network.
4.2.7 Actions Explicit Nodes Network

One diculty 2TBN PSO approach action description action
represented separately, offering opportunity exploit patterns across actions.
instance, fact location persists actions except moving clockwise counterclockwise means \frame axiom" duplicated 2TBN actions
(this case PSOs, course). addition, ramifications (or correlated action
37. Figure 15 describes Markov chain induced policy, representation CClk easily
extracted it.

50

fiDecision-Theoretic Planning: Structural Assumptions

Loc

Hall

a: 1.0
b: 0.0
c: 0.0

Event



RHC

RHM


Loc

else

f

a:0.7 a:0.7
b:0.15 b:0.3
c:0.15 c:0.0

Loc

f

a: 1.0
b: 0.0
c: 0.0





RHC

RHC
1.0



RHM
Time

RHM
Time t+1

Loc

1.0
0.0



RHM

b
0.0

b
0.0

f

0.0

c
0.0

f

0.0

else

Event

Event

RHC

c
1.0

Figure 21: explicit-event model removes correlations.
effects) duplicated across actions well. instance, coffee request occurs (with
probability 0:2) robot ends oce, correlation duplicated
across actions. compelling example might one robot move
briefcase new location one number ways. We'd capture fact (or
ramification) contents briefcase move location briefcase
regardless action moves briefcase.
circumvent diculty, introduce choice action \random variable" network, conditioning distribution state variable transitions
value variable. Unlike state variables (or event variables explicit event models),
generally require distribution action variable|the intent simply
model schematically conditional state-transition distributions given particular
choice action. choice action dictated decision maker
policy determined. reason, anticipating terminology used uence
diagrams (see Section 4.3), call nodes decision nodes depict network diagrams boxes. variable take value action available
agent.
2TBN explicit decision node shown Figure 22. restricted example,
might imagine decision node take one two values, Clk CClk. fact
issuance coffee request t+1 depends whether robot successfully moved
(or remained in) oce represented \once" arc Loct+1 CRt+1 ,
rather repeated across multiple action networks. Furthermore, noisy persistence
actions represented (adding action PUM, however,
undercuts advantage see try combine actions).
One diculty straightforward use decision nodes (which standard
representation uence diagram literature) adding candidate actions
cause explosion network's dependency structure. example, consider two
51

fiBoutilier, Dean, & Hanks

Act

Loc

Loc

CR

CR





Time

Time t+1

Figure 22: uence diagram restricted process.

Act
X

X

X

X

X

Act

X

else

a1
a2














1.0



f


X

0.9

Z

Z
(a) action a1

Z

Z

Z

(b) action a2

1.0

f
0



f


0.9


Z

1.0

f
0

Z

(c) influence diagram

Figure 23: Unwanted dependencies uence diagrams.

52



(d) CPT



f
0

fiDecision-Theoretic Planning: Structural Assumptions

action networks shown Figure 23(a) (b). Action a1 makes true probability
0:9 X true (having effect otherwise), a2 makes true Z true.
Combining actions single network obvious way produces uence
diagram shown Figure 23(c). Notice four parent nodes, inheriting
union parents individual networks (plus action node) requiring
CPT 16 entries actions a1 a2 together eight additional entries
action affect . individual networks ect fact depends
X a1 performed Z a2 performed. fact lost
naively constructed uence diagram. However, structured CPTs used
recapture independence compactness representation: tree Figure 23(d)
captures distribution much concisely, requiring eight entries. structured
representation allows us concisely express persists actions.
large domains, expect variables generally unaffected substantial number
(perhaps most) actions, thus requiring representations uence diagrams.
See (Boutilier & Goldszmidt, 1996) deeper discussion issue relationship
frame problem.
provide distributional information action choice, hard
see 2TBN explicit decision node used represent Markov chain
induced particular policy natural way. Specifically, adding arcs state
variables time decision node, value decision node (i.e., choice
action point) dictated prevailing state.38

4.3 uence Diagrams
uence diagrams (Howard & Matheson, 1984; Shachter, 1986) extend Bayesian networks
include special decision nodes represent action choices, value nodes represent
effect action choice value function. presence decision nodes means
action choice treated variable decision maker's control. Value nodes treat
reward variable uenced (usually deterministically) certain state variables.
uence diagrams typically associated schematic representation
stationary systems, instead used tool decision analysts sequential
decision problem carefully handcrafted. generic use uence diagrams
discussed Tatman Shachter (1990). event, theory plan
construction associated uence diagrams: choice possible actions
stage must explicitly encoded model. uence diagrams are, therefore, usually
used model finite-horizon decision problems explicitly describing evolution
process stage terms state variables.
Section 4.2.7, decision nodes take values specific actions, though set
possible actions tailored particular stage. addition, analyst generally
include stage state variables thought relevant decision
subsequent stages. Value nodes key feature uence diagrams
discussed Section 4.5. Usually, single value node specified, arcs indicating
38. generally, randomized policy represented specifying distribution possible actions
conditioned state.

53

fiBoutilier, Dean, & Hanks


RHM


RHM
Rew

CR



etc.


0

CR

etc.

1 2


3

4

-7 -6.5 -6 -5.5 -5

0

1 2

3

4

-4 -3.5 -3 -2.5 -2

Figure 24: representation reward function uence diagram.
uence particular state decision variables (often multiple stages) overall
value function.
uence diagrams typically used model partially observable problems. arc
state variable decision node ects fact value state variable
available decision maker time action chosen. words,
variable's value forms part observation made time prior action
selected time +1, policy constructed refer variable. again,
allows compact specification observation probabilities associated system.
fact probability given observation depends directly certain variables
others mean far fewer model parameters required.

4.4 Factored Reward Representation

already noted common formulating MDP problems adopt
simplified value function: assigning rewards states costs actions, evaluating histories combining factors according simple function addition.
simplification alone allows representation value function significantly
parsimonious one based complex comparison complete histories. Even
representation requires explicit enumeration state action space, however,
motivating need compact representations parameters. Factored representations rewards action costs often obviate need enumerate state
action parameters explicitly.
action's effect particular variable, reward associated state often
depends values certain features state. example, robot
domain, associate rewards penalties undelivered mail, unfulfilled coffee
requests untidiness lab. reward penalty independent
variables, individual rewards associated groups states differ
values relevant variables. relationship rewards state variables
represented value nodes uence diagrams, represented diamond Figure 24.
conditional reward table (CRT) node table associates reward
every combination values parents graph. table, shown Figure 24,
locally exponential number relevant variables. Although Figure 24 shows
case stationary Markovian reward function, uence diagrams used represent
54

fiDecision-Theoretic Planning: Structural Assumptions

nonstationary history-dependent rewards often used represent value functions
finite-horizon problems.
Although worst case CRT take exponential space store, many
cases reward function exhibits structure, allowing represented compactly using
decision trees graphs (Boutilier et al., 1995), Strips-like tables (Boutilier & Dearden,
1994), logical rules (Poole, 1995, 1997a). Figure 24 shows fragment one possible
decision-tree representation reward function used running example.
independence assumptions studied multiattribute utility theory (Keeney & Raiffa,
1976) provide yet another way reward functions represented compactly.
assume component attributes reward function make independent contributions state's total reward, individual contributions combined functionally.
instance, might imagine penalizing states CR holds (partial) reward
,3, penalizing situations undelivered mail (M _ RHM) ,2,
penalizing untidiness (i) , 4 (i.e., proportion untidy things are).
reward state determined simply adding individual penalties associated feature. individual component rewards along combination
function constitute compact representation reward function. tree fragment
Figure 24, ects additive independent structure described, considerably
complex representation defines (independent) rewards individual
propositions separately. use additive reward functions MDPs considered
(Boutilier, Brafman, & Geib, 1997; Meuleau, Hauskrecht, Kim, Peshkin, Kaelbling, Dean,
& Boutilier, 1998; Singh & Cohn, 1998).
Another example structured rewards goal structure studied classical planning.
Goals generally specified single proposition (or set literals) achieved.
such, generally represented compactly. Haddawy Hanks (1998)
explore generalizations goal-oriented models permit extensions partial goal
satisfaction, yet still admit compact representations.

4.5 Factored Policy Value Function Representation

techniques studied far concerned input specification MDP:
states, actions, reward function. components problem's solution|the policy optimal value function|are candidates compact structured representation.
simplest case, stationary policy fully observable problem, policy
must associate action every state, nominally requiring representation size
O(jSj). problem exacerbated nonstationary policies POMDPs. example,
policy finite-horizon FOMDP stages generates policy size O(T jSj).
finite-horizon POMDP, possible
P observable history length < might require
different action choice; many Tk=1 bk histories generated fixed
policy, b maximum number possible observations one make following
action.39
fact policies require much space motivates need find compact functional representations, standard techniques tree structures discussed
39. methods dealing POMDPs, conversion FOMDPs belief space (see Section 2.10.2),
complex still.

55

fiBoutilier, Dean, & Hanks

CR
RHC

etc.

Loc


L C

Loc


H

H

L

C



DelC Clk Clk Cclk Cclk HRM Clk GetC
PUM Cclk

DelM Cclk

PUM Cclk

Figure 25: tree representation policy.
actions reward functions used represent policies value functions well.
focus stationary policies value functions FOMDPs, logical
function representation may used. example, Schoppers (1987) uses Strips-style
representation universal plans, deterministic, plan-like policies. Decision trees
used policies value functions (Boutilier et al., 1995; Chapman &
Kaelbling, 1991). example policy robot domain specified decision tree
given Figure 25. policy dictates that, instance, CR RHC true: (a)
robot deliver coffee user oce, (b) move toward oce
oce, unless (c) mail mailroom, case
pickup mail way.

4.6 Summary

section discussed number compact factored representations components
MDP. began discussing intensional state representations, temporal Bayesian
networks device representing system dynamics. Tree-structured conditional
probability tables (CPTs) probabilistic Strips operators (PSOs) introduced
alternative transition matrices. Similar tree structures logical representations
introduced representing reward functions, value functions, policies.
representations often used describe problem compactly,
offer guarantee problem solved effectively. next
section explore algorithms use factored representations avoid iterating
explicitly entire set states actions.

5. Abstraction, Aggregation, Decomposition Methods

greatest challenge using MDPs basis DTP lies discovering computationally feasible methods construction optimal, approximately optimal satisficing
policies. course, arbitrary decision problems intractable|even producing satisficing
approximately optimal policies generally infeasible. However, previous sections
suggest many realistic application domains may exhibit considerable structure,
furthermore structure modeled explicitly exploited typical
problems solved effectively. instance, structure type lead compact
56

fiDecision-Theoretic Planning: Structural Assumptions

factored representations input data output policies, often polynomial-sized
respect number variables actions describing problem. suggests
compact problem representations, policy construction techniques developed exploit structure tractable many commonly occurring problem
instances.
dynamic programming state-based search techniques described Section 3 exploit structure different kind. Value functions decomposed
state-dependent reward functions, state-based goal functions, tackled dynamic
programming regression search, respectively. algorithms exploit structure
decomposable value functions prevent search explicitly possible
policies. However, algorithms polynomial size state space,
curse dimensionality makes even algorithms infeasible practical problems.
Though compact problem representations aid specification large problems,
clear large system specified compactly representation exploits
\regularities" found domain. Recent AI research DTP stressed using
regularities implicit compact representations speed planning process.
techniques focus optimal approximately optimal policy construction.
following subsection focus abstraction aggregation techniques, especially manipulate factored representations. Roughly, techniques allow
explicit implicit grouping states indistinguishable respect certain characteristics (e.g., value optimal action choice). refer set states grouped
manner aggregate abstract state , sometimes cluster, assume
set abstract states constitutes partition state space; say, every state
exactly one abstract state union abstract states comprises entire state
space.40 grouping similar states, abstract state treated single state, thus
alleviating need perform computations state individually. techniques
used approximation elements abstract state approximately
indistinguishable (e.g., values states lie within small interval).
look use problem decomposition techniques MDP
broken various pieces, solved independently; solutions
pieced together used guide search global solution. subprocesses whose
solutions interact minimally treated independent, might expect approximately
optimal global solution. Furthermore, structure problem requires solution
particular subproblem only, solutions subproblems ignored
altogether.
Related use reachability analysis restrict attention \relevant" regions
state space. Indeed, reachability analysis communicating structure MDP
used form certain types decompositions. Specifically, distinguish serial
decompositions parallel decompositions.
result serial decomposition viewed partitioning state space
blocks, representing (more less) independent subprocess solved.
serial decomposition, relationship blocks generally complicated
case abstraction aggregation. partition resulting decomposition,
40. might group states non-disjoint sets cover entire state space. consider
soft-state aggregation here, see (Singh, Jaakkola, & Jordan, 1994).

57

fiBoutilier, Dean, & Hanks

states within particular block may behave quite differently respect (say) value
dynamics. important consideration choosing decomposition possible
represent block compactly compute eciently consequences moving
one block another and, further, subproblems corresponding subprocesses
solved eciently.
parallel decomposition somewhat closely related abstract MDP.
MDP divided \parallel sub-MDPs" decision action causes
state change within sub-MDP. Thus, MDP cross product join
sub-MDPs (in contrast union, serial decomposition). brie discuss several
methods based parallel MDP decomposition.

5.1 Abstraction Aggregation
One way problem structure exploited policy construction relies notion
aggregation|grouping states indistinguishable respect certain problem
characteristics. example, might group together states optimal
action, value respect k-stage-to-go value function.
aggregates constructed solution problem.
AI, emphasis generally placed particular form aggregation, namely
abstraction methods, states aggregated ignoring certain problem features.
policy Figure 25 illustrates type abstraction: states CR,
RHC Loc(O) true grouped, action selected
state. Intuitively, three propositions hold, problem features ignored
abstracted away (i.e., deemed irrelevant). decision-tree representation
policy value function partitions state space distinct cluster leaf
tree. representations (e.g., Strips-like rules) abstract state space similarly.
precisely type abstraction used compact, factored representations actions goals discussed Section 4. 2TBN shown Figure 16,
effect action DelC variable CR given CPT CRt+1 ; however,
(stochastic) effect state parent variables
value. representation abstracts away variables, combining states
distinct values irrelevant (non-parent) variables. Intensional representations often
make easy decide features ignore certain stage problem solving,
thus (implicitly) aggregate state space.
least three dimensions along abstractions type compared. first uniformity: uniform abstraction one variables deemed
relevant irrelevant uniformly across state space, nonuniform abstraction allows certain variables ignored certain conditions others.
distinction illustrated schematically Figure 26. tabular representation CPT
viewed form uniform abstraction|the effect action variable
distinguished clusters states differ value parent variable,
distinguished states agree parent variables disagree others|while
decision tree representation CPT embodies nonuniform abstraction.
second dimension comparison accuracy. States grouped together
basis certain characteristics, abstraction called exact states within
58

fiDecision-Theoretic Planning: Structural Assumptions

Uniform
ABC
ABC

ABC
ABC

ABC
ABC

ABC
ABC

Nonuniform

B

5.3
5.3

AB

=

ABC

C

ABC

Exact
5.3
5.3



Approximate
5.3
5.2

2.9
2.9

2.9
2.7

5.5

9.3
9.3

9.3
9.0

5.3

Adaptive

Fixed

Figure 26: Different forms state space abstraction.
cluster agree characteristic. non-exact abstraction called approximate.
illustrated schematically Figure 26: exact abstraction groups together states
agree value assigned value function, approximate abstraction
allows states grouped together differ value. extent states
differ often used measure quality approximate abstraction.
third dimension adaptivity. Technically, property abstraction
itself, abstractions used particular algorithm. adaptive abstraction
technique one abstraction change course computation,
fixed abstraction scheme groups together states (again, see Figure 26).
example, one imagine using abstraction representation value function
V k , revising abstraction represent V k+1 accurately.
Abstraction aggregation techniques studied literature
MDPs. Bertsekas Castanon (1989) develop adaptive aggregation (as opposed
abstraction) technique. proposed method operates state spaces, however,
therefore exploit implicit structure state space itself. adaptive, uniform
abstraction method proposed Schweitzer et al. (1985) solving stochastic queuing models. methods, often referred aggregation-disaggregation procedures,
typically used accelerate calculation value function fixed policy. Valuefunction calculation requires computational effort least quadratic size state
space, impractical large state spaces. aggregation-disaggregation procedures, states first aggregated clusters. system equations solved,
series summations performed, requiring effort cubic number
clusters. Next, disaggregation step performed cluster, requiring effort least
linear size cluster. net result total work, least linear
total number states, worst cubic size largest cluster.
DTP generally assumed computations even linear size full
state space infeasible. Therefore important develop methods perform
59

fiBoutilier, Dean, & Hanks

work polynomial log size state space. problems amenable
reductions without (perhaps unacceptable) sacrifice solution quality.
following section, review recent techniques DTP aimed achieving
reductions.
5.1.1 Goal Regression Classical Planning

Section 3.2 introduced general technique regression (or backward) search
state space solve classical planning problems, involving deterministic actions performance criteria specified terms reaching goal-satisfying state. One
diculty search requires branch search tree lead particular
goal state. commitment goal state may retracted (by backtracking
search process) sequence actions lead particular goal state
initial state. However, goal usually specified set literals G representing set
states, reaching state G equally suitable|it may, therefore, wasteful
restrict search finding plan reaches particular element G.
Goal regression abstraction technique avoids problem choosing particular goal state pursue. regression planner works searching sequence actions
follows: current set subgoals SG0 initialized G. iteration action
selected achieves one current subgoals SGi without deleting
others, whose preconditions con ict \unachieved subgoals."
subgoals achieved removed current subgoal set replaced formula
representing context achieve current subgoals, forming SGi+1 .
process known regressing SGi ff. process repeated one
two conditions holds: (a) current subgoal set satisfied initial state,
case current sequence actions selected successful plan; (b) action
applied, case current sequence cannot extended successful plan
earlier action choice must reconsidered.
Example 5.1 example, consider simplified version robot planning example used Section 3.1 illustrate value iteration: robot four actions
PUM, GetC, DelC DelM, make deterministic obvious way.
initial state sinit hCR; M; RHC; RHMi goal set G fCR; g. Regressing G DelM results SG1 = fCR; M; RHMg. Regressing SG1
DelC results SG2 = fRHC; M; RHMg. Regressing SG2 PUM results
SG3 = fRHC; g. Regressing SG3 GetC results SG4 = fM g. Note
sinit 2 SG4, sequence actions GetC, PUM, DelC, DelM successfully reach
goal state. 2
see algorithm implements form abstraction, first note goal
provides initial partition state space, dividing one set states
goal satisfied (G) second set (G). Viewed partition
zero-stage-to-go value function, G represents states whose value positive
G represents states whose value zero.
Every regression step thought revising partition. planning
algorithm attempts satisfy current subgoal set SGi applying action ff, uses
60

fiDecision-Theoretic Planning: Structural Assumptions

GetC

RHC





4

PUM

RHC

DelC

CR

DelM

CR







RHM

RHM









Goal

3

2

1

Figure 27: example goal regression.
regression compute (largest) set states that, executing ff, subgoals
satisfied. particular, state space repartitioned two abstract states: SGi+1
SGi+1 . way, abstraction mechanism implemented goal regression
considered adaptive. viewed (i + 1)-stage value function: state
satisfying SGi+1 reach goal state +1 steps using action sequence produced
SGi+1 .41 regression process stopped initial state member
abstract state SGi+1 . Figure 27 illustrates repartitioning state space
different regions SGi+1 steps example above.
regression produces compact representation something value function
(as discussion deterministic, goal-based dynamic programming Section 3.2),
analogy exact regions produced regression record property
goal reachability contingent particular choice action action sequence.
Standard dynamic programming methods implemented structured way
simply noticing number different regions produced ith iteration
considering actions regressed stage. union
regressions form states positive values Vi , thus making representation
i-stage-to-go value function exact. Notice iteration costly, since
regression actions must attempted, approach obviates need
backtracking ensure shortest plan found. Standard regression
provide guarantees without commitment particular search strategy (e.g., breadthfirst). use dynamic programming using Strips action descriptions forms basic
idea Schoppers's universal planning method (Schoppers, 1987).
Another general technique solving classical planning problems partial order planning (POP) (Chapman, 1987; Sacerdoti, 1975), embodied popular planning algorithms SNLP (McAllester & Rosenblitt, 1991) UCPOP (Penberthy & Weld, 1992).42
main motivation least-commitment approach comes realization
regression techniques incrementally building plan end beginning (in
temporal dimension). Thus, iteration must commit inserting step last
plan.
many cases determined particular step must appear somewhere
plan, necessarily last step plan; and, indeed, many cases step
41. case, however, states SGi+1 cannot reach goal region + 1 steps.
case cannot using specific sequence actions chosen far.
42. type planning sometimes called nonlinear least-commitment planning. See Weld's
(1994) survey nice overview.

61

fiBoutilier, Dean, & Hanks

consideration cannot appear last, fact cannot recognized later choices
reveal inconsistency. cases, regression algorithm prematurely commit
incorrect ordering eventually backtrack choice. example,
suppose problem scenario robot hold one item time,
coffee mail. Picking mail causes robot spill coffee possession,
similarly grasping coffee makes drop mail. plan generated regression would
longer valid: first two actions (DelC DelM) inserted
plan, action added achieve RHC RHM without making one false;
search plan would backtrack. Ultimately would discovered
successful plan end two actions performed sequence.
Partial-order planning algorithms proceed much regression algorithms, choosing
actions achieve unachieved subgoals using regression determine new subgoals,
leaving actions unordered whatever extent possible. Strictly speaking, subgoal sets
aren't regressed; rather, unachieved goal action precondition addressed separately,
actions ordered relative one another one action threatens negate
desired effect another. example above, algorithm might first place actions
DelC DelM plan, leave unordered. PUM added plan
achieve requirement RHM DelM; ordered DelM still unordered
respect DelC. GetC finally added plan achieve RHC
action DelC, two threats arise. First, GetC threatens desired effect RHM PUM.
resolved ordering GetC PUM DelM. Assume former ordering
chosen. Second, PUM threatens desired effect RHC GetC. threat
resolved placing PUM GetC DelC; since first threat resolved
ordering GetC PUM, latter ordering consistent one. result
plan GetC, DelC, PUM, DelM. backtracking required generate plan,
actions initially unordered, orderings introduced
discovery threats required them.
terms abstraction, incomplete, partially ordered plan threat-free,
perhaps certain \open conditions" (unachieved preconditions subgoals),
viewed much way partially completed regression plan: state satisfying
open conditions reach goal state executing total ordering plan's
actions consistent current set ordering constraints. See (Kambhampati, 1997)
framework unifies various approaches solving classical plan-generation problems.
techniques relying regression studied extensively deterministic
setting, recently applied probabilistic unobservable (Kushmerick
et al., 1995) partially observable (Draper, Hanks, & Weld, 1994b) domains.
part, techniques assume goal-based performance criterion attempt
construct plans whose probability reaching goal state exceeds threshold.
augment standard POP methods techniques evaluating plan's probability
achieving goal, techniques improving probability adding structure
plan. next section, consider use regression-related techniques
solve MDPs performance criteria general goals.
62

fiDecision-Theoretic Planning: Structural Assumptions

5.1.2 Stochastic Dynamic Programming Structured Representations

key idea underlying propositional goal regression|that one need regress relevant propositions action|can extended stochastic dynamic programming
methods, value iteration policy iteration, used solve general MDPs.
are, however, two key diculties overcome: lack specific goal region
uncertainty associated action effects.
Instead viewing state space partitioned goal non-goal clusters,
consider grouping states according expected values. Ideally, might want
group states according value respect optimal policy. consider
somewhat less dicult task, grouping states according value respect
fixed policy. essentially task performed policy evaluation step
policy iteration, insights used construct optimal policies.
fixed policy, want group states value policy.
Generalizing goal versus non-goal distinction, begin partition groups
states according immediate rewards. Then, using analogue regression developed
stochastic case, reason backward construct new partition states
grouped according value respect one-stage-to-go value function.
iterate manner kth iteration produce new partition groups
states according k-stage-to-go value function.
iteration, perform work polynomial number abstract states (and
size MDP representation) and, lucky, total number abstract states
bounded logarithmic factor size state space. implement
scheme effectively, perform operations regression without ever enumerating
set states, structured representations state-transition,
value, policy functions play role.
FOMDPs, approaches type taken (Boutilier, 1997; Boutilier & Dearden, 1996; Boutilier et al., 1995; Boutilier, Dearden, & Goldszmidt, 1999; Dietterich &
Flann, 1995; Hoey et al., 1999). illustrate basic intuitions behind approach
describing value iteration discounted infinite-horizon FOMDPs might work.
assume MDP specified using compact representation reward function
(such decision tree) actions (such 2TBNs).
value iteration, produce sequence value functions V0 ; V1 ; ; Vn , Vk
representing utility optimal k-stage policy. aim produce compact
representation value function and, using Vn suitable n, produce compact
representation optimal stationary policy. Given compact representation
reward function R, clear constitutes compact representation V0 .
usual, think leaf tree cluster states identical utility.
produce V1 compact form, proceed two phases.
branch tree V0 provides intensional description|namely, conjunction variable values labeling branch|of abstract state, region, comprising
states identical value respect initial value function V0 . deterministic action ff, perform regression step using description determine
conditions which, perform ff, would end cluster. would,
furthermore, determine region state space containing states identical future value
63

fiBoutilier, Dean, & Hanks

X

X

X

1.0 0.0
X





0.9



1.0 0.0

Z

Z


0.9

Time

Z

Time t+1
1.0 0.0

Figure 28: example action.
respect execution one stage go.43 Unfortunately, nondeterministic
actions cannot handled quite way: given state, action might lead
several different regions V0 non-zero probability. However, leaf tree
representing V0 (i.e., region V0 ), regress conjunction X describing
region action produce conditions X becomes true
false specified probability. words, instead regressing standard fashion determine conditions X becomes true, produce set distinct
conditions X becomes true different probabilities. piecing together
regions produced different labels description V0 , construct
set regions state given region: (a) transitions (under action ff)
particular part V0 identical probability; hence (b) identical expected future
value (Boutilier et al., 1995). view generalization propositional goal
regression suitable decision-theoretic problems.
Example 5.2 illustrate, consider example action shown Figure 28 value
function V 0 shown left Figure 29. order generate set regions
consisting states whose future value (w.r.t. V 0 ) identical, proceed
two steps (see Figure 29). first determine conditions fixed
probability making true (hence fixed probability moving left
right subtree V 0 ). conditions given tree representing CPT
node , makes first portion tree representing V 1 |see Step 1
Figure 29. Notice tree leaves labeled probability making
true (implicitly) false.
makes true, know future value (i.e., value zero stages
go) 8.1; becomes false, need know whether makes Z true (to
43. ignore immediate reward cost distinctions within region produced description;
recall value performing state given R(s), C (ff; s) expected future value.
simply focus abstract states whose elements identical future expected value. Differences
immediate reward cost added fact.

64

fiDecision-Theoretic Planning: Structural Assumptions



X

8.1

Z

9.0

0.9

0.0

X



1.0



0.0

0.9
Z 0.9

Z

0.9
Z 1.0
0

V

Step 1



1.0

0.9
Z 0.0

Z

0.0
Z 1.0

0.0
Z 0.0

Step 2

Figure 29: iteration decision-theoretic regression. Step 1 produces portion
tree dashed lines, Step 2 produces portion dotted lines.
determine whether future value 0 9:0). probability Z becomes
true given tree representing CPT node Z . Step 2 Figure 29,
conditions CPT conjoined conditions required predicting
's probability (by \grafting" tree Z tree given first step).
grafting slightly different three leaves tree : (a)
full tree Z attached leaf X = t; (b) tree Z simplified
attached leaf X = f ^ = f removal redundant test variable
; (c) notice need attach tree Z leaf X = f ^ = t,
since makes true probability 1 conditions (and Z relevant
determination V 0 false).
leaves newly formed tree Pr(Y ) Pr(Z ).
joint distributions Z (the effect variables independent semantics network) tells us probability Z true
zero stages go given conditions labeling appropriate branch
tree hold one stage go. words, new tree uniquely determines,
state one stage remaining, probability making conditions
labeling branches V 0 true. computation expected future value obtained
performing one stage go placed leaves tree
taking expectation values leaves V 0 . 2
new set regions produced way describes function Qff1 , Qff1 (s)
value associated performing state one stage go acting optimally
thereafter. functions (for action ff) pieced together (i.e., \maxed"|see
Section 3.1) determine V1 . course, process repeated number times
produce Vn suitable n, well optimal policy respect Vn .
basic technique used number different ways. Dietterich Flann
(1995) propose ideas similar these, restrict attention MDPs goal regions
65

fiBoutilier, Dean, & Hanks

deterministic actions (represented using Strips operators), thus rendering true goalregression techniques directly applicable.44 Boutilier et al. (1995) develop version
modified policy iteration produce tree-structured policies value functions,
Boutilier Dearden (1996) develop version value iteration described above.
algorithms extended deal correlations action effects (i.e., synchronic arcs
2TBNs) (Boutilier, 1997). abstraction schemes categorized nonuniform,
exact adaptive.
utility exact abstraction techniques tested real-world problems date. (Boutilier et al., 1999), results series abstract process-planning
examples reported, scheme shown useful, especially larger
problems. example, one specific problem 1.7 million states, tree representation value function 40,000 leaves, indicating tremendous amount
regularity value function. Schemes exploit regularity solve problems
quickly (in example, much less half time required modified policy iteration) much lower memory demands. However, schemes involve
substantial overhead tree construction, smaller problems little regularity,
overhead repaid time savings (simple vector-matrix representations methods
faster), though still generally provide substantial memory savings. might
viewed best- worst-case behavior described (Boutilier et al., 1999).
series \linear" examples (i.e., problems value functions represented
trees whose size linear number problem variables), tree-based scheme solves
problems many orders magnitude faster classical state-based techniques. contrast, problems exponentially-many distinct values tested (i.e., distinct
value state): tree-construction methods required construct complete
decision tree addition performing number expected value maximization
computations classical methods. worst case, tree-construction overhead makes
algorithm run 100 times slower standard modified policy iteration.
(Hoey et al., 1999), similar algorithm described uses algebraic decision
diagrams (ADDs) (Bahar, Frohm, Gaona, Hachtel, Macii, Pardo, & Somenzi, 1993) rather
trees. ADDs simple generalization boolean decision diagrams (BDDs) (Bryant,
1986) allow terminal nodes labeled real values instead boolean values.
Essentially, ADD-based algorithms similar tree-based algorithms except
isomorphic subtrees shared. lets ADDs provide compact representations
certain types value functions. Highly optimized ADD manipulation evaluation
software developed verification community applied solving MDPs.
Initial results provided (Hoey et al., 1999) encouraging, showing considerable savings
tree-based algorithms problems. example, ADD algorithm applied
1.7-million-state example described revealed value function
178 distinct values (cf. 40,000 tree leaves required) produced ADD description
value function less 2200 internal nodes. solved problem
seven minutes, 40 times faster earlier reported timing results using decision
trees (though improvement due use optimized ADD software
packages). Similar results obtain problems (problems 268 million states
44. Dietterich Flann (1995) describe work context reinforcement learning rather
method solving MDPs directly.

66

fiDecision-Theoretic Planning: Structural Assumptions

solved four hours). encouraging fact worst-case
(exponential) examples, overhead associated using ADDs|compared classical,
vector-based methods|is much less trees (about factor 20 compared \ at"
modified policy iteration 12 state variables), lessens problems become larger.
tree-based algorithms, methods yet applied real-world problems.
exact abstraction schemes clear that, examples resulting policies value functions may compact, others set regions may get
large (even reaching level individual states Boutilier et al., 1995), thus precluding
computational savings. Boutilier Dearden (1996) develop approximation scheme
exploits tree-structured nature value functions produced. stage k,
value function Vk pruned produce smaller, less accurate tree approximates Vk . Specifically, approximate value functions represented using trees whose leaves
labeled upper lower bound value function region; decisiontheoretic regression performed bounds. Certain subtrees value tree
pruned leaves subtree close value tree large
given computational constraints. scheme nonuniform, approximate adaptive.
approximation scheme tailored provide (roughly) accurate value
function given maximum tree size, smallest value function (with respect tree
size) given minimum accuracy. Results reported (Boutilier & Dearden, 1996)
show approximation small set examples (including worst-case examples
tree-based algorithms) allows substantial reduction computational cost. instance,
10-variable worst-case example, small amount pruning introduced average error
0.5% reduced computation time factor 50. aggressive pruning tends
increase error decrease computation time rapidly; making appropriate tradeoffs
two dimensions still addressed. method remains tested
evaluated realistic problems.
Structured representations solution algorithms applied problems
FOMDPs. Methods solving uence diagrams (Shachter, 1986) exploit structure
natural way; Tatman Shachter (1990) explore connection uence diagrams FOMDPs relationship uence diagram solution techniques
dynamic programming. Boutilier Poole (1996) show classic history-independent
methods solving POMDPs, based conversion FOMDP belief states, exploit types structured representations described here. However, exploiting structured
representations POMDPs remains explored depth.
5.1.3 Abstract Plans

One diculties adaptive abstraction schemes suggested fact
different abstractions must constructed repeatedly, incurring substantial computational overhead. overhead compensated savings obtained policy
construction|e.g., reducing number backups|then problematic.
many cases savings dominated time space required generate
abstractions, thus motivates development cheaper less accurate approximate
clustering schemes.
67

fiBoutilier, Dean, & Hanks

Another way reduce overhead adopt fixed abstraction scheme
one abstraction ever produced. approach adopted classical planning hierarchical abstraction-based planners, pioneered Sacerdoti's AbStrips system (Sacerdoti, 1974). similar form abstraction studied Knoblock (1993) (see
Knoblock, Tenenberg, & Yang, 1991). work, variables (in case propositional)
ranked according criticality (roughly, important variables solution
planning problem) abstraction constructed deleting problem
description set propositions low criticality. solution abstract problem
plan achieves elements original goal deleted. However,
preconditions effects actions deleted accounted solution, might solution original problem. Even so, abstract solution
used restrict search solution underlying concrete space. often
hierarchies refined abstractions used propositions introduced
back domain stages.
form abstraction uniform (propositions deleted uniformly) fixed. Since
abstract solution need solution problem, might tempted view
approximate abstraction method. However, best think abstract
plan solution all, rather form heuristic information help solve
true problem quickly.
intuitions underlying Knoblock's scheme applied DTP Boutilier Dearden (1994, 1997): variables ranked according degree uence reward
function subset important variables deemed relevant. subset
determined, variables uence relevant variables effects
actions (which determined easily using Strips 2TBN action descriptions)
deemed relevant, on. remaining variables deemed irrelevant
deleted description problem (both action reward descriptions).
leaves abstract MDP smaller state space (i.e., fewer variables) solved
standard methods. Recall state space reduction exponential number
variables removed. view method uniform fixed approximate abstraction
scheme. Unlike output classical abstraction methods, abstract policy produced
implemented value. degree optimal abstract policy
true optimal policy differ value bounded priori abstraction fixed.

Example 5.3 simple illustration, suppose reward satisfying coffee requests

(or penalty satisfying them) substantially greater keeping
lab tidy delivering mail. Suppose time pressure requires agent focus
specific subset objectives order produce small abstract state space.
case, four reward-laden variables problem (see Figure 24), CR
judged important. action descriptions used determine
variables (directly indirectly) affect probability achieving CR,
CR, RHC Loc deemed relevant, allowing , , RHM
ignored. state space thus reduced size 400 size 20. addition, several
action descriptions (e.g., Tidy) become trivial deleted. 2
68

fiDecision-Theoretic Planning: Structural Assumptions

advantage abstractions easily computed incur little
overhead. disadvantages uniform nature abstractions restrictive,
relevant \reward variables" determined policy constructed
without knowledge agent's ability control variables. result, important
variables|those large impact reward|but agent
control, may taken account, less important variables agent actually
uence ignored. However, series abstractions used take
account objectives decreasing importance, posteriori valuable objectives
dealt risk controllability taken account (Boutilier et al.,
1997). policies generated abstract levels used \seed" value
policy iteration less abstract levels, certain cases reducing time convergence
(Dearden & Boutilier, 1997). suggested (Dearden & Boutilier, 1994, 1997)
abstract value function used heuristic online search policies
improve abstract policy constructed, discussed Section 3.2.2. Thus, error
approximate value function overcome extent search, heuristic
function improved asynchronous updates.
different use abstraction adopted DRIPS planner (Haddawy & Suwandi,
1994; Haddawy & Doan, 1994). Actions abstracted collapsing \branches," possible outcomes, maintaining probabilistic intervals abstract, disjunctive effects.
Actions combined decomposition hierarchy, much hierarchical
task networks. Planning done evaluating abstract plans decomposition network, producing ranges utility possible instantiations plans, refining
plans possibly optimal. use task networks means search
restricted finite-horizon, open-loop plans action choice restricted possible refinements network. task networks offer useful way encode priori heuristic
knowledge structure good plans.
5.1.4 Model Minimization Reduction Methods

abstraction techniques defined recast terms minimizing stochastic
automaton, providing unifying view different methods offering new insights
abstraction process (Dean & Givan, 1997). automata theory know
given finite-state machine recognizing language L exists unique minimal
finite-state machine 0 recognizes L. could = 0 , might
0 exponentially smaller . minimal machine, called minimal
model language L, captures every relevant aspect machines
said equivalent. define similar notions equivalence MDPs. Since
primarily concerned planning, important equivalent MDPs agree value
functions policies. practical standpoint, may necessary find
minimal model find reduced model suciently small still equivalent.
apply idea model minimization (or model reduction) planning follows:
begin using algorithm takes input implicit MDP model factored form
produces (if lucky) explicit, reduced model whose size within polynomial
factor size factored representation. use favorite state-based
dynamic programming algorithms solve explicit model.
69

fiBoutilier, Dean, & Hanks

think dynamic programming techniques rely structured representations discussed earlier operating reduced model without ever explicitly constructing
model. cases, building reduced model may appropriate;
cases, one might save considerable effort explicitly constructing parts
reduced model absolutely necessary.
potential computational problems model-minimization techniques sketched above. small minimal model may exist, may hard find.
Instead, might look reduced model easier find necessarily minimal. could fail, case might look model small enough useful
approximately equivalent original factored model. careful
mean \approximate," intuitively two MDPs approximately equivalent
corresponding optimal value functions within small factor one another.
order practical, MDP model reduction schemes operate directly implicit
factored representation original MDP. Lee Yannakakis (1992) call online
model minimization. Online model minimization starts initial partition states.
Minimization iteratively refines partition splitting clusters smaller clusters.
cluster split states cluster behave differently respect
transitions states clusters. local property satisfied
clusters given partition, model consisting aggregate states correspond
clusters partition equivalent original model. addition,
initial partition method splitting clusters satisfy certain properties,45
guaranteed find minimal model. case MDP reduction, initial partition
groups together states reward, nearly reward case
approximation methods.
clusters partitions manipulated online model reduction methods represented intensionally formulas involving state variables. instance, formula
RHC ^ Loc(M ) represents set states robot coffee located
mail room. operations performed clusters require conjoining, complementing, simplifying, checking satisfiability. worst case, operations
intractable, successful application methods depends critically
problem way represented. illustrate basic idea simple
example.

Example 5.4 Figure 30 depicts simple version running example single
action. three boolean state variables corresponding RHC|the robot
coffee (or not, RHC), CR|there outstanding request coffee (or not, CR),
and, considering two location possibilities, Loc(C )|the robot coffee
room (or not, Loc(C )). Whether outstanding coffee request depends
whether request previous stage whether robot
coffee room. Location depends location previous stage,
reward depends whether outstanding coffee request.

45. property required initial partition that, two states cluster partition
defining minimal model (recall minimal model unique), must cluster
initial partition.

70

fiDecision-Theoretic Planning: Structural Assumptions

St 1

St

CR

CR

Pr(CR 1)
CR
CR
Loc(C)
Loc(C)
0.8
0.7
0.9

Loc

Loc

Pr(Loc(C) 1) = 0.7

RHC

Pr(RHC 1)
Loc(C)
Loc(C)
RHC
RHC
0.5
0.7
1.0

R


R(S t) = 1 CR
0 else

RHC

Figure 30: Factored model illustrating model-reduction techniques.
CR Loc(C)

CR
CR

CR
CR Loc(C)
(a)

(b)

Figure 31: Models involving aggregate states: (a) model corresponding initial
partition (b) minimal model.
initial partition shown Figure 31(a) defined terms immediate rewards.
say states particular starting cluster behave respect
particular destination cluster probability ending destination
cluster states starting cluster. property satisfied
starting cluster CR destination cluster CR Figure 31(a), split
cluster labeled CR obtain model Figure 31(b). property satisfied
pairs clusters model Figure 31(b) minimal model. 2
Lee Yannakakis algorithm non-deterministic finite-state machines
extended Givan Dean handle classical Strips planning problems (Givan & Dean,
1997) MDPs (Dean & Givan, 1997). basic step splitting cluster closely
related goal regression, relationship explored (Givan & Dean, 1997). Variants
model reduction approach apply action space large represented
factored form (Dean, Givan, & Kim, 1998); example, action specified
set parameters corresponding allocations several different
resources optimization problem. exist algorithms computing approxi71

fiBoutilier, Dean, & Hanks







R

G

C

G
P

B

B



E

B

(a)

(b)

(c)

Figure 32: Reachability serial problem decomposition.
mate models (Dean, Givan, & Leach, 1997) ecient planning algorithms use
approximate models (Givan, Leach, & Dean, 1997).

5.2 Reachability Analysis Serial Problem Decomposition
5.2.1 Reachability Analysis

existence goal states exploited different settings. instance, deterministic classical planning problems, regression viewed form directed dynamic
programming. Without uncertainty, certain policy either reaches goal state not,
dynamic programming backups need performed goal states,
possible states. Regression, therefore, implicitly exploits certain reachability characteristics domain along special structure value function.
Reachability analysis applied much broadly forms basis various types
problem decomposition. decomposition problem solving, MDP broken several
subprocesses solved independently, roughly independently, solutions
pieced together. subprocesses whose solutions interact marginally treated
independent, might expect good nonoptimal global solution result. Furthermore,
structure problem requires solution particular subproblem
needed, solutions subproblems ignored need computed
all. instance, regression analysis, optimal action states cannot reach
goal region irrelevant solution classical AI planning problem. shown
schematically Figure 32(a), regions B never explored backward
search state space: states reach goal within search horizon
ever deemed relevant. regions B may reachable start state,
fact reach goal state means known irrelevant.
system dynamics stochastic, scheme form basis approximately
optimal solution method: regions B ignored unlikely transition
regression goal region (region R). Similar remarks using progression forward
search start state apply, illustrated Figure 32(b).
72

fiDecision-Theoretic Planning: Structural Assumptions

Several schemes proposed AI literature exploiting reachability
constraints, apart usual forward- backward-search approaches. Peot Smith
(1993) introduce operator graph, structure computed prior problem solving
caches reachability relationships among propositions. graph consulted
planning process deciding actions insert plan resolve
threats.
GraphPlan algorithm Blum Furst (1995) attempts blend considerations
forward backward reachability deterministic planning context. One
diculties regression may regress goal region sequence
operators find region cannot reached initial state.
Figure 32(a), example, states region R may reachable initial
state. GraphPlan constructs variant operator graph called planning graph,
certain forward reachability constraints posted. Regression implemented
usual, current subgoal set violates forward reachability constraints
point, subgoal set abandoned regression search backtracks.
Conceptually, one might think GraphPlan constructing forward search tree
state space initial state root, backward search
goal region backward tree. course, process state-based:
instead, constraints possible variable values hold simultaneously different
planning stages recorded, regression used search backward planning
graph. sense, GraphPlan viewed constructing abstraction
forward-reachable states distinguished unreachable states planning stage,
using distinction among abstract states quickly identify infeasible regression
paths. Note, however, GraphPlan approximates distinction overestimating
set reachable states. Overestimation (as opposed underestimation) ensures
regression search space contains legitimate plans.
Reachability exploited solution general MDPs. Dean
et al. (1995) propose envelope method solving \goal-based" MDPs approximately.
Assuming path generated quickly given start state goal region,
MDP consisting states path perhaps neighboring states solved.
deal transitions lead envelope, heuristic method estimates value
states.46 time permits, set neighboring states expanded, increasing
solution quality accurately evaluating quality alternative actions.
ideas underlying GraphPlan applied general MDPs
(Boutilier, Brafman, & Geib, 1998), construction planning graph generalized deal stochastic, conditional action representation offered 2TBNs. Given
initial state (or set initial states), algorithm discovers reachability constraints
form GraphPlan | instance, two variable values X = x1
= y3 cannot obtain simultaneously; is, action sequence starting
given initial state lead state values hold.47 reachability
constraints discovered process used simplify action reward representation MDP refers reachable states. case, action
46. approximate abstraction techniques described Section 5.1.3 might used generate
heuristic information.
47. General k-ary constraints type considered (Boutilier et al., 1998).

73

fiBoutilier, Dean, & Hanks

requires unreachable set values hold effectively deleted. cases, certain
variables discovered immutable given initial conditions
deleted, leading much smaller MDPs. simplified representation retains original
propositional structure standard abstraction methods applied reachable
MDP. suggested strong synergy exists abstraction reachability analysis together techniques reduce size \effective" MDP
solved much dramatically either isolation. reachability constraints used prune regression paths deterministic domains, used
prune value function policy estimates generated decision-theoretic regression
abstraction algorithms (Boutilier et al., 1998).
results reported (Boutilier et al., 1998) limited single process-planning
domain, show reachability analysis together abstraction provide substantial reductions size effective MDP must solved, least domains.
domain 31 binary variables, reachability considerations generally eliminated
order 10 15 variables (depending initial state arity|binary
ternary|of constraints considered), reducing state space size 231 anywhere
222 215 . Incorporating abstraction reachable MDP provided considerably
reduction, reducing MDP sizes ranging 28 effectively zero states.
latter case would occur discovered values variables impact reward
altered|in case every course action expected utility
MDP needn't solved (or solved applying null actions zero cost).
5.2.2 Serial Problem Decomposition Communicating Structure

communicating reachability structure MDP provides way formalize different types problem decomposition. classify MDP according Markov
chains induced stationary policies admits. fixed Markov chain, group
states maximal recurrent classes transient states, described Section 2.1.
MDP recurrent policy induces Markov chain single recurrent class.
MDP unichain policy induces single recurrent class (possibly) transient states. MDP communicating pair states s; t, policy
reach t. MDP weakly communicating exists closed set
states communicating plus (possibly) set states transient every policy.
call MDPs noncommunicating.
notions crucial construction optimal average-reward policies,
exploited problem decomposition. Suppose MDP discovered consist
set recurrent classes C1 ; Cn (i.e., matter policy adopted, agent cannot
leave class enters class) set transient states.48 clear
optimal policy restricted class Ci constructed without reference policy
decisions made states outside Ci even values. Essentially, Ci
viewed independent subprocess.
48. simple way view classes think agent adopting randomized policy action
adopted state positive probability. classes induced Markov chain correspond
classes MDP.

74

fiDecision-Theoretic Planning: Structural Assumptions

observation leads following suggestion optimal policy construction:49
solve subprocesses consisting recurrent classes MDPs; remove
states MDP, forming reduced MDP consisting transient states.
break reduced MDP recurrent classes solve independently.
key effectively use value function original recurrent
states (computed solving independent subproblems Step 1) take account
transitions recurrent classes reduced MDP. Figure 32(c) shows MDP
broken classes might constructed way. original MDP, classes C
E recurrent solved independently. removed MDP, class
recurrent reduced MDP. can, course, solved without reference classes
B , rely value states transitions class E . However,
value function E available purpose, used solve
consisted jDj states. hand, B solved, finally
solved. Lin Dean (1995) provide version type decomposition
employs factored representation. factored representation allows dimensionality
reduction different state subspaces aggregating states differ values
irrelevant variables subspaces.
key decomposition discovery recurrent classes MDP.
Puterman (1994) suggests adaptation Fox-Landi algorithm (Fox & Landi, 1968)
discovering structure Markov chains O(N 2 ) (recall N = jSj).50 alleviate
diculties algorithms work explicit state-based representation, Boutilier
Puterman (1995) propose variant algorithm works factored 2TBN
representation.
One diculty form decomposition reliance strongly independent
subproblems (i.e., recurrent classes) within MDP. Others explored exact approximate techniques work less restrictive assumptions. One simple method
approximation construct \approximately recurrent classes." Figure 32(c) might
imagine C E nearly independent sense transitions
low-probability high-cost. Treating independent might lead approximately optimal policies whose error bounded. solutions C E interact
strongly enough solutions constructed completely independently,
different approach solving decomposed problem taken.
optimal value function E then, pointed out, calculate
optimal value function D. first thing note don't need know
value function states E , value every state E reachable
state single step. set states outside reachable single
step state inside referred states periphery D. values
states intersection E periphery summarize value exiting
ending E . refer set states periphery block
kernel MDP. different blocks interact one another
states kernel.
49. Ross Varadarajan (1991) make related suggestion solving average-reward problems.
50. slight correction made suggested algorithm (Boutilier & Puterman, 1995).

75

fiBoutilier, Dean, & Hanks

Loc(C)

Loc(L)

Loc(M )

Loc(O)

Figure 33: Decomposition based location.

Loc(C)

Loc(L)
Kernel

Loc(M )

Loc(O)

Figure 34: Kernel-based decomposition depicting kernel states.

76

fiDecision-Theoretic Planning: Structural Assumptions

Example 5.5 Spatial features often provide natural dimension along decom-

pose domain. running example, location robot might used
decompose state space blocks states, one block possible locations. Figure 33 shows decomposition superimposed state-transition
diagram MDP. States kernel shaded might correspond
entrances exits locations. star-shaped topology, induced kernel
decomposition used (Kushner & Chen, 1974) (Dean & Lin, 1995), illustrated
Figure 34. Figure 33, hallway location explicitly represented.
simplification may reasonable hallway conduit moving
one room another; case function hallway accounted
dynamics governing states kernel. Figures 33 34 idealized that, given
full set features running example, kernel would contain many
states. 2

One technique computing optimal policy entire MDP involves repeatedly
solving MDPs corresponding individual blocks. techniques works follows:
initially, guess value every state kernel.51 Given current estimate
values kernel states, solve component MDPs; solution produces new
estimate states kernel. adjust values states kernel
considering difference current new estimates iterate
difference negligible.
iterative method solving decomposed MDP special case Lagrangian
method finding extrema function. literature replete
methods linear nonlinear systems equations (Winston, 1992). possible
formulate MDP linear program (D'Epenoux, 1963; Puterman, 1994). Dantzig
Wolfe (1960) developed method decomposing system equations involving
large number variables set smaller systems equations interacting set
coupling variables (variables shared two blocks). Dantzig-Wolfe
decomposition method, original, large system equations solved iteratively
solving smaller systems adjusting coupling variables iteration
adjustment required. linear programming formulation MDP,
values states encoded variables.
Kushner Chen (1974) exploit fact MDPs modeled linear programs
using Dantzig-Wolfe decomposition method solve MDPs involving large number
states. Dean Lin (1995) describe general framework solving decomposed MDPs
pointing work Kushner Chen special case, neither work addresses
issue decompositions come from. Dean et al. (1995) investigate methods
decomposing state space two blocks: reachable k steps fewer
reachable k steps (see discussion reachability above). set states
reachable k fewer steps used construct MDP basis policy
approximates optimal policy. k increases, size block states reachable
k steps increases, ensuring better solution; amount time required compute
51. Ideally would aggregate kernel states value provide compact representation.
remainder section, however, won't consider opportunities combining
aggregation decomposition methods.

77

fiBoutilier, Dean, & Hanks

solution increases. Dean et al. (1995) discuss methods solving MDPs time-critical
problems trading quality time.
ignored issue obtain decompositions expedite calculations. Ideally, component decomposition would yield simplification via
aggregation abstraction, reducing dimensionality component thereby
avoiding explicit enumeration states. Lin (1997) presents methods exploiting
structure certain special cases communicating structure revealed
domain expert. general, however, finding decomposition minimize effort
spent solving component MDPs quite hard (at least hard finding smallest circuit consistent given input-output behavior) best hope
good heuristic methods. Unfortunately, aware particularly useful
heuristics finding serial decompositions Markov decision processes. Developing
heuristics clearly area investigation.
Related form decomposition development macro operators MDPs
(Sutton, 1995). Macros long history classical planning problem solving (Fikes,
Hart, & Nilsson, 1972; Korf, 1985), recently generalized MDPs
(Hauskrecht, Meuleau, Kaelbling, Dean, & Boutilier, 1998; Parr, 1998; Parr & Russell, 1998;
Precup, Sutton, & Singh, 1998; Stone & Veloso, 1999; Sutton, 1995; Thrun & Schwartz,
1995). work, macro taken local policy region state
space (or block terminology). Given MDP comprising blocks
set macros defined block, MDP solved selecting macro action
block global policy induced set macros picked close
optimal, least best combination macros set available.
(Sutton, 1995; Precup et al., 1998), macros treated temporally-abstract actions
models defined macro treated single action
used policy value iteration (along concrete actions). (Hauskrecht et al., 1998;
Parr, 1998; Parr & Russell, 1998), models exploited hierarchical fashion,
high-level MDP consisting states lying boundaries blocks, macros
\actions" chosen states. issue macro generation|
constructing set macros guaranteed provide exibility select close optimal
global behavior|is addressed (Hauskrecht et al., 1998; Parr, 1998). relationship
serial decomposition techniques quite close; thus, problems discovering good
decompositions, constructing good sets macros, exploiting intensional representations
areas clearer, compelling solutions required. date, work area
provided much computational utility solution MDPs|except cases
good, hand-crafted, region-based decompositions macros provided|and little
work taken account factored nature many MDPs. reason,
discuss detail. However, general notion serial decomposition continues
develop shows great promise.

5.3 Multiattribute Reward Parallel Decomposition
Another form decomposition parallel decomposition, MDP broken
set sub-MDPs \run parallel." Specifically, stage (global)
decision process, state subprocess affected. instance, Figure 35, action
78

fiDecision-Theoretic Planning: Structural Assumptions



MDP1



MDP2



MDP3

Figure 35: Parallel problem decomposition.

affects state subprocess. Intuitively, action suitable execution

original MDP state reasonably good sub-MDPs.
Generally, sub-MDPs form either product join decomposition original
state space (contrast union decompositions state space determined serial
decompositions): state space formed taking cross product sub-MDP state
spaces, join certain states subprocesses cannot linked. subprocesses
may identical action spaces (as Figure 35), may action space,
global action choice factored choice subprocess. latter
case, sub-MDPs may completely independent, case (global) MDP
solved exponentially faster. challenging problem arises constraints
legal action combinations. example, actions subprocesses
require certain shared resources, interactions global choice may arise.
parallel MDP decomposition, wish solve sub-MDPs use policies
value functions generated help construct optimal approximately optimal solution
original MDP, highlighting need find appropriate decompositions MDPs
develop suitable merging techniques. Recent parallel decomposition methods
involved decomposing MDP subprocesses suitable distinct objectives. Since
reward functions often deal multiple objectives, associated independent
reward, whose rewards summed determine global reward, often
natural way decompose MDPs. Thus, ideas multiattribute utility theory
seen play role solution MDPs.
Boutilier et al. (1997) decompose MDP specified using 2TBNs additive reward
function using abstraction technique described Section 5.1.3. component
reward function, abstraction used generate MDP referring variables
relevant component.52 Since certain state variables may present multiple
sub-MDPs (i.e., relevant one objective), original state space join
subspaces. Thus, decomposition tackled automatically. Merging tackled several
ways. One involves using sum value functions obtained solving sub-MDPs
heuristic estimate true value function. heuristic used guide online,
state-based search (see Section 3.2.1). sub-MDPs interact, heuristic
perfect leads backtrack-free optimal action selection; interact, search
52. Note existence factored MDP representation crucial abstraction method.

79

fiBoutilier, Dean, & Hanks

required detect con icts. Note sub-MDP identical sets actions.
action space large, branching factor search process may prohibitive.
Singh Cohn (1998) deal parallel decomposition, though assume
global MDP specified explicitly set parallel MDPs, thus generating decompositions
global MDP issue. global MDP given cross product state
action spaces sub-MDPs reward functions summed. However,
constraints feasible action combinations couple solutions sub-MDPs.
solve global MDP, sum sub-MDP value functions used upper bound
optimal global value function, maximum (at global state)
used lower bound. bounds form basis action-elimination procedure
value-iteration algorithm solving global MDP.53 Unfortunately, value iteration
run explicit state space global MDP. Since action space cross
product, potential computational bottleneck value iteration, well.
Meuleau et al. (1998) use parallel decomposition approximate solution stochastic resource allocation problems large state action spaces. Much Singh
Cohn (1998), MDP specified terms number independent MDPs,
involving distinct objective, whose action choices linked shared resource constraints. value functions individual MDPs constructed oine used
set online action-selection procedures. Unlike many approximation procedures
discussed, approach makes attempt construct policy explicitly (and
similar real-time search RTDP respect) construct value function
explicitly. method applied large MDPs, state spaces size
21000 actions spaces even larger, solve problems roughly half
hour. solutions produced approximate, size problem precludes
exact solution; good estimates solution quality hard derive. However,
method applied smaller problems nature whose exact solution
computed, approximations high quality (Meuleau et al., 1998). able
solve large MDPs (with large, factored, state action spaces), model
relies somewhat restrictive assumptions nature local value functions
ensure good solution quality. However, basic approach appears generalizable,
offers great promise solving large factored MDPs.
algorithms (Singh & Cohn, 1998) (Meuleau et al., 1998) seen
rely least implicitly structured MDP representations involving almost independent
subprocesses. seems likely approaches could take advantage automatic
MDP decomposition algorithms (Boutilier et al., 1997), factored
representations explicitly play part.

5.4 Summary

seen number ways intensional representations exploited
solve MDPs effectively without enumeration state space. include techniques
abstraction MDPs, including based relevance analysis, goal regression
decision-theoretic regression; techniques relying reachability analysis serial decomposition; methods parallel MDP decomposition exploiting multiattribute nature
53. Singh Cohn (1998) incorporate methods removing unreachable states value iteration.

80

fiDecision-Theoretic Planning: Structural Assumptions

reward functions. Many methods can, fortunate circumstances, offer exponential reduction solution time space required represent policy value function;
none come guarantees reductions except certain special cases.
methods described provide approximate solutions (often error bounds provided), offer optimality guarantees general, provide optimal
solutions suitable assumptions.
One avenue explored detail relationship structured solution methods developed MDPs described techniques used solving
Bayesian networks. Since many algorithms discussed section rely structure inherent 2TBN representation MDP, natural ask whether
embody intuitions underlie solution algorithms Bayes nets, thus
whether solution techniques Bayes nets (directly indirectly) applied
MDPs ways give rise algorithms similar discussed here. remains
open question point, undoubtedly strong ties exist. Tatman Shachter
(1990) explored connections uence diagrams MDPs. Kjaerulff
(1992) investigated computational considerations involved applying join tree methods
reasoning tasks monitoring prediction temporal Bayes nets. abstraction methods discussed Section 5.1.2 interpreted form variable elimination
(Dechter, 1996; Zhang & Poole, 1996). Elimination variables occurs temporal order,
good orderings within time slice must exploit tree graph structure
CPTs. Approximation schemes based variable elimination (Dechter, 1997; Poole, 1998)
may related certain approximation methods developed MDPs.
independence-based decompositions MDPs discussed Section 5.3 clearly viewed
exploiting independence relations made explicit \unrolling" 2TBN. development connections Bayes net inference algorithms doubt prove
useful enhancing understanding existing methods, increasing range
applicability pointing new algorithms.

6. Concluding Remarks
search effective algorithms controlling automated agents long important history, problem continue grow importance decisionmaking functionality automated. Work several disciplines, among AI, decision
analysis, OR, addressed problem, carried different problem definitions, different sets simplifying assumptions, different viewpoints, hence
different representations algorithms problem solving. often not, assumptions seem made historical reasons reasons convenience,
often dicult separate essential assumptions accidental. important
clarify relationships among problem definitions, crucial assumptions, solution
techniques, meaningful synthesis take place.
paper analyzed various approaches particular class sequential decision problems studied OR, decision analysis, AI literature.
started general, reasonably neutral statement problem, couched, convenience, language Markov decision processes. demonstrated
various disciplines define problem (i.e., assumptions make), effect
81

fiBoutilier, Dean, & Hanks

assumptions worst-case time complexity solving problem defined.
Assumptions regarding two main factors seem distinguish commonly studied
classes decision problems:

observation sensing: sensing tend fast, cheap, accurate laborious,
costly noisy?

incentive structure agent: behavior evaluated ability perform
particular task, ability control system interval time?

Moving beyond worst-case analysis, generally assumed that, although pathological cases inevitably dicult, agent able solve \typical" \easy"
cases effectively. so, agent needs able identify structure problem
exploit structure algorithmically.
identified three ways structural regularities recognized, represented,
exploited computationally. first structure induced domain-level simplifying
assumptions full observability, goal satisfaction time-separable value functions,
on. second structure exploited compact domain-specific encodings states,
actions, rewards. designer use techniques make structure explicit,
decision-making algorithms exploit structural regularities apply
particular problem hand. third involves aggregation, abstraction decomposition techniques, whereby structural regularities discovered exploited
problem-solving process itself. developing framework|one allows comparison
domains, assumptions, problems, techniques drawn different disciplines|we
discover essential problem structure required specific representations algorithms
prove effective; way insights techniques developed
certain problems, within certain disciplines, evaluated potentially applied
new problems, within disciplines.
main focus work elucidation various forms structure
decision problems exploited representationally computationally.
part, focused propositional structure, commonly associated planning AI circles. complete treatment would included
compact representations dynamics, rewards, policies, value functions often
considered continuous, real-valued domains. instance, discussed linear
dynamics quadratic cost functions, often used control theory (Caines, 1988),
use neural-network representations value functions, frequently adopted within
reinforcement learning community (Bertsekas & Tsitsiklis, 1996; Tesauro, 1994),54
discussed partitioning continuous state spaces often addressed reinforcement
learning (Moore & Atkeson, 1995). Neither addressed relational quantificational structure used first-order planning representations. However, even techniques
cast within framework described here; example, use piecewise-linear
value functions seen form abstraction different linear components
applied different regions clusters state space.
54. Bertsekas Tsitsiklis (1996) provide in-depth treatment neural network linear function
approximators MDPs reinforcement learning.

82

fiDecision-Theoretic Planning: Structural Assumptions

Although certain cases indicated devise methods exploit several
types structure once, research along lines limited. extent,
many representations algorithms described paper complementary
pose obstacles combination. remains seen interact
techniques developed forms structure, used continuous state
action spaces.
analysis raises opportunities challenges: understanding assumptions,
techniques, relationships, designer decision-making agents many
tools build effective problem solvers; challenges lie development
additional tools integration existing ones.

Acknowledgments

Many thanks careful comments referees. Thanks Ron Parr Robert
St-Aubin comments earlier draft paper. students taking CS3710
(Spring 1999) taught Martha Pollack University Pittsburgh CPSC522
(Winter 1999) University British Columbia deserve thanks detailed
comments.
Boutilier supported NSERC Research Grant OGP0121843, NCE IRISII program Project IC-7. Dean supported part National Science Foundation
Presidential Young Investigator Award IRI-8957601 Air Force Advanced
Research Projects Agency Department Defense Contract No. F30602-91-C0041. Hanks supported part ARPA / Rome Labs Grant F30602{95{1{0024
part NSF grant IRI{9523649.

References

Allen, J., Hendler, J., & Tate, A. (Eds.). (1990). Readings Planning. Morgan-Kaufmann,
San Mateo.
Astrom, K. J. (1965). Optimal control Markov decision processes incomplete state
estimation. J. Math. Anal. Appl., 10, 174{205.
Bacchus, F., Boutilier, C., & Grove, A. (1996). Rewarding behaviors. Proceedings
Thirteenth National Conference Artificial Intelligence, pp. 1160{1167 Portland,
OR.
Bacchus, F., Boutilier, C., & Grove, A. (1997). Structured solution methods nonMarkovian decision processes. Proceedings Fourteenth National Conference
Artificial Intelligence, pp. 112{117 Providence, RI.
Bacchus, F., & Kabanza, F. (1995). Using temporal logic control search
forward chaining planner.
Proceedings Third European
Workshop Planning (EWSP'95) Assisi, Italy. Available via URL
ftp://logos.uwaterloo.ca:/pub/tlplan/tlplan.ps.Z.
Bacchus, F., & Teh, Y. W. (1998). Making forward chaining relevant. Proceedings
Fourth International Conference AI Planning Systems, pp. 54{61 Pittsburgh, PA.
83

fiBoutilier, Dean, & Hanks

Bahar, R. I., Frohm, E. A., Gaona, C. M., Hachtel, G. D., Macii, E., Pardo, A., & Somenzi,
F. (1993). Algebraic decision diagrams applications. International Conference Computer-Aided Design, pp. 188{191. IEEE.
Baker, A. B. (1991). Nonmonotonic reasoning framework situation calculus.
Artificial Intelligence, 49, 5{23.
Barto, A. G., Bradtke, S. J., & Singh, S. P. (1995). Learning act using real-time dynamic
programming. Artificial Intelligence, 72 (1{2), 81{138.
Bellman, R. (1957). Dynamic Programming. Princeton University Press, Princeton, NJ.
Bertsekas, D. P., & Castanon, D. A. (1989). Adaptive aggregation infinite horizon
dynamic programming. IEEE Transactions Automatic Control, 34 (6), 589{598.
Bertsekas, D. P. (1987). Dynamic Programming. Prentice-Hall, Englewood Cliffs, NJ.
Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-dynamic Programming. Athena, Belmont,
MA.
Blackwell, D. (1962). Discrete dynamic programming. Annals Mathematical Statistics,
33, 719{726.
Blum, A. L., & Furst, M. L. (1995). Fast planning graph analysis. Proceedings
Fourteenth International Joint Conference Artificial Intelligence, pp. 1636{
1642 Montreal, Canada.
Bonet, B., & Geffner, H. (1998). Learning sorting decision trees POMDPs.
Proceedings Fifteenth International Conference Machine Learning, pp. 73{81
Madison, WI.
Bonet, B., Loerincs, G., & Geffner, H. (1997). robust fast action selection mechanism.
Proceedings Fourteenth National Conference Artificial Intelligence, pp.
714{719 Providence, RI.
Boutilier, C. (1997). Correlated action effects decision theoretic regression. Proceedings Thirteenth Conference Uncertainty Artificial Intelligence, pp. 30{37
Providence, RI.
Boutilier, C., Brafman, R. I., & Geib, C. (1997). Prioritized goal decomposition Markov
decision processes: Toward synthesis classical decision theoretic planning.
Proceedings Fifteenth International Joint Conference Artificial Intelligence,
pp. 1156{1162 Nagoya, Japan.
Boutilier, C., Brafman, R. I., & Geib, C. (1998). Structured reachability analysis Markov
decision processes. Proceedings Fourteenth Conference Uncertainty
Artificial Intelligence, pp. 24{32 Madison, WI.
Boutilier, C., & Dearden, R. (1994). Using abstractions decision-theoretic planning
time constraints. Proceedings Twelfth National Conference Artificial
Intelligence, pp. 1016{1022 Seattle, WA.
84

fiDecision-Theoretic Planning: Structural Assumptions

Boutilier, C., & Dearden, R. (1996). Approximating value trees structured dynamic
programming. Proceedings Thirteenth International Conference Machine
Learning, pp. 54{62 Bari, Italy.
Boutilier, C., Dearden, R., & Goldszmidt, M. (1995). Exploiting structure policy construction. Proceedings Fourteenth International Joint Conference Artificial
Intelligence, pp. 1104{1111 Montreal, Canada.
Boutilier, C., Dearden, R., & Goldszmidt, M. (1999). Stochastic dynamic programming
factored representations. (manuscript).
Boutilier, C., Friedman, N., Goldszmidt, M., & Koller, D. (1996). Context-specific independence Bayesian networks. Proceedings Twelfth Conference Uncertainty
Artificial Intelligence, pp. 115{123 Portland, OR.
Boutilier, C., & Goldszmidt, M. (1996). frame problem Bayesian network action
representations. Proceedings Eleventh Biennial Canadian Conference
Artificial Intelligence, pp. 69{83 Toronto.
Boutilier, C., & Poole, D. (1996). Computing optimal policies partially observable
decision processes using compact representations. Proceedings Thirteenth
National Conference Artificial Intelligence, pp. 1168{1175 Portland, OR.
Boutilier, C., & Puterman, M. L. (1995). Process-oriented planning average-reward optimality. Proceedings Fourteenth International Joint Conference Artificial
Intelligence, pp. 1096{1103 Montreal, Canada.
Brafman, R. I. (1997). heuristic variable-grid solution method POMDPs. Proceedings Fourteenth National Conference Artificial Intelligence, pp. 727{733
Providence, RI.
Bryant, R. E. (1986). Graph-based algorithms boolean function manipulation. IEEE
Transactions Computers, C-35 (8), 677{691.
Bylander, T. (1994). computational complexity propositional STRIPS planning.
Artificial Intelligence, 69, 161{204.
Caines, P. E. (1988). Linear stochastic systems. Wiley, New York.
Cassandra, A. R., Kaelbling, L. P., & Littman, M. L. (1994). Acting optimally partially
observable stochastic domains. Proceedings Twelfth National Conference
Artificial Intelligence, pp. 1023{1028 Seattle, WA.
Cassandra, A. R., Littman, M. L., & Zhang, N. L. (1997). Incremental pruning: simple, fast, exact method pomdps. Proceedings Thirteenth Conference
Uncertainty Artificial Intelligence, pp. 54{61 Providence, RI.
Chapman, D. (1987). Planning conjunctive goals. Artificial Intelligence, 32 (3), 333{377.
85

fiBoutilier, Dean, & Hanks

Chapman, D., & Kaelbling, L. P. (1991). Input generalization delayed reinforcement
learning: algorithm performance comparisons. Proceedings Twelfth
International Joint Conference Artificial Intelligence, pp. 726{731 Sydney, Australia.
Dantzig, G., & Wolfe, P. (1960). Decomposition principle dynamic programs. Operations
Research, 8 (1), 101{111.
Dean, T., Allen, J., & Aloimonos, Y. (1995). Artificial Intelligence: Theory Practice.
Benjamin Cummings.
Dean, T., & Givan, R. (1997). Model minimization Markov decision processes.
Proceedings Fourteenth National Conference Artificial Intelligence, pp. 106{
111 Providence, RI. AAAI.
Dean, T., Givan, R., & Kim, K.-E. (1998). Solving planning problems large state
action spaces. Proceedings Fourth International Conference AI Planning
Systems, pp. 102{110 Pittsburgh, PA.
Dean, T., Givan, R., & Leach, S. (1997). Model reduction techniques computing approximately optimal solutions Markov decision processes. Proceedings
Thirteenth Conference Uncertainty Artificial Intelligence, pp. 124{131 Providence, RI.
Dean, T., Kaelbling, L., Kirman, J., & Nicholson, A. (1993). Planning deadlines
stochastic domains. Proceedings Eleventh National Conference Artificial
Intelligence, pp. 574{579.
Dean, T., Kaelbling, L., Kirman, J., & Nicholson, A. (1995). Planning time constraints stochastic domains. Artificial Intelligence, 76 (1-2), 3{74.
Dean, T., & Kanazawa, K. (1989). model reasoning persistence causation.
Computational Intelligence, 5 (3), 142{150.
Dean, T., & Lin, S.-H. (1995). Decomposition techniques planning stochastic domains. Proceedings Fourteenth International Joint Conference Artificial
Intelligence, pp. 1121{1127.
Dean, T., & Wellman, M. (1991). Planning Control. Morgan Kaufmann, San Mateo,
California.
Dearden, R., & Boutilier, C. (1994). Integrating planning execution stochastic
domains. Proceedings Tenth Conference Uncertainty Artificial Intelligence, pp. 162{169 Washington, DC.
Dearden, R., & Boutilier, C. (1997). Abstraction approximate decision theoretic planning. Artificial Intelligence, 89, 219{283.
Dechter, R. (1996). Bucket elimination: unifying framework probabilistic inference.
Proceedings Twelfth Conference Uncertainty Artificial Intelligence, pp.
211{219 Portland, OR.
86

fiDecision-Theoretic Planning: Structural Assumptions

Dechter, R. (1997). Mini-buckets: general scheme generating approximations
automated reasoning probabilistic inference. Proceedings Fifteenth International Joint Conference Artificial Intelligence, pp. 1297{1302 Nagoya, Japan.
D'Epenoux, F. (1963). Sur un probleme de production et de stockage dans l'aleatoire.
Management Science, 10, 98{108.
Dietterich, T. G., & Flann, N. S. (1995). Explanation-based learning reinforcement
learning: unified approach. Proceedings Twelfth International Conference
Machine Learning, pp. 176{184 Lake Tahoe, NV.
Draper, D., Hanks, S., & Weld, D. (1994a). probabilistic model action leastcommitment planning information gathering. Proceedings Tenth Conference Uncertainty Artificial Intelligence, pp. 178{186 Washington, DC.
Draper, D., Hanks, S., & Weld, D. (1994b). Probabilistic planning information gathering contingent execution. Proceedings Second International Conference
AI Planning Systems, pp. 31{36.
Etzioni, O., Hanks, S., Weld, D., Draper, D., Lesh, N., & Williamson, M. (1992).
approach planning incomplete information. Proceedings Third International Conference Principles Knowledge Representation Reasoning, pp.
115{125 Boston, MA.
Fikes, R., Hart, P., & Nilsson, N. (1972). Learning executing generalized robot plans.
Artificial Intelligence, 3, 251{288.
Fikes, R., & Nilsson, N. J. (1971). STRIPS: new approach application theorem
proving problem solving. Artificial Intelligence, 2, 189{208.
Finger, J. (1986). Exploiting Constraints Design Synthesis. Ph.D. thesis, Stanford University, Stanford.
Floyd, R. W. (1962). Algorithm 97 (shortest path). Communications ACM, 5 (6),
345.
Fox, B. L., & Landi, D. M. (1968). algorithm identifying ergodic subchains
transient states stochastic matrix. Communications ACM, 2, 619{621.
French, S. (1986). Decision Theory. Halsted Press, New York.
Geiger, D., & Heckerman, D. (1991). Advances probabilistic reasoning. Proceedings
Seventh Conference Uncertainty Artificial Intelligence, pp. 118{126 Los
Angeles, CA.
Givan, R., & Dean, T. (1997). Model minimization, regression, propositional STRIPS
planning. Proceedings Fifteenth International Joint Conference Artificial
Intelligence, pp. 1163{1168 Nagoya, Japan.
87

fiBoutilier, Dean, & Hanks

Givan, R., Leach, S., & Dean, T. (1997). Bounded-parameter Markov decision processes.
Proceedings Fourth European Conference Planning (ECP'97), pp. 234|246
Toulouse, France.
Goldman, R. P., & Boddy, M. S. (1994). Representing uncertainty simple planners.
Proceedings Fourth International Conference Principles Knowledge
Representation Reasoning, pp. 238{245 Bonn, Germany.
Haddawy, P., & Doan, A. (1994). Abstracting probabilistic actions. Proceedings
Tenth Conference Uncertainty Artificial Intelligence, pp. 270{277 Washington,
DC.
Haddawy, P., & Hanks, S. (1998). Utility Models Goal-Directed Decision-Theoretic
Planners. Computational Intelligence, 14 (3).
Haddawy, P., & Suwandi, M. (1994). Decision-theoretic refinement planning using inheritence abstraction. Proceedings Second International Conference AI Planning Systems, pp. 266{271 Chicago, IL.
Hanks, S. (1990). Projecting plans uncertain worlds. Ph.D. thesis 756, Yale University,
Department Computer Science, New Haven, CT.
Hanks, S., & McDermott, D. V. (1994). Modeling dynamic uncertain world I: Symbolic
probabilistic reasoning change. Artificial Intelligence, 66 (1), 1{55.
Hanks, S., Russell, S., & Wellman, M. (Eds.). (1994). Decision Theoretic Planning: Proceedings AAAI Spring Symposium. AAAI Press, Menlo Park.
Hansen, E. A., & Zilberstein, S. (1998). Heuristic search cyclic AND/OR graphs.
Proceedings Fifteenth National Conference Artificial Intelligence, pp. 412{
418 Madison, WI.
Hauskrecht, M. (1997). heuristic variable-grid solution method POMDPs. Proceedings Fourteenth National Conference Artificial Intelligence, pp. 734{739
Providence, RI.
Hauskrecht, M. (1998). Planning Control Stochastic Domains Imperfect Information. Ph.D. thesis, Massachusetts Institute Technology, Cambridge.
Hauskrecht, M., Meuleau, N., Kaelbling, L. P., Dean, T., & Boutilier, C. (1998). Hierarchical
solution Markov decision processes using macro-actions. Proceedings
Fourteenth Conference Uncertainty Artificial Intelligence, pp. 220{229 Madison,
WI.
Hoey, J., St-Aubin, R., Hu, A., & Boutilier, C. (1999). SPUDD: Stochastic planning
using decision diagrams. Proceedings Fifteenth Conference Uncertainty
Artificial Intelligence Stockholm. appear.
Howard, R. A. (1960). Dynamic Programming Markov Processes. MIT Press, Cambridge, Massachusetts.
88

fiDecision-Theoretic Planning: Structural Assumptions

Howard, R. A., & Matheson, J. E. (1984). uence diagrams. Howard, R. A., & Matheson, J. E. (Eds.), Principles Applications Decision Analysis. Strategic
Decisions Group, Menlo Park, CA.
Kambhampati, S. (1997). Refinement planning unifying framework plan synthesis.
AI Magazine, Summer 1997, 67{97.
Kearns, M., Mansour, Y., & Ng, A. Y. (1999). sparse sampling algorithm nearoptimal planning large markov decision processes. Proceedings Sixteenth
International Joint Conference Artificial Intelligence Stockholm. appear.
Keeney, R. L., & Raiffa, H. (1976). Decisions Multiple Objectives: Preferences
Value Tradeoffs. John Wiley Sons, New York.
Kjaerulff, U. (1992). computational scheme reasoning dynamic probabilistic networks. Proceedings Eighth Conference Uncertainty AI, pp. 121{129
Stanford.
Knoblock, C. A. (1993). Generating Abstraction Hierarchies: Automated Approach
Reducing Search Planning. Kluwer, Boston.
Knoblock, C. A., Tenenberg, J. D., & Yang, Q. (1991). Characterizing abstraction hierarchies planning. Proceedings Ninth National Conference Artificial
Intelligence, pp. 692{697 Anaheim, CA.
Koenig, S. (1991). Optimal probabilistic decision-theoretic planning using Markovian
decision theory. M.sc. thesis UCB/CSD-92-685, University California Berkeley,
Computer Science Department.
Koenig, S., & Simmons, R. (1995). Real-time search nondeterministic domains.
Proceedings Fourteenth International Joint Conference Artificial Intelligence,
pp. 1660{1667 Montreal, Canada.
Korf, R. (1985). Macro-operators: weak method learning. Artificial Intelligence, 26,
35{77.
Korf, R. E. (1990). Real-time heuristic search. Artificial Intelligence, 42, 189{211.
Kushmerick, N., Hanks, S., & Weld, D. (1995). Algorithm Probabilistic Planning.
Artificial Intelligence, 76, 239{286.
Kushner, H. J., & Chen, C.-H. (1974). Decomposition systems governed Markov
chains. IEEE Transactions Automatic Control, 19 (5), 501{507.
Lee, D., & Yannakakis, M. (1992). Online minimization transition systems. Proceedings
24th Annual ACM Symposium Theory Computing, pp. 264{274 Victoria,
BC.
Lin, F., & Reiter, R. (1994). State constraints revisited. Journal Logic Computation,
4 (5), 655{678.
89

fiBoutilier, Dean, & Hanks

Lin, S.-H. (1997). Exploiting Structure Planning Control. Ph.D. thesis, Department
Computer Science, Brown University.
Lin, S.-H., & Dean, T. (1995). Generating optimal policies high-level plans conditional branches loops. Proceedings Third European Workshop
Planning (EWSP'95), pp. 187{200.
Littman, M. L. (1997). Probabilistic propositional planning: Representations complexity. Proceedings Fourteenth National Conference Artificial Intelligence,
pp. 748{754 Providence, RI.
Littman, M. L., Dean, T. L., & Kaelbling, L. P. (1995). complexity solving
Markov decision problems. Proceedings Eleventh Conference Uncertainty
Artificial Intelligence, pp. 394{402 Montreal, Canada.
Littman, M. L. (1996). Algorithms sequential decision making. Ph.D. thesis CS{96{09,
Brown University, Department Computer Science, Providence, RI.
Lovejoy, W. S. (1991a). Computationally feasible bounds partially observed Markov
decision processes. Operations Research, 39 (1), 162{175.
Lovejoy, W. S. (1991b). survey algorithmic methods partially observed Markov
decision processes. Annals Operations Research, 28, 47{66.
Luenberger, D. G. (1973). Introduction Linear Nonlinear Programming. AddisonWesley, Reading, Massachusetts.
Luenberger, D. G. (1979). Introduction Dynamic Systems: Theory, Models Applications. Wiley, New York.
Madani, O., Condon, A., & Hanks, S. (1999). undecidability probabilistic planning
infinite-horizon partially observable Markov decision problems. Proceedings
Sixteenth National Conference Artificial Intelligence Orlando, FL. appear.
Mahadevan, S. (1994). discount discount reinforcement learning: case
study comparing R-learning Q-learning. Proceedings Eleventh International Conference Machine Learning, pp. 164{172 New Brunswick, NJ.
McAllester, D., & Rosenblitt, D. (1991). Systematic nonlinear planning. Proceedings
Ninth National Conference Artificial Intelligence, pp. 634{639 Anaheim, CA.
McCallum, R. A. (1995). Instance-based utile distinctions reinforcement learning
hidden state. Proceedings Twelfth International Conference Machine
Learning, pp. 387{395 Lake Tahoe, Nevada.
McCarthy, J., & Hayes, P. J. (1969). philosophical problems standpoint
artificial intelligence. Machine Intelligence, 4, 463{502.
90

fiDecision-Theoretic Planning: Structural Assumptions

Meuleau, N., Hauskrecht, M., Kim, K., Peshkin, L., Kaelbling, L., Dean, T., & Boutilier, C.
(1998). Solving large weakly coupled Markov decision processes. Proceedings
Fifteenth National Conference Artificial Intelligence, pp. 165{172 Madison,
WI.
Moore, A. W., & Atkeson, C. G. (1995). parti-game algorithm variable resolution
reinforcement learning multidimensional state spaces. Machine Learning, 21, 199{
234.
Papadimitriou, C. H., & Tsitsiklis, J. N. (1987). complexity Markov chain decision
processes. Mathematics Operations Research, 12 (3), 441{450.
Parr, R. (1998). Flexible decomposition algorithms weakly coupled Markov decision
processes. Proceedings Fourteenth Conference Uncertainty Artificial
Intelligence, pp. 422{430 Madison, WI.
Parr, R., & Russell, S. (1995). Approximating optimal policies partially observable
stochastic domains. Proceedings Fourteenth International Joint Conference
Artificial Intelligence, pp. 1088{1094 Montreal.
Parr, R., & Russell, S. (1998). Reinforcement learning hierarchies machines.
Jordan, M., Kearns, M., & Solla, S. (Eds.), Advances Neural Information Processing
Systems 10, pp. 1043{1049. MIT Press, Cambridge.
Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems: Networks Plausible
Inference. Morgan Kaufmann, San Mateo.
Pednault, E. (1989). ADL: Exploring middle ground STRIPS situation calculus. Proceedings First International Conference Principles
Knowledge Representation Reasoning, pp. 324{332 Toronto, Canada.
Penberthy, J. S., & Weld, D. S. (1992). UCPOP: sound, complete, partial order planner
ADL. Proceedings Third International Conference Principles Knowledge
Representation Reasoning, pp. 103{114 Boston, MA.
Peot, M., & Smith, D. (1992). Conditional Nonlinear Planning. Proceedings First
International Conference AI Planning Systems, pp. 189{197 College Park, MD.
Perez, M. A., & Carbonell, J. G. (1994). Control knowledge improve plan quality.
Proceedings Second International Conference AI Planning Systems, pp. 323{
328 Chicago, IL.
Poole, D. (1995). Exploiting rule structure decision making within independent
choice logic. Proceedings Eleventh Conference Uncertainty Artificial
Intelligence, pp. 454{463 Montreal, Canada.
Poole, D. (1997a). independent choice logic modelling multiple agents uncertainty. Artificial Intelligence, 94 (1{2), 7{56.
91

fiBoutilier, Dean, & Hanks

Poole, D. (1997b). Probabilistic partial evaluation: Exploiting rule structure probabilistic
inference. Proceedings Fifteenth International Joint Conference Artificial
Intelligence, pp. 1284{1291 Nagoya, Japan.
Poole, D. (1998). Context-specific approximation probabilistic inference. Proceedings
Fourteenth Conference Uncertainty Artificial Intelligence, pp. 447{454
Madison, WI.
Precup, D., Sutton, R. S., & Singh, S. (1998). Theoretical results reinforcement learning
temporally abstract behaviors. Proceedings Tenth European Conference
Machine Learning, pp. 382{393 Chemnitz, Germany.
Pryor, L., & Collins, G. (1993). CASSANDRA: Planning contingencies. Technical
report 41, Northwestern University, Institute Learning Sciences.
Puterman, M. L. (1994). Markov Decision Processes. John Wiley & Sons, New York.
Puterman, M. L., & Shin, M. (1978). Modified policy iteration algorithms discounted
Markov decision problems. Management Science, 24, 1127{1137.
Ross, K. W., & Varadarajan, R. (1991). Multichain Markov decision processes
sample-path constraint: decomposition approach. Mathematics Operations Research, 16 (1), 195{207.
Russell, S., & Norvig, P. (1995). Artificial Intelligence: Modern Approach. Prentice Hall,
Englewood Cliffs, NJ.
Sacerdoti, E. D. (1974). Planning hierarchy abstraction spaces. Artificial Intelligence,
5, 115{135.
Sacerdoti, E. D. (1975). nonlinear nature plans. Proceedings Fourth
International Joint Conference Artificial Intelligence, pp. 206{214.
Schoppers, M. J. (1987). Universal plans reactive robots unpredictable environments.
Proceedings Tenth International Joint Conference Artificial Intelligence,
pp. 1039{1046 Milan, Italy.
Schwartz, A. (1993). reinforcement learning method maximizing undiscounted rewards. Proceedings Tenth International Conference Machine Learning,
pp. 298{305 Amherst, MA.
Schweitzer, P. L., Puterman, M. L., & Kindle, K. W. (1985). Iterative aggregationdisaggregation procedures discounted semi-Markov reward processes. Operations
Research, 33, 589{605.
Shachter, R. D. (1986). Evaluating uence diagrams. Operations Research, 33 (6), 871{
882.
Shimony, S. E. (1993). role relevance explanation I: Irrelevance statistical
independence. International Journal Approximate Reasoning, 8 (4), 281{324.
92

fiDecision-Theoretic Planning: Structural Assumptions

Simmons, R., & Koenig, S. (1995). Probabilistic robot navigation partially observable
environments. Proceedings Fourteenth International Joint Conference
Artificial Intelligence, pp. 1080{1087 Montreal, Canada.
Singh, S. P., & Cohn, D. (1998). dynamically merge Markov decision processes.
Advances Neural Information Processing Systems 10, pp. 1057{1063. MIT Press,
Cambridge.
Singh, S. P., Jaakkola, T., & Jordan, M. I. (1994). Reinforcement learning soft state
aggregation. Hanson, S. J., Cowan, J. D., & Giles, C. L. (Eds.), Advances Neural
Information Processing Systems 7. Morgan-Kaufmann, San Mateo.
Smallwood, R. D., & Sondik, E. J. (1973). optimal control partially observable
Markov processes finite horizon. Operations Research, 21, 1071{1088.
Smith, D., & Peot, M. (1993). Postponing threats partial-order planning. Proceedings
Eleventh National Conference Artificial Intelligence, pp. 500{506 Washington, DC.
Sondik, E. J. (1978). optimal control partially observable Markov processes
infinite horizon: Discounted costs. Operations Research, 26, 282{304.
Stone, P., & Veloso, M. (1999). Team-partitioned, opaque-transition reinforcement learning.
Asada, M. (Ed.), RoboCup-98: Robot Soccer World Cup II. Springer Verlag, Berlin.
Sutton, R. S. (1995). TD models: Modeling world mixture time scales.
Proceedings Twelfth International Conference Machine Learning, pp. 531{
539 Lake Tahoe, NV.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: Introduction. MIT Press,
Cambridge, MA.
Tash, J., & Russell, S. (1994). Control strategies stochastic planner. Proceedings
Twelfth National Conference Artificial Intelligence, pp. 1079{1085 Seattle,
WA.
Tatman, J. A., & Shachter, R. D. (1990). Dynamic programming uence diagrams.
IEEE Transactions Systems, Man, Cybernetics, 20 (2), 365{379.
Tesauro, G. J. (1994). TD-Gammon, self-teaching backgammon program, achieves masterlevel play. Neural Computation, 6, 215{219.
Thrun, S., Fox, D., & Burgard, W. (1998). probabilistic approach concurrent mapping
localization mobile robots. Machine Learning, 31, 29{53.
Thrun, S., & Schwartz, A. (1995). Finding structure reinforcement learning. Tesauro,
G., Touretzky, D., & Leen, T. (Eds.), Advances Neural Information Processing
Systems 7 Cambridge, MA. MIT Press.
Warren, D. (1976). Generating conditional plans programs. Proceedings AISB
Summer Conference, pp. 344{354 University Edinburgh.
93

fiBoutilier, Dean, & Hanks

Watkins, C. J. C. H., & Dayan, P. (1992). Q-learning. Machine Learning, 8, 279{292.
Weld, D. S. (1994). introduction least commitment planning. AI Magazine, Winter
1994, 27{61.
White III, C. C., & Scherer, W. T. (1989). Solutions procedures partially observed
Markov decision processes. Operations Research, 37 (5), 791{797.
Williamson, M. (1996). value-directed approach planning. Ph.D. thesis 96{06{03,
University Washington, Department Computer Science Engineering.
Williamson, M., & Hanks, S. (1994). Optimal planning goal-directed utility model.
Proceedings Second International Conference AI Planning Systems, pp.
176{180 Chicago, IL.
Winston, P. H. (1992). Artificial Intelligence, Third Edition. Addison-Wesley, Reading,
Massachusetts.
Yang, Q. (1998). Intelligent Planning : Decomposition Abstraction Based Approach.
Springer Verlag.
Zhang, N. L., & Liu, W. (1997). model approximation scheme planning partially
observable stochastic domains. Journal Artificial Intelligence Research, 7, 199{230.
Zhang, N. L., & Poole, D. (1996). Exploiting causal independence Bayesian network
inference. Journal Artificial Intelligence Research, 5, 301{328.

94


