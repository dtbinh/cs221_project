Journal Artificial Intelligence Research 11 (1999) 241-276

Submitted 1/99; published 9/99

Evolutionary Algorithms Reinforcement Learning
David E. Moriarty

moriarty@isi.edu

University Southern California, Information Sciences Institute
4676 Admiralty Way, Marina del Rey, CA 90292

Alan C. Schultz

Navy Center Applied Research Artificial Intelligence
Naval Research Laboratory, Washington DC 20375-5337

schultz@aic.nrl.navy.mil

John J. Grefenstette

Institute Biosciences, Bioinformatics Biotechnology
George Mason University, Manassas, VA 20110

gref@ib3.gmu.edu

Abstract

two distinct approaches solving reinforcement learning problems, namely,
searching value function space searching policy space. Temporal difference methods evolutionary algorithms well-known examples approaches. Kaelbling,
Littman Moore recently provided informative survey temporal difference methods. article focuses application evolutionary algorithms reinforcement
learning problem, emphasizing alternative policy representations, credit assignment methods, problem-specific genetic operators. Strengths weaknesses evolutionary
approach reinforcement learning presented, along survey representative
applications.

1. Introduction
Kaelbling, Littman, Moore (1996) recently Sutton Barto (1998) provide informative surveys field reinforcement learning (RL). characterize two
classes methods reinforcement learning: methods search space value functions methods search space policies. former class exemplified
temporal difference (TD) method latter evolutionary algorithm (EA)
approach. Kaelbling et al. focus entirely first set methods provide
excellent account state art TD learning. article intended round
picture addressing evolutionary methods solving reinforcement learning
problem.
Kaelbling et al. clearly illustrate, reinforcement learning presents challenging array
diculties process scaling realistic tasks, including problems associated
large state spaces, partially observable states, rarely occurring states, nonstationary environments. point, approach best remains open question,
sensible pursue parallel lines research alternative methods. beyond
scope article address whether better general search value function
space policy space, hope highlight strengths evolutionary
approach reinforcement learning problem. reader advised view
c 1999


AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiMoriarty, Schultz, & Grefenstette

article EA vs. TD discussion. cases, two methods provide complementary
strengths, hybrid approaches advisable; fact, survey implemented systems
illustrates many EA-based reinforcement learning systems include elements TDlearning well.
next section spells reinforcement learning problem. order provide
specific anchor later discussion, Section 3 presents particular TD method. Section 4 outlines approach call Evolutionary Algorithms Reinforcement Learning
(EARL), provides simple example particular EARL system. following three
sections focus features distinguish EAs RL EAs general function optimization, including alternative policy representations, credit assignment methods,
RL-specific genetic operators. Sections 8 9 highlight strengths weaknesses
EA approach. Section 10 brie surveys successful applications EA systems
challenging RL tasks. final section summarizes presentation points
directions research.

2. Reinforcement Learning

reinforcement learning methods share goal: solve sequential decision tasks
trial error interactions environment (Barto, Sutton, & Watkins, 1990;
Grefenstette, Ramsey, & Schultz, 1990). sequential decision task, agent interacts
dynamic system selecting actions affect state transitions optimize
reward function. formally, given time step t, agent perceives state
st selects action at. system responds giving agent (possibly zero)
numerical reward r(st) changing state st+1 = (st ; at). state transition may
determined solely current state agent's action may involve stochastic
processes.
agent's goal learn policy, : ! A, maps states actions.
optimal policy, , defined many ways, typically defined policy
produces greatest cumulative reward states s:

= argmax
V (s); (8s)


(1)

V (s) cumulative reward received state using policy .
many ways compute V (s). One approach uses discount rate discount rewards
time. sum computed infinite horizon:

V

(

1
X
) = ir



i=0

t+i

(2)

rt reward received time step t. Alternatively, V (s) could computed
summing rewards finite horizon h:

V (st) =

Xh r
i=0

t+i

(3)

agent's state descriptions usually identified values returned
sensors, provide description agent's current state state
242

fiEvolutionary Algorithms Reinforcement Learning

world. Often sensors give agent complete state information thus
state partially observable.
Besides reinforcement learning, intelligent agents designed paradigms,
notably planning supervised learning. brie note major differences
among approaches. general, planning methods require explicit model
state transition function (s; a). Given model, planning algorithm search
possible action choices find action sequence guide agent
initial state goal state. Since planning algorithms operate using model
environment, backtrack \undo" state transitions enter undesirable states.
contrast, RL intended apply situations suciently tractable action
model exist. Consequently, agent RL paradigm must actively explore
environment order observe effects actions. Unlike planning, RL agents
cannot normally undo state transitions. course, cases may possible
build action model experience (Sutton, 1990), enabling planning
experience accumulates. However, RL research focuses behavior agent
insucient knowledge perform planning.
Agents trained supervised learning. supervised learning, agent
presented examples state-action pairs, along indication action
either correct incorrect. goal supervised learning induce general policy
training examples. Thus, supervised learning requires oracle supply
correctly labeled examples. contrast, RL require prior knowledge correct
incorrect decisions. RL applied situations rewards sparse;
example, rewards may associated certain states. cases, may
impossible associate label \correct" \incorrect" particular decisions without
reference agent's subsequent decisions, making supervised learning infeasible.
summary, RL provides exible approach design intelligent agents situations planning supervised learning impractical. RL applied
problems significant domain knowledge either unavailable costly obtain.
example, common RL task robot control. Designers autonomous robots often
lack sucient knowledge intended operational environment use either planning
supervised learning regime design control policy robot. case,
goal RL would enable robot generate effective decision policies explores
environment.
Figure 1 shows simple sequential decision task used example later
paper. task agent grid world move state state
selecting among two actions: right (R) (D). sensor agent returns
identity current state. agent always starts state a1 receives reward
indicated upon visiting state. task continues agent moves grid
world (e.g., taking action state a5). goal learn policy returns
highest cumulative rewards. example, policy results sequences
actions R; D; R; D; D; R; R; starting state a1 gives optimal score 17.
243

fiMoriarty, Schultz, & Grefenstette



b

c



e

1

0

2

1

-1

1

2

1

1

2

0

2

3

3

-5

4

3

1

4

1

-2

4

1

2

5

1

1

2

1

1

Figure 1: simple grid-world sequential decision task. agent starts state a1
receives row column current box sensory input. agent moves
one box another selecting two moves (right down),
agent's score increased payoff indicated box. goal find
policy maximizes cumulative score.

2.1 Policy Space vs. Value-Function Space
Given reinforcement learning problem described previous section,
address main topic: find optimal policy, . consider two main approaches,
one involves search policy space involves search value function space.
Policy-space search methods maintain explicit representations policies modify
variety search operators. Many search methods considered,
including dynamic programming, value iteration, simulated annealing, evolutionary
algorithms. paper focuses evolutionary algorithms specialized
reinforcement learning task.
contrast, value function methods maintain explicit representation
policy. Instead, attempt learn value function V , returns expected
cumulative reward optimal policy state. focus research value
function approaches RL design algorithms learn value functions
experience. common approach learning value functions temporal difference (TD) method, described next section.

3. Temporal Difference Algorithms Reinforcement Learning
stated Introduction, comprehensive comparison value function search
direct policy-space search beyond scope paper. Nevertheless, useful
point key conceptual differences typical value function methods typical
evolutionary algorithms searching policy space. common approach learning
value function V RL problems temporal difference (TD) method (Sutton, 1988).
244

fiEvolutionary Algorithms Reinforcement Learning

TD learning algorithm uses observations prediction differences consecutive
states update value predictions. example, two consecutive states j return
payoff prediction values 5 2, respectively, difference suggests payoff
state may overestimated reduced agree predictions
state j . Updates value function V achieved using following update rule:

V (st) = V (st) + ff(V (st+1) , V (st) + rt)
(4)
represents learning rate rt immediate reward. Thus, difference
predictions (V (st+1 ),V (st )) consecutive states used measure prediction error.
Consider chain value predictions V (s0 )::V (sn ) consecutive state transitions
last prediction V (sn ) containing non-zero reward environment.

many iterations sequence, update rule adjust values state
agree successors eventually reward received V (sn ).
words, single reward propagated backwards chain value predictions.
net result accurate value function used predict expected reward
state system.
mentioned earlier,
goal
TD methods learn value function




optimal policy, V . Given V , optimal action, (s), computed using
following equation:
( (s; a))
(s) = argmax
V


(5)

course, already stated RL state transition function (s; a) unknown
agent. Without knowledge, way evaluating (5). alternative
value function used compute (s) called Q-function, Q(s; a) (Watkins,
1989; Watkins & Dayan, 1992). Q-function value function represents
expected value taking action state acting optimally thereafter:

Q(s; a) = r(s) + V ((s; a))

(6)
r(s) represents immediate reward received state s. Given Q-function,
actions optimal policy directly computed using following equation:

(s) = argmax
Q(s; a)


(7)

Q(st; at) = Q(st; at) + ff(max
Q(st+1; at+1) , Q(st; at) + r(st))
+1

(8)

Table 1 shows Q-function grid world problem Figure 1. table-based
representation Q-function associates cumulative future payoffs state-action
pair system. (The letter-number pairs top represent state given row
column Figure 1, R represent actions right down, respectively.)
TD method adjusts Q-values decision. selecting next action,
agent considers effect action examining expected value state
transition caused action.
Q-function learned following TD update equation:


245

fiMoriarty, Schultz, & Grefenstette

a1 a2 a3 a4 a5 b1 b2 b3 b4 b5 c1 c2 c3 c4 c5 d1 d2 d3 d4 d5 e1 e2 e3 e4 e5
R 17 16 10 7 6 17 15 7 6 5 7 9 11 8 4 6 6 7 4 2 1 2 1 2 1
16 11 10 7 1 17 8 1 3 1 15 14 12 8 2 6 7 7 3 1 7 6 4 3 1

Table 1: Q-function simple grid world. value associated state-action
pair.
Essentially, equation updates Q(st ; at) based current reward predicted
reward future actions selected optimally. Watkins Dayan (1992) proved
updates performed fashion every Q-value explicitly represented,
estimates asymptotically converge correct values. reinforcement learning
system thus use Q values select optimal action state. Qlearning widely known implementation temporal difference learning,
use qualitative comparisons evolutionary approaches later sections.

4. Evolutionary Algorithms Reinforcement Learning (EARL)
policy-space approach RL searches policies optimize appropriate objective
function. many search algorithms might used, survey focuses evolutionary
algorithms. begin brief overview simple EA RL, followed detailed
discussion features characterize general class EAs RL.

4.1 Design Considerations Evolutionary Algorithms

Evolutionary algorithms (EAs) global search techniques derived Darwin's theory
evolution natural selection. EA iteratively updates population potential
solutions, often encoded structures called chromosomes. iteration,
called generation, EA evaluates solutions generates offspring based fitness
solution task environment. Substructures, genes, solutions
modified genetic operators mutation recombination. idea
structures associated good solutions mutated combined form
even better solutions subsequent generations. canonical evolutionary algorithm
shown Figure 2. wide variety EAs developed, including genetic
algorithms (Holland, 1975; Goldberg, 1989), evolutionary programming (Fogel, Owens, &
Walsh, 1966), genetic programming (Koza, 1992), evolutionary strategies (Rechenberg,
1964).
EAs general purpose search methods applied variety domains
including numerical function optimization, combinatorial optimization, adaptive control,
adaptive testing, machine learning. One reason widespread success EAs
relatively requirements application, namely,
1. appropriate mapping search space space chromosomes,
2. appropriate fitness function.
246

fiEvolutionary Algorithms Reinforcement Learning

procedure EA
begin
= 0;
initialize P(t);
evaluate structures P(t);
termination condition satisfied
begin
= + 1;
select P(t) P(t-1);
alter structures P(t);
evaluate structures P(t);
end
end.
Figure 2: Pseudo-code Evolutionary Algorithm.
example, case parameter optimization, common represent list
parameters either vector real numbers bit string encodes parameters.
either representations, \standard" genetic operators mutation
cut-and-splice crossover applied straightforward manner produce genetic
variations required (see Figure 3). user must still decide (rather large) number
control parameters EA, including population size, mutation rates, recombination
rates, parent selection rules, extensive literature studies suggest
EAs relatively robust wide range control parameter settings (Grefenstette,
1986; Schaffer, Caruana, Eshelman, & Das, 1989). Thus, many problems, EAs
applied relatively straightforward manner.
However, many applications, EAs need specialized problem domain (Grefenstette, 1987). critical design choice facing user representation, is, mapping search space knowledge structures (or,
phenotype space) space chromosomes (the genotype space). Many studies
shown effectiveness EAs sensitive choice representations.
sucient, example, choose arbitrary mapping search space space
chromosomes, apply standard genetic operators hope best. makes
good mapping subject continuing research, general consensus candidate solutions share important phenotypic similarities must exhibit similar forms
\building blocks" represented chromosomes (Holland, 1975). follows
user EA must carefully consider natural way represent elements
search space chromosomes. Moreover, often necessary design appropriate
mutation recombination operators specific chosen representation.
end result design process representation genetic operators selected
EA comprise form search bias similar biases machine learning meth247

fiMoriarty, Schultz, & Grefenstette

Parent 1:



B

C



E

F

G

Parent 2:



b

c



e

f

g

Offspring 1:



B

C



e

f

g

Offspring 2:



b

c



E

F

G

Figure 3: Genetic operators fixed-position representation. two offspring generated crossing selected parents. operation shown called one-point
crossover. first offspring inherits initial segment one parent
final segment parent. second offspring inherits pattern
genes opposite parents. crossover point position 3, chosen
random. second offspring incurred mutation shaded gene.
ods. Given proper bias, EA quickly identify useful \building blocks" within
population, converge promising areas search space.1
case RL, user needs make two major design decisions. First,
space policies represented chromosomes EA? Second, fitness
population elements assessed? answers questions depend user
chooses bias EA. next section presents simple EARL adopts
straightforward set design decisions. example meant provide baseline
comparison elaborate designs.

4.2 Simple EARL

remainder paper shows, many ways use EAs search space
RL policies. section provides concrete example simple EARL, call
Earl1 . pseudo-code shown Figure 4. system provides EA counterpart
simple table-based TD system described Section 3.
straightforward way represent policy EA use single chromosome per policy single gene associated observed state. Earl1 ,
gene's value (or allele biological terminology) represents action value associated
corresponding state, shown Figure 5. Table 2 shows part Earl1 population
policies sample grid world problem. number policies population
usually order 100 1000.
fitness policy population must ect expected accumulated fitness
agent uses given policy. fixed constraints fitness
individual policy evaluated. world deterministic, sample grid-world,
1. ways exploit problem specific knowledge EAs include use heuristics initialize
population hybridization problem specific search algorithms. See (Grefenstette, 1987)
discussions methods.

248

fiEvolutionary Algorithms Reinforcement Learning

procedure EARL-1
begin
= 0;
initialize population policies, P(t);
evaluate policies P(t);
termination condition satisfied
begin
= + 1;
select high-payoff policies, P(t), policies P(t-1);
update policies P(t);
evaluate policies P(t);
end
end.
Figure 4: Pseudo-code Evolutionary Algorithm Reinforcement Learning system.
Policy i:

s1
a1

s1
a1

s3
a3

...

sN


Figure 5: Table-based policy representation. observed state gene indicates
preferred action state. representation, standard genetic
operators mutation crossover applied.
fitness policy evaluated single trial starts agent
initial state terminates agent reaches terminal state (e.g., falls grid
grid-world). non-deterministic worlds, fitness policy usually averaged
sample trials. options include measuring total payoff achieved
agent fixed number steps, measuring number steps required achieve
fixed level payoff.
fitness policies population determined, new population
generated according steps usual EA (Figure 2). First, parents selected
reproduction. typical selection method probabilistically select individuals based
relative fitness:
(pi )
Pr(pi ) = PnFitness
j =1 Fitness(pj )

(9)

pi represents individual n total number individuals. Using selection
rule, expected number offspring given policy proportional policy's
fitness. example, policy average fitness might single offspring, whereas
249

fiMoriarty, Schultz, & Grefenstette

Policy
1
2
3
4
5

a1


R

R

a2
R





a3






a4


R



a5
R
R
R
R
R

b1
R
R




b2
R
R
R
R
R

b3
R
R

R
R

b4
R
R
R
R


b5
R
R
R
R
R

c1



R
R

c2
R





c3

R

R
R

c4

R
R
R
R

c5
R


R


d1
R
R
R

R

d2



R


d3
R
R
R
R
R

d4
R
R
R

R

d5
R
R
R
R


e1






e2
R
R
R
R
R

e3
R





e4






e5 Fitness
R 8
R 9
17
R 11
16

Table 2: EA population five decision policies sample grid world. simple
policy representation specifies action state world. fitness
corresponds payoffs accumulated using policy grid
world.
policy twice average fitness would two offspring.2 Offspring formed
cloning selected parents. new policies generated applying standard
genetic operators crossover mutation clones, shown Figure 3. process
generating new populations strategies continue indefinitely terminated
fixed number generations acceptable level performance achieved.
simple RL problems grid-world, Earl1 may provide adequate approach. later sections, point ways even Earl1 exhibits
strengths complementary TD methods RL. However, case TD
methods, EARL methods extended handle many challenges inherent
realistic RL problems. following sections survey extensions, organized around three specific biases distinguish EAs Reinforcement Learning (EARL)
generic EAs: policy representations, fitness/credit-assignment models, RLspecific genetic operators.

5. Policy Representations EARL

Perhaps critical feature distinguishes classes EAs one another
representation used. example, EAs function optimization use simple string
vector representation, whereas EAs combinatorial optimization use distinctive representations permutations, trees graph structures. Likewise, EAs RL use
distinctive set representations policies. range potential policy representations unlimited, representations used EARL systems date
largely categorized along two discrete dimensions. First, policies may represented either condition-action rules neural networks. Second, policies may represented
single chromosome representation may distributed one
populations.

5.1 Single-Chromosome Representation Policies
5.1.1 Rule-based Policies

RL problems practical interest, number observable states large,
simple table-based representation Earl1 impractical. large scale state
2. Many parent selection rules explored (Grefenstette, 1997a, 1997b).

250

fiEvolutionary Algorithms Reinforcement Learning

Policy i:

c i1 ai1

c i2 ai2

c i3 ai3

...

c ik aik

Figure 6: Rule-based policy representation. gene represents condition-action rule
maps set states action. general, rules independent
position along chromosome. Con ict resolution mechanisms may
needed conditions rules allowed intersect.
w1
w k1
Policy i:

w1

w2

w3

...

wk

=>
...

wk
wj

Figure 7: simple parameter representation weights neural network. fitness
policy payoff agent uses corresponding neural net
decision policy.
spaces, reasonable represent policy set condition-action rules
condition expresses predicate matches set states, shown Figure 6. Early
examples representation include systems LS-1 (Smith, 1983) LS-2 (Schaffer
& Grefenstette, 1985), followed later Samuel (Grefenstette et al., 1990).
5.1.2 Neural Net Representation Policies

TD-based RL systems, EARL systems often employ neural net representations
function approximators. simplest case (see Figure 7), neural network
agent's decision policy represented sequence real-valued connection weights.
straightforward EA parameter optimization used optimize weights
neural network (Belew, McInerney, & Schraudolph, 1991; Whitley, Dominic, Das, &
Anderson, 1993; Yamauchi & Beer, 1993). representation thus requires least
modification standard EA. turn distributed representations policies
EARL systems.

5.2 Distributed Representation Policies

previous section outlined EARL approaches treat agent's decision policy
single genetic structure evolves time. section addresses EARL approaches
decompose decision policy smaller components. approaches two
potential advantages. First, allow evolution work detailed level task,
e.g., specific subtasks. Presumably, evolving solution restricted subtask
251

fiMoriarty, Schultz, & Grefenstette

Sensors

Message List

Rewards

Classifiers

Decision

Evolutionary
Algorithm

Figure 8: Holland's Learning Classifier System.
easier evolving monolithic policy complex task. Second, decomposition permits
user exploit background knowledge. user might base decomposition
subtasks prior analysis overall performance task; example, might known
certain subtasks mutually exclusive therefore learned independently.
user might decompose complex task subtasks certain components
explicitly programmed components learned.
terms knowledge representation EARL, alternative single chromosome
representation distribute policy several population elements. assigning
fitness individual elements policy, evolutionary selection pressure
brought bear detailed aspects learning task. is, fitness
function individual subpolicies individual rules even individual neurons. general
approach analogous classic TD methods take approach extreme
learning statistics concerning state-action pair. case single-chromosome
representations, partition distributed EARL representations rule-based
neural-net-based classes.
5.2.1 Distributed Rule-based Policies

well-known example distributed rule-based approach EARL Learning Classifier Systems (LCS) model (Holland & Reitman, 1978; Holland, 1987; Wilson,
1994). LCS uses evolutionary algorithm evolve if-then rules called classifiers
map sensory input appropriate action. Figure 8 outlines Holland's LCS framework
(Holland, 1986). sensory input received, posted message list. left
hand side classifier matches message message list, right hand side posted
message list. new messages may subsequently trigger classifiers post
messages invoke decision LCS, traditional forward-chaining model
rule-based systems.
LCS, chromosome represents single decision rule entire population
represents agent's policy. general, classifiers map set observed states set
messages, may interpreted either internal state changes actions. example,
252

fiEvolutionary Algorithms Reinforcement Learning

condition
action strength
a#
! R
0.75
#2
!
0.25
d3

:::
!



0.50

Table 3: LCS population grid world. # don't care symbol allows
generality conditions. example, first rule says \Turn right column
a." strength rule used con ict resolution parent selection
genetic algorithm.

LCS
LCS

LCS

Environment

Figure 9: two-level hierarchical Alecsys system. LCS learns specific behavior.
interactions among rule sets pre-programmed.
learning agent grid world Figure 1 two sensors, one column
one row, population LCS might appear shown Table 3.
first classifier matches state column recommends action R. classifier
statistic called strength estimates utility rule. strength statistics
used con ict resolution (when one action recommended)
fitness genetic algorithm. Genetic operators applied highly fit classifiers
generate new rules. Generally, population size (i.e., number rules policy)
kept constant. Thus classifiers compete space policy.
Another way EARL systems distribute representation policies partition
policy separate modules, module updated EA. Dorigo
Colombetti (1998) describe architecture called Alecsys complex reinforcement learning task decomposed subtasks, learned via separate
LCS, shown Figure 9. provide method called behavior analysis training
(BAT) manage incremental training agents using distributed LCS architecture.
single-chromosome representation extended partitioning policy across multiple co-evolving populations. example, cooperative co-evolution
model (Potter, 1997), agent's policy formed combining chromosomes several independently evolving populations. chromosome represents set rules,
Figure 6, rules address subset performance task. example,
separate populations might evolve policies different components complex task,
253

fiMoriarty, Schultz, & Grefenstette

EA
EA 1

Domain
Model
collaboration

fitness

Evolutionary
Algorithm

Population

representative

Merge

representative

individual

evaluated

representative

EA 2

EA n

representative

Figure 10: Cooperative coevolutionary architecture perspective ith EA instance. EA contributes representative, merged others'
representatives form collaboration, policy agent. fitness
representative ects average fitness collaborations.

might address mutually exclusive sets observed states. fitness chromosome
computed based overall fitness agents employ chromosome part
combined chromosomes. combined chromosomes represent decision policy
called collaboration (Figure 10).
5.2.2 Distributed Network-based Policies

Distributed EARL systems using neural net representations designed.
(Potter & De Jong, 1995), separate populations neurons evolve, evaluation
neuron based fitness collaboration neurons selected population.
SANE (Moriarty & Miikkulainen, 1996a, 1998), two separate populations maintained
evolved: population neurons population network blueprints. motivation SANE comes priori knowledge individual neurons fundamental
building blocks neural networks. SANE explicitly decomposes neural network search
problem several parallel searches effective single neurons. neuron-level evolution provides evaluation recombination neural network building blocks,
population blueprints search effective combinations building blocks. Figure 11
gives overview interaction two populations.
individual blueprint population consists set pointers individuals
neuron population. generation, neural networks constructed
combining hidden neurons specified blueprint. blueprint receives fitness
according well corresponding network performs task. neuron receives
fitness according well top networks participates perform
task. aggressive genetic selection recombination strategy used quickly build
propagate highly fit structures neuron blueprint populations.
254

fiEvolutionary Algorithms Reinforcement Learning

Network Blueprint Population

Neuron Population

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

l

w

Figure 11: overview two populations SANE. member neuron population specifies series connections (connection labels weights)
made within neural network. member network blueprint population specifies series pointers specific neurons used build
neural network.

6. Fitness Credit Assignment EARL

Evolutionary algorithms driven concept natural selection: population
elements higher fitness leave offspring later generations, thus uencing
direction search favor high performance regions search space. concept
fitness central EA. section, discuss features fitness model
common across EARL systems. specifically focus ways fitness
function ects distinctive structure RL problem.

6.1 Agent Model

first common features EARL fitness models fitness computed
respect RL agent. is, however policy represented EA, must
converted decision policy agent operating RL environment. agent
assumed observe description current state, select next action consulting
current policy, collect whatever reward provided environment. EARL
systems, TD systems, agent generally assumed perform little additional
computation selecting next action. neither approach limits agent
strict stimulus-response behavior, usually assumed agent perform
extensive planning reasoning acting. assumption ects fact
RL tasks involve sort control activity agent must respond dynamic
environment within limited time frame.
255

fiMoriarty, Schultz, & Grefenstette

6.2 Policy Level Credit Assignment

shown previous section, meaning fitness EARL systems may vary depending population elements represent. single-chromosome representation,
fitness associated entire policies; distributed representation, fitness may associated individual decision rules. case, fitness always ects accumulated
rewards received agent course interaction environment,
specified RL model. Fitness may ect effort expended, amount delay.
worthwhile considering different approaches credit assignment TD
EA methods. reinforcement learning problem, payoffs may sparse, is,
associated certain states. Consequently, payoff may ect quality
extended sequence decisions, rather individual decision. example, robot
may receive reward movement places \goal" position within room.
robot's reward, however, depends many previous movements leading
point. dicult credit assignment problem therefore exists apportion
rewards sequence decisions individual decisions.
general, EA TD methods address credit assignment problem different ways. TD approaches, credit reward signal explicitly propagated
decision made agent. many iterations, payoffs distributed across
sequence decisions appropriately discounted reward value associated
individual state decision pair.
simple EARL systems Earl1 , rewards associated sequences
decisions distributed individual decisions. Credit assignment
individual decision made implicitly, since policies prescribe poor individual decisions
fewer offspring future generations. selecting poor policies, evolution
automatically selects poor individual decisions. is, building blocks consisting
particular state-action pairs highly correlated good policies propagated
population, replacing state-action pairs associated poorer policies.
Figure 12 illustrates differences credit assignment TD Earl1
grid world Figure 1. Q-learning TD method explicitly assigns credit blame
individual state-action pair passing back immediate reward estimated payoff
new state. Thus, error term becomes associated action performed
agent. EA approach explicitly propagate credit action rather
associates overall fitness entire policy. Credit assigned implicitly, based
fitness evaluations entire sequences decisions. Consequently, EA tend select
policies generate first third sequences achieve lower fitness
scores. EA thus implicitly selects action state b2, example,
present bad sequences present good sequences.

6.3 Subpolicy Credit Assignment

Besides implicit credit assignment performed building blocks, EARL systems
addressed credit assignment problem directly. shown Section 4,
individuals EARL system might represent either entire policies components
policy (e.g., component rule-sets, individual decision rules, individual neurons).
distributed-representation EARLs, fitness explicitly assigned individual components.
256

fiEvolutionary Algorithms Reinforcement Learning

TD Explicit Credit Assignment
2+Max(Q(b1,a))

a1,R

b1,D

2+Max(Q(b1,a))

a1,R

2+Max(Q(a2,a))

a1,D

1+Max(Q(b2,a))

a2,D

a1,R

b1,D

b2,D

b3,D

c3

2

c3

a1,R

b1,D

b2,R

c2,D

c3

9

c3

a1,D

a2,R

b2,D

b3,D

c3

1

d2

a1,D

a2,D

b2,R

c2,D

d2

8

4+Max(Q(c3,a))

4+Max(Q(c3,a))

b3,D

-5+Max(Q(c2,a))

b2,R

c3

c2,D

-5+Max(Q(b3,a))

b2,D

Fitness

4+Max(Q(c3,a))

b3,D

-5+Max(Q(c2,a))

b2,R

1+Max(Q(b2,a))

a2,R

-5+Max(Q(b3,a))

b2,D

1+Max(Q(b2,a))

b1,D

2+Max(Q(a2,a))

a1,D

1+Max(Q(b2,a))

EA Implicit Credit Assignment

4+Max(Q(d2,a))

c2,D

Figure 12: Explicit vs. implicit credit assignment. Q-learning TD method assigns credit
state-action pair based immediate reward predicted future
rewards. EA method assigns credit implicitly associating fitness values
entire sequences decisions.
cases policy represented explicit components, different fitness functions
associated different evolving populations, allowing implementer \shape"
overall policy evolving subpolicies specific subtasks (Dorigo & Colombetti, 1998;
Potter, De Jong, & Grefenstette, 1995). ambitious goal allow system
manage number co-evolving species well form interactions (Potter, 1997).
exciting research still early stage.
example, LCS model, classifier (decision rule) strength
updated using TD-like method called bucket brigade algorithm (Holland, 1986).
bucket brigade algorithm, strength classifier used bid classifiers
right post messages. Bids subtracted winning classifiers passed back
classifiers posted enabling message previous step. Classifier strengths
thus reinforced classifier posts message triggers another classifier.
classifier invokes decision LCS receives strength reinforcement directly
environment. bucket brigade bid passing mechanism clearly bears strong
relation method temporal differences (Sutton, 1988). bucket brigade updates
given classifier's strength based strength classifiers fire direct result
activation. TD methods differ slightly respect assign credit
based strictly temporal succession take account causal relations steps.
remains unclear appropriate distributing credit.
Even single chromosome representations, TD-like methods adopted
EARL systems. Samuel, gene (decision rule) maintains quantity called
strength used resolve con ict one rule matches agent's current
sensor readings. payoff obtained (thereby terminating trial), strengths
257

fiMoriarty, Schultz, & Grefenstette

rules fired trial updated (Grefenstette, 1988). addition resolving
con icts, rule's strength plays role triggering mutation operations, described
next section.

7. RL-Specific Genetic Operators

creation special genetic operators provides another avenue imposing RLspecific bias EAs. Specialized operators EARL systems first appeared (Holland,
1986), so-called triggered operators responsible creating new classifiers
learning agent found classifier existing population matched
agent's current sensor readings. case, high-strength rule explicitly generalized
cover new set sensor readings. similar rule-creation operator included
early versions Samuel (Grefenstette et al., 1990). Later versions Samuel included
number mutation operators created altered rules based agent's early
experiences. example, Samuel's Specialization mutation operator triggered
low-strength, general rule fires episode results high payoff.
case, rule's conditions reduced generality closely match agent's sensor
readings. example, agent sensor readings (range = 40; bearing = 100)
original rule is:
range = [25; 55] bearing = [0; 180] SET turn = 24 (strength
0.1)
new rule would be:
range = [35; 45] bearing = [50; 140] SET turn = 24 (strength
0.8)
Since episode triggering operator resulted high payoff, one might suspect
original rule over-generalized, new, specific version might lead
better results. (The strength new rule initialized payoff received
triggering episode.) considered Lamarckian operator agent's
experience causing genetic change passed later offspring.3
Samuel uses RL-specific crossover operator recombine policies. particular,
crossover Samuel attempts cluster decision rules assigning offspring.
example, suppose traces previous evaluations parent strategies follows (Ri;j denotes j th decision rule policy i):
Trace parent #1:
Episode:
..
.
8. R1;3 ! R1;1 ! R1;7 ! R1;5 High Payoff
9. R1;2 ! R1;8 ! R1;4
Low Payoff
3. Jean Baptiste Lamarck developed evolutionary theory stressed inheritance acquired characteristics, particular acquired characteristics well adapted surrounding environment.
course, Lamarck's theory superseded Darwin's emphasis two-stage adaptation: undirected
variation followed selection. Research generally failed substantiate Lamarckian mechanisms
biological systems (Gould, 1980).

258

fiEvolutionary Algorithms Reinforcement Learning

..
.
Trace parent #2:
..
.
4. R2;7 ! R2;5
5. R2;6 ! R2;2 ! R2;4
..
.
one possible offspring would be:

Low Payoff
High Payoff

fR1;8 ; : : :; R1;3 ; R1;1 ; R1;7 ; R1;5 ; : : :; R2;6 ; R2;2 ; R2;4 ; : : :; R2;7g
motivation rules fire sequence achieve high payoff
treated group recombination, order increase likelihood offspring
policy inherit better behavior patterns parents. Rules
fire successful episodes (e.g., R1;8) randomly assigned one two offspring.
form crossover Lamarckian (since triggered experiences
agent), directly related structure RL problem, since groups
components policies according temporal association among decision rules.

8. Strengths EARL

EA approach represents interesting alternative solving RL problems, offering
several potential advantages scaling realistic applications. particular, EARL
systems developed address dicult challenges RL problems, including:
Large state spaces;
Incomplete state information;
Non-stationary environments.
section focuses ways EARL address challenges.

8.1 Scaling Large State Spaces

Many early papers RL literature analyze eciency alternative learning methods
toy problems similar grid world shown Figure 1. studies useful
academic exercises, number observed states realistic applications RL likely
preclude approach requires explicit storage manipulation statistics
associated observable state-action pair. two ways EARL policy
representations help address problem large state spaces: generalization selectivity.
8.1.1 Policy Generalization

EARL policy representations specify policy level abstraction higher
explicit mapping observed states actions. case rule-based representations,
rule language allows conditions match sets states, thus greatly reducing storage
259

fiMoriarty, Schultz, & Grefenstette

a1 a2 a3 a4 a5 b1 b2 b3 b4 b5 c1 c2 c3 c4 c5 d1 d2 d3 d4 d5 e1 e2 e3 e4 e5
R 16 7 ? 17 12 8 12 11 11 12 14 7 12 13 9 12 11 12 12 11 ? 12 7 ? 9
L 9 13 12 11 ? 15 ? 17 16 ? 11 13 12 7 14 11 12 ? 11 16 12 ? 13 12 16

Table 4: approximated value function population Table 2. table displays average fitness policies select state-action pair ects
estimated impact action overall fitness. Given tiny population
size example, estimates particularly accurate. Note question
marks states actions converged. Since policies select alternative action, population statistics impact actions
fitness. different simple TD methods, statistics actions
maintained.

required specify policy. noted, however, generality rules
within policy may vary considerably, level rules specify action
single observed state way completely general rules recommend action
regardless current state. Likewise, neural net representations, mapping function
stored implicitly weights connections neural net. either case,
generalized policy representation facilitates search good policies grouping together
states action required.
8.1.2 Policy Selectivity

EARL systems selective representations policies. is, EA learns mappings observed states recommended actions, usually eliminating explicit information
concerning less desirable actions. Knowledge bad decisions explicitly preserved,
since policies make decisions selected evolutionary algorithm
eventually eliminated population. advantage selective representations attention focused profitable actions only, reducing space requirements
policies.
Consider example simple EARL operating grid world. population evolves, policies normally converge best actions specific state,
selective pressure achieve high fitness levels. example, population shown
Table 2 converged alleles (actions) states a3; a5; b2; b5; d3; e1; e2.
converged state-action pairs highly correlated fitness. example, policies
converged action R state b2. Taking action R state b2 achieves much higher
expected return action (15 vs. 8 Table 1). Policies select action
state b2 achieve lower fitness scores selected against. simple EARL, snapshot population (Table 2) provides implicit estimate corresponding TD value
function (Table 4), distribution biased toward profitable state-actions
pairs.
260

fiEvolutionary Algorithms Reinforcement Learning

.5
L

3.0
L

Red

R

Blue

R
1.0

Green

L

Blue

L

- 4.0

R
R
1.0
.75

Figure 13: environment incomplete state information. circles represent
states world colors represent agent's sensory input. agent
equally likely start red state green state

8.2 Dealing Incomplete State Information

Clearly, favorable condition reinforcement learning occurs agent
observe true state dynamic system interacts. complete state
information available, TD methods make ecient use available feedback associating
reward directly individual decisions. real world situations, however, agent's
sensors likely provide partial view may fail disambiguate many
states. Consequently, agent often unable completely distinguish current
state. problem termed perceptual aliasing hidden state problem.
case limited sensory information, may useful associate rewards
larger blocks decisions. Consider situation Figure 13, agent must
act without complete state information. Circles represent specific states world,
colors represent sensor information agent receives within state. Square
nodes represent goal states corresponding reward shown inside. state,
agent choice two actions (L R). assume state transitions
deterministic agent equally likely start either state
red green sensor readings.
example, two different states return sensor reading blue,
agent unable distinguish them. Moreover, actions blue
state return different rewards. Q function applied problem treats sensor
reading blue one observable state, rewards action averaged
blue states. Thus, Q(blue; L) Q(blue; R) converge -0.5 1, respectively.
Since reward Q(blue; R) higher alternatives observable states red
green, agent's policy Q-learning choose enter observable state blue
time. final decision policy Q-learning shown Table 5. table
shows optimal policy respect agent's limited view world.
261

fiMoriarty, Schultz, & Grefenstette

Value Function Policy Optimal Policy
R
R
L
R
R
L
Expected Reward
1.0
1.875

Red
Green
Blue

Table 5: policy expected reward returned converged Q function compared
optimal policy given sensory information.
words, policy ects optimal choices agent cannot distinguish two blue
states.
associating values individual observable states, simple TD methods
vulnerable hidden state problems. example, ambiguous state information
misleads TD method, mistakenly combines rewards two different states
system. confounding information multiple states, TD cannot recognize
advantages might associated specific actions specific states, example,
action L top blue state achieves high reward.
contrast, since EA methods associate credit entire policies, rely
net results decision sequences sensor information, may, all,
ambiguous. example, evolutionary algorithm exploits disparity rewards
different blue states evolves policies enter good blue state avoid
bad one. agent remains unable distinguish two blue states, evolutionary algorithm implicitly distinguishes among ambiguous states rewarding policies
avoid bad states.
example, EA method expected evolve optimal policy current
example given existing, ambiguous state information. Policies choose action
sequence R,L starting red state achieve highest levels fitness,
therefore selected reproduction EA. agents using policies
placed green state select action L, receive lowest fitness score, since
subsequent action, L blue sensors, returns negative reward. Thus, many
policies achieve high fitness started red state selected
choose L green state. course many generations, policies must
choose action R green state maximize fitness ensure survival.
confirmed hypotheses empirical tests. Q-learner using single-step updates
table-based representation converged values Table 5 every run.
evolutionary algorithm4 consistently converged 80% population optimal policy.
Figure 14 shows average percentage optimal policy population function
time, averaged 100 independent runs.
Thus even simple EA methods Earl1 appear robust presence
hidden states simple TD methods. However, refined sensor information could
still helpful. previous example, although EA policies achieve better average
reward TD policy, evolved policy remains unable procure 3.0
4. used binary tournament selection, 50 policy population, 0.8 crossover probability, 0.01
mutation rate.

262

fiEvolutionary Algorithms Reinforcement Learning

100

Percentage Optimal

80

60

40

20

0
0

10

20

30

40

50
Generation

60

70

80

90

100

Figure 14: optimal policy distribution hidden state problem evolutionary
algorithm. graph plots percentage optimal policies population,
averaged 100 runs.
1.0 rewards two blue states. rewards could realized, however,
agent could separate two blue states. Thus, method generates additional
features disambiguate states presents important asset EA methods. Kaelbling
et al. (1996) describe several promising solutions hidden state problem,
additional features agent's previous decisions observations automatically
generated included agent's sensory information (Chrisman, 1992; Lin & Mitchell,
1992; McCallum, 1995; Ring, 1994). methods effective disambiguating
states TD methods initial studies, research required determine
extent similar methods resolve significant hidden state information realistic
applications. would useful develop ways use methods augment sensory
data available EA methods well.

8.3 Non-Stationary Environments

agent's environment changes time, RL problem becomes even dicult,
since optimal policy becomes moving target. classic trade-off exploration
exploitation becomes even pronounced. Techniques encouraging exploration
TD-based RL include adding exploration bonus estimated value state-action
pairs ects long since agent tried action (Sutton, 1990),
building statistical model agent's uncertainty (Dayan & Sejnowski, 1996).
Simple modifications standard evolutionary algorithms offer ability track nonstationary environments, thus provide promising approach RL dicult
cases.
fact evolutionary search based competition within population policies
suggest immediate benefits tracking non-stationary environments. extent
population maintains diverse set policies, changes environment bias
263

fiMoriarty, Schultz, & Grefenstette

selective pressure favor policies fit current environment.
long environment changes slowly respect time required evaluate
population policies, population able track changing fitness landscape
without alteration algorithm. Empirical studies show maintaining
diversity within population may require higher mutation rate usually
adopted stationary environments (Cobb & Grefenstette, 1993).
addition, special mechanisms explored order make EAs responsive rapidly changing environments. example, (Grefenstette, 1992) suggests
maintaining random search within restricted portion population. random
population elements analogous immigrants populations uncorrelated
fitness landscapes. Maintaining source diversity permits EA respond rapidly
large, sudden changes fitness landscape. keeping randomized portion
population less 30% population, impact search eciency
stationary environments minimized. general approach easily applied
EARL systems.
useful algorithms developed ensure diversity evolving popultions include fitness sharing (Goldberg & Richardson, 1987), crowding (De Jong, 1975),
local mating (Collins & Jefferson, 1991). Goldberg's fitness sharing model, example, similar individuals forced share large portion single fitness value
shared solution point. Sharing decreases fitness similar individuals causes
evolution select individuals overpopulated niches.
EARL methods employ distributed policy representations achieve diversity automatically well-suited adaptation dynamic environments. distributed
representation, individual represents partial solution. Complete solutions
built combining individuals. individual solve task own,
evolutionary algorithm search several complementary individuals together
solve task. Evolutionary pressures therefore present prevent convergence
population. Moriarty Miikkulainen (1998) showed inherent diversity specialization SANE allow adapt much quickly changes environment
standard, convergent evolutionary algorithms.
Finally, learning system detect changes environment, even direct
response possible. anytime learning model (Grefenstette & Ramsey, 1992),
EARL system maintains case-base policies, indexed values environmental
detectors corresponding environment given policy evolved.
environmental change detected, population policies partially reinitialized,
using previously learned policies selected basis similarity previously
encountered environment current environment. result, environment
changes cyclic, population immediately seeded policies
effect last occurrence current environment. population
policies, approach protected kinds errors detecting environmental
changes. example, even spurious environmental change mistakenly detected,
learning unduly affected, since part current population policies
replaced previously learned policies. Zhou (1990) explored similar approach based
LCS.
264

fiEvolutionary Algorithms Reinforcement Learning

summary, EARL systems respond non-stationary environments, techniques generic evolutionary algorithms techniques specifically designed RL mind.

9. Limitations EARL
Although EA approach RL promising growing list successful applications (as outlined following section), number challenges remain.

9.1 Online Learning
distinguish two broad approaches reinforcement learning |online learning
oine learning. online learning, agent learns directly experiences
operational environment. example, robot might learn navigate warehouse
actually moving physical environment. two problems using EARL
situation. First, likely require large number experiences order
evaluate large population policies. Depending quickly agent performs tasks
result environmental feedback, may take unacceptable amount time
run hundreds generations EA evaluates hundreds thousands policies.
Second, may dangerous expensive permit agent perform actions
actual operational environment might cause harm environment. Yet
likely least policies EA generates bad policies.
objections apply TD methods well. example, theoretical results
prove optimality Q-learning require every state visited infinitely often,
obviously impossible practice. Likewise, TD methods may explore
undesirable states acceptable value-function found.
TD EARL, practical considerations point toward use oine learning,
RL system performs exploration simulation models environment.
Simulation models provide number advantages EARL, including ability
perform parallel evaluations policies population simultaneously (Grefenstette,
1995).

9.2 Rare States
memory record observed states rewards differs greatly EA TD
methods. Temporal difference methods normally maintain statistics concerning every stateaction pair. states revisited, new reinforcement combined previous
value. New information thus supplements previous information, information content agent's reinforcement model increases exploration. manner, TD
methods sustain knowledge good bad state-action pairs.
pointed previously, EA methods normally maintain information good
policies policy components. Knowledge bad decisions explicitly preserved, since
policies make decisions selected evolutionary algorithm
eventually eliminated population. example, refer Table 4,
shows implicit statistics population Table 2. Note question
265

fiMoriarty, Schultz, & Grefenstette

marks states actions converged. Since policies population select
alternative action, EA statistics impact actions fitness.
reduction information content within evolving population disadvantage respect states rarely visited. evolutionary algorithm, value
genes real impact fitness individual tends drift random
values, since mutations tend accumulate genes. state rarely encountered,
mutations may freely accumulate gene describes best action state.
result, even evolutionary algorithm learns correct action rare state,
information may eventually lost due mutations. contrast, since table-based TD
methods permanently record information state-action pairs, may
robust learning agent encounter rare state. course, TD method
uses function approximator neural network value function,
suffer memory loss concerning rare states, since many updates frequently
occurring states dominate updates rare states.

9.3 Proofs Optimality

One attractive features TD methods Q-learning algorithm proof
optimality (Watkins & Dayan, 1992). However, practical importance result
limited, since assumptions underlying proof (e.g., hidden states, state visited
infinitely often) satisfied realistic applications. current theory evolutionary
algorithms provide similar level optimality proofs restricted classes search spaces
(Vose & Wright, 1995). However, general theoretical tools available
applied realistic RL problems. case, ultimate convergence optimal policy
may less important practice eciently finding reasonable approximation.
pragmatic approach may ask ecient alternative RL algorithms are,
terms number reinforcements received developing policy within
tolerance level optimal policy. model probably approximately correct
(PAC) learning (Valiant, 1984), performance learner measured many
learning experiences (e.g., samples supervised learning) required converging
correct hypothesis within specified error bounds. Although developed initially
supervised learning, PAC approach extended recently TD methods
(Fiechter, 1994) general EA methods (Ros, 1997). analytic methods
still early stage development, research along lines may one day
provide useful tools understanding theoretical practical advantages alternative
approaches RL. time, experimental studies provide valuable evidence
utility approach.

10. Examples EARL Methods

Finally, take look significant examples EARL approach results
RL problems. Rather attempt exhaustive survey, selected four EARL
systems representative diverse policies representations outlined Section 5.
Samuel represents class single-chromosome rule-based EARL systems. Alecsys
example distributed rule-based EARL method. Genitor single chromosome
neural-net system, Sane distributed neural net system. brief survey
266

fiEvolutionary Algorithms Reinforcement Learning

provide starting point interested investigating evolutionary approach
reinforcement learning.

10.1

Samuel
Samuel (Grefenstette et al., 1990) EARL system combines Darwinian Lamarckian evolution aspects temporal difference reinforcement learning. Samuel

used learn behaviors navigation collision avoidance, tracking, herding, robots autonomous vehicles.
Samuel uses single-chromosome, rule-based representation policies, is,
member population policy represented rule set gene rule
maps state world actions performed. example rule might be:
range = [35; 45] bearing = [0; 45] SET turn = 16 (strength
0.8)
use high-level language rules offers several advantages low-level binary
pattern languages typically adopted genetic learning systems. First, makes easier
incorporate existing knowledge, whether acquired experts symbolic learning programs. Second, easier transfer knowledge learned human operators. Samuel
includes mechanisms allow coevolution multiple behaviors simultaneously.
addition usual genetic operators crossover mutation, Samuel uses traditional machine learning techniques form Lamarckian operators. Samuel keeps
record recent experiences allow operators generalization, specialization,
covering, deletion make informed changes individual genes (rules) based
experiences.
Samuel used successfully many reinforcement learning applications.
brie describe three examples learning complex behaviors real robots.
applications Samuel, learning performed simulation, ecting fact
initial phases learning, controlling real system expensive
dangerous. Learned behaviors tested on-line system.
(Schultz & Grefenstette, 1992; Schultz, 1994; Schultz & Grefenstette, 1996), Samuel
used learn collision avoidance local navigation behaviors Nomad 200 mobile
robot. sensors available learning task five sonars, five infrared sensors,
range bearing goal, current speed vehicle. Samuel
learned mapping sensors controllable actions { turning rate
translation rate wheels. Samuel took human-written rule set could reach
goal within limited time without hitting obstacle 70 percent time,
50 generations able obtain 93.5 percent success rate.
(Schultz & Grefenstette, 1996), robot learned herd second robot \pasture". task, learning system used range bearing second robot,
heading second robot, range bearing goal, input sensors.
system learned mapping sensors turning rate steering rate.
experiments, success measured percentage times robot could
maneuver second robot goal within limited amount time. second robot
implemented random walk, plus behavior made avoid nearby obstacles.
first robot learned exploit achieve goal moving second robot goal.
267

fiMoriarty, Schultz, & Grefenstette

Samuel given initial, human-designed rule set performance 27 percent,
250 generations able move second robot goal 86 percent
time.
(Grefenstette, 1996) Samuel EA system combined case-based learning
address adaptation problem. approach, called anytime learning (Grefenstette &
Ramsey, 1992), learning agent interacts external environment
internal simulation. anytime learning approach involves two continuously running
interacting modules: execution module learning module. execution
module controls agent's interaction environment includes monitor
dynamically modifies internal simulation model based observations actual agent
environment. learning module continuously tests new strategies agent
simulation model, using genetic algorithm evolve improved strategies,
updates knowledge base used execution module best available results.
Whenever simulation model modified due observed change agent
environment, genetic algorithm restarted modified model. learning system
operates indefinitely, execution system uses results learning become
available. work Samuel shows EA method particularly well-suited
anytime learning. Previously learned strategies treated cases, indexed
set conditions learned. new situation encountered,
nearest neighbor algorithm used find similar previously learned cases.
nearest neighbors used re-initialize genetic population policies new case.
Grefenstette (1996) reports experiments mobile robot learns track another
robot, dynamically adapts policies using anytime learning encounters series
partial system failures. approach blurs line online oine learning,
since online system updated whenever oine learning system develops
improved policy. fact, oine learning system even executed on-board
operating mobile robot.

10.2

Alecsys

described previously, Alecsys (Dorigo & Colombetti, 1998) distributed rule-based
EA supports approach design autonomous systems called behavioral engineering. approach, tasks performed complex autonomous systems
decomposed individual behaviors, learned via learning classifier systems module, shown Figure 9. decomposition performed human designer,
fitness function associated LCS carefully designed ect role
associated component behavior within overall autonomous system. Furthermore,
interactions among modules preprogrammed. example, designer may
decide robot learn approach goal except threatening predator
near, case robot evade predator. overall architecture
set behaviors set evasion behavior higher priority
goal-seeking behavior, individual LCS modules evolve decision rules
optimally performing subtasks.
Alecsys used develop behavioral rules number behaviors
autonomous robots, including complex behavior groups Chase/Feed/Escape
268

fiEvolutionary Algorithms Reinforcement Learning

(Dorigo & Colombetti, 1998). approach implemented tested
simulated robots real robots. exploits human design EARL
methods optimize system performance, method shows much promise scaling
realistic tasks.

10.3

Genitor

Genitor (Whitley & Kauth, 1988; Whitley, 1989) aggressive, general purpose genetic

algorithm shown effective specialized use reinforcement-learning
problems. Whitley et al. (1993) demonstrated Genitor eciently evolve decision
policies represented neural networks using limited reinforcement domain.
Genitor relies solely evolutionary algorithm adjust weights neural
networks. solving RL problems, member population Genitor represents
neural network sequence connection weights. weights concatenated realvalued chromosome along gene represents crossover probability. crossover
gene determines whether network mutated (randomly perturbed) whether
crossover operation (recombination another network) performed. crossover
gene modified passed offspring based offspring's performance compared
parent. offspring outperforms parent, crossover probability decreased.
Otherwise, increased. Whitley et al. refer technique adaptive mutation,
tends increase mutation rate populations converge. Essentially, method
promotes diversity within population encourage continual exploration solution
space.
Genitor uses so-called \steady-state" genetic algorithm new parents
selected genetic operators applied individual evaluated. approach
contrasts \generational" GAs entire population evaluated replaced
generation. steady-state GA, policy evaluated retains
fitness value indefinitely. Since policies lower fitness likely
replaced, possible fitness based noisy evaluation function may
undesirable uence direction search. case pole-balancing RL
application, fitness value depends length time policy maintain
good balance, given randomly chosen initial state. fitness therefore random
variable depends initial state. authors believe noise fitness
function little negative impact learning good policies, perhaps
dicult poor networks obtain good fitness good networks (of
many copies population) survive occasional bad fitness evaluation.
interesting general issue EARL needs analysis.
Genitor adopts specific modification RL applications. First, representation uses real-valued chromosome rather bit-string representation weights.
Consequently, Genitor always recombines policies weight definitions, thus reducing potentially random disruption neural network weights might result crossover
operations occurred middle weight definition. second modification
high mutation rate helps maintain diversity promote rapid exploration
policy space. Finally, Genitor uses unusually small populations order discourage
different, competing neural network \species" forming within population. Whit269

fiMoriarty, Schultz, & Grefenstette

ley et al. (1993) argue speciation leads competing conventions produces poor
offspring two dissimilar networks recombined.
Whitley et al. (1993) compare Genitor Adaptive Heuristic Critic (Anderson,
1989, AHC), uses TD method reinforcement learning. several different
versions common pole-balancing benchmark task, Genitor found comparable AHC learning rate generalization. One interesting difference
Whitley et al. found Genitor consistent AHC solving
pole-balancing problem failure signals occurs wider pole bounds (make
problem much harder). AHC, preponderance failures appears cause states
overpredict failure. contrast, EA method appears effective finding policies
obtain better overall performance, even success uncommon. difference seems
EA tends ignore cases pole cannot balanced, concentrate successful cases. serves another example advantages associated
search policy space, based overall policy performance, rather paying
much attention value associated individual states.

10.4

Sane

Sane (Symbiotic, Adaptive Neuro-Evolution) system designed ecient method
building artificial neural networks RL domains possible generate
training data normal supervised learning (Moriarty & Miikkulainen, 1996a, 1998).
Sane system uses evolutionary algorithm form hidden layer connections
weights neural network. neural network forms direct mapping sensors
actions provides effective generalization state space. Sane's method
credit assignment EA, allows apply many problems
reinforcement sparse covers sequence decisions. described previously, Sane
uses distributed representation policies.
Sane offers two important advantages reinforcement learning normally
present implementations neuro-evolution. First, maintains diverse populations.
Unlike canonical function optimization EA converge population single solution, Sane forms solutions unconverged population. several different types
neurons necessary build effective neural network, inherent evolutionary
pressure develop neurons perform different functions thus maintain several different types individuals within population. Diversity allows recombination operators
crossover continue generate new neural structures even prolonged evolution.
feature helps ensure solution space explored eciently throughout
learning process. Sane therefore resilient suboptimal convergence
adaptive changes domain.
second feature Sane explicitly decomposes search complete solutions search partial solutions. Instead searching complete neural networks
once, solutions smaller problems (good neurons) evolved, combined form effective full solution (a neural network). words, Sane effectively
performs problem reduction search space neural networks.
Sane shown effective several different large scale problems. one problem,
Sane evolved neural networks direct focus minimax game-tree search (Moriarty
270

fiEvolutionary Algorithms Reinforcement Learning

& Miikkulainen, 1994). selecting moves evaluated given game
situation, Sane guides search away misinformation search tree towards
effective moves. Sane tested game tree search Othello using
evaluation function former world champion program Bill (Lee & Mahajan, 1990).
Tested full-width minimax search, Sane significantly improved play Bill,
examining subset board positions.
second application, SANE used learn obstacle avoidance behaviors
robot arm (Moriarty & Miikkulainen, 1996b). approaches learning robot arm
control learn hand-eye coordination supervised training methods examples
correct behavior explicitly given. Unfortunately domains obstacles
arm must make several intermediate joint rotations reaching target, generating
training examples extremely dicult. reinforcement learning approach, however,
require examples correct behavior learn intermediate movements
general reinforcements. Sane implemented form neuro-control networks capable
maneuvering OSCAR-6 robot arm among obstacles reach random target locations.
Given camera-based visual infrared sensory input, neural networks learned
effectively combine target reaching obstacle avoidance strategies.
related examples evolutionary methods learning neural-net control
systems robotics, reader see (Cliff, Harvey, & Husbands, 1993; Husbands,
Harvey, & Cliff, 1995; Yamauchi & Beer, 1993).

11. Summary
article began suggesting two distinct approaches solving reinforcement learning
problems; one search value function space one search policy space. TD
EARL examples two complementary approaches. approaches assume
limited knowledge underlying system learn experimenting different policies using reinforcement alter policies. Neither approach requires precise
mathematical model domain, may learn direct interactions
operational environment.
Unlike TD methods, EARL methods generally base fitness overall performance
policy. sense, EA methods pay less attention individual decisions TD
methods do. first glance, approach appears make less ecient use
information, may fact provide robust path toward learning good policies, especially
situations sensors inadequate observe true state world.
useful view path toward practical RL systems choice EA
TD methods. tried highlight strengths evolutionary
approach, shown EARL TD, complementary approaches,
means mutually exclusive. cited examples successful EARL systems
Samuel Alecsys explicitly incorporate TD elements multilevel credit assignment methods. likely many practical applications depend
kinds multi-strategy approaches machine learning.
listed number areas need work, particularly theoretical side. RL, would highly desirable better tools predicting
amount experience needed learning agent reaching specified level per271

fiMoriarty, Schultz, & Grefenstette

formance. existing proofs optimality Q-learning EA extremely
limited practical use predicting well either approach perform realistic problems. Preliminary results shown tools PAC analysis applied
EA TD methods, much effort needed direction.
Many serious challenges remain scaling reinforcement learning methods realistic applications. pointing shared goals concerns two complementary
approaches, hope motivate collaboration progress field.

References

Anderson, C. W. (1989). Learning control inverted pendulum using neural networks.
IEEE Control Systems Magazine, 9, 31{37.
Barto, A. G., Sutton, R. S., & Watkins, C. J. C. H. (1990). Learning sequential
decision making. Gabriel, M., & Moore, J. W. (Eds.), Learning Computational
Neuroscience. MIT Press, Cambridge, MA.
Belew, R. K., McInerney, J., & Schraudolph, N. N. (1991). Evolving networks: Using
genetic algorithm connectionist learning. Farmer, J. D., Langton, C.,
Rasmussen, S., & Taylor, C. (Eds.), Artificial Life II Reading, MA. Addison-Wesley.
Chrisman, L. (1992). Reinforcement learning perceptual aliasing: perceptual
distinctions approach. Proceedings Tenth National Conference Artificial
Intelligence, pp. 183{188 San Jose, CA.
Cliff, D., Harvey, I., & Husbands, P. (1993). Explorations evolutionary robotics. Adaptive
Behavior, 2, 73{110.
Cobb, H. G., & Grefenstette, J. J. (1993). Genetic algorithms tracking changing environments. Proc. Fifth International Conference Genetic Algorithms, pp. 523{530.
Collins, R. J., & Jefferson, D. R. (1991). Selection massively parallel genetic algorithms.
Proceedings Fourth International Conference Genetic Algorithms, pp.
249{256 San Mateo, CA. Morgan Kaufmann.
Dayan, P., & Sejnowski, T. J. (1996). Exploration bonuses dual control. Machine
Learning, 25 (1), 5{22.
De Jong, K. A. (1975). Analysis Behavior Class Genetic Adaptive Systems.
Ph.D. thesis, University Michigan, Ann Arbor, MI.
Dorigo, M., & Colombetti, M. (1998). Robot Shaping: Experiment Behavioral Engineering. MIT Press, Cambridge, MA.
Fiechter, C.-N. (1994). Ecient reinforcement learning. Proceedings Seventh
Annual ACM Conference Computational Learning Theory, pp. 88{97. Association
Computing Machinery.
Fogel, L. J., Owens, A. J., & Walsh, M. J. (1966). Artificial Intelligence Simulated
Evolution. Wiley Publishing, New York.
272

fiEvolutionary Algorithms Reinforcement Learning

Goldberg, D. E. (1989). Genetic Algorithms Search, Optimization, Machine Learning. Addison-Wesley, Reading, MA.
Goldberg, D. E., & Richardson, J. (1987). Genetic algorithms sharing multimodal
function optimization. Proceedings Second International Conference Genetic Algorithms, pp. 148{154 San Mateo, CA. Morgan Kaufmann.
Grefenstette, J. J. (1986). Optimization control parameters genetic algorithms. IEEE
Transactions Systems, Man & Cybernetics, SMC-16 (1), 122{128.
Grefenstette, J. J. (1987). Incorporating problem specific knowledge genetic algorithms.
Davis, L. (Ed.), Genetic Algorithms Simulated Annealing, pp. 42{60 San Mateo,
CA. Morgan Kaufmann.
Grefenstette, J. J. (1988). Credit assignment rule discovery system based genetic
algorithms. Machine Learning, 3 (2/3), 225{245.
Grefenstette, J. J. (1992). Genetic algorithms changing environments. Manner, R.,
& Manderick, B. (Eds.), Parallel Problem Solving Nature, 2, pp. 137{144.
Grefenstette, J. J. (1995). Robot learning parallel genetic algorithms networked
computers. Proceedings 1995 Summer Computer Simulation Conference
(SCSC '95), pp. 352{257.
Grefenstette, J. J. (1996). Genetic learning adaptation autonomous robots. Robotics
Manufacturing: Recent Trends Research Applications, Volume 6, pp. 265{
270. ASME Press, New York.
Grefenstette, J. J. (1997a). Proportional selection sampling algorithms. Handbook
Evolutionary Computation, chap. C2.2. IOP Publishing Oxford University Press.
Grefenstette, J. J. (1997b). Rank-based selection. Handbook Evolutionary Computation, chap. C2.4. IOP Publishing Oxford University Press.
Grefenstette, J. J., & Ramsey, C. L. (1992). approach anytime learning. Proc.
Ninth International Conference Machine Learning, pp. 189{195 San Mateo, CA.
Morgan Kaufmann.
Grefenstette, J. J., Ramsey, C. L., & Schultz, A. C. (1990). Learning sequential decision
rules using simulation models competition. Machine Learning, 5, 355{381.
Holland, J. H. (1975). Adaptation Natural Artificial Systems: Introductory
Analysis Applications Biology, Control Artificial Intelligence. University
Michigan Press, Ann Arbor, MI.
Holland, J. H. (1986). Escaping brittleness: possibilities general-purpose learning
algorithms applied parallel rule-based systems. Machine Learning: Artificial
Intelligence Approach, Vol. 2. Morgan Kaufmann, Los Altos, CA.
273

fiMoriarty, Schultz, & Grefenstette

Holland, J. H. (1987). Genetic algorithms classifier systems: Foundations future
directions. Proceedings Second International Conference Genetic Algorithms, pp. 82{89 Hillsdale, New Jersey.
Holland, J. H., & Reitman, J. S. (1978). Cognitive systems based adaptive algorithms.
Pattern-Directed Inference Systems. Academic Press, New York.
Husbands, P., Harvey, I., & Cliff, D. (1995). Circle round: state space attractors
evolved sighted robots. Robot. Autonomous Systems, 15, 83{106.
Kaelbling, L. P., Littman, M. L., & Moore, A. W. (1996). Reinforcement learning: survey.
Journal Artificial Intelligence Research, 4, 237{285.
Koza, J. R. (1992). Genetic Programming: Programming Computers Means
Natural Selection. MIT Press, Cambridge, MA.
Lee, K.-F., & Mahajan, S. (1990). development world class Othello program.
Artificial Intelligence, 43, 21{36.
Lin, L.-J., & Mitchell, T. M. (1992). Memory approaches reinforcement learning nonMarkovian domains. Tech. rep. CMU-CS-92-138, Carnegie Mellon University, School
Computer Science.
McCallum, A. K. (1995). Reinforcement Learning Selective Perception Hidden
State. Ph.D. thesis, University Rochester.
Moriarty, D. E., & Miikkulainen, R. (1994). Evolving neural networks focus minimax
search. Proceedings Twelfth National Conference Artificial Intelligence
(AAAI-94), pp. 1371{1377 Seattle, WA. MIT Press.
Moriarty, D. E., & Miikkulainen, R. (1996a). Ecient reinforcement learning
symbiotic evolution. Machine Learning, 22, 11{32.
Moriarty, D. E., & Miikkulainen, R. (1996b). Evolving obstacle avoidance behavior
robot arm. Animals Animats: Proceedings Fourth International
Conference Simulation Adaptive Behavior (SAB-96), pp. 468{475 Cape Cod,
MA.
Moriarty, D. E., & Miikkulainen, R. (1998). Forming neural networks ecient
adaptive co-evolution. Evolutionary Computation, 5 (4), 373{399.
Potter, M. A. (1997). Design Analysis Computational Model Cooperative
Coevolution. Ph.D. thesis, George Mason University.
Potter, M. A., & De Jong, K. A. (1995). Evolving neural networks collaborative
species. Proceedings 1995 Summer Computer Simulation Conference Ottawa,
Canada.
Potter, M. A., De Jong, K. A., & Grefenstette, J. (1995). coevolutionary approach
learning sequential decision rules. Eshelman, L. (Ed.), Proceedings Sixth
International Conference Genetic Algorithms Pittsburgh, PA.
274

fiEvolutionary Algorithms Reinforcement Learning

Rechenberg, I. (1964). Cybernetic solution path experimental problem. Library
Translation 1122. Royal Aircraft Establishment, Farnborough, Hants, Aug. 1965.
Ring, M. B. (1994). Continual Learning Reinforcement Environments. Ph.D. thesis,
University Texas Austin.
Ros, J. P. (1997). Probably approximately correct (PAC) learning analysis. Handbook
Evolutionary Computation, chap. B2.8. IOP Publishing Oxford University Press.
Schaffer, J. D., Caruana, R. A., Eshelman, L. J., & Das, R. (1989). study control
parameters affecting online performance genetic algorithms function optimization. Proceedings Third International Conference Genetic Algorithms,
pp. 51{60. Morgan Kaufmann.
Schaffer, J. D., & Grefenstette, J. J. (1985). Multi-objective learning via genetic algorithms.
Proceedings Ninth International Joint Conference Artificial Intelligence,
pp. 593{595. Morgan Kaufmann.
Schultz, A. C. (1994). Learning robot behaviors using genetic algorithms. Intelligent
Automation Soft Computing: Trends Research, Development, Applications,
pp. 607{612. TSI Press, Albuquerque.
Schultz, A. C., & Grefenstette, J. J. (1992). Using genetic algorithm learn behaviors
autonomous vehicles. Proceedings AiAA Guidance, Navigation, Control
Conference Hilton Head, SC.
Schultz, A. C., & Grefenstette, J. J. (1996). Robo-shepherd: Learning complex robotic behaviors. Robotics Manufacturing: Recent Trends Research Applications,
Volume 6, pp. 763{768. ASME Press, New York.
Smith, S. F. (1983). Flexible learning problem solving heuristics adaptive search.
Proceedings Eighth International Joint Conference Artificial Intelligence,
pp. 422{425. Morgan Kaufmann.
Sutton, R. (1990). Integrated architectures learning, planning, reacting based
approximate dynamic programming. Machine Learning: Proceedings Seventh
International Conference, pp. 216{224.
Sutton, R. S. (1988). Learning predict methods temporal differences. Machine
Learning, 3, 9{44.
Sutton, R. S., & Barto, A. (1998). Reinforcement Learning: Introduction. MIT Press,
Cambridge, MA.
Valiant, L. G. (1984). theory learnable. Communications ACM, 27, 1134{
1142.
Vose, M. D., & Wright, A. H. (1995). Simple genetic algorithms linear fitness. Evolutionary Computation, 2, 347{368.
275

fiMoriarty, Schultz, & Grefenstette

Watkins, C. J. C. H. (1989). Learning Delayed Rewards. Ph.D. thesis, University
Cambridge, England.
Watkins, C. J. C. H., & Dayan, P. (1992). Q-learning. Machine Learning, 8 (3), 279{292.
Whitley, D. (1989). GENITOR algorithm selective pressure. Proceedings
Third International Conference Genetic Algorithms, pp. 116{121 San Mateo, CA.
Morgan Kaufman.
Whitley, D., & Kauth, J. (1988). GENITOR: different genetic algorithm. Proceedings
Rocky Mountain Conference Artificial Intelligence, pp. 118{130 Denver, CO.
Whitley, D., Dominic, S., Das, R., & Anderson, C. W. (1993). Genetic reinforcement
learning neurocontrol problems. Machine Learning, 13, 259{284.
Wilson, S. W. (1994). ZCS: zeroth level classifier system. Evolutionary Computation,
2 (1), 1{18.
Yamauchi, B. M., & Beer, R. D. (1993). Sequential behavior learning evolved
dynamical neural networks. Adaptive Behavior, 2, 219{246.
Zhou, H. (1990). CSM: computational model cumulative learning. Machine Learning,
5 (4), 383{406.

276


