Journal Artificial Intelligence Research 15 (2001) 319-350

Submitted 9/00; published 11/01

Infinite-Horizon Policy-Gradient Estimation
Jonathan Baxter

JBAXTER @ WHIZBANG . COM

WhizBang! Labs.
4616 Henry Street Pittsburgh, PA 15213

Peter L. Bartlett

BARTLETT @ BARNHILLTECHNOLOGIES . COM

BIOwulf Technologies.
2030 Addison Street, Suite 102, Berkeley, CA 94704

Abstract
Gradient-based approaches direct policy search reinforcement learning received
much recent attention means solve problems partial observability avoid
problems associated policy degradation value-function methods. paper introduce GPOMDP, simulation-based algorithm generating biased estimate gradient
average reward Partially Observable Markov Decision Processes (POMDPs) controlled
parameterized stochastic policies. similar algorithm proposed Kimura, Yamamura,
Kobayashi (1995). algorithms chief advantages requires storage twice
number policy parameters, uses one free parameter 2 [0; 1) (which natural interpretation
terms bias-variance trade-off), requires knowledge underlying state. prove
convergence GPOMDP, show correct choice parameter related
mixing time controlled POMDP. briefly describe extensions GPOMDP controlled
Markov chains, continuous state, observation control spaces, multiple-agents, higher-order
derivatives, version training stochastic policies internal states. companion paper
(Baxter, Bartlett, & Weaver, 2001) show gradient estimates generated GPOMDP
used traditional stochastic gradient algorithm conjugate-gradient procedure
find local optima average reward.

1. Introduction
Dynamic Programming method choice solving problems decision making
uncertainty (Bertsekas, 1995). However, application Dynamic Programming becomes problematic large infinite state-spaces, situations system dynamics unknown,
state partially observed. cases one looks approximate techniques
rely simulation, rather explicit model, parametric representations either valuefunction policy, rather exact representations.
Simulation-based methods rely parametric form value function tend go
name Reinforcement Learning, extensively studied Machine Learning
literature (Bertsekas & Tsitsiklis, 1996; Sutton & Barto, 1998). approach yielded
remarkable empirical successes number different domains, including learning play checkers (Samuel, 1959), backgammon (Tesauro, 1992, 1994), chess (Baxter, Tridgell, & Weaver,
2000), job-shop scheduling (Zhang & Dietterich, 1995) dynamic channel allocation (Singh &
Bertsekas, 1997).
Despite success, algorithms training approximate value functions suffer
theoretical flaw: performance greedy policy derived approximate valuefunction guaranteed improve iteration, fact worse old policy

c 2001 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiBAXTER & BARTLETT

amount equal maximum approximation error states. happen even
parametric class contains value function whose corresponding greedy policy optimal.
illustrate concrete simple example Appendix A.
alternative approach circumvents problemthe approach pursue hereis
consider class stochastic policies parameterized 2 R K , compute gradient respect
average reward, improve policy adjusting parameters gradient
direction. Note policy could directly parameterized, could generated indirectly
value function. latter case value-function parameters parameters
policy, instead adjusted minimize error approximate true value
function, parameters adjusted directly improve performance policy generated
value function.
policy-gradient algorithms long history Operations Research, Statistics, Control Theory, Discrete Event Systems Machine Learning. describing contribution
present paper, seems appropriate introduce background material explaining approach. Readers already familiar material may want skip directly section 1.2,
contributions present paper described.
1.1 Brief History Policy-Gradient Algorithms
large-scale problems problems system dynamics unknown, performance
gradient computable closed form1 . Thus challenging aspect policy-gradient
approach find algorithm estimating gradient via simulation. Naively, gradient
calculated numerically adjusting parameter turn estimating effect performance via simulation (the so-called crude Monte-Carlo technique), prohibitively
inefficient problems. Somewhat surprisingly, mild regularity conditions, turns
full gradient estimated single simulation system. technique
called score function likelihood ratio method appears first proposed
sixties (Aleksandrov, Sysoyev, & Shemeneva, 1968; Rubinstein, 1969) computing performance
gradients i.i.d. (independently identically distributed) processes.
Specifically, suppose r X performance function depends random variable
X , q ; x probability X x, parameterized 2 RK . mild regularity
conditions, gradient respect expected performance,

( )

( )

may written

see this, rewrite (1) sum

=

() = Er(X );

(1)

r() = Er(X ) rqq(;(;XX)) :

(2)

() =

X

r() =

X

r(x)q(; x);

x
differentiate (one source requirement mild regularity conditions) obtain
x

r(x)rq(; x);

1. See equation (17) closed-form expression performance gradient.

320

fiP OLICY-G RADIENT E STIMATION

rewrite

X

r() =

r(x)

rq(; x) q(; x);

( )

q ; x
x
observe formula equivalent (2).
simulator available generate samples X distributed according q ; x ,
sequence X1 ; X2 ; : : : ; XN generated i.i.d. according q ; x gives unbiased estimate,

( )

( )
N
X
r^ () = N1 r(Xi) rqq(;(;XX)i) ;
(3)


^ () ! r() probability one. quantity
r ( ). law large numbers, r
rq(; X )=q(; X ) known likelihood ratio score function classical statistics.
performance function r (X ) depends , r (X )rq (; X )=q (; X ) replaced
rr(; X ) + r(; X )rq(; X )=q(; X ) (2).
=1

1.1.1 U NBIASED E STIMATES
P ROCESSES



P ERFORMANCE G RADIENT



R EGENERATIVE

Extensions likelihood-ratio method regenerative processes (including Markov Decision
Processes MDPs) given Glynn (1986, 1990), Glynn LEcuyer (1995) Reiman
Weiss (1986, 1989), independently episodic Partially Observable Markov Decision
Processes (POMDPs) Williams (1992), introduced REINFORCE algorithm2 .
i.i.d. samples X previous section sequences states X0 ; : : : ; XT (of random length)
encountered visits designated recurrent state , sequences states
start state goal state. case rq ; X =q ; X written sum

(

) (

)

rq(; X ) = TX rpXtXt+1 () ;
1

q(; X )

()

t=0

pXt Xt+1 ()

(4)

pXt Xt+1 transition probability Xt Xt+1 given parameters . Equation (4)
admits recursive computation course regenerative cycle form z0
2 RK ,
state transition Xt ! Xt+1 ,

=0

zt+1 = zt +

( ) (
)

) (

)

rpXtXt+1 () ;
pXt Xt+1 ()

term r X rq ; X =q ; X estimate (3) form3
addition, r X0 ; : : : ; XT recursively computed

(

(5)

r(X0 ; : : : ; XT )zT . If,

r(X0 ; : : : ; Xt+1 ) = (r(X0 ; : : : ; Xt ); Xt+1 )
function , estimate r (X0 ; : : : ; XT )zT cycle computed using
storage K + 1 parameters (K zt 1 parameter update performance function
r). Hence, entire estimate (3) computed storage 2K + 1 real parameters,
follows.

2. thresholded version algorithms neuron-like elements described earlier Barto, Sutton, Anderson (1983).
3. vector zT known reinforcement learning eligibility trace. terminology used Barto et al.
(1983).

321

fiBAXTER & BARTLETT

Algorithm 1.1: Policy-Gradient Algorithm Regenerative Processes.
1. Set j

= 0, r = 0, z = 0, = 0 (z ; 2 RK ).
0

0

0

2. state transition Xt




3. j

0

! Xt

+1

0

:

episode finished (that is, Xt+1
j +1
j rt zt ,
j j ,
zt+1 ,
rt+1 .

= +
= +1
=0
=0
Otherwise, set

rp

= i), set



zt+1 = zt + pXXtXtXt+1() ;
t+1
rt+1 = (rt ; Xt+1 ).
( )

= N return N =N , otherwise goto 2.

Examples recursive
performance functions include sum scalar reward cycle,
P

r(X0 ; : : : ; XT ) = Tt=0 r(Xt ) r(i) scalar reward associated state (this corresponds ( ) average reward multiplied expected recurrence time E [T ]);

1
()

negative length cycle (which implemented assigning reward
state,
used task mimimize time taken get goal state, since
case
PT

E ); discounted reward start state, r X0 ; : : : ; XT
t=0 r Xt ,
2 ; discount factor, on.
Williams (1992) pointed out, simplification possible case rT
r X0 ; : : : ; XT sum scalar rewards r Xt ; depending state possibly time
since starting state (such r Xt ; r Xt , r Xt ; fft r Xt above). case,
update single regenerative cycle may written

[ ]
[0 1)

(

(

)

(



=

TX1
t=0

pXt Xt+1 ()

"
X

s=0

( )

=

( )
)= ( ) (

rpXtXt+1 ()

()

)=

)=

r(Xs ; s) +


X
s=t+1

( )

#

r(Xs ; s) :

(

)

changes pXt Xt+1 influence rewards r Xs ; associated earlier
states (s t), able drop first term parentheses right-hand-side
write
TX1

rpXtXt+1 X
r X ;s :
(6)
p
s=t+1
t=0 Xt Xt+1

=

()
()

(

)

Although proof entirely trivial, intuition indeed shown correct.
Equation (6) allows even simpler recursive formula estimating performance gradient. Set z0
, introduce new variable
. before, set zt+1
zt
0
rpXtXt+1 =pXtXt+1 Xt+1 6 ,
zt+1
otherwise.
now, iteration, set t+1 r Xt ; zt
. =t estimate r . Since
updated every iteration, suggests away altogether simply update directly: t+1 r Xt ; zt , suitable step-sizes4 . Proving convergence

()

P

= =0
()

= +1
=
= ( ) +
= + ( )



=0
=0

=0



4. usual requirements convergence stochastic gradient algorithm > 0,
1 2 < .
t=0

1

322

=

()

P1t=0

=

+


1,

fiP OLICY-G RADIENT E STIMATION

algorithm straightforward normal stochastic gradient algorithms
updates r Xt zt gradient direction (in expectation), although sum updates
regenerative cycle are. Marbach Tsitsiklis (1998) provide convergence proof
know of, albeit slightly different update form t+1
r Xt ; zt ,
moving estimate expected performance, updated on-line (this
update first suggested context POMDPs Jaakkola et al. (1995)).
Marbach Tsitsiklis (1998) considered case -dependent rewards (recall discussion (3)), Baird Moore (1999) VAPS algorithm (Value Policy
Search). last paper contains interesting insight: suitable choices performance
function r X0 ; : : : ; XT ; , one combine policy-gradient search approximate value function methods. resulting algorithms viewed actor-critic techniques spirit Barto
et al. (1983); policy actor value function critic. primary motivation
reduce variance policy-gradient estimates. Experimental evidence phenomenon
presented number authors, including Barto et al. (1983), Kimura Kobayashi
(1998a), Baird Moore (1999). recent work subject includes Sutton
et al. (2000) Konda Tsitsiklis (2000). discuss use VAPS-style updates
Section 6.2.
far addressed question parameterized state-transition probabilities pXt Xt+1 arise. course, could simply generated parameterizing matrix
transition probabilities directly. Alternatively, case MDPs POMDPs, state transitions
typically generated feeding observation Yt depends stochastically state Xt
parameterized stochastic policy, selects control Ut random set available controls (approximate value-function based approaches generate controls stochastically
via form lookahead fall category). distribution successor states
pXt Xt+1 Ut fixed function control. denote probability control ut given
parameters observation yt ut ; yt , discussion carries
rpXtXt+1 =pXtXt+1 replaced rUt ; Yt =Ut ; Yt . case, Algorithm 1.1 precisely Williams REINFORCE algorithm.
Algorithm 1.1 variants extended cover multiple agents (Peshkin
et al., 2000), policies internal state (Meuleau et al., 1999), importance sampling methods
(Meuleau et al., 2000). refer reader work Rubinstein Shapiro (1993)
Rubinstein Melamed (1998) in-depth analysis application likelihood-ratio
method Discrete-Event Systems (DES), particular networks queues. worth mentioning
large literature Infinitesimal Perturbation Analysis (IPA), seeks similar goal estimating performance gradients, operates restrictive assumptions likelihoodratio approach; see, example, Ho Cao (1991).

( )

= + [(

^( )

(

) ^( )]

)

()

( )
()

(

()

1.1.2 B IASED E STIMATES



)
(

)

(

)

P ERFORMANCE G RADIENT

algorithms described previous section rely identifiable recurrent state , either
update gradient estimate, case on-line algorithm, zero eligibility trace
z . reliance recurrent state problematic two main reasons:
1. variance algorithms related recurrence time visits ,
typically grow state space grows. Furthermore, time visits depends
323

fiBAXTER & BARTLETT

parameters policy, states frequently visited initial value
parameters may become rare performance improves.
2. situations partial observability may difficult estimate underlying states,
therefore determine gradient estimate updated, eligibility trace
zeroed.
system available simulation, seems difficult (if impossible) obtain
unbiased estimates gradient direction without access recurrent state. Thus, solve 1
2, must look biased estimates. Two principle techniques introducing bias
proposed, may viewed artificial truncations eligibility trace z . first
method takes starting point formula5 eligibility trace time t:

zt =

1
X

rpXsXs+1 ()
pXs Xs+1 ()

s=0

simply truncates (fixed, random) number terms n looking backwards (Glynn,
1990; Rubinstein, 1991, 1992; Cao & Wan, 1998):

zt (n) :=

1
X
s=t n

rpXsXs+1 () :
pXs Xs+1 ()

(7)

(n) updated transition Xt ! Xt
rp
() rpXt nXt n+1 () ;
zt (n) = zt (n) + XtXt+1
pXt Xt+1 ()
pXt n Xt n+1 ()
case state-based rewards r (Xt ), estimated gradient direction steps

^rn() := 1 X zt (n)r(Xt ):
n+1 n
eligibility trace zt

+1

+1

(8)

(9)

=

Unless n exceeds maximum recurrence time (which infinite ergodic Markov chain),
rn biased estimate gradient direction, although n ! 1, bias approaches zero.
However variance rn diverges limit large n. illustrates natural trade-off
selection parameter n: large enough ensure bias acceptable (the
expectation rn least within true gradient direction), large
variance prohibitive. Experimental results Cao Wan (1998) illustrate nicely
bias/variance trade-off.
One potential difficulty method likelihood ratios rpXs Xs+1 =pXs Xs+1
must remembered previous n time steps, requiring storage Kn parameters. Thus,
obtain small bias, memory may grow without bound. alternative approach
requires fixed amount memory discount eligibility trace, rather truncating it:

^ ()

^ ()

^ ()

90

()

zt+1 (fi ) := fizt (fi ) +

rpXtXt+1 () ;
pXt Xt+1 ()

()

(10)

r

5. ease exposition, kept expression z terms likelihood ratios pXs Xs+1 ()=pXs Xs+1 ()
rely availability underlying state Xs . Xs available, pXs Xs+1 ()=pXs Xs+1 ()
replaced Us (; Ys )=Us (; Ys ).

r

r

324

fiP OLICY-G RADIENT E STIMATION

( )=0

z0

steps simply

2 [0; 1) discount factor.
r^fi () := T1

TX1
t=0

case estimated gradient direction

r(Xt )zt (fi ):

(11)

( ) ()
(( ) )

precisely estimate analyze present paper. similar estimate r Xt zt
replaced r Xt
b zt b reward baseline proposed Kimura et al. (1995,
1997) continuous control Kimura Kobayashi (1998b). fact use r Xt
b
place r Xt affect expectation estimates algorithm (although judicious choice reward baseline b reduce variance estimates). algorithm
presented Kimura et al. (1995) provides estimates expectation stationary distribution gradient discounted reward, show fact biased estimates
gradient expected discounted reward. arises stationary distribution
depends parameters. similar estimate (11) proposed Marbach
Tsitsiklis (1998), time r Xt zt replaced r Xt
zt ,
estimate average reward, zt zeroed visits identifiable recurrent state.
final note, observe eligibility traces zt zt n defined (10) (8)
simply filtered versions sequence rpXt Xt+1 =pXt Xt+1 , first-order, infinite impulse
response filter case zt n-th order, finite impulse response filter case
zt n . raises question, addressed paper, whether interesting theory
optimal filtering policy-gradient estimators.

(( )
( )

) ()

( ) ()

( ( ) ^( )) ( )

()

()

()

()

^( )

()
()

1.2 Contribution
describe GPOMDP, general algorithm based upon (11) generating biased estimate
performance gradient r general POMDPs controlled parameterized stochastic policies.
denotes average reward policy parameters 2 RK . GPOMDP
rely access underlying recurrent state. Writing rfi expectation estimate produced GPOMDP, show
r , quantitatively
!1 rfi
rfi close true gradient provided = exceeds mixing time Markov chain
induced POMDP6 . truncated estimate above, trade-off preventing setting
arbitrarily close variance algorithms estimates increase approaches
. prove convergence probability 1 GPOMDP discrete continuous observation control spaces. present algorithms general parameterized Markov chains
POMDPs controlled parameterized stochastic policies.
several extensions GPOMDP investigated since first version
paper written. outline developments briefly Section 7.
companion paper show gradient estimates produced GPOMDP used
perform gradient ascent average reward (Baxter et al., 2001). describe
traditional stochastic gradient algorithms, conjugate-gradient algorithm utilizes gradient
estimates novel way perform line searches. Experimental results presented illustrat-

()

()

()
lim
( )= ( )
1 (1 )

()

1

1

()

6. mixing-time result paper applies Markov chains distinct eigenvalues. Better estimates
bias variance GPOMDP may found Bartlett Baxter (2001), general Markov chains
treated here, refined notions mixing time. Roughly speaking, variance GPOMDP
grows 1=(1 ), bias decreases function 1=(1 ).

325

fiBAXTER & BARTLETT

ing theoretical results present paper toy problem, practical aspects
algorithms number realistic problems.

2. Reinforcement Learning Problem
model reinforcement learning Markov decision process (MDP) finite state space
f ; : : : ; ng, stochastic matrix7 P pij giving probability transition state
state j . state associated reward8 r . matrix P belongs parameterized
class stochastic matrices, P
fP 2 RK g. Denote Markov chain corresponding
P . assume Markov chains rewards satisfy following assumptions:

= 1

()

=[ ]

()

:= ( ):

()

()

Assumption 1. P 2 P unique stationary distribution
satisfying balance equations

() := [(; 1); : : : ; (; n)]0

0 ()P () = 0 ()
(throughout 0 denotes transpose ).
Assumption 2. magnitudes rewards, jr (i)j, uniformly bounded R <
states i.

(12)

1

Assumption 1 ensures Markov chain forms single recurrent class parameters .
Since finite-state Markov chain always ends recurrent class, properties
class determine long-term average reward, assumption mainly convenience
include recurrence class quantifier theorems. However,
consider gradient-ascent algorithms Baxter et al. (2001), assumption becomes
restrictive since guarantees recurrence class cannot change parameters adjusted.
Ordinarily, discussion MDPs would complete without mention actions
available state space policies available learner. particular, parameters
would usually determine policy (either directly indirectly via value function), would
determine transition probabilities P . However, purposes care
dependence P arises, satisfies Assumption 1 (and differentiability
assumptions shall meet next section). Note easy extend setup
case rewards depend parameters transitions ! j .
equally straightforward extend algorithms results cases. See Section 6.1
illustration.
goal find 2 R K maximizing average reward:

"
#
TX1



E
r Xt X0 ;

!1
t=0
E denotes expectation sequences X0 ; X1 ; : : : ; transitions generated according P . Assumption 1, independent starting state equal
n
X

; r 0 r;
(13)
i=1

()

1

( ) := lim

()

=

()

( )=

r

( )

( ) ()= ( )

= [r(1); : : : ; r(n)]0 (Bertsekas, 1995).


P

7. stochastic matrix P = [pij ] pij 0 i; j n
j =1 pij = 1 i.
8. results present paper apply bounded stochastic rewards, case r(i) expectation
reward state i.

326

fiP OLICY-G RADIENT E STIMATION

3. Computing Gradient Average Reward

()

general MDPs little known average reward , hence finding optimum
problematic. However, section see general assumptions gradient
r exists, local optimization possible.
ensure existence suitable gradients (and boundedness certain random variables),
require parameterized class stochastic matrices satisfies following additional assumption.

()

()

Assumption 3. derivatives,

rP () :=
2 RK . ratios

exist



@pij ()
@k

i;j =1:::n;k=1:::K

2 fifi @p () fifi 3
ij
@k
4
5

pij ()

uniformly bounded B



i;j =1:::n;k=1:::K

< 1 2 RK .

second part assumption allows zero-probability transitions pij

() = 0

rpij () zero, case set 0=0 = 0. One example ! j forbidden
transition, pij ( ) = 0 2 RK . Another example satisfying assumption
pij () =


= [

11

; : : : ; 1n ; : : : ; nn ] 2 Rn2

parameters P

@pij ()=@ij
pij ()
@pij ()=@kl
pij ()

Assuming moment r
dependencies,

eij
ij ;
j =1 e

Pn

=1
=

pij ();

(),



pkl ():

() exists (this justified shortly), then, suppressing
r = r0r;
(14)

since reward r depend . Note convention r paper takes
precedence operations, rg f
rg f . Equations (14)
regarded shorthand notation K equations form

( ) ( ) = [ ( )] ( )

@()
@k

k

=





@(; 1)
@(; n)
;:::;
[r(1); : : : ; r(n)]0
@k
@k

= 1; : : : ; K . compute r, first differentiate balance equations (12) obtain
r0P + 0 rP = r0;
327

fiBAXTER & BARTLETT

hence

r0(I P ) = 0 rP:

(15)

system equations defined (15) under-constrained P invertible (the
balance equations show P left eigenvector zero eigenvalue). However, let e
denote n-dimensional column vector consisting s, e 0 n n matrix
stationary distribution 0 row. Since r 0 e r 0 e
r
, rewrite (15)

1

= ( ) = (1) = 0





r0 (P e0) = 0rP:
see inverse
write

[I (P
"

lim (I

e0 )]
A)

!1


X
t=0

1

exists, let matrix satisfying
#

= Tlim
!1



[

t=0

= Tlim
!1
= I:

Thus,

(I

"
X

A)

1

=

1
X
t=0

TX
+1



+1

t=1

limt!1 = 0.

#



:

] =

0

easy prove induction P e 0
P eP0
! 1
tconverges
1
0
0 . Hence, write
Assumption 1.
P e
exists equal 1
P
e
t=0

[

(

)]



r0 = 0rP P + e0
so9



r = 0rP P + e0





;

(16)

r:

(17)

1

1

MDPs sufficiently small number states, (17) could solved exactly yield precise
gradient direction. However, general, state space small enough exact solution
(17) possible, small enough derive optimal policy using policy iteration
table-lookup, would point pursuing gradient based approach first place10 .
Thus, problems practical interest, (17) intractable need find
way computing gradient. One approximate technique presented
next section.
9. argument leading (16) coupled fact () unique solution (12) used justify
existence . Specifically, run steps computing value ( + ) small
show expression (16) unique matrix satisfying ( + ) = () + () + O( 2 ).
10. Equation (17) may still useful POMDPs, since case tractable dynamic programming
algorithm.

r

r

r

328

kk

fiP OLICY-G RADIENT E STIMATION

4. Approximating Gradient Parameterized Markov Chains
section, show gradient split two components, one becomes
negligible discount factor approaches .
2 ; , let Jfi
Jfi ; ; : : : ; Jfi ; n denote vector expected discounted
rewards state i:

[0 1)

( ) = [ ( 1)

Jfi (; i) := E

1

"

( )]

1
X
t=0

fitr



Xt fifi X0


( )

#

=i

:

(18)

dependence obvious, write Jfi .

2 [0; 1),
r = (1 )r0 Jfi + fi0rP Jfi :

Proposition 1. 2 R K

(19)

Proof. Observe Jfi satisfies Bellman equations:

Jfi = r + fiP Jfi :

(20)

(Bertsekas, 1995). Hence,

r = r0r
= r0 [Jfi fiP Jfi ]
= r0Jfi r0Jfi + fi0 rP Jfi
= (1 )r0 Jfi + fi0rP Jfi :

(15)

shall see next section second term (19) estimated single sample path Markov chain. fact, Theorem 1 (Kimura et al., 1997) shows gradient
estimates algorithm presented paper converge
0 rJfi . Bellman equations (20), equal
0 rP Jfi 0 rJfi , implies
0 rJfi fi0 rP Jfi .
Thus algorithm Kimura et al. (1997) estimates second term expression
r given (19). important note 0rJfi 6 r 0 Jfi two quantities disagree
first term (19). arises stationary distribution depends
parameters. Hence, algorithm Kimura et al. (1997) estimate gradient expected discounted reward. fact, expected discounted reward simply =
times
average reward (Singh et al., 1994, Fact 7), gradient expected discounted reward
proportional gradient average reward.
following theorem shows first term (19) becomes negligible approaches .
Notice immediate Proposition 1, since Jfi become arbitrarily large
limit ! .

(1 ) (

+

(1

)

= [

]

)

()

(1 )

=

1 (1

()

)

1

1

Theorem 2. 2 RK ,

r = filim
r ;
!

(21)

rfi := 0 rP Jfi :

(22)

1



329

fiBAXTER & BARTLETT

Proof. Recalling equation (17) discussion preceeding it, 11

r = 0rP
rP e

1
X



e0 r:

Pt

t=0

(23)

= r(P e) = r(1) = 0 since P stochastic matrix, (23) rewritten
r = 0

"
1
X

#

rP P r:

t=0

(24)

2 [0; 1] discount factor consider expression

let

f (fi ) := 0

= lim
( )=

()

"
1
X

t=0

#

rP (fiP )t r

(25)

( )=

Clearly r
rfi .
!1 f . complete proof need show f




0
Since fiP
P ! e ! , invoke observation (16) write

0

1
X
t=0
P

particular, 1
t=0
(25) write12

(fiP )t = [I

(fiP )t converges, take rP back sum right-hand-side
f (fi ) = 0 rP



P1

t=0


fitP r

fiP ] 1 :

= Jfi . Thus f (fi ) = 0rP Jfi

"1
X

t=0

#

fitP

r:

(26)

= rfi .

1

Theorem 2 shows rfi good approximation gradient approaches ,
turns values close lead large variance estimates rfi
describe next section. However, following theorem shows
need
small, provided transition probability matrix P distinct eigenvalues, Markov
chain short mixing time. initial state, distribution states Markov chain
converges stationary distribution, provided assumption (Assumption 1) existence
uniqueness stationary distribution satisfied (see, example, Lancaster & Tismenetsky,
1985, Theorem 15.8.1, p. 552). spectral resolution theorem (Lancaster & Tismenetsky, 1985,
Theorem 9.5.1, p. 314) implies distribution converges stationarity exponential rate,
time constant convergence rate (the mixing time) depends eigenvalues
transition probability matrix. existence unique stationary distribution implies

1

1

()

11. Since e 0 r = e , (23) motivates different kind algorithm estimating based differential rewards
(Marbach & Tsitsiklis, 1998).
12. cannot back P sum right-hand-side (24) 1
P diverges (P e 0 ). reason
1 P P converges P becomes orthogonal P limit tof=0large t. Thus, view 1 P
t=0
t=0
sum two orthogonal components: infinite one direction e finite one direction e? .
1


finite component need estimate. Approximating 1
t=0 P t=0 (fiP ) way rendering
e-component finite hopefully altering e? -component much. substitutions
lead better approximations (in context, see final paragraph Section 1.1).

P

r

r

r

330

P

P

r

!

P

P

fiP OLICY-G RADIENT E STIMATION

1

1

largest magnitude eigenvalue multiplicity , corresponding left eigenvector
stationary distribution. sort eigenvalues decreasing order magnitude,
1 > j2 j > > js j n. turns j2 j determines mixing time
chain.
following theorem shows
small compared
j2j, gradient approximation described accurate. Since using estimate direction
update parameters, theorem compares directions gradient estimate.
theorem, 2 denotes spectral condition number nonsingular matrix A, defined
product spectral norms matrices 1 ,

1=

2

1

1

( )

2 (A) = kAk2 kA



1

k;
2

kAk = x max
kAxk;
kxk
2

:

=1

kxk denotes Euclidean norm vector x.

()

Theorem 3. Suppose transition probability matrix P satisfies Assumption 1 stationary distribution 0
1 ; : : : ; n , n distinct eigenvalues. Let
x1 x2 xn
matrix right eigenvectors P corresponding, order, eigenvalues
1 > j2 j
jn j. normalized inner product r rfi satisfies

=(

)

=(
1=

)


kr(p ; : : : ; p )k p
rfi
1
n
1 rkr

=S
r0r
(27)
k
krk
1 j j ;
= diag( ; : : : ; n ).
Notice r 0 r expectation stationary distribution r (X ) .
well mixing time (via j j), bound theorem depends another parameter
Markov chain: spectral condition number = . Markov chain reversible (which
2

1 2

2

1

2

1

2

2

1 2

implies eigenvectors x1 ; : : : ; xn orthogonal), equal ratio maximum
minimum probability states stationary distribution. However, eigenvectors
need nearly orthogonal. fact, condition transition probability matrix
n distinct eigenvalues necessary; without it, condition number replaced
complicated expression involving spectral norms matrices form P .

(

)



Proof. existence n distinct eigenvalues implies P expressed 1 ,
1 ; : : : ; n (Lancaster & Tismenetsky, 1985, Theorem 4.10.2, p 153). follows
polynomial f , write f P
Sf 1 .
Now, Proposition 1 shows r rfi r 0
Jfi .

= diag(

)

( ) = ()
= (1 )
(1 )Jfi = (1 ) r + fiP r + P r +
= (1 ) + fiP + fi!P + r
1
X
= (1 )S
r
2

2

2

2

1

= (1

fi)

n
X
j =1

t=0

xj 0

331

j

1
X
t=0

(fij )

!



r;

fiBAXTER & BARTLETT

=(
=

)

1
y1 ; : : : ; yn 0 .
easy verify yi left eigenvector corresponding , choose
y1 x1 e. Thus write

=

(1

)Jfi = (1 )e0 r +

n
X

xj yj0

(1

!

)(fij )t r

t=0
j =2


n
X

r
xj yj0

j
j =2

= (1

)e +

= (1

)e + SMS 1 r;


1
1



1
= diag 0;
1



1
X


1 :
;:::;
fi2
1 fin

follows Proposition 1

rfi
r (r r0(1
1 rkr
=
1
k
krk
0
= r r (1 )Jfi
2

2

)Jfi )

krk

r
r0 (1 )e + SMS r
=
krk
0
SMS r
= r rkr
k

r 0 SMS r

krk ;
p
0
Since r = r
0 = , apply Cauchy2

1

2

1

2

1

Cauchy-Schwartz inequality.
Schwartz inequality obtain

1 2

rfi
1 rkr

k





r



p

0





= SMS
1 2

1

krk

2



r

:

(28)

use spectral norms bound second factor numerator. clear definition
spectral norm product nonsingular matrices satisfies kAB k2 kAk2 kB k2 ,
spectral norm diagonal matrix given k
d1 ; : : : ; dn k2
jdi j. follows




= SMS
1 2

1

diag(
) = max


r = = SMS = = r




= = = r kM k

p
= r0r 1 1 jfi j :
1 2

1

1 2

1

2

2

1 2

1 2

1 2

1 2

2

1 2

2

Combining Equation (28) proves (27).
332

2

fiP OLICY-G RADIENT E STIMATION

5. Estimating Gradient Parameterized Markov Chains
Algorithm 1 introduces MCG (Markov Chain Gradient), algorithm estimating approximate gradient rfi single on-line sample path X0 ; X1 ; : : : Markov chain .
MCG requires K reals stored, K dimension parameter space: K
parameters eligibility trace zt , K parameters gradient estimate . Note
time steps average far r Xt zt ,

()

2





( )

TX
1
=
zt r(Xt ):
1



t=0

Algorithm 1 MCG (Markov Chain Gradient) algorithm
1: Given:




Parameter 2 R K .
Parameterized class stochastic matrices P
3 1.

= fP (): 2 RK g satisfying Assumptions

2 [0; 1).
Arbitrary starting state X .
State sequence X ; X ; : : :
0

generated ( ) (i.e. Markov chain transition
()).
Reward sequence r(X ); r(X ); : : : satisfying Assumption 2.
Set z = 0 = 0 (z ; 2 RK ).
state Xt visited
rp
( )
zt = fizt + XtXt+1
pXt Xt+1 ()
= + [r(Xt )zt ]
probabilities P

0

1

0

2:
3:
4:
5:
6:

0

0

0

1

0

+1

+1

1
+1

+1

+1

+1

end

Theorem 4. Assumptions 1, 2 3, MCG algorithm starting initial state X0
generate sequence 0 ; 1 ; : : : ; ; : : : satisfying





lim = rfi

t!1

=

w.p.1:

(29)

()

Proof. Let fXt g fX0 ; X1 ; : : : g denote random process corresponding . X0
entire process stationary. proof easily generalized arbitrary initial distributions using fact Assumption 1, fXt g asymptotically stationary. fXt g
333

fiBAXTER & BARTLETT

stationary, write

0 rP Jfi =

X

=

X

=

X

i;j
i;j
i;j

(i)rpij ()Jfi (j )
(i)pij ()

rpij () J (j )
p ()
ij

Pr(Xt = i)Pr(Xt = j jXt = i) rppij(()) E(J (t + 1)jXt = j );
+1

+1

ij

first probability respect stationary distribution

J (t + 1) =

( ( + 1)

)= (

1
X
s=t+1

fis



1

(30)

J (t + 1) process

r(Xs ):

)

fact E J
jXt+1 Jfi Xt+1 Xt+1 follows boundedness
magnitudes rewards (Assumption 2) Lebesgues dominated convergence theorem.
rewrite Equation (30)




X
rp ()
0 rP Jfi = E (Xt )j (Xt+1 ) ij J (t + 1) ;

pij ()

i;j



() denotes indicator function state i,
(
1 Xt = i;
(Xt ) :=
0 otherwise;

expectation respect stationary distribution. Xt chosen according
stationary distribution, process fXt g ergodic. Since process fZt g defined

Zt := (Xt )j (Xt+1 )

rpij () J (t + 1)
pij ()

obtained taking fixed
fXt g, fZt g stationary ergodic (Breiman, 1966,
function

rpij ()
Proposition 6.31). Since pij () bounded Assumption 3, ergodic theorem
(almost surely):

0 rP Jfi

TX
1
rp ()
= Tlim
(Xt )j (Xt ) ij J (t + 1)
!1
pij ()
i;j
TX
rpXtXt+1 () J (t + 1)
1
= Tlim
!1
pXtXt+1 ()
"
TX
1
X
r
pXtXt+1 () X
1
= Tlim
r(Xs ) +
!1
pXtXt+1 ()

X

1

+1

=0

1

=0

1

1

=0

= +1

334

= +1

#

fis

1

r(Xs ) :

(31)

fiP OLICY-G RADIENT E STIMATION

Concentrating second term right-hand-side (31), observe that:

TX1


fiT

1

t=0

rpXtXt+1 ()
pXt Xt+1 ()

1
X

fis

s=T +1
TX1 fifi

1





1



Xs fifi


r(

)

pXt Xt+1 ()

t=0
1
BR TX1 X



= BR


1
X

rpXtXt+1 () fififi

t=0 s=T +1
TX1



t=0

1

fis





s=T +1

fis



1

jr(Xs)j

1



1 fiT
= BRfi
(1 )2
! 0 ! 1;



jrp j
R B bounds magnitudes rewards pijij Assumptions 2
3. Hence,
TX1

rpXtXt+1 X
0 rP Jfi
(32)
1 r Xs :
!1
p

X
X
t+1
t=0
s=t+1
Unrolling equation MCG algorithm shows equal

()
()

= lim 1



1 TX rpXtXt+1 () X
pXt Xt+1 ()
1

=0

hence

fis

( )



1

r(is );

= +1

! 0rP Jfi w.p.1 required.

6. Estimating Gradient Partially Observable Markov Decision Processes

()

Algorithm 1 applies parameterized class stochastic matrices P compute gradients rpij . section consider special case P arise
parameterized class randomized policies controlling partially observable Markov decision process (POMDP). partially observable qualification means assume policies
access observation process depends state, general may see state.
Specifically, assume N controls U
f ; : : : ; N g observations
f ; : : : ; g. u 2 U determines stochastic matrix P u depend
parameters . state 2 , observation 2 generated independently according
probability distribution observations . denote probability observation
. randomized policy simply function mapping observations 2 probability
distributions controls U . is, observation , distribution
controls U . Denote probability control u given observation u .
randomized policy observation distribution corresponds Markov
chain state transitions generated first selecting observation state according

()

= 1

1

()

()

=

()

()

()
()

()

335

()

fiBAXTER & BARTLETT

()

()

distribution , selecting control u according distribution , generating transition state j according probability pij u . parameterize chains
parameterize policies, becomes function ; set parameters 2 R K
well observation . Markov chain corresponding state transition matrix pij
given

()
( )

[ ( )]

pij () = EY (i) EU (;Y ) pij (U ):

Equation (33) implies

rpij () =

X

(33)

(i)pij (u)ru (; y):

u;y

(34)

Algorithm 2 introduces GPOMDP algorithm (for Gradient Partially Observable Markov
Decision Process), modified form Algorithm 1 updates zt based Ut ; Yt ,
rather pXt Xt+1 . Note Algorithm 2 require knowledge transition probability matrix P , observation process ; requires knowledge randomized
policy . GPOMDP essentially algorithm proposed Kimura et al. (1997) without
reward baseline.
algorithm GPOMDP assumes policy function current observation.
immediate algorithm works finite history observations. general,
optimal policy needs function entire observation history. GPOMDP extended
apply policies internal state (Aberdeen & Baxter, 2001).

(

()

)

Algorithm 2 GPOMDP algorithm.
1: Given:




Parameterized class randomized policies



(; ) : 2 RK



satisfying Assumption 4.

Partially observable Markov decision process controlled randomized
policies ; corresponds parameterized class Markov chains satisfying Assumption 1.

( )

2 [0; 1).
Arbitrary (unknown) starting state X .
Observation sequence ; ; : : : generated POMDP controls U ; U ; : : :
0

0

1

0

(; Yt).

generated randomly according


2:
3:
4:
5:
6:

( ) ( )

Reward sequence r X0 ; r X1 ; : : : satisfying Assumption 2,
(hidden) sequence states Markov decision process.

=0

=0
= + (( ))
= + [ ( )

Set z0
0
(z0 ; 0 2 RK ).
observation Yt , control Ut , subsequent reward r
rUt ; Yt
zt+1 fizt
Ut ; Yt
1
t+1
t+1 r Xt+1 zt+1

end

]

336

(Xt )
+1

X0 ; X1 ; : : :

1



fiP OLICY-G RADIENT E STIMATION

convergence Algorithm 2 need replace Assumption 3 similar bound
gradient :
Assumption 4. derivatives,

exist u 2 U ,

@u (; y)
@k

2 2 RK . ratios

2 fifi
@u (;y) 3
@

k
4
5

u (; y)

uniformly bounded B

y=1:::M ;u=1:::N ;k=1:::K

< 1 2 RK .

Theorem 5. Assumptions 1, 2 4, Algorithm 2 starting initial state
generate sequence 0 ; 1 ; : : : ; ; : : : satisfying





lim = rfi

w.p.1:

t!1

X0



(35)

Proof. proof follows lines proof Theorem 4. case,

0 rP Jfi =

=
=
=

X

i;j

(i)rpij ()Jfi (j )

X

i;j;y;u
X

i;j;y;u
X

i;j;y;u

(i)pij (u)y (i)ru (; y)Jfi (j ) (34)
(i)pij (u)y (i)

ru (; y) (; y)J (j );

(; y) u
u

EZt0;

expectation respect stationary distribution fXt g, process fZt0 g
defined
ru ; J ;
Zt0 Xt j Xt+1 u Ut Yt
u ;

:= ( ) (

) ( ) ( ) (( )) ( + 1)

Ut control process Yt observation process. result follows
arguments used proof Theorem 4.
6.1 Control dependent rewards

many circumstances rewards may depend controls u.
example, controls may consume energy others may wish add penalty
term reward function order conserve energy. simplest way deal
define state expected reward r

( )

r(i) = EY (i) EU (;Y ) r(U; i);
337

(36)

fiBAXTER & BARTLETT



redefine Jfi terms r:

Jfi (; i) :=

lim E

"

N !1

N
X
t=0



Xt fifi X0


( )

fitr

#

=i

;

(37)

X0 ; X1 ; : : : . performance gradient becomes
r = r0r + 0rr;

expectation trajectories

approximated

rfi = 0 rP Jfi + rr ;








due fact Jfi satisfies Bellman equations (20) r replaced r .
GPOMDP take account dependence r controls, fifth line
replaced

1
= + + 1 r(Ut
+1



r
Ut+1 (; Yt )
; Xt ) zt +
:
(; )


+1

+1

+1

+1

Ut+1
t+1
straightforward extend proofs Theorems 2, 3 5 setting.

6.2 Parameter dependent rewards
possible modify GPOMDP rewards depend directly . case,
fifth line GPOMDP replaced

= + +1 1 [r(; Xt )zt + rr(; Xt ) t] :
+1

+1

+1

(38)

+1

( )

Again, convergence approximation theorems carry through, provided rr ; uniformly bounded. Parameter-dependent rewards considered Glynn (1990), Marbach
Tsitsiklis (1998), Baird Moore (1999). particular, Baird Moore (1999) showed
suitable choices r ; lead combination value policy search, VAPS.
example, J ; approximate value-function, setting13

( )

~( )

h

1
~
~
r(; Xt ; Xt ) =
2 r(Xt ) + ffJ (; Xt ) J (; Xt ) ;
r (Xt ) usual reward 2 [0; 1) discount factor, gives update seeks
2

1

1

minimize expected Bellman error
n
X
i=1

2

(; i) 4r(i) +

n
X
j =1

32

pij ()J~(; j ) J~(; i)5 :

(39)

~( )

effect minimizing Bellman error J ; , driving system
(via policy) states small Bellman error. motivation behind approach
understood one considers J zero Bellman error states. case greedy
policy derived J optimal, regardless actual policy parameterized,
expectation zt r ; Xt ; Xt 1 zero gradient computed GPOMDP.
kind update known actor-critic algorithm (Barto et al., 1983), policy playing
role actor, value function playing role critic.

(

~

13. use rewards r(; Xt ; Xt
analysis.

~

)

1 ) depend current previous

338

state substantially alter

fiP OLICY-G RADIENT E STIMATION

6.3 Extensions infinite state, observation, control spaces
convergence proof Algorithm 2 relied finite state (S ), observation (Y ) control (U )
spaces. However, clear modification Algorithm 2 applied immediately POMDPs countably uncountably infinite , countable U .
changes pij u becomes kernel p x; x0 ; u becomes density observations.
addition, appropriate interpretation r=, applied uncountable U . Specifically, U subset R N y; probability density function U u y;
density u. U subsets Euclidean space (but finite set), Theorem 5
extended show estimates produced algorithm converge almost surely rfi .
fact, prove general result implies case densities subsets R N
well finite case Theorem 5. allow U general spaces satisfying following
topological assumption. (For definitions see, example, (Dudley, 1989).)

()

(

)

()

( )

( )

Assumption 5. control space U associated topology separable, Hausdorff,
first-countable. corresponding Borel -algebra B generated topology,
-finite measure defined measurable space U ; B . say reference measure
U .
Similarly, observation space topology, Borel -algebra, reference measure
satisfying conditions.

(

)

case Theorem 5, U finite, associated reference measure
counting measure. U
RN RM , reference measure Lebesgue measure.
assume distributions ; absolutely continuous respect reference
measures, corresponding Radon-Nikodym derivatives (probability masses finite case,
densities Euclidean case) satisfy following assumption.

=

()

=

( )

( )

Assumption 6. every 2 2 R K , probability measure ; absolutely continuous respect reference measure U . every 2 , probability measure
absolutely continuous respect reference measure .
Let reference measure U . u 2 U , 2 , 2 R K , k 2 f ; : : : ; K g,
derivatives

@ d(; y)
(u)
@k


@ du (;y)
@k

exist ratios

< 1.

1



(u)fifi
du ;y
(u)
(

bounded B

()

)

assumptions, replace Algorithm 2 Radon-Nikodym derivative
respect reference measure U . case, following convergence
result. generalizes Theorem 5, applies densities Euclidean space U .

Theorem 6. Suppose control space U observation space satisfy Assumption 5 let

reference measure control space U . Consider Algorithm 2
rUt (; Yt)
Ut (; Yt )
339

fiBAXTER & BARTLETT

replaced

r d;Yt (Ut ) :
(

)

( )

d(;Yt )
Ut

Assumptions 1, 2 6, algorithm, starting initial state
sequence 0 ; 1 ; : : : ; ; : : : satisfying





lim = rfi

t!1

X0

generate

w.p.1:

Proof. See Appendix B

7. New Results
Since first version paper, extended GPOMDP several new settings,
proved new properties algorithm. section briefly outline results.
7.1 Multiple Agents

Instead single agent generating actions according (; ), suppose multiple agents
= 1; : : : ; na , parameter set distinct observation environment
yi , generate actions ui according policy ui (i ; yi ). agents receive reward signal r (Xt ) (they may cooperating solve task, example),

GPOMDP applied collective POMDP obtained concatenating
1 nobserva
1
na , u
tions, controls,
parameters

single
vectors


;
:
:
:
;

u ; : : : ; u ,


1 ; : : : ; na respectively. easy calculation shows gradient estimate generated
GPOMDP collective case precisely obtained applying GPOMDP


1
agent independently, concatenating results. is,
; : : : ; na ,
estimate produced GPOMDP applied agent i. leads on-line algorithm
agents adjust parameters independently without explicit communication,
yet collectively adjustments maximizing global average reward. similar observations context REINFORCE VAPS, see Peshkin et al. (2000). algorithm gives
biologically plausible synaptic weight-update rule applied networks spiking neurons
neurons regarded independent agents (Bartlett & Baxter, 1999), shown
promise network routing application (Tao, Baxter, & Weaver, 2001).

=

=

=

=






7.2 Policies internal states
far considered purely reactive memoryless policies chosen control
function current observation. GPOMDP easily extended cover case
policies depend finite histories observations Yt ; Yt 1 ; : : : ; Yt k , general, optimal
control POMDPs, policy must function entire observation history. Fortunately,
observation history may summarized form belief state (the current distribution
states), updated based upon current observation, knowledge
sufficient optimal behaviour (Smallwood & Sondik, 1973; Sondik, 1978). extension
GPOMDP policies parameterized internal belief states described Aberdeen Baxter
(2001), similar spirit extension VAPS REINFORCE described Meuleau et al.
(1999).
340

fiP OLICY-G RADIENT E STIMATION

7.3 Higher-Order Derivatives
generalized compute estimates second higher-order derivatives
average reward (assuming exist), still single sample Rpath underlying POMDP.
see second-order derivatives, observe
q ; x r x dx twicedifferentiable density q ; x performance measure r x ,

GPOMDP

()= ( )( )
()
Z
r () = r(x) rq(q;(;x)x) q(; x) dx

( )

2

2

r2 denotes matrix second derivatives (Hessian). verified

r q(; x) = r log q(; x) + [r log q(; x)]
q(; x)
2

2

2

(40)

log ( )

second term right-hand-side outer product r
q ; x
(that is, matrix entries @=@i
q ; x @=@j q ; x ). Taking x sequence
states X0 ; X1 ; : : : ; XT visits recurrent state parameterized Markov chain (recall
1p
Section 1.1.1), q ; X
t=0 Xt Xt+1 , combined (40) yields

log ( )
log ( )
)=
()

(

r q(; X ) = TX r pXtXt+1 ()
2

q(; X )

1

TX1

2

pXt Xt+1 ()

t=0

rpXtXt+1 () +
p
()
2

Xt Xt+1

t=0

"T 1
X

rpXtXt+1 ()

#2

pXtXt+1 ()

t=0

(the squared terms expression outer products). expression derive
GPOMDP-like algorithm computing biased estimate Hessian r2 , involves
maintainingin addition usual eligibility trace zt second matrix trace updated follows:

()

Zt+1

= fiZt + rp pXtXt+1(())
2



Xt Xt+1

rpXtXt+1 () :
p
( )
2

Xt Xt+1

( ) +



time steps algorithm returns average far r Xt Zt zt2 second term
outer product. Computation higher-order derivatives could used second-order
gradient methods optimization policy parameters.
7.4 Bias Variance Bounds

()

()

Theorem 3 provides bound bias rfi relative r applies underlying Markov chain distinct eigenvalues. extended result arbitrary Markov chains
(Bartlett & Baxter, 2001). However, extra generality comes price, since latter bound involves number states chain, whereas Theorem 3 not. paper supplies
proof variance GPOMDP scales =
2 , providing formal justification
interpretation terms bias/variance trade-off.

1 (1

)

8. Conclusion
presented general algorithm (MCG) computing arbitrarily accurate approximations
gradient average reward parameterized Markov chain. chains transition
matrix distinct eigenvalues, accuracy approximation shown controlled
341

fiBAXTER & BARTLETT

size subdominant eigenvalue j2 j. showed algorithm could modified apply
partially observable Markov decision processes controlled parameterized stochastic policies,
discrete continuous control, observation state spaces (GPOMDP). finite
state case, proved convergence probability 1 algorithms.
briefly described extensions multi-agent problems, policies internal state, estimating
higher-order derivatives, generalizations bias result chains non-distinct eigenvalues,
new variance result. many avenues research. Continuous time results
follow extensions results presented here. MCG GPOMDP algorithms
applied countably uncountably infinite state spaces; convergence results needed
cases.
companion paper (Baxter et al., 2001), present experimental results showing rapid
convergence estimates generated GPOMDP true gradient r . give on-line
variants algorithms present paper, variants gradient ascent make use
estimates rfi . present experimental results showing effectiveness algorithms
variety problems, including three-state MDP, nonlinear physical control problem,
call-admission problem.
Acknowledgements
work supported Australian Research Council, benefited comments
several anonymous referees. research performed authors
Research School Information Sciences Engineering, Australian National University.

Appendix A. Simple Example Policy Degradation Value-Function Learning
Approximate value-function approaches reinforcement work minimizing form error
approximate value function true value function. long known
may necessarily lead improved policy performance new value function. include
appendix illustrates phenomenon occur simplest possible system,
two-state MDP, provides geometric intuition phenomenon arises.
Consider two-state Markov decision process (MDP) Figure 1. two controls
u1 ; u2 corresponding transition probability matrices

P (u1 ) =

1
3
1
3

2

2
3
2
3



; P (u2 ) =

2

3
2
3

23

1
3
1
3



;

u1 always takes system state probability = , regardless starting state (and
therefore state probability = ), u2 opposite. Since state reward ,
state reward , optimal policy always select action u1 . policy
stationary distribution states 1 ; 2
= ; = , infinite-horizon discounted
value state
; discount value 2 ;

1

1

0

=1 2

13
[

Jff (i) = E

] = [1 3 2 3]
[0 1)

1
X

fft r



Xt fifi X0


( )

=i

2

1

!

;

t=0
expectation state sequences X0 ; X1 ; X2 ; : : : state transitions generated according P u1 . Solving Bellmans equations: Jff r ffP u1 Jff , Jff
Jff ; Jff 0
2ff
2ff
r
r ; r 0 yields Jff
Jff
.
3(1 ff)
3(1 ff)

( )
= [ (1) (2)]

= + ( )
(2) = 1 +

(1) =

342

= [ (1) (2)]

fiP OLICY-G RADIENT E STIMATION

r(1) = 0

r(2) = 1

1

2

Figure 1: Two-state Markov Decsision Process

~

~( ) =

Now, suppose trying learn approximate value function J MDP, i.e. , J
w state ; scalar feature ( must dimensionality ensure
J really approximate). w 2 R parameter learnt. greedy policy obtained
J optimal, J must value state state . purposes illustration choose

;
, J
> J , w must negative.
Temporal Difference learning (or
) one popular techniques training
approximate value functions (Sutton & Barto, 1998). shown linear functions,
converges parameter w minimizing expected squared loss stationary
distribution (Tsitsikilis & Van-Roy, 1997):

()
=1 2
~
~
2
~(2) ~(1)
(1) = 2 (2) = 1
TD( )
TD(1)
~

w = argmin

w

1

1

2
X

i=1

[w(i) Jff (i)]2 :

(41)

Substituting previous expressions 1 ; 2 ; Jff optimal policy solving
3+ff
w , yields w
. Hence w > values 2 ; , wrong
9(1 ff)
sign. situation optimal policy implementable greedy policy based
approximate value function class (just choose w < ), yet
observing
optimal policy converge value function whose corresponding greedy policy implements
suboptimal policy.
geometrical illustration occurs shown Figure 2. figure, points
graph
represent
p
p values states. scales state 1 state 2 axes weighted
respectively. way, squared euclidean distance graph
two points J J corresponds expectation stationary distribution squared
difference values:

=

0

[0 1)

0

(1)

hp




TD(1)

(2)
~

(1)J (1);

p

(2)J (2)



hp

(1)J~(1); (2)J~(2)
p

2





2

= E J (X ) J~(X )

:

value function shaded region, corresponding greedy policy optimal, since
value functions rank state 2 state 1. bold line represents set realizable
approximate value functions w ; w
. solution (41) approximate value
function found projecting point corresponding true value function Jff ; Jff
onto
line. illustrated figure
= . projection suboptimal weighted
mean-squared distance value-function space take account policy boundary.

( (1)

(2))
=3 5

[( (1) (2)]

Appendix B. Proof Theorem 6
proof needs following topological lemma. definitions see, example, (Dudley, 1989,
pp. 2425).
343

fiBAXTER & BARTLETT

3

2

1

0

1

111111111111111
000000000000000
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
[J (1), J (2)]
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
[w * (1), w * (2)]
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
Legend
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
Optimal Policy:
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
Approximate
000000000000000
111111111111111
000000000000000
111111111111111
Value Function:
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
000000000000000
111111111111111
1

0

1

2

3

11
00
00
11
00
11
00
11

4

5

Figure 2: Plot value-function space two-state system. Note scale axis
weighted square root stationary probability corresponding state
optimal policy. solution found TD(1) simply projection true
value function onto set approximate value functions.

(

)

Lemma 7. Let X; topological space Hausdorff, separable, first-countable.
Let B Borel -algebra generated . measurable space X; B sequence
S1; S2 ; : : : B sets satisfies following conditions:

(

1. Si partition X (that is, X
empty intersection).

2. x 2 X , fxg 2 B

1
\

)

= SfS : 2 Sig two distinct elements Si

fS 2 Si : x 2 g = fxg:

i=1

=

Proof. Since X separable, countable dense subset
fx1 ; x2 ; : : :g. Since X firstcountable, xi aScountable neighbourhood base, Ni . Now, construct partitions
Si using countable set N 1
; ; : : :, define
i=1 Ni follows. Let S0 X and,

=

=

=1 2

Si = fS \ Ni : 2 Si g [ fS \ (X Ni) : 2 Si g :
1

1

344

fiP OLICY-G RADIENT E STIMATION

Clearly, Si measurable partition X . Since X Hausdorff, pair x; x0 distinct
points X , pair disjoint open sets A0 x 2 x0 2 A0 . Since
dense, pair s; s0 2 s0 2 A0 . Also, N contains neighbourhoods Ns
Ns0 Ns Ns0 A0 . Ns Ns0 disjoint. Thus, sufficiently large i, x
x0 fall distinct elements partition Si . Since true pair x; x0 , follows

1
\

fS 2 Si : x 2 g fxg:

i=1

reverse inclusion
trivial. measurability singletons fxg follows measuraS
bility Sx
fS 2 Si \ fxg g fact fxg X Sx .

:=

:

=

=

shall use Lemma 7 together following result show approximate
expectations certain random variables using single sample path Markov chain.

(

)

Lemma 8. Let X; B measurable space satisfying conditions Lemma 7, let S1 ; S2 ; : : :
suitable sequence partitions lemma. Let probability measure defined
space. Let f absolutely integrable function X . event , define

f (S ) =
x 2 X
almost x X ,

k

R

f :

(S )

= 1; 2; : : :, let Sk (x) unique element Sk containing x.
lim f (Sk (x)) = f (x):
k!1

Proof. Clearly, signed finite measure defined

(E ) =

Z

E

fd

absolutely continuous respect , Equation (42) defines
derivative respect . derivative defined

(42)

f

Radon-Nikodym

(Sk (x))

(x) = klim
:
!1 (Sk (x))


See, example, (Shilov & Gurevich, 1966, Section 10.2). Radon-Nikodym Theorem (Dudley, 1989, Theorem 5.5.4, p. 134), two expressions equal a.e. ().
Proof. (Theorem 6.) definitions,

rfi = 0 rP Jfi

=

n X
n
X
i=1 j =1

(i)rpij ()Jfi (j ):

(43)

every , absolutely continuous respect reference measure , hence
j write
Z Z
d(; y)
pij () =
pij (u)
(u) d(u) (i)(y):

U
345

fiBAXTER & BARTLETT

Since depend
integral obtain

rpij () =

d(; y)=d absolutely integrable,

Z Z

U

pij (u) r

d(; y)
(u) d(u) (i)(y):


avoid cluttering notation, shall use denote distribution
denote distribution . notation,

()

rpij () =

differentiate

Z Z

U

pij

(; y) U ,



r
d:



Now, let probability measure U generated . write (43)

rfi =

X

i;j

(i)Jfi (j )

Z

YU

pij

r
d:



Using notation Lemma 8, define

pij (S ) =

R

pij ;

(S )

Z

1
r(S ) = (S ) rdd d;


measurable set

U . Notice that, given i, j , ,
pij (S ) = Pr(Xt+1 = j jXt = i; (y; u) 2 )


!


r
r(S ) = E dd fififi Xt = i; (Yt ; Ut ) 2 :


Let S1 ; S2 ; : : : sequence partitions U Lemma 7, let Sk
element Sk containing y; u . Using Lemma 8,

( )

Z

YU

pij

Z
r
=




lim pij (Sk (y; u)) r (Sk (y; u)) d(y; u)

YU k!1

= klim
!1

(y; u) denote

X Z

2Sk



pij (S ) r(S ) d;

346

fiP OLICY-G RADIENT E STIMATION

used Assumption 6 Lebesgue dominated convergence theorem interchange
integral limit. Hence,

rfi = klim
!1

= klim
!1

X X

i;j 2Sk

X

i;j;S

(i)(S )pij (S )Jfi (j )r(S )

Pr(Xt = i)Pr((Yt ; Ut ) 2 )Pr(Xt = j jXt = i; (Yt ; Ut ) 2 )
+1



E (J (t + 1)jXt

+1

= klim
!1

X

i;j;S

"


r
= j ) E dd fififi Xt = i; (Yt; Ut ) 2


!

#


E i(Xt )S (Yt; Ut )j (Xt )J (t + 1) rdd ;
+1



probabilities expectations respect stationary distribution Xt ,
distributions Yt ; Ut . Now, random process inside expectation asymptotically stationary
ergodic. ergodic theorem, (almost surely)

X TX
r
1
rfi = klim
lim
(Xt )S (Yt ; Ut )j (Xt )J (t + 1) dd :
!1 !1
1

+1

i;j;S t=0



easy see double limit exists order reversed,
TX

X
r
1
rfi = Tlim
lim i(Xt )S (Yt ; Ut )j (Xt )J (t + 1) dd
!1
k!1
1

1
= Tlim
!1

t=0
TX1
t=0

+1

i;j;S

(;Yt )
r Ut
d(;Yt ) U



( ) J (t + 1):
( )

argument proof Theorem 4 shows tails



r d(;Yt ) U




d(;Y
t)


U





J (t + 1) ignored

( )
( )
jr (Xt )j uniformly bounded. follows ! 0 rP Jfi w.p.1, required.
References
Aberdeen, D., & Baxter, J. (2001). Policy-gradient learning controllers internal state. Tech.
rep., Australian National University.
Aleksandrov, V. M., Sysoyev, V. I., & Shemeneva, V. V. (1968). Stochastic optimaization. Engineering Cybernetics, 5, 1116.
Baird, L., & Moore, A. (1999). Gradient descent general reinforcement learning. Advances
Neural Information Processing Systems 11. MIT Press.
347

fiBAXTER & BARTLETT

Bartlett, P. L., & Baxter, J. (1999). Hebbian synaptic modifications spiking neurons learn.
Tech. rep., Research School Information Sciences Engineering, Australian National
University. http://csl.anu.edu.au/bartlett/papers/BartlettBaxter-Nov99.ps.gz.
Bartlett, P. L., & Baxter, J. (2001). Estimation approximation bounds gradient-based reinforcement learning. Journal Computer Systems Sciences, 62. Invited Paper: Special
Issue COLT 2000.
Barto, A. G., Sutton, R. S., & Anderson, C. W. (1983). Neuronlike adaptive elements solve
difficult learning control problems. IEEE Transactions Systems, Man, Cybernetics,
SMC-13, 834846.
Baxter, J., Bartlett, P. L., & Weaver, L. (2001). Experiments infinite-horizon, policy-gradient
estimation. Journal Artificial Intelligence Research. appear.
Baxter, J., Tridgell, A., & Weaver, L. (2000). Learning play chess using temporal-differences.
Machine Learning, 40(3), 243263.
Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientific.
Bertsekas, D. P. (1995). Dynamic Programming Optimal Control, Vol II. Athena Scientific.
Breiman, L. (1966). Probability. Addison-Wesley.
Cao, X.-R., & Wan, Y.-W. (1998). Algorithms Sensitivity Analysis Markov Chains
Potentials Perturbation Realization. IEEE Transactions Control Systems Technology,
6, 482492.
Dudley, R. M. (1989). Real Analysis Probability. Wadsworth & Brooks/Cole, Belmont, California.
Glynn, P. W. (1986). Stochastic approximation monte-carlo optimization. Proceedings
1986 Winter Simulation Conference, pp. 356365.
Glynn, P. W. (1990). Likelihood ratio gradient estimation stochastic systems. Communications
ACM, 33, 7584.
Glynn, P. W., & LEcuyer, P. (1995). Likelihood ratio gradient estimation regenerative stochastic
recursions. Advances Applied Probability, 27, 4 (1995), 27, 10191053.
Ho, Y.-C., & Cao, X.-R. (1991). Perturbation Analysis Discrete Event Dynamic Systems. Kluwer
Academic, Boston.
Jaakkola, T., Singh, S. P., & Jordan, M. I. (1995). Reinforcement Learning Algorithm Partially
Observable Markov Decision Problems. Tesauro, G., Touretzky, D., & Leen, T. (Eds.),
Advances Neural Information Processing Systems, Vol. 7. MIT Press, Cambridge, MA.
Kimura, H., & Kobayashi, S. (1998a). analysis actor/critic algorithms using eligibility traces:
Reinforcement learning imperfect value functions. Fifteenth International Conference
Machine Learning, pp. 278286.
348

fiP OLICY-G RADIENT E STIMATION

Kimura, H., & Kobayashi, S. (1998b). Reinforcement learning continuous action using stochastic gradient ascent. Intelligent Autonomous Systems (IAS-5), pp. 288295.
Kimura, H., Miyazaki, K., & Kobayashi, S. (1997). Reinforcement learning POMDPs
function approximation. Fisher, D. H. (Ed.), Proceedings Fourteenth International
Conference Machine Learning (ICML97), pp. 152160.
Kimura, H., Yamamura, M., & Kobayashi, S. (1995). Reinforcement learning stochastic hill
climbing discounted reward. Proceedings Twelfth International Conference
Machine Learning (ICML95), pp. 295303.
Konda, V. R., & Tsitsiklis, J. N. (2000). Actor-Critic Algorithms. Neural Information Processing
Systems 1999. MIT Press.
Lancaster, P., & Tismenetsky, M. (1985). Theory Matrices. Academic Press, San Diego, CA.
Marbach, P., & Tsitsiklis, J. N. (1998). Simulation-Based Optimization Markov Reward Processes. Tech. rep., MIT.
Meuleau, N., Peshkin, L., Kaelbling, L. P., & Kim, K.-E. (2000). Off-policy policy search. Tech.
rep., MIT Artificical Intelligence Laboratory.
Meuleau, N., Peshkin, L., Kim, K.-E., & Kaelbling, L. P. (1999). Learning finite-state controllers
partially observable environments. Proceedings Fifteenth International Conference
Uncertainty Artificial Intelligence.
Peshkin, L., Kim, K.-E., Meuleau, N., & Kaelbling, L. P. (2000). Learning cooperate via policy
search. Proceedings Sixteenth International Conference Uncertainty Artificial
Intelligence.
Reiman, M. I., & Weiss, A. (1986). Sensitivity analysis via likelihood ratios. Proceedings
1986 Winter Simulation Conference.
Reiman, M. I., & Weiss, A. (1989). Sensitivity analysis simulations via likelihood ratios. Operations Research, 37.
Rubinstein, R. Y. (1969). Problems Monte Carlo Optimization. Ph.D. thesis.
Rubinstein, R. Y. (1991). optimize complex stochastic systems single sample path
score function method. Annals Operations Research, 27, 175211.
Rubinstein, R. Y. (1992). Decomposable score function estimators sensitivity analysis optimization queueing networks. Annals Operations Research, 39, 195229.
Rubinstein, R. Y., & Melamed, B. (1998). Modern Simulation Modeling. Wiley, New York.
Rubinstein, R. Y., & Shapiro, A. (1993). Discrete Event Systems. Wiley, New York.
Samuel, A. L. (1959). Studies Machine Learning Using Game Checkers. IBM
Journal Research Development, 3, 210229.
349

fiBAXTER & BARTLETT

Shilov, G. E., & Gurevich, B. L. (1966). Integral, Measure Derivative: Unified Approach.
Prentice-Hall, Englewood Cliffs, N.J.
Singh, S. P., Jaakkola, T., & Jordan, M. I. (1994). Learning Without State-Estimation Partially
Observable Markovian Decision Processes. Proceedings Eleventh International
Conference Machine Learning.
Singh, S., & Bertsekas, D. (1997). Reinforcement learning dynamic channel allocation cellular telephone systems. Advances Neural Information Processing Systems: Proceedings
1996 Conference, pp. 974980. MIT Press.
Smallwood, R. D., & Sondik, E. J. (1973). optimal control partially observable Markov
decision processes finite horizon. Operations Research, 21, 10711098.
Sondik, E. J. (1978). optimal control partially observable Markov decision processes
infinite horizon: Discounted costs. Operations Research, 26.
Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: Introduction. MIT Press,
Cambridge MA. ISBN 0-262-19398-1.
Sutton, R. S., McAllester, D., Singh, S., & Mansour, Y. (2000). Policy Gradient Methods
Reinforcement Learning Function Approximation. Neural Information Processing
Systems 1999. MIT Press.
Tao, N., Baxter, J., & Weaver, L. (2001). multi-agent, policy-gradient approach network
routing. Tech. rep., Australian National University.
Tesauro, G. (1992). Practical Issues Temporal Difference Learning. Machine Learning, 8, 257
278.
Tesauro, G. (1994). TD-Gammon, self-teaching backgammon program, achieves master-level
play. Neural Computation, 6, 215219.
Tsitsikilis, J. N., & Van-Roy, B. (1997). Analysis Temporal Difference Learning Function Approximation. IEEE Transactions Automatic Control, 42(5), 674690.
Williams, R. J. (1992). Simple Statistical Gradient-Following Algorithms Connectionist Reinforcement Learning. Machine Learning, 8, 229256.
Zhang, W., & Dietterich, T. (1995). reinforcement learning approach job-shop scheduling.
Proceedings Fourteenth International Joint Conference Artificial Intelligence, pp.
11141120. Morgan Kaufmann.

350


