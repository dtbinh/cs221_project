journal artificial intelligence

submitted published

infinite horizon policy gradient estimation
jonathan baxter

jbaxter whizbang com

whizbang labs
henry street pittsburgh pa

peter l bartlett

bartlett barnhilltechnologies com

biowulf technologies
addison street suite berkeley ca

abstract
gradient approaches direct policy search reinforcement learning received
much recent attention means solve partial observability avoid
associated policy degradation value function methods introduce gpomdp simulation generating biased estimate gradient
average reward partially observable markov decision processes pomdps controlled
parameterized stochastic policies similar proposed kimura yamamura
kobayashi chief advantages requires storage twice
number policy parameters uses one free parameter natural interpretation
terms bias variance trade requires knowledge underlying state prove
convergence gpomdp correct choice parameter related
mixing time controlled pomdp briefly describe extensions gpomdp controlled
markov chains continuous state observation control spaces multiple agents higher order
derivatives version training stochastic policies internal states companion
baxter bartlett weaver gradient estimates generated gpomdp
used traditional stochastic gradient conjugate gradient procedure
local optima average reward

introduction
dynamic programming method choice solving decision making
uncertainty bertsekas however application dynamic programming becomes problematic large infinite state spaces situations system dynamics unknown
state partially observed cases one looks approximate techniques
rely simulation rather explicit model parametric representations valuefunction policy rather exact representations
simulation methods rely parametric form value function tend go
name reinforcement learning extensively studied machine learning
literature bertsekas tsitsiklis sutton barto yielded
remarkable empirical successes number different domains including learning play checkers samuel backgammon tesauro chess baxter tridgell weaver
job shop scheduling zhang dietterich dynamic channel allocation singh
bertsekas
despite success training approximate value functions suffer
theoretical flaw performance greedy policy derived approximate valuefunction guaranteed improve iteration fact worse old policy

c ai access foundation morgan kaufmann publishers rights reserved

fibaxter bartlett

amount equal maximum approximation error states happen even
parametric class contains value function whose corresponding greedy policy optimal
illustrate concrete simple example appendix
alternative circumvents problemthe pursue hereis
consider class stochastic policies parameterized r k compute gradient respect
average reward improve policy adjusting parameters gradient
direction note policy could directly parameterized could generated indirectly
value function latter case value function parameters parameters
policy instead adjusted minimize error approximate true value
function parameters adjusted directly improve performance policy generated
value function
policy gradient long history operations statistics control theory discrete event systems machine learning describing contribution
present seems appropriate introduce background material explaining readers already familiar material may want skip directly section
contributions present described
brief history policy gradient
large scale system dynamics unknown performance
gradient computable closed form thus challenging aspect policy gradient
estimating gradient via simulation naively gradient
calculated numerically adjusting parameter turn estimating effect performance via simulation called crude monte carlo technique prohibitively
inefficient somewhat surprisingly mild regularity conditions turns
full gradient estimated single simulation system technique
called score function likelihood ratio method appears first proposed
sixties aleksandrov sysoyev shemeneva rubinstein computing performance
gradients independently identically distributed processes
specifically suppose r x performance function depends random variable
x q x probability x x parameterized rk mild regularity
conditions gradient respect expected performance





may written

see rewrite sum



er x



r er x rqq xx





x

r

x

r x q x

x
differentiate one source requirement mild regularity conditions obtain
x

r x rq x

see equation closed form expression performance gradient



fip olicy g radient e stimation

rewrite

x

r

r x

rq x q x



q x
x
observe formula equivalent
simulator available generate samples x distributed according q x
sequence x x xn generated according q x gives unbiased estimate




n
x
r n r xi rqq xx



r probability one quantity
r law large numbers r
rq x q x known likelihood ratio score function classical statistics
performance function r x depends r x rq x q x replaced
rr x r x rq x q x


u nbiased e stimates
p rocesses



p erformance g radient



r egenerative

extensions likelihood ratio method regenerative processes including markov decision
processes mdps given glynn glynn lecuyer reiman
weiss independently episodic partially observable markov decision
processes pomdps williams introduced reinforce
samples x previous section sequences states x xt random length
encountered visits designated recurrent state sequences states
start state goal state case rq x q x written sum







rq x tx rpxtxt


q x





pxt xt



pxt xt transition probability xt xt given parameters equation
admits recursive computation course regenerative cycle form z
rk
state transition xt xt



zt zt








rpxtxt
pxt xt

term r x rq x q x estimate form
addition r x xt recursively computed





r x xt zt

r x xt r x xt xt
function estimate r x xt zt cycle computed
storage k parameters k zt parameter update performance function
r hence entire estimate computed storage k real parameters
follows

thresholded version neuron elements described earlier barto sutton anderson
vector zt known reinforcement learning eligibility trace terminology used barto et al




fibaxter bartlett

policy gradient regenerative processes
set j

r z z rk






state transition xt




j



xt







episode finished xt
j
j rt zt
j j
zt
rt





otherwise set

rp

set



zt zt pxxtxtxt

rt rt xt


n return n n otherwise goto

examples recursive
performance functions include sum scalar reward cycle
p

r x xt tt r xt r scalar reward associated state corresponds average reward multiplied expected recurrence time e




negative length cycle implemented assigning reward
state
used task mimimize time taken get goal state since
case
pt

e discounted reward start state r x xt
r xt
discount factor
williams pointed simplification possible case rt
r x xt sum scalar rewards r xt depending state possibly time
since starting state r xt r xt r xt fft r xt case
update single regenerative cycle may written
















tx


pxt xt


x










rpxtxt







r xs


x






r xs





changes pxt xt influence rewards r xs associated earlier
states able drop first term parentheses right hand side
write
tx

rpxtxt x
r x

p

xt xt










although proof entirely trivial intuition indeed shown correct
equation allows even simpler recursive formula estimating performance gradient set z
introduce variable
set zt
zt

rpxtxt pxtxt xt
zt
otherwise
iteration set r xt zt
estimate r since
updated every iteration suggests away altogether simply update directly r xt zt suitable step sizes proving convergence



p


















usual requirements convergence stochastic gradient











p








fip olicy g radient e stimation

straightforward normal stochastic gradient
updates r xt zt gradient direction expectation although sum updates
regenerative cycle marbach tsitsiklis provide convergence proof
know albeit slightly different update form
r xt zt
moving estimate expected performance updated line
update first suggested context pomdps jaakkola et al
marbach tsitsiklis considered case dependent rewards recall discussion baird moore vaps value policy
search last contains interesting insight suitable choices performance
function r x xt one combine policy gradient search approximate value function methods resulting viewed actor critic techniques spirit barto
et al policy actor value function critic primary motivation
reduce variance policy gradient estimates experimental evidence phenomenon
presented number authors including barto et al kimura kobayashi
baird moore recent work subject includes sutton
et al konda tsitsiklis discuss use vaps style updates
section
far addressed question parameterized state transition probabilities pxt xt arise course could simply generated parameterizing matrix
transition probabilities directly alternatively case mdps pomdps state transitions
typically generated feeding observation yt depends stochastically state xt
parameterized stochastic policy selects control ut random set available controls approximate value function approaches generate controls stochastically
via form lookahead fall category distribution successor states
pxt xt ut fixed function control denote probability control ut given
parameters observation yt ut yt discussion carries
rpxtxt pxtxt replaced rut yt ut yt case precisely williams reinforce
variants extended cover multiple agents peshkin
et al policies internal state meuleau et al importance sampling methods
meuleau et al refer reader work rubinstein shapiro
rubinstein melamed depth analysis application likelihood ratio
method discrete event systems des particular networks queues worth mentioning
large literature infinitesimal perturbation analysis ipa seeks similar goal estimating performance gradients operates restrictive assumptions likelihoodratio see example ho cao






















b iased e stimates












p erformance g radient

described previous section rely identifiable recurrent state
update gradient estimate case line zero eligibility trace
z reliance recurrent state problematic two main reasons
variance related recurrence time visits
typically grow state space grows furthermore time visits depends


fibaxter bartlett

parameters policy states frequently visited initial value
parameters may become rare performance improves
situations partial observability may difficult estimate underlying states
therefore determine gradient estimate updated eligibility trace
zeroed
system available simulation seems difficult impossible obtain
unbiased estimates gradient direction without access recurrent state thus solve
must look biased estimates two principle techniques introducing bias
proposed may viewed artificial truncations eligibility trace z first
method takes starting point formula eligibility trace time

zt


x

rpxsxs
pxs xs



simply truncates fixed random number terms n looking backwards glynn
rubinstein cao wan

zt n


x
n

rpxsxs
pxs xs



n updated transition xt xt
rp
rpxt nxt n
zt n zt n xtxt
pxt xt
pxt n xt n
case state rewards r xt estimated gradient direction steps

rn x zt n r xt
n n
eligibility trace zt











unless n exceeds maximum recurrence time infinite ergodic markov chain
rn biased estimate gradient direction although n bias approaches zero
however variance rn diverges limit large n illustrates natural trade
selection parameter n large enough ensure bias acceptable
expectation rn least within true gradient direction large
variance prohibitive experimental cao wan illustrate nicely
bias variance trade
one potential difficulty method likelihood ratios rpxs xs pxs xs
must remembered previous n time steps requiring storage kn parameters thus
obtain small bias memory may grow without bound alternative
requires fixed amount memory discount eligibility trace rather truncating











zt fizt

rpxtxt
pxt xt





r

ease exposition kept expression z terms likelihood ratios pxs xs pxs xs
rely availability underlying state xs xs available pxs xs pxs xs
replaced us ys us ys

r

r



fip olicy g radient e stimation



z

steps simply

discount factor
r

tx


case estimated gradient direction

r xt zt






precisely estimate analyze present similar estimate r xt zt
replaced r xt
b zt b reward baseline proposed kimura et al
continuous control kimura kobayashi b fact use r xt
b
place r xt affect expectation estimates although judicious choice reward baseline b reduce variance estimates
presented kimura et al provides estimates expectation stationary distribution gradient discounted reward fact biased estimates
gradient expected discounted reward arises stationary distribution
depends parameters similar estimate proposed marbach
tsitsiklis time r xt zt replaced r xt
zt
estimate average reward zt zeroed visits identifiable recurrent state
final note observe eligibility traces zt zt n defined
simply filtered versions sequence rpxt xt pxt xt first order infinite impulse
response filter case zt n th order finite impulse response filter case
zt n raises question addressed whether interesting theory
optimal filtering policy gradient estimators























contribution
describe gpomdp general upon generating biased estimate
performance gradient r general pomdps controlled parameterized stochastic policies
denotes average reward policy parameters rk gpomdp
rely access underlying recurrent state writing rfi expectation estimate produced gpomdp
r quantitatively
rfi
rfi close true gradient provided exceeds mixing time markov chain
induced pomdp truncated estimate trade preventing setting
arbitrarily close variance estimates increase approaches
prove convergence probability gpomdp discrete continuous observation control spaces present general parameterized markov chains
pomdps controlled parameterized stochastic policies
several extensions gpomdp investigated since first version
written outline developments briefly section
companion gradient estimates produced gpomdp used
perform gradient ascent average reward baxter et al describe
traditional stochastic gradient conjugate gradient utilizes gradient
estimates novel way perform line searches experimental presented illustrat






lim











mixing time applies markov chains distinct eigenvalues better estimates
bias variance gpomdp may found bartlett baxter general markov chains
treated refined notions mixing time roughly speaking variance gpomdp
grows bias decreases function



fibaxter bartlett

ing theoretical present toy practical aspects
number realistic

reinforcement learning
model reinforcement learning markov decision process mdp finite state space
f ng stochastic matrix p pij giving probability transition state
state j state associated reward r matrix p belongs parameterized
class stochastic matrices p
fp rk g denote markov chain corresponding
p assume markov chains rewards satisfy following assumptions















assumption p p unique stationary distribution
satisfying balance equations

n

p
throughout denotes transpose
assumption magnitudes rewards jr j uniformly bounded r
states





assumption ensures markov chain forms single recurrent class parameters
since finite state markov chain ends recurrent class properties
class determine long term average reward assumption mainly convenience
include recurrence class quantifier theorems however
consider gradient ascent baxter et al assumption becomes
restrictive since guarantees recurrence class cannot change parameters adjusted
ordinarily discussion mdps would complete without mention actions
available state space policies available learner particular parameters
would usually determine policy directly indirectly via value function would
determine transition probabilities p however purposes care
dependence p arises satisfies assumption differentiability
assumptions shall meet next section note easy extend setup
case rewards depend parameters transitions j
equally straightforward extend cases see section
illustration
goal r k maximizing average reward



tx



e
r xt x



e denotes expectation sequences x x transitions generated according p assumption independent starting state equal
n
x

r r







lim









r





r r n bertsekas


p

stochastic matrix p pij pij j n
j pij
present apply bounded stochastic rewards case r expectation
reward state



fip olicy g radient e stimation

computing gradient average reward



general mdps little known average reward hence finding optimum
problematic however section see general assumptions gradient
r exists local optimization possible
ensure existence suitable gradients boundedness certain random variables
require parameterized class stochastic matrices satisfies following additional assumption





assumption derivatives

rp
rk ratios

exist



pij
k

j n k k

fifi p fifi
ij
k



pij

uniformly bounded b



j n k k

rk

second part assumption allows zero probability transitions pij



rpij zero case set one example j forbidden
transition pij rk another example satisfying assumption
pij






n nn rn

parameters p

pij ij
pij
pij kl
pij

assuming moment r
dependencies

eij
ij
j e

pn




pij





pkl

exists justified shortly suppressing
r r r


since reward r depend note convention r takes
precedence operations rg f
rg f equations
regarded shorthand notation k equations form




k

k








n

r r n
k
k

k compute r first differentiate balance equations obtain
r p rp r


fibaxter bartlett

hence

r p rp



system equations defined constrained p invertible
balance equations p left eigenvector zero eigenvalue however let e
denote n dimensional column vector consisting e n n matrix
stationary distribution row since r e r e
r
rewrite









r p e rp
see inverse
write

p


lim

e





x




exists let matrix satisfying


tlim








tlim



thus




x








x


tx








limt











easy prove induction p e
p ep

tconverges


hence write
assumption
p e
exists equal
p
e










r rp p e




r rp p e









r







mdps sufficiently small number states could solved exactly yield precise
gradient direction however general state space small enough exact solution
possible small enough derive optimal policy policy iteration
table lookup would point pursuing gradient first place
thus practical interest intractable need
way computing gradient one approximate technique presented
next section
argument leading coupled fact unique solution used justify
existence specifically run steps computing value small
expression unique matrix satisfying
equation may still useful pomdps since case tractable dynamic programming


r

r

r



kk

fip olicy g radient e stimation

approximating gradient parameterized markov chains
section gradient split two components one becomes
negligible discount factor approaches
let jfi
jfi jfi n denote vector expected discounted
rewards state





jfi e








x


fitr



xt fifi x












dependence obvious write jfi


r r jfi rp jfi

proposition r k



proof observe jfi satisfies bellman equations

jfi r fip jfi



bertsekas hence

r r r
r jfi fip jfi
r jfi r jfi rp jfi
r jfi rp jfi



shall see next section second term estimated single sample path markov chain fact theorem kimura et al shows gradient
estimates presented converge
rjfi bellman equations equal
rp jfi rjfi implies
rjfi rp jfi
thus kimura et al estimates second term expression
r given important note rjfi r jfi two quantities disagree
first term arises stationary distribution depends
parameters hence kimura et al estimate gradient expected discounted reward fact expected discounted reward simply
times
average reward singh et al fact gradient expected discounted reward
proportional gradient average reward
following theorem shows first term becomes negligible approaches
notice immediate proposition since jfi become arbitrarily large
limit































theorem rk

r filim
r




rfi rp jfi









fibaxter bartlett

proof recalling equation discussion preceeding

r rp
rp e


x



e r

pt





r p e r since p stochastic matrix rewritten
r



x



rp p r





discount factor consider expression

let

f

lim






x





rp fip r





clearly r
rfi
f complete proof need f





since fip
p e invoke observation write




x

p

particular

write

fip

fip converges take rp back sum right hand side
f rp



p




fitp r

fip

jfi thus f rp jfi


x





fitp

r



rfi



theorem shows rfi good approximation gradient approaches
turns values close lead large variance estimates rfi
describe next section however following theorem shows
need
small provided transition probability matrix p distinct eigenvalues markov
chain short mixing time initial state distribution states markov chain
converges stationary distribution provided assumption assumption existence
uniqueness stationary distribution satisfied see example lancaster tismenetsky
theorem p spectral resolution theorem lancaster tismenetsky
theorem p implies distribution converges stationarity exponential rate
time constant convergence rate mixing time depends eigenvalues
transition probability matrix existence unique stationary distribution implies







since e r e motivates different kind estimating differential rewards
marbach tsitsiklis
cannot back p sum right hand side
p diverges p e reason
p p converges p becomes orthogonal p limit tof large thus view p


sum two orthogonal components infinite one direction e finite one direction e



finite component need estimate approximating
p fip way rendering
e component finite hopefully altering e component much substitutions
lead better approximations context see final paragraph section

p

r

r

r



p

p

r



p

p

fip olicy g radient e stimation





largest magnitude eigenvalue multiplicity corresponding left eigenvector
stationary distribution sort eigenvalues decreasing order magnitude
j j js j n turns j j determines mixing time
chain
following theorem shows
small compared
j j gradient approximation described accurate since estimate direction
update parameters theorem compares directions gradient estimate
theorem denotes spectral condition number nonsingular matrix defined
product spectral norms matrices











kak ka





k


kak x max
kaxk
kxk






kxk denotes euclidean norm vector x



theorem suppose transition probability matrix p satisfies assumption stationary distribution
n n distinct eigenvalues let
x x xn
matrix right eigenvectors p corresponding order eigenvalues
j j
jn j normalized inner product r rfi satisfies











kr p p k p
rfi

n
rkr


r r

k
krk
j j
diag n
notice r r expectation stationary distribution r x
well mixing time via j j bound theorem depends another parameter
markov chain spectral condition number markov chain reversible


















implies eigenvectors x xn orthogonal equal ratio maximum
minimum probability states stationary distribution however eigenvectors
need nearly orthogonal fact condition transition probability matrix
n distinct eigenvalues necessary without condition number replaced
complicated expression involving spectral norms matrices form p







proof existence n distinct eigenvalues implies p expressed
n lancaster tismenetsky theorem p follows
polynomial f write f p
sf
proposition shows r rfi r
jfi

diag





jfi r fip r p r
fip p r

x

r














n
x
j



xj



j


x


fij





r

fibaxter bartlett







yn
easy verify yi left eigenvector corresponding choose
x e thus write





jfi e r

n
x

xj yj





fij r


j


n
x

r
xj yj

j
j



e



e sms r








diag





x





fin

follows proposition

rfi
r r r
rkr


k
krk

r r jfi




jfi

krk

r
r e sms r

krk

sms r
r rkr
k

r sms r

krk
p

since r r
apply cauchy











cauchy schwartz inequality
schwartz inequality obtain



rfi
rkr

k





r



p







sms




krk





r





use spectral norms bound second factor numerator clear definition
spectral norm product nonsingular matrices satisfies kab k kak kb k
spectral norm diagonal matrix given k
dn k
jdi j follows




sms




diag
max


r sms r




r km k

p
r r jfi j


























combining equation proves




fip olicy g radient e stimation

estimating gradient parameterized markov chains
introduces mcg markov chain gradient estimating approximate gradient rfi single line sample path x x markov chain
mcg requires k reals stored k dimension parameter space k
parameters eligibility trace zt k parameters gradient estimate note
time steps average far r xt zt











tx


zt r xt






mcg markov chain gradient
given




parameter r k
parameterized class stochastic matrices p


fp rk g satisfying assumptions


arbitrary starting state x
state sequence x x


generated e markov chain transition

reward sequence r x r x satisfying assumption
set z z rk
state xt visited
rp

zt fizt xtxt
pxt xt
r xt zt
probabilities p




































end

theorem assumptions mcg starting initial state x
generate sequence satisfying





lim rfi





w p





proof let fxt g fx x g denote random process corresponding x
entire process stationary proof easily generalized arbitrary initial distributions fact assumption fxt g asymptotically stationary fxt g


fibaxter bartlett

stationary write

rp jfi

x



x



x

j
j
j

rpij jfi j
pij

rpij j j
p
ij

pr xt pr xt j jxt rppij e j jxt j




ij

first probability respect stationary distribution

j






x


fis







j process

r xs



fact e j
jxt jfi xt xt follows boundedness
magnitudes rewards assumption lebesgues dominated convergence theorem
rewrite equation




x
rp
rp jfi e xt j xt ij j

pij

j



denotes indicator function state

xt
xt
otherwise

expectation respect stationary distribution xt chosen according
stationary distribution process fxt g ergodic since process fzt g defined

zt xt j xt

rpij j
pij

obtained taking fixed
fxt g fzt g stationary ergodic breiman
function

rpij
proposition since pij bounded assumption ergodic theorem
almost surely

rp jfi

tx

rp
tlim
xt j xt ij j

pij
j
tx
rpxtxt j

tlim

pxtxt

tx

x
r
pxtxt x

tlim
r xs

pxtxt

x

























fis



r xs



fip olicy g radient e stimation

concentrating second term right hand side observe

tx


fit





rpxtxt
pxt xt


x

fis


tx fifi











xs fifi


r



pxt xt



br tx x



br



x

rpxtxt fififi


tx







fis







fis





jr xs j





fit
brfi





jrp j
r b bounds magnitudes rewards pijij assumptions
hence
tx

rpxtxt x
rp jfi

r xs

p

x
x



unrolling equation mcg shows equal




lim



tx rpxtxt x
pxt xt




hence

fis







r



rp jfi w p required

estimating gradient partially observable markov decision processes



applies parameterized class stochastic matrices p compute gradients rpij section consider special case p arise
parameterized class randomized policies controlling partially observable markov decision process pomdp partially observable qualification means assume policies
access observation process depends state general may see state
specifically assume n controls u
f n g observations
f g u u determines stochastic matrix p u depend
parameters state observation generated independently according
probability distribution observations denote probability observation
randomized policy simply function mapping observations probability
distributions controls u observation distribution
controls u denote probability control u given observation u
randomized policy observation distribution corresponds markov
chain state transitions generated first selecting observation state according


























fibaxter bartlett





distribution selecting control u according distribution generating transition state j according probability pij u parameterize chains
parameterize policies becomes function set parameters r k
well observation markov chain corresponding state transition matrix pij
given






pij ey eu pij u

equation implies

rpij

x



pij u ru

u



introduces gpomdp gradient partially observable markov
decision process modified form updates zt ut yt
rather pxt xt note require knowledge transition probability matrix p observation process requires knowledge randomized
policy gpomdp essentially proposed kimura et al without
reward baseline
gpomdp assumes policy function current observation
immediate works finite history observations general
optimal policy needs function entire observation history gpomdp extended
apply policies internal state aberdeen baxter







gpomdp
given




parameterized class randomized policies



rk



satisfying assumption

partially observable markov decision process controlled randomized
policies corresponds parameterized class markov chains satisfying assumption




arbitrary unknown starting state x
observation sequence generated pomdp controls u u








yt

generated randomly according










reward sequence r x r x satisfying assumption
hidden sequence states markov decision process







set z

z rk
observation yt control ut subsequent reward r
rut yt
zt fizt
ut yt


r xt zt

end





xt


x x





fip olicy g radient e stimation

convergence need replace assumption similar bound
gradient
assumption derivatives

exist u u

u
k

rk ratios

fifi
u


k



u

uniformly bounded b

u n k k

rk

theorem assumptions starting initial state
generate sequence satisfying





lim rfi

w p



x





proof proof follows lines proof theorem case

rp jfi





x

j

rpij jfi j

x

j u
x

j u
x

j u

pij u ru jfi j
pij u

ru j j

u
u

ezt

expectation respect stationary distribution fxt g process fzt g
defined
ru j
zt xt j xt u ut yt
u





ut control process yt observation process follows
arguments used proof theorem
control dependent rewards

many circumstances rewards may depend controls u
example controls may consume energy others may wish add penalty
term reward function order conserve energy simplest way deal
define state expected reward r



r ey eu r u




fibaxter bartlett



redefine jfi terms r

jfi

lim e



n

n
x




xt fifi x




fitr









x x performance gradient becomes
r r r rr

expectation trajectories

approximated

rfi rp jfi rr








due fact jfi satisfies bellman equations r replaced r
gpomdp take account dependence r controls fifth line
replaced


r ut




r
ut yt
xt zt












ut

straightforward extend proofs theorems setting

parameter dependent rewards
possible modify gpomdp rewards depend directly case
fifth line gpomdp replaced

r xt zt rr xt












convergence approximation theorems carry provided rr uniformly bounded parameter dependent rewards considered glynn marbach
tsitsiklis baird moore particular baird moore showed
suitable choices r lead combination value policy search vaps
example j approximate value function setting





h




r xt xt
r xt ffj xt j xt
r xt usual reward discount factor gives update seeks






minimize expected bellman error
n
x




r

n
x
j



pij j j j





effect minimizing bellman error j driving system
via policy states small bellman error motivation behind
understood one considers j zero bellman error states case greedy
policy derived j optimal regardless actual policy parameterized
expectation zt r xt xt zero gradient computed gpomdp
kind update known actor critic barto et al policy playing
role actor value function playing role critic





use rewards r xt xt
analysis





depend current previous



state substantially alter

fip olicy g radient e stimation

extensions infinite state observation control spaces
convergence proof relied finite state observation control u
spaces however clear modification applied immediately pomdps countably uncountably infinite countable u
changes pij u becomes kernel p x x u becomes density observations
addition appropriate interpretation r applied uncountable u specifically u subset r n probability density function u u
density u u subsets euclidean space finite set theorem
extended estimates produced converge almost surely rfi
fact prove general implies case densities subsets r n
well finite case theorem allow u general spaces satisfying following
topological assumption definitions see example dudley













assumption control space u associated topology separable hausdorff
first countable corresponding borel algebra b generated topology
finite measure defined measurable space u b say reference measure
u
similarly observation space topology borel algebra reference measure
satisfying conditions





case theorem u finite associated reference measure
counting measure u
rn rm reference measure lebesgue measure
assume distributions absolutely continuous respect reference
measures corresponding radon nikodym derivatives probability masses finite case
densities euclidean case satisfy following assumption











assumption every r k probability measure absolutely continuous respect reference measure u every probability measure
absolutely continuous respect reference measure
let reference measure u u u r k k f k g
derivatives


u
k


du
k

exist ratios







u fifi
du
u


bounded b





assumptions replace radon nikodym derivative
respect reference measure u case following convergence
generalizes theorem applies densities euclidean space u

theorem suppose control space u observation space satisfy assumption let

reference measure control space u consider
rut yt
ut yt


fibaxter bartlett

replaced

r yt ut






yt
ut

assumptions starting initial state
sequence satisfying





lim rfi



x

generate

w p

proof see appendix b


since first version extended gpomdp several settings
proved properties section briefly outline
multiple agents

instead single agent generating actions according suppose multiple agents
na parameter set distinct observation environment
yi generate actions ui according policy ui yi agents receive reward signal r xt may cooperating solve task example

gpomdp applied collective pomdp obtained concatenating
nobserva

na u
tions controls
parameters

single
vectors








u u


na respectively easy calculation shows gradient estimate generated
gpomdp collective case precisely obtained applying gpomdp



agent independently concatenating
na
estimate produced gpomdp applied agent leads line
agents adjust parameters independently without explicit communication
yet collectively adjustments maximizing global average reward similar observations context reinforce vaps see peshkin et al gives
biologically plausible synaptic weight update rule applied networks spiking neurons
neurons regarded independent agents bartlett baxter shown
promise network routing application tao baxter weaver














policies internal states
far considered purely reactive memoryless policies chosen control
function current observation gpomdp easily extended cover case
policies depend finite histories observations yt yt yt k general optimal
control pomdps policy must function entire observation history fortunately
observation history may summarized form belief state current distribution
states updated upon current observation knowledge
sufficient optimal behaviour smallwood sondik sondik extension
gpomdp policies parameterized internal belief states described aberdeen baxter
similar spirit extension vaps reinforce described meuleau et al



fip olicy g radient e stimation

higher order derivatives
generalized compute estimates second higher order derivatives
average reward assuming exist still single sample rpath underlying pomdp
see second order derivatives observe
q x r x dx twicedifferentiable density q x performance measure r x

gpomdp



z
r r x rq q x x q x dx







r denotes matrix second derivatives hessian verified

r q x r log q x r log q x
q x








log

second term right hand side outer product r
q x
matrix entries
q x j q x taking x sequence
states x x xt visits recurrent state parameterized markov chain recall
p
section q x
xt xt combined yields

log
log





r q x tx r pxtxt


q x



tx



pxt xt



rpxtxt
p



xt xt




x

rpxtxt



pxtxt



squared terms expression outer products expression derive
gpomdp computing biased estimate hessian r involves
maintainingin addition usual eligibility trace zt second matrix trace updated follows



zt

fizt rp pxtxt




xt xt

rpxtxt
p



xt xt





time steps returns average far r xt zt zt second term
outer product computation higher order derivatives could used second order
gradient methods optimization policy parameters
bias variance bounds





theorem provides bound bias rfi relative r applies underlying markov chain distinct eigenvalues extended arbitrary markov chains
bartlett baxter however extra generality comes price since latter bound involves number states chain whereas theorem supplies
proof variance gpomdp scales
providing formal justification
interpretation terms bias variance trade





conclusion
presented general mcg computing arbitrarily accurate approximations
gradient average reward parameterized markov chain chains transition
matrix distinct eigenvalues accuracy approximation shown controlled


fibaxter bartlett

size subdominant eigenvalue j j showed could modified apply
partially observable markov decision processes controlled parameterized stochastic policies
discrete continuous control observation state spaces gpomdp finite
state case proved convergence probability
briefly described extensions multi agent policies internal state estimating
higher order derivatives generalizations bias chains non distinct eigenvalues
variance many avenues continuous time
follow extensions presented mcg gpomdp
applied countably uncountably infinite state spaces convergence needed
cases
companion baxter et al present experimental showing rapid
convergence estimates generated gpomdp true gradient r give line
variants present variants gradient ascent make use
estimates rfi present experimental showing effectiveness
variety including three state mdp nonlinear physical control
call admission
acknowledgements
work supported australian council benefited comments
several anonymous referees performed authors
school information sciences engineering australian national university

appendix simple example policy degradation value function learning
approximate value function approaches reinforcement work minimizing form error
approximate value function true value function long known
may necessarily lead improved policy performance value function include
appendix illustrates phenomenon occur simplest possible system
two state mdp provides geometric intuition phenomenon arises
consider two state markov decision process mdp figure two controls
u u corresponding transition probability matrices

p u















p u


















u takes system state probability regardless starting state
therefore state probability u opposite since state reward
state reward optimal policy select action u policy
stationary distribution states
infinite horizon discounted
value state
discount value












jff e





x

fft r



xt fifi x















expectation state sequences x x x state transitions generated according p u solving bellmans equations jff r ffp u jff jff
jff jff


r
r r yields jff
jff
















fip olicy g radient e stimation

r

r





figure two state markov decsision process





suppose trying learn approximate value function j mdp e j
w state scalar feature must dimensionality ensure
j really approximate w r parameter learnt greedy policy obtained
j optimal j must value state state purposes illustration choose


j
j w must negative
temporal difference learning
one popular techniques training
approximate value functions sutton barto shown linear functions
converges parameter w minimizing expected squared loss stationary
distribution tsitsikilis van roy








td
td


w argmin

w






x



w jff



substituting previous expressions jff optimal policy solving

w yields w
hence w values wrong

sign situation optimal policy implementable greedy policy
approximate value function class choose w yet
observing
optimal policy converge value function whose corresponding greedy policy implements
suboptimal policy
geometrical illustration occurs shown figure figure points
graph
represent
p
p values states scales state state axes weighted
respectively way squared euclidean distance graph
two points j j corresponds expectation stationary distribution squared
difference values











hp




td




j

p

j



hp

j j
p









e j x j x



value function shaded region corresponding greedy policy optimal since
value functions rank state state bold line represents set realizable
approximate value functions w w
solution approximate value
function found projecting point corresponding true value function jff jff
onto
line illustrated figure
projection suboptimal weighted
mean squared distance value function space take account policy boundary








appendix b proof theorem
proof needs following topological lemma definitions see example dudley
pp


fibaxter bartlett





























j j




























w w










legend








optimal policy






approximate




value function



































figure plot value function space two state system note scale axis
weighted square root stationary probability corresponding state
optimal policy solution found td simply projection true
value function onto set approximate value functions





lemma let x topological space hausdorff separable first countable
let b borel algebra generated measurable space x b sequence
b sets satisfies following conditions



si partition x x
empty intersection

x x fxg b






sfs sig two distinct elements si

fs si x g fxg





proof since x separable countable dense subset
fx x g since x firstcountable xi ascountable neighbourhood base ni construct partitions
si countable set n
define
ni follows let x







si fs ni si g fs x ni si g






fip olicy g radient e stimation

clearly si measurable partition x since x hausdorff pair x x distinct
points x pair disjoint open sets x x since
dense pair n contains neighbourhoods ns
ns ns ns ns ns disjoint thus sufficiently large x
x fall distinct elements partition si since true pair x x follows




fs si x g fxg



reverse inclusion
trivial measurability singletons fxg follows measuras
bility sx
fs si fxg g fact fxg x sx









shall use lemma together following approximate
expectations certain random variables single sample path markov chain





lemma let x b measurable space satisfying conditions lemma let
suitable sequence partitions lemma let probability measure defined
space let f absolutely integrable function x event define

f
x x
almost x x

k

r

f



let sk x unique element sk containing x
lim f sk x f x
k

proof clearly signed finite measure defined

e

z

e

fd

absolutely continuous respect equation defines
derivative respect derivative defined



f

radon nikodym

sk x

x klim

sk x


see example shilov gurevich section radon nikodym theorem dudley theorem p two expressions equal e
proof theorem definitions

rfi rp jfi



n x
n
x
j

rpij jfi j



every absolutely continuous respect reference measure hence
j write
z z

pij
pij u
u u

u


fibaxter bartlett

since depend
integral obtain

rpij

absolutely integrable

z z

u

pij u r


u u


avoid cluttering notation shall use denote distribution
denote distribution notation



rpij

differentiate

z z

u

pij

u



r




let probability measure u generated write

rfi

x

j

jfi j

z

yu

pij

r




notation lemma define

pij

r

pij



z


r rdd


measurable set

u notice given j
pij pr xt j jxt u





r
r e dd fififi xt yt ut


let sequence partitions u lemma let sk
element sk containing u lemma



z

yu

pij

z
r





lim pij sk u r sk u u

yu k

klim


u denote

x z

sk



pij r



fip olicy g radient e stimation

used assumption lebesgue dominated convergence theorem interchange
integral limit hence

rfi klim


klim


x x

j sk

x

j

pij jfi j r

pr xt pr yt ut pr xt j jxt yt ut




e j jxt



klim


x

j




r
j e dd fififi xt yt ut







e xt yt ut j xt j rdd




probabilities expectations respect stationary distribution xt
distributions yt ut random process inside expectation asymptotically stationary
ergodic ergodic theorem almost surely

x tx
r

rfi klim
lim
xt yt ut j xt j dd





j



easy see double limit exists order reversed
tx

x
r

rfi tlim
lim xt yt ut j xt j dd

k



tlim



tx




j

yt
r ut
yt u



j


argument proof theorem shows tails



r yt u








u





j ignored



jr xt j uniformly bounded follows rp jfi w p required
references
aberdeen baxter j policy gradient learning controllers internal state tech
rep australian national university
aleksandrov v sysoyev v shemeneva v v stochastic optimaization engineering cybernetics
baird l moore gradient descent general reinforcement learning advances
neural information processing systems mit press


fibaxter bartlett

bartlett p l baxter j hebbian synaptic modifications spiking neurons learn
tech rep school information sciences engineering australian national
university http csl anu edu au bartlett papers bartlettbaxter nov ps gz
bartlett p l baxter j estimation approximation bounds gradient reinforcement learning journal computer systems sciences invited special
issue colt
barto g sutton r anderson c w neuronlike adaptive elements solve
difficult learning control ieee transactions systems man cybernetics
smc
baxter j bartlett p l weaver l experiments infinite horizon policy gradient
estimation journal artificial intelligence appear
baxter j tridgell weaver l learning play chess temporal differences
machine learning
bertsekas p tsitsiklis j n neuro dynamic programming athena scientific
bertsekas p dynamic programming optimal control vol ii athena scientific
breiman l probability addison wesley
cao x r wan w sensitivity analysis markov chains
potentials perturbation realization ieee transactions control systems technology

dudley r real analysis probability wadsworth brooks cole belmont california
glynn p w stochastic approximation monte carlo optimization proceedings
winter simulation conference pp
glynn p w likelihood ratio gradient estimation stochastic systems communications
acm
glynn p w lecuyer p likelihood ratio gradient estimation regenerative stochastic
recursions advances applied probability
ho c cao x r perturbation analysis discrete event dynamic systems kluwer
academic boston
jaakkola singh p jordan reinforcement learning partially
observable markov decision tesauro g touretzky leen eds
advances neural information processing systems vol mit press cambridge
kimura h kobayashi analysis actor critic eligibility traces
reinforcement learning imperfect value functions fifteenth international conference
machine learning pp


fip olicy g radient e stimation

kimura h kobayashi b reinforcement learning continuous action stochastic gradient ascent intelligent autonomous systems ias pp
kimura h miyazaki k kobayashi reinforcement learning pomdps
function approximation fisher h ed proceedings fourteenth international
conference machine learning icml pp
kimura h yamamura kobayashi reinforcement learning stochastic hill
climbing discounted reward proceedings twelfth international conference
machine learning icml pp
konda v r tsitsiklis j n actor critic neural information processing
systems mit press
lancaster p tismenetsky theory matrices academic press san diego ca
marbach p tsitsiklis j n simulation optimization markov reward processes tech rep mit
meuleau n peshkin l kaelbling l p kim k e policy policy search tech
rep mit artificical intelligence laboratory
meuleau n peshkin l kim k e kaelbling l p learning finite state controllers
partially observable environments proceedings fifteenth international conference
uncertainty artificial intelligence
peshkin l kim k e meuleau n kaelbling l p learning cooperate via policy
search proceedings sixteenth international conference uncertainty artificial
intelligence
reiman weiss sensitivity analysis via likelihood ratios proceedings
winter simulation conference
reiman weiss sensitivity analysis simulations via likelihood ratios operations
rubinstein r monte carlo optimization ph thesis
rubinstein r optimize complex stochastic systems single sample path
score function method annals operations
rubinstein r decomposable score function estimators sensitivity analysis optimization queueing networks annals operations
rubinstein r melamed b modern simulation modeling wiley york
rubinstein r shapiro discrete event systems wiley york
samuel l studies machine learning game checkers ibm
journal development


fibaxter bartlett

shilov g e gurevich b l integral measure derivative unified
prentice hall englewood cliffs n j
singh p jaakkola jordan learning without state estimation partially
observable markovian decision processes proceedings eleventh international
conference machine learning
singh bertsekas reinforcement learning dynamic channel allocation cellular telephone systems advances neural information processing systems proceedings
conference pp mit press
smallwood r sondik e j optimal control partially observable markov
decision processes finite horizon operations
sondik e j optimal control partially observable markov decision processes
infinite horizon discounted costs operations
sutton r barto g reinforcement learning introduction mit press
cambridge isbn
sutton r mcallester singh mansour policy gradient methods
reinforcement learning function approximation neural information processing
systems mit press
tao n baxter j weaver l multi agent policy gradient network
routing tech rep australian national university
tesauro g practical issues temporal difference learning machine learning

tesauro g td gammon self teaching backgammon program achieves master level
play neural computation
tsitsikilis j n van roy b analysis temporal difference learning function approximation ieee transactions automatic control
williams r j simple statistical gradient following connectionist reinforcement learning machine learning
zhang w dietterich reinforcement learning job shop scheduling
proceedings fourteenth international joint conference artificial intelligence pp
morgan kaufmann




