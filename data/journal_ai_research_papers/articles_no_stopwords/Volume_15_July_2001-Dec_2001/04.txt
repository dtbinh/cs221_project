Journal Articial Intelligence Research 15 (2001) 163-187

Submitted 10/00; published 09/01

Analysis Reduced Error Pruning
elomaa@cs.helsinki.fi
matti.kaariainen@cs.helsinki.fi

Tapio Elomaa
Matti Kriinen
Department Computer Science
P. O. Box 26 (Teollisuuskatu 23)
FIN-00014 University Helsinki, Finland

Abstract

Top-down induction decision trees observed suer inadequate
functioning pruning phase. particular, known size resulting
tree grows linearly sample size, even though accuracy tree
improve. Reduced Error Pruning algorithm used representative
technique attempts explain problems decision tree learning.
paper present analyses Reduced Error Pruning three dierent settings.
First study basic algorithmic properties method, properties hold independent input decision tree pruning examples. examine situation
intuitively lead subtree consideration replaced leaf node,
one class label attribute values pruning examples independent
other. analysis conducted two dierent assumptions. general
analysis shows pruning probability node tting pure noise bounded
function decreases exponentially size tree grows. specic analysis
assume examples distributed uniformly tree. assumption lets
us approximate number subtrees pruned receive
pruning examples.
paper claries dierent variants Reduced Error Pruning algorithm,
brings new insight algorithmic properties, analyses algorithm less imposed
assumptions before, includes previously overlooked empty subtrees
analysis.
1. Introduction
Decision tree learning usually two-phase process (Breiman, Friedman, Olshen, & Stone,
1984; Quinlan, 1993).

training examples

First tree reecting given sample faithfully possible

constructed. noise prevails, accuracy tree perfect

used build tree. practice, however, data tends noisy,
may introduce contradicting examples training set.

overtted

necessarily obtained even training set.


Hence, 100% accuracy cannot

case, resulting decision tree

sample; addition general trends data, encodes

pruned

peculiarities particularities training data, makes poor predictor
class label future instances. second phase induction, decision tree

order reduce dependency training data. Pruning aims removing
tree parts likely due chance properties training set.
problems two-phased top-down induction decision trees well-known
extensively reported (Catlett, 1991; Oates & Jensen, 1997, 1998). size

c 2001 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiElomaa & Kriinen
tree grows linearly size training set, even though
accuracy gained increased tree complexity. Obviously, pruning intended
ght eect. Another defect observed data contains relevant attributes;
i.e., class labels examples independent attribute values. Clearly,
single-node tree predicting majority label examples result case,
since help obtained querying attribute values. practice, though, often
large decision trees built data.
Many alternative pruning schemes exist (Mingers, 1989a; Esposito, Malerba, & Semeraro, 1997; Frank, 2000).

pruning examples

dier, e.g., whether single pruned tree series

pruned trees produced, whether separate set

used, aspects

(classication error tree complexity) taken account pruning decisions,
aspects determined, whether single scan tree suces whether
iterative processing required.

basic pruning operation applied tree

replacement internal node together subtree rooted leaf.
elaborated tree restructuring operations used pruning techniques

majority leaf
pruning tree

(Quinlan, 1987, 1993). paper, pruning operation considered
replacement subtree
examples reaching it. Hence,

, i.e., leaf labeled majority class
subtree original tree

zero, one, internal nodes changed leaves.

Reduced Error Pruning

(subsequently

rep short) introduced Quinlan (1987)

context decision tree learning. subsequently adapted rule set learning
well (Pagallo & Haussler, 1990; Cohen, 1993).
practical decision tree pruning

overprunes

rep

rep one simplest pruning strategies.

seldom used, disadvantage

requiring separate set examples pruning. Moreover, considered aggressive
pruning strategy

decision tree, deleting relevant parts (Quinlan,

1987; Esposito et al., 1997). need pruning set often considered harmful
scarceness data. However, data mining context examples often
abundant setting part aside pruning purposes presents problem.
Despite shortcomings

rep

baseline method performance

pruning algorithms compared (Mingers, 1989a; Esposito, Malerba, & Semeraro, 1993;
Esposito et al., 1997).

presents good starting point understanding strengths

weaknesses two-phased decision tree learning oers insight decision tree
pruning.

rep advantage producing smallest pruning among

accurate respect pruning set. Recently, Oates Jensen (1999) analyzed

rep attempt explain decision tree pruning fails control growth

tree, even though data warrant increased size. approach
subject, try avoid restricting analysis unnecessary assumptions.



consider explanation unwarranted growth size decision tree.

rep three dierent settings. First, explore basic algorep, apply regardless distribution examples presented

paper analyze
rithmic properties

learning algorithm. Second, study, probabilistic setting, situation
attribute values independent classication example. Even though
pure noise tting situation expected arise whole pruning set considered,
encountered lower levels tree, relevant attributes already
exhausted. assume subtrees receive least one pruning example,

164

fiAn Analysis Reduced Error Pruning
none directly pruned due receiving examples. class value
assigned random pruning examples. third analysis assumed
pruning example equal chance end one subtrees tree
pruned. rather theoretical setting lets us take account subtrees
receive examples. left without attention earlier analyses.
rest paper organized follows. next section discusses dierent
versions

rep

algorithm xes one analyzed subsequently. Section

3 review earlier analyses

rep.

Basic algorithmic properties

Section 4. Then, Section 5, carry probabilistic analysis
assumptions distribution examples.

rep examined
rep, without making

derive bound pruning

probability tree depends exponentially relation number pruning
examples size tree. Section 6 presents analysis, assumes
pruning examples distribute uniformly subtrees tree. assumption lets us
sharpen preceding analysis certain aspects. However, bounds Section 5 hold
certainty, Section 6 approximate results. related research
briey reviewed Section 7 and, nally, Section 8 present concluding remarks
study.

2. Reduced Error Pruning Algorithm

rep

never introduced algorithmically Quinlan (1987), source much

confusion. Even though

rep

considered appears simple, almost trivial,

algorithm pruning, many dierent algorithms go name.
consensus exists whether

rep bottom-up algorithm

iterative method. Neither

obvious whether training set pruning set used decide labels leaves
result pruning.

2.1 High-Level Control
Quinlan's (1987, p. 225226) original description

rep clearly specify pruning

algorithm leaves room interpretation. includes, e.g., following characterizations.
every non-leaf subtree







examine change misclassications

test set would occur



replaced best possible leaf.

new tree would give equal fewer number errors
subtree property,

replaced leaf.

contains

process continues

replacements would increase number errors test
set.
[...] nal tree accurate subtree original tree respect
test set smallest tree accuracy.
Quinlan (1987, p. 227) later continues give following description.
method [pessimistic pruning] two advantages. much faster
either preceding methods [cost-complexity reduced error pruning]
since subtree examined once.

165

fiElomaa & Kriinen
one hand description requires nodes processed bottom-up manner,
since subtrees must checked property pruning node but,
hand, last quotation would indicate

rep

rep

iterative method.

take

following single-scan bottom-up control strategy studies

(Oates & Jensen, 1997, 1998, 1999; Esposito et al., 1993, 1997; Kearns & Mansour, 1998).
Nodes pruned single bottom-up sweep decision tree, pruning node considered encountered.

nodes processed

postorder.
order node processing, tree candidate pruning cannot
contain subtree could still pruned without increasing tree's error.
Due ambiguity
1989a; Mitchell, 1997).

rep's denition, dierent version rep lives (Mingers,

probably due Mingers' (1989) interpretation Quinlan's

ambiguous denition.
Nodes pruned iteratively, always choosing node whose removal
increases decision tree accuracy pruning set. process continues
pruning harmful.
However, algorithm appears incorrect. Esposito et al. (1993, 1997) shown
tree produced algorithm meet objective accurate
subtree respect pruning set.

Moreover, algorithm overlooks explicit

requirement checking whether subtree would lead reduction classication
error.
iterative algorithms could induced Quinlan's original description. However, explicit requirement checking whether subtree could pruned pruning supertree obeyed, versions

rep

reduce ecient

bottom-up algorithm.

2.2 Leaf Labeling
Another source confusion Quinlan's (1987) description

rep

clearly

specied choose labels leaves introduced tree

training

pruning. Oates Jensen (1999) interpreted intended algorithm would label
new leaves according majority class

pruning

examples, analyzed

version algorithm new leaves obtain labels majority
examples. Oates Jensen motivated choice empirical observation

practice little dierence choosing leaf labels either way.
However, choosing labels pruned leaves according majority pruning examples
set leaves dierent status original leaves, label
majority class training examples.

Example

Figure 1 shows decision tree pruned single leaf

training examples used label pruned leaves. negative leaf replaces root
tree makes two mistakes pruning examples, original tree makes three
mistakes.

tree illustrate important dierence using training

166

fiAn Analysis Reduced Error Pruning





+



+



+

0/1

0/1

2/0

2/0

Figure 1: (part a) decision tree. labels inside nodes denote majority classes
training examples arriving nodes. leaves numbers pruning
examples two classes given.

x=y

means

x negative





positive instances reach leaf.

pruning examples label pruned leaves. Using training examples proceeding bottomup, observe neither subtree pruned, since left one replaced negative leaf
would make two mistakes instead original one mistake. Similarly, right subtree
replaced positive leaf would result increased number classication errors.
Nevertheless, root node even though subtrees pruned still
pruned.
pruning examples used label pruned leaves, node two non-trivial
subtrees cannot pruned unless subtrees collapsed leaves.

next

section prove this. tree Figure 1 subtrees would collapsed zeroerror leaves. However, case root node pruned.

possibility labeling leaf nodes would take training
pruning examples account deciding label pruned leaf.

Depending

relation numbers training pruning examples strategy resembles one
above-described approaches. Usually training examples numerous
pruning examples, thus dominate. practice impossible discern
labeling strategy using majority training examples.

2.3 Empty Subtrees
Since

rep

uses dierent sets examples construct prune decision tree,

possible parts tree receive examples pruning phase.
parts decision tree, naturally, replaced single leaf without changing
number classication errors tree makes pruning examples.
words, subtrees obtain pruning examples always pruned. Quinlan (1987)
already noted parts original tree correspond rarer special cases,
represented pruning set, may excised.

167

fiElomaa & Kriinen
DecisionTree REP( DecisionTree T, ExampleArray )
{ ( = 0 S.length-1 ) classify( T, S[i] );
return prune( ); }
void classify( DecisionTree T, Example e )
{ T.total++; ( e.label == 1 ) T.pos++;
// update node counters
( !leaf(T) )
( T.test(e) == 0 ) classify( T.left, e );
else classify( T.right, e ); }
int prune( DecisionTree ) // Output classification error pruning
{ ( leaf(T) )
( T.label == 1 ) return T.total - T.pos;
else return T.pos;
else
{ error = prune( T.left ) + prune( T.right );
( error < min( T.pos, T.total - T.pos ) )
return error;
else
{ replace leaf;
( T.pos > T.total - T.pos )
{ T.label = 1; return T.total - T.pos; }
else
{ T.label = 0; return T.pos; } } } }
Table 1:

rep

algorithm. algorithm rst classies pruning examples top-

pass using method
tree using method

prune.

classify

bottom-up pass prunes

Intuitively, clear best-founded strategy handling

empty subtrees

,

receive examples. one hand obtain support training
set, usually numerous pruning set but, hand, fact
pruning example corresponds parts tree would justify drawing
conclusion parts decision tree built chance properties training
data.

rep,

consistently preferring smaller prunings otherwise, latter view

adopted.
problem empty subtrees connected problem
learning algorithms (Holte, Acker, & Porter, 1989).
number training examples.

small disjuncts

machine

small disjunct covers small

Collectively small disjuncts responsible

small number classication decisions, accumulate error whole
concept. Nevertheless, small disjuncts cannot eliminated altogether, without adversely
aecting disjuncts concept.

168

fiAn Analysis Reduced Error Pruning
2.4 Analyzed Pruning Algorithm
Let us briey reiterate details

rep algorithm analyzed subsequently.

al-

ready stated, control strategy algorithm single-sweep bottom-up processing.
First, top-down traversal drives pruning examples tree appropriate
leaves. counters nodes en route updated. Second, bottom-up traversal pruning operations indicated classication errors executed.

errors

determined basis node counter values. bottom-up traversal
node visited once. pruned leaves labeled majority pruning set
(see Table 1).

3. Previous Work
Pruning decision trees recently received lot analytical attention; existing pruning
methods analyzed (Esposito et al., 1993, 1997; Oates & Jensen, 1997, 1998,
1999) new analytically-founded pruning techniques developed (Helmbold &
Schapire, 1997; Pereira & Singer, 1999; Mansour, 1997; Kearns & Mansour, 1998).



many empirical comparisons pruning appeared (Mingers, 1989a; Malerba, Esposito,
& Semeraro, 1996; Frank, 2000). section review earlier work concerns

rep

algorithm. related research considered Section 7.

Esposito et al. (1993) viewed
search process state space.

rep

algorithm, among pruning methods,

addition noting iterative version

rep

cannot produce optimal result required Quinlan (1987), observed even
though
tree

rep linear-time algorithm size tree, respect height
rep requires exponential time worst case. subsequent comparative

analysis Esposito et al. (1997) sketched proof Quinlan's (1987) claim pruning
produced

rep

tree.
bias

smallest among accurate prunings given decision

rep briey examined Oates Jensen (1997, 1998).

observed

rL , best majority leaf could replace subtree depends
(the class distribution ) examples reach root N . words, tree
structure N decides error rL . Let rT denote error subtree
moment pruning sweep reaches N ; i.e., pruning may already
taken place . pruning operations performed led either rT decrease
initial situation stay unchanged. case, pruning taken place
potentially decreases rT , aect rL . Hence, probability rT < rL i.e.,
pruned increases pruning . error propagation bias
error,

inherent

rep.

Oates Jensen (1997, 1998) conjecture larger original

tree smaller pruning set, larger eect, large tree provides
pruning opportunities high variance small pruning set oers random
chances

rL rT .

Subsequently study eects exactly.

follow-up study Oates Jensen (1999) used

rep

vehicle explaining

problems observed pruning phase top-down induction decision
trees. analyzed

rep situation

decision node consideration ts

noise i.e., class examples independent value attribute tested
node hand built statistical model

169

rep

situation. indicates,

fiElomaa & Kriinen
consistently earlier considerations, even though probability pruning
node ts noise prior pruning beneath close 1, pruning occurs beneath
node reduces pruning probability close 0. particular, model shows even
one descendant node

N

depth

pruned, N

pruned (assuming

+1). consequence result increasing depth
leads exponential decrease node's pruning probability.
leaves depth

rst part Oates Jensen's (1999) analysis easy comprehend, significance uncertain, situation rise bottom-up pruning strategy.
statistical model based assumption number,

n, pruning instances

pass node consideration large, case independence assumptions prevailing errors committed node approximated normal
distribution. expected error original tree mean distribution, while,
pruned leaf, tree would misclassify proportion

n examples corresponds

minority class. Oates Jensen show latter number always less
mean standard distribution errors. Hence, probability pruning
0.5 approaches 1

n grows.

second part analysis, considering pruning probability node

N

pruning taken place beneath it, Oates Jensen assume proportion
positive examples descendant
assuming

N

N

depth

N .

setting,


pruned,

positive majority, descendants level

positive majority. directly follows descendants level

replaced positive leaf. Hence, function represented pruning identically
positive. majority leaf would replace
smaller pruning. Therefore,

N

represents function

rep choose single leaf pruning.



N depth pruned,
N , subtrees maintained nodes

hand, one descendants
pruning tree rooted
level



pruned positive leaves, accurate majority leaf.

case tree pruned.
Oates Jensen (1999) assume starting node level

proba-

bility routing example positive leaf same. following analyses try
rid unnecessary assumptions; results obtained without knowledge
example distribution.

4. Basic Properties

rep

going detailed probabilistic analysis
basic algorithmic properties.

rep

algorithm, examine

Throughout paper review binary case

simplicity. results, however, apply many-valued attributes several classes.
processing control

rep

algorithm settled, actually

prove Quinlan's (1987) claim optimality pruning produced

rep.

Observe

following result holds true independent leaf labeling strategy.

Theorem 1 Applying rep set pruning examples, , decision tree produces
0 pruning smallest prunings minimal
error respect example set S.
170

fiAn Analysis Reduced Error Pruning
Proof prove claim induction size tree. Observe decision
full binary tree, 2L(T ) 1 nodes, L(T ) number leaves

tree

tree.

Base case

.

L(T ) = 1,



original tree



consists single leaf node.





possible pruning itself. Thus, is, trivially, smallest among
accurate prunings

T.

Inductive hypothesis
Inductive step L(T ) = k

. claim holds

.

. Let

N

right subtree, respectively. Subtrees
pruning decision

N

L(T ) < k.

T1

left

must strictly less



root tree

T0



T1

prunings trees.

T00



T10 ,

k leaves.

inductive hypothesis,

smallest possible among accurate

(i): Accuracy
N



taken, bottom-up recursive control strategy

rep T0 T1 already processed algorithm.
subtrees pruning,

T0

. pruning decision node

N consists choosing whether collapse

tree rooted majority leaf, whether maintain whole

tree. alternatives make number errors,

N

collapsed

original accuracy respect pruning set retained. Otherwise,

rep

algorithm, pruning decision based resulting trees would make

. Hence, whichever choice made,
0 make smaller number errors respect .
00
Let us assume pruning makes even less errors respect
0
00
00
00
. must consist root N two subtrees T0 T1 ,
0
00
majority leaf cannot accurate . Since accurate pruning
0 , must either T000 accurate pruning T0 T00 T100
0
accurate pruning T1 T1 . inductive hypothesis possibilities
0
false. Therefore, accurate pruning .

less errors respect pruning set
resulting tree

(ii): Size
0


. see chosen alternative small possible, rst assume

consists single leaf. tree smallest pruning

claim follows.

T00 T10 .

0
Otherwise, consists root node N

, case

two pruned subtrees

Since tree collapsed, tree must accurate

tree consisting single majority leaf. assume exists pruning



0 , smaller. majority leaf less accurate
0



, must consist root node N two subtrees T0 T1 . Then, either
T0 smaller pruning T0 T00 , accurate, T1 smaller pruning
T1 T10 , accurate. cases contradict inductive hypothesis. Hence,
0 smallest among accurate prunings .




accurate

Thus, case, claim follows

T.

2

consider next situation internal node tree, bottom-up
pruning sweep reaches node.

committed leaf labeling

majority pruning examples.

171

fiElomaa & Kriinen

internal node, prior pruning leaves children,
pruned rep non-trivial subtree bottom-up pruning sweep reaches
it.
Theorem 2
Proof

internal node

N

two possible cases non-trivial

subtrees; either subtrees non-trivial (non-leaf ) one trivial. Let us
review cases.
Let

rT

denote error (sub)tree



respect part pruning set

. rL denote misclassication rate majority leaf L
, chosen pruned.

reaches root
would replace

Case I: Let two subtrees

, T0 T1 , non-trivial.

Hence,

rT0 < rL0 rT1 < rL1 ,
T0 T1 , respectively,
pruned. rT = rT0 + rT1 , must rT < rL0 + rL1 .
T0 T1 majority class, majority class .
rL = rL0 + rL1 , L majority leaf corresponding . Otherwise,
rL rL0 + rL1 . case, rL rL0 + rL1 . Combining fact
rT < rL0 + rL1 means rT < rL . Hence, pruned.

retained pruning sweep passed them. Thus,

L0



Case II: Let





L1

majority leaves would replace

one trivial subtree, produced pruning, one non-

T0 non-trivial L1
T1 pruning process. Then, rT0 < rL0 . Hence,
rT = rT0 + rL1 < rL0 + rL1 .
way Case I, deduce rL rL0 + rL1 . Therefore,
rT < rL retained pruned tree.

trivial subtree. assume, without loss generality,
majority leaf replaced



cannot pruned either case, pruning process stopped branch



containing
node

unless original leaf appears along path root

N

2

T.

original leaf, may pruned even subtree

non-trivial.

N

N

two trivial subtrees, may pruned. Whether pruning

takes place depends class distribution examples reaching

N

subtrees.

analysis Oates Jensen (1999) shown prerequisite pruning
node

N

tree descendants depth



pruned.

depth rst (original) leaf subtree rooted
result situation, corroborate nding
descendants depth

retained.

N

N.





apply

pruned one

Applying Theorem 2 recursively gives

result.

tree rooted node N retained rep one
descendants N depth pruned.
Corollary 3

avoid analysis restricted leaf globally closest root, need

fringe

able consider set leaves closest root branches tree. Let us
dene

decision tree contains node prior pruning leaf

172

fiAn Analysis Reduced Error Pruning

Figure 2: fringe (black gray nodes), interior (white nodes), safe nodes
(black ones) decision tree. triangles denote non-trivial subtrees.

child. Furthermore, node subtree rooted node belonging

interior

Safe nodes

fringe tree fringe. nodes belonging fringe make
tree.

belong fringe tree,

parent interior tree (see Figure 2). fringe decision tree closed
downwards, safe nodes tree correspond leaves pruning it. Observe
along path root safe node leaves. Therefore,
pruning process ever reaches safe node, Theorem 2 applies corresponding branch
on.
decision tree consideration pruned single majority leaf, safe
nodes need turned leaves point, necessarily simultaneously.
pruning sweep continues safe nodes, question whether node
pruned settled solely basis whether nodes path root
majority class. pruning whole tree characterized below.
Let



tree pruned

set pruning examples, jS j = n.

assume,

without loss generality, least half pruning examples positive. Let
proportion positive examples

; p 0:5.





p

replaced majority

leaf, leaf would positive class label. assumptions prove
following.

tree pruned single leaf
subtrees rooted safe nodes pruned
least many positive negative pruning examples reach safe node .

Theorem 4

173

fiElomaa & Kriinen
Proof

begin show two conditions necessary pruning

show former condition fullled,
leaf. Second, prove neither
latter not.
hold,





T.

First,

cannot pruned single

pruned former condition holds,

Third, show suciency conditions; i.e., prove



pruned single leaf.



(i): Let us rst assume

safe node

denition safe node, parent
Therefore, Theorem 2,
neither root



P

P



N

N

pruned.

originally leaves children.

pruned.

easy see, inductively,

pruned.

(ii): Let us assume subtrees rooted safe nodes get pruned
one safe nodes



negative positive pruning examples

fall. Observe safe nodes cannot such. Let us consider pruning



leaves situated place safe nodes; leaves receive
examples original safe nodes.

safe nodes internal nodes,

rep

corresponding pruned leaves labeled majority pruning examples.
particular, safe nodes receive negative positive examples
replaced negative leaves. leaves labeled positive. pruning
original tree accurate majority leaf. Hence, Theorem 1,
prune



rep



single-leaf tree.

(iii): Let us assume subtrees rooted safe nodes



pruned

least many positive negative pruning examples reach safe node.
interior nodes must majority positive pruning examples. Otherwise,

negative positive examples. Thus,
N majority negative examples. Carrying
induction way safe nodes shows node N exist .
Hence, interior prunings represent function (identically positive)
error respect . majority leaf unique,

interior node

N



least one children

smallest prunings will, Theorem 1, chosen.

2
5. Probabilistic Analysis

rep

Let us turn attention question prerequisites pruning decision
tree



single majority leaf are. Since, Theorem 1,

rep

produces pruning



accurate respect pruning set small
possible, show



reduce single leaf suces nd pruning

better prediction accuracy pruning examples majority leaf has.
following class example assumed independent attribute
values. Obviously, decision tree node assumption holds
examples arriving it, would pruning algorithm turn majority leaf.
make assumptions decision tree. However, similar analysis
Oates Jensen (1999), obtained bounds tight, shortest path
root tree leaf short.

174

fiAn Analysis Reduced Error Pruning
5.1 Probability Theoretical Preliminaries
Let us recall basic probabilistic concepts results used subsequently.
denote probability event

p

X
X B (n; p),

(integer-valued) random variable
, denoted

E



PrfE g

EE .

binomially distributed parameters n
expectation

said

discrete

!

n k
Prf X = k g =
p (1 p)n k ; k = 0; 1; : : : ; n:
k


X B (n; p), expected
p value mean EX = = np, variance varX = np(1 p),
= np(1 p).

indicator variable

standard deviation


1. indicator variable


A1 ; : : : ;

discrete random variable takes values 0



used denote occurrence non-occurrence event.

Pn

independent events

X=

IA

PrfAi g = p IA1 ; : : : ; IA

n

respective

i=1
Bernoulli
p
density function fX : ! [0; 1]
X
fX (x) = Prf X = x g
cumulative
distribution
function
F
:

!
[0; 1]
X
P
indicator variables,

IA



called



binomially distributed parameters

random variable parameter



discrete random variable

.

n p.

.



dened

FX (y) = Prf X g = xy fX (x).
Let X B (n; p) random variable mean = np standard
p
= np(1 p). normalized random variable corresponding X



X



dened

X

Xe =





deviation

:

central limit theorem approximate cumulative distribution function

e
X



normal Gaussian distribution


n

FXe (y) = Pr Xe



(y):

cumulative distribution function bell curve density function e
Respectively, apply
able

X

normal approximation

FX (y) = Prf X g = FXe

x2 =2 =

FXe

p

2 .

corresponding random vari-





:



5.2 Bounding Pruning Probability Tree
Now, pruning set considered sample distribution class
attribute independent attributes.

assume class attribute

(p) distribution; i.e., class positive probability
p negative probability 1 p. assume p > 0:5.
distributed according Bernoulli

following analyze situation subtrees rooted safe nodes
already pruned leaves. bound pruning probability tree starting
initial conguration.

Since bottom-up pruning may already come

halt situation, following results actually give high probability
pruning. Hence, following upper bounds tight possible.

175

fiElomaa & Kriinen
consider pruning decision tree

rep

trial whose result decided set



pruning examples. Theorem 4 approximate probability tree
pruned majority leaf approximating probability

safe nodes get

positive majority negative majority. latter alternative probable
assumption

p > :5.

safe assume never happens.

consider sampling pruning examples two phases. First attribute values
assigned.

decides leaf example falls.

second phase

independently assign class label example.

Z (T ) = fz1 ; : : : ; zk g let number examples
pruning set jS j = n. number pruning examples falling safe node zi
Pk
denoted ni ;
i=1 ni = n. time assume ni > 0 i. number
positive examples falling safe node zi sum independent Bernoulli variables
and, thus, binomially distributed parameters ni p. Respectively, number
negative pruning examples safe node zi Xi B (ni ; 1
p). probability
majority negative examples safe node zi Prf Xi > ni =2 g. bound
Let safe nodes tree





probability using following inequality (Slud, 1977).

Lemma 5 (Slud's inequality)
m(1 q) h mq,

Let X B(m; q) random variable q 1=2.
!

h mq
:
Prf X h g 1 p
mq(1 q)
p > :5 random variable corresponding number negative examples
zi Xi B (ni; 1 p), rst condition Slud's inequality holds. Furthermore,
see condition m(1
q) h mq holds safe node zi substitute h = ni =2, = ni ,
q = 1
p obtain ni p ni=2 ni(1 p). Thus,
Since

safe node



Pr Xi >


ni

2

!

!

=2 ni(1 p)
= 1 p(p 1=2)ni :
1 nip
ni p(1 p)
ni p(1 p)

(1)

ni , number pruning instances reaching safe node zi , grows, standard

normal distribution term bound grows. Hence, bound probability
majority pruning examples reaching

zi

negative smaller

pruning examples reach it. probability negative majority reduces
growing probability positive class example,

p.

reected

pruning probabilities whole tree.
roughly approximate probability
majority leaf follows. Theorem 4,
node








pruned single

pruned leaf safe

receives majority positive examples.





k

safe nodes

n pruning examples, according pigeon-hole principle least half safe
r = 2n=k examples. safe node zi ni r examples has,

nodes receive

Inequality 1, negative majority least probability

!

1 p(p 1=2)r :
rp(1 p)
176

fiAn Analysis Reduced Error Pruning
Observe Inequality 1 holds

ni < r, becausepthe cumulative

distribution

increasing function. argument ni (p 1=2)= ni p(1 p) canpbe rewritten
p

ni cp , cp ispa positive constant depending value p. Since ( ni cp ) grows
ni grows, 1
( ni cp ) grows decreasing ni . Hence, lower bound Inequality
1 applies values 0 < ni < r .

function

Thus, probability half safe nodes receive

r

examples

positive majority

p(p 1=2)r
rp(1 p)

!!k=2

:

(2)

upper bound probability whole tree



pruned single

leaf. distribution assumption made reach result

p > :5.

order obtain tighter bounds, one make assumptions shape tree
distribution examples.
bound Equation 2 depends size decision tree (reected

n

p





k),

number ( ) class distribution ( ) pruning examples. Keeping parameters
constant letting

k

grow reduces pruning probability exponentially. number

pruning examples grows proportion

r = 2n=k

stays constant,

pruning probability still falls exponentially. Class distribution pruning examples
aects pruning probability smaller, closer

p value .5.

5.3 Implications Analysis
empirically observed size decision tree grows linearly
training set size, even trees pruned (Catlett, 1991; Oates & Jensen, 1997,
1998). analysis gives us possibility explain behavior. However, let us
rst prove correlation attribute values class label
example, size tree perfectly ts training data depends linearly
size sample.
setting simple be. one real-valued attribute
attribute

y, whose value independent



x.

before,



x class

two possible values,

0 1. tree built using binary splits numerical value range; i.e., propositions
type

x < r

assigned internal leaves tree.

analysis duplicate

instances occur probability 0.

Let training examples (x; y) drawn distribution, x uniformly distributed range [0; 1) obtains value 1, independent x, probability
p, value 0 probability 1 p. expected size decision tree ts
data linear size sample.
Theorem 6

= h(x1 ; y1 ); : : : ; (xt ; yt )i sample described distribution.
xi 6= xj , 6= j , probability complement event 0.
Let us, further, assume examples indexed x1 < x2 < : : : < xt .
Let Ai indicator variable event instances + 1 dierent class
labels; i.e., yi 6= yi+1 , 1 1. EAi = Prf Ai = 1 g = p(1 p)+(1 p)p = 2p(1 p),
Proof

Let

may assume

177

fiElomaa & Kriinen
yi = 1 probability p, time event yi+1 = 0
1 p, vice versa. number class alternations = Pti=11 Ai

event
probability

expectation

EA =
Let



1
X
i=1

EAi =

1
X
i=1

2p(1 p) = 2p(1 p)

1
X
i=1

1 = 2(t 1)p(1 p):

decision tree grown sample

continued training error 0. leaf

[a; b) [0; 1).



yi 6= yy+1 ,



xi



xi+1



S.

(3)

growing

corresponds half open interval

must fall dierent leaves

T,



. Thus, upper boundary b
xi falls must value less xi+1 .

otherwise one example falsely classied
interval corresponding leaf

Repetitively applying observation scanning examples left

must least one leaf x1 one leaf class alternation;
+ 1 leaves total. using Equation 3 see expected number leaves

right, see
i.e.,






EA + 1 = 2(t 1)p(1 p) + 1:
particular, linear size sample ; jS j = t.

2

theorem concerns zero training error trees built rst phase
decision tree induction. empirical observations Catlett (1991) Oates Jensen
(1997, 1998), however, concern decision trees pruned second phase
induction. come back topic pruned trees shortly.
Consider

rep used practice.

amount (classied) data available

application domain. Let total

examples available. part
1 reserved

data used tree growing remaining portion
separate pruning set;

0 < < 1.

Quite common practice use two thirds data

growing one third pruning nine tenths growing one tenth pruning
(ten-fold) cross-validation used. decision tree construction phase tree
tted

fft examples perfectly possible.

hypothesize previous result

holds noisy real-world data sets, empirical evidence would appear
case, number safe nodes grows linearly number leaves,
tree grown contain



safe nodes,

> 0. Since pruning set size
r = 2n=k stays constant setting.

linear fraction training set size, ratio

Hence, Equation 2, growing data set size forces pruning probability zero, even
quite fast, reduction probability exponential.

5.4 Limitations Analysis
Empty subtrees, receive pruning examples, left without attention
above; assumed

ni > 0



i.

Empty subtrees, however, decisively aect

analysis; automatically pruned away. Unfortunately, one cannot derive non-trivial
upper bound number empty subtrees. worst case pruning examples
routed safe node, leaves

k 1 empty safe nodes tree.

Subsequently

review case examples distributed uniformly safe nodes.
better approximations obtained.

178

fiAn Analysis Reduced Error Pruning
Even though assume pruning example positive higher probability
.5, guarantees majority examples positive.

However,

probability majority examples changes small, even negligible,
Cherno 's inequality (Cherno, 1952; Hagerup & Rb, 1990) number pruning

n, high p extremely close one half.
Prf X h g, used bound
probability Prf X > h g. continuity correction could used compensate this.
examples,

Slud's inequality bounds probability

practice, inexactness make dierence.
Even though would appear number safe nodes increases proportion leaves size training set grows, proved
result. Theorem 6 essentially uses leaf nodes, lend modication,
safe nodes could substituted place leaves.
relation number safe nodes leaves decision tree depends
shape tree. Hence, splitting criterion used tree growing decisively
aects relation. splitting criteria aim keeping produced split balanced
possible, others aim separating small class coherent subsets data (Quinlan,
1986; Mingers, 1989b). example, common entropy-based criteria bias
favors balanced splits (Breiman, 1996). Using balanced splitting criterion would seem
imply number safe nodes tree depends linearly number leaves
tree.

case reasoning would explain empirically observed linear

growth pruned decision trees.

6. Pruning Probability Uniform Distribution
assume


k

n

pruning examples equal probability end

safe nodes; i.e., pruning example falls safe node

zi

probability

1=k.

Contrary normal uniform distribution assumption analysis, analysis
best case. best distribution examples safe nodes would one pruning
example safe nodes except one, remaining pruning instances
would gather.

Nevertheless, uniformity lets us sharpen general approximation

using standard techniques.

n=k. Let us calculate
cn=k examples, c
event safe node zi receives

expected number examples falling safe node
expected number safe nodes receive

QPi indicator
k Q number safe nodes receive less
i=1
Pk

linearity expectation EQ =
i=1 EQi = kEQ1 ,
last equality follows fact Qi -s identically distributed.
Let Y1 number examples reaching safe node z1 . n examples reaches z1 probability 1=k independent examples, Y1 binomially
distributed parameters n 1=k . Clearly EQ1 = Prf Y1 cn=k g. approxiarbitrary positive constant. Let


cn=k examples.
cn=k examples.



Q=

mate last probability normal approximation, obtain

!
!


cn=k
n=k
(
c
1)
n=k
cn
pn 1=k (1 1=k) = pn=k(1 1=k) :
Pr Y1
k
179

fiElomaa & Kriinen
Hence, observation,

!

(c 1)n=k :
EQ = kEQ1 k p
n=k(1 1=k)


(4)

use Approximation 4 determine probability whole decision tree
pruned single leaf. Let

denote



P

random variable represent number

cn=k examples least one example.
R number empty safe nodes, P = Q R. Hence, EP = E(Q R) =

safe nodes

receive

EQ ER.

following result (Kamath, Motwani, Palem, & Spirakis, 1994; Motwani & Raghavan,
1995) lets us approximate number empty safe nodes

Theorem 7

bins.

n k.

Let Z number empty bins balls thrown randomly h


= EZ = h 1

> 0,

Prf jZ

1

h

j g 2 exp

m=h

!

2 (h 1=2)
:
h2 2

result expected number empty safe nodes approximately
number small

ke

n=k ;



k relatively small compared n.
EQ (Equation 4) using pre-

Substituting obtained approximation
vious result, get

(c 1)n=k
EP = EQ ER k p
n=k(1 1=k)

!

e

n=k

!

:

Applying Slud's inequality can, before, bound probability
majority class change safe node receives
Since

P

cn=k

pruning examples.

safe nodes class distribution examples within

independent, event majority class change safe node receives
least one

cn=k examples

upper bound

p(p :5)r
rp(1 p)


r = cn=k.

Replacing

P

!!P

;

(5)

expected value equation approxi-

mation pruning probability. approximation valid
expected value. consider deviation

P

P

deviate lot

expected value below.

upper bound pruning probability similar upper bound
obtained without assumptions distribution examples. However,
earlier constant 2 replaced new, controllable parameter
explicitly taken account.

c, empty subtrees

c chosen suitably, upper bound strict

one obtained general case.

180

fiAn Analysis Reduced Error Pruning

Upper bound pruning probability
0.5
0.25

1

0.5

0

0.9
0.8
0.7

0.5
1

Figure 3: eect parameters

p

0.6

1.5

c

0.5

p c upper bound pruning probability

tree 100 safe nodes 500 pruning examples used. curves
depicting 0.25 0.5 upper bounds shown.

6.1 Illustration Upper Bound
Figure 3 plots upper bound pruning probability tree 100 safe nodes
500 pruning examples used. value parameter

c varies 0 2 p varies

0.5 1. observe surface corresponding upper bound stays
close 0 class distribution skewed parameter

c

small value. probability example positive class label

c approaches 0, upper bound climbs steeply. least
parameter c due inexactness approximation

hits value 0.75 value
part
extreme values.
probability

p

example positive class approaches 1, error

committed single positive leaf falls 0. Hence, accuracy non-trivial pruning
better, closer

p 1 beat majority leaf.

Intuitively, probability

pruning exists i.e., root node pruned drop zero

p increases.

bound reects intuition.

value parameter

c falls close 0, safe nodes taken account

upper bound receive pruning examples. number nodes

181

fiElomaa & Kriinen
small. hand,

c

increased, number nodes consideration

grows together upper limit number examples reaching single one
them. Thus, small large values
value

c somewhere

c

yield loose bounds. strictest bounds

middle, example around values 1.01.5.

bound Equation 5 argument cumulative distribution function
zero value

c small,

tends towards

time exponent decreases.

approaches 1/2, argument goes zero. hand, c
large value, approaches value 1 exponent P increases.
value

6.2 Exactness Approximation
used expected value P analysis; EP = EQ
ER. probe
deviation P expected value. deviation R directly available
Theorem 7:



!

2 (k 1=2)
:
k2 E2 R

Prf jR ERj g 2 exp

Q similar result yet.

Lipschitz condition

section provide one.

Let us rst recapitulate denition

.

f : D1 Dm ! IR real-valued function arguments
f said satisfy Lipchitz condition
x1 2 D1 ; : : : ; xm 2 Dm , 2 f1; : : : ; mg, yi 2 Di ,
Denition

Let

possibly distinct domains. function

jf (x1 ; : : : ; xi 1 ; xi ; xi+1; : : : ; xm ) f (x1; : : : ; xi 1 ; yi; xi+1 ; : : : ; xm )j 1:

Hence, function satises Lipschitz condition arbitrary change value
one argument change value function 1.

martingales

following result (McDiarmid, 1989) holds functions satisfying Lipschitz condition. general results kind obtained using
(Motwani & Raghavan, 1995)).

(see e.g.,

Theorem 8 (McDiarmid) Let X1 ; : : : ; Xm independent random variables taking values
set V . Let f : V ! IR that, = 1; : : : ; m:

sup

x1 ;:::;xm ;yi 2V

jf (x1; : : : ; xi 1; xi ; xi+1; : : : ; xm ) f (x1; : : : ; xi 1 ; yi; xi+1 ; : : : ; xm )j ci :

> 0,
Prf jf (X1 ; : : : ; Xm ) Ef (X1 ; : : : ; Xm )j g 2 exp

2
P2

c2
i=1

!

:

Wi , = 1; : : : ; n, random variable Wi = j i-th example
directed safe node zj . uniform distribution assumption Wi -s independent.
values within set f1; : : : ; k g. Let us dene function f
f (w1 ; : : : ; wn ) number safe nodes receive r = cn=k examples,
i-th example directed safe node zw . is,
Let



f (w1 ; : : : ; wn ) = jf 2 f 1; : : : ; k g j jSi j r gj;
182

fiAn Analysis Reduced Error Pruning


Si set examples directed safe node zi ;
Si = f h 2 f 1; : : : ; n g j wh = g:
Q = f (W1 ; : : : ; Wn ).

Hence,

Moving one example one safe node another (chang-

ing value one argument
dition


f

Pn

wi ), change one safe node zi

jSij r, one less safe node fulll it, time.

fulll conThus, value

changes 1. Hence, function fullls Lipschitz condition. Therefore,

apply McDiarmid's inequality substituting

c2 = n:

i=1

ci = 1 observing

2
Prf jf (W1 ; : : : ; Wn ) Ef (W1 ; : : : ; Wn )j g 2e 2 =n ;
equally

2
Prf jQ EQj g 2e 2 =n :

Unfortunately, concentration bound tight. Nevertheless, combining
concentration bounds

Q R P

following deviation expected

value.

Since jP
EP j = jQ R E(Q R)j = jQ EQ + ER Rj jQ EQj + jR ERj,
jQ R E(Q R)j implies jQ EQj =2 jR ERj =2. Thus,
Prf jP EP j g = Prf jQ R E(Q R)j g




Pr jQ EQj 2 + Pr jR ERj 2

!

2
2n + 2 exp

2 exp

!

2 (k 1=2)
:
4(k2 E2 R)

7. Related Work
Traditional pruning algorithms cost-complexity pruning (Breiman et al., 1984), pessimistic pruning (Quinlan, 1987), minimum error pruning (Niblett & Bratko, 1986; Cestnik
& Bratko, 1991), critical value pruning (Mingers, 1989a), error-based pruning (Quinlan,
1993) already covered extensively earlier work (Mingers, 1989a; Esposito
et al., 1997; Frank, 2000). Thus touch methods further. Instead,
review recent work pruning.

rep

produces optimal pruning given decision tree respect pruning

set. approaches producing optimal prunings presented (Breiman
et al., 1984; Bohanec & Bratko, 1994; Oliver & Hand, 1995; Almuallim, 1996). However,
often optimality measured training set. possible maintain
initial accuracy, assuming noise present. Neither usually possible reduce
size decision tree without sacricing classication accuracy. example,
work Bohanec Bratko (1994) studied eciently nd optimal
pruning sense output decision tree smallest pruning satises
given accuracy requirement. somewhat improved algorithm problem
presented subsequently Almuallim (1996).

183

fiElomaa & Kriinen
high level control Kearns Mansour's (1998) pruning algorithm

cost-complexity

bottom-up sweep

rep.

However, pruning criterion method kind

condition (Breiman et al., 1984) takes observed classication

error (sub)tree complexity account.

Moreover, pruning scheme

pessimistic

require pruning set separate training set. Mansour's (1997)
Kearns Mansour's (1998) algorithms
(sub)tree training error.

: try bound true error

Since training error nature optimistic,

pruning criterion compensate pessimistic error approximation.
Consider yet another variant

rep, one

otherwise similar one analyzed

above, exception original leaves put special status,
relabeled majority pruning examples internal nodes. version


rep

produces optimal pruning respect performance Kearns

Mansour's (1998) algorithm measured. pessimistic pruning produces decision tree
smaller produced

rep.

Kearns Mansour (1998) able prove algorithm strong performance guarantee. generalization error produced pruning bounded
best pruning given tree plus complexity penalty.
local sense

rep

pruning decisions

basic pruning operation replacing

subtree leaf used pruning algorithm.

8. Conclusion
paper

rep

algorithm analyzed three dierent settings.

First,

rep alone, without assuming anything input
setting possible prove rep fullls

studied algorithmic properties
decision tree pruning set.



intended task produces optimal pruning given tree. algorithm proceeds
prune nodes branch long subtrees internal node pruned
stops immediately even one subtree kept. Moreover, prunes interior node
descendants level



pruned. Furthermore,

rep

either halts

safe nodes reached prunes whole tree case safe nodes
majority class.
second setting tree consideration assumed noise; i.e.,
assumed class label pruning examples independent attribute
values. setting pruning probability tree could bound equation
depends exponentially size tree linearly number class
distribution pruning examples. Thus, analysis corroborates main nding
Oates Jensen (1999)

rep fails control growth decision tree extreme

case tree ts pure noise. Moreover, analysis opened possibility initially
explain learned decision tree grows linearly increasing data set. bound
pruning probability tree based bounding probability safe nodes
majority class. Surprisingly, essentially property, whose probability
try bound close 0, assumed hold probability 1 analysis Oates
Jensen (1999).


rep

may happen pruning examples directed given subtree.

subtrees taken account earlier analyses.

184

nal analysis

fiAn Analysis Reduced Error Pruning
included empty subtrees equation tree's pruning probability.

Taking empty

subtrees account gives realistic bound pruning probability tree.
Unfortunately, one cannot draw denite general conclusions two-phased topdown induction decision trees basis analyses
bias quite unique among pruning algorithms.

fact

rep algorithm,
rep penalize

size tree, rests classication error pruning examples makes
method sensitive small changes class distribution pruning set. decision
tree pruning algorithms individual characteristics. Therefore, unied analysis
decision tree pruning may impossible.
version

rep,

one allowed relabel original leaves, well, used

performance objective Kearns Mansour's (1998) pruning algorithm.

Thus,

performance pruning algorithms use error size penalty related
use error estimation. version

rep

used Kearns Mansour

analysis based safe nodes applies leaves place safe nodes. Hence
algorithm derived bounds stricter.
leave detailed analysis important pruning algorithms future work.
investigation possible disclose dierences similarities
pruning algorithms.

Empirical examination managed reveal clear performance

dierences methods.

Also, relationship number safe nodes

leaves tree ought examined analytically empirically. particular, one
study whether number safe nodes increase linearly growing training set,
conjectured paper. Deeper understanding existing pruning algorithms may help
overcome problems associated pruning phase decision tree learning.

References

Intelligence 83

Almuallim, H. (1996). ecient algorithm optimal pruning decision trees.
,

Learning 15

, 347362.

Bohanec, M., & Bratko, I. (1994). Trading accuracy simplicity decision trees.
,

(3), 223250.

Regression Trees

Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. (1984).
. Wadsworth, Pacic Grove, CA.

tional Joint Conference Articial Intelligence

Machine

Classication

Machine Learning 24
Proceedings Twelfth Interna-

Breiman, L. (1996). properties splitting criteria.
Catlett, J. (1991). Overpruning large decision trees.

Articial

,

(1), 4147.

, pp. 764769, San Mateo, CA. Morgan

Kaufmann.

Machine LearningEWSL-91: Proceedings Fifth European Working
Lecture Notes Articial Intelligence

Cestnik, B., & Bratko, I. (1991). estimating probabilities tree pruning. Kodrato,

Session

Y. (Ed.),

, Vol. 482

, pp. 138150, Berlin, Hei-

delberg, New York. Springer-Verlag.

Annals Mathematical Statistics 23

Cherno, H. (1952). measure asymptotic eciency tests hypothesis based
sum observations.

,

185

(4), 493507.

fiElomaa & Kriinen

Proceedings Thirteenth International Joint Conference Articial

Cohen, W. W. (1993).

Intelligence

systems.

Ecient pruning methods separate-and-conquer rule learning

, pp. 988994, San Mateo, CA. Morgan Kaufmann.

Machine Learning: ECML-93, Proceedings
Lecture Notes Articial Intelligence

Esposito, F., Malerba, D., & Semeraro, G. (1993).

Sixth European Conference

state space. Brazdil, P. B. (Ed.),

Decision tree pruning search

, Vol. 667

, pp.

165184, Berlin, Heidelberg, New York. Springer-Verlag.

IEEE Transactions Pattern Analysis Machine Intelligence 19
Pruning Decision Trees Lists
Information Processing
Letters 33
Machine Learning 27
Proceedings Eleventh International Joint Conference Articial
Intelligence
Proceedings Thirty-Fifth Annual
IEEE Symposium Foundations Computer Science

Esposito, F., Malerba, D., & Semeraro, G. (1997). comparative analysis methods
pruning decision trees.
,

(5), 476491.

Frank, E. (2000).

. Ph.D. thesis, University Waikato,

Department Computer Science, Hamilton, New Zealand.

Hagerup, T., & Rb, C. (1990). guided tour Cherno bounds.
,

(6), 305308.

Helmbold, D. P., & Schapire, R. E. (1997). Predicting nearly well best pruning
decision tree.

,

(1), 5168.

Holte, R. C., Acker, L., & Porter, B. (1989).

Concept learning problem small

disjuncts.

, pp. 813818, San Mateo, CA. Morgan Kaufmann.

Kamath, A., Motwani, R., Palem, K., & Spirakis, P. (1994).

Tail bounds occupancy

satisability threshold conjecture.

, pp. 592603, Los Alamitos,

CA. IEEE Press.

Proceedings Fifteenth Inter-

Kearns, M., & Mansour, Y. (1998). fast, bottom-up decision tree pruning algorithm

national Conference Machine Learning

near-optimal generalization. Shavlik, J. (Ed.),

, pp. 269277, San Francisco, CA. Morgan

Kaufmann.

Learning

Malerba, D., Esposito, F., & Semeraro, G. (1996). comparison simplication

Data: AI Statistics V
Proceedings Fourteenth International Conference Machine Learning

methods decision-tree induction. Fisher, D., & Lenz, H.-J. (Eds.),

, pp. 365374, Berlin, Heidelberg, New York. Springer-Verlag.

Mansour, Y. (1997). Pessimistic decision tree pruning based tree size. Fisher, D. H.
(Ed.),

,

pp. 195201, San Francisco, CA. Morgan Kaufmann.

Surveys Combinatorics: Invited Papers 12th British Combinatorial Conference
Machine Learning 4
Machine Learning 3
Machine Learning

McDiarmid, C. J. H. (1989). method bounded dierences. Siemons, J. (Ed.),
, pp. 148188, Cambridge, U.K. Cambridge University Press.

Mingers, J. (1989a). empirical comparison pruning methods decision tree induction.
,

(2), 227243.

Mingers, J. (1989b). empirical comparison selection measures decision-tree induction.

Mitchell, T. M. (1997).

,

(4), 319342.

. McGraw-Hill, New York.

186

fiAn Analysis Reduced Error Pruning
Motwani, R., & Raghavan, P. (1995).
New York.

Randomized Algorithms

. Cambridge University Press,

Research Development Expert Systems III

Niblett, T., & Bratko, I. (1986). Learning decision rules noisy domains. Bramer, M. A.
(Ed.),

, pp. 2534, Cambridge, UK.

Cambridge University Press.

Proceedings Fourteenth International Conference

Oates, T., & Jensen, D. (1997). eects training set size decision tree complexity.

Machine Learning

Fisher, D. H. (Ed.),

, pp. 254261, San Francisco, CA. Morgan Kaufmann.

Oates, T., & Jensen, D. (1998). Large datasets lead overly complex models: expla-

Proceedings Fourth International Conference Knowledge Discovery Data
Mining
Proceedings Sixteenth National Conference Articial Intelligence
nation solution.

Agrawal, R., Stolorz, P., & Piatetsky-Shapiro, G. (Eds.),

, pp. 294298, Menlo Park, CA. AAAI Press.

Oates, T., & Jensen, D. (1999).

Toward theoretical understanding

decision tree pruning algorithms fail.

, pp. 372378, Menlo Park, CA/Cambridge, MA. AAAI

Press/MIT Press.

Proceedings Twelfth International Conference Machine
Machine

Oliver, J. J., & Hand, D. J. (1995). pruning averaging decision trees. Prieditis, A.,

Learning
Learning 5

& Russell, S. (Eds.),

, pp. 430437, San Francisco, CA. Morgan Kaufmann.

Pagallo, G., & Haussler, D. (1990). Boolean feature discovery empirical learning.
,

(1), 7199.

Machine Learning 36

Pereira, F., & Singer, Y. (1999). ecient extension mixture techniques prediction
decision trees.

,

(3), 183199.

Machine Learning 1
International Journal Man-Machine
C4.5: Programs Machine Learning
Annals Probability

Quinlan, J. R. (1986). Induction decision trees.

Studies 27

Quinlan, J. R. (1987).
,

,

, 81106.

Simplifying decision trees.

(3), 221248.

Quinlan, J. R. (1993).

.

Morgan Kaufmann, San

Slud, E. V. (1977). Distribution inequalities binomial law.

,

Mateo, CA.

5

(3), 404412.

187


