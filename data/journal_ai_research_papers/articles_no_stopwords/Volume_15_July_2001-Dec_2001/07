Journal Artificial Intelligence Research 15 (2001) 383-389

Submitted 6/01; published 11/01

Research Note

Finding Path Harder Finding Tree

Christopher Meek

meek@microsoft.com

Microsoft Research,
Redmond, WA 98052-6399 USA

Abstract

consider problem learning optimal path graphical model data show
problem NP-hard maximum likelihood minimum description length
approaches Bayesian approach. hardness result holds despite fact
problem restriction polynomially solvable problem finding optimal tree
graphical model.
1. Introduction

problem learning graphical models received much attention within Artificial Intelligence community. Graphical models used represent approximate joint
distributions sets variables graphical structure graphical model represents dependencies among set variables. goal learning graphical model
learn graphical structure parameters approximate joint distribution data. note, present negative hardness result learning optimal
path graphical models.
Path graphical models interesting class graphical models respect learning. due fact that, many situations, restricting attention class path
models justified basis physical constraints temporal relationships among
variables. One example problem identifying relative positions loci
segment DNA (e.g., Boehnke, Lange & Cox, 1991). addition, one might interested
obtaining total order set variables purposes visualization
(e.g., & Hellerstein, 1999).
main positive results hardness learning graphical models learning
tree graphical models. presented maximum likelihood (ML) criterion
(Edmonds, 1967; Chow & Liu, 1968) adapted Bayesian criterion Heckerman,
Geiger, & Chickering (1995). Two NP-hardness results learning graphical models
appeared literature. NP-hardness finding optimal Bayesian
network structure in-degree greater equal two using Bayesian optimality
criterion (Chickering, 1996) problem finding ML optimal polytree (Dasgupta,
1999).
note, present proof hardness finding optimal path graphical
models maximum likelihood (ML) criterion, minimum description length (MDL)
criterion, Bayesian scoring criterion. Unlike ML hardness result Dasgupta,
provide explicit construction polynomial sized data set reduction and, unlike
Bayesian hardness result Chickering (1996), use common \uninformative" prior.

c 2001 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiMeek

2. Optimal Graphical Models

One primary goals learning graphical model obtain approximate joint
distribution set variables data. note, focus directed graphical
models set discrete variables fX1 ; : : : ; X g. One component directed graphical
model directed graphical structure describes dependencies variables.
directed graphical model represents family distributions factor according
graphical structure G directed graphical model, specifically,
n

P (X1 ; : : : ; X ) =
G

n

P (X jpa
n

=1



G

(X ))




pa (X ) denotes possibly empty set parents vertex X graph G.
subscript G omitted clear context. common methods guiding
choice distribution family distributions maximum likelihood estimation
Bayesian estimation. Given graphical structure set cases variables
(also prior distribution distributions case Bayesian approach),
methods provide approximate joint distribution. details graphical models
estimation see Heckerman (1998).
leaves open question one choose appropriate graphical structure. remainder section, present maximum likelihood (ML) criterion,
minimum discrimination length (MDL) criterion, Bayesian criterion evaluating
directed graphical models given set cases D. value variable X denoted
x value set variables pa(X ) denoted pa(x ). number cases
X = x pa(X ) = pa(x ) denoted N (x ; pa(x )) total number
cases denoted N .
One important property common scoring criteria scores factor according graphical structure model. is, score graph G data
set written sum local scores variables
G





















Score(G; D) =





X LocalScore(X ; pa(X )):






local score variable X function counts X pa(X )
data set number possible assignments variables X pa(X ). Thus
structure graphical model determines particular variables counts
needed computation local score variable.
log maximum likelihood scoring criterion graphical model


Score

ML

X LocalScore

(G; D) =

ML









(X ; pa(X ))






LocalScore

ML

(X ; pa(X )) = N H (X jpa(X ))










(1)

H (X jpa(X )) empirical conditional entropy X given parents,
equal
N (x ; pa(x )) N (x ; pa(x ))
log
:
N
N (pa(x ))
( )






X













Xi ;pa Xi

384

fiFinding Path Harder Finding Tree

One practical shortcoming ML score comparing two models graphical
structure G G0 G contains proper subset edges G0 ML score
never favor G. Thus, using ML score choose among models without restricting
class graphical structures, fully connected structure guaranteed maximal
score. problematic due potential poor generalization error using
resulting approximation. problem often called overfitting. using principle
best restrict class alternative structures consideration suitable
manner.
minimum description length score viewed penalized version ML
score

Score

DL

(G; D) = Score

log N

(G; D)

X LocalScore
ML

=

2

DL

(G; D)



LocalScore

DL

(X ; pa(X )) =




#(pa(X )) (#(X )
2

LocalScore



ML

P



1) log N

(2)

= (#(pa(X )) (#(X ) 1)) #(Y ) used denote number possible
distinct assignments set variables number assignments empty
set variables #(;) = 1. penalty term leads parsimonious models, thus,
alleviating overfitting problem described above.
Finally, Bayesian score requires prior alternative models and, model,
prior distributions. commonly used family priors directed graphical models described Cooper & Herskovits (1992). approach, one assumes uniform
prior alternative graphs, P (G) / 1, \uninformative" prior distributions.
assumptions lead following scoring function;




Score



Bayes

(G; D) = log P (DjG) + log P (G)
/
LocalScore
(X ; pa(X ))

X

Bayes







LocalScore

Bayes

(X ; pa(X )) =


log





(

pa xi

(#(X ) 1)!
(#(
X
)
1) + N (pa(x )))!
)






N (x ; pa(x ))!




(3)

xi

Although apparent MDL score, Bayesian score built-in
tendency parsimony alleviates problems overfitting. hardness results
presented extended variety alternative types priors including
BDe prior empty prior model (see Heckerman et al. 1995).
problem finding optimal directed graphical model given class structures G data problem finding structure G 2 G maximizes Score(G; D).
385

fiMeek

3. NP-Hardness Finding Optimal Paths

section, consider problem finding optimal directed graphical model
class structures restricted paths. directed graphical structure
path one vertex in-degree zero vertices in-degree one.
show problem finding optimal path directed graphical model NP-hard
commonly used scoring functions described Section 2. demonstrate hardness
finding optimal paths problem needs formulated decision problem.
decision problem version finding optimal path directed graphical model follows
optimal path (OP) decision problem: path graphical model
score greater equal k data set D?
section prove following theorem.
Theorem 1 optimal path problem NP-Hard maximum likelihood score,
minimum description length score Bayesian score.

prove this, reduce Hamiltonian Path (HP) decision problem OP decision
problem.
Hamiltonian path (HP) decision problem: Hamiltonian path
undirected graph G?
Hamiltonian path undirected graph G non-repeating sequence vertices
vertex G occurs path pair adjacent vertices
sequence edge G. Let undirected graph G = hV; E vertex set
V = fX1 ; : : : ; X g edge set E .
HP decision problem NP-complete. Loosely speaking, means HP
decision problem computationally dicult variety problems known
algorithm exists runs time polynomial function size input.
Theorem 1 indicates OP decision problem least dicult NP-complete
problem. information HP decision problem NP-completeness see
Garey & Johnson (1979).
reduce HP decision problem G OP decision problem constructing
set cases following properties;
n

#(X ) = #(X )


(i)

j

LocalScore(X ; ;) = LocalScore(X ; ;) =

(ii)

LocalScore(X ; fX g) 2 fff; g

(iii)





j

ff<fi

j

LocalScore(X ; fX g) = LocalScore(X ; fX g)

(iv)

LocalScore(X ; fX g) = iff fX ; X

(v)

j







j



386

j

j

g2E

fiFinding Path Harder Finding Tree

data set, problem existence Hamiltonian path equivalent
existence path graphical model score equal k = + (jV j 1)
jV j = n number vertices undirected graph G. Thus, reduce
HP problem OP problem one needs eciently construct polynomial sized data
set properties. words, construction, general HP decision
problem transformed OP decision problem. size input
OP problem polynomial function size input HP problem,
one find algorithm solve OP problem polynomial time NP-complete
problems solved polynomial time.
construct data set graph G assuming variable ternary satisfy
condition (i). pair vertices X X (i < j ) edge G,
add following 8 cases every variable X (k 6= i; j ) zero.


j

k

X1 : : : X
0:::0
0:::0
0:::0
0:::0
0:::0
0:::0
0:::0
0:::0



1

X X +1 : : : X
1
0:::0
1
0:::0
1
0:::0
1
0:::0
2
0:::0
2
0:::0
2
0:::0
2
0:::0




j

1

X

j

1
1
1
2
1
2
2
2

X +1 : : : X
0:::0
0:::0
0:::0
0:::0
0:::0
0:::0
0:::0
0:::0
j

n

pair vertices X X (i < j ) edge G, add
following 8 cases.


j

X1 : : : X
0:::0
0:::0
0:::0
0:::0
0:::0
0:::0
0:::0
0:::0



1

X X +1 : : : X
1
0:::0
1
0:::0
1
0:::0
1
0:::0
2
0:::0
2
0:::0
2
0:::0
2
0:::0




j

1

X

j

1
1
2
2
1
1
2
2

X +1 : : : X
0:::0
0:::0
0:::0
0:::0
0:::0
0:::0
0:::0
0:::0
j

n

set cases constructed described above, pairwise counts pair variables
X X connected edge G


j

X



X

j

0
1
2
0 4(n2 5n + 6) 4(n 2) 4(n 2)
1
4(n 2)
3
1
4(n 2)
1
3
2
387

fiMeek

pairwise counts pair variables X X connected edge G


j

X



X

j

0
1
2
0 4(n2 5n + 6) 4(n 2) 4(n 2)
1
4(n 2)
2
2
2
4(n 2)
2
2

Condition (ii) satisfied marginal counts variable identical.
two types pairwise count tables, thus, two values given type
pairwise LocalScore. using two pairwise count tables Equations 1, 2, 3,
one easily verify local scores two tables satisfy condition (iii). follows
symmetry two types pairwise tables condition (ii) condition (iv)
satisfied. follows construction condition (v) satisfied. Furthermore,
set cases eciently constructed size polynomially bounded
size graph G proving result.
4. Conclusion

note, show problem finding optimal path graphical models NPhard variety common learning approaches. negative result learning optimal
path graphical models stands contrast positive result learning tree graphical
models. hardness result highlights one potential source hardness. is,
one make easy problem dicult choosing inappropriate subclass models.
Perhaps, carefully choosing broader class models tree graphical models one
identify interesting classes graphical models problem finding optimal
model tractable.
Another interesting class graphical models described note class
undirected graphical models (e.g., Lauritzen, 1996). methods learning undirected
graphical models closely related methods described Section 2. fact,
case undirected path models, scoring formulas described Section 2 identical
common approaches. Therefore, NP-hardness result directed path
models presented note applies problem learning undirected path models.
Finally, important note good heuristics exist problem finding
weighted Hamiltonian paths (Karp & Held, 1971). heuristics used identify
good quality path models rely fact optimal tree model easily
found score least large path model.
References

Boehnke, M., Lange, K., & Cox, D. (1991). Statistical methods multipoint radiation
hybrid mapping. American Journal Human Genetics, 49, 1174{1188.
Chickering, D. (1996). Learning Bayesian networks NP-complete. Fisher, D., & Lenz,
H. (Eds.), Learning Data, pp. 121{130. Springer-Verlag.
Chow, C., & Liu, C. (1968). Approximating discrete probability distributions dependence trees. IEEE Transactions Information Theory, 14, 462{467.
388

fiFinding Path Harder Finding Tree

Cooper, G., & Herskovits, E. (1992). Bayesian method induction probabilistic
networks data. Machine Learning, 9, 309{347.
Dasgupta, S. (1999). Learning polytrees. Proceedings Fifteenth Conference
Uncertainty Artificial Intelligence, Stockholm, Sweden, pp. 134{141. Morgan Kaufmann.
Edmonds, J. (1967). Optimum branching. J. Res. NBS, 71B, 233{240.
Garey, M., & Johnson, D. (1979). Computers intractability: guide theory
NP-completeness. W.H. Freeman, New York.
Heckerman, D. (1998). tutorial learning Bayesian networks. Jordan, M. (Ed.),
Learning Graphical Models, pp. 301{354. Kluwer Academic Publishers.
Heckerman, D., Geiger, D., & Chickering, D. (1995). Learning Bayesian networks:
combination knowledge statistical data. Machine Learning, 20, 197{243.
Karp, R., & Held, M. (1971). traveling-salesman problem minimum spanning trees:
Part ii. Mathematical Programming, 1, 6{25.
Lauritzen, S. (1996). Graphical Models. Oxford University Press.
Ma, S., & Hellerstein, J. (1999). Ordering categorical data improve visualization.
Proceedings IEEE Symposium Information Visualization, pp. 15{17.

389


