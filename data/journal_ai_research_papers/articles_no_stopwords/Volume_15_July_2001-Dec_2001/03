journal artificial intelligence

submitted published

parameter learning logic programs
symbolic statistical modeling

taisuke sato
yoshitaka kameya

sato mi cs titech ac jp
kame mi cs titech ac jp

dept computer science graduate school information
science engineering tokyo institute technology
ookayama meguro ku tokyo japan

abstract

propose logical mathematical framework statistical parameter learning parameterized logic programs e definite clause programs containing probabilistic facts
parameterized distribution extends traditional least herbrand model semantics
logic programming distribution semantics possible world semantics probability
distribution unconditionally applicable arbitrary logic programs including ones
hmms pcfgs bayesian networks
propose em graphical em runs
class parameterized logic programs representing sequential decision processes
decision exclusive independent runs data structure called support graph
describing logical relationship observations explanations learns
parameters computing inside outside probability generalized logic programs
complexity analysis shows combined oldt search explanations observations graphical em despite generality
time complexity existing em e baum welch hmms
inside outside pcfgs one singly connected bayesian networks
developed independently field learning experiments
pcfgs two corpora moderate size indicate graphical em
significantly outperform inside outside
introduction

parameter learning common fields neural networks reinforcement learning statistics used tune systems best performance classifiers
statistical unlike numerical systems described mathematical formulas however symbolic systems typically programs seem amenable kind
parameter learning actually little literature parameter learning programs
attempt incorporate parameter learning computer programs
reason twofold theoretically wish add ability learning computer
programs authors believe necessary step toward building intelligent systems
practically broadens class probability distributions beyond traditionally used numerical ones available modeling complex phenomena gene inheritance
consumer behavior natural language processing
c ai access foundation morgan kaufmann publishers rights reserved

fisato kameya

type learning consider statistical parameter learning applied logic
programs assume facts unit clauses program probabilistically true
parameterized distribution clauses non unit definite clauses
true encode laws one pair blood type genes b one
blood type ab call logic programs type parameterized logic program
use statistical modeling ground atoms provable program represent
observations one blood type ab parameters program
inferred performing ml maximum likelihood estimation observed atoms
probabilistic first order framework sketched termed statistical abduction
sato kameya amalgamation statistical inference abduction
probabilistic facts play role abducible e primitive hypotheses statistical
abduction powerful subsumes diverse symbolic statistical frameworks
hmms hidden markov rabiner pcfgs probabilistic context free
grammars wetherell manning schutze discrete bayesian networks
pearl castillo gutierrez hadi gives us freedom arbitrarily
complex logic programs modeling
semantic basis statistical abduction distribution semantics introduced sato
defines parameterized distribution actually probability measure set
possible truth assignments ground atoms enables us derive em
ml estimation called graphical em kameya sato
parameter learning statistical abduction done two phases search em learning given parameterized logic program observations first phase searches
explanations observations redundancy first phase eliminated tabulating
partial explanations oldt search tamaki sato warren sagonas
warren ramakrishnan rao sagonas swift warren shen yuan
zhou returns support graph compact representation discovered
explanations second phase run graphical em support graph
logic programs mean definite clause programs definite clause program set definite
clauses definite clause clause form l ln n l ln atoms
called head l ln body variables universally quantified reads l
ln hold holds case n clause called unit clause general clause
one whose body may contain negated atoms program including general clauses sometimes called
general program lloyd doets
throughout familiarity readability somewhat loosely use distribution
synonym probability measure
logic programming adjective ground means variables contained
abduction means inference best explanation set observations logically formalized
search explanation e e kb g g atom representing observation kb
knowledge base e conjunction atoms chosen abducible e class formulas allowed
primitive hypotheses kakas kowalski toni flach kakas e must consistent
kb
existing symbolic statistical modeling frameworks restrictions limitations types compared arbitrary logic programs see section details example bayesian networks
allow recursion hmms pcfgs stochastic grammars allow recursion lack variables data
structures recursive logic programs allowed ngo haddawy framework
assume domains finite function symbols seem prohibited
em stands class iterative ml estimation incomplete data
mclachlan krishnan


fiparameter learning logic programs symbolic statistical modeling

learn parameters distribution associated program redundancy
second phase removed introduction inside outside probability logic
programs computed support graph
graphical em accomplished combined oldt search
explanations time complexity specialized ones e g baum welch
hmms rabiner inside outside pcfgs baker
despite generality surprising conducted learning experiments pcfgs real corpora outperformed inside outside
orders magnitudes terms time one iteration update parameters experimental enhance prospect symbolic statistical modeling parameterized
logic programs even complex systems stochastic grammars whose modeling
dicult simply lack appropriate modeling tool sheer
complexities contributions therefore
distribution semantics parameterized logic programs unifies existing symbolicstatistical frameworks
graphical em combined tabulated search general yet ecient
em runs support graphs
prospect suggested learning experiments modeling learning complex
symbolic statistical phenomena
rest organized follows preliminaries section probability space parameterized logic programs constructed section mathematical
basis subsequent sections propose em graphical
em parameterized logic programs section complexity analysis
graphical em presented section hmms pcfgs pseudo pcsgs
sc bns section contains experimental parameter learning pcfgs
graphical em real corpora demonstrate eciency graphical
em state related work section followed conclusion section
reader assumed familiar basics logic programming lloyd doets
probability theory chow teicher bayesian networks pearl castillo
et al stochastic grammars rabiner manning schutze
preliminaries

since subject intersects logic programming em learning quite different
nature separate preliminaries

logic programming oldt

logic programming program db set definite clauses execution search
sld refutation given goal g top interpreter recursively selects
pseudo pcsgs probabilistic context sensitive grammars context sensitive extension pcfgs
proposed charniak carroll sc bn shorthand singly connected bayesian network
pearl
deal general logic programs


fisato kameya

next goal unfolds tamaki sato subgoals nondeterministically
chosen clause computed sld refutation e solution answer
substitution variable binding db g usually one
refutation g search space refutations described sld tree
may infinite depending program goal lloyd doets
often applications require solutions natural language processing
instance parser must able possible parse trees given sentence
every one syntactically correct similarly statistical abduction need
examine explanations determine likely one solutions obtained
searching entire sld tree choice search strategy prolog
standard logic programming language backtracking used search solutions
conjunction fixed search order goals textually left right clauses
textually top bottom due ease simplicity implementation
backtracking forgets everything previous
choice point hence quite likely prove goal resulting
exponential search time one answer avoid store computed
reuse whenever necessary oldt instance memoizing scheme tamaki
sato warren sagonas et al ramakrishnan et al shen et al
reuse proved subgoals oldt search often drastically reduces search time
solutions especially refutations top goal include many common subrefutations take example logic program coding hmm given string
exist exponentially many transition paths output oldt search applied
program however takes time linear length unlike
exponential time prolog backtracking search
oldt statistical abduction viewpoint statistical abduction reuse proved subgoals equivalently structure sharing sub refutations
top goal g brings structure sharing explanations g addition
reduction search time mentioned thereby producing highly compact representation explanations g

em learning

parameterized distributions multinomial distribution normal distribution provide convenient modeling devices statistics suppose random sample x xt
size random variable x drawn distribution p x x j parameterized
unknown observed value determined ml estimation mle
maximum likelihood estimate e maximizer likelihood p xi j
things get much dicult data incomplete think probabilistic
relationship non observable cause x observable effect one
diseases symptoms medicine assume uniquely determine
cause x incomplete sense carry enough information
completely determine x let p x x j parameterized joint distribution
x task perform ml estimation condition x
q

solution ambiguously mean answer substitution proved atom g
one gives


fiparameter learning logic programs symbolic statistical modeling

non observable observable let yt random sample size drawn
marginal distribution p j x p x x j mle
obtained maximizing likelihood p yi j function
mathematical formulation looks alike cases latter ml estimation
incomplete data far complicated direct maximization practically impossible
many cases people therefore looked indirect approaches tackle
ml estimation incomplete data em standard
solution dempster laird rubin mclachlan krishnan iterative
applicable wide class parameterized distributions including multinomial
distribution normal distribution mle computation replaced
iteration two easier tractable steps n th iteration first calculates value
q function introduced current parameter value n e step
p

q

q j n def


x

x

p x j n ln p x j



next maximizes q j n function updates n step
n argmax q j n

since old value n updated value n necessarily coincide e steps
steps iterated convergence log likelihood assured
increase monotonically mclachlan krishnan
although em merely performs local maximization used variety
settings due simplicity relatively good performance one must notice however
em class name taking different form depending distributions
applications development concrete em baum welch
hmms rabiner inside outside pcfgs baker
requires individual effort case
q function related ml estimation follows assume one data observed
jensen inequality chow teicher concavity ln function follows
x

p x j n ln p x j

x

x

p x j n ln p x j n

x

hence
q j n q n j n
x
x

p x j n ln p x j
p x j n ln p x j n ln p j ln p j n
x

ln p j ln p j n

x

consequently
q j n q n j n ln p j ln p j n p j p j n


fisato kameya
distribution semantics

section introduce parameterized logic programs define declarative semantics basic idea follows start set f probabilistic facts atoms
set r non unit definite clauses sampling f determines set f true
atoms least herbrand model f r determines truth value every atom
db f r hence every atom considered random variable taking
true false follows formalize process construct underlying
probability space denotation db

basic distribution pf

let db f r definite clause program first order language l countably many
variables function symbols predicate symbols f set unit clauses facts
r set non unit clauses rules sequel unless otherwise stated consider
simplicity db set ground instances clauses db assume
f r consist countably infinite ground clauses finite case similarly treated
construct probability space db two steps first introduce probability
space herbrand interpretations f e truth assignments ground atoms
f next extend probability space herbrand interpretations
ground atoms l least model semantics lloyd doets
let fixed enumeration atoms f regard infinite vector
hx x herbrand interpretation f way
ai true resp false xi resp xi isomorphism set
possible herbrand interpretations f coincides cartesian product

def

f f gi




construct probability measure pf sample space
f collection
finite joint distributions pf n x xn n xi f g n

pf n x xn
n

x x pf x xn

n

n
pf x xn pf x xn
x
last equation called compatibility condition proved chow teicher
compatibility condition exists probability space
f f pf
pf probability measure f minimal algebra containing open sets
f
n
pf x xn pf n x xn








p

p



n

n

herbrand interpretation interprets function symbol uniquely function ground terms
assigns truth values ground atoms since interpretation function symbols common
herbrand interpretations given l one one correspondence truth assignments
ground atoms l distinguish
regard
f topological space product topology f g equipped
discrete topology


fiparameter learning logic programs symbolic statistical modeling

call pf basic distribution


n
choice pf free long compatibility condition met want
interpretations equiprobable set pf n x xn n
every hx xn resulting pf uniform distribution
f one
unit interval hand stipulate interpretation except
hc c possible put n
pf n x xn ifo w xi ci n
pf places probability mass gives probability rest
define parameterized logic program definite clause program db f r
f set unit clauses r set non unit clauses clause head r
unifiable unit clause f parameterized basic distribution pf associated
f parameterized pf obtained collection parameterized joint distributions
satisfying compatibility condition generally complex pf n
exible pf cost tractability choice parameterized finite distributions
made sato simple
pf n x x n x n j n
n
pbs x x j


pbs x x j
x x

x x

x x
pbs x x j n represents probabilistic binary switch
e bernoulli trial two exclusive atoms way
one true trial never parameter specifying probability
switch resulting pf probability measure infinite product
independent binary outcomes might look simple expressive enough bayesian
networks markov chains hmms sato sato kameya










extending pf pdb

subsection extend pf probability measure pdb possible world
l e set possible truth assignments ground atoms l least
naming pf despite probability measure partly ects observation behaves
infinite joint distribution pf x x infinite random vector ha
pf n x xn n marginal distributions another reason
intuitiveness considerations apply pdb defined next subsection well
clauses necessarily ground


fisato kameya

herbrand model lloyd doets proceeding however need couple
notations atom define ax
ax x
ax x
next take herbrand interpretation
f f makes atoms f true
others false let f set atoms made true imagine definite clause
program db r f least herbrand model mdb lloyd doets
mdb characterized least fixed point mapping tdb
b bk db k
tdb def

fb bk g
set ground atoms equivalently inductively defined

tdb
mdb















n

taking account fact mdb function
f henceforth employ
functional notation mdb denote mdb
turning back let enumeration ground atoms l
form
db similarly
f cartesian product denumerably many f g identify set possible herbrand interpretations ground atoms
l e possible world l extend pf probability
measure pdb
db

n
follows introduce series finite joint distributions pdb x xn
n
ax axn f def
f
f j mdb j ax axn g
def
n x x p ax ax
pdb


n
n
f
n f



n





n

n

n satisfy
note set ax axn f pf measurable definition pdb
compatibility condition
n x x p n x x
pdb


n
n

n
n
db


n

x

xn

hence exists probability measure pdb
db extension pf

pdb x xn pf x xn
defines mutually herbrand interpretation ground atom true
herbrand model program herbrand interpretation makes every ground instance every
clause program true
note enumeration enumerates ground atoms f well


fiparameter learning logic programs symbolic statistical modeling

finite atoms f every binary vector hx xni xi f g
n define denotation program db f r w r pf pdb denotational semantics parameterized logic programs defined called distribution
semantics remarked regard pdb kind infinite joint distribution
pdb x x mathematical properties pdb listed appendix
semantics proved extension standard least model semantics
logic programming possible world semantics probability measure

programs distributions

distribution semantics views parameterized logic programs expressing distributions traditionally distributions expressed mathematical formulas use
programs discrete distributions gives us far freedom exibility mathematical formulas construction distributions recursion arbitrary composition particular program contain infinitely many random variables
probabilistic atoms recursion hence describe stochastic processes
potentially involve infinitely many random variables markov chains derivations
pcfgs manning schutze
programs enable us procedurally express complicated constraints distributions
sum occurrences alphabets b output string hmm must
multiple three feature procedural expression arbitrarily complex discrete
distributions seems quite helpful symbolic statistical modeling
finally providing mathematically sound semantics parameterized logic programs
one thing implementing distribution semantics tractable way another
next section investigate conditions parameterized logic programs make
probability computation tractable thereby making usable means large scale
symbolic statistical modeling
graphical em

according preceding section parameterized logic program db f r
first order language l parameterized basic distribution pf j herbrand
interpretations ground atoms f specifies parameterized distribution pdb j
herbrand interpretations l section develop step step ecient em
parameter learning parameterized logic programs interpreting pdb
distribution observable non observable events em
termed graphical em applicable arbitrary logic programs satisfying
certain conditions described later provided basic distribution direct product
multi ary random switches slight complication binary ones introduced
section
section assume db consists usual definite clauses containing
universally quantified variables definitions changes relating assumption
infinite derivation occur pcfgs take simple pcfg fp q ss g
start symbol terminal symbol p q p q pcfg rewritten
probability p ss probability q probability occurrence infinite derivation
calculated max f p q g non zero q p chi geman


fisato kameya

listed predicate p introduce iff p iff definition p
iff p def
x p x x w yn x tn wn
x vector variables length equal arity p p ti wi
n n enumeration clauses p db yi vector variables occurring
p ti wi define comp r follows
head r def
fb j b ground instance clause head appearing rg
iff r def
fiff p j p appears clause head rg
eq def
x f x j f function symbolg
x g j f g different function symbolsg
ft x j term properly containing xg
comp r def
iff r eq
eq clark equational theory clark deductively simulates unification likewise
comp r first order theory deductively simulates sld refutation help
eq replacing clause head atom clause body lloyd doets
introduce definitions frequently used let b atom
explanation b w r db f r conjunction r b
set comprised conjuncts f holds proper subset satisfies
set explanations b called support set b designated db b

motivating example

first review distribution semantics concrete example consider following
program dbb fb rb figure modeling one blood type determined blood
type genes probabilistically inherited parents
first four clauses rb state blood type determined genotype e pair
blood type genes b instance btype gtype gtype
gtype says one blood type genotype ha ai ha oi ho ai
propositional rules
succeeding clauses state general rules terms logical variables fifth clause
says regardless values x event gtype x one genotype
hx yi caused two events gene father x inheriting gene x father
gene mother inheriting gene mother gene p g msw gene p g
clause connecting rules rb probabilistic facts fb tells us gene g
inherited parent p choice represented msw gene p g made

definition support set differs one used sato kameya sato
implicitly emphasize procedural reading logic programs prolog conventions employed
sterling shapiro thus stands implied respectively strings
beginning capital letter universally quantified variables quoted ones
constants underscore anonymous variable
msw abbreviation multi ary random switch msw expresses probabilistic choice
finite alternatives framework statistical abduction msw atoms abducibles
explanations constructed conjunction


fiparameter learning logic programs symbolic statistical modeling











btype
btype b
btype
btype ab
gtype x
gene p g



gtype gtype gtype
gtype b b gtype b gtype b
gtype
gtype b gtype b
gene father x gene mother
msw gene p g

rb



fb

fmsw gene father msw gene father b msw gene father










msw gene mother msw gene mother b msw gene mother g

figure abo blood type program dbb
genetic knowledge choice g chance made fa b og expressed
specifying joint distribution fb follows
pf msw gene x msw gene b msw gene z j b def
axby oz
x z f g x z b b
father mother thus probability inheriting gene parent statistical
independence choice gene father mother expressed
putting
pf msw gene father x msw gene father b msw gene father z
msw gene mother x msw gene mother b msw gene mother z
j b
pf x z j b pf x z j b
setting atoms representing observation obs dbb fbtype btype b
btype btype ab g observe one say btype infer possible
explanation e minimal conjunction abducibles msw gene
rb btype
obtained applying special sld refutation procedure goal btype
preserves msw atoms resolved upon refutation three explanations found
msw gene father msw gene mother
msw gene father msw gene mother
msw gene father msw gene mother
db btype support set btype fs g probability
explanation respectively computed pf pf pf ao
proposition appendix follows pdb btype pdb
pf
pdb btype j b pf pf pf
ao
b

b

b

b

b

b

b

b

b

b

b

b

b



b

b

fisato kameya

used fact mutually exclusive choice gene
exclusive parameters e b determined ml estimation performed
random sample fbtype btype btype ab g btype follows
ha b oi argmaxh pdb btype pdb btype pdb btype ab
argmaxh ao ab
program contains neither function symbol recursion though semantics
allows later see example containing program hmm rabiner
juang


b



b

b

b

b

four simplifying conditions

figure simple probability computation easy generally
case since primary interest learning especially ecient parameter learning parameterized logic programs hereafter concentrate identifying property program
makes probability computation easy dbb thereby makes ecient parameter learning
possible
answer question precisely let us formulate whole modeling process suppose
exist symbolic statistical phenomena gene inheritance hope
construct probabilistic computational model first specify target predicate p
whose ground atom p represents observation phenomena explain
empirical distribution p write parameterized logic program db f r
basic distribution pf parameter reproduce observable patterns
p finally observing random sample p p st ground atoms p
adjust ml estimation e maximizing likelihood l tt pdb p st j
pdb p j approximates closely empirically observed distribution p
possible
first sight formulation looks right reality suppose two events
p p observed put l pdb p j pdb p j
cannot likelihood simply distribution semantics p p
two different random variables two realizations random variable
quick remedy note case blood type program dbb obs dbb
fbtype btype b btype btype ab g observable atoms one
true observation atom true others must false
words atoms collectively behave single random variable distribution
pdb whose values obs dbb
keeping mind introduce following condition let obs db head r
set ground atoms represent observable events call observable atom
dbb

q

b

uniqueness condition

pdb g g

g g obs db



p

g obs db pdb

g

fiparameter learning logic programs symbolic statistical modeling

uniqueness condition enables us introduce random variable yo representing
observation fix enumeration g g observable atoms obs db define
yo
k

iff j gk
db k

let gk gk gk obs db random sample size l tt pdb gk j
pdb yo kt j qualifies likelihood function w r yo
second condition concerns reduction probability computation addition
take blood type exmaple computation pdb btype decomposed
summation explanations support set mutualy exclusive
introduce
q

q







b

exclusiveness condition

every g obs db support set db g pdb
db g
exclusiveness condition proposition appendix
pdb g
pf
x



db

g

modeling point view means single event single observation
g may several even infinite explanations db g one db g allowed
true observation
introduce db e set explanations relevant obs db
db def

db g


g obs db

fix enumeration explanations db follows proposition
uniqueness condition exclusiveness condition
pdb si sj j

pdb
pdb
x

db

x

x

g obs db

g
pdb g
db


g obs db

able introduce uniqueness condition exclusiveness condition
yet another random variable xe representing explanation g defined
xe k iff j sk
db

third condition concerns termination



x

g obs db pdb g guarantees measure f j j gk k g one
satisfying gk case put yo values set measure
zero affect part discussion follows applies definition xe
p



fisato kameya

finite support condition

every g obs db db g finite
pdb g computed support set db g fs sm g
help exclusiveness condition finite summation mi pf si condition
prevents infinite summation hardly computable
fourth condition simplifies probability computation multiplication recall
explanation g obs db conjunction abducibles
fa amg f order reduce computation pf pf
multiplication pf pf assume
p

distribution condition

f set fmsw ground atoms parameterized distribution pmsw specified

atom msw n v intended simulate multi ary random switch whose name
whose outcome v trial n generalization primitive probabilistic events
coin tossing dice rolling
fmsw consists probabilistic atoms msw n v arguments n v ground
terms called switch name trial id value switch respectively
assume finite set vi ground terms called value set associated
v vi holds
write vi fv v vmg jvi j one ground atoms f msw n v
msw n v msw n vm g becomes exclusively true takes value
trial parameter v v v v associated v
probability msw v true v vi
ground terms n n v vi v vi random variable msw n v
independent msw n v n n
words introduce family parameterized finite distributions p n
p n msw n v x msw n vm xm j v v
x
x
mk xk
def
v v w

p














p



jvij xk f g k define pmsw infinite product
pmsw def
p n


n

condition compute pmsw probability explanation
product parameters suppose msw ij n v msw ij n v different conjuncts
explanation msw n v msw ik nk vk j j n n holds
independent construction else j j n n v v
independent pmsw construction whichever condition may hold
pmsw computed parameters


fiparameter learning logic programs symbolic statistical modeling

modeling principle

point introduced four conditions uniqueness condition exclusiveness condition finite support condition distribution condition simplify
probability computation last one easy satisfy adopt fmsw together
pmsw assume fmsw parameterized distribution pmsw
introduced previous subsection unfortunately rest satisfied automatically according modeling experiences however mildly dicult satisfy
uniqueness condition exclusiveness condition long obey following
modeling principle
modeling principle db fmsw r describes sequential decision process
modulo auxiliary computations uniquely produces observable atom
g obs db decision expressed msw atom
translated programming level says must take care writing program sample f pmsw must uniquely exist goal g g obs db
successful refutation db f r confirm principle
blood type program dbb fb rb describes process gene inheritance
arbitrary sample fb pmsw say fb fmsw gene father msw gene mother g
exists unique goal btype case successful sld refutation
fb rb
idea behind principle decision process produces
observable atom different decision processes must differ msw thereby entailing
mutually exclusive observable atoms uniqueness condition exclusiveness
condition automatically satisfied
satisfying finite support condition dicult virtually equivalent
writing program db solution search g g obs db terminates apparently general solution far specific
hmms pcfgs bayesian networks concerned met programs
satisfy finite support condition conditions well

four conditions revisited

subsection discuss relax four simplifying conditions introduced subsection purpose exible modeling first examine uniqueness condition
considering crucial role adaptation em semantics
uniqueness condition guarantees exists many one mapping
explanations observations em applicable dempster et al
possible however relax uniqueness condition justifying application
em assume mar missing random condition introduced
rubin statistical condition complete data explanation becomes incomplete data observation customarily assumed implicitly explicitly
statistics see appendix b assuming mar condition apply em
decisions made process finite subset fmsw


fisato kameya

non exclusive observations p uniqueness
condition seemingly destroyed
let us see mar condition action simple example imagine walk along
road front lawn occasionally observe state road dry
lawn wet assume lawn watered sprinkler running probabilistically
program dbrl rrl frl figure describes sequential process outputs
observation observed road x lawn road x lawn
x fwet dryg
rrl observed road x lawn p

frl



msw rain
yes x wet wet
msw sprinkler b
b x dry wet
b x dry dry
msw rain yes msw rain
msw sprinkler msw sprinkler

figure dbrl
basic distribution frl specified pf subsection omit
msw rain program determines whether rains yes
whereas msw sprinkler b determines whether sprinkler works fine b
b since sampled values fyes nog b b
b fon offg uniquely exists observation observed road x lawn x
fwet dryg many one mapping b hx yi words
apply em observations observed road x lawn x
fwet dryg would happen observe exclusively state road
lawn logically means observe observed road x lawn
x observed road x lawn apparently uniqueness condition met
observed road wet lawn x observed road x lawn wet compatible
true rains despite non exclusiveness observations still
apply em mar condition case translates
observe lawn road randomly regardless state
brie check conditions basically relaxed cost
increased computation without exclusiveness condition instance would need
additional process transforming support set db g goal g set exclusive
explanations instance g explanations fmsw n v msw b w g
transform fmsw n v msw n v msw b w g clearly
transformation exponential number msw atoms eciency concern leads
assuming exclusiveness condition
finite support condition practice equivalent condition sld tree
g finite relaxing condition might induce infinite computation
b

msw n v transformed disjunction exclusive msw atoms


w




msw n v

v v v va

fiparameter learning logic programs symbolic statistical modeling

relaxing distribution condition accepting probability distributions
serve expand horizon applicability parameterized logic programs
particular introduction parameterized joint distributions p v vk boltzmann distributions switches msw mswk v vk values switches
makes correlated distributions facilitate writing parameterized logic programs
complicated decision processes decisions independent interdependent obviously hand increase learning time whether added
exibility distributions deserves increased learning time yet seen
pmsw

naive em learning

subsection derive concrete em parameterized logic programs
db fmsw r assuming satisfy uniqueness condition exclusiveness
condition finite support condition
start introduce yo random variable representing observations according
fixed enumeration observable atoms obs db introduce
another random variable xe representing explanations according
fixed enumeration explanations db understanding xe non observable
yo observable joint distribution pdb xe x yo j
denotes relevant parameters immediate following section
derive concrete em q function defined q j def
x pdb x j
ln pdb x j whose input random sample observable atoms whose output
mle
following sake readability substitute observable atom g g
obs db yo write pdb g j instead pdb yo j likewise
substitute explanation db xe x write pdb g j instead
pdb xe x yo j follows uniqueness condition

db g
pdb g j
pmsw j db g
need yet another notation explanation define count msw n v

v def
jf n j msw n v gj
done preparations suppose make observations g g gt
gt obs db put
def
j msw n v db gt g
def
v j msw n v db gt g
set switch names appear explanation one gt denotes
parameters associated switches finite due finite support condition
p





fisato kameya

probabilities q function computed proposition
appendix together assumptions follows
pdb gt j pdb

pmsw j

db gt






pmsw j



q j def




v def




db
v vi

x


x



v
v

v vi

x
x

x



db

gt

pdb j gt ln pdb gt j

v ln v



pdb gt j

x

v vi

x

db

gt


v

v ln p

v vi v



pmsw j v

used jensen inequality obtain note pdb gt j g
pmsw j v expected count msw v sld refutation gt speaking
likelihood function l tt pdb gt j already shown subsection
footnote q j q j implies l l hence reach
procedure learn naive g finds mle parameters array
variable v stores v current
p

db



q

db








procedure

learn naive db g

begin

initialize appropriate values small positive number
ln pdb gt j
compute log likelihood
p

repeat
foreach

v vi

v


x



pdb gt j
foreach v vi
v
v p


v vi v
p
tt ln pdb gt j







end



x

db

gt

pmsw j v

update parameters
compute log likelihood
terminate converged

em simple correctly calculates mle calculation pdb gt j v line may suffer combinatorial explosion
explanations j db gt j often grows exponentially complexity model
instance j db gt j hmm n states n l exponential length l
input output string nonetheless suppressing explosion realize ecient computation polynomial order possible suitable conditions avoiding multiple
computations subgoal see next


fiparameter learning logic programs symbolic statistical modeling

inside probability outside probability logic programs

subsection generalize notion inside probability outside probability
baker lari young logic programs major computations learn naive g
two terms line pdb gt j g pmsw j v computational redundancy lurks naive computation terms example
suppose propositional program dbp fp rp fp fa b c mg
db

p

db









rp






f
f
g
g
h



g
b g
c
h




f observable atom assume b c independent
fa bg fc dg pair wise exclusive support set f calculated
db f fa c b c b g
hence light may compute pdb f
pdb f pf c pf pf b c pf b

computation requires multiplications pf c pf pf c etc
additions hand possible compute pdb f much eciently
factoring common computations let ground atom define inside probability

def
pdb j

applying theorem appendix
comp rp f g b g g c h h

unconditionally holds semantics independent exclusiveness assumption made fp following equations inside probability
derived
f g b g
g c h

h
pdb f f obtained solving f multiplications
additions required
quite straightforward generalize proceeding look program
dbq fmg fg g mg g observable atom msw atom
g semantics compute p g p p p
clearly wrong ignores fact clause bodies g e mutually
exclusive atoms clause body independent p pdb
similarly set b c equation totally incorrect
p

p

p

p

p

p

p

p

p

p

p







p

q

note fact f pmsw j



fisato kameya

therefore add temporarily subsection two assumptions top exclusiveness condition finite support condition equations become
mathematically correct first assumption clause bodies mutually exclusive e two clauses b w b w pdb w w j
second assumption body atoms independent e b bk rule
pdb b bk j pdb b j pdb bk j holds
please note clause used subsection special meaning intended
mean g g goal tabled explanation g obtained oldt search
explained next subsection words additional
conditions imposed source program oldt search
clauses auxiliary computations need satisfy
suppose clauses occur db



b b



bl bl il

bh j h l j ih atom theorem appendix
assumptions ensure


b j bl j




l


j

j

suggests gt considered function equations
inside probabilities hierarchically organized way gt belongs top
layer appearing left hand side refers b belong
lower layers refer condition acyclic support condition acyclic
support condition equations form unique solution computation
pdb g j via inside probabilities allows us take advantage reusing intermediate
stored thereby contributing faster computation pdb gt j
next tackle intricate computation g pmsw j
v since sum equals n msw n v g pmsw j concentrate
computation
gt def

pmsw j
p

p

db

p

db





x



db gt

msw n v first note explanation contains
ah ah gt expressed
gt gt


gt g gt depend generalizing observation arbitrary ground atoms introduce outside probability ground atom
w r gt
gt
gt def




logical relationship corresponds f g h table atoms


fiparameter learning logic programs symbolic statistical modeling

assuming conditions inside probability view computing gt reduced computing gt recursively computable
follows suppose occurs ground program db
b
bk

w b



wk bk

w
wk ik

gt function b bk assumption chain rule derivatives
leads
gt
wk
gt w




gt
b

bk

hence
gt gt



gt gt b w j gt bk wk j













k


x

k
x

j

j

therefore inside probabilities already computed outside probabilities
recursively computed top downward along program layers
case dbp f chosen atoms compute
f f
f g b

f h f g
f f h
desired sum f calculated
f f b
requires two multiplications one addition compared four multiplications
one addition naive computation
gains obtained computing inside outside probability may small case
size grows become enormous compensate enough additional restrictions imposed oldt search










oldt search

compute inside outside probability recursively need
programming level tabulation mechanism structure sharing partial explanations
independence assumption body atoms wh j h k j ih
independent therefore
wh j wh j w
h j




fisato kameya

subgoals henceforth deal programs db set table db table
predicate declared advance ground atom containing table predicate called
table atom purpose table atoms store support sets eliminate
need recomputation construct hierarchically organized explanations
made table atoms msw atoms
let db fmsw r parameterized logic program satisfies finite support
condition uniqueness condition let g g gt random sample
observable atoms obs db make following additional assumptions

assumptions

exists finite set f kt g table atoms associated
k k j
conjunctions sk j

k


e

comp r





gt
kt skt skt mkt
e



e

e



e





e



e






k k j set subset f
sk j

msw fk k g
k
acyclic support condition convention put gt call respectively
def
k explanation
db
f kt g set table atoms gt sk j
kt set explanations k denoted db kt consider
db function table atoms
st
explanations mutually exclusive e k k kt pdb sk j
k j
j j mk exclusiveness condition
k k j conjunction independent atoms independent
sk j

k
condition
assumptions aimed ecient probability computation namely acyclic
support condition makes dynamic programming possible exclusiveness condition reduces pdb b pdb pdb b independent condition reduces pdb b
pdb pdb b one point concerning eciency however note
imcomputation dynamic programming proceeds following partial order db
posed acyclic support condition access table atoms much simplified
respecting said partial
linearly ordered therefore topologically sort db

order call linearized db satisfying three assumptions acyclic support condition exclusiveness condition independent condition hierarchical system
h g assuming
explanations gt write db


db

k

implicitly given hierarchical system explanations gt successfully built
e



e



e

e

e

e

e



e

prefix abbreviation tabled
independence mentioned concerns positive propositions b b head db say
b b independent pdb b b j pdb b j pdb b j
precedes j top execution w r db invokes j directly indirectly
holds precedes j j


fiparameter learning logic programs symbolic statistical modeling

source program equations inside probability outside probability
automatically derived solved time proportional size
equations plays central role ecient em learning
one way obtain explanations use oldt search tamaki sato
warren complete refutation method logic programs oldt search
goal g called first time set entry g solution table store
answer substitutions g call instance g g occurs later stop
solving g instead try retrieve answer substitution g stored solution table
unifying g g record remaining answer substitutions g prepare
lookup table g hold pointer
self containedness look details oldt search sample program
dbh fh rh figure depicts hmm figure hmm two states
fs g state transition probabilistically chooses next destination fs g
b




b

b

b

figure two state hmm
fh

rh








































f values init
f values b
f values tr

h hmm cs msw init si
hmm si cs
h hmm c cs
msw c
msw tr nexts

hmm nexts cs
h hmm











generate string chars cs
set initial state si
enter loop clock
loop
output c state
transit nexts
put clock ahead
continue loop recursion
finish loop clock

figure two state hmm program dbh
f f f h h h temporary marks part program
hmm defines probability distribution strings given set alphabets works
stochastic string generator rabiner juang output string sample
defined distribution


fisato kameya

alphabet fa bg emit note specify fact set fh associated distribution compactly introduce notation values v vm
declares fh contains msw atoms form msw n v v fv vmg whose distribution p n given subsection example f values tr
introduces msw tr n v atoms program ground term
v fs g ground term n distribution
x
p tr n msw tr n x msw tr n j

tr x f g x
program runs prolog program non ground top goal hmm functions stochastic string generator returning list alphabets b
variable follows top goal calls clause h h selects initial state executing subgoal msw init si returns si initial state probabilistically
chosen fs g second clause h called h ground ground
makes probabilistic choice output alphabet c asking msw c
determines nexts next state asking msw tr nexts h
stop transition simplicity length output strings fixed three way
execution termed sampling execution corresponds random sampling
pdb top goal ground hmm b works acceptor e
returning success yes failure
explanations hmm b sought keep msw atoms resolved upon
refutation conjunction explanation repeat process backtracking refutation found need explanations however backtracking must
abandoned sharing partial explanations explanations purpose
explanations becomes impossible therefore instead use oldt search
h











top hmm cs ans tab hmm cs ans
tab hmm cs hmm cs x x hmm cs
tab hmm cs hmm cs x x hmm cs
e msw init msw init x x
e msw init msw init x x
hmm cs x x e msw init si x x tab hmm si cs x x
hmm c cs x x e msw c x x e msw tr nexts x x
tab hmm nexts cs x x
hmm x x

figure translated program dbh
msw n v called ground ground n v logical variable behaves random variable
instantiated term v probability v selected value set vi declared values
atom hand v ground term v called procedural semantics msw n v
equal msw n v v v


fiparameter learning logic programs symbolic statistical modeling

explanation search case hmm program example build hierarchical
system explanations hmm b oldt search first declare hmm
hmm table predicate explanation conjunction hmm atoms hmm
atoms msw atoms translate program another logic program analogously translation definite clause grammars dcgs prolog sterling shapiro
add two arguments forms list predicate purpose
accumulating msw atoms table atoms conjuncts explanation translation
applied dbh yields program figure
translated program clause corresponds top goal hmm l
input string l explanation table atom hmm l returned ans
auxiliary clauses add callee list table atom form hmm l
hmm l respectively time step state general p n table predicate
original program p n becomes table predicate translated program
auxiliary predicate tab p n inserted signal oldt interpreter check
solution table p n e check already exist explanations p n likewise
clauses pair corresponding f insert msw init
callee list clauses respectively correspond h
h h
hmm b hmm b
msw init hmm b
msw init hmm b
hmm b hmm b
msw msw tr q hmm b
msw msw tr hmm b
hmm b hmm b
msw msw tr hmm b
msw msw tr hmm b
hmm b hmm b
msw b msw tr hmm
msw b msw tr hmm
hmm b hmm b
msw b msw tr hmm
msw b msw tr hmm
hmm hmm
msw msw tr hmm
msw msw tr hmm
hmm hmm
msw msw tr hmm
msw msw tr hmm
hmm hmm

hmm hmm


figure solution table
general p n means predicate p arity n although hmm hmm share predicate name
hmm different predicates


fisato kameya

translation apply oldt search top hmm b ans noting added list uence oldt procedure ii associate
solution table atom solution table list explanations resulting
solution table shown figure first row reads call hmm b occurred entered solution table solution hmm b variable binding generated two explanations msw init hmm b
msw init hmm b remaining task topological sorting table atoms stored solution table respecting acyclic support condition
done depth first search trace explanations top goal
example thus obtain hierarchical system explanations hmm b

support graphs

looking back need compute inside outside probability hierarchical system
explanations essentially boolean combination primitive events msw atoms
compound events table atoms intuitively representable
graph reason help visualizing learning introduce
data structure termed support graphs though em next subsection
described solely hierarchical system explanations
illustrated figure support graph gt graphical representation
h g g
hierarchical system explanations db




k
consists totally ordered disconnected subgraphs labeled
k k subgraph labeled comprises two
corresponding table atom kt db

k
special nodes start node end node explanation graphs corresponding


explanation sk j
db k j mk
linear graph node labeled
explanation graph sk j
called table node switch
table atom switch msw sk j
node respectively figure b support graph hmm b obtained
solution table figure table node labeled refers subgraph labeled
data sharing achieved distinct table nodes referring subgraph


e

e

e

e

graphical em

describe ecient em learning termed graphical em
figure introduced kameya sato runs support graphs suppose
random sample g g gt observable atoms suppose support
graphs gt e hierarchical systems explanations satisfying acyclic
support condition exclusiveness condition independent condition
successfully constructed parameterized logic program db satisfying uniqueness
condition finite support condition
graphical em refines learn naive g introducing two subroutines
get inside probs
g compute inside probabilities get expectations
g compute outside probabilities called main routine learn gem g
learning prepare four arrays support graph gt g
p inside probability e pdb j see
db

db

db

db



fiparameter learning logic programs symbolic statistical modeling
k



explanation graph

msw

gt
start

k

msw

end

msw

msw

msw


k
start

end
msw

msw



b
msw init

hmm b

hmm b

start

end

msw init

msw

hmm b

msw tr

hmm b

hmm b

end

start

msw

msw tr

hmm b

msw

msw tr

hmm b

hmm b

end

start

msw

msw tr

hmm b

figure support graph general form b gt hmm b hmm
program dbh double circled node refers table node
q outside probability w r gt e gt see
r explanation probability db kt e pdb j
e

e



e

e

fisato kameya
procedure learn gem db g
begin
select initial







procedure get inside probs db g
begin
begin

let gt

k kt downto begin

p kt

foreach se edb kt begin

let se fa ajsejg

r kt se

l jsej

al msw v

r kt se v

else r kt se p al

p kt r kt se

end foreach se

end k
end
end

parameters

get inside probs
db g
p
tt ln p gt
repeat

get expectations db g
v vi
vp

v p g


foreach v p
vi
v v v vi v
get inside probs db g


p
tt ln p gt

foreach







end

procedure get expectations db g begin
begin

foreach v vi v

let gt q

k kt q kt

k kt

foreach se edb kt begin

let se fa ajsejg

l jsej

al msw v v q kt r kt se

else q al q kt r kt se p al

end foreach se
end
end

figure graphical em
v expected count msw v e

p



db

gt pmsw j v

call procedure learn gem g figure main routine learn gem g initially computes inside probabilities line enters loop get expectations g
called first compute expected count v msw v parameters updated line inside probabilities renewed updated parameters
entering next loop line
db

db

db



fiparameter learning logic programs symbolic statistical modeling

subroutine get inside probs g computes inside probability pdb j
stores p table atom bottom layer topmost layer
gt line hierarchical system explanations gt see subsection
takes explanation db kt one one line decomposes conjuncts
multiplies inside probabilities known line already computed
line
subroutine get expectations g computes outside probabilities following
recursive definitions subsection stores outside probability
gt table atom q first sets outside probability top goal
gt line computes rest outside probabilities line going
layers explanation gt described subsection line adds
q kt r kt gt kt v expected count msw v
contribution msw v kt v line increments outside
probability q al gt al al according equation notice q kt
already computed r kt p al w al w shown
subsection learn naive g mle procedure hence following theorem holds
theorem let db parameterized logic program g g gt randb

e

e

e

db

e

e

e

e

e

db

dom sample observable atoms suppose five conditions uniqueness finite support
subsection acyclic support exclusiveness independence subsection
met thenqlearn gem db g finds mle locally maximizes likelihood
l g j tt pdb gt j

proof sketch since main routine learn gem g learn naive g
except computation v tt v v g pmsw j
v n msw n v g pmsw j however
db

p

p

db

p

db

v

db

p



x

x

kkt





x

n msw n v se e
db k

gt kt se

see line get expectations db g
gt msw n v msw n v
n
gt msw n v see equation
n

pmsw j
q e
x

x

x

x

n msw n v

db

gt

used fact contains msw n v msw n v
msw n v holds hence
gt kt gt kt msw n v
contribution msw n v kt gt msw n v msw n v
e

e

e

e

e

e

e

e

formal proof given kameya proved common parameters v
learn naive db g coincides v learn gem db g parameters updated
values hence starting initial values parameters converge
values


fisato kameya

five conditions applicability graphical em may look hard
satisfy fortunately modeling principle section still stands
due care modeling likely lead us program meets actually
see next section programs standard symbolic statistical frameworks
bayesian networks hmms pcfgs satisfy five conditions
complexity

section analyze time complexity graphical em applied
symbolic statistical frameworks including hmms pcfgs pseudo pcsgs
bayesian networks graphical em competitive
specialized em developed independently field

basic property

since em iterative since unable predict
converges measure time complexity time taken one iteration therefore
estimate time per iteration repeat loop learn gem db g g g gt
observe one iteration support graph gt scanned twice
get inside probs db g get expectations db g scan addition
performed explanations multiplication possibly division performed
msw atoms table atoms time spent gt per iteration
graphical em linear size support graph e number nodes
support graph gt put
tdb def

db
e



e


db

num def
max
j e j
tt db
maxsize def

max
jsej

e
e
tt db
set table atoms g hence set explanations
recall db

db
appearing right hand side subsection num maximum number
explanations support graph gt maxsize maximum size texplanation gt respectively following obvious
e

proposition time complexity graphical em per iteration linear

total size support graphs nummaxsize notation coincides
space complexity graphical em runs support graphs

rather general compare graphical em
em must remember input graphical em
support graphs one observed atom actual total learning time
oldt time number iterations nummaxsizet


fiparameter learning logic programs symbolic statistical modeling

oldt time denotes time construct support graphs g sum
time oldt search time topological sorting table atoms
latter part former order wise represent oldt time time oldt
search observe total size support graphs exceed time oldt
search g order wise
evaluate oldt time specific class hmms need know
time table operations observe oldt search special
sense table atoms ground called resolution solved
goals accordingly solution table used
check goal g already entry solution table e called

add searched explanation g list discovered explanations
g entry
time complexity operations equal table access depends
program implementation solution table first suppose
programs carefully written way arguments table atoms used indecies table access integers actually programs used subsequent complexity
analysis dbh subsection dbg dbg subsection dbg subsection
satisfy satisfy condition replacing non integer terms appropriate integers suppose solution table implemented array table
access done time
follows present detailed analysis time complexity graphical
em applied hmms pcfgs pseudo pcsgs bayesian networks assuming
time access solution table remark way space complexity
total size solution tables support graphs


hmms

standard em hmms baum welch rabiner rabiner juang example hmm shown figure subsection given
observations w wt output string length l computes n lt time
iteration forward probability fftm q p ot ot otm q j backward
probability fimt q p otm otm otl j q state q q time step l
string wt ot ot otl q set states n number
states factor n comes fact every state n possible destinations

think oldt search top goal gt searches msw atoms table atoms create solution table auxiliary computations therefore time complexity never less
jthe number msw atoms table atoms support graph gt j coincides
time need topologically sort table atoms solution table depth first search gt
sagonas et al ramakrishnan et al discuss implementation oldt
arrays available may able use balanced trees giving log n access time n
number data solution table may able use hashing giving average time
access certain condition cormen leiserson rivest
treat state emission hmms emit symbol depending state another type
arc emission hmms emitted symbol depends transition arc treated similarly


fisato kameya

compute forward backward probability every destination every
state computing ffmt q fimt q parameters updated total
computation time iteration baum welch estimated n lt
rabiner juang manning schutze
compare graphical em use hmm program
dbh figure appropriate modifications l length string q
state set declarations fh output alphabets string w ol
hmm n q om om ol dbh reads hmm state q q time n
output om om ol reaches final state declaring hmm
hmm table predicate translation see figure apply oldt search goal
top hmm ol ans w r translated program obtain explanations
hmm ol complexity argument however translated program
dbh talk terms dbh sake simplicity search
fix search strategy multi stage depth first strategy tamaki sato
assume solution table accessible time since length list
third argument hmm decreases one recursion finitely
many choices state transition output alphabet search terminates leaving
finitely many explanations solution table figure satisfy acyclic support condition respectively sampling execution hmm l w r dbh nothing
sequential decision process decisions made msw atoms exclusive
independent generate unique string means dbh satisfies exclusiveness
condition independence condition uniqueness condition respectively
graphical em applicable set hierarchical systems explanations
hmm wt produced oldt search observations w wt output
string put wt ot ot otl follows

db
fhmm q otm otl j l q qg fhmm ot otl g
h

dbh





msw q om msw tr q q
hmm q otm otl










l
top goal hmm ot otl nl calling patterns hmm
call causes n calls hmm implying occur nl n n l
calls hmm since call computed due tabling mechanism
num n l maxsize applying proposition reach
e

hmm q otm otl

q q

proposition suppose strings length l suppose
table operation


oldt search done time oldt time dbh n lt graphical
em takes n lt time per iteration n number states

n lt time complexity baum welch
runs eciently baum welch

graphical em

possible translated program dbh section identify goal pattern
hmm first two arguments constants integers
besides baum welch graphical em whose input support graphs
generated dbh update parameters value initial values


fiparameter learning logic programs symbolic statistical modeling

way viterbi rabiner rabiner juang provides
hmms ecient way finding likely transition path given input output
string similar parameterized logic programs determines
likely explanation given goal derived runs time linear size
support graph thereby n l case hmms complexity viterbi
sato kameya

pcfgs

compare graphical em inside outside baker
lari young inside outside well known em
pcfgs wetherell manning schutze takes grammar chomsky
normal form given n nonterminals production rule grammar takes form
j k j k n nonterminals named numbers n
starting symbol form w n w terminal iteration
computes inside probability outside probability every partial parse tree
given sentence update parameters production rules time complexity
measured time per iteration described n number nonterminals
l number terminals sentence n l observed sentences lari
young
compare graphical em inside outside start
propositional program dbg fg rg representing largest grammar
containing possible rules j k n nonterminals nonterminal starting
symbol e sentence
fg
rg

j k j j k n numbersg
fmsw msw
w j n number w terminalg











q msw j k
q j
q k
n

q msw wd













j k n
l

n l









figure pcfg program dbg
db g artificial parsing program whose sole purpose measure size
oldt tree created oldt interpreter parses sentence w w wl
pcfg probabilistic context free grammar backbone cfg probabilities parameters assigned production rule nonterminal
n production rules fa ffi j ng
p
probability pi assigned ffi n ni pi probability sentence
sum probabilities leftmost derivation latter product probabilities
rules used derivation
precise oldt structure case tree dbg contains constants
datalog program never occurs need creating root node


fisato kameya


td

q l
j n

k n
q
q l

q l


note q


k n

q
q k l

q j
q l

q j
q k l

q k l

q l

q k l

k

td
q already appears
l
l

td

e l
j n

k n
q j e
q e l

q j e
q k e l

e e
n
j n

q e
q j e e
q e l

td

q k e l

n

q

n

q j e e
q e l
q e l


p

p p p n

figure oldt tree query

q l

input sentence w w wl embedded program separately msw wd
l second clauses rg treatment affect complexity argument q reads th nonterminal spans position position
e substring wd wd first clauses q msw q j
q k supposed textually ordered according lexicographic order
tuples hi j k parser top goal set q l asks
parser parse whole sentence w w wl syntactic category sentence
make exhaustive search query oldt search multistage depth first search strategy time access solution table assumed
time complexity oldt search measured number nodes
oldt tree let td k oldt tree q k l figure illustrates td
l msw atoms omitted seen tree many similar
subtrees put together see note figure due depth first strategy
td recursive structure contains td
subtree nodes whose leftmost atom
underlined solution nodes e solve leftmost atoms first time
entire refutation process underlined atoms already computed subtrees
left check solution table entries already




l prolog variable constant denoting sentence length
q table predicate


inductively proved td
contains every computed q l
l n l


fiparameter learning logic programs symbolic statistical modeling

computed time since clauses ground execution generates
single child node

k
enumerate h
number nodes td td k n
k


figure see h
n l let hd k n number nodes
k
td
contained td estimated n l consequently number
k
n


nodes newly created td h
k hd n l
l





total time oldt search computed hd n l size
support graph
consider non propositional parsing program dbg fg rg figure
whose ground instances constitute propositional program dbg dbg probabilistic
variant dcg program pereira warren q q
declared table predicate semantically dbg specifies probability distribution
atoms form fq l j l list terminalsg
p

p

fg

rg

sj sk j j k n numberg
fmsw sfi msw
si w j n number w terminalg































q length q
q c c l l
msw c j k
q j c c l l
q k c c l l
q c c w x x msw c w

figure probabilistic dcg program dbg
top goal parse sentence w wl q w wl invokes
q w wl measuring length input sentence
calling length general q c c l l works identically q
three arguments c c l l added c supplies unique trial id msws
used body c latest trial id current computation l l list holding
substring since added arguments affect shape

focus subtree td j j range n fif e e j e e l gfi
l hence number nodes td n l number nodes td


neither td
n l
td negligible therefore hd


number nodes tl tl negligible
make program simple possible assume integer n represented ground term


n
z



assume ground goal
returns integer time proportional jd j
omit obvious program length l sn computes length sn list l jlj time

sn

def



fisato kameya

search tree figure extra computation caused length l
one insertion nl respectively oldt time remains
n l hence size support graph
apply graphical em correctly need confirm five conditions
applicability rather apparent however oldt refutation topgoal form q w wl w r dbg terminates leaves support graph
satisfying finite support condition acyclic support condition exclusiveness
condition independent condition hold refutation process faithfully
simulates leftmost stochastic derivation w wl choice production
rule made msw si sc sj sk exclusive independent trial ids different
different choices
remains uniqueness condition confirm let us consider another program dbg modification dbg first goal length body
first clause first goal second clause rg moved
last position bodies respectively dbg dbg logically equivalent
semantically equivalent well viewpoint distribution semantics think
sampling execution oldt interpreter top goal q w r dbg
variable multi stage depth first search strategy easy see first
execution never fails second oldt refutation terminates sentence
w wl returned third conversely set msw atoms resolved upon
refutation uniquely determines output sentence w wl hence
sampling execution guaranteed terminate every sampling pf pf
uniquely generates sentence observable atom uniqueness condition satisfied
dbg hence dbg
sampling execution guaranteed terminate words
grammar generate finite sentences giving general answer seems
dicult known parameter values pcfg obtained learning
finite sentences stochastic derivation pcfg terminates probability
one chi geman summary assuming appropriate parameter values
say parameterized logic program dbg largest pcfg n nonterminal
symbols satisfies applicability conditions oldt time sentence length
l n l size support graph proposition
conclude
g

g

proposition let db parameterized logic program representing pcfg n

nonterminals form dbg figure g g g gt sampled atoms
representing sentences length l suppose table operation oldt search done
time oldt search g one iteration learn gem respectively
done n l time





called n l times td called

n l nl

pl


times
trial ids used refutation record rule used step derivation
w wl
dbg represent integers ground terms made keep program short
use integers instead ground terms however first three arguments q enough
check whether goal previously called check done time







fiparameter learning logic programs symbolic statistical modeling
n l

time complexity inside outside per iteration
lari young hence ecient inside outside

pseudo pcsgs

pcfgs improved making choices context sensitive one attempts
pseudo pcsgs pseudo probabilistic context sensitive grammars rule chosen probabilistically depending nonterminal expanded parent
nonterminal charniak carroll
pseudo pcsg easily programmed add one extra argument n representing
parent node predicate q c c l l figure replace
msw c j k msw n c j k since leftmost derivation sentence
pseudo pcsg still sequential decision process described modified program
graphical em applied support graphs generated modified
program observed sentences correctly performs ml estimation parameters
pseudo pcsg
pseudo pcsg thought pcfg rules form n j k
n j k n n parent nonterminal arguments previous
subsection carried minor changes therefore details omitted

proposition let db parameterized logic program pseudo pcsg n

nonterminals shown g g g gt observed atoms representing
sampled sentences length l suppose table operation oldt search done
time oldt search g iteration learn gem completed
n l time

bayesian networks

relationship cause c effect e often probabilistic one diseases symptoms mathematically captured conditional
probability p e e j c c effect e given cause c wish know however
inverse e probability candidate cause c given evidence e e p c c j e e
calculated bayes theorem p e e j c c p c c c p e e j c
c p c c bayesian networks representational computational framework fits
best type probabilistic inference pearl castillo et al
bayesian network graphical representation joint distribution p x x
xn xn finitely many random variables x xn graph dag directed
acyclic graph ones figure node random variable
graph conditional probability table cpt representing p xi xi j ui
n associated node xi represents xi parent nodes ui
values xi parent e topmost node graph table
marginal distribution p xi xi whole joint distribution defined product
p

deal discrete cases


fisato kameya



b





c

b

c

e



f

e

g singly connected

f

g multiply connected

figure bayesian networks
conditional distributions
p x x xn

xn

n



p xi xi j ui



thus graph g figure defines
pg b c e f pg pg b pg c j pg j b pg e j pg f j
b c e f values corresponding random variables b c e
f respectively mentioned one basic tasks bayesian networks
compute marginal probabilities example marginal distribution pg c
computed
pg c
pg pg b pg c j pg j b pg e j p f j

















x



b e f













x


b





pg pg b pg c j pg j b


x

e f

pg e j pg f j

clearly ecient observe graph g
figure would way factorize computations use requiring
exponentially many operations computing marginal probabilities
np hard general factorization assured graph singly
connected g e loop viewed undirected graph case
computation possible jv j time v set vertices graph pearl
otherwise graph called multiply connected might need exponential time
compute marginal probabilities sequel following
discrete bayesian network g defining distribution pg x xn
parameterized logic program dbg predicate bn pdb bn x xn
pg x xn
g

thanks acyclicity graph without losing generality may assume xi ancestor
node xj j holds
notational simplicity shall omit random variables confusion arises


fiparameter learning logic programs symbolic statistical modeling

arbitrary factorizations order compute marginal distribution

exists tabled program accomplishes computation specified way
graph singly connected evidence e given exists tabled
program dbg oldt time bn e jv j hence time
complexity per iteration graphical em jv j well
let g bayesian network defining joint distribution pg x xn fpg xi
xi j ui j n xi val xi ui val g conditional probabilities
associated g val xi set xi possible values val denotes
set possible values parent nodes random vector respectively
construct parameterized logic program defines distribution pg x xn
program dbg fg rg shown figure


fg

f msw par ui xi j n ui val xi val xi g

rg

f bn x xn

xi g

vn

msw par

figure bayesian network program dbg
fg comprised msw atoms form msw par ui xi whose probability
exactly conditional probability pg xi xi j ui xi parents ui
empty list rg singleton containing one clause whose body conjunction
msw atoms corresponds product conditional probabilities note
intentionally identify random variables x xn logical variables x xn
convenience

proposition dbg denotes distributions g

proof let hx xn realization random vector hx xn holds
construction
pdbg bn x xn



n

h
n


pmsw msw par ui xi


pg xi xi j ui
h
pg x xn
q e
case g figure program becomes
bn b c e f



msw par
msw par c c
msw par e e

b prolog constants used place integers


msw par b b
msw par b
msw par f f

fisato kameya

left right sampling execution gives sample realization random vector
h b c e f marginal distribution computed bn x xn adding
clause dbg example compute pg c add bn c bn b c e f
dbg let db g compute pdb bn c equal
pg c
pdb bn c pdb b e f bn b c e f

pdb bn b c e f
b e f
pg c
regrettably computation corresponds factorization ecient
probability computation factorization made possible carrying summations
proper order
next sketch example carry specified summations specified
order introducing clauses suppose joint distribution p x z w
x z w x z w x z w x z w respectively
computed atoms p x p z w p x z w suppose hope
compute sum
p x x
z w x z w






g



g

x

g

g



x

x



z w



first eliminate z w corresponding elimination introduce
two predicates q x compute x z w z w x z w p x
compute p x x x follows
p

p

p x
q x



p x q x
p z w p x z w

note clause body q contains z w existentially quantified local variables
clause head q x contains variables shared atoms view
correspondence easy confirm program realizes
required computation easy see generalizing example though
prove exists parameterized logic program carries given
summations given order arbitrary bayesian network particular
able simulate variable elimination zhang poole ambrosio

ecient computation marginal distributions possible
well known class bayesian networks singly connected bayesian networks
exists ecient compute marginal distributions message passing pearl
castillo et al graph singly connected
construct ecient tabled bayesian network program dbg assigning table predicate
node avoid complications explain construction procedure informally
concentrate case one interested variable let g singly
p





fiparameter learning logic programs symbolic statistical modeling

connected graph first pick node u whose probability pg u seek
construct tree g root node u g letting nodes dangling u
figure shows g transformed tree select node b root node
b





e

f

c


transformed graph g

figure transforming g tree
examine node g one one add node x graph
corresponding clause dbg whose purpose visit nodes connected x except
one calls x suppose started root node u figure evidence
u given generated clause proceed inner node x u calls
x original graph g x parent nodes fu u u g child nodes fv v g u
topmost node g


u
x
u

v
u

v

tree g

figure general situation
node x figure add clause called parent node
u u ground first generate possible values u calling val u u
call call x u u visit nodes connected x u u similary
treated visiting nodes g connecting x parent nodes u
u nodes connected u already visited value random variable x
determined sampling msw atom jointly indexed x values u u


fisato kameya
u visit x children v v topmost node u original graph
add clause
tbn u msw par u u call
call

u x u

u x u val u u call x u u
val u u call x u u
msw par x u u u x
call x v x call x v x






let dbg final program containing clauses apparently
dbg constructed time linear number nodes network
note successive unfolding tamaki sato atoms form call
clause bodies starts yields program db g similar one
figure contains msw atoms call dbg db g define
distribution proved proposition pg u pdb bn u
pdb tbn u holds details omitted way figure assume construction
starts topmost node u evidence u given necessary
suppose change start inner node x case replace clause
call x u u msw par u u time
replace head clause tbn add goal call x u u body
changed program db g rather straightforward prove
pdb tbn pg u holds true construction tabled program
dbg shown crude lot room optimization suces
parameterized logic program singly connected bayesian network runs
jv j time v set nodes
estimate time complexity oldt search w r dbg declare tbn every
predicate form call table predicate verify five conditions
applicability graphical em details omitted estimate time
complexity oldt search goal tbn u w r dbg notice calls occur
according pre order scan parents node children tree g calls
call x occur val times call call x invokes calls rest
nodes x parents x children graph g except caller node diffrent
set variable instantiations second call every call refers solutions
stored solution table time thus number added computation steps
call

x u u msw par u u







g

g



g






since distribution semantics least model semantics unfold fold transformation tamaki sato preserves least herbrand model transformed program unfold fold
transformation applied parameterized logic programs preserves denotation transformed
program
dbg transformed oldt interpreter collect msw atoms case hmm
program


fiparameter learning logic programs symbolic statistical modeling

oldt search x bounded constant val u val u val u val x
case figure oldt time proportional number nodes
original graph g size support graph provided number
edges connecting node values random variable bounded


proposition let g singly connected bayesian network defining distribution pg

v set nodes dbg tabled program derived suppose number
edges connecting node values random variable bounded
constant suppose table access done time oldt time
computing pg u observed value u random variable u means dbg
jv j time per iteration required graphical em
observations time complexity jv jt

jv j time complexity required compute marignal distribution singly
connected bayesian network standard pearl castillo et al
em therefore conclude graphical
em ecient specialzed em singly connected bayesian
networks must quickly add graphical em applicable
arbitrary bayesian networks proposition says explosion
support graph avoided appropriate programming case singly connected
bayesian networks
summarize graphical em single generic em proved
time complexity specialized em e baum welch
hmms inside outside pcfgs one singly connected
bayesian networks developed independently field
table summarizes time complexity em learning oldt search
graphical em case one observation first column sc bns
represents singly connected bayesian networks second column shows program use
dbh hmm proram subsection dbg pcfg program subsection
dbg transformed bayesian network program subsection respectively oldt time
third column time oldt search complete search explanations
gem fourth column time one iteration taken graphical em
update parameters use n l v respectively number states
hmm number nonterminals pcfg length input string
number nodes bayesian network last column standard specialized em
model


marginal distribution pg one variable required construct similar
tabled program computes marginal probabilities still jv j time adding extra arguments
convey evidence embedding evidnece program
check five conditions dbg figure uniqueness condition obvious sampling
uniquely generates sampled value random variable finite support condition
satisfied finite number random variables values acyclic support
condition immediate acyclicity bayesian networks exclusiveness condition
independent condition easy verify


fisato kameya

model
program
hmms
dbh
pcfgs
dbg
dbg
sc bns
user model


oldt time
n l
l
jv j
joldt treej

gem
specialized em

n l
baum welch


l
inside outside
jv j
castillo et al
jsupport graphj

table time complexity em learning oldt search graphical em

modeling language prism

developing symbolic statistical modeling laguage prism since url
http mi cs titech ac jp prism implementation distribution semantics
sato sato kameya sato language intented modeling
complex symbolic statistical phenomena discourse interpretation natural language
processing gene inheritance interacting social rules programming language
looks extension prolog built predicates including msw predicate
special predicates manipulating msw atoms parameters
prism program comprised three parts one directives one modeling
one utilities directive part contains declarations values telling system
msw atoms used execution modeling part set non unit definite
clauses define distribution denotation program msw atoms
last part utility part arbitary prolog program refers predicates defined
modeling part use utility part learn built predicate carry
em learning observed atoms
prism provides three modes execution sampling execution correponds
random sampling drawn distribution defined modeling part second
one computes probability given atom third one returns support set
given goal execution modes available built predicates
must report however implementation graphical em
simpified oldt search mechanism way completed yet
currently prolog search learn naive db g section available em learning though realized partially structrure sharing explanations implemention
learn naive db g putting computational eciecy aside however
expressing learning hmms pcfgs pseudo pcsgs bayesian networks
probailistic current version learning experiments next section
used parser substitute oldt interpreter independently implemented
graphical em
learning experiments

complexity analysis graphical em popular symbolic probabilistic
previous section look actual behavior graphical em
real data section conducted learning experiments pcfgs two


fiparameter learning logic programs symbolic statistical modeling

corpora contrasting characters compared performance graphical
em inside outside terms time per iteration
time updating parameters indicate graphical em
outperform inside outside orders magnigude detalis reported
sato kameya abe shirai proceeding review inside outside
completeness

inside outside

inside outside proposed baker generalization
baum welch pcfgs designed estimate parameters
cfg grammar chomsky normal form containing rules expressed numbers j k
j k n n nonterminals starting symbol suppose input
sentence w wl given
iteration first computes bottom manner

inside probabilities e p ws wt computes outside probabilities
f p
w ws wt wl top manner every
l n computing probabilities parameters
updated process iterates predetermined criterion
convergence likelihood input sentence achieved although baker
give analysis inside outside lari young showed
takes n l time one iteration lafferty proved em
true inside outside recognized standard em
algortihm training pcfgs notoriously slow although much literature
explicitly stating time required inside outside carroll rooth
beil carroll prescher riezler rooth beil et al reported example
trained pcfg rules corpus german subordinate clauses whose average ambiguity trees clause four machines mhz
sun ultrasparc mhz sun ultrasparc ii took hours complete
one iteration discuss later inside outside slow

learning experiments two corpora

report parameter learning existing pcfgs two corpora moderate size
compare graphical em inside outside terms
time per iteration mentioned support graphs input garphical em
generated parser e mslr parser measurements made
mhz sun ultrasparc ii gb memory solaris threshold
increase log likelihood input sentences set stopping criterion
em
experiments used atr corpus edr corpus converted pos
part speech tagged corpus similar size contrasting
characters sentence length ambiguity grammars first experiment
employed atr corpus japanese english corpus used japanese part
developed atr uratani takezawa matsuo morita contains short
mslr parser tomita generalized lr parser developed tanaka tokunaga laboratory tokyo
institute technology tanaka takezawa etoh


fisato kameya

conversational sentences whose minimum length average length maximum length
respectively skeleton pcfg employed context free grammar
gatr comprising rules nonterminals terminals manually developed
atr corpus tanaka et al yields parses sentence
inside outside accepts cfg chomsky normal form
converted gatr chomsky normal form g atr g atr contains rules nonterminals terminals divided corpus subgroups similar length
l l l containing randomly chosen sentences
preparations compare length graphical em applied
gatr g atr inside outside applied g atr terms time per
iteration running convergence
sec

sec

sec






















gem original
gem chomsky nf






























l

l

l


gem original
gem chomsky nf



















figure time per iteration vs gem atr
curves figure learning x axis length l input
sentence axis average time taken em one iteration update
parameters contained support graphs generated chosen sentences
parameters grammar change left graph inside outside
plots cubic curve labeled omitted curve drawn graphical
em drew x axis middle graph magnifies left graph curve
labeled gem original plotted graphical em applied original
grammar gatr whereas one labeled gem chomsky nf used g atr length
average sentence length measured whichever grammar employed graphical
em runs several hundreds times faster times faster case gatr
times faster case g atr inside outside per iteration
right graph shows almost linear dependency updating time graphical em
within measuared sentence length
although difference anticipated learning speed speed gap
inside outside graphical em unexpectedly large
conceivable reason atr corpus contains short sentences gatr


fiparameter learning logic programs symbolic statistical modeling

much ambiguous parse trees sparse generated support graphs small
affects favorably perforamnce graphical em
therefore conducted experiment another corpus contains much
longer sentences ambiguous grammar generates dense parse trees
used edr japanese corpus japan edr containing japanese news article
sentences however process annotation part randomly
sampled sentences recently made available labeled corpus compared
atr corpus sentences much longer average length sentences
minimum length maximum length cfg grammar gedr rules
converted chomsky normal form grammar g edr containing rules developed
ambiguous keep coverage rate parses sentence length
parses sentence length
sec



sec

sec










gem original



gem original




















l






l




l


figure time per iteration vs gem edr
figure shows obtained curves experiments edr corpus graphical em applied gedr vs inside outside applied g edr
condition atr corpus e plotting average time per iteration process
sentences designated length except plotted time inside outside average iterations whereas graphical em
average iterations clear middle graph time graphical
em runs orders magnitude faster inside outside average sentence length former takes second whereas latter takes seconds
giving speed ratio sentence length former takes seconds
latter takes seconds giving speed ratio thus speed ratio even
widens compared atr corpus explained mixed effects l
time complexity inside outside moderate increase total size
support graphs w r l notice right graph shows total size support
graphs grows sentence length l time per iteration graphical em
linear total size support graphs



fisato kameya

since implemented inside outside faithfully baker lari
young much room improvement actually kita gave refined insideoutside kita implementation mark johnson
inside outside loadable http www cog brown edu emj
use implementations may lead different conclusions therefore conducted
learning experiments entire atr corpus two implementations
measured updating time per iteration sato et al turned implementations run twice fast naive implementation take seconds per
iteration graphical em takes second per iteration still
orders magnitude faster former two regrettably similar comparison
entire edr corpus available moment abandoned due memory ow
parsing construction support graphs
learning experiments far compared time per iteration ignore extra time
search parsing required graphical em question naturally arises
w r comparison terms total learning time assuming iterations learning
atr corpus however estimated even considering parsing time graphical
em combined mslr parser runs orders magnitude faster three
implementations kita johnson inside outside sato et al
course estimation directly apply graphical em
combined oldt search oldt interpreter take time parser
much time needed depends implementaiton oldt search
conversely however may able take rough indication far
graphical em combined oldt search via support graphs go
domain em learning pcfgs

examing performance gap

previous subsection compared performance graphical em
inside outside pcfgs given two corpora three
implementations inside outside experiments graphical em
considerably outperformed inside outside despite fact
time complexity look causes performance
gap
simply put inside outside slow primarily lacks parsing
even backbone cfg grammar explicitly given take advantage
constraints imposed grammar see might help review inside
probability e e p nonterminal spans th word th word
calculated inside outside given grammar
e

r

x

p bc e r b e r c
bc grammar r
p bc probability associated production rule bc note
fixed triplet usual term p bc e r b e r c non zero
x

b c

cannnot answer question right implementation oldt search completed


fiparameter learning logic programs symbolic statistical modeling

relatively small number b c r determined successful parses
rest combinations give term nonetheless inside outside
attempts compute term every iteration possible combinations b c
r repeated every possible resulting lot redundancy
kind redundancy occurs computation outside probability inside outside

graphical em free redundancy runs parse trees
parse forest represented support graph must added hand
superiority learning speed graphical em realized cost space
complexity inside outside merely requires nl space
array store probabilities graphical em needs n l space store
support graph n number nonterminals l sentence length
trade understandable one notices graphical em applied
pcfg considered partial evaluation inside outside
grammar introduction appropriate data structure output
finally remark use parsing preprocess em learning pcfgs
unique graphical em fujisaki jelinek cocke black nishino
stolcke approaches however still seem contain redundancies compared
graphical em instance stolcke uses earley chart compute
inside outside probability parses implicitly reconstructed iteration
dynamically combining completed items
related work discussion

related work

work presented crossroads logic programming probability
theory considering enormous body work done fields incompleteness
unavoidable reviewing related work said look attempts
made integrate probability computational logic logic programming reviewing one immediately notice two types usage probability one type
constraint emphasizes role probability constraints necessarily seek unique probability distribution logical formulas type
distribution explicitly defines unique distribution model theoretical means
proof theoretical means compute probabilities propositions
typical constraint seen early work probabilistic logic nilsson
central probabilistic entailment compute upper
lower bound probability p target sentence way bounds
compatible given knowledge base containing logical sentences necessarily
logic programs annotated probability probabilities work constraints
emphasize difference inside outside graphical em
solely computational eciency converge parameter values starting
initial values linguistic evaluations estimated parameters graphical em
reported sato et al
omit literature leaning strongly toward logic logic concerning uncertainty see overview
kyburg


fisato kameya

possible range p used linear programming technique solve
inevitably delimits applicability finite domains
later lukasiewicz investigated computational complexity probabilistic
entailment slightly different setting knowledge base comprises statements
form h j g u u representing u p h j g u showed inferring
tight u u np hard general proposed tractable class knowledge base called
conditional constraint trees
uential work nilsson frish haddawy introduced deductive system probabilistic logic remedies drawbacks nilsson
computational intractability lack proof system system deduces
probability range proposition rules probabilistic inferences unconditional
conditional probabilities instance one rules infers p j
p j x propositional variables x x designates
probability range
turning logic programming probabilistic logic programming formalized ng
subrahmanian dekhtyar subrahmanian constraint program set annotated clauses form f fn n
atom n basic formula e conjunction disjunction
atoms j j n sub interval indicating probability range query
f fn n answered extension sld refutation formalization
assumed language contains finite number constant predicate
symbols function symbol allowed
similar framework proposed lakshmanan sadri syntactic restrictions finitely many constant predicate symbols function
symbols
different uncertainty setting used annotated clauses form c b bn
bi n atoms c h confidence level represents
belief interval doubt interval
expert clause
seen defining unique probability distribution secondary concern
constraint sharp contrast bayesian networks whole
discipline rests ability networks define unique probability distribution
pearl castillo et al researchers bayesian networks seeking
way mixing bayesian networks logical representation increase inherently
propositional expressive power
breese used logic programs automatically build bayesian network
query breese program union definite clause program set
conditional dependencies form p p j q qn p qi atoms
given query bayesian network constructed dynamically connects query
relevant atoms program turn defines local distribution connected
atoms logical variables appear atoms function symbol allowed
ngo haddawy extended breese incorporating mechanism
ecting context used clause form p j l lk
ai called p atoms probabilistic atoms whereas lj context atoms disjoint
p atoms computed another general logic program satisfying certain restric

fiparameter learning logic programs symbolic statistical modeling

tions given query set evidence context atoms relevant ground p atoms
identified resolving context atoms away sldnf resolution local bayesian network built calculate probability query proved soundness
completeness query evaluation procedure condition programs
acyclic domains finite
instead defining local distribution query poole defined global distribution probabilistic horn abduction program consists definite clauses
disjoint declarations form disjoint h p hn pn specifies probability distribution hypotheses abducibles fh hn g assigned probabilities
ground atoms help theory logic programming furthermore proved
bayesian networks representable framework unlike previous approaches
language contains function symbols acyclicity condition imposed programs
semantics definable seems severe restriction probabilities
defined quantified formulas
bacchus et al used much powerful first order probabilistic language
clauses annotated probabilities language allows statistically quantified term
k x j x kx denote ratio individuals finite domain satisfying x
x satisfying x assuming every world interpretation language
equally likely define probability sentence given knowledge
kb

base kb limit limn worlds
worlds kb worldsn number
possible worlds containing n individuals satisfying parameters used judging
approximations although limit necessarily exist domain must finite
showed method cope diculties arising direct inference
default reasoning
linguistic vein muggleton others formulated slps stochastic
logic programs procedurally extension pcfgs probabilistic logic programs
clause c must range restricted annotated probability p
p c probability goal g product ps appearing refutation
modification subgoal g invoke n clauses pi ci n
refutation step probability choosing k th clause normalized pk ni pi
recently cussens enriched slps introducing special class
log linear sld refutations w r given goal example considers
possible sld refutations general goal x defines probability p r
refutation r p r z exp r number associated
clause ci r feature e number occurrences ci r z
normalizing constant probability assigned sum probabilities
refutation



n

n



p

p

condition says every ground atom must assigned unique integer n n
n b n bn holds ground instance clause form b bn
condition program includes p x q x cannot write recursive clauses q
q x h jy q x
syntactic property variables appearing head appear body clause unit
clause must ground


fisato kameya

limitations potential

approaches described far less similar limitations potential
descriptive power confined finite domains common limitation due
use linear programming technique nilsson due syntactic
restrictions allowing infinitely many constant function predicate symbols ng
subrahmanian lakshmanan sadri bayesian networks
limitation well finite number random variables representable
semantic syntactic restrictions logic programs instance acyclicity
condition imposed poole ngo haddawy prevents unconditional
use clauses local variables range restrictedness imposed muggleton
cussens excludes programs usual membership prolog program
another type possibility assigning con icting probabilities
logically equivalent formulas slps p p necessarily coincide
may different refutations muggleton cussens
consequently slps would trouble naively interpret p probability
true assigning probabilities arbitrary quantified formulas seems
scope approaches slps
last least big common probabilities
numbers come generally speaking use n binary random variables
model determine n probabilities completely specify joint distribution
fulfilling requirement reliable numbers quickly becomes impossible n grows
situation even worse unobservable variables model
possible causes disease apparently parameter learning observed data natural
solution parameter learning logic programs well studied
distribution semantics proposed sato attempt solve
along line global distribution defines distribution probability
measure possible interpretations ground atoms arbitrary logic program
first order language assigns consistent probabilities closed formulas
distribution semantics enabled us derive em parameter learning
logic programs first time naive however dealing large
dicult exponentially many explanations observation
hmms believe eciency solved large extent
graphical em presented

em learning

since em learning one central issues separately mention work
related em learning symbolic frameworks koller pfeffer used
kbmc knowledge model construction em learning estimate parameters labeling clauses express probabilistic dependencies among events definite clauses annotated probabilities similarly ngo haddawy
locally build bayesian network relevant context evidence well
however rpms recursive probability proposed pfeffer koller extension
bayesian networks allow infinitely many random variables organized attributes
classes probability measure attribute values introduced


fiparameter learning logic programs symbolic statistical modeling

query parameters learned applying constructed network specialized em
bayesian networks castillo et al
dealing pcfg statically constructed bayesian network proposed pynadath wellman possible combine em method
estimate parameters pcfg unfortunately constructed network singly
connected time complexity probability computation potentially exponential
length input sentence
closely related em learning parameter learning log linear riezler proposed im probabilistic constraint programming im general parameter estimation incomplete data
log linear whose probability function p x takes form p x
z exp ni x p x n parameters estimated x
th feature observed object x z normalizing constant since feature
function x log linear model highly exible includes distribution
pmsw special case z price pay however computational cost
z requires summation exponentially many terms avoid cost exact
computation approximate computation monte carlo method possible whichever
one may choose however learning time increases compared em z
fam failure adjusted maximization proposed cussens
em applicable pure normalized slps may fail deals special
class log linear ecient im statistical
framework fam rather different distribution semantics comparison
graphical em seems dicult
slightly tangential em learning koller et al developed functional
modeling language defining probability distribution symbolic structures
showed cashing computed leads ecient probability computation
singly connected bayesian networks pcfgs cashing corresponds computation inside probability inside outside computation outside
probability untouched
p

future directions

parameterized logic programs expected useful modeling tool complex symbolicstatistical phenomena tried types modeling besides stochastic grammars bayesian networks modeling gene inheritance kariera tribe
white rules bi lateral cross cousin marriage four clans interact
rules genetic inheritance sato model quite interdisciplinary
exibility combining msw atoms means definite clauses greatly facilitated
modeling process
although satisfying five conditions section
uniqueness condition roughly one cause yields one effect
finite support condition finite number explanations one observation
acyclic support condition explanations must cyclic


fisato kameya

exclusiveness condition explanations must mutually exclusive
independence condition events explanation must independent

applicability graphical em seems daunting modeling experiences far tell us modeling principle section effectively guides us successful
modeling return obtain declarative model described compactly high level
language whose parameters eciently learnable graphical em shown
preceding section
one future directions however relax applicability conditions
especially uniqueness condition prohibits generative model failure
generating multiple observable events although pointed section mar
condition appendix b adapted semantics replace uniqueness condition
validates use graphical em even complete data uniquely
determine observed data case partially bracketed corpora pereira
schabes feel need topic investigating
role acyclicity condition seems theoretically interesting acyclicity often
related learning logic programs arimura reddy tadepalli
scratched surface individual fields hmms
pcfgs bayesian networks therefore remains much done clarifying
experiences field ected framework parameterized logic
programs example need clarify relationship symbolic approaches
bayesian networks spi li z ambrosio b
unclear compiled junction tree bayesian
networks incorporated aside exact methods approximate
methods probability computation specialized parameterized logic programs must
developed
direction improving learning ability introducing priors instead ml
estimation cope data sparseness introduction basic distributions make
probabilistic switches correlated seems worth trying near future important
take advantage logical nature handle uncertainty example
already shown sato learn parameters negative examples
grass wet treatment negative examples parameterized
logic programs still infancy
concerning developing complex statistical programs distributions scheme stochastic natural language processing exploits semantic information
seems promising instance unification grammars hpsgs abney
may good target beyond pcfgs use feature structures logically describable ambiguity feature values seems expressible probability
distribution
building mathematical basis logic programs continuous random variables
challenging topic



fiparameter learning logic programs symbolic statistical modeling
conclusion

proposed logical mathematical framework statistical parameter learning
parameterized logic programs e definite clause programs containing probabilistic facts
parameterized probability distribution extends traditional least herbrand
model semantics logic programming distribution semantics possible world semantics
probability distribution possible worlds herbrand interpretations
unconditionally applicable arbitrary logic programs including ones hmms pcfgs
bayesian networks
presented em graphical em section
learns statistical parameters observations class parameterized logic programs representing sequential decision process decision exclusive
independent works support graph data structure specifying logical relationship observed goal explanations estimates parameters computing
inside outside probability generalized logic programs
complexity analysis section showed oldt search complete tabled
refutation method logic programs employed support graph construction
table access done time graphical em despite generality
time complexity existing em e baum welch
hmms inside outside pcfgs one singly connected bayesian
networks developed independently field addition
pseudo probabilistic context sensitive grammars n nonterminals showed
graphical em runs time n l sentence length l
compare actual performance graphical em insideoutside conducted learning experiments pcfgs section two
real corpora contrasting characters one atr corpus containing short sentences
grammar much ambiguous parses sentence edr
corpus containing long sentences grammar rather ambiguous
average sentence length cases graphical em outperformed
inside outside orders magnitude terms time per iteration
suggests effectiveness em learning graphical em
since semantics limited finite domains finitely many random variables
applicable logic programs arbitrary complexity graphical em
expected give general yet ecient method parameter learning complex
symbolic statistical phenomena governed rules probabilities
acknowledgments

authors wish thank three anonymous referees comments suggestions
special thanks go takashi mori shigeru abe stimulating discussions learning
experiments tanaka tokunaga laboratory kindly allowing use
mslr parser linguistic data



fisato kameya
appendix properties

pdb

appendix list properties pdb defined parameterized logic program
db f r countable first order language l first pdb assigns consistent
probabilities every closed formula l
pdb def
pdb f
db j j g
guaranteeing continuity sense
limn pdb tn pdb x x
limn pdb tn pdb x x
enumeration ground terms l
next proposition proposition relates pdb herbrand model prove
need terminology factor closed formula prenex disjunctive normal
form q qnm qi n existential quantification universal
quantification matrix length quantifications n called rank
factor define set formulas made factors conjunctions disjunctions
associate formula multi set r ranks

factor quantification
r
fng
factor rank n
r r
stands union two multi sets instance f g f g f g
use multi set ordering proof proposition usual induction
complexity formulas work
lemma let boolean formula made ground atoms l pdb
pf f
f j mdb j g
proof prove lemma conjunction atoms form x
dnx xi f g n
pdb x dnx pdb f
db j j x dnx g
pdb x dn xn
pf f
f j mdb j x dnx g q e








n



n

n





n

proposition let closed formula l pdb pf f
f j mdb j g
definitions
f pf mdb
db pdb others used see section
consistent mean probabilities assigned logical formulas respect laws probability
p p p p b p p b p b


fiparameter learning logic programs symbolic statistical modeling

proof recall closed formula equivalent prenex disjunctive normal form
belongs prove proposition formulas induction
multi set ordering fr j g r quantification
proposition correct lemma suppose otherwise write g q q qn f
q q qnf indicates single occurrence factor g assume q x
q x similarly treated assume bound variables renamed avoid
name clash g xq qn f equivalent xg q qnf light validity
xa b x b xa b x b b contains free x
pdb pdb g q q qnf
pdb xg q qn f x
klim
p g q qn f g q qn f tk
db
klim
p g q qn f q qn f tk
db
klim
p f
f j mdb j g q qnf q qnf tk g
f
induction hypothesis
pf f
f j mdb j xg q qnf x g
pf f
f j mdb j g
q e
next prove theorem iff definition introduced section distribution
semantics considers program db f r set infinitely many ground definite
clauses f set facts probability measure pf r set rules
clause head r appears f put
head r def
fb j b appears r clause headg
b head r let b wi enumeration clauses b r
define iff b iff form rules b db
iff b def
b w w
since mdb least herbrand model following obvious
lemma b head r
f mdb j iff b
theorem iff b states general level sides iff
definition p x x w yn x tn wn p coincide random
variables whenever x instantiated ground term
theorem let iff b b w w iff form rules b head r
pdb iff b pdb b pdb w w
expression e e means may occur specified positions e e
indicates single occurrence positive boolean formula e e e e holds
definition different usual one lloyd doets talking ground
level w w true one disjuncts true


fisato kameya

proof

pdb iff b



pdb f
db j j b w w g
pdb f
db j j b w w g

klim
p f
db j j b
db

k




wi g

klim
p f
db j j b
db
klim
p f
f j mdb j b
f

k



k



wi g

wi g
k


klim
p f
f j mdb j b wi g
f

lemma
pf f
f j mdb j iff b g
pf
f lemma

follows pdb iff b
pdb b pdb b iff b pdb w w
q e
prove proposition useful probability computation let db b
support set atom b introduced section set explanations b
sequel b ground atom write db b fs g db b
define set b
b def
f
db j j b db b g
proposition every b head r pdb b pdb b pdb db b
proof first prove pdb b proof exactly parallels theorem
except w w replaced fact b
true every least herbrand model form mdb pdb b

pdb b pdb b b
db b
pdb db b
q e
finally distribution semantics probabilistic extension traditional
least herbrand model semantics logic programming proving theorem says
probability mass distributed exclusively possible least herbrand
define set least herbrand generated fixing r varying subset
f program db f r symbols
w



w





w

set k fe e g formulas k denotes n infinite disjunction e e


fiparameter learning logic programs symbolic statistical modeling

def
f
db j mdb
f g
note merely subset
db cannot conclude pdb priori
next theorem theorem states pdb e distribution semantics distributes
probability mass exclusively e possible least herbrand
prove theorem need preparations recalling atoms outside head r
f chance proved db introduce
def
f
db j j every ground atom head r f g
herbrand interpretation
db jf
f restriction atoms
f
lemma let
db herbrand
interpretation

mdb
f iff j b db b every b head r
proof part immediate property least herbrand model
part suppose satisfies right hand side mdb jf
mdb jf coincide w r atoms head r enough prove give
truth values atoms head r take b head r write db b
suppose j b j b j sj j
thereby jf j sj hence mdb jf j sj implies mdb jf j b otherwise
j b j sj every j follows mdb jf j b since b arbitrary
conclude mdb jf agree truth values assigned atoms head r
well
q e
w

w

theorem pdb

proof lemma
f
db j mdb
f g

b


b head r

pdb b proposition prove pdb let enumeration
atoms belonging head r f provable db f r
hence false every least herbrand model mdb
f
pdb

mlim
pdb f
db j j dm g
mlim
pf f
f j mdb j dm g
pf
f
since countable conjunction measurable sets probability measure one
probability measure one follows pdb b every b head r pdb
pdb
q e


fisato kameya
appendix b mar missing random condition

original formulation em dempster et al assumed
exists many one mapping x complete data x incomplete
observed data case parsing x parse tree input sentence x
uniquely determines uniqueness condition ensures existence
many one mapping explanations observations however sometimes face
situation many one mapping complete data incomplete
data nonetheless wish apply em
dilemma solved introduction missing data mechanism
makes complete data incomplete missing data mechanism distribution
g j x parameterized observed data described x says
x becomes incomplete correspondence x e fhx j
x g naturally becomes many many
rubin derived two conditions g data missing random data
observed random collectively called mar missing random condition showed
assume missing data mechanism behind observations satisfies mar
condition may estimate parameters distribution x simply applying
em observed data
adapt mar condition parameterized logic programs follows keep
generative model satisfying uniqueness condition outputs goals g parse
trees extend model additionally inserting missing data mechanism
g observation g assume satisfies mar
condition extended model many many correspondence explanations observations generates non exclusive observations p
causes p p g g pdb g thanks
mar condition however still allowed apply em nonexclusive observations put differently even uniqueness condition seemingly
destroyed em applicable imaginarily assuming missing data
mechanism satisfying mar condition
p

p



references

abney stochastic attribute value grammars computational linguistics

arimura h learning acyclic first order horn sentences entailment
proceedings eighth international workshop algorithmic learning theory
ohmsha springer verlag
bacchus f grove halpern j koller statistical knowledge bases
degrees belief artificial intelligence
baker j k trainable grammars speech recognition proceedings spring
conference acoustical society america pp



fiparameter learning logic programs symbolic statistical modeling

beil f carroll g prescher riezler rooth inside outside estimation
lexicalized pcfg german proceedings th annual meeting
association computational linguistics acl pp
breese j construction belief decision networks computational intelligence
carroll g rooth valence induction head lexicalized pcfg proceedings rd conference empirical methods natural language processing
emnlp
castillo e gutierrez j hadi expert systems probabilistic
network springer verlag
charniak e carroll g context sensitive statistics improved grammatical language proceedings th national conference artificial
intelligence aaai pp
chi z geman estimation probabilistic context free grammars computational linguistics
chow teicher h probability theory rd ed springer
clark k negation failure gallaire h minker j eds logic
databases pp plenum press
cormen leiserson c rivest r introduction mit press
cussens j loglinear first order probabilistic reasoning proceedings
th conference uncertainty artificial intelligence uai pp
cussens j parameter estimation stochastic logic programs machine learning

ambrosio b inference bayesian networks ai magazine summer
dekhtyar subrahmanian v hybrid probabilistic programs proceedings
th international conference logic programming iclp pp
dempster p laird n rubin b maximum likelihood incomplete
data via em royal statistical society b
doets k logic logic programming mit press
flach p kakas eds abduction induction essays relation
integration kluwer academic publishers
frish haddawy p anytime deduction probabilistic logic journal
artificial intelligence
fujisaki jelinek f cocke j black e nishino probabilistic parsing
method sentence disambiguation proceedings st international workshop
parsing technologies pp
japan edr l edr electronic dictionary technical guide nd edition technical
report japan electronic dictionary institute ltd


fisato kameya

kakas c kowalski r toni f abductive logic programming journal
logic computation
kameya learning representation symbolic statistical knowledge
japanese ph dissertation tokyo institute technology
kameya sato ecient em learning parameterized logic programs
proceedings st conference computational logic cl vol
lecture notes artificial intelligence pp springer
kita k probabilistic language japanese tokyo daigaku syuppan kai
koller mcallester pfeffer effective bayesian inference stochastic programs proceedings th national conference artificial intelligence
aaai pp
koller pfeffer learning probabilities noisy first order rules proceedings th international joint conference artificial intelligence ijcai
pp
kyburg h uncertainty logics gabbay hogger c robinson j eds
handbook logics artificial intelligence logic programming pp
oxford science publications
lafferty j derivation inside outside em
technical report ibm j watson center
lakshmanan l v sadri f probabilistic deductive databases proceedings
international symposium logic programming ilps pp
lari k young j estimation stochastic context free grammars
inside outside computer speech language
li z ambrosio b ecient inference bayes networks combinatorial
optimization international journal approximate reasoning
lloyd j w foundations logic programming springer verlag
lukasiewicz probabilistic deduction conditional constraints basic
events journal artificial intelligence
manning c schutze h foundations statistical natural language processing mit press
mclachlan g j krishnan em extensions wiley
interscience
muggleton stochastic logic programs de raedt l ed advances
inductive logic programming pp ios press
ng r subrahmanian v probabilistic logic programming information
computation
ngo l haddawy p answering queries context sensitive probabilistic
knowledge bases theoretical computer science
nilsson n j probabilistic logic artificial intelligence


fiparameter learning logic programs symbolic statistical modeling

pearl j probabilistic reasoning intelligent systems morgan kaufmann
pereira f c n schabes inside outside reestimation partially bracketed
corpora proceedings th annual meeting association computational linguistics acl pp
pereira f c n warren h definite clause grammars language analysis
survey formalism comparison augmented transition networks
artificial intelligence
pfeffer koller semantics inference recursive probability
proceedings seventh national conference artificial intelligence aaai
pp
poole probabilistic horn abduction bayesian networks artificial intelligence
pynadath v wellman p generalized queries probabilistic context free
grammars ieee transaction pattern analysis machine intelligence

rabiner l r tutorial hidden markov selected applications
speech recognition proceedings ieee
rabiner l r juang b foundations speech recognition prentice hall
ramakrishnan rao p sagonas k swift warren ecient tabling
mechanisms logic programs proceedings th international conference
logic programming iclp pp mit press
reddy c tadepalli p learning first order acyclic horn programs entailment proceedings th international conference machine learning
proceedings th international conference inductive logic programming morgan kaufmann
riezler probabilistic constraint logic programming ph thesis universitat
tubingen
rubin inference missing data biometrika
sagonas k warren xsb ecient deductive database engine
proceedings acm sigmod international conference management
data pp
sato statistical learning method logic programs distribution semantics
proceedings th international conference logic programming iclp
pp
sato modeling scientific theories prism programs proceedings ecai
workshop machine discovery pp
sato minimum likelihood estimation negative examples statistical abduction proceedings ijcai workshop abductive reasoning pp
sato kameya prism language symbolic statistical modeling
proceedings th international joint conference artificial intelligence
ijcai pp


fisato kameya

sato kameya viterbi em learning statistical
abduction proceedings uai workshop fusion domain knowledge
data decision support
sato kameya abe shirai k fast em learning family pcfgs
titech technical report dept cs tr tokyo institute technology
shen yuan l j zhou n linear tabulated resolution prolog
control strategy theory practice logic programming
sterling l shapiro e art prolog mit press
stolcke ecient probabilistic context free parsing computes
prefix probabilities computational linguistics
tamaki h sato unfold fold transformation logic programs proceedings
nd international conference logic programming iclp lecture notes
computer science pp springer
tamaki h sato old resolution tabulation proceedings rd
international conference logic programming iclp vol lecture notes
computer science pp springer
tanaka h takezawa etoh j japanese grammar speech recognition
considering mslr method proceedings meeting sig slp spoken
language processing slp pp information processing society
japan japanese
uratani n takezawa matsuo h morita c atr integrated speech
language database technical report tr atr interpreting telecommunications laboratories japanese
warren memoing logic programs communications acm

wetherell c probabilistic languages review open questions computing surveys
white h c anatomy kinship prentice hall
zhang n poole exploiting causal independence bayesian network inference journal artificial intelligence




