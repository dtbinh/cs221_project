Journal Artificial Intelligence Research 15 (2001) 391-454

Submitted 6/18; published 12/01

Parameter Learning Logic Programs
Symbolic-statistical Modeling

Taisuke Sato
Yoshitaka Kameya

sato@mi.cs.titech.ac.jp
kame@mi.cs.titech.ac.jp

Dept. Computer Science, Graduate School Information
Science Engineering, Tokyo Institute Technology
2-12-1 Ookayama Meguro-ku Tokyo Japan 152-8552

Abstract

propose logical/mathematical framework statistical parameter learning parameterized logic programs, i.e. definite clause programs containing probabilistic facts
parameterized distribution. extends traditional least Herbrand model semantics
logic programming distribution semantics , possible world semantics probability
distribution unconditionally applicable arbitrary logic programs including ones
HMMs, PCFGs Bayesian networks.
propose new EM algorithm, graphical EM algorithm, runs
class parameterized logic programs representing sequential decision processes
decision exclusive independent. runs new data structure called support graph
describing logical relationship observations explanations, learns
parameters computing inside outside probability generalized logic programs.
complexity analysis shows combined OLDT search explanations observations, graphical EM algorithm, despite generality,
time complexity existing EM algorithms, i.e. Baum-Welch algorithm HMMs,
Inside-Outside algorithm PCFGs, one singly connected Bayesian networks
developed independently research field. Learning experiments
PCFGs using two corpora moderate size indicate graphical EM algorithm
significantly outperform Inside-Outside algorithm.
1. Introduction

Parameter learning common various fields neural networks reinforcement learning statistics. used tune systems best performance, classifiers
statistical models. Unlike numerical systems described mathematical formulas however, symbolic systems, typically programs, seem amenable kind
parameter learning. Actually little literature parameter learning programs.
paper attempt incorporate parameter learning computer programs.
reason twofold. Theoretically wish add ability learning computer
programs, authors believe necessary step toward building intelligent systems.
Practically broadens class probability distributions, beyond traditionally used numerical ones, available modeling complex phenomena gene inheritance,
consumer behavior, natural language processing on.
c 2001 AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiSato & Kameya

type learning consider statistical parameter learning applied logic
programs.1 assume facts (unit clauses) program probabilistically true
parameterized distribution.2 clauses, non-unit definite clauses, always
true encode laws \if one pair blood type genes b, one's
blood type AB". call logic programs type parameterized logic program
use statistical modeling ground atoms3 provable program represent
observations \one's blood type AB" parameters program
inferred performing ML (maximum likelihood) estimation observed atoms.
probabilistic first-order framework sketched termed statistical abduction
(Sato & Kameya, 2000) amalgamation statistical inference abduction
probabilistic facts play role abducible s, i.e. primitive hypotheses.4 Statistical
abduction powerful subsumes diverse symbolic-statistical frameworks
HMMs (hidden Markov models, Rabiner, 1989), PCFGs (probabilistic context free
grammars, Wetherell, 1980; Manning & Schutze, 1999) (discrete) Bayesian networks
(Pearl, 1988; Castillo, Gutierrez, & Hadi, 1997) gives us freedom using arbitrarily
complex logic programs modeling.5
semantic basis statistical abduction distribution semantics introduced Sato
(1995). defines parameterized distribution, actually probability measure, set
possible truth assignments ground atoms enables us derive new EM algorithm6
ML estimation called graphical EM algorithm (Kameya & Sato, 2000).
Parameter learning statistical abduction done two phases, search EM learning. Given parameterized logic program observations, first phase searches
explanations observations. Redundancy first phase eliminated tabulating
partial explanations using OLDT search (Tamaki & Sato, 1986; Warren, 1992; Sagonas, T.,
& Warren, 1994; Ramakrishnan, Rao, Sagonas, Swift, & Warren, 1995; Shen, Yuan, You, &
Zhou, 2001). returns support graph compact representation discovered
explanations. second phase, run graphical EM algorithm support graph
1. paper, logic programs mean definite clause programs. definite clause program set definite
clauses. definite clause clause form L1 ; : : : ; Ln (0 n) A; L1 ; : : : ; Ln atoms.
called head, L1 ; : : : ; Ln body. variables universally quantified. reads L1
1 1 1 Ln hold, holds. case n = 0, clause called unit clause. general clause
one whose body may contain negated atoms. program including general clauses sometimes called
general program (Lloyd, 1984; Doets, 1994).
2. Throughout paper, familiarity readability, somewhat loosely use \distribution"
synonym \probability measure".
3. logic programming, adjective \ground" means variables contained.
4. Abduction means inference best explanation set observations. Logically, formalized
search explanation E E; KB ` G G atom representing observation, KB
knowledge base E conjunction atoms chosen abducible s, i.e. class formulas allowed
primitive hypotheses (Kakas, Kowalski, & Toni, 1992; Flach & Kakas, 2000). E must consistent
KB.
5. Existing symbolic-statistical modeling frameworks restrictions limitations various types compared arbitrary logic programs (see Section 7 details). example, Bayesian networks
allow recursion. HMMs PCFGs, stochastic grammars, allow recursion lack variables data
structures. Recursive logic programs allowed Ngo Haddawy's (1997) framework
assume domains finite function symbols seem prohibited.
6. \EM algorithm" stands class iterative algorithms ML estimation incomplete data
(McLachlan & Krishnan, 1997).
392

fiParameter Learning Logic Programs Symbolic-statistical Modeling

learn parameters distribution associated program. Redundancy
second phase removed introduction inside outside probability logic
programs computed support graph.
graphical EM algorithm accomplished, combined OLDT search
explanations, time complexity specialized ones, e.g. Baum-Welch
algorithm HMMs (Rabiner, 1989) Inside-Outside algorithm PCFGs (Baker,
1979), despite generality. surprising that, conducted learning experiments PCFGs using real corpora, outperformed Inside-Outside algorithm
orders magnitudes terms time one iteration update parameters. experimental results enhance prospect symbolic-statistical modeling parameterized
logic programs even complex systems stochastic grammars whose modeling
dicult simply lack appropriate modeling tool sheer
complexities. contributions paper therefore
distribution semantics parameterized logic programs unifies existing symbolicstatistical frameworks,
graphical EM algorithm (combined tabulated search), general yet ecient
EM algorithm runs support graphs
prospect suggested learning experiments modeling learning complex
symbolic-statistical phenomena.
rest paper organized follows. preliminaries Section 2, probability space parameterized logic programs constructed Section 3 mathematical
basis subsequent sections. propose new EM algorithm, graphical
EM algorithm, parameterized logic programs Section 4. Complexity analysis
graphical EM algorithm presented Section 5 HMMs, PCFGs, pseudo PCSGs
sc-BNs.7 Section 6 contains experimental results parameter learning PCFGs
graphical EM algorithm using real corpora demonstrate eciency graphical
EM algorithm. state related work Section 7, followed conclusion Section 8.
reader assumed familiar basics logic programming (Lloyd, 1984; Doets,
1994), probability theory (Chow & Teicher, 1997), Bayesian networks (Pearl, 1988; Castillo
et al., 1997) stochastic grammars (Rabiner, 1989; Manning & Schutze, 1999).
2. Preliminaries

Since subject intersects logic programming EM learning quite different
nature, separate preliminaries.

2.1 Logic Programming OLDT

logic programming, program DB set definite clauses8 execution search
SLD refutation given goal G. top-down interpreter recursively selects
7. Pseudo PCSGs (probabilistic context sensitive grammars) context-sensitive extension PCFGs
proposed Charniak Carroll (1994). sc-BN shorthand singly connected Bayesian network
(Pearl, 1988).
8. deal general logic programs paper.
393

fiSato & Kameya

next goal unfolds (Tamaki & Sato, 1984) subgoals using nondeterministically
chosen clause. computed result SLD refutation, i.e. solution, answer
substitution (variable binding) DB ` G.9 Usually one
refutation G, search space refutations described SLD tree
may infinite depending program goal (Lloyd, 1984; Doets, 1994).
often not, applications require solutions. natural language processing
instance, parser must able find possible parse trees given sentence
every one syntactically correct. Similarly statistical abduction, need
examine explanations determine likely one. solutions obtained
searching entire SLD tree, choice search strategy. Prolog,
standard logic programming language, backtracking used search solutions
conjunction fixed search order goals (textually left-to-right) clauses
(textually top-to-bottom) due ease simplicity implementation.
problem backtracking forgets everything previous
choice point, hence quite likely prove goal again, resulting
exponential search time. One answer avoid problem store computed results
reuse whenever necessary. OLDT instance memoizing scheme (Tamaki
& Sato, 1986; Warren, 1992; Sagonas et al., 1994; Ramakrishnan et al., 1995; Shen et al.,
2001). Reuse proved subgoals OLDT search often drastically reduces search time
solutions, especially refutations top goal include many common subrefutations. Take example logic program coding HMM. given string s,
exist exponentially many transition paths output s. OLDT search applied
program however takes time linear length find unlike
exponential time Prolog's backtracking search.
OLDT statistical abduction? viewpoint statistical abduction, reuse proved subgoals, equivalently, structure sharing sub-refutations
top-goal G brings structure sharing explanations G, addition
reduction search time mentioned above, thereby producing highly compact representation explanations G.

2.2 EM Learning

Parameterized distributions multinomial distribution normal distribution provide convenient modeling devices statistics. Suppose random sample x1; : : : ; xT
size random variable X drawn distribution P (X = x j ) parameterized
unknown , observed. value determined ML estimation MLE
(maximum likelihood estimate) , i.e. maximizer likelihood 1iT P (xi j ).
Things get much dicult data incomplete. Think probabilistic
relationship non-observable cause X observable effect one
diseases symptoms medicine assume uniquely determine
cause X . incomplete sense carry enough information
completely determine X . Let P (X = x; = j ) parameterized joint distribution
X . task perform ML estimation condition X
Q

9. solution ambiguously mean answer substitution proved atom G,
one gives other.
394

fiParameter Learning Logic Programs Symbolic-statistical Modeling

non-observable observable. Let y1 ; : : : ; yT random sample size drawn
marginal distribution P (Y = j ) = x P (X = x; = j ). MLE
obtained maximizing likelihood 1iT P (yi j ) function .
mathematical formulation looks alike cases, latter, ML estimation
incomplete data, far complicated direct maximization practically impossible
many cases. People therefore looked indirect approaches tackle problem
ML estimation incomplete data EM algorithm standard
solution (Dempster, Laird, & Rubin, 1977; McLachlan & Krishnan, 1997). iterative
algorithm applicable wide class parameterized distributions including multinomial
distribution normal distribution MLE computation replaced
iteration two easier, tractable steps. n-th iteration, first calculates value
Q function introduced using current parameter value (n) (E-step)10 :
P

Q

Q( j (n)) def
=

X

x

P (x j y; (n) ) ln P (x; j ):

(1)

Next, maximizes Q( j (n)) function updates (n) (M-step):
(n+1) = argmax Q( j (n) ):
(2)
Since old value (n) updated value (n+1) necessarily coincide, E-steps
M-steps iterated convergence, (log) likelihood assured
increase monotonically (McLachlan & Krishnan, 1997).
Although EM algorithm merely performs local maximization, used variety
settings due simplicity relatively good performance. One must notice however
EM algorithm class name, taking different form depending distributions
applications. development concrete EM algorithm Baum-Welch
algorithm HMMs (Rabiner, 1989) Inside-Outside algorithm PCFGs (Baker,
1979) requires individual effort case.
10. Q function related ML estimation follows. assume one data, , observed.
Jensen's inequality (Chow & Teicher, 1997) concavity ln function, follows
X

P (x j y; (n) ) ln P (x j y; ) 0

x

X

P (x j y; (n) ) ln P (x j y; (n) ) 0

x

hence
Q( j (n) ) 0 Q((n) j (n) )
X
X
=
P (x j y; (n) ) ln P (x j y; ) 0
P (x j y; (n) ) ln P (x j y; (n) ) + ln P (y j ) 0 ln P (y j (n) )
x

ln P (y j ) 0 ln P (y j (n) ):

x

Consequently,
Q( j (n) ) Q((n) j (n) ) ) ln p(y j ) ln p(y j (n) ) ) p(y j ) p(y j (n) ):
395

fiSato & Kameya
3. Distribution Semantics

section, introduce parameterized logic programs define declarative semantics. basic idea follows. start set F probabilistic facts (atoms)
set R non-unit definite clauses. Sampling F determines set F 0 true
atoms, least Herbrand model F 0 [ R determines truth value every atom
DB = F [ R. Hence every atom considered random variable, taking 1
(true) 0 (false). follows, formalize process construct underlying
probability space denotation DB.

3.1 Basic Distribution PF

Let DB = F [R definite clause program first-order language L countably many
variables, function symbols predicate symbols F set unit clauses (facts)
R set non-unit clauses (rules). sequel, unless otherwise stated, consider
simplicity DB set ground instances clauses DB, assume
F R consist countably infinite ground clauses (the finite case similarly treated).
construct probability space DB two steps. First introduce probability
space Herbrand interpretations11 F i.e. truth assignments ground atoms
F . Next extend probability space Herbrand interpretations
ground atoms L using least model semantics (Lloyd, 1984; Doets, 1994).
Let A1 ; A2 ; : : : fixed enumeration atoms F . regard infinite vector ! =
hx1; x2; : : :i 0s 1s Herbrand interpretation F way = 1; 2; : : :
Ai true (resp. false) xi = 1 (resp. xi = 0). isomorphism, set
possible Herbrand interpretations F coincides Cartesian product:
1
def

F = f0; 1gi:


i=1

construct probability measure PF sample space
F 12 collection
finite joint distributions PF(n)(A1 = x1; : : : ; = xn) (n = 1; 2; : : : ; xi 2 f0; 1g; 1 n)

0 PF(n)(A1 = x1 ; : : : ; = xn) 1
(n)
(3)
x ;:::;x PF (A1 = x1 ; : : : ; = xn ) = 1
(
n+1)
(
n)
PF (A1 = x1 ; : : : ; An+1 = xn+1 ) = PF (A1 = x1 ; : : : ; = xn ):
x
last equation called compatibility condition. proved (Chow & Teicher,
1997) compatibility condition exists probability space (
F ; F ; PF )
PF probability measure F , minimal algebra containing open sets
F ,
n,
PF (A1 = x1 ; : : : ; = xn ) = PF(n) (A1 = x1 ; : : : ; = xn ):
8
>
>
<
>
>
:

P

P

1

n

n+1

11. Herbrand interpretation interprets function symbol uniquely function ground terms
assigns truth values ground atoms. Since interpretation function symbols common
Herbrand interpretations, given L, one-to-one correspondence truth assignments
ground atoms L. distinguish them.
12. regard
F topological space product topology f0; 1g equipped
discrete topology.
396

fiParameter Learning Logic Programs Symbolic-statistical Modeling

call PF basic distribution
.13
(
n)
choice PF free long compatibility condition met. want
interpretations equiprobable, set PF(n)(A1 = x1; : : : ; = xn) = 1=2n
every hx1; : : : ; xn i. resulting PF uniform distribution
F one
unit interval [0; 1]. If, hand, stipulate interpretation except
!0 = hc1 ; c2; : : :i possible, put, n,
PF(n) (A1 = x1 ; : : : ; = xn ) = 10 ifo.w. 8i xi = ci (1 n)
PF places probability mass !0 gives probability 0 rest.
Define parameterized logic program definite clause program14 DB = F [ R
F set unit clauses, R set non-unit clauses clause head R
unifiable unit clause F parameterized basic distribution PF associated
F . parameterized PF obtained collection parameterized joint distributions
satisfying compatibility condition. Generally, complex PF(n)'s are,
exible PF is, cost tractability. choice parameterized finite distributions
made Sato (1995) simple:
PF(2n) (ON 1 = x1; 2 = x2 ; : : : ; 2n = x2n j 1 ; : : : ; n)
n
= Pbs (ON 2i01 = x2i01; 2i = x2i j i)
i=1

Pbs (ON 2i01 = x2i01 ; 2i = x2i j )
0 x2i01 = x2i
=
x2i01 = 1; x2i = 0
(4)
1 0 x2i01 = 0; x2i = 1:
Pbs (ON 2i01 = x2i01 ; 2i = x2i j ) (1 n) represents probabilistic binary switch,
i.e. Bernoulli trial, using two exclusive atoms 2i01 2i way either
one true trial never both. parameter specifying probability
switch on. resulting PF probability measure infinite product
independent binary outcomes. might look simple expressive enough Bayesian
networks, Markov chains HMMs (Sato, 1995; Sato & Kameya, 1997).
(



8
>
<
>
:

3.2 Extending PF PDB

subsection, extend PF probability measure PDB possible world
L, i.e. set possible truth assignments ground atoms L least
13. naming PF , despite probability measure, partly ects observation behaves
infinite joint distribution PF (A1 = x1 ; A2 = x2 ; : : :) infinite random vector hA1 ; A2 ; : : :i
PF(n) (A1 = x1 ; : : : ; = xn ) (n = 1; 2; : : :) marginal distributions. Another reason
intuitiveness. considerations apply PDB defined next subsection well.
14. clauses necessarily ground.
397

fiSato & Kameya

Herbrand model (Lloyd, 1984; Doets, 1994). proceeding however, need couple
notations. atom A, define Ax
Ax = x = 1
Ax = :A x = 0:
Next take Herbrand interpretation 2
F F . makes atoms F true
others false. Let F set atoms made true . imagine definite clause
program DB0 = R [ F least Herbrand model MDB0 (Lloyd, 1984; Doets, 1994).
MDB0 characterized least fixed point mapping TDB0 (1)
B1 ; : : : ; Bk 2 DB0 (0 k)
TDB0 (I ) def
=
fB1 ; : : : ; Bk g
set ground atoms.15 equivalently, inductively defined
I0 = ;
In+1 = TDB0 (In )
MDB0 =
:
(

(







)

[

n

Taking account fact MDB0 function 2
F , henceforth employ
functional notation MDB ( ) denote MDB0 .
Turning back, let A1 ; A2 ; : : : enumeration, ground atoms L.16
Form
DB , similarly
F , Cartesian product denumerably many f0; 1g's identify set possible Herbrand interpretations ground atoms A1 ; A2 ; : : :
L, i.e. possible world L. extend PF probability
measure PDB
DB
(
n)
follows. Introduce series finite joint distributions PDB (A1 = x1 ; : : : ; = xn )
n = 1; 2; : : :
[Ax1 ^ 1 1 1 ^ Axn ]F def
= f 2
F j MDB ( ) j= Ax1 ^ 1 11 ^ Axn g
def
(n) (A = x ; : : : ; = x ) = P ([Ax ^ 11 1 ^ Ax ] ):
PDB
1
1
n
n
F
n F
1
1

n

1

1

n

n

(n) 's satisfy
Note set [Ax1 ^ 1 1 1 ^ Axn ]F PF -measurable definition, PDB
compatibility condition
(n+1) (A = x ; : : : ; = x ) = P (n) (A = x ; : : : ; = x ):
PDB
1
1
n+1
n+1
1
n
n
DB 1
1

n

X

xn+1

Hence exists probability measure PDB
DB extension PF

PDB (A1 = x1 ; : : : ; = xn ) = PF (A1 = x1 ; : : : ; = xn )
15. defines, mutually, Herbrand interpretation ground atom true 2 .
Herbrand model program Herbrand interpretation makes every ground instance every
clause program true.
16. Note enumeration enumerates ground atoms F well.
398

fiParameter Learning Logic Programs Symbolic-statistical Modeling

finite atoms A1; : : : ; F every binary vector hx1 ; : : : ; xni (xi 2 f0; 1g; 1
n). Define denotation program DB = F [ R w.r.t. PF PDB . denotational semantics parameterized logic programs defined called distribution
semantics. remarked before, regard PDB kind infinite joint distribution
PDB (A1 = x1 ; A2 = x2 ; : : :). Mathematical properties PDB listed Appendix
semantics proved extension standard least model semantics
logic programming possible world semantics probability measure.

3.3 Programs Distributions

Distribution semantics views parameterized logic programs expressing distributions. Traditionally distributions expressed using mathematical formulas use
programs (discrete) distributions gives us far freedom exibility mathematical formulas construction distributions recursion arbitrary composition. particular program contain infinitely many random variables
probabilistic atoms recursion, hence describe stochastic processes
potentially involve infinitely many random variables Markov chains derivations
PCFGs (Manning & Schutze, 1999).17
Programs enable us procedurally express complicated constraints distributions
\the sum occurrences alphabets b output string HMM must
multiple three". feature, procedural expression arbitrarily complex (discrete)
distributions, seems quite helpful symbolic-statistical modeling.
Finally, providing mathematically sound semantics parameterized logic programs
one thing, implementing distribution semantics tractable way another.
next section, investigate conditions parameterized logic programs make
probability computation tractable, thereby making usable means large scale
symbolic-statistical modeling.
4. Graphical EM Algorithm

According preceding section, parameterized logic program DB = F [ R
first-order language L parameterized basic distribution PF (1 j ) Herbrand
interpretations ground atoms F specifies parameterized distribution PDB (1 j )
Herbrand interpretations L. section, develop, step step, ecient EM
algorithm parameter learning parameterized logic programs interpreting PDB
distribution observable non-observable events. new EM algorithm
termed graphical EM algorithm. applicable arbitrary logic programs satisfying
certain conditions described later provided basic distribution direct product
multi-ary random switches, slight complication binary ones introduced
Section 3.1.
section on, assume DB consists usual definite clauses containing
(universally quantified) variables. Definitions changes relating assumption
17. infinite derivation occur PCFGs. Take simple PCFG fp : ! a; q : ! SS g
start symbol, terminal symbol, p + q = 1 p; q > 0. PCFG, rewritten either
probability p SS probability q . probability occurrence infinite derivation
calculated max f0; 1 0 (p=q)g non-zero q > p (Chi & Geman, 1998).
399

fiSato & Kameya

listed below. predicate p, introduce iff (p), iff definition p
iff(p) def
= 8x (p(x) $ 9y1(x = t1 ^ W1 ) _ 1 11 _ 9yn (x = tn ^ Wn )) :
x vector new variables length equal arity p, p(ti) Wi (1
n; 0 n), enumeration clauses p DB, yi , vector variables occurring
p(ti) Wi. define comp(R) follows.
head(R) def
= fB j B ground instance clause head appearing Rg
iff (R) def
= fiff (p) j p appears clause head Rg
Eq def
= (x) = f (y) ! x = j f function symbolg
[ (x) 6= g(y) j f g different function symbolsg
[ ft 6= x j term properly containing xg
comp(R) def
= iff (R) [ Eq
Eq , Clark's equational theory (Clark, 1978), deductively simulates unification. Likewise
comp(R) first-order theory deductively simulates SLD refutation help
Eq replacing clause head atom clause body (Lloyd, 1984; Doets, 1994).
introduce definitions frequently used. Let B atom.
explanation B w.r.t. DB = F [ R conjunction S; R ` B,
set comprised conjuncts, F holds proper subset satisfies this.
set explanations B called support set B designated DB (B).18

4.1 Motivating Example

First all, review distribution semantics concrete example. Consider following
program DBb = Fb [ Rb Figure 1 modeling one's blood type determined blood
type genes probabilistically inherited parents.19
first four clauses Rb state blood type determined genotype, i.e. pair
blood type genes a, b o. instance, btype('A'):- (gtype(a,a) ; gtype(a,o) ;
gtype(o,a)) says one's blood type (her) genotype ha; ai, ha; oi ho; ai.
propositional rules.
Succeeding clauses state general rules terms logical variables. fifth clause
says regardless values X Y, event gtype(X,Y) (one's genotype
hX; Yi) caused two events, gene(father,X) (inheriting gene X father)
gene(mother,Y) (inheriting gene mother). gene(P,G):- msw(gene,P,G)
clause connecting rules Rb probabilistic facts Fb. tells us gene G
inherited parent P choice represented msw(gene,P,G)20 made.

18. definition support set differs one used Sato (1995) Kameya Sato (2000).
19. implicitly emphasize procedural reading logic programs, Prolog conventions employed
(Sterling & Shapiro, 1986). Thus, ; stands \or", , \and" :- \implied by" respectively. Strings
beginning capital letter (universally quantified) variables, quoted ones 'A'
constants. underscore anonymous variable.
20. msw abbreviation \multi-ary random switch" msw(1; 1; 1) expresses probabilistic choice
finite alternatives. framework statistical abduction, msw atoms abducibles
explanations constructed conjunction.
400

fiParameter Learning Logic Programs Symbolic-statistical Modeling

8
>
>
>
>
>
>
>
<

btype('A')
btype('B')
btype('O')
btype('AB')
gtype(X,Y)
gene(P,G)

::::::-

(gtype(a,a) ; gtype(a,o) ; gtype(o,a)).
(gtype(b,b) ; gtype(b,o) ; gtype(o,b)).
gtype(o,o).
(gtype(a,b) ; gtype(b,a)).
gene(father,X), gene(mother,Y).
msw(gene,P,G).

Rb

=

Fb

= fmsw(gene,father,a); msw(gene,father,b); msw(gene,father,o);

>
>
>
>
>
>
>
:

msw(gene,mother,a); msw(gene,mother,b); msw(gene,mother,o)g

Figure 1: ABO blood type program DBb
genetic knowledge choice G chance made fa; b; og expressed
specifying joint distribution Fb follows.
PF (msw(gene,t,a) = x; msw(gene,t,b) = y; msw(gene,t,o) = z j ; b ; ) def
= axby oz
x; y; z 2 f0; 1g, x + + z = 1, a; b; 2 [0; 1], + b + = 1 either
father mother. Thus probability inheriting gene parent. Statistical
independence choice gene, father mother, expressed
putting
PF ( msw(gene,father,a) = x; msw(gene,father,b) = y; msw(gene,father,o) = z;
msw(gene,mother,a) = x0; msw(gene,mother,b) = 0; msw(gene,mother,o) = z 0
j ; b ; )
= PF (x; y; z j a; b; o)PF (x0; y0 ; z0 j a; b ; o):
setting, atoms representing observation obs(DBb ) = fbtype('A'); btype('B');
btype('O'); btype('AB')g. observe one them, say btype('A'), infer possible
explanation , i.e. minimal conjunction abducibles msw(gene,1,1)
S; Rb ` btype('A').
obtained applying special SLD refutation procedure goal btype('A')
preserves msw atoms resolved upon refutation. Three explanations found.
S1 = msw(gene,father,a) ^ msw(gene,mother,a)
S2 = msw(gene,father,a) ^ msw(gene,mother,o)
S3 = msw(gene,father,o) ^ msw(gene,mother,a)
DB (btype(a)), support set btype(a), fS1 ; S2 ; S3g. probability
explanation respectively computed PF (S1) = a2 PF (S2 ) = PF (S3) = ao.
Proposition A.2 Appendix A, follows PDB (btype('A')) = PDB (S1 _ S2 _ S3) =
PF (S1 _ S2 _ S3 )
PDB (btype('A') j ; b ; ) = PF (S1 ) + PF (S2 ) + PF (S3 )
= a2 + 2ao:
b

b

b

b

b

b

b

b

b

b

b

b

b

401

b

b

fiSato & Kameya

used fact S1, S2 S3 mutually exclusive choice gene
exclusive. Parameters, i.e. a, b determined ML estimation performed
random sample fbtype('A'); btype('O'); btype('AB')g btype follows.
ha ; b ; oi = argmaxh ; ; PDB (btype('A'))PDB (btype('O'))PDB (btype('AB'))
= argmaxh ; ; (a2 + 2ao)o2 ab
program contains neither function symbol recursion though semantics
allows them. Later see example containing both, program HMM (Rabiner
& Juang, 1993).


b



b

b

b

b

4.2 Four Simplifying Conditions

Figure 1 simple probability computation easy. generally
case. Since primary interest learning, especially ecient parameter learning parameterized logic programs, hereafter concentrate identifying property program
makes probability computation easy DBb, thereby makes ecient parameter learning
possible.
answer question precisely, let us formulate whole modeling process. Suppose
exist symbolic-statistical phenomena gene inheritance hope
construct probabilistic computational model. first specify target predicate p
whose ground atom p(s) represents observation phenomena. explain
empirical distribution p, write parameterized logic program DB = F [ R
basic distribution PF parameter reproduce observable patterns
p(s). Finally, observing random sample p(s1); : : : ; p(sT ) ground atoms p,
adjust ML estimation, i.e. maximizing likelihood L() = Tt=1 PDB (p(st) j )
PDB (p(1) j ) approximates closely empirically observed distribution p
possible.
first sight, formulation looks right, reality not. Suppose two events
p(s) p(s0 ) (s 6= s0) observed. put L() = PDB (p(s) j )PDB (p(s0) j ).
cannot likelihood simply distribution semantics, p(s) p(s0 )
two different random variables, two realizations random variable.
quick remedy note case blood type program DBb obs(DBb) =
fbtype('A'); btype('B'); btype('O'); btype('AB')g observable atoms, one
true observation, atom true, others must false.
words, atoms collectively behave single random variable distribution
PDB whose values obs(DBb ).
Keeping mind, introduce following condition. Let obs(DB) ( head(R))
set ground atoms represent observable events. call observable atom s.
DBb

Q

b

Uniqueness condition:
0
PDB (G ^ G ) = 0

G 6= G0 2 obs(DB),

402

P

G2obs(DB) PDB

(G) = 1.

fiParameter Learning Logic Programs Symbolic-statistical Modeling

uniqueness condition enables us introduce new random variable Yo representing
observation. Fix enumeration G1 ; G2 ; : : : observable atoms obs(DB) define
Yo by21
(! ) = k

iff ! j= Gk ! 2
DB (k 1):
(5)
Let Gk T; Gk ; : : : ; Gk 2 obs(DB) random sample size . L() = Tt=1 PDB (Gk j
) = t=1 PDB (Yo = kt j ) qualifies likelihood function w.r.t. Yo .
second condition concerns reduction probability computation addition.
Take blood type exmaple. computation PDB (btype('A')) decomposed
summation explanations support set mutualy exclusive.
introduce
Q

Q1

2





b

Exclusiveness condition:

every G 2 obs(DB) support set DB (G), PDB (S ^ 0) = 0 6=
0 2 DB (G).
Using exclusiveness condition (and Proposition A.2 Appendix A),
PDB (G) =
PF (S ):
X

S2

DB

(G)

modeling point view, means single event, single observation,
G, may several (or even infinite) explanations DB (G), one DB (G) allowed
true observation.
introduce 9DB , i.e. set explanations relevant obs(DB)
9DB def
=
DB (G)
[

G2obs(DB)

fix enumeration S1; S2 ; : : : explanations 9DB . follows Proposition A.2,
uniqueness condition exclusiveness condition
PDB (Si ^ Sj ) = 0 6= j

PDB (S ) =
PDB (S )
X

29DB

X

X

G2obs(DB) 2

(G)
PDB (G)
DB

=
G2obs(DB)
= 1:
able introduce uniqueness condition exclusiveness condition
yet another random variable Xe, representing explanation G, defined
Xe (!) = k iff ! j= Sk ! 2
DB :
(6)
third condition concerns termination.

21.

X

G2obs(DB) PDB (G) = 1 guarantees measure f ! j ! j= Gk k ( 1)g one,
! satisfying Gk 's. case, put Yo (!) = 0. values set measure
zero affect part discussion follows. applies definition Xe (6).
P

403

fiSato & Kameya

Finite support condition:

every G 2 obs(DB) DB (G) finite.
PDB (G) computed support set DB (G) = fS1 ; : : : ; Sm g (0 m),
help exclusiveness condition, finite summation mi=1 PF (Si). condition
prevents infinite summation hardly computable.
fourth condition simplifies probability computation multiplication. Recall
explanation G 2 obs(DB) conjunction a1 ^ 1 11 ^ abducibles
fa1; : : : ; amg F (1 m). order reduce computation PF (S ) = PF (a1 ^11 1^ am)
multiplication PF (a1) 1 11 PF (am ), assume
P

Distribution condition:

F set Fmsw ground atoms parameterized distribution Pmsw specified below.

atom msw(i,n,v) intended simulate multi-ary random switch whose name
whose outcome v trial n. generalization primitive probabilistic events
coin tossing dice rolling.
1. Fmsw consists probabilistic atoms msw(i,n,v). arguments i, n v ground
terms called switch name, trial-id value (of switch i), respectively.
assume finite set Vi ground terms called value set associated
i, v 2 Vi holds.
2. Write Vi fv1 ; v2 ; : : : ; vmg (m = jVi j). Then, one ground atoms f msw(i,n,v1),
msw(i,n,v2 ), . .. , msw(i,n,vm )g becomes exclusively true (takes value 1)
trial. i, parameter i;v 2 [0; 1] v2V i;v = 1 associated. i;v
probability msw(i,1,v) true (v 2 Vi).
3. ground terms i, i0 , n, n0, v 2 Vi v0 2 Vi0 , random variable msw(i,n,v)
independent msw(i0 ,n0 ,v0 ) n 6= n0 6= i0 .
words, introduce family parameterized finite distributions P(i;n)
P(i;n)(msw(i,n,v1 ) = x1 ; : : : ; msw(i,n,vm ) = xm j i;v ; : : : ; i;v )
x
x
mk=1 xk = 1
def
= 0i;v 1 11 i;v o.w.
(7)
P



(

1

1

1




P



= jVij, xk 2 f0; 1g (1 k m), define Pmsw infinite product
Pmsw def
= P(i;n) :


i;n

condition, compute Pmsw(S ), probability explanation S,
product parameters. Suppose msw(ij ,n,v) msw(ij0 ,n0,v0) different conjuncts
explanation = msw(i1 ,n1,v1 ) ^ 11 1^ msw(ik ,nk ,vk ). either j 6= j 0 n 6= n0 holds,
independent construction. Else j = j 0 n = n0 v 6= v 0,
independent Pmsw(S ) = 0 construction. result, whichever condition may hold,
Pmsw (S ) computed parameters.
404

fiParameter Learning Logic Programs Symbolic-statistical Modeling

4.3 Modeling Principle

point, introduced four conditions, uniqueness condition, exclusiveness condition, finite support condition distribution condition, simplify
probability computation. last one easy satisfy. adopt Fmsw together
Pmsw . So, on, always assume Fmsw parameterized distribution Pmsw
introduced previous subsection. Unfortunately rest satisfied automatically. According modeling experiences however, mildly dicult satisfy
uniqueness condition exclusiveness condition long obey following
modeling principle.
Modeling principle: DB = Fmsw [ R describes sequential decision process
(modulo auxiliary computations) uniquely produces observable atom
G 2 obs(DB) decision expressed msw atom.22
Translated programming level, says must take care writing program sample F 0 Pmsw, must uniquely exist goal G (G 2 obs(DB))
successful refutation DB0 = F 0 [ R. confirm principle
blood type program DBb = Fb [ Rb. describes process gene inheritance,
arbitrary sample Fb0 Pmsw, say Fb0 = fmsw(gene,father,a); msw(gene,mother,o)g,
exists unique goal, btype('A') case, successful SLD refutation
Fb0 [ Rb.
idea behind principle decision process always produces result (an
observable atom), different decision processes must differ msw thereby entailing
mutually exclusive observable atoms. uniqueness condition exclusiveness
condition automatically satisfied.
Satisfying finite support condition dicult virtually equivalent
writing program DB solution search G (G 2 obs(DB)) always terminates. Apparently general solution problem, far specific models
HMMs, PCFGs Bayesian networks concerned, met. programs
models satisfy finite support condition (and conditions well).

4.4 Four Conditions Revisited

subsection, discuss relax four simplifying conditions introduced Subsection 4.2 purpose exible modeling. first examine uniqueness condition
considering crucial role adaptation EM algorithm semantics.
uniqueness condition guarantees exists (many-to-one) mapping
explanations observations EM algorithm applicable (Dempster et al., 1977).
possible, however, relax uniqueness condition justifying application
EM algorithm. assume MAR (missing random) condition introduced
Rubin (1976) statistical condition complete data (explanation) becomes incomplete data (observation), customarily assumed implicitly explicitly
statistics (see Appendix B). assuming MAR condition, apply EM
22. Decisions made process finite subset Fmsw .
405

fiSato & Kameya

algorithm non-exclusive observations P (O) 1 uniqueness
condition seemingly destroyed.
Let us see MAR condition action simple example. Imagine walk along
road front lawn. occasionally observe state \the road dry
lawn wet". Assume lawn watered sprinkler running probabilistically.
program DBrl = Rrl [ Frl Figure 2 describes sequential process outputs
observation observed(road(x),lawn(y)) (\the road x lawn y")
x; 2 fwet; dryg.
Rrl = { observed(road(X),lawn(Y)):P

Frl

=

msw(rain,once,A),
( = yes, X = wet, = wet
; = no, msw(sprinkler,once,B),
( B = on, X = dry, = wet
; B = off, X = dry, = dry ) ). }
{ msw(rain,once,yes), msw(rain,once,no),
msw(sprinkler,once,on), msw(sprinkler,once,off) }

Figure 2: DBrl
basic distribution Frl specified PF (1) Subsection 4.1, omit it.
msw(rain,once,A) program determines whether rains (A = yes) (A = no),
whereas msw(sprinkler,once,B) determines whether sprinkler works fine (B = on)
(B = off). Since sampled values = (a 2 fyes; nog) B = b
(b 2 fon; offg), uniquely exists observation observed(road(x),lawn(y)) (x; 2
fwet; dryg), many-to-one mapping : (a; b) = hx; yi. words,
apply EM algorithm observations observed(road(x),lawn(y)) (x; 2
fwet; dryg). would happen observe exclusively either state road
lawn? Logically, means observe 9y observed(road(x),lawn(y))
9x observed(road(x),lawn(y)). Apparently uniqueness condition met,
9y observed(road(wet),lawn(y)) 9x observed(road(x),lawn(wet)) compatible
(they true rains). Despite non-exclusiveness observations, still
apply EM algorithm MAR condition, case translates
observe either lawn road randomly regardless state.
brie check conditions. Basically relaxed cost
increased computation. Without exclusiveness condition instance, would need
additional process transforming support set DB (G) goal G set exclusive
explanations. instance, G explanations fmsw(a,n,v); msw(b,m,w)g,
transform fmsw(a,n,v); :msw(a,n,v) ^ msw(b,m,w)g on.23 Clearly,
transformation exponential number msw atoms eciency concern leads
assuming exclusiveness condition.
finite support condition practice equivalent condition SLD tree
G finite. relaxing condition might induce infinite computation.
b

23. :msw(a,n,v ) transformed disjunction exclusive msw atoms
406

W

6

0
2 msw(a,n,v ).

v 0 =v;v 0 Va

fiParameter Learning Logic Programs Symbolic-statistical Modeling

Relaxing distribution condition accepting probability distributions
serve expand horizon applicability parameterized logic programs.
particular introduction parameterized joint distributions P (v1; : : : ; vk ) Boltzmann distributions switches msw1 ; : : : ; mswk v1; : : : ; vk values switches,
makes correlated. distributions facilitate writing parameterized logic programs
complicated decision processes decisions independent interdependent. Obviously, hand, increase learning time, whether added
exibility distributions deserves increased learning time yet seen.
Pmsw

4.5 Naive Approach EM Learning

subsection, derive concrete EM algorithm parameterized logic programs
DB = Fmsw [ R assuming satisfy uniqueness condition, exclusiveness
condition finite support condition.
start, introduce Yo, random variable representing observations according
(5) based fixed enumeration observable atoms obs(DB). introduce
another random variable Xe representing explanations according (6) based
fixed enumeration explanations 9DB . understanding Xe non-observable
Yo observable, joint distribution PDB (Xe = x; Yo = j )
denotes relevant parameters. immediate, following (1) (2) Section 2,
derive concrete EM algorithm Q function defined Q( j 0 ) def
= x PDB (x j
y; 0) ln PDB (x; j ) whose input random sample observable atoms whose output
MLE .
following, sake readability, substitute observable atom G (G 2
obs(DB)) Yo = write PDB (G j ) instead PDB (Yo = j ). Likewise
substitute explanation (S 2 9DB ) Xe = x write PDB (S; G j ) instead
PDB (Xe = x; Yo = j ). follows uniqueness condition
0
62 DB (G)
PDB (S; G j ) =
Pmsw (S j ) 2 DB (G):
need yet another notation here. explanation S, define count msw(i,n,v)

i;v (S ) def
= jf n j msw(i,n,v) 2 gj :
done preparations now. Suppose make observations G = G1 ; : : : ; GT
Gt 2 obs(DB) (1 ). Put
def
= j msw(i,n,v) 2 2 DB (Gt); 1 g
def
= fi;v j msw(i,n,v) 2 2 DB (Gt); 1 g:
set switch names appear explanation one Gt 's denotes
parameters associated switches. finite due finite support condition.
P

(

407

fiSato & Kameya

Various probabilities Q function computed using Proposition A.2
Appendix together assumptions follows.
PDB (Gt j ) = PDB
=
Pmsw (S j )
(8)
DB (Gt )




_

Pmsw (S j )

=

Q( j 0 ) def
=

=

(i; v; ) def
=



t=1 S29DB
i2I;v2Vi

X
t=1

X

S2

i;v (S )
i;v

i2I;v2Vi

X
X

X



DB

(Gt )

PDB (S j Gt ; 0 ) ln PDB (S; Gt j )

(i; v; 0 )ln i;v

1

PDB (Gt j ) 2

X

i2I;v2Vi

X

DB

(Gt )

!
(i; v; 0 )
0
(i; v; ) ln P
0 0
v0 2Vi (i; v ; )

(9)

Pmsw (S j )i;v (S )

used Jensen's inequality obtain (9). Note PDB (Gt j )01 S2 (G )
Pmsw (S j )i;v (S ) expected count msw(i,1,v) SLD refutation Gt. Speaking
likelihood function L() = Tt=1 PDB (Gt j ), already shown Subsection 2.2
(footnote) Q( j 0 ) Q(0 j 0 ) implies L() L(0 ). Hence (9), reach
procedure learn-naive( ,G) finds MLE parameters. array
variable [i; v] stores (i; v; ) current .
P

DB



Q

DB

1:
2:
3:
4:
5:
6:

procedure

learn-naive(DB; G )

begin

Initialize appropriate values " small positive number ;
(0) := t=1 ln PDB (Gt j );
% Compute log-likelihood.
P

repeat
foreach

2 I; v 2 Vi

[i; v ] :=


X

1

PDB (Gt j ) 2
foreach 2 I; v 2 Vi
[i; v]
i;v := P
0;
0
v 2Vi [i; v ]
:= +P1;
(m) := Tt=1 ln PDB (Gt j )
(m) 0 (m01) < "

7:
8:
9:
10:
11:
12: end

t=1

X

DB

(Gt )

Pmsw (S j )i;v (S );

% Update parameters.
% Compute log-likelihood again.
% Terminate converged.

EM algorithm simple correctly calculates MLE , calculation PDB (Gt j ) [i; v](Line 3, 6 10) may suffer combinatorial explosion
explanations. is, j DB (Gt)j often grows exponentially complexity model.
instance, j DB (Gt)j HMM N states O(N L), exponential length L
input/output string. Nonetheless, suppressing explosion realize ecient computation polynomial order possible, suitable conditions, avoiding multiple
computations subgoal see next.
408

fiParameter Learning Logic Programs Symbolic-statistical Modeling

4.6 Inside Probability Outside Probability Logic Programs

subsection, generalize notion inside probability outside probability
(Baker, 1979; Lari & Young, 1990) logic programs. Major computations learn-naive( ,G)
two terms Line 6, PDB (Gt j ) S2 (G ) Pmsw(S j )i;v (S). Computational redundancy lurks naive computation terms. show example.
Suppose propositional program DBp = Fp [ Rp Fp = fa; b; c; d; mg
DB

P

DB

8
>
>
>
>
>
<

Rp = >
>
>
>
>
:

f
f
g
g
h



a^g
b^g
c
d^h
m:

(10)

f observable atom. assume a, b, c, independent
fa; bg fc; dg pair-wise exclusive. support set f calculated
DB (f) = fa ^ c; ^ ^ m; b ^ c; b ^ ^ g:
Hence, light (8), may compute PDB (f)
PDB (f) = PF (a ^ c) + PF (a ^ ^ m) + PF (b ^ c) + PF (b ^ ^ m):
(11)
computation requires 6 multiplications (because PF (a ^ c) = PF (a)PF (c) etc.)
3 additions. hand, possible compute PDB (f) much eciently
factoring common computations. Let ground atom. Define inside probability
(A)
(A) def
= PDB (A j ):24
(12)
applying Theorem A.1 Appendix
comp(Rp) ` f $ (a ^ g) _ (b ^ g); g $ c _ (d ^ h); h $
(13)
unconditionally holds semantics, using independent exclusiveness assumption made Fp, following equations inside probability
derived.
(f) = (a)fi (g) + (b)fi (g)
(g) = (c) + (d)fi (h)
(14)
(h) = (m)
PDB (f)(= (f)) obtained solving (14) (f), 3 multiplications
2 additions required.
quite straightforward generalize (14) proceeding, look program
DBq = fmg [ fg:-m ^ m; g:-mg g observable atom msw atom.
g $ (m ^ m) _ semantics, compute P (g) = P (m)P (m) + P (m)
clearly wrong ignores fact clause bodies g, i.e. m^m mutually
exclusive, atoms clause body m^m independent (here P (1) = PDB (1)).
Similarly, set = b = c = = m, equation (14) totally incorrect.
p

p

p

p

p

p

p

p

p

p

p

8
>
<
>
:

p

q

24. Note fact F , (A) = Pmsw (A j ).

409

fiSato & Kameya

therefore add, temporarily subsection, two assumptions top exclusiveness condition finite support condition equations (14) become
mathematically correct. first assumption \clause" bodies mutually exclusive i.e. two clauses B W B W 0 , PDB (W ^ W 0 j ) = 0,
second assumption body atoms independent, i.e. B1 ^ 1 1 1 ^ Bk rule,
PDB (B1 ^ 11 1 ^ Bk j ) = PDB (B1 j ) 11 1 PDB (Bk j ) holds.
Please note \clause" used subsection special meaning. intended
mean G G goal tabled explanation G obtained OLDT search
explained next subsection.25 words, additional
conditions imposed source program result OLDT search.
clauses auxiliary computations need satisfy them.
suppose clauses occur DB



B1;1 ^ 1 1 1 ^ B1;i1

11 1

BL;1 ^ 11 1 ^ BL;iL

Bh;j (1 h L; 1 j ih) atom. Theorem A.1 Appendix
assumptions ensure


(A) = (B1;j ) + 1 11 + (BL;j ):
(15)
1


L


j =1

j =1

(15) suggests (Gt) considered function (A) equations
inside probabilities hierarchically organized way fi(Gt) belongs top
layer fi(A) appearing left hand side refers (B)'s belong
lower layers. refer condition acyclic support condition. acyclic
support condition, equations form (15) unique solution, computation
PDB (G j ) via inside probabilities allows us take advantage reusing intermediate
results stored (A), thereby contributing faster computation PDB (Gt j ).
Next tackle intricate problem, computation S2 (G ) Pmsw(S j
)i;v (S ). Since sum equals n msw(i,n,v )2S 2 (G ) Pmsw (S j ), concentrate
computation
(Gt; m) def
=
Pmsw (S j )
P

P

DB

P

DB





X

( )

m2S 2 DB Gt

= msw(i,n,v). First note explanation contains = a1 ^1 11^
ah ^ m, (S ) = (a1 ) 1 1 1 (ah)fi (m). (Gt ; m) expressed
(Gt; m) = ff(Gt ; m)fi (m)

(16)
ff(Gt; m) = @@fi(G(m;)m) ff(Gt; m) depend (m). Generalizing observation arbitrary ground atoms, introduce outside probability ground atom
w.r.t. Gt
(Gt)
ff(Gt; A) def
= @fi
@fi (A)


25. logical relationship (13) corresponds (20) f, g h table atoms.
410

fiParameter Learning Logic Programs Symbolic-statistical Modeling

assuming conditions inside probability. view (16), problem computing (Gt; m) reduced computing ff(Gt; m), recursively computable
follows. Suppose occurs ground program DB
B1
BK

^ W1;1 ; 11 1 ; B1

11 1

^ WK;1 ; 11 1 ; BK

^ W1;i1
^ WK;iK :

(Gt) function (B1 ); : : : ; fi(BK ) assumption, chain rule derivatives
leads
@fi (Gt )
@fi (A ^ WK;i )
@fi (Gt ) @fi (A ^ W1;1 )
+
11
1
+
ff(Gt ; A) =
@fi (B1 )
@fi (A)
@fi (BK )
@fi (A)
hence to26
ff(Gt; Gt ) = 1
(17)


ff(Gt ; A) = ff(Gt ; B1 ) (W1;j ) + 11 1 + ff(Gt ; BK ) (WK;j ):
(18)












K

1
X

K
X

j =1

j =1

Therefore inside probabilities already computed, outside probabilities
recursively computed top (17) using (18) downward along program layers.
case DBp f chosen atoms, compute
ff(f; f) = 1
ff(f; g) = (a) + (b)
(19)
ff(f; h) = ff(f; g)fi (d)
ff(f; m) = ff(f; h):
(19), desired sum (f; m) calculated
(f; m) = ff(f; m)fi (m) = (fi (a) + (b))fi (d)fi (m)
requires two multiplications one addition compared four multiplications
one addition naive computation.
Gains obtained computing inside outside probability may small case,
problem size grows, become enormous, compensate enough additional restrictions imposed result OLDT search.
8
>
>
>
<
>
>
>
:

4.7 OLDT Search

compute inside outside probability recursively (15) (17) (18), need
programming level tabulation mechanism structure-sharing partial explanations
26. independence assumption body atoms, Wh;j (1 h K; 1 j ih )
independent. Therefore
@fi (A ^ Wh;j ) = @fi (A)fi (Wh;j ) = (W ):
h;j
@fi(A)
@fi (A)
411

fiSato & Kameya

subgoals. henceforth deal programs DB set table(DB) table
predicate declared advance. ground atom containing table predicate called
table atom. purpose table atoms store support sets eliminate
need recomputation, so, construct hierarchically organized explanations
made table atoms msw atoms.
Let DB = Fmsw [ R parameterized logic program satisfies finite support
condition uniqueness condition. let G1 ; G2; : : : ; GT random sample
observable atoms obs(DB). make following additional assumptions.

Assumptions:

(1 ), exists finite set f1t; : : : ; Kt g table atoms associated
(0 k K ; 1 j )
conjunctions Sk;j

k


e

comp(R)





` Gt $ S0t;1 _ 1 1 1 _ S0t;m
^ 1t $ S1t;1 _ 11 1 _ S1t;m ^ 1 11 ^ Kt $ SKt ;1 _ 11 1 _ SKt ;mKt
e



e

e

0

e





e

1

e

(20)




(0 k K ; 1 j ) is, set, subset F
Sk;j

msw [ fk+1 ; : : : ; K g
k
(acyclic support condition). convention, put 0 = Gt call respectively
def
(k 0) t-explanation
DB
= f0 ; 1t; : : : ; Kt g set table atoms Gt Sk;j
kt .27 set t-explanations k denoted DB (kt ) consider
DB (1) function table atoms.
^ St ) =
t-explanations mutually exclusive, i.e. k (0 k Kt ), PDB (Sk;j
k;j 0
0 (1 j 6= j 0 mk ) (t0exclusiveness condition).
(0 k K ; 1 j ) conjunction independent atoms (independent
Sk;j

k
condition).28
assumptions aimed ecient probability computation. Namely, acyclic
support condition makes dynamic programming possible, t-exclusiveness condition reduces PDB (A _ B) PDB (A)+ PDB (B) independent condition reduces PDB (A ^ B )
PDB (A)PDB (B). one point concerning eciency however. Note
29 imcomputation dynamic programming proceeds following partial order DB
posed acyclic support condition access table atoms much simplified
respecting said partial
linearly ordered. therefore topologically sort DB

order call linearized DB satisfying three assumptions (the acyclic support condition, t-exclusiveness condition independent condition) hierarchical system
= h ; ; : : : ; ( = G ) assuming
t-explanations Gt . write DB
0

DB (1)
0 1
K
30
implicitly given. hierarchical system t-explanations Gt successfully built
e



e



e

e

e

e

e



e

27. Prefix \t-" abbreviation \tabled-".
28. independence mentioned concerns positive propositions. B1 ; B2 2 head(DB), say
B1 B2 independent PDB (B1 ^ B2 j ) = PDB (B1 j )PDB (B2 j ) .
29. precedes j top-down execution w.r.t. DB invokes j directly indirectly.
30. holds precedes j < j .
412

fiParameter Learning Logic Programs Symbolic-statistical Modeling

source program, equations inside probability outside probability
(14) (19) automatically derived solved time proportional size
equations. plays central role approach ecient EM learning.
One way obtain t-explanations use OLDT search (Tamaki & Sato, 1986;
Warren, 1992), complete refutation method logic programs. OLDT search,
goal G called first time, set entry G solution table store
answer substitutions G there. call instance G0 G occurs later, stop
solving G0 instead try retrieve answer substitution G stored solution table
unifying G0 G. record remaining answer substitutions G, prepare
lookup table G0 hold pointer them.
self-containedness, look details OLDT search using sample program
DBh = Fh [Rh Figure 431 depicts HMM32 Figure 3. HMM two states
fs0; s1g. state transition, probabilistically chooses next destination fs0; s1g
a,b
s1

s0

a,b

a,b

a,b

Figure 3: Two state HMM
Fh

Rh

=

=

8
<
:
8
>
>
>
>
>
>
>
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
>
>
>
>
>
>
>
:

f1: values(init, [s0,s1]).
f2: values(out(_),[a,b]).
f3: values(tr(_), [s0,s1]).

h1: hmm(Cs):msw(init,once,Si),
hmm(1,Si,Cs).
h2: hmm(T,S,[C|Cs]):- T=<3,
msw(out(S),T,C),
msw(tr(S),T,NextS),
T1 T+1,
hmm(T1,NextS,Cs).
h3: hmm(T,_,[]):- T>3.

%
%
%
%
%
%
%
%
%

generate string (chars) Cs,
Set initial state Si,
Enter loop clock = 1.
Loop:
Output C state S.
Transit NextS.
Put clock ahead.
Continue loop (recursion).
Finish loop clock > 3.

Figure 4: Two state HMM program DBh
31. f1, f2, f3, h1, h2 h3 temporary marks, part program.
32. HMM defines probability distribution strings given set alphabets, works
stochastic string generator (Rabiner & Juang, 1993) output string sample
defined distribution.
413

fiSato & Kameya

alphabet fa; bg emit. Note specify fact set Fh associated distribution compactly, introduce new notation values(i,[v1,...,vm]).
declares Fh contains msw atoms form msw(i,n,v) (v 2 fv1 ; : : : ; vmg) whose distribution P(i;n) given (7) Subsection 4.2. example, (f3), values(tr( ),[s0,s1])
introduces msw(tr(t),n,v) atoms program ground term,
v 2 fs0; s1g ground term n, distribution
x
P(tr(t);n) (msw(tr(t),n,s0) = x; msw(tr(t),n,s1) = j i;s0 ; i;s1) = i;s
0 i;s1
= tr(t), x; 2 f0; 1g x + = 1.
program runs Prolog program. non-ground top-goal hmm(S), functions stochastic string generator returning list alphabets [a,b,a]
variable follows. top-goal calls clause (h1) (h1) selects initial state executing subgoal msw(init,once,Si)33 returns Si initial state probabilistically
chosen fs0, s1g. second clause (h2) called (h1) ground ground
T. makes probabilistic choice output alphabet C asking msw(out(S),T,C)
determines NextS, next state, asking msw(tr(S),T,NextS). (h3)
stop transition. simplicity, length output strings fixed three. way
execution termed sampling execution corresponds random sampling
PDB . top-goal ground hmm([a,b,a]), works acceptor, i.e.
returning success (yes) failure (no).
explanations hmm([a,b,a]) sought for, keep msw atoms resolved upon
refutation conjunction (explanation), repeat process backtracking refutation found. need t-explanations however, backtracking must
abandoned sharing partial explanations t-explanations, purpose
t-explanations itself, becomes impossible. therefore instead use OLDT search
h

t1:
t2:
t3:
t4:
t4':
:
t7:
t8:
t9:

top_hmm(Cs,Ans):- tab_hmm(Cs,Ans,[]).
tab_hmm(Cs,[hmm(Cs)|X],X):- hmm(Cs,_,[]).
tab_hmm(T,S,Cs,[hmm(T,S,Cs)|X],X):- hmm(T,S,Cs,_,[]).
e_msw(init,T,s0,[msw(init,T,s0)|X],X).
e_msw(init,T,s1,[msw(init,T,s1)|X],X).
hmm(Cs,X0,X1):- e_msw(init,once,Si,X0,X2), tab_hmm(1,Si,Cs,X2,X1).
hmm(T,S,[C|Cs],X0,X1):T=<3, e_msw(out(S),T,C,X0,X2), e_msw(tr(S),T,NextS,X2,X3),
T1 T+1, tab_hmm(T1,NextS,Cs,X3,X1).
hmm(T,S,[],X,X):- T>3.

Figure 5: Translated program DBh
33. msw(i,n,V) called ground ground n, V, logical variable, behaves random variable.
instantiated term v probability i;v selected value set Vi declared values
atom. If, hand, V ground term v called, procedural semantics msw(i,n,v)
equal msw(i,n,V) ^ V = v.
414

fiParameter Learning Logic Programs Symbolic-statistical Modeling

t-explanation search. case HMM program example, build hierarchical
system t-explanations hmm([a,b,a]) OLDT search, first declare hmm=1
hmm=3 table predicate.34 t-explanation conjunction hmm=1 atoms, hmm=3
atoms msw atoms. translate program another logic program, analogously translation definite clause grammars (DCGs) Prolog (Sterling & Shapiro,
1986). add two arguments (which forms D-list) predicate purpose
accumulating msw atoms table atoms conjuncts t-explanation. translation
applied DBh yields program Figure 5.
translated program, clause (t1) corresponds top-goal hmm(l)
input string l, t-explanation table atom hmm(l) returned Ans. (t2)
(t3) auxiliary clauses add callee's D-list table atom form hmm(l)
hmm(t,s,l) respectively (t: time step, s: state). general, p=n table predicate
original program, p=(n + 2) becomes table predicate translated program
auxiliary predicate tab p=(n +2) inserted signal OLDT interpreter check
solution table p=n, i.e. check already exist t-explanations p=n. Likewise
clauses (t4) (t4') pair corresponding (f1) insert msw(init,T,1)
callee's D-list = once. Clauses (t7), (t8) (t9) respectively correspond (h1),
(h2) (h3).
hmm([a,b,a]):[hmm([a,b,a])]
[ [msw(init,once,s0), hmm(1,s0,[a,b,a])],
[msw(init,once,s1), hmm(1,s1,[a,b,a])] ]
hmm(1,s0,[a,b,a]):[hmm(1,s0,[a,b,a])]
[ [msw(out(s0),1,a), msw(tr(s0),1,q0), hmm(2,s0,[b,a])],
[msw(out(s0),1,a), msw(tr(s0),1,s1), hmm(2,s1,[b,a])] ]
hmm(1,s1,[a,b,a]):[hmm(1,s1,[a,b,a])]
[ [msw(out(s1),1,a), msw(tr(s1),1,s0), hmm(2,s0,[b,a])],
[msw(out(s1),1,a), msw(tr(s1),1,s1), hmm(2,s1,[b,a])] ]
hmm(2,s0,[b,a]):[hmm(2,s0,[b,a])]
[ [msw(out(s0),2,b), msw(tr(s0),2,s0), hmm(3,s0,[a])],
[msw(out(s0),2,b), msw(tr(s0),2,s1), hmm(3,s1,[a])] ]
hmm(2,s1,[b,a]):[hmm(2,s1,[b,a])]
[ [msw(out(s1),2,b), msw(tr(s1),2,s0), hmm(3,s0,[a])],
[msw(out(s1),2,b), msw(tr(s1),2,s1), hmm(3,s1,[a])] ]
hmm(3,s0,[a]):[hmm(3,s0,[a])]
[ [msw(out(s0),3,a), msw(tr(s0),3,s0), hmm(4,s0,[])],
[msw(out(s0),3,a), msw(tr(s0),3,s1), hmm(4,s1,[])] ]
hmm(3,s1,[a]):[hmm(3,s1,[a])]
[ [msw(out(s1),3,a), msw(tr(s1),3,s0), hmm(4,s0,[])],
[msw(out(s1),3,a), msw(tr(s1),3,s1), hmm(4,s1,[])] ]
hmm(4,s0,[]):[hmm(4,s0,[])]
[[]]
hmm(4,s1,[]):[hmm(4,s1,[])]
[[]]

Figure 6: Solution table
34. general, p=n means predicate p arity n. although hmm=1 hmm=3 share predicate name
hmm, different predicates.
415

fiSato & Kameya

translation, apply OLDT search top hmm([a,b,a],Ans) noting (i) added D-list uence OLDT procedure, (ii) associate
solution table atom solution table list t-explanations. resulting
solution table shown Figure 6. first row reads call hmm([a,b,a]) occurred entered solution table solution, hmm([a,b,a]) (no variable binding generated), two t-explanations, msw(init,once,s0) ^ hmm(1,s0,[a,b,a])
msw(init,once,s1) ^ hmm(1,s1,[a,b,a]). remaining task topological sorting table atoms stored solution table respecting acyclic support condition.
done using depth-first search (trace) t-explanations top-goal
example. Thus obtain hierarchical system t-explanations hmm([a,b,a]).

4.8 Support Graphs

Looking back, need compute inside outside probability hierarchical system
t-explanations, essentially boolean combination primitive events (msw atoms)
compound events (table atoms) intuitively representable
graph. reason, help visualizing learning algorithm, introduce new
data-structure termed support graphs, though new EM algorithm next subsection
described solely hierarchical system t-explanations.
illustrated Figure 7 (a), support graph Gt graphical representation
= h ; ; : : : ; ( = G ) G (20).
hierarchical system t-explanations DB


0 1
0
K
consists totally ordered disconnected subgraphs, labeled
(0 k K ). subgraph labeled comprises two
corresponding table atom kt DB

k
special nodes (the start node end node) explanation graphs, corresponding


t-explanation Sk;j
DB (k ) (1 j mk ).
linear graph node labeled either
explanation graph Sk;j
. called table node switch
table atom switch msw(1,1,1) Sk;j
node respectively. Figure 7 (b) support graph hmm([a,b,a]) obtained
solution table Figure 6. table node labeled refers subgraph labeled ,
data-sharing achieved distinct table nodes referring subgraph.


e

e

e

e

4.9 Graphical EM Algorithm

describe ecient EM learning algorithm termed graphical EM algorithm
(Figure 8) introduced Kameya Sato (2000), runs support graphs. Suppose
random sample G = G1; : : : ; GT observable atoms. suppose support
graphs Gt (1 ), i.e. hierarchical systems t-explanations satisfying acyclic
support condition, t-exclusiveness condition independent condition,
successfully constructed parameterized logic program DB satisfying uniqueness
condition finite support condition.
graphical EM algorithm refines learn-naive( ,G ) introducing two subroutines,
get-inside-probs(
, G ) compute inside probabilities get-expectations(
, G ) compute outside probabilities. called main routine learn-gEM( ,G ).
learning, prepare four arrays support graph Gt G :
P [t; ] inside probability , i.e. ( ) = PDB ( j ) (see (12))
DB

DB

DB

DB

416

fiParameter Learning Logic Programs Symbolic-statistical Modeling
k

(a)

explanation graph

msw

Gt:
start

k

msw

end

msw

msw

msw

:
k :
start

end
msw

msw

:

(b)
msw(init,once,s0)

hmm(1,s0,[a,b,a])

hmm([a,b,a]):

start

end

msw(init,once,s1)

msw(out(s0),1,a)

hmm(1,s1,[a,b,a])

msw(tr(s0),1,s0)

hmm(2,s0,[b,a])

hmm(1,s0,[a,b,a]):

end

start

msw(out(s0),1,a)

msw(tr(s0),1,s1)

hmm(2,s1,[b,a])

msw(out(s1),1,a)

msw(tr(s1),1,s0)

hmm(2,s0,[b,a])

hmm(1,s1,[a,b,a]):

end

start

msw(out(s1),1,a)

msw(tr(s1),1,s1)

hmm(2,s1,[b,a])

Figure 7: support graph (a) general form, (b) Gt = hmm([a,b,a]) HMM
program DBh. double-circled node refers table node.
Q[t; ] outside probability w.r.t. Gt , i.e. ff(Gt; ) (see (17) (18))
R[t; ; ] explanation probability (2 DB (kt )), i.e. PDB (S j )
e

e

417

e

e

fiSato & Kameya
1: procedure learn-gEM (DB; G )
2: begin
3: Select initial
4:
5:
6:
7:
8:
9:

1: procedure get-inside-probs (DB; G )
2: begin
3: := 1 begin
4:
Let 0t = Gt;
5:
k := Kt downto 0 begin
6:
P [t; kt ] := 0;
7:
foreach Se 2 eDB (kt ) begin
8:
Let Se = fA1 ; A2 ; : : : ; AjSejg;
9:
R[t; kt ; Se] := 1;
10:
l := 1 jSej
11:
Al = msw(i,1,v )
12:
R[t; kt ; Se] 3 = i;v
13:
else R[t; kt ; Se] 3 = P [t; Al ];
14:
P [t; kt ]+= R[t; kt ; Se]
15:
end /* foreach Se */
16:
end /* k */
17: end /* */
18: end.

parameters;

get-inside-probs
(DB; G);
P
(0) := Tt=1 ln P [t; Gt ];
repeat

get-expectations (DB; G );
2 I; v 2 Vi
[i; vP
] :=
[t; i; v]=P [t; G ];

t=1
foreach 2 I; v P
2 Vi
i;v := [i; v]= v0 2Vi [i; v0 ];
get-inside-probs (DB; G );
:= +
1;
P
(m) := Tt=1 ln P [t; Gt ]
(m) 0 (m01) < "
foreach

10:
11:
12:
13:
14:
15:
16: end.

1: procedure get-expectations (DB; G ) begin
2: := 1 begin
3:
foreach 2 I; v 2 Vi [t; i; v] := 0;
4:
Let 0t = Gt; Q[t; 0t] := 1:0;
5:
k := 1 Kt Q[t; kt ] := 0;
6:
k := 0 Kt
7:
foreach Se 2 eDB (kt ) begin
8:
Let Se = fA1 ; A2 ; : : : ; AjSejg;
9:
l := 1 jSej
10:
Al = msw(i,1,v ) [t; i; v] += Q[t; kt ] 1 R[t; kt ; Se]
11:
else Q[t; Al ] += Q[t; kt ] 1 R[t; kt ; Se]=P [t; Al ]
12:
end /* foreach Se */
13: end /* */
14: end.

Figure 8: graphical EM algorithm.
[t; i; v] expected count msw(i,1,v), i.e.

P

S2

DB

(Gt) Pmsw (S j )i;v (S )

call procedure learn-gEM( ,G) Figure 8. main routine learn-gEM( ,G) initially computes inside probabilities (Line 4) enters loop get-expectations( ,G )
called first compute expected count [t; i; v] msw(i,1,v) parameters updated (Line 11). Inside probabilities renewed using updated parameters
entering next loop (Line 12).
DB

DB

DB

418

fiParameter Learning Logic Programs Symbolic-statistical Modeling

subroutine get-inside-probs( ,G ) computes inside probability ( ) = PDB ( j )
(and stores P [t; ]) table atom bottom layer topmost layer 0 =
Gt (Line 4) hierarchical system t-explanations Gt (see (20) Subsection 4.6).
takes t-explanation DB (kt ) one one (Line 7), decomposes conjuncts
multiplies inside probabilities either known (Line 12) already computed
(Line 13).
subroutine get-expectations( ,G ) computes outside probabilities following
recursive definitions (17) (18) Subsection 4.6 stores outside probability
ff(Gt; ) table atom Q[t; ]. first sets outside probability top-goal
0 = Gt 1:0 (Line 4) computes rest outside probabilities (Line 6) going
layers t-explanation Gt described (20) Subsection 4.6. (Line 10) adds
Q[t; kt ] 1 R[t; kt ; ] = ff(Gt ; kt ) 1 fi(S ) [t; i; v], expected count msw(i,1,v),
contribution msw(i,1,v) kt [t; i; v]. (Line 11) increments outside
probability Q[t; Al ] = ff(Gt; Al ) Al according equation (18). Notice Q[t; kt ]
already computed R[t; kt ; S]=P [t; Al ] = fi(W ) = Al ^ W . shown
Subsection 4.5, learn-naive( ,G ) MLE procedure, hence following theorem holds.
Theorem 4.1 Let DB parameterized logic program, G = G1; : : : ; GT ranDB

e

e

e

DB

e

e

e

e

e

DB

dom sample observable atoms. Suppose five conditions (uniqueness, finite support
(Subsection 4.2), acyclic support, t-exclusiveness independence (Subsection 4.7))
met. ThenQlearn-gEM (DB; G ) finds MLE 3 (locally) maximizes likelihood
L(G j ) = Tt=1 PDB (Gt j ).

(Proof) Sketch.35 Since main routine learn-gEM( ,G ) learn-naive( ,G)
except computation [i; v] = Tt=1 [t; i; v], show [t; i; v] = S2 (G ) Pmsw(S j
)i;v (S ) (= n msw(i,n,v )2S2 (G ) Pmsw (S j )). However,
DB

P

P

DB

P

DB

[t; i; v]

DB

P

=

X

X

0kKt





X

n msw(i,n,v )2Se2 e ( )
DB k

ff(Gt ; kt )fi (Se)

(see (Line 10) get-expectations(DB; G))
= ff(Gt; msw(i,n,v))fi(msw(i,n,v))
n
= (Gt; msw(i,n,v)) (see equation (16))
n
=
Pmsw (S j ):
Q.E.D.
X

X

X

X

n msw(i,n,v )2S2

DB

(Gt )

used fact contains msw(i,n,v) = S0 ^ msw(i,n,v), fi(S) =
(S 0 )fi (msw(i,n,v )) holds, hence
ff(Gt ; kt )fi (S ) = ff(Gt ; kt )fi (S 0 )fi (msw(i,n,v ))
= (contribution msw(i,n,v) kt ff(Gt; msw(i,n,v)))fi (msw(i,n,v)):
e

e

e

e

e

e

e

e

35. formal proof given Kameya (2000). proved common parameters , [i; v]
learn-naive(DB,G ) coincides [i; v] learn-gEM(DB,G ). So, parameters updated
values. Hence, starting initial values, parameters converge
values.
419

fiSato & Kameya

five conditions applicability graphical EM algorithm may look hard
satisfy once. Fortunately, modeling principle Section 4.3 still stands,
due care modeling, likely lead us program meets them. Actually,
see next section, programs standard symbolic-statistical frameworks
Bayesian networks, HMMs PCFGs satisfy five conditions.
5. Complexity

section, analyze time complexity graphical EM algorithm applied
various symbolic-statistical frameworks including HMMs, PCFGs, pseudo PCSGs
Bayesian networks. results show graphical EM algorithm competitive
specialized EM algorithms developed independently research field.

5.1 Basic Property

Since EM algorithm iterative algorithm since unable predict
converges, measure time complexity time taken one iteration. therefore
estimate time per iteration repeat loop learn-gEM (DB; G) (G = G1 ; : : : ; GT ).
observe one iteration, support graph Gt (1 ) scanned twice,
get-inside-probs (DB; G ) get-expectations (DB; G). scan, addition
performed t-explanations, multiplication (possibly division) performed
msw atoms table atoms each. time spent Gt per iteration
graphical EM algorithm linear size support graph, i.e. number nodes
support graph Gt. Put
1tDB def
=
DB ( )
e

[

e


2DB

num def
= 1max
j1e j
tT DB
maxsize def
=
max
jSej:

e
e
1tT;S 21DB
set table atoms G , hence 1t set t-explanations
Recall DB

DB
appearing right hand side (20) Subsection 4.7. num maximum number
t-explanations support graph Gt's maxsize maximum size texplanation Gt's respectively. following obvious.
e

Proposition 5.1 time complexity graphical EM algorithm per iteration linear

total size support graphs, (nummaxsize ) notation, coincides
space complexity graphical EM algorithm runs support graphs.

rather general result, compare graphical EM algorithm
EM algorithms, must remember input graphical EM algorithm
support graphs (one observed atom) actual total learning time
OLDT time + (the number iterations) 2O(nummaxsizeT )
420

fiParameter Learning Logic Programs Symbolic-statistical Modeling

\OLDT time" denotes time construct support graphs G . sum
time OLDT search time topological sorting table atoms,
latter part former order-wise,36 represent \OLDT time" time OLDT
search. observe total size support graphs exceed time OLDT
search G order-wise.
evaluate OLDT time specific class models HMMs, need know
time table operations. Observe OLDT search paper special
sense table atoms always ground called resolution solved
goals. Accordingly solution table used
check goal G already entry solution table, i.e. called
before,
add new searched t-explanation G list discovered t-explanations
G's entry.
time complexity operations equal table access depends
program implementation solution table.37 first suppose
programs carefully written way arguments table atoms used indecies table access integers. Actually programs used subsequent complexity
analysis (DBh Subsection 4.7, DBg DBg0 Subsection 5.3, DBG Subsection 5.5)
satisfy satisfy condition replacing non-integer terms appropriate integers. suppose solution table implemented using array table
access done O(1) time.38
follows, present detailed analysis time complexity graphical
EM algorithm applied HMMs, PCFGs, pseudo PCSGs Bayesian networks, assuming
O(1) time access solution table. remark way space complexity
total size solution tables (support graphs).


5.2 HMMs

standard EM algorithm HMMs Baum-Welch algorithm (Rabiner, 1989; Rabiner & Juang, 1993). example HMM shown Figure 3 Subsection 4.7.39 Given
observations w1 ; : : : ; wT output string length L, computes (N 2 LT ) time
iteration forward probability fftm(q) = P (ot1 ot2 1 11 otm01; q j ) backward
probability fimt (q) = P (otm otm+1 1 1 1 otL j q; ) state q 2 Q, time step (1 L)
string wt = ot1 ot2 11 1 otL (1 ), Q set states N number
states. factor N 2 comes fact every state N possible destinations

36. Think OLDT search top-goal Gt . searches msw atoms table atoms create solution table, auxiliary computations. Therefore time complexity never less
O(jthe number msw atoms table atoms support graph Gt j), coincides
time need topologically sort table atoms solution table depth-first search 0 = Gt .
37. Sagonas et al. (1994) Ramakrishnan et al. (1995) discuss implementation OLDT.
38. arrays available, may able use balanced trees, giving O(log n) access time n
number data solution table, may able use hashing, giving average (1) time
access certain condition (Cormen, Leiserson, & Rivest, 1990).
39. treat \state-emission HMMs" emit symbol depending state. Another type,
\arc-emission HMMs" emitted symbol depends transition arc, treated similarly.
421

fiSato & Kameya

compute forward backward probability every destination every
state. computing ffmt (q)'s fimt (q)'s, parameters updated. So, total
computation time iteration Baum-Welch algorithm estimated O(N 2LT )
(Rabiner & Juang, 1993; Manning & Schutze, 1999).
compare result graphical EM algorithm, use HMM program
DBh Figure 4 appropriate modifications L, length string, Q,
state set, declarations Fh output alphabets. string w = o1o2 11 1 oL,
hmm(n,q ,[om ; om+1 ; : : : ; oL ]) DBh reads HMM state q 2 Q time n
output [om ,om+1 ,...,oL] reaches final state. declaring hmm=1
hmm=3 table predicate translation (see Figure 5), apply OLDT search goal
top hmm([o1,...,oL ],Ans) w.r.t. translated program obtain t-explanations
hmm([o1,...,oL]). complexity argument however, translated program
DBh same, talk terms DBh sake simplicity. search,
fix search strategy multi-stage depth-first strategy (Tamaki & Sato, 1986).
assume solution table accessible O(1) time.40 Since length list
third argument hmm=3 decreases one recursion, finitely
many choices state transition output alphabet, search terminates, leaving
finitely many t-explanations solution table Figure 6 satisfy acyclic support condition respectively. sampling execution hmm(L) w.r.t. DBh nothing
sequential decision process decisions made msw atoms exclusive,
independent generate unique string, means DBh satisfies t-exclusiveness
condition, independence condition uniqueness condition respectively. So,
graphical EM algorithm applicable set hierarchical systems t-explanations
hmm(wt ) (1 ) produced OLDT search observations w1 ; : : : ; wT output
string. Put wt = ot1 ot2 1 11 otL. follows

DB
= fhmm(m,q,[otm ,...,otL]) j 1 L + 1; q 2 Qg [ fhmm([ot1,...,otL])g
h

DBh

(

(

msw(out(q),m,om ); msw(tr(q),m,q 0);
hmm(m + 1,q0 ,[otm+1 ,...,otL ])







)

) =
(1 L)
top-goal hmm([ot1 ,...,otL]), O(NL) calling patterns hmm=3
call causes N calls hmm=3, implying occur O(NL 1 N ) = O(N 2L)
calls hmm=3. Since call computed due tabling mechanism,
num = O(N 2 L). maxsize = 3. Applying Proposition 5.1, reach
e

hmm(m,q ,[otm ,...,otL ])

q0 2 Q

Proposition 5.2 Suppose strings length L. suppose
table operation
2

OLDT search done (1) time. OLDT time DBh O(N LT ) graphical
EM algorithm takes O(N 2 LT ) time per iteration N number states.

O(N 2LT ) time complexity Baum-Welch algorithm.
algorithm runs eciently Baum-Welch algorithm.41

graphical EM

40. O(1) possible translated program DBh Section 4.7, identify goal pattern
hmm(1,1,1,1,1) first two arguments constants (integers).
41. Besides, Baum-Welch algorithm graphical EM algorithm whose input support graphs
generated DBh update parameters value initial values same.
422

fiParameter Learning Logic Programs Symbolic-statistical Modeling

way, Viterbi algorithm (Rabiner, 1989; Rabiner & Juang, 1993) provides
HMMs ecient way finding likely transition path given input/output
string. similar algorithm parameterized logic programs determines
likely explanation given goal derived. runs time linear size
support graph, thereby O(N 2 L) case HMMs, complexity Viterbi
algorithm (Sato & Kameya, 2000).

5.3 PCFGs

compare graphical EM algorithm Inside-Outside algorithm (Baker,
1979; Lari & Young, 1990). Inside-Outside algorithm well-known EM algorithm
PCFGs (Wetherell, 1980; Manning & Schutze, 1999).42 takes grammar Chomsky
normal form. Given N nonterminals, production rule grammar takes form
! j; k (1 i; j; k N ) (nonterminals named numbers 1 N 1
starting symbol) form ! w 1 N w terminal. iteration,
computes inside probability outside probability every partial parse tree
given sentence update parameters production rules. Time complexity
measured time per iteration, described N , number nonterminals,
L, number terminals sentence. O(N 3 L3T ) observed sentences (Lari
& Young, 1990).
compare graphical EM algorithm Inside-Outside algorithm, start
propositional program DBg = Fg [ Rg representing largest grammar
containing possible rules ! j; k N nonterminals nonterminal 1 starting
symbol, i.e. sentence.
Fg
Rg

d,d0],[j ,k]) j 1 i; j; k N; d; d0 numbersg
= fmsw([if,[msw(
i,d,w ) j 1 N; number; w terminalg

=

8
>
<
>
:



q(i,d0,d2 ) :- msw(i,[d0,d2 ],[j ,k ]),
q(j ,d0,d1 ),
q(k ,d1 ,d2).
n

q(i,d,d +1) :- msw(i,d,wd+1 ).













1 i; j; k N;
0 d0 < d1 < d2 L

1 N; 0 L 0 1

9
>
=
>
;



Figure 9: PCFG program DBg
DB g artificial parsing program whose sole purpose measure size
OLDT tree43 created OLDT interpreter parses sentence w1 w2 1 11 wL.
42. PCFG (probabilistic context free grammar) backbone CFG probabilities (parameters) assigned production rule. nonterminal
n production rules fA ! ffi j 1 ng,
P
probability pi assigned ! ffi (1 n) ni=1 pi = 1. probability sentence
sum probabilities (leftmost) derivation s. latter product probabilities
rules used derivation.
43. precise, OLDT structure, case, tree DBg contains constants
(Datalog program) never occurs need creating new root node.
423

fiSato & Kameya
(1)

Td

q(1,d,L)
2 j N

2 k N
q(1,d,d+1),
q(1,d+1,L)

q(1,d+1,L)
(1)

[Note] q
1

2 k N

q(1,d,d+1),
q(k,d+1,L)

q(j,d,d+1),
q(1,d+1,L)

q(j,d,d+1),
q(k,d+1,L)

q(k,d+1,L)

q(1,d+1,L)

q(k,d+1,L)

(k)

Td+1
q(i,d,d) already appears
d+1 L, 1
d-d L-d-2

Td+1

d+2 e L-1
1 j N

2 k N
q(j,d,e),
q(1,e,L)

q(j,d,e),
q(k,e,L)

d+2 e e
1 N,
1 j N,

q(i,d,e),
q(j,e,e),
q(1,e,L)

Td

q(k,e,L)

N

q

N

q(j,e,e),
q(1,e,L)
q(1,e,L)

...
p(i)

p(1) p(2) p(N)

Figure 10: OLDT tree query

q(1,d,L)

input sentence w1 w2 11 1 wL embedded program separately msw(i,d,wd+1)
(0 L0 1) second clauses Rg (this treatment affect complexity argument). q(i,d0,d1) reads i-th nonterminal spans position d0 position d1,
i.e. substring wd +1 1 11 wd . first clauses q(i,d0 ,d2 ) :- msw(1,1,1), q(j ,d0 ,d1),
q(k,d1 ,d2) supposed textually ordered according lexicographic order
tuples hi; j; k; d0 ; d2 ; d1i. parser, top-goal set q(1,0,L).44 asks
parser parse whole sentence w1 w2 1 1 1 wL syntactic category \1" (sentence).
make exhaustive search query OLDT search.45 before, multistage depth-first search strategy O(1) time access solution table assumed.
time complexity OLDT search measured number nodes
OLDT tree. Let Td(k) OLDT tree q(k,d,L). Figure 10 illustrates Td(1)
(0 L 0 3) msw atoms omitted. seen, tree many similar
subtrees, put together (see Note Figure 10). Due depth-first strategy,
Td(1) recursive structure contains Td(1)
+1 subtree. Nodes whose leftmost atom
underlined solution nodes, i.e. solve leftmost atoms first time
entire refutation process. underlined atoms already computed subtrees
left.46 check solution table entries (= already
0

1

44. L Prolog variable constant denoting sentence length.
45. q table predicate.
0 00
0
46. inductively proved Td(1)
+1 contains every computed q(i,d ,d ) (0 L 0 3; + 1 <
d00 L; 1 N; d00 0 d0 L 0 0 2).
424

fiParameter Learning Logic Programs Symbolic-statistical Modeling

computed) O(1) time. Since clauses ground, execution generates
single child node.
(1)
(k)
enumerate h(1)
, number nodes Td Td+1 (1 k N ).
(k)
3
2 47
Figure 10, see h(1)
= O(N (L 0 d) ). Let hd (2 k N ) number nodes
k)
Td(+1
contained Td(1)+1. estimated O(N 2 (L 0 0 2)). Consequently, number
(k)
N
3
2
nodes newly created Td(1) h(1)
+ k=2 hd = (N (L 0 d) ). result,
L
0
3
3
3
48
total time OLDT search computed d=0 hd = O(N L ) size
support graph.
consider non-propositional parsing program DBg0 = Fg0 [ Rg0 Figure 11
whose ground instances constitute propositional program DBg . DBg0 probabilistic
variant DCG program (Pereira & Warren, 1980) q'/1, q'/6 between/3
declared table predicate. Semantically DBg0 specifies probability distribution
atoms form fq'(l) j l list terminalsg.
P

P

Fg0

Rg0

t,[sj ,sk ]) j 1 i; j; k N; numberg
= fmsw([sfi,msw(
si ,t,w) j 1 N; number; w terminalg

=

8
>
>
>
>
>
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
>
>
>
>
>
:

q'(S) :- length(S,D), q'(s1 ,0,D,0, ,S-[]).
q'(I,D0,D2,C0,C2,L0-L2) :- between(D0,D1,D2),
msw(I,C0,[J,K]),
q'(J,D0,D1,s(C0),C1,L0-L1),
q'(K,D1,D2,s(C1),C2,L1-L2).
q'(I,D,s(D),C0,s(C0),[W|X]-X) :- msw(I,C0,W).

Figure 11: Probabilistic DCG program DBg0
top-goal parse sentence = [w1; : : : ; wL] q'([w1; : : : ; wL]). invokes
q'(s1 ,0,D,0, ,[w1 ,: : :,wL ]-[]) measuring length input sentence
calling length=2. 49 50 general, q'(i,d0,d2 ,c0 ,c2 ,l0-l2) works identically q(i,d0 ,d2 )
three arguments, c0 , c2 l0-l2, added. c0 supplies unique trial-id msws
used body, c2 latest trial-id current computation, l0 -l2 D-list holding
substring d0 d2 . Since added arguments affect shape

47. focus subtree Td0 . j , i0 j 0 range 1 N , fif(e; e0 ) j + 2 e0 < e L 0 1 gfi =
O((L 0 d)2 ). Hence, number nodes Td0 O(N 3 (L 0 d)2 ). number nodes Td(1)
(1)
0
neither Td(1)
= (N 3 (L 0 d)2 ).
+1 Td negligible, therefore hd
(1)
(1)
48. number nodes TL01 TL02 negligible.
49. make program simple possible, assume integer n represented ground term


(n)
z }| {



s(1 1 1s (0)1 1 1). assume D0 D2 ground, goal between(D0, D1, D2)
returns integer D1 time proportional jD1 0 D0j.
50. omit obvious program length(l,sn ) computes length sn list l O(jlj) time.

sn =

def

425

fiSato & Kameya

search tree Figure 10 extra computation caused length=2 O(L)
one insertion between(D0,D1,D2) O(NL3) respectively,51 OLDT time remains
O(N 3 L3), hence size support graph.
apply graphical EM algorithm correctly, need confirm five conditions
applicability. rather apparent however OLDT refutation topgoal form q'([w1 ,: : :,wL]) w.r.t. DBg0 terminates, leaves support graph
satisfying finite support condition acyclic support condition. t-exclusiveness
condition independent condition hold refutation process faithfully
simulates leftmost stochastic derivation w1 11 1 wL choice production
rule made msw(si ,sc,[sj ,sk ]) exclusive independent (trial-ids different
different choices).
remains uniqueness condition. confirm it, let us consider another program DBg00 , modification DBg0 first goal length(S,D) body
first clause first goal between(D0,D1,D2) second clause Rg0 moved
last position bodies respectively. DBg00 DBg0 logically equivalent,
semantically equivalent well viewpoint distribution semantics. think
sampling execution OLDT interpreter top-goal q'(S) w.r.t. DBg00
variable, using multi-stage depth-first search strategy. easy see first
execution never fails, second OLDT refutation terminates, sentence
[w1 ; : : : ; wL ] returned S, third conversely, set msw atoms resolved upon
refutation uniquely determines output sentence [w1 ; : : : ; wL].52 Hence,
sampling execution guaranteed always terminate, every sampling PF 00 (= PF 0 )
uniquely generates sentence, observable atom, uniqueness condition satisfied
DBg00 , hence DBg0 .
sampling execution guaranteed always terminate? words,
grammar generate finite sentences? Giving general answer seems
dicult, known parameter values PCFG obtained learning
finite sentences, stochastic derivation PCFG terminates probability
one (Chi & Geman, 1998). summary, assuming appropriate parameter values,
say parameterized logic program DBg0 largest PCFG N nonterminal
symbols satisfies applicability conditions, OLDT time sentence length
L O(N 3 L3)53 size support graph. Proposition 5.1,
conclude
g

g

Proposition 5.3 Let DB a0 parameterized logic program representing PCFG N

nonterminals form DBg Figure 11, G = G1 ; G2 ; : : : ; GT sampled atoms
representing sentences length L. suppose table operation OLDT search done
O(1) time. OLDT search G one iteration learn-gEM respectively
done O(N 3 L3T ) time.

51.

between(D0,D1,D2)

called O(N (L 0 d)2 ) times Td(1) . called

0 O(N (L 0 d)2 ) = O(NL3 )

PL 3
d=0

times .
52. trial-ids used refutation record rule used step derivation
w1 1 1 1 wL .
53. DBg , represent integers ground terms made 0 s(1) keep program short.
use integers instead ground terms however, first three arguments q'(1,1,1,1,1,1) enough
check whether goal previously called not, check done O(1) time.
(1)
0

0

426

fiParameter Learning Logic Programs Symbolic-statistical Modeling
(N 3 L 3 )

time complexity Inside-Outside algorithm per iteration
(Lari & Young, 1990), hence algorithm ecient Inside-Outside algorithm.

5.4 Pseudo PCSGs

PCFGs improved making choices context-sensitive, one attempts
pseudo PCSGs (pseudo probabilistic context sensitive grammars) rule chosen probabilistically depending nonterminal expanded parent
nonterminal (Charniak & Carroll, 1994).
pseudo PCSG easily programmed. add one extra-argument, N, representing
parent node, predicate q'(I,D0,D2,C0,C2,L0-L2) Figure 11 replace
msw(I,C0,[J,K]) msw([N,I],C0,[J,K]). Since (leftmost) derivation sentence
pseudo PCSG still sequential decision process described modified program,
graphical EM algorithm applied support graphs generated modified
program observed sentences correctly performs ML estimation parameters
pseudo PCSG.
pseudo PCSG thought PCFG rules form [n; i] ! [i; j ][i; k]
(1 n; i; j; k N ) n parent nonterminal i, arguments previous
subsection carried minor changes. therefore (details omitted)

Proposition 5.4 Let DB parameterized logic program pseudo PCSG N

nonterminals shown above, G = G1; G2 ; : : : ; GT observed atoms representing
sampled sentences length L. Suppose table operation OLDT search done
O(1) time. OLDT search G iteration learn-gEM completed
O(N 4 L3T ) time.

5.5 Bayesian Networks

relationship cause C effect E often probabilistic one diseases symptoms, mathematically captured conditional
probability P (E = e j C = c) effect e given cause c. wish know however
inverse, i.e. probability candidate cause c given evidence e, i.e. P (C = c j E = e)
calculated Bayes' theorem P (E = e j C = c)P (C = c)= c0 P (E = e j C =
c0)P (C = c0 ). Bayesian networks representational/computational framework fits
best type probabilistic inference (Pearl, 1988; Castillo et al., 1997).
Bayesian network graphical representation joint distribution P (X1 = x1 ; : : : ;
XN = xN ) finitely many random variables X1 ; : : : ; XN . graph dag (directed
acyclic graph) ones Figure 12, node random variable.54
graph, conditional probability table (CPT) representing P (Xi = xi j 5i = ui)
(1 N ) associated node Xi 5i represents Xi's parent nodes ui
values. Xi parent, i.e. topmost node graph, table
marginal distribution P (Xi = xi). whole joint distribution defined product
P

54. deal discrete cases.
427

fiSato & Kameya



B





C

B

C

E



F

E

( G 1 ) Singly-connected

F

( G 2 ) Multiply-connected

Figure 12: Bayesian networks
conditional distributions:
P (X1 = x1 ; : : : ; XN

= xN )55 =

N

i=1

P (Xi = xi j 5i = ui ):

(21)

Thus graph G1 Figure 12 defines
PG (a; b; c; d; e; f ) = PG (a)PG (b)PG (c j a)PG (d j a; b)PG (e j d)PG (f j d)
a, b, c, d, e f values corresponding random variables A, B, C , D, E
F , respectively.56 mentioned before, one basic tasks Bayesian networks
compute marginal probabilities. example, marginal distribution PG (c; d)
computed either (22) (23) below.
PG (c; d) =
PG (a)PG (b)PG (c j a)PG (d j a; b)PG (e j d)P (f j d)
(22)
1

1

1

1

1

1

1

1

X

1

a;b;e;f

1

1

1

1

0

=

X
@

a;b

1

10

PG1 (a)PG1 (b)PG1 (c j a)PG1 (d j a; b)A @

1
X

e;f

PG1 (e j d)PG1 (f j d)A (23)

(23) clearly ecient (22). Observe graph G2
Figure 12, would way factorize computations (23) use (22) requiring
exponentially many operations. problem computing marginal probabilities
NP-hard general, factorization (23) assured graph singly
connected G1 , i.e. loop viewed undirected graph. case,
computation possible O(jV j) time V set vertices graph (Pearl,
1988). Otherwise, graph called multiply-connected, might need exponential time
compute marginal probabilities. sequel, show following.
discrete Bayesian network G defining distribution PG (x1 ; : : : ; xN ),
parameterized logic program DBG predicate bn(1) PDB (bn(x1,: : :,xN ))
= PG(x1 ; : : : ; xN ).
G

55. Thanks acyclicity graph, without losing generality, may assume Xi ancestor
node Xj , < j holds.
56. notational simplicity, shall omit random variables confusion arises.
428

fiParameter Learning Logic Programs Symbolic-statistical Modeling

arbitrary factorizations order compute marginal distribution,

exists tabled program accomplishes computation specified way.
graph singly connected evidence e given, exists tabled
program DBG OLDT time bn(e) O(jV j), hence time
complexity per iteration graphical EM algorithm O(jV j) well.
Let G Bayesian network defining joint distribution PG(x1 ; : : : ; xN ) fPG (Xi =
xi j 5i = ui ) j 1 N; xi 2 val(Xi ); ui 2 val(5i )g conditional probabilities
associated G val(Xi) set Xi's possible values val(5i) denotes
set possible values parent nodes 5i random vector, respectively.
construct parameterized logic program defines distribution PG (x1 ; : : : ; xN ).
program DBG = FG [ RG shown Figure 13.


FG

= f msw(par(i,ui),once,xi) j 1 N; ui 2 val(5i); xi 2 val(Xi) g

RG

= f bn(X1 ,: : :,XN ):-

5i),once,Xi). g

VN

i=1 msw(par(i,

Figure 13: Bayesian network program DBG
FG comprised msw atoms form msw(par(i,ui ),once,xi ) whose probability
exactly conditional probability PG (Xi = xi j 5i = ui). Xi parents, ui
empty list []. RG singleton, containing one clause whose body conjunction
msw atoms corresponds product conditional probabilities. Note
intentionally identify random variables X1 ; : : : ; XN logical variables X1 ; : : : ; XN
convenience.

Proposition 5.5 DBG denotes distributions G.

(Proof) Let hx1 ; : : : ; xN realization random vector hX1; : : : ; XN i. holds
construction
PDBG (bn(x1 ,: : :,xN ))

=

N

h=1
N


Pmsw (msw(par(i,ui ),once,xi ))

=
PG (Xi = xi j 5i = ui )
h=1
= PG (x1 ; : : : ; xN ):
Q:E:D:
case G1 Figure 12, program becomes57
bn(A,B,C,D,E,F)

:-

msw(par('A',[]),once,A),
msw(par('C',[A]),once,C),
msw(par('E',[D]),once,E),

57. 0 A0 ; 0 B0 ; : : : Prolog constants used place integers.
429

msw(par('B',[]),once,B),
msw(par('D',[A,B]),once,D),
msw(par('F',[D]),once,F).

fiSato & Kameya

left-to-right sampling execution gives sample realization random vector
h A; B; C; D; E; F i. marginal distribution computed bn(x1 ,: : :,xN ) adding new
clause DBG. example, compute PG (c; d), add bn(C,D):- bn(A,B,C,D,E,F)
DBG (let result DB0G ) compute PDB0 (bn(c,d)) equal
PG (c; d)
PDB0 (bn(c,d)) = PDB (9 a; b; e; f bn(a,b,c,d,e,f ))
=
PDB (bn(a,b,c,d,e,f ))
a;b;e;f
= PG (c; d):
Regrettably computation corresponds (22), factorization (23). Ecient
probability computation using factorization made possible carrying summations
proper order.
next sketch example carry specified summations specified
order introducing new clauses. Suppose joint distribution P (x; y; z; w) =
1 (x; )2 (y; z; w)3(x; z; w) 1(x; ), 2(y; z; w) 3 (x; z; w) respectively
computed atoms p1 (X,Y), p2 (Y,Z,W) p3 (X,Z,W). Suppose hope
compute sum
P (x) = 1(x; )
2 (y; z; w )3 (x; z; w)
1

1

1

G1

1

G1

X

G1

G1

1

X

X



z;w

!

first eliminate z; w y. Corresponding elimination, introduce
two new predicates, q(X,Y) compute 4 (x; y) = z;w 2 (y; z; w)3 (x; z; w) p(X)
compute P (x) = 1 (x; y)4(x; y) follows.
P

P

p(X)
q(X,Y)

::-

p1(X,Y), q(X,Y).
p2(Y,Z,W), p3 (X,Z,W).

Note clause body q=2 contains Z W (existentially quantified) local variables
clause head q(X,Y) contains variables shared atoms. view
correspondence 9, easy confirm program realizes
required computation. easy see generalizing example, though
prove here, exists parameterized logic program carries given
summations given order arbitrary Bayesian network, particular
able simulate (variable elimination, Zhang & Poole, 1996; D'Ambrosio, 1999)
approach.
Ecient computation marginal distributions always possible
well-known class Bayesian networks, singly connected Bayesian networks,
exists ecient algorithm compute marginal distributions message passing (Pearl,
1988; Castillo et al., 1997). show graph singly connected,
construct ecient tabled Bayesian network program DBG assigning table predicate
node. avoid complications, explain construction procedure informally
concentrate case one interested variable. Let G singly
P



430

fiParameter Learning Logic Programs Symbolic-statistical Modeling

connected graph. First pick node U whose probability PG (u) seek.
construct tree G root node U G, letting nodes dangling U .
Figure 14 shows G1 transformed tree select node B root node.
B





E

F

C


Transformed graph G1

Figure 14: Transforming G1 tree
examine node G one one. add node X graph
corresponding clause DBG whose purpose visit nodes connected X except
one calls X . Suppose started root node U1 Figure 15 evidence
u given, generated clause (24). proceed inner node X (U1 calls
X ). original graph G, X parent nodes fU1 ; U2; U3g child nodes fV1 ; V2 g. U3
topmost node G.


U1
X
U2

V2
U3

V1

Tree G

Figure 15: General situation
node X Figure 15, add clause (25). called parent node
U1 U1 ground, first generate possible values U2 calling val U2 (U2),
call call X U2 (U2 ) visit nodes connected X U2 . U3 similary
treated. visiting nodes G connecting X parent nodes U2
U3 (nodes connected U1 already visited), value random variable X
determined sampling msw atom jointly indexed 'X' values U1 , U2
431

fiSato & Kameya
U3 . visit X 's children, V1 V2 . topmost node U3 original graph,
add clause (26).
tbn(U1 ) :- msw(par('U1 ',[]),once,U1 ), call
call

U1 X (U1 ).

U1 X (U1 ) :- val U2 (U2), call X U2 (U2 ),
val U3 (U3), call X U3 (U3 ),
msw(par('X',[U1 ,U2 ,U3 ]),once,X),
call X V1 (X), call X V2(X).

(24)

(25)

(26)
Let DBG final program containing clauses (24), (25) (26). Apparently
DBG constructed time linear number nodes network.
note successive unfolding (Tamaki & Sato, 1984) atoms form call ...(1)
clause bodies starts (24) yields program DB0G similar one
Figure 13 contains msw atoms call ...(1)'s. DBG DB0G define
distribution,58 proved Proposition 5.5 PG (u) = PDB0 (bn(u)) =
PDB (tbn(u)) holds (details omitted). way, Figure 15 assume construction
starts topmost node U1 evidence u given, necessary.
Suppose change start inner node X. case, replace clause (24)
call X U1(U1 ) :- msw(par('U1',[]),once,U1 ) (26). time
replace head clause (25) tbn() add goal call X U1 (u) body
on. changed program DB00G , rather straightforward prove
PDB00 (tbn()) = PG(u) holds. true construction tabled program
DBG shown crude lot room optimization, suces
show parameterized logic program singly connected Bayesian network runs
O(jV j) time V set nodes.
estimate time complexity OLDT search w.r.t. DBG , declare tbn every
predicate form call ...(1) table predicate verify five conditions
applicability graphical EM algorithm (details omitted). estimate time
complexity OLDT search goal tbn(u) w.r.t. DBG .59 notice calls occur
according pre-order scan (parents { node { children) tree G , calls
call X (1) occur val(Y ) times. call call X (1) invokes calls rest
nodes, X 's parents X 's children graph G except caller node, diffrent
set variable instantiations, second call on, every call refers solutions
stored solution table O(1) time. Thus, number added computation steps
call

X U3(U3 ) :- msw(par('U3 ',[]),once,U3 ).







G

G



G






58. Since distribution semantics based least model semantics, unfold/fold transformation (Tamaki & Sato, 1984) preserves least Herbrand model transformed program, unfold/fold
transformation applied parameterized logic programs preserves denotation transformed
program.
59. DBG transformed OLDT interpreter collect msw atoms case HMM
program.
432

fiParameter Learning Logic Programs Symbolic-statistical Modeling

OLDT search X bounded above, constant O(val(U 1)val(U 2)val(U 3)val(X ))
case Figure 15. result OLDT time proportional number nodes
original graph G (and size support graph) provided number
edges connecting node, values random variable bounded
above.

Proposition 5.6 Let G singly connected Bayesian network defining distribution PG ,

V set nodes, DBG tabled program derived above. Suppose number
edges connecting node, values random variable bounded
constant. suppose table access done O(1) time. Then, OLDT time
computing PG(u) observed value u random variable U means DBG
O(jV j) time per iteration required graphical EM algorithm.
observations, time complexity O(jV jT ).

O(jV j) time complexity required compute marignal distribution singly
connected Bayesian network standard algorithm (Pearl, 1988; Castillo et al., 1997),
EM algorithm using it. therefore conclude graphical
EM algorithm ecient specialzed EM algorithm singly connected Bayesian
networks.60 must quickly add graphical EM algorithm applicable
arbitrary Bayesian networks,61 Proposition 5.6 says explosion
support graph avoided appropriate programming case singly connected
Bayesian networks.
summarize, graphical EM algorithm, single generic EM algorithm, proved
time complexity specialized EM algorithms, i.e. Baum-Welch algorithm
HMMs, Inside-Outside algorithm PCFGs, one singly connected
Bayesian networks developed independently research field.
Table 1 summarizes time complexity EM learning using OLDT search
graphical EM algorithm case one observation. first column, \sc-BNs"
represents singly connected Bayesian networks. second column shows program use.
DBh HMM proram Subsection 4.7, DBg0 PCFG program Subsection 5.3
DBG transformed Bayesian network program Subsection 5.5, respectively. OLDT time
third column time OLDT search complete search t-explanations.
gEM fourth column time one iteration taken graphical EM algorithm
update parameters. use N , , L V respectively number states
HMM, number nonterminals PCFG, length input string
number nodes Bayesian network. last column standard (specialized) EM
algorithm model.


60. marginal distribution PG one variable required, construct similar
tabled program computes marginal probabilities still O(jV j) time adding extra-arguments
convey evidence embedding evidnece program.
61. check five conditions DBG Figure 13. uniqueness condition obvious sampling
always uniquely generates sampled value random variable. finite support condition
satisfied finite number random variables values. acyclic support
condition immediate acyclicity Bayesian networks. t-exclusiveness condition
independent condition easy verify.
433

fiSato & Kameya

Model
Program
HMMs
DBh
PCFGs
DBg0
DBG
sc-BNs
user model


OLDT time
(N 2L)
(M 3 L 3 )
O(jV j)
O(jOLDT treej)

gEM
Specialized EM
2
O(N L)
Baum-Welch
3
3
(M L )
Inside-Outside
O(jV j)
(Castillo et al., 1997)
(jsupport graphj)

Table 1: Time complexity EM learning OLDT search graphical EM algorithm

5.6 Modeling Language PRISM

developing symbolic-statistical modeling laguage PRISM since 1995 (URL
= http://mi.cs.titech.ac.jp/prism/) implementation distribution semantics
(Sato, 1995; Sato & Kameya, 1997; Sato, 1998). language intented modeling
complex symbolic-statistical phenomena discourse interpretation natural language
processing gene inheritance interacting social rules. programming language,
looks extension Prolog new built-in predicates including msw predicate
special predicates manipulating msw atoms parameters.
PRISM program comprised three parts, one directives, one modeling
one utilities. directive part contains declarations values, telling system
msw atoms used execution. modeling part set non-unit definite
clauses define distribution (denotation) program using msw atoms.
last part, utility part, arbitary Prolog program refers predicates defined
modeling part. use utility part learn built-in predicate carry
EM learning observed atoms.
PRISM provides three modes execution. sampling execution correponds
random sampling drawn distribution defined modeling part. second
one computes probability given atom. third one returns support set
given goal. execution modes available built-in predicates.
must report however implementation graphical EM algorithm
simpified OLDT search mechanism way, completed yet.
currently, Prolog search learn-naive(DB; G) Section 4 available EM learning though realized, partially, structrure sharing explanations implemention
learn-naive(DB; G ). Putting computational eciecy aside however, problem
expressing learning HMMs, PCFGs, pseudo PCSGs, Bayesian networks
probailistic models current version. learning experiments next section
used parser substitute OLDT interpreter, independently implemented
graphical EM algorithm.
6. Learning Experiments

complexity analysis graphical EM algorithm popular symbolic-probabilistic
models previous section, look actual behavior graphical EM algorithm
real data section. conducted learning experiments PCFGs using two
434

fiParameter Learning Logic Programs Symbolic-statistical Modeling

corpora contrasting characters, compared performance graphical
EM algorithm Inside-Outside algorithm terms time per iteration
(= time updating parameters). results indicate graphical EM algorithm
outperform Inside-Outside algorithm orders magnigude. Detalis reported
Sato, Kameya, Abe, Shirai (2001). proceeding, review Inside-Outside
algorithm completeness.

6.1 Inside-Outside Algorithm

Inside-Outside algorithm proposed Baker (1979) generalization
Baum-Welch algorithm PCFGs. algorithm designed estimate parameters
CFG grammar Chomsky normal form containing rules expressed numbers ! j; k
(1 i; j; k N N nonterminals, 1 starting symbol). Suppose input
sentence w1 ; : : : ; wL given.
iteration, first computes bottom manner
3
inside probabilities3 e(s; t; i) = P (i ) ws; : : : ; wt) computes outside probabilities
f (s; t; i) = P (S )
w1 ; : : : ; ws01 wt+1 ; : : : ; wL ) top-down manner every s,
(1 L; 1 N ). computing probabilities, parameters
updated using them, process iterates predetermined criterion
convergence likelihood input sentence achieved. Although Baker
give analysis Inside-Outside algorithm, Lari Young (1990) showed
takes O(N 3 L3 ) time one iteration Lafferty (1993) proved EM algorithm.
true Inside-Outside algorithm recognized standard EM
algortihm training PCFGs, notoriously slow. Although much literature
explicitly stating time required Inside-Outside algorithm (Carroll & Rooth, 1998;
Beil, Carroll, Prescher, Riezler, & Rooth, 1999), Beil et al. (1999) reported example
trained PCFG 5,508 rules corpus 450,526 German subordinate clauses whose average ambiguity 9,202 trees/clause using four machines (167MHz
Sun UltraSPARC22 296MHz Sun UltraSPARC-II22), took 2.5 hours complete
one iteration. discuss later Inside-Outside algorithm slow.

6.2 Learning Experiments Using Two Corpora

report parameter learning existing PCFGs using two corpora moderate size
compare graphical EM algorithm Inside-Outside algorithm terms
time per iteration. mentioned before, support graphs, input garphical EM
algorithm, generated parser, i.e. MSLR parser.62 measurements made
296MHz Sun UltraSPARC-II 2GB memory Solaris 2.6 threshold
increase log likelihood input sentences set 1006 stopping criterion
EM algorithms.
experiments, used ATR corpus EDR corpus (each converted POS
(part speech)-tagged corpus). similar size (about 10,000) contrasting
characters, sentence length ambiguity grammars. first experiment
employed ATR corpus Japanese-English corpus (we used Japanese part)
developed ATR (Uratani, Takezawa, Matsuo, & Morita, 1994). contains 10,995 short
62. MSLR parser Tomita (Generalized LR) parser developed Tanaka-Tokunaga Laboratory Tokyo
Institute Technology (Tanaka, Takezawa, & Etoh, 1997).
435

fiSato & Kameya

conversational sentences, whose minimum length, average length maximum length
respectively 2, 9.97 49. skeleton PCFG, employed context free grammar
Gatr comprising 860 rules (172 nonterminals 441 terminals) manually developed
ATR corpus (Tanaka et al., 1997) yields 958 parses/sentence.
Inside-Outside algorithm accepts CFG Chomsky normal form,
converted Gatr Chomsky normal form G3atr . G3atr contains 2,105 rules (196 nonterminals 441 terminals). divided corpus subgroups similar length
(L = 1; 2); (L = 3; 4); : : : ; (L = 25; 26), containing randomly chosen 100 sentences.
preparations, compare length graphical EM algorithm applied
Gatr G3atr Inside-Outside algorithm applied G3atr terms time per
iteration running convergence.
(sec)

(sec)

(sec)

60
I-O
50

0.7

0.04

0.6

0.035

0.5

40

0.4

0.03

I-O
gEM (original)
gEM (Chomsky NF)

0.025
0.02

30
0.3
20

0.015

0.2

10

0.01

0.1

0.005

5

10

15

20

25

L

L

L
0

gEM (original)
gEM (Chomsky NF)

0

5

10

15

20

25

0

5

10 15 20 25

Figure 16: Time per iteration : I-O vs. gEM (ATR)
Curves Figure 16 show learning results x-axis length L input
sentence y-axis average time taken EM algorithm one iteration update
parameters contained support graphs generated chosen 100 sentences
(other parameters grammar change). left graph, Inside-Outside
algorithm plots cubic curve labeled \I-O". omitted curve drawn graphical
EM algorithm drew x-axis. middle graph magnifies left graph. curve
labeled \gEM (original)" plotted graphical EM algorithm applied original
grammar Gatr whereas one labeled \gEM (Chomsky NF)" used G3atr. length 10,
average sentence length, measured whichever grammar employed, graphical
EM algorithm runs several hundreds times faster (845 times faster case Gatr
720 times faster case G3atr) Inside-Outside algorithm per iteration.
right graph shows (almost) linear dependency updating time graphical EM
algorithm within measuared sentence length.
Although difference anticipated learning speed, speed gap
Inside-Outside algorithm graphical EM algorithm unexpectedly large.
conceivable reason ATR corpus contains short sentences Gatr
436

fiParameter Learning Logic Programs Symbolic-statistical Modeling

much ambiguous parse trees sparse generated support graphs small,
affects favorably perforamnce graphical EM algorithm.
therefore conducted experiment another corpus contains much
longer sentences using ambiguous grammar generates dense parse trees.
used EDR Japanese corpus (Japan EDR, 1995) containing 220,000 Japanese news article
sentences. however process re-annotation, part (randomly
sampled 9,900 sentences) recently made available labeled corpus. Compared
ATR corpus, sentences much longer (the average length 9,900 sentences 20,
minimum length 5, maximum length 63) CFG grammar Gedr (2,687 rules,
converted Chomsky normal form grammar G3edr containing 12,798 rules) developed
ambiguous (to keep coverage rate), 3:0 2 108 parses/sentence length
20 6:7 2 1019 parses/sentence length 38.
(sec)

5000

(sec)

(sec)
3

10

6000
I-O

8

I-O
gEM (original)

2.5

gEM (original)

2

4000

6
1.5

3000

4
1

2000

2

1000
0

L
5 10 15 20 25 30 35 40

0

0.5

L
5 10 15 20 25 30 35 40

0

L
5 10 15 20 25 30 35 40

Figure 17: Time per iteration : I-O vs. gEM (EDR)
Figure 17 shows obtained curves experiments EDR corpus (the graphical EM algorithm applied Gedr vs. Inside-Outside algorithm applied G3edr)
condition ATR corpus, i.e. plotting average time per iteration process 100
sentences designated length, except plotted time Inside-Outside algorithm average 20 iterations whereas graphical EM algorithm
average 100 iterations. clear middle graph, time again, graphical
EM algorithm runs orders magnitude faster Inside-Outside algorithm. average sentence length 20, former takes 0.255 second whereas latter takes 339 seconds,
giving speed ratio 1,300 1. sentence length 38, former takes 2.541 seconds
latter takes 4,774 seconds, giving speed ratio 1,878 1. Thus speed ratio even
widens compared ATR corpus. explained mixed effects O(L3 ),
time complexity Inside-Outside algorithm, moderate increase total size
support graphs w.r.t. L. Notice right graph shows total size support
graphs grows sentence length L time per iteration graphical EM algorithm
linear total size support graphs.

437

fiSato & Kameya

Since implemented Inside-Outside algorithm faithfully Baker (1979), Lari
Young (1990), much room improvement. Actually Kita gave refined InsideOutside algorithm (Kita, 1999). implementation Mark Johnson
Inside-Outside algorithm down-loadable http://www.cog.brown.edu/%7Emj/.
use implementations may lead different conclusions. therefore conducted
learning experiments entire ATR corpus using two implementations
measured updating time per iteration (Sato et al., 2001). turned implementations run twice fast naive implementation take 630 seconds per
iteration graphical EM algorithm takes 0.661 second per iteration, still
orders magnitude faster former two. Regrettably similar comparison using
entire EDR corpus available moment abandoned due memory ow
parsing construction support graphs.
Learning experiments far compared time per iteration ignore extra time
search (parsing) required graphical EM algorithm. question naturally arises
w.r.t. comparison terms total learning time. Assuming 100 iterations learning
ATR corpus however, estimated even considering parsing time, graphical
EM algorithm combined MSLR parser runs orders magnitude faster three
implementations (ours, Kita's Johnson's) Inside-Outside algorithm (Sato et al.,
2001). course estimation directly apply graphical EM algorithm
combined OLDT search, OLDT interpreter take time parser
much time needed depends implementaiton OLDT search.63
Conversely, however, may able take rough indication far approach,
graphical EM algorithm combined OLDT search via support graphs, go
domain EM learning PCFGs.

6.3 Examing Performance Gap

previous subsection, compared performance graphical EM algorithm
Inside-Outside algorithm PCFGs given, using two corpora three
implementations Inside-Outside algorithm. experiments, graphical EM
algorithm considerably outperformed Inside-Outside algorithm despite fact
time complexity. look causes performance
gap.
Simply put, Inside-Outside algorithm slow (primarily) lacks parsing.
Even backbone CFG grammar explicitly given, take advantage
constraints imposed grammar. see it, might help review inside
probability e(s; t; A), i.e. P(nonterminal spans s-th word t-th word) (s t),
calculated Inside-Outside algorithm given grammar.
e(s; t; A) =

r=
t01
X

P(A ! BC )e(s; r; B)e(r + 1; t; C )
s.t. A!BC grammar r=s
P(A ! BC ) probability associated production rule ! BC . Note
fixed triplet (s; t; A), usual term P(A ! BC )e(s; r; B )e(r +1; t; C ) non-zero
X

B;C

63. cannnot answer question right implementation OLDT search completed.
438

fiParameter Learning Logic Programs Symbolic-statistical Modeling

relatively small number (B; C; r)'s determined successful parses
rest combinations always give 0 term. Nonetheless Inside-Outside algorithm
attempts compute term every iteration possible combinations B, C
r repeated every possible (s; t; A), resulting lot redundancy.
kind redundancy occurs computation outside probability Inside-Outside
algorithm.
graphical EM algorithm free redundancy runs parse trees (a
parse forest) represented support graph.64 must added, hand,
superiority learning speed graphical EM algorithm realized cost space
complexity Inside-Outside algorithm merely requires O(NL2 ) space
array store probabilities, graphical EM algorithm needs O(N 3 L3 ) space store
support graph N number nonterminals L sentence length.
trade-off understandable one notices graphical EM algorithm applied
PCFG considered partial evaluation Inside-Outside algorithm
grammar (and introduction appropriate data structure output).
Finally remark use parsing preprocess EM learning PCFGs
unique graphical EM algorithm (Fujisaki, Jelinek, Cocke, Black, & Nishino, 1989;
Stolcke, 1995). approaches however still seem contain redundancies compared
graphical EM algorithm. instance Stolcke (1995) uses Earley chart compute
inside outside probability, parses implicitly reconstructed iteration
dynamically combining completed items.
7. Related Work Discussion

7.1 Related Work

work presented paper crossroads logic programming probability
theory, considering enormous body work done fields, incompleteness
unavoidable reviewing related work. said that, look various attempts
made integrate probability computational logic logic programming.65 reviewing, one immediately notice two types usage probability. One type,
constraint approach, emphasizes role probability constraints necessarily seek unique probability distribution logical formulas. type,
distribution approach, explicitly defines unique distribution model theoretical means
proof theoretical means, compute various probabilities propositions.
typical constraint approach seen early work probabilistic logic Nilsson
(1986). central problem, \probabilistic entailment problem", compute upper
lower bound probability P() target sentence way bounds
compatible given knowledge base containing logical sentences (not necessarily
logic programs) annotated probability. probabilities work constraints
64. emphasize difference Inside-Outside algorithm graphical EM algorithm
solely computational eciency, converge parameter values starting
initial values. Linguistic evaluations estimated parameters graphical EM algorithm
reported Sato et al. (2001).
65. omit literature leaning strongly toward logic. logic(s) concerning uncertainty, see overview
Kyburg (1994).
439

fiSato & Kameya

possible range P(). used linear programming technique solve problem
inevitably delimits applicability approach finite domains.
Later Lukasiewicz (1999) investigated computational complexity probabilistic
entailment problem slightly different setting. knowledge base comprises statements
form (H j G)[u1 ; u2 ] representing u1 P(H j G) u2 . showed inferring
\tight" u1 ; u2 NP-hard general, proposed tractable class knowledge base called
conditional constraint trees.
uential work Nilsson, Frish Haddawy (1994) introduced deductive system probabilistic logic remedies \drawbacks" Nilsson's approach,
computational intractability lack proof system. system deduces
probability range proposition rules probabilistic inferences unconditional
conditional probabilities. instance, one rules infers P (ff j ) 2 [0 y]
P (ff _ j ) 2 [x ] ff,fi propositional variables [x y] (x ) designates
probability range.
Turning logic programming, probabilistic logic programming formalized Ng
Subrahmanian (1992) Dekhtyar Subrahmanian (1997) constraint approach. program set annotated clauses form : F1 : 1; : : : ; Fn : n
atom, (1 n) basic formula, i.e. conjunction disjunction
atoms, j (0 j n) sub-interval [0; 1] indicating probability range. query
9 (F1 : 1 ; : : : ; Fn : n) answered extension SLD refutation. formalization,
assumed language contains finite number constant predicate
symbols, function symbol allowed.
similar framework proposed Lakshmanan Sadri (1994) syntactic restrictions (finitely many constant predicate symbols function
symbols)
different uncertainty setting. used annotated clauses form c B1 ; : : : ; Bn
Bi (1 n) atoms c = h[ff; ]; [ ; ]i, confidence level, represents
belief interval [ff; ] (0 1) doubt interval [ ; ] (0 1),
expert clause.
seen above, defining unique probability distribution secondary concern
constraint approach. sharp contrast Bayesian networks whole
discipline rests ability networks define unique probability distribution
(Pearl, 1988; Castillo et al., 1997). Researchers Bayesian networks seeking
way mixing Bayesian networks logical representation increase inherently
propositional expressive power.
Breese (1992) used logic programs automatically build Bayesian network
query. Breese's approach, program union definite clause program set
conditional dependencies form P(P j Q1 ^ 1 11 ^ Qn ) P Qi atoms.
Given query, Bayesian network constructed dynamically connects query
relevant atoms program, turn defines local distribution connected
atoms. Logical variables appear atoms function symbol allowed.
Ngo Haddawy (1997) extended Breese's approach incorporating mechanism
ecting context. used clause form P(A0 j A1 ; : : : ; ) = L1 ; : : : ; Lk ,
Ai's called p-atoms (probabilistic atoms) whereas Lj 's context atoms disjoint
p-atoms, computed another general logic program (satisfying certain restric440

fiParameter Learning Logic Programs Symbolic-statistical Modeling

tions). Given query, set evidence context atoms, relevant ground p-atoms
identified resolving context atoms away SLDNF resolution, local Bayesian network built calculate probability query. proved soundness
completeness query evaluation procedure condition programs
acyclic66 domains finite.
Instead defining local distribution query, Poole (1993) defined global distribution \probabilistic Horn abduction". program consists definite clauses
disjoint declarations form disjoint([h1 :p1,...,hn:pn]) specifies probability distribution hypotheses (abducibles) fh1; : : : ; hn g. assigned probabilities
ground atoms help theory logic programming, furthermore proved
Bayesian networks representable framework. Unlike previous approaches,
language contains function symbols, acyclicity condition imposed programs
semantics definable seems severe restriction. Also, probabilities
defined quantified formulas.
Bacchus et al. (1996) used much powerful first-order probabilistic language
clauses annotated probabilities. language allows statistically quantified term
k (x)j(x) kx denote ratio individuals finite domain satisfying (x) ^
(x) satisfying (x). Assuming every world (interpretation language)
equally likely, define probability sentence ' given knowledge
('^KB)

base KB limit limN !1 ##worlds
worlds (KB) #worldsN () number
possible worlds containing N individuals satisfying , parameters used judging
approximations. Although limit necessarily exist domain must finite,
showed method cope diculties arising \direct inference"
default reasoning.
linguistic vein, Muggleton (1996, others) formulated SLPs (stochastic
logic programs) procedurally, extension PCFGs probabilistic logic programs.
So, clause C , must range-restricted,67 annotated probability p
p : C . probability goal G product ps appearing refutation
modification subgoal g invoke n clauses, pi : Ci (1 n)
refutation step, probability choosing k-th clause normalized pk = ni=1 pi.
recently, Cussens (1999, 2001) enriched SLPs introducing special class
log-linear models SLD refutations w.r.t. given goal. example considers
possible SLD refutations general goal s(X ) defines probability P(R)
refutation R P(R) = Z 01 exp( (R; i)). number associated
clause Ci (R; i) feature, i.e. number occurrences Ci R. Z
normalizing constant. Then, probability assigned s(a) sum probabilities
refutation s(a).



N

N



P

P

66. condition says every ground atom must assigned unique integer n(A) n(A) >
n(B1 ); : : : ; n(Bn ) holds ground instance clause form B1 ; : : : ; Bn .
condition, program includes p(X ) q(X; ), cannot write recursive clauses q
q (X; [H jY ]) q(X; ).
67. syntactic property variables appearing head appear body clause. unit
clause must ground.
441

fiSato & Kameya

7.2 Limitations Potential Problems

Approaches described far less similar limitations potential problems.
Descriptive power confined finite domains common limitation, due
use linear programming technique (Nilsson, 1986), due syntactic
restrictions allowing infinitely many constant, function predicate symbols (Ng
& Subrahmanian, 1992; Lakshmanan & Sadri, 1994). Bayesian networks
limitation well (only finite number random variables representable).68
various semantic/syntactic restrictions logic programs. instance acyclicity
condition imposed Poole (1993) Ngo Haddawy (1997) prevents unconditional
use clauses local variables, range-restrictedness imposed Muggleton
(1996) Cussens (1999) excludes programs usual membership Prolog program.
another type problem, possibility assigning con icting probabilities
logically equivalent formulas. SLPs, P(A) P(A ^ A) necessarily coincide
^ may different refutations (Muggleton, 1996; Cussens, 1999, 2001).
Consequently SLPs, would trouble naively interpret P(A) probability
A's true. assigning probabilities arbitrary quantified formulas seems
scope approaches SLPs.
Last least, big problem common approach using probabilities:
numbers come from? Generally speaking, use n binary random variables
model, determine 2n probabilities completely specify joint distribution,
fulfilling requirement reliable numbers quickly becomes impossible n grows.
situation even worse unobservable variables model
possible causes disease. Apparently parameter learning observed data natural
solution problem, parameter learning logic programs well studied.
Distribution semantics proposed Sato (1995) attempt solve problems
along line global distribution approach. defines distribution (probability
measure) possible interpretations ground atoms arbitrary logic program
first order language assigns consistent probabilities closed formulas.
distribution semantics enabled us derive EM algorithm parameter learning
logic programs first time. naive algorithm however, dealing large
problems dicult exponentially many explanations observation
HMMs. believe eciency problem solved large extent
graphical EM algorithm presented paper.

7.3 EM Learning

Since EM learning one central issues paper, separately mention work
related EM learning symbolic frameworks. Koller Pfeffer (1997) used
approach KBMC (knowledge-based model construction) EM learning estimate parameters labeling clauses. express probabilistic dependencies among events definite clauses annotated probabilities, similarly Ngo Haddawy's (1997) approach,
locally build Bayesian network relevant context evidence well
68. However, RPMs (recursive probability models) proposed Pfeffer Koller (2000) extension
Bayesian networks allow infinitely many random variables. organized attributes
classes probability measure attribute values introduced.
442

fiParameter Learning Logic Programs Symbolic-statistical Modeling

query. Parameters learned applying constructed network specialized EM
algorithm Bayesian networks (Castillo et al., 1997).
Dealing PCFG statically constructed Bayesian network proposed Pynadath Wellman (1998), possible combine EM algorithm method
estimate parameters PCFG. Unfortunately, constructed network singly
connected, time complexity probability computation potentially exponential
length input sentence.
Closely related EM learning parameter learning log-linear models. Riezler (1998) proposed IM algorithm approach probabilistic constraint programming. IM algorithm general parameter estimation algorithm incomplete data
log-linear models whose probability function P(x) takes form P(x) =
Z 01 exp( ni=1 (x)) p0 (x) (1 ; : : : ; n ) parameters estimated, (x)
i-th feature observed object x Z normalizing constant. Since feature
function x, log-linear model highly exible includes distribution
Pmsw special case Z = 1. price pay however; computational cost
Z . requires summation exponentially many terms. avoid cost exact
computation, approximate computation Monte Carlo method possible. Whichever
one may choose however, learning time increases compared EM algorithm Z = 1.
FAM (failure-adjusted maximization) algorithm proposed Cussens (2001)
EM algorithm applicable pure normalized SLPs may fail. deals special
class log-linear models ecient IM algorithm. statistical
framework FAM rather different distribution semantics, comparison
graphical EM algorithm seems dicult.
slightly tangential EM learning, Koller et al. (1997) developed functional
modeling language defining probability distribution symbolic structures
showed \cashing" computed results leads ecient probability computation
singly connected Bayesian networks PCFGs. cashing corresponds computation inside probability Inside-Outside algorithm computation outside
probability untouched.
P

7.4 Future Directions

Parameterized logic programs expected useful modeling tool complex symbolicstatistical phenomena. tried various types modeling, besides stochastic grammars Bayesian networks, modeling gene inheritance Kariera tribe
(White, 1963) rules bi-lateral cross-cousin marriage four clans interact
rules genetic inheritance (Sato, 1998). model quite interdisciplinary,
exibility combining msw atoms means definite clauses greatly facilitated
modeling process.
Although satisfying five conditions Section 4
uniqueness condition (roughly, one cause yields one effect)
finite support condition (there finite number explanations one observation)
acyclic support condition (explanations must cyclic)
443

fiSato & Kameya

t-exclusiveness condition (explanations must mutually exclusive)
independence condition (events explanation must independent)

applicability graphical EM algorithm seems daunting, modeling experiences far tell us modeling principle Section 4 effectively guides us successful
modeling. return, obtain declarative model described compactly high level
language whose parameters eciently learnable graphical EM algorithm shown
preceding section.
One future directions however relax applicability conditions,
especially uniqueness condition prohibits generative model failure
generating multiple observable events. Although pointed Section 4.4 MAR
condition Appendix B adapted semantics replace uniqueness condition
validates use graphical EM algorithm even complete data uniquely
determine observed data case \partially bracketed corpora" (Pereira &
Schabes, 1992), feel need research topic. investigating
role acyclicity condition seems theoretically interesting acyclicity often
related learning logic programs (Arimura, 1997; Reddy & Tadepalli, 1998).
paper scratched surface individual research fields HMMs,
PCFGs Bayesian networks. Therefore, remains much done clarifying
experiences research field ected framework parameterized logic
programs. example, need clarify relationship symbolic approaches
Bayesian networks SPI (Li, Z. & D'Ambrosio, B., 1994) approach.
unclear compiled approach using junction tree algorithm Bayesian
networks incorporated approach. Aside exact methods, approximate
methods probability computation specialized parameterized logic programs must
developed.
direction improving learning ability introducing priors instead ML
estimation cope data sparseness. introduction basic distributions make
probabilistic switches correlated seems worth trying near future. important
take advantage logical nature approach handle uncertainty. example,
already shown Sato (2001) learn parameters negative examples
\the grass wet" treatment negative examples parameterized
logic programs still infancy.
Concerning developing complex statistical models based \programs distributions" scheme, stochastic natural language processing exploits semantic information
seems promising. instance, unification-based grammars HPSGs (Abney, 1997)
may good target beyond PCFGs use feature structures logically describable, ambiguity feature values seems expressible probability
distribution.
building mathematical basis logic programs continuous random variables
challenging research topic.

444

fiParameter Learning Logic Programs Symbolic-statistical Modeling
8. Conclusion

proposed logical/mathematical framework statistical parameter learning
parameterized logic programs, i.e. definite clause programs containing probabilistic facts
parameterized probability distribution. extends traditional least Herbrand
model semantics logic programming distribution semantics , possible world semantics
probability distribution possible worlds (Herbrand interpretations)
unconditionally applicable arbitrary logic programs including ones HMMs, PCFGs
Bayesian networks.
presented new EM algorithm, graphical EM algorithm Section 4,
learns statistical parameters observations class parameterized logic programs representing sequential decision process decision exclusive
independent. works support graph s, new data structure specifying logical relationship observed goal explanations, estimates parameters computing
inside outside probability generalized logic programs.
complexity analysis Section 5 showed OLDT search, complete tabled
refutation method logic programs, employed support graph construction
table access done O(1) time, graphical EM algorithm, despite generality,
time complexity existing EM algorithms, i.e. Baum-Welch algorithm
HMMs, Inside-Outside algorithm PCFGs one singly connected Bayesian
networks developed independently research field. addition,
pseudo probabilistic context sensitive grammars N nonterminals, showed
graphical EM algorithm runs time O(N 4 L3) sentence length L.
compare actual performance graphical EM algorithm InsideOutside algorithm, conducted learning experiments PCFGs Section 6 using two
real corpora contrasting characters. One ATR corpus containing short sentences
grammar much ambiguous (958 parses/sentence), EDR
corpus containing long sentences grammar rather ambiguous (3:0 2 108
average sentence length 20). cases, graphical EM algorithm outperformed
Inside-Outside algorithm orders magnitude terms time per iteration,
suggests effectiveness approach EM learning graphical EM algorithm.
Since semantics limited finite domains finitely many random variables
applicable logic programs arbitrary complexity, graphical EM algorithm
expected give general yet ecient method parameter learning models complex
symbolic-statistical phenomena governed rules probabilities.
Acknowledgments

authors wish thank three anonymous referees comments suggestions.
Special thanks go Takashi Mori Shigeru Abe stimulating discussions learning
experiments, Tanaka-Tokunaga Laboratory kindly allowing use
MSLR parser linguistic data.

445

fiSato & Kameya
Appendix A. Properties

PDB

appendix, list properties PDB defined parameterized logic program
DB = F [ R countable first-order language L.69 First all, PDB assigns consistent
probabilities70 every closed formula L
PDB () def
= PDB (f! 2
DB j ! j= g)
guaranteeing continuity sense
limn!1 PDB ((t1 ) ^ 1 11 ^ (tn)) = PDB (8x(x))
limn!1 PDB ((t1 ) _ 1 11 _ (tn)) = PDB (9x(x))
t1 ; t2 ; : : : enumeration ground terms L.
next proposition, Proposition A.1, relates PDB Herbrand model. prove
it, need terminology. factor closed formula prenex disjunctive normal
form Q1 1 1 1 QnM Qi (1 n) either existential quantification universal
quantification matrix. length quantifications n called rank
factor. Define 8 set formulas made factors, conjunctions disjunctions.
Associate formula 8 multi-set r() ranks
;
factor quantification
r () =
fng
factor rank n
r(1) ] r (2 ) = 1 _ 2 = 1 ^ 2:
] stands union two multi-sets. instance f1; 2; 3g]f2; 3; 4g = f1; 2; 2; 3; 3; 4g.
use multi-set ordering proof Proposition A.1 usual induction
complexity formulas work.
Lemma A.1 Let boolean formula made ground atoms L. PDB() =
PF (f 2
F j MDB ( ) j= g).
(Proof) prove lemma conjunction atoms form D1x ^
1 11 ^ Dnx (xi 2 f0; 1g; 1 n).
PDB (D1x ^ 1 11 ^ Dnx ) = PDB (f! 2
DB j ! j= D1x ^ 11 1 ^ Dnx g)
= PDB (D1 = x1 ; : : : ; Dn = xn)
= PF (f 2
F j MDB ( ) j= D1x ^ 1 11 ^ Dnx g) Q.E.D.
8
>
<
>
:

1

n

1

n

n

1

1

n

Proposition A.1 Let closed formula L. PDB() = PF (f 2
F j MDB( ) j= g).
69. definitions
F , PF , MDB ( ),
DB , PDB others used below, see Section 3.
70. consistent, mean probabilities assigned logical formulas respect laws probability
0 P (A) 1, P (:A) = 1 0 P (A) P (A _ B ) = P (A) + P (B ) 0 P (A ^ B ).
446

fiParameter Learning Logic Programs Symbolic-statistical Modeling

(Proof) Recall closed formula equivalent prenex disjunctive normal form
belongs 8. prove proposition formulas 8 using induction
multi-set ordering fr() j 2 8g. r() = ;, quantification.
proposition correct Lemma A.1. Suppose otherwise. Write = G[Q1 Q2 11 1 Qn F ]
Q1 Q2 1 11 QnF indicates single occurrence factor G.71 assume Q1 = 9x
(Q1 = 8x similarly treated). assume bound variables renamed avoid
name clash. G[9xQ2 11 1 Qn F ] equivalent 9xG[Q2 1 11 QnF ] light validity
(9xA) ^ B = 9x(A ^ B) (9xA) _ B = 9x(A _ B) B contains free x.
PDB () = PDB (G[Q1 Q2 1 11 QnF ])
= PDB (9xG[ Q2 11 1 Qn F [x]])
= klim
P (G[ Q2 11 1 Qn F [t1 ]] _ 1 11 _ G[Q2 11 1 Qn F [tk ]])
!1 DB
= klim
P (G[ Q2 11 1 Qn F [t1 ] _ 1 11 _ Q2 11 1 Qn F [tk ]])
!1 DB
= klim
P (f 2
F j MDB ( ) j= G[ Q2 1 11 QnF [t1] _ 11 1 _ Q2 1 11 QnF [tk ] ]g)
!1 F
(by induction hypothesis)
= PF (f 2
F j MDB ( ) j= 9xG[Q2 1 11 QnF [x]]g)
= PF (f 2
F j MDB ( ) j= g)
Q.E.D.
next prove theorem iff definition introduced Section 4. Distribution
semantics considers program DB = F [ R set infinitely many ground definite
clauses F set facts (with probability measure PF ) R set rules,
clause head R appears F . Put
head(R) def
= fB j B appears R clause headg:
B 2 head(R), let B Wi (i = 1; 2; : : :) enumeration clauses B R.
Define iff (B), iff (if-and-only-if) form rules B DB72
iff (B) def
= B $ W1 _ W2 _ 1 1 1
Since MDB ( ) least Herbrand model, following obvious.
Lemma A.2 B head(R) 2
F , MDB( ) j= iff (B).
Theorem A.1 iff (B). states general level, sides iff
definition p(x) $ 9y1 (x = t1 ^ W1 ) _ 1 11 _ 9yn (x = tn ^ Wn) p(1) coincide random
variables whenever x instantiated ground term.
Theorem A.1 Let iff (B ) = B $ W1 _ W2 _11 1 iff form rules B 2 head(R).
PDB (iff (B )) = 1 PDB (B ) = PDB (W1 _ W2 _ 1 11).
71. expression E , E [ ] means may occur specified positions E . 1 _ 2 E [ 1 _ 2 ]
indicates single occurrence 1 _ 2 positive boolean formula E , E [ 1 _ 2 ] = E [ 1 ] _ E [ 2 ] holds.
72. definition different usual one (Lloyd, 1984; Doets, 1994) talking ground
level. W1 _ W2 _ 1 1 1 true one disjuncts true.
447

fiSato & Kameya

(Proof)

PDB (iff (B ))

=

PDB (f! 2
DB j ! j= B ^ (W1 _ W2 _ 11 1)g)
+PDB (f! 2
DB j ! j= :B ^ :(W1 _ W2 _ 1 11)g)

= klim
P (f! 2
DB j ! j= B ^
!1 DB

k
_

i=1

Wi g)

+ klim
P (f! 2
DB j ! j= :B ^ :
!1 DB
= klim
P (f 2
F j MDB ( ) j= B ^
!1 F

k
_
i=1

k
_
i=1

Wi g)

Wi g)
k
_

+ klim
P (f 2
F j MDB ( ) j= :B ^ : Wi g)
!1 F
i=1
(Lemma A.1)
= PF (f 2
F j MDB ( ) j= iff (B)g)
= PF (
F ) (Lemma A.2)
= 1
follows PDB (iff (B)) = 1
PDB (B ) = PDB (B ^ iff(B )) = PDB (W1 _ W2 _ 1 1 1):
Q.E.D.
prove proposition useful probability computation. Let DB (B )
support set atom B introduced Section 4 (it set explanations B).
sequel, B ground atom. Write DB (B) = fS1 ; S2; : : :g DB (B) = S1 _S2 _1 1173
Define set 3B
3B def
= f! 2
DB j ! j= B $ DB (B)g:
Proposition A.2 every B 2 head(R), PDB(3B ) = 1 PDB (B) = PDB( DB (B)).
(Proof) first prove PDB (3B ) = 1 proof exactly parallels Theorem A.1
except W1 _ W2 _ 1 1 1 replaced S1 _ S2 _ 11 1 using fact B $ S1 _ S2 _ 11 1
true every least Herbrand model form MDB ( ). PDB (3B ) = 1,

PDB (B ) = PDB (B ^ (B $
DB (B )))
= PDB ( DB (B)):
Q.E.D.
Finally, show distribution semantics probabilistic extension traditional
least Herbrand model semantics logic programming proving Theorem A.2. says
probability mass distributed exclusively possible least Herbrand models.
Define 3 set least Herbrand models generated fixing R varying subset
F program DB = F [ R. symbols,
W

_

W

_

_

W

73. set K = fE1 ; E2 ; : : :g formulas, K denotes (-n infinite) disjunction E1 _ E2 _ 1 1 1
448

fiParameter Learning Logic Programs Symbolic-statistical Modeling

3 def
= f! 2
DB j ! = MDB () 2
F g:
Note 3 merely subset
DB , cannot conclude PDB (3) = 1 priori,
next theorem, Theorem A.2, states PDB (3) = 1, i.e. distribution semantics distributes
probability mass exclusively 3, i.e. possible least Herbrand models.
prove theorem, need preparations. Recalling atoms outside head(R)[
F chance proved DB, introduce
30 def
= f! 2
DB j ! j= :D every ground atom 62 head(R) [ F g:
Herbrand interpretation ! 2
DB , !jF (2
F ) restriction ! atoms
F .
Lemma A.3 Let ! 2
DB Herbrand
interpretation.
0
! = MDB ( ) 2
F iff ! 2 3 ! j= B $ DB (B ) every B 2 head(R).
(Proof) Only-if part immediate property least Herbrand model.
if-part, suppose ! satisfies right hand side. show ! = MDB (!jF ). !
MDB (! jF ) coincide w.r.t. atoms head(R), enough prove give
truth values atoms head(R). Take B 2 head(R) write DB (B) =
S1 _ S2 _ 1 11 Suppose ! j= B $ S1 _ S2 _ 11 1 ! j= B , ! j= Sj j ,
thereby !jF j= Sj , hence MDB (!jF ) j= Sj , implies MDB (!jF ) j= B. Otherwise
! j= :B . ! j= :Sj every j . follows MDB (! jF ) j= :B . Since B arbitrary,
conclude ! MDB (!jF ) agree truth values assigned atoms head(R)
well.
Q.E.D.
W

W

Theorem A.2 PDB(3) = 1.

(Proof) Lemma A.3,
3 = f! 2
DB j ! = MDB ( ) 2
F g
= 30 \
3B :
\

B2head(R)

PDB (3B ) = 1 Proposition A.2. prove PDB (30 ) = 1, let D1; D2 ; : : : enumeration
atoms belonging head(R) [ F . provable DB = F [ R,
hence false every least Herbrand model MDB ( ) ( 2
F ).
PDB (30 )

= mlim
!1 PDB (f! 2
DB j ! j= :D1 ^ 11 1 ^ :Dm g)
= mlim
!1 PF (f 2
F j MDB ( ) j= :D1 ^ 1 1 1 ^ :Dm g)
= PF (
F ) = 1:
Since countable conjunction measurable sets probability measure one
probability measure one, follows PDB (3B ) = 1 every B 2 head(R) PDB (30) =
1 PDB (3) = 1.
Q.E.D.
449

fiSato & Kameya
Appendix B. MAR (missing random) Condition

original formulation EM algorithm Dempster et al. (1977), assumed
exists many-to-one mapping = (x) complete data x incomplete
(observed) data y. case parsing, x parse tree input sentence x
uniquely determines y. paper, uniqueness condition ensures existence
many-to-one mapping explanations observations. however sometimes face
situation many-to-one mapping complete data incomplete
data nonetheless wish apply EM algorithm.
dilemma solved introduction missing-data mechanism
makes complete data incomplete. missing-data mechanism, m, distribution
g (m j x) parameterized , observed data, described = (x). says
x becomes incomplete m. correspondence x , i.e. fhx; j 9m(y =
(x))g naturally becomes many-to-many.
Rubin (1976) derived two conditions g (data missing random data
observed random) collectively called MAR (missing random) condition, showed
assume missing-data mechanism behind observations satisfies MAR
condition, may estimate parameters distribution x simply applying
EM algorithm y, observed data.
adapt MAR condition parameterized logic programs follows. keep
generative model satisfying uniqueness condition outputs goals G parse
trees. extend model additionally inserting missing-data mechanism
G observation = (G) assume satisfies MAR
condition. extended model many-to-many correspondence explanations observations, generates non-exclusive observations P (O ^ O0 ) > 0
(O 6= O0 ), causes P (O) 1 P (O) = G:9m O= (G) PDB (G). Thanks
MAR condition however, still allowed apply EM algorithm nonexclusive observations. Put differently, even uniqueness condition seemingly
destroyed, EM algorithm applicable (imaginarily) assuming missing-data
mechanism satisfying MAR condition.
P

P



References

Abney, S. (1997). Stochastic attribute-value grammars. Computational Linguistics, 23 (4),
597{618.
Arimura, H. (1997). Learning acyclic first-order horn sentences entailment.
Proceedings Eighth International Workshop Algorithmic Learning Theory.
Ohmsha/Springer-Verlag.
Bacchus, F., Grove, A., Halpern, J., & Koller, D. (1996). statistical knowledge bases
degrees belief. Artificial Intelligence, 87, 75{143.
Baker, J. K. (1979). Trainable grammars speech recognition. Proceedings Spring
Conference Acoustical Society America, pp. 547{550.

450

fiParameter Learning Logic Programs Symbolic-statistical Modeling

Beil, F., Carroll, G., Prescher, D., Riezler, S., & Rooth, M. (1999). Inside-Outside estimation
lexicalized PCFG German. Proceedings 37th Annual Meeting
Association Computational Linguistics (ACL'99), pp. 269{276.
Breese, J. S. (1992). Construction belief decision networks. Computational Intelligence, 8 (4), 624{647.
Carroll, G., & Rooth, M. (1998). Valence induction head-lexicalized PCFG. Proceedings 3rd Conference Empirical Methods Natural Language Processing
(EMNLP 3).
Castillo, E., Gutierrez, J. M., & Hadi, A. S. (1997). Expert Systems Probabilistic
Network Models. Springer-Verlag.
Charniak, E., & Carroll, G. (1994). Context-sensitive statistics improved grammatical language models. Proceedings 12th National Conference Artificial
Intelligence (AAAI'94), pp. 728{733.
Chi, Z., & Geman, S. (1998). Estimation probabilistic context-free grammars. Computational Linguistics, 24 (2), 299{305.
Chow, Y., & Teicher, H. (1997). Probability Theory (3rd ed.). Springer.
Clark, K. (1978). Negation failure. Gallaire, H., & Minker, J. (Eds.), Logic
Databases, pp. 293{322. Plenum Press.
Cormen, T., Leiserson, C., & Rivest, R. (1990). Introduction Algorithms. MIT Press.
Cussens, J. (1999). Loglinear models first-order probabilistic reasoning. Proceedings
15th Conference Uncertainty Artificial Intelligence (UAI'99), pp. 126{133.
Cussens, J. (2001). Parameter estimation stochastic logic programs. Machine Learning,
44 (3), 245{271.
D'Ambrosio, B. (1999). Inference Bayesian networks. AI Magazine, summer, 21{36.
Dekhtyar, A., & Subrahmanian, V. S. (1997). Hybrid probabilistic programs. Proceedings
14th International Conference Logic Programming (ICLP'97), pp. 391{405.
Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood incomplete
data via EM algorithm. Royal Statistical Society, B39 (1), 1{38.
Doets, K. (1994). Logic Logic Programming. MIT Press.
Flach, P., & Kakas, A. (Eds.). (2000). Abduction Induction { Essays Relation
Integration. Kluwer Academic Publishers.
Frish, A., & Haddawy, P. (1994). Anytime deduction probabilistic logic. Journal
Artificial Intelligence, 69, 93{122.
Fujisaki, T., Jelinek, F., Cocke, J., Black, E., & Nishino, T. (1989). probabilistic parsing
method sentence disambiguation. Proceedings 1st International Workshop
Parsing Technologies, pp. 85{94.
Japan EDR, L. (1995). EDR electronic dictionary technical guide (2nd edition). Technical
report, Japan Electronic Dictionary Research Institute, Ltd.
451

fiSato & Kameya

Kakas, A. C., Kowalski, R. A., & Toni, F. (1992). Abductive logic programming. Journal
Logic Computation, 2 (6), 719{770.
Kameya, Y. (2000). Learning Representation Symbolic-Statistical Knowledge (in
Japanese). Ph. D. dissertation, Tokyo Institute Technology.
Kameya, Y., & Sato, T. (2000). Ecient EM learning parameterized logic programs.
Proceedings 1st Conference Computational Logic (CL2000), Vol. 1861
Lecture Notes Artificial Intelligence, pp. 269{294. Springer.
Kita, K. (1999). Probabilistic Language Models (in Japanese). Tokyo Daigaku Syuppan-kai.
Koller, D., McAllester, D., & Pfeffer, A. (1997). Effective Bayesian inference stochastic programs. Proceedings 15th National Conference Artificial Intelligence
(AAAI'97), pp. 740{747.
Koller, D., & Pfeffer, A. (1997). Learning probabilities noisy first-order rules. Proceedings 15th International Joint Conference Artificial Intelligence (IJCAI'97),
pp. 1316{1321.
Kyburg, H. (1994). Uncertainty logics. Gabbay, D., Hogger, C., & Robinson, J. (Eds.),
Handbook Logics Artificial Intelligence Logic Programming, pp. 397{438.
Oxford Science Publications.
Lafferty, J. (1993). derivation Inside-Outside Algorithm EM algorithm.
Technical report, IBM T.J.Watson Research Center.
Lakshmanan, L. V. S., & Sadri, F. (1994). Probabilistic deductive databases. Proceedings
1994 International Symposium Logic Programming (ILPS'94), pp. 254{268.
Lari, K., & Young, S. J. (1990). estimation stochastic context-free grammars using
Inside-Outside algorithm. Computer Speech Language, 4, 35{56.
Li, Z., & D'Ambrosio, B. (1994). Ecient inference Bayes networks combinatorial
optimization problem. International Journal Approximate Reasoning, 11, 55{81.
Lloyd, J. W. (1984). Foundations Logic Programming. Springer-Verlag.
Lukasiewicz, T. (1999). Probabilistic deduction conditional constraints basic
events. Journal Artificial Intelligence Research, 10, 199{241.
Manning, C. D., & Schutze, H. (1999). Foundations Statistical Natural Language Processing. MIT Press.
McLachlan, G. J., & Krishnan, T. (1997). EM Algorithm Extensions. Wiley
Interscience.
Muggleton, S. (1996). Stochastic logic programs. de Raedt, L. (Ed.), Advances
Inductive Logic Programming, pp. 254{264. IOS Press.
Ng, R., & Subrahmanian, V. S. (1992). Probabilistic logic programming. Information
Computation, 101, 150{201.
Ngo, L., & Haddawy, P. (1997). Answering queries context-sensitive probabilistic
knowledge bases. Theoretical Computer Science, 171, 147{177.
Nilsson, N. J. (1986). Probabilistic logic. Artificial Intelligence, 28, 71{87.
452

fiParameter Learning Logic Programs Symbolic-statistical Modeling

Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems. Morgan Kaufmann.
Pereira, F. C. N., & Schabes, Y. (1992). Inside-Outside reestimation partially bracketed
corpora. Proceedings 30th Annual Meeting Association Computational Linguistics (ACL'92), pp. 128{135.
Pereira, F. C. N., & Warren, D. H. D. (1980). Definite clause grammars language analysis
| survey formalism comparison augmented transition networks.
Artificial Intelligence, 13, 231{278.
Pfeffer, A., & Koller, D. (2000). Semantics inference recursive probability models.
Proceedings Seventh National Conference Artificial Intelligence (AAAI'00),
pp. 538{544.
Poole, D. (1993). Probabilistic Horn abduction Bayesian networks. Artificial Intelligence, 64 (1), 81{129.
Pynadath, D. V., & Wellman, M. P. (1998). Generalized queries probabilistic context-free
grammars. IEEE Transaction Pattern Analysis Machine Intelligence, 20 (1),
65{77.
Rabiner, L. R. (1989). tutorial hidden markov models selected applications
speech recognition. Proceedings IEEE, 77 (2), 257{286.
Rabiner, L. R., & Juang, B. (1993). Foundations Speech Recognition. Prentice-Hall.
Ramakrishnan, I., Rao, P., Sagonas, K., Swift, T., & Warren, D. (1995). Ecient tabling
mechanisms logic programs. Proceedings 12th International Conference
Logic Programming (ICLP'95), pp. 687{711. MIT Press.
Reddy, C., & Tadepalli, P. (1998). Learning first-order acyclic horn programs entailment. Proceedings 15th International Conference Machine Learning;
(and Proceedings 8th International Conference Inductive Logic Programming). Morgan Kaufmann.
Riezler, S. (1998). Probabilistic Constraint Logic Programming. Ph.D. thesis, Universitat
Tubingen.
Rubin, D. (1976). Inference missing data. Biometrika, 63 (3), 581{592.
Sagonas, K., T., S., & Warren, D. (1994). XSB ecient deductive database engine.
Proceedings 1994 ACM SIGMOD International Conference Management
Data, pp. 442{453.
Sato, T. (1995). statistical learning method logic programs distribution semantics.
Proceedings 12th International Conference Logic Programming (ICLP'95),
pp. 715{729.
Sato, T. (1998). Modeling scientific theories PRISM programs. Proceedings ECAI'98
Workshop Machine Discovery, pp. 37{45.
Sato, T. (2001). Minimum likelihood estimation negative examples statistical abduction. Proceedings IJCAI-01 workshop Abductive Reasoning, pp. 41{47.
Sato, T., & Kameya, Y. (1997). PRISM: language symbolic-statistical modeling.
Proceedings 15th International Joint Conference Artificial Intelligence
(IJCAI'97), pp. 1330{1335.
453

fiSato & Kameya

Sato, T., & Kameya, Y. (2000). Viterbi-like algorithm EM learning statistical
abduction. Proceedings UAI2000 Workshop Fusion Domain Knowledge
Data Decision Support.
Sato, T., Kameya, Y., Abe, S., & Shirai, K. (2001). Fast EM learning family PCFGs.
Titech technical report (Dept. CS) TR01-0006, Tokyo Institute Technology.
Shen, Y., Yuan, L., You, J., & Zhou, N. (2001). Linear tabulated resolution based Prolog
control strategy. Theory Practice Logic Programming, 1 (1), 71{103.
Sterling, L., & Shapiro, E. (1986). Art Prolog. MIT Press.
Stolcke, A. (1995). ecient probabilistic context-free parsing algorithm computes
prefix probabilities. Computational Linguistics, 21 (2), 165{201.
Tamaki, H., & Sato, T. (1984). Unfold/fold transformation logic programs. Proceedings
2nd International Conference Logic Programming (ICLP'84), Lecture Notes
Computer Science, pp. 127{138. Springer.
Tamaki, H., & Sato, T. (1986). OLD resolution tabulation. Proceedings 3rd
International Conference Logic Programming (ICLP'86), Vol. 225 Lecture Notes
Computer Science, pp. 84{98. Springer.
Tanaka, H., Takezawa, T., & Etoh, J. (1997). Japanese grammar speech recognition
considering MSLR method. Proceedings meeting SIG-SLP (Spoken
Language Processing), 97-SLP-15-25, pp. 145{150. Information Processing Society
Japan. Japanese.
Uratani, N., Takezawa, T., Matsuo, H., & Morita, C. (1994). ATR integrated speech
language database. Technical report TR-IT-0056, ATR Interpreting Telecommunications Research Laboratories. Japanese.
Warren, D. S. (1992). Memoing logic programs. Communications ACM, 35 (3),
93{111.
Wetherell, C. S. (1980). Probabilistic languages: review open questions. Computing Surveys, 12 (4), 361{379.
White, H. C. (1963). Anatomy Kinship. Prentice-Hall.
Zhang, N., & Poole, D. (1996). Exploiting causal independence Bayesian network inference. Journal Artificial Intelligence Research, 5, 301{328.

454


