Journal of Artificial Intelligence Research 53 (2015) 659-697

Submitted 4/15; published 8/15

Evolutionary Dynamics of Multi-Agent Learning:
A Survey
Daan Bloembergen
Karl Tuyls

d.bloembergen@liverpool.ac.uk
k.tuyls@liverpool.ac.uk

Department of Computer Science, University of Liverpool
Ashton Building, Ashton Street, Liverpool L69 3BX, UK

Daniel Hennes

daniel.hennes@esa.int

Advanced Concepts Team, European Space Agency
Keplerlaan 1, 2201 AZ Noordwijk, NL

Michael Kaisers

m.kaisers@cwi.nl

Centrum Wiskunde & Informatica
Science Park 123, 1098 XG Amsterdam, NL

Abstract
The interaction of multiple autonomous agents gives rise to highly dynamic and nondeterministic environments, contributing to the complexity in applications such as automated
financial markets, smart grids, or robotics. Due to the sheer number of situations that may
arise, it is not possible to foresee and program the optimal behaviour for all agents beforehand. Consequently, it becomes essential for the success of the system that the agents
can learn their optimal behaviour and adapt to new situations or circumstances. The past
two decades have seen the emergence of reinforcement learning, both in single and multiagent settings, as a strong, robust and adaptive learning paradigm. Progress has been
substantial, and a wide range of algorithms are now available. An important challenge in
the domain of multi-agent learning is to gain qualitative insights into the resulting system
dynamics. In the past decade, tools and methods from evolutionary game theory have
been successfully employed to study multi-agent learning dynamics formally in strategic
interactions. This article surveys the dynamical models that have been derived for various
multi-agent reinforcement learning algorithms, making it possible to study and compare
them qualitatively. Furthermore, new learning algorithms that have been introduced using these evolutionary game theoretic tools are reviewed. The evolutionary models can be
used to study complex strategic interactions. Examples of such analysis are given for the
domains of automated trading in stock markets and collision avoidance in multi-robot systems. The paper provides a roadmap on the progress that has been achieved in analysing
the evolutionary dynamics of multi-agent learning by highlighting the main results and
accomplishments.

1. Introduction
In a multi-agent system, several autonomous agents interact in the same environment.
Therefore, multi-agent systems can be used to model many complex problems of todays
society, such as urban and air traffic control (Agogino & Tumer, 2012), multi-robot coordination (Ahmadi & Stone, 2006; Claes, Hennes, Tuyls, & Meeussen, 2012), distributed
sensing (Mihaylov, Tuyls, & Nowe, 2014), energy distribution (Pipattanasomporn, Feroze,
& Rahman, 2009), and load balancing (Schaerf, Shoham, & Tennenholtz, 1995; Verbeeck,
c
2015
AI Access Foundation. All rights reserved.

fiBloembergen, Tuyls, Hennes, & Kaisers

Nowe, & Tuyls, 2005). The fact that multiple agents interact leads to a highly dynamic,
non-deterministic environment. In such an environment, defining proper behaviour for each
agent in advance is non-trivial and therefore learning is crucial. Recent publications at
agents and machine learning conferences, as well as papers published in related mainstream
journals, make clear that the number of newly proposed multi-agent learning algorithms is
constantly growing. An overview of well-established multi-agent learning algorithms with
their various purposes can be attained from previous multi-agent learning survey papers
(Panait & Luke, 2005; t Hoen, Tuyls, Panait, Luke, & Poutre, 2005; Busoniu, Babuska, &
De Schutter, 2008; Tuyls & Weiss, 2012), and demonstrates the need for a comprehensive
understanding of their qualitative similarities and differences. Although single-agent learning  in particular reinforcement learning  has been extensively studied and acquired a
strong theoretical foundation (Kaelbling, Littman, & Moore, 1996; Sutton & Barto, 1998),
a thorough understanding of learning in multi-agent settings has long remained an open
problem (Tuyls, t Hoen, & Vanschoenwinkel, 2006).
Learning in multi-agent systems is not only relevant within the field of artificial intelligence but has been extensively studied in game theory and economics as well (Shoham,
Powers, & Grenager, 2007). It is not surprising then, that these fields share a lot of common ground. Indeed, game theory often provides the context in which multi-agent systems
are modelled and evaluated. Recently, some multi-agent learning research has shifted its
focus from traditional game theory to evolutionary game theory (Tuyls et al., 2006; Tuyls
& Parsons, 2007). The concepts employed by evolutionary game theory prove well suited to
describe learning in multi-agent systems. Both fields are concerned with dynamic environments with a high level of uncertainty, characterised by the fact that agents lack complete
information (Tuyls et al., 2006). Moreover, there exists a formal relation between the behaviour of one of the most basic reinforcement learning algorithms, Cross learning (Cross,
1973), and the population dynamics of evolutionary game theory, described by the replicator dynamics (Borgers & Sarin, 1997). Although this link was originally established in the
context of stateless normal-form games only, it has since been extended to more complex
scenarios as well (e.g., Hennes, Tuyls, & Rauterberg, 2009; Tuyls & Westra, 2009; Galstyan,
2013; Panozzo, Gatti, & Restelli, 2014).
Understanding this relation sheds light into the black box of reinforcement learning
by making it possible to analyse the learning dynamics of multi-agent systems in detail
and to compare the behaviour of different algorithms in a principled manner. This in turn
facilitates important tasks such as parameter tuning, and helps in selecting a specific learner
for a given problem. Tuyls and Nowe (2005) and Tuyls et al. (2006) were the first to present
an overview of this evolutionary game theoretic approach to multi-agent learning. They
build on the connection between Cross learning and the replicator dynamics and extend
this link to learning automata and Q-learning as well. However, much progress has been
made in the past decade, warranting an up-to-date overview and roadmap of this research
area. This is precisely our aim in this work.
We believe that evolutionary game theory has a lot to offer for both the understanding
and application of multi-agent learning. Shoham et al. (2007) call for a more grounded
approach to research in multi-agent learning, suggesting five agendas to which such research
could contribute. They also caution not to rely too strongly on requirements such as
convergence to a Nash equilibrium when evaluating learning algorithms in a multi-agent
660

fiEvolutionary Dynamics of Multi-Agent Learning

setting. In response, Tuyls and Parsons (2007) argue in favour of evolutionary game theory,
rather than classical game theory, as the preferable framework within which to study multiagent learning formally. They show how research on the evolutionary framework contributes
to each of the five agendas of research identified by Shoham et al. (2007). Moreover, it allows
us to move away from the static Nash equilibrium, and focus instead on the transient
dynamics of the learning process.
In this article we describe the formal relation between evolutionary game theory and
multi-agent learning in detail and survey the recent advances and extensions that have
been made in this area.1 To this end we present a categorisation of related work based
on the nature of the environment (stateless or multi-state games) and the actions (discrete
or continuous) available to the learning agents. Moreover, we provide examples of the
successful application of this approach in relation to parameter tuning, the design of new
learning algorithms, and the analysis of complex strategic interactions such as automated
trading and multi-robot collision avoidance. The evolutionary game theoretic approach
offers a promising new paradigm within which to study multi-agent learning, as it provides
new insights towards the understanding, analysis, and design of multi-agent reinforcement
learning algorithms.
The paper proceeds as follows. Section 2 provides the necessary background on (multiagent) reinforcement learning and evolutionary game theory. Section 3 introduces the link
between the replicator dynamics and Cross learning and outlines the methodology of this
survey. An overview of recent advances and extensions is given in Section 4, supported
by empirical validation in Section 5. Section 6 presents examples of the application of the
evolutionary game theoretic approach. Section 7 concludes the article.

2. Preliminaries
In this section, we outline the fundamentals of multi-agent learning. Firstly, we concisely
present Markov decision processes (MDPs) as the standard formal framework for singleagent decision making, together with a well-known reinforcement learning algorithm, Qlearning. We then proceed to present stochastic games, also called Markov games, as a multiagent extension to MDPs, as well as three approaches to learning in this extended setting:
independent learning, joint action learning, and gradient based methods. Finally, we discuss
how evolutionary game theory can be used to reason about multi-agent interactions, paving
the way to formally relate these two fields in Section 3.
2.1 Reinforcement Learning
Reinforcement learning is based on the concept of trial-and-error learning, which underlies
many theories of (human) learning and intelligence (Sutton & Barto, 1998). The reinforcement learning agent continuously interacts with the environment, perceiving its state,
taking actions, and observing the effect of those actions (see Figure 1). Actions that yield
a positive effect will have a higher chance of being executed again in the future, that is to
1. We do not attempt to present a broad survey on multi-agent learning in general but focus solely on
those works that explicitly model multi-agent learning using methods from evolutionary game theory.
An excellent survey and taxonomy of multi-agent learning algorithms in general can be found in the
work of Busoniu et al. (2008).

661

fiBloembergen, Tuyls, Hennes, & Kaisers

Agent
st

rt

at
rt+1
st+1

Environment

Figure 1: A reinforcement learning agent perceives state st of the environment at time
t, decides to take action at , upon which the environment transitions to state st+1 and the
agent receives reward rt+1 .
say they are reinforced within the agents behaviour. To this effect, the agent receives a
reward signal that indicates the quality of the actions taken; however, the reward may be
stochastic, delayed, or accumulated over sequences of actions. Therefore, the agent needs to
balance exploration and exploitation, to avoid getting stuck in local optima. The objective
of the learning agent is to discover a policy, represented as a mapping from states to actions,
that maximises its long-term expected reward.
The single-agent reinforcement learning setting can be formalised as a Markov decision
process (MDP) (Puterman, 1994). An MDP is defined by finite state and action sets, S
and A, one-step state transition dynamics
a
0
Pss
0 = P (st+1 = s |st = s, at = a)

(1)

describing the probability of transitioning to state s0  S after taking action a  A in state
s, and the expected value of the next reward
Rass0 = E(rt+1 |st = s, at = a, st+1 = s0 )

(2)

given the previously executed action and resulting state transition. Both state transitions
and rewards can be stochastic, and in fact learning these stochastic models is a key task
in many reinforcement learning problems. The learning goal in an MDP is to find a policy
 that maps states to action selection probabilities, maximising expected reward. When
following a fixed policy  we can define the value of a state s under that policy as the total
amount of reward R the agent expects to accumulate when starting in state s and following
 thereafter:

X
V  (s) = E (Rt |st = s) = E (
 k rt+k+1 |st = s).
k=0

The rewards are discounted by factor   [0, 1) to ensure a bounded sum in infinite horizon
MDPs. The value function for a policy  can be calculated iteratively using the Bellman
equation (Bellman, 1957). Starting with an arbitrarily chosen value function V  , at each
iteration and for each state s the value function is then updated based on the immediate
reward and the current estimate of V  :
X
X
a
 0
a
(s, a)
Pss
V  (s) 
0 [Rss0 + V (s )].
aA(s)

s0 S

662

fiEvolutionary Dynamics of Multi-Agent Learning

The Bellman equation expresses the recursive relation between the value of a state and
its successor states, and averages over all possibilities, weighing each by its probability of
occurring. In this setting, finding an optimal policy   is equivalent to finding a policy that
maximises the value function, i.e.,


V  (s) = max V  (s) s  S.


When a model of the environment is available, in particular if P and R are known, the
Bellman equation can be applied to compute an optimal policy directly, using a dynamic
programming technique such as value iteration or policy iteration (Sutton & Barto, 1998).
In general, however, such a model may not be available. In this case, reinforcement learning can be used to find an optimal mapping from states to actions. Arguably the most
famous example of a reinforcement learning algorithm is the model-free temporal difference
algorithm Q-learning (Watkins & Dayan, 1992). Q-learning maintains a value function
over state-action pairs, Q(s, a), which it updates based on the immediate reward and the
discounted expected future reward according to Q:
h
i
Q(st , at )  Q(st , at ) +  rt+1 +  max Q(st+1 , a)  Q(st , at ) .
(3)
a

Here,  is the discount factor for future rewards as before, and   [0, 1] is the learning rate
that determines how quickly Q is updated based on new reward information. Q-learning
is proven to converge to the optimal policy, given sufficient updates for each state-action
pair, and a decreasing learning rate   0 (Watkins & Dayan, 1992).
Choosing which action to take is a crucial aspect of the learning process. Should the
agent exploit actions that yielded high reward in the past, or should it explore in order to
achieve potentially better results in the future, thereby risking a low reward now? Neither
of the two is sufficient on its own, and the dilemma is to find the right balance (Kaelbling
et al., 1996; Sutton & Barto, 1998). Two often used action selection mechanisms are greedy and softmax or Boltzmann exploration (Sutton & Barto, 1998). -Greedy selects
the best action (greedy w.r.t. Q) with probability 1  , and with probability  it selects
an action at random. The Boltzmann exploration mechanism makes use of a temperature
parameter  that controls the balance between exploration and exploitation. Action ai is
chosen in state s with probability
e Q(s,ai )/
pi = X
.
e Q(s,aj )/

(4)

j

A high temperature drives the mechanism towards exploration, whereas a low temperature
promotes exploitation, favouring actions with higher Q-values.
2.2 From Single-Agent to Multi-Agent Learning
The MDP framework assumes that a single agent is active in the environment. Once multiple
agents interact and learn simultaneously, the model needs to be extended. Stochastic games,
or Markov games, offer a generalisation of MDPs to the multi-agent domain (Littman, 1994).
In a stochastic game, each agent has its own set of actions, i.e., for n agents the joint-action
663

fiBloembergen, Tuyls, Hennes, & Kaisers

space is A = A1  A2  . . .  An . The state transition and reward functions now depend on
the joint action of all agents:
R : S  A1  . . .  An  S 7 Rn
P : S  A1  . . .  An  S 7 [0, 1].
The immediate rewards may be the same for all agents but they need not be in general.
A special case of stochastic games is the stateless setting described by normal-form games.
Normal-form games are one-shot interactions, where all agents simultaneously select an
action and receive a reward based on their joint action, after which the game ends. There is
no state transition function, and the reward function can be represented by an n-dimensional
payoff matrix, for n agents. An agents policy is simply a probability distribution over its
actions. Repeated normal form games are common benchmarks for multi-agent learning.
Such scenarios are detailed further in Section 2.3.
Learning in a multi-agent setting is inherently more complex than in the single-agent
case described previously, as agents interact both with the environment and potentially
with each other. Learning is simultaneous, meaning that changes in the policy of one agent
may affect the rewards and hence the optimal policy of others. Moreover, agents may
have conflicting interests, yet cooperation with competitors may yield short or long term
benefits. This makes it difficult to judge the learning process, since myopic maximisation
of individual rewards might not lead to the best overall solution. The fact that the reward
function depends on the actions of other agents leads to an important characteristic of
multi-agent reinforcement learning: the environment is non-stationary and as a result each
agent is essentially pursuing a moving target (Busoniu et al., 2008). Moreover, the fact
that multiple agents influence the environment means that, from the perspective of the
individual agents, the Markov property no longer holds. Two different approaches to multiagent learning can be distinguished: independent learning and joint-action learning (Claus
& Boutilier, 1998). In the following we briefly discuss both approaches, and list notable
algorithms in each class. For a detailed taxonomy of multi-agent learning algorithms, we
refer to the excellent survey of Busoniu et al. (2008).
2.2.1 Independent Learning
Independent learners mutually ignore each other, thereby effectively reducing the multiagent learning problem to a single-agent one. Interaction with other agents is implicitly
perceived as noise in a stochastic environment. The advantage of this approach is that
single-agent learning algorithms can straightforwardly be applied to a multi-agent setting,
and scalability in the number of agents is not an issue.2 However, stochasticity of the
environment means that convergence guarantees from the single-agent setting are lost. In
particular, the Markov property on which such proofs are typically based, no longer holds.
Moreover, no explicit mechanism for coordination is available to the agents. Despite these
drawbacks, independent learners have shown good performance in many multi-agent settings (Busoniu et al., 2008).
2. Computational complexity increases only linearly with the number of agents. Performance may vary
depending on the specific domain and algorithm.

664

fiEvolutionary Dynamics of Multi-Agent Learning

Traditional single-agent reinforcement learning algorithms, such as Q-learning (Watkins
& Dayan, 1992) and (networks of) learning automata (Narendra & Thathachar, 1974;
Wheeler Jr & Narendra, 1986; Vrancx, Verbeeck, & Nowe, 2008b), can be directly applied
in this setting. Moreover, various new independent learning algorithms have been proposed
specifically with the multi-agent setting in mind. For example, Bowling and Veloso (2001)
proposed policy hill climbing with the win or learn fast heuristic (WoLF-PHC), and show
that the algorithm is rational and convergent in multi-agent domains. Other examples
include frequency maximum Q-learning (Kapetanakis & Kudenko, 2002), an algorithm tailored to coordinate in cooperative multi-agent systems, and a class of regret minimisation
algorithms (Blum & Mansour, 2007) that guarantee performance close to the best fixed
action in hindsight against any opponent. Finally, two extensions to Q-learning have been
proposed that alleviate certain artifacts of this algorithm in non-stationary (e.g. multiagent) environments: frequency-adjusted Q-learning (Kaisers & Tuyls, 2010), and repeated
update Q-learning (Abdallah & Kaisers, 2013). Frequency-adjusted Q-learning has been
proven to converge in two-player two-action normal-form games (Kaisers & Tuyls, 2011;
Kianercy & Galstyan, 2012).
2.2.2 Joint-Action Learning
Whereas independent learners completely ignore the presence of other agents, joint-action
learners explicitly take them into account. Joint-action learners achieve this by learning
in the space of joint actions, rather than in their individual action space only (Claus &
Boutilier, 1998). They observe the actions of other agents in order to estimate their policy,
and then act optimally given those estimated policies. This way, joint action learners have
better means of coordination. The drawback is that the agent needs to be able to observe the
other agents actions, and assumptions about the opponents adaptation mechanism are necessary to derive reasonable predictions of the opponents future actions. Moreover, the complexity of the algorithm grows exponentially with the number of agents. Examples of joint
action learners are minimax-Q (Littman, 1994), fictitious play and AWESOME (Brown,
1951; Conitzer & Sandholm, 2007), hyper-Q (Tesauro, 2003), and Nash-Q (Hu & Wellman,
2003). Worth mentioning here as well is the related stream of work on Bayesian reinforcement learning (Dearden, Friedman, & Russell, 1998; Strens, 2000). Of particular interest
to the discussion of multi-agent learning is the work of Chalkiadakis and Boutilier (2003),
who use the Bayesian framework to explicitly model an agents uncertainty about both the
model of the environment and the strategies of the other agents.
2.2.3 Gradient Ascent Optimisation
A somewhat separate stream of multi-agent learning research revolves around gradient ascent based algorithms. These methods often fall in between independent learning and
joint-action learning, but are worth mentioning separately as they are important for our
discussion in Section 4.1. Gradient ascent (or descent) is a well-known optimisation technique in the field of machine learning. Given a well-defined differentiable objective function,
the learning process can follow the direction of its gradient in order to find a local optimum.
This concept can be adapted for multi-agent learning by having the learning agents policies
follow the gradient of their individual expected reward.
665

fiBloembergen, Tuyls, Hennes, & Kaisers

Examples of gradient ascent algorithms are infinitesimal gradient ascent (IGA), which
is designed specifically for two-player two-action normal-form games (Singh, Kearns, &
Mansour, 2000), and generalized infinitesimal gradient ascent (GIGA), which extends IGA
to games with an arbitrary number of actions (Zinkevich, 2003). Both algorithms can be
combined with the win or learn fast (WoLF) heuristic in order to improve convergence in
stochastic games (Bowling & Veloso, 2002; Bowling, 2005). Both IGA and GIGA assume
that the agents have knowledge of the (reward) structure of the game, or at least have some
mechanism for approximating the gradient of the value function, which is not generally
feasible in practice. However, the more recent algorithm weighted policy learning (WPL)
relaxes this assumption (Abdallah & Lesser, 2008).
2.3 Game Theory
Game theory (Von Neumann & Morgenstern, 1944; Gibbons, 1992) is a theory of interactive
strategic decision making, and is therefore of utmost importance for multi-agent systems.
It studies this decision making in the form of cooperative and competitive games. In these
games, each player has a set of actions and a preference over the joint action outcome, which
is captured by a numerical payoff signal. For games between two players that are played
only once, i.e., one-shot two-player games, the payoffs can be represented by a bi-matrix
(A, B), which gives the payoff for the row player in A, and the column player in B (see
Figure 2). In this example, the row player chooses one of the two rows, the column player
chooses on of the columns, and the outcome of their joint action determines the payoff to
both. The goal for each player is to come up with a strategy (a probability distribution over
his actions) that maximises their expected payoff in the game. Note that games, players,
strategies, and payoffs of game theory map one-to-one to environments, agents, policies,
and rewards in the multi-agent systems literature.
The players are thought of as individually rational, in the sense that each player is
perfectly logical and tries to maximise his own payoff, assuming the others are doing likewise.
Under this assumption, the Nash equilibrium (NE) solution concept can be used to study
what players will reasonably choose to do. A set of strategies forms a NE if no single player
can do better by unilaterally switching to a different strategy. In other words, each strategy
in a NE is a best response against all other strategies in that equilibrium.
A game can have more than one Nash equilibrium, some of which may not be preferred
equally. Moreover, the NE may not be the best outcome from a social point of view. As an
example, consider the Prisoners Dilemma (Axelrod & Hamilton, 1981), depicted in Figure 3
(left). In this game, two players simultaneously choose either to cooperate (C) or defect
(D). Individually, defection is a best response against any opponent strategy, and as a result
mutual defection is the single Nash equilibrium of the game. However, both players would
be better off if both would cooperate  hence the dilemma.



a11 , b11 a12 , b12
a21 , b21 a22 , b22



Figure 2: General payoff bi-matrix (A, B) for a two-player two-action normal form game.
666

fiEvolutionary Dynamics of Multi-Agent Learning

C
D

D 
 C
3, 3 0, 5
5, 0 1, 1

S
H

H 
 S
4, 4 1, 3
3, 1 3, 3

H
T

T 
 H
1, 0 0, 1
0, 1 1, 0

Figure 3: Payoff matrix for the Prisoners Dilemma (left), the Stag Hunt (center), and
Matching Pennies (right).
A second example is given by the Stag Hunt (Skyrms, 2004), shown in Figure 3 (center).
In this coordination game, both players prefer to jointly choose either to hunt for stag (S) or
hare (H). Hunting for hare provides a safe choice, as the payoff for this action is independent
of the choice of the opponent. Hunting stag is more risky, however both players yield a higher
payoff if they manage to coordinate. This game has two pure Nash equilibria, (S, S) and
(H, H), and one mixed Nash equilibrium where both players randomise and play S with
probability 32 .
Finally, in the Matching Pennies game (Figure 3, right) two players simultaneously
choose which side of their coin to display, either heads (H) or tails (T). If both choose the
same side, the first player gets to keep both coins. If they pick opposite sides, the second
player keeps the coins. In this zero-sum game, the single mixed NE is for both players to
randomise uniformly over their actions.
2.4 Evolutionary Game Theory
Classical game theory assumes that full knowledge of the game is available to all players,
which together with the assumption of individual rationality does not necessarily reflect
the dynamic nature of real world interactions. Evolutionary game theory relaxes the rationality assumption and replaces it by biological concepts such as natural selection and
mutation (Maynard Smith & Price, 1973; Weibull, 1997; Hofbauer & Sigmund, 1998; Gintis, 2009). Central to evolutionary game theory are the replicator dynamics that describe
how a population of individuals evolves over time under evolutionary pressure. Each individual is of a certain type, and individuals are randomly paired in interaction. Their
reproductive success is determined by their fitness, which results from these interactions.
The replicator dynamics dictate that the population share of a certain type will increase
if the individuals of this type have a higher fitness than the population average; otherwise
their population share will decrease. The population
can be described by the state vector
P
x = (x1 , x2 , . . . , xn )>, with 0  xi  1 i and i xi = 1, representing the fractions of the
population belonging to each of n types. Now suppose the fitness of type i is given
Pby the fit
ness function fi (x), and the average fitness of the population is given by f (x) = j xj fj (x).
i
Using xi to denote dx
dt , the population change over time can then be written as


xi = xi fi (x)  f(x) .
(5)
These replicator dynamics describe the change over time of a large population of individuals.
However, the model can be interpreted alternatively as representing the strategy of a single
player, where the population share of each type represents the probability with which the
player selects the corresponding pure action, as summarised in Table 1. The replicator
dynamics now describe the players strategy change over time as he repeatedly plays the
667

fiBloembergen, Tuyls, Hennes, & Kaisers

Table 1: Correspondence of terminology between the domains of reinforcement learning,
game theory, and evolutionary game theory.
Reinforcement Learning
environment
agent
action
policy
reward

Game Theory
game
player
action
strategy
payoff

Evolutionary Game Theory
game
population
type
distribution over types
fitness

game and iteratively updates his policy.
Evolutionary game theory refines the static
Nash equilibrium (NE) concept with the notion of evolutionarily stable strategies (ESS). A
strategy x is an ESS if it is immune to invasion by mutant strategies, given that the mutants
initially occupy only a small fraction of the population. Let f (x, y) be the (expected) fitness
of strategy x against strategy y. Formally then, strategy x is an ESS iff, for any mutant
strategy y, the following hold:
1. f (x, x)  f (y, x), and
2. if f (x, x) = f (y, x), then f (x, y) > f (y, y).
The first condition states that an ESS is also a NE of the original game. The second
condition states that if the invading strategy does as well against the original strategy as
the original strategy does against itself, then the original strategy must do better against
the invader than the invader does against itself. This means that ESS are a refinement of
the NE solution concept. Moreover, every ESS is an asymptotically stable fixed point of
the replicator dynamics (Weibull, 1997).
In a two-player game, each player is described by his own evolving population, and at
every iteration of the game one individual of each players population is drawn to interact.
Therefore, the fitness of each type now depends on the population distribution of the coplayer, i.e., the two populations are co-evolving. If the two players populations are given
by x and y and their fitness functions by the payoff matrices A and B, we can write the
expected fitness of type i of population x as
X
fi (x) =
aij yj = (Ay)i
j

and similarly we can write the average population fitness as
X X
f(x) =
xi
aij yj = x>Ay.
i

j

Following similar reasoning for the population y, we can rewrite Equation 5 for the two
populations as
h
i
xi = xi (Ay)i  x>Ay
h
i
(6)
yi = yi (x>B)i  x>By .
668

fiEvolutionary Dynamics of Multi-Agent Learning

1

1

1

0.75

0.75

0.75

y

1

y

1

0.5

0.25

0
0

y

1

0.5

0.25

0.25

0.5

x1

0.75

1

0
0

0.5

0.25

0.25

0.5

x1

0.75

1

0
0

0.25

0.5

x1

0.75

1

Figure 4: The replicator dynamics, plotted in the unit simplex, for the prisoners dilemma
(left), the stag hunt (center), and matching pennies (right).

To illustrate the dynamics of Equation 6, we analyse the three games presented in Figure 3.
Since a players strategy over two actions is fully defined by the probability of the first action
(as x2 = 1  x1 ), we can plot the strategy space of these games as the two-dimensional unit
simplex over the tuple (x1 , y1 ). Plugging the payoff matrix of each game into the replicator
dynamics of Equation 6, we find the direction and relative speed of change for each point
in the unit simplex. The resulting vector fields for the three games are shown in Figure 4.
Figure 4 shows that the players in the prisoners dilemma are drawn to the (D, D)
equilibrium, which is both a NE and an ESS. In the stag hunt, both pure NE, (S, S) and
(H, H), are also ESS, but the mixed NE is not. It is a fixed point, but not asymptotically
stable. Finally, the matching pennies game has a single mixed NE at ( 21 , 12 ), where both
players randomise uniformly over their actions. However, this again is not an ESS; instead
all trajectories cycle around this fixed point.

3. Relating Reinforcement Learning and Replicator Dynamics
Recent research analysing the dynamics of multi-agent learning builds on seminal work
by Borgers and Sarin (1997), who first proved the formal relation between the replicator
dynamics of evolutionary game theory and reinforcement learning. In this section, we will
first summarise their proof. Next, we present a categorisation of recent work, based on the
nature of the environment and actions available to the agents.
3.1 Replicator Dynamics as the Continuous Time Limit of Cross Learning
Multi-agent learning and evolutionary game theory share a substantial part of their foundation, in that they both deal with the decision making processes of boundedly rational
agents, or players, in uncertain environments. The link between these two fields is not only
an intuitive one, but was made formal with the proof that the continuous time limit of
Cross learning converges to the replicator dynamics (Borgers & Sarin, 1997).
669

fiBloembergen, Tuyls, Hennes, & Kaisers

Cross learning (Cross, 1973) is one of the most basic stateless reinforcement learning
algorithms, which updates policy3  based on the reward r received after taking action j as

r  (i)r if i = j
(i)  (i) +
.
(7)
(i)r
otherwise
A valid policy is ensured by the update rule as long as the rewards are normalised, i.e.,
0  r  1. Cross learning is closely related to finite action-set learning automata (Narendra
& Thathachar, 1974; Thathachar & Sastry, 2002). In particular, it is equivalent to a learning
automaton with a linear reward-inaction (LRI ) update scheme and a learning step size ()
of 1.
We can estimate the expected change in the policy, E [(i)], induced by Equation 7
(Borgers & Sarin, 1997). The probability (i) of action i is affected both if i is selected and
if another action j is selected. Let Ei [r] be the expected reward after taking action i. We
can now write
h
i X
h
i
E [(i)] = (i) Ei [r]  (i)Ei [r] +
(j)  Ej [r](i)
j6=i

h

= (i) Ei [r] 

i
(j)E
[r]
.
j
j

P

(8)

Assuming the learner takes infinitesimally small update steps, the continuous time limit of
Equation 8 can be taken as
t+ (i) = t (i) + t (i)
with lim   0. This yields a continuous time system which can be expressed with the
partial differential equation
h
i
P
(i) = (i) Ei [r]  j (j)Ej [r]
In a two-player normal form game, we can write the policy of an agent simply as a probability
distribution over actions, i.e.   x. As such defined, and given payoff matrices A and B
and policies x and y for the two players, respectively, this yields
h
i
xi = xi (Ay)i  x>Ay
h
i
(9)
yi = yi (x>B)i  x>By
which are exactly the multi-population replicator dynamics of Equation 6.
This link can be made explicit not only theoretically but also empirically, as shown
in Figure 5. Here, we simulate the learning process of two Cross learners when taking
very small policy update steps (by multiplying the update term of Equation 7 by  =
0.001), starting at different initial policies, and overlaying the resulting policy traces on
the replicator dynamics of Figure 4. As can be observed, the learning traces follow the
replicator dynamics precisely. In a similar fashion, dynamical models of different (and more
complex) reinforcement learning algorithms can be derived. These are discussed in the
following sections.
3. The dependency of the policy on state s is dropped for stateless environments, and the dependence on
time is implied but omitted for notational convenience.

670

fiEvolutionary Dynamics of Multi-Agent Learning

1

1

1

0.75

0.75

0.75

y

1

y

1

0.5

0.25

0
0

y

1

0.5

0.25

0.25

0.5

x1

0.75

1

0
0

0.5

0.25

0.25

0.5

x1

0.75

1

0
0

0.25

0.5

x1

0.75

1

Figure 5: Policy traces of Cross learning, plotted in the unit simplex and overlaid on the
replicator dynamics, for the prisoners dilemma (left), the stag hunt (center), and matching
pennies (right).

3.2 Categorisation of Learning Dynamics
We divide the learning algorithms and corresponding dynamics that are presented in this
work into four categories, based on the nature of the environment and the actions available to the agent. We distinguish stateless normal-form games, and games with multiple
states with probabilistic transitions between them, represented by stochastic games (see
also Section 2.2). Moreover, we differentiate between settings where the agent has a finite,
discrete choice of actions, and settings which offer a continuous range of choices. Table 2
lists the four resulting categories, along with references to work that has been done in each
category. We focus solely on work that explicitly relates the dynamical models to learning
in multi-agent systems. A large body of work is available that discusses extensions to the
replicator dynamics from an evolutionary game theoretic viewpoint only, however, these fall
outside the scope of this survey.4
Cross learning, detailed previously, belongs to the first category of stateless games with
discrete actions. Other examples in this category are stateless Q-learning and the related
frequency adjusted (FAQ) and lenient (LFAQ) versions, regret minimisation, and gradient
ascent algorithms. The second category comprises stateless games with a continuous action
space. Typically, function approximators are used in such settings (a recent overview is
provided in Busoniu, Babuska, De Schutter, & Ernst, 2010), however most work in that
category has so far been limited to single-agent learning. Here, we summarise approaches
to model such games using continuous action replicator dynamics. The third category is that
of stochastic (i.e. multi-state) games with discrete actions. Dynamics have been derived
for networks of learning automata, in particular piece-wise and state-coupled replicator
dynamics, and the variation RESQ-learning that incorporates exploration in the learning
process.
The fourth category in Table 2, comprising stochastic games with continuous actionspaces, is strikingly empty. Indeed, this domain is of main interest for future work, as no
attempts have been made so far to derive learning dynamics for this setting. Combining
techniques and approaches from the second and third category could be a fruitful starting
4. See e.g. the textbooks by Weibull (1997) and Hofbauer and Sigmund (1998) for an introduction.

671

fiBloembergen, Tuyls, Hennes, & Kaisers

Table 2: Categorisation of dynamical models of multi-agent learning that are available in
the literature.
Discrete actions

Continuous actions

Q-learning1
Normal form
games

Stochastic
games
1

FAQ-learning2
Regret Minimisation3
Lenient FAQ-learning4
Gradient ascent5
Piecewise replicator dynamics8
State-coupled replicator
dynamics9
RESQ-learning10

Tuyls et al. (2003)

4

Kianercy and Galstyan (2012)
2
3

Continuous action replicator
dynamics6
Q-learning7

Kaisers and Tuyls (2010, 2011)

5

Klos et al. (2010)

6

Panait et al. (2008)

7

Galstyan (2013)

Bloembergen et al. (2011)

8

Vrancx et al. (2008a)

Kaisers et al. (2012)

9

Hennes et al. (2009)

Tuyls and Westra (2009)

10

Hennes et al. (2010)

point for such an endeavour. In the next section we provide an overview of the work listed
in Table 2, following the same categorisation.

4. Overview of Learning Dynamics
With the categorisation presented in Table 2 in hand, we now give an overview of the
dynamics of various multi-agent reinforcement learning algorithms. First, learning dynamics
in normal-form games will be discussed. Next, we present replicator dynamics for continuous
strategy spaces. Finally, multi-state learning dynamics are described.
4.1 Learning Dynamics in Normal-Form Games
Repeated normal-form games are characterised by being stateless games, in which agents
choose from a discrete and finite set of actions at each time step. This greatly simplifies
analytical approaches, while at the same time still allowing to capture interesting strategic
interactions. As a result, normal-form games have been frequently used as a test-bed for
multi-agent learning (Busoniu et al., 2008). Several learning algorithms have been devised
specifically for normal-form games; other multi-state algorithms such as Q-learning can
straightforwardly be applied by removing the state dependency from the learning update
rule. As before, in the remainder we define x   and y   to be the policies of the two
agents in the stateless setting.
4.1.1 Independent Reinforcement Learners
As described in Section 3, Cross learning (CL) was the first algorithm to be linked to the
replicator dynamics of evolutionary game theory (Borgers & Sarin, 1997). In particular,
the infinitesimal time limit of the Cross learning update rule (Equation 7) converges to the
672

fiEvolutionary Dynamics of Multi-Agent Learning

replicator dynamics. The link between a simple policy learner such as Cross learning, and
a dynamical system in the policy space may seem intuitive. However, this link has been
extended to value-based (and more complex policy-based) learners as well. A selectionmutation model of Boltzmann Q-learning has been proposed by Tuyls et al. (2003), assuming
a constant temperature  .5 Tuyls et al. show that the dynamical system can be decomposed
into terms for exploitation (selection following the replicator dynamics) and exploration
(mutation through randomisation based on the Boltzmann mechanism):6
xi =

h
i
i
P
xi h
(Ay)i  x>Ay xi log xi  k xk log xk .
 |
{z
}
{z
}
|
exploitation

(10)

exploration

Another way to view the two terms of Equation 10 is in relation to the thermodynamical
concepts of energy and entropy, where selection is analogous to energy, and mutation to
entropy. The entropy term can be further subdividedPin the entropy of one individual strategy, log xi , and the entropy of the entire population, k xk log xk . In this sense, mutation is
determined by the difference in entropy of an individual strategy compared to the entropy
of the whole population (Tuyls et al., 2003).
The dynamical model of Equation 10 assumes that all actions are updated simultaneously, as is the case for Cross learning. Q-learning, however, only updates the Q-value of the
selected action, causing discrepancies between the predicted dynamics and the actual learning behaviour of the algorithm. The variation frequency-adjusted Q-learning (FAQ) (Kaisers
& Tuyls, 2010) mimics simultaneous action updates by modulating the update rule (Equation 3) inversely proportional to xi , thereby following the dynamical model of Equation 10
precisely. Dropping the state dependency, this yields


1
Q(i)  Q(i) +  r + max Q(j)  Q(i) .
j
xi
Using the replicator dynamics model of Equation 10, two independent proofs of convergence
for FAQ have been derived for two-player two-action normal-form games, showing convergence near Nash equilibria given a decreasing exploration temperature  (Kaisers & Tuyls,
2011; Kianercy & Galstyan, 2012).
Lenient FAQ (LFAQ) (Bloembergen et al., 2011) is a variation aimed at overcoming
convergence to suboptimal equilibria by mis-coordination in the early phase of cooperative
learning processes, when mistakes by one agent may lead to penalties for others, irrespective
of the quality of their actions. Leniency towards such mistakes can be achieved by collecting
 rewards for each action, and updating the Q-value based on the highest of those rewards.
This causes an (optimistic) change in the expected reward for the actions of the learning
agent, incorporating the probability of a potential reward for that action being the highest
of  consecutive tries (Panait et al., 2008). The expected reward for each action Ay in
5. For a model of Boltzmann Q-learning dynamics with varying temperature, see the work of Kaisers, Tuyls,
Parsons, and Thuijsman (2009) and Kaisers (2012).
6. From here on, we will derive the dynamics of one agent only. The dynamics of other agents follow
straightforwardly, similar to Equation 9.

673

fiBloembergen, Tuyls, Hennes, & Kaisers

Equation 10 is replaced by the utility vector u, with
hP
 P
 i
a
y
y

y
X ij j
k:aik aij k
k:aik <aij k
P
ui =
k:aik =aij yk

(11)

j

P
where k:statement implies summing over all indices k for which the statement holds.
Recently, the evolutionary framework has also been extended to the polynomial weights
algorithm, which implements regret minimisation (RM) (Blum & Mansour, 2007; Klos
et al., 2010). The learner calculates the loss (or regret) li of taking action i rather than
the best action in hindsight as li = r  r where r is the actual reward received, and r is
the optimal reward. The learner maintains a set of weights w for all actions, updates these
weights according to the perceived loss, and derives a new policy by normalisation:
wi  wi [1  li ]
wi
xi = P
.
j wj

(12)

This form of regret, computed at each time step, is known as external regret, whereas
internal regret is computed as the loss with respect to a policy that replaces a given action
i with some other action j at each occurrence (Blum & Monsour, 2007). Despite the great
difference in update rule and policy generation compared to Cross learning or Q-learning,
Klos et al. (2010) show that the infinitesimal time limit of regret minimisation can similarly
be linked to a dynamical system with replicator dynamics in the numerator:


xi (Ay)i  x>Ay
xi =
.
(13)
1   [maxk (Ay)k  x>Ay]
The denominator can be interpreted as a learning rate modulation dependent on the best
actions update, corresponding to weighting the action probability update by the expected
loss.
4.1.2 Gradient Ascent Algorithms
Gradient ascent (or descent) is a well known optimisation technique in the field of Machine
Learning. Given a well-defined differentiable objective function, the learning process can
follow the direction of its gradient in order to find a local optimum. This concept can be
adapted for multi-agent learning by having the learning agents policies follow the gradient
of their individual expected payoff. This approach assumes that the expected payoff function
is known to (or can be accurately learned by) the agents, which may not generally be feasible
in practice, since in multi-agent settings the payoff function usually depends on (possibly
frequently changing) unobservable internal states of other agents.
One algorithm implementing gradient ascent for multi-agent learning is infinitesimal
gradient ascent (IGA) (Singh et al., 2000), in which each learner updates its policy by
taking infinitesimal steps in the direction of the gradient of its expected payoff. It has been
proven that, in two-player two-action games, IGA either converges to a Nash equilibrium,
or the asymptotic expected payoff of the two players converges to the expected payoff of a
Nash equilibrium (Singh et al., 2000). A discrete time algorithm using a finite decreasing
674

fiEvolutionary Dynamics of Multi-Agent Learning

step size shares these properties. Take V (x) : Rn  R to be the value function that maps
a policy to its expected payoff. The policy update rule for IGA can now be defined as
V (x)
xi
x  projection(x + x)

xi  

(14)

where  denotes the learning step size. The intended change x may take x outside of
the valid policy space, in which case it is projected back to the nearest valid policy by the
projection function.
Win or learn fast (IGA-WoLF) (Bowling & Veloso, 2002) is a variation on IGA which
uses a variable learning rate. The intuition behind this scheme is that an agent should
adapt quickly if it is performing worse than expected, whereas it should be more cautious
when it is winning. The modified learning rule of IGA-WoLF is

V (x) min if V (x) > V (x )
xi 
xi
max otherwise
(15)
x  projection(x + x)
where x is a reference policy, e.g., a policy belonging to an arbitrary Nash equilibrium.
The weighted policy learner (WPL) (Abdallah & Lesser, 2008) is a second variation of
IGA that also modulates the learning rate, but in contrast to IGA-WoLF it does not require
a reference policy. The update rule of WPL is defined as

V (x) xi
if Vx(x)
<0
i
xi  
xi
1  xi otherwise
(16)
x  projection(x + x)
where the update is weighted either by xi or by 1  xi depending on the sign of the gradient.
This means that x is driven away from the boundaries of the policy space.
In the next section, we will derive the dynamical model of these gradient ascent algorithms in two-player two-action games, and show their remarkable similarities to the
dynamics of the reinforcement learners discussed in Section 4.1.1.
4.1.3 Comparative Overview of Learning Dynamics in 2x2 Games
For two-agent two-action games, the dynamical models presented in the previous sections
can be simplified (Kaisers et al., 2012). Let h = (1, 1), x = (x, 1x) and y = (y, 1y). The
learning dynamics are completely described by the pair (x, y), which denote the probability
change of the first action for both learners. For Cross learning (CL) this leads to the
simplified form


x = x (Ay)1  x>Ay


= x(1  x) y (a11  a12  a21 + a22 ) + a12  a22


= x(1  x) yhAh> + a12  a22
where a12 and a22 are elements of the payoff matrix A. To shorten the notation for twoaction games, let
 = (Ay>)1  (Ay>)2 = yhAh> + a12  a22
675

fiBloembergen, Tuyls, Hennes, & Kaisers

denote the gradient, such that the CL dynamics are written as x = x(1  x). Then,
similarly, the simplified FAQ dynamics read



x
x = x(1  x)
.
 log

1x
The dynamics of RM are slightly more complex, as the denominator depends on which
action gives the highest reward. This can be derived from the gradient: the first action will
be maximal iff  > 0. Using this insight, the dynamics of RM in two action games can be
written as follows:

(1 + x)1
if  < 0
x = x(1  x) 
1
(1  (1  x))
otherwise.
For IGA, the update rule can be worked out in a similar fashion. The main term in this
update rule is the gradient of the expected reward, which in two player two-action games
can be written as


V (x)

y
=
(x, 1  x)A
1y
x
x
= y(a11  a12  a21 + a22 ) + a12  a22
= yhAh> + a12  a22
= .
This reduces the dynamics of the update rule for IGA in two-player two-action games to
x = . The extension of the dynamics of IGA to IGA-WoLF and WPL are straightforward.
Table 3 lists the dynamics of the six discussed algorithms: IGA, IGA-WoLF, WPL,
CL, FAQ and RM. It is immediately clear from this table that all algorithms share the
same basic term in their dynamics: the gradient . Depending on the algorithm, the
gradient is scaled with a learning speed modulation. Interestingly, the dynamics of IGA are
completely independent of the learners own current policy x. In other words, IGA is an
Table 3: This table shows an overview of the learning dynamics, rewritten for the specific
case of two-agent two-action games (Kaisers et al., 2012).
Algorithm
IGA
IGA-WoLF
WPL
CL
FAQ
RM

x


min if V (x) > V (x )

max otherwise

x
if  < 0
 
(1  x) otherwise
x(1  x) 


x
x(1  x)   1  log 1x

(1 + x)1
if  < 0
x(1  x)  
1
(1  (1  x))
otherwise
676

fiEvolutionary Dynamics of Multi-Agent Learning

off-policy algorithm that assumes that all actions are sampled equally often. In contrast, in
any multi-agent learning setting in which the (gradient of the) value function is not known,
the learners necessarily need to be on-policy, as otherwise they cannot learn the true value
function. In this light, it can be argued that Cross learning implements stochastic onpolicy gradient ascent (Kaisers et al., 2012). Finally, FAQ yields the only dynamics that
additionally add exploration terms to the process.
This analysis shows the merits of the evolutionary game theoretic approach to the study
of multi-agent learning. By deriving mathematical models of the infinitesimal time limit
of various learning algorithms, we can formally establish their underlying differences and
commonalities.
4.2 Replicator Dynamics in Continuous Action Spaces
The dynamics and algorithms discussed previously assume a discrete, finite action space.
In contrast, many real-world settings feature actions that are rather of a continuous nature.
In such settings the tabular notation for Q-functions and policies is no longer feasible and
function approximators need to be used. One approach is to discretise such continuous parameters in ranges, and treat each range as one instance. However, it is not straightforward
in general to design a good discretisation, and details might be lost. Another approach is to
model those continuous actions directly, moving from discrete probability vectors over actions to probability density functions. Examples of Q-learning algorithms for continuous action spaces are fitted Q-iteration (Ernst, Geurts, & Wehenkel, 2005) and NEAT+Q-learning
(Whiteson & Stone, 2006). Learning automata can similarly be extended to continuous action spaces. For instance, Santharam, Sastry, and Thathachar (1994) propose continuous
action-set learning automata (CALA) which use a Gaussian distribution to model the policy.
It is also possible to use a nonparametric distribution, as is employed in continuous action
reinforcement learning automata (CARLA), allowing a more diverse exploration strategy
and convergence to a potentially multi-modal distribution (Howell, Frost, Gordon, & Wu,
1997; Rodrguez, Vrancx, Grau, & Nowe, 2012). An in-depth discussion of these learning
methods falls outside the scope of this paper. Instead, we focus here on modelling such
algorithms by extending the replicator dynamics to continuous action spaces.7
Suppose each agents action space can be described by D continuous parameters x =
(x1 , x2 , . . . , xD ), with x    RD , where  is the allowed action space. In a two-player
setting, the reward of playing action x against an opponent who plays y is given by f (x, y).
An agents policy at time t is now given by a probability density function over the action
space, (x, t), where
Z
(x, t)dx = 1.


We can now write the continuum limit of the standard replicator dynamics (Equation 5) as
a partial differential equation (Oechssler & Riedel, 2001; Cressman, 2005; Tuyls & Westra,
2009):
h
i
(x, t)
= (x, t) V (x, t)  E(t)
(17)
t
7. A recent extensive overview of function approximation methods for reinforcement learning is provided
by Busoniu et al. (2010).

677

fiBloembergen, Tuyls, Hennes, & Kaisers

where

Z
f (x, y)(y, t)dy

V (x, t) =
Z
E(t) =

V (x, t)(x, t)dx.


Here, V (x) depicts the expected reward of action x, and E the overall average reward. In
the context of statistical mechanics these terms can be thought of as local potential and
total energy, respectively.
Similar to the discrete action dynamics discussed before, we can add mutation terms
to Equation 17 to model exploration by the learning agents. The most straightforward
approach to do this is by including a diffusion term to the replicator dynamics, which
allows the mutation of strategies to slightly different ones nearby. This procedure was
originally proposed by Hofbauer and Sigmund (1998), for which Ruijgrok and Ruijgrok
(2005) have provided a rigorous mathematical foundation and subsequent analysis. Ruijgrok
and Ruijgrok add such a diffusion term to the continuous action replicator dynamics and
find that even a small mutation rate may greatly alter the outcome of the learning process,
leading to more favourable results in, e.g., the ultimatum game. Specifically, they add
a transition probability per unit time qij for the spontaneous transfer to strategy i from
strategy j. In this way the discrete replicator equations with mutation can be written as
follows:
X
xi = xi (Vi  E) +
(qij xj  qji xi ).
j

Ruijgrok and Ruijgrok follow the method of Van Kampen (1992) to derive the continuum
limit of the mutation term. Using this approach, they arrive at the same continuous replicator equation as Hofbauer and Sigmund:
h
i
(x, t)
= (x, t) V (x, t)  E(t) + (x, t)2 (x, t).
(18)
t
Ruijgrok and Ruijgrok study this equation for a number of games that have been transformed to a continuous strategy setting. There is, however, no reason to assume that mutations are only restricted to adjacent strategies. Therefore, building on this work, Tuyls
and Westra (2009) compare three different diffusion-based mutation terms, and find that
the type of mutation can also significantly influence the resulting dynamics. Specifically,
they replace the simple isotropous diffusion term containing the mutation rate of Ruijgrok
and Ruijgrok by mimicking the effect of anisotropous deterministic mutations. They start
from the continuous time discrete action replicator equations with mutation, and from there
derive a different formulation for the continuous action space:
h
i
h
i
(x, t)
= (x, t) V (x, t)  E(t) + T  (x) V (x)(x, t) .
t
This new formulation is able to capture certain aspects of an evolution driven by game
theoretic interactions that are absent in the original formulation of Hofbauer and Sigmund,
and Ruijgrok and Ruijgrok in Equation 18. The dynamics of this approach are even more
complex as different mutation rates provide entirely different stationary solutions. Full
details can be found in the work of Tuyls and Westra.
678

fiEvolutionary Dynamics of Multi-Agent Learning

Although the dynamical models of Equations 17 and 18 have not yet been directly linked
to a specific learning algorithm, Galstyan (2013) recently proposed a dynamical model for a
continuous action version of Boltzmann Q-learning. Galstyans model has a similar selection
term based on local potential V (x) and total energy E, but a mutation term that is again
an entropy term derived from the Boltzmann distribution, as in Equation 10. Galstyan
finds that mutation drives the learning process away from pure Nash equilibria but helps
convergence to uniformly mixed equilibria, similar to discrete action Q-learning (see Kaisers
& Tuyls, 2011; Kianercy & Galstyan, 2012).
4.3 State-Coupled Replicator Dynamics
So far, our discussion of learning dynamics has been limited to stateless games. Although
many real-world interactions can indeed be cast in the form of repeated normal-form games,
this is not always the case. Therefore, there is a need to understand learning dynamics in
statefull environments as well. Vrancx et al. (2008a) propose a combination of replicator dynamics and switching dynamics to model learning automata in stochastic (Markov) games.
As learning automata are by definition stateless (see Section 3.1), an extension is needed for
multi-state games. One option is for an agent to maintain a network of learning automata
(Wheeler Jr & Narendra, 1986; Vrancx et al., 2008b), one for each state. As the game
progresses, control is passed from one automaton to the other depending on the current
state of the game. Instead of performing policy updates for the active automaton based on
the immediate reward rt , the update is delayed until the automaton becomes active again,
at which time it is updated based on the average reward received during that period. Based
on this mechanism, Vrancx et al. (2008a) partition the state-space in cells, corresponding to
different attractors in these average reward games. Each cell has a fixed replicator dynamic,
based on which the agents policies are updated. When this update makes the system leave
the current cell, a new replicator dynamic takes over. Hennes, Tuyls, and Rauterberg (2008)
further formalise these piece-wise replicator dynamics.
The piece-wise model suffers from several shortcomings, however, as demonstrated by
Hennes et al. (2009). Firstly, the model only approximates the learning behaviour by assuming fixed dynamics in each cell, and secondly, the discrete switching between cells causes
discontinuities that are not present in real traces of the learning process. In order to alleviate these shortcomings, Hennes et al. propose the state-coupled replicator dynamics
(SC-RD) model which uses direct state coupling rather than piece-wise switching dynamics. The direct state coupling eliminates anomalies and discontinuities caused by linear
approximations and discrete cell switching. The SC-RD
 are defined as follows. Let  be
the set of policies of n agents, i.e.,  =  1 ,  2 , . . . ,  n . Moreover, let a = a1 , a2 , . . . , an
be their joint action. Assuming the game has no absorbing states (i.e., the set of states S
is ergodic), there exists a stationary
 over all states S under , where 
s
P distribution
is the frequency of state s and sS 
s = 1. We can then calculate the limiting average
reward r of playing a specific joint-action a in state s, given fixed policies  in all other
states s0 , as
i
ri (s, a) = 
s r (s, a) +

X
s0 S{s}

679

i
0

s0 f s



(19)

fiBloembergen, Tuyls, Hennes, & Kaisers

where f i (s0 ) is the expected reward (fitness) of agent i in state s0 , calculated as
X

i

f (s) =
a

i

r (s, a)

Qn

Aj

j=1

n
Y



k



k

s, a



!
.

k=1

We can set up a system of differential equations for each agent i and action j, similar to
Equation 9, where the payoff matrix A is substituted by the limiting average reward r.
Furthermore, instead of the single
opponent policy  we now have all other agents policies

 i =  1 . . .  i1 ,  i+1 . . .  n . The expected payoff for player i playing pure action j in
state s is given by



X
Y 

ri s, a0
fji (s) =
 k s, a0k 
a0 

Q

l6=i

Al

k6=i


where a0 = a1 . . . ai1 , j, ai . . . an . Essentially, we enumerate all possible joint actions a
with fixed action j for agent i. In general, for some mixed policy , agent i receives an
expected payoff of


Y 

ri s, a0
 k s, a0k 


f i (s, ) =

X
ai Ai


X

(s, ai )
a0 

Q

l6=i

Al

k6=i

where r is the limiting average reward given in Equation 19. Writing xi (s)   i (s) to
be the probability distribution over actions of agent j in state s, we can now define the
multi-population state-coupled replicator dynamics as the following system of differential
equations:


xij (s) = xij (s)xs f i (s, ej )  f i s, xi (s)
(20)
where ej is the j th -unit vector,
to the policy that plays pure action j. In
P corresponding
P
total this system has N = sS ni=1 |Ai | replicator equations. Hennes et al. show that
the SC-RD model describes the true learning dynamics of networks of learning automata
far more precise than the piece-wise replicator dynamics.
Recently, the replicator dynamics have been extended to the sequence form representation of extensive-form games as well (Gatti, Panozzo, & Restelli, 2013; Lanctot, 2014). This
allows us to model even more complex games, e.g. with sequential moves and imperfect information. Panozzo et al. (2014) have developed a new version of Q-learning that works on
the sequence form, along with a dynamical model that matches the learning process based
on the sequence-form replicator dynamics with a mutation term similar to Equation 10.
They show that, although the selection mechanism of the sequence-form and normal-form
replicator dynamics are realization equivalent (Gatti et al., 2013; Lanctot, 2014), the mutation term is not. An in-depth discussion of these findings falls outside the scope of this
article, as it would require the addition of a much broader background on extensive-form
games. However, these recent works show the promise of the evolutionary framework for
multi-agent learning beyond normal-form or even stochastic games as well.
680

fiEvolutionary Dynamics of Multi-Agent Learning

5. Experimental Overview
We established the link between multi-agent reinforcement and the replicator dynamics of
evolutionary game theory in Section 3, and provided an overview of learning dynamics in
normal-form games, continuous strategy spaces, and stochastic (Markov) games in Section 4.
Here, we show a set of experiments that empirically validate these models in two-player twoaction normal-form games. We restrict ourselves to these games as their simplicity allows
easy visual analysis, while preserving the explanatory power of such dynamical models. At
the end of this section we provide an overview of related empirical work in more complex
interactions.
As described earlier, in two-player two-action games the policy space can be compactly
represented by the unit simplex as it is completely defined by the probability with which
both agents select their first action. This makes it easy to plot and visually inspect the
learning dynamics in such interactions. In Section 3.1, Figure 5, an example of such analysis
can be found for Cross learning, as compared to the standard replicator dynamics. Similar
analysis can be performed for different learning algorithms, by plotting policy traces of their
learning behaviour overlaid on the vector field of their corresponding dynamical model (see
e.g. Tuyls et al., 2006; Klos et al., 2010; Kaisers & Tuyls, 2010, 2011; Bloembergen et al.,
2011). Figure 6 shows this for standard frequency-adjusted Q-learning (FAQ), and its
lenient counterpart (LFAQ), in the top two rows. Whereas the dynamics of these different
algorithms are similar in their convergence behaviour when only one equilibrium is present,
as is the case in the prisoners dilemma and matching pennies, in the stag hunt differences
can be observed. The notion of leniency, introduced to overcome convergence to suboptimal
equilibria, works to drive the learning process towards the optimal outcome of the game (S, S
 top right corner), as can be easily observed by investigating the vector field. Interestingly,
while Cross learning and the standard replicator dynamics do not converge in the matching
pennies game, (L)FAQ does sprial inwards towards the single Nash equilibrium at ( 12 , 21 ),
which is not evolutionarily stable in the classical replicator dynamics model (Section 2.3).
The additional exploration term makes a difference here.
So far, we have analysed the dynamics of two identical learners pitted against each
other. However, the replicator dynamics model allows for heterogeneous systems as well,
in which different agents follow different learning rules. In such cases, the policy change
of each individual agent is modelled by a different variation of the replicator dynamics,
corresponding to that agents learning rule. The bottom row of Figure 6 shows this for the
situation where FAQ and LFAQ are pitted against each other. In games where the selfplay dynamics of both learners are similar, such as the prisoners dilemma and matching
pennies, the mixed dynamics do not change significantly. In other cases, such as the stag
hunt, the learning process is clearly influenced as different dynamics mix. As LFAQ has
a stronger tendency to play the optimal action S, FAQ is persuaded to do likewise. Such
analysis makes it possible to compare the behaviour of learning algorithms in heterogeneous
environments, or to compare different parameter settings of a single algorithm.
As a final example, we revisit the comparison of gradient ascent-based and reinforcement
learning algorithms of Section 4.1.3. Figure 7 shows the learning dynamics as predicted by
the models derived there and presented in Table 3, for the matching pennies game. Regret
minimisation (RM) is omitted as its dynamics are visually indistinguishable from Cross
681

fiBloembergen, Tuyls, Hennes, & Kaisers

FAQ-learning
1

1

1

0.75

0.75

0.75

y1

y1

0.5

0.25

y1

0.5

0.25

0
0

0.25

0.5
x1

0.75

0.25

0
0

1

0.5

0.25

0.5
x1

0.75

0
0

1

0.25

0.5
x1

0.75

1

0.25

0.5
x1

0.75

1

0.75

1

Lenient FAQ-learning
1

1

1

0.75

0.75

0.75

y1

y1

0.5

0.25

y1

0.5

0.25

0
0

0.25

0.5
x1

0.75

0.25

0
0

1

0.5

0.25

0.5
x1

0.75

0
0

1

1

1

0.75

0.75

0.75

0.5

0.25

0
0

LFAQ: y1

1

LFAQ: y1

LFAQ: y1

FAQ vs LFAQ

0.5

0.25

0.25

0.5
FAQ: x1

0.75

Prisoners Dilemma

1

0
0

0.5

0.25

0.25

0.5
FAQ: x1

Stag Hunt

0.75

1

0
0

0.25

0.5
FAQ: x1

Matching Pennies

Figure 6: Policy traces of FAQ and LFAQ, plotted in the unit simplex and overlaid on
their respective dynamical model, for the prisoners dilemma (left), the stag hunt (center),
and matching pennies (right) (Bloembergen et al., 2011).

682

fiEvolutionary Dynamics of Multi-Agent Learning

1

y

1

y

1

0.5

0
0

y

1

0.5

0.5
x

0
0

1

1

0.5

0.5
x

0
0

1

y

0.5

1

y

1

1

CL

1

0
0

IGA
WoLF
WPL
CL
FAQ

1

0.5

0.5

0.5
x

1

WPL

1

1

0.5
x
1

IGA-WoLF

1

0
0

1

1

IGA

y

1

0.5
x
1

FAQ

1

0
0

0.5
x

1

1

Traces

Figure 7: Overview of the dynamics of various gradient ascent and reinforcement learning algorithms in matching pennies. The bottom right panel shows a single trace of the
dynamical models, using the same initial policy (indicated with ) (Kaisers et al., 2012).

learning (CL). We can clearly observe the similarity between CL and infinitesimal gradient
ascent (IGA), which both cycle around the central equilibrium point without converging.
Win-or-learn-fast IGA (WoLF) and the weighted policy learner (WPL) both converge due
to their learning rate modulator; in the case of WoLF, two learning rates are used (for
winning and loosing), whereas WPL uses a continuum of learning rates, resulting in nonlinear dynamics. This difference can be clearly observed by comparing the vector fields of
their respective dynamical models. As noted before, FAQ spirals inwards towards the Nash
equilibrium  although this is hard to observe from the vector field alone, this fact can be
verified by following a trace of the dynamics, as shown in the bottom right panel of Figure 7
for the different algorithms. These traces highlight the (subtle) similarities and differences
between the diverse algorithms.
Similar analyses have been performed to investigate learning dynamics in more complex
(e.g., multi-state) games, or to highlight the influence of certain parameters on the learning
process. For instance, Tuyls et al. (2003) were the first to derive the selection-mutation
dynamics of Q-learning in discrete-action normal-form games, and to visually compare traces
of the learning algorithm with the predicted dynamics of the model. Similar derivations and
analyses have later been provided for other algorithms such as learning automata (Tuyls
683

fiBloembergen, Tuyls, Hennes, & Kaisers

et al., 2006) and regret minimisation (Klos et al., 2010). Others have focused on learning
dynamics in multi-state (stochastic) games, in particular deriving dynamics of networks of
learning automata (Vrancx et al., 2008a; Hennes et al., 2009). Again others have derived new
learning algorithms based on a desirable dynamical model, leading to FAQ-learning (Kaisers
& Tuyls, 2010) and lenient FAQ (Bloembergen et al., 2011) for normal-form games, and
RESQ-learning for stochastic games (Hennes et al., 2010). Finally, several authors have
used insights stemming from the dynamical models to compare different algorithms, or
to investigate their convergence. For example, Bowling and Veloso (2002) introduce the
variable learning rate WoLF heuristic and prove that it can be used to make IGA convergent
in normal-form games. Abdallah and Lesser (2008) introduce WPL and compare it to IGA
and IGA-WoLF, and show that WPL converges in normal-form games as well, without
requiring as much information as IGA-WoLF. Kaisers and Tuyls (2011) use the dynamical
model of FAQ to demonstrate its near-NE convergence in normal-form games, and Galstyan
(2013) similarly shows this for continuous-action Q-learning (see also Section 6.1.2).
Table 4 presents an overview of these related works, clustered by interaction type  i.e.,
normal form games with discrete or continuous actions spaces, and stochastic games  and
lists the relevant learning algorithms that are investigated. We purposely list only those
works that explicitly focus on the relation between multi-agent learning algorithms and the
evolutionary dynamical model. Each of these works utilises and extends this connection
in order to gain qualitative insights into the behaviour of such algorithms in a variety of
settings.

Table 4: An overview of related empirical evaluations of learning dynamics. NFG: normalform games; CNFG: continuous action normal-form games; SG: stochastic (Markov) games.
Type

Algorithm

Reference

NFG

Q-learning

Tuyls et al. (2003, 2006)

NFG

regret minimisation

Klos et al. (2010)

NFG

FAQ

Kaisers and Tuyls (2010, 2011)

NFG

lenient FAQ

Bloembergen et al. (2011)
Kaisers (2012)

NFG

WoLF

Bowling and Veloso (2002)

NFG

IGA, IGA-WoLF, WPL

Abdallah and Lesser (2008)

CNFG

Q-learning

Galstyan (2013)

SG

networks of learning
automata

Vrancx et al. (2008a)
Hennes et al. (2009)

SG

RESQ-learning

Hennes et al. (2010)

684

fiEvolutionary Dynamics of Multi-Agent Learning

6. Applications
In the previous sections we highlighted the descriptive power of the replicator dynamics
model of multi-agent learning. Here, we focus on its prescriptive power as well. For example, we can use the dynamical models for easy parameter tuning of learning algorithms.
Moreover, starting from desired dynamics, it is possible to reverse-engineer a learning algorithm that exhibits the preferred behaviour. Finally, the evolutionary models can be
used to analyse complex strategic interactions, such as automated trading. Focusing on
meta-strategies rather than primitives reduces the complexity of such interactions enough
to study their dynamics analytically.
6.1 Parameter Tuning
Parameter tuning is traditionally a cumbersome task involving many simulation trials, often
following some evolutionary optimisation approach. However, with a deterministic dynamical model the effect of various parameters on the learning process is readily observable. In
the following, we provide examples for lenient learning, and for balancing exploration and
exploitation.
6.1.1 Degree of Leniency
Lenient learning was introduced as a way to overcome the problem of suboptimal convergence in cooperative multi-agent settings, where initial mis-coordination leads to an
undervaluation of the optimal action (Panait et al., 2008). This problem is also known as
relative overgeneralisation (Wiegand, 2003) or action shadowing (Fulda & Ventura, 2007).
By focusing the learning update on maximal rewards rather than average, the learner effectively ignores the low rewards that are due to suboptimal behaviour by others in the early
phases of the learning process. This can be achieved by collecting  rewards for each action
before performing an update based on the highest of those rewards (Panait et al., 2008;
Bloembergen et al., 2011). The details and dynamical model of lenient frequency-adjusted
Q-learning (LFAQ) are given in Section 4.1.1, Equation 11.
The main parameter of LFAQ is the degree of leniency, . One of the main advantages
of having a dynamical model is that it allows studying and tuning such a parameter without
the need for extensive simulations. Instead, we can directly analyse the dynamical model.
Figure 8 shows the dynamics of LFAQ, following Equation 11, in the stag hunt, for  
{1, 2, 5, 25}. The stag hunt has two pure Nash equilibria that are also evolutionarily stable
strategies, where both agents either play H, at (0, 0), or S, at (1, 1) (see Section 2.3). The
dilemma in this game is the choice between the safe action H, which always gives the same
reward independent of the other agents action, or S, which is optimal, but only if both
players coordinate. This is precisely the problem that leniency aims to solve. Figure 8
shows that as the degree of leniency increases, so does the basin of attraction of the optimal
outcome (1, 1). Depending on the nature of the game and the opponent, a balance needs
to be found between leniency on the one hand, and the risk of being exploited, or lagging
behind a changing environment, on the other. The dynamical model greatly facilitates
gaining quick insight into these effects.
685

fi1

1

0.75

0.75

0.75

0.75

0.5

0.25

0
0

0.5

0.25

0.25

0.5
0.75
LFAQ: x1

=1

1

0
0

LFAQ: y1

1

LFAQ: y1

1

LFAQ: y1

LFAQ: y1

Bloembergen, Tuyls, Hennes, & Kaisers

0.5

0.25

0.25

0.5
0.75
LFAQ: x1

1

=2

0
0

0.5

0.25

0.25

0.5
0.75
LFAQ: x1

=5

1

0
0

0.25

0.5
0.75
LFAQ: x1

1

 = 25

Figure 8: The effect of the degree of leniency  on convergence in the stag hunt game. The
solid line indicates the boundary between the basins of attraction for the two equilibria; the
global optimum is located at (1, 1) (Bloembergen et al., 2011).

6.1.2 Tuning the Exploration Rate in FAQ-Learning
Balancing exploration and exploitation is of vital importance to any learning task, in particular in dynamic environments where multiple learning agents interact. In (FA)Q-learning
with Boltzmann exploration (Equation 4), the temperature parameter  controls the level of
exploration  a high temperature promotes exploration, whereas a low temperature favours
exploitation. The dynamical model of FAQ (Equation 10) allows to study the effect of the
exploration rate on the behaviour and convergence of the learner in a multi-agent setting,
as demonstrated by Kaisers and Tuyls (2011).
Figure 9 shows the effect of  on the dynamics and convergence of FAQ in battle of the
sexes. In this game, players need to coordinate on either one of their two actions, however
both have a different preference over those joint outcomes:
S 
 B
B
2, 1 0, 0
S
0, 0 1, 2
The first three frames of the figure show the dynamics and computed fixed points for
different temperatures (first frame  = , second frame  = 0.72877, third frame  = 0).
The fixed points move between these discrete values for  as indicated by the paths shown in
the fourth frame. The game has three Nash equilibria, at (0, 0), (1, 1), and ( 32 , 13 ). However,
for high values of  the replicator model yields only one attracting fixed point, that moves
from ( 12 , 21 ) towards the mixed equilibrium ( 23 , 13 ). Kaisers and Tuyls show that this fixed
point splits in a supercritical pitchfork bifurcation at the critical temperature crit  0.72877
and at position (x1 , y1 )  (0.5841, 0.4158). For low temperatures  < crit , the dynamics
yield three fixed points that move closer to the corresponding equilibria as  is decreased.
The two fixed points moving toward the pure equilibria (0, 0) and (1, 1) are attracting, and
the third one moving toward ( 32 , 13 ) is repelling.
A similar analysis for Boltzmann Q-learning has been performed by Kianercy and Galstyan (2012). They also observe that the fixed points of the dynamical model move towards
Nash equilibria as   0 and relate the temperature of the Boltzmann mechanism to the
thermodynamical concept of free energy (Galstyan, 2013). Gomes and Kowalczyk (2009)
propose a dynamical model of -greedy Q-learning, and show that this model accurately
686

fiEvolutionary Dynamics of Multi-Agent Learning

1

1

1

1

0.8

0.8

0.8

0.8

0.6

0.6

y1

0.6

0.6

y1

y1

y1

0.4

0.4

0.4

0.4

0.2

0.2

0.2

0.2

0
0

0.2

0.4

x1

0.6

0.8

1

0
0

0.2

0.4

x1

0.6

0.8

0
0

1

0.2

0.4

x1

0.6

0.8

0
0

1

0.2

0.4

x1

0.6

0.8

1

Figure 9: Replicator dynamics (arrows) and fixed points () for  = {, 0.72877, 0}
(first three frames). Right-most frame shows trajectories of fixed points as temperature is
decreased (Kaisers & Tuyls, 2011).

predicts the empirical findings. Similarly, Wunder, Littman, and Babes (2010) present a
detailed study of -greedy Q-learning in various classes of normal-form games and provide
proofs of (non-) convergence for each class, varying from rapid convergence to stable oscillations. Each of these works shows the great applicability and benefit of the replicator
dynamics model of multi-agent learning, when investigating the effect of various parameters
on the learning process.
6.2 Design of New Learning Algorithms
So far, we have taken a forward approach: starting from a learning algorithm we derived a
dynamical model that accurately predicts the behaviour of that algorithm in the limit. We
can, however, also take an inverse approach by starting from a set of desired dynamics and
reverse-engineering a learning algorithm that exhibits those dynamical properties (Hennes
et al., 2010; Tuyls, Heytens, Nowe, & Manderick, 2003).
As an example, consider again the state-coupled replicator dynamics (SC-RD) introduced in Section 4.3. These dynamics describe the behaviour of networks of learning automata, which are essentially exploiting, and exploration is solely induced by the stochastic
action-selection process. However, results from the domain of stateless games suggest that
exploration aids convergence to mixed equilibria, where purely exploitative learners enter
cycles (see Section 5, in particular Figure 7). Hennes et al. (2010) extend the SC-RD model
(Equation 20) with the exploration term of FAQ-learning (Equation 10), which leads to

xij (s)

=

xij (s)xs

"


f i (s, ej )  f i s, xi (s)




log xij +

X



xik log xik

#
.

k

These dynamics can be translated to a learning algorithm by adding a similar exploration
term to the policy update of a network of learning automata. The reward remains equal to
the average accumulated reward since the last visit to that particular automata, while the
687

fiBloembergen, Tuyls, Hennes, & Kaisers

policy update after taking action j is now computed as
(
P
r  (i)r   (log (i) + k (k) log (k)) if i = j
P
(i)  (i) + 
(i)r   (log (i) + k (k) log (k))
otherwise.
Hennes et al. show that RESQ-learning is able to converge in a two-state version of matching
pennies, where the standard SC-RD cycle around the equilibrium.
Another example is given in the work of Tuyls et al. (2003), where the standard replicator
dynamics are extended to ensure stable convergence to Nash equilibria in all classes of twoagent two-action normal-form games. Based on these extended replicator dynamics Tuyls
et al. derive an extended Cross learning algorithm that adheres to the preferred dynamics.
6.3 Evolutionary Analysis of Complex Strategic Interactions
In addition to relatively simple, stylised games, we also can analyse much more complex
systems. This is accomplished by taking a high-level view and focusing on meta-strategies,
rather than atomic actions, in such scenarios. For example, this allows us to study the
evolutionary dynamics of various trading strategies in stock markets (Kaisers et al., 2009;
Hennes, Bloembergen, Kaisers, Tuyls, & Parsons, 2012; Bloembergen, Hennes, McBurney,
& Tuyls, 2015). Similarly, it is possible to compare auction mechanisms (Phelps, Parsons, &
McBurney, 2005), strategies in the game of poker (Ponsen, Tuyls, Kaisers, & Ramon, 2009),
or even collision avoidance methods in multi-robot systems (Hennes, Claes, & Tuyls, 2013).
Moreover, the link between the replicator dynamics and reinforcement learning allows us to
predict what will happen when agents learn to optimise their strategy in such scenarios.
In the following, we present two examples of such analyses. Firstly, we detail heuristic
payoff tables as a method to estimate the payoff of high-level meta-strategies empirically.
Next, we present how to use this analysis to evaluate trading strategies in stock markets
(Section 6.3.2), and collision avoidance strategies in multi-robot systems (Section 6.3.3).
6.3.1 Heuristic Payoff Tables
In order to analyse the evolutionary strength of high-level meta-strategies, we need to
estimate the expected payoff of such strategies relative to each other. In evolutionary
game theoretic terms, this is the relative fitness of the various strategies, dependent on the
current frequencies of those strategies in the population. The evolutionary model assumes
an infinite population. We cannot compute the payoff for such a population directly, but
we can approximate it from evaluations of a finite population. All possible distributions
over k strategies can be enumerated for a finite population of n individuals. Let N be
a matrix,
 where each row Ni contains one discrete distribution. The matrix will yield
n+k1
rows. Each distribution over strategies can be simulated (using an appropriate
n
model of the environment), returning a vector of expected relative rewards u(Ni ). Let U be
a matrix which captures the rewards corresponding to the rows in N , i.e., Ui = u(Ni ). A
heuristic payoff table H = (N, U ) is proposed by Walsh, Das, Tesauro, and Kephart (2002)
to capture the payoff information for all possible discrete distributions in a finite population.
An example of such a heuristic payoff table is given in Table 5. In this example, we have
k = 3 different meta-strategies, distributed over a population of n = 6 individuals. Each
row in N specifies exactly how many individuals use each of the three strategy types, and
688

fiEvolutionary Dynamics of Multi-Agent Learning

Table 5: Example of a heuristic payoff table for k = 3 strategies and a finite population
of n = 6 individuals.
Ni1
6
5
3
0

Ni2
0
1

1

0

Ni3
0
0

Ui1
0.5
0.4

2

0.3

6

0

Ui2
0
0.7

0.5

0

Ui3
0
0
0.8
0.9

each row in U specifies their estimated payoff. If a discrete distribution Ni features zero
individuals of type j, their payoff naturally cannot be measured, and we set Uij = 0.
In order to approximate the payoff for an arbitrary mix of strategies in an infinite population distributed over the species according to x, n individuals are drawn randomly from
the infinite distribution. The probability for selecting a specific row Ni can be computed
from x and Ni as

Y
k
n
N
P (Ni |x) =
xj ij .
Ni1 , Ni2 , . . . , Nik
j=1

The expected payoff of strategy i, fi (x), is then computed as the weighted combination of
the payoffs given in all rows:
P
j P (Nj |x)Uji
fi (x) =
.
1  (1  xi )k
This expected payoff function can be used in Equation 5 to compute the evolutionary
population change according to the replicator dynamics.
6.3.2 The Value of Information in Markets
As an example, consider a market in which differently informed traders bid for a certain
asset (Bloembergen et al., 2015). Depending on their information level, traders have a
certain amount of foresight regarding the future value of the stock. More information gives
a better approximation of the real current value of the asset; however, information comes at
a price. We can use the replicator dynamics model to analyse the effect of various pricing
schemes for information, e.g. no cost, fixed cost for any amount of information, or a cost
function that is linear in the amount of information. Traders can also choose not to acquire
additional information but instead rely solely on the current market price. By running
market simulations with different distributions of information levels among the traders, we
can compute a heuristic payoff table as described above, and use this table as basis for the
replicator model.
Figure 10 shows the resulting dynamics for three types of traders (uninformed  ZI;
averagely informed  I3; insiders  I9), and the three different cost functions described
above. In the absence of costs, the best strategy is to obtain as much information as
689

fiBloembergen, Tuyls, Hennes, & Kaisers

F9

ZI

F9

F3

ZI

F9

F3

ZI

F3

Figure 10: Vector field showing the evolutionary dynamics of a market with three information levels and different cost functions for information: no cost (left), fixed cost (center),
and linear cost (right) (Bloembergen et al., 2015).

possible, leading to a domination of I9 traders over the entire interior of the simplex. Adding
fixed costs gives a small boost to the I0 traders (who do not have to pay), allowing them
to survive in equilibrium alongside the insiders. The linear cost function gives rise to an
internal equilibrium where all types can coexist. Such analysis can be a valuable tool to
gain insight into the dynamics of stock markets, and helps to predict the effect of external
influences (such as costs, e.g. in the form of taxing schemes) on the market as a whole.
6.3.3 Collision Avoidance in Multi-robot Systems
Autonomous collision avoidance is a complex task in the field of robotics, especially in
the presence of dynamic obstacles. The task increases in complexity when the dynamic
obstacles are mobile robots that also take actions to avoid collisions. However, assuming
mutual avoidance (reciprocity) may potentially improve avoidance behaviour since each
robot only needs to take half of the responsibility of avoiding pairwise collisions. In order
to test this hypothesis we can employ the same meta-strategy approach to evaluate the
evolutionary strength of different collision avoidance strategies (Hennes et al., 2013).
One approach to collision avoidance in continuous spaces is the velocity obstacle (VO)
paradigm, first introduced by Fiorini and Shiller (1998) for local collision avoidance and
navigation in dynamic environments with multiple moving objects. The VO strategy can
be modified to include reciprocity, yielding the reciprocal velocity obstacle (RVO); a hybrid
between these two is given by the HRVO. Figure 11 (left) shows the evolutionary dynamics
resulting from a heuristic payoff table over these three strategies. All pure population states
are asymptotically stable fixed points under the replicator dynamics. Although the basin of
attraction for RVO is considerably smaller, we do not see any clearly dominant strategy in
this setting. For pairwise comparison between two strategies, along the faces of the simplex,
no strategy is inferior; all three strategies are evolutionarily stable.
The VO takes all obstacles into account, independent of their distance. This may greatly
reduce the mobility of robots in a highly cluttered environment. In order to overcome this
problem, the VO can be truncated to ignore obstacles that are farther away. Truncation
yields significantly different and more complex dynamics, as shown in Figure 11 (right). In
a pairwise comparison (faces of the simplex), RVO is dominated by VO as well as by HRVO.
690

fiEvolutionary Dynamics of Multi-Agent Learning

VO

HRVO

VO

RVO

HRVO

RVO

Figure 11: Evolutionary dynamics of three strategies for multi-robot collision avoidance,
without truncation (left) and with truncation (right) (Hennes et al., 2013).
However, the reciprocal velocity obstacle is more robust in the presence of all three strategies
(interior of the simplex), which can be explained by it being the more aggressive, or the
least restricting, strategy.

7. Conclusions
In this article we have surveyed recent advances in the study of the evolutionary dynamics
of multi-agent learning. In particular, we presented the formal relation between reinforcement learning and the replicator dynamics of evolutionary game theory. By modifying the
standard replicator dynamics, the behaviour of various state-of-the-art reinforcement learning algorithms in a multi-agent setting can be modelled accurately. So far, this link has
been established in stateless environments (e.g. normal form games), both with discrete
and continuous action spaces, and multi-state environments (e.g. stochastic games) with a
discrete action space. Therefore, an important avenue for future work is the extension of
the theory to stochastic games with continuous action spaces.
The analytical study of multi-agent learning dynamics offers several important advantages. In particular, it sheds light into the black box of reinforcement learning, by making
it possible to analyse the learning dynamics of multi-agent systems in detail, and to compare the behaviour of different algorithms in a principled manner. This in turn facilitates
important tasks such as parameter tuning. Furthermore, studying the dynamics of different
learning algorithms helps in selecting a specific learner for a given problem. Moreover, it
is possible to derive new learning algorithms by first designing preferred dynamics. Finally, the theory can be applied to complex strategic interactions in real-world settings by
analysing meta-strategies, as demonstrated for automated trading and multi-robot collision
avoidance.

691

fiBloembergen, Tuyls, Hennes, & Kaisers

Acknowledgments
We are grateful to the editor and anonymous reviewers of JAIR for their valuable feedback
and helpful suggestions.

References
Abdallah, S., & Kaisers, M. (2013). Addressing the policy-bias of Q-learning by repeating
updates. In Proc. of the 2013 int. conf. on Autonomous Agents and Multi-Agent
Systems (AAMAS 2013), pp. 10451052. International Foundation for AAMAS.
Abdallah, S., & Lesser, V. (2008). A multiagent reinforcement learning algorithm with
non-linear dynamics. Journal of Artificial Intelligence Research, 33 (1), 521549.
Agogino, A. K., & Tumer, K. (2012). A multiagent approach to managing air traffic flow.
Autonomous Agents and Multi-Agent Systems, 24 (1), 125.
Ahmadi, M., & Stone, P. (2006). A multi-robot system for continuous area sweeping tasks.
In ICRA, pp. 17241729.
Axelrod, R., & Hamilton, W. D. (1981). The evolution of cooperation. Science, 211 (4489),
13901396.
Bellman, R. (1957). Dynamic Programming. Princeton University Press.
Bloembergen, D., Hennes, D., McBurney, P., & Tuyls, K. (2015). Trading in markets with
noisy information: An evolutionary analysis. Connection Science, to appear.
Bloembergen, D., Kaisers, M., & Tuyls, K. (2011). Empirical and theoretical support for
lenient learning. In Tumer, Yolum, Sonenberg, & Stone (Eds.), Proc. of 10th Intl.
Conf. on Autonomous Agents and Multiagent Systems (AAMAS 2011), pp. 1105
1106. International Foundation for AAMAS.
Blum, A., & Mansour, Y. (2007). Learning, regret minimization and equilibria. Cambridge
University Press.
Blum, A., & Monsour, Y. (2007). From external to internal regret. Journal of Machine
Learning Research, 8, 13071324.
Borgers, T., & Sarin, R. (1997). Learning through reinforcement and replicator dynamics.
Journal of Economic Theory, 77 (1).
Bowling, M., & Veloso, M. (2002). Multiagent learning using a variable learning rate.
Artificial Intelligence, 136, 215250.
Bowling, M. (2005). Convergence and no-regret in multiagent learning. Advances in neural
information processing systems, 17, 209216.
Bowling, M., & Veloso, M. (2001). Rational and convergent learning in stochastic games.
In Proc. of the 17th Intl. Joint Conf. on Artificial Intelligence, pp. 10211026.
Brown, G. W. (1951). Iterative solution of games by fictitious play. Activity analysis of
production and allocation, 13 (1), 374376.
692

fiEvolutionary Dynamics of Multi-Agent Learning

Busoniu, L., Babuska, R., & De Schutter, B. (2008). A comprehensive survey of multiagent
reinforcement learning. IEEE Transactions on Systems, Man, and Cybernetics, Part
C: Applications and Reviews, 38 (2), 156172.
Busoniu, L., Babuska, R., De Schutter, B., & Ernst, D. (2010). Reinforcement learning and
dynamic programming using function approximators. CRC Press.
Chalkiadakis, G., & Boutilier, C. (2003). Coordination in multiagent reinforcement learning:
a Bayesian approach. In Proc. of the 2nd intl. joint conf. on Autonomous Agents and
MultiAgent Systems, pp. 709716. ACM.
Claes, D., Hennes, D., Tuyls, K., & Meeussen, W. (2012). Collision avoidance under bounded
localization uncertainty. In IROS, pp. 11921198.
Claus, C., & Boutilier, C. (1998). The dynamics of reinforcement learning in cooperative
multiagent systems. In AAAI/IAAI, pp. 746752.
Conitzer, V., & Sandholm, T. (2007). Awesome: A general multiagent learning algorithm
that converges in self-play and learns a best response against stationary opponents.
Machine Learning, 67 (1-2), 2343.
Cressman, R. (2005). Stability of the replicator equation with continuous strategy space.
Mathematical Social Sciences, 50 (2), 127147.
Cross, J. G. (1973). A stochastic learning model of economic behavior. The Quarterly
Journal of Economics, 87 (2), 239266.
Dearden, R., Friedman, N., & Russell, S. (1998). Bayesian Q-learning. In AAAI/IAAI.
Ernst, D., Geurts, P., & Wehenkel, L. (2005). Tree-based batch mode reinforcement learning.
In Journal of Machine Learning Research, pp. 503556.
Fiorini, P., & Shiller, Z. (1998). Motion planning in dynamic environments using velocity
obstacles. International Journal of Robotics Research, 17, 760772.
Fulda, N., & Ventura, D. (2007). Predicting and preventing coordination problems in cooperative Q-learning systems.. In Proceedings of the International Joint Conference
on Artificial Intelligence (IJCAI-07), Vol. 2007, pp. 780785.
Galstyan, A. (2013). Continuous strategy replicator dynamics for multi-agent q-learning.
Autonomous agents and multi-agent systems, 26 (1), 3753.
Gatti, N., Panozzo, F., & Restelli, M. (2013). Efficient evolutionary dynamics with
extensive-form games. In Twenty-Seventh AAAI Conference on Artificial Intelligence,
pp. 335341.
Gibbons, R. (1992). A Primer in Game Theory. Pearson Education.
Gintis, H. (2009). Game Theory Evolving (2nd edition). University Press, Princeton NJ.
Gomes, E. R., & Kowalczyk, R. (2009). Dynamic analysis of multiagent Q-learning with
-greedy exploration. In Proceedings of the 26th Annual International Conference on
Machine Learning, pp. 369376. ACM.
Hennes, D., Bloembergen, D., Kaisers, M., Tuyls, K., & Parsons, S. (2012). Evolutionary
advantage of foresight in markets. In Proceedings of the Genetic and Evolutionary
Computation Conference (GECCO 2012), pp. 943950.
693

fiBloembergen, Tuyls, Hennes, & Kaisers

Hennes, D., Claes, D., & Tuyls, K. (2013). Evolutionary advantage of reciprocity in collision avoidance. In Proc. of the AAMAS 2013 Workshop on Autonomous Robots and
Multirobot Systems (ARMS 2013).
Hennes, D., Kaisers, M., & Tuyls, K. (2010). RESQ-learning in stochastic games. In
Adaptive and Learning Agents Workshop at AAMAS 2010, p. 8.
Hennes, D., Tuyls, K., & Rauterberg, M. (2008). Formalizing multi-state learning dynamics. In Proceedings of the 2008 IEEE/WIC/ACM International Conference on Web
Intelligence and Intelligent Agent Technology, pp. 266272. IEEE Computer Society.
Hennes, D., Tuyls, K., & Rauterberg, M. (2009). State-coupled replicator dynamics. In Proceedings of The 8th International Conference on Autonomous Agents and Multiagent
Systems-Volume 2, pp. 789796. International Foundation for Autonomous Agents
and Multiagent Systems.
Hofbauer, J., & Sigmund, K. (1998). Evolutionary games and population dynamics. Cambridge University Press.
Howell, M. N., Frost, G. P., Gordon, T. J., & Wu, Q. H. (1997). Continuous action reinforcement learning applied to vehicle suspension control. Mechatronics, 7 (3), 263276.
Hu, J., & Wellman, M. P. (2003). Nash q-learning for general-sum stochastic games. The
Journal of Machine Learning Research, 4, 10391069.
Kaelbling, L., Littman, M., & Moore, A. (1996). Reinforcement learning: A survey. Journal
of Artificial Intelligence Research, 4, 237285.
Kaisers, M., & Tuyls, K. (2010). Frequency adjusted multi-agent Q-learning. In Proc. of
9th Intl. Conf. on Autonomous Agents and Multiagent Systems (AAMAS 2010), pp.
309315.
Kaisers, M. (2012). Learning against Learning - Evolutionary Dynamics of Reinforcement
Learning Algorithms in Strategic Interactions. Ph.D. thesis, Maastricht University.
Kaisers, M., Bloembergen, D., & Tuyls, K. (2012). A common gradient in multi-agent reinforcement learning. In Proc. of 11th Int. Conf. on Autonomous Agents and Multiagent
Systems (AAMAS 2012), pp. 13931394.
Kaisers, M., & Tuyls, K. (2011). Faq-learning in matrix games: Demonstrating convergence
near nash equilibria, and bifurcation of attractors in the battle of sexes. In Workshop
on Interactive Decision Theory and Game Theory (IDTGT 2011). Assoc. for the
Advancement of Artif. Intel. (AAAI).
Kaisers, M., Tuyls, K., Parsons, S., & Thuijsman, F. (2009). An evolutionary model of
multi-agent learning with a varying exploration rate. In Proc. of The 8th Intl. Conf.
on Autonomous Agents and Multiagent Systems (AAMAS 2009), pp. 12551256. International Foundation for Autonomous Agents and Multiagent Systems.
Kapetanakis, S., & Kudenko, D. (2002). Reinforcement learning of coordination in cooperative multi-agent systems. AAAI/IAAI, 2002, 326331.
Kianercy, A., & Galstyan, A. (2012). Dynamics of boltzmann Q learning in two-player
two-action games. Phys. Rev. E, 85 (4), 041145.
694

fiEvolutionary Dynamics of Multi-Agent Learning

Klos, T., Van Ahee, G. J., & Tuyls, K. (2010). Evolutionary dynamics of regret minimization. In Machine Learning and Knowledge Discovery in Databases, pp. 8296.
Springer.
Lanctot, M. (2014). Further developments of extensive-form replicator dynamics using the
sequence-form representation. In Proceedings of the 2014 international conference on
Autonomous agents and multi-agent systems, pp. 12571264. International Foundation
for Autonomous Agents and Multiagent Systems.
Littman, M. L. (1994). Markov games as a framework for multi-agent reinforcement learning.. In ICML, Vol. 94, pp. 157163.
Maynard Smith, J., & Price, G. R. (1973). The logic of animal conflict. Nature, 246 (2),
1518.
Mihaylov, M., Tuyls, K., & Nowe, A. (2014). A decentralized approach for convention
emergence in multi-agent systems. Autonomous Agents and Multi-Agent Systems,
28 (5), 749778.
Narendra, K. S., & Thathachar, M. A. L. (1974). Learning automata - a survey. IEEE
Transactions on Systems, Man, and Cybernetics, 4 (4), 323334.
Oechssler, J., & Riedel, F. (2001). Evolutionary dynamics on infinite strategy spaces. Economic Theory, 17 (1), 141162.
Panait, L., Tuyls, K., & Luke, S. (2008). Theoretical advantages of lenient learners: An
evolutionary game theoretic perspective. Journal of Machine Learning Research, 9,
423457.
Panait, L., & Luke, S. (2005). Cooperative multi-agent learning: The state of the art.
Autonomous Agents and Multi-Agent Systems, 11 (3), 387434.
Panozzo, F., Gatti, N., & Restelli, M. (2014). Evolutionary dynamics of Q-learning over
the sequence form. In Twenty-Eighth AAAI Conference on Artificial Intelligence, pp.
20342040.
Phelps, S., Parsons, S., & McBurney, P. (2005). An evolutionary game-theoretic comparison
of two double-auction market designs. In Faratin, P., & Rodrguez-Aguilar, J. (Eds.),
Agent-Mediated Electronic Commerce VI. Theories for and Engineering of Distributed
Mechanisms and Systems, Vol. 3435 of Lecture Notes in Computer Science, pp. 101
114. Springer Berlin Heidelberg.
Pipattanasomporn, M., Feroze, H., & Rahman, S. (2009). Multi-agent systems in a distributed smart grid: Design and implementation. In Power Systems Conference and
Exposition, pp. 18. IEEE.
Ponsen, M., Tuyls, K., Kaisers, M., & Ramon, J. (2009). An evolutionary game-theoretic
analysis of poker strategies. Entertainment Computing, 1 (1), 3945.
Puterman, M. L. (1994). Markov decision processes: Discrete dynamic stochastic programming. John Wiley & Sons, New York.
Rodrguez, A., Vrancx, P., Grau, R., & Nowe, A. (2012). An RL approach to commoninterest continuous action games. In Proceedings of the 11th International Conference
695

fiBloembergen, Tuyls, Hennes, & Kaisers

on Autonomous Agents and Multiagent Systems, pp. 14011402. International Foundation for Autonomous Agents and Multiagent Systems.
Ruijgrok, M., & Ruijgrok, T. W. (2005). Replicator dynamics with mutations for games
with a continuous strategy space. arXiv preprint nlin/0505032.
Santharam, G., Sastry, P. S., & Thathachar, M. A. L. (1994). Continuous action set learning
automata for stochastic optimization. Journal of the Franklin Institute, 331 (5), 607
628.
Schaerf, A., Shoham, Y., & Tennenholtz, M. (1995). Adaptive load balancing: A study in
multi-agent learning. J. Artif. Intell. Res. (JAIR), 2, 475500.
Shoham, Y., Powers, R., & Grenager, T. (2007). If multi-agent learning is the answer, what
is the question?. Artificial Intelligence, 171 (7), 365377.
Singh, S., Kearns, M., & Mansour, Y. (2000). Nash convergence of gradient dynamics
in general-sum games. In Proceedings of the Sixteenth conference on Uncertainty in
artificial intelligence, pp. 541548. Morgan Kaufmann Publishers Inc.
Skyrms, B. (2004). The stag hunt and the evolution of social structure. Cambridge University
Press.
Strens, M. (2000). A Bayesian framework for reinforcement learning. In ICML, pp. 943950.
Sutton, R., & Barto, A. (1998). Reinforcement Learning: An introduction. MA: MIT Press,
Cambridge.
t Hoen, P. J., Tuyls, K., Panait, L., Luke, S., & Poutre, J. A. L. (2005). An overview of
cooperative and competitive multiagent learning. In LAMAS, pp. 146.
Tesauro, G. (2003). Extending q-learning to general adaptive multi-agent systems.. In
NIPS, Vol. 4.
Thathachar, M., & Sastry, P. S. (2002). Varieties of learning automata: an overview. IEEE
Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics, 32 (6), 711722.
Tuyls, K., t Hoen, P. J., & Vanschoenwinkel, B. (2006). An evolutionary dynamical analysis of multi-agent learning in iterated games. Autonomous Agents and Multi-Agent
Systems, 12, 115153.
Tuyls, K., Heytens, D., Nowe, A., & Manderick, B. (2003). Extended replicator dynamics as
a key to reinforcement learning in multi-agent systems. In Machine Learning: ECML
2003, pp. 421431. Springer.
Tuyls, K., & Nowe, A. (2005). Evolutionary game theory and multi-agent reinforcement
learning. The Knowledge Engineering Review, 20 (01), 6390.
Tuyls, K., & Parsons, S. (2007). What evolutionary game theory tells us about multiagent
learning. Artificial Intelligence, 171 (7), 406416.
Tuyls, K., Verbeeck, K., & Lenaerts, T. (2003). A selection-mutation model for q-learning
in multi-agent systems. In Proc. of 2nd Intl. Conf. on Autonomous Agents and Multiagent Systems (AAMAS 2003), pp. 693700, New York, NY, USA. ACM.
Tuyls, K., & Weiss, G. (2012). Multiagent learning: Basics, challenges, and prospects. AI
Magazine, 33 (3), 41.
696

fiEvolutionary Dynamics of Multi-Agent Learning

Tuyls, K., & Westra, R. (2009). Replicator dynamics in discrete and continuous strategy
spaces. In Uhrmacher, A. M., & Weyns, D. (Eds.), Multi-Agent Systems: Simulation
and Applications, pp. 215240. CRC Press.
Van Kampen, N. (1992). Stochastic Processes in Physics and Chemistry. Elsevier Science
Publishers, Amsterdam.
Verbeeck, K., Nowe, A., & Tuyls, K. (2005). Coordinated exploration in multi-agent reinforcement learning: an application to load-balancing. In AAMAS, pp. 11051106.
Von Neumann, J., & Morgenstern, O. (1944). Theory of Games and Economic Behavior.
Princeton University Press.
Vrancx, P., Tuyls, K., Westra, R., & Nowe, A. (2008a). Switching dynamics of multi-agent
learning. In Proc. of the 7th intl. joint conf. on autonomous agents and multiagent systems (AAMAS 2008), pp. 307313. International Foundation for Autonomous Agents
and Multiagent Systems.
Vrancx, P., Verbeeck, K., & Nowe, A. (2008b). Decentralized learning in markov games.
Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on, 38 (4),
976981.
Walsh, W., Das, R., Tesauro, G., & Kephart, J. (2002). Analyzing complex strategic interactions in multi-agent systems. In AAAI-02 Workshop on Game-Theoretic and
Decision-Theoretic Agents.
Watkins, C. J. C. H., & Dayan, P. (1992). Q-learning. Machine Learning, 8 (3), 279292.
Weibull, J. W. (1997). Evolutionary game theory. MIT press.
Wheeler Jr, R., & Narendra, K. S. (1986). Decentralized learning in finite markov chains.
IEEE Transactions on Automatic Control, 31 (6), 519526.
Whiteson, S., & Stone, P. (2006). Evolutionary function approximation for reinforcement
learning. The Journal of Machine Learning Research, 7, 877917.
Wiegand, R. P. (2003). An Analysis of Cooperative Coevolutionary Algorithms. Ph.D.
thesis, George Mason University.
Wunder, M., Littman, M. L., & Babes, M. (2010). Classes of multiagent q-learning dynamics
with epsilon-greedy exploration. In Proceedings of the 27th International Conference
on Machine Learning (ICML-10), pp. 11671174.
Zinkevich, M. (2003). Online convex programming and generalized infinitesimal gradient
ascent. In Proc. of the 20th Intl. Conf. on Machine Learning (ICML-2003).

697

fiJournal of Artificial Intelligence Research 53 (2015) 745-778

Submitted 02/15; published 08/15

AutoFolio:
An Automatically Configured Algorithm Selector
Marius Lindauer

lindauer@cs.uni-freiburg.de

University of Freiburg

Holger H. Hoos

hoos@cs.ubc.ca

University of British Columbia

Frank Hutter

fh@cs.uni-freiburg.de

University of Freiburg

Torsten Schaub

torsten@cs.uni-potsdam.de

University of Potsdam
INRIA Rennes

Abstract
Algorithm selection (AS) techniques  which involve choosing from a set of algorithms
the one expected to solve a given problem instance most efficiently  have substantially
improved the state of the art in solving many prominent AI problems, such as SAT, CSP,
ASP, MAXSAT and QBF. Although several AS procedures have been introduced, not too
surprisingly, none of them dominates all others across all AS scenarios. Furthermore, these
procedures have parameters whose optimal values vary across AS scenarios. This holds
specifically for the machine learning techniques that form the core of current AS procedures, and for their hyperparameters. Therefore, to successfully apply AS to new problems,
algorithms and benchmark sets, two questions need to be answered: (i) how to select an AS
approach and (ii) how to set its parameters effectively. We address both of these problems
simultaneously by using automated algorithm configuration. Specifically, we demonstrate
that we can automatically configure claspfolio 2, which implements a large variety of
different AS approaches and their respective parameters in a single, highly-parameterized
algorithm framework. Our approach, dubbed AutoFolio, allows researchers and practitioners across a broad range of applications to exploit the combined power of many different
AS methods. We demonstrate AutoFolio can significantly improve the performance of
claspfolio 2 on 8 out of the 13 scenarios from the Algorithm Selection Library, leads
to new state-of-the-art algorithm selectors for 7 of these scenarios, and matches state-ofthe-art performance (statistically) on all other scenarios. Compared to the best single
algorithm for each AS scenario, AutoFolio achieves average speedup factors between 1.3
and 15.4.

1. Introduction
Over the last decade, tremendous progress in Boolean constraint solving technology has been
achieved in several areas within AI, such as SAT (Biere, 2013), ASP (Gebser, Kaufmann,
& Schaub, 2012), CSP (Tamura, Taga, Kitagawa, & Banbara, 2009), Max-SAT (Abrame
& Habet, 2014) and QBF (Janota, Klieber, Marques-Silva, & Clarke, 2012). In all these
areas, multiple algorithms with complementary solving strategies exist, and none dominates all others on all kinds of problem instances. This fact can be exploited by algorithm selection (AS) (Rice, 1976) methods, which use characteristics of individual probc
2015
AI Access Foundation. All rights reserved.

fiLindauer, Hoos, Hutter, & Schaub

lem instances (so-called instance features) to choose a promising algorithm for each instance. Algorithm selectors have empirically been shown to improve the state of the art
for solving heterogeneous instance sets and, as a result, have won many prizes at competitions. For instance, SATzilla (Xu, Hutter, Hoos, & Leyton-Brown, 2008) won several
categories in multiple SAT competitions, claspfolio 1 (Gebser, Kaminski, Kaufmann,
Schaub, Schneider, & Ziller, 2011b) won the NP-track of the 2011 ASP Competition, CPHydra (OMahony, Hebrard, Holland, Nugent, & OSullivan, 2008) won the the 2008 CSP
competition, ISAC++ (Ansotegui, Malitsky, & Sellmann, 2014) won the partial Max-SAT
Crafted and Industrial track of the 2014 Max-SAT Competition, and AQME (Pulina &
Tacchella, 2009) won the first stage of the main track of the 2010 QBF Competition.
Although many new AS approaches have been proposed over the years (cf. Smith-Miles,
2008; Kotthoff, 2014), there are only two flexible frameworks that allow for re-implementing
and comparing existing approaches in a fair and uniform way: LLAMA (Kotthoff, 2013)
and claspfolio 2 (Hoos, Lindauer, & Schaub, 2014). Of these, claspfolio 2 is more
comprehensive, encompassing strategies from the algorithm selection systems 3S (Kadioglu,
Malitsky, Sabharwal, Samulowitz, & Sellmann, 2011), aspeed (Hoos, Kaminski, Lindauer,
& Schaub, 2015), claspfolio 1 (Gebser et al., 2011b), ISAC (Kadioglu, Malitsky, Sellmann, & Tierney, 2010), ME-ASP (Maratea, Pulina, & Ricca, 2014), SNNAP (Collautti,
Malitsky, Mehta, & OSullivan, 2013) and SATzilla (Xu et al., 2008; Xu, Hutter, Hoos,
& Leyton-Brown, 2011).
Figure 1 illustrates the performance benefits these existing selection strategies (as realized in claspfolio 2) yield across the wide range of AS benchmarks in the Algorithm Selection Library (Bischl et al., 2015b, 2015a). We observe that each approach has strengths
and weaknesses on different scenarios. The SATzilla11-like approach (the default of
claspfolio 2) performs best overall, but only achieves better performance than the other
approaches considered on 8 out of the 13 scenarios, with 3S, aspeed and ISAC yielding
better performance in the remaining cases.
We further note that each of the selection approaches used a fixed default parameter
configuration and might therefore fall short of its full performance potential. For example, imputation of missing instance features was not used at all in the approaches considered in Figure 1; while its use does not improve performance on some scenarios (e.g.,
ASP-POTASSCO), it yields improvements on others (e.g., SAT12-RAND, where the
SATzilla11-like approach plus mean imputation outperforms the single best algorithm
by a factor of 1.2).
Generally, it is well known that the performance of many machine learning techniques
depends on hyper-parameter settings (e.g., in the case of an SVM, the kernel, kernel hyperparameter and soft margin; cf. Bergstra, Bardenet, Bengio, & Kegl, 2011; Snoek, Larochelle,
& Adams, 2012; Thornton, Hutter, Hoos, & Leyton-Brown, 2013). However, the hyperparameters of the machine learning models used in Figure 1 were fixed manually, based on
limited experiments. Therefore, the performance of some of the algorithm selection systems
we considered could likely be improved by using more carefully chosen hyper-parameter
settings.
Facing a new algorithm selection problem, we thus have to answer three salient questions: (i) which selection approach to use; (ii) how to set the parameters of the selection
approach (and its underlying machine learning model) effectively; and (iii) how to make
746

filio
-1.
0-l
ISA
ike
C-l
ike
ME
-AS
P-l
ike
SA
Tzi
lla
'09
-lik
SA
e
Tzi
lla
'11
-lik
Au
e
toF
oli
o

cla

sp
fo

d
asp

ee

3S

-lik

e

AutoFolio: An Automatically Configured Algorithm Selector

ASP-POTASSCO
CSP-2010
MAXSAT12-PMS
PREMARSHALLING
PROTEUS-2014
QBF-2011
SAT11-HAND
SAT11-INDU
SAT11-RAND
SAT12-ALL
SAT12-HAND
SAT12-INDU
SAT12-RAND

4.1
1.5
6.5
2.9
10.9
7.7
2.6
1.2
3.9
1.5
1.7
1.2
0.8

1.4
1.0
2.7
3.6
6.3
4.9
3.6
1.1
4.7
1.1
1.8
0.8
0.8

2.8
2.1
1.6
1.2
3.5
2.3
1.1
1.2
1.2
1.2
1.1
1.2
0.6

3.8
2.1
4.9
1.3
4.3
2.8
1.2
1.3
2.5
1.1
1.1
1.2
0.9

1.9
2.6
2.1
1.1
3.1
2.8
1.0
1.2
1.8
1.1
1.0
1.1
0.9

2.9
2.5
3.4
1.5
4.9
3.7
1.9
1.1
2.6
1.4
1.5
1.3
0.9

4.2
3.1
8.6
2.3
6.5
9.8
2.3
1.2
3.8
1.8
1.9
1.3
0.9

4.2
3.2
8.6
3.5
7.8
10.1
3.2
1.5
15.4
3.0
3.2
1.8
1.3

geo. mean

2.6

2.0

1.5

1.9

1.5

2.0

2.8

3.9

Figure 1: Factors by which the selection approach re-implemented in claspfolio 2 outperformed the single best algorithm in the 13 ASlib scenarios w.r.t. penalized average runtime
(PAR10, which counts each timeout as 10 times the given runtime cutoff). These results
are for 10-fold cross-validation, ignoring test instances that were not solved by any solver.
The last row shows the geometric mean over all 13 scenarios.

best use of techniques augmenting pure AS, such as pre-solving schedules (Xu et al., 2008;
Kadioglu et al., 2011). Instead of the common, manual trial-and-error approach, we propose to automatically answer these questions by using automated algorithm configuration
methods (Hutter, Hoos, Leyton-Brown, & Stutzle, 2009) to configure flexible AS frameworks. While the manual approach is error-prone, potentially biased and requires substantial human expert time and knowledge, the approach we introduce here is fully automatic,
unbiased, and leverages the full power of a broad range of AS methods. It thus facilitates
an easier and more effective use of algorithm selection and makes AS techniques accessible
to a broader community.
Specifically, we present AutoFolio, a general approach for automatically determining a
strong algorithm selection method for a particular dataset, by using algorithm configuration
to search through a flexible design space of algorithm selection methods. We also provide an
open-source implementation of AutoFolio (www.ml4aad.org/autofolio/) based on the
algorithm configurator SMAC (Hutter, Hoos, & Leyton-Brown, 2011) and the algorithm
selection framework claspfolio 2 (Hoos et al., 2014). The last column of Figure 1 previews
the results obtained with AutoFolio and clearly shows significant improvements over
claspfolio 2 on 10 of the 13 scenarios in ASlib.
747

fiLindauer, Hoos, Hutter, & Schaub

Instance

Algorithm
Portfolio

Compute
Features

Select Algorithm

Solve Instance
with Algorithm

Figure 2: General outline of algorithm selection.

2. Background: Algorithm Configuration And Selection
In this section, we briefly introduce standard approaches to algorithm selection and algorithm configuration that form the basis of our AutoFolio approach.
2.1 Algorithm Selection
Figure 2 shows the general outline of algorithm selection (Rice, 1976). For a given problem
instance, we first compute cheap instance features; these are numerical characteristics,
including simple ones (such as the number of variables or clauses in a SAT instance) and
more complex ones (such as statistics gathered from short probing runs of an actual SAT
solver on the given instance). Based on these features, an appropriate algorithm from an
algorithm portfolio (Huberman, Lukose, & Hogg, 1997; Gomes & Selman, 2001) is selected
to solve the given instance. The overall workflow is subject to a runtime cutoff.
One major challenge in algorithm selection is to find a good mapping from instance
features to algorithms. In the general offline algorithm selection approach we consider, this
is done based on training data. Specifically, given a portfolio of algorithms A and a set of
problem instances I, we use as training data a performance matrix of size #I  #A and a
feature matrix containing a fixed-size feature vector for each i  I. Based on this training
data, we learn a mapping from instance features to algorithms using machine learning
techniques, such as k-NN (Maratea et al., 2014), g-means (Kadioglu et al., 2010) or random
forests (Xu et al., 2011).
2.1.1 Related Work On Algorithm Selection Systems
Recent successful algorithm selection systems include SATzilla (Xu et al., 2008; Xu, Hutter, Hoos, & Leyton-Brown, 2012a), 3S (Kadioglu et al., 2011; Malitsky, Sabharwal, Samulowitz, & Sellmann, 2012, 2013b), ISAC (Kadioglu et al., 2010; Ansotegui et al., 2014),
CSHC (Malitsky, Sabharwal, Samulowitz, & Sellmann, 2013a) and claspfolio 1 (Gebser
et al., 2011b). In recent years, these systems showed excellent performance in competitions
for SAT, MAXSAT and ASP. We briefly review them in the following.
The original version of the pioneering algorithm selection system SATzilla (Xu et al.,
2008) learned the mapping from instance features to algorithms by training ridge regression models. Each regression model predicts the performance of an algorithm for a given
instance. Based on these predicted performances, SATzilla selects the algorithm with the
best predicted performance. SATzillas latest version (Xu et al., 2011) uses classification
models that, for each pair of algorithms, predict the better-performing one, and selects the
algorithm to be run using simple voting over the predictions thus obtained. These models
are also cost-sensitive, that is, each training instance in the pairwise classification models is
748

fiAutoFolio: An Automatically Configured Algorithm Selector

weighted by the performance loss incurred when selecting the worse of the two algorithms.
Furthermore, SATzilla introduced the concept of pre-solving schedules, that is, a short
instance-independent schedule of algorithms running for a limited amount of time. If one
algorithm of the pre-solving schedule solves the given instance, SATzilla can immediately
terminate successfully, saving the time required to compute instance features. Furthermore,
pre-solving schedules increase the robustness of algorithm selectors by not only relying on
one selected algorithm but also on the pre-solvers to solve a given instance. One drawback
of SATzilla is its use of grid search over all possible pre-solving schedules with up to three
pre-solvers; for each schedule considered, SATzilla performs algorithm subset selection
and trains the classification models, which can require substantial amounts of time (in our
experiments, up to 4 CPU days).
3S (Kadioglu et al., 2011; Malitsky et al., 2012, 2013b) uses a k-nearest neighbour
approach to select an algorithm. For a given problem instance to be solved, it determines
a set of similar training instances in the instance feature space and selects the algorithm
with the best performance on this instance set. The performance of this k-NN approach
is further improved by distance-based weighting (that is, weighting algorithm performance
on an instance by the instances distance to the new given instance) and using a clusteringbased adaptive neighbourhood size (to adjust the size of the neighbourhood in different
areas of the feature space). Furthermore, 3S uses mixed integer programming to compute
pre-solving schedules more efficiently than SATzilla.
ISAC (Kadioglu et al., 2010) clusters instances in the instance feature space using the
g-means algorithm and stores the cluster centre as well as the best-performing algorithm for
each cluster. For each new problem instance, it then determines the nearest cluster centre
(1-NN) and selects the algorithm associated with it.
The cost-sensitive hierarchical clustering system CSHC (Malitsky et al., 2013a) also
partitions the feature space into clusters, but instead of ISACs unsupervised clustering
approach, it creates this partitioning in a supervised top-down fashion, much like a decision
or regression tree algorithm. Starting with all instances (the entire feature space) at the
root of a tree, it recursively splits the instances associated with a node into two child
nodes, choosing each split along a single feature value, such that the performance of the
best-performing algorithm in each child node is optimized. This cost-sensitive supervised
approach based on trees closely resembles the cost-sensitive random forests in SATzilla,
with the difference that, in contrast to SATzillas pairwise voting approach, it only builds
a single model.
Last but not least, claspfolio 1 (Gebser et al., 2011b) is the predecessor of claspfolio 2, which we use here (and describe in Section 2.1.2). In contrast to the flexible framework of claspfolio 2, claspfolio 1 was inspired by the earlier version of SATzilla and
uses the same regression approach, but with a different machine learning method (support
vector regression instead of ridge regression).
Further systems for algorithm selection combine and extend these techniques, for example, by combining regression and clustering approaches (Collautti et al., 2013), or by
selecting algorithm portfolios (Yun & Epstein, 2012; Lindauer, Hoos, & Hutter, 2015a)
or schedules (Amadini, Gabbrielli, & Mauro, 2014) instead of a single algorithm. For additional information, we refer the interested reader to two recent surveys on algorithm
selection (Smith-Miles, 2008; Kotthoff, 2014).
749

fiLindauer, Hoos, Hutter, & Schaub

Feature
Generator

Training Instances

Algorithms

Instance Features
and Groups

Algorithm
Performance
ASlib Scenario

Feature
Preprocessing

Performance
Preprocessing
Train
Selection Model(s)

I

Performance Estimation

Pre-Solving Schedule
by aspeed

Selection

Scheduling
Offline Training

(Test) Instance

Compute Features

Select Algorithm

failed
Run Backup
Algorithm

Run Pre-Solving
Schedule
if not successful
Run Selected
Algorithm
Online Solving

Figure 3: General workflow of claspfolio 2. Objects such as algorithms and instances
are shown as rectangles, and activities are depicted as rectangles with rounded corners.
Activities related to algorithm selection are shown in red and activities related to algorithm
schedules in yellow.
2.1.2 The Algorithm Selection Framework claspfolio 2
We now explain the algorithm selection framework claspfolio 2 (Hoos et al., 2014; Lindauer, Hoos, & Schaub, 2015c) in some more detail, since it provides the basis for the
concrete implementation of our general AutoFolio approach, as used in our experiments.
The claspfolio 2 framework implements the idea of algorithm selection in a flexible
and general way. It provides a general view on the individual components of algorithm
selectors, based on which it implements many different selection approaches and associated
techniques. Therefore, claspfolio 2 is a natural candidate to serve as a basis for our
AutoFolio approach.
Figure 3 shows the workflow of claspfolio 2, which is divided into an ASlib Scenario
as input of claspfolio 2; Offline Training of Selection and Scheduling; and Online Solving
a new instance:
ASlib scenario. As an input, claspfolio 2 reads an algorithm selection scenario, supporting the format of the Algorithm Selection library, ASlib. This consists of a
performance matrix, instance features, groups of instance features1 and some optional information, such as cross-validation splits or ground truth about the problem
1. We note that, according to the definition of ASlib, each feature group enables a list of instance features
that are computed with a common block of feature computation code, and jointly incur the cost for
running this code.

750

fiAutoFolio: An Automatically Configured Algorithm Selector

instances (for example, whether a SAT instance is satisfiable or unsatisfiable). For a
full specification of the ASlib format, we refer the interested reader to aslib.net.
Offline training  selection. Based on the given scenario (training) data, claspfolio 2
pre-processes the instance features (for example, normalization or feature imputation)
and performance data (for example, log-transformation). Using machine learning
techniques, claspfolio 2 learns a selection model that maps instance features to
algorithms.
Offline training  scheduling. To compute an efficient pre-solving schedule, claspfolio 2 first estimates the performance of the Selection module by using an internal
cross-validation on the training data (Arrow I). Based on this performance estimation,
claspfolio 2 computes a timeout-minimal pre-solving schedule using Answer Set
Programming in aspeed (Hoos et al., 2015), assigning each algorithm a (potentially
zero-length) time slice of the overall runtime budget. The estimation of the Selection
module is necessary to compute the runtime budget for the pre-solving schedule. If
the Selection module performs well, the pre-solving schedule may be empty, because
the pre-solving schedule cannot perform better than a perfect predictor (that is, a predictor that always selects the best solver). In contrast, if the prediction performs very
poorly (for example, as a result of non-informative instance features), the pre-solving
schedule may be allocated the complete time budget, with the Selection module being
ignored.
Online solving. The Solving workflow is as follows: a feature generator computes the
instance features of a new problem instance to be solved; if this computation fails
(for example, because of time or memory constraints) and no feature imputation
strategy is selected, a backup solver  i.e., the single best performing solver in the
offline training  is run on the instance; otherwise, the previously trained selection
model uses the instance features to select an algorithm expected to perform well. If
a pre-solving schedule is available, the schedule runs either before instance feature
computation or after the selection of the algorithm, depending on a parameter setting
of claspfolio 2  this latter version being shown in Figure 3. The former has the
advantage that the time to compute instance features can be saved if the instance is
solved during pre-solving. The latter has the advantage that the algorithm chosen by
the selector can be removed from the pre-solving schedule to prevent running it twice.
A list of all techniques we implemented for these modules is given in Section 3.2.
2.2 Algorithm Configuration
Figure 4 shows a general outline for algorithm configuration methods. Given a parameterized algorithm A with possible parameter settings C, a set of training problem instances
I, and a performance metric m : C  I  R, the objective in the algorithm configuration
problem is to find a parameter configuration c  C that minimizes m across the instances
in I. Prominent examples for the performance metric to be optimized are the runtime,
solution quality, or misclassification cost the target algorithm achieves. The configuration
751

fiLindauer, Hoos, Hutter, & Schaub

Instances I

Algorithm A and
its Configuration Space C

Select c  C

Assess A(c) on
some I 0  I

Returns
Best Found
Configuration c

Return Performance
Configuration Task

Figure 4: General outline of algorithm configuration.
procedure (or short configurator ) iteratively evaluates the performance of parameter configurations c  C (by running A with them on one or more instances in I) and uses the
result to decide about the next configurations to evaluate. After a given budget for the configuration process has been exhausted, the configurator returns the best known parameter
configuration it found until then.
When A has n parameters p1 , . . . , pn , with respective domains D1 , . . . , Dn , the parameter
configuration space C = D1      Dn is the cross-product of these domains, and each
parameter configuration c  C assigns a value to each parameter. There are several types of
parameters, including real-valued, integer-valued and categorical ones (which have a finite,
unordered domain; for example, a choice between different machine learning algorithms).
Furthermore, configuration spaces can be structured; specifically, a parameter pi can be
conditional on another parameter pj , such that the value of pi is only relevant if the parent
parameter pj is set to a specific value. For example, this is the case when pj is a categorical
choice between machine learning algorithms, and pi is a sub-parameter of one of these
algorithms; pi will only be active if pj chooses the algorithm it parameterizes further.
To date, there are four general configuration procedures: ParamILS (Hutter et al.,
2009), GGA (Ansotegui, Sellmann, & Tierney, 2009), irace (Lopez-Ibanez, Dubois-Lacoste,
Stutzle, & Birattari, 2011), and SMAC (Hutter et al., 2011). In principle, we could use any
of these as the configurator in our general AutoFolio approach. In practice, we have
found SMAC to often yield better results than ParamILS and GGA (Hutter et al., 2011;
Hutter, Lindauer, Balint, Bayless, Hoos, & Leyton-Brown, 2015; Lindauer, Hoos, Hutter, &
Schaub, 2015b), and thus use it as the basis for the concrete implementation of AutoFolio
discussed in the following. We now describe SMAC in more detail.
2.2.1 SMAC: Sequential Model-Based Algorithm Configuration
The sequential model-based algorithm configuration method SMAC (Hutter et al., 2011;
Hutter, Hoos, & Leyton-Brown, 2015a) uses regression models that approximate the performance metric m : C  I  R (Hutter, Xu, Hoos, & Leyton-Brown, 2014). It follows the
general algorithm configuration workflow from above, alternating evaluations of m for some
parameter configurations and instances with decision phases, in which the configurator uses
the data gathered so far to select which configurations to evaluate next on which instances.
SMACs decision phases involve constructing a regression model m : C  I  R based
on the data observed so far, and then using this model (as well as the models uncertainty
in its predictions) to select promising configurations to try next. This step automatically
752

fiAutoFolio: An Automatically Configured Algorithm Selector

trades off exploration (evaluating in regions of the configuration space where the model m
is very uncertain) and exploitation (evaluating configurations predicted to perform well).
In order to save time in evaluating new configurations cnew  C, SMAC first evaluates
them on a single instance i  I; additional evaluations are only carried out (using a doubling
schedule) if, based on the evaluations to date, cnew appears to outperform SMACs best
known configuration c. Once it has evaluated the same number of runs for cnew as for c, if
cnew still performs better, SMAC updates its best known configuration c to cnew .
2.2.2 Previous Applications Of Algorithm Configuration
Algorithm configuration has been demonstrated to be very effective in optimizing algorithms for a wide range of problems, including SAT-based formal verification (Hutter, Babic,
Hoos, & Hu, 2007), timetabling (Chiarandini, Fawcett, & Hoos, 2008), multi-objective optimization (Lopez-Ibanez & Stutzle, 2010), mixed integer programming (Hutter, Hoos, &
Leyton-Brown, 2010), AI planning (Vallati, Fawcett, Gerevini, Hoos, & Saetti, 2013), generation of heuristics (Mascia, Lopez-Ibanez, Dubois-Lacoste, & Stutzle, 2014), occupancy
scheduling (Lim, van den Briel, Thiebaux, Backhaus, & Bent, 2015) and kidney exchange
matching (Dickerson & Sandholm, 2015). An important special case of algorithm configuration is hyperparameter optimization in machine learning (Bergstra et al., 2011; Snoek
et al., 2012; Eggensperger et al., 2013).
The previous line of work most related to our application of configuration to algorithm
selection is Auto-WEKA (Thornton et al., 2013). Auto-WEKA addresses the combined
problem of selecting a machine learning algorithm from the WEKA framework (Hall, Frank,
Holmes, Pfahringer, Reutemann, & Witten, 2009) and optimizing its hyperparameters.
AutoFolio also needs to solve this combined algorithm selection and hyperparameter
optimization problem, and in particular needs to do so for each of the problem formulations
it considers: regression, classification and clustering. Further important design choices in
AutoFolio are pre-solving and its parameters, as well as which instance features to use.
AutoFolio applies one meta-solving strategy (algorithm configuration) to another one
(algorithm selection). A previous application of a meta-solving strategy to itself was the selfconfiguration of ParamILS (Hutter et al., 2009). However, in that case, self-configuration
only yielded a modest improvement over the default configuration of ParamILS, whereas
here, we achieve substantial improvements over the default configuration of claspfolio 2.
Algorithm configuration and algorithm selection have previously been combined in a
different way, by using configuration to find good parameter settings of a highly parameterized algorithm, and then using selection to choose between these on a per-instance
basis. Two systems implement this approach to date: ISAC (Kadioglu et al., 2010) and
Hydra (Xu, Hoos, & Leyton-Brown, 2010). ISAC first clusters training problem instances
into homogeneous subsets, uses a configurator to find a good solver parameterization for
each cluster, and then uses a selector to choose between these parameterizations. Hydra
iteratively adds new solver parameterizations to an initially empty portfolio-based selector,
at each step tasking a configurator to find the solver parameterization that most improves
the current portfolio.
753

fiLindauer, Hoos, Hutter, & Schaub

Training Data

Test Data

10-fold cross-validation = 10 meta instances

Figure 5: Split of instance sets in algorithm selection scenarios; cross-validation is performed
inside the configuration process, the test set is withheld for evaluating the configured selector.

3. Configuration Of Algorithm Selectors
We now present our AutoFolio approach of using algorithm configurators to automatically
customize flexible algorithm selection (AS) frameworks to specific AS scenarios. To apply
algorithm configuration in this context, we need to specify a parameterized selector and its
configuration space, as well as the performance metric by which we judge its performance.
3.1 Formal Problem Statement
To judge the performance of an algorithm selection (AS) system on an AS scenario, it is
crucial to partition the given set of problem instances into a training and a test set, use the
AS system only on the training set to train a selector s, and evaluate s only on the test
set instances. (If the training set was instead used to evaluate performance, a perfect AS
system could simply memorize the best solver for each instance without learning anything
useful for new problem instances). This is the standard notion of a training-test split from
machine learning.
An AS scenario includes algorithms A, problem instances I, performance and feature
data D, and a loss function l : A  I  R to be minimized (for example, the algorithms
runtime or solution cost), with the data split into disjoint sets Dtrain and Dtest . Let
S(Dtrain ) : I  A denote the selector learned by the AS system S when trained on the data
in Dtrain . Then, the performance of S, P (S) is the average performance of the algorithms
it selects on the instances in the test data set Dtest :
P (S) =

X
1

l(S(Dtrain ), i).
|Dtest |

(1)

iDtest

Likewise, we can evaluate the performance of an AS system Sc parameterized by a
configuration c as P (Sc ). However, we can not perform algorithm configuration by simply
minimizing P (Sc ) with respect to c  C: this would amount to peeking at the test set
many times, and even though it would yield a configuration c with low P (Sc ), it could
not be expected to perform well on instances not contained in Dtest . Instead, in order to
obtain an unbiased evaluation of the configured selectors performance in the end, we need
to hold back a test set of instances that is not touched during the configuration process. In
order to still be able to optimize parameters without access to that test set, the standard
solution in machine learning is to partition the training set further, into k cross-validation
folds. Overall, we use the instance set for each selection scenario as illustrated in Figure 5:
(i) we split the full set of instances into a training and a test set and (ii) the training data
754

fiAutoFolio: An Automatically Configured Algorithm Selector

Algorithm 1: AutoFolio: Automated configuration of an algorithm selector
Input : algorithm configurator AC, algorithm selector S, configuration space C of
S, training data of algorithm scenario D (with performance and feature
matrix), number of folds k

5

randomly split D into D(1) , . . . , D(k)
start AC with D(1) , . . . , D(k) as meta instances, using average loss across
meta-instances as performance metric m, and using S as target algorithm with
configuration space C
while configuration budget remaining do
AC selects configuration c  C and meta instance n  {1 . . . k}
train Sc on D\D(n) , assess its loss on D(n) and return that loss to AC

6

return best configuration c of S found by AC

1
2

3
4

is further partitioned into k folds (in our experiments, we use k = 10), which are used as
follows.
(1)
(k)
Let Dtrain , . . . , Dtrain be a random partition of the training set Dtrain . The crossvalidation performance CV (Sc ) of Sc on the training set is then:


k
X
X
1
1


(j)
CV (Sc ) = 
l(Sc (Dtrain \Dtrain ), i)
(2)
 (j) 
k
|Dtrain |
(j)
j=1
iDtrain

In the end, we optimize the performance CV (Sc ) by determining a configuration c  C
of the selector S with good cross-validation performance
c  arg min CV (Sc ),

(3)

cC

and evaluate c by training a selector Sc with it on the entire training data and evaluating
P (Sc ) on Dtest , as defined in Equation 1.
(j)
Following Thornton et al. (2013), we use each of the k folds Dtrain as one instance within
the configuration process. In order to avoid confusion between these AS instances and the
base-level problem instances (e.g., SAT instances) to be solved inside the AS instance, we
refer to the AS instance as a meta-instance. We note that many configurators, such as FocusedILS (Hutter et al., 2009), irace (Lopez-Ibanez et al., 2011) and SMAC (Hutter et al.,
2011), can discard configurations when they perform poorly on a subset of meta-instances
and therefore do not have to evaluate all k cross-validation folds for every configuration.
This saves time and lets us evaluate more configurations within the same configuration
budget. Based on these considerations, Algorithm 1 outlines the process to configure an
algorithm selector with AutoFolio.
Since the instances in an AS scenario could be split into configuration and testing sets in
many different ways, one such split does not necessarily yield a representative performance
estimate. Therefore, to yield more confident results in our evaluation, we perform an additional outer cross-validation (as given by an ASlib scenario) instead of a single training-test
755

fiLindauer, Hoos, Hutter, & Schaub

split. That is, we consider multiple training-test splits, configure the selector for each training set, assess its final configurations on the respective test data sets, and average results.
We note, however, that in a practical application of AS, one would only have a single training set (which we would still split into k cross-validation splits internally) and a single test
set.
3.2 Configuration Space Of Selectors
Most existing algorithm selectors implement one specific algorithm selection approach, using
one specific machine learning technique. We note, however, that most selection approaches,
at least implicitly, admit more flexibility, and in particular could be used with a range of
machine learning techniques. For example, SATzilla11 (Xu et al., 2011) uses voting on
pairwise performance predictions obtained from cost-sensitive random forest classifiers, but,
in principle, it could use other cost-sensitive binary classifiers instead of random forests.
Based on this observation, we consider a hierarchically structured configuration space
with a top-level parameter that determines the overall algorithm selection approach 
for example, a regression approach, as used in SATzilla09 (Xu et al., 2008) or a k-NN
approach, as used in ME-ASP (Maratea et al., 2014). For most selection approaches, we
can then choose between different regression techniques, for example, ridge regression, lasso
regression, support vector regression or random forest regression. Each of these machine
learning techniques can be configured by its own (hyper-)parameters.
Besides the selection approach, further techniques are used for preprocessing the training data (for example, z-score feature normalization as a feature preprocessing step or
log-transformation of runtime data as a performance preprocessing step). Preprocessing
techniques can be configured independently from the selection approach, and are therefore
also handled by top-level parameters.
We use a third group of parameters to control pre-solving schedules (Kadioglu et al.,
2011; Xu et al., 2011), including parameters that determine the time budget for pre-solving
and the number of pre-solvers considered. Pre-solving techniques can be freely combined
with selection approaches; because they are not always needed, we added a top-level binary
parameter that completely activates or deactivates the use of pre-solvers; all other presolving parameters are conditional on this switch.
We implemented these choices in the claspfolio 2 system described in Section 2.1.2.
Figure 6 illustrates the complete configuration space thus obtained. Our current version,
which we use for the concrete implementation of our AutoFolio approach, covers six
different algorithm selection approaches:
(hierarchical) regression (inspired by SATzilla09; Xu et al., 2008) learns a regression
model for each algorithm; for a new instace, it then selects the algorithm with best
predicted performance;
multiclass classification (inspired by LLAMA; Kotthoff, 2013) learns a classification
model that directly selects an algorithm based on the features of a new instance;
pairwise classification (inspired by SATzilla11; Xu et al., 2011) learns a (cost-sensitive)
classification model for all pairs of algorithms; for a new instance, it evaluates all models and selects the algorithm with the most votes;
756

fiAutoFolio: An Automatically Configured Algorithm Selector

transformation

pre-solving

yes

instance
weighting

contribution
filtering

normalization

AS approach

imputation

p : PCA

Performance Preprocessing

max_feature_time

Feature Preprocessing

4
multi-class
classification

pairwise
classification

c (SVM)
gamma(SVM)

(hierarchical)
regression

clustering

SNNAP

k : k-NN

max cluste r

k
best_n

random
forest

SVM

gradient
boosting

random
forest

SVM

gradient
boosting

ridge

lasso

SVR

random
forest

ridge

lasso

SVR

random
forest

3

2

3

3

2

3

1

1

3

2

1

1

3

2

k-means

Gaussian
mixture

spectral
clustering

Figure 6: Configuration space of claspfolio 2, including 22 categorial parameters, 15
integer valued parameters and 17 continous parameters. Parameters in double boxes are
top-level parameters; single boxes represent algorithm selection approaches based on classes
of machine learning techniques, dashed boxes machine learning techniques and dotted boxes
indicate the number of low-level parameters. Parameter boxes used in the default configuration are filled in grey.
clustering (inspired by ISAC; Kadioglu et al., 2010) determines subsets of similar training
instances in the feature space and the best algorithm on these subsets; for a new
instance, it determines the nearest cluster center and selects the associated algorithm;
k-NN (inspired by 3S; Kadioglu et al., 2011, and ME-ASP; Maratea et al., 2014) determines a set of similar training instances in the feature space for a given new instance
and selects the algorithm with the best performance on this instance set;
SNNAP (inspired by Collautti et al., 2013) predicts the performance of each algorithm
with regression models and uses this information for a k-NN approach in the predicted
performance space.
For each of these approaches, claspfolio 2 covers at least three different machine
learning techniques (where appropriate). These are listed in Figure 6; for example, pairwise
classification can be based on random forests, SVMs or gradient boosting (with 3, 2 and 3
hyper-parameters, respectively). For preprocessing strategies, it supports:
Performance preprocessing:
transformation applies log (Xu et al., 2008) or z-score normalization (Collautti
et al., 2013) to the performance data;
instance weighting weights the instances by their impact on the performance of
an algorithm selector, that is, instances get a low weight if all available algorithms perform equally, and high weight if the algorithms differ substantially in
performance (Kotthoff, Gent, & Miguel, 2012);
contribution filtering removes algorithms that have less than a specified contribution to the performance of the oracle (also known as virtual best solver) (Xu
et al., 2012a); this is a form of algorithm subset selection.

757

fiLindauer, Hoos, Hutter, & Schaub

Feature preprocessing:
normalization transforms the instance features with min-max, z-score, decimalpoint, log scheme or by application of PCA;
p:PCA applies principal component analysis on the features and selects the top p
principal components, where p is a parameter (if PCA was activated);
imputation fills in missing feature values as the median, average or most frequent
value of a feature  if imputation is deactivated and a feature vector is incomplete
for a given instance, the single best solver is statically selected;
max feature time limits the amount of time spent to collect features  this ensures
that not too much time is spent on feature computation; however, it can result
in incomplete features with missing values (which get imputed if imputation is
active).
We chose the default configuration of claspfolio 2 (used to initialize the algorithm
configurator) to be a SATzilla11-like configuration, since this was shown to be effective on
SAT (Xu et al., 2012a) and ASP (Hoos et al., 2014), and since its overall high performance is
evident from the results in Figure 1. This configuration uses pairwise cost-sensitive random
forest classifiers, z-score feature normalization and a pre-solving schedule with at most three
pre-solvers. Since we assume no prior knowledge about the algorithm selection scenarios, the
default configuration uses the default instance features as defined by the scenario designers.
We chose claspfolio 2 as the basis for AutoFolio, because it has been designed to
be flexible and is known to perform well.2 We note that in principle, other selectors, such
as SATzilla (Xu et al., 2008), ISAC (Kadioglu et al., 2010), SNNAP (Collautti et al.,
2013) and LLAMA (Kotthoff, 2013), could be generalized in a similar way.
In addition to using claspfolio 2 as its algorithm selection framework, our current
version of AutoFolio employs the algorithm configurator SMAC (described in Section
2.2.1). Like the selection framework, this configurator is also exchangeable: while we chose
SMAC, because it performed best across the algorithm configuration problems we studied
so far, in principle, other configurators could also be used, such as, GGA (Ansotegui et al.,
2009) or irace (Lopez-Ibanez et al., 2011). Preliminary results (Lindauer et al., 2015b)
showed that ParamILS can also optimize the performance of claspfolio 2, but was
inferior to SMAC in all but one scenario, on which its performance advantage was small.

4. Empirical Performance Analysis
In this section, we empirically analyze the performance of our AutoFolio approach. In
these experiments, AutoFolio employs claspfolio 2 using the well-known machine learning package scikit-learn (Pedregosa et al., 2011) (version 0.14.1) and the algorithm configurator SMAC (version 2.08.00). We ran AutoFolio on the thirteen algorithm selection
scenarios that make up the Algorithm Selection Library 1.0 (Bischl et al., 2015b).3
2. Results on the performance of claspfolio 2 compared to other state-of-the-art algorithm selectors can
be found at aslib.net.
3. We note that for experiments on ASlib scenarios, claspfolio 2 and other algorithm selectors do not
need to perform actual runs of algorithms or feature generators, because the ASlib scenarios already

758

fiAutoFolio: An Automatically Configured Algorithm Selector

As shown in Table 1, these scenarios comprise a wide variety of hard combinatorial
problems; each of them includes the performance data of a range of solvers (between 2 and
31) for a set of instances, and instance features organized in feature groups with associated
costs. For all scenarios we consider here, the performance objective is runtime minimization.
On a high level, these scenarios comprise the following data:
 ASP-POTASSCO: runtimes of different parameter configurations of the ASP solver
clasp on a broad range of ASP instances collected by the Potassco group (Gebser,
Kaminski, Kaufmann, Ostrowski, Schaub, & Schneider, 2011a);
 CSP-2010: runtimes of a single solver with two different configurations (with and
without lazy learning; Gent, Jefferson, Kotthoff, Miguel, Moore, Nightingale, & Petrie,
2010) on a collection of CSP instances;
 MAXSAT12-PMS: runtime data from the 2012 MaxSAT Evaluation;
 PREMARSHALLING: runtimes of A -based and IDA -based solvers for the premarshalling problem, on real-world, time-sensitive pre-marshalling problem instances
from the operations research literature;
 PROTEUS-2014: runtimes of different CSP and SAT solvers on a range of CSP
instances, preprocessed with various CSP-to-SAT translation techniques;
 QBF-2011: runtime data for the QBF solvers from the AQME system (Pulina &
Tacchella, 2009) on QBF instances from the 2010 QBF Solver Evaluation;
 SAT11-HAND, SAT11-INDU and SAT11-RAND: runtime data from the respective tracks of the 2011 SAT Competition;
 SAT12-ALL, SAT12-HAND, SAT12-INDU and SAT12-RAND: runtimes of various SAT solvers on a broad range of SAT instances used to train the algorithm
selection system SATzilla (Xu, Hutter, Shen, Hoos, & Leyton-Brown, 2012b) for
the respective tracks of the 2012 SAT Challenge.
We refer to Bischl et al. (2015b) for further details on these scenarios, including baseline
experiments showing that algorithm selection can be applied effectively to all these scenarios. We point out that using this common library allows us to compare AutoFolio in a
fair and uniform way against other algorithm selection methods. However, the price we pay
for this uniform comparison is that we do not necessarily consider current state-of-the-art
algorithms for solving the respective problems, since some of the ASlib data was collected
several years ago. Furthermore, we note that the current version of ASlib only consists of
deterministic performance data. We expect that future versions will also consider scenarios with stochastic performance data and multiple runs per algorithm and instance, using
different pseudo-random number seeds. AutoFolio can be applied to such stochastic scenarios in a straightforward manner, by optimizing mean performance across all runs for the
same instance
contain all necessary performance data and feature vectors (in order to allow for a fair comparison of
algorithm selectors based on the same data, without confounding factor due to the hardware platform
used to run experiments).

759

fiLindauer, Hoos, Hutter, & Schaub

Scenario
ASP-POTASSCO
CSP-2010
MAXSAT12-PMS
PREMARSHALLING
PROTEUS-2014
QBF-2011
SAT11-HAND
SAT11-INDU
SAT11-RAND
SAT12-ALL
SAT12-HAND
SAT12-INDU
SAT12-RAND

#I
1294
2024
876
527
4021
1368
296
300
600
1614
767
1167
1362

#U #A #f #fg
82
253
129
0
428
314
77
47
108
20
229
209
322

11
2
6
4
22
5
15
18
9
31
31
31
31

138
86
37
16
198
46
115
115
115
115
115
115
115

4
1
1
1
4
1
10
10
10
10
10
10
10

tc
600
5000
2100
3600
3600
3600
5000
5000
5000
1200
1200
1200
1200

Reference
(Hoos et al., 2014)
(Gent et al., 2010)
(Malitsky et al., 2013)
(Tierney & Malitsky, 2015)
(Hurley et al., 2014)
(Pulina & Tacchella, 2009)
(Xu et al., 2008)
(Xu et al., 2008)
(Xu et al., 2008)
(Xu et al., 2012b)
(Xu et al., 2012b)
(Xu et al., 2012b)
(Xu et al., 2012b)

Table 1: Overview of algorithm selection scenarios in the Algorithm Selection Library,
showing the number of instances #I, number of unsolvable instances #U (U  I), number
of algorithms #A, number of features #f , number of feature groups #fg , cutoff time tc
and literature reference.

4.1 Algorithm Configuration Setup
Following standard practice (Hutter et al., 2009), we performed multiple (in our case, 12)
independent runs of the algorithm configurator SMAC for each scenario and then selected
the configuration of claspfolio 2 with the best performance on training data. Each configurator run was allocated a total time budget of 2 CPU days. A single run of claspfolio 2
was limited to 1 CPU hour, using the runsolver tool (Roussel, 2011). As a performance
metric, we used penalized average runtime with penalty factor 10 (PAR10), which counts
each timeout as 10 times the given runtime cutoff (runtime cutoffs differ between the ASlib
scenarios). We further study how the optimization of PAR10 influenced other metrics, such
as the number of timeouts. The time required to evaluate a single configuration of claspfolio 2 varied between 10 CPU seconds and 1 CPU hour on our reference machine (see
below), mostly depending on the difficulty of optimizing pre-solving schedules.
To obtain a robust estimate of AutoFolios performance, we used 10-fold outer crossvalidation as given in the specific ASlib scenarios, that is, we configured claspfolio 2 ten
times for each scenario (with different training-test splits). Therefore, in total, we performed
12  10 = 120 configuration runs of 2 CPU days each for three different configuration spaces
(see Section 4.2) and each of the thirteen ASlib benchmarks, requiring a total of 9 360
CPU days (25 CPU years). We note that although our thorough evaluation of AutoFolio
required substantial amounts of computation, applying it to a single benchmark set with
a given training-test split would only require 12 independent configuration runs of two
days each and could thus be performed over the weekend on a modern desktop machine.
Furthermore, applying AutoFolio to a new algorithm selection benchmark set is cheap
in comparison to collecting the data for a new benchmark set. For instance, to collect
760

fiAutoFolio: An Automatically Configured Algorithm Selector

AutoFoliovote
AutoFolio
AutoFolioext

categorical

integer

real

conditionals

configurations

18  28
28  38
47  247

7
15
15

3
15
15

14
44
44

1  106  6  108
3  1011  2  1014
2  1017  2  1077

Table 2: Overview of configuration spaces with the number of categorical, integer-valued
and real-valued parameters, the number of conditionals, and an estimation of the number
of configurations by ignoring the real-valued parameters. The number of categorical values
varies between the scenarios depending on the number of algorithms, features and feature
groups.
the algorithm performance data for the ASlib scenarios required between 25.7 CPU days
(ASP-POTASSCO) and 596.7 CPU days (PROTEUS-2014), with an average of 212.3
CPU days (9 times as much as our configuration budget for AutoFolio).
We performed these experiments on the bwUniCluster in Karlsruhe, whose machines
are equipped with two Octa-Core Intel Xeon E5-2670 (2.6 GHz, 20 MB cache) CPUs and
64 GB RAM each, running Hat Enterprise Linux 6.4. We note, however, that the runtimes
of the selected algorithms and feature computations are part of the ASlib scenarios and do
not depend on the hardware we used.
4.2 Different Configuration Spaces
As mentioned earlier, AutoFolio can be used to optimize the performance of single approach algorithm selectors, such as SATzilla, or multi-approach selectors, such as LLAMA
or claspfolio 2, with much larger configuration spaces (see Figure 6). Therefore, we studied three different parameter spaces of AutoFolio based on claspfolio 2:
AutoFolio considers the configuration space described in Section 3.2 and additionally
adds binary parameters that enable or disable feature groups4 that are defined by
the specific algorithm selection scenario. Algorithm subset selection is done using a
heuristic based on the marginal contribution of each algorithm to the oracle performance;
AutoFoliovote considers only a subset of the configuration space of AutoFolio, that
fixes the algorithm selection approach to pairwise classification with a voting scheme;
AutoFolioext considers the same configuration space as AutoFolio, but instead of parameters for each feature group, we added binary parameters for each instance feature
and for each selectable algorithm. This increases the number of parameters substantially  for example, it adds 220 additional parameters for the PROTEUS-2014
scenario.

4. If the selected feature groups result in an empty feature set, claspfolio 2 will statically select the single
best algorithm on training data.

761

fiLindauer, Hoos, Hutter, & Schaub

Scenario
ASP-POTASSCO
CSP-2010
MAXSAT12-PMS
PREMARSHALLING
PROTEUS-2014
QBF-2011
SAT11-HAND
SAT11-INDU
SAT11-RAND
SAT12-ALL
SAT12-HAND
SAT12-INDU
SAT12-RAND

Default
PAR10 #TOs
124.8
384.7
264.0
2513.8
3274.2
1068.4
7093.2
7851.2
3684.0
2087.0
2081.2
1019.8
708.2

19
10
7
33
321
26
29
37
34
261
86
69
52

AutoFoliovote
PAR10
#TOs
119.8
329.7
135.7
1953.6
1274.0
866.6
5781.4
6616.5
1441.9
890.4
1079.5
682.9
391.6

18
8
3
24
110
19
23
31
12
102
43
44
29

AutoFolio
PAR10
#TOs
125.0
355.1
246.3
2005.1
1379.2
910.2
5552.8
5932.3
967.4
979.1
1212.3
774.6
440.8

19
9
7
25
117
21
22
27
7
115
49
52
33

AutoFolioext
PAR10
#TOs
152.7
358.1
268.2
1922.5
3102.7
946.9
8085.8
7671.3
1301.7
1077.0
1285.5
990.7
543.1

25
9
8
24
280
22
33
36
10
126
52
67
41

Table 3: Comparing different configuration spaces of AutoFolio based on test performance. The best performance is shown in bold face;  and  indicate performance significantly better than that of the default configuration of claspfolio 2 at significance levels
 = 0.05 and  = 0.1, respectively, according to a one-sided permutation test with 100 000
permutations. Performances values that, according to the permutation test, are not significantly worse (at  = 0.05) than the best performance for a given scenario are marked with
.

We fixed the selection approach in AutoFoliovote to pairwise classification with a
voting scheme, since SATzilla11-like was the most promising single approach in our experiments (see, e.g., Figure 1). On the other hand, the extended configuration space,
AutoFolioext , was obtained by adding algorithm subset selection and feature selection to
the configuration task. Feature selection is well known to improve many machine learning
models, and often only a small subset of instance features is necessary to predict the runtime
of algorithms (Hutter, Hoos, & Leyton-Brown, 2013).
We note that each configuration in AutoFoliovote can also be found in AutoFolio,
and each configuration of AutoFolio is also part of AutoFolioext , that is, AutoFoliovote
 AutoFolio  AutoFolioext . Table 2 gives an overview of the configuration space sizes.
4.3 Analysis Of Configuration Process
In Table 3, we compare the performance of the default configuration of claspfolio 2
(namely, SATzilla11-like) with that of the configurations optimized by AutoFoliovote ,
AutoFolio and AutoFolioext . For all selection scenarios, AutoFoliovote improved
performance on test data in comparison to the default configuration of claspfolio 2.
AutoFolio improved on all but one scenario and AutoFolioext on all but three scenarios.
Performance improvements on test data were statistically significant at  = 0.1 and  = 0.05
for ten and seven scenarios for AutoFoliovote , for nine and seven for AutoFolio, and
five and four for AutoFolioext , respectively, according to a one-sided permutation test
with 100 000 permutations.
762

fiAutoFolio: An Automatically Configured Algorithm Selector

On 11 of the 13 ASlib scenarios, configuration in at least one of the configuration
spaces we considered led to statistically significant improvements ( = 0.1); we now discuss
the remaining two scenarios, ASP-POTASSCO and CSP-2010. On ASP-POTASSCO,
performance improved substantially on the training data (AutoFolio reduced the PAR10
score by  30%), but this did not transfer to test data (with none of the differences between
test performances being statistically significant). We note that the default configuration
of claspfolio 2 was manually optimized on this scenario (Hoos et al., 2014), and that
AutoFolio found very similar configurations with very similar performance. On CSP2010, all AutoFolio variants improved over the default, but only insignificantly so. We
note that it is hard to improve performance substantially on this benchmark, which only
contains two algorithms.
On PREMARSHALLING, AutoFolio solved 8 additional problem instances and reduced PAR10 by more than 25%; nevertheless, this performance difference was only weakly
significant (at  = 0.1). This is due to the strong constraints on the pre-solving schedule
in the default configuration of claspfolio 2 (at most 3 solvers for at most 256 seconds).
While more extensive pre-solving schedules decreased the number of timeouts on PREMARSHALLING, they also introduced overhead on many of the other instances in this
scenario, making it harder for AutoFolio to achieve more significant performance improvements. The scatter plot of Figure 7a shows that AutoFolio produced fewer timeouts
than default claspfolio 2, but AutoFolio required higher runtime on some other instances (points above the diagonal). Similarly, AutoFolio solved a lot more instances on
PROTEUS-2014 and some more on QBF-2011, but AutoFolio had a higher runtime
on some other instances (see Figure 7c and 7b). However, the number of timeouts improved so much on PROTEUS-2014 (from 321 to 117) that the performance improvement
was statistically significant here. Finally, SAT12-ALL is an example of a more clear-cut
case: AutoFolio improved the performance of claspfolio 2 on most instances and also
substantially reduced the number of timeouts (see Figure 7d).
Overall, AutoFoliovote performed best in these experiments, followed by AutoFolio,
and with some distance, AutoFolioext . With respect to statistical significance, AutoFoliovote and AutoFolio performed quite similarly, the former being better three times
and the latter being better once. Based on the results, we suspect that the added flexibility
in AutoFolio as compared to AutoFoliovote pays off when the configuration budget
is large enough to evaluate enough configurations to effectively search its larger space.
This was the case for the three SAT11 scenarios, for which AutoFolio reached the best
performance: these scenarios only contain relatively few problem instances, making each
evaluation of claspfolio 2 quite fast and allowing SMAC to evaluate about 40 000 configurations within 2 days. In contrast, an evaluation of a configuration on the largest ASlib
scenario, PROTEUS-2014, can cost up to an hour, and SMAC evaluated only about 600
configurations, which was not enough to explore the design space of AutoFolio; accordingly, the performance of AutoFolioext on PROTEUS-2014 improved only slightly in
comparison to the default configuration, while AutoFoliovote made progress faster and
performed statistically significantly better than AutoFolio. Therefore, we believe that
AutoFolio is a good choice when we can evaluate many configurations, be it because the
scenario is small or because a large configuration budget is available. On the other hand,
763

fiLindauer, Hoos, Hutter, & Schaub

100x

10x

2x

100x

10x

2x

2x

2x

1000

1000
10x

10x

100

100
100x

Configured

Configured

100x

10

10

1

1

0.1

0.1

0.01
0.01

0.1

1

10
Default

100

0.01
0.01

1000

0.1

1

10
Default

100

1000

(a) PREMARSHALLING. Number of timeouts (b) QBF-2011. Number of timeouts reduced
reduced from 33 (default) to 25 (configured).
from 26 (default) to 21 (configured).
100x

10x

2x

100x

1000

2x

10x

2x
2x

1000
10x

100

10x

10

100x

100

Configured

Configured

100x

10

1

1

0.1

0.1
0.01
0.01

0.1

1

10
Default

100

0.01
0.01

1000

0.1

1

10
Default

100

1000

(c) PROTEUS-2014. Number of timeouts
(d) SAT12-ALL. Number of timeouts reduced
reduced from 321 (default) to 117 (configured). from 261 (default) to 115 (configured).

Figure 7: Scatter plots comparing the per-instance performance of default claspfolio 2
(SATzilla11-like) and AutoFolio. Left: On PREMARSHALLING, AutoFolio improved penalized average runtime (PAR10) by reducing the number of timeouts, at the
cost of increased runtimes on many other instances. Right: on SAT12-ALL, AutoFolio
improved performance on most instances and also reduced the number of timeouts.

AutoFoliovote should be used on larger scenarios or when the configuration budget is
quite small.
Figure 8 shows the progress of the configuration process in terms of training performance
as a function of time for SAT11-HAND and PROTEUS-2014, as the scenarios with the
764

fiAutoFolio: An Automatically Configured Algorithm Selector

8000
3000
Performance

PAR10

7000
6000
5000

Autofolio
Autofolio_ext
Autofolio_vote

4000
29

210

211

212

213 214
Time (s)

2500
2000

Autofolio
Autofolio_ext
Autofolio_vote

1500
1000
215

216

217

29

210

(a) SAT11-HAND

211

212

213 214
Time (s)

215

216

217

(b) PROTEUS-2014

Figure 8: The training PAR10 performance of the best configuration over time. The line
shows the median over the 10 folds of the outer cross-validation and the filled area indicates
performance between the 25 and 75-quantile.

most and the fewest configuration evaluations performed in the fixed configuration budget.
For both scenarios, the very large configuration space of AutoFolioext resulted in a period
of stagnation before performance improved. On PROTEUS-2014, the performance started
to improve only near the end of the configuration budget. In contrast, AutoFolio and
AutoFoliovote performed quite similarly on both scenarios, with AutoFoliovote being
somewhat faster to make progress (note the logarithmic time axis). Surprisingly to us, very
different selection approaches were chosen for AutoFolio and AutoFoliovote . Because
of its restricted configuration space, AutoFoliovote had to choose pairwise classification
with a voting scheme, but AutoFolio also used other approaches for some outer folds of
these scenarios: regression (2 times in each of the two scenarios), clustering (1 and 3 times,
resp.) and SNNAP (3 and 4 times, resp.).
From Figure 8, we can also estimate the influence of the configuration budget on the
performance of our final algorithm selector. For example, if we halve the configuration time
budget to 1 day, the penalized average runtime on the training set only increases by about
8%.
4.4 Which Choices Lead To Good Performance?
To analyze which choices were most important in AutoFolio, we applied two complementary methods for assessing parameter importance in algorithm configuration spaces:
functional ANOVA (Hutter, Hoos, & Leyton-Brown, 2014, 2015b) for a global measure of
parameter importance and ablation analysis (Fawcett & Hoos, 2015b, 2015a) for a local
measure. For a high-level overview of the parameters in AutoFolio, we refer back to
Section 3.2; full details, including the default values and ranges of all parameters, are given
in an online appendix available at www.ml4aad.org/autofolio.
765

fiLindauer, Hoos, Hutter, & Schaub

4.4.1 Functional ANOVA (fANOVA)
Functional ANOVA (fANOVA, see, e.g., Sobol, 1993) is a general method for partitioning the
variance of a function into components corresponding to subsets of its arguments. Hutter
et al. (2014) demonstrated that this technique can be applied to efficiently quantify the
importance of an algorithms parameters. Their approach can re-use the performance data
collected during the configuration process for this purpose (without requiring new algorithm
executions) and is therefore computationally very efficient (in our experiments, it required
minutes). The overall approach is to fit an empirical performance model (Hutter, Xu, Hoos,
& Leyton-Brown, 2014) m : C  I  R to the measured performance data, which can be
used to predict performance for arbitrary configurations and instances, and to then study
parameter importance in that model. After fitting that model, fANOVA marginalizes it
across problem instances:
1 X
f(c) =

m(c, i).
(4)
|I|
iI

It then computes the variance of the function f across the entire configuration space C
and partitions this variance into additive components due to each subset of the algorithms
parameters. Of particular interest are unary subsets, which often explain a substantial part
of the variance and tend to be easiest to interpret. It is important to note that fANOVA
partitions the variance of f over the entire configuration space. While this provides a
global measure of parameter importance, it takes into account many poorly-performing
configurations.
To use fANOVA in the context of our study, for each ASlib scenario, we merged the
performance data from 12 independent SMAC runs and removed all data points that reported a timeout5 or that resulted in an empty feature set. We did the latter, because in
this case claspfolio 2 statically selects the single best solver, causing most parameters to
become unimportant for the performance of claspfolio 2.
For brevity, we only report results for scenario SAT12-ALL. Table 4 shows the ten
most important parameters of AutoFolio and AutoFolioext for this scenario. In both
configuration spaces, the maximal time spent to compute the instance features (max-featuretime) turned out to be the most important parameter. This parameter is so important,
because setting it too small will result in too few features (or even none, disabling the
selection mechanism) and setting it too large will lead to an increased overhead in feature
computation (see Figure 9).
The second most important parameter of AutoFolio was the marginal contribution
filtering as a heuristic for algorithm subset selection. Algorithm subset selection is especially important for the AS scenarios based on SAT solving, because they include many SAT
solvers and because the performance of these solvers is often highly correlated (Xu et al.,
2012a). For AutoFolioext , the contribution filtering heuristic is less important, because
the configuration space includes binary parameters for each individual algorithm, allowing
the configurator (here SMAC) to directly perform subset selection. In this context, including mphaseSATm and marchrw is of special importance. The solver mphaseSATm is the
single best algorithm on SAT12-ALL and has one of the highest marginal contributions to
5. We only observed timeouts for a particular configuration on the larger data sets: the clustering approach
with spectral clustering.

766

fiAutoFolio: An Automatically Configured Algorithm Selector

Parameter

Main Effect

max-feature-time
contr-filter
approach
feature-step:CG
pre-solving
impute
perf:transformation
time-pre-solving
feature:normalization
pre-solving:max-solver

Parameter

23.43%  2.05
6.82%  2.30
6.39%  0.63
0.76%  0.09
0.69%  0.09
0.29%  0.06
0.26%  0.05
0.22%  0.06
0.06%  0.01
0.05%  0.01

Main Effect

max-feature-time
approach
pre-solving
contr-filter
algorithms:mphaseSATm
imputation
F:algorithms:marchrw
time-pre-solving
pre-solving:sec mode
perf:transformation

(a) AutoFolio

11.07%  5.32
5.90%  4.40
1.29%  1.61
0.80%  0.92
0.72%  0.22
0.69%  0.27
0.30%  0.18
0.23%  0.41
0.11%  0.24
0.11%  0.04

(b) AutoFolioext

Table 4: Average main effects ( stdev) over outer cross-validation splits of the ten most
important claspfolio 2 parameters on SAT12-ALL according to fANOVA.

Marginal PAR10 Scores

5851

4475

30920

100 200 300 400 500 600
max-feat-time [sec]

Figure 9: Marginal performance predictions for parameter max-feature-time on the data of
one outer fold in the configuration space of AutoFolio. The blue line indicates the mean
of the predicted marginal performance and the red area its standard deviation.
the oracle. Similarly, marchrw has a high marginal contribution and is the only algorithm
whose performance is not highly correlated with that of another solver (see the exploratory
data analysis by Bischl et al., 2015b).
We note once again that this analysis determines global parameter importance with
respect to the entire parameter space. For example, the importance of the maximal feature
computation time is mostly so high, not because it is crucial to change it to improve the
performance of claspfolio 2, but because the configuration space contains settings that
will drastically worsen its performance. To gain complementary insights about which parameters should be changed to improve performance, we next performed ablation analysis.6
6. We note that fANOVA can also be used to yield a more local analysis of parameter importance by partitioning the variance of performance in high-performance regions of a given configuration space (Hutter

767

fi2000

2200

1800

2000

1600

1800

1400

1600
PAR10

PAR10

Lindauer, Hoos, Hutter, & Schaub

1200
1000

1400
1200

800

1000

600

800

400
ute ime lter lize :sp ans opt :CG eaf aps sic res jois
impture-t ontr-finorma -stepsnce_trspeed--steps ples_pl s:ls_s eps:Ba_featups:lob
a
c
ure ma a ature n_same-ste ure-st f-max re-ste
-fe
featperfor
fe f-mi atur feat ing:r eatu
max
vot f
ng:r fe
i
t
o
v

600

e e t e s s s p f s r c
s:CG put tim -op aliz ran joi sap s:s lea ure filte asi
tep im ature- speed normance_teps:lobps:ls_re-stepmples_x_featcontr- teps:B
s
e
t te
-s
a a
ur
-fe a
form re-s e-s atu n_s f-m eature
feat
max
per featu featur fge:rf-mioting:r
f
v
n
voti

(a) 2nd outer fold

(b) 7th outer fold

Figure 10: Ablation paths on two outer-folds of SAT12-ALL. In (a), the most important
parameter is impute and feature-step:CG has a smaller effect. In (b), feature-step:CG is the
most important parameter and impute has no effect on the performance.

4.4.2 Ablation Analysis
Ablation analysis provides an answer to the question Which changes in parameter values
from one configuration to another caused the biggest improvement in performance?. It
does so by iteratively changing the parameter value with the largest impact on performance
on a path between two given configurations, e.g., the default configuration of an algorithm
and an optimized configuration. Unlike fANOVA, ablation analysis does not attempt to
summarize parameter importance across an entire configuration space, but focusses locally
on paths between configurations of interest. The results obtained from ablation analysis
are therefore complementary to those from fANOVA. Unfortunately, ablation is costly,
since it requires new algorithm runs to assess the performance of configurations on the path
between the two given configurations. For our AutoFolio experiments on SAT12-ALL,
we allocated a time budget of 6 days  the maximum wall-clock time permitted for jobs
on our cluster  for ablation on each of our 10 outer cross-validation folds, and within that
budget, obtained results for 6 of those.
Our ablation results indicate that feature-step:CG  a Boolean parameter that enables or
disables the computation of clause graph features  is the single most important parameter
to change from claspfolio 2s default. By default, feature-step:CG was activated, but
it turns out that the clause graph features are often too expensive to compute within the
time we allow for feature computation. Therefore, it was indeed a good decision by the
configuration procedure to deactivate this optional feature computation step. According to
our ablation results, this was done in 5 out of 6 outer cross-validation folds and, on average,
on these 5 folds, it was responsible for 99% of the performance improvements achieved
et al., 2014); here, we did not do this, since we used ablation analysis to study parameter importance
locally.

768

fiAutoFolio: An Automatically Configured Algorithm Selector

by configuration (standard deviation 37%7 ). In contrast, as seen in our fANOVA results,
feature-step:CG is quite unimportant globally, with a main effect of only 0.76%. The second
most important parameter to change was the activation of feature imputation (impute); on
average, this was responsible for 39% of the overall performance improvement (standard
deviation 56%) and was made in all 6 outer cross-validation folds we analyzed.8 However,
impute only had an effect on the performance if feature-step:CG was not deactivated before
impute was changed in the ablation path. This was only the case in 2 out of the 6 ablation
paths (e.g., see Figure 10a) and hence, impute had no impact on performance for the other 4
paths (e.g., see Figure 10b). These two parameters have dependent effects, since imputation
is particularly important if clause graph features are computed: these features time out for
many large instances and thus require imputation.
The globally most important parameter, according to fANOVA, max-feature-time, was
found to be rather unimportant to change from its default value. The parameter was
changed between the default and optimized configuration in all outer folds of SAT12-ALL,
but  since the default value already was very good  on average only 2% of the overall performance improvement could be attributed to this change. We note that along the
Ablation path, max-feature-time was never flipped to a value that resulted in worse performance than the default configuration, while many such such poorly-performing values
exist and explain the globally high importance of this parameter.
4.5 Comparison Against Other Algorithm Selectors
In Table 5, we compare AutoFolio with SATzilla159 (Xu et al., 2011), SNNAP (version 1.4; Collautti et al., 2013) and ISAC (implementation in SNNAP 1.4; Kadioglu
et al., 2010).10 We note that ISAC and SNNAP are pure algorithm selectors, whereas
SATzilla15 and claspfolio 2 can additionally use pre-solver schedules. Furthermore,
we added a nave approach, RandSel, by simulating an uninformed user who selects uniformly at random between SNNAP, ISAC and SATzilla15. Overall, AutoFolio performed best for 7 out of the 13 scenarios and was statistically indistinguishable from the
best system for all other scenarios, according to a one-sided permutation test with 100 000
permutations and significance level  = 0.05. Therefore, AutoFolio is the only system
that achieves state-of-the-art performance for all scenarios.
SATzilla15 performed second best, but yielded statistically significantly worse performance than AutoFolio on 5 of the 13 scenarios. Even though not statistically significant,
SATzilla15 performed slightly better than AutoFolio on 5 scenarios. The reason for
7. This large standard deviation arises from the fact that in some folds, the parameter change was actually
responsible for more than 100% of the performance difference: in those folds, this change alone would
have sufficed to achieve better performance than the optimized configuration.
8. The sum of the relative performance of a subset of parameter improvements is not limited to 100%, since
it was computed relative to the difference between the default and the optimized configuration. In 5 out
of the 6 ablation paths, some parameter changes lead to a better performance than the final optimized
configuration, and some parameter changed worsened the performance again.
9. Alexandre Frechette, the current main developer of SATzilla, provided an internal new implementation
of SATzilla (version 0.9.1b-count-cheap-feat-12) that is no longer limited to SAT.
10. Other state-of-the-art selectors, such as 3S (Kadioglu et al., 2011) and CSHC (Malitsky et al., 2013a),
are not publicly available with their training procedures, and we were therefore unable to train them on
our scenarios.

769

fiLindauer, Hoos, Hutter, & Schaub

ASP-POTASSCO
CSP-2010
MAXSAT12-PMS
PREMARSHALLING
PROTEUS-2014
QBF-2011
SAT11-HAND
SAT11-INDU
SAT11-RAND
SAT12-ALL
SAT12-HAND
SAT12-INDU
SAT12-RAND

Oracle

SB

SNNAP

ISAC

21.3
107.7
40.7
227.6
26.3
95.9
478.3
419.9
227.3
93.7
113.2
88.1
46.9

534.1
1087.4
2111.6
7002.9
10756.3
9172.3
17815.8
8985.6
14938.6
2967.9
3944.2
1360.6
568.5

203.8
1087.5
895
9042.1
4058.7
7386.2
9209.3
6632.6
4859
1427.5
2180.5
789
593.1

291.9
1027
786.4
5880.8
3328
3813.5
13946.2
8461.2
3140.4
2989.3
4110.8
1409.5
434.5

SATzilla15 RandSel AutoFolio
170
276
166.8
3179.1
2050.3
1245.2
6211.5
8048.8
877.5
876.9
1031.5
839.7
485.3

221.6
796.8
615.6
6034
3145.6
4148.3
9789
7714.2
2958.9
1764.5
2440.9
1012.7
504.3

125
355
246.3
2005.1
1379.2
910
5552.7
5932.3
967
979
1212
774.6
440

Table 5:
Performance comparison between AutoFolio, SNNAP, ISAC, and
SATzilla15, as well as the single best solver (SB, selected based on PAR10 on the training
set) as a baseline, and the oracle (also known as virtual best solver) as a bound on the optimal performance of an algorithm selector. We show PAR10 scores averaged over 10 outer
cross-validation folds, with instances not solved by any solver removed from the test set
to avoid artificially inflating the PAR10-scores. The RandSel column shows the expected
performance by picking uniformly at random one of SNNAP, ISAC and SATzilla15.
The best performance is shown in bold face. All performance values that are not statistically significantly better than the best-performing system for a given scenario, according to
a one-sided permutation test with 100 000 permutations and a significance level  = 0.05,
are marked with .
this might be that SATzilla15 performs an extensive search to determine the best combination of pre-solving schedule (grid search), algorithm subset (iterated local search) and
trained selection model.
We note that, in order to compensate for the 24 CPU days spent to find a well-performing
configuration of AutoFolio, compared to simply using the single best solver, on average
across all scenarios AutoFolio would have to consecutively solve instances for 42 CPU
days (standard deviation 23), less than two times the configuration budget.
Although AutoFolio improved substantially over the single best solver (SB) on all
scenarios (up to a speedup factor of 15.4 on SAT11-RAND), there is still a gap to the
Oracle performance (also known as virtual best solver in the SAT community). This
gap could be closed further in at least two ways: (i) using a larger configuration budget
for AutoFolio, or (ii) by developing better instance features, which are the basis for all
algorithm selection methods.

5. Conclusions
We presented AutoFolio  to the best of our knowledge, the first approach to automatically configuring algorithm selectors. Using a concrete realization of this approach based
on the highly parameterized algorithm selection framework claspfolio 2, we showed that
by using state-of-the-art algorithm configurators, algorithm selectors can be customized to
770

fiAutoFolio: An Automatically Configured Algorithm Selector

robustly achieve peak performance across a range of algorithm selection scenarios. The
resulting approach performs significantly (and sometimes substantially) better than manually configured selectors and can be applied out-of-the-box to previously unseen algorithm
selection scenarios.
In comprehensive experiments with the 13 algorithm selection scenarios from different
domains (SAT, Max-SAT, CSP, ASP, QBF, and container pre-marshalling) that make up the
algorithm selection library ASlib, our concrete realization AutoFolio outperformed the
best single solver for each selection benchmark by factors between 1.3 and 15.4 (geometric
average: 3.9) in terms of PAR10 scores. Overall, AutoFolio established improved stateof-the-art performance on 7 out of 13 scenarios and performed on par with the previous
state-of-the-art approaches on all other scenarios; overall, it clearly yielded the most robust
performance across our diverse set of benchmarks.
We also studied the effect of different configuration spaces. Here, we showed that the
medium-size configuration space of AutoFolio can lead to state-of-the-art performance
if the configuration budget allows the evaluation of sufficiently many configurations. In
contrast, if the selection scenario is large (in terms of number of algorithms and problem
instances), or if the configuration budget is limited, configuration in a more constrained
space, as used in AutoFoliovote , typically leads to better performance.
The performance of AutoFoliovote was independently verified in the ICON Challenge
on Algorithm Selection (Kotthoff, 2015), which evaluated 8 different AS systems with a
small configuration budget of 12 CPU hours with respect to three metrics: PAR10 score,
number of instances solved and misclassification penalty. As throughout this paper, the
metric we optimized in AutoFolio was PAR10 score, and AutoFolio ranked first with
respect to this metric. It also ranked first with respect to number of instances solved and
second with respect to misclassification penalty (leading to an overall second place).
In future work, we plan to investigate how the potential gains of larger configuration
spaces (including feature and algorithm subset selection) can be used more effectively. To
this end, we would like to (i) study performance with larger configuration budgets that
allow the configurator to assess more configurations; (ii) evaluate other algorithm configurators, such as irace (Lopez-Ibanez et al., 2011) and GGA (Ansotegui et al., 2009); (iii)
extend the configuration space of AutoFolio by implementing more algorithm selection
approaches (e.g., CSHC; Malitsky et al., 2013a); (iv) shrink the larger configuration space
based on the analysis of parameter importance through fANOVA (Hutter et al., 2014) and
Ablation (Fawcett & Hoos, 2015b), allowing the configurator to focus on the most important parameters; and (v) automatically select between pre-configured algorithm selectors,
based on features of a given algorithm selection scenario, and further improve performance
by starting automatic configuration from the configurations thus selected (Feurer, Springenberg, & Hutter, 2015). Another promising avenue for reducing the computational cost of our
approach would be to pre-select algorithms, features, and problem instances based on the
techniques proposed by Hoos et al. (2013) or based on the collaborative filtering approach
by Misir and Sebag (2013). Finally, we plan to investigate to which extent AutoFolio
can configure algorithm selection systems for selecting parallel portfolios (Lindauer et al.,
2015a) to exploit the increasing availability of parallel computing resources.
Overall, we believe that the automated configuration of algorithm selection systems improves the performance and versatility of those systems across a broad range of application
771

fiLindauer, Hoos, Hutter, & Schaub

domains. Our AutoFolio approach also facilitates future improvements, by making it easier to realize and assess the performance potential inherent in new design choices for the
various components of an algorithm selection system. Our open-source implementation of
AutoFolio is available at www.ml4aad.org/autofolio/.

Acknowledgements
M. Lindauer was supported by the DFG (German Research Foundation) under Emmy
Noether grant HU 1900/2-1 and project SCHA 550/8-3, H. Hoos by an NSERC Discovery
Grant, F. Hutter by the DFG under Emmy Noether grant HU 1900/2-1 and T. Schaub
by the DFG under project SCHA 550/8-3, respectively. This work was performed on the
computational resource bwUniCluster funded by the Ministry of Science, Research and Arts
and the universities of the State of Baden-Wurttemberg, Germany, within the framework
program bwHPC.

References
Abrame, A., & Habet, D. (2014). On the extension of learning for Max-SAT. In Endriss, U.,
& Leite, J. (Eds.), Proceedings of the 7th European Starting AI Researcher Symposium
(STAIRS14), Vol. 264 of Frontiers in Artificial Intelligence and Applications, pp. 1
10. IOS Press.
Amadini, R., Gabbrielli, M., & Mauro, J. (2014). SUNNY: a lazy portfolio approach for
constraint solving. Theory and Practice of Logic Programming, 14 (4-5), 509524.
Ansotegui, C., Malitsky, Y., & Sellmann, M. (2014). Maxsat by improved instance-specific
algorithm configuration. In Brodley, C., & Stone, P. (Eds.), Proceedings of the Twentyeighth National Conference on Artificial Intelligence (AAAI14), pp. 25942600. AAAI
Press.
Ansotegui, C., Sellmann, M., & Tierney, K. (2009). A gender-based genetic algorithm
for the automatic configuration of algorithms. In Gent, I. (Ed.), Proceedings of the
Fifteenth International Conference on Principles and Practice of Constraint Programming (CP09), Vol. 5732 of Lecture Notes in Computer Science, pp. 142157. SpringerVerlag.
Bergstra, J., Bardenet, R., Bengio, Y., & Kegl, B. (2011). Algorithms for hyper-parameter
optimization. In Shawe-Taylor, J., Zemel, R., Bartlett, P., Pereira, F., & Weinberger,
K. (Eds.), Proceedings of the 25th International Conference on Advances in Neural
Information Processing Systems (NIPS11), pp. 25462554.
Biere, A. (2013). Lingeling, plingeling and treengeling entering the sat competition 2013.
In Balint, A., Belov, A., Heule, M., & Jarvisalo, M. (Eds.), Proceedings of SAT Competition 2013: Solver and Benchmark Descriptions, Vol. B-2013-1 of Department of
Computer Science Series of Publications B, pp. 5152. University of Helsinki.
Bischl, B., Kerschke, P., Kotthoff, L., Lindauer, M., Malitsky, Y., Frechette, A., Hoos, H.,
Hutter, F., Leyton-Brown, K., Tierney, K., & Vanschoren, J. (2015a). www.aslib.net.
772

fiAutoFolio: An Automatically Configured Algorithm Selector

Bischl, B., Kerschke, P., Kotthoff, L., Lindauer, M., Malitsky, Y., Frechette, A., Hoos,
H., Hutter, F., Leyton-Brown, K., Tierney, K., & Vanschoren, J. (2015b). Aslib: A
benchmark library for algorithm selection. Computing Research Repository (CoRR),
abs/1506.02465.
Chiarandini, M., Fawcett, C., & Hoos, H. (2008). A modular multiphase heuristic solver
for post enrolment course timetabling. In Proceedings of the Seventh International
Conference on the Practice and Theorysy of Automated Timetabling (PATAT08, pp.
18.
Collautti, M., Malitsky, Y., Mehta, D., & OSullivan, B. (2013). SNNAP: Solver-based
nearest neighbor for algorithm portfolios. In Blockeel, H., Kersting, K., Nijssen,
S., & Zelezny, F. (Eds.), Machine Learning and Knowledge Discovery in Databases
(ECML/PKDD13), Vol. 8190 of Lecture Notes in Computer Science, pp. 435450.
Springer-Verlag.
Dickerson, J., & Sandholm, T. (2015). Futurematch: Combining human value judgments
and machine learning to match in dynamic environments. In Bonet, B., & Koenig,
S. (Eds.), Proceedings of the Twenty-nineth National Conference on Artificial Intelligence (AAAI15), pp. 622628. AAAI Press.
Eggensperger, K., Feurer, M., Hutter, F., Bergstra, J., Snoek, J., Hoos, H., & Leyton-Brown,
K. (2013). Towards an empirical foundation for assessing Bayesian optimization of
hyperparameters. In NIPS Workshop on Bayesian Optimization in Theory and Practice.
Fawcett, C., & Hoos, H. (2015a). www.cs.ubc.ca/labs/beta/Projects/Ablation/.
Fawcett, C., & Hoos, H. (2015b). Analysing differences between algorithm configurations
through ablation. Journal of Heuristics, 128.
Feurer, M., Springenberg, T., & Hutter, F. (2015). Initializing Bayesian hyperparameter
optimization via meta-learning. In Bonet, B., & Koenig, S. (Eds.), Proceedings of the
Twenty-nineth National Conference on Artificial Intelligence (AAAI15), pp. 1128
1135. AAAI Press.
Gebser, M., Kaminski, R., Kaufmann, B., Ostrowski, M., Schaub, T., & Schneider, M.
(2011a). Potassco: The Potsdam answer set solving collection. AI Communications,
24 (2), 107124.
Gebser, M., Kaminski, R., Kaufmann, B., Schaub, T., Schneider, M., & Ziller, S. (2011b).
A portfolio solver for answer set programming: Preliminary report. In Delgrande, J.,
& Faber, W. (Eds.), Proceedings of the Eleventh International Conference on Logic
Programming and Nonmonotonic Reasoning (LPNMR11), Vol. 6645 of Lecture Notes
in Computer Science, pp. 352357. Springer-Verlag.
Gebser, M., Kaufmann, B., & Schaub, T. (2012). Conflict-driven answer set solving: From
theory to practice. Artificial Intelligence, 187-188, 5289.
Gent, I., Jefferson, C., Kotthoff, L., Miguel, I., Moore, N., Nightingale, P., & Petrie, K.
(2010). Learning when to use lazy learning in constraint solving. In Coelho, H., Studer,
R., & Wooldridge, M. (Eds.), Proceedings of the Nineteenth European Conference on
Artificial Intelligence (ECAI10), pp. 873878. IOS Press.
773

fiLindauer, Hoos, Hutter, & Schaub

Gomes, C., & Selman, B. (2001). Algorithm portfolios. Artificial Intelligence, 126 (1-2),
4362.
Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, P., & Witten, I. (2009). The
WEKA data mining software: An update. SIGKDD Explorations, 11 (1), 1018.
Hoos, H., Kaminski, R., Lindauer, M., & Schaub, T. (2015). aspeed: Solver scheduling via
answer set programming. Theory and Practice of Logic Programming, 15, 117142.
Hoos, H., Kaufmann, B., Schaub, T., & Schneider, M. (2013). Robust benchmark set selection for boolean constraint solvers. In Pardalos, P., & Nicosia, G. (Eds.), Proceedings
of the Seventh International Conference on Learning and Intelligent Optimization
(LION13), Vol. 7997 of Lecture Notes in Computer Science, pp. 138152. SpringerVerlag.
Hoos, H., Lindauer, M., & Schaub, T. (2014). claspfolio 2: Advances in algorithm selection
for answer set programming. Theory and Practice of Logic Programming, 14, 569585.
Huberman, B., Lukose, R., & Hogg, T. (1997). An economic approach to hard computational
problems. Science, 275, 5154.
Hurley, B., Kotthoff, L., Malitsky, Y., & OSullivan, B. (2014). Proteus: A hierarchical
portfolio of solvers and transformations. In Simonis, H. (Ed.), Proceedings of the
Eleventh International Conference on Integration of AI and OR Techniques in Constraint Programming (CPAIOR14), Vol. 8451 of Lecture Notes in Computer Science,
pp. 301317. Springer-Verlag.
Hutter, F., Babic, D., Hoos, H., & Hu, A. (2007). Boosting verification by automatic tuning
of decision procedures. In OConner, L. (Ed.), Formal Methods in Computer Aided
Design (FMCAD07), pp. 2734. IEEE Computer Society Press.
Hutter, F., Hoos, H., & Leyton-Brown, K. (2010). Automated configuration of mixed integer
programming solvers. In Lodi, A., Milano, M., & Toth, P. (Eds.), Proceedings of the
Seventh International Conference on Integration of AI and OR Techniques in Constraint Programming (CPAIOR10), Vol. 6140 of Lecture Notes in Computer Science,
pp. 186202. Springer-Verlag.
Hutter, F., Hoos, H., & Leyton-Brown, K. (2011). Sequential model-based optimization
for general algorithm configuration. In Coello, C. (Ed.), Proceedings of the Fifth
International Conference on Learning and Intelligent Optimization (LION11), Vol.
6683 of Lecture Notes in Computer Science, pp. 507523. Springer-Verlag.
Hutter, F., Hoos, H., & Leyton-Brown, K. (2014). An efficient approach for assessing
hyperparameter importance. In Xing, E., & Jebara, T. (Eds.), Proceedings of the 31th
International Conference on Machine Learning, (ICML14), Vol. 32, pp. 754762.
Omnipress.
Hutter, F., Hoos, H., & Leyton-Brown, K. (2015a). www.ml4aad.org/smac.
Hutter, F., Hoos, H., & Leyton-Brown, K. (2015b). www.ml4aad.org/fanova.
Hutter, F., Hoos, H., Leyton-Brown, K., & Stutzle, T. (2009). ParamILS: An automatic
algorithm configuration framework. Journal of Artificial Intelligence Research, 36,
267306.
774

fiAutoFolio: An Automatically Configured Algorithm Selector

Hutter, F., Hoos, H. H., & Leyton-Brown, K. (2013). Identifying key algorithm parameters
and instance features using forward selection. In Pardalos, P., & Nicosia, G. (Eds.),
Proceedings of the Seventh International Conference on Learning and Intelligent Optimization (LION13), Vol. 7997 of Lecture Notes in Computer Science, pp. 364381.
Springer-Verlag.
Hutter, F., Lindauer, M., Balint, A., Bayless, S., Hoos, H., & Leyton-Brown, K. (2015). The
Configurable SAT Solver Challenge (CSSC). Artificial Intelligence. under review.
Hutter, F., Xu, L., Hoos, H., & Leyton-Brown, K. (2014). Algorithm runtime prediction:
Methods and evaluation. Artificial Intelligence, 206, 79111.
Janota, M., Klieber, W., Marques-Silva, J., & Clarke, E. (2012). Solving QBF with counterexample guided refinement. In Cimatti, A., & Sebastiani, R. (Eds.), Proceedings
of the Fifteenth International Conference on Theory and Applications of Satisfiability Testing (SAT12), Vol. 7317 of Lecture Notes in Computer Science, pp. 114128.
Springer-Verlag.
Kadioglu, S., Malitsky, Y., Sabharwal, A., Samulowitz, H., & Sellmann, M. (2011). Algorithm selection and scheduling. In Lee, J. (Ed.), Proceedings of the Seventeenth International Conference on Principles and Practice of Constraint Programming (CP11),
Vol. 6876 of Lecture Notes in Computer Science, pp. 454469. Springer-Verlag.
Kadioglu, S., Malitsky, Y., Sellmann, M., & Tierney, K. (2010). ISAC - instance-specific
algorithm configuration. In Coelho, H., Studer, R., & Wooldridge, M. (Eds.), Proceedings of the Nineteenth European Conference on Artificial Intelligence (ECAI10),
pp. 751756. IOS Press.
Kotthoff, L. (2013). LLAMA: leveraging learning to automatically manage algorithms.
Computing Research Repository (CoRR), abs/1306.1031.
Kotthoff, L. (2014). Algorithm selection for combinatorial search problems: A survey. AI
Magazine, 4860.
Kotthoff, L. (2015). ICON Challenge on Algorithm Selection..
icon-fet.eu/challengeas.

http://challenge.

Kotthoff, L., Gent, I., & Miguel, I. (2012). An evaluation of machine learning in algorithm
selection for search problems. AI Communications, 25 (3), 257270.
Lim, B., van den Briel, M., Thiebaux, S., Backhaus, S., & Bent, R. (2015). HVAC-Aware
Occupancy Scheduling. In Bonet, B., & Koenig, S. (Eds.), Proceedings of the Twentynineth National Conference on Artificial Intelligence (AAAI15), pp. 679686. AAAI
Press.
Lindauer, M., Hoos, H., & Hutter, F. (2015a). From sequential algorithm selection to parallel
portfolio selection. In Dhaenens, C., Jourdan, L., & Marmion, M. (Eds.), Proceedings of the Nineth International Conference on Learning and Intelligent Optimization
(LION15), Lecture Notes in Computer Science, pp. 116. Springer-Verlag.
Lindauer, M., Hoos, H., Hutter, F., & Schaub, T. (2015b). Autofolio: Algorithm configuration for algorithm selection. In Proceedings of the Workshops at Twenty-nineth
National Conference on Artificial Intelligence (AAAI15).
775

fiLindauer, Hoos, Hutter, & Schaub

Lindauer, M., Hoos, H., & Schaub, T. (2015c). www.cs.uni-potsdam.de/claspfolio/.
Lopez-Ibanez, M., Dubois-Lacoste, J., Stutzle, T., & Birattari, M. (2011). The irace package,
iterated race for automatic algorithm configuration. Tech. rep., IRIDIA, Universite
Libre de Bruxelles, Belgium.
Lopez-Ibanez, M., & Stutzle, T. (2010). Automatic configuration of multi-objective ACO
algorithms. In Dorigo, M., M-Birattari, Caro, G. D., Doursat, R., Engelbrecht,
A. P., Floreano, D., Gambardella, L., Gro, R., Sahin, E., Sayama, H., & Stutzle,
T. (Eds.), Proceedings of the Seventh International Conference on Swarm Intelligence
(ANTS10), Lecture Notes in Computer Science, pp. 95106. Springer-Verlag.
Malitsky, Y., Mehta, D., & OSullivan, B. (2013). Evolving instance specific algorithm
configuration. In Helmert, M., & Roger, G. (Eds.), Proceedings of the Sixth Annual
Symposium on Combinatorial Search (SOCS14). AAAI Press.
Malitsky, Y., Sabharwal, A., Samulowitz, H., & Sellmann, M. (2012). Parallel SAT solver
selection and scheduling. In Milano, M. (Ed.), Proceedings of the Eighteenth International Conference on Principles and Practice of Constraint Programming (CP12),
Vol. 7514 of Lecture Notes in Computer Science, pp. 512526. Springer-Verlag.
Malitsky, Y., Sabharwal, A., Samulowitz, H., & Sellmann, M. (2013a). Algorithm portfolios
based on cost-sensitive hierarchical clustering. In Rossi, F. (Ed.), Proceedings of the
23rd International Joint Conference on Artificial Intelligence (IJCAI13), pp. 608
614.
Malitsky, Y., Sabharwal, A., Samulowitz, H., & Sellmann, M. (2013b). Boosting sequential solver portfolios: Knowledge sharing and accuracy prediction. In Pardalos, P.,
& Nicosia, G. (Eds.), Proceedings of the Seventh International Conference on Learning and Intelligent Optimization (LION13), Vol. 7997 of Lecture Notes in Computer
Science, pp. 153167. Springer-Verlag.
Maratea, M., Pulina, L., & Ricca, F. (2014). A multi-engine approach to answer-set programming. Theory and Practice of Logic Programming, 14, 841868.
Mascia, F., Lopez-Ibanez, M., Dubois-Lacoste, J., & Stutzle, T. (2014). Grammar-based
generation of stochastic local search heuristics through automatic algorithm configuration tools. Computers & OR, 51, 190199.
Misir, M., & Sebag, M. (2013). Algorithm selection as a collaborative filtering problem.
Tech. rep., INRIA & LRI, Universite Paris Sud XI.
OMahony, E., Hebrard, E., Holland, A., Nugent, C., & OSullivan, B. (2008). Using casebased reasoning in an algorithm portfolio for constraint solving. In Bridge, D., Brown,
K., OSullivan, B., & Sorensen, H. (Eds.), Proceedings of the Nineteenth Irish Conference on Artificial Intelligence and Cognitive Science (AICS08).
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel,
M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau,
D., Brucher, M., Perrot, M., & Duchesnay, E. (2011). Scikit-learn: Machine learning
in Python. Journal of Machine Learning Research, 12, 28252830.
Pulina, L., & Tacchella, A. (2009). A self-adaptive multi-engine solver for quantified boolean
formulas. Constraints, 14 (1), 80116.
776

fiAutoFolio: An Automatically Configured Algorithm Selector

Rice, J. (1976). The algorithm selection problem. Advances in Computers, 15, 65118.
Roussel, O. (2011). Controlling a solver execution with the runsolver tool. Journal on
Satisfiability, Boolean Modeling and Computation, 7, 139144.
Smith-Miles, K. (2008). Cross-disciplinary perspectives on meta-learning for algorithm
selection. ACM Computing Surveys, 41 (1).
Snoek, J., Larochelle, H., & Adams, R. P. (2012). Practical Bayesian optimization of machine learning algorithms. In Bartlett, P., Pereira, F., Burges, C., Bottou, L., &
Weinberger, K. (Eds.), Proceedings of the 26th International Conference on Advances
in Neural Information Processing Systems (NIPS12), pp. 29602968.
Sobol, I. (1993). Sensitivity estimates for nonlinear mathematical models. Mathematical
Modeling and Computational Experiment, 1 (4), 407414.
Tamura, N., Taga, A., Kitagawa, S., & Banbara, M. (2009). Compiling finite linear CSP
into SAT. Constraints, 14 (2), 254272.
Thornton, C., Hutter, F., Hoos, H., & Leyton-Brown, K. (2013). Auto-WEKA: combined
selection and hyperparameter optimization of classification algorithms. In I.Dhillon,
Koren, Y., Ghani, R., Senator, T., Bradley, P., Parekh, R., He, J., Grossman, R., &
Uthurusamy, R. (Eds.), The 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD13), pp. 847855. ACM Press.
Tierney, K., & Malitsky, Y. (2015). An algorithm selection benchmark of the container premarshalling problem. In Dhaenens, C., Jourdan, L., & Marmion, M. (Eds.), Proceedings of the Nineth International Conference on Learning and Intelligent Optimization
(LION15), Lecture Notes in Computer Science, pp. 1722. Springer-Verlag.
Vallati, M., Fawcett, C., Gerevini, A., Hoos, H., & Saetti, A. (2013). Automatic generation of
efficient domain-optimized planners from generic parametrized planners. In Helmert,
M., & Roger, G. (Eds.), Proceedings of the Sixth Annual Symposium on Combinatorial
Search (SOCS14). AAAI Press.
Xu, L., Hoos, H., & Leyton-Brown, K. (2010). Hydra: Automatically configuring algorithms for portfolio-based selection. In Fox, M., & Poole, D. (Eds.), Proceedings of the
Twenty-fourth National Conference on Artificial Intelligence (AAAI10), pp. 210216.
AAAI Press.
Xu, L., Hutter, F., Hoos, H., & Leyton-Brown, K. (2008). SATzilla: Portfolio-based algorithm selection for SAT. Journal of Artificial Intelligence Research, 32, 565606.
Xu, L., Hutter, F., Hoos, H., & Leyton-Brown, K. (2011). Hydra-MIP: Automated algorithm configuration and selection for mixed integer programming. In RCRA workshop
on Experimental Evaluation of Algorithms for Solving Problems with Combinatorial
Explosion at the International Joint Conference on Artificial Intelligence (IJCAI).
Xu, L., Hutter, F., Hoos, H., & Leyton-Brown, K. (2012a). Evaluating component solver
contributions to portfolio-based algorithm selectors. In Cimatti, A., & Sebastiani,
R. (Eds.), Proceedings of the Fifteenth International Conference on Theory and Applications of Satisfiability Testing (SAT12), Vol. 7317 of Lecture Notes in Computer
Science, pp. 228241. Springer-Verlag.
777

fiLindauer, Hoos, Hutter, & Schaub

Xu, L., Hutter, F., Shen, J., Hoos, H., & Leyton-Brown, K. (2012b). SATzilla2012: improved
algorithm selection based on cost-sensitive classification models. In Balint, A., Belov,
A., Diepold, D., Gerber, S., Jarvisalo, M., & Sinz, C. (Eds.), Proceedings of SAT
Challenge 2012: Solver and Benchmark Descriptions, Vol. B-2012-2 of Department of
Computer Science Series of Publications B, pp. 5758. University of Helsinki.
Yun, X., & Epstein, S. (2012). Learning algorithm portfolios for parallel execution. In
Hamadi, Y., & Schoenauer, M. (Eds.), Proceedings of the Sixth International Conference on Learning and Intelligent Optimization (LION12), Vol. 7219 of Lecture Notes
in Computer Science, pp. 323338. Springer-Verlag.

778

fiJournal of Artificial Intelligence Research 53 (2015) 169-222

Submitted 11/14; published 06/15

Using Machine Translation to Provide Target-Language
Edit Hints in Computer Aided Translation Based on
Translation Memories
Miquel Espla-Gomis
Felipe Sanchez-Martnez
Mikel L. Forcada

mespla@dlsi.ua.es
fsanchez@dlsi.ua.es
mlf@dlsi.ua.es

Dept. de Llenguatges i Sistemes Informatics
Universitat dAlacant, E-03071 Alacant, Spain

Abstract
This paper explores the use of general-purpose machine translation (MT) in assisting
the users of computer-aided translation (CAT) systems based on translation memory (TM)
to identify the target words in the translation proposals that need to be changed (either
replaced or removed) or kept unedited, a task we term as word-keeping recommendation.
MT is used as a black box to align source and target sub-segments on the fly in the translation units (TUs) suggested to the user. Source-language (SL) and target-language (TL)
segments in the matching TUs are segmented into overlapping sub-segments of variable
length and machine-translated into the TL and the SL, respectively. The bilingual subsegments obtained and the matching between the SL segment in the TU and the segment to
be translated are employed to build the features that are then used by a binary classifier to
determine the target words to be changed and those to be kept unedited. In this approach,
MT results are never presented to the translator. Two approaches are presented in this
work: one using a word-keeping recommendation system which can be trained on the TM
used with the CAT system, and a more basic approach which does not require any training.
Experiments are conducted by simulating the translation of texts in several language
pairs with corpora belonging to different domains and using three different MT systems.
We compare the performance obtained to that of previous works that have used statistical
word alignment for word-keeping recommendation, and show that the MT-based approaches
presented in this paper are more accurate in most scenarios. In particular, our results
confirm that the MT-based approaches are better than the alignment-based approach when
using models trained on out-of-domain TMs. Additional experiments were also performed
to check how dependent the MT-based recommender is on the language pair and MT
system used for training. These experiments confirm a high degree of reusability of the
recommendation models across various MT systems, but a low level of reusability across
language pairs.

1. Introduction
Computer-aided translation (CAT) systems based on translation memory (TM) (Bowker,
2002; Somers, 2003) are the translation technology of choice for most professional translators, especially when translation tasks are repetitive and the effective recycling of previous
translations is feasible. The reasons for this choice are the conceptual simplicity of fuzzymatch scores (FMS) (Sikes, 2007) and the ease with which they can be used to determine
the usefulness of the translations proposed by the CAT system and to estimate the remainc
2015
AI Access Foundation. All rights reserved.

fiEspla-Gomis, Sanchez-Martnez & Forcada

Figure 1: Procedure followed to translate a document using a TM-based CAT system.
ing effort needed to turn them into adequate translations. The FMS function measures the
similarity between two text segments, usually by computing a variant of the word-based edit
distance (Levenshtein, 1966), although the FMS of some proprietary tools are not publicly
described.
When a TM-based CAT system is used to translate a new source document, the system
first segments the document, and then, for each source segment S 0 , provides the translator
with the subset of translation units (TUs) (S, T ) in the TM for which the FMS between S 0
and S is above a selected threshold . The translator must then choose the TU (S, T ) that
best fits his or her needs and post-edit its target segment T to produce T 0 , an adequate
translation of S 0 . Figure 1 illustrates this procedure.
When showing the subset of matching TUs to the translator, most TM-based CAT
systems highlight the words in S that differ from those in S 0 in order to ease the task of
post-editing T . It is, however, up to the translator to identify the specific words in T that
should be changed (either replaced or removed) in order to convert T into T 0 , which is the
problem that we deal with in this paper and term as word-keeping recommendation. Our
experiments with professional translators show that a TM-based CAT system capable of
word-keeping recommendation improves their productivity by up to 14% in the ideal case
that all recommendations are indeed correct (see Appendix A for more details).
Word-keeping recommendation is related to translation spotting (Veronis & Langlais,
2000; Simard, 2003; Sanchez-Martnez, Carrasco, Martnez-Prieto, & Adiego, 2012), which
consists of solving the problem of finding parallel sub-segments in parallel texts. Translation
spotting is used, for example, by bilingual concordancers (Bourdaillet, Huet, Langlais, & Lapalme, 2010), types of tools which help a translator to retrieve occurrences of a sub-segment
in a parallel corpus and its corresponding translation. Some examples of commercial bilingual concordancers are Webitext,1 Linguee,2 or Reverso Context.3 Translation spotting is
also particularly relevant for example-based machine translation (Somers, 1999), which uses
this technique to build the sub-segmental TM used to translate new materials. MT quality
estimation (de Gispert, Blackwood, Iglesias, & Byrne, 2013; Specia, Raj, & Turchi, 2010),
also shares some features with this task: in both cases the objective is to discover whether
1. http://www.webitext.com [last visit: 15th May 2015]
2. http://www.linguee.com [last visit: 15th May 2015]
3. http://context.reverso.net/translation/ [last visit: 15th May 2015]

170

fiTarget-Language Edit Hints in CAT Tools Based on TM by Means of MT

a translation proposal T 4 is a valid translation for a given source language segment S 0 . The
parallelisms become stronger in the case of word-level quality estimation (Ueffing & Ney,
2005; Bojar et al., 2014; Espla-Gomis, Sanchez-Martnez, & Forcada, 2015), in which, as in
word-keeping recommendation, every word of a proposal is analysed to decide whether or
not it is likely to belong to the final translation. There are critical differences between the
scenarios in which quality estimation and word-keeping recommendation operate: quality
estimation detects words which should be changed in segments T which are likely to be
inadequately written in TL, but are intended to be translations of S 0 ; conversely, wordkeeping recommendation is intended to work on segments T which are usually adequately
written in TL, but they are not a translation of S 0 (unless an exact match between S and
S 0 is found).
Espla, Sanchez-Martnez, and Forcada (2011) have performed word-keeping recommendation by using statistical word-alignment models (Och & Ney, 2003) to align the sourcelanguage (SL) and target-language (TL) words of each TU in the TM. When a TU (S, T )
is suggested to the translator, the pre-computed word alignments are then used to determine the target words to be changed or kept unedited. Analogously, Kranias and Samiotou
(2004) align the words in each TU at different sub-segment levels by using, among other
resources, a bilingual dictionary of words and phrases (Meyers, Kosaka, & Grishman, 1998),
suffix lists to deal with morphological variations, and a list of closed-class words and their
categories (Ahrenberg, Andersson, & Merkel, 2000). The authors use these alignments to
detect the words to be changed and then use MT to propose a translation for them. To
the best of our knowledge, the specific details on how the Kranias and Samiotou method
works have not been published. A patent published by Kuhn, Goutte, Isabelle, and Simard
(2011) describes a similar method that is also based on statistical word alignment in order
to detect the words to be changed in a translation proposal. Unfortunately, the patent does
not provide a detailed description of the actual procedure used.
Espla-Gomis, Sanchez-Martnez, and Forcada (2011) follow a different approach which
does not necessitate the computation of word alignments. Instead, they make use of any
available MT system as a source of bilingual information to compute a set of features that
are used by a perceptron classifier to estimate the probability pK of each target word being
kept unedited. This is done by: obtaining the matching TUs in the TM by using FMS
above a given threshold; segmenting the SL and TL segments in each of these TUs into
overlapping sub-segments of variable length; machine translating these sub-segments into
the TL and the SL, respectively, in order to learn sub-segment alignments; and using these
sub-segment alignments and the matching between S and S 0 to build the features to be
used by the classifier. The basic idea behind this method is that a word in T is likely to be
kept unedited if it appears in the translation of sub-segments which are common to S and
S 0 , the segment to be translated. Finally, pK is used for word-keeping recommendation by
marking the words for which pK < 12 as change, or otherwise as keep.
Although the latter approach requires a training procedure to be run on a TM, EsplaGomis et al. (2011) show that, for the translation of Spanish texts into English, the model
used by the perceptron classifier can be trained on a TM from a domain that is different
from that of the actual TM to be used and the text to be translated. Furthermore, the
4. In the case of quality estimation, the segment T to be evaluated originates from MT, while in wordkeeping recommendation, it originates from a TM-based CAT tool proposal.

171

fiEspla-Gomis, Sanchez-Martnez & Forcada

results obtained by this system are similar to those obtained by Espla et al. (2011), based
on statistical word alignments, for models trained on texts from the same domain as the
text translated, and much better for models trained on out-of-domain texts, as shown in
Section 7.
In this paper we revisit the approach of Espla-Gomis et al. (2011), and propose new
feature sets that capture the information in the machine-translated sub-segments in a more
successful way than the features therein. In addition, a more complex multilayer perceptron binary classifier is used in this work, which improves the results obtained with the
simpler perceptron classifier proposed by Espla et al. (2011). These improvements on binary classification are compared to the previous approach on a more exhaustive evaluation
framework, including new domains and language pairs (see below). Finally, we introduce
a new method for word-keeping recommendation that is also able to use any available MT
system as a source of bilingual information, and does not require any training procedure to
be run in advance. This training-free method uses the sub-segment pairs that match both
S and T to compute the alignment strength (Espla-Gomis, Sanchez-Martnez, & Forcada,
2012b) between the words in S and T . The alignment strength between two words sk in
S and tj in T measures the amount of evidence that relates the two words by giving more
weight to the evidence from shorter sub-segments, which involves a sharper picture of the
relation between sk and tj . Alignment strengths are then used in a similar fashion to that
of Espla et al. (2011) to determine the words to be changed or kept unedited.
As mentioned above, the experiments performed in this work compare the two MTbased approaches (that which requires training and that which is training-free) to the
alignment-based approach of Espla et al. (2011) using ten different language pairs, TMs
from three different domains, and three different MT systems. The experiments not only
cover the ideal scenario, in which the trained recommendation models are tested under
the same conditions language pair, TM domain, and MT system used for training,
but also scenarios in which some conditions change in order to test the reusability of the
models. Namely, experiments were carried out by using recommendation models trained on:
a TM from a different domain, a different MT system, and a different language pair. The
results obtained show that the MT-based approaches are superior to the alignment-based
approach as regards accuracy in all the scenarios. These results additionally confirm that
the MT-based approaches produce recommendation models that are more portable across
TM domains than those based on word alignment. This also provides good reusability for
different MT systems, but poor reusability when translating a different pair of languages to
that used for training; in fact, the training-free MT-based method provides better results
in this scenario.
The remainder of the paper is organised as follows. The following section reviews the
previous works on the integration of TMs and MT. Section 3 reviews the statistical wordalignment-based approach defined by Espla et al. (2011), which is used in this paper as a
reference to compare the new methods presented. Section 4 tackles the problem of wordkeeping recommendation by using a binary classifier and several sets of features based on
the coverage of sub-segment pairs obtained by machine translating sub-segments of different
sizes from the segments in a translation proposal, and the matching between the source-side
of the proposal and the segment to be translated. Section 5 then shows how to use these
bilingual sub-segments to compute the alignment strength between the SL and TL words
172

fiTarget-Language Edit Hints in CAT Tools Based on TM by Means of MT

in each TU, and how they can be used for word-keeping recommendation without training.
Section 6 describes the experimental framework, while Section 7 presents and discusses
the results obtained. The paper ends with some concluding remarks. Two appendices are
included in this paper: one including experiments aimed at measuring the impact of wordkeeping recommendation on the productivity of professional translators, and the other one
reporting the results on a filtered-out data set to check their performance in an ideal setting.

2. Integration of Machine Translation and Translation Memories
The literature on this subject contains several approaches that combine the benefits of MT
and TMs in ways that are different from those presented in this paper, and that go beyond
the obvious -combination scenario defined by Simard and Isabelle (2009), in which MT is
used to translate a new segment when no matching TU above a fuzzy-match score threshold
 is found in the TM.
Marcu (2001) integrates word-based statistical machine translation (SMT) with a subsegmental TM. The method uses IBM model 4 (Brown, Della Pietra, Della Pietra, &
Mercer, 1993) word-based translation model to build a sub-segmental TM and learn wordlevel translation probabilities. This is done by training the the IBM model 4 on the TM used
for translation. The source language (SL) segments and the target language (TL) segments
in each translation unit (TU) in the TM are then aligned at the word level by using the
Viterbi algorithm. Finally, the sub-segmental TM is built from parallel phrases in a very
similar way to that which occurs in modern phrase-based statistical MT systems (Koehn,
2010): parallel phrases are identified as all those pairs of sub-segments for which all the
words on the SL side are aligned with a word on the TL side or with NULL (unaligned),
and vice versa. The translation process is then carried out in two stages: first, occurrences
of SL phrases in the sub-segmental TM are translated using the corresponding TL phrase;
second, words not covered are translated using the word-level translation model learned by
the IBM model 4. A similar approach is proposed by Langlais and Simard (2002), who
also use translation at sub-segment level. In this case, the segment to be translated is split
into sub-segments, and an online bilingual concordancer is used to find their translations.
The word-based SMT decoder by Nieen, Vogel, Ney, and Tillmann (1998) is then used to
choose the best sub-segments and put them in the best order according to the model.
Bicici and Dymetman (2008) integrate a phrase-based SMT (PBSMT) (Koehn, 2010)
system into a TM-based CAT system using discontinuous bilingual sub-segments. The
PBSMT system is trained on the same TM, and when a new source segment S 0 is to be
translated, the segments S and T in the best matching TU are used to bias the statistical
translation of S 0 towards T . This is done by augmenting the translation table of the PBSMT
system with bilingual sub-segments originating from the fuzzy match (S, T ) in which the
source part is a common sub-sequence of S and S 0 , and the target part is a sub-sequence
of T which has been detected to be aligned with its counterpart sub-sequence in S. Simard
and Isabelle (2009) propose a similar approach in which a new feature function is introduced
in the linear model combination of a PBSMT system to promote the use of the bilingual
sub-segments originating from the best fuzzy match (S, T ). Following a similar approach,
Laubli, Fishel, Volk, and Weibel (2013) use the mixture-modeling technique (Foster & Kuhn,
2007) to learn a domain-adapted PBSMT system combining an in-domain TM and more
173

fiEspla-Gomis, Sanchez-Martnez & Forcada

general parallel corpora. It is worth noting that none of these three approaches guarantees
that the PBSMT system will produce a translation containing the translation in T of the
sub-segments that are common to S and S 0 . In contrast, Zhechev and van Genabith (2010)
and Koehn and Senellart (2010), who also use a PBSMT system, do guarantee that the subsegments of T that have been detected to be aligned with the sub-segments in S matched
by S 0 appear in the translation.
Example-based machine translation (EBMT) (Somers, 1999) has also frequently been
used to take advantage of TMs at the sub-segment level (Simard & Langlais, 2001). EBMT
systems are based on partial matches from TMs, as in the case of TM CAT tools. In
this case, the matching TUs are aligned to detect sub-segment pairs that can be reused
for translation. These sub-segment pairs are then combined to produce the most suitable
translation for S 0 . For instance, the commercial TM-based CAT tool Deja Vu 5 integrates
example-based MT in order to suggest candidate translations in those cases in which an
exact match is not found, but partial matches are available (Garcia, 2005; Lagoudaki, 2008).
The example-based-inspired MT system is used to propose a translation by putting together
sub-segments of the partial matchings available. Unfortunately, we have been unable to find
further details on how this method works.6 Approaches that combine several MT systems
are also available. For example, Gough, Way, and Hearne (2002) use several online MT
systems to enlarge the example database of an EBMT system. The authors claim that this
permits a better exploitation of the parallel information in the TM for new translations.
Our approach differs from those described above in two ways. First, the aforementioned
approaches use the TM to improve the results of MT, or use MT to translate sub-segments
of the TUs, while the MT-based approaches presented in this paper use MT to improve
the experience of using a TM-based CAT system without actually showing any machine
translated material to the translator. Second, the approaches above, with the sole exception
of that by Gough et al. (2002), focus on a specific MT system or family of MT systems
(namely, SMT and EBMT), whereas our MT-based approaches use MT as a black box,
and are therefore able to use one or more MT systems at once. In addition, as our MTbased approaches do not need to have access to the inner workings of the MT systems,
they are capable of using MT systems that are available on-line (thus avoiding the need
for local installation) or even any other source of bilingual information such as dictionaries,
glossaries, terminology databases, or sub-segment pairs from bilingual concordancers.
Some works cited in this section, such as those by Zhechev and van Genabith (2010)
and Koehn and Senellart (2010) use PBSMT, but they may easily be extended in order
to use a different MT system. Their approaches share some similarities with ours: they
also try to detect word or phrase alignments as a source of information to find out which
parts of a translation proposal should be kept unedited. The main difference between our
approach and those by Zhechev and van Genabith and Koehn and Senellart is that they use
MT to produce a final translation for the segment to be translated, which comes closer to
MT than to TM-based CAT. One of the aims of our approach is to minimally disturb the
way translators work with the TM-based CAT system by keeping the translation proposals
as found in the TM.
5. http://www.atril.com [last visit: 15th May 2015]
6. This is usually called advanced leveraging (Garcia, 2012).

174

fiTarget-Language Edit Hints in CAT Tools Based on TM by Means of MT

T

[keep]

[edit]

[?]

[?]

tj

tj 0

tj 00

tj 000

?
S

si

si0

matched unmatched
with S 0
with S 0

si00
matched
with S 0

 A
 ? A

A

000
si 
si0000AA

matched
with S 0

unmatched
with S 0

Figure 2: Example of the possible word-alignments that can be obtained for a pair of segments (S, T ). Target word tj may have to remain unedited because it is aligned
with source word si which is in the part of S that matches S 0 . Target word tj 0
may have to be changed because it is aligned with source word si0 which is in
the part of S that does not match S 0 . As target word tj 00 is not aligned with
any source word in S, there is no evidence that could be used to make a recommendation for it. The case of word tj 000 is special, since it is aligned with two
words, one matching S 0 and the other not matching S 0 , and a straightforward
recommendation cannot be provided.

3. Word-Keeping Recommendation Based on Statistical Word Alignment
This section reviews the first approach for word-keeping recommendation, which was introduced by Espla et al. (2011), who used statistical word alignment to detect the words to
be kept or edited in a translation proposal. Given a segment S 0 to be translated and a TU
(S, T ) proposed by the TM-based CAT system, this method first computes the matching
between S and S 0 and aligns the words in S and T by using the word-based statistical
translation models implemented in GIZA++ (Och & Ney, 2003). Alignments are then used
as follows: let tj be the word in the j-th position of T which is aligned with a word si , the
word in the i-th position of S. If si is part of the matching between S and S 0 , this indicates
that tj might be part of the translation of S 0 and that it should therefore remain unedited,
as occurs with the word tj in Figure 2. Conversely, if si is not part of the match between S
and S 0 , this indicates that tj might not be the translation of any of the words in S 0 and it
should be edited, as occurs with word tj 0 in Figure 2. More complex situations in which a
TL word is aligned with more than one SL word are tackled by following a voting scheme,
as will be explained below. The main limitation of this approach is that, when a word tj is
unaligned, as occurs with the word tj 00 in Figure 2, there is no evidence that could be used
to make a recommendation for it. Although it might be possible to decide on unaligned
words by, for example, using the aligned words surrounding them, a wrong recommendation
could be worse for the translator than not making any recommendation at all. The idea
behind this claim is that a wrong keep recommendation may lead to a wrong translation,
which would clearly be undesirable.
In order to determine whether the word tj in the target proposal T should be changed
or kept unedited, the fraction of words aligned with tj which are common to both S and S 0
175

fiEspla-Gomis, Sanchez-Martnez & Forcada

La
situacin
humanitaria
parece
ser

t
ul
ic

be

hu

m

di
ff

to

th
e
an
ita
ria
n
si
tu
at
io
ap n
pe
ar
s

difcil

Figure 3: Word alignments for the TU (la situacion humanitaria parece ser difcil, the
humanitarian situation appears to be difficult).

are computed:
X
fK (tj , S 0 , S, T ) =

matched(si )

si aligned(tj )

|aligned(tj )|

where aligned(tj ) is the set of source words in S which are aligned with target word tj in T ,
and matched(si ) equals 1 if the word si is part of the match between S and S 0 , the segment
to be translated, and 0 otherwise. Function matched(x) is based on the optimal edit path,7
obtained as a result of the word-based edit distance (Levenshtein, 1966) between S and S 0 .
The fraction fK (tj , S 0 , S, T ) may be interpreted as the likelihood of having to keep word tj
unedited. As mentioned above, tj may be aligned with several words in S, some of which
may be common to S and S 0 while others may not, as occurs with the word tj 000 in Figure 2.
Espla et al. (2011) propose two possible heuristics to deal with this:
 unanimity: for a word tj , a recommendation is made if it is aligned only with matched
words (fK () = 1), or only with unmatched words (fK () = 0) in S, while no recommendation is made otherwise; and
 majority: this heuristic uses a voting scheme, in which if tj is aligned with more
matched words than unmatched words (fK () > 21 ), a recommendation is made that
it should be kept, and vice versa. Only if tj is aligned with the same number of
matched and unmatched words (fK () = 21 ) no recommendation is made.
Let us suppose that the TU (S, T ) = (la situacion humanitaria parece ser difcil, the
humanitarian situation appears to be difficult) is proposed in order to translate a new
segment S 0 = la situacion poltica parece ser difcil, and that the word-alignment for
7. It may occur that more than one optimal path is available to align two segments S and S 0 . In this case,
one of them is chosen arbitrarily.

176

fiTarget-Language Edit Hints in CAT Tools Based on TM by Means of MT

(S, T ) is that depicted in Figure 3. The words the, situation, to and be would be marked to
be kept, since they are aligned with a single word which is part of the matching between S
and S 0 , which is compatible with a possible translation T 0 =the political situation appears
to be difficult. The word difficult would also be marked to be kept, since, even though
it is aligned with two words, both are part of the matching between S and S 0 . However,
the evidence for the word humanitarian is ambiguous; it is aligned with the words la and
situacion, which are part of the matching, but also with humanitaria which is not. If
the unanimity criterion were to be used, no recommendation would be made for it, while
the use of the majority criterion would result in a keeping recommendation. Finally, no
recommendation would be made for the word appears, since it is not aligned with any word.
The main disadvantage of this approach is that it requires a word-alignment model to
be trained directly on the TM to be used for translation in order to maximise the coverage,
which means re-training the alignment model every time the TM is updated with new TUs.
It may also occur that the TM is not sufficiently large to be able to obtain recommendations
with an acceptable quality, signifying that it is necessary to use external parallel corpora in
order to train these models. Incremental training (Gao, Lewis, Quirk, & Hwang, 2011) or
online training (Bertoldi, Farajian, & Federico, 2009) of statistical word alignment models
could be a means to reduce the training time when a TM is modified, or even to adapt a
general alignment models to more specific domains, thus improving the coverage. In this
case, incremental training would be useful as regards adapting existing models to a new
TM, while the on-line training would allow the models to be updated after a new TU has
been added to a TM. However, this paper focuses on using machine translation as a source
of bilingual information for word-keeping recommendation: we therefore keep the original
word-alignment-based approach as described by Espla et al. (2011) and only use it as a
reference when comparing the new approaches proposed here.

4. Word-Keeping Recommendation as Binary Classification
In this work we tackle the problem of word-keeping recommendation as a binary classification problem. For a new segment S 0 and the TU (S, T ) suggested to the translator by the
TM-based CAT system, a set of features are computed for each word tj in T , and a binary
classifier is then used to determine whether tj should be kept unedited or changed (either
replaced or deleted). Henceforth, we shall refer to this approach as the trained MT-based
recommender, to differentiate it from the training-free MT-based recommender presented in
Section 5.
The features we use are based on the assumption that MT, or any other source of
bilingual information, can provide evidence as to whether each word tj in T should be
changed or kept unedited. Let  be a sub-segment of S from one of the matching TUs
(S, T ), which is related by MT to a sub-segment  of T . By related by MT we mean that
machine translating  leads to  , or vice versa. We hypothesise that:
 words in  matching the new segment to be translated S 0 provide evidence that the
words in  should be kept unedited (keeping evidence); and
 words in  not matching the new segment to be translated S 0 provide evidence that
the words in  should be changed (changing evidence).
177

fiEspla-Gomis, Sanchez-Martnez & Forcada

(la,the) [keeping evidence],
(situacion,situation) [keeping evidence],
(humanitaria, humanitarian) [changing evidence],
(ser,be) [keeping evidence],
(ser,to be) [keeping evidence],
(difcil,difficult) [keeping evidence],
(situacion humanitaria,humanitarian situation),
(ser difcil, be difficult) [keeping evidence], and
(la situacion humanitaria, the humanitarian situation).

Figure 4: Example of the collection M of overlapping machine translated pairs of subsegments for (S, T ) = (la situacion humanitaria parece ser difcil, the humanitarian situation appears to be difficult). Sub-segment pairs (,  ) with 
matching S and  matching T are highlighted in bold type.

To continue with the example proposed in Section 3, in which: (S, T ) = (la situacion humanitaria parece ser difcil, the humanitarian situation appears to be difficult), and S 0 =
la situacion poltica parece ser difcil, we segment S and T into all possible overlapping
sub-segments and translate them with an MT system to obtain the collection M of subsegment pairs (,  ) matching (S, T ) and shown in Figure 4. Some sub-segment pairs, such
as (parece, appear), are not included in that list because the translations of the subsegment on one side do not match their equivalents on the other side. For example, parece is
translated into English as seems, while appear is translated into Spanish as aparecer. Those
pairs (,  ) in that list for which all the words in  match S 0 provide strong evidence that
the words in the corresponding target part should be kept unedited. In this example, these
words are the, situation, be and difficult, which are compatible with a possible translation
T 0 =the political situation appears to be difficult. Conversely, the pairs (,  ) for which
all the words in  do not match S 0 provide strong evidence that the words in the target part
should be changed. In this case, this only occurs for the word humanitarian. On the other
hand, there is one word about which no evidence can be obtained (appears) because it is not
matched by the MT system. In this case, it is not possible to provide the translator with
any recommendations as occurs, for analogous reasons, with the alignment-based approach
described in Section 3. Note that the pair (,  ) =(situacion humanitaria,humanitarian
situation) contains a source word (situacion) which matches S 0 and another (humanitaria)
which does not match S 0 . Dealing with this ambiguous evidence, along with combining the
evidence from different (,  ) (which may be contradictory) leads to an additional problem.
In order to deal with ambiguous evidence, we define three feature sets that model and combine keeping and changing evidence, and which will be described in Sections 4.1, 4.2,
and 4.3.
It is worth noting that some pre-processing methods could be used in order to, hopefully, exploit the evidence from bilingual sources of information more efficiently, such as
stemming/lemmatisation, morphological analysis, or even the integration of syntactic features such as those proposed by Ma, He, Way, and van Genabith (2011). However, the
178

fiTarget-Language Edit Hints in CAT Tools Based on TM by Means of MT

objective of this approach is to avoid any complex processing in order to obtain fast recommendations when translating texts between any pair of languages, in any domain, and
only re-using already available sources of bilingual information, such as the numerous MT
systems available on the Internet.
4.1 Features Based on Matching/Mismatching Sub-segments with
Unconstrained Length Relations [MM-U]
This feature set was proposed by Espla-Gomis et al. (2011) and is used as a reference for
the remaining feature sets proposed in this work. This feature set considers, for a given
(,  ) pair of segments, that:
 if  is a common sub-segment of both the new segment to be translated S 0 and the
source segment S, then it is likely that the words in  will not have to be changed
(keeping evidence);
 if  is a sub-segment of S but not of S 0 , then it is likely that the words in  will have
to be changed (changing evidence).
As can be seen, this is a rather conservative criterion which discards the information from
matching words in a partially matching sub-segment  between S 0 and S,8 and will probably
be capable of providing high accuracy when recommending that a word should be kept. A
more flexible approach is presented in Section 4.2.
Based on the proposed rationale, four sets of features are computed: two sets of keeping
features, which provide information about the chances of keeping tj , and two sets of changing
features, which provide information about the chances of changing tj . Given the maximum
sub-segment length L, the keeping feature set Km is defined for the word tj and for every
value of m  [1, L] as follows:
Km (j, S 0 , S, T ) =

tcover(j, segm (S)  segm (S 0 ), seg (T ), M )
,
tcover(j, segm (S), seg (T ), M )

where segm (X) represents the set of all possible m-word sub-segments of segment X,
seg (X) is similar to segm (X) but without length constraints, and tcover(j, S, T , M ) is
defined as:
tcover(j, S, T , M ) = |{  T :   S  (,  )  M  j  span( , T )}|,
where S  seg (S), T  seg (T ), and function span( , T ) returns the set of word positions
spanned by the sub-segment  in the segment T .9 Function tcover(j, S, T , M ) therefore
computes the number of target sub-segments   T containing the word tj that are related
by MT to a sub-segment   S.
Similarly to Km , Kn is computed by using only target sub-segments  of length n:
Kn (j, S 0 , S, T ) =

tcover(j, seg (S)  seg (S 0 ), segn (T ), M )
.
tcover(j, seg (S), segn (T ), M )

8. For example, a sub-segment of 5 words in which 4 of them are matched and only one is unmatched would
be considered as changing evidence.
9. Note that a sub-segment  may be found more than once in segment T : function span( , T ) returns all
the possible positions spanned.

179

fiEspla-Gomis, Sanchez-Martnez & Forcada

Analogously, changing feature sets Cm and Cn are defined as:
Cm (j, S 0 , S, T ) =

tcover(j, segm (S)  segm (S 0 ), seg (T ), M )
,
tcover(j, segm (S), seg (T ), M )

Cn (j, S 0 , S, T ) =

tcover(j, seg (S)  seg (S 0 ), segn (T ), M )
.
tcover(j, seg (S), segn (T ), M )

In the case of all four features, when both the numerator and the denominator happen to
be zero because no pair (,  ) covers tj , the value of the feature is set to 12 .
These four features are computed for every value of 1  m  L and 1  n  L, where
L is the maximum sub-segment length used, resulting in 4L features. All these features
take values in [0, 1] and may have a probabilistic interpretation, in which 12 means dont
know. This feature set will from here on be termed as MM-U features. A similar collection
of features was tried that constrained the length of both  (m) and  (n). However, the
results confirmed that no improvement was obtained by adding it to the feature set.
For the running example, we show the feature set that could be computed at word be,
the sixth word in T (t6 ). Please recall that (S, T ) = (la situacion humanitaria parece
ser difcil, the humanitarian situation appears to be difficult). Using the collection of
translated pairs of overlapping sub-segments shown in Figure 4, there are three sub-segment
pairs (,  ) in M that cover the word be:
M = {(ser, be), (ser, to be), (ser difcil, be difficult)}.
These pairs (,  ) only contain sub-segments  with m  [1, 2]. The value of the function
tcover is:
tcover(6, seg1 (S), seg (T ), M ) = |{be, to be}| = 2
tcover(6, seg2 (S), seg (T ), M ) = |{be difficult}| = 1
for both values of m. In addition, the value of tcover for all the sub-segments  that match
S 0 is:
tcover(6, seg1 (S)  seg1 (S 0 ), seg (T ), M ) = |{be, to be}| = 2
tcover(6, seg2 (S)  seg2 (S 0 ), seg (T ), M ) = |{be difficult}| = 1
The value of the corresponding features is therefore:
K1 (6, S 0 , S, T ) =

tcover(6, seg1 (S)  seg1 (S 0 ), seg (T ), M )
2
= =1
tcover(6, seg1 (S), seg (T ), M )
2

K2 (6, S 0 , S, T ) =

tcover(6, seg2 (S)  seg2 (S 0 ), seg (T ), M )
1
= =1
tcover(6, seg2 (S), seg (T ), M )
1

Features K1 (6, S 0 , S, T ) and K2 (6, S 0 , S, T ) can be computed analogously. This case is
rather simple, since all the evidence available for this word indicates that it should be kept.
However, for the word situation (t3 ), both keeping and changing evidence coexist in
the set M of translated sub-segments pairs:
M = {(situacion, situation), (situacion humanitaria, humanitarian situation),
(la situacion humanitaria, the humanitarian situation)}
180

fiTarget-Language Edit Hints in CAT Tools Based on TM by Means of MT

In this case,  sub-segments take lengths m  [1, 3], which produces the following values for
tcover():
tcover(3, seg1 (S), seg (T ), M ) = |{situation}| = 1
tcover(3, seg2 (S), seg (T ), M ) = |{humanitarian situation}| = 1
tcover(3, seg3 (S), seg (T ), M ) = |{the humanitarian situation}| = 1
However, in this case, not all of them match S 0 :
tcover(3, seg1 (S)  seg1 (S 0 ), seg (T ), M ) = |{situation}| = 1
tcover(3, seg2 (S)  seg2 (S 0 ), seg (T ), M ) = || = 0
tcover(3, seg3 (S)  seg3 (S 0 ), seg (T ), M ) = || = 0
This allows us to compute the following keeping features:
K1 (3, S 0 , S, T ) =

1
tcover(3, seg1 (S)  seg1 (S 0 ), seg (T ), M )
= =1
tcover(3, seg1 (S), seg (T ), M )
1

K2 (3, S 0 , S, T ) =

tcover(3, seg2 (S)  seg2 (S 0 ), seg (T ), M )
0
= =0
tcover(3, seg2 (S), seg (T ), M )
1

K3 (3, S 0 , S, T ) =

tcover(3, seg3 (S)  seg3 (S 0 ), seg (T ), M )
0
= =0
tcover(3, seg3 (S), seg (T ), M )
1

Analogously, for the changing features, we have:
tcover(3, seg1 (S)  seg1 (S 0 ), seg (T ), M ) = || = 0
tcover(3, seg2 (S)  seg2 (S 0 ), seg (T ), M ) = |{humanitarian situation}| = 1
tcover(3, seg3 (S)  seg3 (S 0 ), seg (T ), M ) = |{the humanitarian situation}| = 1
which allow us to obtain the following features:
C1 (3, S 0 , S, T ) =

tcover(3, seg1 (S)  seg1 (S 0 ), seg (T ), M )
0
= =0
tcover(3, seg1 (S), seg (T ), M )
1

C2 (3, S 0 , S, T ) =

1
tcover(3, seg2 (S)  seg2 (S 0 ), seg (T ), M )
= =1
tcover(3, seg2 (S), seg (T ), M )
1

C3 (3, S 0 , S, T ) =

tcover(3, seg3 (S)  seg3 (S 0 ), seg (T ), M )
1
= =1
tcover(3, seg3 (S), seg (T ), M )
1

In this case, the ambiguity in the features will be managed by the binary classifier, which
will determine the corresponding weights during training.
181

fiEspla-Gomis, Sanchez-Martnez & Forcada

4.2 Features Based on Partially Matching Sub-segments with Constrained
Length Relations [PM-C]
This feature set is slightly different from the previous one as regards the way in which
the evidence from the pairs of sub-segments (,  )  M is used. In this case, the features
represent the fraction of words in S that match S 0 to which a given word tj in T is related
by means of sub-segment pairs (,  ). It is worth noting that in the previous feature set, the
matching of sub-segment pairs (,  ) was evaluated for the whole sub-segment . However,
in this new feature set, both the keeping and the changing features are computed using
the matched/unmatched words in . The objective of this feature set is to use the positive
evidence from partially matching sub-segments  more efficiently. The following equation
W :
defines the new keeping feature Kmn
W
Kmn
(j, S 0 , S, T )

=

|S|
X

stcover(j, k, segm (S), segn (T ), M )  match(k, S 0 , S)

k=1

where j is the position of tj in T , k is the position of sk in S, match(k, S 0 , S) is 1 if sk is
part of the match between S and S 0 , and 0 otherwise,10 and function stcover(j, k, S, T , M )
is defined as:
stcover(j, k, S, T , M ) = |{(,  )  M :   T    S  j  span( , T )  k  span(, S)}|
W as:
Similarly, we define the changing feature Cmn

W
Cmn
(j, S 0 , S, T ) =

|S|
X

stcover(j, k, segm (S), segn (T ), M )  (1  match(k, S 0 , S)).

k=1

Function stcover(j, k, S, T , M ) differs from tcover(j, S, T , M ) in that, for a given pair
(,  ), the former takes into account both  and  while the latter only takes into account
W
W and C W complementary, whereas K
 . This makes Kmn
mn and Cmn are not. Kmn and
mn
W may be combined in a single normalised feature that we term as KCW :
Cmn
mn
|S|
X
0
KCW
mn (j, S , S, T ) =

stcover(j, k, segm (S), segn (T ), M )  match(k, S 0 , S)

k=1
|S|
X
stcover(j, k, segm (S), segn (T ), M )

,

(1)

k=1

As in the feature set described in Section 4.1, KCW
mn takes values in [0, 1], and, as in that
case, when no evidence is found for tj , the value of the corresponding feature is set to 21 .
This feature set results in L2 features and will be referred to as PM-C.
10. The function match(k, S 0 , S) is based on the optimal edit path obtained as a result of the word-based
edit distance (Levenshtein, 1966) between S 0 and S. Although this is not frequent, it may occur that
more than one optimal paths are available: in this case, one of them is chosen arbitrarily.

182

fiTarget-Language Edit Hints in CAT Tools Based on TM by Means of MT

For the running example, we compute the PM-C features for the word situation, as
occurred in Section 4.1. As in the previous example, we use the collection of translated
sub-segments in Figure 4. The set of sub-segments pairs covering the word situation is:
M = {(situacion, situation), (situacion humanitaria, humanitarian situation),
(la situacion humanitaria, the humanitarian situation)}
W (3, S 0 , S, T ), KC W (3, S 0 , S, T ), and KC W (3, S 0 , S, T ) can be comand the features KC1,1
2,2
3,3
puted from them. As can be seen stcover() happens to be different to zero only for k = 1:

stcover(3, 1, seg1 (S), seg1 (T ), M ) = |{(situacion, situation)}| = 1
In this case, we see that situacion (s2 ) is related to situation through the sub-segment pair
(,  )=(situacion, situation). In this case,  completely matches S 0 , and we therefore
have that:
|S|
X
stcover(3, k, seg1 (S), seg1 (T ), M )  match(k, S 0 , S)
W
KC1,1
(3, S 0 , S, T ) =

k=1
|S|
X

=

1
=1
1

stcover(3, k, seg1 (S), seg1 (T ), M )

k=1
W (3, S 0 , S, T ) is slightly more complex. Here, stcover() happens to be
The case of KC2,2
different to zero only for k  [1, 2]:

stcover(3, 1, seg2 (S), seg2 (T ), M ) =
|{(situacion humanitaria, humanitarian situation)}| = 1
stcover(3, 2, seg2 (S), seg2 (T ), M ) =
|{(situacion humanitaria, humanitarian situation)}| = 1
As will be observed, the words situacion and humanitaria are related to situation through
the same pair (,  )=(situacion humanitaria,humanitarian situation). Here, only one
of the two words matches S 0 , hence:
|S|
X
W
KC2,2
(3, S 0 , S, T ) =

stcover(3, k, seg2 (S), seg2 (T ), M )  match(k, S 0 , S)

k=1

=

|S|

X

1
= 0.5
2

stcover(3, k, seg2 (S), seg2 (T ), M )

k=1
W (3, S 0 , S, T ), stcover() happens to be different to zero
Finally we have that, for KC3,3
for k  [1, 3]:

stcover(3, 1, seg3 (S), seg3 (T ), M ) =
|{(la situacion humanitaria, the humanitarian situation)}| = 1
183

fiEspla-Gomis, Sanchez-Martnez & Forcada

stcover(3, 2, seg3 (S), seg3 (T ), M ) =
|{(la situacion humanitaria, the humanitarian situation)}| = 1
stcover(3, 3, seg3 (S), seg3 (T ), M ) =
|{(la situacion humanitaria, the humanitarian situation)}| = 1
This time, three words are related to situation, all of them through the same sub-segment
pair (,  )=(la situacion humanitaria,the humanitarian situation). In this case, la and
situacion match S 0 , while humanitaria does not. The resulting feature is therefore:
|S|
X
W
KC3,3
(3, S 0 , S, T ) =

stcover(3, k, seg3 (S), seg3 (T ), M )  match(k, S 0 , S)

k=1

=

|S|

2
' 0.67
3

X
stcover(3, k, seg3 (S), seg3 (T ), M )
k=1

Note that this feature collection constrains the length of  and  at the same time. This
configuration was also tried in the previous feature set (MM-U), but no improvements were
obtained as compared to constraining the lengths of  and  separately. With this feature
set both possibilities were also tried, but here constraining the length of  and  at the
same time proved to lead to better results.
4.3 Features Combining Partially Matching Sub-segments with Constrained
Length Relations and Information about Coverage [PM-C+C]
Features KCW
mn above may hide the amount of keeping/changing evidence, since they only
take into account the fraction of keeping evidence from the total amount of evidence.11
To deal with this, we propose that the feature set defined in Section 4.2 be combined with
a new feature Emn :
Emn (j, S, T ) = |{(,  )  M :   segm (S)    segn (T )  j  span( , T )}|

(2)

This feature counts the number of sub-segment pairs (,  ) covering word tj , thus providing a measure of the amount of evidence supporting the value of feature KCW
mn . We
0 , S, T ), which
propose a new feature set, with 2L2 features, using both KCW
and
E
(j,
S
mn
mn
will be referred to as PM-C+C. A similar feature set was tried, in which Emn was normalised
by dividing it by the maximum number of pairs (,  ) that could have covered tj (m  n).
However, this set of features did not show any improvement and was therefore discarded.
For the running example, the pairs (,  ) in M that cover the word be (t6 ) are:
M = {(ser, be), (ser, to be), (ser difcil, be difficult)}.
Therefore, the features Emn that are different to zero for the word be are:
E1,1 (6, S, T ) = |{(ser, be)}| = 1
11. For example, it would be the same to have 1 keeping evidence out of 1 evidence and 5 keeping evidences
out of 5 evidences; however, the second case should be considered to be more reliable, since more evidence
confirms the keeping recommendation.

184

fiTarget-Language Edit Hints in CAT Tools Based on TM by Means of MT

E1,2 (6, S, T ) = |{(ser, to be)}| = 1
E2,2 (6, S, T ) = |{(ser difcil, be difficult)}| = 1
Given that a single pair (,  ) covers the word be with the same m and n, all these features
are set to 1. However, if the evidence (,  )=(ser,be difficult) could be used, the value
of E1,2 would become higher:
E1,2 (6, S, T ) = |{(ser, to be), (ser, be difficult)}| = 2

5. Word-Keeping Recommendation Based on MT Alignment Strengths
The collection M of sub-segment pairs (,  ) which are related by MT can be used for wordkeeping recommendation directly, i.e., without having to run any training procedure. We
propose to do this by using a training-free MT-based recommender based on the alignment
strengths described by Espla-Gomis, Sanchez-Martnez, and Forcada (2012a) and EsplaGomis et al. (2012b). This metric determines the relatedness or association strength between
the j-th word in T and the k-th word in S and is defined as:
L X
L
X
stcover(j, k, segm (S), segn (T ), M )
A(j, k, S, T ) =
mn
m=1 n=1

The alignment strength is based on the idea that matched sub-segment pairs apply pressure
to the words, signifying that the larger the surface covered by a sub-segment pair, the lower
the pressure applied to each individual word. Figure 5 shows how the words in a TU are
covered by the bilingual sub-segments in M (left), and the result of computing the alignment
strengths (right).
In order to perform a word-keeping recommendation using the alignment strengths, we
define function G(j, S 0 , S, T ) which computes the fraction of the alignment strength that
relates a word tj to those words sk which are part of the matching between S 0 and S, over
the sum of the alignment strength for all the words in S:
|S|
X

G(j, S 0 , S, T ) =

A(j, k, S, T )  match(k, S 0 , S)

k=1
|S|
X
A(j, k, S, T )

(3)

k=1

Word keeping recommendation is then performed in the following simple manner: if
G(j, S 0 , S, T )  21 , then word tj is marked to be changed, otherwise to be kept. If there is
no evidence (,  ) with  spanning tj , then no recommendation is provided for tj .
It is worth noting that A(j, k, S, T ) is similar to a particular linear combination of the
PM-C feature set described in Section 4.2, in which the weight of each feature is directly set
1
to mn
, rather than being chosen by optimising the recommendation accuracy in a training
set. The results shown in Section 6 prove that this method is less accurate than the trained
MT-based approach, but still provides reasonably good results.
185

fiEspla-Gomis, Sanchez-Martnez & Forcada

La

La 1.11 0.11 0.11

situacin

situacin 0.11 0.36 1.36

humanitaria

humanitaria 0.11 1.36 0.36

ul
ic

hu

m

di
ff

ul
ic

di
ff

hu

m

t

0.25 1.25

be

difcil
to

difcil

th
e
an
ita
ria
n
si
tu
at
io
ap n
pe
ar
s

0.50 1.75 0.25

t

ser

be

ser

to

parece

th
e
an
ita
ria
n
si
tu
at
io
ap n
pe
ar
s

parece

Figure 5: Sub-segment pairs covering the words in the TU (la situacion humanitaria parece
ser difcil, the humanitarian situation appears to be difficult) (left), and the
alignment strengths obtained from them (right). The weight of each sub-segment
pair is taken to be 1 and is divided by the surface it covers to compute the
pressure exerted on each individual word.

For the running example, we use the scores shown in Figure 5; as can be seen, the word
situation is related to three words: La, with a score of 0.11, situacion, with a score of 1.36,
and humanitaria, with a score of 0.36. The value of G(3, S 0 , S, T ) is therefore:
G(3, S 0 , S, T ) =

0.11  1 + 1.36  1 + 0.36  0
1.47
=
' 0.8
0.11 + 1.36 + 0.36
1.83

In this case, G(3, S 0 , S, T ) > 12 , which means that the word situation must remain unedited.
However, for humanitarian, which is related to the same words in Spanish, we have:
G(2, S 0 , S, T ) =

0.47
0.11  1 + 0.36  1 + 1.36  0
=
' 0.3
0.11 + 0.36 + 1.36
1.83

Since G(2, S 0 , S, T )  21 , the word humanitarian would be marked to be changed.

6. Experimental Settings
The experiments conducted consisted of simulating the translation of texts between several
language pairs and text domains. The language pairs involved in the experiments were
GermanEnglish (deen), EnglishGerman (ende), EnglishSpanish (enes), Spanish
English (esen), EnglishFinnish (enfi), FinnishEnglish (fien), EnglishFrench (enfr),
FrenchEnglish (fren), SpanishFrench (esfr), and FrenchSpanish (fres). Three
thematic TMs were created for each language pair by extracting domain-specific TUs
from the DGT-TM (Steinberger, Eisele, Klocek, Pilos, & Schluter, 2012), a TM published
by the European Commission Directorate-General for Translation (European Commission
Directorate-General for Translation, 2009).
186

fiTarget-Language Edit Hints in CAT Tools Based on TM by Means of MT

We compared the two MT-based approaches described in Sections 4 and 5 with a nave
baseline which is also based on a binary classifier, but using only the fuzzy-match score
(FMS) between S and S 0 as a feature, (henceforth FMS-only baseline, see Section 6.5),
and with the statistical word-alignment-based approach described in Section 3, in different
scenarios. We first evaluated all the approaches in the optimal case, in which the models
(either word-alignment models or classification models) are trained on the same TM used for
translating, and, in the case of the MT-based approaches, employing the same MT system
used for translation. This evaluation was then extended to evaluate:
 the reusability across domains: the word-alignment models and the classification models are trained on out-of-domain TMs;
 the reusability across MT systems: the models are trained on the same TM used for
translation, but using a different MT system; and
 the reusability across language pairs: the models are trained on a TM from the same
domain as that used for translation, but with a different language pair, and obviously,
using a different MT system.
The reusability across MT systems can evidently be evaluated only in the case of MT-based
approaches. As regards reusability across language pairs, on the one hand, the FMS-only
baseline is language independent, while on the other, the statistical word-alignment models
used by the alignment-based approach have to be trained on the same pair of languages as
that used for translation.
This extensive evaluation will allow us to ascertain the degree of independence of the
recommendation model with regard to the domain of the TM, the MT system, and the
language pair used during training. This is a key point, since high independence of all or
part of these variables would allow computer-aided translation (CAT) users to reuse existing
feature weights obtained without having to run any training procedure when they change
the domain of the texts to be translated, the MT system they use or even the languages
they are working with. The case of domain independence is particularly relevant since it
covers not only the problem of using a different TM, but also the case in which new TUs
which were not seen during training are added to a TM.
With regard to the MT systems, we have used the statistical MT system by Google,12
Power Translator (Depraetere, 2008) version 15,13 and the free/open-source, shallow-transfer
MT system Apertium (Forcada et al., 2011).14 Unfortunately, not all the MT systems mentioned above were available for all the language pairs. Table 1 shows the MT system(s)
available for each language pair included in the experiments.
Even though we used large data sets in a batch mode to obtain the results reported in
this paper, we wanted to ensure that the MT-based approaches would be able to provide
recommendations in real time for translation tasks. The main part of the computation
time for the MT-based approaches is spent segmenting S and T and machine-translating
the resulting sub-segments. In order to prove that this could be done in a real MT-based
12. http://translate.google.com [last visit: 15th May 2015]
13. http://www.lec.com/power-translator-software.asp [last visit: 15th May 2015]
14. http://www.apertium.org [last visit: 15th May 2015]

187

fiEspla-Gomis, Sanchez-Martnez & Forcada

Language pair
GermanEnglish (deen)
EnglishGerman (ende)
EnglishSpanish (enes)
SpanishEnglish (esen)
EnglishFinnish (enfi)
FinnishEnglish (fien)
EnglishFrench (enfr)
FrenchEnglish (fren)
SpanishFrench (esfr)
FrenchSpanish (fres)

Apertium

3
3

3
3

Google Translate

Power Translator

3
3
3
3
3
3
3
3
3
3

3
3
3
3

3
3
3
3

Table 1: MT systems available for each translation direction () used in the experiments.

CAT scenario, a prototype15 plug-in implementing the training-free approach was built for
the free/open-source CAT system OmegaT16 and, after some experiments using the on-line
MT systems Apertium and Google Translate, we can confirm that recommendations are
obtained almost instantaneously.
6.1 Evaluation
The FMS-only baseline, the statistical alignment-based approach proposed in Section 3,
and the MT-based approaches proposed in Sections 4, and 5 were tested by using a test set
(TS) of parallel segments {(Sl0 , Tl0 )}N
in the same domain. For each SL segment
l=1 , and a TM
0
N
0
Sl in TS, the set of matching TUs {(Si , Ti )}i=1 in the TM with a FMS above threshold
 is obtained. Please recall that the FMS measures the similarity between the translation
proposals and the segments to be translated. The FMS threshold  is usually set to values
above 60% (Bowker, 2002, p. 100) and in our experiments we therefore used several values
of  of between 60% and 90%.17 Once the set of matching TUs has been obtained, the
recommendations for every word tj in every target-language segment Ti are obtained and
evaluated by using Tl0 , the translation of Sl0 , as a gold standard. The words in Tl0 and Ti
are matched by using the Levenshtein edit distance (Levenshtein, 1966), which allows us
to check whether or not a given word tj , the j-th word of Ti , is actually kept in the final
translation. It is thus possible to determine whether a recommendation for tj is successful
both if:
 tj is recommended to be changed in Ti and it does not match any word in Tl0 , or
 tj is recommended to be kept in Ti and it does match a word in Tl0 .
15. http://www.dlsi.ua.es/~mespla/edithints.html [last visit: 15th May 2015]
16. http://www.omegat.org [last visit: 15th May 2015]
17. For those MT-based approaches that require training, different models were trained for every value of 
included in the experiments.

188

fiTarget-Language Edit Hints in CAT Tools Based on TM by Means of MT

Once all the pairs (Sl0 , Tl0 )  TS have been used to obtain their corresponding sets of
0
matching TUs {(Si , Ti )}N
i=1 , and all the recommendations have been obtained and checked,
several metrics are used for evaluation. Accuracy (A) is computed as the fraction of successful recommendations out of the total number of words for which a recommendation
was made. It is worth noting that most of the methods proposed do not provide recommendations for all the words; another interesting metric is therefore the fraction of
words not covered (NC) by the system, that is, the fraction of words for which no recommendation is made. The combination of these two metrics helps us to understand
how each method perform on each test set. In addition to the accuracy and the fraction of words not covered, we also compute the precision and recall as regards keeping
recommendations (PK and RK , respectively) and changing recommendations (PC and
RC , respectively). The latter metrics are useful since they provide specific information
about the successful keeping recommendations and change recommendations, separately,
while A and NC provide information about the general performance of the recommender.
The code used to perform these experiments is freely available under license GNU General Public License v3.0 (Free Software Fundation, 2007) and can be downloaded from
http://transducens.dlsi.ua.es/~mespla/resources/wkr/.
6.2 Corpora
The corpus used in our experiments is the DGT-TM (Steinberger et al., 2012). This translation memory is a collection of documents from the Official Journal of the European Union 18
which is aligned at the segment level for several languages (multilingual TUs). Segment
alignment in DGT-TM is expected to have a high level of quality, since part of these alignments were manually checked, or were actually generated during computer-aided translation
by professional translators.
The TUs in DGT-TM contain segments in many official languages of the European Union
and are labelled with domain codes19 which were used to create three domain-specific TU
collections. This was done by using the following domain codes: elimination of barriers
to trade (code 02.40.10.40), safety at work (code 05.20.20.10), and general information of
public contracts (code 06.30.10.00). Only those TUs containing the corresponding segments
for all the five languages used in our experiments were included in these TU collections.
Each collection of TUs was used to build a bilingual TM and a test set for each language pair by randomly selecting pairs of segments without repetition.20 In addition to
the pre-processing already performed by the creators of DGT-TM (European Commission
Joint Research Center, 2007) the segments included in the TMs and the test set used in
our experiments were tokenised and lowercased. The TMs consist of 6, 000 TUs each, and
simulate the TM that a translator may use when translating with a CAT tool. The test set
consist of 1, 500 TUs whose source language side simulates the segments to be translated
by using the TMs (the translators job), while their target language side may be considered
as a reference translation for each segment to be translated.
18. http://eur-lex.europa.eu [last visit: 15th May 2015]
19. http://old.eur-lex.europa.eu/RECH_repertoire.do [last visit: 15th May 2015]
20. The TMs and the test set obtained in this way can be downloaded from http://transducens.dlsi.ua.
es/~mespla/resources/mtacat/ [last visit: 15th May 2015]

189

fiEspla-Gomis, Sanchez-Martnez & Forcada

02.40.10.40
02.40.10.40
05.20.20.10
06.30.10.00

05.20.20.10

(8o )

(76o )

0.99
0.26 (75o )
0.24 (76o )

0.24
0.98 (11o )
0.23 (76o )

06.30.10.00
0.20 (78o )
0.23 (76o )
0.97 (14o )

Table 2: Cosine similarity (and the corresponding angle) between the English side of the
EnglishSpanish TMs belonging to the three domains in the experiments: elimination of barriers to trade (code 02.40.10.40), safety at work (code 05.20.20.10),
and general information of public contracts (code 06.30.10.00).

The domains chosen for the experiments have little overlap in vocabulary, as evidenced
by the cosine similarity measure shown in Table 2.21 This technique maps a text onto a
vocabulary vector, in which each word is a dimension and the number of occurrences of
this word in the text is the value for that dimension. These vocabulary vectors can be
used to compare two texts by computing the cosine of the angle between them. The cosine
similarity was computed using the English side of the three EnglishSpanish TMs and by
splitting the 6,000 segments into two halves. The table shows the cosine similarity between
the first half of each domain (rows) and the second half (columns).
As will be noted, the cosines between the vocabulary vectors from the same domain
are very close to 1, with angles of between 8o and 14o . However, the cosines between the
vocabulary vectors from different domains are much smaller, with angles of between 75o and
79o . We can therefore conclude that there are considerable differences between the TMs
used in our experiments.
As regards the number of TUs matched when simulating the translation of Spanish
segments into English in the test set, Table 3 reports, for fuzzy-match scores in four different
ranges, the average number of TUs matched per segment to be translated and the total
number of words for which to provide a recommendation. These data provide an idea of
the repetitiveness of the corpora used to carry out the experiments. As can be seen, the
corpus from domain 02.40.10.40 is more repetitive than the other two. It is worth noting
that domains 05.20.20.10 and 06.30.10.00 have notable differences for low values of the FMS
threshold , while they do not differ that much for higher values.
6.3 Fuzzy-Match Score Function
For our experiments, as in many TM-based CAT systems, we have chosen a fuzzy-match
score function based on the word-based Levenshtein edit distance (Levenshtein, 1966):
FMS(S 0 , S) = 1 

D(S 0 , S)
max(|S 0 |, |S|)

21. The cosine similarity was computed on the lowercased corpora, removing punctuation characters and the
stopwords provided in the 4.7.2 version of Lucene: http://lucene.apache.org/core/ [last visit: 15th
May 2015].

190

fiTarget-Language Edit Hints in CAT Tools Based on TM by Means of MT

(%)

no filtering

domain

TUavg

Nwords

 60

02.40.10.40
05.20.20.10
06.30.10.00

3.71
0.62
5.68

95,881
9,718
34,339

 70

02.40.10.40
05.20.20.10
06.30.10.00

2.36
0.37
0.99

65,865
6,883
10,327

 80

02.40.10.40
05.20.20.10
06.30.10.00

1.58
0.14
0.45

46,519
3,015
4,726

 90

02.40.10.40
05.20.20.10
06.30.10.00

0.70
0.05
0.03

26,625
1,599
1,268

Table 3: Average number of matching TUs (TUavg ) per segment and total number of target
words (Nwords ) for which a recommendation has to be provided when translating
Spanish into English for the three different domains. The results were obtained
for different values of the FMS threshold ().

where |x| is the length (in words) of string x and D(x, y) refers to the word-based Levenshtein
edit distance between x and y.22
6.4 Binary Classifier
Espla-Gomis et al. (2011) used a simple perceptron classifier which defined, for the translation of a source segment S 0 , the probability of keeping the j-th word in T , the targetlanguage segment of the TU (S, T ) as:
pk (j, S 0 , S, T ) =

1
1+

eg(j,S 0 ,S,T )

(4)

with
0

g(j, S , S, T ) = 0 +

NF
X

k fk (j, S 0 , S, T ).

(5)

k=1

This perceptron uses a sigmoid function that incorporates the linear combination of the
different features fk and the corresponding weights k learned by the classifier.
22. Many TM-based CAT tools implement variations of this FMS to rank the translation proposals as regards
the edition effort required (for instance, by disregarding punctuation signs or numbers in S and S 0 , or
using stemmed versions of S and S 0 ). In our experiments we continue to use the original FMS, since
ranking is not important in our experiments. This is owing to the fact that all the proposals above the
threshold are evaluated, and not only that with the highest score.

191

fiEspla-Gomis, Sanchez-Martnez & Forcada

In this work, a more complex multilayer perceptron (Duda, Hart, & Stork, 2000, Section 6) was used, namely, that implemented in Weka 3.7 (Hall et al., 2009). Multilayer
perceptrons (also known as feedforward neural networks) have a complex structure which
incorporates one or more hidden layers, consisting of a collection H of perceptrons, placed
between the input of the classifier (the features) and the output perceptron. This hidden
layer makes multilayer perceptrons suitable for non-linear classification problems (Duda
et al., 2000, Section 6). In fact, Hornik, Stinchcombe, and White (1989) proved that neural
networks with a single hidden layer containing a finite number of neurons are universal
approximators and may therefore be able to perform better than a simple perceptron for
complex problems. In this case, the output perceptron that provides the classification takes
the output hl of each of the perceptrons in H as its input. Eq. (5) therefore needs to be
updated as follows:
0

g(j, S , S, T ) = 0 +

|H|
X

l hl (j, S 0 , S, T ).

(6)

l=1

Each perceptron hl in H works similarly to the perceptron described in eq. (4):
hl (j, S 0 , S, T ) =

1
1 + egl (j,S 0 ,S,T ) )

with
gl (j, S 0 , S, T ) = l0 +

NF
X

lk fk (j, S 0 , S, T ).

k=1

As can be seen, besides the collection of weights  for the main perceptron, a different
collection of weights 0l is needed for each perceptron hl in the hidden layer H. These
weights are obtained by using the backpropagation algorithm (Duda et al., 2000, Section
6.3) for training, which updates them using gradient descent on the error function. In our
case, we have used a batch training strategy, which iteratively updates the weights in order
to minimise an error function. The training process stops when the error obtained in an
iteration is worse than that obtained in the previous 10 iterations.23
A validation set with 10% of the training examples was used during training, and the
weights were therefore iteratively updated on the basis of the error computed in the other
90%, but the decision to stop the training (usually referred as the convergence condition)
was based on this validation set. This is a usual practice whose objective is to minimise the
risk of overfitting.
Hyperparameter optimisation was carried out using a grid search (Bergstra, Bardenet,
Bengio, & Kegl, 2011) strategy based on the accuracy obtained for the EnglishSpanish
TM from the 02.40.10.40 domain. A 10-fold cross-validation was performed on this training
corpus in order to choose the following hyperparameters:
23. It is usual to set a number of additional iterations after the error stops improving, in case the function
is in a local minimum, and the error starts decreasing again after a few more iterations. If the error
continues to be worsen after these 10 iterations, the weights used are those obtained after the iteration
with the lowest error.

192

fiTarget-Language Edit Hints in CAT Tools Based on TM by Means of MT

 Number of nodes in the hidden layer : Weka (Hall et al., 2009) makes it possible
to choose from among a collection of predefined network designs; that which best
performed for our training corpus was that with the same number of nodes in the
hidden layer as the number of features.
 Learning rate: this parameter allows the dimension of the weight updates to be regulated by applying a factor to the error function after each iteration; the value that
best performed for our experiment was 0.4.
 Momentum: when updating the weights at the end of a training iteration, momentum
modifies the new value, signifying that it not only depends on the current gradient
direction, but also on the previous weight value. The objective of this technique is to
smooth the training process for faster convergence. In the case of our experiments, it
was set to 0.1.
6.5 Reference Results
As mentioned previously, the performance of the two MT-based approaches proposed in this
work is compared to that of two different approaches: a nave FMS-only baseline, which
uses the classifier described in Section 6.4 and employs only the FMS between S 0 and S as
a feature, and the approach reviewed in Section 3, which uses statistical word alignment
to relate the words in the two segments of a TU (S, T ). The nave FMS-only baseline was
trained on the datasets described in Section 6.2 for different values of the FMS threshold
. It is worth mentioning that the resulting models classify all the target words as having
to be kept. This is a consequence of the fact that, for any value of the FMS in the training
set, there are more words to be kept than to be changed.
The alignments used by the alignment-based approach were obtained by means of
the free/open-source MGIZA++ toolkit (Gao & Vogel, 2008), an implementation of the
GIZA++ toolkit (Och & Ney, 2003) which eases the task of training alignment models on
a parallel corpus and then aligning a different one using the models learned. The wordbased alignment models (Brown et al., 1993; Vogel, Ney, & Tillmann, 1996) were separately
trained on the TMs defined in Section 6.2 and on the JRC-Acquis 3.0 (Steinberger et al.,
2006) corpus (a large multilingual parallel corpus which includes, among others, texts from
these TMs, given that it is built from the same texts as the DGT-TM).24 The alignments
we have used are the result of running MGIZA++ in both translation directions (source-totarget and target-to-source) and then symmetrising both sets of alignments by means of the
usual grow-diag-final-and (Koehn et al., 2005) heuristic. This symmetrisation technique
was found to be that which provided the best compromise between coverage and accuracy
for word-keeping recommendation (Espla et al., 2011).25
Table 4 shows the accuracy obtained by the nave FMS-only baseline. The fraction
of words not covered, that is, the words for which no recommendation is provided, is not
included in this table since this baseline provides a recommendation for every word in the
24. https://ec.europa.eu/jrc/en/language-technologies/jrc-acquis [last visit: 15th May 2015]
25. Symmetrisation is necessary because MGIZA++ produces alignments in which a source word can be
aligned with many target words, whereas a target word is aligned with at most one source word. The
use of symmetrisation allows alignments to be combined in both directions in order to obtain M to N
alignments.

193

fiEspla-Gomis, Sanchez-Martnez & Forcada

(%)

A(%)

 60
 70
 80
 90

82.69.24
88.37.25
91.65.25
93.98.29

Table 4: Accuracy A(%) obtained with the nave FMS-only baseline when translating
enes in domain 02.40.10.40. Accuracy was obtained for different FMS thresholds
. The other language pairs and domains behave in the same way.

test set. This is due to the fact that the nave FMS-only baseline does not depend on the
coverage of a source of information.
In general, we can see that the accuracy obtained with the nave FMS-only baseline
is quite high. This is, in fact, a hard-to-beat nave baseline, although these results are
reasonable, since the relatively high values of the FMS threshold  imply that a high
number of words should be kept unedited in the translation proposals.
With regard to the alignment-based approach, several options were evaluated in order
to choose its configuration. On the one hand, we tried the two decision criteria described
in Section 3 (unanimity and majority). On the other hand, we tried two alignment models:
one trained on the same translation memory used for the experiments (as had occurred
with the trained MT-based recommender), and another trained on the JRC Acquis parallel
corpus. The objective of comparing both models was to confirm which corpus was the most
adequate as regards training our alignment-based recommender: one which was reduced
and domain focused, or one which was bigger and more generic, although still containing
text in the same domain. The results are presented in Table 5, in which the accuracy
and percentage of words not covered are measured for the four combinations of decision
criteria and training corpora. As already mentioned in Section 3, the unanimity criterion
is more focused on accuracy, while the majority criterion is more focused on coverage. In
order to confirm which method was better, a statistical significance test was performed on
the results obtained by using an approximate randomisation test.26 The free/open-source
tool SIGF V.2 (Pado, 2006) was used for the statistical significance testing of the results
described throughout this section. The test confirmed that in both cases the alignment
models trained on the TM used for testing outperform those trained on the JRC Acquis
corpus. The approach trained on the TM used for testing will be used in all the experiments
shown in the following section, while the decision criterion used will be that of unanimity,
the reason being that we consider accuracy to be more relevant than coverage for wordkeeping recommendation, since as mentioned above, we believe that it is better not to make
a recommendation than to make a wrong one.
26. Approximate randomisation compares the difference between the accuracy/coverage of two classifiers
in the same test set. This method randomly interchanges the predictions of both classifiers in every
instance of the test set. The difference between the accuracy/coverage in both randomised datasets is
then compared to the original set. This process is iterativelly repeated to confirm whether the results as
regards randomised predictions are consistently worse than the original results.

194

fiTarget-Language Edit Hints in CAT Tools Based on TM by Means of MT

(%)

training on 02.40.10.40

method

training on JRC Acquis

A (%)

NC (%)

A (%)

NC (%)

 60

unanimity
majority

93.90.16
92.96.17

6.09.15
4.39.13

93.14.17
93.05.17

6.29.15
5.39.14

 70

unanimity
majority

94.32.18
93.47.19

5.90.18
4.45.16

93.67.19
93.59.19

6.15.18
5.42.17

 80

unanimity
majority

95.10.20
94.49.21

5.37.21
4.31.19

94.56.21
94.52.21

5.87.21
5.33.20

 90

unanimity
majority

95.34.26
95.10.27

4.93.26
4.35.25

94.97.27
94.95.27

5.48.27
5.04.26

Table 5: Accuracy (A) and fraction of words not covered (NC) obtained with the alignmentbased approach described in Section 3 for different FMS thresholds , when translating Spanish into English in domain 02.40.10.40. The results show the accuracy
obtained when using a model trained on the TM belonging to the 02.40.10.40 domain and on the JRC-Acquis corpus, both using the unanimity and the majority
decision criteria. This behaviour is also observed for the remaining TMs used in
the experiments. Statistically significant differences in the accuracy of each approach for the different values of  with p  0.05 are highlighted in bold type, as
also occurs for the fraction of words not covered.

7. Results and Discussion
In this section we present the results obtained by the two approaches proposed in this paper
and compare their performance with the nave FMS-only baseline and the alignment-based
approach of Espla et al. (2011). The large amount of variables to be taken into consideration
(feature sets, language pairs, domains, MT systems, and sub-segment length) forced us to
select the experiments to be performed. Some parameters were therefore chosen on the basis
of the results obtained for the translation from Spanish into English, which is the language
pair used by Espla et al. (2011) and Espla-Gomis et al. (2011). The domain chosen for these
preliminary experiments is elimination of barriers to trade (02.40.10.40), which has higher
matching rates (see Table 3) and is therefore that from which more data can be obtained.
7.1 Parameter Selection
We first attempted to determine the optimal sub-segment maximum length L for the experiments with the training-free recommender and with the trained recommender. Table
6 shows the fraction of words not covered depending on the value of L for both recommenders together. The fraction of words not covered is between 16% and 19% when using
sub-segments of only one word, and the percentage diminishes as more context is provided
for translations. As can be seen, the fraction of words not covered starts to stabilise with
L = 4, since the difference between this and L = 5 is only about 0.25%.
Table 7 shows the impact of the value of L on the accuracy obtained by the trainingfree recommender and by the trained recommender when using the different sets of features
195

fiEspla-Gomis, Sanchez-Martnez & Forcada

(%)
 60
 70
 80
 90

Fraction of words without recommendation (%)
L=1

L=2

L=3

L=4

L=5

16.42.24
16.74.28
17.37.34
18.18.46

10.22.19
10.66.24
11.25.29
11.80.39

7.24.16
7.34.20
7.65.24
8.05.33

5.13.14
5.18.17
5.53.21
5.86.28

4.90.14
4.94.17
5.29.20
5.59.28

Table 6: Percentage of words not covered by both MT-based approaches for the enes language pair in domain 02.40.10.40 using a combination of all MT systems available.
The fraction of words not covered was obtained for different FMS thresholds 
when using different values of the maximum sub-segment length L.

(%)

Accuracy (%) in classification

Method
L=1

L=2

L=3

L=4

L=5

 60

MM-U
PM-C
PM-C+C
training-free

93.51.17
93.62.17
93.59.17
93.63.17

93.40.17
94.07.16
94.36.15
93.78.16

93.58.16
94.31.15
94.57.15
93.79.16

93.57.16
94.18.15
95.14.14
93.27.16

93.77.16
94.37.15
95.41.14
92.90.17

 70

MM-U
PM-C
PM-C+C
training-free

94.79.19
94.75.19
94.81.19
94.76.19

94.72.18
94.89.18
95.16.17
94.77.18

94.77.18
95.12.17
95.33.17
94.77.18

94.70.18
94.94.17
95.63.16
94.14.18

94.82.17
95.05.17
95.92.16
93.78.19

 80

MM-U
PM-C
PM-C+C
training-free

96.09.19
96.09.19
96.11.19
96.05.20

96.14.19
96.14.19
96.24.18
95.97.19

95.92.19
96.11.18
96.34.18
95.88.19

96.00.18
96.01.18
96.39.17
95.29.20

96.02.18
95.98.18
96.58.17
94.98.20

 90

MM-U
PM-C
PM-C+C
training-free

96.84.23
96.84.23
96.84.23
96.80.23

96.85.22
96.82.23
96.95.22
96.72.23

96.80.22
96.87.22
96.95.22
96.70.22

96.74.22
96.85.22
96.90.21
96.61.22

96.75.22
96.87.22
97.00.21
96.42.23

Table 7: Accuracy obtained by the trained MT-based recommender when using the different
feature combinations described in Section 4 and by the training-free MT-based
recommender for the enes language pair. Accuracy was obtained for different
FMS thresholds  when using different values of the maximum sub-segment length
L. Statistically significant accuracy results for L = 4 (the value of L that will be
used for the remaining experiments) with p  0.05 are highlighted in bold type.

196

fiTarget-Language Edit Hints in CAT Tools Based on TM by Means of MT

described in Section 4. As can be seen, the accuracy of the training-free system drops
slightly as longer sub-segments are introduced. This is reasonable since the longer the subsegments used, the higher the number of words for which a recommendation is made (see
Table 6). Words which are covered only by very long sub-segments are more difficult to
classify, since these sub-segments contain evidence regarding more words and are therefore
less precise. It is interesting to observe that, in the case of most of the feature sets, the
trained recommender does not behave in this manner, since it is able to learn how reliable
the longer sub-segments are. In the case of the feature set MM-U, the accuracy is almost
constant for all the values of L, which means that using longer sub-segments does not
have an impact on the accuracy in this case. In any event, it is worth noting that the
results obtained using the training-free recommender are quite accurate, which confirms
that the sub-segment pairs discovered using MT are a good source of information for wordkeeping recommendation. Moreover, these results indicate that long sub-segments are less
informative than short sub-segments. In general, only small improvements in accuracy and
coverage occur for values of L that are higher than 4. The remaining experiments in this
section will therefore be performed with L = 4.
The results in Table 7, namely those in the column with L = 4, were also used to
determine which is the best feature combination for the trained MT-based recommender.
At first glance, the set of features based on matching words, namely PM-C (see Section 4.2),
and PM-C+C (see Section 4.3), are those which perform best. As commented on in Section
4.2, MM-U features consider partial matching sub-segments as negative evidence, while
PM-C and PM-C+C also attempt to extract the positive evidence from these sub-segments,
thus using this bilingual information more efficiently. However, the results that they obtain
are very close, particularly in the case of high values of . A statistical significance test
confirmed that PM-C+C is superior to all the other feature combinations for any value of
 with p  0.05. The feature set PM-C+C will therefore be used for the trained classifier
approach in the remaining experiments in this section.
The accuracy obtained using all the approaches presented in both this and the previous
section is lower than expected, particularly when considering the results obtained by EsplaGomis et al. (2011) and Espla et al. (2011) in which both the trained MT-based approach
and the alignment-based approach obtained an average accuracy that was about 5% higher
than that obtained in our experiments. Our intuition leads us to believe that this drop in
accuracy may be due to the fact that the data sets used in previous works might have been
cleaner than those used here. To confirm this, an additional set of experiments was carried
out using some additional cleaning criteria to ensure the quality of the datasets used for
evaluation. The results of this study are presented in Appendix B.
7.2 General Results
The parameters chosen (maximum sub-segment length L = 4 for the MT-based approaches,
PM-C+C feature set for the trained MT-based approach, and unanimity criterion and
models trained on the TM to be used in the experiments for the alignment-based approach)
were used to perform several experiments in order to check the performance of our system.
Tables 8 and 9 show the results obtained by the trained MT-based recommender when
translating Spanish segments into English. In Table 8, the different MT systems available
197

fiEspla-Gomis, Sanchez-Martnez & Forcada

(%)
 60
 70
 80
 90

Apertium

Google Translate

Power Translator

A (%)

NC (%)

A (%)

NC (%)

A (%)

NC (%)

94.61.17
95.40.19
96.49.20
97.25.23

27.89.28
28.32.34
28.39.41
28.49.54

95.08.14
95.54.16
96.34.18
96.99.21

6.22.15
6.38.19
6.52.22
6.31.29

94.52.17
95.47.19
96.46.20
97.01.24

28.99.29
29.62.35
29.59.42
28.50.54

Table 8: Accuracy (A) and fraction of words not covered (NC) obtained when translating
esen in domain 02.40.10.40 with the trained MT-based approach. The results
were obtained with the separate use of the MT systems available for the language
pair: Apertium, Google Translate, and Power Translator. For every value of ,
those results that supersede the rest by a statistically significant margin of p  0.05
are highlighted in bold type, both for accuracy and for the fraction of words not
covered.

were used separately to obtain the recommendations in the 02.40.10.40 domain. The results
confirm that, while accuracy remains stable,27 coverage strongly depends on the MT system.
This may be interpreted as follows: MT-based approaches are robust to bilingual sources
of information with low coverage. The experiments confirmed that the best coverage is
obtained with Google Translate, whereas Apertium and Power Translator produce similar
results. However, Apertium and Power Translator produce higher precision for change
recommendations, while all three MT systems perform similarly as regards the precision for
keep recommendations.
Table 9 compares the performance of the alignment-based approach and the trained
MT-based recommender when all three MT systems available are used at the same time
for the language pair esen. The table shows the results obtained separately for the
alignment-based approach and the trained MT-based approach as regards the three domains: 02.40.10.40, 05.20.20.10 and 06.30.10.00. The results are quite similar for both
approaches. The MT-based approach slightly outperforms the alignment-based approach
in accuracy, while the results of the alignment-based approach are better for coverage, particularly in the case of the 06.30.10.00 domain. In any event, this leads us to believe that
both approaches can obtain comparable results across domains.
Tables 10 and 11 present the results as regards the accuracy and fraction of words
not covered, respectively, obtained with both the trained MT-based approach and the
alignment-based approach for all language pairs. In the case of the trained MT-based
approach, all the MT systems available were used (Table 1 lists the MT systems available
for each language pair).
The results confirm the hypothesis that the alignment-based approach generally obtains
better results as regards coverage than the MT-based approach for most language pairs,
which is reasonable, given that the alignment models have been trained on the same TM
to be used for translation. Accuracy is yet again the strongest point of the trained MT27. Although some differences in accuracy can be observed, it is only possible to state which MT system is
better with a statistical significance of p  0.05 in the case of =60% and =90%.

198

fiTarget-Language Edit Hints in CAT Tools Based on TM by Means of MT

02.40.10.40

(%) method

A (%)
trained
alignment
trained
 70
alignment
trained
 80
alignment
trained
 90
alignment

 60

95.14.1
93.90.2
95.63.2
94.32.2
96.39.2
95.10.2
96.90.2
95.34.3

05.20.20.10

NC (%)
5.13.1
6.09.2
5.18.2
5.90.2
5.53.2
5.37.2
5.86.3
4.93.3

A (%)

06.30.10.00

NC (%)

95.62.4
92.97.5
97.02.4
94.16.6
95.78.8
94.56.8
95.361.1
94.261.2

7.20.5
6.22.5
6.44.6
5.96.6
10.421.1
5.51.8
11.131.5
5.191.1

A (%)

NC (%)

94.87.3
91.38.3
94.93.5
94.57.5
95.00.7
95.20.6
97.65.9
96.471.1

11.33.3
18.31.4
10.55.6
7.78.5
12.841
4.80.6
9.311.6
6.151.3

Table 9: Accuracy (A) and fraction of words not covered (NC) obtained with the trained
MT-based approach and the alignment-based approach when translating esen in
the three different domains and using all the MT systems. For each corpus and for
each value of , a statistical significance test was performed for both approaches
as regards both the accuracy and the fraction of words not covered. Those results
that show an improvement which is statistically significant with p  0.05 are
highlighted in bold type.

  60(%)
lang.
pair
esen
enes
deen
ende
fren
enfr
fien
enfi
esfr
fres

trained
95.1.1
90.0.2
94.2.2
88.9.2
95.2.1
89.7.2
93.2.2
89.1.2
91.2.2
89.4.2

  70(%)

alignment

trained

93.9.2
88.7.2
92.6.2
87.9.2
93.4.2
89.4.2
91.9.2
87.1.2
89.4.2
88.6.2

95.5.2
91.5.2
95.8.2
91.1.2
96.3.2
91.9.2
94.6.2
90.2.3
93.2.2
92.1.2

alignment
94.3.2
89.8.2
93.8.2
89.7.2
95.1.2
91.3.2
93.1.2
88.5.3
90.7.2
91.1.2

  80(%)
trained
96.3.2
92.3.2
96.6.2
93.0.2
97.0.2
93.3.2
94.7.2
90.3.3
94.8.2
93.5.2

alignment
95.1.2
90.4.2
94.7.2
91.5.2
96.0.2
92.1.2
93.3.3
88.4.3
92.8.2
92.2.2

  90(%)
trained

alignment

97.0.2
93.0.2
97.2.2
92.8.3
97.6.2
95.7.2
94.8.3
90.6.4
96.0.2
93.6.3

95.3.3
91.8.3
95.5.3
91.5.3
96.4.2
94.3.3
92.8.4
88.9.4
94.3.2
92.2.3

Table 10: Accuracy (A) obtained with both the trained MT-based approach and the
alignment-based approach when translating between all the language pairs in
domain 02.40.10.40. The results were obtained for several values of the FMS
threshold  and using all available MT systems for each language pair. For each
language pair and for each value of , a statistical significance test was performed
between the accuracy obtained by both approaches. Those results that show an
improvement which is statistically significant with p  0.05 are highlighted in
bold type.

199

fiEspla-Gomis, Sanchez-Martnez & Forcada

  60(%)
lang.
pair

trained

esen
enes
deen
ende
fren
enfr
fien
enfi
esfr
fres

6.2.1
7.3.1
10.5.2
11.6.2
7.7.2
7.9.1
11.2.2
11.6.2
17.8.2
19.5.2

alignment
6.1.1
7.7.1
5.8.1
5.6.1
4.3.1
5.9.1
9.9.2
7.2.2
4.9.1
10.3.1

  70(%)

  80(%)

trained

alignment

trained

6.4.2
8.3.2
10.7.2
12.4.2
7.7.2
8.8.2
11.3.3
11.0.3
18.4.2
19.4.3

5.9.2
8.8.2
6.2.2
6.2.2
4.1.2
6.0.2
10.2.2
7.0.2
4.9.1
9.5.2

6.5.2
8.6.2
10.9.3
13.0.3
8.1.3
9.6.2
11.5.3
12.0.3
18.6.3
20.4.3

alignment
5.4.2
9.4.2
6.0.2
6.5.2
4.2.2
6.0.2
10.6.3
7.2.2
5.0.2
10.5.2

  90(%)
trained

alignment

6.3.3
9.5.3
12.4.4
14.5.4
7.6.3
9.4.3
11.6.4
12.6.4
21.2.4
17.6.4

4.9.3
10.7.3
6.0.3
6.6.3
3.5.2
7.8.3
10.9.4
8.0.3
5.6.2
5.9.2

Table 11: Fraction of words not covered (NC) obtained with both the trained MT-based
approach and the alignment-based approach when translating between all the
language pairs in domain 02.40.10.40. The results were obtained for several values of the FMS threshold  and using all available MT systems for each language
pair. For each language pair and for each value of , a statistical significance
test was performed between the fraction of words not covered obtained by both
approaches. Those results that show an improvement which is statistically significant with p  0.05 are highlighted in bold type.

200

fiTarget-Language Edit Hints in CAT Tools Based on TM by Means of MT

based recommender. Another interesting detail is that the experiments for language pairs
with English as the target language obtain better accuracy than the experiments for the
inverse language pairs. This is due to the fact that in the DGT-TM, it is usual to find free
translations in languages other than English, which is the original language of most of the
documents in this TM.28 It is particularly frequent to find additional information about
technical English words in the other languages. For example, when software is translated
into Spanish, the translated segments include the text equipo logico (software). The
translation includes both the correct translation and the original word, in order to keep the
meaning of the original English segment. This is an important issue since, when these free
translations are used as a reference to evaluate the accuracy of the approaches presented in
this work, they lead to lower accuracy. This problem is analysed, and partially bypassed, in
the additional experiments presented in Appendix B. As can be seen, the trained MT-based
approaches provide, in general, better accuracy. However, for most of the language pairs,
the coverage obtained with the alignment-based approach is much better. Nevertheless the
results are still reasonably similar and can be considered comparable.
Table 12 shows the results obtained with the different approaches:29 the nave FMSonly baseline, the alignment-based approach, and both the trained and the training-free
MT-based approaches, for the esen language pair in the 02.40.10.40 domain and using all
the MT systems available simultaneously. This table provides more detail: in addition to
the accuracy and percentage of words not covered, it also includes the precision and recall
for both keeping recommendations and changing recommendations. This table allows to
better understand the differences between the approaches before starting a more complex
comparison in different scenarios. Throughout this section we provide information regarding
precision and recall where it is significant for all the tables presented.
Leaving aside the nave FMS-only baseline, it can be observed that the accuracy is
similar for all the approaches, and that the trained MT-based recommender obtains slightly
better results. As already mentioned, the amount of words not covered is very similar for
the MT-based approaches and the alignment-based approach. As regards precision, the
trained MT-based approach seems to outperform the others, although all the approaches
obtain comparable scores. These results are coherent with those obtained for the rest of
language pairs: in general recall and precision in keep recommendations are similar for
both approaches, while the MT-based approach seems to be more precise in the case of
change recommendations, where the difference is much higher, specially for higher values
of the FMS threshold . These conclusions are extensible to the data shown in Table 13.
The results shown in Table 12 were extended by repeating the experiment and computing
recommendations only for content words (i.e. ignoring stopwords). This was done by using
the list of stopwords provided in the 4.7.2 version of Lucene30 for the language utilised in
our experiments. The results of this experiments can be found in Table 13. As can be seen,
the results do not change much, thus confirming that all the approaches perform equally
well with content and stopwords.
28. According to Steinberger et al. (2012), English is the source language in 72% of the documents.
29. The nave FMS-only baseline did not recommend that any word be changed, as explained in Section 6.5,
signifying that Pc cannot be computed for this approach and Rc is always 0. Similarly, all the words in
this approach are covered, and therefore Rk is always 1.
30. http://lucene.apache.org/core/ [last visit: 15th May 2015]

201

fiEspla-Gomis, Sanchez-Martnez & Forcada

(%)

Method

A (%)

NC (%)

Pk (%)

Rk (%)

Pc (%)

Rc (%)

 60

FMS-only
alignment
trained
training-free

82.7.2
93.9.2
95.1.1
93.3.2

100%0
6.1.2
5.1.1
5.1.1

82.7.2
96.3.1
96.5.1
95.2.1

100%0
96.5.1
97.8.1
96.8.1


80.8.3
87.6.2
82.2.3

0%0
80.2.3
81.8.3
74.9.3

 70

FMS-only
alignment
trained
training-free

88.4.3
94.3.2
95.6.2
94.1.2

100%0
5.9.2
5.2.2
5.2.2

88.4.3
96.6.1
96.8.1
95.9.2

100%0
97.2.1
98.4.1
97.7.1


73.0.4
83.7.3
75.8.3

0%0
69.1.4
71.8.4
63.6.4

 80

FMS-only
alignment
trained
training-free

91.7.3
95.1.2
96.4.2
95.3.2

100%0
5.4.2
5.5.2
5.5.2

91.7.3
96.8.2
97.2.2
96.5.2

100%0
98.0.1
99.0.1
98.5.1


68.4.4
80.8.4
71.0.4

0%0
57.2.5
61.1.5
51.3.5

 90

FMS-only
alignment
trained
training-free

94.0.3
95.3.3
96.9.2
96.6.2

100%0
4.9.3
5.9.3
5.9.3

94.0.3
96.5.2
97.3.2
97.4.2

100%0
98.7.1
99.6.1
99.2.1


57.1.6
72.9.6
60.2.6

0%0
32.4.6
30.1.6
32.6.6

Table 12: Comparison of the results obtained using the trained MT-based approach, the
training-free MT-based approach, the alignment-based approach and the nave
FMS-only baseline. The accuracy (A) and fraction of words not covered (NC) are
reported, together with the precision (P ) and recall (R) as regards both keeping
recommendations and changing recommendations. The results were obtained
for several values of the FMS threshold  when translating esen in domain
02.40.10.40, using all the MT systems available. Statistically significant results
with p  0.05 are highlighted in bold type. For some values of  two values
are highlighted in the same column; this means that there is not a statistically
significant difference between these results, but both of them are significantly
better than the other values.

202

fiTarget-Language Edit Hints in CAT Tools Based on TM by Means of MT

(%)

Method

A (%)

NC (%)

Pk (%)

Rk (%)

Pc (%)

Rc (%)

 60

FMS-only
alignment
trained
training-free

81.2.3
93.7.2
95.1.2
93.3.2

100%0
6.2.2
5.6.2
5.6.2

81.2.3
96.1.1
96.5.1
95.3.2

100%0
96.4.1
97.6.1
96.6.1


82.1.3
88.1.2
83.0.3

0%0
80.8.3
83.7.3
77.9.3

 70

FMS-only
alignment
trained
training-free

87.3.3
94.0.2
95.6.2
93.9.2

100%0
5.9.2
5.7.2
5.7.2

87.3.3
96.3.2
96.9.2
95.9.2

100%0
97.0.1
98.2.1
97.3.1


74.0.4
84.3.3
76.0.4

0%0
69.6.4
74.9.4
67.1.4

 80

FMS-only
alignment
trained
training-free

90.8.3
94.9.2
96.4.2
95.1.2

100%0
5.2.2
6.1.2
6.1.2

90.8.3
96.6.2
97.3.2
96.5.2

100%0
97.9.1
98.8.1
98.2.1


70.5.5
81.6.4
71.5.5

0%0
58.7.5
65.6.5
55.4.5

 90

FMS-only
alignment
trained
training-free

93.4.3
94.9.3
96.9.2
96.5.3

100%0
4.5.3
6.6.3
6.6.3

93.4.3
96.1.3
97.3.2
97.4.2

100%0
98.6.2
99.5.1
99.0.1


58.9.7
74.0.6
60.5.7

0%0
33.0.7
34.5.7
36.9.7

Table 13: Comparison of the results obtained using the trained MT-based approach, the
training-free MT-based approach, the alignment-based approach and the nave
FMS-only baseline. The accuracy (A) and fraction of words not covered (NC)
are reported, together with the precision (P ) and recall (R) for both keeping
recommendations and changing recommendations. Unlike Table 12, these metrics
were computed on content words only. The results were obtained for several
values of the FMS threshold  when translating esen in domain 02.40.10.40,
using all the MT systems available. Statistically significant results with p  0.05
are highlighted in bold type. For some values of  two values are highlighted in
the same column; this means that there is not a statistically significant difference
between these results, but both of them are significantly better than the other
values.

203

fiEspla-Gomis, Sanchez-Martnez & Forcada

training
language pair
esen
enes
deen
ende
fren
enfr
fien
enfi
esfr
fres
training-free

(%)
 60

 70

 80

 90

95.08.14
90.84.19
92.52.17
92.20.18
92.65.17
90.91.19
92.38.17
91.05.19
92.36.17
91.91.18
93.36.16

95.54.16
91.80.22
93.46.20
93.22.20
94.16.19
92.23.21
93.93.19
93.36.20
93.40.20
92.24.21
94.28.18

96.34.18
93.35.23
95.09.20
94.31.22
95.78.19
93.84.23
95.30.20
95.20.20
94.56.21
93.16.24
95.41.20

96.99.21
94.94.27
96.41.23
94.88.27
96.67.22
95.24.26
96.35.23
96.47.23
96.42.23
95.49.26
96.72.22

Table 14: Accuracy obtained by the trained MT-based recommender when translating
Spanish into English in domain 02.40.10.40 by using recommendation models
trained for other language pairs in the same domain. The first row, highlighted
in grey, corresponds to the reference results obtained with the model trained
on esen. Only Google Translate was used for this experiment. Statistically
significant results with p  0.05 are highlighted in bold type. For some values
of  two values are highlighted in the same column; this means that there is
no statistically significant difference between these results, but both of them are
significantly better than the other values.

The experiments carried out up to this point have confirmed that the three approaches
proposed in this work perform similarly in different scenarios. While the word-alignment
based approach provides the highest coverage and is, therefore, able to provide more recommendations, the MT-based approaches are more robust and obtain higher and more stable
accuracy independently of the language pair or domain used. These results have led us
to believe that no approach clearly stands out as being better, and that all of them may
be useful in different scenarios, depending on the resources available and the translation
conditions.
7.3 Experiments on Reusability Across Language Pairs
Table 14 presents the results obtained with the trained MT-based recommender when used
in the same domain but with a different language pair. The experiments were performed
in domain 02.40.10.40 when translating Spanish segments into English and re-using models
trained on other language pairs. The results obtained with the model trained on this
pair of languages are included in the table to give an idea of the upper-bound, and the
corresponding row is filled in grey. For all the models used, both for training and testing,
the only source of information used was Google Translate, since it is the only MT system
which is available for all the language pairs used in our experiments.
204

fiTarget-Language Edit Hints in CAT Tools Based on TM by Means of MT

The results show a clear decline as regards the results obtained when the recommendation model is learned for esen and when it is learned from the other language pairs,
particularly for low values of the FMS threshold . In most cases, the accuracy obtained
when training the recommender on language pairs that are different from those used for
testing is worse than that obtained using the training-free approach. The only exception
is the model trained on the fren pair, which is the most similar pair to esen. The
statistical significance test confirms that, for all the values of , either this model or the
training-free approach are the best ones. The difference between the accuracy obtained for
these approaches for =70% and for =90% is not in fact statistically significant.
These results have led us to believe that the trained method is highly dependent on the
language pair used for training, thus making it reasonable to conclude that it is better to
use the training-free MT-based recommender than an MT-based recommender trained on
a different language pair.
7.4 Experiments on Reusability Across Domains
Table 15 presents the results of the experiments concerning domain independence. The objective of these experiments is to verify how dependent the trained MT-based recommender
is on the domain of the training corpus. In this case, we re-used the recommendation models
trained in the three domains for the esen translation to translate Spanish segments from
domain 02.40.10.40 into English, and using all the MT systems available.
A drop in accuracy can be observed when re-using models trained on out-of-domain
TMs rather than training on the TM to be used for translation. However, in this case
the accuracy is closer to that obtained when the recommendation model is trained on
the same TM used for testing (in-domain). With regard to the results obtained with the
alignment-based approach, the difference in accuracy of all the MT-based approaches is
higher and statistically significant with p  0.05. That is to say, training-free MT-based
approach and the models trained on domain 05.20.20.10 are those which perform best, with
no statistically significant difference for most values of . Similarly, the coverage of the
alignment-based approach clearly drops when using out-of-domain models. This is due to
the fact that, in the case of the alignment-based approach, those words which were not seen
during training cannot be aligned, since no translation probabilities are learned for them.
In contrast, in the case of the MT-based approaches the linguistic resources are not learned
during training, but are rather obtained from the MT systems available: the training of the
MT-based recommender instead focuses on the relevance of sub-segment lengths and the
amount of sub-segment pairs covering each word. In general, the conclusion drawn from this
experiment is that using either the training-free approach or a classification model trained
on a corpus from a different domain are both valid options and they perform better than
the alignment-based approach. Having a closer look to the data one can observe that the
bigger differences are in the precision for change recommendations, where the MT-based
approach outperforms the alignment-based approach.
7.5 Experiments on Reusability Across Machine Translation Systems
Table 16 presents the results of the experiments concerning MT system independence. Three
models were trained on the three TM belonging to domain 02.40.10.40, but in each case
205

fiEspla-Gomis, Sanchez-Martnez & Forcada

alignment

MT-based
training-free
A (%)

(%)

training corpus

 60

02.40.10.40
05.20.20.10
06.30.10.00

93.90.16 6.09.15
91.77.18 9.63.19
90.23.20 11.67.20

95.14.14
93.34.16
92.02.18

93.27.16

5.13.14

 70

02.40.10.40
05.20.20.10
06.30.10.00

94.32.18 5.90.18
92.68.21 9.71.23
91.60.23 11.61.25

95.63.16
94.03.19
92.74.20

94.14.18

5.18.17

 80

02.40.10.40
05.20.20.10
06.30.10.00

95.10.20 5.37.21
93.82.23 9.20.26
93.21.24 11.05.28

96.39.17
95.56.19
94.27.22

95.29.20

5.53.21

 90

02.40.10.40
05.20.20.10
06.30.10.00

95.34.26 4.93.26
94.61.28 8.90.34
94.20.30 10.27.37

96.90.21
96.53.23
96.31.23

96.61.22

5.86.28

A (%)

NC (%)

trained
A (%)

NC (%)

Table 15: Accuracy (A) and fraction of words not covered (NC) obtained by the alignmentbased recommender, the trainng-free MT-based recommender, and the trained
MT-based recommender when translating Spanish segments into English in domain 02.40.10.40. The results were obtained after re-using recommendation and
alignment models learned from the TMs belonging to the domains indicated in the
second column. The results obtained with models trained on domain 02.40.10.40
are included (highlighted in gray) as a reference. All the MT systems available
were used for both training and testing. Statistical significance tests were carried
out, separately for accuracy and for the fraction of words not covered. Differences
in the results which are statistically significant with p  0.05 are highlighted in
bold type. For most values of , the difference between the accuracy of both
MT-based approaches is not statistically significant, but their differences with
the alignment-based approach are statistically significant with p  0.05.

206

fiTarget-Language Edit Hints in CAT Tools Based on TM by Means of MT

A (%) trained

(%)
 60
 70
 80
 90

Apertium

Google Translate

92.94.17
93.41.20
95.57.19
96.65.22

95.08.14
95.54.16
96.34.18
96.99.21

A (%) training-free
Power Translator
91.12.19
93.02.20
94.78.21
96.47.23

93.36.16
94.28.18
95.41.20
96.72.22

Table 16: Accuracy (A) obtained by the trained MT-based recommender and the trainingfree MT-based recommender when translating Spanish segments into English in
domain 02.40.10.40 and using Google Translate as the MT system for testing. For
the trained approach, the results were obtained after re-using recommendation
models learned from the same TM and language pairs but using the three different
MT systems. The results obtained using a model trained on Google (column in
gray) are included as an upper-bound, but are not included in the comparison.
For every value of  the highest accuracy, with a statistically significant difference
with p  0.05 compared to the other values, is highlighted in bold type.

using one of the three MT systems available. The models were used to translate segments
in Spanish into English within the same domain and using Google Translate as the MT
system, in order to obtain the sub-segment translations during testing. The results in this
table are similar to those presented in the last set of experiments, in which the reusability
across different domains was studied. In general terms, it would appear that the drop in
accuracy when making change recommendations is quite similar for the models trained
on Apertium and Power Translator. In addition, we observed that the accuracy obtained
for these two models is similar to that obtained by the training-free recommender. The
training-free approach is, in fact, that which performs best for   60% and   70% and
the difference in accuracy is statistically significant with p  0.05. However, for   80%
the trained approach using a model trained with Apertium is that which performs best.
Finally, there is no difference among the results for  = 90%.
In general, it would appear that re-using models trained on an MT system which is
different to that used for translation is feasible, although using the training-free approach
can provide better results.
7.6 Error Analysis
The following is a sample of the most frequent errors made by the different approaches
proposed in this work for word-keeping recommendation. The objective of this error analysis
is to propose strategies to deal with these errors (when possible) in future work.
7.6.1 Errors Caused by Synonyms or Equivalent Expressions
Some of the incorrect change recommendations in our experiments resulted from the use
of different synonyms in the translation proposal T and the reference T 0 used as a gold
standard. Let us suppose a translation proposal (S, T )= (the natural isotopic abundance
207

fiEspla-Gomis, Sanchez-Martnez & Forcada

of lithium-6 is approximately 6,5 weight per cent (7,5 atom per cent)., la proporcion
natural del isotopo litio-6 es de aproximadamente 6,5% del peso (7,5% de atomos).), for
the sentence S 0 = the natural isotopic abundance of lithium-6 is approximately 6,5 weight
% (7,5 atom %). whose reference translation is T 0 = la proporcion natural del isotopo 6
en el litio es de aproximadamente 6,5% en peso (7,5% de atomos).. As can be seen, S and
S 0 are semantically equivalent and are written almost the same, although the percentage
symbol (%) is used in S 0 , while the expression per cent is used in S. Although these two
options are equivalent, per cent is not considered to be part of the matching between S and
S 0 in any of the occurrences. A sub-segment pair (,  ) with  =per cent and  =%,
may have led the two occurrences of the symbol % in T to be changed, when this is obviously
not necessary. Since the symbol % was also used in the reference translation T 0 , these are
in fact considered to be wrong recommendations in the evaluation.
7.6.2 Errors Caused by Morphological Differences Between the Languages
This problem is in some respects similar to the previous one, although here the problem does
not concern using different words for the same concept, but rather the presence of a word
in one of the languages that may have different morphologies in the other language. For
example, we found the proposal (S, T )= (optical equipment as follows:,equipo optico
segun se indica:), for the sentence S 0 = optical detectors, as follows: whose reference
translation is T 0 = detectores opticos segun se indica:. In this case the word optical is
matched in both S and S 0 , being singular in S and plural in S 0 . While in English both
forms share the same orthography, in Spanish, the plural mark is added in T 0 (opticos),
therefore differing from the singular form in T (optico). As a result, the word optico would
be probably recommended to be kept, although it is not actually in the final translation
(or at least not inflected in this way). It is worth noting that this would not be such a bad
recommendation, since the difference between the word to be kept and the word needed for
the translation is the same, but inflected in a different way. Whatever the case might be, it
would be necessary to indicate this situations in some way, in order to let the user know
that a change must be made.
7.6.3 Errors Caused by Fertility
This refers to the fact that the translation of a single word in one language is translated by
two or more word in the other language. These words form a multi-word expression that
can be translated properly only when using a sub-segment covering the whole expression.
Sub-segments covering only a part of the expression can lead to out-of-context translations
that produce wrong evidence. For instance, in the TU (S, T )= (wavelength of less than
1400nm, longitud de onda inferior a 1400nm), proposed for the sentence S 0 = wavelength equal to or greater than 1400nm whose reference translation is T 0 = longitud de
onda igual o superior a 1400nm the word wavelength in English is translated as longitud
de onda in Spanish. Since wavelength appears in both S and S 0 , it is obvious that the three
words in this multi-word expression should be kept. However, out of this context, word de
can be translated as of, which also appears in S and is not matched in S 0 . The word de
may therefore be obtaining keeping evidence from the sub-segments covering the whole
expression longitud de onda, but changing evidence from the sub-segments covering only
208

fiTarget-Language Edit Hints in CAT Tools Based on TM by Means of MT

a part of the expression. This situation may easily result in a change recommendation.
This is probably the most difficult error to fix, since it is motivated by the specifics of each
language and may, in some cases, be extremely complex.

8. Concluding Remarks
In this paper we have presented a new approach to assist CAT users who employ TMs
by providing them with recommendations as to which target-side words in the translation
proposals provided by the CAT system have to be changed (modified or removed) or kept
unedited. The method we propose imposes no constraints on the type of MT system to be
used, since it is used as a black box. This method may use more than one MT system at
the same time to obtain a set of features that are then combined using a binary classifier
to determine whether target words have to be changed or should remain unedited. In any
event, MT results are never presented to the translator. A version of this method which
does not require any training is also proposed as an alternative.
The experiments carried out bear witness to the feasibility of the methods proposed by
comparing the accuracy and coverage to those of a previous approach based on statistical
word alignment for word-keeping recommendation. These experiments tackle the problem
in different scenarios, comparing the results obtained for different domains, MT systems
and language pairs. The results obtained confirm the viability of the MT-based methods
proposed in this work, particularly in the case of the trained MT-based approach (see
Section 4), which obtains better results as regards accuracy than those obtained with the
statistical alignment-based approach (see Section 3). In the case of coverage, the results
obtained with the MT-based approaches are in general worse than those obtained using the
alignment-based approach when the alignment models are trained on in-domain TMs, but
better when they are trained on out-of-domain TMs. The results also show a reasonable
degree of independence of the MT-based models with respect to the domain of the TM and
the MT system(s) used for training. These results suggest that there is no need to re-train
the classifier for each new TM and, even more importantly, that it is not necessary to do
so every time a new TU is added to the TM.
In general, the models trained for the MT-based recommender are much more portable
across domains than those trained for the alignment-based approach. These approaches
were compared to a training-free approach (see Section 5), which also uses MT as a source
of evidence for word-keeping recommendation, but which does not need any training. The
experiments confirm that the results obtained with the training-free MT-based recommender
are worse than those obtained with the trained recommender when it is trained on the same
TM to be used for translating new texts. However, it is advisable to use the training-free
MT-based approach when no recommendation models for the same TM are available for
the trained MT-based recommender.
In summary, the MT-based approaches perform better than the alignment-based approaches when accuracy is more important than coverage, or when they are trained on
out-of-domain TMs. With regard to the MT-based approaches, it is better to use a trained
MT-based recommender when a model is available for the pair of languages and MT system(s) to be used for translating, and to use the training-free MT-based recommender
otherwise.
209

fiEspla-Gomis, Sanchez-Martnez & Forcada

The principal conclusion of this work is that the three approaches are comparable and
useful depending on the needs of the translator and the resources available for translation.
It might even be possible to combine the three approaches (trained MT-based, training-free
MT-based, and statistical alignment-based) in order to prove, for example, recommendations for those words not covered by some of the approaches and not by the others.
The results obtained in this study have also opened up other new horizons for future
work, such as: extending the method so as to be able to not only provide the user with
recommendations as to which words to keep unedited, but also actively suggest a translation
for the words to change; trying alternative parametric classifiers; and using other sources
of bilingual knowledge, such as glossaries, dictionaries, bilingual concordancers, etc. to
improve the results of the MT-based approaches for word-keeping recommendation.
Appendix A presents a study that confirms the usefulness of word-keeping recommendation for translators, showing an improvement in productivity of up to 14% in a translation
task from Spanish into English. We plan to extend these experiments to explore new
ways of performing word-keeping recommendation. For instance, it would be interesting
to compare the productivity of translators when receiving recommendations only for content words, optionally with partial recommendations (on stems), or receiving only change
recommendations. We also believe that it would be interesting to evaluate the amount of
minimum recommendations needed in a segment to make this tool useful for the translators, by computing the productivity of translators as regards proposals with low coverage.
One of our main interests is to be able to model the cost of errors in recommendations, i.e.
to confirm whether a wrong keeping recommendation is more expensive for a translator
than a wrong changing recommendation. All these ideas require a new set of experiments
with professional translators in order to obtain the optimal method with which to present
recommendations, in order to maximise the improvement in productivity already shown in
Appendix A.
A study on the impact of noise in the data set used for evaluation in this paper is included
in Appendix B. This study uses an heuristic31 to filter out free or wrong translations in the
data sets. The translated materials obtained from the experiments described in Appendix A
are additionally used as a clean data set produced directly from professional translators.
The results in this appendix show that the accuracy in classification can be significantly
improved when using clean data sets.
Finally, a prototype of a plug-in for the free/open-source CAT system OmegaT32 which
implements the training-free approach described in Section 5 as a proof of concept, is available and can be downloaded from http://www.dlsi.ua.es/~mespla/edithints.html.
This prototype uses free on-line MT systems to perform word-keeping recommendation,
thus confirming the technical feasibility of this approach as regards making on-the-fly recommendations in real-world settings.

31. This heuristic is based on the distance between the segment to translate S 0 and the source side of the
translation proposal S, and the distance between the reference translation T 0 used as a gold standard
for evaluation and the target side of the translation proposal T .
32. http://www.omegat.org [last visit: 15th May 2015]

210

fiTarget-Language Edit Hints in CAT Tools Based on TM by Means of MT

Acknowledgments
This work is supported by the Spanish government through projects TIN2009-14009-C02-01
and TIN2012-32615. We would like to thank Juan Antonio Perez-Ortiz, Andy Way and
Harold Somers for their suggestions. We also thank the anonymous reviewers who helped
to improve the original manuscript with their suggestions.

Appendix A. Experiment Concerning the Effect of Word-Keeping
Recommendation on Translator Productivity
Word-keeping recommendation is a relatively new task. It is based on the assumption that
providing translators using translation memory (TM) tools with hints about the words to
change and to keep unedited in a translation proposal will increase their productivity. Although this might appear to be an obvious assumption, it needs to be empirically confirmed.
The objective of the experiment described in this appendix is to verify the impact of wordkeeping recommendation on translation productivity, independently of the approach used
to obtain the recommendations.
A.1 Methodology
In this experiment, the productivity of professional translators was measured when translating several documents from English into Spanish by using the computer-aided translation
(CAT) tool OmegaT, first without word-keeping recommendations and then with them.
For this task, five translators with previous experience of using OmegaT were hired. Each
of them had to translate three projects: a short training project (training), used only for
familiarisation with the tool and the kind of documents to translate; a project to be translated with a standard version of OmegaT (standard ); and a project to be translated with a
modified version of OmegaT that provided word-keeping recommendations (recommendation).
The training project was the same for all five translators, while five different standard
projects were created (one for each translator). The standard projects were reused as recommendation projects by rotating the translators, thus signifying that none of them translated
the same project twice. The decision was made to use the translations obtained for the standard projects as the reference when computing the word-keeping recommendations for the
recommendation projects; this would be equivalent to having a perfect classifier. This is
often called an oracle setting.
Following the structure described, the experiment was driven in such a way that all
the translations would be done at the same time, in the same room, and using identical
computers. The experiment was divided into two phases: first, the training and the standard
projects were translated, after which a break of about half an hour took place and the
recommendation project was translated.
A.1.1 Corpus
The DGT-TM (Steinberger et al., 2012) translation memory published by the European
Commission Directorate-General for Translation was used to build the translation memories
and the translation projects used in this experiment. 90% of the document pairs in it were
211

fiEspla-Gomis, Sanchez-Martnez & Forcada

used as a TM after segment alignment. The remaining 10% was used to build the translation
projects: documents were selected so that all their segments matched at least one translation
unit (TU) in the TM with a fuzzy-match score (FMS) that was higher than or equal to 50%.
Six translation projects were created from this selection: one containing a single document
with 127 words (the training project) and five containing three different documents of about
1,000 words in total.
A.1.2 OmegaT
For the experiment, version 3.1 of the free/open-source CAT tool OmegaT was used with
the plug-in OmegaT SessionLog 33 version 0.2, which silently logs all the actions performed
by the translator. The initial version of OmegaT was modified to avoid exact matches
(FMS=100%) being proposed, since it would not be possible to evaluate the impact of wordkeeping recommendation on this kind of proposals.34 A modified version of OmegaT was
created that can also make word-keeping recommendations based on former translations.
This version of the tool computes, for a given translation proposal, the edit distance between
the reference translation and the proposal, and colours the words to be kept in green and
the words to be removed from or replaced in the proposal in red. This means that the
recommendations made by this version of OmegaT are the same as those that a professional
translator would make when translating, i.e. perfect recommendations. (oracle setting).
A.2 Results
The results of the experiment are shown in Table 17. It is worth noting that it contains
only the results for four of the data sets, given that one of the translators forgot to translate
part of the standard project assigned to her, thus invalidating the corresponding results.
This table shows the time devoted to translating each test set, both with and without using
the word-keeping recommendation. Translation time was measured for each segment. The
tool used to capture the edition information revealed that each segment is usually selected
(or visited ) several times during the whole process, both to translate it and to review it.
In order to show this information more clearly, two different measures were obtained for
each segment: total translation time (columns 2 and 3), which is the the time spent on
a segment taking into account every visit to it, and edit time (columns 4 and 5), which
is the time spent on translating it for the first time using a translation proposal. For
this second measure, only the longest edit visit to each segment was taken into account,
assuming that edits made during later visits corresponded to the review process. The last
row of the table presents the total translation time for each column. As can be seen, the
total time devoted to translation is reduced by more than 14% when using word-keeping
recommendation. Moreover, editing time, on which word-keeping recommendation has the
main impact, is reduced by more than 20%. This gain in translation time proved to be
statistically significant with p  0.05 when performing an approximate randomisation test

33. https://github.com/mespla/OmegaT-SessionLog [last visit: 15th May 2015]
34. It is assumed that an exact match provides a translation that does not need to be edited, and therefore,
it is not possible to evaluate the advantage of word-keeping recommendations.

212

fiTarget-Language Edit Hints in CAT Tools Based on TM by Means of MT

test set
1
2
3
4
all test sets

total time

edition time

without WKR

with WKR

without WKR

with WKR

3,664s
3,613s
4,251s
3,787s

2,611s
3,467s
3,709s
3,315s

2,441s
3,080s
2,674s
2,432s

1,917s
2,293s
2,310s
1,937s

15,315s

13,102s

10,627s

8,457s

Table 17: Time spent on translation. Columns 2 and 3 compare the total time spent translating each test set, respectively, using the version of OmegaT without and with
word-keeping recommendation. Columns 4 and 5 present the same comparison,
but only taking into account the time actually spent reviewing the test set.

with 1,000 iterations.35 The free/open-source tool SIGF V.2 by Pado (2006) was used for
these experiments.
The results obtained in this experiment confirm the assumption that word-keeping recommendation can significantly improve the productivity of translators who use translation
memory tools. Although a more extensive experiment, including more translators and documents from other domains, would be needed to confirm this, current results are encouraging.
In addition, all the translators participating in the experiment agreed that word-keeping
recommendation is useful for translators when working with TM-based CAT tools.
It is worth noting that the experimental framework presented in this appendix has been
specifically designed to measure word-keeping recommendation and the results obtained
here cannot therefore be straightforwardly assumed for every translation project. For example, the projects translated in this experiment used a TM, thus ensuring that at least one
translation proposal would be provided with an FMS that was higher than 50%, but a TM
with this type of coverage may not be available for a given project. In addition, translations
performed by humans were used in this experiment to compute the word-keeping recommendations, in what would usually be called a gold standard. These translations would
obviously not be available in a real scenario and recommendations would be approximate.
The use of gold-standard-based recommendations may also boost the confidence of the
translators when using the tool, since in this experiment it was correct most of the times.
We can therefore consider that these results correspond to an upper bound in productivity
gain. Nevertheless, the results obtained in this experiment have allowed us to obtain a
clearer idea of the usefulness of word-keeping recommendation and confirm the relevance of
the problem of obtaining fast and accurate word-keeping recommendations.
35. Here, approximate randomisation is applied to the time devoted to translating each segment with and
without word-keeping recommendation in the concatenated data sets. This method first computes the
difference in time needed to translate the entire data sets. It then randomly interchanges the time spent
translating some of these segments between both sets of results and recomputes this total time. If an
equal or higher time gain can be obtained with these randomised lists of times, this means that the result
is not significant.

213

fiEspla-Gomis, Sanchez-Martnez & Forcada

Appendix B. Experiments with High Quality Gold Standards
In this appendix we tackle the problem associated with the use of free translations as references for evaluation. As already mentioned in Section 7, for a pair of segments (Sl0 , Tl0 ) in
our test set, we obtain all the matching TUs (Si , Ti ), and a set of word-keeping recommendations that are provided for every segment Ti . Tl0 is then used as a gold standard for these
recommendations for the purpose of evaluation. This method assumes that the way in which
Sl0 is translated into Tl0 is similar to the way in which Si is translated into Ti , thus enabling
the use of Tl0 as a reference.36 However, this may not be the case for several reasons, such as
wrong segment alignments, errors in translations, or, in our case, free (but still adequate)
translations. Following the example shown in Section 4, we illustrate the impact of a free
translation on our evaluation method. Let us assume that the segment S 0 to be translated is
la situacion poltica parece ser difcil, that a matching TU (S, T ) retrieved by the CAT
tool is (la situacion humanitaria parece ser difcil, the humanitarian situation appears
to be difficult), and that the gold standard T 0 in the test set is the situation, from a
political point of view, appears tortuous, which is a semantically valid translation of S 0 ,
but very different to T . When checking the validity of the translation as occurred in Section
4, the only words common to T 0 and T are the, appears, and situation, and the remaining
words would be considered as words to change in order to produce a correct translation.
However, it is sufficient to replace the word humanitarian with political in T to produce a
valid translation of S 0 . It is therefore obvious that T 0 is not a good reference with which to
evaluate the recommendations performed on T .
As a consequence of the free translations in our test set, a fraction of the recommendations which are actually correct are considered inadequate during the evaluation since they
do not match the reference in the test set. The accuracy obtained for all the approaches
presented in this work is therefore lower than expected.
Although the loss of accuracy affects all the methods in this work in the same way and
the conclusions that are obtained are therefore valid, we wished to see if more reliable results
as regards the performance of these approaches could be attained. We therefore performed
a set of additional experiments in order to bypass the problem of the free translations.
On the one hand, we repeated some of the experiments shown in Section 7 but by using
a constrained test set in which all those pairs of segments which were likely to be wrong
(or free) translations were discarded. On the other hand, we performed an experiment by
re-using the test set and the TMs described in Appendix A.
As mentioned previously, in the first group of experiments we defined a constraint in
order to attempt to evaluate only those pairs of segments from the test set in Section 6.2
which are more reliable. This was done by employing a filtering based on the fuzzy-match
score (FMS) used to choose the candidate TUs for a given segment to be translated. This
condition relies on the assumption that the FMS between S and S 0 (FMSS ) should be
similar to the FMS between T and T 0 (FMST ), since the number of words that differed in
both pairs of segments should be proportional for both languages. Based on this idea, we
set a threshold  so that only pairs of TUs fulfilling the condition |FMSS  FMST |   were
36. By similar we mean that the matching parts between Sl0 and Si are translated in the same way, thus
producing differences between Tl0 and Ti only in those parts corresponding to the differences between Sl0
and Si .

214

fiTarget-Language Edit Hints in CAT Tools Based on TM by Means of MT

(%)

domain

no filtering

  0.05

TUavg

Nwords

TUavg

Nwords

 60

02.40.10.40
05.20.20.10
06.30.10.00

3.71
0.62
5.68

95,881
9,718
34,339

1.59
0.39
0.52

44,240
6,198
6,956

 70

02.40.10.40
05.20.20.10
06.30.10.00

2.36
0.37
0.99

65,865
6,883
10,327

1.16
0.26
0.26

31,022
4,862
4,194

 80

02.40.10.40
05.20.20.10
06.30.10.00

1.58
0.14
0.45

46,519
3,015
4,726

0.97
0.09
0.09

24,928
1,889
2,143

 90

02.40.10.40
05.20.20.10
06.30.10.00

0.70
0.05
0.03

26,625
1,599
1,268

0.50
0.03
0.03

15,154
975
943

Table 18: Average number of matching TUs (TUavg ) per segment and total number of
words (Nwords ) for which to provide a recommendation when translating esen
for the three different domains. The results were obtained for different ranges of
the FMS threshold (), both when filtering with  = 0.05 and when no filtering
was applied.

used for both training and testing. It is worth mentioning that some experiments were also
performed by applying this filtering only to the test set, but the difference in the results
was not significant. For our experiments, we arbitrarily set the value of  to 0.05, i.e. a
divergence of 5% was permitted between the FMS of the source language segments and that
of the target language segments, since it is a threshold that constrains the examples used
in a highly controlled scenario, but a reasonable number of samples is maintained for our
experiments, as shown in Table 18.37
B.1 Experiments with Constrained Test Sets
This table shows, for the esen language pair and for fuzzy-match scores in four different
ranges, the average number of TUs matched per segment to be translated and the total
number of words for which a recommendation should be provided. The results were obtained
both when filtering with threshold  and when no filtering was applied. It is worth noting
that in the case of domains 05.20.20.10 and 06.30.10.00 there were noticeable differences
in matching when no restriction was applied, while more similar data was obtained when
filtering with  = 0.05. This has led us to believe that the TUs belonging to domain
05.20.20.10 are more regular in translation than those in domain 06.30.10.00. As will be
37. Note that the objective of these experiments is not to compare the different approaches (this has been already done), but rather to confirm whether an improvement in accuracy exists when using less noisy data
sets. The statistical significance between the different approaches has not therefore been re-computed.

215

fiEspla-Gomis, Sanchez-Martnez & Forcada

(%)

Method

A (%)

NC (%)

Pk (%)

Rk (%)

Pc (%)

Rc (%)

 60

FMS-only
alignment
trained
training-free

84.7.3
96.4.2
96.9.2
95.2.2

100%0
4.8.2
4.7.2
4.7.2

84.7.3
98.2.1
97.9.1
96.5.2

100%0
97.5.2
98.4.1
97.9.1


85.8.3
90.8.3
87.3.3

0%0
89.4.3
88.2.3
79.9.4

 70

FMS-only
alignment
trained
training-free

90.5.3
97.0.2
97.4.2
96.1.2

100%0
4.7.2
4.4.2
4.4.2

90.5.3
98.5.1
98.3.2
97.1.2

100%0
98.2.2
98.8.1
98.6.1


81.7.4
87.3.4
83.4.4

0%0
83.9.4
82.7.4
69.8.5

 80

FMS-only
alignment
trained
training-free

93.2.3
97.4.2
98.0.2
96.7.2

100%0
4.6.3
4.6.3
4.6.3

93.2.3
98.7.1
98.7.2
97.5.2

100%0
98.5.2
99.2.1
98.0.2


77.1.5
86.5.4
79.8.5

0%0
79.4.5
79.5.5
61.7.6

 90

FMS-only
alignment
trained
training-free

96.5.3
97.9.2
98.6.2
98.3.2

100%0
4.1.3
4.2.3
4.2.3

96.5.3
98.9.2
99.0.2
98.9.2

100%0
98.9.2
99.6.1
99.4.1


68.0.8
81.7.6
74.0.7

0%0
68.0.8
62.8.8
60.8.8

Table 19: Comparison of the results obtained using the trained MT-based approach, the
training-free MT-based approach, the alignment-based approach and the nave
FMS-only baseline. The accuracy (A) and fraction of words not covered (NC)
are reported, together with the precision (P ) and recall (R) for both keeping
recommendations and changing recommendations. The results were obtained
for several values of the FMS threshold  when translating esen in domain
02.40.10.40, using all the MT systems available and filtering with  = 0.05 (see
text).

observed, with this threshold, approximately half of the training samples are kept for domain
02.40.10.40 and about two thirds for domain 05.20.20.10. The case of domain 06.30.10.00 is
different; the filtering removes far more training samples for low values of the FMS threshold
, while for higher values the loss is not so high, and similar to that of domain 05.20.20.10.
Table 19 is the equivalent of Table 12, which contains a detailed comparison of all the
approaches, but using the filtering described above on the data set. As will be noted, the
results obtained in this case are clearly better for all the approaches than those obtained in
the experiments with no filtering.
Finally, Table 20 shows the accuracy obtained by both the trained MT-based approach
and the alignment-based approach for all language pairs, as occurs in Table 10. It is worth
noting that, although the differences between the results obtained with both approaches
are similar, all of them are noticeably better.
B.2 Experiment With Human-Produced Test Sets
In this second group of experiments we used the documents described in Appendix A as a
test set to evaluate word-keeping recommendation. In this case, the original documents in
216

fiTarget-Language Edit Hints in CAT Tools Based on TM by Means of MT

  60(%)

  70(%)

  80(%)

  90(%)

lang.
pair

trained

alignment

trained

alignment

trained

alignment

trained

alignment

esen
enes
deen
ende
fren
enfr
fien
enfi
esfr
fres

96.9.2
95.1.2
96.9.2
96.3.2
96.9.2
95.9.2
96.0.2
96.3.2
95.6.2
95.2.2

96.4.2
93.6.2
96.3.2
94.8.2
96.7.2
95.5.2
96.0.2
94.8.3
95.3.2
94.8.2

97.5.2
96.4.2
97.7.2
97.2.2
97.9.2
97.4.2
97.3.2
97.2.2
96.8.2
96.7.2

97.0.2
94.8.2
96.8.2
95.7.2
97.7.2
96.8.2
97.1.2
95.5.3
96.5.2
96.3.2

98.0.2
97.1.2
98.3.2
97.6.2
98.4.2
98.1.1
97.7.2
97.7.2
97.7.2
97.3.2

97.4.2
95.5.2
97.4.2
96.2.2
98.0.2
97.3.2
97.5.2
96.0.3
97.2.2
97.0.2

98.5.2
97.8.2
98.3.2
97.9.2
98.5.2
98.3.2
98.0.3
97.7.3
98.1.2
97.4.2

97.9.2
96.9.3
97.5.3
97.0.3
98.1.2
97.5.2
97.5.3
97.5.3
97.4.2
97.0.3

Table 20: Accuracy (A) obtained with both the trained MT-based approach and the
alignment-based approach when translating between all the language pairs in
domain 02.40.10.40. The results were obtained for several values of the FMS
threshold  and using all available MT systems for each language pair.

(%)

A (%)

NC (%)

(%)

A (%)

NC (%)

 60
 70
 80
 90

97.8.1
98.6.1
99.0.1
98.7.2

10.0.2
8.5.2
8.1.3
7.3.4

 60
 70
 80
 90

95.6.1
96.2.1
96.7.2
96.4.2

9.8.2
8.5.2
8.1.2
8.1.3

Table 21: Accuracy (A) and fraction of words not covered (NC) obtained when translating
with the trained MT-based approach by reusing the data set described in Appendix A. The left-hand table contains the results when translating Spanish into
English, while the right-hand table contains the results when translating English
into Spanish.

Spanish were translated into English by professional translators, who were told to translate
them as faithfully as possible. These parallel documents were therefore expected to totally
fit the requirements of the evaluation.
In this experiment, the TM used by the professional translators in Appendix A was used
to evaluate the translation of the texts from English into Spanish and vice versa. In-domain
models were also trained on this TM which, as already mentioned above, consists of only
629 TUs. Table 21 presents the accuracy and the fraction of words not covered that were
obtained for this data set, for enes and for esen. Although the coverage is slightly lower
than that obtained by the system with other data sets, the accuracy is much better, and is
even better than that obtained with the constrained test sets.
217

fiEspla-Gomis, Sanchez-Martnez & Forcada

The results presented in this appendix allow us to confirm that the accuracy of the
approaches presented in this work may be noticeably higher than those presented in Section
7, but the lack of a valid gold standard for our experiments only allows us to approximate
these results.

References
Ahrenberg, L., Andersson, M., & Merkel, M. (2000). Parallel text processing: alignment
and use of translation corpora, chap. A knowledge-lite approach to word alignment.
Kluwer Academic Publishers. Edited by J. Veronis.
Bergstra, J. S., Bardenet, R., Bengio, Y., & Kegl, B. (2011). Algorithms for hyper-parameter
optimization. In Advances in Neural Information Processing Systems 24, pp. 2546
2554. Curran Associates, Inc.
Bertoldi, N., Farajian, A., & Federico, M. (2009). Online word alignment for online adaptive
machine translation. In Proceedings of the Workshop on Humans and Computerassisted Translation, pp. 120127, Gothenburg, Sweden.
Bicici, E., & Dymetman, M. (2008). Dynamic translation memory: Using statistical machine translation to improve translation memory fuzzy matches. In Proceedings of
the 9th International Conference on Intelligent Text Processing and Computational
Linguistics, Vol. 4919 of LNCS, pp. 454465, Haifa, Israel.
Bojar, O., Buck, C., Federmann, C., Haddow, B., Koehn, P., Leveling, J., Monz, C., Pecina,
P., Post, M., Saint-Amand, H., Soricut, R., Specia, L., & Tamchyna, A. (2014). Findings of the 2014 Workshop on Statistical Machine Translation. In Proceedings of the
Ninth Workshop on Statistical Machine Translation, pp. 1258, Baltimore, USA.
Bourdaillet, J., Huet, S., Langlais, P., & Lapalme, G. (2010). TransSearch: from a bilingual
concordancer to a translation finder. Machine Translation, 24 (34), 241271.
Bowker, L. (2002). Computer-aided translation technology: a practical introduction, chap.
Translation-memory systems, pp. 92127. University of Ottawa Press.
Brown, P. F., Della Pietra, S. A., Della Pietra, V. J., & Mercer, R. L. (1993). The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19 (2), 263311.
de Gispert, A., Blackwood, G., Iglesias, G., & Byrne, W. (2013). N-gram posterior probability confidence measures for statistical machine translation: an empirical study.
Machine Translation, 27 (2), 85114.
Depraetere, I. (2008). LEC Power Translator 12. MultiLingual, September 2008, 1822.
Duda, R. O., Hart, P. E., & Stork, D. G. (2000). Pattern Classification (second edition).
John Wiley and Sons Inc.
Espla, M., Sanchez-Martnez, F., & Forcada, M. L. (2011). Using word alignments to
assist computer-aided translation users by marking which target-side words to change
or keep unedited. In Proceedings of the 15th Annual Conference of the European
Associtation for Machine Translation, pp. 8189, Leuven, Belgium.
218

fiTarget-Language Edit Hints in CAT Tools Based on TM by Means of MT

Espla-Gomis, M., Sanchez-Martnez, F., & Forcada, M. L. (2011). Using machine translation in computer-aided translation to suggest the target-side words to change. In
Proceedings of the 13th Machine Translation Summit, pp. 172179, Xiamen, China.
Espla-Gomis, M., Sanchez-Martnez, F., & Forcada, M. L. (2015). Using on-line available
sources of bilingual information for word-level machine translation quality estimation. In Proceedings of the 18th Annual Conference of the European Associtation for
Machine Translation, pp. 1926, Antalya, Turkey.
Espla-Gomis, M., Sanchez-Martnez, F., & Forcada, M. L. (2012a). Using external sources of
bilingual information for on-the-fly word alignment. Tech. rep., Universitat dAlacant.
Espla-Gomis, M., Sanchez-Martnez, F., & Forcada, M. L. (2012b). A simple approach
to use bilingual information sources for word alignment. Procesamiento de Lenguaje
Natural, 49.
European Commission Directorate-General for Translation (2009). Translation Tools and
Workflow. Directorate-General for Translation of the European Commission.
European Commission Joint Research Center (2007). EUR-Lex pre-processing. http:
//optima.jrc.it/Resources/Documents/DGT-TM_EUR-LEX-preprocessing.pdf.
Last retrieved: 15th May 2015.
Forcada, M. L., Ginest-Rosell, M., Nordfalk, J., ORegan, J., Ortiz-Rojas, S., Perez-Ortiz,
J. A., Sanchez-Martnez, F., Ramrez-Sanchez, G., & Tyers, F. M. (2011). Apertium:
a free/open-source platform for rule-based machine translation. Machine Translation,
25 (2), 127144. Special Issue on Free/Open-Source Machine Translation.
Foster, G., & Kuhn, R. (2007). Mixture-model adaptation for SMT. In Proceedings of
the Second Workshop on Statistical Machine Translation, StatMT 07, pp. 128135,
Prague, Czech Republic.
Free Software Fundation (2007). GNU general public license, version 3. http://www.gnu.
org/licenses/gpl.html. Last retrieved: 15th May 2015.
Gao, Q., Lewis, W., Quirk, C., & Hwang, M. (2011). Incremental training and intentional
over-fitting of word alignment. In Proceedings of the 13th Machine Translation Summit, pp. 106113, Xiamen, China.
Gao, Q., & Vogel, S. (2008). Parallel implementations of word alignment tool. In Proceedings
of the Software Engineering, Testing, and Quality Assurance for Natural Language
Processing Workshop, pp. 4957, Columbus, USA.
Garcia, I. (2005). Long term memories: Trados and TM turn 20. Journal of Specialised
Translation, 4, 1831.
Garcia, I. (2012). Machines, translations and memories: language transfer in the web
browser. Perspectives, 20 (4), 451461.
Gough, N., Way, A., & Hearne, M. (2002). Example-based machine translation via the web.
In Richardson, S. D. (Ed.), Machine Translation: From Research to Real Users, Vol.
2499 of Lecture Notes in Computer Science, pp. 7483. Springer Berlin Heidelberg.
Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, P., & Witten, I. H. (2009).
The WEKA Data Mining Software: an Update. SIGKDD Explorations, 11 (1), 1018.
219

fiEspla-Gomis, Sanchez-Martnez & Forcada

Hornik, K., Stinchcombe, M., & White, H. (1989). Multilayer feedforward networks are
universal approximators. Neural Networks, 2 (5), 359366.
Koehn, P. (2010). Statistical Machine Translation. Cambridge University Press.
Koehn, P., & Senellart, J. (2010). Convergence of translation memory and statistical machine translation. In Proceedings of the 2nd Joint EM+/CNGL, Workshop Bringing
MT to the User: Research on Integrating MT in the Translation Industry, pp. 2131,
Denver, USA.
Koehn, P., Axelrod, A., Mayne, A. B., Callison-Burch, C., Osborne, M., & Talbot, D.
(2005). Edinburgh system description for the 2005 IWSLT speech translation evaluation. In Proceedings of the International Workshop on Spoken Language Translation,
Pittsburgh, USA.
Kranias, L., & Samiotou, A. (2004). Automatic translation memory fuzzy match postediting: a step beyond traditional TM/MT integration. In Proceedings of the 4th
International Conference on Language Resources and Evaluation, pp. 331334, Lisbon, Portugal.
Kuhn, R., Goutte, C., Isabelle, P., & Simard, M. (2011). Method and system for using
alignment means in matching translation. USA patent application: US20110093254
A1.
Lagoudaki, E. (2008). The value of machine translation for the professional translator. In
Proceedings of the 8th Conference of the Association for Machine Translation in the
Americas, pp. 262269, Waikiki, USA.
Langlais, P., & Simard, M. (2002). Merging example-based and statistical machine translation: An experiment. In Richardson, S. (Ed.), Machine Translation: From Research
to Real Users, Vol. 2499 of Lecture Notes in Computer Science, pp. 104113. Springer
Berlin Heidelberg.
Laubli, S., Fishel, M., Volk, M., & Weibel, M. (2013). Combining statistical machine translation and translation memories with domain adaptation. In Proceedings of the 19th
Nordic Conference of Computational Linguistics, pp. 331341, Oslo, Norway.
Levenshtein, V. (1966). Binary codes capable of correcting deletions, insertions and reversals. Soviet Physics Doklady, 10 (8), 707710.
Ma, Y., He, Y., Way, A., & van Genabith, J. (2011). Consistent translation using discriminative learning: A translation memory-inspired approach. In Proceedings of the 49th
Annual Meeting of the Association for Computational Linguistics: Human Language
Technologies - Volume 1, HLT 11, pp. 12391248, Portland, Oregon.
Marcu, D. (2001). Towards a unified approach to memory- and statistical-based machine
translation. In Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics, ACL 01, pp. 386393, Toulouse, France.
Meyers, A., Kosaka, M., & Grishman, R. (1998). A multilingual procedure for dictionarybased sentence alignment. In Machine translation and the information soup: Proceedings of the third conference of the Association for Machine Translation in the
Americas, Vol. 1529 of LNCS, pp. 187198, Langhorne, USA.
220

fiTarget-Language Edit Hints in CAT Tools Based on TM by Means of MT

Nieen, S., Vogel, S., Ney, H., & Tillmann, C. (1998). A DP based search algorithm for
statistical machine translation. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics - Volume 2, ACL 98, pp. 960967, Montreal, Canada.
Och, F. J., & Ney, H. (2003). A systematic comparison of various statistical alignment
models. Computational Linguistics, 29 (1), 1951.
Pado, S. (2006). Users guide to sigf: Significance testing by approximate randomisation.
Sanchez-Martnez, F., Carrasco, R. C., Martnez-Prieto, M. A., & Adiego, J. (2012). Generalized biwords for bitext compression and translation spotting. Journal of Artificial
Intelligence Research, 43, 389418.
Sikes, R. (2007). Fuzzy matching in theory and practice. MultiLingual, 18 (6), 3943.
Simard, M. (2003). Translation spotting for translation memories. In Proceedings of the
HLT-NAACL 2003, Workshop on Building and Using Parallel Texts: Data Driven
Machine Translation and Beyond, pp. 6572, Edmonton, Canada.
Simard, M., & Isabelle, P. (2009). Phrase-based machine translation in a computer-assisted
translation environment. In Proceedings of the 12th Machine Translation Summit, pp.
120127, Ottawa, Canada.
Simard, M., & Langlais, P. (2001). Sub-sentential exploitation of translation memories.
In Proceedings of the Machine Translation Summit VIII, pp. 335339, Santiago de
Compostela, Spain.
Somers, H. (2003). Computers and translation: a translators guide, chap. Translation memory systems, pp. 3148. John Benjamins Publishing, Amsterdam, Netherlands.
Somers, H. (1999). Review article: Example-based machine translation. Machine Translation, 14 (2), 113157.
Specia, L., Raj, D., & Turchi, M. (2010). Machine translation evaluation versus quality
estimation. Machine Translation, 24 (1), 3950.
Steinberger, R., Pouliquen, B., Widiger, A., Ignat, C., Erjavec, T., & Tufis, D. (2006). The
JRC-Acquis: A multilingual aligned parallel corpus with 20+ languages. In Proceedings
of the 5th International Conference on Language Resources and Evaluation, pp. 2142
2147, Genoa, Italy.
Steinberger, R., Eisele, A., Klocek, S., Pilos, S., & Schluter, P. (2012). DGT-TM: A freely
available translation memory in 22 languages. In Proceedings of the 8th International
Conference on Language Resources and Evaluation, LREC12, pp. 454459, Istambul,
Turkey.
Ueffing, N., & Ney, H. (2005). Word-level confidence estimation for machine translation
using phrase-based translation models. In Proceedings of the Conference on Human
Language Technology and Empirical Methods in Natural Language Processing, HLT
05, pp. 763770, Vancouver, Canada.
Veronis, J., & Langlais, P. (2000). Evaluation of parallel text alignment systems. In Veronis,
J. (Ed.), Parallel Text Processing, Vol. 13 of Text, Speech and Language Technology,
pp. 369388. Springer Netherlands.
221

fiEspla-Gomis, Sanchez-Martnez & Forcada

Vogel, S., Ney, H., & Tillmann, C. (1996). HMM-based word alignment in statistical translation. In Proceedings of the 16th International Conference on Computational Linguistics, pp. 836841, Copenhagen, Denmark.
Zhechev, V., & van Genabith, J. (2010). Seeding statistical machine translation with translation memory output through tree-based structural alignment. In Proceedings of the
COLING10, Workshop on Syntax and Structure in Statistical Translation, pp. 4351,
Beijing, China.

222

fiJournal of Artificial Intelligence Research 53 (2015) 497-540

Submitted 01/15; published 07/15

Satisfiability and Systematicity
Matthew L. Ginsberg
Connected Signals, Inc.
355 Goodpasture Island Road, Suite 200
Eugene, Oregon 97401

Abstract
We introduce a new notion of systematicity for satisfiability algorithms with restarts,
saying that an algorithm is strongly systematic if it is systematic independent of restart
policy but weakly systematic if it is systematic for some restart policies but not others. We
show that existing satisfiability engines are generally only weakly systematic, and describe
flex, a strongly systematic algorithm that uses an amount of memory polynomial in the
size of the problem. On large number factoring problems, flex appears to outperform
weakly systematic approaches.

1. Introduction
Once upon a time, dpll (Davis, Logemann, & Loveland, 1962; Davis & Putnam, 1960) was
the algorithm of choice for solving Boolean satisfiability problems, or sat. There were three
reasons for this.
First, dpll was systematic in that if a particular problem had a solution, dpll would
eventually find it. If a problem had no solution, dpll would identify it as unsatisfiable. Such
properties are essential if we want to simply invoke a solver, allow it as long as necessary
to solve a problem, and be assured that some answer will always result.
Second, dpll used an amount of memory polynomial in the size of the problem. The
amount of time used was, of course, exponential given the NP-complete nature of sat. But
the memory used was polynomial, and therefore only logarithmic in the running time.
And finally, dpll worked. Its success was only a harbinger of the uses to which later sat
engines would be put, but the range of problems to which dpll could practically be applied
exceeded that of any previous general-purpose NP algorithm. Today that range is wider still,
including microprocessor verification (Kaivola, Ghughal, Narasimhan, Telfer, Whittemore,
Pandav, Slobodov, Taylor, Frolov, Reeber, & Naik, 2009), device driver validation (Moura
& Bjrner, 2010) and many others (Biere, Heule, van Maaren, & Walsh, 2009).
Two changes led to significant algorithmic improvements. The first was the recognition
that a dpll backtrack was best thought of not as a move in a tree-like search space,
but instead as a resolution-based inference step. This idea appeared first in Stallman and
Sussmans (1977) work on dependency-directed backtracking and Doyles subsequent (1979)
work on truth maintenance. All of these earlier authors, however, described algorithms
that might accumulate an exponential number of resolution consequences as the search
proceeded.
Ginsbergs (1993) work on dynamic backtracking was the first to show that systematic nonchronological inference methods could be constructed using only a polynomial
amount of memory, describing the algorithm as an extension of backjumping (Gaschnig,
c
2015
AI Access Foundation. All rights reserved.

fiGinsberg

1979). Bayardo and Schrag (1997) continued to work on the approach, renaming it (sensibly) relevance-bounded learning. These ideas are currently known as conflict driven clause
learning, or cdcl, a name that was introduced by Ryan (2002) and then popularized by
Marques-Silva, Lynce and Malik (2009).
The work through 1997 involved using a scheme for retaining learned clauses that guaranteed that only a polynomial number of such clauses would be kept at any particular
point in the search. With grasp (Marques-Silva & Sakallah, 1999), this requirement was
dropped. Virtually all modern sat solvers use cdcl, generally modified in a way that
either allows an unlimited number of clauses to be collected (as in tinisat, Huang, 2006;
or Quest0.5, Lynce, Baptista, & Marques-Silva, 2001) or, more commonly, in a way that
no longer guarantees systematicity (zChaff, Moskewicz, Madigan, Zhao, Zhang, & Malik,
2001; rsat, Pipatsrisawat & Darwiche, 2007; minisat, Sorensson & Een, 2005, and many
others).
The abandonment of systematicity was linked to the inclusion of restarts in satisfiability
algorithms. This appears to have been due to Gomes, Selman and colleagues (1998); the
basic idea is to escape from areas where the solver is thrashing (searching nearly endlessly
without ever identifying the real source of a problem) by simply restarting from scratch. Put
somewhat differently (but equivalently), restarts provide an out when the solver makes a
mistake near the root of the search tree, what Harvey (1995) refers to as an early mistake.
In the setting provided by dpll, restarting the prover will inevitably result in nonsystematicity. This is because crucial information about the state of the search is stored in the
position in the search tree, and restarting the search abandons this information.
The work on restarts has matured in two separate directions. First, there has been a
considerable body of work examining the theoretical properties of search with restarts in
the cdcl case where all of the clauses are retained as the search proceeds. Such a search
is obviously systematic, since only a limited number of new clauses can be learned before
either a solution is found or the empty clause is derived (proving the original problem to
be unsatisfiable).
There are more interesting conclusions to be drawn, however, The fact that restarts can
prevent thrashing appears to be related to the fact that they appear to allow foxr more
flexible inference schema than previous methods. This work was begun by Beame and
colleagues (2004), continued by Buss et al. (2008) and Hertel et al. (2008), and extended
further by Pipatsrisawat and Darwiche (2011), who showed that cdcl with restarts is
equivalent to general resolution. Cdcl without restarts is not known to be generally able
to produce proofs exponentially shorter than tree-based resolution, and general resolution
can lead to exponentially shorter proofs than tree-based resolution in many instances (BenSasson, Impagliazzo, & Wigderson, 2000).1
In addition to this theoretical work, practical work was proceeding as well, focusing
in large measure on the development of strategies identifying points at which the search
should be restarted. It was realized fairly quickly that virtually any restart policy was an
improvement on not restarting at all. Luby et al. (1993) had developed a restart policy that
they showed was within a logarithmic factor of optimal on a wide range of problems. The
1. It is also not known that tree-based resolution cant polynomially simulate general resolution (Bonet &
Johannsen, 2014).

498

fiSatisfiability and Systematicity

Luby restart policy was shown to outperform a variety of alternatives in the sat domain
by Huang (2007).
The Luby restart policy involves gradually (but not monotonically) increasing the number of backtracks between restarts; roughly speaking, the 2kth restart is the first that allows
2k backtracks before restarting once again. We will refer to the effort undertaken between
consecutive restarts as a probe; the number of backtracks between the restarts will be called
the size of the probe.
The Luby approach has two useful consequences. The first is that the total number of
backtracks grows quadratically with the number of restarts because of the infrequency of
probes of large size.
The second consequence of the Luby policy is that the search is once again systematic,
assuming that the original procedure, without restarts, was itself systematic. The reason
is that the Luby restart policy will eventually involve probes of sufficiently large size that
systematicity is guaranteed.
This is a distinction worth formalizing. Given an algorithm A that involves a restart
policy, we will say that A is strongly systematic if it is systematic independent of the restart
policy chosen. We will say that A is weakly systematic if it is systematic for some restart
policies but not for others, and we will say that A is nonsystematic if it is not systematic
for any restart policy.
The connection between systematicity and the amount of memory used is sufficiently
important that we present the following classification of some sat solvers:

Exp-space
Polyspace

Strongly
Systematic
tinisat
?

Weakly
Systematic

minisat

Nonsystematic

wsat

By exp-space, we mean algorithms that demonstrably may take an amount of space
exponential in the size of the problem being solved; polyspace algorithms are those that
remove learned clauses and thus could, at least in theory, use only polynomial amounts of
space as a result.
Memory is cheap these days; perhaps exp-space is not as much of a drawback as it once
was. But this turns out not to be the case, for the following reason.
Most of the CPU time consumed by sat engines is spent in the unit propagation procedure, which finds obvious consequences of the variable assignments made at any particular
point in the search. Although there have been a variety of engineering improvements that
speed this process considerably (Moskewicz et al., 2001), the overall time can still be linear
in the size of the accumulated clausal database. As this database grows exponentially in
size, the time needed for a single inference can therefore grow exponentially as well.
Given tinisats existence in the upper left, there has been little interest in an exp-space
solver with weaker systematicity properties. But tinisat cannot really be considered a
production solver; on difficult problems, the number of stored clauses grows impractically
and the solver itself essentially grinds to a halt.
We have not previously discussed the wsat family of algorithms (Selman, Kautz, &
Cohen, 1993), nor shall we. These algorithms are very different in spirit from the ones
that we have described thus far, working with complete variable assignments and using hill
499

fiGinsberg

climbing to attempt to gradually convert these assignments to solutions of the problems
being investigated. While competitive in the 1990s, the progress on systematic algorithms
has been such that these fundamentally nonsystematic approaches are currently given relatively little attention, although recent work has shown that the connection between these
algorithms and the cdcl approach is closer than one might think (Goultiaeva & Bacchus,
2012).
Most modern sat solvers join minisat in the middle of the bottom row, weakly systematic but using only a relatively modest amount of memory to get the job done. Removing
some clauses is essential in any practical solver and there simply has been no known strongly
systematic algorithm capable of doing so. The general view has been that although not
strongly systematic, these systems work exceptionally well in practice, including managing
to produce proofs of unsatisfiability when necessary.
It should be noted, however, that this loss of systematicity is more than a simple theoretical concession. It is obviously important, for example, that distinct probes begin their
search from distinct locations in the search space. Should those starting locations be close
in some sense, it is also important that the corresponding probes overlap as little as possible.
We believe that the experimental results in Section 4 support this intuition.
As we have remarked, our work in dynamic backtracking introduced the first nonchronological, polyspace, weakly systematic sat engine. Our goal in the current paper is to describe a new cdcl-based sat engine flex that is both polyspace and strongly systematic.
The outline of the paper is as follows. In Section 2, we introduce the necessary notation
and describe a standard cdcl algorithm. We also describe the choices typically made in
implementing such algorithms.
Section 3 describes the flex algorithm; the proof that it is both systematic and polyspace
is deferred to an appendix. We discuss the additional data structures needed to guarantee
systematicity, and the restrictions that must be placed on some standard choices as a result.
Experimental results are presented in Section 4 and concluding remarks are in Section 5.

2. Background, Notation and Existing Work
By a satisfiability problem, we will mean a collection of clauses over Boolean variables such
as:
ab
a  b
a  b
a  b  c
Each of the clauses is a disjunction, and the goal is to value all of the variables so that
each clause is satisfied. In this particular example, we have to make a and b true, and c
false.
Definition 2.1 A variable is a letter, such as a, b, c and so on. A literal is a variable or
the negation of a variable. A clause is a disjunction of literals; the (unsatisfiable) empty
clause will be denoted . A theory is a conjunction of clauses.
500

fiSatisfiability and Systematicity

An interpretation or bias is an assignment of true or false to each variable in a theory.
An interpretation will be said to be a model of a theory T if every clause in T is satisfied
by the interpretation.
We will often call theories sat instances or satisfiability problems, and will think of
clauses simply as sets of the literals they contain, writing, for example, that a  a  b. We
will similarly think of theories as sets of clauses, and will think of the size of a theory T as
simply the number of clauses in T , denoting it by |T |.2
Modern satisfiability algorithms work by manipulating partial assignments of values
to variables; the idea is to either extend the partial assignment to value more variables
(and eventually to a model of the theory in question) or to show that a particular partial
assignment cannot be extended to a model.
Definition 2.2 A partial assignment is a sequence P = hl1 , l2 , . . . , ln i of literals such that
if i 6= j, then li 6= lj and li 6= lj . A variable v will be called valued by P if v  P or
v  P . A literal l will be called satisfied by P if l  P , and will be called falsified by P if
l  P . A clause c will be called falsified by P if every literal l  c is falsified by P .
In practice, the variables assigned values in a partial assignment are typically annotated
with reasons of some sort. A variable value may be the result of a choice made by the search
engine, or the obvious result of previous choices:
Definition 2.3 An annotated partial assignment is a sequence h(l1 , c1 ), (l2 , c2 ), . . . , (ln , cn )i
of pairs, where each pair contains a literal li and a clause ci . The clause ci will be called
the reason for li , and the set {c1 , . . . , cn }  {} will be denoted R(P ) and called the reasons
of the annotated partial assignment.
Given an annotated partial assignment P , the ith pair (l, c)  P will be called justified if
either c =  or l  c and every literal in c other than l is lj for some j < i. An annotated
partial assignment P will be called justified if every (l, c)  P is justified.
Note that a justified partial assignment can always contain pairs of the form (l, ), since
the justification condition is trivially satisfied. We will use this to indicate that l is a choice
made by the search algorithm:
Definition 2.4 If P is an annotated partial assignment and (l, )  P , we will say that l
is a choice in P . The set of choices in P will be denoted by C(P ). The set of negations of
choices in P will be denoted by C(P ).
Similarly, a justified partial assignment can always contain pairs of the form (l, l), which
we will use to indicate that l itself has been proven to be a consequence of the theory being
investigated.
In general, all of the partial assignments that we consider will be justified.
2. From a complexity perspective, it might seem more natural to think of the size of a theory T as the total
number of literals in the clauses in T . Our choice is a bit more convenient notationally and for a theory
involving v variables, the total number of literals in T is obviously at most v|T |.

501

fiGinsberg

Definition 2.5 Let P be an annotated partial assignment. Then if (l, c) is the ith element
of P , we will say that the assertion level of l is
|{j  i|cj = }|
For a clause , we will say that the assertion level of  is the maximum of the assertion
levels of the literals in .
In other words, the assertion level of a literal is the number of choice variables that
precede it in P .
Definition 2.6 Let P be an annotated partial assignment for a theory T . A clause  =
l1      ln such that no literal in  is satisfied by P , and exactly one literal is unvalued by
P , will be called unit.
If a single literal l   is unvalued by P for some unsatisfied clause , then any extension
of P that is a model of T must include l, and P will remain a justified partial assignment
if we add (l, ) to its end. This is called unit propagation:
Definition 2.7 Unit propagation is a procedure unit that accepts as input a theory T and
a justified partial assignment P . It returns a pair (P 0 , c) where P 0 is a maximal justified
partial assignment that extends P , so that P  P 0 , and either c = true or c  T is falsified
by P 0 .
If unit propagation returns c = true, unit propagation succeeded. If it returns a partial
assignment P 0 and a clause c falsified by P 0 , a contradiction has been found and c is the
reason.
Definition 2.8 We will say that the backtrack level of  is zero if every literal in  has the
same assertion level, or the second greatest of the assertion levels of literals in  otherwise.
Given an annotated partial assignment P and clause , we define the result of backtracking to , to be denoted backtrack(P, ), to be the result of removing from P any
literal with assertion level greater than the backtrack level of .
If  is a clause with backtrack level n 6= 0, there is clearly some literal l   such that
the assertion level of l is greater than n. If this literal l is unique, then backtracking to the
backtrack level of  will leave every element of  other than l unchanged, so that  itself
(which was a contradiction prior to the backtrack) will be a unit clause after the backtrack
is complete.
Most sat implementations spend the bulk of their time in the unit procedure; this is
a consequence of the fact that they must search the theory T for unit clauses. Moskewicz
et al.s (2001) introduction of so-called watched literals in zChaff sped this process considerably, but it continues to consume the bulk of the computational resources in implemented
systems.
Eventually, unit propagation either terminates without finding a contradiction (presumably because no unit clauses remain in T ), or returns a clause c  T that is falsified by
the current justified partial assignment P . In the latter case, the satisfiability engine needs
502

fiSatisfiability and Systematicity

to respond to this new information and modify P in some way. Dpll, for example, would
modify P simply by returning to the most recent choice point and then resuming the search.
Cdcl implementations work differently. They take P and c and produce a new clause
that is entailed by T and that captures, as best possible, the reason for what went wrong.
As an example, suppose that P gives  = a  b  c as the reason for c (so that P contains
a, b and therefore c), and that  = a  c appears in T . This clause is falsified by
P . We can now resolve  and  to get the new clause a  b, which is also falsified by P
and suggests (correctly) that either a or b is the source of the problem even if many other
variable choices appear subsequently in P itself. This may enable a much deeper backtrack
than simply returning to the most recent choice point.
Definition 2.9 Let P be a justified partial assignment, and c a clause falsified by P . By a
(P, c)-conflict we will mean any clause  that is a logical consequence of c together with the
reasons in P and that is also falsified by P , so that c  R(P ) |=  and P |= .
As shown above, once unit propagation identifies a contradiction, we can use resolution
to derive a (P, c)-conflict where P is the current partial assignment and c is the conflicting
clause in T .
But we can also sometimes do more. Consider the following theory:
a  b

(1)

b  e  c

(2)

a  e  d

(3)

d  f  c

(4)

and imagine that after e and f are true, we choose to make a true as well. Now unit
propagation allows us to conclude b and then c from b (and e), and also d and then c from
d (and e and f ). Now a single resolution of (2) (the reason for c) and (4) (the reason for
c) will allow us to conclude
b  d  e  f
(5)
But we can we go further, resolving (5) with the reason (1) for b to get a  d  e  f
and then also resolving with the reason (3) for d to get simply a  e  f . This last
conclusion correctly identifies a itself as the source of the problem once e and f are both
true.
We see from this example that a single (P, c) pair may have many conflicts and we will
generally need to identify a single such conflict to continue the search.
In general, we will want to select a conflict that has a single literal at its assertion level.
That will allow the algorithm to redraw the conclusion in question after backtracking to the
backtrack level of the conflict. Continuing with our example, suppose that the levels of the
choice literals involved were 1 for e and f and 4 for the choice literal a. As we explained,
all of the following are possible learned clauses in this case:
clause
b  d  e  f
a  d  e  f
a  e  f

assertion
level
4
4
4
503

literals at
assertion level
b, d
a, d
a

backtrack
level
1
1
1

fiGinsberg

It is only if we learn the last of these clauses that we will be able to draw a unit conclusion
(a) after backtracking to level one.

Definition 2.10 Given a justified partial assignment P and a clause , let n be the assertion level of . Then  will be called a unique implication point, or uip, if it has a unique
literal whose assertion level is n.

Uips were introduced in grasp (Marques-Silva & Sakallah, 1999), and experience has
shown that part of the strategy for selecting a conflict clause should be to always select a
uip. This can be done by resolving away reasons for any multiple literals at the assertion
level of .
We can now describe a cdcl-based inference engine. As we do so, recall from Definition 2.1 that a bias is a complete assignment of values to variables. If B is a bias and v a
variable, we will denote by B(v) the value specified by the bias for v, so that B(v)  B and
B(v) = v or B(v) = v. We also define:

Definition 2.11 For a bias B and set of literals S, we define the result of restricting B to
S, to be denoted B|S , to be B|S = B  S  {l|l  S}.

Informally, the result of restricting a bias is to label all of the literals in S as true, removing
any negations that appeared in the bias before the restriction was performed.
The bias is intended to capture our current guess as to the most likely values in a model
of T . So when we extend our partial assignment to take a value at a new variable, we choose
the variable and then use the value suggested by the bias. A specific example of this idea
was introduced by Pipatsrisawat and Darwiche (2007) in rsat and was called phase saving
in that work.3

3. A similar idea was introduced slightly earlier by Selmann and Ansotegul (2006); earlier still, Beck (2005)
had introduced the idea of saving some preferred values to use in the next portion of the search.

504

fiSatisfiability and Systematicity

Procedure 2.12 (CDCL) Let T be a theory,  a set of clauses such that T |= , and B a
bias for T . Then to compute cdcl(T, , B, n), one of:  if T is shown to be unsatisfiable,
a model P of T if one is found, or unknown if no solution is found after n steps:
1 i  0;
2 while i < n do
3
(P, c)  unit(T  , P );
4
while c = true do
5
if P assigns a value to every variable in T then return P ;
6
v  a variable unvalued by P ;
// choice
7
(P, c)  unit(T  , hP, (B(v), )i);
8
  any (P, c)-conflict that is a uip;
// choice
9
    {};
10
B  B|P ;
11
if    then return ;
12
P  backtrack(P, );
13
    discard(, P );
// choice
14
ii+1
15 return unknown
This algorithm is hardly original with us, but let us give an explanation in any event.
As the algorithm proceeds, P is the justified partial assignment being considered and  is
the collection of learned clauses.
The algorithm works by extending the partial assignment P until either a solution is
found (line 5) or a new clause is learned (line 7 with c 6= true). In the latter case, we
identify a new conflict-driven uip  (line 8), add it to , backtrack until  is unit, and
repeat. We update the bias as the search proceeds; we present in line 10 the rsat choice
of updating the bias every time a variable is set by unit propagation. As we will see in
Section 3, other choices are also possible.4
After the new clause is discovered and added to  on line 9, we discard some elements
of  (line 13); it is here that we can potentially shrink  so that only a relatively small (and
potentially polynomial) number of clauses is retained. Then we either give up if we have
encountered n backtracks, or continue searching.
Recall that for an annotated partial assignment P , R(P ) is the set of reasons for literals
in P . If (P, c) is a value such as is returned by unit propagation, we will abuse notation
somewhat and use R(P, c) to denote simply R(P ). We now have:
Lemma 2.13 Suppose that R(unit(T  , P ))  discard() =  for all , so that we
never discard reasons or unit clauses that are about to become reasons. Then throughout the
execution of Procedure 2.12, we will have T |=  and P will be a justified partial assignment
of T  . In addition:
4. The attentive reader may wonder why the test on line 11 does not appear until after  is added to  on
line 9 and the bias is updated on line 10. The reason is that flex may perform additional inference at
this point.

505

fiGinsberg

1. If Procedure 2.12 returns , then T is unsatisfiable. If Procedure 2.12 returns P for
some partial assignment P , then the literals in P are a model of T .
2. For a theory with v variables and n  2v , Procedure 2.12 will not return unknown,
independent of the further nature of discard.
Proof. We first show the invariants mentioned above. That P is a justified partial assignment follows from the fact that it is returned by unit; any justified partial assignment will
obviously remain so during a backtrack such as appears on line 12. That T |=  follows
from the fact that T entails any (P, c)-conflict if P is a justified partial assignment and
c  T.
Now consider the various points at which Procedure 2.12 returns. On line 5, P is a
model of T . And on line 11, unit propagation and resolution have collectively derived  as
a (P, c)-conflict, so that T |=  and T is unsatisfiable.
That Procedure 2.12 does not return unknown is a bit more subtle. To see this, suppose
that we have a partial assignment P . Now we can associate to P a sequence of positive
integers hn0 , n1 , . . . , nk i, where nj is the number of literals in P with assertion level j. We
will call this sequence the size of P and denote it by size(P ).
Now let n = hn0 , n1 , . . . , nk i and m = hm0 , m1 , . . . , ml i be two sizes. We will say that
m is smaller than n, writing m < n, if one of the following two conditions holds:
1. There is some j with mj 6= nj and, for the first such j, mj > nj .
2. mi = ni for all i  k, and there is a j with k < j  l and mj > 1.
Informally, if the size of a partial assignment gets smaller, we have made progress
in that we have derived more consequences earlier in the search tree, and the unexplored
region of the search space has therefore gotten smaller. We formalize this via the following
two claims:
1. Denote by Pi the partial assignment after line 3 in Procedure 2.12 for a given value
of i. Then size(Pi+1 ) < size(Pi ).
2. If s1 > s2 >    > sz is a properly descending sequence of sizes, then z  2v , where v
is the number of variables in the theory in question.
Note that these two results suffice to complete the proof, since it will follow that the
loop in Procedure 2.12 can be repeated no more than 2v times.
For the first claim, note that the new clause  is unit after the backtrack on line 12.
Since it is unit, it will be a reason in unit(T  , P ) and therefore cannot be discarded
by discard. Now let j be the assertion level of . It is immediate that after the unit
propagation on line 12, there will be at least one new variable assigned a value at level j.
Now suppose that size(Pi ) = hn0 , n1 , . . . , nk i, and size(Pi+1 ) = hm0 , m1 , . . . , mj i. (The
number of assertion levels in Pi+1 is necessarily equal to j.) Pi and Pi+1 agree before the
point of the backtrack, so mk = nk for k < j. Now if j  k (so that the backtrack brought
us into Pi ), then mj > nj and size(Pi+1 ) < size(Pi ). If, on the other hand, j > k, then the
506

fiSatisfiability and Systematicity

sizes agree up to k and mj > 1 because the conclusion of  is a unit consequence at this
level. This proves the first claim.
For the second, note first that the ordering conditions
are unchanged if we append to the
P
end of any size hn0 , n1 , . . . , nk i enough 1s so that i ni = v. This essentially pretends that
we have added values for all subsequent variables, but there have been no unit propagations
from those variables. If we write n for the result of extending a sequence in this way, it is
easy to see that n < m if and only if n < m.
Now let n be a size. We claim that any descending sequence of length at least 2vn0
steps beginning with n and ending in a size m will have m0 > n0 . The proof is by induction
on v  n0 .
If v  n0 = 1, then we have n0 = v  1 and must have n = hv  1, 1i. But now the only
way to make it smaller at all is to have m = hvi, and m0 > n0 .
For the inductive step, suppose that n = hn0 , 1, . . .i. Now after at most 2vn0 1 steps,
we will have gotten to hn0 , 2, . . .i by the inductive hypothesis. After at most 2vn0 2 steps,
we will have gotten to hn0 , 3, . . .i, and so on. We will therefore arrive at hn0 , v  n0 i after
at most
vn
0 1
X

2vn0 i = 2vn0  2

i=1

steps. One more step, a total of 2vn0  1, causes n0 to increment. 2vn0 steps thus cause
n0 to increment as well. This completes the induction.
Now imagine that we add a new variable w to our theory, where w appears in no clauses
but is evaluated first when the search begins. For our new theory with v + 1 variables, given
that the size of any partial assignment must have its first component incremented after at
most 2(v+1)1 steps, we will either have learned something about w (an impossibility) or
derived the empty clause . Thus the second claim follows and the lemma is proved.
This proof is very similar in spirit to the proof of Theorem 3.10, our main theoretical
result.
In general, Lemma 2.13 appears to be common knowledge in the sat community,
although I am unaware of any other published proof of the second claim and it is not
completely clear if the sat community knows it to be true.5
Until this point, we have not yet included restarts in our algorithm, but doing so is
straightforward. We define:

Definition 2.14 A restart policy is a mapping r : IN  IN, where IN is the set of positive
integers.
The restart policy simply gives the number of backtracks permitted as a function of
probe.
5. A similar result, with a similar proof but based on the assumption that no clauses are ever deleted, does
appear in Section 4.2 of Zhangs (2003) Ph.D. thesis.

507

fiGinsberg

Procedure 2.15 (CDCL with restarts) Let T be a theory and r a restart policy. Then
to compute sat(T, n), either the empty clause  if T is unsatisfiable or a model of T :
1   ;
2 B  any bias for T ;
// choice
3 i  1;
4 while true do
5
x  cdcl(T, , B, r(i));
6
if x 6= unknown then return x;
7
i  i + 1;

2.1 Theoretical Results
We can now show the following:
Proposition 2.16 Let T be a theory involving v variables, and r a restart policy. Then if
Procedure 2.15 returns , T is unsatisfiable. If Procedure 2.15 returns a partial assignment
P , then the literals in P are a model of T . In addition:
1. If R(unit(T  , P ))  discard(, P ) =  and if there is some i such that r(i)  2v ,
then Procedure 2.15 will always terminate.
2. If discard(, P ) = R(unit(T , P )), then ||  v as Procedure 2.15 is executed.
3. If discard(, P ) = , then the size of  may grow to be exponential in v, independent
of the restart policy r.

Proof. The proof of Lemma 2.13 also shows that Procedure 2.15 is correct.
Claim 1 of Proposition 2.16 follows immediately from claim 2 of Lemma 2.13. Claim
2 follows from the fact that each variable can have at most one reason in R(P ). Claim 3
follows from the fact that the amount of memory used by Procedure 2.15 is linear in the
run time if no clauses are ever discarded, and there are known to be many problems for
which the shortest resolution proof is of exponential length (Bonet, Pitassi, & Raz, 1997;
Haken, 1985, 1995; Krajicek, 1997; Pudlak, 1997; Tseitin, 1970).
Corollary 2.17 There are weakly systematic, polyspace instances of Procedure 2.15. There
are strongly systematic, exp-space instances of Procedure 2.15.
Proof. Take discard(, P ) =   R(unit(T  , P )) to get a weakly systematic polyspace
instance by virtue of the first two claims of Proposition 2.16. If discard() = , the
resulting instance is strongly systematic because any instantiation will eventually run out
of new clauses to learn, and may use an exponential amount of memory because of the third
claim of Proposition 2.16.
Tinisat is an exp-space, strongly systematic instance. Minisat is a (potentially) polyspace, weakly systematic instance.
508

fiSatisfiability and Systematicity

2.2 Practical Concerns
In addition to the selection of restart policy, there are three further points where an explicit
choice must be made in Procedure 2.12, and one where a choice must be made in Procedure 2.15. In line 6 of Procedure 2.12, a variable is selected for addition to P . In line 8,
a particular conflict clause is selected for addition to . In line 13, the set  is reduced,
presumably in order to conserve memory (and thereby speed unit propagation). And in
line 2 of Procedure 2.15, an initial bias is selected.
Given that much work on sat currently focuses on one implementation of Procedure 2.15
(and therefore of Procedure 2.12) or another, there is a considerable volume of literature
on each of these choices. We will summarize that work here, providing additional details in
areas where our ideas will constrain the allowed options.
Variable and bias selection Variables are generally selected in a cdcl prover using
a heuristic called vsids (variable state independent decaying sum) that was introduced
in zChaff (Moskewicz et al., 2001). We continue to use vsids in our work and do not
describe it further here.
We also need to specify an initial bias, before search or choice has valued any of the
variables. There is no real reason to prefer one initial bias over another, especially given
that the values are likely to be overwritten as the search moves forward. We will follow
minisats example and set every variable to false in our initial bias.6
Conflict clause selection We have already remarked both that a particular (P, c) pair
may have many associated conflicts, and that it is generally wise to select one that is a uip.
That will allow the algorithm to redraw the conclusion in question after backtracking to
the backtrack level of the conflict, and is central to the proof of Lemma 2.13.
But we can sometimes do more. Recall that in our running example, we derived the uip
a  e  f . Suppose also that the actual choice literal at level one was g, with e and f
following from g  e and g  f respectively. Now we can continue the resolution process,
using
g  a
as the learned clause instead of a  e  f . Should we do so?
Definition 2.18 A uip  will be called a decision uip, or duip, if C(P ) |= , so that
every literal in  is the negation of a choice in P .
Proposition 2.19 Let P be a justified partial assignment and  a clause falsified by P .
Then (P, ) has a unique duip.
Proof. The only way to construct a duip is to resolve literals away until only decisions
remain. It is not hard to see that the set of decisions so obtained is independent of the
order of such resolutions.
Should we always work with a duip as our algorithm proceeds? Surprisingly, the answer
appears to be no; the extra resolutions seem to move the conflict away from the real
problem more often than not. But one thing we should do is to remove any literals that
6. On naturally occurring problems, the effectiveness of biasing every variable to be false is perhaps rooted
in the so-called closed world assumption (Clark, 1978; Reiter, 1978), but that will not concern us here.

509

fiGinsberg

are simply unnecessary. So if in our running example the reason for f is e  f , it is
obviously wise to resolve this into the conflict clause being learned, replacing a  e  f
with the properly stronger a  e.
Zhang et al. define a first uip as follows.7 The basic idea is that we want to define a
first uip as a uip that is both minimal and as close to the eventual conflict as possible.
Definition 2.20 For an annotated partial assignment P and conflict c, a (P, c)-conflict 
that is a uip will be called reduced if no proper subset of  is a (P, c)-conflict.
By close to the eventual conflict, Zhang et al. actually mean that the uip in question is
as late in the conflict graph as possible:
Definition 2.21 Let P be an annotated partial assignment and suppose that 1 and 2 are
clauses falsified by P . We will write 1 <P 2 if every literal in 1 either appears in 2 or
has a reason that includes at least one literal in 2 . A (P, c)-conflict that is reduced and
minimal under the transitive closure of <P will be called a first uip, or fuip.
As remarked above, 1 <P 2 if 1 is closer to the eventual contradiction (and therefore
later in the implication graph) than is 2 .
Proposition 2.22 (Zhang, Madigan, & Moskewicz, 2001) Let P be a justified partial
assignment and  a clause falsified by P . Then (P, ) has a unique fuip.
The basic idea of the proof is that we can construct an fuip by resolving just enough to
ensure that we are working with a uip and then removing any literals that can be subsumed
by other literals in . Additional resolutions are not needed and produce new clauses that
are not minimal under <P .
Cdcl-based provers can work with any uip, but performance is generally best when
fuips are used (Zhang et al., 2001).
The learned clauses interact with the bias; in general, we want to learn a clause that is
in conflict with the bias (and therefore encourages us to adjust the bias in a useful way).
Definition 2.23 Given a bias B, a uip  will be called a bias uip for B, or buip for B, if
B |= . A buip that is minimal under the transitive closure of <p as in Definition 2.21
will be called a first buip, or fbuip.
In the case where the bias B contains negations of all of the literals in P (as in Procedure 2.12 as written), any uip will be a buip, since the literals in P will always conflict
with the values in B. So in this case, the fuip will be an fbuip as well.
We also have:
Definition 2.24 Let P be an annotated partial assignment and B a bias. We will say that
P is supported by B if C(P )  B, so that every choice literal in P appears in B.
7. Zhangs description of an fuip is a bit less formal; Definitions 2.20 and 2.21 are simply a formalization
of their ideas.

510

fiSatisfiability and Systematicity

In other words, a bias supports an annotated partial assignment if all of the choices made
in P were as suggested by the bias.
Lemma 2.25 Let P be a justified partial assignment and c a clause falsified by P , and
suppose that B is a bias that supports P . Then any duip for (P, c) is also a buip.
It follows from this and Proposition 2.19 that if the bias supports the annotated partial
assignment, it will have at least one buip, and therefore at least one fbuip. It is not
difficult to see that this fbuip is unique (basically, we stop resolving as soon as the necessary
conditions are met). Fbuips share the crucial property that they modify the learned clause
 minimally subject to the restriction that  contradicts the bias and is a uip.
Discard strategy Clause reduction generally proceeds by estimating the likely future
value of each clause in  in the subsequent search. The least valuable clauses are periodically
removed as the search proceeds. We will be free to follow an identical strategy, although
some    will be marked as essential to preserving systematicity. Those clauses will
need to be retained independent of their estimated future value. If our procedure is to use
polynomial amounts of memory, the number of such clauses must be guaranteed to grow at
most polynomially with the size of the problem.

3. FLEX
Consider now the search for a polyspace, strongly systematic satisfiability algorithm. We
immediately have:
Observation 3.1 Strongly systematic, polyspace satisfiability algorithms exist.
The reason for this is obvious. Imagine any implementation of cdcl with restarts, and
suppose that we maintain, from one restart to another, a separate partial assignment PD as
constructed by dpll. This additional data structure requires polynomial memory (at most
v clauses, each of which is of size at most v), and if we simply incorporate the clauses in
PD into , the resulting algorithm is clearly systematic.
In practice, however, this is of essentially no value. The systematicity guaranteed by
dpll is completely separate from the inference going on in the individual probes, so it is
unlikely that the additional clauses in  will do much to constrain the search. In addition,
most restart policies restart relatively infrequently, so while a guarantee that no more than
2v restarts will be required in satisfying from a theoretical perspective, it may be of little
or no value in practice.
3.1 Pure FLEX
Far more interesting would be to find an instance of Procedure 2.15 itself that was strongly
systematic. Showing that to be possible is our main theoretical result in this paper.
The basic idea behind our method is that we will manage the set of learned clauses so as
to retain enough information to ensure that the solver is making progress in a lexicographic
sense, where we identify some variables as being lexicographically earlier than others. The
lexicographic ordering is quite weak, and is weakened further when possible as the search
511

fiGinsberg

proceeds. So, for example, if we make progress by eliminating one possible value for the
variable a, then we can drop any restrictions on the relative ordering of variables that follow
a (although we do need to remember that they do indeed follow a).
To get a somewhat better feel for this, imagine that we have ordered the variables in
our theory in some way, so that the m variables are ordered as v1 < v2 <    < vm .
We will now view it as progress if we eliminate a value for the variable vi without
introducing new possible values for any vj with j < i. If we can repeatedly do this, we will
eventually manage to eliminate a value for v1 , and that value will remain eliminated as the
search proceeds. Then we will eventually eliminate one of the two values for v2 , and so on.
We will say that we are making lexicographic progress when this happens.
In fact, we dont need to maintain a complete ordering on the variables in question.
When we make progress by eliminating a value for vi , all we really need to do is to remember which variables precede vi in the ordering (since their eliminated values must remain
eliminated). Variables following vi in the ordering can be reordered if the subsequent search
suggests doing so.
To formalize this, we maintain a partial order  on the variables appearing in the theory
as Procedure 2.12 is executed.8 When we learn something new about a particular variable
x, we should, as described above, be free to discard information regarding the relative
ordering of variables that follow x in the partial order. Since we have eliminated a value
for the variable x, this further ordering information is no longer needed. We formalize this
as follows:
Definition 3.2 If  is a partial order and x is a point, we define the weakening of  at
x, to be denoted x , to be the partial order given by y x z if and only if y  z and either
y = z or x 6< y.
In other words, the weakened partial order is the same as the original but forgets that
y < z if x < y < z, so that both y and z follow x in the partial order.
Proposition 3.3 x is a partial order.
Proof. Reflexivity is immediate. For transitivity, suppose that u x v x w. Now if u = v,
u x w since v x w. If u 6= v, then x 6< u since u x v. But we also have u  v  w so
u  w. Thus u x w. Anti-symmetry of x follows immediately from the anti-symmetry
of .
As a specific example, if the initial partial order < is given by a < b < c < d, then weakening this partial order at b produces a <b b, b <b c and b <b d. c and d are incomparable
under b , since they both followed b under the original partial order.
Now imagine a justified partial assignment P . If (l, ) is a pair in P , it isWreasonable to
think of  as implying l. In other words, we think of the disjunction  = j lj as
l1      li1  li+1      lk  li
8. We note in passing that it is possible, with some difficulty, to modify all of our work to deal with the
case where  is a preorder instead of a partial order. This provides some additional flexibility in the
manipulation of the bias, but it is not obvious that that additional flexibility is of practical value. We
discuss only the partial order case in the interest of maintaining the clarity of the exposition.

512

fiSatisfiability and Systematicity

where li = l is the literal in the (l, ) pair. While we do not retain the partial assignment
from one search probe to another, we will retain the partial ordering  and use it to split
the antecedent from the conclusion in any particular clause. As a specific example, if c
follows a and b in the ordering, so a  c and b  c, the clause a  b  c will be interpreted
as a  b  c.
In general, if  is a clause and  a partial order, we will think of the -maximal elements
of  as its conclusion, and the rest of  as its antecedent:
Definition 3.4 Let  be a clause and  a partial order on the variables it contains. The
conclusion of , to be denoted   , is the disjunction
  = {l  | 6 m  , m > l}
The antecedent of , to be denoted  , is  = (    ).
The conclusion of the learned clause  is the disjunction of the maximal literals in , and
the antecedent is the negation of whats left.
Again, an example may help. Suppose that  = a  b  c  d and the partial order has
a  c and b  d. Now c and d are the maximal elements of , so the conclusion   is the
disjunction c  d. The antecedent is
 = (    )
= ({a, b, c, d}  {c, d})
= {a, b}
= (a  b)
= a  b
as one would expect. The learned clause has effectively been rewritten as
a  b  c  d
We have been cavalier in our use of notation, switching freely between representing  as a
disjunction and as a set.
Definition 3.5 If  is a set of learned clauses and  a partial order, we will denote by 
the conjunction of the antecedents  for all   . We will denote by  the conjunction
of the conclusions   for all   .
We are finally in a position to describe the procedure by which we add a new clause 
to . In Procedure 2.12, this was via the simple     {} on line 9, but here it is a bit
more complex. The addition of  to  can also both modify the partial order  and replace
 with a lexicographically stronger clause in some circumstances:
513

fiGinsberg

Procedure 3.6 To compute add(, , ):
1 if  =  then return (,   {}, );
2 select l    ;
// l is the intended conclusion
3 if there is a    with  = l then
4
return add(resolve(, ), , )
5 add x  l to  for each x  ;
6 l ;
7 remove from  any c with either c  {l, l} =
6  or both c  {x|l < x} =
6  and
|c | > 1;
8 return (,   {}, )
Line 1 handles the edge case where we have actually derived a contradiction. Line 4
calls for a resolution if both l and l appear as conclusions of clauses in , and line 7 says
that clauses should be deleted from  if they include either l or l in their antecedent, or
if they have ambiguous conclusions based on variables that follow l in the partial order.
This is in keeping with our overall lexicographic approach: Once we make progress on
a particular literal l, we can forget everything about following literals in the partial order,
removing both their relative orderings (as per the weakening on line 6 of Procedure 3.6)
and clauses that contain them (as per the removal on line 7).

Proposition 3.7 Suppose that  contains n clauses involving v variables. Then Procedure 3.6 can be executed in O(v 3 + nv 2 ) time.

Proof. Except for the recursion, the most expensive steps are line 6, which can potentially
take time O(v 2 ) and line 7, which can take time O(nv). This assumes that we can determine
whether x < y in time O(1), but whatever the data structure used to retain the partial
order, we can always compute the entire partial order at each iteration of the loop, which
will take time at most O(v 2 ). The total time taken without the recursion is thus bounded
by O(v 2 + nv).
The maximum number of recursive calls is O(v), since each variable will be resolved
upon at most once.
Just as the o(v 2 ) unit propagation procedure remains practical on problems containing
millions of variables, so we expect the add procedure 3.6 to do so as well. (And for roughly
the same reasons: Clauses are not of length v, literals generally appear in only a relative
handful of clauses, and so on.)
Procedure 3.6 is the primary difference between a standard cdcl algorithm and flex;
we will replace the act of adding  to  with the more involved Procedure 3.6. When we
do so, note that Procedure 3.6 may actually derive the empty clause in line 4; we then
return a theory containing the empty clause on line 1, leading to a failure on line 11 of
Procedure 2.12. We have:
514

fiSatisfiability and Systematicity

Procedure 3.8 (Pure FLEX) Let T be a theory,  a set of clauses such that T |= ,  a
partial order on the variables in T , and B a bias for T . Then to compute flex(T, , , B, n),
one of:  if T is shown to be unsatisfiable, a model P of T if one is found, or unknown
if no solution is found after n steps:
1 i  0;
2 while i < n do
3
(P, c)  unit(T  , P );
4
while c = true do
5
if P assigns a value to every variable in T then return P ;
6
v  a variable unvalued by P ;
7
(P, c)  unit(T  , hP, (B(v), )i);
8
  any (P, c)-conflict that is a uip with B |= ;
9
(, , )  add(, , );
10
B  B|  ;
11
if    then return ;
12
P  backtrack(P, );
13
ii+1
14 return unknown
Note that we require the selection of a buip on line 8 of Procedure 3.8 (since we require
B |= ), and that we adjust the bias only to cater to the conclusion of the new clause  on
line 10. Both of these observations will be important as we discuss practical implementation
of our methods.
The only modification needed to Procedure 2.15 is to initialize and then use the partial
order :
Procedure 3.9 (Pure FLEX with restarts) Let T be a theory and r a restart policy.
Then to compute sat(T, n), either the empty clause  if T is unsatisfiable or a model of T :
1   ;
2  ;
3 B  any bias for T ;
4 i  1;
5 while true do
6
x  flex(T, , , B, r(i));
7
if x 6= unknown then return x;
8
i  i + 1;
We finally have:
Theorem 3.10 Suppose that T is a theory with n clauses involving v variables. Then:
1. Each loop of Procedure 3.8 can be executed in time O(v 3 + nv 2 ),
2. ||  v as Procedure 3.9 is executed, and
515

fiGinsberg

3. For any n with
n.

Pn

i=0 r(i)

 2v , Procedure 3.9 will finish before completing iteration

Corollary 3.11 Procedure 3.9 is strongly systematic and polyspace.

The proof of Theorem 3.10 is lengthy and appears in the appendix.
Note that our contribution here is not to show that it is possible to ensure systematicity
by retaining a sufficient number of learned clauses. Tinisat has already shown this, and
Lecoutre et al. have shown (2007) that retaining a single nogood per restart can suffice.
But in this other work, the number of restarts grows polynomially with the number of
backtracks, and therefore potentially exponentially with problem size. All of these earlier
results thus may require keeping an exponential number of learned clauses; our contribution
is to show that a polynomial number of nogoods can suffice.
3.2 Hybrid FLEX
While Procedures 3.8 and 3.9 achieve the basic theoretical goal set in this paper, a direct
implementation of these procedures as written encounters two practical difficulties.
First, the add procedure is expensive. Executing it at every backtrack introduces an
overall computational burden greater than that of unit propagation.
Second, the search in Procedure 3.8 is guided by the bias B, and that bias is modified
not to match every unit propagation, but only to match the conclusions of newly learned
clauses. Experimentation shows that the rsat bias, matching as it does the result of every
unit propagation, is more effective in practice. We cannot value newly selected variables as
in rsat, since if the bias doesnt support the nogood, there may be no buip on line 8 of
Procedure 3.8. In fact, experimentation on number factoring problems shows that approximately 40% of the time, none of the clauses learned in any particular probe can be resolved
to a buip.
Nevertheless, there is a straightforward way to deal with these issues and to still preserve
the intuition underlying our approach. In each probe, we follow our bias only until a new
clause is learned. That new clause will have a buip (because the bias was followed in
constructing it), and we then incorporate that new clause into  as usual. We then complete
the probe using the rsat bias. By doing this, we follow the rsat bias almost all of the time
and only call add once per probe, eliminating the substantial computational overhead that
this procedure might introduce.
We will refer to this compound procedure as hybrid flex, or simply flex. In order
to implement it, we need to maintain two biases, which we refer to as B (for the flex
component) and R (for the rsat component), and two sets of learned clauses, which we
refer to as  (for the flex component) and  (for the conventional component). We finally
have:
516

fiSatisfiability and Systematicity

Procedure 3.12 (FLEX) Let T be a theory,  and  sets of clauses such that T |=   ,
 a partial order on the variables in T , and B and R biases for T . Then to compute
flex(T, , , B, , R, n), one of:  if T is shown to be unsatisfiable, a model P of T if one
is found, or unknown if no solution is found after n steps:
1 i  0;
2 while i < n do
3
(P, c)  unit(T    , P );
4
while c = true do
5
if P assigns a value to every variable in T then return P ;
6
v  a variable unvalued by P ;
7
if i = 0 then l  B(v);
8
else l  R(v);
9
(P, c)  unit(T    , hP, (l, )i);
10
if i = 0 then
11
  any (P, c)-conflict that is a uip with B |= ;
12
(, , )  add(, , );
13
B  B| 
14
else
  any (P, c)-conflict that is a uip;
15
    {};
16
R  R|P
17
if      then return ;
18
P  backtrack(P, );
19
    discard();
20
ii+1
21 return unknown

Procedure 3.13 (FLEX with restarts) Let T be a theory and r a restart policy. Then
to compute sat(T, n), either the empty clause  if T is unsatisfiable or a model of T :
1     ;
2  ;
3 B  R  any bias for T ;
4 i  1;
5 while true do
6
x  flex(T, , , B, , R, r(i));
7
if x 6= unknown then return x;
8
i  i + 1;
As in Theorem 3.10 and Corollary 3.11, we have:
Corollary 3.14 Procedure 3.13 is strongly systematic and polyspace for any discard procedure that ensures that  is polyspace.
517

fiGinsberg

4. Experimental Results
Before we present experimental results based on our methods, let us describe the setting in
which these experiments were performed.
First, we expect the value of our results to vary with problem size. We have one major
advantage over existing methods, in that the strong systematicity of our approach will cause
probes late in the search to examine areas of the search space that were unexplored earlier.
This advantage is also a drawback, however. Especially early in the search, where the
probes are small, the rsat bias is known to be extremely effective and we are likely to see
a performance degradation from the fact that we cannot follow this bias on the first set of
choices in any particular probe.
So for easy problems, we would expect our methods to perform worse than conventional ones because of our inability to begin probes by following the rsat bias. For difficult
problems, we would expect our methods to perform better than conventional ones, as we
manage to reach new areas of the search space while older methods are reexamining previously explored regions. What we dont know is where the transition will be between these
two general problem types.
In order to evaluate this, we used two separate sets of problems in our experiments. The
first was the set of 1030 problems in the sat 2013 benchmarks. These problems were solved
with a 5000 second cutoff in order to ensure that all processes terminated.
The second set of problems used were number factoring problems (Bebel & Yuen, 2013).
Although subexponential algorithms for number factoring are known to exist (Buhler,
Lenstra, & Pomerance, 1993, and a great deal of subsequent work), there is no reason
to believe that the techniques involved will bear significantly on the sat encodings of such
problems.
One advantage of using number factoring is that problems of gradually increasing difficulty can be run to completion in every case, avoiding the fact that a simple timeout is less
informative regarding overall computational efficacy than is the time needed to actually
solve a problem. Problems of gradually increasing difficulty can be generated by simply
increasing the magnitudes of the numbers being factored.
The number factoring problems were generated by using an iterated Miller-Rabin test to
generate pairs of primes and then using the Purdom/Sabry (2005) sat generator to produce
a sat encoding of the problem of factoring the product of these primes. In order to avoid
situations where the factoring problem might be too easy, the prime factors were selected
so that the sizes of their binary representations were within three bits of one another. For
any given problem size (number of bits in the number being factored), fifty instances were
generated randomly.
As a baseline solver, we used minisat 2.2 (Sorensson & Een, 2005). But there were in
fact three separate sets of changes that we needed to make to produce flex:
1. First, we converted minisat from C to C++ and changed it to use the C++ standard
library in many locations instead of the variety of specialized techniques included in
minisat. We will refer to this solver as stlsat.
2. Second, flex can be expected to be more efficient as the number of restarts increases.
In minisat and similar solvers, the number of backtracks before restarting is not given
518

fiSatisfiability and Systematicity

10000

1.22 x ** 0.9916
x

1000

stlsat

100

10

1

0.1

0.01
0.01

0.1

1

10
minisat

100

1000

10000

Figure 1: minisat vs. stlsat, sat competition problems (CPU time in seconds)
as specified by the original Luby policy (Luby et al., 1993), but instead by multiplying
the Luby numbers by some constant (generally taken to be 100, so that the first probe
involves 100 backtracks). In order to increase the number of probes, we reduced the
multiplicative factor to four (so that the first probe involves only four backtracks).
We will refer to this solver as stlsat4 .
3. Finally, we modified stlsat to produce flex, also with an initial probe size of four.
We present results for the sat 2013 benchmarks first, followed by results for number
factoring. We compare minisat to stlsat, stlsat to stlsat4 , and then stlsat4 to flex.
All experiments were run on a 24-core Intel Xeon machine running at 2.2GHz with 256K
of L2 cache per processor, 24GB of main memory, and 75GB of swap space. 24 problems
were solved at a time, and the swap space was needed to deal with some of the larger sat
competition instances, since many were running in parallel.9
The software used in these experiments is available as an online appendix to this paper.
stlsat.zip contains the source for stlsat, and flex.zip contains the source for flex. (Stlsat
is just flex with the various extensions removed.)
4.1 Sat Competition Problems
Figure 1 compares minisat to stlsat for the sat competition problems. The solver that is
closer to original minisat will always be on the x-axis, with the modification on the y-axis,
9. We did not evaluate the impact of the use of swap on the running times of larger problems, although
it obviously might be substantial. In practice, however, once significant amounts of swap were used,
none of the systems being evaluated was able to solve the problem in question. In addition, running the
problems serially would simply have been impractical given the available hardware.

519

fiGinsberg

10000

1.14 x ** 0.9999
x

1000

stlsat_4

100

10

1

0.1

0.01
0.01

0.1

1

10
stlsat

100

1000

10000

Figure 2: stlsat vs. stlsat4 , sat competition problems (CPU time in seconds)

with both axes plotted using a log scale and a dashed line giving the y = x baseline. One
point is plotted for each problem solved by either of the two methods. A point above x = y
means that the old solver is better; a point below x = y, that the new solver is better.
In many cases, we will also give the best polynomial fit of the form y = axb for fixed
a and b. So in Figure 1, stlsat is seen to be 22% worse than minisat at the outset, but
the exponent slightly below one indicates that the disparity is shrinking as the problems
become more difficult. (Indeed, this is apparent from looking at the graph itself.)
To find the polynomial fit, we did not use the standard technique of minimizing the
squared vertical distance between a point and the line in question. The reason is that
if we flipped the axes, we would then be minimizing the squared horizontal distance to
the line, and a potentially different fit would be found. To ensure that the same fit was
found independent of the axis selection, we minimized the sum of the squares of the actual
(perpendicular) distances between the data points and the line in question.
In some cases, we will split the data depending on whether the problem instances being
solved were or were not satisfiable. This is not an interesting distinction here.
These results are as one might expect. Without the fine-tuned data structures of minisat, stlsat performs slightly worse. This is more significant on easy problems, and the
two solvers appear to be virtually identical on more difficult instances.
Figures 2 and 3 show the impact of reducing the number of backtracks per probe. Overall
(Figure 2), there is a 14% cost to making the change, and the cost is nearly independent of
problem size. Somewhat curiously, however, the cost is significantly different for satisfiable
instances (a 22% cost, top of Figure 3) when compared to unsatisfiable ones (a 7% savings,
bottom of Figure 3).
520

fiSatisfiability and Systematicity

10000

1.22 x ** 1.0043
x

stlsat_4 -- SAT ONLY

1000

100

10

1

0.1

0.01
0.01

stlsat_4 -- UNSAT ONLY

10000

0.1

1

10
stlsat -- SAT ONLY

100

1000

10000

0.93 x ** 0.9999
x

1000

100
100

1000
stlsat -- UNSAT ONLY

10000

Figure 3: stlsat vs. stlsat4 , sat competition problems (CPU time in seconds)

521

fiGinsberg

100000

3.60 x ** 0.9935
x

10000

1000

flex

100

10

1

0.1

0.01
0.01

0.1

1

10
stlsat_4

100

1000

10000

Figure 4: stlsat4 vs. flex, sat competition problems (CPU time in seconds)

Note that the scatter in these cases is much more significant than that of Figure 1.
This is as expected  stlsat is intended to be essentially unchanged from minisat from
an algorithmic perspective, while the change in restart frequency can be expected to have
fairly dramatic effects. Note also the collection of points with either x = 5000 or y = 5000,
indicating that the problem was solved by one method but timed out using the other.
Finally, Figure 4 compares stlsat4 to flex on the sat competition problems, and flex
is about a factor of 3.6 slower (4.53 times slower for satisfiable instances; 2.01 times slower
for unsatisfiable ones).10 Many more problems time out for flex than for the modified
version of minisat.
This is shown clearly in Figure 5, where we compare the number of problems being
solved by stlsat4 and by flex on the sat competition problems. For any specific time
cutoff, stlsat4 solves about 2.5 times as many problems as does flex.
It does appear to be the case, however, that the performance gap is narrowing as the
problems become more difficult; for example, flex solves 36 problems in between 3000
and 5000 seconds; stlsat4 solves 42 problems in that time. It is for this reason that we
examined number factoring problems as well.11

10. The graphs for satisfiable and unsatisfiable instances are not shown; they look basically the same as
Figure 4.
11. Note that we cannot simply increase the timeout limit for the sat competition problems; doing so caused
only a relative handful of new problems to be solved. If we are to generate an interesting number of
more difficult problems, and want the instances to be known to be satisfiable, number factoring seems
to be the simplest way to go.

522

fiSatisfiability and Systematicity

300

flex
stlsat4

250

problems solved

200

150

100

50

0
0.01

0.1

1

10
CPU time (secs)

100

1000

10000

Figure 5: stlsat4 vs. flex, sat competition problems (5000 second timeout)
100000

1.06 x ** 1.0069
x

10000
1000
100

stlsat_4

10
1
0.1
0.01
0.001
0.0001
1e-05
0.0001

0.001

0.01

0.1

1
10
stlsat (all SAT)

100

1000

10000

100000

Figure 6: stlsat vs. stlsat4 , factoring problems (CPU time in seconds)

4.2 Number Factoring
For number factoring, we generated 50 factoring problems of each size from five to 45 bits.
These problems were then run through stlsat, stlsat4 and flex.
523

fiGinsberg

100000

1.41 x ** 0.9743
x

10000
1000
100

flex

10
1
0.1
0.01
0.001
0.0001
1e-05
1e-05

0.0001

0.001

0.01

0.1
1
10
stlsat_4 (all SAT)

100

1000

10000

100000

Figure 7: stlsat4 vs. flex, factoring problems (CPU time in seconds; 5000 second timeout)

The (relatively uninteresting) result of comparing stlsat and stlsat4 is shown in
Figure 6. As for the sat competition problems, reducing the size of the first probe to four
backtracks leads to a small degradation in performance on satisfiable problems. We include
this primarily to ensure that the change in probe size is not responsible for the results in
the following figures.
Flex and stlsat4 are compared in Figure 7. Here, we show only problems that one
method or the other was able to solve in 5000 seconds or less, although exact times to
solution are used for both solvers. The figure also includes dotted lines indicating the 5000
second timeout for either solver.
The overall results are similar to those of the previous section, although flexs performance is improved somewhat. On sat competition problems, it was 3.6 times slower overall
and 4.53 times slower for satisfiable problems specifically. On number factoring problems,
flex is only 41% slower than is stlsat4 .
The curve fit is being dominated by the large number of easy problems in this case.
In addition, it appears that flex is actually doing better than stlsat4 as the problems
become more difficult; many more problems appear to be timing out with stlsat4 (points
to the right of x = 5000 seconds in Figure 7) than with flex (points above y = 5000 in the
figure). This suggests that instead of looking at the easiest factoring problems, we look at
the hardest ones.
This is done in Figure 8, where we consider only problems for which one of the two solvers
required at least 5000 seconds to reach a solution. The results are strikingly different. A
glance shows that flex is in general solving problems more quickly than is stlsat4 ; in fact,
524

fiSatisfiability and Systematicity

100000

0.24 x ** 1.0010
x

flex

10000

1000

100

10

10

100

1000
stlsat_4 (all SAT)

10000

100000

Figure 8: stlsat4 vs. flex, factoring problems (CPU time in seconds; 5000 seconds or
harder)

100000

0.13 x ** 1.0101
x

flex

10000

1000

100

10
100

1000

10000

100000

stlsat_4 (all SAT)

Figure 9: stlsat4 vs. flex, factoring problems (CPU time in seconds; 10,000 seconds or
harder)

525

fiGinsberg

100000

stlsat
flex

10000

average CPU time (secs)

1000

100

10

1

0.1

0.01

0.001

0.0001

5

10

15

20

25
bits

30

35

40

45

Figure 10: stlsat4 vs. flex, average time to factor an n-bit number

the line of best fit shows that flex is about four times as fast is as stlsat4 . Restricting
to problems that took at least 10,000 seconds makes the distinction sharper still; now
(Figure 9) flex is eight times faster than is stlsat4 .
Similar results appear in Figure 10, which shows the average running time needed by
flex or by stlsat to factor a number of n bits. Flex is slower for smaller numbers but
faster for larger ones. It is unfortunately impractical to extend this graph much further, since
the average runtime for stlsat on a single 45-bit factoring problem is already approximately
six hours.
As previously, we show the number of difficult problems (40 bits or more) solved by the
solvers as a function of time in Figure 11. With a cutoff of less than 1000 seconds, stlsat4
performs best; as the problems get harder, this is reversed.

5. Conclusion and Future Work
The fundamental claim that we have made in this paper is that the sat community did
not need to abandon the systematicity of its methods when it switched from dpll-style
provers to cdcl-based ones. We showed that it was possible to simultaneously guarantee
systematicity, maintain a polynomially-sized set of learned clauses, and restart the prover
as frequently as desired. Furthermore, we showed that while a hybrid between a pure
systematic prover and a more typical cdcl engine incurs significant computational cost in
achieving these goals, that cost and more is recovered and run time appears to be reduced
for difficult problems.
Looking forward, there are two obvious ways in which this work can be extended.
526

fiSatisfiability and Systematicity

300

flex
stlsat4

250

problems solved

200

150

100

50

0
1

10

100
1000
CPU time (secs)

10000

100000

Figure 11: stlsat4 vs. flex, factoring problems (40 bits or larger)

First, we have seen that the value of our methods increases as the number of restarts
increases. One of the principal reasons to limit the number of restarts in a cdcl-based
prover is that restarting the prover is expensive. Recent work by Ramos et al. (2011),
however, suggests that a significant part of this expense can be avoided. The basic idea
is that any particular probe may well begin its search by repeating many of the literal
choices and unit propagations from the previous probe. In such a case, there is no reason
to backtrack past the point at which the two probes first diverge. Incorporating this idea
into our methods should improve their value further by enabling a larger number of restarts
and thus making the systematic component that we offer more effective.
Second, we hope that the overall approach that we have proposed  guaranteeing systematicity by retaining a partial order and an appropriate set of learned clauses  finally
puts to rest the notion that sat engines achieve systematicity only by careful management
of a search space defined by partial assignments. It is instead the learned clausal database
that guarantees systematicity through semantic methods.
This general conclusion should lead to a variety of new algorithmic choices. As an
example, the values represented by the bias are presumably best thought of as probabilistic
estimates regarding the likelihood of any particular literal appearing in a model of the
theory in question. This probabilistic approach gives a very different flavor to satisfiability
in general, and suggests that recent results from Monte Carlo Tree Search, or mcts (Browne,
Powley, Whitehouse, Lucas, Cowling, Rohlfshagen, Tavener, Perez, Samothrakis, & Colton,
2012; Kocsis & Szepesvari, 2006, and others) may well find a place in sat as well.
527

fiGinsberg

Acknowledgments
I would like to thank my Connected Signals and On Time Systems coworkers, especially
Aran Clauson and Heidi Dixon, for useful technical advice and assistance. I would also
like to thank the anonymous referees for their many carefully considered comments and
suggestions, which improved this paper enormously. Finally, I would like to thank David
McAllester for many contributions to this work over the years; the work presented here draws
greatly from McAllesters original work (1993) on partial-order dynamic backtracking.

Appendix A. Proof of Theorem 3.10
The proof of Theorem 3.10 will depend on certain invariants that are maintained as Procedure 3.8 is executed. We begin by discussing those invariants.
A.1 Learned Clauses
To understand them, suppose that a set of learned clauses  has been derived during the
course of a systematic search. What conditions would we expect  to satisfy?
To answer this, imagine that we have a partial assignment P and that we have derived
a new learned clause  that we are about to add to .
We certainly expect P to satisfy the existing learned clauses. We will also assume that
P satisfies the antecedents of those learned clauses; this is in keeping with the idea that the
learned clauses should continue to be relevant to the portion of the search space being
explored.
What about the new clause ? First, it will turn out that  should have a unique
conclusion under the partial order . In addition, the new learned clause  should be new
knowledge in the sense that it eliminates a portion of the search space that is still admissible
under . In other words, we expect the conclusion   to be distinct from the conclusions
 of other elements of .
Formally, we have:
Definition A.1 We will say that a partial order  parses a learned clause  if |  | = 1.
We will say that a set of learned clauses  is acceptable for a partial order  if the
following conditions hold:
1. For any   ,  parses ,
2. For any 1 , 2  , if 1 = 2 , then 1 = 2 , and
3.  6|= .
The first condition repeats the requirement that the conclusions   are all individual
literals. The second condition says that  does not contain two distinct learned clauses
with the same conclusion. The third says that it is at least possible to find a partial
assignment P as described above, since the collection of learned clauses  is consistent with
the antecedents  of those learned clauses.
Lemma A.2 If  is acceptable for a partial order , then  contains at most one learned
clause with any particular variable x in its conclusion.
528

fiSatisfiability and Systematicity

Proof. If  contains clauses with both x and x as conclusions, it will be impossible to
have both  and  , contradicting the requirement of acceptability.
Proposition A.3 If  is a set of clauses involving v variables and is acceptable for a partial
order , then ||  v.
A.2 Search Space Size
Before considering the general way in which the new clause  allows us to improve the bias,
and the way in which the new clause is incorporated into , let us examine a simple special
case where  is in fact a total order on the variables in question. This means that any
particular learned clause  is always interpreted as forcing the value of the single variable
it contains that occurs latest under the ordering .
Now given a set of learned clauses , we extend P until we either solve the problem or
derive a new learned clause . Given the bias B, we are assuming that B |=  so that the
learned clause allows us to improve the bias, and we have already remarked that we assume
that B is consistent with    . It follows that  is consistent with    as well. If l
is the conclusion of , how are we to update ?
Note first that there can be no    with l as its conclusion. If there were, then we
would have    |= l and therefore    |=  also. There are now two cases:  may
contain a learned clause with conclusion l, or  may contain no such learned clause.
If  does not contain a learned clause with conclusion l, we would like to simply add
 to . But doing so may cause  to become unacceptable, since  may contain a learned
clause with l in its antecedent. If we simply add  to , the result will be unacceptable
with respect to  because it will be impossible to satisfy all of the elements of  while
simultaneously satisfying their antecedents.
As an example, suppose that  contains the learned clause a  c  d, the antecedent of
which is a  c. If we learn a  c and simply add it to , then    includes both a  c
and a  c, a contradiction.
What we can do in this case is to remove from  every learned clause that has l in
its antecedent; call the result 0 . We now take add(, ) to be 0  {}.  parses  and
obviously continues to parse all of the learned clauses remaining in 0 . 0  {} is therefore
acceptable with respect to , since l can no longer appear in the antecedent of any learned
clause in 0 .
Given that we have discarded some of the learned clauses in , do we have any basis
for concluding that the procedure will eventually terminate? We do. As mentioned in the
main body of the text, the key observation is that we have made lexicographic progress in
the search space. While it is true that the allowed domains of all of the variables following
l may have gotten bigger when we replaced  with 0 , the domain for l itself has gotten
smaller. Since l precedes these other variables in the total order, we have made lexicographic
progress and systematicity remains guaranteed.
Consider now the second case, where  does contain a learned clause with conclusion l.
Note first that since  is acceptable, the learned clause concluding l must be unique. We
can resolve this learned clause with  to obtain a new learned clause . Every variable in 
precedes l according to the ordering , so the conclusion of  must precede l as well. We can
thus continue to make lexicographic progress if we define add(, ) to be add(, ), where
529

fiGinsberg

we have essentially chosen to incorporate the lexicographically more powerful resolvent 
instead of the original learned clause .
To ensure that these ideas are clear, suppose that our original learned clause set  is
given by the following, where the ordering  ranks the variables alphabetically:
ac  d

(6)

cd  e
Now if the new learned clause is a  b  f , we simply add it to . No learned clause
need be removed because the conclusion f does not appear in the antecedent of any learned
clause in .
Having added a  b  f , suppose that we now learn a  c. As in the example above,
we add the learned clause to  and remove every learned clause whose antecedent includes
c, which means that both of the original learned clauses are removed from , although
a  b  f is retained. The new  set is:
ab  f

(7)

a  c
We have made lexicographic progress. If a is true, the original learned clause set (6)
allowed c to be either true or false; the new learned clause set (7) forces c to be false.
Reducing the number of possible values for c counts as progress, whatever happens to the
possible values for subsequent variables.
Suppose that instead of learning a  c, we had learned c  f . This conflicts with
a  b  f , so we resolve the two together to obtain a  b  c, which the total order
causes us to interpret as a  b  c. We add this to  as in the previous paragraph; once
again, lexicographic progress has been made.
This restricted example contains the essence of the ideas we will use in the more general
case. The general case is more complex because, just as we must ensure that  remains
acceptable, we would also like to ensure that the partial order  remains as flexible as
possible. If  were to approach a total order, it would tend to force specific interpretations
of newly learned clauses, reducing our ability to reverse decisions that were made early in
the search. This would mean that we would have relatively little flexibility in modifying the
bias; we would always have to simply flip the bias variable that was most recently valued
even as evidence accumulated that this variable was valued correctly.
We begin by formalizing the notion of lexicographic progress.
Definition A.4 If  is a set of learned clauses and  is a total order, we will denote by
||x the number of elements of  that have conclusion either x or x. We will denote by
|>x| the number of y with y > x.
If v is the number of variables being considered, we now define the total order size of 
under  to be
X
sizet (, ) = 2v 
2|>x| ||x
(8)
x

Proposition A.5 If  is acceptable for a total order , then 2v  sizet (, ) > 0.
530

fiSatisfiability and Systematicity

Proof. The first inequality is immediate. For the second, since ||x  1,
sizet (, )  2v 

X

2|>x|

x

But consider the summation in isolation, where we have:
X

2|>x| 

x

v
X

2i1 = 2v  1.

i=1

Thus sizet (, ) > 0.
Before moving on, let us explain the intuition underlying Definition A.4. The basic
idea is that the expression in (8) should reflect the size of the search space remaining to be
considered, because
X
2|>x| ||x
x

counts the number of potential models eliminated by the learned clauses in .
That it does should be fairly clear. The unique learned clause in ||x eliminates a value
for x. Each such elimination corresponds to the removal of a subtree in the overall search
space. The size of the subtree is given by 2|>x| , since |>x| is the number of variables properly
following x in the ordering.
Proposition A.6 Fix a total order , and let  and 0 be two sets of learned clauses.
Assume that there is some x with ||x < |0 |x but ||y = |0 |y for all y < x.
Now if 0 is a total order that agrees with  when restricted to the set {y, z|y, z  x},
then sizet (0 , 0 ) < sizet (, ).
Proof. This is simply a formalization of the lexicographic argument made earlier. To show
sizet (, )  sizet (0 , 0 ) > 0, we have
sizet (, )  sizet (0 , 0 ) =

X

0

2|> y| |0 |y 

X

y0

2|>y| ||y

(9)

y

We will show:
1. For any y < x,
0

2|> y| |0 |y = 2|>y| ||y

(10)

2|> x| |0 |x  2|>x| ||x  2|>x|

(11)

2. For y = x,
0

3. For y > x,
X

2|>y| ||y < 2|>x|

y

531

(12)

fiGinsberg

Collectively, these suffice. The terms (10) with y < x have no effect on the difference in (9).
The amount contributed by the term (11) with y = x is greater than the amount subtracted
by the terms (12) with y > x. Thus the total sum is positive and the result follows.
(10) is a consequence of the fact that for y  x, |>0 y| = |>y| and |0 |y = ||y . For (11),
since |0 |x > ||x , we get
2|>x| ||x  2|>x| (|0 |x  1)
0

= 2|> x| |0 |x  2|>x|
For (12), the sum is essentially identical to that appearing at the end of the previous
proof.
Definition A.7 A partial order 0 will be called a refinement of a partial order  if x 
y  x 0 y. A refinement of  that is a total order will be called a total refinement of .
Definition A.8 For a set of learned clauses  and partial order , we define the size of 
under , denoted size(, ), to be the maximum of sizet (, 0 ) over all total refinements
0 of .
The following is an immediate consequence of Proposition A.5:
Corollary A.9 If  is acceptable for a partial order , then 2v  size(, ) > 0.
The point of all this is that we can now state our desiderata for add a bit more precisely.
We need:
1. Polynomial space: If  is acceptable with respect to  and ( 0 , 0 , 0 ) = add(, , ),
then we want 0 to be acceptable with respect to 0 . This will allow us to use Proposition A.3 to conclude that  can be stored in polynomial space as the algorithm
proceeds.  and the bias B can obviously be stored in polynomial space as well.
2. Systematicity: If ( 0 , 0 , 0 ) = add(, , ), then we want size(0 , 0 ) < size(, ).
This will allow us to use Corollary A.9 to conclude that add can be invoked at most
2v times before the entire search space is eliminated.
A.3 Systematicity
Recall:
Procedure 3.6 To compute add(, , ):
1
2
3
4
5
6
7

8

if  =  then return (,   {}, );
select l    ;
// l is the intended conclusion
if there is a    with  = l then
return add(resolve(, ), , )
add x  l to  for each x  ;
l ;
remove from  any c with either c  {l, l} =
6  or both c  {x|l < x} =
6  and
|c | > 1;
return (,   {}, )
532

fiSatisfiability and Systematicity

Proposition A.10 Suppose that  is acceptable with respect to , and that    6|= .
Then if ( 0 , 0 , 0 ) = add(, , ), 0 is acceptable with respect to 0 .
Proof. Note first that the procedure terminates. The only possible source of nontermination is the recursive call on line 4, but the conclusion of the resolvent is necessarily <
the conclusion of . The number of recursive calls is therefore bounded, and the procedure
terminates.
To show that 0 is acceptable, we consider first the nonrecursive case where there is no
   with conclusion l. For the three conditions of Definition A.1, we have:
1. To show that 0 parses c for any c  0 , there are two cases.
If c is the learned clause , we know by construction that 0 includes y  x for every
y appearing in c.
For the case where c  , we need the following lemma:
Lemma A.11 Suppose that x  u but x 60 u. Then l <0 x and l < x.
Proof. Suppose that we denote by + the partial order constructed in line 5 of the
procedure, where additional arcs are added to the original . Since + is a refinement
of , it is clear that if x  u, we will have x + u as well.
+
0
+
We now have 0 =+
l , so the only way to have x  u but not x 6 u is if l < x, so
0
that l < x as well.

To show that l < x, note that if l <0 x, then l <+ x because arcs are removed in the
construction of <0 from <+ . But if l <+ x we must also have l < x, since the arcs
added in the construction of <+ are all of the form z < l.
0

Returning to the proof of the proposition, suppose that y, z  c with y and z
incomparable under 0 . Now if either l <0 y or l <0 z, the clause in question would
have been eliminated by line 7 of the add construction, so it follows that l 6<0 y and
l 6<0 z.
We claim that there is no u  c with y < u. If there were such a u, then we would
0
have y 0 u by virtue of the lemma, so y 6 c . Similarly there is no u  c with z < u.
Thus y, z  c and therefore y = z, since  parses c.
2. To show that no two elements of 0 have the same conclusion, note first that no two
elements of 0 have the same conclusion under the original partial order . This is
true by construction for two elements of . If the new learned clause  were to have
the same conclusion as an element of , we would have had    |= .
The only way for the conclusion of c  0 under 0 to be different than the conclusion
under  is if there are y, z  c with y < z but z <0 y. But then by the lemma we
must have l < y and l <0 y. Since l < y < z, l < z and therefore l <0 z. But now the
weakening in line 6 causes y and z to be incomparable under <0 .
533

fiGinsberg

3. Finally, we must show that 00 6|= 0 , or that 00 and 0 are consistent.
To see this, we can use the fact that    6|=  to find a complete assignment P of
values to variables that satisfies ,  and . Note that P must both satisfy the
antecedent of  and include l in order to falsify .
We now claim that P |l satisfies both 00 and 0 . P |l continues to satisfy the antecedent
 , since l 6  , and P |l satisfies  since it contains l. We therefore need only consider
c  0 that appear in  as well.
Since P satisfies both c and c , the only way that P |l might not satisfy c and c0 is
if P |l fails to satisfy c because the conclusion of c is no longer satisfied, or P |l fails
to satisfy c0 because one of the terms in the antecedent is no longer satisfied. The
first of these cannot happen because P |l differs from P only at l, and l cannot be cs
conclusion. The second cannot happen because if l appears in the antecedent of c,
c would have been removed in the construction of 0 .
This concludes the proof that  remains acceptable if a nonrecursive path is taken
through add; to see that the recursive path remains acceptable as well, we need to show
that the fundamental requirement that    6|=  holds for the recursive call. In other
words, the proof will be complete if we can show that    6|= resolve(, ) in line 4.
Suppose that    |= resolve(, ), and note that
resolve(, ) =   
Now    , so we must therefore have
   |= 
But this would imply    |= , contradicting the assumptions of the proposition. The
proof is complete.
Proposition A.12 Suppose that  is acceptable with respect to , and that    6|= .
Then if ( 0 , 0 , 0 ) = add(, , ), size(0 , 0 ) < size(, ).
Proof. We need the following lemma:
Lemma A.13 Let  be a partial order and x a point. Then if Tx is a total refinement
of x , there is a total refinement T of  such that Tx and T agree when restricted to
{y, z|y, z Tx x}.
Proof. Define y 0 z if and only if any of the following three conditions hold:
1. y, z Tx x and y Tx z,
2. x <Tx y, z and y  z, or
3. y Tx x <Tx z.
534

fiSatisfiability and Systematicity

We claim that 0 is a partial order satisfying the requirements of the lemma. (Informally,
0 is the partial order that matches  above x and Tx below x.)
That 0 is reflexive is clear. For transitivity, suppose that u 0 v 0 w. If x <Tx u, the
second clause of the definition is the only one that can support u 0 v, so we must have
x <Tx v and similarly x <Tx w together with u  v  w, so that u 0 w. If w Tx x, an
analogous argument about applicability of the first clause requires v Tx x and similarly
u Tx x; now the transitivity of Tx gives u Tx w so u 0 w also. Finally, if u Tx x <Tx w,
we get u 0 w directly.
To see that 0 is anti-symmetric, suppose u 0 v 0 u with u 6= v. We cannot have
both u Tx x Tx v and v Tx x Tx u because Tx is a partial order, so either the first or the
second clause of the definition of 0 must apply. Either way, we must have u = v because
both Tx and  are partial orders.
To see that Tx and 0 agree when restricted to points y and z with y, z Tx x, note that
it is only the first clause in the definition that ever asserts y 0 x for such y, and this clause
says that Tx and 0 match.
It remains only to show that 0 is a refinement of , so that if y  z, then y 0 z.
So suppose that y  z. Now if x  y, then x x y and therefore x Tx y. Similarly,
x Tx z. Thus case 2 of the definition applies, and y 0 z.
If, on the other hand, x 6 y, then y x z by the definition of the weakening x . Thus
y Tx z as well. Now there are two cases, depending on the Tx relationship between x and
y:
1. If x Tx y, then x Tx z as well and case 2 of the definition implies y 0 z.
2. If y Tx x (recall that Tx is total), there are two subcases:
(a) If x Tx z, then case 3 of the definition applies and y 0 z.
(b) If z Tx x, then case 1 applies.
In all cases, we can conclude y 0 z from y  z.
This shows that 0 is a partial order satisfying the conditions of the lemma. Taking T
to be any total refinement of 0 completes the proof.
We can now return to the proof of Proposition A.12. As earlier, we denote by + the
partial order obtained in line 5 of the add procedure, where we have added new constraints
based on the selection of l as the conclusion of . We will show that
size(0 , 0 ) < size(, + )  size(, )

(13)

The second inequality is easy; since + is a refinement of , there are fewer further
refinements to consider in Definition A.4 of size. For the first inequality, if 1 is any total
order used to evaluate size(0 , 0 ), we must show that there is some total order 2 used to
evaluate size(, + ) for which sizet (0 , 1 ) < sizet (, 2 ).
Recall that lines 5 and 6 of Procedure 3.6 constructed 0 as +
l . It follows that 1 is
a total refinement of +
.
We
therefore
know
from
the
lemma
that
there is some 2 that
l
is a total refinement of + such that 1 and 2 agree for {x, y|x, y 1 l}. Since   0 ,
|0 |l > ||l and we can thus apply Proposition A.6 to conclude
sizet (0 , 1 ) < sizet (, 2 )
535

fiGinsberg

It follows that the maximum over the 1 s is less than the maximum over the 2 s, so
that
size(0 , 0 ) < size(, + )
as needed in (13). The proof is complete.
Procedure 3.8 Let T be a theory,  a set of clauses such that T |= ,  a partial order on
the variables in T , and B a bias for T . Then to compute flex(T, , , B, n), one of:  if
T is shown to be unsatisfiable, a model P of T if one is found, or unknown if no solution
is found after n steps:
1 i  0;
2 while i < n do
3
(P, c)  unit(T  , P );
4
while c = true do
5
if P assigns a value to every variable in T then return P ;
6
v  a variable unvalued by P ;
7
(P, c)  unit(T  , hP, (B(v), )i);
8
  any (P, c)-conflict that is a uip with B |= ;
9
(, , )  add(, , );
10
B  B|  ;
11
if    then return ;
12
P  backtrack(P, );
13
ii+1
14 return unknown
Proposition A.14 At every iteration of the loop in Procedure 3.8, the following conditions
hold:
1. B |=     C(P ).
2.    6|= C(P ).
3. T |= .
4.  is acceptable for .
Proof. To see that B |= C(P ), note that this is obviously true at the start, where C(P ) =
. Then there are three places where it could fail: line 7 where a new variable assignment
is made and unit propagated, line 3 where P is extended by unit propagation, and line 10,
where B is modified to incorporate the conclusion of .
Line 7 is consistent with B by construction. Line 3 does not change C(P ), since no new
choices are made. On line 10,  is always unit after the backtrack on line 12, so adding the
conclusion of  to B will not conflict with any choice that survives that backtrack.
That B |=    throughout is a bit more subtle; we begin by showing that B |=   
specifically. Clearly B |= , since the bias has been modified to satisfy the conclusion   .
To see that B |=  , note that prior to the adjustment on line 10, B |=  by construction
so that B |=  . Negating the conclusion   will not change this.
536

fiSatisfiability and Systematicity

Next, we must show that for any other   , B |=    after line 10 is complete.
In order for B to stop entailing a particular learned clause , the conclusion of  must
be   , since only   is modified in line 10. But in this case, a resolution will have been
performed by add and a different  returned. Thus B |= .
In order for B to stop entailing a particular antecedent  ,   must have been included
in that antecedent. But in this case, the learned clause in question will have been removed
by the add procedure. Thus B |=  as well. This concludes the proof of the first claim of
Proposition A.14.
The second claim follows from this; if  |= C(P ), then B would entail both C(P )
and C(P ) and would be inconsistent as a result.
To see that T |= , note first that T |=  in line 8 by the construction of a buip. The
add procedure 3.6 adds to  either  or the result of resolving  with existing elements of
; either way, the clause being added is necessarily entailed by T . The fact that various
learned clauses may be removed from  clearly does not affect the conclusion that T |= .
Finally,    6|=  by virtue of the facts that B |=    and B |= . Thus
Proposition A.10 can be used to conclude that  remains acceptable for .
Theorem 3.10 Suppose that T is a theory with n clauses involving v variables. Then:
1. Each loop of Procedure 3.8 can be executed in time O(v 3 + nv 2 ),
2. ||  v as Procedure 3.9 is executed, and
P
3. For any n with ni=0 r(i)  2v , Procedure 3.9 will finish before completing iteration
n.
Proof. Since  is acceptable with respect to , the second claim follows from Proposition A.3. The first claim now follows from Proposition 3.7. The third claim follows from
Proposition A.5, which bounds size(, ) to be between 0 and 2v , and Proposition A.12,
which ensures that the size decreases on each pass through the loop in Procedure 3.8.

References
Bayardo, R. J., & Schrag, R. C. (1997). Using CSP look-back techniques to solve real-world
SAT instances. In Proceedings of the Fourteenth National Conference on Artificial
Intelligence, pp. 203208.
Beame, P., Kautz, H., & Sabharwal, A. (2004). Towards understanding and harnessing the
potential of clause learning. Journal of Artificial Intelligence Research, 22, 319351.
Bebel, J., & Yuen, H. (2013). Hard SAT instances based on factoring. In Proceedings of
SAT Competition 2013: Solver and Benchmark Description, p. 102, Helsinki, Finland.
Beck, J. C. (2005). Multi-point constructive search. In Proc. of the Eleventh Intl. Conf.
on Principles and Practice of Constraint Programming (CP05), pp. 737741.
Ben-Sasson, E., Impagliazzo, R., & Wigderson, A. (2000). Near-optimal separation of treelike and general resolution. Tech. rep., Electronic Colloquium in Computation Complexity.
537

fiGinsberg

Biere, A., Heule, M. J. H., van Maaren, H., & Walsh, T. (Eds.). (2009). Handbook of
Satisfiability, Vol. 185 of Frontiers in Artificial Intelligence and Applications. IOS
Press.
Bonet, M. L., & Johannsen, J. (2014). Improved separations of regular resolution from clause
learning proof systems. Journal of Artificial Intelligence Research, 49, 669703.
Bonet, M. L., Pitassi, T., & Raz, R. (1997). Lower bounds for cutting planes proofs with
small coefficients. Journal of Symbolic Logic, 62 (3), 708728.
Browne, C., Powley, E., Whitehouse, D., Lucas, S., Cowling, P. I., Rohlfshagen, P., Tavener,
S., Perez, D., Samothrakis, S., & Colton, S. (2012). A survey of Monte Carlo tree
search methods. IEEE Transactions on Computational Intelligence and AI in Games,
4 (1), 149.
Buhler, J., Lenstra, H., & Pomerance, C. (1993). Factoring integers with the number field
sieve, Vol. 1554 of Lecture Notes in Mathematics, pp. 5094. Springer-Verlag.
Buss, S. R., Hoffmann, J., & Johannsen, J. (2008). Resolution trees with lemmas: Resolution
refinements that characterize DLL algorithms with clause learning. Logical Methods
in Computer Science, 4.
Clark, K. L. (1978). Negation as failure. In Gallaire, H., & Minker, J. (Eds.), Logic and
Data Bases, pp. 293322. Plenum, New York.
Davis, M., & Putnam, H. (1960). A computing procedure for quantification theory. J.
Assoc. Comput. Mach., 7, 201215.
Davis, M., Logemann, G., & Loveland, D. (1962). A machine program for theorem-proving.
Communications of the ACM, 5 (7), 394397.
Doyle, J. (1979). A truth maintenance system. Artificial Intelligence, 12, 231272.
Gaschnig, J. (1979). Performance Measurement and Analysis of Certain Search Algorithms.
Ph.D. thesis, Carnegie-Mellon University.
Ginsberg, M. L. (1993). Dynamic backtracking. Journal of Artificial Intelligence Research,
1, 2546.
Gomes, C. P., Selman, B., & Kautz, H. (1998). Boosting combinatorial search through
randomization. In Proceedings of the Fifteenth National Conference on Artificial Intelligence (AAAI98), pp. 431437, Madison, Wisconsin.
Goultiaeva, A., & Bacchus, F. (2012). Off the trail: Re-examining the CDCL algorithm.
In Proceedings of the 15th International Conference on Theory and Applications of
Satisfiability Testing (SAT-2012), pp. 3043.
Haken, A. (1985). The intractability of resolution. Theoretical Computer Science, 39, 297
308.
Haken, A. (1995). Counting bottlenecks to show monotone P 6= N P . In Proceedings 36th
Annual IEEE Symp. on Foundations of Computer Science (FOCS-95), pp. 3640,
Milwaukee, MN. IEEE.
Harvey, W. D. (1995). Nonsystematic Backtracking Search. Ph.D. thesis, Stanford University, Stanford, CA.
538

fiSatisfiability and Systematicity

Hertel, P., Bacchus, F., & Pitassi, T. (2008). Clause learning can effectively P-simulate general propositional resolution. In Proceedings of the Twenty-Third National Conference
on Artificial Intelligence, pp. 283290.
Huang, J. (2006). TINISAT in SAT-race 2006..
Huang, J. (2007). The effect of restarts on the efficiency of clause learning. In Proceedings
of the 20th International Joint Conference on Artifical Intelligence, IJCAI07, pp.
23182323, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.
Kaivola, R., Ghughal, R., Narasimhan, N., Telfer, A., Whittemore, J., Pandav, S., Slobodov,
A., Taylor, C., Frolov, V., Reeber, E., & Naik, A. (2009). Replacing testing with formal
verification in Intelr Core i7 processor execution engine validation. In Bouajjani,
A., & Maler, O. (Eds.), Computer Aided Verification, Vol. 5643 of Lecture Notes in
Computer Science, pp. 414429. Springer Berlin Heidelberg.
Kocsis, L., & Szepesvari, C. (2006). Bandit based Monte-Carlo planning. In In: ECML-06.
Number 4212 in LNCS, pp. 282293. Springer.
Krajicek, J. (1997). Interpolation theorems, lower bounds for proof systems, and independence results for bounded arithmetic. J. Symb. Logic, 62 (2), 457486.
Lecoutre, C., Sas, L., Tabary, S., & Vidal, V. (2007). Recording and minimizing nogoods
from restarts. J. on Satisfiability, Boolean Modeling and Computation (JSAT), 1,
147167.
Luby, M., Sinclair, A., & Zuckerman, D. (1993). Optimal speedup of Las Vegas algorithms.
Information Processing Letters, 47, 173180.
Lynce, I., Baptista, L., & Marques-Silva, J. (2001). Stochastic systematic search algorithms
for satisfiability. In In LICS Workshop on Theory and Applications of Satis Testing,
pp. 17.
Marques-Silva, J. P., & Sakallah, K. A. (1999). GRASP  A search algorithm for propositional satisfiability. Computers, 48 (5), 506521.
Marques-Silva, J., Lynce, I., & Malik, S. (2009). Conflict-Driven Clause Learning SAT
Solvers, Vol. 185 of Frontiers in Artificial Intelligence and Applications, pp. 131153.
IOS Press.
McAllester, D. A. (1993). Partial order backtracking. Unpublished.
Moskewicz, M., Madigan, C., Zhao, Y., Zhang, L., & Malik, S. (2001). Chaff: Engineering
an efficient SAT solver. In 39th Design Automation Conference.
Moura, L., & Bjrner, N. (2010). Bugs, moles and skeletons: Symbolic reasoning for software
development. In Giesl, J., & Hahnle, R. (Eds.), Automated Reasoning, Vol. 6173 of
Lecture Notes in Computer Science, pp. 400411. Springer Berlin Heidelberg.
Pipatsrisawat, K., & Darwiche, A. (2007). A lightweight component caching scheme for
satisfiability solvers. In Proceedings of 10th International Conference on Theory and
Applications of Satisfiability Testing (SAT), pp. 294299.
Pipatsrisawat, K., & Darwiche, A. (2011). On the power of clause-learning SAT solvers as
resolution engines. Artif. Intell., 175 (2), 512525.
539

fiGinsberg

Pudlak, P. (1997). Lower bounds for resolution and cutting planes proofs and monotone
computations. J. Symbolic Logic, 62 (3), 981998.
Purdom, P., & Sabry, A. (2005). CNF generator for factoring problems.. cgi.cs.indiana.
edu/~sabry/cnf.html.
Ramos, A., Tak, P., & Heule, M. (2011). Between restarts and backjumps. In Sakallah,
K., & Simon, L. (Eds.), Theory and Applications of Satisfiability Testing - SAT 2011,
Vol. 6695 of Lecture Notes in Computer Science, pp. 216229. Springer.
Reiter, R. (1978). On closed world data bases. In Gallaire, H., & Minker, J. (Eds.), Logic
and Data Bases, pp. 119140. Plenum, New York.
Ryan, L. (2002). Efficient algorithms for clause-learning SAT solvers. Masters thesis, Simon
Fraser University.
Sellmann, M., & Ansotegui, C. (2006). Disco  Novo  GoGo: Integrating local search and
complete search with restarts. In Proceedings of the Twenty-First National Conference
on Artificial Intelligence, pp. 10511056.
Selman, B., Kautz, H. A., & Cohen, B. (1993). Local search strategies for satisfiability testing. In Proceedings 1993 DIMACS Workshop on Maximum Clique, Graph Coloring,
and Satisfiability.
Sorensson, N., & Een, N. (2005). Minisat v1.13 - a SAT solver with conflict-clause minimization. 2005. SAT-2005 poster. Tech. rep..
Stallman, R. M., & Sussman, G. J. (1977). Forward reasoning and dependency-directed
backtracking in a system for computer-aided circuit analysis. Artificial Intelligence,
9, 135196.
Tseitin, G. (1970). On the complexity of derivation in propositional calculus. In Slisenko,
A. (Ed.), Studies in Constructive Mathematics and Mathematical Logic, Part 2, pp.
466483. Consultants Bureau.
Zhang, L. (2003). Searching for Truth: Techniques for Satisfiability of Boolean Formulas.
Ph.D. thesis, Princeton University, Princeton, NJ.
Zhang, L., Madigan, C. F., & Moskewicz, M. H. (2001). Efficient conflict driven learning in
a boolean satisfiability solver. In In ICCAD, pp. 279285.

540

fiJournal of Artificial Intelligence Research 53 (2015) 699-720

Submitted 03/15; published 08/15

Tree-Width and the Computational Complexity
of MAP Approximations in Bayesian Networks
Johan Kwisthout

j.kwisthout@donders.ru.nl

Radboud University Nijmegen
Donders Institute for Brain, Cognition and Behaviour
Montessorilaan 3, 6525 HR Nijmegen, The Netherlands

Abstract
The problem of finding the most probable explanation to a designated set of variables given partial evidence (the MAP problem) is a notoriously intractable problem in
Bayesian networks, both to compute exactly and to approximate. It is known, both from
theoretical considerations and from practical experience, that low tree-width is typically
an essential prerequisite to efficient exact computations in Bayesian networks. In this
paper we investigate whether the same holds for approximating MAP. We define four
notions of approximating MAP (by value, structure, rank, and expectation) and argue
that all of them are intractable in general. We prove that efficient value-approximations,
structure-approximations, and rank-approximations of MAP instances with high tree-width
will violate the Exponential Time Hypothesis. In contrast, we show that MAP can sometimes be efficiently expectation-approximated, even in instances with high tree-width, if
the most probable explanation has a high probability. We introduce the complexity class
FERT, analogous to the class FPT, to capture this notion of fixed-parameter expectationapproximability. We suggest a road-map to future research that yields fixed-parameter
tractable results for expectation-approximate MAP, even in graphs with high tree-width.

1. Introduction
One of the most important computational problems in Bayesian networks is the MAP problem, i.e., the problem of finding the joint value assignment to a designated set of variables
(the MAP variables) with the maximum posterior probability, given partial observation of
the remaining variables. The MAP problem is notably intractable; as it is NPPP -hard, it
is strictly harder (given usual assumptions in computational complexity theory) than the
PP-hard inference problem (Park & Darwiche, 2004). In a sense, it can be seen as combining
an optimization problem with an inference problem, both of which potentially contribute
to the problems complexity (Park & Darwiche, 2004, p. 113). Even when all variables in
the network are binary and the network has the (very restricted) polytree topology, MAP
remains NP-hard (De Campos, 2011). Only when both the optimization and the inference
part of the problem can be computed tractably (for example, if both the tree-width of the
network is small, the cardinality of the variables is low, and the most probable joint value
assignment has a high probability) MAP can be computed tractably (Kwisthout, 2011).
It is known that, for arbitrary probability distributions and under the assumption of the
Exponential Time Hypothesis, low tree-width of the moralized graph of a Bayesian network
is a necessary condition for the Inference problem in Bayesian networks to be tractable

2015 AI Access Foundation. All rights reserved.

fiKwisthout

(Kwisthout, Bodlaender, & van der Gaag, 2010); this result can easily be extended to MAP,
as we will show in Section 4.
MAP is also intractable to approximate (Abdelbar & Hedetniemi, 1998; Kwisthout,
2011, 2013; Park & Darwiche, 2004). While it is obviously the case that a particular instance
to the MAP problem can be approximated efficiently when it can be computed exactly
efficiently, it is as yet unclear whether approximate MAP computations can be rendered
tractable under different conditions than exact MAP computations. Crucial here is the
question what we mean with a statement as algorithm A approximates the MAP problem.
Typically, in computer science, approximation algorithms guarantee that the output of the
algorithm has a value that is within some bound of the value of the optimal solution. For
example, the canonical approximation algorithm to the Vertex Cover problem selects an
edge at random, puts both endpoints in the vertex cover, and removes these nodes from
the instance. This algorithm is guaranteed to get a solution that has at most twice the
number of nodes in the vertex cover as the optimal vertex set. However, typical Bayesian
approximation algorithms have no such guarantee; in contrast, they may converge to the
optimal value given enough time (such as the Metropolis-Hastings algorithm), or they may
find an optimal solution with a high probability of success (such as repeated local search
strategies).
In this paper we assess four different notions of approximation relevant for the MAP
problem; in particular value-approximation, structure-approximation, rank-approximation,
and expectation-approximation of MAP. After introducing notation and providing some
preliminaries (Section 2), we show that each of these approximations is intractable under
the assumption that P 6= NP, respectively NP 6 BPP (Section 3). Building on the result
by Kwisthout et al. (2010) we show in Section 4 that bounded tree-width is indeed a
necessary condition for efficient value-approximation, structure-approximation, and rankapproximation of MAP. In Section 5 we argue that this need not be the case for expectationapproximation. We introduce the parameterized complexity classes FERT (Fixed Error
Randomized Tractable) and FPERT (Fixed Parameter and Error Randomized Tractable)
as natural extensions to the class FPT. We introduce a MAP variant with some additional
constraints and we show that Constrained-MAP is intractable (PP-hard) in general;
however, Constrained-MAP is in FERT when parameterized by the probability of the
most probable explanation, even when tree-width is high. We conclude the paper in Section
6.

2. Preliminaries
In this section, we introduce our notational conventions and provide some preliminaries on
Bayesian networks, graph theory, and complexity theory; in particular definitions of the
MAP problem, tree-width, parameterized complexity theory, and the Exponential Time
Hypothesis. For a more thorough discussion of these concepts, the reader is referred to
textbooks such as those by Darwiche (2009), Arora and Barak (2009), and Downey and
Fellows (1999).
700

fiTree-Width and MAP Approximations

2.1 Bayesian Networks
A Bayesian network B = (GB , Pr) is a graphical structure that succinctly represents a joint
probability distribution over a set of stochastic variables. B includes a directed acyclic graph
GB = (V, A), where V models (in a one-to-one mapping) the stochastic variables and A
models the conditional (in)dependences between them, and a set of parameter probabilities
Pr in the form of conditional probability tables (CPTs), capturing the strengths of the
relationships
Q between the variables. The network models a joint probability distribution
Pr(V) = ni=1 Pr(Vi | (Vi )) over its variables; here, (Vi ) denotes the parents of Vi in GB .
As notational convention we will use upper case letters to denote individual nodes in the
network, upper case bold letters to denote sets of nodes, lower case letters to denote value
assignments to nodes, and lower case bold letters to denote joint value assignments to sets
of nodes. We will use node and variable interchangeably.
One of the key computational problems in Bayesian networks is the problem to find the
most probable explanation for a set of observations, i.e., a joint value assignment to a designated set of variables (the explanation set) that has maximum posterior probability given
the observed variables (the joint value assignment to the evidence set) in the network. If
the network is bi-partitioned into explanation variables and evidence variables this problem
is known as Most Probable Explanation (MPE). The more general problem, where
the network also includes variables that are neither observed nor to be explained (referred
to as intermediate variables) is known as (Partial or Marginal) MAP. This problem is
typically defined formally as follows:
MAP
Instance: A Bayesian network B = (GB , Pr), where V is partitioned into a set of
evidence nodes E with a joint value assignment e, a set of intermediate nodes I, and an
explanation set H.
Output: A joint value assignment h to H such that for all joint value assignments h0 to
H, Pr(h | e)  Pr(h0 | e).
In the remainder, we use the following definitions. For an arbitrary MAP instance
{B, H, E, I, e}, let cansol B refer to the set of candidate solutions to {B, H, E, I, e}, with
optsol B  cansol B denoting the optimal solution (or, in case of a draw, one of the optimal
solutions) to the MAP instance. When cansol B is ordered according to the probability of the
candidate solutions (breaking ties between candidate solutions with the same probability
arbitrarily), then optsol 1...m
refers to the set of the first m elements in cansol B , viz. the m
B
most probable solutions to the MAP instance. For a particular notion of approximation,
we refer to an (unspecified) approximate solution as approxsol B  cansol B .
2.2 Tree-Width
An important structural property of a Bayesian network B is its tree-width, which can be
defined as the minimum width of any tree-decomposition (or equivalently, the minimal size
of the largest clique in any triangulation) of the moralization GM
B of the network. Treewidth plays an important role in the complexity analysis of Bayesian networks, as many
otherwise intractable computational problems can be rendered tractable, provided that the
tree-width of the network is small. The moralization (or moralized graph) GM
B is the
701

fiKwisthout

undirected graph that is obtained from GB by adding arcs so as to connect all pairs of
parents of a variable, and then dropping all directions. A triangulation of GM
B is any
M
chordal graph GT that embeds GB as a subgraph. A chordal graph is a graph that does
not include loops of more than three variables without any pair being adjacent.
A tree-decomposition (Robertson & Seymour, 1986) of a triangulation GT now is a tree
TG such that each node Xi in TG is a bag of nodes which constitute a clique in GT ; and
for every i, j, k, if Xj lies on the path from Xi to Xk in TG , then Xi  Xk  Xj . In the
context of Bayesian networks, this tree-decomposition is often referred to as the junction
tree or clique tree of B. The width of the tree-decomposition TG of the graph GT is defined
as the size of the largest bag in TG minus 1, i.e., maxi (|Xi |  1). The tree-width tw of
a Bayesian network B now is the minimum width over all possible tree-decompositions of
triangulations of GM
B .
2.3 Complexity Theory
We assume that the reader is familiar with basic notions from complexity theory, such
as intractability proofs, the computational complexity classes P and NP, and polynomialtime reductions. In this section we shortly review some additional concepts that we use
throughout the paper, namely the complexity classes PP and BPP, the Exponential Time
Hypothesis and some basic principles from parameterized complexity theory.
The complexity classes PP and BPP are defined as classes of decision problems that are
decidable by a probabilistic Turing machine (i.e., a Turing machine that makes stochastic
state transitions) in polynomial time with a particular (two-sided) probability of error. The
difference between these two classes is in the bound on the error probability. Yes-instances
for problems in PP are accepted with probability 1/2 + , where  may depend exponentially
on the input size (i.e.,  = 1/cn for a constant c > 1). Yes-instances for problems in BPP
are accepted with a probability that is polynomially bounded away from 1/2 (i.e.,  = 1/nc ).
PP-complete problems, such as the problem of determining whether the majority of truth
assignments to a Boolean formula  satisfies , are considered to be intractable; indeed, it
can be shown that NP  PP. In contrast, problems in BPP are considered to be tractable.
Informally, a decision problem  is in BPP if there exists an efficient randomized (Monte
Carlo) algorithm that decides  with high probability of correctness. Given that the error is
polynomially bounded away from 1/2, the probability of answering correctly can be boosted
to be arbitrarily close to 1 while still requiring only polynomial time. While obviously
BPP  PP, the reverse is unlikely; in particular, it is conjectured that BPP = P (Clementi,
Rolim, & Trevisan, 1998).
The Exponential Time Hypothesis (ETH), introduced by Impagliazzo and Paturi (2001),
states that there exists a constant c > 1 such that deciding any 3Sat instance with n
variables takes at least (cn ) time. Note that the ETH is a stronger assumption than
the assumption that P 6= NP. A sub-exponential
but not polynomial-time algorithm for

3
3Sat, such as an algorithm running in O(2 n ), would contradict the ETH but would not
imply that P = NP. We will assume the ETH in our proofs that show the necessity of low
tree-width for efficient approximation of MAP.
Sometimes problems are intractable (i.e., NP-hard) in general, but become tractable
if some parameters of the problem can be assumed to be small. A problem  is called
702

fiTree-Width and MAP Approximations

fixed-parameter tractable for a parameter  (or a set {1 , . . . , m } of parameters) if it can
be solved in time, exponential (or even worse) only in  and polynomial in the input size
|x|, i.e., in time O(f ()  |x|c ) for a constant c > 1 and an arbitrary computable function
f . In practice, this means that problem instances can be solved efficiently, even when
the problem is NP-hard in general, if  is known to be small. In contrast, if a problem
is NP-hard even when  is small, the problem is denoted as para-NP-hard for . The
parameterized complexity class FPT consists of all fixed parameter tractable problems .
While traditionally  is defined as a mapping from problem instances to natural numbers
(e.g., Flum & Grohe, 2006, p. 4), one can easily enhance the theory for rational parameters
(Kwisthout, 2011). In the context of this paper, we will in particular consider rational
parameters in the range [0, 1], and we will liberally mix integer and rational parameters.

3. Approximating MAP
It is widely known, both from practical experiences and from theoretical results, that small
tree-width is often a necessary constraint to render exact Bayesian inferences tractable.
However, it is often assumed that such intractable computations can be efficiently approximated using inexact algorithms; this assumption appears to be warranted by the observation
that in many cases approximation algorithms seem to do a reasonable job in, e.g., estimating posterior distributions, even in networks with high tree-width where exact computations
are infeasible (Cheng & Druzdzel, 2000; Sontag, Meltzer, Globerson, Weiss, & Jaakkola,
2008). Whether this observation has a firm theoretical basis, i.e., whether approximation
algorithms can or cannot in principle perform well even in situations where tree-width can
grow large, is to date not known.
Crucial in answering this question is to make precise what efficiently approximated actually pertains to. The on-line Merriam-Webster dictionary lists as one of its entries for
approximate to be very similar to but not exactly like (something). In computer science,
this similarity is typically defined in terms of value: approximate solution A has a value
that is close to the value of the optimal solution. However, other notions of approximation
can be relevant. One can think of approximating not the value of the optimal solution, but
the appearance: approximate solution A0 closely resembles the optimal solution. Also, one
can define an approximate solution as one that ranks close to the optimal solution: approximate solution A00 ranks within the top-m solutions. Note that these notions can refer
to completely different solutions. One can have situations where the second-best solution
does not resemble the optimal solution at all, whereas solutions that look almost the same
have a very low value as compared to the optimal solution (Van Rooij & Wareham, 2012;
Kwisthout, 2013). Similarly, the second-best solution may either have a value that is almost
as good as the optimal solution, or much worse.
In many practical applications, in particular of Bayesian inferences, these definitions
of approximation do not (fully) capture the actual notion we are interested in. For example, when trying to approximate the MAP explanation using some sort of randomized
computation, we have no guarantee on the quality of the solution found, however, we may
have a bound on the likeliness of a good solution. The current state-of-the-art approximate
algorithms for MAP (AnnealedMAP, Yuan, Lu, & Druzdzel, 2004; P-Loc, Park & Darwiche, 2001; BP-LS, Park & Darwiche, 2004) all employ this strategy. The added notion of
703

fiKwisthout

approximation here, induced by the use of randomized computations, is the allowance of a
bounded amount of error.1
In the remainder of this section we will elaborate on these notions of approximation
when applied to the MAP problem. We will give formal definitions of these approximate
problems and show why all of them are intractable in general. For MAP-approximation
by value and by structure we will interpret known results in the literature. For MAPapproximation by rank we give a formal proof of intractability; for MAP-approximation
using randomized algorithms we give an argument from complexity theory.
3.1 Value-Approximation
Value-approximating MAP is the problem of finding an explanation approxsol B  cansol B
that has a value, close to the value of the optimal solution. This closeness can be defined in
an additive or in a relative manner; additive meaning that the absolute difference between
the probability of the optimal and the approximate solution is smaller than some value ;
relative that the ratio between the probability of the optimal and the approximate solution
is smaller than some value . Both problems are intractable in general. Abdelbar and
Hedetniemi (1998) proved NP-hardness of relative value-approximation for any constant
  1. This result holds for networks with only binary variables, with at most three incoming
arcs per variable, and no evidence. In addition, Kwisthout (2011) showed that it is NP-hard
in general to find an explanation approxsol B with Pr(approxsol B , e) >  for any constant
 > 0, and thus that Pr(optsol B , e)  Pr(approxsol B , e)   for  > Pr(optsol B , e)  .
The latter result holds even for networks with only binary variables, at most two incoming
arcs per variable, a single evidence variable, and no intermediate variables (i.e., when we
approximate an MPE problem).
Definition 3.1 (additive value-approximation of MAP) Let optsol B be the optimal
solution to a MAP problem. An explanation approxsol B  cansol B is defined to -additive
value-approximate optsol B if Pr(optsol B , e)  Pr(approxsol B , e)  .
Result 3.2 (Kwisthout, 2011) It is NP-hard to -additive value-approximate MAP for
 > Pr(optsol B , e)   for any constant  > 0.
Definition 3.3 (relative value-approximation of MAP) Let optsol B be the optimal
solution to a MAP problem. An explanation approxsol B  cansol B is defined to -relative
Pr(optsol B | e)
value-approximate optsol B if Pr(approxsol
| e)  .
B

Result 3.4 (Abdelbar & Hedetniemi, 1998) It is NP-hard to -relative value-approximate
Pr(optsol B | e)
MAP for Pr(approxsol
| e)   for any  > 1.
B

1. Observe that some algorithms always converge to the optimal solution, but may take exponential time
to do so (e.g., MCMC-type approaches). However, we can turn such an algorithm into an expectationapproximation algorithm by adding a clock that halts computations after time, polynomially in the input
size, and returning the current best solution which may or may not be optimal (Gill, 1977).

704

fiTree-Width and MAP Approximations

3.2 Structure-Approximation
Structure-approximating MAP is the problem of finding an explanation approxsol B 
cansol B that structurally resembles the optimal solution. This is captured using a solution
distance function, a metric associated with each optimization problem relating candidate
solutions with the optimal solution (Hamilton, Muller, van Rooij, & Wareham, 2007). For
MAP, the typical structure distance function dH (approxsol B , optsol B ) is the Hamming distance between explanation approxsol B and the most probable explanation optsol B . It has
been shown by Kwisthout (2013) that no algorithm can calculate the value of even a single
variable in the most probable explanation in polynomial time, unless P = NP; that is, it
is NP-hard to find an explanation with dH (approxsol B , optsol B )  |optsol B |  1, even if
the variables of the network are bi-partitioned into explanation and evidence variables, and
each variable has at most three possible values.
Definition 3.5 (structure-approximation of MAP) Let optsol B be the optimal solution to a MAP problem and let dH be the Hamming distance. An explanation approxsol B 
cansol B is defined to d-structure-approximate optsol B if dH (approxsol B , optsol B )  d.
Result 3.6 (Kwisthout, 2013) It is NP-hard to d-structure-approximate MAP for any
d  |optsol B |  1.
3.3 Rank-Approximation
Apart from allowing an explanation that resembles, or has a probability close to, the most
probable explanation, we can also define an approximate solution approxsol B as an explanation which is one of the m best explanations, for a constant m, that is, approxsol B 
optsol 1...m
for some m. Note that this explanation may not resemble the most probable
B
explanation nor needs to have a relatively high probability, only that it is ranked within the
m most probable explanations. We will denote this approximation as a rank-approximation.
Definition 3.7 (rank-approximation of MAP) Let optsol 1...m
 cansol B be the set of
B
the m most probable solutions to a MAP problem and let optsol B be the optimal solution. An
explanation approxsol B  cansol B is defined to m-rank-approximate optsol B if approxsol B 
optsol 1...m
.
B
We will prove that it is NP-hard to m-rank-approximate MAP for any constant m. We do so
by a reduction from a variant of LexSat, based on the reduction in Kwisthout, Bodlaender,
and van der Gaag (2011). LexSat is defined as follows:
LexSAT
Instance: A Boolean formula  with n variables X1 , . . . , Xn .
Output: The lexicographically largest truth assignment x to X = {X1 , . . . , Xn } that
satisfies ; the output is  if  is not satisfiable.
Here, the lexicographical order of truth assignments maps a truth assignment x = x1 , . . . , xn
to a string {0, 1}n , with {0}n (all variables set to false) is the lexicographically smallest, and
{1}n (all variables set to true) is the lexicographically largest truth assignment. LexSat is
NP-hard; in particular, LexSat has been proven to be complete for the class FPNP (Krentel,
705

fiKwisthout

V









X0

X1





X2

X3

X4

X
Figure 1: Example construction of Bex from LexSat0 instance ex
1988). In our proofs we will use the following variant that always returns a truth assignment
(rather than , in case  is unsatisfiable):
LexSAT0
Instance: A Boolean formula  with n variables X1 , . . . , Xn .
Output: The lexicographically largest satisfying truth assignment x to  = (X0 )  
that satisfies .
Note that if  is satisfiable, then X0 is never set to false in the lexicographically largest
satisfying truth assignment to , yet X0 is necessarily set to false if  is not satisfiable;
hence, unsatisfying truth assignments to  are always ordered after satisfying truth assignments in the lexicographical ordering. Note that LexSat trivially reduces to LexSat0 using
a simple transformation. We claim the following.
Theorem 3.8 No algorithm can find an approximation approxsol B  optsol 1...m
, for any
B
constant m, in polynomial time, unless P = NP.
In our proof we describe a polynomial-time one-Turing reduction2 from LexSat0 to mrank-approximated-MAP for an arbitrary constant m. The reduction largely follows the
reduction as presented by Kwisthout et al. (2011) with some additions. We will take the
following LexSat0 -instance as running example in the proof: ex = X1  (X2  X3 );
correspondingly, ex = (X0 )  (X1  (X2  X3 )) in this example. We set m = 3 in the
example construct. We now construct a Bayesian network B from  as follows (Figure 1).
For each variable Xi in , we introduce a binary root variable Xi in B with possible
values true and false. We set the prior probability distribution of these variables to
i+1 1
Pr(Xi = true) = 1/2  22n+2
. In addition, we include a uniformly distributed variable
Xn+1 in B with m values x1n+1 , . . . , xm
n+1 . The variables X0 , . . . , Xn together form the set
X. Note that the prior probability of a joint value assignment x to X is higher than the prior
probability of a different joint value assignment x0 to X, if and only if the corresponding
2. A a function problem f is polynomial-time one-Turing reducible to a function problem g if there exist
polynomial-time computable functions T1 and T2 such that for every x,f (x) = Tl (x, g(T2 (x))) (Toda,
1994). One-Turing reductions can be seen as equivalent to many-one reductions, but then applied to
function problems.

706

fiTree-Width and MAP Approximations

truth assignment x to the LexSat0 instance has a lexicographically larger truth assignment
than x0 . In the running example, we have that Pr(X0 = true) = 15/32, Pr(X1 = true) =
13/32, Pr(X2 = true) = 9/32, and Pr(X3 = true) = 1/32, and Pr(X4 = x1 ) = Pr(X4 =
4
x24 ) = Pr(X4 = x34 ) = 1/3. Observe that we have that Pr(X0 )  . . .  Pr(Xi1 )  Pr(Xi ) >
Pr(X0 )      Pr(Xi1 )  Pr(Xi ) for every i, i.e., the ordering property such as stated above
is attained.
For each logical operator T in , we introduce an additional binary variable in B
with possible values true and false, and with as parents the sub-formulas (or single subformula, in case of a negation operator) that are bound by the operator. The conditional
probability distribution of that variable matches the truth table of the operator, i.e., Pr(T =
true | (T )) = 1 if and only if the operator evaluates to true for that particular truth
value of the sub-formulas bound by T . The top-level operator is denoted by V . It is readily
seen that Pr(V = true | x) = 1 if and only if the truth assignment to the variables in
 that matches x satisfies , and Pr(V = true | x) = 0 otherwise. Observe that the
m-valued variable Xn+1 is independent of every other variable in B . Further note that the
network, including all prior and conditional probabilities, can be described using a number
of bits which is polynomial in the size of . In the MAP instance constructed from , we set
V as evidence set with V = true as observation and we set X  {Xn+1 } as explanation
set.
Proof. Let  be an instance of LexSat0 , and let B be the network constructed from 
as described above. We have for any joint value assignment x to X that Pr(X = x | V =
true) =   Pr(X = x) for a normalization constant  > 0 if x corresponds to a satisfying
truth assignment to , and Pr(X = x | V = true) = 0 if x corresponds to a non-satisfying
truth assignment to . Given the prior probability distribution of the variables in X, we
have that all satisfying joint assignments x to X are ordered by the posterior probability
Pr(x | V = true) > 0, where all non-satisfying joint value assignments have probability
Pr(x | V = true) = 0 and thus are ordered after satisfying assignments. The joint value
assignment that has the highest posterior probability thus is the lexicographically largest
satisfying truth assignment to .
If we take the m-th valued variable Xn+1 into account, we have that for every x, the m
joint value assignments to X  {Xn+1 } have the same probability since Pr(x, Xn+1 | V =
true) = Pr(x | V = true)  Pr(Xn+1 ). But then, the m joint value assignments xm to
X  {Xn+1 } that correspond to the lexicographically largest satisfying truth assignment x
to  all have the same posterior probability Pr(xm | V = true). Thus, any algorithm that
returns one of the m-th ranked joint value assignments to the explanation set X  {Xn+1 }
with evidence V = true can be transformed in polynomial time to an algorithm that
solves LexSat0 . We conclude that no algorithm can m-rank-approximate MAP, for any
constant m, in polynomial time, unless P = NP.

Note that, technically speaking, our result is even stronger: as LexSat0 is FPNP complete and the reduction described above actually is a one-Turing reduction from LexSat0
to m-rank-approximation-MAP, the latter problem is FPNP -hard. We can strengthen the
result further by observing that all variables (minus V ) that mimic operators deterministically depend on their parents and thus can be added to the explanation set without
substantially changing the proof above. This implies that m-rank-approximation-MPE is
also FPNP -hard. Lastly, we can strengthen the result by replacing the m-th valued variable
707

fiKwisthout

Xn+1 by dlog2 me unconnected binary variables Xn+1 to Xn+dlog2 me with uniform probability. Still, any algorithm returning one of the m-th ranked joint value assignments to
X{Xn+1 , . . . , Xn+dlog2 me } in polynomial time will effectively solve LexSat0 in polynomial
time.
Result 3.9 It is NP-hard to m-rank-approximate MAP for any constant m.
3.4 Expectation-Approximation
The last notion of MAP approximation we will discuss here returns in polynomial time
an explanation approxsol B  cansol B that is likely to be the most probable explanation,
but allows for a small margin of error; i.e., there is a small probability that the answer is
not the optimal solution, and then no guarantees are given on the quality of that solution.
These approximations are closely related to randomized algorithms that run in polynomial
time but whose output has a small probability of error, viz., Monte Carlo algorithms. This
notion of approximationwhich we will refer to as expectation-approximation (Kwisthout &
van Rooij, 2013)is particularly relevant for typical Bayesian approximation methods, such
as Monte Carlo sampling and repeated local search algorithms.
Definition 3.10 (expectation-approximation of MAP) Let optsol B be the optimal solution to a MAP problem and let E be the the expectation function (Papoulis, 1984).
An explanation approxsol B  cansol B is defined to -expectation-approximate optsol B if
E(Pr(optsol B ) 6= Pr(approxsol B )) < .
In order to be of practical relevance, we want the error to be small, i.e., when casted
as a decision problem, we want the probability of answering correctly to be bounded away
from 1/2. In that case, we can amplify the probability of answering correctly arbitrarily
close to 1 in polynomial time, by repeated evocation of the algorithm. Otherwise, e.g., if
the error depends exponentially on the size of the input, we need an exponential number
of repetitions to achieve such a result. Problems that enjoy polynomial-time Monte Carlo
algorithms are in the complexity class BPP; problems that may need exponential time to
reduce the probability of error arbitrarily close to 0 are in the complexity class PP.
As MAP is NP-hard, an efficient randomized algorithm solving MAP in polynomial time
with a bounded probability of error, would imply that NP  BPP. This is considered to be
highly unlikely, as almost every problem that enjoys an efficient randomized algorithm has
been proven to be in P, i.e., be decidable in deterministic polynomial time.3 On various
grounds it is believed that BPP = P (Clementi et al., 1998), and thus an efficient randomized algorithm for MAP would (under that assumption) establish P = NP. Therefore, no
algorithm can expectation-approximate MAP in polynomial time with bounded margin of
error unless NP  BPP. This result holds also for MPE, which is in itself already NP-hard,
even for binary variables and in-degree 2 (Kwisthout, 2011).4
3. The most dramatic example of such a problem is PRIMES: given a natural number, decide whether
it is prime. While efficient randomized algorithms for PRIMES have been around quite some time
(establishing that PRIMES  BPP), only fairly recently it has been proven that PRIMES is in P (Agrawal,
Kayal, & Saxena, 2004).
4. In fact, it holds for value-approximation, structure-approximation, and rank-approximation of MAP as
well, as all three problems are in themselves NP-hard (see also Abdelbar & Hedetniemi, 1998, p. 35).

708

fiTree-Width and MAP Approximations

Result 3.11 There cannot exist a randomized algorithm that -expectation-approximates
MAP in polynomial time for  < 1/2  1/nc for a constant c unless NP  BPP.
3.5 Discussion
In the previous subsections we showed that all approximation notions we established are in
fact intractable, under various assumptions. The results hold for MAP in general, but can
in many cases be strengthened to hold for MPE (i.e., where the network is two-partitioned
into evidence and explanation variables); in either case, both the cardinality c and in-degree
d of the nodes (and consequently, the size of the CPTs) is bounded. The results hold with
empty (or singleton) evidence sets. The results are summarized in Table 1.
Approximation
value, additive
value, ratio
structure
rank
expectation

constraints
c = 2, d = 2,
|E| = 1, I = 
c = 2, d = 3,
E=
c = 3, d = 3,
I=
c = 2, d = 2,
|E| = 1, I = 
c = 2, d = 2,
|E| = 1, I = 

assumption
P 6= NP

reference
(Kwisthout, 2011, p. 1462)

P 6= NP

(Abdelbar & Hedetniemi, 1998, p. 24)

P 6= NP

(Kwisthout, 2013, p. 345)

P 6= NP

Section 3.3

NP 6 BPP

Section 3.4

Table 1: Summary of intractability results for MAP approximations

4. The Necessity of Low Tree-Width for Efficient Approximation of MAP
In the previous section we have shown that for four notions of approximating MAP, no
efficient general approximation algorithm can be constructed unless either P = NP or NP 
BPP. However, MAP is fixed-parameter tractable for a number of problem parameters;
for example, {tw, c, q}MAP is in FPT for parameters tree-width (tw), cardinality of the
variables (c = maxi |(Vi  V)|), and probability of the most probable solution (q =
Pr(optsol B , e)). Surely, if we can compute {1 , . . . , m }MAP exactly in FPT time, we can
also approximate {1 , . . . , m }MAP in FPT time. A question remains, however, whether
approximate MAP can be fixed-parameter tractable for a different set of parameters than
exact MAP.
Tree-width has been shown to be a necessary parameter for efficient exact computation of the Inference problem (and, by a trivial adjustment illustrated in Section 4.3,
also of MAP), under the assumption that the ETH holds (Kwisthout et al., 2010). In
this section, we will show that low tree-width is also a necessary parameter for efficient
approximate computation for value-approximations, structure-approximations, and rankapproximations. We also argue (in Section 5) that it is not a necessary parameter for
efficient expectation-approximation. In the next sub-section we will review so-called treewidth-preserving reductions (tw-reductions), a special kind of polynomial many-one reductions that preserve tree-width of the instances (Kwisthout et al., 2010). In Sub-section
709

fiKwisthout

4.2 we sketch how this notion can be used to tw-reduce Constraint Satisfaction to
Inference. Together with the known result that Constraint Satisfaction instances
with high tree-width cannot have sub-exponential algorithms, unless the ETH fails (Marx,
2007), it was established by Kwisthout et al. that there cannot be a (general-purpose) algorithm that decides Inference on instances with high tree-width in sub-exponential time,
unless the ETH fails. Here, the Inference problem is the problem of deciding whether in
a Bayesian network B with designated sets H and E and a rational number q, it is the case
that Pr(H = h | E = e) > q. More precisely, the following theorem was proved:
Theorem 4.1 If there exists a computable function f such that Inference can be decided
by an algorithm running in time
f (GM
B )

o(

 kBk

tw(GM
B ) )
log tw(GM )
B

for arbitrary Inference instances (B, H, h, E, e, q) with a moralized graph GM
B with treeM
width tw(GB ), then the ETH fails.
The reader is referred to Kwisthout et al. (2010) for the full proof.5 In the remainder of
this section, we will show how this proof can be augmented to establish similar results for
MAP, value-approximate MAP, structure-approximate MAP, and rank-approximate MAP
(Sub-sections 4.3 and 4.4).
4.1 Tree-Width-Preserving Reductions
Tree-width-preserving reductions are defined by Kwisthout et al. (2010) as a means to reduce
Constraint Satisfaction to Inference while ensuring that tree-width is preserved
between instances in the reduction, modulo a linear factor.
Definition 4.2 (Kwisthout et al., 2010) Let A and B be computational problems such
that tree-width is defined on instances of both A and B. We say that A is polynomialtime tree-width-preserving reducible, or tw-reducible, to B if there exists a polynomial-time
computable function g and a linear function l such that x  A if and only if g(x)  B and
tw(g(x)) = l(tw(x)). The pair (g, l) is called a tw-reduction.
We will use this notion to show that Constraint Satisfaction also tw-reduces to MAP,
value-approximate MAP, structure-approximate MAP, and rank-approximate MAP.
4.2 Proof Sketch
The tw-reduction from (binary) Constraint Satisfaction to Inference, as presented by
Kwisthout et al. (2010), constructs a Bayesian network BI from an instance I = (V, D, C)
of Constraint Satisfaction, where V denotes the set of variables of I, D denotes the set
of values of these variables, and C denotes the set of binary constraints defined over V  V.
5. The results by Kwisthout et al. (2010) did not rule out the existence of special-case algorithms, that
assume (and utilize) a particular property of the instance, such as a particular orientation of the arcs
or particular planarity properties of the graph structure, failing when the assumption is violated. The
results in the current paper, that are built on this result, inherit this constraint.

710

fiTree-Width and MAP Approximations

R1

R4
X1

X2

X4
X3
R2

R3

Figure 2: Example construction of BI from example CSP instance I
The constructed network BI includes uniformly distributed variables Xi , corresponding
with the variables in V, and binary variables Rj , corresponding with the constraints in C.
The parents of the variables Rj are the variables Xi that are bound by the constraints;
their conditional probability distributions match the imposed constraints on the variables
(i.e., Pr(Rj = true | x  ((Rj ))) = 1 if and only if the joint value assignment x to
the variables bound by Rj matches the constraints imposed on them by Rj . Figure 2,
taken from Kwisthout et al., shows the result of the construction so far for an example
Constraint Satisfaction instance with four variables X1 to X4 , where C contains four
constraints that bind respectively (X1 , X2 ), (X1 , X4 ), (X2 , X3 ), and (X3 , X4 ).
The tree-width of the thus obtained network equals max(2, tw(GI )), where GI is the
primal graph of I; note that the tree-width of BI at most increases the tree-width of GI by
1. In order to enforce that all constraints are simultaneously enforced, the constraint nodes
Rj need to be connected by extra nodes mimicking and operators. A crucial aspect of
the tw-reduction is the topography of this connection of the nodes Rj : care must be taken
not to blow up tree-width by arbitrarily connecting the nodes, e.g., by a log-deep binary
tree. The original proof uses a minimal tree-decomposition of the moralization of BI and
describes a procedure to select which nodes need to be connected such that the tree-width
of the resulting graph is at most the tree-width of GI plus 3. The conditional probability
distribution of the nodes Ak is defined as follows.
V

1 if x = V (Ak ) (V = true)
Pr(Ak = true | x) =
0 otherwise
For a node Ak without any parents, Pr(Ak = true) = 1. The graph that results from
applying this procedure to the example is given in Figure 3 (also taken from Kwisthout
et al., 2010). Now, Pr(A1 = true | x) = 1 if x corresponds to a satisfying value assignment
to V and 0 otherwise; correspondingly, Pr(A1 = true) > 0 if and only if the Constraint
Satisfaction instance is satisfiable.
4.3 MAP Result
The tw-reduction described in the previous sub-section can be easily modified to a twreduction from Constraint Satisfaction to MAP. We do this by adding a binary node
711

fiKwisthout

R1

R4
X1

X2

X4
X3

A1

R2

R3

A2

A3

A4

A5

A6

Figure 3: Resulting graph BI after adding nodes Ak and appropriate arcs
VI to the thus obtained graph, with A1 as its only parent and with conditional probability
Pr(VI = true | A1 = true) = 1 and Pr(VI = true | A1 = false) = 1/2  , where 
is a number, smaller than 1/|D||V| . Consequently, we have that Pr(VI = true) > 1/2 if I
is satisfiable, and Pr(VI = true) < 1/2 if I is not satisfiable; hence, a MAP query with
explanation set H = {VI } will return VI = true if and only if I is satisfiable. We added a
single node to BI , with A1 as only parent, thus increasing the tree-width of BI by at most
1. Hence, Constraint Satisfaction tw-reduces to MAP.
4.4 Approximation Intractability Results
In a similar way we can modify the reduction from Sub-section 4.2 to show that valueapproximations, structure-approximations, and rank-approximations can be tw-reduced
from Constraint Satisfaction, as sketched below.
4.4.1 Value-Approximation
We add a binary node VI , with A1 as its only parent, and with conditional probability
Pr(VI = true | A1 = true) = 1 and Pr(VI = true | A1 = false) = 0. We observe
this variable to be set to true. This enforces that Pr(A1 = true | VI = true) has
a non-zero probability (i.e., I is solvable) since otherwise there is conflicting evidence in
the thus constructed network. Thus, any value-approximation algorithm with explanation
set H = {A1 } and evidence e = {VI = true} that can return a solution approxsol B 
cansol B with Pr(approxsol B , e) >  for any constant  > 0, (that is, approximates additively
B)
) effectively solves Constraint
for  > Pr(optsol B )   or relatively for  > Pr(optsol

Satisfaction: if there exists a solution with non-zero probability, the construction dictates
that I must be solvable. Given that we added a single node to BI , with A1 as only parent,
this increases the tree-width of BI by at most 1. Hence, Constraint Satisfaction twreduces to value-approximate MAP.
712

fiTree-Width and MAP Approximations

4.4.2 Structure-Approximation
Observe from the tw-reduction to MAP in Sub-section 4.3 that, since H consists of a
singleton binary variable, we trivially have that no algorithm can find an explanation
approxsol B  cansol B with dH (approxsol B , optsol B )  |optsol B |  1 = 0 since that would
solve the MAP query. We can extend this result to hold for explanation sets with size
k for any constant k, i.e., no structure-approximation algorithm can guarantee to return
the correct value of one of the k variables in H in polynomial time in instances of high
tree-width, unless the ETH fails.
Instead of adding a single binary node VI as in the tw-reduction to MAP, we add k
binary nodes VI1 . . . VIk , all with A1 as their only parent and with Pr(VIj = true | A1 =
true) = 1 and Pr(VIj = true | A1 = false) = 1/2   for 1  j  k and with  as
S
described in Sub-section 4.3. A MAP query with explanation set H = 1jk VIj will
then return 1jk VIk = true if and only if I is satisfiable; if I is not satisfiable, a MAP
query will return 1jk VIk = false as most probable explanation. Hence, any structureapproximation algorithm that can correctly return the value of one of the variables in H,
effectively solves Constraint Satisfaction. As we added k nodes to BI , with A1 as
their only parent and no outgoing arcs, the tree-width of BI increases by at most 1. Hence,
Constraint Satisfaction tw-reduces to structure-approximate MAP.
4.4.3 Rank-Approximation
We modify the proof of Sub-section 4.3 as follows. In addition to adding a binary node
VI as specified in that section, we also add dlog2 me unconnected binary variables MI =
dlog me
{MI1 . . . MI 2 } with uniform probability to H; an m-rank-approximate MAP query with
explanation set H = {VI }  MI will return VI = true (and MI set to an arbitrary value)
if and only if I is satisfiable. The addition of MI does not increase tree-width, hence,
Constraint Satisfaction tw-reduces to m-rank-approximate MAP.
4.5 Discussion
For efficient exact computation, value-approximation, structure-approximation, and rankapproximation of MAP we showed that bounded tree-width is a necessary condition, under the assumption of the ETH, for any general-purpose algorithm that accepts arbitrary
instances. This does not rule out the possibility that there may exist special-purpose algorithms that can compute or approximate MAP explanations of specific networks with
a special structure or distribution (as was already concluded by Kwisthout et al. (2010)
for the Inference problem in Bayesian networks). However, as the previous sub-section
shows, the approximation problems are intractable even for extreme lower bounds on the
approximation quality, and by the very nature of the reductions it follows that if we can
effectively approximate MAP explanations by value, structure, or rank, we can decide the
problem exactly as well. This leaves little room for efficient approximation algorithms for
MAP instances with high tree-width when we approximate by value, structure, or rank.
713

fiKwisthout

5. Expectation-Approximation and the Classes FERT and FPERT
In the previous section we showed that we cannot value-approximate, structure-approximate,
or rank-approximate MAP on instances with high tree-width, unless the ETH fails. Now
what about expectation-approximation? It appears that the strategy that was employed in
the previous subsection cannot be used to show a similar result for expectation-approximation. In fact, we have reasons to believe that efficient expectation-approximation of MAP
indeed depends on a different set of parameters than the other notions of approximation
discussed above, and that bounded tree-width is not necessary for this particular notion
of approximation. This notion of parameterized approximability is not well captured by
the traditional fixed parameter tractable class FPT; therefore, we will introduce the parameterized complexity classes FERT (Fixed Error Randomized Tractable) and FPERT
(Fixed Parameter and Error Randomized Tractable) that characterize this notion of efficient parameterized expectation-approximation. Intuitively, in contrast to the class FPT,
that parameterizes running time, these classes parameterize the error probability (FERT),
respectively both the running time and the error probability (FPERT).
To the best of our knowledge, there is no previous work that proposes to parameterize the probability of acceptance of a probabilistic Turing machine. Montoya and Muller
(2013) define the class BPFPT that assumes a bounded error which is independent of the
parameterization , but where the amount of randomness (operationalized by the number
of coins used) is bounded. Arvind and Raman (2002) propose randomized approximation
algorithms for counting problems, where both the running time and the approximation ratio
are parameterized, but the error probability is constant. Both authors, however, assume a
bounded (rather than parameterized) error.
In the next section we will set up the formal machinery for our results. We introduce natural parameterizations of MajSAT, respectively E-MajSAT, that are in FERT,
respectively FPERT. We will show that a restricted variant of MAP indeed is in FERT, parameterized by the probability of the most probable explanation, and that the Most Frugal
Explanations problem (Kwisthout, 2015) is in FPERT for a number of parameterizations.
We will elaborate on the relation between these classes and the classes BPP, PP, and FPT,
and finally, we will propose a road map for future research.
5.1 Parameterizing the Error Bound in Randomized Algorithms
We formally define the complexity class FERT as follows:
Definition 5.1 Let  be a decision problem and let  be a parameterization of . We
have that   FERT if and only if there exists a probabilistic Turing machine M that
halts after time, polynomial in the size of the input x, with the following acceptance criteria.
M accepts Yes-instances of  with probability 1/2 + min(f (), 1/|x|c ) for a constant c and
arbitrary function f : R  h0, 1/2]; No-instances are accepted with probability at most 1/2.
Observe that in this definition we demand that M halts after time, polynomial in the input
size (and independent from the parameterization ), yet that the probability of acceptance
of Yes-instances may depend on any function of . Intuitively, the class FERT characterizes
problems that can be efficiently computed with a randomized algorithm (i.e., in polynomial
time, with error arbitrarily close to 0) if  is bounded. The canonical parameterized problem
714

fiTree-Width and MAP Approximations

that is in FERT is {r}MajSAT, where r denotes the fraction of satisfying truth assignments (or, equivalently, the probability that a random truth assignment accepts). This
follows as a corollary from the following result by Littman, Majercik, and Pitassi (2001):
Lemma 5.2 (adapted from Littman et al., 2001) Let v be the number of accepting
truth assignments to a Boolean formula , and let v be the estimate of v found via random
sampling using w samples over the variables of . Let  > 0 be a target approximation
2
error. The probability that |v  v| >  is less than 2e2 w .
Note that when solving a MajSAT-instance, the target approximation error  directly
depends on the probability r that a random truth assignment accepts, as the acceptable error
(i.e., the error that still gives the correct answer to the MajSAT instance) in the random
sampling algorithm is  = |r  1/2|. So, if the probability of acceptance of a random truth
assignment is polynomially bounded away from 1/2, we can guarantee an arbitrarily small
error using only polynomially many samples using a straightforward randomized algorithm.
Corollary 5.3 {r}MajSAT  FERT.
When we allow both parameterization of the running time and of the probability of acceptance, we get the complexity class FPERT defined as follows:
Definition 5.4 Let  be a decision problem and let {1 , 2 } be a parameterization of
. We have that {1 , 2 }  FPERT if and only if there exists a probabilistic Turing
machine M that halts after time O(f1 (1 )  |x|c1 ), that accepts Yes-instances of  with
probability 1/2 + min(f2 (2 ), 1/|x|c2 ), and accepts No-instances with probability at most 1/2.
Here, f1 : R  R and f2 : R  h0, 1/2] are arbitrary computable functions and c1 and c2 are
constants.
We can also define a canonical problem in FPERT, based on the observation that {p}-SAT
is in FPT (Flum & Grohe, 2006, here parameter p denotes the number of variables in the
formula) and on Corollary 5.3:
E-MajSAT
Instance: Let  be a Boolean formula with n variables xi , i = 1, . . . , n, n  1,
furthermore we partition the variables into sets XE and XM .
Question: Is there a truth assignment to XE such that the majority of the truth
assignments to XM satisfy ?
Parameter: 1 = p; 2 = r; here p is the number of variables in the set XE ; we define r
as follows. Let rxE denote the ratio of accepting truth assignments to XM given a
particular truth assignment xE to XE . We then define r = minxE (|1/2  rxE |).
Informally, r describes the minimum absolute distance to 1/2 of the fraction of accepting
truth assignments for any truth assignment xE to XE . Observe that we can try (bruteforce) all truth assignments to XE and, for each truth assignment, expectation-approximate
whether that truth assignment is such that the majority of truth assignments to XM satisfy
. This algorithm runs in time O(2p  nc ) for a constant c, and has a probability at least
1/2 + f (r) of answering correctly (using a polynomial number of samples).
Corollary 5.5 {p, r}E-MajSAT  FPERT.
715

fiKwisthout

5.2 Parameterized Expectation-Approximation of MAP
Proving that a problem  is in FPT is normally done constructively, i.e., by giving a
deterministic algorithm that decides  in time O(f ()  |x|c ) for a constant c > 1. Similarly,
proving that  is in FERT is done by giving a randomized algorithm6 that decides  in
polynomial time with error at most 1/2  min(f (), 1/|x|c ). We did not succeed in giving such
an algorithm for MAP in general, however, we can prove that a restricted variant of MAP
is in FERT when parameterized only by the probability of the most probable explanation,
despite that this restricted variant remains PP-complete in general and that bounded treewidth is a necessary parameter to approximate this problem by value, structure, or rank:
ConstrainedMAP
Instance: As in MAP. In addition, we demand that E = , H consists of a singleton
node H with no outgoing edges, and (H) = {true, false}.
Question: Is Pr(H = true) > 1/2?
Parameter: q = Pr(H = true).
PP-completeness of Constrained-MAP follows from a trivial modification of the PPcompleteness proof of Inference as described by Kwisthout (2009, Lemma 2.7 and Lemma
2.9). Furthermore, given that the reductions from Constraint Satisfaction to MAP,
value-approximate MAP, structure-approximate MAP, and rank-approximate MAP respect
the same restrictions as imposed to Constrained-MAP, the necessity of bounded treewidth follows.
To show that {q}Constrained-MAP is in FERT, for the parameter q = Pr(H =
true), we give the following approximation algorithm. Observe that H is a binary sink
node (i.e., has no outgoing edges) and that B has no evidence. A simple forward sampling
strategy (Henrion, 1986) can approximate the distribution of H by sampling values for
the variables in the network according to the probability distribution in the CPTs. We
thus estimate Pr(H) by taking samples; we decide upon approxsol B using this estimation.
Note that the degree of error given a particular number of samples depends directly on the
probability q. To be precise, using the Chernoff bound we can compute than the number

of samples N needed to have a degree of error lower than  is 1/(q  1/2)2 ln 1/ . This gives
us a fixed-parameter randomized tractable algorithm for parameter {q}.
Corollary 5.6 {q}Constrained-MAP  FERT.
Another parameterized problem that can be shown to be fixed-error, fixed parameter
randomized tractable is the Most Frugal Explanations heuristic approach to MAP, introduced by Kwisthout (2015). This heuristic (that either marginalizes or samples over intermediate variables, based on some subjective partition of these intermediate variables (into
a set I+ and a set I ) according to their expected contribution to deciding upon the MAP
explanation) can be expectation-approximated tractably when the tree-width of the network is low, the cardinality of the variables is small, the set I+ is small, and the probability
distribution is such that a few samples over I suffice to decide upon the MFE explanation
with high probability. The first three parameters ensure bounded running time, whereas
6. We will refer to such an algorithm as a fixed-error randomized tractable algorithm.

716

fiTree-Width and MAP Approximations

para-NPPP
FPERT

para-PP

FERT

para-NP

BPP

FPT
P

Figure 4: Inclusion properties for the complexity classes P, BPP, FERT, FPERT, FPT,
para-NP, para-PP, and para-NPPP

the final parameter ensures bounded probability of error. Hence, {tw, c, |I+ |}  {b}MFE
is in FPERT, where tw denotes the tree-width of the network, c denotes the cardinality of
the variables, |I+ | denotes the number of variables we marginalize (not sample) over, and b
denotes some parameter describing a bias towards a particular explanation. In addition it
can be shown that {|H|, c, |I+ |}  {b}MFE is in FPERT, where |H| denotes the size of the
explanation set.
Corollary 5.7 {tw, c, |I+ |}  {b}MFE  FPERT and {|H|, c, |I+ |}  {b}MFE  FPERT.
5.3 The Relation Between FERT, FPERT, and Other Complexity Classes
The complexity class FERT introduced here is the randomized analog of FPT. Rather than
parameterizing the running time (as an arbitrary function of  and polynomially in the
input size), here we parameterize the probability of acceptance of Yes-instances. BPP, FERT,
and para-PP form natural analogs of P, FPT, and para-NP, respectively. The class FPERT
parameterizes both the running time and the probability of acceptance, using two parameter
sets 1 and 2 . We thus have that BPP  FERT  PP, that FERT  FPERT, and that
FPT  FPERT. Obviously, FPERT  para-NPPP , as every slice {1 , 2 }1  NPPP (see
Flum & Grohe, 2006, for a discussion of slices of parameterized problems). The inclusion
relations are depicted in Figure 4.
It is conjectured that BPP = P (Clementi et al., 1998); however, it is not clear whether
this conjecture can be transposed to the parameterized world; that is, whether it can be
conjectured that FERT = FPT. It is known that {q, |I|}MAP and {q, tw}MAP are fixed
parameter tractable (Kwisthout, 2011); as Constrained-MAP is a special case of MAP,
these results also hold for Constrained-MAP. However, in the intractability proof above
neither |I| nor tw is bounded. To the best of our knowledge there is no parameterized
complexity result known (in either direction) for {q}Constrained-MAP.
717

fiKwisthout

5.4 Efficient MAP Approximation: A Road-map for Future Research
We have established that a particular, constrained version of MAP can be efficiently approximated under expectation-approximations if the probability of the MAP explanation
is high (where the tree-width of the instance may be unbounded). A next step would be
to investigate the parameterized approximability of current state-of-the-art approximation
algorithms for MAP and show under which parameterization regimes these algorithms can
be shown to be in FERT or FPERT.
From a different perspective, it is interesting to further explore the parameterization
of the error in randomized algorithms and (for example) establish an analog for the Whierarchy for these parameterizations. That allows us to derive more fine-grained negative
parameterization results, in a similar way as proving W[1]-hardness leads to more finegrained negative results than proving para-NP-hardness.

6. Conclusion
In this paper we analyzed whether low tree-width is a prerequisite for approximating MAP
in Bayesian networks. We formalized four distinct notions of approximating MAP (by value,
structure, rank, or expectation) and argued that approximate MAP is intractable in general
using either of these notions. In case of value-approximation, structure-approximation, and
rank-approximation we showed that MAP cannot be approximated using these notions
in (non-trivial) instances with high tree-width, if the ETH holds. However, we showed
that a constrained version of MAP, despite being PP-hard in general, can be tractably
expectation-approximated when the most probable explanation has a high probability. We
proposed the complexity classes FERT and FPERT that capture the parameterization of the
error (respectively error and running time), rather than running time. With these results
we contributed to a fuller understanding of what does and does not make state-of-the-art
approximation algorithms for MAP feasible in practice.

7. Acknowledgements
A previous version of this paper (Kwisthout, 2014) was published in the Proceedings of the
Seventh European Workshop on Probabilistic Graphical Models (PGM 2014). The author
wishes to thank the workshop participants and (both sets of) anonymous reviewers for stimulating discussion and worthwhile suggestions. He thanks in particular Hans Bodlaender
and Todd Wareham for valuable comments on an earlier version of this manuscript.

References
Abdelbar, A. M., & Hedetniemi, S. M. (1998). Approximating MAPs for belief networks is
NP-hard and other theorems. Artificial Intelligence, 102, 2138.
Agrawal, M., Kayal, N., & Saxena, N. (2004). PRIMES is in P. Annals of Mathematics,
160 (2), 781793.
Arora, S., & Barak, B. (2009). Computational Complexity: A Modern Approach. Cambridge
University Press.
718

fiTree-Width and MAP Approximations

Arvind, V., & Raman, V. (2002). Approximation algorithms for some parameterized counting problems. In Bose, P., & Morin, P. (Eds.), Algorithms and Computation, Vol. 2518
of Lecture Notes in Computer Science, pp. 453464. Springer Berlin Heidelberg.
Cheng, J., & Druzdzel, M. (2000). AIS-BN: An adaptive importance sampling algorithm
for evidential reasoning in large Bayesian networks. Journal of Artificial Intelligence
Research, 13 (1), 155188.
Clementi, A., Rolim, J., & Trevisan, L. (1998). Recent advances towards proving P=BPP.
In Allender, E. (Ed.), Bulletin of the EATCS, Vol. 64. EATCS.
Darwiche, A. (2009). Modeling and Reasoning with Bayesian Networks. Cambridge University Press.
De Campos, C. P. (2011). New complexity results for MAP in Bayesian networks. In
Walsh, T. (Ed.), Proceedings of the Twenty-Second International Joint Conference on
Artificial Intelligence, pp. 21002106.
Downey, R. G., & Fellows, M. R. (1999). Parameterized Complexity. Springer Verlag, Berlin.
Flum, G., & Grohe, M. (2006). Parameterized Complexity Theory. Springer, Berlin.
Gill, J. T. (1977). Computational complexity of Probabilistic Turing Machines. SIAM
Journal on Computing, 6 (4), 675695.
Hamilton, M., Muller, M., van Rooij, I., & Wareham, H. (2007). Approximating solution
structure. In Demaine, E., Gutin, G., Marx, D., & Stege, U. (Eds.), Structure Theory
and FPT Algorithmics for Graphs, Digraphs and Hypergraphs, No. 07281 in Dagstuhl
Seminar Proceedings.
Henrion, M. (1986). Propagating uncertainty in Bayesian networks by probabilistic logic
sampling. In Kanal, L., & Lemmer, J. (Eds.), Proceedings of the Second Annual
Conference on Uncertainty in Artificial Intelligence, pp. 149164. New York: Elsevier
Science.
Impagliazzo, R., & Paturi, R. (2001). On the complexity of k-SAT. Journal of Computer
and System Sciences, 62 (2), 367  375.
Krentel, M. W. (1988). The complexity of optimization problems. Journal of Computer
and System Sciences, 36, 490509.
Kwisthout, J. (2009). The Computational Complexity of Probabilistic Networks. Ph.D.
thesis, Faculty of Science, Utrecht University, The Netherlands.
Kwisthout, J. (2011). Most probable explanations in Bayesian networks: Complexity and
tractability. International Journal of Approximate Reasoning, 52 (9), 1452  1469.
Kwisthout, J. (2013). Structure approximation of most probable explanations in Bayesian
networks. In van der Gaag, L. (Ed.), Proceedings of the Twelfth European Conference
on Symbolic and Quantitative Approaches to Reasoning with Uncertainty, Vol. 7958
of LNAI, pp. 340351. Springer-Verlag.
Kwisthout, J. (2014). Treewidth and the computational complexity of MAP approximations.
In van der Gaag, L., & Feelders, A. (Eds.), Proceedings of the Seventh European
Workshop on Probabilistic Graphical Models, Vol. 8754 of Lecture Notes in Computer
Science, pp. 271285. Springer International Publishing.
719

fiKwisthout

Kwisthout, J. (2015). Most frugal explanations in Bayesian networks. Artificial Intelligence,
218, 56  73.
Kwisthout, J., Bodlaender, H. L., & van der Gaag, L. C. (2010). The necessity of bounded
treewidth for efficient inference in Bayesian networks. In Coelho, H., Studer, R.,
& Wooldridge, M. (Eds.), Proceedings of the 19th European Conference on Artificial
Intelligence, pp. 237242. IOS Press.
Kwisthout, J., Bodlaender, H. L., & van der Gaag, L. C. (2011). The complexity of finding
kth most probable explanations in probabilistic networks. In Cerna, I., Gyimothy, T.,
Hromkovic, J., Jefferey, K., Kralovic, R., Vukolic, M., & Wolf, S. (Eds.), Proceedings
of the 37th International Conference on Current Trends in Theory and Practice of
Computer Science, Vol. LNCS 6543, pp. 356367. Springer.
Kwisthout, J., & van Rooij, I. (2013). Bridging the gap between theory and practice of
approximate Bayesian inference. Cognitive Systems Research, 24, 28.
Littman, M. L., Majercik, S. M., & Pitassi, T. (2001). Stochastic boolean satisfiability.
Journal of Automated Reasoning, 27 (3), 251296.
Marx, D. (2007). Can you beat treewidth?. In Proceedings of the 48th Annual IEEE
Symposium on Foundations of Computer Science, pp. 169179.
Montoya, J.-A., & Muller, M. (2013). Parameterized random complexity.. Theory of Computing Systems, 52 (2), 221270.
Papoulis, A. (1984). Probability, Random Variables, and Stochastic Processes (2nd edition).
New York: McGraw-Hill.
Park, J. D., & Darwiche, A. (2001). Approximating MAP using local search. In Proceedings
of the 17th Conference on Uncertainty in Artificial Intelligence, pp. 403410. Morgan
Kaufmann Publishers, San Francisco, California, 2001.
Park, J. D., & Darwiche, A. (2004). Complexity results and approximation settings for
MAP explanations. Journal of Artificial Intelligence Research, 21, 101133.
Robertson, N., & Seymour, P. (1986). Graph minors II: Algorithmic aspects of tree-width.
Journal of Algorithms, 7, 309322.
Sontag, D., Meltzer, T., Globerson, A., Weiss, Y., & Jaakkola, T. (2008). Tightening LP
relaxations for MAP using message-passing. In Proceedings of the 24th Conference in
Uncertainty in Artificial Intelligence, pp. 503510. AUAI Press.
Toda, S. (1994). Simple characterizations of P(#P) and complete problems. Journal of
Computer and System Sciences, 49, 117.
Van Rooij, I., & Wareham, H. (2012). Intractability and approximation of optimization
theories of cognition. Journal of Mathematical Psychology, 56 (4), 232  247.
Yuan, C., Lu, T., & Druzdzel, M. J. (2004). Annealed MAP. In Chickering, D., & Halpern,
J. (Eds.), Proceedings of the Twentieth Conference in Uncertainty in Artificial Intelligence, pp. 628635. AUA.

720

fiJournal of Artificial Intelligence Research 53 (2015) 439-496

Submitted 12/14; published 07/15

Bypassing Combinatorial Protections: Polynomial-Time
Algorithms for Single-Peaked Electorates
Felix Brandt

brandtf@in.tum.de

Institut fur Informatik
TU Munchen
85748 Garching, Germany

Markus Brill

brill@cs.duke.edu

Department of Computer Science
Duke University
Durham, NC 27708, USA

Edith Hemaspaandra

eh@cs.rit.edu

Department of Computer Science
Rochester Institute of Technology
Rochester, NY 14623, USA

Lane A. Hemaspaandra

lane@cs.rochester.edu

Department of Computer Science
University of Rochester
Rochester, NY 14627, USA

Abstract
For many election systems, bribery (and related) attacks have been shown NP-hard using constructions on combinatorially rich structures such as partitions and covers. This paper shows that for voters who follow the most central political-science model of electorates
single-peaked preferencesthose hardness protections vanish. By using single-peaked preferences to simplify combinatorial covering challenges, we for the first time show that NPhard bribery problemsincluding those for Kemeny and Llull electionsfall to polynomial
time for single-peaked electorates. By using single-peaked preferences to simplify combinatorial partition challenges, we for the first time show that NP-hard partition-of-voters problems fall to polynomial time for single-peaked electorates. We show that for single-peaked
electorates, the winner problems for Dodgson and Kemeny elections, though p2 -complete
in the general case, fall to polynomial time. And we completely classify the complexity of
weighted coalition manipulation for scoring protocols in single-peaked electorates.

1. Introduction
Elections are perhaps the most important framework for preference aggregation. An election
system (or election rule) is a mapping that takes as input the preferences of the voters with
respect to the set of candidates (alternatives) and returns a set of winners, which is some
subset of the candidate set. Elections are central in preference aggregation among humans
in everything from political elections to selecting good singers on popular television shows.
Elections are rapidly increasing in importance in electronic settings such as multiagent
systems, and have been used or proposed for such varied tasks as recommender systems
and collaborative filtering (Ghosh, Mundhe, Hernandez, & Sen, 1999; Pennock, Horvitz, &
c
2015
AI Access Foundation. All rights reserved.

fiBrandt, Brill, Hemaspaandra, & Hemaspaandra

Giles, 2000), web spam reduction and improved web-search engines (Dwork, Kumar, Naor,
& Sivakumar, 2001), and planning (Ephrati & Rosenschein, 1997). In electronic settings,
elections may have huge numbers of voters and alternatives.
One natural worry with elections is that agents may try to slant the outcome, for example, by bribing voters. Motivated by work from economics and political science showing
that reasonable election systems always allow manipulation in some cases (Gibbard, 1973;
Satterthwaite, 1975; Duggan & Schwartz, 2000), starting in 1989, Bartholdi, Orlin, Tovey,
and Trick (Bartholdi, Tovey, & Trick, 1989; Bartholdi & Orlin, 1991; Bartholdi, Tovey,
& Trick, 1992) made the thrilling suggestion that elections be protected via complexity
theorynamely, by making the attackers task NP-hard. This line has been active ever
since. It has resulted in NP-hardness protections being proven for many election systems,
against such attacks as bribery (the attacker has a budget with which to buy and alter
voters votes, Faliszewski, Hemaspaandra, & Hemaspaandra, 2009), manipulation (a coalition of voters wishes to set its votes to make a given candidate win, Bartholdi et al.,
1989; Bartholdi & Orlin, 1991), and control (an agent seeks to make a given candidate win
by adding/deleting/partitioning voters or candidates, Bartholdi et al., 1992). The book
chapter of Faliszewski, Hemaspaandra, Hemaspaandra, and Rothe (2009b) surveys such
NP-hardness results, which apply to many important election systems such as plurality,
single transferable voting, and approval voting.
In the past few years, a flurry of papers have come out asking whether the NP-hardness
protections are satisfying. In particular, the papers explore the possibility that heuristic
algorithms may do well frequently or that approximation algorithms may exist. These
papers can themselves be questioned. For example, the most influential frequency paper (Friedgut, Kalai, & Nisan, 2008, see also its journal version, Friedgut, Kalai, Keller,
& Nisan, 2011) assumes each voter has a random and independent candidate preference
ordering, and that model does not seem to reflect typical voter behavior. And as to approximations, there is work showing that for certain voter-control settings (different than those
studied in this paper) there are polynomial-time algorithms that use, for example, at most
the log of the number of candidates times as many added voters as an optimal approach
would need (Faliszewski, Hemaspaandra, & Hemaspaandra, 2013). However, a campaign
manager might well not have the financial resources to motivate that many extra people to
come out and vote, but rather would want to know the smallest possible number of votes
to add to reach victory.
The present paper questions the NP-hardness results from a completely different direction. In political science, perhaps the most canonical model of electorates is the unidimensional single-peaked model. In that model, the electorate has preferences over some
one-dimensional spectrum (e.g., very liberal through very conservative) along which the
candidates are also located, and in which each voters preferences (loosely put) have a
peak, with affinity declining as one moves away from the peak. A brilliant paper by Walsh
(2007) recently asked whether NP-hardness protections against manipulation fall apart if
electorates are single-peaked. For the case Walsh looked at, the answer he proved is no;
he looked at a particular NP-hardness manipulation protection and proved it holds even
for single-peaked societies. Faliszewski, Hemaspaandra, Hemaspaandra, and Rothe (2011),
inspired by Walshs work, looked at a range of election systems and came to the sharply dif440

fiBypassing Combinatorial Protections

Problem

General cases complexity

Single-peaked cases
complexity

approval:
bribery
negative-bribery
strongnegative-bribery

NP-comp. (Faliszewski et al., 2009)
NP-comp. (Thm. 4.3, part 1)
NP-comp. (Thm. 4.3, part 1)

P (Thm. 4.2)
P (Thm. 4.3, part 2)
P (Thm. 4.3, part 2)

NP-comp.
NP-comp.
NP-comp.
NP-comp.
NP-comp.

P (Thm. 4.7)
P (Thm. 4.7)
P (Thm. 4.7)
NP-comp. (Thm. 4.7)
P (Thm. 5.3)

Llull:
bribery
$bribery
weighted-bribery
weighted-$bribery
control by
voter partition
Kemeny:
winner
bribery
$bribery
weighted-bribery
weighted-$bribery

(Faliszewski
(Faliszewski
(Faliszewski
(Faliszewski
(Faliszewski

et
et
et
et
et

al.,
al.,
al.,
al.,
al.,

2009)
2009)
2009)
2009)
2009a)

p2 -comp. (Hemaspaandra et al., 2005)
p2 -hard (Thm. 4.8)
p2 -hard (Thm. 4.8)
p2 -hard (Thm. 4.8)
p2 -hard (Thm. 4.8)

P (Thm. 3.3)
P (Thm. 4.9)
P (Thm. 4.9)
P (Thm. 4.9)
NP-comp. (Thm. 4.9)

Table 1: How single-peakedness (often) lowers the complexity of some key election problems.

fering conclusion that for many crucial cases, NP-hardness protections against manipulation
and control vanish for single-peaked electorates.
The present paper is in this young line of research on complexity of manipulative actions
in the context of single-peaked electorates. Our work seeks to take this line of research in
new directions, and to improve one existing direction, through the following contributions:
1. We (Section 3) show that checking who the winner is in Dodgson, Young, and Kemeny elections, which is known to be p2 -complete in the general case (respectively due
to Hemaspaandra, Hemaspaandra, & Rothe, 1997, due to this papers Theorem A.2
based on adapting a proof of Rothe, Spakowski, & Vogel, 2003, and due to Hemaspaandra, Spakowski, & Vogel, 2005), is in polynomial time for single-peaked electorates
(Corollary 3.3 and Theorem 3.4).
Our algorithm that shows this for Dodgson elections is a good example of the general
technical theme of this paper: That single-peakedness often precludes combinatorial explosion. In this particular case, single-peakedness will simplify the seemingly
exponential-sized search space over series of exchanges to provide upper bounds on
Dodgson scores, and will allow us to instead search over a polynomial-sized possibility space related to a particular, simple set of exchanges happening and limited to at
most two voters.
2. We (Section 4) for the first time study the effect of single-peaked electorates on the
complexity of bribery. We show that many NP-hardness protections against bribery
441

fiBrandt, Brill, Hemaspaandra, & Hemaspaandra

in the general case vanish for single-peaked electorates. (Table 1 provides some key
examples of this and other examples of lowering complexity.) To show this, we
give polynomial-time bribery algorithms for single-peaked electorates in many settings. Our polynomial-time algorithms apply to approval voting (Theorem 4.2 and
Theorem 4.3) and to the rich range of weak-Condorcet consistent election systems
and even to systems that are merely known to be weak-Condorcet consistent when
the electorate is single-peaked (Corollary 4.5), including weakBlack, weakDodgson,
Fishburn, Kemeny, Llull, Maximin, Schwartz, Young, and the two variants of Nanson
due to Fishburn and Schwartz.
The right general interpretation of what underlies this is that the NP-hardness results
use (in the outputs of the reductions establishing NP-hardness) sets of voter preferences
that are so intricate that they simply cannot be realized by single-peaked societies. The
practical lesson is that we should be very skeptical about NP-completeness results if
our electorate may have limitations (such as single-peakedness) on the ensembles of
votes it produces. And the specific technical reason we can obtain polynomial-time
bribery algorithms is that the NP-hardness proofs were based on the combinatorially
rich structure of covering problems (whose core challenge is the incomparability of
voters), but we will (see the proof of Theorem 4.2) use single-peakedness to create
a directional attack on covering problems that has the effect of locally removing
incomparability.
3. We (Section 5) for the first time study the effect of single-peaked electorates on the
complexity of control by partition of voters, in which the voters are partitioned into
two groups that vote on the candidates in primary elections, and only the winners
of the primaries compete in the final election. This is one of the seven types of control
introduced in the seminal control paper of Bartholdi et al. (1992), but control by
partition of voters has not been previously addressed for the single-peaked case. We
show that some known NP-hardness protections against control-by-partition vanish
for single-peaked electorates, and we do so by giving polynomial-time algorithms for
single-peaked control by partition (Theorems 5.2 and 5.6, and Corollary 5.3).
The general interpretation and practical lesson from this are the same as was just
mentioned for the bribery case. However, the technical way we obtain this control-bypartition result differs here. The technical challenge here is the exponential number
of partitions, and our algorithms circumvent this by using single-peakedness to allow
us to in effect structure that huge number of partitions into a polynomial number of
classes of partitions such that for each class we can look just at the class rather than
having to explore each of its member partitions.
The shared technical theme here and in the bribery case is that single-peakedness can
be used to tame the combinatorial explosion (of partitions and covers) that in the
general case protected elections from attack, and in particular single-peakedness yields
polynomial-time attack algorithms.
4. Our final contribution (Section 6) is a strong extension of an important result from
Faliszewski et al. (2011). For the broad class of election systems known as scoring
protocols, Faliszewski et al. gave a complete characterization of the computational
442

fiBypassing Combinatorial Protections

complexity of the (weighted, coalition) manipulation problem in the case of singlepeaked elections with three candidates. Such characterizations are important as they
tell both which systems are easily manipulable and what it is about the systems
that makes them easily manipulable. We extend this by providing, for single-peaked
electorates, a complete characterization of easy manipulability for scoring protocols
(Theorem 6.2). That is, we extend the three-candidate theorem of Faliszewski et al.
(2011) to a result that holds for any number of candidates, and that allows one to
immediately read off the complexity of manipulation of any scoring protocol, for
single-peaked electorates.
Our proof organization is as follows. Each of our four result sections contains one
spotlight theorem, whose proof we give within the section itself. These proofs seek to
give the key flavor of our techniques, and in text just before these proofs we will often try to
informally describe these proofs ideas or approaches. The first three of the four spotlight
proofs directly support, as does the fourth spotlight proof in part, this papers technical
theme that single-peakedness tames combinatorial explosion. The appendix contains, for
completeness, proofs of all our other results, and some definitions omitted from the main
text.

2. Preliminaries
This section presents preliminaries on such topics as election systems, preferences, notions
related to Condorcet consistency, and single-peakedness.
2.1 Election Systems, Preferences, and weakCondorcet Consistency
An election system (or election rule) is a mapping from a finite set of candidates C and
a finite collection V of voter preferences over those candidates to a collection W  C
called the winner set.1 For all but one of the election systems we cover, each voters
preference is a linear order (by which we always mean a strict linear order: an irreflexive,
antisymmetric, complete, transitive relation) over the candidates. For the election system
called approval voting, each voter votes by a bit-vector, approving or disapproving of each
candidate separately. Voters preferences are input as a list of ballots (i.e., votes), so if
multiple voters have the same preference, the ballot of each will appear separately in V .
We now very briefly describe the election systems most central to this paper. In approval
voting, preferences are approval vectors, and each candidate who gets the highest number of
approvals among the candidates belongs to the winner set. In all the other systems we use,
voters will vote by linear orders. A candidate is said to be a Condorcet winner (respectively,
weak Condorcet winner ), if that candidate is preferred to each other candidate by a strict
majority (respectively, by at least half) of the voters. In Condorcet voting (or Condorcet
elections) the winners are precisely the set of Condorcet winners. In the election system
1. In social choice theory, this is called a social choice correspondence. Social choice theorists often exclude
the case allowing the function to have an empty set of winners, but following Bartholdi et al. (1992)
and many computationally oriented papers, we do not artificially exclude that case in our definitions.
However, except for elections with zero candidates, the only systems we discuss that might ever output
an empty set of winners are Condorcet and weakCondorcet.

443

fiBrandt, Brill, Hemaspaandra, & Hemaspaandra

weakCondorcet, the winners are precisely the set of weak Condorcet winners. It has been
known for two hundred years that some election instances have neither Condorcet winners
nor weak Condorcet winners (Condorcet, 1785). And of course, no election instance can
have more than one Condorcet winner, whereas there might be several weak Condorcet
winners.
Let , 0    1, be a rational number. Copeland (Copeland, 1951, for  = 12 ;
Faliszewski et al., 2009a, for general rational ) is the election system defined as follows.
For each pair of distinct candidates, consider the one (if any) who is preferred between
the two by a strict majority of the voters. That one gets one Copeland point from
the pairwise contest and the other gets zero Copeland points. If the candidates in the
pair tie in their pairwise contest (which can happen only when the number of voters is
even), each gets  points. When  = 1, Copeland is known as Llull, a system defined
by the mystic Ramon Llull in the thirteenth century (see Hagele & Pukelsheim, 2001).
Llulls election system is known to be remarkably resistant, computationally, to bribery
and control attacks (Faliszewski et al., 2009a, although see also Erdelyi, Nowak, & Rothe,
2009, Erdelyi & Rothe, 2010, Erdelyi, Piras, & Rothe, 2011, and Menton, 2013, for different
highly resistant systems, and see Hemaspaandra, Hemaspaandra, & Rothe, 2009, regarding
how extremely resistant artificial systems can be constructed).
An important class of elections, which we will focus on in Section 6, is the class of
scoring protocols. Each scoring protocol has a fixed number m of candidates and is defined
by a scoring vector  = (1 , 2 , . . . , m )  Nm , 1  2      m . Voters votes are
linear orders, and each voter contributes 1 points to his or her most preferred candidate,
2 points to his or her next most preferred candidate, and so on. Each candidate whose
total number of points is at least as great as the totals of each other candidate is a winner.
For example, m-candidate plurality voting is the scoring protocol defined by the scoring
m1
z }| {
vector  = (1, 0, . . . , 0). And m-candidate Borda voting is the scoring protocol defined by
the scoring vector  = (m  1, m  2, . . . , 0).
Kemeny elections are based on the
P concept of a Kemeny consensus. Each linear order > with minimum Kemeny score, a,bC, a > b k{v  V | v prefers b to a}k, is said to be
a Kemeny consensus. As usual, kSk denotes the cardinality of finite set S. A candidate c
is a Kemeny winner if c is ranked first in some Kemeny consensus. Kemeny elections were
introduced by Kemeny (1959, see also Kemeny & Snell, 1960).
In Black elections (respectively, weakBlack elections), if there is a Condorcet winner (respectively, if there are weak Condorcet winners), then that defines the winners, and otherwise Bordas method is used to select the winners. Black elections were introduced by Black
(1958) and weakBlack elections (somewhat confusingly called Black elections there) were
introduced by Fishburn (1977). In Dodgson elections (respectively, weakDodgson elections),
whichever candidates can by the fewest repeated transpositions of adjacent candidates in
voters orders become Condorcet winners (respectively, weak Condorcet winners) are the
winners. Dodgson elections were introduced in the 1800s by Dodgson (1876) and weakDodgson elections (somewhat confusingly called Dodgson elections there) were introduced
by Fishburn (1977) and further studied by McCabe-Dansted, Pritchard, and Slinko (2008).
In Young elections (respectively, strongYoung elections), whichever candidates can by the
deletion of the fewest voters become weak Condorcet (respectively, Condorcet) winners are
444

fiBypassing Combinatorial Protections

the winners. Young elections were introduced by Young (1977) and strongYoung elections
(somewhat confusingly called Young elections there) were introduced by Rothe et al. (2003).
An important notion in this paper is that of being weakCondorcet-consistent. An election system is said to be weakCondorcet-consistent (which we earlier wrote, equivalently, as
weak-Condorcet consistent), if on every input that has at least one weak Condorcet winner, the winners of the election system are exactly the set of weak Condorcet winners.2
Some of our bribery results will hold for all election systems that are weakCondorcetconsistent, and even for all election systems that when restricted to single-peaked electorates
are weakCondorcet-consistent.
Fishburn (1977) has noted that the election systems weakBlack, weakDodgson, Fishburn, Maximin, and Young are weakCondorcet-consistent. We add to that the observation
that Llull elections are easily seen from their definition to be weakCondorcet-consistent.
We also make the (new) observation that the election systems Kemeny, Schwartz, and the
two variants of Nanson due to Fishburn and Schwartz are weakCondorcet-consistent when
restricted to single-peaked electorates. (By Fishburn, 1977, and Niou, 1987, those systems
are known not to be weakCondorcet-consistent in the general case.) We also note that
Black, Dodgson, the original version of Nanson, and for each , 0   < 1, Copeland elections are not weakCondorcet-consistent even when restricted to single-peaked electorates.
This is seen by the following universal counterexample. Let there be two voters with preferences b > a > c and c > b > a. These preferences are single-peaked with respect to
the societal ordering a L b L c (the notion of societal orders will be explained two paragraphs after the present one). Candidates b and c are weak Condorcet winners, but each
of the mentioned election systems chooses only b. Similarly, we note that strongYoung is
not weakCondorcet-consistent for single-peaked electorates because in an election with two
voters whose preferences are a > b > c and c > b > a, all candidates are weak Condorcet
winners, but strongYoung yields only candidates a and c. The appendix includes definitions
of the election systems Fishburn, Maximin (a.k.a. Simpson), and Nanson, and proves all of
the new observations made in this paragraph.
2.2 Single-Peaked Preferences
This papers theme is that combinatorial protections crumble for the case of single-peaked
electorates. We now briefly define what single-peaked preferences are and what their motivation is.
The single-peaked preference model was introduced over half a century ago by Black
(1948, 1958) and has been influential ever since. The model captures the case where the
electorate is polarized by a single issue or dimension, and each voters utility along that
dimension has either one peak or just rises or just falls. Candidates have positions (locations)
along that dimension. And a voters preferences (in the linear order model) simply order the
candidates by utility (except with no ties allowed). Since the utility curves are very flexible,
what this amounts to is that there is an overall societal ordering L of the candidates, and
each voter can be placed in some location such that for all the candidates to his or her
2. The nomenclature in the literature is varied here. Some authors use the term weak Condorcetconsistent to mean systems that always select all weak Condorcet winners but perhaps have additional winners. And what we denote weakCondorcet-consistent is precisely what Fishburn (1977) calls
[obeying the] strict Condorcet principle.

445

fiBrandt, Brill, Hemaspaandra, & Hemaspaandra

voters
v1
v2
v3
v4

utility

c1
liberal

c2

c3

c4

candidates

c5
conservative

Figure 1: Example of a single-peaked electorate of four voters, with their utility functions
shown.

right the preferences drop off and the same to the left, although within that framework,
the right and the left candidates can be interspersed with each other. A picture will make
this clearer. Figure 1 shows an electorate with four voters and five candidates, in which
societys polarization is on a (liberal-to-conservative) axis. From the picture, we can see
that v1 has preferences c5 > c4 > c3 > c2 > c1 , v2 has preferences c1 > c2 > c3 > c4 > c5 ,
v3 has preferences (note the interleaving) c2 > c3 > c1 > c4 > c5 , and v4 has preferences
c4 > c5 > c3 > c2 > c1 .
Formally, there are many equivalent ways to capture this behavior, and we use the
following as our definition. A collection V of votes (each a linear ordering >i of the
candidates) over candidate set C is said to be single-peaked exactly if there exists a linear ordering L over C such that for each triple of candidates a, b, and c, it holds that
(a L b L c  c L b L a)  (i) [a >i b  b >i c].
The single-peaked model has been intensely studied, and has both strengths and limitations. On the positive side, it is an excellent rough model for a wide range of elections.
Votes ranging from American presidential elections to US Supreme Court votes to hiring
votes within a CS department are often shockingly close to reflecting single-peaked preferences. It certainly is a vastly more reasonable model in most settings than is assuming
that all voters are random and independent, although the latter model has been receiving
a huge amount of study recently. In fact, a wide range of scholarly studies have argued for
the value of the single-peaked model (Black, 1948, 1958; Davis, Hinich, & Ordeshook, 1970;
Niemi & Wright, 1987; Procaccia & Rosenschein, 2007; Krehbiel, 1998), and the model is
one of the first taught to students in positive (i.e., theoretical) political science courses. On
the other hand, some electorates certainly are driven by multidimensional concerns, and
even a heavily unidimensional electorate may have a few outside-the-box voters (e.g., voters
who in an election polarized on a liberal-conservative axis decide their votes instead based
on, for example, religion or race). Simply put, it is a model, and so it speaks of a simplified
version of the world.
446

fiBypassing Combinatorial Protections

The single-peaked model also makes sense for approval voting (Faliszewski et al., 2011):
There, a voter intuitively may be thought to have some utility threshold starting at which he
or she approves of candidates. What this means is that each voters approved candidates
must be contiguous within societys linear order L. Formally, we define this by saying
that an election instance (of approval voters) is single-peaked exactly if there exists a linear
order L such that for each triple of candidates a, b, and c, we have a L b L c  (i) [{a, c} 
Approvesi  b  Approvesi ], where Approvesi is the set of candidates voter i approves.
Following the suggestion in Walshs (2007) seminal work, we will assume (except when
we make something else clear) that societys linear order is part of the input in our singlepeaked winner, bribery, manipulation, and control problems. However, we mention in
passing that given an election instance, one can in polynomial time tell whether the voters
are single-peaked and when so can also in polynomial time compute a societal linear order
instantiating the single-peakedness (by Bartholdi and Trick, 1986, Doignon and Falmagne,
1994, and Escoffier, Lang, and Ozturk, 2008, for linear-order preferences and, as pointed
out in Theorem 2.1 of Faliszewski et al., 2011, by Fulkerson and Gross, 1965, and Booth
and Lueker, 1976, for approval preferences). One of course also can easily, in polynomial
time, check whether a given linear order is one with respect to which a given set of votes is
single-peaked.
Because we want to get to the results as quickly as possible, we will define the needed
notions of winner, bribery, control, and manipulation each at the start of the section on the
particular topic.

3. WeakCondorcet Elections, Single-Peaked Electorates, and Bypassing
Winner-Problem Complexity
The main results sections of this paper study whether single-peakedness bypasses
complexity-theoretic protections against attacks on elections. Before moving to those sections, we quickly present some results showing that single-peakedness also bypasses the
complexity results some systems have for even telling who won. Unlike the protection
from attack complexity-shield bypassings, which are in some sense bad news (for the security of the election systems), these winner-hardness complexity-shield bypassings are
good newstaming the complexity of election systems such as Dodgson and Kemeny for the
single-peaked case, despite the fact that they are known to have NP-hard winner problems
in the general case.
For a given election system E, the winner problem takes as input an election, (C, V ),
and a candidate p  C, and asks if p is a winner in the election whose candidates are C and
whose votes are V (where V is a collection of votes over the candidate set C). When we
speak of the single-peaked case of the winner problem, the input will also contain a linear
order L relative to which the election is single-peaked. (Formally, part of the winner-problem
task is to check that the input indeed is single-peaked relative to L. However, since that
is a polynomial-time check for all caseslinear orders and approval vectorsthat we deal
with, we will tacitly view the appropriateness of L as a syntactic condition on the input,
although it is not really syntactic.) Note that the weakCondorcet winner problem is in P
in the general case and thus certainly in the single-peaked case. Furthermore, something
447

fiBrandt, Brill, Hemaspaandra, & Hemaspaandra

used often in our papers proofs is the following standard fact about Condorcet voting and
medians.
Fact 3.1. Let (C, V ) be an election with votes being linear orders over C, and let L be
a linear order with respect to which (C, V ) is single-peaked. Associate each voter with the
candidate at the top of that voters preference ordering. Order the voters with respect to L
in terms of that association.
If kV k is odd there is a unique weakCondorcet and Condorcet winner and that winner
is the top preference of the median voter. If kV k is even the weakCondorcet winner set is
the set of all candidates who in L fall in the range, inclusively, from the top preference of
the leftmost of the two median voters through the top preference of the rightmost of the two
median voters (and if those two coincide, then that candidate is a Condorcet winner and
otherwise there is no Condorcet winner).
For example, if this is our ordered-by-L picture of the candidates and what the voters
top choices are:
v3
v6
v1
v2
v4
v5
c1

c2

c3

c4

c5

c6

then c2 , c3 , and c4 are weak Condorcet winners, since each of these candidates is preferred
to all candidates to its right by v1 , v2 , and v3 , and to all candidates to its left by v4 , v5 ,
and v6 . c1 is not a weak Condorcet winner, since all voters other than v1 prefer c2 to c1 .
c5 and c6 are not weak Condorcet winners, since v1 , v2 , v3 , and v4 prefer c4 to c5 and c6 .
Finally, note that there is no Condorcet winner, since any Condorcet winner is a unique
weak Condorcet winner.
An immediate consequence of Fact 3.1 is the well-known fact that for single-peaked
elections, there is always at least one weak Condorcet winner (we are tacitly here assuming
C 6= ). Since we earlier noted that the winner problem is in P for weakCondorcet elections,
the following holds.
Theorem 3.2. For each election system E that is weakCondorcet-consistent when restricted
to single-peaked electorates, the winner problem is in P when restricted to single-peaked
elections.
Of course, for many such systems the winner problem is obviously in P even in general.
Yet we do get some interesting consequences from Theorem 3.2 such as the following (recall
from Section 2 that Young and weakDodgson are weakCondorcet-consistent, and Kemeny
is weakCondorcet-consistent when restricted to single-peaked electorates).
Corollary 3.3. When restricted to single-peaked electorates, the winner problems for Kemeny, Young, and weakDodgson elections are in P.
In contrast, the general-case Kemeny winner problem problem was proven by Hemaspaandra et al. (2005) to be p2 -complete.3 And we prove in this paper that the generalcase winner problems for Young and weakDodgson elections are p2 -complete as well (see
3. p2 is the class of sets that can be solved through polynomial-time parallel access to NP (Papadimitriou &
Zachos, 1983; Hemachandra, 1989). Throughout this paper, completeness always refers to completeness
with respect to polynomial-time many-one reductions.

448

fiBypassing Combinatorial Protections

Theorems A.2 and A.4 in the appendix). So, Theorem 3.2 implies sharp complexity simplifications for these three election systems. We mention in passing that for even the generalization of single-peakedness known as bounded single-peaked width (Cornaz, Galand,
& Spanjaard, 2012), work of Cornaz, Galand, and Spanjaard (2013) that was done subsequent to our Corollary 3.3 (Brandt, Brill, Hemaspaandra, & Hemaspaandra, 2010) shows
that in polynomial time one can find a Kemeny winner, and can find the score that that
winnerand thus all Kemeny winnerswill have. (This does not necessarily mean that one
has a polynomial-time algorithm for testing, in their generalized setting, whether a given
candidate is a Kemeny winner.)
The identify with weakCondorcet approach that just worked on Young and weakDodgson elections does not apply to Dodgson and strongYoung elections. However, we
have constructed direct algorithms that solve their winner problems in polynomial time
in the single-peaked case. We state that as a theorem, and prove it immediately as our
spotlight proof for this section.
Theorem 3.4. When restricted to single-peaked electorates, the winner problems for Dodgson and strongYoung elections are in P.
Proof. Recall the following easy characterization of Condorcet winners in the singlepeaked setting. If kV k is odd, the top choice of the median voter is a Condorcet winner.
If kV k is even, there are two cases: either both median voters have the same top choice or
not. In the former case, the median voters preferred candidate is a Condorcet winner, and
in the latter case there is no Condorcet winner (since the top choices of two medians, if
different, will tie each other).
Given an election instance (C, V ) and a valid single-peaked order L, we now show how
to compute all strongYoung winners in polynomial time. Recall that strongYoung winners
are all candidates that can be made Condorcet winners by the fewest voter deletions. We
mention that if C =  there can never be winners. If there are zero voters, all candidates
are strongYoung winners, as they all tie at distance , by convention.4 If (C, V ) has a
Condorcet winner, then that is the unique strongYoung winner. Otherwise kV k  2 is even
and the two median voters have different top choices, say m` and mr . Then the strongYoung
winner set is {m` , mr }, as those two candidates have a strongYoung score of 1, no one has
score 0, and everyone else has score at least 2.
We now show that Algorithm 1, which clearly runs in polynomial time, computes all
Dodgson winners. Recall that Dodgson winners are the candidates who can by the fewest
repeated transpositions of adjacent candidates in voters orders (so-called switches) become
Condorcet winners. If kCk = 0, then there are no winners, if kV k = 0, all of C ties
as winners, and if (C, V ) has a Condorcet winner, this candidate is the unique Dodgson
winner. So assume kV k  2 is even and the two median voters have different top choices,
say m` and mr , m` L mr , and no candidate has Dodgson score 0. The intuition behind the
4. Regarding this and the line in Algorithm 1 handling the zero-voter or zero-candidate case, one might
wonder why we dont just define all our election problems to not allow those cases. The answer is, first,
that it is unattractive to simplify proofs by altering problems. But, more compellingly, control problems
are important to our and other papers, and control problems on inputs not having a small number
of candidates (respectively, voters) can themselves create situations with small numbers of candidates
(respectively, voters). In particular, legal partitions within partition-control types can leave one with no
candidates or no voters.

449

fiBrandt, Brill, Hemaspaandra, & Hemaspaandra

algorithm is as follows. We show that every Dodgson winner is a weak Condorcet winner.
And we show that we can always turn a weak Condorcet winner into a Condorcet winner
with a minimum number of switches by making changes in only two voters.5 The proof
of correctness follows immediately from Claims 3.5 and 3.6 below. Note that by Fact 3.1,
the set of weak Condorcet winners consists of all candidates who in L fall in the range,
inclusively, from m` to mr . We will denote this set by [m` , mr ]L .
Claim 3.5. Algorithm 1 will find the correct Dodgson score of each candidate p  [m` , mr ]L .
Claim 3.6. Every Dodgson winner is in [m` , mr ]L .
In our proofs of these two claims, we use the following simple claim.
Claim 3.7.

1. Let p  [m` , mr ]L . If (a L b L p or p L b L a) and a ties p, then b ties p.

2. If mr L d, then mr beats d. If d L m` , then m` beats d.6
Proof of Claim 3.7.
1. We know that half of the voters prefer a to p and half of the voters prefer p to a. All
voters that prefer a to p also prefer b to p. This implies that p at best ties b. By
Fact 3.1, p is a weak Condorcet winner. It follows that p ties b.
2. We prove the first statement. The proof of the second statement is analogous. Suppose
for a contradiction that that mr does not beat d. Since mr is a weak Condorcet winner,
mr ties d. Let db be the candidate immediately to the right of mr (with respect to L).
Using part 1 of this claim, it follows that db ties mr .
Since db is not a weak Condorcet winner, there exists a candidate c such that c beats
b Note that c L mr L db or mr L db L c. If c L mr L d,
b then every voter who prefers c
d.
b And so mr beats d,
b which contradicts the fact that mr ties
to db also prefers mr to d.
b
b
d. If mr L d L c, then every voter who prefers c to db also prefers db to mr . And so c
and db beat mr , which contradicts the fact that mr is a weak Condorcet winner.
q Claim 3.7
Proof of Claim 3.5. Consider an optimal (with respect to the number of switches) way
to turn p into a Condorcet winner. We first assume that T` 6=  and that Tr 6= . Let c` be
the leftmost candidate in T` and let cr be the rightmost candidate in Tr . Since p needs to
gain a vote over c` , there exists a voter v` such that c` >v` p and p gets switched beyond c`
in v` . Since p needs to gain a vote over cr , there exists a voter vr such that cr >vr p and p
gets switched beyond cr in vr . Let A` , B` , and C` be sets of candidates such that v` s order
is of the form
A` > c` > B` > p > C` .
5. One might think that turning a weak Condorcet winner into a Condorcet winner would be equivalent
to making sure that the median voters have this candidate as their top choice. However, note that
the electorate may no longer be single-peaked after switches, and so the footnoted statement is more
surprising and harder to prove then one might think.
6. Note that it is possible for a candidate that is not a weak Condorcet winner to tie a weak Condorcet
winner. For example, in the universal counterexample at the end of Section A.2, candidate a, which
is not a weak Condorcet winner, ties weak Condorcet winner c.

450

fiBypassing Combinatorial Protections

Algorithm 1 Dodgson winners
1: if kV k = 0 or kCk = 0 then
2:
return C
3: else if kV k is odd or (kV k is even and the two median voters have the same top choice)
then
4:
return the candidate chosen by the median voter(s)
5: else
6:
Let m` and mr , m` L mr , be the top choices of the median voters
7:
for all p  [m` , mr ]L do
8:
DodgsonScore(p)  
9:
Let T` be the set of candidates c such that c L p and c ties p
10:
Let Tr be the set of candidates c such that p L c and c ties p
11:
for all v` , vr  V such that T` >v` p and Tr >vr p do
12:
Move p up in the order of v` beyond every candidate in T`
13:
Move p up in the order of vr beyond every candidate in Tr
14:
Let n be the number of switches used to do this
15:
if n < DodgsonScore(p) then
16:
DodgsonScore(p)  n
17:
return {p  C | DodgsonScore(p)  DodgsonScore(c) for all c  [m` , mr ]L }
Let Ar , Br , and Cr be sets of candidates such that vr s order is of the form
Ar > cr > Br > p > Cr .
Note that T` >v` p >v` Tr and Tr >vr p >vr T` . Clearly, v` 6= vr and it takes kB` k + 1
switches to switch p beyond c` in v` and it takes kBr k + 1 switches to switch p beyond cr in
vr . After these kB` k + kBr k + 2 switches, v` s order has turned into A` > p > c` > B` > C`
and vr s order has turned into Ar > p > cr > Br > Cr . Since T`  Cr , we still have to
ensure that p gains a vote over every candidate in A`  T` and since Tr  C` , we still have
to ensure that p gains a vote over every candidate in Ar  Tr . So,
DodgsonScore(p)  kB` k + kBr k + 2 + kA`  T` k + kAr  Tr k.
We now show that Algorithm 1 correctly computes the Dodgson score of p. First note
that the algorithm computes an upper bound on the Dodgson score, since p is made a
Condorcet winner in each iteration of the for loop (recall from Fact 3.1 that p is already a
weak Condorcet winner). Now consider the score computed by the algorithm for voters v`
and vr above (since T` >v` p and Tr >vr p, the algorithm will consider these two voters).
In the analysis, it may help to keep in mind that c` L T`  {c` } L p L Tr  {cr } L cr .
If the top choice of v` is c` itself or to the left of c` , then A`  T` = . In this case,
moving p up in the order of v` beyond every candidate in T` gives
A` > p > c` > B` > C` .
If the top choice of v` is to the right of c` , then for every candidate c  A` , c` L c L p. It
follows by Claim 3.7.1 that A` = A`  T` . In this case, the algorithm changes v` to
p > A` > c` > B` > C` .
451

fiBrandt, Brill, Hemaspaandra, & Hemaspaandra

In both cases, the algorithm uses kB` k + 1 + kA`  T` k switches on v` . The same argument
shows that the algorithm uses kBr k + 1 + kAr  Tr k switches on vr . This clearly makes p
a Condorcet winner using kB` k + kBr k + 2 + kA`  T` k + kAr  Tr k switches. Since this is
also an upper bound (see above), it follows that
DodgsonScore(p) = kB` k + kBr k + 2 + kA`  T` k + kAr  Tr k.
We still have to handle the case that T` =  or Tr = . Without loss of generality,
assume that Tr = . Since m` and mr tie p and m` 6= mr , it follows that T` 6= . Let c`
be the leftmost candidate in T` . As in the previous case, there exist a voter v` and sets of
candidates A` , B` , and C` such that v` s order is of the form
A` > c` > B` > p > C`
and
DodgsonScore(p)  kB` k + 1 + kA`  T` k.
We now show that Algorithm 1 correctly computes the Dodgson score of p. Consider
the score computed by the algorithm for voter v` above and letting vr be an arbitrary voter.
Since T` >v` p and Tr >vr p (since Tr = ), the algorithm will consider these two voters.
As in the previous case, the algorithm uses kB` k + 1 + kA`  T` k switches on v` . And
since Vr = , the algorithm uses zero switches on vr . This clearly makes p a Condorcet
winner using kB` k + 1 + kA`  T` k switches. Since this is also an upper bound, it follows
that
DodgsonScore(p) = kB` k + 1 + kA`  T` k.
q Claim 3.5
Proof of Claim 3.6. Let d 6 [m` , mr ]L . Without loss of generality, assume that mr L d.
We will show that DodgsonScore(d) > DodgsonScore(mr ), which implies that d is not a
Dodgson winner. Let T be the set of candidates in C  {mr } that mr ties with. Note that
T 6= , since m` ties with mr . For every c  T , c L mr (by Claim 3.7.2) and d does not beat
c (half of the voters prefer c to mr , and since c L mr L d, these voters prefer c to mr to d).
Consider an optimal (with respect to the number of switches) way to turn d into a
Condorcet winner. Let c` be the leftmost candidate in T . Since half of the voters prefer c`
to mr to d, there exists a voter v such that c` >v mr >v d and d gets switched beyond c`
in v. Let A, B, C1 , and C2 be sets of candidates such that vs order is of the form
A > c` > B > mr > C1 > d > C2 .
It takes kBk + kC1 k + 2 switches to switch d beyond c` in v, and after these switches, vs
order has turned into
A > d > c` > B > mr > C1 > C2 .
We still have to ensure that d gains a vote over every candidate in A  T . So,
DodgsonScore(d)  kBk + kC1 k + 2 + kA  T k.
Now consider mr . Since c` is the leftmost candidate in T and for every c  T , c L mr ,
it holds that T >v mr . Since mr is a weak Condorcet winner, moving mr up in the order
452

fiBypassing Combinatorial Protections

of v beyond every candidate in T makes mr a Condorcet winner and gives an upper bound
on the Dodgson score of mr .
If the top choice of v is c` itself or to the left of c` , then A  T = . In this case, moving
mr up in the order of v beyond every candidate in T gives
A > mr > c` > B > C1 > d > C2 .
If the top choice of v is to the right of c` , then for every candidate c  A, c` L c L mr . It
follows by Claim 3.7.1 that A = A  T . In this case, moving mr up in the order of v beyond
every candidate in T gives
mr > A > c` > B > C1 > d > C2 .
In both cases, we use kBk + 1 + kA  T k switches on v. This clearly makes mr a Condorcet
winner, and
DodgsonScore(mr )  kBk + 1 + kA  T k < DodgsonScore(d).
q Claim 3.6

It follows that d is not a Dodgson winner.

q Theorem 3.4
Both claims in Theorem 3.4 contrast directly with the known p2 -completeness of the
general-case Dodgson (Hemaspaandra et al., 1997) and strongYoung (Rothe et al., 2003)
winner problems, and thus reflect a substantial complexity simplification that holds when
electorates are single-peaked. In this section we have focused on the election systems of
Dodgson, Kemeny, and Young, which are natural, important and were the first three election
systems to be proven to have p2 -complete winner problems (for at least one of their strong
or weak variants). We commend to the reader the issue of obtaining, for other election
systems with hard winner problems, reductions in winner complexity for the single-peaked
case.
Although Theorem 3.4 is about winners rather than about bribery/manipulation/control
protections, its proof is a good, simple example of this papers theme that single-peakedness
tames combinatorial explosions. Taking Dodgson as an example: In the general case (not
necessarily single-peaked votes), the set of paths to potentially implement best Dodgson
scores is combinatorially explosive (to the best of current knowledge). In contrast, in the
single-peaked case in searching for paths to implement best Dodgson scores it turns out
we can restrict ourselves to changing just two voters in a particularly simple way that yields
a polynomial-sized set of options in our search space.

4. Bribery of Single-Peaked Elections
This section shows that single-peakedness undercuts many, although not all, NP-hardness
protections for bribery problems.
453

fiBrandt, Brill, Hemaspaandra, & Hemaspaandra

4.1 Notions
All bribery notions presented here, except negative approval bribery, are from the paper that started the complexity-theoretic study of bribery (Faliszewski, Hemaspaandra, &
Hemaspaandra, 2009). Given an election system E, the E-bribery problem takes as input
C, V , p  C, and k  {0, 1, 2, . . .}, and asks if, by changing the votes of at most k members of V , p can be made a winner of this election with respect to E. That is the basic
bribery problem. And it can be modified by any combination of the following items. $
means each voter has a price (belonging to {1, 2, 3, . . .}) and the question is whether there
is some set of voters whose total price is at most k such that by changing their votes we
can make p be a winner. The intuition for prices is that some voters can be swayed more
easily than others. Weighted means each vote has a weight (belonging to {1, 2, 3, . . .}),
and each weight w vote is bribed as an indivisible object, but when applying E, is viewed
as w identical regular votes. The intuition for weights is that in some electionse.g., by
stockholdersvoters have differing weights.
For the case where V consists of linear orders, by negative we mean that if we bribe a
voter then after that bribe the voter must not have p as his or her top choice unless p already
was the top choice before the bribe.7 The intuition is that in negative bribery one is trying
to stay under the radar by not directly helping ones candidate. For voting by approval
vectors, we give definitions to capture both the analog of the linear-preference negative
notion we just defined (negative) and of the one one would get by taking Faliszewski
et al. (2009) utterly literally (strong negative)see Footnote 7 for more background.
For approval-vector votes, by negative we mean that when you bribe a voter, his or her
after-bribe vector can approve p only if his or her before-bribe vector approved p. By
strongnegative we mean that when you bribe a voter the voter after being bribed cannot
approve p. The notions described above can occur in any combination, e.g., we can speak
of Llull-negative-weighted-$bribery.
When we speak of the single-peaked case of any of the above, we mean the electorate
is single-peaked, and an L relative to which the votes are single-peaked is itself part of
the input. Further, all bribes must result only in votes that are consistent with the input
societal order L. Taking L as part of the input, and as binding the legal bribes, is the
natural bribery analog of the manipulation model of Walsh (2007) and Faliszewski et al.
(2011). Binding the bribes to respect L is natural, e.g., if L is widely known, the central
authority may simply reject (as obviously manipulative votes) votes that violate L. But
although this is our core model, many of our results carry over to models more flexible on
these points, and we will at times point that outsee Footnote 8, Footnote 9, and the final
paragraph of Section 5.1.
7. The Faliszewski et al. (2009) definition of negative bribery more naturally can be read to have the quite
different semantics that each bribed voter must, after the bribe, not have p as his or her top choice.
Since that paper used negative bribery only for plurality, this issue made no difference in that paper, and
indeed since we look at negative bribery for linear orders here mostly with respect to weakCondorcet
in single-peaked contexts, it is not a key issue here either. But we have switched to a definition that
captures a more attractive notion: You cannot directly boost your preferred candidate p to the top, but
for votes where p is already at the top you can shift the remaining preferences. The distinction between
these two approaches to negative does change proofs for the case of approval voting, and so for that
we will give separate definitions that capture each notion.

454

fiBypassing Combinatorial Protections

We will often speak about bribery (or, later, manipulation and control) problems being
in P (or in polynomial time). Although formally that just asserts that a P algorithm
exists to say whether a successful bribery (or manipulation or control) action exists, in fact
in every such instance we in our proof show how to in addition obtain an actual, successful
bribery (or manipulation or control) action when one exists. The reason this is worth
mentioning is that, even in the context of elections, search can plausibly be harder than
decision (Hemaspaandra, Hemaspaandra, & Menton, 2013).
4.2 Approval-Bribery Results
As our spotlight result for approval-bribery, we will prove that the bribery protection that
complexity gives there fails on single-peaked electorates.
Theorem 4.1 (Faliszewski et al., 2009). Approval-bribery is NP-complete.
Theorem 4.2. Approval-bribery is in P for single-peaked electorates.8
Before we prove Theorem 4.2, we will informally explain what key challenge (namely,
incomparability) exists regarding proving it and how our proof overcomes that challenge
(namely, by using directionality).
So, recall that in approval bribery in the single-peaked setting, the societal order, L, is
part of the input and each voter approves of some (possibly empty) set of candidates that are
contiguous with respect to L. Suppose the input linear order L is c1 L c2 L c3 L    L c100
with respect to which the society is single-peaked. Suppose the candidate the briber is trying
to make win is c25 . Suppose that the input limit on the number of people the briber can
bribe is 2009 and suppose our input election has 5000 voters of which 3000 (call them V )
initially do not approve of c25 and 2000 (call them V+ ) initially do approve of c25 . Now,
clearly, we will not spend any of our 2009 bribes on voters from V+ , as those voters already
approve of c25 , and so bribing a voter from V+ is never better than bribing a voter from
V . So, our goal is to seek a good set of 2009 voters from V , if such exists.
The key challenge, even given single-peakedness, can be stated in a word: incomparability. That is, given that we know that the number of approvals for c25 will go up by exactly
2009 after the bribe, and given we know the total number of approvals each other candidate
gets before the bribe, we for each candidate ci other than c25 have a target number ni
such that among the 2009 votes we choose to bribe from V , at least ni must initially have
approved of ci (in order for c25 to beat ci after the bribe).
Now, here is the rub. Consider two voters from V , one of whom approves of c30 through
c55 and the other of whom approves of c40 through c80 . Among these two votes, only the
former helps us address positive ni values for 30  i  39, and only the latter helps us
address positive ni values for 56  i  80. Since neither voters approval set contains
the others, they offer differing advantages, and neither is, at first glance, obviously one
we should include in our 2009. And in fact the 3000 voters of V are a thicket of such
incomparabilities. Indeed, trying to find a subset of size 2009 (in this particular example,
8. This result holds both in the model where L is part of the input and the model in which we must find
an L consistent with the input and relative to which bribery is possible. The result also holds even in
the modelnot our core model for which we will prove itin which the bribed voters need not respect
the societal order after being bribed.

455

fiBrandt, Brill, Hemaspaandra, & Hemaspaandra

the number will vary with the input) feels very much like a covering problem, which in fact
is exactly the path by which the general case was proven NP-hard by Faliszewski et al.
(2009).
However, we will use single-peakedness to tame the combinatorial challenge of choosing
a good subset. We in particular will use single-peakedness to induce a directional attack
that will locally make incomparability disappear at each moment at which we need to make
a decision about choosing some of the 2009. (Although the focus here is bribery, and our
construction and arguments are tailored to that, we mention that our line of attack is
modeled on what Faliszewski et al., 2011, call smart greedy algorithms and that they
use to study control attacks.) Our full proof is in the appendix, but the key idea can be
easily conveyed if the reader will be so kind as to visualize along with us for a moment.
Consider the largest i such that ni > 0, and (for the purpose of this example) suppose
that i is strictly greater than 25let us say that it is 93, and suppose n93 = 3. Then we
must include in the 2009 at least three members of V who approve of c93 . But which
three? Isnt incomparability still a problem? No! Since we chose the largest i for which
ni > 0, clearly n94 =    = n100 = 0. So although among the voters in V who approve
of c93 there may be incomparability of approval ranges, range differences to the right of
c93 are utterly irrelevant, as c25 is already beating all those candidates anyway. The only
interesting issue is, among the voters of V who approve of c93 , what their leftmost approved
candidate isthe more leftward the better as that will help on the most possible ni deficits.
But now we can do direct comparisons and take action: We will put into our 2009 the
three voters (among the V voters who initially approve of c93 ) whose approval range is the
leftmost. (If V lacks three voters who approve c93 , then since n93 = 3, successful bribery
is impossible.) And the process now continues as one would expect: Based on those three
votes, all ni values are updated and the next leftmost ni > 0 satisfying i  26 is similarly
handled, and so until all ni > 0 with i  26 are handled, and then starting from the other
end of L we will analogously handle c1 through c24 . If we neutralize all ni > 0 within
2009 bribes among V , we have a successful bribery, and otherwise none is possible. This
concludes our example of how single-peakedness creates a directionality that tames the rich
covering problem caused by incomparability.
In fact, this example is essentially (if one removes the particular integer values we
used) a complete proof for the case that k  kV k. And note that if k  kV k, we can
always make p a winner by bribing all voters from V , since after the bribe all voters approve
of p. The following proof of Theorem 4.2 gives a more careful exposition of the process.
However, a reader comfortable with the somewhat informal presentation just given may
wish to at least initially simply skip over the following detailed proof.
Proof of Theorem 4.2. Let (C, V ) be an instance of a single-peaked election where the
societal order L is given by c1 L c2 L    L ckCk and let k be our bribe limit. We have to
decide whether a designated candidate p  C can be made an approval winner by bribing
at most k voters. Without loss of generality, we can assume that after the bribe, all bribed
voters approve of p and disapprove of all other candidates.
Partition the multiset V of all voters into the multiset V+ of voters that approve of p
and the multiset V of voters that disapprove of p. If k  kV k, we can obviously make p a
winner by bribing all voters from V , since after the bribe all voters approve of p. (The only
case in which we need to bribe a voter from V+ is if everybody approves p (i.e., V = ),
456

fiBypassing Combinatorial Protections

there exists a candidate other than p that is also approved by every voter, and we want
to make p the unique approval winner; in this case, bribing one arbitrary voter obviously
suffices.)
Now assume that k < kV k. Without loss of generality, we can assume that we bribe
exactly k voters, because there is a successful bribe that involves at most k voters if and only
if there is a successful bribe that involves exactly k voters. We can also argue that bribing
a voter from V+ is never more profitable than bribing a voter from V . The reason is that
for each ci  C  {p}, bribing a voter from V+ lowers the quantity number of approvals
for ci  number of approvals for p by at most 1, whereas bribing a voter from V lowers
this quantity by at least 1. Thus we assume without loss of generality that we bribe only
voters from V and we know that after the bribe, p has exactly b
k = kV+ k + k approvals.
For each candidate ci  C  {p}, let Vci be the multiset of voters that approve of ci and
define the surplus ni of ci as ni = kVci k  b
k. In order to make p a winner, we have to bribe
at least ni voters from Vci  V for all candidates ci that have a positive surplus.
Let us first consider candidates to the right of p, i.e., candidates c with p L c. In order
to avoid incomparability problems, we start at the right end of L. Let ci be the rightmost
candidate that has a positive surplus ni > 0. We know that we have to bribe at least ni
voters from Vci  V , but the question is which ones. As nj  0 for all j > i, we can solely
focus on candidates to the left of ci and bribe the ni voters from Vci  V whose approval
range extends the furthest to the left of ci . After the bribe, those voters approve of p only.
We have thereby achieved that ni = 0 and it is clear that our choice was optimal in the sense
that no other choice would have removed a greater number of approvals from candidates
other than p (ignoring candidates to the right of ci ).
We now update the surplus for all candidates and move to the next rightmost candidate ci0 to the right of p with a positive surplus. (Observe that i0 < i because for all j > i,
nj was initially nonpositive and the surplus of a candidate never grows when bribing a voter
to approve only of p.) We repeat this procedure until all candidates to the right of p have
a nonpositive surplus, at which point we mirror the societal order and repeat the whole
process, thereby handling all candidates c with c L p.
If we exceed our bribe limit k during this process, there cannot be a successful bribery
action: All the choices we have made during the algorithm are provably at least as good
as any other choice would have been. If, on the other hand, bribing k 0  k voters suffices
to make all surpluses nonpositive, we bribe (k  k 0 ) additional voters chosen arbitrarily
from V (to ensure that p has b
k approvals) and have thereby found a successful bribery
action.
q Theorem 4.2
By the same general approach used to prove Theorem 4.2using a directional attack
to in the single-peaked setting tame the incomparability challenges of covering problems
we can establish the following two additional cases in which NP-hard bribery problems fall
to P for the single-peaked case.
Theorem 4.3. The following hold:9
9. The claim is in our standard model: nonunique-winner model (i.e., we ask if the preferred candidate p can
be made to be a winner); the societal order L is given as part of the input; and bribed voters must still
respect L. However, we note in passing that the claim still holds with any of the choices one can make

457

fiBrandt, Brill, Hemaspaandra, & Hemaspaandra

1. Approval-negative-bribery and approval-strongnegative-bribery are NP-complete.
2. For single-peaked electorates, approval-negative-bribery and approval-strongnegativebribery are in P.
Faliszewski, Hemaspaandra, and Hemaspaandra (2011a, see also Faliszewski, Hemaspaandra, & Hemaspaandra, 2011b) have observed that the constructions in our proofs of
Theorem 4.2 and of Theorem 4.3 (part 2) can be used to show that those results hold even
in the more flexible model in which up to a logarithmic number of voters can violate the
societal linear order.
4.3 Llull-Bribery and Kemeny-Bribery Results: weakCondorcet Consistency
We now state the following eight-case result. The membership-in-P claims of Theorem 4.4
below are proven by direct algorithmic attacks using the connection between weakCondorcet
and median voters. That theorems NP-completeness claims are shown by reductions from
the NP-complete problems Knapsack and Partition.
Theorem 4.4. For single-peaked electorates, weakCondorcet-weighted-$bribery, weakCondorcet-negative-weighted-bribery, and weakCondorcet-negative-weighted-$bribery are
NP-complete, and the remaining five cases (weakCondorcet-bribery, weakCondorcet$bribery, weakCondorcet-weighted-bribery, weakCondorcet-negative-bribery, weakCondorcetnegative-$bribery) are in P.
Theorem 4.4 is most interesting not for what it says about weakCondorcet elections,
but for its immediate consequences on other election systems.
Corollary 4.5. Let E be any election system that is weakCondorcet-consistent or that is
merely weakCondorcet-consistent on single-peaked inputs. Then the three NP-completeness
and five P results of Theorem 4.4 hold (for single-peaked electorates) for E.
From our discussions earlier in the paper, Corollary 4.5 applies to the Llull, Kemeny,
Young, weakDodgson, Maximin, Schwartz, weakBlack, Fishburn, and the two variants of
Nanson due to Fishburn and Schwartz. In light of this, Corollary 4.5 is quietly establishing
a large number of claims about NP-hardness shields failing for single-peaked electorates.
For example, we have the following claims.
Theorem 4.6 (Faliszewski et al., 2009). Llull-bribery, Llull-$bribery, Llull-weighted-bribery,
and Llull-weighted-$bribery are each NP-complete.
Theorem 4.7 (follows from Corollary 4.5). For single-peaked electorates: Llull-bribery,
Llull-$bribery, and Llull-weighted-bribery are each in P and Llull-weighted-$bribery is NPcomplete.
regarding: nonunique-winner model vs. unique-winner model; L is part of the input vs. we are asking
whether there exists a valid L with respect to which a successful bribery can be accomplished; and the
bribed voters respect L model vs. the model in which the bribed voters may violate L. Seeing that the
result holds in these various alternate models requires natural modifications of the proof in some cases
(e.g., nonunique vs. unique), and requires taking advantage of specific properties of the construction in
other cases (e.g., regarding allowing bribed voters to violate L, the constructions actually only bribe
voters to end up approving zero or one candidate, and such votes are consistent with every ordering).

458

fiBypassing Combinatorial Protections

To the best of our knowledge, bribery of Kemeny elections has never been studied. Note,
however, that the winner problem for any election system E many-one reduces to each of the
eight types of bribery problems mentioned in Theorem 4.4, except with weakCondorcet
replaced by E. This holds because we can ask whether the preferred candidate wins
given the bribe limit of 0, and this captures the winner problem. So, from the known
p2 -completeness of the winner problem for Kemeny elections (Hemaspaandra et al., 2005),
we have the following result, which gives us eight contrasts of hardness (three between
p2 -hardness and NP membership and five between p2 -hardness and P membership).
Theorem 4.8. For Kemeny elections, all eight types of bribery mentioned in Theorem 4.4
are p2 -hard.
Theorem 4.9 (follows from Corollary 4.5). For single-peaked electorates, Kemeny-weighted$bribery, Kemeny-negative-weighted-bribery, Kemeny-negative-weighted-$bribery are NPcomplete (and so in particular each belongs to NP). For single-peaked electorates, the
remaining five types of bribery of Kemeny elections are in P.
As a final remark regarding Theorem 4.4, we note that even within the single-peaked
cases that it studies, there is one twist, in which changing from bribery to negative bribery
changes the complexity, namely, for single-peaked electorates, weakCondorcet-weightedbribery is in P but weakCondorcet-negative-weighted-bribery is NP-complete. Here, decreasing the set of bribes available to the briber actually boosts the complexity of the
bribers task. (The explanation for this is, very loosely and intuitively speaking, that
among the set of bribes that negativity removes from the search space are the set of bribes
used in the P-time nonnegative case bribery attack.)

5. Control by Partition of Single-Peaked Electorates
The control problems for elections ask whether by various types of changes in an elections
structure a given candidate can be made a winner. (In some papers, seeking to make a
candidate a winner through structural changes is called constructive control to distinguish
it from the destructive case in which we are trying to preclude a candidate from winning.
However, in this paper we always use control in the constructive sense, unless we explicitly
mention otherwise.) The types of control that were introduced in 1992 by Bartholdi, Tovey,
and Trick, and that (give or take some slight refinements) have been studied in subsequent
papers, are addition/deletion/partition of voters/candidates. However, to the best of our
knowledge previous study of the complexity of control for single-peaked electorates (such as
that of Faliszewski et al., 2011) focused exclusively on additions and deletions of candidates
and voters.
We for the first time study the complexity of partition problems for the case of singlepeaked electorates. And we show that for a broad range of election systems the control by
partition of voters problem is in P for single-peaked electorates. Among the systems we do
this for are Llull and Condorcet elections, whose control by partition of voters problem is
known to be NP-complete for general electorates. Our proofs again work by using singlepeakedness to tame combinatorial explosionin this case, the number of partitions that
must be examined is reduced from an exponential number of partitions to a polynomial
number of classes of partitions each of which can be checked as a block.
459

fiBrandt, Brill, Hemaspaandra, & Hemaspaandra

5.1 Notions
We will now define the key types of control that we will study: control by partition of
voters, control by adding voters, and control by deleting voters.
Partition of voters models the case where the partitioned electorate has primaries. An
example from Faliszewski et al. (2009a) is a business group where a powerful manager studies
an issue by splitting his or her group into two task forces each to (by voting) recommend
alternatives to be part of a final vote to be conducted by the entire group. This (loosely)
corresponds to control by partition. Control by adding voters loosely models such actions
as targeted get-out-the-vote drives. Control by deleting voters loosely models such actions
as targeted attempts at voter suppression.
For the partition case, in which there are two first-round elections and a second-round
election, there are two different approaches to which candidates move forward from each
first-round election. One is the Ties Promote (TP) model, in which all winners of a firstround election move forward. The other is the Ties Eliminate (TE) model, in which, for
each first-round election, its unique winner moves forward if it has a unique winner and
otherwise no one moves forward from that first-round election. For consistency, these control
definitions are adopted, often word-for-word, from those in the papers by Hemaspaandra
et al. (2013), Faliszewski et al. (2009a), and Faliszewski, Hemaspaandra, and Hemaspaandra
(2014).
Definition 5.1. Let E be an election system.
1. In the control by partition of voters problem for E, in the TP or TE tie-handling rule
model, we are given an election (C, V ) and a candidate p  C. Is there a partition10 of
V into V1 and V2 such that p is a winner of the two-stage election where the winners
of election (C, V1 ) that survive the tie-handling rule compete against the winners of
(C, V2 ) that survive the tie-handling rule? Each of the two first-round and the one
second-round elections is conducted using election system E.11
2. In the control by adding voters problem for E we are given a set of candidates C,
two collections of voters, V (often referred to as the collection of registered voters)
and W (often referred to as the collection of unregistered voters), with preferences
over C, a candidate p  C, and a nonnegative integer K. We ask whether there is a
subcollection W 0  W such that (a) kW 0 k  K, and (b) p is a winner of E election
(C, V  W 0 ).
3. In the control by deleting voters problem for E we are given an election (C, V ), a
candidate p  C, and a nonnegative integer K. We ask whether there is a collection
V 0 of voters that can be obtained from V by deleting at most K voters such that p is
a winner of E election (C, V 0 ).
10. A partition of a collection A is a pair of collections (A1 , A2 ) such that A1  A2 = A and A1  A2 = ;
since different voters can have the same preferences, these are multiset operations.
11. It is very important to note that in this definition and when we draw on it, when we speak of an election,
(C 0 , V 0 ), we always implicitly mean that each vote in V 0 is passed to the election system only as the
version of itself restricted to the candidates in C 0 . In particular, this is relevant to the second-round
election here.

460

fiBypassing Combinatorial Protections

The above three definitions are all for what is called the nonunique-winner model,
namely, the question is: Can p be made a winner of the final election? Another model
that has been studied in the literature is called the unique-winner model, in which the
questions above are replaced with: Can p be made to uniquely win the final election? We
find most natural for study the TP, nonunique-winner model. (TP and nonunique-winner
pair naturally, as do TE and unique-winner.) In contrast, the seminal control paper of
Bartholdi et al. (1992) used the unique-winner model. To be as clear and broad as possible
as to which of these models our results hold in, we have checked that our results hold for all
four model combinations (i.e., TP and unique-winner, TP and nonunique winner, TE and
unique-winner, and TE and nonunique-winner).
When we speak about a control problem for single-peaked electorates, we will mean that
the societal order L is part of the input. And we mean that single-peakedness must hold
for the entire input (including any potentially added candidates and voters). However, for
control, it turns out that in the just-mentioned model polynomial-time membership holds if
and only if polynomial-time membership holds in the model in which one is not given an L
as part of the input but rather one is asked whether there exists some linear order L relative
to which the input (as before, even including all potentially added candidates and voters)
is single-peaked and is such that the goal can be achieved by the given control action. This
claim is formalized as Theorem A.5 and proven in the appendix. In light of Theorem A.5,
we will simply assume all our control results are in the default model (societal order is part
of the input).
5.2 Control Results Related to weakCondorcet Elections
In this section we present our control results, with a focus on control by partition of voters. We will see that although Llull and Condorcet elections have NP-hard voter-partition
control problems, those problems fall to polynomial time for single-peaked electorates. Our
spotlight result for this section states that partition-by-voters control for weakCondorcet
elections is in P.
Theorem 5.2. For weakCondorcet elections, control by partition of voters is in P for singlepeaked electorates, in both the nonunique-winner model and the unique-winner model, and
in both the Ties Promote model and the Ties Eliminate model.
Before giving Theorem 5.2s proof, let us note some consequences and contrasts.
Corollary 5.3. Let E be any election system that is weakCondorcet-consistent on singlepeaked inputs. Then for election system E, control by partition of voters is in P for singlepeaked electorates, in both the nonunique-winner model and the unique-winner model, and
in both the Ties Promote model and the Ties Eliminate model. In particular, this holds for
the election systems Llull, Kemeny, Young, weakDodgson, Maximin, Schwartz, weakBlack,
Fishburn, and the two variants of Nanson due to Fishburn and Schwartz.
For Llull elections, this provides a clear contrast with the known NP-completeness for
that same control type in the general case.
Theorem 5.4 (Faliszewski et al., 2009a). For Llull elections, control by partition of voters
is NP-complete, in both the nonunique-winner model and the unique-winner model, and in
both the Ties Promote model and the Ties Eliminate model.
461

fiBrandt, Brill, Hemaspaandra, & Hemaspaandra

Algorithm 2 weakCondorcet Control by Partition of Voters
1: for all a, b, c, d  C such that a L b, c L d, a L c, and p is a weakCondorcet winner
in (C 0 , V ) with C 0 = {e  C | a L e L b  c L e L d} do
2:
for all k  {0, 1, 2, . . . , kV k} do
3:
let r be the number of interesting regions
P
4:
for all ` = (`1 , `2 , . . . , `r )  Nr with ri=1 `i = k do
5:
define P` as the set of partitions (V1 , V2 ) such that for each i, `i is the
number of voters in V1 whose top choice is in Ri
6:
if P` 6=  then
7:
let (V1 , V2 ) be an arbitrary partition from P`
8:
if the set of weakCondorcet winners of (C, V1 ) equals [a, b] and the
set of weakCondorcet winners of (C, V2 ) equals [c, d] then
9:
return (V1 , V2 )
10: return there is no partition of voters that makes p a weakCondorcet winner

We now turn to the proof of Theorem 5.2. The idea behind the proof differs completely
from the approach used in the polynomial-time control proofs in Faliszewski et al. (2011),
and is, we think, novel.
Proof of Theorem 5.2. Let (C, V ) be an election and L a linear order of C with respect
to which the electorate is single-peaked. We have to decide whether a designated candidate
p  C can be made an overall winner by partitioning the set of voters in an appropriate
way.
Our algorithm is tailored to the natural Ties Promote, nonunique-winner model, but
we will at the end of the proof mention how it can be adapted to the other models. In our
natural model, we want to find a partition (V1 , V2 ) such that p is a weakCondorcet winner
in the election (C 0 , V ) where C 0 is the union of the weakCondorcet winners in (C, V1 ) and
(C, V2 ).
We show that Algorithm 2 returns a partition with this property whenever one exists.
Algorithm 2 loops over all 4-tuples a, b, c, d of candidates and tests whether the voters can
be partitioned into (V1 , V2 ) in such a way that (a) the weakCondorcet winners of (C, V1 )
are [a, b] (i.e., all candidates in {x  C | a L x L b}, writing y L z for (y = z or y L z)),
(b) the weakCondorcet winners of (C, V2 ) are [c, d], and (c) p is a weakCondorcet winner in
([a, b]  [c, d], V ).
For each 4-tuple a, b, c, d, we divide the set C of candidates into disjoint interesting
regions. Regions are defined as follows. Each of the candidates a, b, c, d constitutes a
region in itself. Furthermore, each contiguous (with respect to L) interval between two
of those four candidates is a region. Finally, there are two additional regions, namely one
interval consisting of all candidates that are to the left of a, b, c, d and one interval consisting
of all candidates that are to the right of a, b, c, d.
Note that all of the intervals not containing a, b, c, or d may be empty, because the
set {a, b, c, d} may contain adjacent or even identical candidates. It is easy to see that the
number of interesting (i.e., nonempty) regions is at most nine, and is equal to nine if and
only if there are no adjacent or identical candidates among a, b, c, d. Assuming that this is
462

fiBypassing Combinatorial Protections

the case, there are three possible situations, depending on the relation between the intervals
[a, b] and [c, d].
1. The intervals are disjoint:
a
R1

R2

c

b
R3

R4

R5

R6

d
R7

R8

R9

2. The intervals have a nonempty intersection, but neither contains the other:
a

R1

R2

c

R3

R4

b

R5

R6

d

R7

3. One interval contains the other:
a
c
d

R1

R2

R3

R4

R5

R6

R8

R9

b

R7

R8

R9

If the set {a, b, c, d} contains adjacent or identical elements, some of the regions pictured
above will be empty or identical, and so there will be less than nine interesting regions. If
there are r interesting regions, we will denote them by R1 , . . . , Rr , from left to right with
respect to L.
Associate each voter with the candidate at the top of that voters preference order. The
following observation turns out to be helpful. If [a, b] is the set of weakCondorcet winners
in (C, V1 ), then there can be no voters in V1 that have a top choice x with a <L x <L b.
Similarly, no voter in V2 can have a top choice strictly between c and d. That is, a region
that consists of voters whose top choice lies strictly between either a and b or c and d is fully
determined with regard to the question of how many voters are in V1 and V2 . For example,
in the case where [c, d] is contained in [a, b], five of the nine regions are fully determined:
All voters from R3 , R4 , R6 , and R7 have to be in V2 and there must not be any voters in
region R5 , as such voters would lie both between a and b and between c and d. By this
argument, one can see that the maximum number of regions that are not fully determined
is 7 (in the case where the intervals [a, b] and [c, d] are disjoint). Clearly, the number of
ways that kV k can be divided into 7 ordered numbers is bounded by kV k6 .
The fact that the weakCondorcet winners in (C, V1 ) and (C, V2 ) can be efficiently checked
is due to the following key observation. Within each region, the only thing that affects the
winner set is the number of voters we put into V1 , not which voters we use to achieve
that number. That is, we do not need to check each partition individually (there are an
exponential number of them), but rather deal with a large number of partitions simultaneously. More formally, suppose we have r interesting regions and let ` = (`1 , `2 , . . . , `r ) be
463

fiBrandt, Brill, Hemaspaandra, & Hemaspaandra

an r-dimensional vector of natural numbers. We now define P` as the set of all partitions
(V1 , V2 ) of V such that for each i, `i is the number of the voters from V1 whose top choice
is in Ri .
For P` 6= , our key observation can now be restated as follows. [a, b] is the set of
weakCondorcet winners in some election (C, V1 ) with (V1 , V2 )  P` if and only if [a, b] is
the set of weakCondorcet winners in every election (C, V1 ) with (V1 , V2 )  P` . That is, if
we want to check whether [a, b] is the set of weakCondorcet winners in any of the primary
elections (C, V1 ) induced by any of the partitions (V1 , V2 )  P` , it suffices to check just one
of them to obtain the answer. This check can easily be done just by counting. By symmetry,
the same statement holds for [c, d] and for weakCondorcet winners of the elections (C, V2 )
(all four combinations).
The reason why this is true is that, given the number of voters in each region, it is easy
to recognize the region(s) of the median voter(s) (just by counting). Since a, b, c, and d all
constitute a region on their own, it is equally easy to tell whether a median voter has any
of these four candidates as his or her top choice.
Implementing this idea, the algorithm loops over all possible sizes k of V1 (line 2) and
over all possibilities how k can be divided into r numbers `1 , `2 , . . . , `r , and checks (line 8)
if this gives a partition as required. As we have argued in the last paragraphs, this query
can be efficiently answered.
The running time of Algorithm 2 can thus be bounded as follows. The number of
iterations for the loops in lines 1, 2, and 4, are bounded by kCk4 , kV k + 1, and kV k6 ,
respectively. Moreover, we have just shown that the queries in line 8 can be answered in
polynomial time. Altogether, this yields a running time that is obviously polynomial in the
size of the input.
Correctness of Algorithm 2 should be clear from the explanations above: We find a
partition that makes p an overall weakCondorcet winner if and only if such a partition
exists. In particular, observe that setting k = 0 in line 2 handles the case where p is
already a weakCondorcet winner in the original election.12 Note that the algorithms theme
perfectly supports the theme of this paper: The algorithm used single-peakedness to bypass
the combinatorial richness of partitions.
This completes the proof for the TP, nonunique-winner model. For the TP, uniquewinner model, line 1 of Algorithm 2 needs to be adapted to only loop over all choices of
a, b, c, d that make p a unique weakCondorcet winner in (C 0 , V ). The TE, unique-winner
model is dealt with in Theorem 5.6 on page 465. (By Fact 3.1 on page 448, being a
unique weakCondorcet winner is tantamount to being a Condorcet winner for single-peaked
electorates.)
The same is true for the TE, nonunique-winner model. Here, being a weakCondorcet
winner in the final election suffices, but Algorithm 3 (which can be found in the appendix)
can easily be adapted to take that into consideration.
q Theorem 5.2
12. To see this, assume that p  [m` , mr ], where [m` , mr ] is the interval of all weakCondorcet winners in
(C, V ). Observe that if we set a, b, c, d such that [a, b] = C and [c, d] = [m` , mr ], then C 0 = {e  C |
a L e L b  c L e L d} = C and obviously p is a weakCondorcet winner of (C 0 , V ). Thus, this
choice of a, b, c, d is considered in line 1. Now setting k to 0 uniquely defines ` to be (0, . . . , 0) and P`
consists of the partition (, V ) only. Due to the choice of a, b, c, d, the answer to the query in line 8 is
yes and (, V ) is correctly output as a partition that makes p an overall winner.

464

fiBypassing Combinatorial Protections

Before we turn from weakCondorcet to Condorcet elections, we state a result that will
quickly give us a number of additional contrasts between general-case control complexity
and single-peaked control complexity.
Theorem 5.5. For weakCondorcet elections, control by adding voters and control by deleting voters are each in P for single-peaked electorates, in both the nonunique-winner model
and the unique-winner model.
As usual, it immediately follows that the above result applies to our standard long
list of systems (e.g., Kemeny, Young, and weakDodgson elections) that are weakCondorcetconsistent on single-peaked electorates. The appendix contains similar results for Condorcet
elections. However, the winner problem in the general case trivially many-one polynomialtime reduces to control by adding voters (via asking if p can be made to win by adding 0
voters; see Hemaspaandra et al., 2009, Section 2.4). Thus, the existing p2 -hardness results
for the Kemeny winner problem (both in the nonunique-winner model, Hemaspaandra et al.,
2005, and in the unique-winner model, Hemaspaandra et al., 2009), the Young winner
problem (both in the nonunique-winner model and in the unique-winner model, see this
papers Theorem A.2), and the weakDodgson winner problem (both in the nonunique-winner
model and in the unique-winner model, see this papers Theorem A.4), themselves imply
that control by adding voters is p2 -hard for Kemeny, Young, and weakDodgson elections
(in both the nonunique-winner model and the unique-winner model). The same comments
hold for control by deleting voters. Thus the single-peaked and general cases of control by
adding and deleting voters differ for Kemeny, Young, and weakDodgson elections.
5.3 Control Results Related to Condorcet Elections
Control of Condorcet elections has been studied in much detail (Bartholdi et al., 1992;
Hemaspaandra, Hemaspaandra, & Rothe, 2007), and (see Table 1 of Hemaspaandra et al.,
2007) each standard control type is known to either never change the outcome at all or
to have a polynomial-time algorithm, with three exceptions. Namely, as Bartholdi et al.
(1992) proved in their seminal paper on control, control by addition of voters and control by
deletion of voters are both NP-complete for Condorcet elections; and control by partition of
voters is also NP-complete for Condorcet elections (due to Bartholdi et al., 1992, for a nownonstandard partition model, and due to Faliszewski et al., 2009a, for the now-standard
partition model).13 However, the following results show that these resistance results vanish
for single-peaked electorates.
Theorem 5.6. For Condorcet elections, control by partition of voters is in P for singlepeaked electorates, in both the nonunique-winner model and the unique-winner model, and
in both the Ties Promote model and the Ties Eliminate model (note that all four cases
coincide here).
13. That entire Bartholdi, Tovey, and Trick paper is in the unique-winner model, and so all the above discussion is in the unique-winner model. And we will thus need to establish our contrasting polynomial-time
results in the unique-winner model if we want a meaningful contrast. To address this, we will ensure
that our contrasting results hold in both models. But that holds trivially if we prove it in either model,
as for Condorcet elections, nonunique-winner and unique-winner coincide and Ties Promote and Ties
Eliminate coincideboth because one can never have two or more winners.

465

fiBrandt, Brill, Hemaspaandra, & Hemaspaandra

Theorem 5.7. For Condorcet elections, control by adding voters and control by deleting
voters are each in P for single-peaked electorates, in both the nonunique-winner model and
the unique-winner model.
So, for all the standard control cases that Condorcet voting is known to be NP-hard for
in the general case (Bartholdi et al., 1992; Faliszewski et al., 2009a), we have shown that
Condorcet-voting control falls to polynomial time for single-peaked electorates.14

6. Manipulation of Single-Peaked Electorates: Dichotomy for
Constructive Coalition Weighted Manipulation
Faliszewski et al. (2011) completely characterized, for three-candidate elections, which scoring protocols have polynomial-time constructive coalition weighted manipulation problems
and which have NP-complete constructive coalition weighted manipulation problems. We
achieve a far more sweeping dichotomy theoremour result applies to all scoring protocols,
regardless of the number of candidates. Scoring protocols are arguably the most important
broad class of election systems.
In the constructive coalition weighted manipulation problem, the input is the candidate
set C, the nonmanipulative voters (each a preference order over C and a weight), the
manipulative voters (each just a weight), and a candidate p  C, and the question is
whether there is a way of setting the preferences of the manipulative voters such that p is a
winner under the given election rule when all the manipulative and nonmanipulative voters
vote in a weighted election.
Theorem 6.1 (Faliszewski et al., 2011). Consider a three-candidate scoring protocol,
namely,  = (1 , 2 , 3 ), 1  2  3 , 1  N, 2  N, 3  N. For the single-peaked case,
the constructive coalition weighted manipulation problem (in both the nonunique-winner
model and the unique-winner model) is NP-complete when 1  3 > 2(2  3 ) > 0 and is
in polynomial time otherwise.
Our extension of this three-candidate, single-peaked electorate result to the case of any
scoring protocol over single-peaked electorates is somewhat complicated. Yet, since it is
a complete characterizationa dichotomization of the complexities, in factit is in some
sense simply reflecting the subtlety and complexity of scoring systems. (For the general
i.e., not necessarily single-peakedcase, the known characterization is simple regardless
of the number of candidates: NP-completeness holds when kCk  2 and 2 6= kCk and
otherwise the problem is in P (Hemaspaandra & Hemaspaandra, 2007, see also Conitzer,
Sandholm, and Lang, 2007, and Procaccia and Rosenschein, 2007.) The following theorem
is this sections soleand spotlightresult.
Theorem 6.2. Consider an m-candidate scoring protocol, namely,  = (1 , 2 , . . . , m ) 
Nm , 1  2      m .
14. The sharp-eyed reader may wonder whether it is in concept possible that some of the general-case
polynomial-time results for control (e.g., Condorcet control by deleting candidates) might suddenly,
freakishly get harder in the single-peaked case. After all, Faliszewski et al. (2011) show a freakish case
where limiting to the single-peaked case increases manipulation complexity. However, it is not hard to
seeby reasoning related to that used to prove Theorem A.5that if a control type is in polynomial
time in the general case then in the single-peaked case it remains in polynomial time.

466

fiBypassing Combinatorial Protections

 If m  2 and 2 > b m1 c+2 and there exist integers i, j > 1 such that i + j  m + 1
2
and (1 i )(1 j ) > (i i+1 )(j j+1 ), then the constructive coalition weighted
manipulation problem for the single-peaked case is NP-complete.
 If m  2 and 2 = b m1 c+2 and 1 > 2 > m and (2 > m1 or 1  m >
2
2(2  m )), then the constructive coalition weighted manipulation problem for the
single-peaked case is NP-complete.
 In all other cases, the constructive coalition weighted manipulation problem for the
single-peaked case is in P.
For example, the constructive coalition weighted manipulation for the single-peaked case
for m-candidate plurality and m-candidate veto is in P, and for m-candidate Borda it is in
P if m  3 and NP-complete otherwise.
Note that Theorem 6.1 follows from Theorem 6.2, since 1 3 > 2(2 3 ) is equivalent
to 1  2 > 2  3 . We also note that the specific cases of more-than-three-candidate
scoring protocolssuch as four-candidate Borda and m-candidate vetothat were analyzed
for the single-peaked case by Faliszewski et al. (2011) yielded results completely consistent
with Theorem 6.2s characterization. The P cases of Theorem 6.2s dichotomy align with
our theme of single-peakedness often foiling combinatorial protections.
Proof of Theorem 6.2. We first give some intuition for the conditions of this theorem.
The P cases are exactly the cases where there is an optimal manipulator vote. For example,
we will show in Lemma 6.6 that if (1  i )(1  j )  (i  i+1 )(j  j+1 ) for all i, j > 1
such that i + j  m + 1, then the candidates scoring higher than p are all on one side of p,
say on the left, in the societal order. In that case it is optimal for the manipulators to rank
p first, then all candidates on ps right, and then all candidates on ps left.
In contrast, for the NP-complete cases there exists a societal order for which we can
construct elections where p has two main rivals, say a and b, and there are two different
types of optimal votes for the manipulators that have different effects on the scores of a
and b. Our NP-hardness proofs follow via reductions from the well-known NP-complete
problem Partition (see, e.g., Garey and Johnson, 1979). In this problem, we are given a
nonempty collection (k1 , . . . , kn ) of positive integers that sum to 2K, and we ask whether
there exists a subcollection of k1 , . . . , kn that sums to K. In all these cases we will carefully
define the societal order, the weights and votes of the nonmanipulators, and the weights
of the manipulators such that the weights of the manipulators voting for one of the two
optimal vote types in a successful manipulation correspond to a partition and vice versa.
We now turn to the formal proof of the theorem. For m  1, the problem is trivial and
thus in P. So, assume that m  2. We split the proof of Theorem 6.2 into three lemmas:
The 2 > b m1 c+2 NP-complete cases (Lemma 6.4), the 2 > b m1 c+2 P cases (follow
2
2
from Lemma 6.6), and the 2 = b m1 c+2 cases (Lemma 6.7).
2
In the proof, we use the following notation. For V a collection of voters and c a candidate,
scoreV (c) denotes the score of c in V , i.e., the number of points that c receives from the
voters in V . If V is clear from context, we will simply write score(c). In this section, we
usually denote the collection of nonmanipulators by S and the collection of manipulators
by T .
First we prove the following simple lemma.
467

fiBrandt, Brill, Hemaspaandra, & Hemaspaandra

Lemma 6.3. In the constructive coalition weighted manipulation problem for the singlepeaked case for scoring protocols, if p can be made a winner, then p can be made a winner
by a manipulation in which all manipulators rank p first.
Proof of Lemma 6.3. Let A > p > b1 >    > b` be a single-peaked vote, where
A is a set of candidates in some order. Then there exists an ordering of A such that
p > A > b1 >    > b` is also single-peaked. Note that in this new vote, for every candidate
c, score(p)  score(c) does not decrease. So, if p is a winner of an election, then p is a still
a winner if we replace every single-peaked vote of the form A > p > b1 >    > b` by a
single-peaked vote of the form p > A > b1 >    > b` .
q Lemma 6.3
Lemma 6.4. Let  = (1 , 2 , . . . , m ) be a scoring protocol such that m  2 and 2 >
b m1 c+2 . If there exist integers i1 , i2 > 1 such that i1 +i2  m+1 and (1 i1 )(1 i2 ) >
2
(i1  i1 +1 )(i2  i2 +1 ), then the constructive coalition weighted manipulation problem
for the single-peaked case is NP-complete.
Proof of Lemma 6.4. Let i1 , i2 be integers that fulfill the conditions of the lemma such
that i1  i2 and i2 is minimal, i.e., 1 < i1  i2 , i1 + i2  m + 1, (1  i1 )(1  i2 ) >
(i1  i1 +1 )(i2  i2 +1 ), and for all i01 , i02 such that 1 < i01  i02 < i2 and i01 + i02  m + 1,
it holds that (1  i01 )(1  i02 )  (i01  i01 +1 )(i02  i02 +1 ).
We will now reduce Partition to the constructive coalition weighted manipulation
problem for single-peaked electorates. Let (k1 , . . . , kn ) be an instance of Partition, i.e.,
(k1 , . . . , kn ) is a nonempty collection of positive integers that sum to 2K. Let societys
order be
am1 L    L a1 L p L b1 L    L bm2 ,
where m2 = max(d m1
2 e, i2  1) and m1 = m  m2  1. Note that i2  m2 + 1. Since
i1 +i2  m+1, it follows immediately that i1  m1 +1. Also note that 1  m1  m2  m2.
To make the reduction work, we also need the following claim.
Claim 6.5. m1 +2 < 2 .
Proof of Claim 6.5.

If m1 = b m1
2 c, this is immediate, since 2 > b m1 c+2 . If not,
2

then m2 = i2  1 and i2  1 > d m1
2 e  2. Since i2 is minimal, the conditions of the lemma
can not be fulfilled with i1 = i2 = 2, and so (1  2 )  (2  3 ). If 2 = m1 +2 , then
2 = 3 , and it follows that 1 = 2 and thus 1 = m1 +2 . But then this choice of i1 , i2
does not fulfill the conditions of the lemma, since 1  i1 = 0.
q Claim 6.5
Let S consist of two voters, one voter of weight `1 with preference order
a1 >    > ai1 1 > p > b1 >   
(the    at the end of the vote denotes that the remaining candidates are listed in arbitrary,
single-peaked order) and one voter with weight `2 with preference order
b1 >    > bi2 1 > p > a1 >    ,
468

fiBypassing Combinatorial Protections

and let the weights of T be k1 , k2 , . . . , kn , where
 = (1  i1 )(1  i2 )  (i1  i1 +1 )(i2  i2 +1 ),
`1 = ((21  2  m2 +2 )(1  i2 ) + (i2  i2 +1 )(21  2  m1 +2 ))K, and
`2 = ((1  i1 )(21  2  m1 +2 ) + (i1  i1 +1 )(21  2  m2 +2 ))K.
These values were chosen (using Cramers Rule) so that
(1  i1 )`1  (i2  i2 +1 )`2 = (21  2  m2 +2 )K and
(i1  i1 +1 )`1 + (1  i2 )`2 = (21  2  m1 +2 )K.
Note that  is positive, since (1  i1 )(1  i2 ) > (i1  i1 +1 )(i2  i2 +1 ). Note
that `1 and `2 are also positive, since (1  i1 ) and (1  i2 ) are positive, and so are
(21  2  m1 +2 ) and (21  2  m2 +2 ), by Claim 6.5 and the fact that m1  m2 .
To prove that the reduction works, first suppose that k1 , . . . , kn has a partition, i.e.,
there is a subcollection of k1 , . . . , kn that sums to K. We show that p can be made a winner
as follows. Let K weight of T vote
p > a1 >    > am1 > b1 >   
and let K weight vote
p > b1 >    > bm2 > a1 >    .
Note that for all i, 1  i  m1 , score(ai )  score(a1 ) and that for all i, 1  i  m2 ,
score(bi )  score(b1 ) and so it suffices to show that score(a1 )  score(p) and that score(b1 ) 
score(p). Note that
score(p) = i1 `1 + i2 `2 + 21 K,
score(a1 ) = 1 `1 + i2 +1 `2 + 2 K + m2 +2 K, and
score(b1 ) = 1 `2 + i1 +1 `1 + 2 K + m1 +2 K.
From the choice of , `1 , and `2 , it follows that score(a1 ) = score(p) and score(b1 ) = score(p).
For the converse, suppose that the voters in T vote such that p becomes a winner in
S  T . From the observations above, it follows that
scoreT (a1 )  (2 + m2 +2 )K and that
scoreT (b1 )  (2 + m1 +2 )K.
By Lemma 6.3 we can assume that p is ranked first by every voter in T . This implies that
every voter in T ranks a1 or b1 second. Let Wa be the total weight of all T voters that rank
a1 second. It follows that
scoreT (a1 )  2 Wa + m2 +2 (2K  Wa ) and
scoreT (b1 )  2 (2K  Wa ) + m1 +2 Wa .
From these observations, and the fact that  > 0 and that 2 > m1 +2 (by Claim 6.5) (and
thus also 2 > m2 +2 ), it follows that Wa  K and that (2K  Wa )  K. So, exactly
half of the vote weight of T ranks a1 second. Then the weights of the voters in T that rank
a1 second correspond to a partition.
q Lemma 6.4

469

fiBrandt, Brill, Hemaspaandra, & Hemaspaandra

Lemma 6.6. Let  = (1 , 2 , . . . , m ) be a scoring protocol. If for all i, j > 1 such that
i + j  m + 1, it holds that
(1  i )(1  j )  (i  i+1 )(j  j+1 ),
then the constructive coalition weighted manipulation problem for the single-peaked case is
in P.
Proof of Lemma 6.6. Let am1 L    L a1 L p L b1 L    L bm2 be societys order. It is
immediate that if scoreS (ai )  scoreS (p) for all i, 1  i  m1 , then p can be made a winner
if and only if p is a winner if every voter in T votes
p > a1 >    > am1 > b1 >    > bm2 .
Similarly, if scoreS (bi )  scoreS (p) for all i, 1  i  m2 , then p can be made a winner if
and only if p is a winner if every voter in T votes
p > b1 >    > bm2 > a1 >    > am1 .
We will show that these are the only cases that can occur. This immediately implies
Lemma 6.6.
So, for the remainder of the proof, suppose for a contradiction that we have a collection of
voters S and integers i1 and i2 such that 1  i1  m1 , 1  i2  m2 , scoreS (ai1 ) > scoreS (p),
and scoreS (bi2 ) > scoreS (p).
For 1 < i  m1 +1, let `i be the total weight of the voters in S that rank some candidate
in {a1 , . . . , am1 } first and that rank p in ith place. Note that these voters rank all candidates
in {b1 , . . . , bm2 } after p. For 1 < i  m2 + 1, let `0i be the total weight of the voters in S that
rank some candidate in {b1 , . . . , bm2 } first and that rank p in ith place. Note that these
voters rank all candidates in {a1 , . . . , am1 } after p. It follows immediately that
scoreS (ai1 )  scoreS (p) 

X

(1  i )`i +

1<im1 +1

scoreS (bi2 )  scoreS (p) 

X

X

(i+1  i )`0i and

1<im2 +1

(1 

i )`0i

+

1<im2 +1

X

(i+1  i )`i .

1<im1 +1

Since scoreS (ai1 ) > scoreS (p) and scoreS (bi2 ) > scoreS (p), it follows that
X

1<im1 +1

X

X

(1  i )`i >
(1 

1<im2 +1

(i  i+1 )`0i and

1<im2 +1

i )`0i

X

>

(i  i+1 )`i .

1<im1 +1

Since both sides of both inequalities are nonnegative, it follows that

 

X
X

(1  i )`i   
(1  i )`0i  >
1<im1 +1

1<im2 +1

470

fiBypassing Combinatorial Protections



 
X



(i  i+1 )`0i   

1<im2 +1

(i  i+1 )`i  .

1<im1 +1

Multiplying this out, it follows that
X
(1  i )(1  j )`i `0j >
1<im1 +1
1<jm2 +1


X

X

(i  i+1 )(j  j+1 )`i `0j .

1<im1 +1
1<jm2 +1

Since m = m1 + m2 + 1, this contradicts the assumption that for all i, j > 1 such that
i + j  m + 1 it holds that
(1  i )(1  j )  (i  i+1 )(j  j+1 ).
q Lemma 6.6
Lemma 6.6 handles all 2 > b m1 c+2 P cases of Theorem 6.2. Note that this lemma is of
2
limited use if 2 = b m1 c+2 , since in that case the lemma applies only if m = 2 or 1 = 2 =
2
b m1 c+2 . The 2 = b m1 c+2 cases of Theorem 6.2 are handled by the following lemma in
2
2
combination with the standard observation that the scoring protocol (1 , 2 , . . . , m ) has
the same behavior as the scoring protocol (1  m , 2  m , . . . , m1  m , 0).
Lemma 6.7. Let  = (1 , 2 , . . . , m ) be a scoring protocol such that m  2, m = 0, and
2 = b m1 c+2 . If 2 = 0 or 1 = 2 or (2 = m1 and 1  22 ) then the constructive
2
coalition weighted manipulation problem for the single-peaked case is in P; otherwise, it is
NP-complete.
Proof of Lemma 6.7. If 2 = 0, then any vote of the form p >    is an optimal
manipulator vote. If 1 = 2 , then 1 = b m1 c+2 . Let i and j be such that i, j > 1 and
2

m1
i + j  m + 1. Then min(i, j)  b m+1
2 c = b 2 c + 1 and so (1  i )(1  j ) = 0. It
follows from Lemma 6.6 that this case is in P.
For the remainder of the proof, we assume that 1 > 2 > 0.
Now consider the case that 2 = m1 and 1  22 . Consider societys order. If p is
the leftmost or rightmost candidate, then there is exactly one vote that puts p first, and
this is an optimal manipulator vote. Otherwise, let a be the leftmost candidate in societys
order, and b the rightmost. Then a and b are the only candidates that can occur last in a
vote. Since 1  22 , it follows that scoreS (a) + scoreS (b)  2scoreS (p). Without loss of
generality, let scoreS (a)  scoreS (b). Then scoreS (a)  scoreS (p) and any vote of the form
p >    > b is an optimal manipulator vote.
This concludes the P cases. We now turn to the NP-complete cases. In both cases, we
will reduce from Partition.
First assume that 2 = m1 and that 1 > 22 . Let k1 , . . . , kn be a nonempty
collection of positive integers that sum to 2K. We construct the following election: Societys
order is a L p L    L b. S consists of two voters, each of weight (21  2 )K. One
voter votes a > p >    > b and the other voter votes b >    > p > a. Note that
for all candidates c 6 {a, b}, scoreS (c) = scoreS (p). The weights of the manipulators are
(1  22 )k1 , . . . , (1  22 )kn . The proof of the correctness of the reduction is similar to

471

fiBrandt, Brill, Hemaspaandra, & Hemaspaandra

(but easier than) the corresponding proof in Lemma 6.4. First suppose that there exists a
subcollection of k1 , . . . , kn that sums to K. Then set K vote weight of T to p > a >    > b
and K vote weight to p >    > b > a. Note that in the resulting election,
score(p) = (22 (21  2 ) + 21 (1  22 ))K = (212  222 )K and
score(a) = score(b) = (1 (21  2 ) + 2 (1  22 ))K = (212  222 )K.
For the converse, suppose that p can be made a winner. From the observations above,
it is immediate that
scoreT (a)  2 (1  22 )K and scoreT (b)  2 (1  22 )K.
Let Wa be the total weight of the voters in T that rank b last. It follows that
scoreT (a)  2 Wa and scoreT (b)  (2(1  22 )K  Wa )2 .
From these observations, and the assumption that 2 > 0, it follows that Wa = (1 22 )K.
So, exactly half of the vote weight of T ranks b last. Then the weights of the voters that
rank b last correspond to a partition.
Finally, we handle the last case, namely, 1 > 2 > m1 . Let m
b be the smallest index
m1
c
+
2
<
m
b
<
m.
Take
societys
order to be
such that 2 > m
.
Note
that
b
b
2
ab m1 c L    L a1 L p L b1 L    L bd m1 e .
2

2

We are again reducing from Partition. Let k1 , . . . , kn be a nonempty collection of positive
integers that sum to 2K. Let S consist of two voters, each of weight (21  2  m
b )K,
voting
ab m1 c(mm)
b >    > a1 > p > b1 >    > bd m1 e(mm)
b >    and
2

2

bd m1 e(mm)
b >    > b1 > p > a1 >    > ab m1 c(mm)
b > 
2

2

and let the weights of T be (1  2 )k1 , . . . , (1  2 )kn . Since b m1
b =m
b 
2 c  (m  m)
m1
m1
m1
(d m1
e+1)

m(b
b
c+2)
>
0
and
b
c(m
m)+1+d
b
e(m
m)
b
=
2
mm
b
<
m,
b
2
2
2
2
we have the following scores.
scoreS (p) = 22 (21  2  m
b )K,
scoreS (ab m1 c(mm)
b )K, and
b ) = (1 + 2 )(21  2  m
2

scoreS (bd m1 e(mm)
b ) = scoreS (ab m1 c(mm)
b ).
2

2

If k1 , . . . , kn has a partition, then set (1  2 )K vote weight of T to
p > a1 >    > ab m1 c > b1 >    > bd m1 e(mm)
b > 
2

2

and set (1  2 )K vote weight of T to
p > b1 >    > bd m1 e > a1 >    > ab m1 c(mm)
b >  .
2

2

472

fiBypassing Combinatorial Protections

Note that ab m1 c(mm)
b and bd m1
e(mm)
b are the only candidates that can score higher
2
2
than p and that
2
2
score(p) = (22 (21  2  m
b ) + 21 (1  2 ))K = (21 + 21 2  22 m
b  22 )K.

And score(ad m1 e(mm)
b ) = score(bd m1 e(mm)
b )=
2

2

2
2
((1 + 2 )(21  2  m
b ) + (2 + m
b )(1  2 ))K = (21 + 21 2  22 m
b  22 )K.

So, p is a winner of the resulting election.
For the converse, suppose that p can be made a winner. Assume (using Lemma 6.3)
that p is ranked first by every manipulator. From the observations above, it is immediate
that
scoreT (ab m1 c(mm)
b )(1  2 ) and
b )  (2 + m
2

scoreT (bd m1 e(mm)
b )(1  2 ).
b )  (2 + m
2

Let Wa be the total weight of all T voters such that ab m1 c(mm)
b > bd m1
e(mm)
b . It
2
2
follows that
scoreT (ab m1 c(mm)
b (2(1  2 )K  Wa ) and
b )  2 Wa + m
2

scoreT (bd m1 e(mm)
b Wa + 2 (2(1  2 )K  Wa ).
b )  m
2

From these observations, and the fact that 2 > m , it follows that Wa = (1  2 )K.
Then the weights of the voters such that ab m1 c(mm)
b > bd m1
e(mm)
b correspond to a
2
2
partition.
q Lemma 6.7
q

Theorem 6.2

7. Related Work, Additional Discussion, and Open Problems
The two papers most related to our work are by Walsh (2007) and by Faliszewski, Hemaspaandra, Hemaspaandra, and Rothe (2011). Walshs paper first raised (among many other
interesting issues, such as possible and necessary winners, Konczak & Lang, 2005, in singlepeaked settings) the issue of the effect of single-peaked electorates on manipulation. For
the particular case he looked atweighted coalition manipulation under single transferable vote electionshe showed that manipulation remains hard even for single-peaked electorates. Faliszewski et al. showed cases where single-peakedness removes complexity shields
against manipulation, and also opened the study of (nonpartition) control. Our paper in
contrast with Walshs stresses cases where single-peakedness removes combinatorial protections. And we go beyond Faliszewski et al. (2011) by for the first time studying bribery
of single-peaked electorates and partition-control of single-peaked electorates. For both
these new cases, we show that many election systems (for example Llull elections) have
polynomial-time algorithms for single-peaked electorates, even if the system is known to
be NP-hard in the analogous general case. We also generalize the Faliszewski et al. (2011)
473

fiBrandt, Brill, Hemaspaandra, & Hemaspaandra

dichotomy theorem for manipulation of three-candidate scoring protocols to the case of
arbitrary scoring protocols.
Although it is more closely related to Faliszewski et al. (2011) than to the present paper,
and came after the present paper, it is important to mention that the work of Faliszewski
et al. (2011b, 2014) explores the interesting issue of seeing whether results such as those
of Faliszewski et al. (2011) still hold even when the electorate is merely near to being
single-peaked (see also Cornaz et al., 2012; Bredereck et al., 2013; Erdelyi et al., 2013; Sui
et al., 2013, regarding nearness to single-peakedness and weaker forms of single-peakedness).
Since large real-world electorates are unlikely to be (perfectly) single-peaked, it is natural
and important to study such weaker forms of single-peakedness.
Although Walsh (2007) and Faliszewski et al. (2011) are by far the most related work,
other work is much worth mentioning. Bartholdi and Trick (1986), Doignon and Falmagne
(1994), and Escoffier et al. (2008) provided efficient algorithms for finding single-peaked
orderings. And Conitzer (2009) studied the effect of single-peaked electorates on preference
elicitation. Indeed, single-peakedness is of much current interest in computational settings.
For example, at least four papers in the IJCAI-2013 conference, including the Bredereck
et al. (2013) paper mentioned above are related to single-peakedness.
The p2 -completeness of the winner problems of Dodgson, Kemeny, and strongYoung
elections was established, respectively, by Hemaspaandra et al. (1997), Hemaspaandra et al.
(2005), and Rothe et al. (2003). The literature now contains many papers on the complexity
(when single-peaked preferences are not assumed) of manipulation and control (as a pointer
to some of those, see Faliszewski et al., 2009b, and Faliszewski, Hemaspaandra, & Hemaspaandra, 2010, and the citations therein), and even contains a number of papers on the
younger topic of the complexity of bribery (e.g., Faliszewski et al., 2009; Faliszewski, 2008;
Faliszewski et al., 2009a). Although the nonunique-winner model and the unique-winner
model very typically have the same complexity results, Faliszewski, Hemaspaandra, and
Schnoor (2008, drawing also on Conitzer et al., 2007) show that this is not always the case.
NP-completeness and p2 -completeness are worst-case notions. So it is natural to wonder
whether problems in those classes can be solved frequently by heuristic algorithms. There
has been much experimental study on that theme (see, e.g., Walsh, 2009). On the other
hand, it is known (see Hemaspaandra and Williams, 2012) that if any polynomial-time deterministic heuristic algorithm for any NP-complete (or p2 -complete) problem asymptotically
makes subexponentially many errors, then the polynomial hierarchy collapses.
A worry that comes immediate to the minds of social choice theorists can be expressed as
follows: Since it is known that, for single-peaked electorates, median voting leaves voters
with voting sincerely being an optimal strategy, single-peaked elections are not interesting in
terms of other election systems, since median voting should be used. A detailed discussion
of this worry would itself fill a paper. But we briefly mention three reasons why the above
objection is not as serious as it might at first seem. First, the nonmanipulability claims
regarding single-peaked elections and median voting are about manipulability, and so say
nothing at all about, for example, control. Indeed, weakCondorcet in effect is a type of
median voting on single-peaked electorates, and so for example our partition of voters
algorithm makes it clear that control can be exercised in interesting ways.15 Second, even if
15. To be more explicit, under the most natural way of framing median voting, median voting and weakCondorcet (and so all weakCondorcet-consistent rules) are exactly the same on single-peaked electorates.

474

fiBypassing Combinatorial Protections

median voting does have nice properties, the simple truth is that in the real world, society
for virtually all elections and electorateshas chosen (perhaps due to transparency, comfort,
or tradition) to use voting systems that clash sharply with median voting. The prominence
of plurality voting is the most dramatic such case. So since in the real world we do use a
rich range of election systems, it does make sense to understand their behavior. Third, one
must be very careful with terms such as strategy-proof. The paper people most commonly
mention as showing that median voting is strategy-proof is that of Barbera (2001). But that
papers results are about social choice functions (election rules that when kCk  1 always
have exactly one winner), notas this paper isabout election rules that select a set of
winners. So one cannot simply assume that for our case median voting (say, weakCondorcet
elections) never gives an incentive to misrepresent preferences.
Actually, in certain problem settings, one never has an incentive to misrepresent ones
top choice (in single-peaked weakCondorcet elections ones top choice is all that affects
the outcome) in weakCondorcet elections (which are a social choice correspondence). For
example, if ones goal is Seek to make your top choice be a weakCondorcet winner, then
one never has an incentive to misstate ones top choice. As another example, if ones
goal is (for any fixed k) Seek to make at least one of your first k choices be among
the weakCondorcet winners, then again one never has an incentive to misstate ones top
choice (which is the only thing that matters about ones vote).16 On the other hand, in
some reasonable problem settings, misstating may make sense. If ones goal is Make your
top choice the unique winner or failing that make your second choice the unique winner or
... or failing that make your last choice the unique winner or failing that make there be
multiple winners, then the two-candidate single-peaked election where voter 1 votes a > b
but voter 2 prefers b to a already gives voter 2 an incentive to vote, insincerely, a > b. Before
leaving this topic, we should stress that this and the previous paragraphs discussions are
for the model in which manipulators come in with complete preference orders. However, in
the Bartholdi et al. (1989) model (which this paper and most complexity papers use when
studying manipulation), the manipulative coalition is a blank slate with its only goal being
to make a certain candidate p be a winner.
An open issue not already mentioned in this paper is the following. Section 6 provided for
single-peaked electorates a manipulation-complexity dichotomy that applies to all scoring
rules (and see Hemaspaandra & Hemaspaandra, 2007, for what holds without the singlepeaked restriction). Although that is a broad set of rules, our theorem is connecting the
specification of the system to the systems complexitya natural connection. However, it
And this paper establishes many results regarding bribery and control of weakCondorcet in the context
of single-peaked electorates.
16. We mention in passing that the two no incentive to manipulate claims we just made for weakCondorcet
elections for single-peaked electorates also hold for a family of related election systems for single-peaked
electorates. In weakCondorcet, after sorting the voters by first choice (under the single-peaked ordering
of candidates), all candidates who are or fall between the median voter(s) are winners. If we think about
that as being MedianVoting 1 , then for each rational , 0    12 , for single-peaked voting we can
2
consider the rule, MedianVoting , that after ordering the voters by first-choice makes the winners be all
candidates that in the societal order L fall in the inclusive interval between (a) the leftmost voter vleft
such that at least d  kV ke voters have first preference the same as vleft or to the left under L as that of
vleft , and (b) the rightmost voter vright such that at least d  kV ke voters have first preference the same
as vright or to the right under L as that of vright . Each rule MedianVoting will share the no incentive
to manipulate properties mentioned in the text.

475

fiBrandt, Brill, Hemaspaandra, & Hemaspaandra

is also natural to wonder whether one can tightly link the social-choice-favored properties of
a rule to its manipulation (or bribery or control) complexity. To give an idea of the kind of
theorem we are thinking of, we mention the following known theorem linking social-choiceproperties to winner-problem complexity (the statement involves some notions we will not
define here): Every election system that is neutral, Condorcet-consistent, and consistent
has a p2 -complete winner problem (Hemaspaandra et al., 2005, see also the discussion
of Hemaspaandra and Hemaspaandra, 2000). However, the just-quoted winner result is
something of a cheat as there is just one system that satisfies those propertiesKemeny
elections. The dream case for manipulationand bribery and controlwould be to find
a broad link between social-choice properties and complexity in the single-peaked case or
in the general case. In the true dream case, we might completely characterize in terms of
some statement about social-choice properties the election systems with easy manipulation
(or bribery or control) problems, in the single-peaked case or in the general case.
A final open direction is to find cases where partition-of-candidates control shifts from
NP-hard to polynomial time when restricted to single-peaked electorates.

8. Conclusions
The theme of this paper is that single-peaked electorates often tame combinatorial explosion.
We saw this first for the case of the winner problem. In that case, this taming is good.
It shows that for single-peaked electorates, election systems such as Kemeny have efficient
winner algorithms, despite their p2 -hardness in the general case. But then for bribery
and control (and in part, manipulation), we saw many cases where NP-hard problems fell
to polynomial time for single-peaked electorates, via algorithms that bypassed the generalcase combinatorial explosions of covers and partitions. Since those NP-hardness results were
protections against attacks on elections, our results should serve as a warning that those
protections are at their very core dependent on the extreme flexibility of voter preference
collections the general case allows. For single-peaked electorates, those protections vanish.

Acknowledgments
This work was supported in part by ARC grant DP1101011792, DFG grants BR-2312/{32,6-1}, NSF grants CCF-{0426761,0915792,1101452,1101479} and IIS-0713061, the European Science Foundations EUROCORES program LogICCC, and Friedrich Wilhelm Bessel
Research Awards to Edith Hemaspaandra and Lane A. Hemaspaandra. This work was done
in part while Felix Brandt was at Ludwig-Maximilians-Universitat Munchen and Markus
Brill was at Ludwig-Maximilians-Universitat Munchen and TU Munchen, and was done in
part during visits of Edith Hemaspaandra and Lane A. Hemaspaandra to Heinrich-HeineUniversitat Dusseldorf and Ludwig-Maximilians-Universitat Munchen. A preliminary version of this paper appeared in the proceedings of the 24th AAAI Conference on Artificial
Intelligence, July 2010 (Brandt, Brill, Hemaspaandra, & Hemaspaandra, 2010).
We are grateful to Steven Brams, Piotr Faliszewski, Felix Fischer, Zack Fitzsimmons,
Paul Harrenstein, Jerome Lang, Ariel Procaccia, Jorg Rothe, Hans Georg Seedig, Troels
Srensen, and the anonymous conference and journal referees for helpful comments and
476

fiBypassing Combinatorial Protections

valuable suggestions. We are grateful to Piotr Faliszewski for the email exchange that led
to Theorem A.5 and to Paul Harrenstein for preparing the figures.

Appendix A. Additional Definitions and Proofs
This appendix provides additional definitions and proofs.
A.1 Additional Definitions
Let us define some additional election systems mentioned in Section 2.
Nanson elections are runoff methods based on Bordas scoring protocol. In Nansons
(1882) original definition, a series of Borda elections is held and all candidates who at
any stage have not more than the average Borda score are excluded unless all candidates
have identical Borda scores, in which case these candidates are declared the winners of the
election. There exist two variants of Nanson due to Fishburn and Schwartz, which exclude
candidates with the lowest Borda score and candidates whose Borda score is less than the
average score, respectively. All three versions fail to be weakCondorcet-consistent (Niou,
1987).
Maximin (a.k.a. Simpson-Kramer) elections (Simpson, 1969; Kramer, 1977) choose those
candidates that fare best in their worst pairwise comparison against any other candidate.
The remaining three election systems are based on the pairwise majority relation. In
Schwartz (1972) elections (sometimes also called the top cycle), the winners are defined
as the maximal elements of the asymmetric part of the transitive closure of the majority relation. The winners in Fishburn (1977) elections are the maximal elements of the Fishburn
relation F , which is defined by letting a F b if every candidate that beats a in a pairwise
comparison also beats b and there exists a candidate that beats b but not a.
A.2 Proofs for Section 2
Theorem A.1. Kemeny, Schwartz, and Fishburns and Schwartzs versions of Nanson are
weakCondorcet-consistent for single-peaked electorates.
Proof. All statements rely on the observation that the pairwise majority relation, >m ,
for single-peaked electorates is transitive (Black, 1948, 1958). Further observe that weak
Condorcet winners are precisely the maximal elements of the pairwise majority relation. It
follows immediately that Schwartz is weakCondorcet-consistent.
P In the case of Kemeny, note that (again writing >m for the pairwise majority relation)
{a,b}C,a6=b,am b k{v  V | v prefers b to a}k is a lower bound on the Kemeny score of any
linear order. This score is realized by linear order > if and only if > is consistent with >m ,
i.e., for every a, b  C, if a >m b then a > b. If the pairwise majority relation is transitive
and c is a weak Condorcet winner, then c is a Kemeny winner, as evidenced by the following
Kemeny consensus: rank c first and then greedily keep adding, in the successive positions
in the consensus, maximal (with respect to >m ) unranked candidates. Since > is consistent
with >m , > is a Kemeny consensus. It is immediate that if d is not a weak Condorcet
winner, then d can not be ranked first in any linear order consistent with >m .
For Nanson, we prove that no weak Condorcet winner is eliminated at any stage of the
election. First let us normalize Borda scores by subtracting (kV k  (m  1))/2 from the
477

fiBrandt, Brill, Hemaspaandra, & Hemaspaandra

Borda score of every candidate. This ensures that the average normalized Borda score is
precisely zero. Now, observe that the normalized Borda score of candidate a is simply half
of the sum, over all candidates b, of the number of voters who prefer a to b minus the
number of voters who prefer b to a. As a consequence, the normalized Borda score of every
weak Condorcet winner is at least zero. Moreover, in the case of a single-peaked electorate,
due to the transitivity of the majority relation, there is always a candidate with a negative
normalized Borda score unless all candidates have an identical score of zero. In the latter
case, all three versions of Nanson will terminate. In the former case, neither Fishburns nor
Schwartzs variant will exclude weak Condorcet winners because their score is at least as
large as the average score.
q
The fact that Black, Dodgson, the original version of Nanson, and Copeland elections
for all   [0, 1) are not weakCondorcet-consistent for single-peaked electorates is seen by
the following universal counterexample. Let there be two voters with preferences b > a > c
and c > b > a. These preferences are single-peaked with respect to the societal ordering
a L b L c. Candidates b and c are weak Condorcet winners, but each of the mentioned
election systems chooses only b. Similarly, strongYoung is not weakCondorcet-consistent
for single-peaked electorates because in an election with two voters whose preferences are
a > b > c and c > b > a, all candidates are weak Condorcet winners, but strongYoung
yields only candidates a and c.
A.3 Additional Proofs for Section 3
Theorem A.2. The winner problem for Young elections is p2 -complete, both in the
nonunique-winner model and the unique-winner model.
Proof. p2 -completeness of the (nonunique) winner problem for strongYoung elections
(somewhat confusingly called Young elections there) was shown by Rothe et al. (2003). The
same proof also establishes p2 -completeness in the unique winner model (Hemaspaandra
et al., 2009). The p2 upper bound is easy to show, and the same argument can be used
to show a p2 upper bound for the Young winner problem (in both the nonunique-winner
model and the unique-winner model). p2 -hardness of the winner problem for strongYoung
elections was shown as Theorem 2.5 of Rothe et al. (2003) via a reduction from the p2 complete problem Maximum Set Packing Compare (MSPC, for short): Given two sets, B1
and B2 , and two collections of sets S1 and S2 , where each S  Si is a nonempty, finite
subset of Bi , is it the case that (S1 )  (S2 )? Here, (Si ) denotes the maximum number
of pairwise disjoint sets in Si . As in Rothe et al., we assume that (Si ) > 2.
The proofs of Theorems 2.3 and 2.5 of Rothe et al. (2003) show how to construct
from a given MSPC instance I = (B1 , B2 , S1 , S2 ) an election (D, W ) with two designated
candidates, c and d, such that (a) if (S1 )  (S2 ) then c and d are strongYoung winners
of (D, W ), and (b) if (S2 ) > (S1 ) then d is the unique strongYoung winner of (D, W ).
We now show how this proof can be adapted to work for Young elections. Please
refer to Rothe et al. (2003) for definitions and details of the construction, as we will only
point out the differences here. Given an MSPC instance I = (B1 , B2 , S1 , S2 ), we construct
478

fiBypassing Combinatorial Protections

an election (C 0 , V 0 ) such that c and d are designated candidates in C 0 , and it holds that
YoungScore(C 0 , c, V 0 ) = 2  (S1 ) and YoungScore(C 0 , d, V 0 ) = 2  (S2 ).17
Let C 0 = C and V 0 = V  {v(2.4) , v(2.7) }, where (C, V ) is the election constructed
in the proof of Theorem 2.3 of Rothe et al. (2003), v(2.4) is one of the two voters in V
referred to as voters of the form (2.4) in Rothe et al. and v(2.7) is one of the two voters
in V referred to as voters of the form (2.7) in Rothe et al.18 One can then define a
submultiset V0 of the voters V 0 as V0 = V  {v(2.4) }, where V is defined on page 381
of Rothe et al. Then kV0 k = 2  (S1 ) and c is a weak Condorcet winner in (C 0 , V0 ),
implying that YoungScore(C 0 , c, V 0 )  2  (S1 ).
To show that YoungScore(C 0 , c, V 0 )  2  (S1 ), we adapt Lemma 2.4 of Rothe et al.
(2003) in the following way. (The proof of Lemma A.3 is similar to the proof of Lemma 2.4
of Rothe et al., 2003, and we omit it here.)
Lemma A.3. For any  with 3 <   kS1 k + 1, let V be a submultiset of V 0 such that V
contains exactly   1 voters of the form (2.4) or (2.5) and c is a weak Condorcet winner in
(C 0 , V 0 ). Then V contains exactly   1 voters of the form (2.3) and no voters of the form
(2.6), (2.7), or (2.8). Moreover, the   1 voters of the form (2.3) in V represent pairwise
disjoint sets from S1 .
Let V0  V 0 be such kV0 k  2  (S1 ) and such that c is a weak Condorcet winner in
Suppose there are exactly   1 voters of the form (2.4) or (2.5) in V0 . Since
(S1 ) > 2, it follows that kV0 k  6, and thus  > 3 in order for a to not beat c. Also,
1  kS1 k, since there are exactly kS1 k voters of the form (2.4) or (2.5) in V 0 . Lemma A.3
then implies that there are exactly 1 voters of the form (2.3) in V0 , those voters represent
pairwise disjoint sets from S1 , and V0 contains no voters of the form (2.6), (2.7), or (2.8).
Hence, kV0 k = 2  (  1)  2  (S1 ).
We thus have YoungScore(C 0 , c, V 0 ) = 2  (S1 ). Analogously, one can show that
YoungScore(C 0 , d, V 0 ) = 2  (S2 ). Thus
(C 0 , V0 ).

(S1 )  (S2 )

if and only if

YoungScore(C 0 , c, V 0 )  YoungScore(C 0 , d, V 0 ),

which proves the p2 -hardness of comparing Young scores. To show that the winner problem
for Young elections is p2 -hard, we alter the election (C 0 , V 0 ) in the same way as (C, V ) is
17. Note that in this proof, we follow Rothe et al.s (2003) convention for Young scores and define the
YoungScore(C, c, V ) of a candidate c in an election (C, V ) as the size of the maximal submultiset of
voters for which c is a weak Condorcet winner. Hence, the set of Young winners consists of all candidates
whose YoungScore is at least as large as the YoungScore of each other candidate.
18. For completeness, we give the definition of (C 0 , V 0 ). C 0 = B1  B2  {a, b, c, d} and V 0 consists of the


following voters. (For an ordered set M = {m1 , . . . , mk }, we write M for m1 >    > mk .)


 

(2.3) For each E  S1 , one voter vE with preference order E > a > c > B1  E > B2 > b > d.




(2.4) One voter with preference order c > B1 > a > B2 > b > d.




(2.5) kS1 k  1 voters with preference order B1 > c > a > B2 > b > d.


 

(2.6) For each F  S2 , one voter vF with preference order F > b > d > B2  F > B1 > a > c.




(2.7) One voter with preference order d > B2 > b > B1 > a > c.




(2.8) kS2 k  1 voters with preference order B2 > d > b > B1 > a > c.

479

fiBrandt, Brill, Hemaspaandra, & Hemaspaandra

altered in Theorem 2.5 of Rothe et al. (2003). Let (D0 , W 0 ) be the altered election.19 One
can then show that the Young scores of c and d in (D0 , W 0 ) are the same as in (C 0 , V 0 ),
and that all other candidates have a Young score in (D0 , W 0 ) of at most 2. Thus, since the
Young scores of c and d are at least 6, comparing the Young scores of c and d in (D0 , W 0 )
is tantamount to deciding if c is a Young winner.
Altogether, we have that (a) if (S1 )  (S2 ) then c is a Young winner of (D0 , W 0 ),
and (b) if (S2 ) > (S1 ) then d is the unique Young winner of (D0 , W 0 ). It follows that
an MSPC-instance I is in MSPC if and only if c is a Young winner of (D0 , W 0 ), implying
p2 -hardness of the Young winner problem in the nonunique-winner model. For the uniquewinner model, we follow the argument from the proof of Theorem 2.3 of Hemaspaandra
et al. (2009): Observe that I is in MSPC if and only if d is not the unique winner of
(D0 , W 0 ). Thus the complement of the unique-winner problem for Young elections is p2 hard. Since p2 is closed under complement, this proves that the unique-winner problem for
Young elections is p2 -hard as well.
q
Theorem A.4. The winner problem for weakDodgson elections is p2 -complete, both in the
nonunique-winner model and the unique-winner model.
Proof. p2 -completeness of the (nonunique) winner problem for Dodgson elections was
shown in Hemaspaandra et al. (1997). The same proof also establishes p2 -completeness
in the unique winner model (Hemaspaandra et al., 2009). The p2 upper bound is easy to
show, and the same argument can be used to show a p2 upper bound for the weakDodgson
winner problem (in both the nonunique-winner model and the unique-winner model). p2 hardness of the winner problem for Dodgson was shown by a reduction from the p2 -hard
problem Two Election Ranking (2ER, for short) (Hemaspaandra et al., 1997): Given two
Dodgson triples (C, c, V ) and (D, d, W ), where both kV k and kW k are odd and c 6= d, is
the Dodgson score of c in (C, V ) less than or equal to the Dodgson score of d in (D, W )?
Here, a Dodgson triple (C, c, V ) is an election (C, V ) with a designated candidate c  C.
The reduction from 2ER to the winner problem for Dodgson elections works by merging
the elections E1 = (C, V ) and E2 = (D, W ) into a new election E3 = (C 0 , V 0 ) such that
C  D  C 0 and the following properties are satisfied:
(i) Dodgson-ScoreE3 (c) = Dodgson-ScoreE1 (c) + 1,
(ii) Dodgson-ScoreE3 (d) = Dodgson-ScoreE1 (d) + 1, and
(iii) Dodgson-ScoreE3 (x) > Dodgson-ScoreE3 (c) for all x  C 0  {c, d}.
Here, Dodgson-ScoreE (x) denotes the minimal number of switches required to make candidate x a Condorcet winner in election E. Thus, we immediately have that c is a Dodgson
winner in E3 if and only if Dodgson-ScoreE1 (c)  Dodgson-ScoreE2 (d).20
19. For completeness, (D0 , W 0 ) is defined as follows. We replace every candidate g  C 0  {c, d} by kV 0 k
0
candidates g 0 , . . . , g kV k1 . And we replace each occurrence of candidate g in the ith voter of V 0 by
0
0
0
0
g i mod kV k > g (i+1) mod kV k >    > g (i+kV k1) mod kV k .
20. We have noted a fixable problem in the construction of Hemaspaandra et al. (1997). The problem is on
page 822 at the end of the proof of their Lemma 3.7, where after proving that Dodgson-ScoreE3 (dkDk1 ) >

480

fiBypassing Combinatorial Protections

We now show how the approach from Hemaspaandra et al. (1997) can be adapted
to work for weakDodgson. First, observe that the problem Weak Two Election Ranking
(w2ER), which is defined like 2ER except with Dodgson score replaced by weakDodgson
score, inherits p2 -hardness from 2ER because Dodgson scores and weakDodgson scores
coincide for all instances with an odd number of voters. (If the number of voters is odd,
being a weak Condorcet winner is tantamount to being a Condorcet winner, since there
are no ties in pairwise comparisons.) Also observe, by inspection of the p2 -hardness proof
from Hemaspaandra et al. (1997), that 2ER and w2ER are still p2 -hard if we restrict the
Dodgson-ScoreE3 (c), it is claimed that The same argument applies to each candidate in (C D)\{c, d}.
Though this claim is clearly true for each candidate in D{d}, it is not true for each candidate in C {c},
since in the preference orders of voter groups (b) and (c), candidates in C  {c} are not separated from
candidates in D by a set of separating candidates. As a consequence, it may be possible to make a
candidate in C  {c} a Condorcet winner by less than kSk/2 switches in those voter groups.
This problem can be avoided by changing the t1 <    < tkT k < c < s1 <    < skSk < c1 <    <
ckCk1 prefix in the preference orders of the voters in groups (b) and (c) to c < t1 <    < tkT k <
c1 <    < ckCk1 < s1 <    < skSk . This gives the following set of voters. (In this footnote we use
a < b < c-notation in specifying votes, rather than c > b > a-notation as is used throughout the rest
of our paper, in order to match the approach to expressing votes used by Hemaspaandra et al., 1997.
We do this to make comparisons with that paper as straightforward as possible.)
(a) For each voter e1 <    < ekCk in V , a voter d < s1 <    < skSk < d1 <    < dkDk1 < t1 <    <
tkT k < e1 <    < ekCk .
(b) For each voter e1 <    < ekDk in W , a voter c < t1 <    < tkT k < c1 <    < ckCk1 < s1 <    <
skSk < e1 <    < ekDk .
l
m l
m
k
(c) kV2 k  kW
voters c < t1 <    < tkT k < c1 <    < ckCk1 < s1 <    < skSk < d1 <    <
2
dkDk1 < d.
l
m
(d) kV2 k voters t1 <    < tkT k < c1 <    < ckCk1 < d1 <    < dkDk1 < skSk <    < s1 < c < d.
l
m
k
(e) kW
voters t1 <    < tkT k < c1 <    < ckCk1 < d1 <    < dkDk1 < s1 <    < skSk < d < c.
2
Intuitively, the changed construction is more symmetrical than the original one, as the preferences of
voter groups (a) and (b) are defined analogously, with the roles of C and D (and those of S and T )
interchanged.
Using the proof of Lemma 3.7 of Hemaspaandra et al. (1997), it is easy to see that the three
properties mentioned above hold. In particular, c is preferred to each t  T by more than half of the
voters, and so the Dodgson score of c does not change. For every candidate d0  D, d0 does not occur
in the prefix of the preference order that is changed and so the Dodgson score of d0 does not change. It
remains to show that Dodgson-ScoreE3 (x) > Dodgson-ScoreE3 (c) for all x  T  S  (C  {c}). From
the proof of Lemma 3.7, we know that Dodgson-ScoreE3 (c) < kSk/2 and that kSk = kT k. In order for
t  T to become a Condorcet winner, t needs to gain at least one vote over d. Note that in the changed
preferences, we need more than kSk switches to switch t beyond d, and so the change in preferences will
not lower the Dodgson score of t below kSk/2. In order for s  S to become a Condorcet winner, it
is shown in Hemaspaandra et al. that s needs to gain at least one vote over c and that we need more
than kSk/2 switches to do that. Since s is preferred to c by the voters in groups (b) and (c) in both the
original and the changed election, these voters can not be used for s to gain a vote over c. It follows that
in the changed election, s needs more than kSk/2 switches to become a Condorcet winner. Finally, let
c0  C  {c}. The argument from Hemaspaandra et al. that every d0  D  {d} needs to gain at least
one vote over c in order to become a Condorcet winner, and that we need more than kSk/2 switches to
do that can be used (by symmetry of the construction) to show that every c0  C  {c} needs to gain
at least one vote over d in order to become a Condorcet winner, and that we need more than kSk/2
switches to do that.

481

fiBrandt, Brill, Hemaspaandra, & Hemaspaandra

problem to Dodgson triples with at least three voters. Let w2ER0 be the thus-restricted
version of of w2ER.
Here is a reduction from w2ER0 to the winner problem for weakDodgson elections.
Given two Dodgson triples (C, c, V ) and (D, d, W ), denote the corresponding elections by
E1 = (C, V ) and E2 = (D, W ). For convenience, denote v = kV k and w = kW k. Recall
that both v and w are odd and at least three and assume without loss of generality that
C  D =  and that v  w. Define m = v  kCk + w  kDk and observe that m is a strict
upper bound for all weakDodgson
in E1 and E2 : Even in the worst case (a candidate
 scores

is least preferred by all voters), v2  (kCk  1) < m switches suffice
to make a candidate a

weak Condorcet winner in E1 (by making it the top choice of v2 voters), and an analogous
statement holds for E2 .
We now define the new election E3 = (C 0 , V 0 ). The set C 0 of candidates of E3 is defined
as C 0 = C  D  S  T  U , where C and D are the candidates from elections E1 and E2
and S = {s1 , . . . , sm }, T = {t1 , . . . , tm }, and U = {u1 , . . . , um } are 3m new candidates.
In order to define the voters of E3 , we introduce the following abbreviations for notational convenience. For a (ordered) set M = {m1 , . . . , mk }, a > M > b is shorthand for
a > mk >    > m1 > b. Furthermore, let c1 , . . . , ckCk1 and d1 , . . . , dkDk1 be arbitrary
enumerations of C = C  {c} and D = D  {d}, respectively. The voters V 0 of E3 consist
of the following subgroups:
(a) For every voter in E1 with preference order Ci over C, there is one voter with preference order
Ci > T > D > S > d > U.
(b) For every voter in E2 with preference order Dj over D, there is one voter with preference order
Dj > S > C > T > c > U.
   
(c) There are v2  w2 voters with preference order
d > D > S > C > T > c > U.
(d) There are

v
2

voters with preference order
d > c > U > S > D > C > T.

(e) There are

w
2

 1 voters with preference order
c > d > U > S > D > C > T.

(f) There is one voter with preference order
S > D > C > T > c > d > U.
Since v is odd, the total number of voters is kV 0 k = 2v + w + 1. As both v and w are odd,
kV 0 k is even and a weak Condorcet winner has to be preferred to every other candidate
 
0
by at least kV2 k = v + w2 voters. We now show that the following three properties are
satisfied:
482

fiBypassing Combinatorial Protections

(i) weakDodgson-ScoreE3 (c) = weakDodgson-ScoreE1 (c),
(ii) weakDodgson-ScoreE3 (d) = weakDodgson-ScoreE1 (d), and
(iii) weakDodgson-ScoreE3 (x) > weakDodgson-ScoreE3 (c) for all x  C 0  {c, d}.
0

For (i), observe that c is preferred to every candidate in C 0  C by at least kV2 k of
the voters. Thus, in computing the weakDodgson score of c, we only have to take care
of candidates in C = {c1 , . . . , ckCk1 }. Let xi be the number of voters in group
 (a) that
prefer c to ci . Then the number of voters in V 0 that prefer c to ci is xi + v2 + w2  1.
Candidate c is a weak Condorcet winner in E3 if and only if this number is greater than
 
 
0
or equal to kV2 k = v + w2 , and this is the case if and only if xi  v2 for each i 
{1, . . . , kCk  1}. But this means that c is a Condorcet winner in E1 . By definition,
this can be achieved by k switches, where k = weakDodgson-ScoreE1 (c). We have thus
shown the upper bound weakDodgson-ScoreE3 (c)  weakDodgson-ScoreE1 (c). Now assume
that weakDodgson-ScoreE3 (c) < weakDodgson-ScoreE1 (c). Due to the construction, all
the switches in an optimal sequence occur in voters of group (a), as making c beat any
candidate in C would require more than m switches in all the other relevant voter groups
(b), (c), and (f). This means that there is a way to make c a weak Condorcet winner in E1
with less than weakDodgson-ScoreE1 (c) switches, a contradiction. We have thereby shown
that weakDodgson-ScoreE3 (c) = weakDodgson-ScoreE1 (c). The proof of property (ii) is
analogous.
For (iii), recall that m was chosen sufficiently large to be a strict upper bound on
weakDodgson-ScoreE1 (c) and thus, by (i), on weakDodgson-ScoreE3 (c). We now show that
all candidates in C 0 other than c and d have a weakDodgson score of at least m in E3 .
Consider a candidate s  S. In order to become a weak Condorcet winner, s must
 v  in
beat
contest. As s is preferred
to c by only w + 2 
particular

 v  or tie
 c in their pairwise
 
w
w
0 , it needs to gain at least v extra votes over c in voter
+
1
=
+
voters
in
V
2
2
2
2
groups (a), (d), and (e). But gaining just one extra vote over c would require more than
m switches, because s is separated from c by at least m other candidates in all these voter
groups.
 
A similar argument applies to all other candidates: Candidates in T need w2 extra
votes over d in (b), (c), (d), and (e), but one extra
vote requires more than m switches in

v
each of these voters. Candidates in U need 2 extra votes over di  D in (a), (b), (c),
and (f), but one extra vote requires more than m switches. Candidates in D need v2
extra votes over c in (a), (d),
 and (e), but one extra vote requires more than m switches.
w
Candidates in C need 2 extra votes over d in (b), (c), (d), and (e), but one extra vote
requires more than m switches. Thus, we have shown that weakDodgson-ScoreE3 (x) > m >
weakDodgson-ScoreE3 (c) for all x  C 0  {c, d}.
It is now easy to see that (1) if weakDodgson-ScoreE1 (c)  weakDodgson-ScoreE2 (d),
then c is a weakDodgson winner in E3 , and (2) if weakDodgson-ScoreE1 (c) >
weakDodgson-ScoreE2 (d), then d is the unique weakDodgson winner in E3 . Let I =
((C, c, V ), (D, d, W )) be an instance of w2ER0 . We have just argued that I is in w2ER0
if and only if c is a weakDodgson winner in E3 , which immediately implies p2 -hardness of
the weakDodgson winner problem in the nonunique-winner model. For the unique-winner
model, we follow the argument from the proof of Theorem 2.3 of Hemaspaandra et al.
483

fiBrandt, Brill, Hemaspaandra, & Hemaspaandra

(2009): Observe that I is in w2ER0 if and only if d is not the unique weakDodgson winner
in E3 . Thus the complement of the unique-winner problem for weakDodgson elections is
p2 -hard. Since p2 is closed under complement, this proves that the unique-winner problem
for weakDodgson elections is also p2 -hard.
q

A.4 Additional Proofs for Section 4
Theorem 4.3. The following hold (see Footnote 9):
1. Approval-negative-bribery and approval-strongnegative-bribery are NP-complete.
2. For single-peaked electorates, approval-negative-bribery and approval-strongnegativebribery are in P.
Proof. The polynomial-time algorithms witnessing P-membership of approval-negativebribery and approval-strongnegative-bribery have the same flavor as the algorithm for
approval-bribery (Theorem 4.2), and we present them first.
Let (C, V ) be an instance of a single-peaked approval election and L a valid single-peaked
ordering of the candidates. We have to decide whether a designated candidate p  C can
be made a winner by bribing at most k voters. As in the proof of Theorem 4.2, we define Vc
as the multiset of voters who approve of candidate c. V+ = Vp is the multiset of voters
approving p and V = V  V+ is the multiset of voters disapproving p.
Approval-strongnegative-bribery is like approval-bribery, except that bribed voters after
the bribe are not allowed to approve of p. Consequently, bribing a voter who approves of p
is always pointless and we will bribe only voters from V . Also, we can without loss of
generality assume that all bribed voters disapprove of all candidates after the bribe, as this
is clearly the best possible action with regard to our goal to make p a winner. The algorithm
is similar to the one presented in the proof of Theorem 4.2. Define the surplus n(c) of a
candidate c  C  {p} as n(c) = kVc k  kVp k and consider the rightmost candidate c0 to
the right of p that has a positive surplus. In order to make p a winner, we obviously have
to bribe at least n(c0 ) voters from Vc0  V . By definition, all candidates to the right of c0
have a nonpositive surplus, and that is whyin deciding which n(c0 ) voters from Vc0  V to
bribewe can solely focus on candidates to the left of c0 and choose the n(c0 ) voters from
Vc0  V whose approval range extends the furthest to the left. As mentioned above, we
bribe these voters to disapprove of everyone, thereby making n(c0 ) = 0. We then recalculate
the surpluses of all candidates to the left of c0 (note that a candidates surplus never grows
and we can thus ignore the candidates to the right of c0 ) and repeat the process with the
now rightmost candidate to the right of p that has a positive surplus.
By the time when all candidates to the right of p have a nonpositive surplus, we will
mirror the societal order L and repeat the same procedure for all nonpositive-surplus candidates that were originally to the left of p with respect to L. If we can make all surpluses
nonpositive without exceeding the bribe limit of k, we have found a successful bribery action. Otherwise, a successful bribery action does not exist, as all our decisions (concerning
which voters to bribe) were provably optimal.
In the case of approval-negative-bribery, bribed candidates may approve of p after the
bribe only if they approved of p before the bribe. In this model, it does sometimes make sense
484

fiBypassing Combinatorial Protections

to bribe voters from V+ . But that does not pose a problem, as the following observation
shows. The approval score of p remains unchanged for every optimal bribe. Here, an optimal
bribe is defined as either bribing a voter from V+ to disapprove of everyone except p, or
bribing a voter from V to disapprove of everyone. Again, as in the case of strongnegativebribery, we can without loss of generality assume that all our bribes are optimal. Thus,
the observation tells us that we do not have to differentiate between bribing voters from V+
and V , as in both cases the only thing we care about is the removal of approvals of
candidates other than p. Hence, the algorithm is the same as for strongnegative bribery,
except that when considering a positive-surplus candidate c and deciding which voters
from Vc to bribe, we not only consider voters that disapprove of p but also voters that
approve of p.
We now go on to show that both bribery problems are NP-complete in the general (i.e.,
not necessarily single-peaked) case. Membership in NP is obvious for both problems.
The hardness proof of approval-strongnegative-bribery is an adaptation from the proof
that approval-bribery is NP-hard (Faliszewski et al., 2009, Thm. 4.2). Please refer to that
proof as we here only point out the differences. In the reduction from the NP-hard problem
Exact Cover by 3-sets (X3C), define the bribery instance as in Theorem 4.2 of Faliszewski
et al. (2009), except that the number of voters who approve only of S
p is changed from m  t
to instead m. Then if there exists a cover A with kAk = t and iA Si = B, we bribe
each vi with i  A to approve of zero candidates (this is a slight additional change). So all
candidates tie at m approvals and all win. Looking at the other direction, if there exists a
successful bribe of t voters, then since p has m approvals and each of the 3t candidates in
B has m + 1 approvals, and no bribe can increase the number of approvals of p (in both
the negative and the strongnegative bribery setting), then clearly p still has m approvals
after the bribes. So, each of the 3t candidates in B (that started with m + 1 approvals)
must have lost exactly one approval (if any lost more than one approval, one would have
lost zero approvals and would beat p; if any lost zero approvals, it would beat p). So, we
have an exact cover by 3-sets.
Due to the construction, one can use the same proof to show NP-hardness of approvalnegative-bribery: the only voters approving p are voters who approve only p and are obviously never bribed.
q
Theorem 4.4. For single-peaked electorates, weakCondorcet-weighted-$bribery, weakCondorcet-negative-weighted-bribery, and weakCondorcet-negative-weighted-$bribery are
NP-complete, and the remaining five cases (weakCondorcet-bribery, weakCondorcet$bribery, weakCondorcet-weighted-bribery, weakCondorcet-negative-bribery, weakCondorcetnegative-$bribery) are in P.
Proof. The general setting is the same in all of the eight bribery problems and we describe
it here. Let (C, V ) be an election instance and let L be a linear order over the candidates
such that the electorate is single-peaked with respect to L. The question is whether a
designated candidate p  C can be made a weakCondorcet winner by bribing at most k
voters.
If p is a weakCondorcet winner of the election (C, V ), a successful bribery action is obviously possible as we do not have to bribe any voter. We can thus focus on the case where p
485

fiBrandt, Brill, Hemaspaandra, & Hemaspaandra

is not a weakCondorcet winner of (C, V ). In this case, the definition of weakCondorcet
implies that p is not contained in the median interval (i.e., p 
/ [m` , mr ]L , where m` and
mr are the top choices of the median voters). Assume without loss of generality that p lies
strictly to the right of the median interval, i.e., mr L p (otherwise, we can just mirror the
societal order L).
Identify each voter with his or her most preferred candidate. Define V` = {v  V | v L p}
as the multiset of voters lying to the left of p with respect to L and Vr = V  V` = {v 
V | v = p or p L v} as the multiset of voters lying on p or to the right of p. In settings
where voters have no weights, mr L p immediately implies that
l kV` k >m kVr k and that, in
rk
order to make p a weakCondorcet winner, we need to bribe kV` kkV
voters from V` to
2
make p their top choice (or, in the negative-bribery settings, to make the candidate to the
right
of p their
m top choice). If voters have weights, we have to shift a total weight of at least
l
w(V` )w(Vr )
, where w(V 0 ) for a submultiset V 0  V of voters is defined in the natural way
2
P
as the sum of the weights of voters contained in V 0 , i.e., w(V 0 ) = vV 0 w(v).
Observe that all eight bribery problems are easily seen to be in NP. We now prove the
assertions of Theorem 4.4 in the order in which they are mentioned in the statement of the
theorem.
(i) weakCondorcet-weighted-$bribery is NP-complete.
Define k as the weight that needs to be shifted from V` to p, i.e.,


w(V` )  w(Vr )
k =
.
2
The problem weakCondorcet-weighted-$bribery can now be stated as follows. We
are given a collection of objects (voters), each with a positive integer weight and a
positive integer price, and bounds k and k, and the question is whether there exists
a subset whose price is at most k and whose weight is at least k. If so, bribe them
all to first-choose p. If this holds (or if p initially wins), we succeed, else we fail.
It is straightforward to show that the NP-hard problem Knapsack (see, e.g., Garey
& Johnson, 1979) reduces directly to this problem, establishing the NP-hardness of
weakCondorcet-weighted-$bribery.
(ii) weakCondorcet-negative-weighted-bribery is NP-complete.
We give a reduction from the NP-complete problem Partition (see the proof
Theorem 6.2). Given a collection (k1 , k2 , . . . , kn ) of positive integers that sum to
2K, define the single-peaked election (C, V ) with C = {a, p, c}, a L p L c, and for
each ki we have one voter vi whose first choice is a and whose weight is ki . Set the
budget k equal to n. It is easy to see that p can win via negative bribery if and only
if (k1 , k2 , . . . , kn ) can be partitioned into two equal-sum parts.
(iii) weakCondorcet-negative-weighted-$bribery is NP-complete.
This follows from (ii).
(iv) weakCondorcet-bribery is in P.
486

fiBypassing Combinatorial Protections

The algorithm is easy: Bribe k voters chosen arbitrarily from V` to make p their top
choice. If, after the bribe, p is a weakCondorcet winner, we were successful. Otherwise,
no successful bribery action exists.
(v) weakCondorcet-$bribery is in P.
The algorithm is the same as in (iv), except that we bribe voters in the order of their
price tags, starting with the cheapest voter.
(vi) weakCondorcet-weighted-bribery is in P.
The algorithm is the same as in (iv), except that we bribe voters in the order of their
weights, starting with the voter that has the highest weight.
(vii) weakCondorcet-negative-bribery is in P.
This follows from (viii).
(viii) weakCondorcet-negative-$bribery is in P.
Recall that p lies strictly to the right of the median interval. In the case of negative
bribery, bribed voters must not have p as their top choice. Thus, p can never be made
a weakCondorcet winner if either (a) no voter has p as his or her first choice and kV k
is odd, or (b) p is on the right end of the societal order L. (Since p lies strictly to the
right of the median interval, p cannot be on the left end of L.)
Otherwise, let p be the candidate to the right of p with respect to L. Successful
bribery is possible if and only if, by greedily bribing voters from V` (starting with the
cheapest voter) to have p as their top choice, we can make p a weakCondorcet winner.
q
As a final comment, we note that it is easy to see that these problems can be solved
in pseudo-polynomial time by dynamic programming, and so the NP-completeness results
above can not be strengthened to strong NP-completeness unless P = NP.
A.5 Additional Proofs for Section 5
Let T be one of the 22 control types defined by Faliszewski et al. (2009a), i.e., the eleven
types (adding/deleting candidates/voters (4 types); unlimited-adding of candidates (1
type); and three partition types each with TP and TE (6 types)), each for both the constructive cases (making c win or uniquely win) and the destructive cases (making c not win
or not be a unique winner). Let E be any election system. The following holds for both the
unique-winner model and the nonunique-winner model. (Recall from Section 5.1 that when
we speak of a control problem being single-peaked, we mean that the single-peakedness
holds even including all potentially added candidates and voters.)
Theorem A.5. The T control problem for E is in P for single-peaked electorates in our
default model (in which the societal order L is part of the input) if and only if the T control
problem for E is in P for single-peaked electorates in the exists-L model (in which we ask
if there exists some order L relative to which the input is single-peaked and is such that the
goal can be achieved by a type T control action).
487

fiBrandt, Brill, Hemaspaandra, & Hemaspaandra

Proof. Note that for any control problem instance and any L0 and L00 that are valid
societal orders relative to the instance (and here it is crucial that our notion is that all of
the instancenot just the voters/candidates left in at the endmust respect an order for
it to be valid), then the set of successful (resp., unsuccessful) control actions under L0 is
exactly the same as under L00 . Thus, we have the following key observation:
(?) There exists a valid single-peaked order relative to which the control problem instance can be successfully controlled if and only if the control problem instance can
be successfully controlled for every valid single-peaked order.
So, if a control problem is in P for the model in which L is part of the input via algorithm A,
then for the exists-L model, we can on a given instance I compute a valid L (e.g., using the
algorithm of Escoffier et al., 2008) and hand it to A. By (?) and the correctness of A, we
know that A gives the correct answer.
As to the other direction, if a control problem is in P for the exists-L model, then thanks
to (?), we can simply strip L off the input in the model in which L is part of the input and
can safely (knowing the answer is correct for our original issue) ask the existential question
to the hypothetical P algorithm for the exists-L version.
q
Some particular instances of Theorem A.5 were argued directly for particular cases of E
and T by Faliszewski et al. (2011), but Theorem A.5 provides a tool that removes the need
for case-specific arguments.
Theorem 5.5. For weakCondorcet elections, control by adding voters and control by deleting voters are each in P for single-peaked electorates, in both the nonunique-winner model
and the unique-winner model.
Proof. We here give algorithms for the nonunique-winner model. For the unique-winner
model, see the proof of Theorem 5.7 (by Fact 3.1 on page 448, being a unique weakCondorcet
winner is tantamount to being a Condorcet winner for single-peaked electorates).
Associate each voter with his or her most preferred candidate. Our goal is to make p a
weakCondorcet winner, i.e., we want to end up in one of the following two situations:
1. kV k is odd and the median voter has p as his or her top choice.
2. kV k is even and p lies in the (inclusive) interval [m` , mr ]L , where m` and mr are the
median voters. (This includes the case where m` = mr .) In the following, we will
refer to [m` , mr ]L as the median interval.
Here is an easy algorithm for the case of control by addition of voters: See where the
current median (or median interval) is. If p is the median (or if p lies in the median interval),
we are done. Otherwise, assume without loss of generality that the median (interval) lies to
the left of p. Then add unregistered voters whose top choice is either p or some candidate
to the right of p until p is a weakCondorcet winner or we hit our addition bound or we
have added all the unregistered voters of this sort. If at this point we have not succeeded,
success is impossible.
The algorithm for the control-by-deletion-of-voters case is similar: See where the current
median (or median interval) is. If p is the median (or if p lies in the median interval), we are
488

fiBypassing Combinatorial Protections

done. Otherwise, we need to shift the location of the median voter(s) towards p. Without
loss of generality assume that the median (interval) lies to the left of p. Then we start
deleting voters at the left end of L until we make p a weakCondorcet winner or we hit our
deletion bound without success.
q
Theorem 5.6. For Condorcet elections, control by partition of voters is in P for singlepeaked electorates, in both the nonunique-winner model and the unique-winner model, and
in both the Ties Promote model and the Ties Eliminate model (note that all four cases
coincide here).
Proof. Let (C, V ) be an instance of a Condorcet election and let L be a linear order with
respect to which the electorate is single-peaked. Without loss of generality assume kCk  2
(otherwise, the problem is trivial). Furthermore, p  C is a designated candidate and the
question is whether there exists some partition (V1 , V2 ) such that p is a Condorcet winner
of the overall election. It is clear that this is the case if and only if there exists a partition
(V1 , V2 ) such that p is a Condorcet winner in (C, V1 ) and (C, V2 ) has no Condorcet winner, a
Condorcet winner that p beats in a pairwise comparison, or p itself as a Condorcet winner.
We show that Algorithm 3 returns a partition with this property whenever one exists.
Algorithm 3 loops over all candidates c that p beats in a pairwise comparison and over
all possible sizes of V1 (line 2). Then the set C of candidates is divided into five regions
R1 , R2 , . . . , R5 that are defined as follows. Without loss of generality assume that p <L c,
i.e., c lies to the right of p with respect to the societal order L (otherwise, just mirror
everything). Region R1 consists of all candidates to the left of p, i.e., R1 = {x  C | x <L p},
and region R2 consists of p only. Similarly, R5 consists of all candidates to the right of c
and R4 consists of c only. Finally, R3 consists of all remaining candidates, namely, the
candidates that lie between p and c with respect to L. Note that all regions except R2 and
R4 may be empty. We have the following picture:
p
R1

R2

c
R3

R4

R5

Associate each voter with the candidate at the top of that voters preference order.
We now define P` , a set of partitions of V with respect to the regions just defined. Let
` = (`1 , `2 , . . . , `5 ) be a five-dimensional vector of natural numbers. Define P` as the set of
all partitions (V1 , V2 ) of V that have the property that, for each i, `i is the number of voters
in V1 whose top choice is in Ri .
For P` 6= , the key observation is the following: For x  {p, c} it holds that x is a
Condorcet winner in some election (C, V1 ) with (V1 , V2 )  P` if and only if x is a Condorcet
winner in every election (C, V1 ) with (V1 , V2 )  P` . That is, if we want to check whether x is
a Condorcet winner in any of the primary elections (C, V1 ) induced by any of the partitions
(V1 , V2 )  P` , it suffices to check just one of them to obtain the answer. By symmetry, the
same statement holds for Condorcet winners of the elections (C, V2 ).
The reason why this is true is that, given the number of voters in each region, it is
easy to compute the region(s) of the median voter(s) (just by counting). Since both p
489

fiBrandt, Brill, Hemaspaandra, & Hemaspaandra

Algorithm 3 Condorcet Control by Partition of Voters
1: for all candidates c that lose against p in a pairwise comparison do
2:
for all k  {1, 2, . . . , kV k} do
P
3:
for all ` = (`1 , `2 , . . . , `5 )  N5 with 5i=1 `i = k do
4:
define P` as the set of partitions (V1 , V2 ) of V such that for each i, `i is
the number of voters in V1 whose top choice is in Ri
5:
if P` 6=  then
6:
let (V1 , V2 ) be an arbitrary partition from P`
7:
if p is a Condorcet winner of (C, V1 ) then
8:
if either c or p is a Condorcet winner of (C, V2 ) then
9:
return (V1 , V2 )
10:
else if  (V1 , V2 )  P` : (C, V2 ) has no Condorcet winner then
11:
return (V1 , V2 )
12: return there is no partition that makes p an overall Condorcet winner

and c constitute a region on their own, it is equally easy to tell whether they are Condorcet
winners (using Fact 3.1).
We have just shown that the queries in lines 7 and 8 of Algorithm 3 can be efficiently
answered. We now go on to show how the query in line 10 can be efficiently answered,
i.e., given `, is there a partition (V1 , V2 )  P` such that (C, V2 ) has no Condorcet winner.
Clearly, this cannot happen if there are an odd number of voters in V2 . So assume kV2 k to
be even and let m1 and m2 be the median voters. For each region Ri , we know the number
of V2 -voters whose top choice is in Ri (this number is kRi k  `i ). Thus we know in which
regions the median voters fall (again just by counting). Now, if at least one of m1 or m2 fall
in R2 or R4 (i.e., are p or c), then (C, V2 ) cannot possibly have a Condorcet winner other
than p or c (there may be no Condorcet winners). In any of these three cases, our partition
action was successful21 and we can return an arbitrary partition in P` .
The only remaining cases are that m1 and m2 both fall in R1  R3  R5 . If m1 and
m2 fall in different regions, there can obviously be no Condorcet winner and we are done.
Assume, therefore, that m1 and m2 both fall in Ri with i  {1, 3, 5}. Our goal is to assign
qi = kRi k  `i voters with top choice in Ri to V2 in such a way that the median pair in
V2 does not fall on the same candidate. Let the median pair be the rth and (r + 1)st
V2 -voter in Ri . Here, r (1  r  qi  1) is known (by the numbers of V2 -voters to the left
and right of Ri ) and we count from left to right with respect to the societal order L. We
will try to accomplish our goal by brute force, namely, for each pair of candidates (d` , dr )
in Ri with d` <L dr , let us try to ensure that the rth V2 -voter in Ri from the left falls
on or left of d` and the (r + 1)st falls on or right of dr . We can do this if and only if
k{x  V | x falls in Ri  x L d` }k  r and k{x  V | x falls in Ri  dr L x}k  qi  r.
The cost of this check is in O(kCk2 ), as for each pair of candidates in Ri , we do some easy
counting.
Summing up, the running time of Algorithm 3 can be bounded as follows. The number of
iterations for the loops in lines 1, 2, and 3, are bounded by kCk, kV k, and kV k5 , respectively.
The cost of one iteration of the inner loop is clearly dominated by the cost of answering
21. In fact, if either p or c is a Condorcet winner in (C, V2 ), this would already have been detected in line 8.

490

fiBypassing Combinatorial Protections

the query in line 10. This cost is bounded by O(kCk2 ), as argued in the last paragraph.
Altogether, this yields a running time that is obviously polynomial in the size of the input.
Correctness of Algorithm 3 should be clear from the explanations above, as we have
argued that we find a partition that makes p an overall Condorcet winner if and only if
such a partition exists. In particular, observe that we never need to consider cases with
k = 0 as p can never be a Condorcet winner of (C, ). And the case where p is already a
Condorcet winner of the original election (C, V ) is handled by setting k to kV k in line 2
(and c to some arbitrary candidate from C  {p} in line 1).
q
Theorem 5.7. For Condorcet elections, control by adding voters and control by deleting
voters are each in P for single-peaked electorates, in both the nonunique-winner model and
the unique-winner model.
Proof. Associate each voter with his or her most preferred candidate. Our goal is to
make p a Condorcet winner, i.e., we want to end up in a situation where p is the most
preferred candidate of the median voter(s).
Here is an easy algorithm for the case of control by addition of voters: See where the
current median (or median pair) is. If it is on p, we are done. Otherwise, add unregistered
voters whose top choice is p until we have added all those or we succeed or we hit our
addition bound. If we succeed or hit our addition bound, we are done (with or without
success). If we have not yet hit our addition bound, move on as follows. If p is strictly
between two median voters at this point, success is impossible. But if it is one of two distinct
median voters (without loss of generality say it is the rightmost of the two median voters)
or the median interval does not contain p (without loss of generality say the interval lies to
the left of p), then add unregistered voters to the right of p until we make p a Condorcet
winner or we hit our addition bound or we have added all the unregistered voters of this
sort. If at this point we have not succeeded, success is impossible.
The algorithm for the control-by-deletion-of-voters case is similar: See where the current
median (or median pair) is. If it is on p, we are done. Otherwise, we need to shift the location
of the median voter(s) towards p. Without loss of generality assume that the median (or
median interval) lies to the left of p. Then we start deleting voters at the left end of L until
we make p a Condorcet winner or we hit our deletion bound without success. Note that if p
initially lies in between the two median voters, it can never be made a Condorcet winner
by deleting voters.
q

References
Barbera, S. (2001). An introduction to strategy-proof social choice functions. Social Choice
and Welfare, 18 (4), 619653.
Bartholdi, III, J., & Orlin, J. (1991). Single transferable vote resists strategic voting. Social
Choice and Welfare, 8 (4), 341354.
Bartholdi, III, J., Tovey, C., & Trick, M. (1989). The computational difficulty of manipulating an election. Social Choice and Welfare, 6 (3), 227241.
491

fiBrandt, Brill, Hemaspaandra, & Hemaspaandra

Bartholdi, III, J., Tovey, C., & Trick, M. (1992). How hard is it to control an election?
Mathematical and Computer Modeling, 16 (8/9), 2740.
Bartholdi, III, J., & Trick, M. (1986). Stable matching with preferences derived from a
psychological model. Operations Research Letters, 5 (4), 165169.
Black, D. (1948). On the rationale of group decision-making. Journal of Political Economy,
56 (1), 2334.
Black, D. (1958). The Theory of Committees and Elections. Cambridge University Press.
Booth, K., & Lueker, G. (1976). Testing for the consecutive ones property, interval graphs,
and graph planarity using PQ-tree algorithms. Journal of Computer and System
Sciences, 13 (3), 335379.
Brandt, F., Brill, M., Hemaspaandra, E., & Hemaspaandra, L. (2010). Bypassing combinatorial protections: Polynomial-time algorithms for single-peaked electorates. In Proceedings of the 24th AAAI Conference on Artificial Intelligence, pp. 715722. AAAI
Press.
Bredereck, R., Chen, J., & Woeginger, G. (2013). Are there any nicely structured preference
profiles nearby? In Proceedings of the 23rd International Joint Conference on Artificial
Intelligence, pp. 6268. AAAI Press.
Condorcet, J. (1785). Essai sur lApplication de LAnalyse a la Probabilite des Decisions
Rendues a la Pluralite des Voix. Facsimile reprint of original published in Paris, 1972,
by the Imprimerie Royale.
Conitzer, V. (2009). Eliciting single-peaked preferences using comparison queries. Journal
of Artificial Intelligence Research, 35, 161191.
Conitzer, V., Sandholm, T., & Lang, J. (2007). When are elections with few candidates
hard to manipulate? Journal of the ACM, 54 (3), Article 14.
Copeland, A. (1951). A reasonable social welfare function. Mimeographed notes from
Seminar on Applications of Mathematics to the Social Sciences, University of Michigan.
Cornaz, D., Galand, L., & Spanjaard, O. (2012). Bounded single-peaked width and proportional representation. In Proceedings of the 20th European Conference on Artificial
Intelligence, pp. 270275. IOS Press.
Cornaz, D., Galand, L., & Spanjaard, O. (2013). Kemeny elections with bounded singlepeaked or single-crossing width. In Proceedings of the 23rd International Joint Conference on Artificial Intelligence, pp. 7682. AAAI Press.
Davis, O., Hinich, M., & Ordeshook, P. (1970). An expository development of a mathematical model of the electoral process. American Political Science Review, 54 (2),
426448.
Dodgson, C. (1876). A method of taking votes on more than two issues. Pamphlet printed
by the Clarendon Press, Oxford, and headed not yet published.
Doignon, J., & Falmagne, J. (1994). A polynomial time algorithm for unidimensional unfolding representations. Journal of Algorithms, 16 (2), 218233.
492

fiBypassing Combinatorial Protections

Duggan, J., & Schwartz, T. (2000). Strategic manipulability without resoluteness or shared
beliefs: GibbardSatterthwaite generalized. Social Choice and Welfare, 17 (1), 8593.
Dwork, C., Kumar, R., Naor, M., & Sivakumar, D. (2001). Rank aggregation methods for
the web. In Proceedings of the 10th International World Wide Web Conference, pp.
613622. ACM Press.
Ephrati, E., & Rosenschein, J. (1997). A heuristic technique for multi-agent planning.
Annals of Mathematics and Artificial Intelligence, 20 (14), 1367.
Erdelyi, G., Lackner, M., & Pfandler, A. (2013). Computational aspects of nearly singlepeaked electorates. In Proceedings of the 27th AAAI Conference on Artificial Intelligence, pp. 283289.
Erdelyi, G., Nowak, M., & Rothe, J. (2009). Sincere-strategy preference-based approval
voting fully resists constructive control and broadly resists destructive control. Mathematical Logic Quarterly, 55 (4), 425443.
Erdelyi, G., Piras, L., & Rothe, J. (2011). The complexity of voter partition in Bucklin
and fallback voting: Solving three open problems. In Proceedings of the 10th International Conference on Autonomous Agents and Multiagent Systems, pp. 837844.
International Foundation for Autonomous Agents and Multiagent Systems.
Erdelyi, G., & Rothe, J. (2010). Control complexity in fallback voting. In Proceedings the
16th Australasian Theory Symposium, pp. 3948. Australian Computer Society.
Escoffier, B., Lang, J., & Ozturk, M. (2008). Single-peaked consistency and its complexity.
In Proceedings of the 18th European Conference on Artificial Intelligence, pp. 366370.
IOS Press.
Faliszewski, P. (2008). Nonuniform bribery. In Proceedings of the 7th International Conference on Autonomous Agents and Multiagent Systems, pp. 15691572. International
Foundation for Autonomous Agents and Multiagent Systems.
Faliszewski, P., Hemaspaandra, E., & Hemaspaandra, L. (2009). The complexity of bribery
in elections. Journal of Artificial Intelligence Research, 35, 485532.
Faliszewski, P., Hemaspaandra, E., & Hemaspaandra, L. (2010). Using complexity to protect
elections. Communications of the ACM, 53 (11), 7482.
Faliszewski, P., Hemaspaandra, E., & Hemaspaandra, L. (2011a). The complexity of manipulative attacks in nearly single-peaked electorates. Tech. rep. arXiv:1105.5032 [cs.GT],
Computing Research Repository, arXiv.org/corr/. Revised, July 2012.
Faliszewski, P., Hemaspaandra, E., & Hemaspaandra, L. (2011b). The complexity of manipulative attacks in nearly single-peaked electorates. In Proceedings of the 13th Conference on Theoretical Aspects of Rationality and Knowledge, pp. 228237. ACM Digital
Library.
Faliszewski, P., Hemaspaandra, E., & Hemaspaandra, L. (2013). Weighted electoral control. In Proceedings of the 12th International Conference on Autonomous Agents and
Multiagent Systems, pp. 367374. International Foundation for Autonomous Agents
and Multiagent Systems.
493

fiBrandt, Brill, Hemaspaandra, & Hemaspaandra

Faliszewski, P., Hemaspaandra, E., & Hemaspaandra, L. (2014). The complexity of manipulative attacks in nearly single-peaked electorates. Artificial Intelligence, 207, 6999.
Faliszewski, P., Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2009a). Llull and
Copeland voting computationally resist bribery and constructive control. Journal of
Artificial Intelligence Research, 35, 275341.
Faliszewski, P., Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2009b). A richer understanding of the complexity of election systems. In Ravi, S., & Shukla, S. (Eds.), Fundamental Problems in Computing: Essays in Honor of Professor Daniel J. Rosenkrantz,
pp. 375406. Springer.
Faliszewski, P., Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2011). The shield that
never was: Societies with single-peaked preferences are more open to manipulation
and control. Information and Computation, 209, 89107.
Faliszewski, P., Hemaspaandra, E., & Schnoor, H. (2008). Copeland voting: Ties matter.
In Proceedings of the 7th International Conference on Autonomous Agents and Multiagent Systems, pp. 983990. International Foundation for Autonomous Agents and
Multiagent Systems.
Fishburn, P. (1977). Condorcet social choice functions. SIAM Journal on Applied Mathematics, 33 (3), 469489.
Friedgut, E., Kalai, G., Keller, N., & Nisan, N. (2011). A quantitative version of the
Gibbard-Satterthwaite Theorem for three alternatives. SIAM Journal on Computing,
40 (3), 934952.
Friedgut, E., Kalai, G., & Nisan, N. (2008). Elections can be manipulated often. In Proceedings of the 49th IEEE Symposium on Foundations of Computer Science, pp. 243249.
IEEE Computer Society Press.
Fulkerson, D., & Gross, G. (1965). Incidence matrices and interval graphs. Pacific Journal
of Mathematics, 15 (5), 835855.
Garey, M., & Johnson, D. (1979). Computers and Intractability: A Guide to the Theory of
NP-Completeness. W. H. Freeman.
Ghosh, S., Mundhe, M., Hernandez, K., & Sen, S. (1999). Voting for movies: The anatomy of
recommender systems. In Proceedings of the 3rd Annual Conference on Autonomous
Agents, pp. 434435. ACM Press.
Gibbard, A. (1973). Manipulation of voting schemes. Econometrica, 41 (4), 587601.
Hagele, G., & Pukelsheim, F. (2001). The electoral writings of Ramon Llull. Studia Lulliana,
41 (97), 338.
Hemachandra, L. (1989). The strong exponential hierarchy collapses. Journal of Computer
and System Sciences, 39 (3), 299322.
Hemaspaandra, E., & Hemaspaandra, L. (2000). Computational politics: Electoral systems.
In Proceedings of the 25th International Symposium on Mathematical Foundations of
Computer Science, pp. 6483. Springer-Verlag Lecture Notes in Computer Science
#1893.
494

fiBypassing Combinatorial Protections

Hemaspaandra, E., & Hemaspaandra, L. (2007). Dichotomy for voting systems. Journal of
Computer and System Sciences, 73 (1), 7383.
Hemaspaandra, E., Hemaspaandra, L., & Menton, C. (2013). Search versus decision for
election manipulation problems. In Proceedings of the 30th Annual Symposium on
Theoretical Aspects of Computer Science, pp. 377388. Leibniz International Proceedings in Informatics (LIPIcs).
Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (1997). Exact analysis of Dodgson
elections: Lewis Carrolls 1876 voting system is complete for parallel access to NP.
Journal of the ACM, 44 (6), 806825.
Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2007). Anyone but him: The complexity
of precluding an alternative. Artificial Intelligence, 171 (56), 255285.
Hemaspaandra, E., Hemaspaandra, L., & Rothe, J. (2009). Hybrid elections broaden
complexity-theoretic resistance to control. Mathematical Logic Quarterly, 55 (4), 397
424.
Hemaspaandra, E., Spakowski, H., & Vogel, J. (2005). The complexity of Kemeny elections.
Theoretical Computer Science, 349 (3), 382391.
Hemaspaandra, L., & Williams, R. (2012). An atypical survey of typical-case heuristic
algorithms. SIGACT News, 43 (4), 7189.
Kemeny, J. (1959). Mathematics without numbers. Ddalus, 88 (4), 571591.
Kemeny, J., & Snell, L. (1960). Mathematical Models in the Social Sciences. Ginn.
Konczak, K., & Lang, J. (2005). Voting procedures with incomplete preferences. In Proceedings of the Multidisciplinary IJCAI-05 Workshop on Advances in Preference Handling,
pp. 124129.
Kramer, G. (1977). A dynamical model of political equilibrium. Journal of Economic
Theory, 16 (2), 310334.
Krehbiel, K. (1998). Pivotal Politics: A Theory of U.S. Lawmaking. University of Chicago
Press.
McCabe-Dansted, J., Pritchard, G., & Slinko, A. (2008). Approximability of Dodgsons
rule. Social Choice and Welfare, 31 (2), 311330.
Menton, C. (2013). Normalized range voting broadly resists control. Theory of Computing
Systems, 53 (4), 507531.
Nanson, E. (1882). Methods of election. Transactions and Proceedings of the Royal Society of
Victoria, 19, 197240. Available as a 2009 facsimile reprint from Kessinger Publishing.
Niemi, R., & Wright, J. (1987). Voting cycles and the structure of individual preferences.
Social Choice and Welfare, 4 (3), 173183.
Niou, E. (1987). A note on Nansons Rule. Public Choice, 54 (2), 191193.
Papadimitriou, C., & Zachos, S. (1983). Two remarks on the power of counting. In Proceedings of the 6th GI Conference on Theoretical Computer Science, pp. 269276.
Springer-Verlag Lecture Notes in Computer Science #145.
495

fiBrandt, Brill, Hemaspaandra, & Hemaspaandra

Pennock, D., Horvitz, E., & Giles, C. (2000). Social choice theory and recommender systems:
Analysis of the axiomatic foundations of collaborative filtering. In Proceedings of the
17th National Conference on Artificial Intelligence, pp. 729734. AAAI Press.
Procaccia, A., & Rosenschein, J. (2007). Junta distributions and the average-case complexity
of manipulating elections. Journal of Artificial Intelligence Research, 28, 157181.
Rothe, J., Spakowski, H., & Vogel, J. (2003). Exact complexity of the winner problem for
Young elections. Theory of Computing Systems, 36 (4), 375386.
Satterthwaite, M. (1975). Strategy-proofness and Arrows conditions: Existence and correspondence theorems for voting procedures and social welfare functions. Journal of
Economic Theory, 10 (2), 187217.
Schwartz, T. (1972). Rationality and the myth of the maximum. Nous, 6 (2), 97117.
Simpson, P. (1969). On defining areas of voter choice: Professor Tullock on stable voting.
The Quarterly Journal of Economics, 83 (3), 478490.
Sui, X., Boutilier, C., & Sandholm, T. (2013). Analysis and optimization of multidimensional percentile mechanisms. In Proceedings of the 23rd International Joint
Conference on Artificial Intelligence, pp. 367374. AAAI Press.
Walsh, T. (2007). Uncertainty in preference elicitation and aggregation. In Proceedings of
the 22nd AAAI Conference on Artificial Intelligence, pp. 38. AAAI Press.
Walsh, T. (2009). Where are the really hard manipulation problems? The phase transition in
manipulating the veto rule. In Proceedings of the 21st International Joint Conference
on Artificial Intelligence, pp. 324329. AAAI Press.
Young, H. (1977). Extending Condorcets rule. Journal of Economic Theory, 16 (2), 335
353.

496

fiJournal of Artificial Intelligence Research 53 (2015) 271-314

Submitted 12/14; published 07/15

Model Theory of XPath on Data Trees.
Part I: Bisimulation and Characterization
Diego Figueira

diego.figueira@labri.fr

CNRS, LaBRI, France

Santiago Figueira

santiago@dc.uba.ar

Universidad de Buenos Aires and CONICET, Argentina

Carlos Areces

carlos.areces@gmail.com

Universidad Nacional de Cordoba and CONICET, Argentina

Abstract
We investigate model theoretic properties of XPath with data (in)equality tests over
the class of data trees, i.e., the class of trees where each node contains a label from a finite
alphabet and a data value from an infinite domain.
We provide notions of (bi)simulations for XPath logics containing the child, parent,
ancestor and descendant axes to navigate the tree. We show that these notions precisely characterize the equivalence relation associated with each logic. We study formula
complexity measures consisting of the number of nested axes and nested subformulas in a
formula; these notions are akin to the notion of quantifier rank in first-order logic. We show
characterization results for fine grained notions of equivalence and (bi)simulation that take
into account these complexity measures. We also prove that positive fragments of these
logics correspond to the formulas preserved under (non-symmetric) simulations. We show
that the logic including the child axis is equivalent to the fragment of first-order logic
invariant under the corresponding notion of bisimulation. If upward navigation is allowed
the characterization fails but a weaker result can still be established. These results hold
both over the class of possibly infinite data trees and over the class of finite data trees.
Besides their intrinsic theoretical value, we argue that bisimulations are useful tools to
prove (non)expressivity results for the logics studied here, and we substantiate this claim
with examples.

1. Introduction
We study the expressive power and model theory of XPatharguably the most widely used
XML query language. Indeed, XPath is implemented in XSLT and XQuery and it is used
as a constituent part of many specification and update languages. XPath is, fundamentally,
a general purpose language for addressing, searching, and matching pieces of an XML
document. It is an open standard and constitutes a World Wide Web Consortium (W3C)
Recommendation (Clark & DeRose, 1999).
Core-XPath (term coined in Gottlob, Koch, & Pichler, 2005) is the fragment of XPath
1.0 containing the navigational behavior of XPath. It can express properties of the underlying tree structure of the XML document, such as the label (tag name) of a node, but it
cannot express conditions on the actual data contained in the attributes. In other words, it
only allows reasoning about trees over a finite alphabet. Core-XPath has been well studied
and its satisfiability problem is known to be decidable even in the presence of Document
c
2015
AI Access Foundation. All rights reserved.

fiFigueira, Figueira, & Areces

Type Definitions (DTDs) (Marx, 2004; Benedikt, Fan, & Geerts, 2008). Moreover, it is
known that it is equivalent to FO2 (first-order logic with two variables) over an appropriate
signature on trees in terms of expressive power (Marx & de Rijke, 2005), and that it is
strictly less expressive than PDL with converse over trees (Benedikt & Koch, 2008). From
a database perspective, however, Core-XPath fails to include the single most important
construct in a query language: the join. Without the ability to relate nodes based on the
actual data values of the attributes, the logics expressive power is inappropriate for many
applications.
The extension of Core-XPath with (in)equality tests between attributes of elements
in an XML document is named Core-Data-XPath in the work of Bojanczyk, Muscholl,
Schwentick, and Segoufin (2009). Here, we will call this logic XPath= . Models of XPath=
are data trees which can be seen as XML documents. A data tree is a tree whose nodes
contains a label from a finite alphabet and a data value from an infinite domain (see Figure 1
for an example). We will relax the condition on finiteness and consider also infinite data
trees, although all our results hold also on finite structures.
The main characteristic of XPath= is to allow formulas of the form h = i, where
,  are path expressions, that navigate the tree using axes: descendant, child, ancestor,
next-sibling, etc., and can make tests in intermediate nodes. The formula is true at a
node x of a data tree if there are nodes y, z that can be reached by the relations denoted
by , , respectively, and such that the data value of y is equal to the data value of z.
Recent articles investigate several algorithmic problems of logics evaluated over data
trees. For example, satisfiability and evaluation are discussed in the works of Figueira
(2010) and Bojanczyk and Parys (2011). In particular, all the logics studied in this article
have a decidable satisfiability problem (Figueira & Segoufin, 2011; Figueira, 2012); but tools
to investigate their expressive power are still lacking. There are good reasons for this: in
the presence of joins and data values, classical notions such as Ehrenfeucht-Frasse games
or structural bisimulations are difficult to handle. In this article we take the first steps
towards understanding the expressive power and model theory of XPath= on data trees.
In this article we focus on the basic model theory tool of bisimulations, which defines the
structural conditions necessary for ensuring that two models coincide in all the properties
expressible by a logic. Whereas the basic notion of bisimulation was introduced for the
basic modal logic, one can find adequate notions of bisimulations for different logics, in the
sense that they capture the notion of indistiguishability of the logic. The challenge here is
to find adequate notions of bisimulation for logics such as XPath, whose navigation is akin
to modal logics such as PDL, but which can also test for equality of data values in the data
tree.
Contribution: XPath= can navigate the data tree by means of axes like child (that we
will note ), descendant ( ), parent (), ancestor ( ), etc. XPath= can also navigate the
data tree horizontally, by going to a next or previous sibling of the current node. However,
we focus on the vertical axes that allow downward and upward exploration. In particular,
we will discuss the following languages: XPath= () (XPath= with ); XPath= () (XPath=
with  and ); XPath= ( ) (XPath= with  and  ); XPath= (  ) (XPath= with ,
,  and  ); and its positive fragments. Our main contributions can be summarized as
follows:
272

fiModel Theory of XPath on Data Trees. Part I: Bisimulation and Characterization

 In 3 we introduce bisimulation notions for XPath= (), XPath= ( ), XPath= ( ),
XPath= (), XPath= (  ) and show that they precisely characterize the logical
equivalence relation of the corresponding logic. We also consider fine grained versions
of these bisimulations indexed by two measures of formula complexity. The first
measure of formula complexity consists on the maximum number of nested axes in a
formula, which we call downward depth in the case of XPath= () and vertical depth in
the case of XPath= (). The second one is the number of nested subformulas, called
nesting depth.
The notion of bisimulation for XPath= () relies on a normal form which we also
introduce. Basically, this normal form restricts the navigation of the expressions to
be very simple: either going downward or going upward and then downward. Similar
normal forms for langauges on trees are folklore, and our work here consists mainly
in adapting them to the setup of tests for data values.
We also show that the simulations associated to the defined bisimulations characterize
the positive fragments of the logics: a formula is equivalent to a positive formula if
and only if it is invariant under simulations.
 In 4 we characterize XPath= () as the fragment of first-order logic over data trees
(over a signature that includes the child relation and an equivalence relation) that
is invariant under bisimulations. If we consider XPath= () instead the characterization fails as we show in a counter-example. However, a weaker result can still be
established, namely that if a first-order formula is bisimulation-invariant, for the bisimulation notion corresponding to any fixed number of nested axes, then it is equivalent
to an XPath= () formula.
 Using bisimulations we show some (non)expressivity results about XPath= in 5. We
show, for example, that formulas of XPath= () with nesting depth n+1 and downward
depth d have more expressive power than those of nesting and downward depth n and
d respectively, as long as n < d.
 All results are proved both over the class of arbitrary (possibly infinite) data trees,
and over the class of finite data trees.
1.1 Related Work
The notion of bisimulation was introduced independently by van Benthem (1976) in the
context of modal correspondence theory, by Milner (1980) and Park (1981) in concurrency
theory, and by Forti and Honsell (1983) in non-wellfounded set theory (for a historical
outlook see the work of Sangiorgi, 2009). This classical work defines a standard notion of
bisimulation but this notion has to be suitably adapted for a particular, given logic. The
notion of bisimulation for a given logic L defines when two models are indistinguishable
for L, that is, when there is no formula of L that is true in one model but false in the
other. XPath has been known to be closely connected to known modal languages, such
as PDL and modal -calculus, depending on the fragments taken into account (ten Cate,
Fontaine, & Litak, 2010). However, the fragments studied hitherto are data unaware,
that is, allowing to express of the structure of the model as well of the fixed set of labels.
273

fiFigueira, Figueira, & Areces

To the best of our knowledge the present is the first work on bisimulations and invariance
with logics with data tests.
Bisimulations can also be used to obtain model theoretic characterizations that identifies
the expressive power of a logic L1 in terms of the bisimulation invariant fragment of a
logic L2 which, hopefully, is better understood. The challenge, here, is to pinpoint both
the appropriate notion of bisimulation required and the adequate framework logic L2 .
The classical example of a result of this kind is Van Benthems characterization for the
basic modal logic as the bisimulation (with the standard notion of bisimulation) invariant
fragment of first-order logic (van Benthem, 1976). Van Benthems original result over
arbitrary structures was proved to hold for finite structures by Rosen (1997). The proof was
then simplified and unified by Otto (2004a, 2006), and later expanded by Dawar and Otto
(2009) to other classes of structures. Other formalisms of different expressive power have
also been considered for querying data trees, such as first-order logic with two variables
(Bojanczyk et al., 2009), tree patterns (David, 2008; Figueira & Libkin, 2014), register
automata (Neven, Schwentick, & Vianu, 2004), -calculus with registers (Jurdzinski &
Lazic, 2011), or datalog programs (Abiteboul, Bourhis, Muscholl, & Wu, 2013). In the
absence of data values, logics for semi-structured databases can often be seen as modal
logics. In fact, structural characterizations for XPath without equality test were studied in
the work of Gyssens, Paredaens, Gucht, and Fletcher (2006), and XPath is known to be
captured by PDL (Harel, 1984), whose bisimulation is well-understood (Blackburn, de Rijke,
& Venema, 2001). It is then natural to look for an intuitive bisimulation definition for
XPath= .
The first significant result concerning an algorithmic solution to the bisimulation problem for the basic modal logic was given by Hopcroft (1971), with a polynomial time algorithm for state minimization in a deterministic finite automaton. This problem is equivalent
to determine a coarsest partition of a set that is stable with respect to a finite set of functions. Paige and Tarjan (1987) solved the problem for the more general case, where the
restriction of stability concerns a finite set of relations. Kanellakis and Smolka (1990) are
the first to recognize that the algorithm of Paige and Tarjan can be used to determine the
maximum bisimulation for the basic modal logic in an arbitrary graph. Hence it can be
decided in polynomial time whether two nodes in finite models are bisimilar for the basic
modal logic though the length of the actual formulas which distinguish two non-bisimilar
nodes cannot be polynomially bounded with respect to the size of the models (Figueira &
Gorn, 2010). In our case, deciding whether two nodes in finite data trees are bisimilar can
also be solved in polynomial time.
A preliminary version of the present paper appeared in a work of Figueira, Figueira, and
Areces (2014). Also, there is a continuation (a Part II): a work by Abriola, Descotte, and
Figueira (2015), after the preliminary version by Abriola, Descotte, and Figueira (2014),
where they address further model theoretical questions such as definability and separation,
and bisimulation notions for pairs of nodes (instead of single nodes) to capture the idea of
indistinguishability by means of path expressions (instead of node expressions).
274

fiModel Theory of XPath on Data Trees. Part I: Bisimulation and Characterization

2. Preliminaries
Let N = {1, 2, 3, . . . } and let [n] = {1, . . . , n} for n  N. We use the symbol A to denote
a finite alphabet, and D to denote an infinite domain (e.g., N) of data values. In our
examples we will consider D = N. We write  for the empty string.
Let Trees(A) be the set of ordered and unranked trees over an arbitrary alphabet A. We
say that T is a data tree if it is a tree from Trees(A  D) where A is a finite set of labels
and D is an infinite set of data values. Figure 1 shows an example of a (finite) data tree.

x
y

a, 2

a, 2

b, 2

b, 9

z b, 5

b, 3

a, 2

b, 1

b, 2

Figure 1: A data tree of Trees(AD) with A = {a, b} and D = N.
A data tree is finitely branching if every node has finitely many children. For any
given data tree T , we denote by T its set of nodes. We use letters x, y, z, v, w as variables
for nodes. Given a node x  T of T , we write label (x)  A to denote the nodes label, and
data(x)  D to denote the nodes data value.
n
Given two nodes x, y  T we write xy if y is a child of x, and xy if y is a descendant
1
0
of x at distance n. In particular,  is the same as , and  is the identity relation. We

n
write xy to denote that (x, y) is in the reflexive transitive closure of . (x) denotes
n
the set of all descendants of x at distance n, and (y) denotes the sole ancestor of y at
distance n (assuming it has one).
Let P be a property on nodes of data trees. When property P is true at node u of data
tree T , we say that (T , u) satisfies P . For any binary relation R over nodes of data trees,
we say that a property P is R-invariant whenever the following condition holds: for every
data tree T and u  T , if (T , u) satisfies P and (T , u) is R-related to (T 0 , u0 ) then (T 0 , u0 )
satisfies P .
We introduce the query language XPath adapted to data trees as abstractions of XML
documents. We work with a simplification of XPath, stripped of its syntactic sugar. We
consider fragments of XPath that correspond to the navigational part of XPath 1.0 with data
equality and inequality. XPath= is a two-sorted language, with path expressions (that
we write , , ) and node expressions (that we write , , ). The fragment XPath= (O),
with O  {,  , ,  }, is defined by mutual recursion as follows:
,  ::= o |  |    | []
,  ::= a |  |    |    | hi | h = i | h 6= i

o  O  {}

aA

A formula of XPath= (O) is either a node expression or a path expression.
We formally define the semantics of XPath= as follows, for T a data tree:
275

fiFigueira, Figueira, & Areces

[[]]T = {(x, y) | xy}

[[ ]]T = reflexive transitive closure of [[]]T
[[]]T = {(x, y) | yx}

[[ ]]T = reflexive transitive closure of [[]]T
[[]]T = {(x, x) | x  T }

[[]]T = {(x, z) | (y  T ) (x, y)  [[]]T , (y, z)  [[]]T }

[[  ]]T = [[]]T  [[]]T

[[[]]]T = {(x, x) | x  [[]]T }

[[a]]T = {x  T | label (x) = a}

[[]]T = T \ [[]]T

[[  ]]T = [[]]T  [[]]T

[[hi]]T = {x  T | (y  T ) (x, y)  [[]]T }

[[h = i]]T = {x  T | (y,z  T )(x, y)  [[]]T , (x, z)  [[]]T , data(y) = data(z)}
[[h 6= i]]T = {x  T | (y,z  T )(x, y)  [[]]T , (x, z)  [[]]T , data(y) 6= data(z)}
As an example, if T is the data tree shown in Figure 1, then
[[h [ b  h[b] 6= [b]i ]i]]T = {x, y, z},

and the formula reads: there is a descendant node labeled b, with two children labeled b
with different data values.
For a data tree T and u  T , we say that T , u is a pointed data tree, we write
T , u |=  to denote u  [[]]T , and we say that T , u satisfies . We say that the node
expressions ,  of XPath= are equivalent (notation:   ) iff [[]]T = [[]]T for all data
trees T . Similarly, path expressions ,  of XPath= are equivalent (notation:   ) iff
[[]]T = [[]]T for all data trees T .
The fragment of downward XPath is denoted XPath= () and vertical XPath is
denoted XPath= ().
In terms of expressive power, it is easy to see that  is unessential: every XPath= node
expression  has an equivalent 0 with no  in its path expressions. 0 can be computed
in exponential time without incrementing the maximum number of nested axes or the
maximum number of nested subformulas. It is enough to use the following equivalences to
eliminate occurrences of 
h fi i  h fi i

h(  0 ) 0 i  h 0 i  h0  0 i

h fi (  0 ) 0 i  h fi  0 i  h fi 0  0 i
where fi  {=, 6=}. We will henceforth assume that formulas do not contain union of path
expressions. In the sequel we will see that in some situations also expressions of the form
[] can be sometimes avoided (3.2.1), although not when we only have downward axes
(Lemma 10).
276

fiModel Theory of XPath on Data Trees. Part I: Bisimulation and Characterization

2.1 Translating to First-Order Logic
In this section we show that there is a truth-preserving translation from XPath= () to
first-order logic over an appropriate signature. To do so, we first must interpret data trees
into relational structures, and we do this in the most standard way: using a binary child
relation, an equivalence relation for testing data equality, and monadic relations to test for
labels. Fix the signature  with binary relations
and , and a unary predicate Pa for
each a  A. Any data tree T can be seen as a first-order -structure such that
T

T

PaT

= {(x, y)  T 2 | y is a child of x};

= {(x, y)  T 2 | data(x) = data(y)};
= {x  T | label (x) = a}.

We can now give the translation from XPath to first-order logic over . The tranlsation
function is indexed by the free variables of the formula produced one for node expressions,
and two for path expressions.
(a  A)

Trx (a) = Pa (x)
Trx (  ) = Trx ()  Trx ()

(  {, })

Trx () = Trx ()

Trx (hi) = (y)Trx,y ()


Trx (h = i) = (y)(z) y  z  Trx,y ()  Trx,z ()

Trx (h 6= i) = (y)(z) y 6 z  Trx,y ()  Trx,z ()

(y a new variable)
(y, z new variables)
(y, z new variables)

Trx,y () = (x = y)

Trx,y () = (x

y)

Trx,y () = (y

x)


Trx,y () = (z) Trx,z ()  Trz,y ()

(z a new variable)

Trx,y (  ) = Trx,y ()  Trx,y ()
Trx,y ([]) = Trx ()  (x = y).

Proposition 1.
1. If  is an XPath= () node expression, then u  [[]]T iff T |= Trx ()(u).
2. If  is an XPath= () path expression, then (u, v)  [[]]T iff T |= Trx,y ()(u, v).
Proof. The proof is by structural induction on  and .
For   FO(), let qr() be its quantifier rank, i.e., the depth of nesting of its quantifiers.
Observe that for fi  {=, 6=} we have
qr(Trx (hi)) = 1 + qr(Trx ())
qr(Trx (h fi i)) = 2 + max(qr(Trx ()), qr(Trx ()))

qr(Trx,y ()) = 1 + max(qr(Trx,y ()), qr(Trx,y ()))
qr(Trx,y ([])) = qr(Trx ()).
277

fiFigueira, Figueira, & Areces

3. Bisimulation
In this section we define notions of bisimulation for the downward and vertical fragments of
XPath, and we show that they coincide with the corresponding logical equivalence relation.
For the case of vertical XPath the bisimulation notion relies on a normal form which we
introduce for this purpose.
3.1 Downward XPath
We write dd() to denote the downward depth of  defined as follows:
dd(a)
dd(  )
dd()
dd(hi)
dd(h fi i)

=
=
=
=
=

0
dd() =
max{dd(), dd()}
dd() =
dd()
dd([]) =
dd()
dd() =
max{dd(), dd()}

0
dd()
max{dd(), dd()}
1 + dd()

where a  A, fi  {=, 6=}, and  is any path expression or the empty string . Let
`-XPath= () be the fragment of XPath= () consisting of all formulas  with dd()  `.
Let T and T 0 be data trees, and let u  T , u0  T 0 . We say that T , u and T 0 , u0
are equivalent for XPath= () (notation: T , u  T 0 , u0 ) iff for all node expression  
XPath= (), we have T , u |=  iff T 0 , u0 |= . We say that T , u and T 0 , u0 are `-equivalent
for XPath= () (notation: T , u ` T 0 , u0 ) iff for all node expression   `-XPath= (), we
have T , u |=  iff T 0 , u0 |= .1
We will first show that, for every `, there are finitely many different formulas  of
dd()  ` up to logical equivalence. Formally, this is usually referred by saying that ` has
finite index.
Proposition 2. ` has finite index.
Proof. It can be easily shown by induction that for any node expression   `-XPath= ()
with unnecessary uses of  (recall that   ) we have that qr(Trx ()) is bounded. It
is a well-known result of first order that there are finitely many nonequivalent formulas of
bounded quantifier rank. Hence there are finitely many nonequivalent node expressions of
bounded downward depth.
Corollary 3. {T 0 , u0 | T , u ` T 0 , u0 } is definable by a node expression `,T ,u of `XPath= ().
Proof. Consider the conjunction of all `-XPath= () formulas  such that T , u |= . By
Proposition 2, up to logical equivalence, there are finitely many such s, and hence the
conjunction is equivalent to a finite one. Define `,T ,u as this finite conjunction.
1. Two pointed data trees are equivalent when they are indistinguishable by the formulas of a given logic.
We adopted the terminology of the literature, and so we used the first word, and not he second. The
reader should not confuse this notion with that of equivalent formulas.

278

fiModel Theory of XPath on Data Trees. Part I: Bisimulation and Characterization

3.1.1 Bisimulation and `-bisimulation
Let T and T 0 be two data-trees. We say that u  T and u0  T 0 are bisimilar for
XPath= () (notation: T , u T 0 , u0 ) if there is a relation Z  T  T 0 such that uZu0 and
for all x  T and x0  T 0 we have
 Harmony: If xZx0 then label (x) = label (x0 ).
n

m

n

m

 Zig: If xZx0 , xv and xw then there are v 0 , w0  T 0 such that x0 v 0 , x0 w0 and
1. data(v) = data(w)  data(v 0 ) = data(w0 ),
i

i

2. (v) Z (v 0 ) for all 0  i < n, and
i

i

3. (w) Z (w0 ) for all 0  i < m.

The following picture illustrates the intended requirements.
T

x

x

T0

0

Z
n
m

9v 0

n

=

)

8w

(6=

(6=

)

=

8v

9w0

m

n

m

 Zag: If xZx0 , x0 v 0 and x0 w0 then there are v, w  T such that xv, xw and
items 1, 2 and 3 above are verified.
This bisimulation generalizes the classical bisimulation relation (Sangiorgi, 2009), in
which the Zig is simply: If xZx0 , xv, there is v 0 such that x0 v 0 and vZv 0 (the Zag being
symmetrical). In fact, if we restrict our Zig condition to having n = m = 1 and v = w,
and the Zag condition to n = m = 1 and v 0 = w0 , we obtain a relation that corresponds,
precisely, to the classical bisimulation relation. Thus, being bisimilar for the notion we
define implies being bisimlar for the classical notion.
For example, the dotted lines in the following two data trees represent a bisimulation
for XPath= ().
$#

x

a, 1

y1

a, 2 a, 2

y2

x0

a, 1

a, 2

y0

v a, 1 a, 3 w

w0a, 3 a, 1 v 0

T

T0

279

fiFigueira, Figueira, & Areces

This notion of bisimulation can be seen, as usual, as an Ehrenfeucht-Frasse game, where
Spoiler tries to find a difference (through the logics glasses) between nodes u and u0 , while
Duplicator tries to copy him, showing that u and u0 are indistinguishable. To gain intuition
on our notion of bisimulation, we will briefly explain its associated game (without going
into the technical details). The board consists of the data trees T and T 0 , and there are
two pebbles p and p0 , so that all along the game, p will always be over a node of T , and
p0 will always be over a node of T 0 . Initially, p is over u, and p0 is over u0 . If u and u0 do
not satisfy the same label then Spoiler is declared winner and the game finishes. The game
proceeds by rounds. Suppose that at some round, the pebbles p and p0 are in positions x
and x0 satisfying the same label. A round consists of one move of Spoiler, followed by an
answer by Duplicator, and a final decision made by spoiler.
 Step 1. Spoilers first move: he chooses a pebble, two integers n and m, and two
paths, one of length m and the other of length n. These paths start in x if he chose p
or in x0 if he chose p0 . Suppose the chose p (the case for p0 is analogous but over data
m
tree T 0 instead of T ). His first path is represented by some w  T such that xw,
n
and his second path is represented by some v  T such that xv.
 Step 2. Duplicators answer: he shows two paths in T 0 (or in T in case duplicator
had chosen p0 instead of p), one of length m and the other of length n, both starting
on x0 , such that data(u) = data(v) iff data(u0 ) = data(v 0 ). If there are no such paths,
then Spoiler is declared the winner and the game finishes.
 Step 3. Final move by Spoiler: he chooses
i

i

 either i  {0 . . . n  1}, and places pebble p over (v) and pebble p0 over (v 0 )
i

i

 or i  {0 . . . m  1}, and places pebble p on (w) and pebble p0 over (w0 )

If now the pebbles are over two nodes which satisfy the same label, the game proceeds.
Else, Spoiler is declared the winner and the game finishes.
Duplicator wins if spoiler is never declared winner in a game at infinitely many rounds. The
resemblance of the game rules with Harmony, Zig and Zag is evident. One can see that
spoiler has a winning strategy in the game whose initial pebbles are placed over x and x0 if
and only if T , u T 0 , u0 .
It is interesting to compare this game to the one for capturing bisimulation for the
basic modal logic. In the latter, Spoiler just chooses a successor of x (or x0 ) following the
accessibility relation. This is also the case for Core-XPath or PDL. But in our case, because
of adding comparison of data values, Spoiler has to choose a whole path (in fact, two paths),
making the game less local that the one for the basic modal logic (or Core-XPath or PDL).
An analogous view for the game of XPath= () would be to allow Spoiler to build his paths
in a step-by-step fashion, that is, extending the paths constructed so far with a new single
node, thus giving Duplicator less certainty. Is it possible to change the rules of the game
so that in the second step of the round Spoiler builds his paths in a step-by-step fashion?
No. Even if Spoiler has the freedom to extend step-by-step only one of his paths, this
would be too unfair for Duplicator. Indeed, consider our last example of bisimilar T , x
and T 0 , x0 . Spoiler would initially declare that his first path will be x0 of length 0, and so
280

fiModel Theory of XPath on Data Trees. Part I: Bisimulation and Characterization

Duplicator can only define his first path to be x of length 0. Then Spoiler will construct,
step-by-step, his second path. Initially he shows the path x0 y 0 to Duplicator. Duplicator
has two possible answers: either the path xy1 or xy2 . Suppose the chooses xy1 (the
case for xy2 is analogous). Now Spoiler extends his path x0 y 0 to x0 y 0 w0 . Since y1
has only one child, Spoiler has one possible answer for extending his own path: xy1 v.
Next, Spoiler declares that he is done: he has constructed two paths starting in x0 , one is
of length 0 and the other one is x0 y 0 w0 , of length 2. Duplicator has constructed one
path of length 0 and the other as xy1 v. Duplicator looses, as data(x0 ) 6= data(w0 ) but
data(x) = data(v). This example shows that the game fairness is established when spoiler
tells duplicator in advance which are the two paths he chooses.
For a data tree T and u  T , let T |u denote the subtree of T induced by {v  T |
n
(n) uv}. Observe that the root of T |u is u. The following results are straightforward
consequences of the definition of bisimulation:
Observation 4. T , u  (T |u), u.
Observation 5. If T is a subtree of T 0 and u  T then T , u  T 0 , u.
We say that u  T and u0  T 0 are `-bisimilar for XPath= () (notation: T , u `
T
if there is a family of relations (Zj )j` in T  T 0 such that uZ` u0 and for all j  `,
x  T and x0  T 0 we have
0 , u0 )

 Harmony: If xZj x0 then label (x) = label (x0 ).
n

m

 Zig: If xZj x0 , xv and xw with n, m  j then there are v 0 , w0  T 0 such that
n
m
x0 v 0 , x0 w0 and
1. data(v) = data(w)  data(v 0 ) = data(w0 ),
i

i

2. (v) Zjn+i (v 0 ) for all 0  i < n, and
i

i

3. (w) Zjm+i (w0 ) for all 0  i < m.
n

m

 Zag: If xZj x0 , x0 v 0 and x0 w0 with n, m  j then there are v, w  T such that
n
m
xv, xw and items 1, 2 and 3 above are verified.
This notion of `-bisimulation corresponds to the game between spoiler and duplicator
at ` rounds.
Clearly if T , u T 0 , u0 then T , u` T 0 , u0 for all `.
Observation 6. Suppose T and T 0 have height at most `, u  T , and u0  T 0 . Then
T , u ` T 0 , u0 iff T , u  T 0 , u0 .
For a data tree T and u  T , let T |` u denote the subtree of T induced by {v  T |
n
(n  `) uv}.
Observation 7. T , u ` (T |` u), u.
281

fiFigueira, Figueira, & Areces

3.1.2 Equivalence and Bisimulation
We now show that  coincides with  on finitely branching data trees, and that `
coincides with ` .
Theorem 8.
1. T , u  T 0 , u0 implies T , u  T 0 , u0 . The converse also holds when T and T 0 are finitely
branching.
2. T , u ` T 0 , u0 iff T , u ` T 0 , u0 .
The above theorem will be shown as a consequence of Propositions 9 and 11:
Proposition 9. T , u ` T 0 , u0 implies T , u ` T 0 , u0 .
Proof. We actually show that if T , u` T 0 , u0 via (Zi )i` then for all 0  n  j  `, for all
 with dd()  j, and for all  with dd()  j:
1. If xZj x0 then T , x |=  iff T 0 , x0 |= ;
n

n

i

i

2. If xv, x0 v 0 and (v) Z(jn)+i (v 0 ) for all 0  i  n, then (x, v)  [[]]T iff
0
(x0 , v 0 )  [[]]T .
We show 1 and 2 by induction on || + ||.
Let us see item 1. The base case is  = a for some a  A. By Harmony, label (x) =
label (x0 ) and then T , x |=  iff T 0 , x0 |= . The Boolean cases for  are straightforward.
Suppose  = h = i. We show T , x |=   T 0 , x0 |= , so assume T , x |= .
n
m
Suppose there are v, w  T and n, m  j such that xv, xw, (x, v)  [[]]T , (x, w) 
n
m
[[]]T and data(v) = data(w). By Zig, there are v 0 , w0  T 0 such that x0 v 0 , x0 w0 ,
i
i
i
i
(v) Zjn+i (v 0 ) for all 0  i  n, (w) Zjm+i (w0 ) for all 0  i  m, and data(v 0 ) =
0
0
data(w0 ). By inductive hypothesis 2 (twice), (x0 , v 0 )  [[]]T and (x0 , w0 )  [[]]T . Hence
T 0 , x0 |= . The implication T 0 , x0 |=   T , x |=  is analogous. The case  = h 6= i is
shown similarly. The case  = hi is similar (and simpler) to the previous case.
Let us now analyze item 2. We only show the only if direction. The base case is when
0
  {, }. If  =  then v = x and so n = 0. Since v 0 = x0 , we conclude (x0 , v 0 )  [[]]T . If
0
 = then xv in T , and so n = 1. Since x0 v 0 , we have (x0 , v 0 )  [[]]T .
For the inductive step, let x0 , . . . , xn  T and x00 , . . . , x0n  T 0 be such that
x = x0 x1 x2     xn = v
0

x =

x00 x01 x02     x0n

=v

0

in T ,

in T 0 ,
0

and xi Zji x0i for all 0  i  n. Assume, for contradiction, that (x0 , v 0 ) 
/ [[]]T . Then, there
0
is a subformula  of  and k  {0, . . . , n} such that T , xk |=  and T , x0k 6|=  as the next
Lemma shows.
n

n

Lemma 10. Let  be a path expression of XPath= ( ). Let xv and x0 v 0 such
0
that (x, v)  [[]]T and (x0 , v 0 ) 
/ [[]]T . Then there is a subformula  of  and
k

k

k  {0, . . . , n} such that T , (v) |=  and T 0 , (v 0 ) 6|= .
282

fiModel Theory of XPath on Data Trees. Part I: Bisimulation and Characterization

Proof of Lemma. Let x = v0 v1     vn = v and x0 = v00 v10     vn0 = v 0 . We
proceed by induction on ||. If  =  then x = v and so n = 0. Hence x0 = v 0 and
0
(x0 , v 0 )  [[]]T , which contradicts the hypothesis, and thus the statement is trivially
0
true. If  = then xv and so n = 1. Hence x0 v 0 and (x0 , v 0 )  [[]]T . This case is
also trivial. The case  =  is similar.
0
Suppose  = []. Since (x0 , v 0 ) 
/ [[]]T , we have x0 = v 0 and T 0 , v 0 6|= . Taking
k = 0 and  =  the statement holds. Observe that  is a subformula of .
Suppose  = . Then there is k such that (x, vk )  [[]]T and (vk , v)  [[]]T .
0
0
0
Since (x0 , v 0 ) 
/ [[]]T , we have (x0 , vk0 ) 
/ [[]]T or (vk0 , v 0 ) 
/ [[]]T . In either case, apply
inductive hypothesis straightforwardly.

But this contradicts the inductive hypothesis 1.
Proposition 11. T , u ` T 0 , u0 implies T , u ` T 0 , u0 .
Proof. Fix u  T and u0  T 0 such that T , u ` T 0 , u0 . Define (Zi )i` by
xZi x0

iff T , x i T 0 , x0 .

We show that Z is an `-bisimulation between T , u and T 0 , u0 . By hypothesis, uZ` u0 . Fix
h  `. By construction, Zh satisfies Harmony. Let us see that Zh satisfies Zig (the case for
Zag is analogous). Suppose xZh x0 ,
x = v0 v1     vn = v

x = w0 w1     wm = w

in T ,
in T ,

and data(v) = data(w) (the case data(v) 6= data(w) is shown in a similar way), where
m, n  h. Let P  T 02 be defined by
n

m

P = {(v 0 , w0 ) | x0 v 0  x0 w0  data(v 0 ) = data(w0 )}.
Since T , x h T 0 , x0 , dd(hn =m i)  h and T , x |= hn =m i, we conclude that P 6= . We
next show that there exists (v 0 , w0 )  P such that
i. x0 = v00 v10     vn0 = v 0 in T 0 ,
0 = w 0 in T 0 ,
ii. x0 = w00 w10     wm

iii. (i  {0, . . . , n}) T , vi hi T 0 , vi0 , and
iv. (j  {0, . . . , m}) T , wj hj T 0 , wj0 ,
and hence Zig is satisfied by Zh . By way of contradiction, assume that for all (v 0 , w0 )  P
satisfying i and ii we have either
(a) (i  {0, . . . , n}) T , vi 6hi T 0 , vi0 , or
(b) (j  {0, . . . , m}) T , wj 6hj T 0 , wj0 .
283

fiFigueira, Figueira, & Areces

Fix > as any tautology such that dd(>) = 0. For each (v 0 , w0 )  P we define two families
of node expressions,
0v0 ,w0 , . . . , nv0 ,w0

and v00 ,w0 , . . . , vm0 ,w0 ,

satisfying that dd(iv0 ,w0 )  h  i for all i  {0, . . . , n} and dd(vj 0 ,w0 )  h  j for all
j  {0, . . . , m} as follows:
 Assume (a) and that i is the smallest number such that T , vi 6hi T 0 , vi0 . Let iv0 ,w0
be such that dd(iv0 ,w0 )  h  i and T , vi |= iv0 ,w0 but T 0 , vi0 6|= iv0 ,w0 . For k 
{0, . . . , n} \ {i}, let kv0 ,w0 = >, and for k  {0, . . . , m}, let vk0 ,w0 = >.
 Suppose (a) does not hold. Then (b) holds. Let j be the smallest number such that
T , wj 6hj T 0 , wj0 . Let vj 0 ,w0 be such that dd(vj 0 ,w0 )  h  j and T , wj |= vj 0 ,w0 but
T 0 , wj0 6|= vj 0 ,w0 . For k  {0, . . . , m} \ {j}, let vk0 ,w0 = >, and for k  {0, . . . , n}, let
kv0 ,w0 = >.

For each i  {0, . . . , n} and j  {0, . . . , m}, let
i =

^

iv0 ,w0

and

(v 0 ,w0 )P

j =

^

vj 0 ,w0 .

(1)

(v 0 ,w0 )P

However these conjunctions could be potentially infinite the if trees are infinitely branching. Since dd(iv0 ,w0 )  h  i, by Proposition 2 there are finitely many non-equivalent node

expressions iv0 ,w0 and the same applies to vj 0 ,w0 . Hence, both infinite conjunctions in (1)
are equivalent to finite ones, and we may assume that i and j are well-formed formulas. Finally, let  = [0 ][1 ]    [n ] and  = [0 ][1 ]    [m ]. By construction,
dd(), dd()  h and so dd(h = i)  h.
It is clear that by construction (x, v)  [[]]T and (x, w)  [[]]T , and therefore T , x |=
h = i. We see next that T 0 , x0 6|= h = i. This contradicts T , x h T 0 , x0 , and
hence we are done. Suppose that T 0 , x0 |= h = i. Then there is (v 0 , w0 )  P such that
0
0
(x0 , v 0 )  [[]]T and (x0 , w0 )  [[]]T . In particular, i and ii are true, and then either (a) or
0
(b) hold. In the first case, we have, by construction, that (x0 , v 0 ) 
/ [[]]T , and in the second
0
it is clear that (x0 , w0 ) 
/ [[]]T . In either case, we arrive to a contradiction.
Proof of Theorem 8. Item 2 is a direct consequence of Propositions 9 and 11.
The left-to-right argument for item 1 can be seen as a consequence of item 2. Indeed,
T , u  T 0 , u0 implies T , u ` T 0 , u0 for all `, which by item 2 implies T , u ` T 0 , u0 for all
`, which in turn entails T , u  T 0 , u0 .
The right-to-left argument for item 1 is similar to that of Proposition 11, but defining
Z by xZx0 iff T , x  T 0 , x0 . The conjunctions in (1) are then finite because T 0 is finitely
branching, and so P is finite (the fact that T is finitely branching is used to show that Zag
is satisfied).
284

fiModel Theory of XPath on Data Trees. Part I: Bisimulation and Characterization

3.2 Vertical XPath
We now study bisimulation for XPath= (). Interestingly, the notion we give is simpler
than the one for XPath= () due to a normal form enjoyed by the logic.
In the downward fragment of XPath= we used dd() to measure the maximum depth
from the current point of evaluation that the formula can access. For the vertical fragment
of XPath= , we need to define both the maximum distance r going downward and the
maximum distance s going upward that the formula can reach. We call the pair (r, s)
the vertical depth of a formula (notation: vd()). The nesting depth of a formula 
(notation: nd()) is the maximum number of nested [ ] appearing in .
vd(a)
vd(  )
vd()
vd(hi)
vd(h fi i)

=
=
=
=
=

(0, 0)
vd() = (0, 0)
max{vd(), vd()}
vd() = vd()
vd()
vd([]) = max{vd(), vd()}
vd()
vd() = max{(0, 0), vd() + (1, 1)}
max{vd(), vd()}
vd() = max{(0, 0), vd() + (1, 1)}

nd(a)
nd(  )
nd()
nd(hi)
nd(h fi i)

=
=
=
=
=

0
max{nd(), nd()}
nd()
nd()
max{nd(), nd()}

nd()
nd()
nd([])
nd()
nd()

=
=
=
=
=

max{nd(), nd()}
0
1 + nd()
0
0

where, a  A, fi  {=, 6=}, + and max are performed component-wise, and  is any path
expression or the empty string .
Let (r, s, k)-XPath= () be the set of formulas  in XPath= () with vd()  (r, s)
and nd()  k. Let T , u and T 0 , u0 be pointed data trees. We say that T , u and T 0 , u0 are
equivalent for XPath= () (notation: T , u  T 0 , u0 ) iff for all   XPath= (), we have
T , u |=  iff T 0 , u0 |= . T , x and T 0 , x0 are (r, s)-equivalent [resp. (r, s, k)-equivalent]

0 0
0 0
for XPath= () (notation: T , x 
r,s T , x [resp. T , x r,s,k T , x ]) if they satisfy the
same node expressions  of XPath= () so that vd()  (r, s) [resp. vd()  (r, s) and
nd()  k].
3.2.1 Normal Form
We define a normal form for XPath= () that will be implicitly used in the definition of
bisimulation in this section. For n  0, let n denote the concatenation of n symbols .
I.e., 0 is the empty string , 1 = , and n+1 = n (similarly for n ).
A path expression  of XPath= () is downward [resp. upward] if it is of the form
n
 [] [resp. []n ] for some n > 0 with   XPath= (). For example, [hi] is a downward
expression whereas [hi] is not. An up-down expression is any expression of the form ,
 ,  or   where  is upward and  is downward. Henceforth we will use  ,   ,   to
denote upward expressions and  ,   ,   to denote downward expressions and  ,   ,  
to denote up-down expressions. Note that in particular any downward or upward expression
is an up-down expression. An XPath= () formula is in up-down normal form if every
path expression contained in it is up-down and every data test is of the form h fi  i with
fi  {=, 6=}. Next, we show that for every XPath= () formula there is an equivalent
285

fiFigueira, Figueira, & Areces

one in up-down normal form. Of course, the idea of replacing child axes with parent axes
is by no means novel, and there have been a number of works on rewriting expressions
into equivalent ones to improve performance or streamability, such as the works of Olteanu
(2007) and Olteanu, Meuss, Furche, and Bry (2002). However, these rewrite systems aim
at removing backward axes (parent, ancestor, etc) maintaining equivalence at the root,
different to our end, which is to maintain equivalence of path expressions over any pair of
nodes. Further, we doubt our normal form can be useful in such scenarios since from a
computational or streamability perspective the resulting formula in up-down normal from
seems more complex. Although being more regular, it has more nested modalities and
introduces parent axes, as it will be shown later. Our motivation for this normal form is
to simplify the defintion of bisimulation by means of rendering the formula as simple as
possible in form.
Given a path expression , the navigation of  (notation: nav()) is the string of
{, } that results from removing all node expressions [] and  from . For example,
nav([hi][h = i][b]) = .
Proposition 12. Let   (r, s, k)-XPath= (), then there is   XPath= () in up-down
normal form such that the following hold
1.   ;
2. vd( ) = (r, s); and
3. nd( )  k  (r + s + 2).
Proof. The idea is that we can factorize any path in the tree going down and up as a node
test in the expression. Consider for instance the expression  = [a]. It is immediate
that  is equivalent to the up-down expression [h[h[a]i]i], which is in up-down normal
form.
We use the following directed equivalences to translate any path expression into an
equivalent up-down expression.
  

()

[1 ][2 ]  [1  2 ]


n     1 0 1     n  



n n1     0 



0 1     n  

(merge)

[hn n     1 1 0 i]
n

 [h0 1  . . . n i]
n

[h0 1     n i] 

(factor )
(shift-r )
(shift-l )

In the expressions above, each i is the empty string, or it is of the form  or [1 ][2 ] . . . [n ],
 and  can be any path expression, or the empty string, and  is any path expression. The
idea is that (factor ) converts an expression that goes down n times and then up n times
into a node expression, and when doing this, any test done in the i-th node when going
down is merged with the (n  i)-th test when going up. For example, [a][c][b] 
[h[a][b][c]i]. On the other hand, (shift-r ) and (shift-l ) group all the node tests in the
lowest node in the expression, making use of the fact that the parent relation is functional.
Thus, for example [a][b]  [h[b][a]i] and [a][b]  [h[a][b]i]. It is thus clear
that the left and right expressions above are semantically equivalent.
286

fiModel Theory of XPath on Data Trees. Part I: Bisimulation and Characterization

The following lemma treats the case of path expressions:
Lemma 13. Let  be an XPath= ()-path expression with vd() = (r, s) and nd() =
k, then there is an up-down path expression  such that:
1.   

2. vd( ) = (r, s), and
3. nd( )  k + r + s + 1.

Proof of Lemma. We first apply rule (factor ) as many times as possible. It is clear
that if nav() is of the form n m for some n, m  0 then rule (factor ) cannot be
applied and we are done. Hence, suppose nav() contains the pattern . Let
 =  1 
1
1 = 1 n
 . . . 01  . . . n1 1
{z
}
| 1
matches (factor )

2
2 n
 . . . 02  . . . n2 2
2

|

{z

matches (factor )

..
.

}

m
m1 n
 . . . 0m  . . . nmm m ,
| m
{z
}
matches (factor )

where nav( ), nav(m )   , nav( ), nav(1 )   , and ji are the empty string,  or
i,j
i,j
[i,j
1 ][2 ] . . . [hi,j ]. Furthermore, assume that m is maximal (i.e., it is impossible to
apply (factor ) in any of the i s) and that the length of each i is minimal (i.e., it
is not the case that nav(i ) ends with  and that nav(i+1 ) begins with ). Observe
that nav(i )    . We apply rule (factor ) in the m  1 marked places and obtain
1
1 1 1
2 = 1 [hn
 1     1
1 0 i]
1 n1
|
{z
}
(factor ) applied

2
2 2 2
2 [hn
 2     1
1 0 i]
2 n2

|

{z

(factor ) applied

..
.

}

m
m m m
m1 [hn
 m     1
1 0 i] m ,
m nm
{z
}
|
(factor ) applied

Let vd(nav(1 )) = (r1 , s1 ). Since nav() = nav( 1  ) contains the pattern ,
we have that r1 > 0. It can be shown that vd( 2  ) = (r, s), nd(2 )  nd(1 ) + 1,
and vd(nav(2 ))  (r1  1, s1 ). If we repeat this procedure with 2 and so on until
we can no longer apply rule (factor ), we end up with an up-down path expression f
so that
287

fiFigueira, Figueira, & Areces

1. f  1 ,
2. vd( f  ) = (r, s), and
3. nd(f )  nd(1 ) + r1 .
After applying () and (merge) to  f  as many times as possible, we obtain an
equivalent 0 , of the same vertical and nesting depth as  f  , of the form
0 = 1 2  . . . n n+1 n+2  . . . n+m ,
where i is a (possibly empty) string of the form [1i ] . . . [ni i ]. We apply (shift-l ) and
(shift-r ) to 0 to obtain an equivalent 00 of the same vertical depth (i.e. vd(00 ) =
vd(0 ) = (r, s)) and of nesting depth equal to nd(0 ) + 1 of the form
 = 0 n m 00 ,
where 0 and 00 are the empty string or of the form []. Observe that nd( ) =
nd(0 ) + 1  k + r1 + 1  r + s + 1, and so  satisfies all requirements of the lemma.
This concludes the proof of Lemma 13.


The following lemma treats the case of data tests:
Lemma 14. Let  ,   be up-down path expressions and let  = h fi   i (for
fi  {=, 6=}) with vd() = (r, s) and nd() = k. Then there is an up-down path
expression   such that:
1. h  i  ,
2. vd(  ) = (r, s), and
3. nd(  )  k + 1.
Proof of Lemma. Let us analyse the case where  = [ ]n m [ ] and   =
[ ]n m [ ], where n + m > 0, n + m > 0, and  ,  ,  ,  are in up-down
normal form (the remaining cases where  =  or   =  being simpler). Suppose,
without loss of generality, that n  n .
Hence, we have h fi   i  h  i, where
  = [   ]n m [  h fi m n n m [ ]i].
It is clear that the formulas are equivalent (cf. the picture below).
288

fiModel Theory of XPath on Data Trees. Part I: Bisimulation and Characterization

}

}

|
|
{z

|

}

|

|

{z

}
{z

{z

n

{z

{z



[ ]

|

m

l

|

|

z

"#

[ ]

m

n

}

{

n

n

n

{z

m

"#

m

}

[

]
[

[ ]

}

}

[ ]

[

]



^

]

Moreover, the right-hand formula has at most one more nesting than the lefthand formula, and its vertical depth is at most (r, s). This concludes the proof of
Lemma 14.

By induction on , and using Lemmas 13 and 14, one can show that there is  as
desired.
3.2.2 Finite Index
Contrary to the case of XPath= () (cf., Proposition 2), the logical equivalence relation
restricted to XPath= ()-formulas of bounded vertical depth has infinitely many equivalence
classes.
Proposition 15. If r + s  2 then 
r,s has infinite index.
i }
Proof. We show that for every r, s so that r + s = 2 there is an infinite set {r,s
i0 of
non-equivalent node expressions with vertical depth (r, s). It thus follows that for every r, s
so that r + s  2, 
r,s has infinite index.
Consider the following formulas.
i+1
i
1,1
= h = [1,1
]i

0
1,1
= h = i

i+1
i
0,2
= h = [1,1
]i

0
0,2
= h = i

i+1
i
2,0
= h = [1,1
]i

0
2,0
= h = i

n ) = (r, s) and nd( n ) = n for every n. The formula  n intuitively says
Note that vd(r,s
r,s
r,s
that there is a chain of length n as depicted in Figure 2.
n but T , x
n0
In the data tree Tn of the figure, we have that Tn , xr,s |= r,s
n r,s 6|= r,s for any
i }
n0 > n. Therefore, {r,s
i0 is an infinite set of non-equivalent formulas of vertical depth
(r, s).

In the proof of the above proposition we need to use formulas with unbounded nesting
depth. In fact, when restricted to bounded nesting depth there are only finitely many
formulas up to logical equivalence, as stated next.
289

fiFigueira, Figueira, & Areces

x2,0
Tn :

x1,1

...
x0,2





n times



Figure 2: Model verifying ij for all i  n and not verifying l for no l < n. Dotted lines
represent equal data values.

Proposition 16. 
r,s,k has finite index.
Proof. Same argument as in the proof of Proposition 2.
0 0
Corollary 17. {T 0 , u0 | T , u 
r,s,k T , u } is definable by a node expression of (r, s, k)XPath= ().

Proof. Similar to the proof of Corollary 3.
3.2.3 Bisimulation and (r, s, k)-bisimulation
The advantage of the normal form presented in Section 3.2.1 is that it makes it possible to use
a very simple notion of bisimulation. The disadvantage is that, since it does not preserve

 corresponds
nesting depth, 
r,s,k does not correspond precisely to r,s,k , although 
precisely to  . Nonetheless, we obtain, for all r, s, k,

r,s,k  
r,s,k  r,s,k(r+s+2) .

Let T and T 0 be two data-trees. We say that u  T and u0  T 0 are bisimilar for
XPath= () (notation: T , u  T 0 , u0 ) iff there is a relation Z  T  T 0 such that uZu0
and for all x  T and x0  T 0 we have
 Harmony: If xZx0 then label (x) = label (x0 ),
n

m

n

m

 Zig: If xZx0 , y x and y z then there are y 0 , z 0  T 0 such that y 0 x0 , y 0 z 0 , data(z)
= data(x)  data(z 0 ) = data(x0 ), and zZz 0 .
The following picture illustrates the intended requirements
290

fiModel Theory of XPath on Data Trees. Part I: Bisimulation and Characterization

T0

T

9y 0

8y

m
n

9z 0

n

=

)

x

(6=

(6=

)

=

8z
Z

x0

m

n

m

 Zag: If xZx0 , y 0 x0 and y 0 z 0 then there are y, z  T such that y x, y z, data(z)
= data(x)  data(z 0 ) = data(x0 ), and zZz 0 .
The above definitions heavily rely on the normal form of Proposition 12. In fact, the
normal form is not strictly necessary for giving a notion of bisimulation for the vertical
fragment. For the case of the downward fragment, no normal form was used, as every path
expression is essentially a repetition of node test and child relation. On the contrary, for
the case of the vertical fragment, the use of a normal form would be very beneficial. A
notion of bisimulation not taking into account the existence of normal form would have a
rule Zig of the form: If If xZx0 and n1 , . . . , nk , m1 , . . . , mk , n1 , . . . , nk and m1 , . . . , mk are
such that
i in T for i  {1, . . . , k},
 v1i  . . . vni i in T and w1i  . . . wm
i
i in T for i  {1, . . . , k},
 v1i  . . . vni i in T and w1i  . . . wm
i

 v1i = w1i and v1i = w1i
i = v i+1 and w i = v i+1
 wm
ni+1
mi
ni+1
i

 x = vn1 1 and x = vn1 1
0i for i  {1, . . . , k}, and v 0i , . . . , v 0i , w 0i , . . . , w 0i
then there are v10i , . . . , vn0ii in T 0 , w10i , . . . , wm
1
1
ni
mi
i
in T 0 for i  {1, . . . , k} such that
0i in T 0 ,
 v10i  . . . vn0ii in T 0 and w10i  . . . wm
i
0i in T 0 ,
 v10i  . . . vn0ii in T 0 and w10i  . . . wm
i

 v10i = w10i and v10i = w10i
0i = v 0i+1 and w 0i = v 0i+1
 wm
ni+1
mi
ni+1
i

 x = vn011 and x = vn011
 vji Zvj0i , wji Zwj0i , vji Z vj0i , wji Z wj0i and
 data(wnk k ) = data(wnk k ) iff data(wn0k ) = data(wn0k )
k

k

291

fiFigueira, Figueira, & Areces

Intuitively, this definition establishes that for every paths p and p going up and down, many
times in T , there has to be a similar paths p0 and p0 in T , going up and down many times,
and respecting the shape of p and p respectively such that the j-th node of p is connected
to the j-the node of p0 via Z, and the same for the nodes along p, and such that the data
values of the last node of p and p in T are equal if and only if the data values of the last
nodes of p0 and p0 are so.
This definition is the natural extension of the downward bisimulation, but instead
of considering downward paths, it considers more general ones. As it happens with the
downward bisimulation, every intermediate node of T in both paths have to be related with
the corresponding node in T 0 . As one can immediately see, this definition is quite long and
checks too many conditions. Working with normal forms allow us to get a much simpler
definition of bisimulation, which has two main advantages: a) there is only one path in
each data tree, and b) we do not require intermediate nodes to be related by Z. These two
features are direct consequences of the up-down normal form, as the normal form a) only
compares values against the root (so there is only one non-empty path expression in each
diamond node expression) and b) it only make tests at the beginning and at the end of
each path expressions (but not in intermediate nodes). However, notice that due to the
normal form both definitions denote the same bisimulation relation.
We say that u  T and u0  T 0 are (r, s, k)-bisimilar for XPath= () (notation:
0
0 0
k
T , u 
r,s,k T , u ) if there is a family of relations (Zr,s )r+sr+s,kk in T  T such that
k u0 and for all r + s  r + s, k  k, x  T and x0  T 0 we have that the following
uZr,s
conditions hold.
k x0 then label (x) = label (x0 ).
 Harmony: If xZr,s
n

m

k x0 , y x and y z with n  s and m  r + n then there are y 0 , z 0  T 0
 Zig: If xZr,s
n

m

such that y 0 x0 , y 0 z 0 , and the following hold
1. data(z) = data(x)  data(z 0 ) = data(x0 ),

0
0
0
2. if k > 0, zZrk1
0 ,s0 z for r = r + n  m, s = s  n + m.
n

m

k x0 , y 0 x0 and y 0 z 0 with n  s and m  r + n then there are y, z  T
 Zag: If xZr,s
n

m

such that y x, y z, and items (1) and (2) above are verified.
n

n

k x0 , y x and y 0 x0 then it follows that yZ k1 y 0 , for r 0 = r + n,
Observation 18. If xZr,s
r0 ,s0
k for the case of bisimilarity.
s0 = s  n. The same occurs with Z instead of Zr,s

For a data tree T and u  T , let T |sr u denote the subtree of T induced by
m

n

{v  T | (m  s) (n  r + m) (w  T ) wu  wv}.
s
Observation 19. T , u 
r,s,k (T |r u), u.

292

fiModel Theory of XPath on Data Trees. Part I: Bisimulation and Characterization

3.2.4 Equivalence and Bisimulation
The next result says that coincides with  on finitely branching data trees, and states

precisely in what way 
r,s,k is related to r,s,k .
Theorem 20.
1. T , u  T 0 , u0 implies T , u  T 0 , u0 . The converse also holds when T and T 0 are
finitely branching.

0 0
0 0
2. T , u 
r,s,k(r+s+2) T , u implies T , u r,s,k T , u .

0 0
0 0
3. T , u 
r,s,k T , u implies T , u r,s,k T , u .

The above theorem will be shown as a consequence of the following Propositions 21 and
22.

0 0
0 0
Proposition 21. T , u 
r,s,k(r+s+2) T , u implies T , u r,s,k T , u .
0 0
Proof. We show that if T , u
r,s,k T , u via
k
(Zr,s
)r+sr+s,kk

then for all n  s and m  r + n, for all  in up-down normal form with vd()  (r, s),
nd()  k, for all upward expression  in up-down normal form, and for all downward
expression  in up-down normal form with vd( ), vd( )  (r, s), nd( ), nd( )  k:
k x0 then T , x |=  iff T 0 , x0 |= .
1. If xZr,s
n

n

m

m

0

k1 0
2. If y x, y 0 x0 , x Zr,s
x , then (x, y)  [[ ]]T iff (x0 , y 0 )  [[ ]]T .
0
0
0
 T iff
3. If y z, y 0 z 0 , z Zrk1
0 ,s0 z for r = r + n  m, s = s  n + m, then (y, z)  [[ ]]
0
(y 0 , z 0 )  [[ ]]T .

Hence, by Proposition 12, the main statement follows. We simultaneously show 1, 2
and 3 by induction on || + | | + | |.
Let us see item 1. The base case is  = a for some a  A. By Harmony, label (x) =
label (x0 ) and then T , x |=  iff T 0 , x0 |= . The Boolean cases for  are straightforward.
Suppose  = h =   i. We show T , x |=   T 0 , x0 |= , so assume T , x |= .
n
m
Suppose there are y, z  T and n  s, m  r + n such that y x, y z, (x, y)  [[ ]]T ,
0
(y, z)  [[ ]]T and data(x) = data(z). By Zig, there are y 0 , z 0  T 0 such that zZrk1
0 ,s0 z
for r0 = r + n  m, s0 = s  n + m, and data(x0 ) = data(z 0 ). By inductive hypothesis 2
0
0
and 3, (x0 , y 0 )  [[ ]]T and (y 0 , z 0 )  [[ ]]T . Hence T 0 , x0 |= . The implication T 0 , x0 |=
  T , x |=  is analogous. The cases  = h 6=  i, and  = h fi  i,  = h fi  i
(fi  {=, 6=}) and  = hi (for  in up-down normal form) are shown in a similar way. The
cases  = h fi i (fi  {=, 6=}) are trivial.
293

fiFigueira, Figueira, & Areces

Let us now analyze item 2. Let  = []n (n  0), and let x0 , . . . , xn  T and
 T 0 be such that

x00 , . . . , x0n

y = x0 x1     xn = x
0

y =

x00 x01     x0n

=x

0

in T ,

in T 0 ,

k1 0
0
0
0
x . By Observation 18, we have x0 Zrk1
and xZr,s
0 ,s0 x0 , for r = r + n, s = s  n. Assume by
0
contradiction that (x0 , y 0 ) 
/ [[ ]]T . This necessarily means that T , x0 |=  but T 0 , x00 6|= .
But  is a subformula of  , nd()  k  1 and nd()  (r0 , s0 ) and this contradicts
inductive hypothesis 1.
Item 3 is shown in a similar way. Let  = m [] (m  0), and let z0 , . . . , zm  T and
0
0  T 0 be such that
z0 , . . . , z m

y = z0 z1     zm = z
0

y =

0
z00 z10     zm

=z

0

in T ,

in T 0 ,
0

0
0 0 / [[ ]]T . This necessarily means that
and zZrk1
0 ,s0 z . Assume by contradiction that (y , z ) 

T , xm |=  but T 0 , x0m 6|= . But  is a subformula of  , nd()  k  1 and nd()  (r0 , s0 )
and this contradicts inductive hypothesis 1.

0 0
0 0
Proposition 22. T , u 
r,s,k T , u implies T , u r,s,k T , u .
0 0
k
Proof. Fix u  T and u0  T 0 such that T , u 
r,s,k T , u . Define (Zr,s )r+sr+s,kk by
k 0
x
xZr,s

iff

T , x 

r,s,k

T 0 , x0 .

k is a (r, s, k)-bisimulation between T , u and T 0 , u0 . By hypothesis, uZ k u0 .
We show that Zr,s
r,s

k satisfies Harmony. Let us see that Z k
Now fix r + s  r + s, k  k. By construction, Zr,s
r,s
k x0 ,
satisfies Zig (the case for Zag is analogous). Suppose xZr,s

y = x0 x1     vn = x
y = z0 z1     zm = z

in T ,
in T ,

and data(x) = data(z) (the case data(x) 6= data(z) is shown in a similar way), where
m  r + n. Let P  T 02 be defined by
n

m

P = {(y 0 , z 0 ) | y 0 x0  y 0 z 0  data(x0 ) = data(z 0 )}.
n m
n m
0 0
Since T , x 
r,s,k T , x , vd(h =   i)  (r, s), nd(h =   i) = 0, and T , x |= h =
n m
  i, we conclude that P 6= . We next show that there exists (y 0 , z 0 )  P such that

i. y 0 = x00 x01     x0n = x0 in T 0
0 = z 0 in T 0 ,
ii. y 0 = z00 z10     zm

294

fiModel Theory of XPath on Data Trees. Part I: Bisimulation and Characterization

iii. T , x 

r,s,k1

T 0 , x0 , and

iv. T , z 0

r ,s0 ,k1

T 0 , z 0 , where r0 = r + n  m, s0 = s  n + m,

k . By way of contradiction, assume
and hence, by inductive hypothesis, Zig is satisfied by Zr,s
0
0
that for all (y , z )  P satisfying i and ii we have either

(a) T , x 6

r,s,k1

(b) T , z 60

T 0 , x0 ; or

r ,s0 ,k1

T 0 , z 0 for r0 = r + n  m, s0 = s  n + m.

Fix > as any tautology such that vd(>) = (0, 0), nd(>) = 0. For each (y 0 , z 0 )  P we
define node expressions, y0 ,z 0 and y0 ,z 0 , satisfying that vd(y0 ,z 0 )  (r, s), nd(y0 ,z 0 ) < k
and vd(y0 ,z 0 )  (r0 , s0 ), nd(y0 ,z 0 ) < k as follows:
 Suppose (a) holds. Let y0 ,z 0 be such that vd(v0 ,w0 )  (r, s), nd(v0 ,w0 ) < k, and such
that T , x |= y0 ,z 0 but T 0 , x0 6|= y0 ,z 0 ; and let v0 ,w0 = >.
 Suppose (a) does not hold. Then (b) holds. Let y0 ,z 0 be such that vd(y0 ,z 0 )  (r0 , s0 ),
nd(y0 ,z 0 ) < k and such that T , z |= y0 ,z 0 but T 0 , z 0 6|= y0 ,z 0 ; and let y0 ,z 0 = >.
Let
=

^

y0 ,z 0

and

(y 0 ,z 0 )P

=

^

y0 ,z 0 .

(2)

(y 0 ,z 0 )P

Since vd(y0 ,z 0 )  (r, s), nd(y0 ,z 0 ) < k, by Proposition 16, there are finitely many nonequivalent formulas y0 ,z 0 . The same applies to formulas y0 ,z 0 . Hence both infinite conjunctions in (2) are equivalent to finite ones, and therefore without loss of generality we
may assume that  and  are well-formed formulas.
Finally, let
 = []n and  = m [].
By construction, vd(  )  (r, s), nd(  )  k. Furthermore, T , x |= h =   i and
T 0 , x0 6|= h =   i, but this contradicts the fact that T , x  T 0 , x0 .
r,s,k

Proof of Theorem 20. Items 2 and 3 are shown in Propositions 21 and 22.
The left-to-right argument for item 1 can be seen as a consequence of item 2. Indeed,

0 0
0 0
T , u T 0 , u0 implies T , u
(r,s) T , u for all r, s, which by item 2 implies T , u (r,s) T , u

for all r, s, which in turn entalis T , u  T 0 , u0 .
The right-to-left argument for item 1 is similar to that of Proposition 22, but working
k )
0
with a single Z instead of (Zr,s
r,s,k . For the converse implication, define Z by xZx iff
T , x  T 0 , x0 . The conjunctions in (2) are then finite because T 0 is finitely branching,
and so P is finite (the fact that T is finitely branching is used for showing that Zag is
satisfied).

Corollary 23. 
r,s,k has finite index.
Proof. Immediate from Theorem 20 and Proposition 16.
295

fiFigueira, Figueira, & Areces

3.3 Simulation
In this section we define notions of directed (non-symmetric) simulations for XPath= ()
and XPath= (), as it is done, e.g., in the works of Kurtonina and de Rijke (1997) or Lutz,
Piro, and Wolter (2011) for some modal logics. We obtain results similar to Theorems 8
and 20 but relating each simulation notion with the corresponding logical implication.
We say that an XPath= formula is positive if it contains no negation  and no inequality data tests h 6= i. For L one of XPath= (), XPath= (), XPath= ( ), or
XPath= (  ), we write L+ for the positive fragment of L.
A simulation for XPath= () [resp. for XPath= ()] is simply a bisimulation from
which the Zag clause and half of the first condition in the Zig clause have been omitted.
Observe that simulations need not be symmetric.
Formally, we say that u  T is similar to u0  T 0 for XPath= () (notation: T , u 
0
T , u0 ) iff there is a relation Z  T  T 0 such that uZu0 and for all x  T and x0  T 0 we
have
 Harmony: If xZx0 then label (x) = label (x0 ).
n

m

n

m

 Zig: If xZx0 , xv and xw then there are v 0 , w0  T 0 such that x0 v 0 , x0 w0 and
1. data(v) = data(w)  data(v 0 ) = data(w0 ),
i

i

2. (v) Z (v 0 ) for all 0  i < n, and
i

i

3. (w) Z (w0 ) for all 0  i < m.

u  T is similar to u0  T 0 for XPath= () (notation: T , u  T 0 , u0 ) iff there is a
relation Z  T  T 0 such that uZu0 and for all x  T and x0  T 0 we have
 Harmony: If xZx0 then label (x) = label (x0 ).
n

m

n

m

 Zig: If xZx0 , y x and y z then there are y 0 , z 0  T 0 such that y 0 x0 , y 0 z 0 , zZz 0 ,
and if data(z) = data(x) then data(z 0 ) = data(x0 ).
Relations ` and 
r,s,k are defined accordingly. We define one-way (non-symmetric)
logical implication between models as follows. We write T , u V T 0 , u0 for
(  XPath= ()+ ) [T , u |=   T 0 , u0 |= ].
+
+
Define V` , V , and V
r,s,k in an analogous way for `-XPath= () , XPath= () , (r, s, k)XPath= ()+ , respectively. As for bisimulation, we have that  coincides with V.

Theorem 24.
1. Let   {, }. T , u  T 0 , u0 implies T , u V T 0 , u0 . The converse holds when T 0 is
finitely branching.
2. T , u ` T 0 , u0 iff T , u V` T 0 , u0 .

0 0
0 0
3. T , u 
r,s,k(r+s+2) T , u implies T , u Vr,s,k T , u .

296

fiModel Theory of XPath on Data Trees. Part I: Bisimulation and Characterization


0 0
0 0
4. T , u V
r,s,k T , u implies T , u r,s,k T , u .

Proof. The proofs are straightforward adaptations of the proofs of Propositions 9 and 11
and Propositions 21 and 22 respectively, and are ommitted here. In particular, for the if
part, in the adaptation of the proofs of Propositions 11 and 22, the simulations are defined
by
xZi x0 iff T , x Vi T 0 , x

k 0
xZr,s
x iff T , x V

r,s,k

T 0, x

respectively, and the conditions (a) and (b) on page 283 become now
(a) [i  {0, . . . , n}   XPath= ()+ ] dd()  h  i  T , vi |=   T 0 , vi0 6|= ; or
(b) [j  {0, . . . , m}   XPath= ()+ ] dd()  h  j  T , wj |=   T 0 , wj0 6|= ,
and
(a) [i  {0, . . . n}   XPath= ()+ ] vd()  (r + i, s  i)  nd()  k  1  T , vi |=
  T 0 , vi0 6|= ; or
(b) [j  {0, . . . m}   XPath= ()+ ] vd()  (r + j 0 , s  j 0 ) for j 0 = n  m + j
nd()  k  1  T , wj |=   T 0 , wj0 6|= 
respectively.
We say that T 0 is a substructure of T if T 0 is a data tree which results from removing
some nodes of T , i.e., T 0  T and for all u, v  T 0 we have: 1) uv on T iff uv on T 0 ; 2)
label (u) on T 0 equals label (u) on T ; and 3) data(u) on T 0 equals data(u) on T . Equivalently,
seen as -structures, T 0 is the -substructure of T induced by T 0  T . One can verify that
the identity on T 0 is a simulation for XPath= () from T 0 to T .
Lemma 25. If T 0 is a substructure of T and u0  T 0 then T 0 , u0  T , u0 .
Lemma 26.
+
(1) {T 0 , u0 | T , u ` T 0 , u0 } is definable by a node expression +
`,u,T of XPath= () with
downward depth  `.
+
0 0
+
(2) {T 0 , u0 | T , u 
r,s,k T , u } is definable by a node expression r,s,k,u,T of XPath= ()
with vertical depth  (r, s) and nesting depth  k.

0 0
0 0
Proof. For item (2), let sim
r,s,k (T , u) = {T , u | T , u r,s,k T , u }. Let T 0 ,u0 be the set of
all positive node expressions   XPath= ()+ of vertical depth at most (r, s) and nesting
depth at most k so that T 0 , u0 |= . Let  be
_
^
=
T 0 ,u0 .
T 0 ,u0 sim
r,s,k (T ,u)

297

fiFigueira, Figueira, & Areces

Since every T 0 ,u0 is finite up to logical equivalence by Proposition 16, it follows that 
is a valid node expression. We show that it defines sim
r,s,k (T , u).
V

0
0
0
0
Let T , u  simr,s,k (T , u). Then, T , u |= T 0 ,u0 and thus T 0 , u0 |= . If on the
V
other hand T 0 , u0 |=  we have that T 0 , u0 |= T 00 ,u00 for some T 00 , u00  sim
r,s,k (T , u)


00 00
0 0
00 00
and then T 0 , u0 
r,s,k T , u . By Theorem 20-3 we then have that T , u r,s,k T , u , and



0 0
00 00
00 00
0 0
in particular T 00 , u00 
r,s,k T , u . Since T , u r,s,k T , u and T , u r,s,k T , u , then


0 0
0 0
T , u 
r,s,k T , u (by transitivity of r,s,k ) and thus T , u  simr,s,k (T , u).
Item (1) is shown in a similar way, using Proposition 2 and Theorem 8-2.

We obtain that the node expressions of XPath= invariant under simulations are, precisely, the positive ones.
Theorem 27.
1.   XPath= () is  -invariant [resp. ` ] iff it is equivalent to a node expression of
XPath= ()+ [resp. `-XPath= ()+ ].
2.   XPath= () is  -invariant iff it is equivalent to a node expression of XPath= ()+ .
3. If   XPath= () is 
r,s,k -invariant then it is equivalent to a node expression of
(r, s, k)-XPath= ()+ .
4. If   XPath= () is equivalent to a node expression of (r, s, k)-XPath= ()+ then  is
0

r,s,k0 -invariant, for k = k  (r + s + 2).
Proof. We start with item (1), for the case of ` . The if part is straightforward from
Theorem 24-2, and here we focus on the only if part. Let  be preserved under ` .
Let {(Ti , ui )}in be the set of all pointed models of  modulo ` (which is finite due to
Theorem 8-2 together with Proposition 2). We claim that
T , u |=  iff Ti , ui ` T , u for some i  n.

(3)

On the one hand, if T , u |=  then there is i  n such that Ti , ui ` T , u, and so
Ti , ui ` T , u. On the other hand, suppose Ti , ui ` T , u for some i. Since  is preserved
under ` and Ti , ui |= , we conclude T , u |= .
+
Let
W `,ui ,Ti  XPath= () , dd(i )  `, be as in Lemma 26-(1). Using (3) one shows
that in `,ui ,Ti  .
For the case of  of item (1), the if direction follows from Theorem 24-1. For the
only if direction, let  be preserved under  . It is easy to see that  is preserved under
 iff it is preserved under dd() . We can then apply the same reasoning as before and
the statement follows.
Item (3) follows the same argument as item (1) but this time using Corollary 23 and
Lemma 26-(2).
Item (4) is straightforward from Theorem 24-3.
Item (2) follows from items (3) and (4) and the observation that  is preserved under

 iff it is preserved under 
r,s,k(r+s+2) for vd() = (r, s) and nd() = k.
298

fiModel Theory of XPath on Data Trees. Part I: Bisimulation and Characterization

3.4 Transitive Axes
As it happens, for example, with the basic modal logic and propositional dynamic logic,
the same notion of bisimulation [resp. simulation] of each logic captures the logical equivalence [resp. logical implication] for the corresponding fragments including also the reflexivetransitive closure of the axes which are present. Intuitively, this occurs because  is an
infinite union of compositions of , and similarly for . Hence the notions of bisimulations

for XPath= ( ) and XPath= (  ) (denoted  and   respectively) coincide
with  and  , respectively.

Let  and   be the logical equivalence relation for the fragments XPath= ( )

and XPath= (  ) respectively, and let V and V  be the logical implication for
XPath= ( )+ and XPath= (  )+ respectively.
Theorem 28. Let   { ,   }.

1. T , u  T 0 , u0 implies T , u  T 0 , u0 . The converse also holds when T 0 is finitely
branching.

2. T , u  T 0 , u0 implies T , u V T 0 , u0 . The converse also holds when T 0 is finitely
branching.
Proof. The proof that T , u T 0 , u0  T , u  T 0 , u0 follows from a simple adaptation of
Proposition 9 to the logic XPath= ( ) and Lemma 10. The fact that for finitely branching,
T , u  T 0 , u0  T , u  T 0 , u0 is straightforward from Theorem 8-1 since    .
The cases for XPath= (  ), XPath= ( ) and XPath= (  )+ are analogous.
It is not hard to see that the adequate notion of (bi)simulation of any other intermediate fragment between XPath= () and XPath= (  ) such as XPath= ( ) and
XPath= ( ) also corresponds to that of XPath= () in the sense of the statement
above.
On the other hand, if we restrict formulas to have only transitive axes, we obtain that
the notion of bisimulation for XPath= ( ) has a coarser bisimulation notion, as we define
next.
Let T and T 0 be two data-trees. We say that u  T and u0  T 0 are bisimilar for
XPath= ( ) (notation: T , u  T 0 , u0 ) iff there is a relation Z  T  T 0 such that uZu0
and for all x  T and x0  T 0 we have
 Harmony: If xZx0 then label (x) = label (x0 ).


















 Zig: If xZx0 and there are xv1     vn , and xw1     wm in T then there





 0
are x0 v10     vn0 , and x0 w10     wm
in T 0 such that
0 ),
1. data(vn ) = data(wm )  data(vn0 ) = data(wm

2. vi Z vi0 for all 1  i  n, and
3. wi Z wi0 for all 1  i  m.







0 in T 0 then there
 Zag: If xZx0 and there are x0 v10     vn0 , and x0 w10     wm






are xv1     vn , and xw1     wm in T such that items 1, 2 and 3 above are
verified.

299

fiFigueira, Figueira, & Areces

As before, one can define `-bisimilarity for XPath= ( ), notated T , u `  T 0 , u0 , and
also the notions of equivalence  and `  as expected.
These notions of bisimulation coincide with the corresponding logical equivalences:
Theorem 29.
1. T , u  T 0 , u0 implies T , u  T 0 , u0 . The converse also holds when T and T 0 are
finite.
2. T , u `  T 0 , u0 iff T , u `  T 0 , u0 .
For the case of XPath= (  ) it is not obvious how to adapt the bisimulation of vertical
XPath, since the normal form results does not hold for XPath= (  ).

4. Characterization
In Section 4.1 we characterize XPath= () as the fragment of first-order logic  -invariant
over data trees. In Section 4.2 we show that this result fails for XPath= () in general, but
a weaker result holds: Any first-order formula 
r,s,k -invariant for some r, s, k is equivalent
to a XPath= () formula.
4.1 Downward XPath
Recall that a data tree T and u  T , let T |` u denote the subtree of T induced by {v 
n
T | (n  `) uv}. Any data tree can be regarded as a -structure, as explained in 2.1.
An FO()-formula (x) is `-local if for all data trees T and u  T , we have T |= (u) 
T |` u |= (u). Recall that   FO(), qr() is the quantifier rank of .
Observe that the following result has two readings: one classical, and one restricted to
finite models.
Theorem 30 (Characterization). Let (x)  FO(). The following are equivalent:
1.  is  -invariant over [finite] data-trees;
2.  is logically equivalent over [finite] data-trees to a node expression of `-XPath= (),
where ` = 2qr()  1.
The proof of this theorem, whose proof is afterwards, will be a consequence of the
following three propositions:
Proposition 31. Any  -invariant (x)  FO() over [finite] data-trees is `-local for
` = 2qr()  1.
Proof. We follow the proof by Otto (2004a). Assume that (x)  FO() is  -invariant,
let q = qr(), and put ` = 2q  1. Given a data tree T and u  T it suffices to show the
existence of data trees T 0 and T 00 , with corresponding elements u0  T 0 and u00  T 00 such
that
(a) T 0 , u0  T , u,
300

fiModel Theory of XPath on Data Trees. Part I: Bisimulation and Characterization

(b) T 00 , u00  (T |` u), u, and
(c) T 0 , u0 q T 00 , u00 .
Indeed, from the above conditions it follows that
T |= (u) iff T 0 |= (u0 )

((a) and  -invariance of )

iff T 00 |= (u00 )

(c)

iff (T |` u) |= (u),

((b)

and  -invariance

of )

and hence  is `-local. By Observation 4 one may assume that u  T is the root of T .
We define T 0 and T 00 , as structures that are disjoint copies of sufficiently many isomorphic
copies of T and T |` u, respectively, all tied together by some common root. Both structures
have q isomorphic copies of both T and T |` u, and only distinguish themselves by the nature
of the one extra subtree, in which u0 and u00 live, respectively: u0 is the root of one of the
copies of T and u00 is the root of one of the copies of T |` u. Consider the structures T 0 and
T 00 in the diagram below,

u00

u0

q
| {z }
q copies

| {z }

| {z }

q copies

q copies

| {z }
q copies

with distinguished elements u0 and u00 marked by ; the open cones stand for copies of
T , the closed cones for copies of T |` u. The new isomorphic copies have the same data
values as the original one. The new root has an arbitrary, fixed, data value and label. By
Observation 5, it is straightforward that conditions (a) and (b) are satisfied. Condition (c)
is true because one can exhibit a strategy for player II in the q-round Ehrenfeucht-Frasse
game on structures T 0 and T 00 . The strategy is exactly the same as the one used in the
paper by Otto (2004a).
Proposition 32. Any  -invariant (x)  FO() over [finite] data-trees that is `-local, is
` -invariant.
Proof. Let (x) be `-local and  -invariant. Suppose T , u ` T 0 , u0 and T |= (u). By
`-locality, T |` u |= (u). Now
T , u` T 0 , u0 iff (T |` u), u` (T 0 |` u0 ), u0

iff (T |` u), u (T 0 |` u0 ), u0 .

(Obs. 7)
(Obs. 6)

By  -invariance, T 0 |` u0 |= (u0 ) and by `-locality again, T 0 |= (u0 ).
Proposition 33. If (x)  FO() is ` -invariant over [finite] data-trees, then there is
  `-XPath= () such that Trx () is logically equivalent to  over [finite] data-trees.
301

fiFigueira, Figueira, & Areces

Proof. By Corollary 3, for every data tree T and u  T there is a node expression `,T ,u of
`-XPath= () such that T , u ` T 0 , u0 iff T 0 , u0 |= `,T ,u . Let
_
`,T ,u .
=
T |=(u)

Since `,T ,u  `-XPath= () and, by Proposition 2, ` has finite index, it follows that  is
equivalent to a finite disjunction.
We now show that   Trx (). Let us see that  |= Trx (). Suppose T |= (u). Since
T , u |= `,T ,u , we have T , u |=  and so T |= Trx ()(u). Let us now see that Trx () |= .
Assume T |= Trx ()(u), and so T , u |= . Then there exists T 0 , u0 such that T 0 |= (u0 )
and T , u |= `,T 0 ,u0 . By the property of `,T 0 ,u0 , we have T , u ` T 0 , u0 and since  is
` -invariant (and hence ` -invariant by Theorem 8-2) we conclude T |= (u).
Proof of theorem 30. The implication 2  1 follows straightforwardly from Theorem 8. The
proof of 1  2 is as follows: First, we show that any  -invariant (x)  FO() is `-local
for ` = 2qr()  1 (Proposition 31). Then, we prove that any  -invariant (x)  FO()
that is `-local is ` -invariant (see Proposition 32 below). Finally, we show that any FO()definable property which is ` -invariant is definable in `-XPath= () (see Proposition 33
below).
4.2 Vertical XPath
The analog of Theorem 30 fails for XPath= ().
This basically because the property
p(x) =the tree where x belongs contains a label a is  -invariant while it is not expressible in XPath= (). Notice that, however, p is not  -invariant. This is as expected, since
otherwise p would be expressed by a XPath= () formula. Indeed, consider two data trees
with only two nodes, the root and a leaf, labeled respectively a, b in one tree, and b, b in the
other tree. Note that the leafs of these trees are  -bisimilar although they dont coincide
in the property p.
Lemma 34. The FO()-formula (x) Pa (x) is  -invariant though not logically equivalent
over [finite] data-trees to any node expression of XPath= ().
Proof. Let (x) be the FO()-formula for there is a node labeled a in the tree, i.e., (x) =
(y) Pa (y). We prove that  is -invariant over [finite] data-trees, though it is not logically
equivalent over [finite] data-trees to any node expression of XPath= ().
To see that  is  -invariant over [finite] data-trees, take T , u and T 0 , u0 such that
T , u  T 0 , u0 and T |= (u). Furthermore, suppose that T , u |= m n a for adequate n
and m. By Theorem 20, T 0 , u0 |= n m a and so T 0 |= (u0 ).
Assume by contradiction that there is   XPath= () such that T , u |=  iff T |= (u)
for all data-tree T and u  T . Suppose vd() = (r, s) and nd() = k. Let T be a data tree
formed by a chain of length r+1 starting from the root u with all its nodes containing a label
b except the leave, which has label a (the data values are irrelevant). By Observation 19
s
s
we have T , u 
r,s,k (T |r u), u. Since T , u |= , by Theorem 20, we have (T |r u), u |= ,
and so T |sr u |= (u). This last fact is a contradiction because no node of T |sr u is labeled
with a.
302

fiModel Theory of XPath on Data Trees. Part I: Bisimulation and Characterization

Hence XPath= () is not the fragment of FO() which is -invariant over [finite] datatrees. However, the following analog of Proposition 33 (needed for the proof of Theorem 30)
still holds for the case of XPath= (): For any r, s, k, every first-order formula 
r,s,k invariant is equivalent to a XPath= () formula.
Proposition 35. Let k 0 = k  (r + s + 2). If (x)  FO() is 
r,s,k0 -invariant over [finite]
data-trees, then there is   (r, s, k)-XPath= () such that Trx () is logically equivalent to
 over [finite] data-trees.
Proof. By Corollary 17, for every data tree T and u  T there is a node expression r,s,k,T ,u
0 0
0 0
of (r, s, k)-XPath= () such that T , u 
r,s,k T , u iff T , u |= r,s,k,T ,u . Let
=

_

r,s,k,T ,u .

T |=(u)

As r,s,k,T ,u  (r, s, k)-XPath= () and, by Proposition 16, 
r,s,k has finite index, it
follows that  is equivalent to a finite disjunction. The proof that (x)  Trx () is similar
to Proposition 33, as we show next. Let us see that  |= Trx (). Suppose T |= (u). Since
T , u |= r,s,k,T ,a , we have T , u |=  and so T |= Trx ()(u). Let us see that Trx () |= .
Assume T |= Trx ()(u), and so T , u |= . Then there exists T 0 , u0 such that T 0 |= (u0 )
0 0
and T , u |= r,s,k,T 0 ,u0 . By the property of r,s,k,T 0 ,u0 , we have T , u 
r,s,k T , u and since
 is r,s,k(r+s+2) -invariant (and hence r,s,k -invariant by Theorem 20-2) we conclude T |=
(u).

Notice that the counterexample in Lemma 34 is an unrestricted, existential formula.
One may wonder if it might be possible to extend the expressive power of XPath= () to
accout for unrestricted quantification. The natural candidate would be the modal operator
E (usually known as the existential modality) which, intuitively, let us express that there
is some node in the model where a formula holds. But even with the additional expressive power provided by E the analog of Theorem 30 fails. Formally, consider the logic
XPath= (l E), which results from adding the operator E to XPath= () with the following
semantics: [[E]]T = T if [[]]T 6= , and [[E]]T =  otherwise.
The following lemma shows a counterexample to the analog of Theorem 30, showing
that XPath= (l E) is not the fragment of FO()  -invariant over [finite] data-trees.
Lemma 36. The FO()-formula (y, z) [y  z  Pa (y)  Pb (z)] is  -invariant though not
logically equivalent over [finite] data-trees to any node expression of XPath= (l E).
Proof. Let (x) be the FO()-formula for there are two nodes with same data value and
labels a and b respectively, i.e., (x) = (y, z) [y  z Pa (y)Pb (z)]. We show that  cannot
be expressed in XPath= (, , E). Suppose, by means of contradiction, that there is a node
expression   XPath= (, , E) expressing , with vd() = (r, s) (vd() for XPath= (, , E)
is defined as before together with the clause vd(E) = vd()). Let n = r + s, and let T
be the chain-like data-tree u0  u1      un such that label (u0 ) = a, label (un ) = b,
label (ui ) = c for i  {1, . . . n  1} and data(ui ) = i for i  {0, . . . , n}.
Let T 0 be the chain-like data-tree u00  u01      u0n such that label (u0i ) = label (ui )
for i  {0, . . . n}, data(u0i ) = data(ui ) for i  {0, . . . , n  1} and data(u0n ) = 0. Note that
303

fiFigueira, Figueira, & Areces

T 6|= (u0 ) and T 0 |= (u00 ). However, one can show that for all i  {0, . . . , n} we have
T , ui |=  iff T 0 , u0i |= . Hence,  does not express  and thus  is not expressible in
XPath= (, , E).

5. Applications
We devote this section to exemplify how the model theoretic tools we developed can be
used to show expressiveness results for XPath= . We do not intend to be comprehensive;
rather we will exhibit a number of different results that show possible uses of the notions
of bisimulation we introduced.
5.1 Safe Operations on Models
Bisimulations can also be used to show that certain operations on models preserve truth.
Such operations are usually called safe for a given logic, as they can be applied to a model
without changing the truth values of any formula in the language. Observation 4, for
example, is already an example of this kind of results showing that the class of models of a
formula is closed under sub-model generation. We will now show a more elaborate example.
We say that T 0 is a subtree replication of T , if T 0 is the result of inserting T |x into
T as a sibling of x, where x is any node of T different from the root. Figure 3 gives a
schematic representation of this operation.


Figure 3: Closure under subtree replication.
Proposition 37. XPath= (  ) is closed under subtree replication, i.e. if T 0 is a subtree

replication of T , and u  T then T 0 , u   T , u.
Proof. Suppose that x  T is not the root of T , and that T 0 is the result of inserting T |x
into T as a sibling of x. Let us call Tx to the new copy of T |x inserted into T 0 , and let X
be the set of nodes of T |x. Furthermore, if v  X then vx is the corresponding node of Tx .
Nodes v and vx have the same label and data value, and the position of v in T |x coincides
with the position of vx in Tx .
By Theorem 28, it suffices to verify that T , u T 0 , u via Z  T  T 0 defined by:
Z = {(y, y) | y  T }  {(v, vx ) | v  X}.
Z is depicted as dotted lines in Figure 3.
304

fiModel Theory of XPath on Data Trees. Part I: Bisimulation and Characterization

5.2 Non-expressivity Results
Finally, we will use bisimulation to show the expressivity limits of different fragments of
XPath. Let key(a) be the property stating that every node with label a has a different data
value. Let fk(a, b) (for foreign key) be the property (x)[Pa (x)  (y)[Pb (y)  x  y]].
Proposition 38.
1. key(a) is not expressible in XPath= (  ).
2. fk(a, b) is expressible in XPath= (  ) but it is not expressible in XPath= ( ) or
XPath= (  )+ .
Proof. The first item follows from Proposition 37. Since the logic is closed under subtree
replication, the trees below are equivalent.
x

a, 1

a, 2

$l

a, 1

x0

a, 2 a, 2

As key(a) holds in one and not in the other, the statement follows.
For the second item, it is easy to see that fk(a, b) is expressible with the formula
h  [a  h =   [b]i]i. However, this property cannot be expressed in XPath= ( )
because the models T and T 0 below are bisimilar for XPath= () via Z, depicted as dotted
lines.
T

x

c, 0

b, 1 b, 2

a, 1 a, 2



x

T
c, 0

a, 3 a, 2 a, 1

b, 2

b, 1

Since T , x satisfies fk(a, b) but T 0 , x0 does not, from Theorem 28 it follows that fk(a, b)
is not expressible in XPath= ( ).
Finally, suppose there exists   XPath= (  )+ expressing fk(a, b). Since T is a
substructure of T 0 we have T , x  T 0 , x by Lemma 25. By Theorem 28(2) and the fact
that T , x |= , we have T 0 , x |= , which is a contradiction.
Let dist3 (x) be the property stating that there are nodes y, z so that xyz and x, y, z
have pairwise distinct data values.

305

fiFigueira, Figueira, & Areces

Proposition 39.
1. dist3 is expressible in XPath= ();
2. dist3 is not expressible in XPath= ( );
3. neither dist3 nor its complement can be expressed in XPath= (  )+ .
Proof. For 1, one can check that T , x |=  iff T , x satisfies dist3 , for
 = h 6= [h 6= [h 6= i]i]i.
Let us see 2. Consider the data trees T , x and T 0 , x0 depicted below. It is straightforward
that T , x satisfies dist3 and T 0 , x0 does not.
x

a, 2

T

$#

a, 1

a, 1 a, 2

a, 1 a, 2

a, 3

x

a, 2

0

T0
a, 1

a, 1

a, 1 a, 2

Let v10 and v20 be the leaves of T 0 and let v be the only node of T with data value 3.
One can check that T , x T 0 , x0 via Z  T  T 0 defined by
Z = {hu, u0 i | h(u) = h(u0 )  data(u) = data(u0 )}  {hv, v10 i, hv, v20 i},
where h(y) is the height of y, i.e., the distance from y to the root of the corresponding tree
(Z is depicted as dotted lines in picture above). Since T , x satisfies dist3 but T 0 , x0 does
not, from Theorem 28 it follows that dist3 is not expressible in XPath= ( ).
For 3, one can verify that T , x  T 0 , x0 via Z as defined above. If dist3 were definable
in XPath= (  )+ via  and the fact that T , x |= , by Theorem 28(2) we would have
T 0 , x0 |= , and this is a contradiction.
Let dist3 denote the complement of dist3 , i.e., dist3 (x) iff for all y, z so that xyz,
we have that x, y, z do not have pairwise distinct data values. Now T 0 , x0 satisfies dist3 and
T , x does not. Since T 0 is a substructure of T , by an argument analog to the one used in
the proof of Proposition 38-2, dist3 is not expressible in XPath= (  )+ .
5.3 Expressiveness Hierarchies
Define `,k as the equivalence ` restricted to formulas of nesting depth at most k, that
is, T , u `,k T 0 , u0 iff for all   XPath= () such that dd()  ` and nd()  k we have
T , u |=  iff T 0 , u0 |= . Define a more fine-grained notion of bisimulation in a similar
way. We say that u  T and u0  T 0 are (`, k)-bisimilar for XPath= () (notation:
T , u`,k T 0 , u0 ) if there is a family of relations (Zj,t )j`,tk in T  T 0 such that uZ`,k u0 and
for all j  `, t  k, x  T and x0  T 0 we have
306

fiModel Theory of XPath on Data Trees. Part I: Bisimulation and Characterization

0,0 = 0,1 = 0,2 = 0,3 = 0,4   










1,0  1,1 = 1,2 = 1,3 = 1,4   










2,0  2,1  2,2 =

2,3 = 2,4   










3,0  3,1  3,2  3,3 = 3,4   
..
.

..
.

..
.

.

..
.

..

..
.

Figure 4: Hierarchy of XPath= ().
 Harmony: If xZj,t x0 then label (x) = label (x0 ).
n

m

 Zig: If xZj,t x0 , xv and xw with n, m  j then there are v 0 , w0  T 0 such that
n
m
x0 v 0 , x0 w0 and
1. data(v) = data(w)  data(v 0 ) = data(w0 ),
i

i

2. if t > 0, (v) Zjn+i,t1 (v 0 ) for all 0  i < n, and
i

i

3. if t > 0, (w) Zjm+i,t1 (w0 ) for all 0  i < m.
n

m

 Zag: If xZj,t x0 , x0 v 0 and x0 w0 with n, m  j then there are v, w  T such that
n
m
xv, xw and items 1, 2 and 3 above are verified.
Following the same ideas used in Propositions 9 and 11, it is easy to show that (`, k)bisimulations characterize (`, k)-equivalence.
Proposition 40. T , u `,k T 0 , u0 iff T , u `,k T 0 , u0 .
The following theorem characterizes when an increase in nesting depth results in an
increase in expressive power (see Figure 4). We speculate that a similar hierarchy holds in
the absence of data values, but this is not a direct consequence of our result.
Theorem 41. For all `, k  0, i  1, `,0 ) `,1 )
`,k ) `+i,k .



) `,` = `,`+i , and

Proof. Consider the data trees Tni , Tn0i (n  0, i  {1, 2}) defined for every k.
307

fiFigueira, Figueira, & Areces

1,0

x10

a, 1
a, 2

b, 1

b, 2

0,0
a, 1

b, 1

0,0

T01

x1n
a, 1

Tn1

Tn1

Tn2

1
Tn+1

a, 1

x1
0

b, 2

a, 1

a, 2

a, 1

n+1,n
Tn1

1,0
a, 2

b, 1

T01

n+2,n+1

Tn2

x20

b, 2

0,0
a, 1

b, 1

0,0

T02

x1
n

Tn1

x2n
a,12

Tn2

Tn1

1
Tn+1

Tn1

Tn2

2
Tn+1

x2
0
a, 2

b, 2

T02

n+2,n+1
a,12

n+1,n
Tn2

a, 2

Tn2

x2
n

Tn1

Tn2

2
Tn+1

Note that `,k+1  `,k and `+1,k  `,k by definition. We show that `,k 6= `,k+1


1 1
for all `  k + 1. For this purpose, we show that Tk1 , x1k k+1,k Tk01 , x01
k but Tk , xk 6k+1,k+1
Tk01 , x01
k.
That Tk1 , x1k 6k+1,k+1 Tk01 , x01
k results from the fact that the property there is a path of
length k + 1 ending with a label a whose every pair of consecutive nodes have distinct data
value is definable with the following formula k+1 of depth k + 1 and nesting depth k + 1,

1 = h 6= [a]i

i+1 = h 6= [i ]i

for i > 0.


01 01
1 1
Since Tk1 , x1k |= k+1 but Tk01 , x01
k 6|= k+1 , it follows that Tk , xk 6k+1,k+1 Tk , xk .


1 1
01 01
To show Tk1 , x1k k+1,k Tk01 , x01
k we use Proposition 40 and show Tk , xk k+1,k Tk , xk .
Note that Tk1 and Tk2 (resp. Tk01 and Tk02 ) are equal modulo renaming of data values, so we
are also showing that the roots of any two data trees with subindex k are (k +1, k)-bisimilar.

Observation 42. Note that the set of immediate subtrees of the roots of Tk1 , Tk01 , Tk2 , Tk02
are the same as those of Tk01 , Tk2 , Tk02 (and of Tk1 , Tk01 , Tk02 ) by construction.

We show Tk1 , x1k k+1,k Tk01 , x01
k . For every j  k + 1, t  k, let Zj,t be the set of all pairs
(x, y)  Tk1  Tk01 so that x and y are some xik0 or x0ik0 for i  {1, 2} and k 0  t (the notation
xik0 and x0ik0 does not necessarily identify a unique node but, possibly, many; the intended
meaning is that they can refer to any of them). Observe that
Zj+1,t  Zj,t

for all j, t  k.

(4)

We show that (Zj,t )jk+1,tk verify the bisimulation conditions. We proceed by induction on j + t. The base case, j = t = 0, is trivial. The case l > 0, t = 0 is also
straightforward.
Suppose then that t > 0. Let (u, u0 )  Zj,t . Again, Harmony is met since Zl,t relates
0
only nodes with label a. Let us suppose that u is some x1t0 and u0 is x01
t0 for some t  t, the
other cases being similar or simpler.
n
m
Let us now show Zig. Let v, w be so that x1t0 v and x01
t0 w with n, m  j.
308

fiModel Theory of XPath on Data Trees. Part I: Bisimulation and Characterization

 If v is inside the subtree Tt20 1 of Tt10 , but it is not x2t0 1 , then we choose v 0 as the
1
2
corresponding node inside the subtree Tt10 1 of Tt01
0 . Remember that Tt0 1 and Tt0 1 are
isomorphic modulo a renaming of data values, so by corresponding we mean the node
in the same position in the tree. data(v) = data(v 0 ) by Observation 42. Furthermore,
since every node of Tt10 1 is in a Zj,t1 -relation with the corresponding node in Tt20 1
i

i

by construction of Zj,t1 , it follows that (v)Zj,t1 (v 0 ) for all i  n. Thus, by (4),
i

i

(v)Zjn+i,t1 (v 0 ) for all i  n.

02
 If, on the other hand, v is x2t0 1 , we choose v 0 as the root of Tt02
0 1 , xt0 1 . Again,
0
0
we have that data(v ) = data(v) and by construction that vZj,t1 v . Thus, by (4),
vZj1,t1 v 0 .
0
01
 Finally, if v falls outside Tt02
0 1 , we choose v as the same node in Tt0 , where of course
i

i

we will have that data(v) = data(v 0 ) and that (v)Zj,t1 (v 0 ) for all i  n. Thus,
i

i

by (4), (v)Zjn+i,t1 (v 0 ) for all i  n.

We do the same with w and w0 . Since in every case we can reach a node with the same
data value and so that the corresponding nodes in the path are Zj,t1 -related, it follows
that the Zig condition is satisfied. The Zag condition is only easier, and hence we conclude
that Tk1 , xk+1,k Tk0 1 , x0 for every k.
We therefore have that `,k+1 ( `,k for all `  k + 1.

The fact that `+1,k ( `,k is of course trivial, formulas of depth ` + 1 can express the
tree has at least depth ` + 1, which cannot be expressed by formulas of depth `.
It remains to show that `,k = `,k+1 for all `  k. To show this, we prove T , x`,k+1

T 0 , x0 for every T , T 0 so that T , x`,k T 0 , x0 . We prove it by induction on ` + k. The base
case is easy.
For the inductive case, let Zj,t = j,t for all j  `, t  k. Hence, (Zj,t )j`,tk verify
the bisimulation conditions. Let Z`,k+1 = {(x, x0 )}. We show that Z`,k+1 together with
(Zj,t )j`,tk verifies the bisimulation conditions. Harmony follows from xZ`,k x0 . We show
n
m
Zig since Zag is equivalent. Suppose xv, xw with n, m  `. Then, since Z`,k verifies
n
m
Zig, there are x0 v 0 , x0 w0 where
(1) data(v) = data(w0 ) iff data(v 0 ) = data(w0 ),
i

i

(2) (v)Z`n+i,k1 (v 0 ) for all i  {0, . . . , n  1}, and
i

i

(3) (w)Z`m+i,k1 (w0 ) for all i  {0, . . . , m  1}.
Since `  k, then `n+i  k 1. Further, `n+i+k < `+k, which means that we can
i
i
apply the inductive hypothesis. Hence, by inductive hypothesis, T , (v)`n+i,k T 0 , (v 0 )
i

i

i

i

and thus (v)Z`n+i,k (v 0 ). By an indentical reasoning, T , (w)`n+i,k T 0 , (w0 ) and
i

i

thus (w)Z`n+i,k (w0 ). Thus, the Zig condition for `,k+1 is verified. The Zag condition
holds by symmetry.
309

fiFigueira, Figueira, & Areces


With respect to vertical XPath, note that since 
r,s,k  r0 ,s0 ,k0 for all (r, s, k) 
as a consequence of Proposition 15 we obtain that for every r, s, k with r + s 


2 there is some k 0 > k so that 
r,s,k ) r,s,k0 . In fact, we conjecture that r,s,k )

(r0 , s0 , k 0 ),


r,s,k+1 for every k. We argue that this can be proven through the models (Tn )n in the

proof of Proposition 15, by showing that Tk , xr0 ,s0 
r,s,k Tk+1 , xr0 ,s0 but Tk , xr0 ,s0 6r,s,k+1



Tk+1 , , xr0 ,s0 for every (r, s)  (r0 , s0 ). The fact that 
r,s,k ) r+1,s,k and r,s,k ) r,s+1,k
are straightforward. We would then obtain the following.

0 0 0
Conjecture 43. 
r,s,k ) r0 ,s0 ,k0 for all (r, s, k) < (r , s , k ), r + s  2.

6. Discussion
In this article we studied model theoretic properties of XPath over both finite and arbitrary
data trees using bisimulations. One of the main results we discuss is the characterization of
the downward and vertical fragments of XPath as the fragments of first-order logic which
are invariant under suitable notions of bisimulation. This can be seen as a first step in
the larger program of studying the model theory and expressiveness of XPath with data
values and, more generally, of logics on data trees. It would be interesting to study notions of
bisimulation with only descendant; or characterizations of XPath with child and descendant,
as a fragment of FO with the descendant relation on data trees.
We did not considered XPath with horizontal navigation between siblings, such as the
axes next-sibling () and previous-sibling (). In fact, adding these axes results
in a fragment that is somewhat less interesting since the adequate bisimulation notion
on finite data trees corresponds precisely to data tree isomorphism modulo renaming of
data values. Next we explain why this is so. Consider the following notation for denoting
positions of nodes in a tree. Positions are elements of (N  N) , where the roots position
is the empty string , and the position of any other node in the tree is the concatenation
of the position of its parent and the pair (l, r), where l is the number of siblings to the
left of the node and r the number of siblings to its right. For every position p  (N  N)
of a tree, there is a path expression p of XPath= () that can access the node in this
position (and only this position) from the root. The p s are defined as  =  for the root;

(l,r) = l+1 [hk i  hk+1 i] for (l, r)  N  N;
V and p(l,r) = p (l,r) for p  (N  N) ,
(l, r)  N  N. It is easy to check that testing p hp [hi]i where p ranges over all leaf
positions of a given tree T tests the property that the subtree hanging from the node where
the formula is evaluated is structurally isomorphic to T . Further, by also adding the tests
hp [a] = p0 [a0 ]i for every pair of positions p, p0 with label a, a0 of T having the same data
value, as well as hp [a] 6= p0 [a0 ]i for those having different data value, yields the property
of being equal to T , up to isomorphism of data values.
In Section 5 we show a number of concrete application of the model theoretic tools we
developed, discussing both expressivity and non-expressivity results. We also show examples
of operations which are safe for a given XPath fragment. It would be worthwhile to devise
other model operations that preserve truth of XPath formulas as we show is the case for
subtree replication.
An important application of bisimulation is as a minimization method: given a data
tree T1 we want to find a data tree T2 , as small as possible, so that T1 and T2 are bisimilar
310

fiModel Theory of XPath on Data Trees. Part I: Bisimulation and Characterization

for some fragment L of XPath. Since L cannot distinguish between T1 and T2 , we can
use T2 as representative of T1 while the expressive power of L is all that is required by a
given application. The complexity of several inference tasks (e.g., model checking) depends
directly on the model size. This is why in some cases it may be profitable to first apply a
minimization step.
The existence of efficient minimization algorithms is intimately related to bisimulations:
we can minimize a data tree T by partitioning it in terms of its maximum auto-bisimulation
(observe that the identity is always an auto-bisimulation, and that the union of two bisimulations is a bisimulation; therefore there is a maximum, often called coarsest, bisimulation).
The idea is to find the coarsest auto-bisimulation Z over a given data-tree T . One
cannot simply make the quotient of T over Z, as the result is not necessarily a tree and it is
not clear how to assign data values to each class in the quotient. However, one can make a
quotient of T over the equivalence relation Z and same data value. If we do so, we obtain
a smaller structure, and then evaluate all queries here.
Determining the maximum auto-(bi)simulation, either downward or vertical, of a finite
data T tree can be done in polynomial time. A naive algorithm starts by defining Z as the
set of all nodes which satisfy Harmony. Each time Zig or Zag is not satisfied, it removes
from Z the pair responsible for Zig or Zag not being true. It repeats this until a fixed point
of Z is found; this is the maximum auto-bisimulation in T . If one is interested in deciding if
two nodes of different finite data trees are bisimilar, one can use the same idea: the answer
is yes if and only if the fixed point is not the empty set. Since checking the validity of Zig
or Zag is polynomial time computable (because there are linearly many paths in a tree),
and at each step the algorithm decreases the size of Z, the whole process has polynomial
time. Better implementations, based on more sophisticated ideas, such as the works of
Henzinger, Henzinger, and Kopke (1995) or Dovier, Piazza, and Policriti (2004), can lead to
more efficient polynomial time algorithms. We plan to design and implement algorithms for
data tree minimization using bisimulation and investigate their computational complexity.

Acknowledgements
This work was partially supported by grant ANPCyT-PICT-2013-2011, ANPCyT-PICT2010-688, ANPCyT-PICT-2011-0365, UBACyT 20020110100025 and the FP7-PEOPLE2011-IRSES Project Mobility between Europe and Argentina applying Logics to Systems
(MEALS) and the Laboratoire International Associe INFINIS.

References
Abiteboul, S., Bourhis, P., Muscholl, A., & Wu, Z. (2013). Recursive queries on trees and
data trees. In International Conference on Database Theory, pp. 93104.
Abriola, S., Descotte, M. E., & Figueira, S. (2014). Definability for downward and vertical
XPath on data trees. In Workshop on Logic, Language, Information and Computation,
Vol. 6642 of Lecture Notes in Computer Science, pp. 2034.
Abriola, S., Descotte, M. E., & Figueira, S. (2015). Model theory of XPath on data trees.
Part II: Binary bisimulation and definability. To appear in Information and Computation. http://www.glyc.dc.uba.ar/santiago/papers/xpath-part2.pdf.
311

fiFigueira, Figueira, & Areces

Benedikt, M., Fan, W., & Geerts, F. (2008). XPath satisfiability in the presence of DTDs.
Journal of the ACM, 55 (2), 179.
Benedikt, M., & Koch, C. (2008). XPath leashed. ACM Computing Surveys, 41 (1).
Blackburn, P., de Rijke, M., & Venema, Y. (2001). Modal Logic, Vol. 53 of Cambridge Tracts
in Theoretical Computer Science. Cambridge University Press.
Bojanczyk, M., Muscholl, A., Schwentick, T., & Segoufin, L. (2009). Two-variable logic on
data trees and XML reasoning. Journal of the ACM, 56 (3), 148.
Bojanczyk, M., & Parys, P. (2011). XPath evaluation in linear time. Journal of the ACM,
58 (4), 17.
Clark, J., & DeRose, S. (1999). XML path language (XPath). Website. W3C Recommendation. http://www.w3.org/TR/xpath.
David, C. (2008). Complexity of data tree patterns over XML documents. In Mathematical
Foundations of Computer Science, Vol. 5162 of Lecture Notes in Computer Science,
pp. 278289. Springer.
Dawar, A., & Otto, M. (2009). Modal characterisation theorems over special classes of
frames. Annals of Pure and Applied Logic, 161 (1), 142.
Dovier, A., Piazza, C., & Policriti, A. (2004). An efficient algorithm for computing bisimulation equivalence. Theor. Comput. Sci, 311, 221256.
Figueira, D. (2010). Reasoning on Words and Trees with Data. PhD thesis, Laboratoire
Specification et Verification, ENS Cachan, France.
Figueira, D. (2012). Decidability of downward XPath. ACM Transactions on Computational
Logic, 13 (4).
Figueira, D., & Segoufin, L. (2011). Bottom-up automata on data trees and vertical XPath.
In International Symposium on Theoretical Aspects of Computer Science, Vol. 9 of
LIPIcs, pp. 93104. Leibniz-Zentrum fur Informatik.
Figueira, D., Figueira, S., & Areces, C. (2014). Basic model theory of XPath on data trees.
In International Conference on Database Theory, pp. 5060.
Figueira, D., & Libkin, L. (2014). Pattern logics and auxiliary relations. In Logic in
Computer Science, pp. 40:140:10.
Figueira, S., & Gorn, D. (2010). On the size of shortest modal descriptions.. In Advances
in Modal Logic, Vol. 8, pp. 114132.
Forti, M., & Honsell, F. (1983). Set theory with free construction principles. Annali Scuola
Normale Superiore, Pisa, X (3), 493522.
Goranko, V., & Otto, M. (2007). Model theory of modal logic. In P. Blackburn, J. V. B., &
Wolter, F. (Eds.), Handbook of Modal Logic, Vol. 3 of Studies in Logic and Practical
Reasoning, chap. 5, pp. 249329. Elsevier.
Gottlob, G., Koch, C., & Pichler, R. (2005). Efficient algorithms for processing XPath
queries. ACM Transactions on Database Systems, 30 (2), 444491.
312

fiModel Theory of XPath on Data Trees. Part I: Bisimulation and Characterization

Gyssens, M., Paredaens, J., Gucht, D. V., & Fletcher, G. (2006). Structural characterizations of the semantics of XPath as navigation tool on a document. In Principles of
Database Systems, pp. 318327. ACM.
Harel, D. (1984). Dynamic logic. In Gabbay, D., & Guenthner, F. (Eds.), Handbook of
Philosophical Logic. Vol. II, Vol. 165 of Synthese Library, pp. 497604. D. Reidel
Publishing Co., Dordrecht. Extensions of classical logic.
Henzinger, M. R., Henzinger, T. A., & Kopke, P. W. (1995). Computing simulations on
finite and infinite graphs. In Proc. of 36th Annual Symposium on Foundations of
Computer Science, pp. 453462. IEEE Computer Society Press.
Hopcroft, J. (1971). An nlog(n) algorithm for minimizing states in a finite automaton. In
Z. Kohave, editor, Theory of Machines and Computations. Academic Press.
Jurdzinski, M., & Lazic, R. (2011). Alternating automata on data trees and xpath satisfiability. ACM Transactions on Computational Logic, 12 (3), 19.
Kanellakis, P., & Smolka, S. (1990). CCS expressions, finite state processes, and three
problems of equivalence. Inf. Comput., 86 (1), 4368.
Kurtonina, N., & de Rijke, M. (1997). Simulating without negation. Journal of Logic and
Computation, 7, 503524.
Lutz, C., Piro, R., & Wolter, F. (2011). Description logic tboxes: Model-theoretic characterizations and rewritability. In International Joint Conference on Artificial Intelligence,
Barcelona, Catalonia, Spain, July 16-22, 2011, pp. 983988.
Marx, M. (2004). XPath with conditional axis relations. In International Conference on
Extending Database Technology, Vol. 2992 of Lecture Notes in Computer Science, pp.
477494. Springer.
Marx, M., & de Rijke, M. (2005). Semantic characterizations of navigational XPath. SIGMOD Record, 34 (2), 4146.
Milner, R. (1980). A Calculus of Communicating Systems, Vol. 92 of Lecture Notes in
Computer Science. Springer.
Neven, F., Schwentick, T., & Vianu, V. (2004). Finite state machines for strings over infinite
alphabets. ACM Transactions on Computational Logic, 5 (3), 403435.
Olteanu, D. (2007). Forward node-selecting queries over trees. ACM Transactions on
Database Systems, 32 (1), 3.
Olteanu, D., Meuss, H., Furche, T., & Bry, F. (2002). XPath: Looking forward. In International Conference on Extending Database Technology, pp. 109127.
Otto, M. (2004a). Elementary proof of the van Benthem-Rosen characterisation theorem.
Tech. rep. 2342, Fachbereich Mathematik, Technische Universitat Darmstadt.
Otto, M. (2004b). Modal and guarded characterisation theorems over finite transition
systems. Annals of Pure and Applied Logic, 130 (1-3), 173205.
Otto, M. (2006). Bisimulation invariance and finite models. In Logic Colloquium02, Vol. 27
of Lecture Notes in Logic, pp. 276298.
313

fiFigueira, Figueira, & Areces

Paige, R., & Tarjan, R. (1987). Three partition refinement algorithms. SIAM J. Comput.,
16 (6), 973989.
Park, D. (1981). Concurrency and automata on infinite sequences. In Theoretical Computer
Science, Vol. 104 of Lecture Notes in Computer Science, pp. 167183. Springer.
Rosen, E. (1997). Modal logic over finite structures. Journal of Logic, Language and
Information, 6 (4), 427439.
Sangiorgi, D. (2009). On the origins of bisimulation and coinduction. ACM Transactions
on Programming Languages and Systems, 31 (4).
ten Cate, B., Fontaine, G., & Litak, T. (2010). Some modal aspects of XPath. Journal of
Applied Non-Classical Logics, 20 (3), 139171.
van Benthem, J. (1976). Modal Correspondence Theory. PhD thesis, Universiteit van
Amsterdam.

314

fiJournal of Artificial Intelligence Research 53 (2015) 633-658

Submitted 01/15; published 08/15

Placement of Loading Stations for Electric Vehicles:
No Detours Necessary!
Stefan Funke
Andre Nusser

funke@fmi.uni-stuttgart.de
nusser@fmi.uni-stuttgart.de

Universitat Stuttgart
Institut fur Formale Methoden der Informatik
70569 Stuttgart, Germany

Sabine Storandt

storandt@cs.uni-freiburg.de

Albert-Ludwigs-Universitat Freiburg
Institut fur Informatik
79110 Freiburg, Germany

Abstract
Compared to conventional cars, electric vehicles (EVs) still suffer from considerably
shorter cruising ranges. Combined with the sparsity of battery loading stations, the complete transition to E-mobility still seems a long way to go. In this paper, we consider the
problem of placing as few loading stations as possible so that on any shortest path there
are sufficiently many not to run out of energy. We show how to model this problem and
introduce heuristics which provide close-to-optimal solutions even in large road networks.

1. Introduction
Battery-powered, electric vehicles (EVs) are an important means towards a reduction of
carbon dioxide emissions when recharged using renewable energies, e.g. from solar or wind
power. Despite their environmental advantages EVs still wait for their breakthrough with
the main reason being their limited cruising range (often less than 200km) together with
the sparsity of battery loading stations (BLSs). Planning a trip from A to B with an EV
nowadays is a non-trivial undertaking; the locations of BLSs have to be taken into account,
and many destinations are completely out of range.
Hence, in this early phase of E-mobility an important goal is to establish a network of
BLSs so that using an EV becomes a worry-free enterprise. As modern BLSs require only
little space (see Figure 1, left, for an illustration), they can be placed almost everywhere.
But as this generates costs, a natural objective is to minimize the number of installed BLSs.
In previous work (Storandt & Funke, 2013), a heuristic was proposed to determine BLS
locations such that one can get from anywhere to anywhere in the road network without
running out of energy (when choosing a suitable route). Unfortunately, this approach
only guarantees connectivity but not reasonability of the routes. In fact, even rather close
destinations where routes with only one recharging stop are possible, might require long
detours with several recharging stops due to the placement of BLSs. A related approach by
Lam, Leung and Chu (2013) suffers from similar drawbacks. In the long run, E-Mobility
will only prevail if a road trip with an EV can be undertaken without unreasonable detours
c
2015
AI Access Foundation. All rights reserved.

fiFunke, Nusser, & Storandt

Figure 1: Inner-City battery loading station (left image), and feasible loading station cover
for a small map cut-out (right image).

being introduced. In this paper we ask for a placement of the BLSs such that on any
shortest path there are enough BLSs not to get stranded when starting with a fully loaded
battery  just like it is typically the case with gas stations for conventional cars. We call
such a set of BLS locations an EV Shortest Path Cover (ESC) and define the respective
optimization problem as follows.
Definition 1 (EV Shortest Path Cover (ESC)). Given a (di)graph G(V, E), edge costs
c : E  R+ and a function  which for a path  decides whether this path can be traveled
along without recharging the EV, the problem of determining a minimum subset L  V of
BLSs such that every shortest path wrt c can be traveled without running out of energy is
called the EV Shortest Path Cover Problem.
See Figure 1, right, for an idea of how a valid ESC looks like. In the remainder we
will define n = |V | and m = |E| if not otherwise noted. Also consider that for sake
of a clearer presentation we assume unique shortest paths (which can be enforced using
standard techniques like symbolic perturbation). We will describe how to deal with ambiguous shortest paths towards the end of the paper. The function  captures all the energy
characteristics of the network and the considered vehicle. Typically, in mountainous areas
or on roads with rough surfaces, the minimal paths where an EV runs out of energy are
considerably shorter than in flat terrain or downhill. For our experiments we determined
the energy consumption of a road segment e = (v, w)  E with elevations h(v), h(w) as
distance(e) +   max(h(w)  h(v), 0) for some weighting parameter  (dependent on the
EV). That is, the energy consumption is determined by the Euclidean distance and the
height differences, similar to the energy model used in previous work (Artmeier, Haselmayr,
Leucker, & Sachenbacher, 2010) but disregarding energy recuperation (negative edge costs).
The function  compares for a path  the accumulated energy consumption along its edges
with the EVs battery capacity B  R+ to determine if recharging is necessary. Note that
one could employ any kind of monotonous function  here, all approaches we will introduce
in the following will work notwithstanding this particular choice.
634

fiPlacement of Loading Stations for Electric Vehicles

1.1 Contribution
We describe how to model the ESC problem as an instance of the Hitting Set problem
with the sets being shortest paths which require at least one battery recharge. This allows
us to use algorithms developed for solving Hitting Set problems, e.g. the standard greedy
approach. Unfortunately, it turns out that the difficulty of computing an ESC solution
is already the instance construction. With (n2 ), n = |V | shortest paths in the network,
extracting and storing them naively requires too much time and space to be practical.
We therefore design new shortest path extraction and representation techniques, which
allow to tackle even very large road networks. Moreover, we develop several refinements
and heuristics which provide feasible ESC solutions more efficiently. While no a priori
approximation guarantee can be shown for the solutions, we can prove a posteriori  using
instance-based lower bounds  that for real-world instances the actual approximation ratio
is only a small constant.
In this extended version of the original paper (Funke, Nusser, & Storandt, 2014b), we
present the following new results and insights regarding ESC: We prove the ESC problem to be NP-hard (and even hard to approximate). The proven hardness is motivation
and justification for the development of heuristic algorithms. Furthermore, we explain how
to transform shortest paths from one (newly developed) representation into another, along
with a theoretical run time analysis and experiments. The ability to transform between representations allows for more flexibility and might prove useful for other applications where
compact representations of shortest paths are used. We also describe simple minimality
checks for shortest paths deciding whether they need to be considered in the respective Hitting Set instance. Especially for large networks, such checks reduce the number of shortest
paths that have to be stored significantly. In addition, we provide details for more involved
lower bound constructions. Finally, we lay out methods for dealing with ambiguous shortest
paths, for augmenting an already existing loading station set and for the case that loading
station locations are restricted to a subset of the nodes in the network.

2. Theoretical Analysis
Let us first prove the ESC problem to be NP-hard, so there is no hope for efficient algorithms
that solve ESC to optimality (unless P=NP). Hence in the remainder of the paper we will
focus on designing algorithms that compute good approximate solutions.
Theorem 1 (Hardness). The EV-Shortest Path Cover problem (ESC) is NP-hard.
We prove NP-hardness of ESC by a solution size preserving reduction from Vertex Cover
(VC), which is one of the classical NP-hard problems. We use the following definition and
notation for VC:
Definition 2 (Vertex Cover). Given a graph G(V, E), the goal is to find a vertex set C  V
of minimal cardinality such that e  E : e  C 6= .
To prove Theorem 1, we show that the ability to solve the ESC problem efficiently implies
the ability to solve VC efficiently as well. Hence, ESC has to be NP-hard as otherwise we
would have a contradiction to the NP-hardness of VC. To that end we construct for a given
635

fiFunke, Nusser, & Storandt

1
1

1

1
1

1
1

1

1

1

1

1

1
1

Figure 2: Left: Vertex Cover instance with optimal solution of size 3 (red circled nodes).
Right: Respective ESC instance constructed by inserting an auxiliary edge per
node in the graph on the left, and augmenting all edges with cost 1. The circled
nodes indicate an optimal Hitting Set such that no path of cost at least 3 is unhit.

VC instance Gvc (Vvc , Evc ) a corresponding instance of ESC specified by G(V, E), c and  as
follows:
 V = Vvc , E = Evc i.e. the ESC instance initially contains all nodes and edges of the
VC instance
 for all v  V add an auxiliary vertex v 0 and an auxiliary edge {v, v 0 } to G
 for all e  E set the costs c(e) uniformly to 1
  is true for all paths consisting of less than three edges and false otherwise, i.e. every
shortest path that traverses at least three edges has to have a loading station on it to
make the set of loading stations a feasible ESC
The construction requires only polynomial time in the size of Gvc . Figure 2 illustrates the
transformation from a Vertex Cover instance to an ESC instance on a small example.
We first show that any Vertex Cover solution in Gvc is also an ESC solution for the ESC
instance constructed as described above.
Lemma 1. A Vertex Cover C in Gvc yields an ESC L for G, c,  with |L| = |C|.
Proof. Let C  Vvc be a VC solution in Gvc . As the ESC graph G contains a corresponding
vertex for every v  Vvc , we simply set L = C (so obviously |L| = |C|). It remains to show
that after placing loading stations according to L, every shortest path in G can be traveled
without running out of energy. Assume for contradiction that there exists a shortest path
in G consisting of three edges {u, v}{v, w}{w, x} and neither v nor w are in L. As every
edge e  Evc has at least one of its two vertices in L (because L = C is a Vertex Cover
for Evc ), it follows that {v, w} has to be an auxiliary edge only present in G but not in
Gvc . But auxiliary edges cannot be in the middle of a path containing three or more edges,
as every auxiliary vertex has degree 1. So every path consisting of three edges contains a
loading station, hence L is a valid ESC.
To complete the proof of Theorem 1, we have to show that a valid ESC solution leads
to a valid Vertex Cover solution as well.
636

fiPlacement of Loading Stations for Electric Vehicles

Lemma 2. An ESC L for G, c,  yields a Vertex Cover C in Gvc with |C|  |L|.
Proof. Let L be a valid ESC solution. As L might contain auxiliary vertices, we construct
C by replacing every auxiliary vertex v 0 in L with its respective original vertex v. As v
might also be part of L, we conclude that |C|  |L|. To show that C is a valid Vertex
Cover in Gvc , we prove that for every edge {v, w}  Evc either v or w or both are in C.
Assume for contradiction that there exists an edge {v, w}  Evc with {v, w}  C = .
Accordingly, neither v, nor w, nor the respective auxiliary vertices v 0 and w0 were part of
the ESC solution L. This implies a shortest path {v 0 , v}, {v, w}, {w, w0 } of three edges with
no loading station on it in G. As this contradicts L being a valid ESC solution, we conclude
that for every edge we have {v, w}  C 6=  and therefore C is a valid Vertex Cover.
If L is an optimal solution for ESC, then obviously |C| = |L| is fulfilled in Lemma 2, as
placing a loading station at an auxiliary vertex v 0 and the corresponding original vertex v at
the same time renders the loading station at v 0 superfluous. Hence in an optimal solution,
we never have both vertices v and v 0 in L. So in combination with Lemma 1, we showed
that for every instance of VC, we can construct in polynomial time an instance of ESC
for which an optimal solution translates into an optimal solution for the VC instance of
the same size in a straightforward manner. Therefore any hardness results for VC carry
over to ESC. This proves Theorem 1 and furthermore rules out the existence of (1 + )
polynomial-time approximation schemes for ESC via the proven APX-hardness of a factor
better than 1.3606 for VC (Dinur & Safra, 2004):
Corollary 1. ESC cannot be approximated better than 1.3606.
With the proven hardness, efficient algorithms that solve the ESC problem to optimality
might be difficult to design unless one can make use of certain problem aspects as the battery
capacity parameter or road network characteristics.

3. Modeling ESC as a Hitting Set Problem
In the following, we aim for good approximation algorithms and heuristics that solve the
ESC problem in practice. In particular, we will exploit the fact that ESC can be modeled
as an instance of the well-known Hitting Set problem and therefore algorithms suitable for
Hitting Set computations transfer to ESC. The classical Hitting Set (HS) problem is defined
as follows:
Definition 3 (Hitting Set). Given a set system (U, S) with U being a universe of elements
and S a collection of subsets of U , the goal is to find a minimum cardinality subset L  U
such that each set S  S is hit by at least one element in L, i.e. S  S : L  S 6= .
In our case, U consists of all nodes of the road network (the possible BLS locations). The
set S is composed of the vertex sets of all shortest s-t-paths (excluding s and t themselves)
for which a fully charged battery at s does not suffice to reach t. Our function  characterizes
the paths where the energy consumption of traversing them exceeds the battery capacity
B  R+  we call these paths B-violating. Clearly, we only need to consider set-minimal
paths as supersets are hit automatically. Theorem 2 shows that this Hitting Set formulation
indeed solves our ESC problem (see also Figure 3 for an illustration).
637

fiFunke, Nusser, & Storandt

v
s
B

l
l

v

l

s
B

s
B

s
B

t
v

t
t
t

Figure 3: If a shortest path from s to t exhibits a B-violating prefix (s-v, marked red), then
according to the Hitting Set formulation there has to be a loading station l on
this subpath. As the vehicle fully reloads at l, the same argumentation applies
to the subpath l-t, which is illustrated in the picture by cutting off the prefix
s-l in every layer. The final s-t-path is not B-violating anymore. Therefore the
originally considered s-t-path can be traveled when visiting the three indicated
loading stations. The blue Bs mark the nodes where the battery is fully loaded,
i.e. where the charge level is equal to the battery capacity.

Theorem 2 (Correctness). The Hitting Set formulation leads to a feasible ESC solution,
i.e. when placing loading stations according to the Hitting Set solution L, every shortest
path in G can be traversed with an EV without running out of energy according to .
Proof. Let (s, t) be a shortest path from s to t in G which is B-violating, and let the EV
be fully loaded at s. Let (s, v) be the minimal B-violating prefix of (s, t). This prefix
has to be hit by a loading station l  L with l 6= s, l 6= v as demanded by the Hitting Set
formulation. The EV can reach l from s, as (s, v) is the minimal B-violating prefix and l
appears before v on (s, v). At l the EV is fully re-charged. Hence the whole argumentation
transfers now to the subpath (l, t). Applying this argument recursively, the EV will finally
reach a loading station on (s, t) from which the suffix of the path is no longer B-violating.
Therefore the EV will reach t via (s, t) without running out of energy.

Note that there is no precision loss in reformulating ESC as a Hitting Set problem
instance, since every solution to ESC is a feasible solution of exactly the same cardinality
for the respective Hitting Set problem instance.
At this point, common Hitting Set solving techniques can be applied to solve ESC, e.g.
the standard greedy algorithm. The greedy algorithm repeatedly picks the node hitting
most so far unhit sets in S and adds it to the solution. It terminates as soon as all sets are
hit. A solution computed with the greedy algorithm is guaranteed to be a ((ln |S|) + (1))approximation (Chvatal, 1979); we ignore the lower-order term (1) from now on. In our
application, the number of sets in the system is upper bounded by the number of shortest
paths in the graph. Therefore, we have |S|  n2 and hence the greedy algorithm provides
a 2 ln(n) approximation guarantee. The running time of the greedy algorithm depends
crucially on fast access to the so far unhit sets in S in each round.
638

fiPlacement of Loading Stations for Electric Vehicles

112
43

34
16
10
7

11

9
6

1

28

s

t

4

Figure 4: Cut-out of a CH-graph: Black edges are original edges, blue edges indicate shortcuts. The red node labels reflect the contraction order. So, for example, at the
moment the node labeled 6 was contracted, the shortcut from node 10 to node 9
was inserted as the shortest path from 10 to 9 went over 6. On the right side, the
search space for a query from the leftmost to the rightmost node is illustrated.
Green edges show Gout (s) and purple edges Gin (t).

In the remainder of the paper, we will investigate the efficient construction of the set
system using different path extraction and representation schemes and study their influence
on the greedy algorithm.

4. Basics for Practical ESC Computation
To determine suitable BLS positions, we first have to construct the set of shortest paths
on which the EV would run out of energy (according to ). Computing the shortest path
between two nodes or from one to all other nodes is classically performed using Dijkstras
algorithm (also shortly called Dijkstra). In large street networks Dijkstra is too slow to
process a large number of such queries (as it is necessary for our application), though.
Therefore, we will instrument speed-up techniques developed for accelerating shortest path
queries to achieve better running times for our approaches. In particular, we will employ
Contraction Hierarchies (CH) (Geisberger, Sanders, Schultes, & Delling, 2008) for this
purpose. The basic idea behind CH is to augment the graph G(V, E) with a set E 0 of so
called shortcuts, which span (large) sections of shortest paths. Using these shortcuts instead
of original edges allows for a dramatic reduction of operations in a Dijkstra run.
The central operation in the CH preprocessing is that of a node contraction. Considering
a graph G(V, E) with a node v to be contracted, the goal is to remove v from G without
affecting shortest path distances between all remaining nodes. This can be achieved by
creating additional shortcut edges between neighbors of v as follows: For every pair of
neighbors u, w of v with (u, v), (v, w)  E, a shortcut (u, w) is created (with cost of the
path uvw) if uvw is the only shortest path from u to w. Then the resulting graph (with
v removed and all necessary shortcuts added) exhibits the same shortest path distances as
the original graph. The CH preprocessing phase instruments this node contraction by first
assigning a label l : V  N to each node. Then, the nodes are contracted in increasing label
order. Having contracted all nodes and constructed a set of shortcuts E 0 on the way, we
639

fiFunke, Nusser, & Storandt

return as result of the preprocessing phase the CH-graph G0 (V, E  E 0 ), that is, the original
graph augmented with the shortcuts (and the labeling), see Figure 4 (left) for an example.
According to these labels, original or shortcut edges (v, w) are referred to as upwards
if l(v) < l(w) and downwards otherwise; paths are called up/downwards if they consist
exclusively of edges of that type. It can be shown that by the way we created shortcuts,
for each node pair s, t  V a shortest path exists in G0 = G(V, E  E 0 ) which can be
decomposed into an upward path starting at s followed by a downward path ending in t;
The highest node of the path wrt l is called the peak node in the following. This property of
the path allows to restrict a bidirectional Dijkstra run to Gout (s) and Gin (t) which refer to
the subgraphs of G0 containing only all upwards paths starting in s or all downwards paths
ending in t respectively. In Figure 4 (right), these subgraphs are illustrated. The resulting
optimal path found by the bidirectional Dijkstra has the same costs as the shortest path
in the original graph. But the representation of the path is different, because the path
now consists (partly) of shortcuts. To get the shortest path in the original graph, an
unpacking procedure is applied. For every shortcut, the two edges (original or shortcut) it
directly spans are memorized during CH construction. Thus, for unpacking one just has to
recursively replace shortcuts by these spanned edges until all original edges are identified.
Note that the CH scheme can also be employed to just represent a shortest path 
very concisely by replacing as many subpaths of  as possible by shortcuts. We also call
this representation of  a CH-path. In our experiments it turns out that CH-paths are an
extremely economic representation scheme for shortest paths.
For the one-to-all shortest path problem, the PHAST algorithm (Delling, Goldberg,
Nowatzyk, & Werneck, 2011) also takes advantage of the CH-preprocessing scheme. Here
in a first phase all nodes in Gout (s) for a source s  V are settled via a Dijkstra run. In the
second phase all downward edges (v, w) are relaxed in decreasing order induced by l(w),
thereby computing correct distances to all nodes in V . So the second phase is simply a sweep
over a subset of the edges which requires only linear time. Correctness of PHAST results
again from the fact that for every node t the shortest path from s to t can be decomposed
into an upwards path (with all contained nodes settled in the first phase) and a downwards
path. As the set of downwards edges forms a directed acyclic graph and the labels l(w)
induce a topological order, a sweep over the edges in this order assures that at the moment
an edge (v, w) is relaxed, node v is already settled. Hence PHAST computes exact shortest
path distances from s to all nodes in the network. Note that for a single shortest path query
PHAST is not the method of choice, as other techniques like pure CH-Dijktra computations
normally run in time clearly sublinear in the number of edges in practice.

5. Construction of the Set System
In this section, we investigate several strategies to extract the set system for a given ESC
instance, i.e. the set of all minimal shortest paths in G which are B-violating. Along with
different extraction strategies, we present different ways to represent and store the respective
set of shortest paths and discuss the advantages and disadvantages of these representations.
640

fiPlacement of Loading Stations for Electric Vehicles

5.1 Naive Extraction
The simplest approach that comes to mind is to compute the shortest path tree (via Dijkstra)
for every s  V and to identify all nodes in the tree with accumulated energy cost values
above B. Once all nodes in the priority queue of Dijkstra have settled predecessors that
already belong to B-violating paths, we can abort the exploration from that source s. The
respective paths in the search tree can then be backtracked and stored as e.g. complete
vertex sets. For small exploration radii (small bounds B) this is a practical approach, but

for larger B the space consumption of O(n2 n), n = |V | is enormous (assuming an average

path length of n). Even if we only store the Dijkstra search tree for each s  V via
predecessor labels, we still have a space consumption of O(n2 ). Of course, we could easily
achieve linear space consumption in the number of B-violating paths, by only storing the
source vertex s and the target vertex t for each path. But if we want to have access to the
nodes on a certain path, we again have to run a Dijkstra computation in the network from
s to t. For huge sets of paths computing the nodes between s and t always on demand is
very time-intensive. No matter which representation we use to store the paths, the time
complexity of O(n  (n log n + m)), m = |E| for the naive extraction already limits usability
for real-world instances; O(n log n + m) being the runtime for one Dijkstra run.
In fact, this is also the main difficulty for other Hitting Set-type problems on street
networks. For example, speed-up techniques for shortest path queries like Transit Nodes
(TN) (Bast, Funke, & Matijevic, 2009) or Hub Labeling (HL) (Abraham, Delling, Goldberg,
& Werneck, 2012) are based on hitting a certain set of shortest paths as well. Methods
for complete instance construction are impractical there. Therefore several custom-tailored
heuristics were developed that allow for efficient computation without explicitly constructing
S (Arz, Luxen, & Sanders, 2013). But their setting differs significantly from ours, as in our
setting  poses an additional criterion to c for identifying the paths contained in S. Hence
the distance bound employed in their setting leads to a set of equal length paths, while
in our scenario, due to different energy consumption when driving uphill or downhill, the
lengths of minimal B-violating paths differ vastly. So unfortunately these TN and HL (and
other related) heuristics do not carry over to our setting. Therefore we need to explore new
ways of extracting and storing shortest path sets.
5.2 PHAST-Based Extraction
For large bounds B finding all B-violating paths from a source node resembles the one-to-all
shortest path problem. PHAST was explicitly designed to solve this task efficiently. The
paths we can backtrack in the respective search tree are in CH-representation, i.e. they
consist partly of shortcuts. This is a huge advantage compared to conventional paths in
terms of storage, because with shortcuts spanning large portions of the shortest path the
number of nodes in the CH-path is significantly smaller (about two orders of magnitude for
the street network of Germany). There are some downsides, though: Nodes are processed in
the second phase of PHAST in l-order and not increasingly by distance; hence incorporating
B as stopping criterion seems difficult. Moreover, if B is not that large or leads to paths with
vastly differing lengths, the (n2 ) lower bound on the (accumulated) runtime of PHAST
from every source might already result in a large overhead. Hence we propose a different
strategy which is also based on CH but has the potential of being significantly faster.
641

fiFunke, Nusser, & Storandt

p

B = 10
3

2
3
4
2

1

5

G2
2 ]8,10]

G1

p

9

6
4

7
4

7
8 ]4,8]

3
]7,8]

9

]1,8]

1

Figure 5: Left: Schematic representation of a CH-graph with the height of the nodes indicating the contraction order. For the blue-marked peak p, G1 := Gin (p) is colored
red and G2 := Gout (p) is colored green. Note that for all nodes in the bottom
layer, both G1 and G2 are empty and nothing has to be done (which in practice
is true for about 50% of the nodes). Right: Energy cost labels (black) assigned
to G1 and G2 resulting from two Dijkstra runs starting at the blue node. The
resulting intervals for nodes in G2 are expressed in purple. So if we, for example,
search for matching targets for the source node labeled 3 in G1 , the intervals
reveal that only the node labeled 9 in G2 is a suitable candidate.

5.3 Peak Node Mapping (PNM)
A large number of B-violating paths can originate from a source s  V . Exploring all
these paths with Dijkstra or PHAST is very time-consuming. The core idea of PNM is to
enumerate B-violating paths completely different but also taking the CH-representation of
paths into consideration. As explained above, shortest CH-paths are unimodal with respect
to the labeling l and the node with the maximal label is called the peak. Intuitively, nodes
with a high label appear in more shortest paths as peaks. In fact, in real-world graphs, the
5% nodes with highest level constitute the peaks of all reasonably long shortest paths. This
gives rise to a path enumeration algorithm, which explores paths not from the source but
from the peak, resulting in dramatically reduced search spaces for the majority of nodes.
Algorithm. Our PNM algorithm works as follows: We consider one by one every node
p  V as potential peak. As all shortest paths with peak p can only contain further nodes
with a smaller label, we only need to search upwards paths ending in p and downwards paths
starting in p for prefix and suffix candidates. The respective subgraphs of the CH-graph
G0 containing these paths are called G1 := Gin (p) and G2 := Gout (p), see Figure 5, left,
for an illustration. A conventional Dijkstra run in each of G1 and G2 (which are typically
very sparse) reveals the distances from p to the contained nodes. Now we are interested
in combinations of shortest (upward) paths in G1 and shortest (downward) paths in G2
leading to minimal B-violating paths. Testing them all naively is too expensive. Therefore,
we construct for each p an interval tree (Berg, Cheong, Kreveld, & Overmars, 2008) on the
nodes in G2 . The interval ]a, b], which we associate with such a node t, denotes the range of
possible energy consumption values of a path prefix (s, p) in G1 such that (s, p)  (p, t)
is a minimal B-violating path. These intervals can easily be computed by a single pass over
the Dijkstra search tree in G2 , see Figure 5, right, for an example. So for every possible
642

fiPlacement of Loading Stations for Electric Vehicles

Figure 6: Illustration of three methods for representing and storing shortest paths (stored
elements are always colored orange): On the left as complete vertex set, in the
middle as shortcut set/CH-path (the heights of the vertices in the image correspond to their l-value), and on the right as triple consisting of source, target and
peak vertex.

source s  G1 , we query the interval tree for the set of targets T in time O(log(|G2 |) + |T |)
storing the resulting paths as quadruples (s, p, t, c(s, p) + c(p, t)), t  T . Note that for the
employment of the interval trees we make use of the special choice of . For different choices
of  the interval computation procedure has to be adapted.
Filtering. After all nodes are processed, we have a set of B-violating paths from which,
unfortunately, not all are shortest paths. The concatenation of two shortest paths ((s, p)
and (p, t)) does not need to be a shortest path itself. So it remains to filter this set
appropriately. This can be achieved by using distance oracles with quasi constant look-up
time as e.g. provided by HL or by another pass over all nodes in the role of the peak, always
pruning a quadruple if for s, t a shorter path was found for p0 6= p. Note, that pruning
can already be employed during the construction phase if the intermediate path set sizes
become too large. The final set of paths is then stored as list of triples (s, p, t)  an even
more compact representation than CH-representation. In Figure 6, a visual comparison is
provided for storing a path as vertex set, as shortcut set, or as PNM triple.
Accessing all nodes in the respective shortest s-t-path in G as required for the greedy
Hitting Set algorithm is no longer trivial for our sophisticated representation schemes,
though. Therefore, we develop suitable adaptions of the greedy algorithm to work on
the CH-representations and on PNM triples in Section 6.
5.4 Minimality Checks
As mentioned when introducing the Hitting Set formulation of ESC, we only need to extract
and store minimal B-violating shortest paths, i.e. paths with no subpath being B-violating
as well. Adding non-minimal paths to the set system of course does not invalidate the
solution, but increases the complexity of storing all sets, and the running time of the greedy
algorithm later on.
For the naive extraction scheme, a B-violating path  = sv1 . . . vk t identified in a Dijkstra run from s might not be minimal, as the path might still be B-violating without some
prefix. More precisely, we have to check if the path remains B-violating when removing the
first edge (s, v1 ), and if so, we dont have to include  into S. The respective Dijkstra run
when considering v1 as source will ensure that no B-violating path is missed.
643

fiFunke, Nusser, & Storandt

For the PHAST-based extraction, the minimality check becomes more complex. As the
paths are in CH-representation, we might not have direct access to the first edge of the path
in the original graph (if the path starts with a shortcut). So we have to unpack the first
shortcut before performing the minimality check.
For PNM both prefix and suffix deletion might lead to a subpath that is still B-violating.
So immediately after the decision in favor of a triple (s, p, t), we check for the first edge on
the path from s to p and the last edge on the path from p to t if their removal will not
destroy the property of the path being B-violating. Again, like with PHAST, paths are in
CH-representation during the construction, therefore we have to unpack the shortcuts first.
5.5 Transformability
Note, that the extraction scheme does not tie us to a certain path representation. In fact,
all mentioned representations (vertex sets, source-target-pairs, CH-paths, triples) can be
converted into each other. Especially the transformation from vertex sets to CH-paths will
turn out to be favorable, as CH-paths yield a fair trade-off between space consumption and
applicability of the greedy algorithm as explained in more detail in Section 6. We will now
provide the details for all transformations including theoretical transformation times. For
the latter we assume that the complete vertex set representation contains k elements.
 From source-target-pairs to vertex sets. Given s, t, the complete path is computed via
a Dijkstra run in G and backtracking, requiring O(n log n + m) time.
 From vertex sets to CH-paths. We assume the CH-labels l : V  N are available.
Then a recursive procedure allows to turn the vertex set into a CH-path. We first
identify the node v0 in the vertex set with the highest l-value (the peak). Then we
split the vertex set into the prefix before v0 and the suffix after v0 . For both of those
sub-paths we again search for the node with the highest l-value, providing us with
v1 , v2 . These two nodes are connected with v0 via a direct edge/shortcut in the CHgraph (as all nodes in between were contracted before), so we have (v1 , v0 ), (v0 , v2 ).
This again provides us with a prefix (nodes before v1 ) and a suffix (nodes after v2 )
on which we recurse. The algorithm stops when the prefix is only s and the suffix
only t (or the prefix is monotonously increasing wrt l and the suffix monotonously
decreasing), see Figure 7, top, for an illustration. Assuming the CH-representation
contains h shortcuts, the transformation can be performed in O(k  h).
 From CH-paths to PNM triples. We just need to extract the peak vertex (besides
source and target) which can be done in O(h) if the CH-path consists of h shortcuts.
 From PNM triples to CH-paths. Given source s, target t, and peak p, we run a Dijkstra
starting at p in G1 := Gin (p) and G2 := Gout (p) until s and t are settled. As the
number of edges in a CH-graph is assumed to be in O(m), the runtime for a single
transformation is in O(n log n+m) just like for the transformation from source-targetpairs to vertex sets. But as typically a peak with a small l-value has very small G1
and G2 , and a peak with high l-value generates many s-t-paths at once, the amortized
costs per path are considerably smaller.
644

fiPlacement of Loading Stations for Electric Vehicles

Figure 7: From vertex sets to CH-paths and back. In the first row, a vertex set is given.
The l-values derived in the CH construction are indicated by the vertex elevations.
Then recursively the vertex with the highest l value (marked red in the images)
in the prefix and suffix of the path is extracted and shortcuts are added to span
path sections with lower l-value. In the second row, a CH-path is recursively
unpacked. Shortcuts are colored blue. The red arrows point to their replacement
edges in the next unpacking step. The final path is the same we started from in
the first row, and does not contain any shortcuts.

 From CH-paths to vertex sets. Given a CH-path, we apply the unpacking method
described in Section 4, i.e. we recursively replace shortcuts by their spanned edges
until the path consists of original edges only (see Figure 7, bottom). If the resulting
path consists of k vertices, the unpacking can be performed in O(k).
 From vertex sets to source-target-pairs. The first and the last vertex of the complete
path are stored and the rest is neglected. This transformation costs O(1) or O(k) if
we consider the deletion of the k  2 other elements as well.
In the following, we will no longer investigate the source-target pair representation, as
storing PNM triples requires only one item more per path but at the same time allows for
more efficient access to the paths vertices.

6. Greedy Hitting Set Computation
As explained above, the greedy approach is a natural strategy to solve the Hitting Set
problem approximately. Theoretically it yields solutions within a factor of 2 ln(n) of the
optimum in our setting. But in practice greedy performs much better than the theoretical,
a priori approximation guarantee implies.
For all but the simplest set system representation the application of the greedy Hitting
Set algorithm is not straightforward and requires some deliberate operations on the set
system/path representations as we will see in the following.
6.1 Complete Vertex Sets
If the paths are simply given as the set of contained vertices, a single scan over all these
sets can determine the best node hitting most paths. Another scan can remove the paths
645

fiFunke, Nusser, & Storandt

that have been hit by the selected node. These two scans can also be combined into one
by updating the counter values when removing the newly hit paths (an initial count is
still necessary). Unfortunately, the space consumption of this approach is enormous, also
making a single scan quite expensive.
6.2 CH-Paths
When representing the minimal B-violating paths as CH-paths, we could convert the single
paths into original paths by unpacking the shortcuts and then operate on the complete
vertex set of the path. Note that the paths would be processed one by one and only one
unpacked path would be kept in memory at a time. But there is a much better strategy
than just uncompressing every single CH-path to get to the original node sets: maintaining
a usage counter for each edge (counting how many shortest paths use that edge), we first
scan over all edges of all CH-paths in the set system to be hit, incrementing the respective
counters. Then we traverse all shortcut edges of the graph in decreasing order of their
construction in the CH-preprocessing, incrementing the counters of the spanned edges. The
node counters, maintained to identify the maximum node, can then be derived by a final
scan over all original (non-shortcut) edges. Keeping reverse information about which edges
are spanned by which shortcut also allows the identification of all sets that have been hit
by a node. If we update the edge counters when removing CH-paths from the set, picking
a node requires only one scan over the edges to push the counts down on the non-shortcut
edges, one scan over the usage counters and one scan over the set of CH-paths.
6.3 Peak Node Triples
When paths are described as triples of source, target, and peak node, we can get the CHpath representation as described in Section 5.5. Then we proceed as with the CH-path
representation. Note, that this CH-path representation is computed on demand for each
peak in every round to avoid keeping all paths in CH-representation permanently in memory.

7. Multi-stage Construction
For country-sized graphs, even the improved set system extraction methods and representations do not reduce the space and time consumption enough to be practical. Therefore
we introduce a procedure which interleaves the set extraction and the greedy Hitting Set
computation in a multi-stage algorithm. This multi-stage algorithm requires significantly
less space than the complete construction of the set system, and therefore can be applied
to considerably larger instances.
7.1 Nested Hitting Sets
For an instance of our ESC problem determined by the battery capacity B, we make the
following important observation: For every capacity B 0  B, a Hitting Set L0 for the
instance corresponding to B 0 is also feasible for the original instance (having enough BLSs
for a smaller battery capacity also suffices for a larger battery capacity). So, for example, if
B = 20kWh, solving the problem for e.g. B 0 = 5kWh would be feasible as well. While the
construction of L0 for some B 0  B might be considerably faster due to smaller exploration
646

fiPlacement of Loading Stations for Electric Vehicles

4

4
s
4
s

6

s

6

5

6

t

B 0 = 5, B  B 0 = 15

t

B 0 = 10, B  B 0 = 10

t

B 0 = 13, B  B 0 = 7

4
5

2
4

t B = 20

4

2
4

s

5

2

4
5

2
6

Figure 8: If the battery capacity is B = 20 kWh, the shortest path from s to t needs to
be hit by a loading station as it exhibits energy costs of 21. In the conventional
construction, the complete s-t-path would be part of the Hitting Set instance.
The three lower images illustrate what happens when we use a nested construction with different values of B 0 . The red+purple/dashed subpath indicates the
minimal B 0 -violating path starting at s. The brown square is a possible hitter for
this path. The purple/dashed+blue path indicates the minimal (B-B 0 )-violating
path starting at the brown square. This path is always a subpath of s-t no matter how B 0 is chosen. So hitting the blue+purple path (large dot in the image)
assures that all B-violating paths are hit as well. The nodes marked by brown
squares are not part of the final solution.

radii, L0 is typically also much larger than necessary for the instance defined by B. So
simply using the solution for B 0 = 5kWh although the real battery capacity is B = 20kWh,
we expect a result with many superfluous loading stations.
But there is another advantage of first quickly computing a Hitting Set for a small value
B 0 : It allows us to construct a new, smaller problem instance for which any feasible Hitting
Set L00 is also feasible for our original problem (defined by B)  and L00 is hopefully much
smaller than L0 . This second instance is defined by the set of paths originating in L0 that
are (B-B 0 )-violating.
We prove in Lemma 3 that the Hitting Set L00 for the second instance is indeed also a
Hitting Set for the original instance.
Lemma 3. Given a battery capacity B, and a second capacity B 0 < B. Let L0 be a feasible
ESC solution for B 0 , and L be a feasible Hitting Set for all minimal shortest (B-B 0 )-violating
paths originating in L0 , then L is a valid ESC solution for the battery capacity B.
Proof. Consider some B-violating s-t-path  in the original instance. Then  must be hit
by a node v  L0 less than B 0 away from s. The path v, . . . , t from this hitter to the
target (or a prefix thereof) has to be in the new constructed path set of (B-B 0 )-violating
paths originating in L0 . Therefore this subpath has a hitter in L. Hence L hits any Bviolating shortest path, which makes L a valid ESC solution for battery capacity B. Figure
8 illustrates the proof on a small example.
647

fiFunke, Nusser, & Storandt

7.2 Path Cover
For very small values of B, we can even compute Hitting Sets without any exploration and
evaluation of the  function purely based on the connectivity structure of the graph using
a so-called k-Hop Path Cover (Funke, Nusser, & Storandt, 2014a) which is a generalization
of a Vertex Cover. We construct a set of vertices C  V such that any directed (not
necessarily shortest) k-hop path in G contains at least one vertex from C (for k = 1 this is
simply a Vertex Cover). Then C is an ESC solution for B  where B  is the maximal energy
cost of a k-hop path, which can easily be upper bounded by k times the maximal energy
cost of an edge. For values k  48 this takes only a few minutes even on large graphs using
a variant of depth first search, making this step negligible for the overall running time.
7.3 Combination
In our implementation we combined nested Hitting Sets and k-Hop Path Covers to a multistage procedure, constructing a sequence of Hitting Sets Lr , Lr1 ,    , L1 = L for a sequence
of values Br < Br1 <    < B1 = B, finally returning L as the Hitting Set for the given
instance.
The first Br results from a k-Hop Cover with small value of k, for all subsequent solutions
we apply the nested Hitting Set approach (and choose Bi manually). There might be some
loss in terms of quality compared to the greedy algorithm on the full set system due to the
nested construction. Our experimental evaluation will show, though, that the loss in terms
of quality is not that pronounced, but the running times are drastically improved, and the
graph sizes which we can handle with this approach are much larger.

8. Refinements and Lower Bounds
In this section we introduce a speed-up strategy for the greedy algorithm which is independent of the employed set representation. Then we develop algorithms to construct
instance-based lower bounds for the ESC solution. These bounds will be helpful in our
experimental evaluation, as they prove  a posteriori, after running our algorithms  that
the computed solutions are in fact pretty close to the optima.
8.1 Multiple Hitters Heuristic
Even with the non-naive representations, there is considerable work involved when picking
the next best node in the greedy algorithm. So it might be worthwhile to add several
nodes to the Hitting Set in each round. Normally, we pick only the node which hits most
so far unhit sets, and refrain from picking other nodes in the same round as picking the
first node influences the hit counters of others. On the other hand, if we pick nodes that
do not interfere with each other, the quality of the solution should not decline too severely.
One way to achieve this is to generate the list of nodes sorted in ascending order of their
hit counters, always picking the first one and then going down the list selecting the next
nodes which have the shortest path distances of at least D to all nodes already picked.
Here D is appropriately chosen, e.g. an upper bound on the longest shortest path that is
not B-violating. Thereby we make sure no path in our set increased the hit counter of two
or more nodes picked in one round.
648

fiPlacement of Loading Stations for Electric Vehicles

Figure 9: Set of seven node-disjoint B-violating paths (highlighted in grey) in a small example graph. Every valid Hitting Set for all B-violating paths in the graph has
to contain at least seven vertices, as no vertex can hit more than one of the grey
marked paths. Therefore the size of a set of node-disjoint paths is a feasible lower
bound for the optimal Hitting Set size.

8.2 Simple Instance-Based Lower Bounds
To evaluate the quality of our heuristics, we would like to compare the outcome to the
optimal solution. But as the optimal value is typically unknown, we instead compare to a
good, but easily computable lower bound. In a study on Transit Nodes (Eisner & Funke,
2012) a rather involved lower bound was proposed, which takes an effort comparable to
solving the Hitting Set problem itself. We propose a much simpler alternative which suffices
for our purposes: As a by-product of the generation of the set system itself we can obtain
a set of node-disjoint B-violating paths. So no two paths in the set have a non-empty
intersection. Clearly, any feasible solution must contain an extra node per path in this set.
Hence the size of a set of such node-disjoint paths yields a valid lower bound, see Figure 9
for an illustration.
In case we do not generate the set system explicitly (because we use nested Hitting Sets),
we can greedily extract a set of node-disjoint paths by running Dijkstra computations from
random sources and adding B-violating paths to our set as long as they do not intersect
with previously selected ones. The size of the set provides a valid lower bound at any time.

9. Dealing with Real-world Settings
Throughout the paper, we made some assumptions about the ESC problem for sake of a
clean definition and easier algorithm descriptions. As these assumptions are not necessarily
met in practice, we explain in the following how to adapt our algorithms to still perform
well under real-world settings.
649

fiFunke, Nusser, & Storandt

9.1 Ambiguous Shortest Paths
In our exposition we always assume uniqueness of shortest paths. In this section we will
discuss necessary modifications in case shortest paths are ambiguous.
First of all, we can enforce uniqueness of shortest paths by symbolic perturbation. To
that end we define the cost of a path  = v0 v1 . . . vk not only as the sum c of its edge costs,
but by the vector (c , v0 , v1 , . . . , vk ). Two such cost vectors are compared lexicographically,
that is, if two s  t-paths 1 = sv1 . . . t and 2 = sw1 . . . t have the same aggregated edge
costs, and i is minimal with vi 6= wi , then 1 is considered shorter if vi < wi , otherwise
2 is considered shorter. This symbolic perturbation can easily be incorporated into e.g.
Dijkstras algorithm. During the computation of shortest paths from a node s we consider
as (possibly tentative) distance label of a node v from s not only the aggregated edge costs
ds (v) along the respective path but the tuple (ds (v), preds (v)), where preds (v) denotes the
predecessor on the current path from s to v. The ordering in case of identical ds (v) values is
determined by the node ID of the predecessor. Edge relaxations as well as the organization
of the priority queue is made according to these augmented distance labels. It is easy to
see that this yields the canonical and unique shortest paths as described above.
With edge lengths typically measured at a precision of around one meter it rarely happens that two paths exhibit exactly the same length. Under some circumstances, though,
it might be desirable to actually maintain multiple shortest paths between nodes (hitting
each of them / allowing to travel along each of them without running out of energy). Fortunately, we can adapt our algorithms to cater for all shortest paths. There is a minimal
change when backtracking after a Dijkstra exploration as well as in the PNM approach and
a slight change in the CH construction. For the former, when we retrieve paths/sets, instead
of following a predecessor reference of a node v (which was set during edge relaxation), we
inspect all adjacent nodes and check whether their distance labels with the respective edge
cost sums up to the distance label at v. This yields all neighbors of v that lie on a shortest
path from s to v. Recursing on these neighbors we obtain all shortest paths. In the CH
construction, the crucial operation is the contraction of a node v. In the original version,
for every pair of neighbors u, w of v with (u, v), (v, w)  E, a shortcut (u, w) is created
(with cost of the path uvw) if uvw is the only shortest path from u to w. To maintain all
shortest paths, we add the shortcut if uvw is a shortest path from u to w (but with possible
existence of other shortest paths). In this way every shortest path has a CH representation;
this comes at the cost of slightly more shortcuts added.
Our lower bound construction in Section 8.2 can be modified to also yield a lower bound
for the case with ambiguous shortest paths as follows: An s-t-pair can contribute to the
lower bound if all shortest paths between s and t require at least one recharging event. To
compute a valid lower bound we retrieve a maximal set of such vertex pairs, where for any
two vertex pairs in this set the respective shortest path node sets are not allowed to overlap.
This generalizes the idea of node-disjoint shortest paths in case of ambiguous shortest paths.
Our experiments showed, though, that considering all shortest paths does not yield
noticeably different results  mainly due to the rarity of reasonably long ambiguous shortest
paths. When we disregard ambiguities in our implementation, only for extremely small
battery capacities (corresponding to a cruising range of less than 2km), we found ambiguous
650

fiPlacement of Loading Stations for Electric Vehicles

shortest paths that were not covered by our BLS placement. For all larger battery capacities
our BLS placement in fact covered all (of the few) ambiguous shortest paths.
9.2 Restricted Loading Station Placement
We assume in our ESC problem definition that loading stations can be placed on every
node in the network. In practice, though, the set of possible locations might be restricted
due to technical, financial or legal reasons. If there is a set V 0  V of candidate nodes for
loading stations, we can incorporate this in our algorithms as follows: During construction
of the set system, we check for every shortest path whether at least one of its nodes is in
V 0 . Otherwise, we ignore the path completely (as it can never be hit anyway). For the
CH-based extraction methods, we can set a flag for every edge/shortcut indicating whether
the spanned path contains a node from V 0 or not. This allows to perform the check for a
CH-path without having to unpack it. In the actual Hitting Set computation we simply
skip over nodes that are not in V 0 to compute a feasible solution.
Note, that depending on the choice of V 0 the final Hitting Set might not allow to drive
on all shortest paths without running out of energy, though. To incorporate that some
locations are more suitable to become loading stations than others without losing global
reachability as demanded by our ESC problem formulation, we introduce a prize function
p : V  R+ . The higher the prize the more complicated or expensive it is to place a
charging station at this node. Then we can exploit the weighted Hitting Set problem as
basis for our computations. Here the goal is to find a set L of elements in
P the universe
which hit all sets in the set system while minimizing the accumulated prize lL p(l). Our
set system extraction methods remain unaffected by the prizes. Only in the greedy Hitting
Set computation step, the selection of the next best hitter changes. Previously, we selected
the node next that hits most so far unhit sets. Now, if S(v) denotes the set of so far unhit
sets that contain v, we select the node that minimizes the average prize per set p(v)/|S(v)|.
The approximation guarantee of the greedy algorithm for the weighted Hitting Set problem
is the same as for the unweighted Hitting Set problem (Chvatal, 1979). We expect the
solution to consist mainly of cheap charging stations and possibly some expensive ones that
are required to establish reachability between any two nodes.
9.3 Placement with Given Initial Loading Station Set
Another assumption made in the ESC problem definition is that we try to construct a
network of loading stations from scratch, i.e. starting with no loading stations at all. While
loading stations are still sparse in many areas, the ones that are already installed should
not be ignored. Existing loading stations can easily be taken into account when solving the
ESC problem: For a given initial set of loading stations L0 , we check during the extraction
of the set system for each path if it is already hit by L0 . If this is the case, the path can be
pruned. The remaining steps for the Hitting Set computation work just like before.

10. Experimental Evaluation
Our proposed techniques for computing ESC solutions were evaluated in a multi-threaded
implementation written in C++ and executed on 2nd generation Intel Core desktop hard651

fiFunke, Nusser, & Storandt

ware, an i7-3930 (6 cores, 64GB of RAM) for complete set generations and an i7-2700 (4
cores, 32GB RAM) for the multi-stage construction with nested Hitting Sets. We use the
following abbreviations to state results: K=103 , M=106 , s=seconds, m=minutes, h=hours,
d=days, GB=109 Bytes. We distinguish between CPU time (total CPU usage) and real time
(wall clock time). Several road networks of Germany derived from OpenStreetMap data
region
Pforzheim
Tubingen
Baden-Wurttemberg South
Southern Germany
Germany

abb.
(PF)
(TU)
(BW)
(SG)
(GE)

|V |
0.2M
0.5M
2.2M
4.2M
17.7M

|E|
0.4M
1.0M
4.6M
8.6M
36.0M

Table 1: Test graph characteristics.
(OSM, 2015) were used for evaluation, see Table 1 for an overview. As edge cost function
c we used travel time along an edge, so the paths to hit are indeed quickest paths (the
term shortest paths is conventionally used for subsuming all kinds of metrics). Energy consumption of an EV was modeled as explained in the introduction using distance data from
OSM and elevations provided by the Shuttle Radar Topography Mission (NASA, 2015). B
corresponds to a battery capacity which translates to a certain terrain dependent cruising
range. We use a capacity B for PF and TU that allows to drive 40 kilometers on average,
and about 125 kilometers for the larger graphs. Our , which models how much going uphill
increases the energy consumption, equals 4.
10.1 Dealing with Complete Set Systems
Construction and Representation. Let us first examine the time and space complexity of
extracting the complete set of minimal B-violating paths. We constructed set systems
using the naive strategy (NAIVE  representing each path as the complete sequence of
its vertices), the PHAST-based exploration (PHAST  with paths in CH representation),
and peak node mapping (PNM  representing each path as source,peak,target triple). The
respective results can be found in Table 2. Unfortunately, only the two smallest instances
were feasible to process using all strategies; already for the BW graph, the time and space
consumption of NAIVE exploded (extrapolated more than 500GB and more than 23 CPU
days). In comparison, PHAST is about a factor of 3 faster than NAIVE, and the space
consumption of CH-paths is an improvement by at least an order of magnitude. PNM can
construct the BW instance in 4.4 CPU hours, compared to the week needed by PHAST, and
the space consumption using triples decreases by another factor of 2 (note that for longer
paths the advantage of PNM vs. CH representation increases). But for SG and GE, also
PHAST and PNM took too much time and space (e.g. extrapolated 557GB/112days for
PHAST). So for larger networks, constructing complete set systems seems to be infeasible.

Comparison of Path Representations. As explained in Section 5.5, the path extraction
scheme does not tie us to a path representation. Instead, we can transform the extracted
652

fiPlacement of Loading Stations for Electric Vehicles

Graph

PF
TU
BW
Graph

PF
TU
BW

# paths

38M
168M
2715M

space consumption
NAIVE
PHAST
PNM
vertex sets CH-paths
triples
5.2GB
0.2GB
0.1GB
24.0GB
2.1GB
0.9GB
[526.3GB]
34.6GB 14.3GB

computation time
NAIVE
PHAST
PNM
CPU
real
CPU
real
CPU
real
1.5h
0.3h 26.7m 5.0m
1.8m 25.1m
24.6h
4.1h
7.5h
1.4h 27.3m
5.4m
[23.1d] [3.9d]
7.5d 33.1h
4.4h 47.0m

Table 2: Comparison of path extraction/representation schemes. B corresponds to about
40km (PF and TU) or 125km (BW) cruising range on flat terrain. Timings include
the CH-construction for PHAST/PNM. Values in brackets are extrapolated.

paths to any of the introduced representations before storing them. Each representation
provides some trade-off between space consumption and access times for single paths. Figure
10 illustrates these values for a small and a large benchmark instance (TU and GER).
Note, that the access times for paths represented as triples are amortized. If we really
want to extract a single path only, the costs are comparable to the ones for the (s, t)
representation. But in the greedy algorithm, we require access to huge sets of paths in
every round, so the (s, p, t) representation pays off. For the CH representation, the access
times are reported in the figure purely for completeness. They are not significant for the
greedy Hitting Set computation, though, as our specialized greedy algorithm on CH paths
does not require to unpack those paths. In fact, the access times relevant for the greedy
algorithm are even below the ones for the vertex set representation, as the CH representation
contains far less elements we have to sweep over. Hence, we regard CH-paths as the best
path representation as soon as the set system fits in memory in this representation.
The transformation times between two path representations can be estimated from the
results reported in Figure 10 as well. Every transformation that runs in constant or linear
time according to our analysis has cost less or comparable to accessing paths in vertex
set representation. The transformation time for CH-paths to vertex sets or vice versa
corresponds to the access time for paths in CH representation. And to transform triples
into CH-paths, we need the time to access a path in triple form minus the time to unpack
a CH-path.
Hitting Set Computation. We evaluated both the standard greedy algorithm as well as the
multiple hitters (MH) variation on the set systems for PF, TU, and BW with varying choices
for B. Figure 11 shows their performance in terms of quality (standard greedy vs. MH) as
well as running time (how much faster MH is compared to standard greedy). The ratios are
averaged over all test graphs; the bound B is chosen between almost zero and 60 percent of
653

fiFunke, Nusser, & Storandt

Figure 10: Comparison of several path representation schemes in terms of space consumption and access time. Both axes are in log scale.

the maximum energy consumption of some shortest path in the respective network (in fact
for very long paths, the set systems got so simple that greedy even constructed the optimum,
hence the approximation ratio of 1 in Figure 11). In all cases, greedy produces results much
closer to the optimum as the theoretical 2 ln n guarantee, the maximum deviation from the
lower bound was indeed less than 4.5. Employing the MH strategy increases the HS size
slightly, but yields significantly decreased running times especially for smaller bounds B
(where more hitters have to be chosen). Still, compared to the construction time of the
set systems, the Hitting Set computation times were negligible, so we do not state them
explicitly here. This will change when we employ the multi-stage construction, though.
10.2 Multi-stage Construction
As the construction of the complete set system has proven to be infeasible for larger road
networks, we will make use of the idea of a multi-stage construction.
k-Hop Cover+PNM. Let us first examine how a compact set system can be constructed
using the PNM approach after an initial k-Hop Path Cover. For the BW network we
computed a k = 32-Hop Cover C (146, 494 nodes) which corresponds to an ESC solution
with B 0 = 8832 (and cruising range of about 9km in flat terrain). Then PNM is used to
create a final compact set system by only considering (B  B 0 )-violating paths that start
at nodes in C. Not surprisingly, the number of paths to be hit reduces drastically from
2715M in Table 2 to 24M in Table 3. The running times are still quite high, though, as
this approach does not save the exploration from each peak (therefore more stages will not
654

fiPlacement of Loading Stations for Electric Vehicles

40
35
runtime ratio

6
quality greedy
quality MH
runtime greedy/runtime MH

5

30
4

25
20

3

15
10

2

approximation factor

45

5
0

1
10

20
30
40
percentage of max B

50

60

Figure 11: Performance of the greedy algorithm and the multiple hitters variant (MH)
averaged over PF, TU and BW.
Graph
BW
SG
GE

|C|
146,494
180,455
769,760

B0
8832
10048
15808

CPU
2.2h
6.0h
64.8h

real
35.5m
2.8h
27.6h

# paths
24M
75M
1085M

Table 3: Instance creation (B = 40K) via PNM with initial k-hop solution C for k = 32.
help much here). Since further improvements in terms of running time using PNM in a
multi-stage approach cannot be expected, let us now concentrate on the naive approach
with the extracted paths being converted into their CH-representation.
Multi-Stage Hitting Sets. We employ the following strategy: first, construct a k-Hop Cover
C with e.g. k = 32 which yields an initial Hitting Set Lr for some bound Br . Then we
construct a reduced set system consisting of all (Br1  Br )-violating paths starting at
nodes from Lr only and compute a Hitting Set Lr1 for that set system. Here, we use naive
Graph
TU
SG
TU
SG
GE
GE
GE

Br


1.8k
4.2k
15.8k
6.2k
15.8k

Bi s of nested HS
1k,5k,40k
2k,10k,125k
4.8k,40k
12.2k,125k
17.8k,33.8k,125k
8.2k,24.2k,125k
17.8k,25.8k,49.8k,125k

|L|
120
106
116
110
868
728
1212

LB
33
33
33
33
190
190
190

APX
3.64
3.21
3.52
3.33
4.57
3.83
6.38

CPU
9m
404m
8m
242m
908m
1156m
645m

real
3m
106m
2m
63m
265m
322m
209m

Table 4: Multi-stage Hitting Set computation (LB = lower bound, APX = approximation
factor). The last two experiments can be seen in detail in Table 5.

655

fiFunke, Nusser, & Storandt

i
4
3
2
1
P

Bi
6.2k
8.2k
24.2k
125k

i
5
4
3
2
1
P

Bi
15.8k
17.8k
25.8k
49.8k
125k

tSS

363m
249m
467m
1079m
tSS

203m
101m
81m
146m
531m

#paths

34.6M
36.8M
13.5M

#paths

19.7M
17.1M
9.2M
5.6M


tHS
14s
25m
21m
31m
77m
tHS
36s
25m
21m
17m
50m
113m

|Li |
1388k
223k
16k
728

|Li |
770k
208k
38.6k
8445
1212


CPU
14s
388m
270m
498m
1156m
CPU
36s
228m
122m
98m
196m
645m

Table 5: Statistics for a 4-stage run starting with a k = 16-Hop Cover (above), and a 5stage construction initialized with a k = 32-Hop Cover (below) on GE. The column
#paths gives the number of sets to hit in the respective stage. tSS and tHS
denote CPU time for the set system construction and the Hitting Set computation
respectively.

extraction but transform vertex sets to CH-paths for more efficient storage. We proceed
iteratively until reaching B1 = B and the final Hitting Set L1 = L. It is intuitive to demand
that the gap between B2 and B1 should be large to make sure that the last Hitting Set
instance still faithfully characterizes the original Hitting Set instance. Table 4 shows the
results for various choices of multi-stage parameters. In Table 5 we give a more detailed
account on the intermediate calculations for the large GE graph. The experiments confirm
that the larger the gap between B2 and B1 is, the better the quality of the final Hitting
Set. This comes at the cost of an expensive last stage, though. In contrast to the other
experiments, the first two calculations in Table 4 have been conducted without an initial
k-Hop Cover. The results obtained on TU and SG suggest that an initial k-Hop Cover
accelerates the calculation while maintaining a similar Hitting Set size. Furthermore, all
the APX values remain low even though the lower bounds were obtained in a naive way.
Note that for example for the graph of Germany, the a priori approximation guarantee of
the plain greedy algorithm (which is not feasible due to excessive running time and space
consumption) is 2 ln n  19.5. This proves the excellent quality of the Hitting Sets for the
particular instances. Table 5 shows that introducing multiple stages keeps the intermediate
set systems rather compact, so efficient computation is actually possible.

11. Conclusions and Future Work
We showed how to model and solve a natural and important facility location problem in
the E-mobility context, taking a radically different approach than previous ones avoiding
detours to loading stations for EVs.
656

fiPlacement of Loading Stations for Electric Vehicles

While a naive strategy only allows for the solution of small instances of few hundred
thousand nodes, our compact representation schemes for the underlying set systems and
heuristic modifications of the standard greedy approach make the computation of a solution
even for country-sized networks like that of Germany possible. Instance-based lower bounds
certify that the solution quality is close to optimal (within a factor of 4) and far from the
pessimistic theoretically achievable approximation bound. In fact it is remarkable that after
all, it was possible to compute a 4-approximate solution to a seemingly intractable Hitting
Set problem within a few hours on a quadcore desktop PC. Our computation determined
around 800 locations where placing BLSs would establish complete coverage for Germany.
Our framework does not require the metric that decides which shortest paths have to
be hit to be identical with the metric that determines which paths are shortest. In fact this
was factored out using our  function, which  depending on the application scenario  can
also be used to implement other hitting criteria (e.g. hop distances or risk values).
In future work we intend to examine how the exact hitting requirement can be relaxed.
Naturally, it is not necessary that there is always a BLS right on the respective shortest
path, but a nearby one suffices. This could be modeled by enlarging the vertex sets of
the respective shortest paths by surrounding vertices. Hitting Set sizes for this variant are
expected to be considerably smaller than for hitting all shortest paths directly. Another
direction of research is to take into account capacity constraints of the BLSs (Lam et al.,
2013); in particular in urban areas it is certainly necessary to provide recharging stations
for a very large number of vehicles.

References
Abraham, I., Delling, D., Goldberg, A. V., & Werneck, R. F. (2012). Hierarchical hub
labelings for shortest paths. In European Symposium on Algorithms (ESA), pp. 24
35. Springer.
Artmeier, A., Haselmayr, J., Leucker, M., & Sachenbacher, M. (2010). The shortest path
problem revisited: Optimal routing for electric vehicles. In German Conference on
Artificial Intelligence (KI), pp. 309316.
Arz, J., Luxen, D., & Sanders, P. (2013). Transit Node routing reconsidered. In International
Symposium on Experimental algorithms (SEA), pp. 5566. Springer.
Bast, H., Funke, S., & Matijevic, D. (2009). Ultrafast shortest-path queries via Transit
Nodes. In The shortest path problem : 9th DIMACS implemenation challenge, Vol. 74
of DIMACS Series on Discrete Mathematics and Theoretical Computer Science, pp.
175192, Providence, RI. AMS.
Berg, M. d., Cheong, O., Kreveld, M. v., & Overmars, M. (2008). Computational Geometry:
Algorithms and Applications (3rd ed. edition). Springer-Verlag TELOS, Santa Clara,
CA, USA.
Chvatal, V. (1979). A greedy heuristic for the set-covering problem. Math. of Oper. Res.,
4 (3), 233235.
657

fiFunke, Nusser, & Storandt

Delling, D., Goldberg, A. V., Nowatzyk, A., & Werneck, R. F. F. (2011). PHAST: Hardwareaccelerated shortest path trees. In International Parallel and Distributed Processing
Symposium (IPDPS), pp. 921931.
Dinur, I., & Safra, S. (2004). On the hardness of approximating minimum Vertex Cover.
Annals of Mathematics, 162, 2005.
Eisner, J., & Funke, S. (2012). Transit Nodes - lower bounds and refined construction. In
Algorithm Engineering and Experiments (ALENEX).
Funke, S., Nusser, A., & Storandt, S. (2014a). On k-Path Covers and their applications. In
International Conference on Very Large Databases (VLDB).
Funke, S., Nusser, A., & Storandt, S. (2014b). Placement of loading stations for electric
vehicles: No detours necessary!. In AAAI Conference on Artificial Intelligence.
Geisberger, R., Sanders, P., Schultes, D., & Delling, D. (2008). Contraction Hierarchies:
faster and simpler hierarchical routing in road networks. In International Workshop
on Experimental Algorithms (WEA), pp. 319333. Springer.
Lam, A., Leung, Y.-W., & Chu, X. (2013). Electric vehicle charging station placement.
In International Conference on Smart Grid Communications (SmartGridComm), pp.
510515.
NASA (2015). Shuttle Radar Topography Mission. Online. http://www2.jpl.nasa.gov/srtm.
OSM (2015). The OpenStreetMap Project. Online. http://www.openstreetmap.org.
Storandt, S., & Funke, S. (2013). Enabling E-Mobility: Facility location for battery loading
stations. In Conference on Artificial Intelligence (AAAI).

658

fiJournal of Artificial Intelligence Research 53 (2015) 1-40

Submitted 08/14; published 05/15

Coactive Learning
Pannaga Shivaswamy

pshivaswamy@linkedin.com

LinkedIn Corporation
2029 Stierlin Ct
Mountain View, CA 94043, USA

Thorsten Joachims

tj@cs.cornell.edu

Department of Computer Science
Cornell University
Ithaca, NY 14853, USA

Abstract
We propose Coactive Learning as a model of interaction between a learning system and
a human user, where both have the common goal of providing results of maximum utility
to the user. Interactions in the Coactive Learning model take the following form: at each
step, the system (e.g. search engine) receives a context (e.g. query) and predicts an object
(e.g. ranking); the user responds by correcting the system if necessary, providing a slightly
improved  but not necessarily optimal  object as feedback. We argue that such preference
feedback can be inferred in large quantity from observable user behavior (e.g., clicks in
web search), unlike the optimal feedback required in the expert model or the cardinal
valuations required for bandit learning. Despite the relaxed requirements for the feedback,
we show that it is possible to adapt many existing online learning algorithms
to the coactive

framework. In particular, we provide algorithms that achieve O(1/ T ) average regret in
terms of cardinal utility, even though the learning algorithm never observes cardinal utility
values directly. We also provide an algorithm with O(log(T )/T ) average regret in the
case of -strongly convex loss functions. An extensive empirical study demonstrates the
applicability of our model and algorithms on a movie recommendation task, as well as
ranking for web search.

1. Introduction
In a wide range of systems in use today, the interaction between human and system takes
the following form. The user issues a command (e.g. query) and receives a  possibly
structured  result in response (e.g. ranking). The user then interacts with the results (e.g.
clicks), thereby providing implicit feedback about the users utility function. Here are three
examples of such systems and their typical interaction patterns:
Web Search: In response to a query, a search engine presents the ranking [A, B, C, D, ...]
and observes that the user clicks on documents B and D.
Movie Recommendation: An online service recommends movie A to a user. However,
the user rents movie B after browsing the collection.
Machine Translation: An online machine translator is used to translate a wiki page from
language A to B. The system observes some corrections the user makes to the translated text.
c
2015
AI Access Foundation. All rights reserved.

fiShivaswamy & Joachims

In all the above examples, the user provides some feedback about the results of the
system. However, the feedback is only an incremental improvement, not necessarily the
optimal result. For example, from the clicks on the web search results we can infer that the
user would have preferred the ranking [B, D, A, C, ...] over the one we presented. However,
this is unlikely to be the best possible ranking. Similarly in the recommendation example,
movie B was preferred over movie A, but there may have been even better movies that
the user did not find while browsing. And in the machine translation example, the corrected text need not be the best possible translation from language A to language B. In all
three examples, the algorithm typically receives a slightly improved result from the user as
feedback, but not necessarily the optimal prediction or any cardinal utilities. We conjecture
that many other applications fall into this schema, ranging from news filtering to personal
robotics.
In this paper, we propose Coactive Learning as a model of such system-user interactions.
We formalize Coactive Learning as a general model of interaction between a learning system
and its user, define a suitable notion of regret, and validate the key modeling assumption
 namely whether observable user behavior can provide valid feedback in our model  in a
user study for web search. The new model can be viewed as a cooperative learning process
between system and user, where both parties aim to optimize utility but lack the means to
achieve this goal on their own. Specifically, the (boundedly rational) user is computationally
limited in maximizing utility over the space of alternatives, while the system is limited in
how well it knows the users utility function.
The proposed online learning framework differs significantly from existing online learning
models in terms of the observed feedback (see the related works section for a comparison). A
strength of the proposed framework is that it is possible to derive a wide range of coactive
learning algorithms by adapting existing online algorithms for convex optimization. We
provide a template for Coactive Learning algorithms and then show several instances of
this template in this paper, and in each case, we prove that the worst case analysis of
the algorithm carries over from the conventional online learning framework to coactive
learning despite the differences between the two models.In particular, in the cases of linear
utility models and convex cost functions we show O(1/ T ) regret bounds with a matching
lower bound. We also show that the regret bound can be improved with a second order
algorithm for strongly convex functions. The learning algorithms perform structured output
prediction (see Bakir, Hofmann, Scholkopf, Smola, Taskar, & Vishwanathan, 2007) and thus
can be applied in a wide variety of problems. We study several interesting extensions of
the framework using batch updates, expected feedback, and an exponentiated learning
algorithm. Finally, we provide extensive empirical evaluations of our algorithms on a movie
recommendation and a web search task, showing that the algorithms are highly efficient
and effective in practical settings.
The rest of this paper is organized as follows. We discuss related work in Section 2. In
Section 3 we formally introduce the coactive learning model and also motivate the model
with a real-world user study. We present the linear version of our algorithm along with
several extensions in Section 4. In Section 5, we then detail a general schema for deriving
coactive learning algorithms and their regret bounds. In particular, we derive an exponentiated gradient algorithm in Section 5.1, and we propose coactive learning algorithms for
minimizing general convex losses and -strongly convex losses in Sections 5.2 and 5.3. An
2

fiCoactive Learning

empirical evaluation of the proposed framework and algorithms is done in Section 6 and we
conclude in Section 7. We include most of our proofs in the Appendix.

2. Related Works
The Coactive Learning Model bridges the gap between two forms of feedback that have been
well studied in online learning. On one side there is the multi-armed bandit model (e.g.,
Auer, Cesa-Bianchi, Freund, & Schapire, 2002b; Auer, Cesa-Bianchi, & Fischer, 2002a),
where an algorithm chooses an action and observes the utility of (only) that action. On
the other side, utilities of all possible actions are revealed in the case of learning with
expert advice (e.g., Cesa-Bianchi & Lugosi, 2006a). Online convex optimization (Zinkevich,
2003; Hazan, Agarwal, & Kale, 2007) and online convex optimization in the bandit setting
(Flaxman, Kalai, & McMahan, 2005) are continuous relaxations of the expert and the
bandit problems respectively. Our model, where information about two arms is revealed
at each iteration (the one we presented and the one we receive as feedback from the user),
sits between the expert and the bandit setting. Most closely related to Coactive Learning
is the dueling bandits setting (Yue, Broder, Kleinberg, & Joachims, 2009; Yue & Joachims,
2009). The key difference is that both arms are chosen by the algorithm in the dueling
bandits setting, whereas one of the arms is chosen by the user in the Coactive Learning
setting. Our model allows contextual information like in contextual bandits (Langford &
Zhang, 2007), however, the arms in our problem are structured objects such as rankings.
A summary of how our framework compares with other existing frameworks is shown in
Table 1. Other types of feedback have also been explored in the literature. For example, in
the multi-class classification problems, after the algorithm makes a prediction based on the
context, the feedback received is only whether the prediction is correct or wrong as opposed
to the actual label (Crammer & Gentile, 2011; Kakade, Shalev-Shwartz, & Tewari, 2008).
This can be seen as observing partial feedback (as opposed to the actual cardinal feedback)
in a bandit problem.
As pointed out above, Coactive Learning algorithms and conventional online learning
algorithms operate in different types of environments. Coactive Learning algorithms present
an object and observe another object as a feedback, while online convex learning algorithms
pick a vector in each step and observe the gradient at that vector as feedback. Despite the
contrast between online learning and Coactive Learning, two of the algorithms presented
in this paper are closely related to those in the work of Zinkevich (2003) and Hazan et al.
(2007). We show that it is possible to adapt the regret bounds of these algorithms to
corresponding regrets bounds for Coactive Learning. At the heart of all our algorithms and
analysis is the well-known idea (Polyak & Tsypkin, 1973) that the descent algorithms do
not necessarily need to know the gradients, but that a vector with positive inner product
with the gradient in expectation suffices.
While feedback in Coactive Learning takes the form of a preference, it is different from
ordinal regression and ranking. Ordinal regression (e.g., Crammer & Singer, 2001) assumes
training examples (x, y), where y is a rank. In the Coactive Learning model, absolute ranks
are never revealed. More closely related is learning with pairs of examples (Herbrich, Graepel, & Obermayer, 2000; Freund, Iyer, Schapire, & Singer, 2003; Chu & Ghahramani, 2005),
since it circumvents the need for absolute ranks; only relative orderings are required. Vari3

fiShivaswamy & Joachims

Framework
Bandits
Experts
Dueling Bandits
Coactive Learning

Algorithm
pull an arm
pull an arm
pull two arms
pull an arm

Feedback
observe cardinal reward for the arm pulled
observe cardinal rewards for all the arms
observe feedback on which one is better
observe another arm which is better

Table 1: A comparison of different online learning frameworks.

ants of such pairwise ranking algorithms have been applied to Natural Language Processing
(Haddow, Arun, & Koehn, 2011; Zhang, Lei, Barzilay, Jaakkola, & Globerson, 2014) and
image annotation (Weston, Bengio, & Usunier, 2011). However, existing approaches require
an iid assumption and typically perform batch learning. Finally, there is a large body of
work on ranking (see Liu, 2009). These approaches are different from Coactive Learning as
they require training data (x, y) where y is the optimal ranking for query x. However, we
will draw upon structured prediction approaches for ranking problems in the design of our
models.
Coactive learning was first proposed by Shivaswamy and Joachims (2012); this paper
serves as a journal extension of that paper, adding a complete discussion of batch updates
and expected feedback, the exponentiated gradient algorithm, the O(log(T )/T ) algorithm
for -strongly convex loss functions, and a substantially extended empirical evaluation.
Since then, coactive learning has been applied to intrinsically diverse retrieval (Raman,
Shivaswamy, & Joachims, 2012), learning ranking function from click feedback (Raman,
Joachims, Shivaswamy, & Schnabel, 2013), optimizing social welfare (Raman & Joachims,
2013), personal robotics (Jain, Wojcik, Joachims, & Saxena, 2013), pattern discovery (Boley,
Mampaey, Kang, Tokmakov, & Wrobel, 2013), robotic monitoring (Somers & Hollinger,
2014), and extended to allow approximate inference (Goetschalckx, Fern, & Tadepalli, 2014).

3. Coactive Learning Model
We now introduce coactive learning as a model of interaction (in rounds) between a learning
system (e.g. search engine) and a human (search user) where both the human and learning
algorithm have the same goal (of obtaining good results). At each round t, the learning
algorithm observes a context xt  X (e.g. a search query) and presents a structured object
yt  Y (e.g. a ranked list of URLs). The utility of yt  Y to the user for context xt  X is
described by a utility function U (xt , yt ), which is unknown to the learning algorithm. As
feedback the human user returns an improved object yt  Y (e.g. reordered list of URLs),
i.e.,1
U (xt , yt ) > U (xt , yt ),

(1)

when such an object yt exists. In fact, we will also allow violations of (1) when we formally
model user feedback in Section 3.1.
The process by which the user generates the feedback yt can be understood as an
approximately utility-maximizing action, where the user is modeled as a boundedly rational
1. The improvements should be not just strict, but by a margin, this will be clear in Section 3.1.

4

fiCoactive Learning

agent. In particular, the user selects the feedback object yt by approximately maximizing
utility over a user-defined subset Yt of all possible Y.
yt = argmaxyY U (xt , y)

(2)

This approximately and boundedly rational user may employ various tools (e.g., query
reformulations, browsing) to construct the subset Y and to perform this search. Importantly,
however, the feedback yt is typically not the optimal label which is defined as,
yt := argmaxyY U (xt , y).

(3)

In this way, Coactive Learning covers settings where the user cannot manually optimize
the argmax over the full Y (e.g. produce the best possible ranking in web search), or has
difficulty expressing a bandit-style cardinal rating U (xt , yt ) for yt in a consistent manner.
This puts our preference feedback yt in stark contrast to supervised learning approaches
which require (xt , yt ). But even more importantly, our model implies that reliable preference feedback (1) can be derived from observable user behavior (i.e., clicks), as we will
demonstrate in Section 3.2 for web search. We conjecture that similar feedback strategies
exist in many application settings (e.g., Jain et al., 2013; Boley et al., 2013; Somers &
Hollinger, 2014; Goetschalckx et al., 2014), especially when users can be assumed to act
approximately and boundedly rational according to U .
Despite the weak preference feedback, the aim of the algorithm is nevertheless to present
objects with utility close to that of the optimal yt . Whenever, the algorithm presents an
object yt under context xt , we say that it suffers a regret U (xt , yt )  U (xt , yt ) at time
step t. Formally, we consider the average regret suffered by the algorithm over T steps as
follows:
REGT =

T
1X
(U (xt , yt )  U (xt , yt )) .
T

(4)

t=1

The goal of the learning algorithm is to minimize REGT , thereby providing the human with
predictions yt of high utility. Note, however, that a cardinal value of U is never observed
by the learning algorithm, but U is only revealed ordinally through preferences (1).
3.1 Quantifying Preference Feedback Quality
To provide any theoretical guarantees about the regret of a learning algorithm in the coactive
setting, we need to quantify the quality of the user feedback. Note that this quantification is
a tool for theoretical analysis, not a prerequisite or parameter to the algorithm. We quantify
feedback quality by how much improvement y provides in utility space. In the simplest case,
we say that user feedback is strictly -informative when the following inequality is satisfied:
U (xt , yt )  U (xt , yt )  (U (xt , yt )  U (xt , yt )).

(5)

In the above inequality,   (0, 1] is an unknown parameter. Feedback is such that utility of
yt is higher than that of yt by a fraction  of the maximum possible utility range U (xt , yt )
U (xt , yt ). The term on the right hand side in the above inequality ensures that user feedback
5

fiShivaswamy & Joachims

yt is not only better than yt , but also better by a margin (U (xt , yt )U (xt , yt )). Violations
of the above feedback model are allowed by introducing slack variables t :2
U (xt , yt )  U (xt , yt ) = (U (xt , yt )  U (xt , yt ))  t .

(6)

Note that the t are not restricted to be positive, but can be negative as well. We refer to
the above feedback model as -informative feedback. Note also that it is possible to express
feedback of any quality using (6) with an appropriate value of t . Our regret bounds will
contain t , quantifying to what extent the -informative modeling assumption is violated.
Finally, we will also consider an even weaker feedback model where a positive utility
gain is only achieved in expectation over user actions:
Et [U (xt , yt )  U (xt , yt )] = (U (xt , yt )  U (xt , yt ))  t .

(7)

We refer to the above feedback as expected -informative feedback. In the above equation,
the expectation is over the users choice of yt given yt under context xt (i.e., under a
distribution Pxt [yt |yt ] which is dependent on xt ).
In the rest of this paper, we use a linear model for the utility function,
U (x, y) = w> (x, y),

(8)

where w  RN is a parameter vector unknown both to the learning system and users and
 : X  Y  RN is a known joint feature map known to the system such that3
k(x, y)k`2  R,

(9)

for any x  X and y  Y. Note that both x and y can be structured objects.
3.2 User Study: Preferences from Clicks
We now validate that reliable preferences as specified in Equation (1) can indeed be inferred
from implicit user behavior. In particular, we focus on preference feedback from clicks in
web-search and draw upon data from a user study (Joachims, Granka, Pan, Hembrooke,
Radlinski, & Gay, 2007). In this study, subjects (undergraduate students, n = 16) were
asked to answer 10 identical questions  5 informational, 5 navigational  using the Google
search engine. All queries, result lists, and clicks were recorded. For each subject, queries
were grouped into query chains by question4 . On average, each query chain contained 2.2
queries and 1.8 clicks in the result lists.
We use the following strategy to infer a ranking y from the users clicks: prepend to the
ranking y from the first query of the chain all results that the user clicked throughout the
whole query chain. To assess whether U (x, y) is indeed larger than U (x, y) as assumed in
our learning model, we measure utility in terms of a standard
measure of retrieval quality
P10 r(x,y[i])
from Information Retrieval. We use DCG@10(x, y) = i=1 log i+1 , where r(x, y[i]) is the
relevance score of the i-th document in ranking y (see Manning, Raghavan, & Schutze,
2. Strictly speaking, the value of the slack variable depends on the choice of  and the definition of utility.
However, for brevity, we do not explicitly show this dependence in the notation.
3. We make a slightly different assumption in Section 5.1.
4. This was done manually, but can be automated with high accuracy (Jones & Klinkner, 2008).

6

fiCumulative Distribution Function

Coactive Learning

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

Normal Condition
Swapped Condition
Reversed Condition
All Conditions
-5

-4

-3

-2

-1
0
1
DCG(x,ybar)-DCG(x,y)

2

3

4

5

Figure 1: Cumulative distribution of utility differences between presented ranking y and
click-feedback ranking y in terms of DCG@10 for three experimental conditions
and overall.
2008). To get ground-truth relevance assessments r(x, d), five independent human assessors
(students) were asked to manually rank the set of results encountered during each query
chain. The assessors were also given the true answers for navigational queries. We then
linearly normalize the resulting ranks to a relative relevance score r(x, d)  [0..5] for each
document.
We can now evaluate whether the feedback ranking y is indeed better than the ranking
y that was originally presented, i.e., DCG@10(x, y) > DCG@10(x, y). Figure 1 plots the
Cumulative Distribution functions (CDFs) of DCG@10(x, y)  DCG@10(x, y) for three
experimental conditions, as well as the average over all conditions. All CDFs are shifted far
to the right of 0, showing that preference feedback from our strategy is highly accurate and
informative. Focusing first on the average over all conditions, the utility difference is strictly
positive on  60% of all queries, and strictly negative on only  10%. This imbalance is
significant (binomial sign test, p < 0.0001). Among the remaining  30% of cases where
the DCG@10 difference is zero, 88% are due to y = y (i.e. click only on top 1 or no click).
Note that a learning algorithm can easily detect those cases and may explicitly eliminate
them as feedback. Overall, this shows that implicit feedback can indeed produce accurate
preferences.
What remains to be shown is whether the reliability of the feedback is affected by the
quality of the current prediction, i.e., U (xt , yt ). In the user study, some users actually
received results for which retrieval quality was degraded on purpose. In particular, about
one third of the subjects received Googles top 10 results in reverse order (condition reversed) and another third received rankings with the top two positions swapped (condition
swapped). As Figure 1 shows, we find that users provide accurate preferences across this
substantial range of retrieval quality. Intuitively, a worse retrieval system may make it
harder to find good results, but it also makes an easier baseline yt to improve upon. This
intuition is formally captured in our definition of -informative feedback. The optimal value
of the  vs.  trade-off, however, will likely depend on many application-specific factors,
like user motivation, corpus properties, and query difficulty. In the following, we therefore
present algorithms that do not require knowledge of , theoretical bounds that hold for any
value of , and experiments that explore a large range of .
7

fiShivaswamy & Joachims

Algorithm 1 Preference Perceptron.
Initialize w1  0
for t = 1 to T do
Observe xt
Present yt  argmaxyY wt> (xt , y)
Obtain feedback yt
Update: wt+1  wt + (xt , yt )  (xt , yt )
end for

4. The Preference Perceptron for Coactive Learning
In this section, we start by presenting and analyzing the most basic algorithm for the coactive learning model, which we call the Preference Perceptron (Algorithm 1). The Preference
Perceptron maintains a weight vector wt which is initialized to 0. At each time step t, the
algorithm observes the context xt and presents an object y that maximizes wt> (xt , y).
The algorithm then observes user feedback yt and the weight vector wt is updated in the
direction (xt , yt )  (xt , yt ).
Although the update of the preference perceptron appears similar to the standard perceptron for multi-class classification problems, there are key differences. First, the standard
perceptron algorithm requires the true label y as feedback, whereas much weaker feedback
y suffices for our algorithm. Second, the standard analysis of the perceptron bounds the
number of mistakes made by the algorithm based on margin and the radius of the examples.
In contrast, our analysis bounds a different regret that captures a graded notion of utility.
Further, the standard perceptron mistake bound (Novikoff, 1962) contains R2 kwk2 while
our bound in the following Theorem contains Rkwk where R is as defined in (9).
Theorem 1 The average regret of the preference perceptron algorithm can be upper bounded,
for any   (0, 1] and for any w as follows:
T

1 X
2Rkw k
 .
REGT 
t +
T
 T
t=1

(10)

Proof First, consider kwT +1 k2 , we have,
wT>+1 wT +1 = wT> wT + 2wT> ((xT , yT )  (xT , yT ))
+ ((xT , yT )  (xT , yT ))> ((xT , yT )  (xT , yT )
 wT> wT + 4R2  4R2 T.
On line one, we simply used our update rule from algorithm 1. On line two, we used the
fact that wT> ((xT , yT )  (xT , yT ))  0 from the choice of yT in Algorithm 1 and that
k(x, y)k  R. Further, from the update rule in algorithm 1, we have,
wT>+1 w = wT> w + ((xT , yT )  (xT , yT ))> w
=

T
X

(U (xt , yt )  U (xt , yt )) .

t=1

8

(11)

fiCoactive Learning

We now use the fact that wT>+1 w  kw kkwT +1 k (Cauchy-Schwarz inequality), which
implies
T
X

(U (xt , yt )  U (xt , yt ))  2R T kw k.
t=1

From the -informative modeling of the user feedback in (6), we have


T
X

(U (xt , yt )  U (xt , yt )) 

t=1

T
X


t  2R T kw k,

t=1

from which the claimed result follows.
The first term in the regret bound denotes the quality of feedback in terms of violation
of the -informative feedback. In particular, if the user feedback is strictly
-informative

for some , then all slack variables in (10) vanish and REGT = O(1/ T ).
It is trivial to design algorithms (with even better regret) under strict -informative
assumption when the cardinality of the context set X is finite. One of the interesting aspects
of the above bound (Theorem 1) and the subsequent results is that we can minimize the
regret even when the context xt is different in every step. Thus, |X | could be infinite and
the regret bound still holds.
We note that the bound in Theorem 1 holds for any w and   (0, 1]. The slacks have
to be based on the corresponding  and w .
Though user feedback is modeled via -informative feedback, the algorithm itself does
not require knowledge of ;  plays a role only in the analysis.
So far, we have characterized user behavior in terms of deterministic feedback actions.
However, if a bound on the expected regret suffices, the weaker model of Expected Informative Feedback from Equation (7) is applicable.
Corollary 2 Under the expected -informative feedback model, the expected regret (over
user behavior distribution) of the preference perceptron algorithm can be upper bounded as
follows:
T

E[REGT ] 

1 X  2Rkw k
 .
t +
T

T
t=1

(12)

The above corollary can be proved by following the argument of Theorem 1, but taking
expectations over user feedback:
E[wT>+1 wT +1 ] = E[wT> wT ] + E[2wT> ((xT , yT )  (xT , yT ))]
+ ET [((xT , yT )  (xT , yT ))> ((xT , yT )  (xT , yT )]
 E[wT> wT ] + 4R2 .
In the above, E denotes expectation over all user feedback yt given yt under the context
xt . It follows that E[wT>+1 wT +1 ]  4T R2 .
9

fiShivaswamy & Joachims

Algorithm 2 Batch Preference Perceptron.
Initialize w1  0
l1
s0
for t = 1 to T do
Observe xt
Present yt  argmaxyY wl> (xt , y)
Obtain feedback yt
if t == s + k then
P
Update: wl+1  wl + tj=s (xj , yj )  (xj , yj )
l l+1
st
end if
end for

Applying Jensens inequality on the concave function , we get:
q
>
E[wT w ]  kw kE[kwT k]  kw k E[wT> wT ].
The corollary follows from the definition of expected -informative feedback.
4.1 Lower Bound
We now show that the upper bound in Theorem 1 cannot be improved in general with respect
to its scaling with T . In the following lemma, given any Coactive Learning algorithm, we
construct a sequence of examples
where, even with  = 1 feedback, the algorithm suffers

an average regret of (1/ T ).
Lemma 3 For any coactive learning algorithm A with
 linear utility, there exist xt , objects
Y and w such that REGT of A in T steps is (1/ T ).
Proof Consider a problem where Y = {1, +1}, X = {x  RT : kxk = 1}. Define
the joint feature map as (x, y) = yx. Consider T contexts e1 , . . . , eT such that ej has
only the j th component equal to one and all the others equal to zero. Let y1 , . 
. . yT be
T, 
the sequence of outputs
of
A
on
contexts
e
,
.
.
.
,
e
.
Construct
w
=
[y
/
1

1
T
 >
y2 / T , . . . , yT / T ] , we have for this construction kw k = 1. Let the user feedback on
the tth step be yt . With these choices, the user feedback
is always -informative with  = 1
1 PT

since yt = yt . Yet, the regret of the algorithm is T t=1 (w> (et , yt )  w> (et , yt )) =
( 1T ).
4.2 Batch Update
The Preference Perceptron as stated in Algorithm 1 makes an update after every iteration.
In some applications, due to high volumes of feedback, it might not be possible to do an
update that frequently. For such scenarios, it is natural to consider a variant of Algorithm 1
that makes an update every k iterations; the algorithm simply uses wt obtained from the
10

fiCoactive Learning

Algorithm 3 Generic Template for Coactive Learning Algorithms
Initialize w1
for t = 1 to T do
Observe xt
Present yt  argmaxyY wt> (xt , y)
Obtain feedback yt
Perform an update on wt using the gradient of w> ((xt , yt )(xt , yt )) to obtain wt+1 .
end for
previous update until the next update. The type of updates shown in Algorithm 2 are called
mini-batch updates and have been used in distributed online optimization (Dekel, GiladBachrach, Shamir, & Xiao, 2012). The steps of the batch update algorithm are shown in
Algorithm 2. It is easy to show the following regret bound for batch updates:
Lemma 4 The average regret of the batch preference perceptron algorithm can be upper
bounded, for any   (0, 1] and for any w as follows:

T
1 X
2Rkw k k

REGT 
t +
.
T

T
t=1
While this lemma implies that mini-batches slow down learning by a factor equal to the
batch size, we will see in Section 6.2.3 that empirically convergence is substantially faster.

5. Deriving Algorithms for Coactive Learning
The Preference Perceptron and the regret it minimizes, as defined in Eqn. (4), is only
one point in the design space of different regret definitions and learning algorithms for
coactive learning. In this section, we will outline a general strategy for deriving coactive
learning algorithms from existing algorithms for online optimization. Furthermore, we will
demonstrate that more general notions of regret are meaningful and feasible in coactive
learning, and derive coactive learning algorithms for general convex and -strongly convex
losses.
All coactive learning algorithms that we derive in this section follow the general template
outlined in Algorithm 3. After initializing w1 , in each iteration the context xt is observed
and the algorithm presents yt by maximizing its current utility estimate represented by wt .
Once the feedback yt is observed, the algorithm simply takes the gradient of w> ((xt , yt ) 
(xt , yt )) and uses an update from a standard online convex optimization algorithm to
obtain wt+1 from wt .
In each case, an upper bound on the regret of the proposed algorithm is derived by
using the following strategy. First, we start with a notion of regret that is suited for
coactive learning. We then upper bound this regret by first reducing it to a form such
that results from a standard online convex opimization regret bound can be applied. This
gives a regret bound for the original coactive algorithm in turn. In each case, we use this
template algorithm to derive a specific algorithm. However, we still provide a self-contained
proof (in the appendix) clearly pointing out where we have used the regret bound from a
corresponding online convex optimization algorithm.
11

fiShivaswamy & Joachims

Algorithm 4 Exponentiated Preference Perceptron
Intialize w1i  N1
  2S1T
for t = 1 to T do
Observe xt
Present yt  argmaxyY wt> (xt , y)
Obtain feedback yt
i
wt+1
 wti exp((i (xt , yt )  i (xt , yt )))/Zt , where Zt is such that the weights add
to one.
end for

5.1 Exponentiated Preference Perceptron
To illustrate the generic strategy for deriving coactive learning algorithms, we first derive
an exponentiated gradient algorithm for coactive learning that can be used as an alternative
to the Preference Perceptron. The exponentiated gradient algorithm inherits the ability to
learn quickly for sparse weight vectors.
Unlike the additive updates of the Preference Perceptron, the exponentiated gradient
algorithm summarized in Algorithm 4 performs multiplicative updates. This exponentiated
algorithm is closely related to the exponentiated algorithms for classification (Kivinen &
Warmuth, 1997). At the start, it initializes all weights uniformly. Each subsequent update
step has a rate  associated with it. The rate depends on an upper bound on the ` norm
of the features (i.e., k(, )k`  S) and the time horizon T . After each multiplicative
update, the weights are normalized to sum to one, and the steps of the algorithm repeat.
Since the updates are multiplicative and the weights are initially positive, wt is guaranteed
to remain in the positive orthant for this algorithm. We note that Algoithm 4 is assumed
to know both T and S. There are standard techniques (see Cesa-Biachi & Lugosi, 2006b)
to convert such an algorithm to not have dependence on T , however, such extensions are
not the focus of this paper.
We now provide a regret bound for Algorithm 4. While the regret bounds for Algorithm 1
and Algorithm 2 depended on the `2 norm of the features, the bound for the exponentiated
algorithm depends on the ` norm of the features.
Theorem 5 For any w  RN such that kw k`1 = 1, w  0, under (expected) informative feedback the average (expected) regret of the Exponentiated Preference Perceptron can be upper bounded as:
T
1 X
2 log(m)S
S

REGT 
t +
+  ,
T
 T
2 T
t=1

E[REGT ] 

T
1 X  2 log(m)S
S

t +
+  ,
T
 T
2 T
t=1

where k(x, y)k`  S.
12

fiCoactive Learning

Proof We start with the regret of the coactive learning algorithm as defined in (4):
REGT =

=

T
1X
(U (xt , yt )  U (xt , yt ))
T
t=1
T
X

1
T

1
=
T

(U (xt , yt )  U (xt , yt )) +

t=1
T 
X

T
1 X
t
T
t=1

w> (xt , yt )

t=1





w> (xt , yt )

T
1 X
t
+
T

(13)

t=1

In the above equation, we have used the definition of -informative feedback as defined in
Eqn. (6). By viewing Algorithm 4 as an exponentiated online gradient descent algorithm, it
is easy to derive the following regret bound using techniques initially introduced by Kivinen
and Warmuth (1997),

T
T
X
X

S T
>
(wt ((xt , yt )  (xt , yt ))) 
(U (xt , yt )  U (xt , yt )) + 2 log(N )S T +
.
2
t=1

t=1

Since we could not find this specific bound in the literature, a self-contained proof is provided
in Appendix A. In the proof, REGT is first upper bounded in terms of the difference between
KL(w||wt+1 ) and KL(w||wt ). A telescoping argument is then used to get the above result.
Observing that wt> ((xt , yt )  (xt , yt ))  0, we get,
T
X
t=1



S T
(U (xt , yt )  U (xt , yt ))  2 log(N )S T +
.
2

(14)

Combining (13) and (14), we obtain the average regret bound. The proof of the expected
regret bound is analogous to that of the Preference Perceptron.
Like the result in Theorem 1, the above result (Theorem 5) also bounds the regret in
terms of the noisein the feedback (first term) and additional terms which converge to zero
at the rate O(1/ T ). The key difference to Theorem 1 is that the regret bound of the
exponentiated algorithm scales logarithmically with the number of features, but with the
`1 -norm of w, which can be advantageous if the optimal w is sparse.
5.2 Convex Preference Perceptron
Generalizing the definition of regret from Eqn. (4), we now allow that at every time step
t, there is an (unknown) convex loss function ct : R  R which determines the loss
ct (U (xt , yt )  U (xt , yt )) at time t based on the difference in utility between yt and the
optimal yt . The functions ct are assumed to be non-increasing. Non-increasing assumption
on ct is based on the intuition that the loss should be higher as U (xt , yt ) is farther from
U (xt , yt ). Further, sub-derivatives of the ct s are assumed to be bounded. Formally, c0t () 
[G, 0] for all t and for all   R where c0t () denotes the sub-derivative of ct () at . The
vector w which determines the utility of yt under context xt is assumed from a closed and
13

fiShivaswamy & Joachims

Algorithm 5 Convex Preference Perceptron.
Initialize w1  0
for t = 1 to T do
Set t  1t
Observe xt
Present yt  argmaxyY wt> (xt , y)
Obtain feedback yt
Update: wt+1  wt + t ((xt , yt )  (xt , yt ))
Project: wt+1  arg minuB ku  wt+1 k2
end for
bounded convex set B whose diameter is denoted as |B|. In the case of convex losses, we
consider the following notion of regret:
T
T
1X
1X

CREGT :=
ct (U (xt , yt )  U (xt , yt )) 
ct (0)
T
T
t=1

(15)

t=1

In the bound (16), ct (0) is the minimum possible convex loss since U (xt , yt )  U (xt , yt ) can
never be greater than zero by definition of yt . Hence the above regret compares the loss of
our algorithm with the best loss that could be achieved with a convex loss. Note that, for
the case ct () = , the above definition of regret reduces to our earlier definition of regret
in the linear case (Eqn. (4)).
Algorithm 5 minimizes the average convex loss. There are two differences between this
algorithm and Algorithm 1. First, there is a rate t associated with the update at time
t. Second, after every update, the resulting vector wt+1 is projected back to the set B.
Algorithm 5 is also closely related to the online convex optimization algorithm propsed
by Zinkevich (2003). However, the online convex optimization algorithm assumes that the
gradient of the loss (ct ()) is observed after each iteration. Our algorithm doesnt observe
the gradient directly, but only observes an improved object yt after presenting an object
yt .
Our earlier regret bounds were expressed in terms of slack variables t . However, here
and in the following section, our bounds will be expressed in terms of the clipped version
of the slack variables defined as t+ := max(0, t ).
Theorem 6 For the Convex Preference Perceptron under -informative feedback, for nonincreasing convex losses ct () with bounded sub-derivative, we have, for any   (0, 1] and
any w  B,


T
2G X + G
|B|
|B| 4R2
 +
CREGT 
t +
+ 
.
T
 2 T
T
T
t=1

(16)

Similarly, under expected -informative feedback, we have,


T
2G X + G
|B|
|B| 4R2
 +
E[CREGT ] 
t +
+ 
.
T
 2 T
T
T
t=1
14

(17)

fiCoactive Learning

The proof for the above Theorem is provided in the Appendix B. The idea of the proof is
to first divide the time steps into two types depending
on the nature of the feedback. This
PT
allows us to upper bound CREGT in terms of t=1 (wt  w )> ((xt , yt )  (xt , yt )). This
term can further be upper bounded by following the argument from Zinkevich (2003) even
in the Coactive Learning framework.
From the definition of CREGT in Eqn. (15), the above theorem upper bounds the
average convex loss via the minimum achievable loss and the quality of the feedback. Like
the previous result (Theorem 1), under strict
 -informative feedback, the average loss approaches the best achievable loss at O(1/ T ), albeit with larger constant factors.
In the case of the linear utility bounds in Theorem 1 and Theorem 5, it was sufficient
to have the average of the slack variables be zero to achieve zero regret. However, in the
case of convex losses, our upper bound on regret approaches zero only when the average of
the clipped slack variables is zero.
5.3 Second-Order Preference Perceptron
For a particular class of convex functions, it turns out that we can give much stronger
regret bounds than for general convex losses. The improvement for this special class of losses
parallels improvements in online convex optimization from general convex losses (Zinkevich,
2003) to -strongly convex losses (Hazan et al., 2007).
Definition 7 A convex function f : D  R is -strongly convex if for all points x and y
in D, the following condition is satisfied for a fixed  > 0:
f (x)  f (y) + f (x)> (x  y) 


||y  x||2 ,
2

(18)

where f (x) denotes a sub-derivative at x.
Algorithm 6 shows the Second-order Preference Perceptron for -strongly convex losses.
Like our previous algorithms, the Second-order Preference Perceptron also maintains a
weight vector wt , and the step of presenting yt based on a context xt is still the same as
in our previous algorithms. However, in addition to the weight vector, it also maintains an
additional matrix At which is constructed from the outer product of the vector (xt , yt ) 
(xt , yt ). The update step and the projection steps now involve both At as shown in
Algorithm 6. Algorithm 6 is closely related to the online convex optimization algorithm
proposed by Hazan et al. (2007). However, as we pointed out in the case of Algorithm 5,
our algorithm only observes a user preference feedback after each step unlike online convex
optimization algorithms which observe gradients. It is still possible to prove a regret bound
for the -strongly convex case, and we have the following result.
Theorem 8 For the second order preference learning algorithm, for (expected) -strongly
convex, non-increasing functions ct , with bounded sub-derivatives, we have,
 2

T
T
 X + 2 2G X + G|B|
GN
4R T 
CREGT 
t +
t +
+
log
+1 ,
(19)
2T 2
T
T
2T 

t=1
t=1
 2

T
T
4R T 
 X +2 2G X + G|B|
GN
E[CREGT ] 
t +
t +
+
log
+1 ,
(20)
2T 2
T
T
2T 

t=1

t=1

15

fiShivaswamy & Joachims

Algorithm 6 Second-order Preference Perceptron.
Intialize w1  0
A0  I
  /G.
for t = 1 to T do
Observe xt
Present yt  argmaxyY wt> (xt , y)
Obtain feedback yt
At  At1 + [(xt , yt )  (xt , yt )][(xt , yt )  (xt , yt )]>
Update: wt+1  wt + A1
t [(xt , yt )  (xt , yt )]
Project: wt+1 = arg minwB (wt+1  w)> At (wt+1  w)
end for
where,  > 0 is an initialization parameter, as shown in Algorithm 6.
We prove the above Theorem in the Appendix C. Like in the proof of Theorem 6, we divide
time steps into two types. Starting with this, it is possible to upper bound CREGT to such
a form that the resulting terms can be upper bounded using similar arguments as that for
online strongly convex optimization (Hazan et al., 2007).
When user feedback is strictly -informative for some  and some w  B, the first two
terms of the regret bound (19) result in an O( logT T ) scaling with T . However, there is a
linear dependence on the dimensionality of the joint feature map in the regret bound for
the Second-order Preference Perceptron algorithm.
Even though it appears like we need to invert the matrix At in the Second-order Preference Perceptron, this can be avoided since the updates on At are of rank one. By the
Woodbury matrix inversion lemma, we have:
> > 1
A1
t = (At1 + [(xt , yt )  (xt , yt )][(xt , yt )  (xt , yt )] ) )

= A1
t1 

> 1
A1
t1 [(xt , yt )  (xt , yt )][(xt , yt )  (xt , yt )] At1

1/() + [(xt , yt )  (xt , yt )]> A1
t1 [(xt , yt )  (xt , yt )]

.

Thus, in practice, the Second-order Preference Perceptron can update both At and
Bt in each iteration. Nevertheless, the projection step to obtain wt+1 involves solving a
quadratically-constrained quadratic program where B is a ball of fixed radius, which still
takes O(N 3 ) time. Hence, the Second-order Preference Perceptron is computationally more
demanding than the Convex Preference Perceptron. As we show in the experiments, the
Second-order Preference Perceptron might be still quite useful for low-noise data.

6. Experiments
We empirically evaluate our Coactive Learning algorithms on two real-world datasets. The
two datasets differ in the nature of prediction and feedback. On the first dataset, the
algorithms operate on structured objects (rankings) whereas on the second dataset, atomic
items (movies) were presented and received as feedback.
16

fiCoactive Learning

6.1 Datasets And User Feedback Models
First, we provide a detailed description of the two datasets that were used in our experiments. Along with this, we provide the details of the strategies that we used on each dataset
for generating user feedback.
6.1.1 Structured Feedback: Web-Search
Our first dataset is a publicly available dataset from Yahoo! (Chapelle & Chang, 2011) for
learning to rank in web-search. This dataset consists of query-url feature vectors (denoted as
xqi for query q and URL i), each with a relevance rating riq that ranges from zero (irrelevant)
to four (perfectly relevant). To pose ranking as a structured prediction problem, we defined
our joint feature map as follows:
w> (q, y) =

5
X
w> xqyi
.
log(i + 1)

(21)

i=1

In the above equation, y denotes a ranking such that yi is the index of the URL which is
placed at position i in the ranking. Thus, the above measure considers the top five URLs
for a query q and computes a score based on graded relevance. Note that the above utility
function defined via the feature-map is analogous to DCG@5 (see, Manning et al., 2008)
after replacing the relevance label with a linear prediction based on the features.
For query qt at time step t, the Coactive Learning algorithms present the ranking ytq
that maximizes wt> (qt , y). Note that this merely amounts to sorting documents by the
scores wt> xqi t , which can be done very efficiently. The utility regret in Eqn. (4), based on
P
the definition of utility w> (q, y) is given by T1 Tt=1 w> ((qt , yqt  )  (qt , yqt )). Here yqt 
denotes the optimal ranking with respect to w , which we consider to be the best least
squares fit to the relevance labels from the features using the entire dataset. We obtain
yqt  from Eqn. 3, that is, yqt  = argmaxyY w> (qt , y). In all our experiments, query
ordering was randomly permuted twenty times and we report average and standard error
of the results.
We used the following two user models for generating simulated user feedback in our
experiments. The first feedback model is an idealized version of feedback whereas the second
feedback is based directly on relevance labels that are available in the dataset:
 Strict -informative feedback: In this model, the user is assumed to provide
strictly -informative feedback at a given  value (i.e., slacks zero). Given the predicted ranking yt , the user would go down the list until she found five URLs such that,
when placed at the top of the list, the resulting yt satisfied the strictly -informative
feedback condition w.r.t. the optimal w . This model assumes that the user has
access to w hence it is an idealized feedback.
 Noisy feedback at depth k: In this feedback model, given a ranking for a query,
the user would go down the list inspecting the top k URLs (or all the URLs if the list
is shorter) for a specified k value. Five URLs with the highest relevance labels (riq )
are placed at the top five locations in the user feedback. Note that this produces noisy
feedback since no linear model can perfectly fit the relevance labels on this dataset.
17

fiShivaswamy & Joachims

6.1.2 Item Feedback: Movie Recommendation
In contrast to the structured prediction problem in the previous dataset, we considered a
second dataset with atomic predictions, namely movie recommendation. In each iteration,
a movie is presented to the user, and the feedback consists of a single movie as well. We
used the MovieLens dataset from grouplense.org which consists of a million ratings over
3900 movies as rated by 6040 users. The movie ratings range from one to five.
We randomly divided users into two equally sized sets. The first set was used to obtain
a feature vector xj for each movie j using the SVD embedding method for collaborative
filtering (see Bell & Koren, 2007, Eqn. (15)). The dimensionality of the feature vectors
and the regularization parameters were chosen to optimize cross-validation accuracy on the
first dataset in terms of squared error. For the second set of users, we then considered the
problem of recommending movies based on the movie features xj . This experiment setup
simulates the task of recommending movies to a new user based on movie features from old
users.
Tx
For each user i in the second set, we found the best least squares approximation wi
j
to the users utility functions on the available ratings. This enabled us to impute utility
values for movies that were not explicitly
rated by this user. Furthermore, it allowed us
P
> (x  x ), which is the average difference in
to measure regret for each user as T1 Tt=1 wi
t
t
utility between the recommended movie xt and the best available movie xt . We denote the
best available movie at time t by xt which is obtained from Eqn. 3. In this experiment,
once a user gave a particular movie as feedback, both the recommended movie and the
feedback movie were removed from the set of candidates for subsequent recommendations.
In all the experiments we report (average) regret values averaged over all 3020 users in the
test set.
To simulate user behavior, we considered the following two feedback models on this
dataset:
 Strict -informative feedback: As in the previous dataset, in this model, the user
is assumed to provide strictly -informative feedback at a given  value (i.e., slacks
zero). Given the predicted movie yt , the user is assumed to watch the movie if it
already has the highest rating in the remaining corpus of movies. If not, the user
picks another movie from the corpus with lowest utility that still satisfies strict informative assumption. This model again assumes that the user has access to w ,
hence it is an idealized feedback.
 Noisy feedback: In this feedback model, given a movie y, the user is assumed to
have access to either the actual rating of the movies (when available) or is assumed to
round the imputed rating to the nearest legal rating value. We used two sub-strategies
by which the user provides feedback. In better feedback, the user provides y such
that it has the smallest rating (actual rating or rounded rating) but strictly better
rating than that of y. In best feedback, the user provides y such that it has the
highest rating (actual rating or rounded rating) in the remaining corpus. There could
be multiple movies satisfying the above criteria, and ties were broken uniformly at
random among such movies. Note that this feedback model results in a noisy feedback
due to rounding of movie ratings to discrete values.
18

fiCoactive Learning

6.2 Preference Perceptron
In the first set of experiments, we analyze the empirical performance and scaling behavior
of the basic Preference Perceptron Algorithm and its variants.
6.2.1 Strong Versus Weak Feedback
The goal of the first experiment to explore how the regret of the algorithm changed with feedback quality. To get feedback at different quality levels , we used strict -informative
feedback for various  values.
1.8

6

 = 0.1
 = 0.5
 = 1.0

1.6

 = 0.1
 = 0.5
 = 1.0

5

avg. util regret

avg. dcg regret

1.4
1.2
1
0.8

4

3

2

0.6
0.4

1
0.2
0 0
10

1

10

2

3

10

10

0 0
10

4

10

t

1

2

10

10

3

10

t

Figure 2: Regret based on strict -informative feedback for various  values for websearch (left) and movie recommendation (right).
Figure 2 shows the results of this experiment for three different  values. Overall, regret
is typically substantially reduced after only tens or hundreds of iterations. As expected,
the regret for  = 1.0 is lower compared to the regret for lower  values. Note, however,
that the difference between the two curves is much smaller than a factor of ten. Also note
that the differences are less prominent in the case of web-search. This is because strictly
-informative feedback is also strictly -informative feedback for any   . So, in our
user feedback model, we could be providing much stronger feedback than that required by
a particular  value. As expected from the theoretical bounds, since the user feedback is
based on a linear model with no noise, utility regret approaches zero in all the cases. Note
that we show standard error in the plots, giving an indication of statistical significance. In
the left plots in Figure 2, the standard errors are high at lower iterations but become lower
with more iterations. In some plots in the rest of the paper, the error bars are small and
may be difficult to visually identify.
In the rest of this paper, for strict -informative feedback, we consider  = 0.5
unless we explicitly mention otherwise.
6.2.2 Noisy Feedback
In the previous experiment, user feedback was based on actual utility values computed from
the optimal w . We next study how regret changes with noisy feedback where user behavior
19

fiShivaswamy & Joachims

does not follow a linear utility model. For the web-search dataset, we use noisy feedback
at depths 10 and 25, and for the movie dataset we use noisy feedback with both the
better and the best variant of it.
1.6

6

depth=10
depth=25

1.4

better
best

5

avg. util regret

avg. util regret

1.2
1
0.8
0.6

4

3

2

0.4
1

0.2
0 0
10

1

10

2

3

10

10

0 0
10

4

10

t

1

2

10

10

3

10

t

Figure 3: Regret based on noisy feedback for web-search (left) and movie recommendation
(right).

The results for this experiment are shown in Figure 3. The first observation to make is
that in the case of web-search, the regret values now do not converge to zero. Similarly, in
the case of movie recommendation the regret values are higher than those in the previous
experiment. These results are in line with our theory which shows regret converging to
average slack variables when the user does not provide strict  informative feedback for any
. Interestingly, in the case of web-search the average regret is slightly higher when the
user goes to greater depth in providing feedback. This is due to the fact that the relevance
labels in this dataset are noisy  when the user maximizes (noisy) utility over a larger set of
URLs, the selection of the (true) utility maximizers becomes less reliable, which degrades
user feedback quality.
In the rest of this paper, for web-search we consider noisy feedback with depth=10. In
the case of movie recommendation, we consider the better version of the noisy feedback
unless we explicitly mention otherwise.
6.2.3 Batch Updates
In this section, we consider the Batch Preference Perceptron
algorithm (Algorithm 2). Its

regret bound from Section 4.2 scales by a factor k under strict -informative feedback,
if the update is made only every k iterations of the algorithm. We now verify whether
empirical performance scales as suggested by the bound. For both web-search and movies,
we considered both strict -informative feedback and noisy feedback. For both types
of feedback, we use the Batch Perceptron Algorithm with various values of k and report the
resulting average regret.
The results of these experiments are shown in Figure 4 and Figure 5. As expected,
as the value of k becomes smaller, regret converges faster. However, we observe that the
20

fiCoactive Learning

1.6

6

k=1
k = 10
k=20
k=50

1.4

k=1
k=10
k=20
k=50

5

avg. util regret

avg. util regret

1.2
1
0.8
0.6

4

3

2

0.4
1

0.2
0 0
10

1

10

2

3

10

10

0 0
10

4

10

1

2

10

t

10

3

10

t

Figure 4: Regret versus time based on batch updates with strict -informative feedback
for web-search (left) and movie recommendation (right).

1.6
1.4
1.2
1

4

3

0.8

2

0.6

1

0.4 0
10

1

10

2

3

10

10

k=1
k=10
k=20
k=50

5

avg. util regret

avg. util regret

6

k=1
k = 10
k=20
k=50

0 0
10

4

10

t

1

2

10

10

3

10

t

Figure 5: Regret versus time based on batch updates with noisy feedback for web-search
(left) and movie recommendation (right).


empirical scaling with k is substantially better than the k factor suggested by Lemma 4.
These results show the feasibility of using Coactive Learning algorithms in systems where
it might be impractical to do an update after every iteration.
6.2.4 Expected User Feedback
The user feedback was deterministic in our experiments so far. In this sub-section, we consider probabilistic feedback and study the behavior of the Preference Perceptron algorithm.
Recall that we provided an upper bound on the expected regret for expected user feedback
in Corollary 2.
To provide -informative feedback under expectation, we consider the following strategy.
Given an object yt on context xt , the user would first generate deterministic feedback yt
21

fiShivaswamy & Joachims

following a strict -informative feedback model ( = 0.5 for web-search and  = 1.0
for movie recommendation).5 In addition, we consider five randomly generated objects
as feedback. We then put uniform probability mass over the randomly generated objects
and remaining mass over the deterministic feedback such that the user feedback is still
-informative at  = 0.5 in expectation.
6

Expct. feedback
Det. feedback

1.6
1.4

Expct. Feedback
Det. Feedback

5

avg. util regret

avg. util regret

1.2
1
0.8
0.6

4

3

2

0.4
1
0.2
0 0
10

1

10

2

3

10

10

0 0
10

4

10

t

1

2

10

10

3

10

t

Figure 6: Expected feedback versus deterministic feedback on web-search (left) and movie
recommendation (right).

The results for this experiment are shown in Figure 6. As a reference, we also plot the
regret curve with deterministic -informative feedback with  = 0.5. It can be seen that
there is not much difference between deterministic and expected feedback at higher numbers
of iterations. It can also be seen that the regret converges to zero even with -informative
feedback in expectation as suggested by Corollary 2.
6.2.5 Comparison with Ranking SVM
We now compare our algorithms against several baselines, starting with a conventional
Ranking SVM (Joachims, 2002) that is repeatedly trained. At each iteration, the previous
qt
SVM model is used to present a ranking to the user (ysvm
). The user returns a ranking
qt
(ysvm ) based on strict -informative feedback in one experiment and based on noisy
qt
qt
feedback in the other. The pairs of examples (qt , ysvm
) and (qt , ysvm
) are used as training
pairs for the ranking SVM. Note that training a ranking SVM after each iteration would be
prohibitive expensive, since it involves solving a quadratic program and cross-validating the
regularization parameter C. Thus, we retrained the SVM whenever 10% more examples
were added to the training set. The first training was after the first iteration with just
one pair of examples (starting with a random yq1 ), and the C value was fixed at 100 until
there were 50 pairs of examples, when reliable cross-validation became possible. After there
were more than 50 pairs in the training set, the C value was obtained via five-fold cross5. Note that, in the case of web-search, our user model can provide strictly -informative where  larger
than 0.5.

22

fiCoactive Learning

validation. Once the C value was determined, the SVM was trained on all the training
examples available at that time. The same SVM model was then used to present rankings
until the next retraining.
1.6

6

SVM
Pref. Perceptron

1.4

SVM
Pref. Perceptron

5

avg. util regret

avg. util regret

1.2
1
0.8
0.6

4
3
2

0.4
1
0.2
0 0
10

1

10

2

3

10

10

0 0
10

4

10

1

2

10

10

3

10

t

t

Figure 7: Preference Perceptron versus Ranking SVM with strict -informative feedback on web-search (left) and movie recommendation (right).

6

SVM
Pref. Perceptron

1.4

5

1.2

4

avg. util regret

avg. util regret

1.6

1

3

0.8

2

0.6

1

0.4 0
10

1

10

2

3

10

10

0 0
10

4

10

SVM
Pref. Perceptron

1

2

10

10

3

10

t

t

Figure 8: Preference Perceptron versus Ranking SVM with noisy feedback on web-search
(left) and movie recommendation (right).

The results of this experiment are shown in Figure 7 and Figure 8. In the case of
strict -informative feedback, the Preference Perceptron performed much better than
the SVM for the movie recommendation, and comparably for web search. In the case of
noisy feedback, the Preference Perceptron performs significantly better than the SVM
over most of the range on both the datasets. While it took around 20 minutes to run the
Preference Perceptron experiment, it took about 20 hours to run the SVM experiment on
23

fiShivaswamy & Joachims

web-dataset for each permutation of the dataset. Similary, on the movie recommendation
task it took around 125 seconds to run the preference perceptron for each user while it took
around 260 seconds to run the SVM for each user. These results show that the preference
perceptron can perform on par or better than SVMs on these tasks at a fraction of the
computational cost.
6.2.6 Comparison with Dueling Bandit
As a second baseline, we compare the Preference Perceptron algorithm with the dueling
bandit approach of Yue and Joachims (2009). In each step, the dueling bandit algorithm
makes a comparison between a vector w and a perturbed version of it w1 (in a random
direction u such that w1 = w + u). The results produced by these two weight vectors are
assessed by the user through techniques such as interleaving (Radlinski, Kurup, & Joachims,
2008), providing a preference between w and w1 . The preference feedback determines the
update that the dueling bandits algorithm makes to w. If w is preferred, it is retained
for the next round. If w1 is preferred, a small step of length  is taken in the direction of
perturbation u.
1.8

Dueling Bandit
Pref. Perceptron

1.6

1.6

1.4

1.4

1.2

1.2

avg. util regret

avg. util regret

1.8

1
0.8

1
0.8

0.6

0.6

0.4

0.4

0.2

0.2

0 0
10

1

10

2

3

10

10

0 0
10

4

10

t

Dueling Bandit
Pref. Perceptron

1

10

2

3

10

10

4

10

t

Figure 9: Preference Perceptron versus Dueling Bandit on web-search. The left plot is based
on strict -informative feedback, the right plot shows noisy feedback.
In our first experiment on web-search, in each step, we first obtained two ranked lists
based on w and w1 . The features used to obtain these ranked lists were identical to those
used for Preference Perceptron. The two rankings were then interleaved. The interleaved
ranking was presented to a user. In the first experiment, the user provided strict informative feedback on the interleaved ranking. In the second experiment, the user
provided noisy feedback. Depending on the feedback, we inferred which of the two rankings was preferred using the Team-Game method proposed by Radlinski et al. (2008). When
w was preferred or when there was a tie, no step was taken. When w1 was preferred, a
step of length  was taken in the direction u. The regret of the dueling bandit algorithm
was measured by considering the utility of the interleaved ranking. Unlike the Preference
Perceptron algorithm, the dueling bandit algorithm has two parameters ( and ) that need
24

fiCoactive Learning

to be tuned. We considered 25 values for these parameters (5x5 grid) and simply chose the
best parameter values of the dueling bandits algorithm in hindsight.
The results for this experiment are shown in Figure 9. Despite the advantage of setting
the parameters to best possible values, it can be seen that dueling bandit algorithm performs
significantly worse compared to the preference perceptron algorithm by orders of magnitude.
For example, the performance of the dueling bandit at around 28,000 iterations is matched
by preference perceptron at less than 100 iterations with both types of feedback. This is
not surprising, since the dueling bandit algorithm basically relies on random vectors to
determine the direction in which a step needs to be taken. In the Coactive Learning model,
the user feedback provides a (better than random) direction to guide the algorithm.
6

Dueling Bandit
Pref. Perceptron

5

5

4

4

avg. util regret

avg. util regret

6

3

3

2

2

1

1

0 0
10

1

2

10

10

0 0
10

3

10

t

Dueling Bandit
Pref. Perceptron

1

2

10

10

3

10

t

Figure 10: Preference Perceptron versus Dueling Bandit on movie recommendation. The
left plot is based on utility values whereas the right plot shows results with
rounded values.

Similarly, we also conducted a comparison with the dueling bandit algorithm on the
movie recommendation dataset. However, unlike the web-search experiment, the dueling
bandit model is somewhat unnatural on this dataset in our experimental setup, since interleaving two rankings is natural whereas interleaving two items is not. We therefore consider
a different setup. Two movies were obtained based on w and w1 for the dueling bandit
algorithm. User feedback was to merely indicate which of these two movies has a higher
rating. In the noisy case, user feedback was based on the actual rating or the rounded rating. In the noise-free case, user feedback was based on the utility values. In either case, the
utility of dueling bandit was considered to be the average utility of the two movies selected
for comparison.
The performance of the dueling bandit algorithm in this experiment is shown in Figure 10. For the Preference Perceptron algorithm, regret curves for strict -informative
feedback ( = 0.5) and better noisy feedback are also shows as reference. It can be
seen that the dueling bandit algorithm again performs substantially worse compared to the
Preference Perceptron algorithm.
25

fiShivaswamy & Joachims

6.3 Exponentiated versus Additive Updates
In this experiment, we compare the exponentiated algorithm (Algorithm 4) with the additive
Preference Perceptron algorithm. For the exponentiated algorithm, all the components of
we must be non-negative.6 We obtained a non-negative we as follows:
[we ]i


=

min(0, [w ]i )
1  i  m,
max(0, [w ]im ) m + 1  i  2m.

(22)

In the above equation, [we ]i denotes the ith component of we . Moreover, we also modified
the joint feature map for the exponentiated algorithm as follows:
e



[ (x, y)]i =

+[(x, y)]i
1im
[(x, y)]im m + 1  i  2m

(23)

With the above modifications, we will have only non-negative components and moreover, it is easy to verify that we > e (x, y) = w> (x, y). This makes the regret of the
exponentiated algorithm directly comparable with the regret of the additive algorithm.
The exponentiated algorithm has a fixed rate parameter  that inversely depends on
the time horizon T . When T is large,  is small. In this situation, consider the update in
Algorithm 4:
i
wt+1
 wti exp((i (xt , yt )  i (xt , yt )))/Zt .
Since,  is small, we can approximate the exponential term in the above equation with
a first order approximation:
exp((i (xt , yt )  i (xt , yt )))  1 + (i (xt , yt )  i (xt , yt )).
Thus the exponentiated updates resemble the updates of the additive algorithm up to
a normalization factor. Despite the normalization factor, we empirically observed the behavior of the two algorithms to be nearly identical (though not exact). We thus empirically
evaluated the exponentiated algorithm with a variable rate parameter t = 2S1t at time t.
Note that this is an empirical result without formal theoretical guarantees for this variable
rate.
Results of this experiment are shown in Figure 11 and Figure 12 for strict -informative
feedback and noisy feedback respectively. It can be seen that the exponentiated algorithm tends to performs slightly better than the additive algorithm for small number of
iterations. As the time horizon becomes large, the two algorithms seem to have comparable
performance in most cases.
6.4 Minimizing Convex Losses
In this section, we empirically evaluate the Convex Preference Perceptron (Algorithm 5)
and the Second-Order Preference Perceptron (Algorithm 6).
6. We put a superscript e to distinguish the joint feature map and w that we used in our experiments for
the exponentiated algorithm.

26

fiCoactive Learning

1.6

6

Exponentiated
Pref. Perceptron

1.4

Exponentiated
Pref. Perceptron

5

avg. util regret

avg. util regret

1.2
1
0.8
0.6

4

3

2

0.4
1

0.2
0 0
10

1

10

2

3

10

10

0 0
10

4

10

1

2

10

t

10

3

10

t

Figure 11: Exponentiated versus Additive with strict -informative feedback on websearch (left) and movie recommendation (right).

1.6

6

Exponentiated
Pref. Perceptron

1.4

Exponentiated
Pref. Perceptron

5

avg. util regret

avg. util regret

1.2
1
0.8
0.6

4

3

2

0.4
1

0.2
0 0
10

1

10

2

3

10

10

0 0
10

4

10

t

1

2

10

10

3

10

t

Figure 12: Exponentiated versus Additive with noisy feedback on web-search (left) and
movie recommendation (right).

6.4.1 Convex Perceptron Versus Second-Order Algorithm
The regret bounds from Section 5 show that one can get lower regret for -strongly convex
functions using a second-order algorithm, while the first-order Convex Perceptron applies
to general convex functions. In this section, we evaluate the relative performance of the
first-order and the second-order algorithms empirically. For this purpose, we considered the
quadratic loss c() = (  M )2 where M is the largest utility value on any possible (x, y)
with any w in a convex ball of radius kw k. It can be verified this loss function is -strongly
convex. B was set to 100 for both the algorithms for both the datasets.
27

fiShivaswamy & Joachims

Second Order
Convex Perceptron

0.05

500

0.04

400

0.03

300

0.02

200

0.01

100

0 0
10

1

2

10

Second Order
Convex Perceptron

600

Util regret

Quad regret

0.06

3

10

10

0 0
10

4

10

1

2

10

3

10

t

10

4

10

t

Figure 13: Cumulative regret of the convex perceptron and the second order convex perceptron for web-search.

0.03

250

Second Order
Convex Perceptron

200

0.02

Util regret

Quad. regret

0.025

Second Order
Convex Perceptron

0.015

150

100

0.01
50

0.005
0 0
10

1

2

10

10

0 0
10

3

10

t

1

2

10

10

3

10

t

Figure 14: Cumulative regret of the convex perceptron and the second order convex perceptron for movie recommendation.

In the first set of experiments, we considered strict -informative feedback. We ran
both the second-order algorithm as well as the Convex Preference Perceptron algorithm 5.
The  value in the second order perceptron was simply set to one. We recorded the REGT
and CREGT values for both the methods. Note that REGT corresponds to the utility
regret as defined in 4.
Results of this experiment are shown in Figure 13 and Figure 14. To demonstrate
the qualitative difference between the two algorithms, we plot cumulative regret (i.e. T 
REGT and T  CREGT ) in these figures. The cumulative regret of the second-order
algorithm is linear on a log-scale. This shows that the convergence of the regret is indeed
28

fiCoactive Learning

logarithmic, compared to the much slower convergence of the Convex Preference Perceptron.
Interestingly, even the cumulative regret based on raw utility values empirically shows a
similar behavior. This is purely an empirical result, since theoretically, O(log(T )/T ) average
regret holds only for strongly convex losses and the linear loss is not strongly convex.
0

10

weak@5000
strong@5000
weak@15000
strong@15000
weak@25000
strong@25000

1

Util. regret

Quad regret

10

4

weak@5000
strong@5000
weak@15000
strong@15000
weak@25000
strong@25000

10

3

10

2

10

3

10

2

6

4

10

2

10

0

10

10

2

10

10 6
10

4

10

4

2

10

0

10



10

2

10

4

10



Figure 15: Sensitivity of the second order preference perceptron algorithm to the parameter
value .

1.2

10

weak@5000
strong@5000
weak@15000
strong@15000
weak@25000
strong@25000

Quad. regret

1.4

10

10

2.4

10
Util. regret

1.3

10

weak@5000
strong@5000
weak@15000
strong@15000
weak@25000
strong@25000

2.5

1.5

10

2.3

10

1.6

10

2.2

10
1.7

10

2.1

4

10

2

0

10

10

2

10

4

10



10

4

10

2

0

10

10

2

10

4

10



Figure 16: Sensitivity of the second order preference perceptron algorithm to the parameter
value  on movie recommendation.

In the previous experiment, we fixed the  value in the second-order algorithm to one.
We now study the sensitivity of the second-order algorithm to the value of this parameter.
Figures 15 and 16 show regret values after a given number of iterations when  is swept
over a range of values. The dotted lines show the performance of the Convex Preference
Perceptron for comparison. In the case of web-search, there is a wide range of parameter
29

fiShivaswamy & Joachims

values where the performance of the algorithm is good. As the parameter  takes an extreme
value on either side, the performance of the algorithm deteriorates. The range of suitable
 values is much broader for the web-search dataset than for movie recommendation. It is
interesting to note that both the algorithms performed empirically best at  = 1 among the
values that were tried.
0.12

14000

Second Order
Convex Perceptron

12000

0.1

Second Order
Convex Perceptron

Util regret

Quad regret

10000
0.08
0.06
0.04

6000
4000

0.02
0 0
10

8000

2000
1

2

10

3

10

10

0 0
10

4

10

1

2

10

3

10

10

4

10

t

t

Figure 17: Strong convex versus weak convex with noisy feedback on web-seach.

1400

Second Order
Convex Perceptron

0.14

1200

0.12

1000

0.1

Util regret

Quad. regret

0.16

0.08

Second Order
Convex Perceptron

800
600

0.06
400

0.04

200

0.02
0 0
10

1

2

10

10

0 0
10

3

10

t

1

2

10

10

3

10

t

Figure 18: Strong convex versus weak convex with noisy feedback on movie recommendation.

We also tested the convex algorithms under noisy feedback. Both regret bounds contain the slack terms on the right hand side. Thus, when user feedback is not -informative
for any , the regret bounds for the second-order algorithm and the first-order algorithm
are both dominated by the slack variables. The empirical performance of the two algorithms under noisy feedback are shown in Figures 17 and 18. In the case of web-search,
the results for the second-order algorithm and the first-order algorithm are nearly identi30

fiCoactive Learning

cal. However, in the case of movie recommendation, there is still some advantage to the
second-order algorithm.
In summary, the second-order algorithm performs substantially superior under no-noise
circumstances. In the presence of noise in the feedback, the two algorithms do not show
drastically different behaviors.

7. Conclusions
We proposed Coactive Learning as a new model of online learning with preferences that is
especially suited for implicit user feedback. Unlike most supervised learning approaches,
Coactive Learning algorithms do not require optimal labels, but merely (noisy) feedback
that improves over the prediction. Our model, where no cardinal utilities are observed,
sits between the experts and the bandits settings, and we argue that Coactive Learning is
applicable to a wide range of systems that aim to optimize themselves based on observable
user actions.
We provide several algorithms that provably optimize regret in the Coactive Learning
framework, and we empirically validate the effectiveness of the proposed framework on
web-search ranking and movie recommendation datasets with simulations of both noisy and
noise-free feedback. A recurring theme in this paper is that a wide variety of conventional
online learning algorithms can be converted into Coactive Learning algorithms, despite the
differences in the learning model itself, in the nature of feedback and in the notion of regret.
We conjecture that many other online learning algorithms could similarly be converted to
practically useful Coactive Learning algorithms.
The Coactive Learning model, the algorithms we proposed, and the ability to use weak
feedback from observable user behavior offer a wide range of opportunities for new learning
approaches to application problems ranging from natural language processing and information retrieval to robotics. There are also several opportunities for further developing
algorithms for the Coactive Learning model. For example, our algorithm for convex loss
minimization assume only that the gradient of the convex losses are bounded. However,
in most practical situations, the convex loss to be minimized is known apriori. It is an
interesting research direction to study whether there are algorithms that can utilize the
gradient of the loss to perform better either theoretically or empirically. Another question
is whether better algorithms exist for special cases of the linear utility model. Our lower
bound is based on an argument where the dimensionally of the joint feature maps grow
with the given horizon T . When the dimensionality of the joint feature map is fixed, an
interesting research question is: are there algorithms with better regret than our proposed
algorithms?

Acknowledgments
This work was funded in part under NSF awards IIS-0905467, IIS-1247637, and IIS-1142251.
This was work was done when Pannaga Shivaswamy was a postdoctoral associate at Cornell
University. We thank Peter Frazier, Bobby Kleinberg, Karthik Raman, Tobias Schnabel and
31

fiShivaswamy & Joachims

Yisong Yue for helpful discussions. We also thank anonymous reviewers for their thoughtful
comments on an earlier version of this paper.

Appendix A. Proof of Theorem 5
Proof We look at how the KL divergence between w and wt evolves,
KL(w||wt )  KL(w||wt+1 ) =

=

N
X
i=1
N
X

i
wi log(wt+1
/wti )

wi ((i (xt , yt )  i (xt , yt )))  log(Zt )

i=1

= w> ((xt , yt )  (xt , yt ))  log(Zt ).
(24)
P
i
On the second line, we pulled out log(Zt ) from the sum since N
i=1 w = 1. Now, consider
i
i
the last term in the above equation. Denoting  (xt , yt )   (xt , yt ) by i t for brevity, we
have, by definition,
!
N
X
log(Zt ) = log
wti exp(i t )
 log

i=1
N
X

wti (1

i

2

i

2

!

+  t +   t )

i=1



 log 1 + wt> t +  2 S 2
 wt> t +  2 S 2 .

(25)

On the second line we used the fact that exp(x)  1 + x + x2 for x  1. The rate  ensures
that (i )  1. On the last line, we used the fact that log(1 + x)  x. Combing (24) and
(25), we get,
(w  wt )> t 

KL(w||wt )  KL(w||wt+1 )
+ S 2 .


Adding the above inequalities, we get:
T
T
T 1
X
X
KL(w||wt )  KL(w||wt+1 ) X 2
(w  wt )> ((xt , yt )  (xt , yt )) 
+
S .

t=1

t=1



t=1

KL(w||w0 )
+ S 2 T.


Rearranging the above inequality, and substituting the value of  from Algorithm 4, we
get:

T
T
X
X

S T
>
(U (xt , yt )  U (xt , yt )) 
wt ((xt , yt )  (xt , yt )) + 2 log(N )S T +
2
t=1
t=1


S T
 2 log(N )S T +
.
2
32

fiCoactive Learning

In the above, we also used the fact that KL(w||w1 )  log(N ) since w1 is initialized uniformly. Moreover, from Holders inequality, we obtained
wt> (xt , yt )  kwt k`1 k(xt , yt )k`  S.
The above inequality along with -informative feedback gives the claimed result.

Appendix B. Proof of Theorem 6
Proof First, we divide the set of time steps into two different sets based on the nature of
the feedback:
I := {t : U (xt , yt )  U (xt , yt ))  0; 1  t  T },
J := {t : U (xt , yt )  U (xt , yt )) < 0; 1  t  T }.
For brevity we denote (xt , a)  (xt , b) by7 (a, b) in the rest of this proof. We start
by considering the following term for a single time step t:
ct (U (xt , yt )  U (xt , yt ))  ct (0)

 >
wt (yt , yt )

ct (U (xt , yt )  U (xt , yt ))  ct


 >

 >
wt (yt , yt )
w (yt , yt ) t

 ct
=ct




  >

>
(w  wt ) (yt , yt ) t 0 w (yt , yt ) t


ct






+
>
>
 (wt (yt , yt ) + t  w (yt , yt ))G/ t  I


(wt> (yt , yt ) + t+ )G/
t  J.
w> (y ,y )

In the above inequalities, the second line follows from the fact that t  t t  0 and ct ()
is non-increasing. The third line follows from -informative feedback (Eqn. (6)). The fourth
line follows since the function ct is convex.8 We obtain the first term in the next inequality
(in either case) since c0t ()  [G, 0] and wt> (yt , yt )  0 from the choice of yt in the
algorithm. The second terms (in either case) is obtained by the fact that t c0t (w> (yt , yt ))
is upper bounded by t+ G. This is the step in which the clipped version of the slack variables
are needed in the proof. Finally, w> (yt , yt ) is either positive or negative depending on
the feedback which leads to two different cases depending on whether t  I or t  J.
7. Since the context xt will always be clear, we suppress this in our notation for brevity.
8. For any convex function f , f (y)  f (x)  (y  x)f 0 (y) where f 0 (y) denotes a sub-derivative of f at y.

33

fiShivaswamy & Joachims

Summing the above inequalities from 1 to T , we get:
T
X

ct (w> (yt , yt )) 

t=1





T
GX


G


wt> (yt , yt ) +

t=1
T
X

T
X

ct (0)

t=1
T
X

G


t+ 

t=1

tI

(wt  w )> (yt , yt ) +

t=1

GX >
w (yt , yt )


G


T
X
t=1

t+ +

GX >
w (yt , yt ).


(26)

tJ

P
We obtained the last line above simply by adding and and subtracting G tJ w> (yt , yt )/
on the right side of the previous inequality. From this point, we mostly follow the proof
techniques from online convex optimization (Zinkevich, 2003).
We now bound the first term on the right hand side of (26). For this purpose, consider
the following:
kwt+1  w k2 = kwt + t (yt , yt )  w k2
= kwt  w k2 + t2 k(yt , yt )k2 + 2t (wt  w )> (yt , yt ).

(27)

Rearranging terms in the above equation, we get:
1
kwt  w k2 
2t
1

kwt  w k2 
2t

(wt  w )> (yt , yt ) =

1
t
kwt+1  w k2 + k(yt , yt )k2
2t
2
1
kwt+1  w k2 + 2t R2
2t

where, on the last line, we used the fact that kwt+1  w k2  kwt+1  w k2 since the wt+1
is just the projection of wt+1 to the convex set B (which contains the vector w ). We can
bound the first term in (26) using the following telescoping argument.

T 
X
1
1
2
2
2
kwt  w k 
kwt+1  w k + 2t R
2t
2t
t=1

T 
T
X
X
1
1
1
2
kw1  w k +

kwt  w k2 + 2R2
t

21
2t 2t1
t=2
t=1

T 
X

1
1
1

|B| +

|B| + 2R2 (2 T  1)
21
2t 2t1
t=2


T +1

|B| + 4R2 T .
2
In the above, we obtained the second line by simply rearranging the terms in the expression
above.
line, we used the boundedness property of the set B, as well as the

PTOn the third
fact t=1
 t  2 T  1. The final line follows from cancelling out terms and the fact that
T = 1/ T .
34

fiCoactive Learning

Now, consider the third term on the right hand side of (26):

w> (yt , yt )
+
+
 w> (yt , yt ) + t  t .




The first inequality above follows from -informative feedback. Whereas the second inequal

ity follows from the fact w> (yt , yP
t )  0 from the definition of yt . Finally, the bound (16)
+
G
follows from the trivial fact 0   tI t .
To obtain the bound on the expected regret, consider the convex loss at step t conditioned on user behavior so far:


wt> (yt , yt )
 Et ct



 >

>
Et [w (yt , yt )  t ]
wt (yt , yt )
ct
 Et ct




 >
 >

w (yt , yt )  t
wt (yt , yt )
Et ct
 Et ct



+
>
>

 GEt [wt (yt , yt ) + t  w (yt , yt )]/ t  I


GEt [wt> (yt , yt ) + t+ ]/
tJ
ct (w> (yt , yt ))



where the second line follows from the definition of expected -informative feedback and the
third line follows from Jensens inequality. We obtain the last line following an argument
similar to that in the proof of Theorem 6. The bound follows from an expected version of
(27).

Appendix C. Proof of Theorem 8
Proof First, we divide time steps into two different sets based on the nature of feedback:

I := {t : U (xt , yt )  U (xt , yt ))  0; 1  t  T },
J := {t : U (xt , yt )  U (xt , yt )) < 0; 1  t  T }.
35

fiShivaswamy & Joachims

We start by considering a single time step t, we have:
ct (w> (yt , yt ))  ct (0)
 >

wt (yt , yt )
>

ct (w (yt , yt ))  ct

 >
 >

+
w (yt , yt ) t
wt (yt , yt )
ct
 ct






2

+  >
>
w (yt , yt ) t+
 (w  wt )> (yt , yt ) t+
(w  wt ) (yt , yt ) t
0
ct









2


 




2
+
+
>
>



wt> (yt ,yt )
t ,yt )

tI
+ t  w (y
 2 (w wt ) (yt ,yt )  t
G






(28)





+
+ 2
> (y ,y )
> (y ,y )



w
(w
w
)

t
t

t
t t
t

+ t  2
 t
t  J.
 G


w> (y ,y )

In the above inequalities, the second line follows from the fact that t  t t  0 and ct () is
non-increasing. The third line follows from the fact that the function ct () is non-increasing
and the following inequality which follows from the definition of t+ :
U (xt , yt )  U (xt , yt ) + (U (xt , yt )  U (xt , yt ))  t+ .
The fourth line follows by strong convexity. The last line follows from a same line of
reasoning as in the proof of Theorem 6.
Now consider the last term in both the cases:


2
=


2

=


2




2




2

2
(w  wt )> (yt , yt ) t+





2
2
(w  wt )> (yt , yt )
t+
t+ (w  wt )> (yt , yt )

+

22
2


2
2
(w  wt )> (yt , yt )
 + w> (yt , yt ) t+ wt> (yt , yt ) t+
+ t  2




2
22

2


2
(w  wt )> (yt , yt )
t+
t+
t+
>

+
w (yt , yt ) +




22


2
2
(w  wt )> (yt , yt )
+
+  t 2.
(29)

2


In the above equations, the second and the third lines follow from simple algebraic expansion
of the expression on the first line. The fourth line follows from the definition of -informative
feedback and the fact that wt> (yt , yt )  0. The last line follows from the fact that
w> (yt , yt )  0 from the definition of yt .
36

fiCoactive Learning

Now, summing the terms in (28) and then substituting the above bound, we get,

T
X

ct (w> (yt , yt )) 

t=1

T
X

ct (0)

t=1

2 !
X w> (yt , yt )  PT  + 2
(w  wt )> (yt , yt )

t=1 t
G
+
G


22
t=1
tI
!


2
T
>
>
X (wt  w ) (yt , yt )  (w  wt ) (yt , yt )
G


2

t=1
P
P
2
 Tt=1 t+
G Tt=1 t+
GX >
+
w (yt , yt ) +
+

22

tJ
T 
2   PT  + 2 2G PT  +
GX

>
>
t=1 t
t=1 t

(wt  w ) (yt , yt ) 
(wt  w ) (yt , yt )
+
.
+

2
22

T
X

wt> (yt , yt ) t+ 
+



2



t=1

>
P
t ,yt )
In the above, we obtained the third inequality by adding and subtracting G tJ w (y
.

2
To obtain the last line, we used the fact that 1/  1/ since   (0,
we
P 1]). Finally,
> (y , y )
used an argument similar to that in the proof of theorem 6 to bound G
w
t
t

tJ

and obtained a factor of two with the sum of slacks term. From this point, we use arguments
similar to those from online convex optimization with strongly convex losses (Hazan et al.,
2007).

Next, we consider (wt+1  w )> At (wt+1  w ) and express it interms of wt and At1 :

(wt+1  w )> At (wt+1  w )
1
>
=(wt  A1
t (yt , yt )  w ) At (wt  At (yt , yt )  w )
>
=(wt  w )> At (wt  w ) + (yt , yt )> A1
t (yt , yt )  2(wt  w ) (yt , yt )

=(wt  w )> (yt , yt )(yt , yt )> (wt  w ) + (wt  w )> At1 (wt  w )
>
+ (yt , yt )> A1
t (yt , yt ) + 2(w  wt ) (yt , yt )

Rearranging terms in the above equation, we get:


(wt  w )> (yt , yt )(yt , yt )> (wt  w )
2
(wt  w )> At1 (wt  w )  (wt+1  w )> At (wt+1  w ) + (yt , yt )> A1
t (yt , yt ).
2(wt  w )> (yt , yt ) 

37

fiShivaswamy & Joachims

We now identify that the term on the left hand side in the inequality occurs in the
expression that we would like to bound in (29). We therefore have,
2



T 
X

t=1
T
X

(wt  w )> (yt , yt )  ((wt  w )> (yt , yt ))2




(wt  w )> At1 (wt  w )  (wt+1  w )> At (wt+1  w ) + (yt , yt )> A1
t ( byt , yt )

t=1

(w1  w )> A0 (w1  w ) +

T
X

(yt , yt )> A1
t (yt , yt )

t=1

|B| +

N
log




4R2 T 



+1 .

P
2
In the above, we have used the fact that Tt=1 (yt , yt )> A1
t (yt , yt )  N log(4R T /+1),
where N is the dimens ionality of (x, y) and R is an upper bound on the norm of the joint
feature maps (i.e. k(x, y)k`2  R. A proof of this fact can be found in Hazan et al. (2007).

References
Auer, P., Cesa-Bianchi, N., & Fischer, P. (2002a). Finite-time analysis of the multiarmed
bandit problem. Machine Learning, 47 (2-3), 235256.
Auer, P., Cesa-Bianchi, N., Freund, Y., & Schapire, R. (2002b). The non-stochastic multiarmed bandit problem. SIAM Journal on Computing, 32 (1), 4877.
Bakir, G. H., Hofmann, T., Scholkopf, B., Smola, A., Taskar, B., & Vishwanathan, S. (Eds.).
(2007). Predicting Structured Data. The MIT Press.
Bell, R. M., & Koren, Y. (2007). Scalable collaborative filtering with jointly derived neighborhood interpolation weights. In ICDM.
Boley, M., Mampaey, M., Kang, B., Tokmakov, P., & Wrobel, S. (2013). One click mining:
Interactive local pattern discovery through implicit preference and performance learning. In Proceedings of the ACM SIGKDD Workshop on Interactive Data Exploration
and Analytics, pp. 2735.
Cesa-Bianchi, N., & Lugosi, G. (2006a). Prediction, learning, and games. Cambridge University Press.
Cesa-Bianchi, N., & Lugosi, G. (2006b). Prediction, Learning, and Games. Cambridge
University Press, Cambridge, UK.
Chapelle, O., & Chang, Y. (2011). Yahoo! learning to rank challenge overview. JMLR Proceedings Track, 14, 124.
Chu, W., & Ghahramani, Z. (2005). Preference learning with gaussian processes. In ICML.
Crammer, K., & Singer, Y. (2001). Pranking with ranking. In NIPS.
38

fiCoactive Learning

Crammer, K., & Gentile, C. (2011). Multiclass classification with bandit feedback using
adaptive regularization. In Proceedings of the 28th International Conference on Machine Learning (ICML).
Dekel, O., Gilad-Bachrach, R., Shamir, O., & Xiao, L. (2012). Optimal distributed online
prediction using mini-batches. JMLR, 13, 165202.
Flaxman, A., Kalai, A. T., & McMahan, H. B. (2005). Online convex optimization in the
bandit setting: gradient descent without a gradient. In SODA.
Freund, Y., Iyer, R. D., Schapire, R. E., & Singer, Y. (2003). An efficient boosting algorithm
for combining preferences. Journal of Machine Learning Research, 4, 933969.
Goetschalckx, R., Fern, A., & Tadepalli, P. (2014). Coactive learning for locally optimal
problem solving.. In Conference of the American Association for Artificial Intelligence
(AAAI), pp. 18241830.
Haddow, B., Arun, A., & Koehn, P. (2011). Samplerank training for phrase-based machine
translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation,
pp. 261271, Edinburgh, Scotland. Association for Computational Linguistics.
Hazan, E., Agarwal, A., & Kale, S. (2007). Logarithmic regret algorithms for online convex
optimization. Machine Learning, 69 (2-3), 169192.
Herbrich, R., Graepel, T., & Obermayer, K. (2000). Large margin rank boundaries for
ordinal regression. In Advances in Large Margin Classifiers. MIT Press.
Jain, A., Wojcik, B., Joachims, T., & Saxena, A. (2013). Learning trajectory preferences for
manipulators via iterative improvement. In Neural Information Processing Systems
(NIPS), pp. 575583.
Joachims, T. (2002). Optimizing search engines using clickthrough data. In ACM SIGKDD
Conference on Knowledge Discovery and Data Mining (KDD), pp. 133142.
Joachims, T., Granka, L., Pan, B., Hembrooke, H., Radlinski, F., & Gay, G. (2007). Evaluating the accuracy of implicit feedback from clicks and query reformulations in web
search. ACM Transactions on Information Systems (TOIS), 25 (2).
Jones, R., & Klinkner, K. (2008). Beyond the session timeout: automatic hierarchical
segmentation of search topics in query logs. In CIKM.
Kakade, S. M., Shalev-Shwartz, S., & Tewari, A. (2008). Efficient bandit algorithms for
online multiclass prediction. In Proceedings of the 25th International Conference on
Machine Learning (ICML).
Kivinen, J., & Warmuth, M. (1997). Exponentiated gradient versus gradient gradient descent for linear predictors. Journal of Information and Computation, 132 (1), 164.
Langford, J., & Zhang, T. (2007). The epoch-greedy algorithm for multi-armed bandits
with side information. In NIPS.
Liu, T.-Y. (2009). Learning to rank for information retrieval. Foundations and Trends in
Information Retrieval, 3.
Manning, C., Raghavan, P., & Schutze, H. (2008). Introduction to Information Retrieval.
Cambridge University Press.
39

fiShivaswamy & Joachims

Novikoff, A. (1962). On convergence proofs on perceptrons. In Proceedings of the Symposium
on the Mathematical Theory of Automata, Vol. XII, pp. 615622.
Polyak, B., & Tsypkin, Y. (1973). Pseudogradient adaptation and training algorithms.
Automatic Remote Control, 12, 8394.
Radlinski, F., Kurup, M., & Joachims, T. (2008). How does clickthrough data reflect retrieval quality?. In Conference on Information and Knowledge Management (CIKM).
Raman, K., & Joachims, T. (2013). Learning socially optimal information systems from
egoistic users. In European Conference on Machine Learning (ECML), pp. 128144.
Raman, K., Joachims, T., Shivaswamy, P., & Schnabel, T. (2013). Stable coactive learning
via perturbation. In International Conference on Machine Learning (ICML), pp.
837845.
Raman, K., Shivaswamy, P., & Joachims, T. (2012). Online learning to diversify from
implicit feedback. In KDD.
Shivaswamy, P., & Joachims, T. (2012). Online structured prediction via coactive learning.
In ICML.
Somers, T., & Hollinger, G. (2014). Coactive learning with a human expert for robotic
monitoring. In RSS Workshop on Robotic Monitoring.
Weston, J., Bengio, S., & Usunier, N. (2011). Wsabie: Scaling up to large vocabulary
image annotation. In Proceedings of the International Joint Conference on Artificial
Intelligence (IJCAI).
Yue, Y., Broder, J., Kleinberg, R., & Joachims, T. (2009). The k-armed dueling bandits
problem. In COLT.
Yue, Y., & Joachims, T. (2009). Interactively optimizing information retrieval systems as
a dueling bandits problem. In ICML.
Zhang, Y., Lei, T., Barzilay, R., Jaakkola, T., & Globerson, A. (2014). Steps to excellence: Simple inference with refined scoring of dependency trees. In Proceedings of
the 52nd Annual Meeting of the Association for Computational Linguistics (Volume
1: Long Papers), pp. 197207, Baltimore, Maryland. Association for Computational
Linguistics.
Zinkevich, M. (2003). Online convex programming and generalized infinitesimal gradient
ascent. In ICML.

40

fiJournal of Artificial Intelligence Research 53 (2015) 721-744

Submitted 9/14; published 8/15

Mechanisms for Multi-unit Combinatorial
Auctions with a Few Distinct Goods
Piotr Krysta

p.krysta@liverpool.ac.uk

Deparment of Computer Science
University of Liverpool, United Kingdom

Orestis Telelis

telelis@gmail.com

Department of Digital Systems
University of Piraeus, Greece

Carmine Ventre

c.ventre@tees.ac.uk

School of Computing
Teesside University, United Kingdom

Abstract
We design and analyze deterministic truthful approximation mechanisms for multiunit Combinatorial Auctions involving only a constant number of distinct goods, each
in arbitrary limited supply. Prospective buyers (bidders) have preferences over multisets
of items, i.e., for more than one unit per distinct good. Our objective is to determine
allocations of multisets that maximize the Social Welfare. Our main results are for multiminded and submodular bidders. In the first setting each bidder has a positive value for
being allocated one multiset from a prespecified demand set of alternatives. In the second
setting each bidder is associated to a submodular valuation function that defines his value
for the multiset he is allocated. For multi-minded bidders, we design a truthful Fptas
that fully optimizes the Social Welfare, while violating the supply constraints on goods
within factor (1 + ), for any fixed  > 0 (i.e., the approximation applies to the constraints
and not to the Social Welfare). This result is best possible, in that full optimization is
impossible without violating the supply constraints. For submodular bidders, we obtain a
Ptas that approximates the optimum Social Welfare within factor (1 + ), for any fixed
 > 0, without violating the supply constraints. This result is best possible as well. Our
allocation algorithms are Maximal-in-Range and yield truthful mechanisms, when paired
with Vickrey-Clarke-Groves payments.

1. Introduction
In this paper we study the design and analysis of truthful multi-unit Combinatorial Auctions, for a constant number of distinct goods, each in limited supply. Arguably, the most
widespread modern application of this general setting is the allocation of radio spectrum
licences (Milgrom, 2004); each such license is for the use of a specific frequency band of electromagnetic spectrum, within a certain geographic area. In the design of such Spectrum
Auctions, licenses for the same area are considered as identical units of a single good (the
area), while the number of distinct geographic areas is, of course, bounded by a constant.
More formally, we consider the problem of auctioning (allocating) in one go multiple
units of each out of a constant number of distinct goods, to prospective buyers with private
multi-demand combinatorial valuation functions, so as to maximize the Social Welfare. A
c
2015
AI Access Foundation. All rights reserved.

fiKrysta, Telelis, & Ventre

multi-demand buyer in this setting may have distinct positive values for distinct multisets of
goods, i.e., for each such multiset they may demand more than one unit per good. Our aim
is to devise deterministic truthful auction mechanisms, wherein every bidder finds it to his
best interest to reveal his value truthfully for each multiset of items (i.e., truthful report of
valuation functions is a dominant strategy). Additionally, we are interested in mechanisms
that can compute an approximately efficient allocation in polynomial time. This problem generalizes simultaneously Combinatorial Auctions of multiple goods and Multi-unit
Auctions of a single good to the multi-unit and combinatorial settings respectively.
Since the work of Lehmann, OCallaghan, and Shoham (2002), Mechanism Design for
Combinatorial Auctions of multiple heterogeneous goods (each in unitary supply) has received significant attention in recent years (Holzman, Kfir-Dahav, Monderer, & Tennenholtz, 2004; Lehmann, Lehmann, & Nisan, 2006; Dobzinski, Nisan, & Schapira, 2010; Lavi
& Swamy, 2011), due to their various applications, especially in online trading systems
over the Internet. A mechanism elicits bids from interested buyers, so as to determine an
assignment of bundles to them and payments in such a way, that it is to each bidders best
interest to reveal his valuation function truthfully to the mechanism. This line of research,
that introduces algorithmic efficiency considerations in the design of truthful mechanisms,
was initialized by the work of Nisan and Ronen (2001).
The related problem of auctioning multiple  say s  units of a single good to multidemand bidders has already been considered by Vickrey in his seminal paper (Vickrey,
1961). For bidders with submodular private valuation functions, Vickrey gave an extension
of his celebrated single-item Second-Price Auction mechanism, that retains truthful revelation of valuation functions as a (weakly) dominant strategy for bidders and fully optimizes
the Social Welfare. The only drawback of this mechanism is that it is computationally
efficient only for a few (constant number of) units, in that the allocation algorithm must
process (s) bids in at least as many steps, whereas because s is an input number, it should
require a number of steps bounded by a polynomial in log s. Several other drawbacks of
the generalized Vickrey-Clarke-Groves (truthful) auction mechanism have been identified
by Ausubel and Milgrom (2010). Polynomial-time approximation mechanisms for multiunit auctions were designed relatively recently (Mualem & Nisan, 2002, 2008; Dobzinski &
Nisan, 2010; Vocking, 2012; Nisan, 2014). In particular, Nisan (2014) devised a deterministic, polynomial time auction mechanism, for the multi-unit setting with submodular bidders
first considered by Vickrey (1961). Vocking designed and analyzed recently a randomized
universally truthful polynomial-time approximation scheme, for bidders with unrestricted
valuation functions (Vocking, 2012).
Results for the more general setting of multi-unit Combinatorial Auctions are relatively
scarcer (Bartal, Gonen, & Nisan, 2003; Grandoni, Krysta, Leonardi, & Ventre, 2014; Lavi
& Swamy, 2011). It is exactly this setting we consider here, with a constant number of
distinct goods, similarly to the setting considered by Grandoni et al. (2014); in particular,
for a number of cases of such auctions we analyze Maximal-in-Range (MiR) allocation
algorithms (Nisan & Ronen, 2007), that can be paired with the Vickrey-Clarke-Groves
payment rule, so as to yield truthful mechanisms.
722

fiMechanisms for Multi-unit Combinatorial Auctions

1.1 Contribution
Our main results concern multi-unit Combinatorial Auctions with a constant number of
distinct goods for two broad classes of bidders, as specified by their associated valuation
functions:
1. Multi-minded Bidders: in this setting each bidder is associated with a demand set of
alternative multisets (the multiple minds). Each bidders valuation function assigns a
(possibly distinct) positive value for every alternative in the demand set (and at least
as much for the value of every superset of the alternative) and zero elsewhere.
2. Submodular Bidders: in this setting the value of each bidder for a particular multiset
of items is given by a submodular valuation function.
For multi-minded bidders we design and analyze in Section 4 a truthful Fptas1 , that
fully optimizes the Social Welfare in polynomial time, while violating the supply constraints
on the goods by a factor at most (1 + ), for any fixed  > 0. The violation of the supply
constraints has a practical as well as a theoretical justification. On one hand it is conceivable
that, in certain environments, a slight augmentation of supply can be economically viable,
for the sake of better solutions (e.g., auctioneers with well supplied stocks can easily handle
occurrences of modest overselling). On the other hand, we note that a relaxation of the
supply constraints is necessary for obtaining an Fptas, as the problem is otherwise strongly
NP-hard, for m  2 goods (please see the related discussion in Section 4). This result
significantly improves upon an Fptas by Grandoni et al. (2014), which approximates the
Social Welfare and the supply constraints within factor2 (1+), only when bidders are singleparameter (i.e., associate the same positive value with each multiset from their demand set)
and do not overbid their demands. Technically, the Fptas of Grandoni et al. (2014) is based
on the design of monotone algorithms (Lehmann et al., 2002; Briest, Krysta, & Vocking,
2011) and it requires a no-overbidding assumption on the demands (cf. discussion therein).
In Section 5 we revisit the general technique introduced by Dobzinski and Nisan (2010),
for multi-unit auction Mechanism Design, and generalize it for the setting of multiple distinct goods, each in limited supply. We discuss how this generalization yields a truthful
Ptas immediately for multi-minded bidders, that does not violate the supply constraints
and approximates the Social Welfare within factor (1+), for any fixed  > 0. Subsequently,
we use the technique to design a truthful Ptas for bidders with submodular valuation functions, assuming that the values (bids) are accessed through value queries by the algorithm.
Prior to this result, no time-efficient deterministic truthful mechanism was known for submodular bidders, even when m  2.3 Although the technique of Dobzinski and Nisan
facilitated the development of a factor 2 approximation mechanism for bidders with general
valuation functions in the single-good multi-unit setting, its direct extension for the setting
of multiple distinct goods does not appear to work (for general valuation functions). We
1. Fully Polynomial Time Approximation Scheme.
2. In the context of Social Welfare maximization, by approximation within factor   1 (or, equivalently,
-approximation, for   1) we mean recovering at least a fraction 1 of the welfare of an optimum
allocation. We switch temporarily to using   1 in Section 5, for technical convenience.
3. Nisan (2014) devised an optimal polynomial time such auction for m = 1, i.e., a single good.

723

fiKrysta, Telelis, & Ventre

show, however, that an appropriate extension of a more dedicated treatment of this case
by Dobzinski and Nisan yields a constant (m + 1)-approximation (Section 6).
The assumption of a constant number m = O(1) of distinct goods is important, for
otherwise our problems become Combinatorial Auctions, thus, hard to approximate in

polynomial time within less than O( m) (Lehmann et al., 2002) for multi-minded bide
ders and within less than e1
for submodular bidders (Khot, Lipton, Markakis, & Mehta,
2008; Mirrokni, Schapira, & Vondrak, 2008). Moreover, recent results of Daniely, Schapira,
and Shahaf (2015) imply that, for unrestricted m, our techniques cannot yield truthful

polynomial-time mechanisms with approximation factors less than O(m) and O( m), respectively. Regarding the generalization of the Dobzinski-Nisan technique, existence of an
Fptas for multi-minded bidders and a single good is excluded, unless P = NP (Dobzinski
& Nisan, 2010). These lower bounds imply that our results are best possible. Finally, as
shown by Nisan and Segal (2006) and Dobzinski and Nisan (2010), regarding general valuation functions, no deterministic MiR algorithm achieves better than 2-approximation for
a single good  with communication complexity o(s), where s is the supply of this good.
Closing the gap between this lower bound and our upper bound of (m + 1) for a constant
number m of multiple distinct goods, remains an open problem.

2. Related Work
Mechanism Design for multi-unit auctions was initiated already by the celebrated work of
Vickrey (1961), where he extended his famous mechanism for the case of multiple units,
when bidders have symmetric submodular valuation functions (Lehmann et al., 2006). This
mechanism is however not computationally efficient with respect to the number of available
units, as we already discussed. It requires that bidders place a marginal bid per additional
unit they wish to receive and the allocation algorithm processes all these marginal bids. Very
recently, Nisan (2014) exhibited a polynomial-time truthful mechanism for this case. The
design of multi-unit mechanisms with polynomially bounded running time in log s, s denoting the number of units, was first considered by Mualem and Nisan (2008). In this work,
Mualem and Nisan designed and analyzed a truthful polynomial-time 2-approximation
mechanism for a multi-unit combinatorial setting, involving multiple distinct goods, each in
limited supply, and single-minded bidders. Subsequently, Archer, Papadimitriou, Talwar,
and Tardos (2003) improved upon this approximation ratio for a similar setting, but the
developed mechanism was based on randomized rounding and was truthful only in expectation. More recently, Briest et al. (2011) designed and analysed an Fptas, for single-minded
bidders in the multi-unit combinatorial setting.
Dobzinski and Nisan (2010) analyzed a general scheme for designing MiR polynomialtime truthful approximation mechanisms, for single-good multi-unit auctions. This resulted
in a Ptas for the case of multi-minded bidders, a 2-approximation for general valuation
functions that are accessed (by the allocation algorithm) through value queries, and a
4
3 -approximation for symmetric subadditive valuation functions. Moreover, the authors
applied their scheme to a class of piecewise linear (multi-unit) valuation functions over the
number of units of a single good, to obtain a truthful Ptas mechanism. A special case
of this class of valuation functions had been earlier studied by Kothari, Parkes, and Suri
(2005); the authors designed an Fptas mechanism that was, however, only approximately
724

fiMechanisms for Multi-unit Combinatorial Auctions

truthful. Dobzinski and Dughmi (2013) gave a randomized truthful in expectation Fptas
for multi-minded bidders. Relatively recently, Vocking (2012) gave a universally truthful
randomized Ptas for general valuation functions accessed by value queries (in contrast, all
of our mechanisms are deterministic). For the multi-unit combinatorial setting (i.e., with
more than one distinct goods) the known results concern mainly bidders that have demands
for a single unit from each good (Lehmann et al., 2002; Briest et al., 2011; Blumrosen &
Nisan, 2007). In contrast, we consider a constant number of goods, but multi-demand
bidders. Bartal et al. (2003) proved approximation and competitiveness results for truthful
multi-unit Combinatorial Auctions with multi-demand bidders, where the bidders demands
on numbers of units are upper and lower bounded. The derived approximation guarantees
depend on these bounds. Lavi and Swamy (2011) improved upon these approximation
guarantees, by devising randomized mechanisms that are truthful in expectation.
The study of a constant number of goods, each in arbitrary limited supply, was initiated
by Grandoni et al. (2014). The authors utilized methods from multi-objective optimization
(approximate Pareto curves and Langrangian relaxation) to design and analyze truthful
polynomial-time approximation schemes for a variety of settings. In particular, they devised truthful Fptases that approximate both the objective function (Social Welfare or
Cost) of multi-capacitated versions of problems within factor (1 + ), while violating the
capacity constraints by a factor (1 + ) (capacity here corresponds to limited supply of each
out of a few distinct goods). Problems considered by Grandoni et al. include multi-unit
auctions, minimum spanning tree, shortest path, maximum (perfect) matching and matroid
intersection; for a subclass of these problems a truthful Ptas is also analyzed, which does
not violate any of the capacity constraints.
Of particular interest to the practice of Combinatorial Auctions (also in the multi-unit
case, where each good is available in a limited supply of identical copies), is the efficient
and (near-)optimal resolution of the Winner Determination problem (Lehmann, Muller, &
Sandholm, 2010). Given as input a set of bids in a prespecified format (formally termed
language), for the items on sale, the Winner Determination problem prescribes the determination of a feasible allocation of the items to the bidders, so that the sum of their bids
corresponding to their received allocation is maximized. Thus, the Winner Determination
problem implicitly prescribes determination of winning bidders and their receiving allocation, so that the revenue collected by the corresponding bids is maximized. Notice that, in
comparison to our work, truthful report of the bidders valuation functions is not a concern
in this setting. A significant volume of research has concerned the study of approximation
algorithms and derivation of hardness results (see, e.g., Lehmann et al., 2010), as much as
the development of global optimization techniques (Sandholm, 2010). Kelly (2004) studies
Multi-unit Combinatorial Auctions with only a few distinct goods, for determining the allocation of computational resources. In particular, he devises an optimal algorithm for the
Winner Determination problem, in a low-dimensional setting as ours.
Finally let us mention that the work of Bikhchandani, de Vries, Schummer, and Vohra
(2011) investigates multi-unit Combinatorial Auction premises very similar to ours, without
a restriction on the number of distinct goods. Instead, the authors devise an ascending price
auction for selling subsets of goods that constitute bases of a matroid, or polymatroid, in case
of multi-unit demand by the bidders and limited supply for each distinct good. Their auction
is truthful and runs in polynomial or pseudopolynomial time, respectively. It accesses
725

fiKrysta, Telelis, & Ventre

the bidders combinatorial valuation functions through Demand Queries; the bidders are
presented with prices on the goods and announce the subset they are willing to pay for. In
comparison, all of our mechanisms use Value Queries, where the mechanism asks for the
value of each bidder for a particular set of items. Value queries are a weaker device in
that they can be simulated by (but cannot generally simulate) demand queries (Blumrosen
& Nisan, 2007).

3. Definitions
Let [m] = {1, . . . , m} be a set of m goods, where m is assumed to be a fixed constant. There
are s`  N units (copies) of good `  [m] available. A multiset of goods is denoted by a vector
x = (x(1), x(2), . . . , x(m)), where x(`) is the number of units of good `  [m], ` = 1, . . . , m.
The set of all multisets is denoted by U = m
`=1 {0, 1, . . . , s` }. Let [n] = {1, . . . , n} be the
set of n agents (prospective buyers/bidders). Every bidder i  [n] has a private valuation
function
vi : U 7 R+ ,
so that vi (x) for any x  U denotes the maximum monetary amount that i is willing to
pay for x  U, referred to as his value for x. The valuation functions are normalized, i.e.,
vi (0, . . . , 0) = 0 and assumed to be monotone non-decreasing: for any two multisets x  y
where  holds component-wise, we assume vi (x)  vi (y). That is, in auction theory
terms, we assume free disposal (i.e., enlarging the set or increasing the number of items in
an allocation never decreases the value incurred to any bidder).
A mechanism consists of an allocation method (algorithm), A, and a payment rule, p.
The allocation method A elicits bids b = ( b1 , b2 , . . . , bn ) from the bidders that, presumably, describe their valuation functions and outputs an allocation A(b) = (x1 , x2 , . . . , xn ),
where xi  U is the multiset of goods allocated to bidder i. For the purposes of our discussion in this section, we deliberately ignore the fact that the bidders valuation functions may
not have a succinct representation that will facilitate their efficient communication to the
allocation algorithm; recall that the bidders valuation functions are  generally  defined
over U = m
`=1 {0, 1, . . . , s` }. When they do not have a succinct representation indeed, the
allocation algorithms discussed in this paper access the bidders valuation functions iteratively, through polynomially many Value Queries; that is, the algorithm in each iteration
asks every bidder for a bid on a specific multiset of items.
The payment rule determines a vector p(b) = ( p1 (b), p2 (b), . . . , pn (b) ), where pi (b)
is the payment of bidder i. Every bidder i bids so as to maximize his quasi-linear utility,
defined as:
ui (b) = vi ( A(b) )  pi (b),
where, by an assumption of no externalities, i.e., that the value of any bidder for A(b)
depends only on his own individual allocation and not at all on the others, we obtain
vi ( A(b) ) = vi (xi ).
We study truthful mechanisms (A, p) wherein each bidder i maximizes his utility by
reporting his valuation function truthfully, i.e., by bidding bi = vi , independently of the
other bidders reports, bi = (b1 , . . . , bi1 , bi+1 , . . . , bn ):
726

fiMechanisms for Multi-unit Combinatorial Auctions

Definition 1 A mechanism (A, p) is truthful if, for every bidder i and bidding profile bi ,
it satisfies ui (vi , bi )  ui (vi0 , bi ), for every vi0 .
Under this definition, the profile b = v is a dominant strategy equilibrium. Our objective
is to design and analyze truthful mechanisms, (A, p) that render truthful reporting of the
bidders valuation a dominant strategy equilibrium, wherein, the Social Welfare of the
resulting allocation, SW ( A(b) ) = SW ( A(v) ) is (approximately) optimized. The social
welfare of an allocation, X = (x1 , x2 , . . . , xn ) is defined as:
SW (X) =

n
X

vi (xi ),

i=1

In the sequel we will use simply X, for an allocation output by A, without a specific reference
to b, since we analyze truthful mechanisms, that dictate b = v.
The only well understood general method for the design of truthful mechanisms is the
Vickrey-Clarke-Groves (VCG) auction mechanism (Vickrey, 1961; Clarke, 1971; Groves,
1973), a generalization of Vickreys Single-Item 2nd Price and Multi-unit Auctions (Vickrey,
1961). Deployment of the VCG auction, however, requires utilization of an allocation
algorithm, A, that outputs a welfare-maximizing allocation for the underlying setting; it
rarely constitutes a computationally efficient alternative for combinatorial settings, as the
underlying optimization problem is NP-hard.
As the problems that we consider in our work are indeed NP-hard, our mechanisms use
Maximal-in-Range (MiR) allocation algorithms (Nisan & Ronen, 2007), that maximize the
social welfare only approximately.
Definition 2 (Nisan & Ronen, 2007) An algorithm choosing its output from the set A of
all possible allocations is MiR, if it fully optimizes the Social Welfare over a subset R  A
of allocations.
Note that the subset R, also called a range, is defined independently from the bidders declarations. Nisan and Ronen (2007) identified MiR allocation algorithms as the sole device
that, along with VCG payments, yields truthful mechanisms for Combinatorial Auctions.
In particular, given any MiR allocation algorithm, A, using this algorithm for computing
the output allocation and for computing payments in the manner of the VCG payments
scheme, suffices to obtain a truthful mechanism. In particular, given any such MiR allocation algorithm, the payment for each bidder i is computed as follows:
pi (b) =

X

vi0 ( A(bi ) ) 

i0 6=i

X

vi0 ( A(b) )

i0 6=i

Notice how this payment scheme coincides with the VCG payment scheme, once we use the
optimal allocation algorithm in place of A. The starting point for the work of Nisan and
Ronen (2007) is the pair of observations that: (i) the VCG mechanism itself requires full
optimization of the social welfare of the underlying setting, which is an NP-hard problem in
most interesting settings (ii) VCG-based mechanisms (wherein a polynomial-time allocation
algorithm outputs welfare-suboptimal allocations) are not necessarily truthful.
727

fiKrysta, Telelis, & Ventre

4. Multi-minded Bidders
In this section we consider multi-minded bidders; every such bidder i  [n] is associated
with a collection of multisets Di  U, referred to as his demand set. We assume that each
i  [n] values each multiset d = (d(1), . . . , d(m))  Di by an amount vi (d) > 0. For every
other multiset e  U \ Di we define:

fi
n
o
 max vi (d) fifi d  e
if such d  Di exists
dDi
vi (e) =

0
otherwise.
Naturally, vi (0) = 0, where 0 = (0, . . . , 0). Consequently, in this setting, the valuation
function of a bidder i can be compactly expressed as the collection (vi (d), d)dDi . As in
related literature, we assume therefore that an algorithm expects input bids of this form,
rather than (an oracle representing) the entire valuation function. We say that a bidder
i is a winner of the auction, if he is assigned exactly one of his alternatives from Di (or
a superset of one of these alternatives); this corresponds to the XOR-bidding language in
Combinatorial Auctions (Lehmann et al., 2006).
We design a Fptas, that maximizes the Social Welfare and may violate the supply
constraints on goods by a factor at most (1 + ), for any fixed  > 0. This will be the
allocation algorithm of our mechanism. After analyzing its performance with respect to
the welfare optimality of the allocation that it outputs and the bounded violation of the
supply constraints, we will prove that it is a MiR algorithm, thus can be paired with VCG
payments, to yield a truthful mechanism. At a high level, the algorithm is reminiscent of
the one that yields the Fptas for the well-known one-dimensional knapsack problem (see
e.g., Vazirani, 2003, ch. 8). It proceeds as follows. For any chosen fixed  > 0, it first
discards any alternatives of bidders from their demand sets, that cannot be satisfied, given
the supply constraints. These alternatives are multisets that already exceed the supply
of at least one good. Subsequently, the quantities of goods in the multisets remaining
within the bidders demand sets are appropriately rounded; the supply is adjusted as well.
We thus obtain a rounded instance. Then, we search for a welfare maximizing allocation
of the rounded instance, by usage of dynamic programming. This allocation is shown to
be optimal for the initial instance, as well, and feasible, modulo a violation of the initial
supply constraints within a factor of at most (1 + ). In light of turning this algorithm into
a truthful mechanism, we use notation of actual valuation functions in its definition and
analysis below.
Fix any constant  > 0. First, for any i  [n], remove all the alternatives d  Di such
that d(`) > s` for any ` = 1, . . . , m (if all alternatives of some bidder i are removed, remove
i). Henceforth, we use the same notation, U, [n], Di , etc., for the remaining alternatives
and bidders. The demands of the alternatives d  Di of each i  [n] are rounded as follows.
For every i  [n] and for every d  Di , we produce a multiset d0 = (d0 (1), . . . , d0 (m)) so
that, for each distinct good `  [m], we have d0 (`) = b nd(`)
s` c. Then we adapt the supply of
n
0
each good appropriately, to s` = d  e. Given this rounded version of the problem instance,
we will use dynamic programming to produce an allocation for it, which will immediately
translate into an allocation for the original problem instance, that is welfare-optimal and
violates the (original) supply constraints by a factor at most (1 + ). For the purposes of
the description that follows, we denote by d0 the rounded version of a demand d.
728

fiMechanisms for Multi-unit Combinatorial Auctions

We define the dynamic programming table V(i, Y1 , . . . Ym ) for i = 1, . . . , n and Y` 
{0, 1, 2, . . . , s0` } for any `  [m]. The cell V(i, Y1 , . . . , Ym ) stores the maximum welfare of an
P
nx (`)
allocation X, i.e., j vj (xj ), whose rounded version X0 = (b sj` c)j,` uses only multisets
that are in the demand sets of the bidders
P in {1, 2, . . . , i}, and has total demand w.r.t. good
` = 1, . . . , m which is precisely Y` , i.e., i x0i (`) = Y` .
To compute the entries of table V, we observe that, the problem V(1, Y1 , . . . Ym ) for
any collection of Y` s such that: (Y1 , . . . , Ym )  {0, 1, . . . , d n e}m , is easy to solve. For
each such entry V(1, Y1 , . . . Ym ) we check if bidder 1 has an alternative d  D1 such that
d0 (`) = Y` , for all `  [m]. If yes, let d be an alternative of maximum valuation; we
assign V(1, Y1 , . . . , Ym ) = v1 (d) and build an auxiliary table A[1, Y1 , . . . Ym ] which we set in
this case to {(1, d)}. Otherwise, if bidder 1 does not have any such alternative, we assign
V(1, Y1 , . . . Ym ) = 0 and A[1, Y1 , . . . Ym ] = {(1, 0)}. To define V(i + 1, Y1 , . . . , Ym ), consider
bidder i + 1 and his alternatives d = (d(1), . . . , d(m))  Di+1 ; let now
i+1

n

fi
o
fi
0
0
= max vi+1 (d) + V i, Y1  d (1), ..., Ym  d (m) fi d0  Y
dDi+1

(1)

where, for all i, we define V(i, Y1 , . . . Ym ) =  and, accordingly, A[i, Y1 , . . . Ym ] = { (i, 0) },
if there is no demand d  Di satisfying d0  Y. Consequently:
n
o
V(i + 1, Y1 , . . . , Ym ) = max i+1 , V(i, Y1 , . . . Ym ) .
Accordingly, if i+1  V(i, Y1 , . . . Ym ), we set:
A[i + 1, Y1 , . . . , Ym ] = A[i, Y1 , . . . , Ym ]  {(i + 1, 0)},
otherwise:
A[i + 1, Y1 , . . . , Ym ] = A[i, Y1  d0 (1), . . . , Ym  d0 (m)]  {(i + 1, d)},
where d is an alternative in Di+1 maximizing (1). Finally, we inspect all the solutions
from entries V(n, Y1 , . . . , Ym ) for all vectors (Y1 , . . . , Ym )  {0, 1, . . . , d n e}m , take one which
maximizes the Social Welfare and output the solution given by the corresponding entry of
the A table.
The size of table V is n(d n e+1)m and we need time roughly O(maxi |Di |+m) to compute
one entry of the table, so the overall time of the algorithm leads to an Fptas. The optimality
with respect to the sum of the bidders values is easy to verify. Let X = (x1 , x2 , . . . , xn )
denote any feasible allocation to the original problem instance. For
every good, ` = 1, . . . , m,
P j xi (`)n k  n 
P
P xi (`)n
n
we have: i xi (`)  s` , or, equivalently, i s`   , thus i s`   = s0` . That
is, X is also feasible for the rounded problem instance. Because the dynamic programming
algorithm will inspect all feasible solutions to the rounded problem instance and output the
one with the largest welfare for it, an optimum solution to the original problem instance
will be inspected as well.
We argue that the supply constraints s` , ` = 1, . . . , m, are violated by at most a factor
of 1 + 2. Fix any good `  {1, . . . , m} and let X be the output allocation, with respect
729

fiKrysta, Telelis, & Ventre

to the original problem instance. Because X was chosen by the algorithm by means of dynamic programming search over the rounded
problem instance, it is feasible for the rounded
P j nxi (`) k
 s0` = d n e and, since:
problem instance. Thus, we have:
i
s`
X n  xi (`)
i

  s`



i


we obtain:

P

i xi (`)

X  n  xi (`) 
lnm


  s`
+n

+ |{i|xi  Di }|

n
+ 1 + n,


 (1 + 2)s` .

Example Let us illustrate the algorithms functionality through a very simple example.
Consider n = 3 bidders and m = 2 distinct goods. Let the supplies of goods be s1 = s2 = 4.
The bidders values and demand sets are defined as follows:
Bidder
Valuation Function
Demand Set
1
v1 ( (3, 4) ) = 1 v1 ( (4, 3) ) = 2 D1 = { (3, 4), (4, 3) }
2
v2 ( (3, 3) ) = 3
D2 = { (3, 3) }
3
v3 ( (2, 3) ) = 4 v3 ( (3, 2) ) = 5 D3 = { (2, 3), (3, 2) }
In this example it is evident that all feasible allocations involve assignment of a demand to
a single bidder, given that the supplies of both goods are 4. Thus, the optimal allocation
X is ( 0, 0, (3, 2) ). Consider the rounded problem for  = 2. The rounded supply for
each of the two goods is dn/e = d4/2e = 2. The rounded demands of the bidders are
as follows:
Bidder
1
2
3
Demands (3, 4) (4, 3) (3, 3) (2, 3) (3, 2)
Rounded Demands (1, 1) (1, 1) (1, 1) (0, 1) (1, 0)
Observe that both demands of bidder 1 are rounded to (1, 1). This does not pose any
problem, as the algorithm processes the original demands, and only uses their rounded
versions to validate feasibility of the allocation it builds with respect to the rounded supply.
In this example, because the rounded supply of each good is 2, the algorithm will output
the allocation X = ( 0, (3, 3), (3, 2) ), which has welfare 8 and is superoptimal for the
inital instance. Although the rounded versions of the allocated demands do not violate the
rounded supplies of the goods (equal to 2), they do violate the original supplies of 4, by less
than a factor of 1 +  = 3 (particularly in this example, by no more than a factor of 1.5).
Note that the algorithm is exact, in that it grants every bidder a multiset from his
demand set (or none). Assuming m = O(1) is essential for the result, even in presence of
the supply constraints relaxation. A proof of this claim is given at the end of this section.
The truthfulness of the Fptas, denoted by A below, follows from the fact that it optimizes
over a fixed range of solutions.
Theorem 1 There exists a truthful Fptas for the multi-unit combinatorial auction problem
with a fixed number of goods, when bidders have private multi-minded valuation functions,
defined, for each bidder, over a private collection of multisets of goods. For a fixed  > 0, the
Fptas fully optimizes the social welfare, while violating the supplies of goods within factor
at most (1 + ).
730

fiMechanisms for Multi-unit Combinatorial Auctions

Proof. To prove the theorem we show that A is MiR with range R = {X|b : A(b) =
X}. That is, for any allocation X  R and bid vector
 b, we show SW (A(b), b) 
SW (X, b), where for a bid vector b = (bi (d), d)dDi
and an allocation X  R,
iN

we let SW (X, b) be the Social
P Welfare of allocation X, evaluated according to the bid
vector b, i.e., SW (X, b) = i bi (X).


Fix allocation X and bid vector b = (bi (d), d)dDi
; by definition of the range,
iN


such that A(b) = X. Recall that
there exists a bid vector b, with b = (bi (d), d)dDi
iN

xi (`), for bidder i and ` = 1, . . . , m, is the variable indicating how many copies of item
`, the allocation X grants to bidder i. Note that because X = A(b) and A grants only
demanded alternatives (by its exactness), there exists a demand di  Di  {0} such that,
for ` = 1, . . . , m, xi (`)j= di (`).
Since X is output of A, by definition of A we have that for
P ndi (`) k  n 
any ` = 1, . . . , m, i s`   .
Now let C be the set of bidders such that b = (bC , bC ) and b = (bC , bC ), that is,
b and b only differ in the bids of bidders in the set C. For all bidders i  C we assume
that their true valuation function is bi . Any such bidder i evaluates the alternative xi = di
granted to him by allocation X as some ei  Di  {0}. That is, vi (di ) = vi (ei ). Assume,
for the sake of contradiction, that SW (X, b) > SW (A(b), b), i.e.:
X

bi (ei ) +

X

bj (X) >

j6C

iC

X

bi (A(b)) +

X

bj (A(b)).

(2)

j6C

iC

Since di (`)  ei (`) for ` = 1, . . . , m and i  C, then by setting ei = di for i 6 C, we obtain:
X  n  ei (`) 
i

  s`



X  n  di (`) 
i

  s`



lnm


,

for ` = 1, . . . , m. Then the solution which grants to bidder i the alternative ei  Di  {}
is considered by algorithm A on input b. This solution has Social Welfare SW (X, b) and
therefore (2) is in contradiction with the definition of A.
2
A related result of Briest et al. (2011) is a truthful Fptas for a single good in limited
(not violated) supply; this cannot be generalized for our setting of more than one supply
constraints.
4.1 A Note on Hardness
Note that this problem is strongly NP-hard, when we do not allow to violate supply constraints and m  2 (Chekuri & Khanna, 2005). It is well known that if a problem is strongly
NP-hard, there does not exist any FPTAS for this problem, unless P=NP (see, e.g., Vazirani, 2003). Also the assumption that m is a fixed constant is necessary. Otherwise the
problem is equivalent to multi-unit Combinatorial Auctions and is hard to approximate in
polynomial time within m1/2 , for any  > 0 (Lehmann et al., 2002). This claim is true,
even if we allow for solutions to violate the supplies. In particular:
731

fiKrysta, Telelis, & Ventre

Proposition 1 In a multi-unit combinatorial auction with m distinct goods, it is NPhard to approximate the Social Welfare within factor better than m1/2 , even if we allow a
multiplicative (1 + )-relaxation of the supply constraints, for any  < 1.
Proof. The argument is as follows: it is known that it is hard to approximate the maximum
independent set problem in a graph G = (V, E) within a factor m1/2 for any  > 0, where
|E| = m (Hastad, 1996). By using a reduction of Lehmann et al. (2002), we reduce this
problem to our problem by having the set of goods [m] = E and the set of single-minded
bidders V ; each bidders u  V set contains all edges adjacent to u in the graph G and
each bidders valuation for his set is 1. Now if we allow to violate the supply of 1 of each
good by a factor of 1 + , where  < 1, then a feasible solution to the relaxed problem is
an independent set in graph G. Thus the relaxed problem is equivalent to the maximum
independent set problem in G.
2
4.2 Multi-dimensional Knapsack
We discuss an application of our Fptas, in relation to the Multi-dimensional Knapsack
Problem (MdKP) (Chekuri & Khanna, 2005). Suppose we are given a MdKP instance,
with a constant number of distinct compartments, m = O(1), and each compartment,
` = 1, 2, . . . , m, has capacity s` . The problem asks to fit in the knapsack a subset out of
a universe, U, of n given m-dimensional objects, so that the sum of the collected objects
sizes in each dimension, `, does not exceed s` and the total value of all collected objects
is maximized. Each object, i = 1, . . . , n of the MdKP instance can be represented by a
vector di , which represents its m dimensions, (di (1), . . . , di (m)) and its value, vi , so that
hvi , di i  ( (R+  {0})  U ). Then, each object corresponds to a single bidder i, from the
setting that we analyzed in Section 4, with valuation function vi (d)  vi , for every d  di ,
vi (d) = 0, for every d such that d(`) < di (`), for some ` = 1, . . . , m. Notice that the bidder
is single-parameter, in that his valuation function takes on a single non-zero value for every
d  di and his demand set, Di , is singleton, i.e., it contains a single multiset, di . Thus, the
MdKP corresponds to a single-parameter version of the problem we treated above.
We can apply our Fptas to the MdKP, because the algorithm is exact, as mentioned
previously, in that it allocates every bidder (read as: fits in the knapsack) either an exact
alternative from his demand set, Di , or none. It is worth mentioning that for this singleparameter version, our Fptas from Section 4 can be shown to be monotone (Lehmann et al.,
2002; Briest et al., 2011), when one carefully fixes a tie-breaking rule. A monotone allocation
algorithm ensures that: if a (single-parameter) bidder i is allocated his single demand di
when he declares truthfully hvi , di i, he also receives his (declared) demand d0i , when he
declares hvi0 , d0i i, with vi0  vi and d0i  di (i.e., intuitively, asks for less items while offering
more money). An exact and monotone allocation algorithm can yield a truthful mechanism
for this single-parameter setting, with the incorporation of critical value payments  see the
work of Lehmann et al. (2002) for details.
Let us note that, we can generalize the MdKP further, in the following manner. Instead
of having only packing constraints (of the form ) on the dimensions of the knapsack, we
can handle any mix of packing and covering constraints (i.e., of any of the forms {, }),
as long as there is only a constant number of dimensions, m = O(1), and one covering
or packing constraint per dimension. For such a generalized scenario we can follow an
732

fiMechanisms for Multi-unit Combinatorial Auctions

approach similar to our approach in Section 4 and obtain a truthful Fptas which fully
optimizes the total value of fitted items and violates each of the constraints by a factor at
most (1 + ). Violation of the constraints is needed for the reason mentioned above, in our
note on computational hardness, in the end of Section 4.

5. The Generalized Dobzinski-Nisan Method
We discuss here a direct generalization of a method designed by Dobzinski and Nisan (2010),
for truthful single-good multi-unit auction mechanisms. We will use the methods generalization for multiple goods in the next subsection, to obtain a truthful Ptas for bidders
with submodular valuation functions (over multisets). Let A be a polynomial-time MiR
allocation algorithm for t = O(1) bidders and s` units from each good ` = 1, . . . , m, with
time complexity TA (t, s), s = (s1 , . . . , sm ), and approximation ratio   1. Then, algorithm
A can be used as a routine within the procedure of Figure 1, to obtain a polynomial-time
m
MiR algorithm for n bidders, with approximation ratio (  t+1
).
Given t = O(1), the procedure executes algorithm A on every subset of at most t bidders
and for every combination of certain pre-specified quantities of the goods. For each output
allocation it considers the rest of the bidders and allocates optimally to them an integral
number of (multi-unit) bundles from each good. The main result shown by Dobzinski and
Nisan (2010) for a single good can be also proved for m goods:
Theorem 2 Let A be a Maximal-in-Range algorithm with complexity TA (t, (s1 , . . . , sm )),
for t bidders and at most s` units from each good ` = 1, . . . , m. The Dobzinski-Nisan Method
is MiR and runs in time polynomial in log s1 , . . . , log sm , n, TA (t, (s1 , . . . , sm )), for every
m
t = O(1). Moreover, it outputs an allocation with value at least a fraction (  t+1
) of the
optimum Social Welfare.
The proof is a direct extension of the proof given by Dobzinski and Nisan (2010) for a single
good. Consider the MiR algorithm A, to be used within the Dobzinski-Nisan method; it
executes in polynomial time for t = O(1) bidders and m = O(1) distinct goods, each in
limited supply s` , `  [m]. Let RA denote the range of this algorithm. It can be verified
that the method outputs allocations that are (R, t, 1 , . . . , m )-round, given the following
definition for such round allocations (Dobzinski & Nisan, 2010):
Definition 3 For some t = O(1), an allocation is (R, t, 1 , . . . , m )-round if:
 R is a set of allocations and, in each X  R, at most t bidders are allocated nonempty bundles. The bidders are allocated together up to s`  ` units from each good
` = 1, . . . , m.
 There exists a set T of |T |  t bidders, such that they are all allocated according to
some allocation in R.
 `
	
 Each P
bidder i  [n] \ T receives an exact
multiple of max b 2n
units from good `
2 c, 1
	
`
and:
x
(`)

n

max
b
c,
1
,
for
`
=
1,
.
.
.
,
m
i[n]\T i
2n2

733

fiKrysta, Telelis, & Ventre

1. for ` = 1, . . . , m do:
1
(a) define u` := (1 + 2n
)


 
blogu s` c
2
`
(b) define L` := 0, 1, bu` c, bu` c, . . . , u`
, s`

2. for every subset T  [n] of bidders, |T |  t, do:


1. for every (1 , . . . , m )  m
L
`=1 ` do:
1 Run A with s`  ` units from each good `  [m] and bidders in T .
2 Split the remaining ` units (if ` > 0) fromeach good
	 `  [m]
`
c,
1
units.
into  2n2 bundles (per good), each of max b 2n
2
3 Find the optimal allocation of the equi-sized bundles among bidders [n] \ T .
3. Return the best allocation found.
Figure 1: The Dobzinski-Nisan Method for multiple goods.
In this definition, R corresponds to the range of A, parameterized by the subset of
bidders T , i.e., R = RA (T ), on which it is executed. Then, for some t = O(1), the range of
the method is the subset of all allocations that are (RA (T ), , 1 , . . . , m )-round, so that:
(1 , . . . , m )  (m
`=1 L` ), where L` is defined as in step 1.(b) of the method in Figure 1, and
T  [n], with  = |T |  t. Formally, the methods range RDN is the subset of allocations:
o
n fi
fi
L
)
and

=
|T
|

t
RDN = X fi X is (RA (T ), , 1 , . . . , m )-round, for (` )`  (m
`=1 `
Example  Part (I) Before continuing to analyze the methods range, let us exemplify
the concept of (R, t, 1 , . . . , m )-round allocations. We will consider a small instance of
multi-minded bidders, similar to the considered in the previous section. As we will argue
later, the Dobzinski-Nisan method can yield a truthful Ptas (that respects the supply
constraints on goods), for multi-minded bidders. Assume m = 2 distinct goods, and n = 5
bidders. We assume supplies s1 = 200 = s2 for the goods. The bidders demands are as
follows:
Bidder
Demand Set
1
D1 = { (75, 51), (49, 73) }
2
D2 = { (51, 27), (25, 49) }
3
D3 = { (48, 1) }
4
D4 = { (1, 1) }
D5 = { (1, 48) }
5
Let us exhibit a round allocation for this instance, according to Definition 3. For t = 2
and 1 = 2 = 100, consider first the allocation:
X = ( (75, 51), (25, 49), (48, 2), (2, 2), 0 ),
734

fiMechanisms for Multi-unit Combinatorial Auctions

where x1 = (75, 51), x2 = (25, 49), x3 = (48, 2), x4 = (2, 2), x5 = 0. This allocation is
(R, 2, 100, 100)-round, according to Definition 3, where R denotes the subset of allocations
with at most 2 bidders receiving non-empty multisets and the remaining ones receiving appropriate multi-unit bundles per good. Indeed, we can set T = {1, 2} (for the corresponding
subset of at most 2 bidders); each of bidders 1 and 2 obtains one of his demands. The total
number of units allocated to these two bidders per good is exactly 100 = s`  ` . For the
remaining 100 = ` units from each good, we make 50 bundles of ` /(2n2 ) = 2 units per
bundle. Bidder 3 receives 24 such 2-units bundles from good 1 and one 2-units bundle from
good 2. Bidder 4 obtains one 2-units bundle from each good. Finally, bidder 5 receives an
empty allocation. Notice that x3 and x4 essentially satisfy the unique demands (48, 1) and
(1, 1) of bidders 3 and 4 respectively.
Another (R, 2, 100, 100)-round allocation (according to Definition 3) is:
X0 = ( (75, 51), (52, 28), (48, 2), 0, 0 ),
where the required subset T of bidders is T = {1}. Bidder 1 obtains one of his demands;
bidders 2 and 3 receive 2-units bundles from each good; and bidders 4 and 5 receive empty
allocations (i.e., zero 2-units bundles from each good). Notice that X0 is also (R, 1, 100, 100)round (i.e., when we set t = 1). Now let us choose algorithm A for the Dobzinski-Nisan
method, to be an exhaustive search procedure, that optimizes the welfare (thus, has approximation ratio  = 1). Notice that the range RA (T ) of A, for the chosen values of
1 , 2 , trivially contains the allocation that bidders 1 and 2 receive in X (when T = {1, 2})
and the allocation of bidder 1 under X0 (when T = {1, 2} or T = {1}); this is because A
optimizes over all feasible allocations up to supplies 200  1 = 100 and 200  2 = 100 for
each of the two choices of T . Thus, the allocations X and X0 also belong in the range RDN ,
as defined above.
We show that optimization over RDN approximates the socially optimal allocation within
m
factor (  t+1
).
Lemma 1 Let X = (x1 , . . . , xn ) be a socially optimal allocation. There exists an allocation
m
X  RDN with SW (X)  (  t+1
)  SW (X ).
Proof. In the proof we make use of notation L` and u` , as defined in Figure 1. Without
loss of generality (because of monotonicity of valuation functions), assume that all units of
all goods are allocated in X and that v1 (x1 )  v2 (x2 )    P
 vn (xn ). For every good
` = 1, . . . , m choose the largest value `  L` so that s`  `  ti=1 xi (`). When executed
on the subset of bidders T = {1, . . . , t} with s` P
` units from good
P ` = 1, . . . , m, algorithm
A outputs an allocation (x1 , . . . , xt ) such that ti=1 vi (xi )   ti=1 vi (xi ).
Now consider for each good ` = 1, . . . , m a bidder jP
`  {t + 1, . . . , n} with the maximum

number of units in X from this good. Define r` = ni=t+1 xi (`). Then xj` (`)  rn` . By
definition of r` and ` for each good `, we have r`  ` . Also, because ` was chosen
Pt to have
2
the largest possible value in L` = { 0, 1, bu` c, bu` c, . . . , s` } satisfying s`  ` + i=1 xi (`),
r`
. For every bidder i  t + 1 with i 6= j` for ` = 1, . . . , m, we
it must be `  ur``  r`  2n
 `
	
round up his allocation with respect to good ` to a multiple of max b 2n
2 c, 1 . The extra
units for each good ` we take from bidders j` who may not obtain any unit of the good.
735

fiKrysta, Telelis, & Ventre

`
`
Observe that we may need to add at most n  2n
2  2n extra units from each good `, that
`
r`
we take from bidder j` , who has at least n  n units.
Thus, for all bidders except for j` , ` = 1, . . . , m we increased the units of P
goods they
t
1

obtain. Because j`  t + 1 and v1 (x1 )      vn (xn ), we have vj` (xj` )  t+1
i=1 vi (xi )
and vi (xi )  vi (xi ) for i 6= j` , ` = 1, . . . m. Then:

SW (X) =

X

vi (xi )  

i



t
X

=

X

X
t
i=1

vi (xi )

X

vi (xi )

it+1

vi (xi ) 

it+1

m

t+1

vi (xi ) +

i=1

vi (xi ) +

i=1



t
X

m
X

vi (xj` )

`=1

+

X

vi (xi )




it+1

m

t+1



SW (X )
2

which concludes the proof.
The lemma completes the proof of Theorem 2.

Example  Part (II) We revisit the example discussed right before the statement and
proof of Lemma 1, in order to exemplify the approximation implied by the Lemma. To
this end, we assign values to the bidders demands as described in the following table,
where v > 0 is a very small positive number and V >> v is a very large one. As before,
s1 = s2 = 200.
Bidder
Valuation Function
1
v1 ( (75, 51) ) = v v1 ( (49, 73) ) = V
2
v2 ( (51, 27) ) = V v2 ( (25, 49) ) = v
3
v3 ( (48, 1) ) = V
4
v4 ( (1, 1) ) = v
5
v5 ( (1, 48) ) = V

D1
D2
D3
D4
D5

=
=
=
=
=

Demand Set
{ (75, 51), (49, 73) }
{ (51, 27), (25, 49) }
{ (48, 1) }
{ (1, 1) }
{ (1, 48) }

The following socially optimal allocation X for this instance has welfare 4V + v:
X = ( (49, 73), (51, 27), (48, 1), (1, 1), (1, 48) )
By choosing t = 2 and T = {1, 2}, we can exhibit the welfare-approximate allocation
implied by Lemma 1, as follows. The maximum value possible for each of 1 and 2
satisfying s`  `  x1 (`) + x2 (`) is 100 = 1 = 2 . From the remaining bidders, bidder
3 has the maximum number of units from good 1 in X and bidder 5 has the maximum
number of units from good 2 in X . Thus, we have j1 = 3 and j2 = 5. Then, we round up
the allocations of bidders 3 and 4 w.r.t. good 2, to one 2-units bundle (for each of them),
by taking two units from bidder j2 = 5. Accordingly, we round up the allocations of bidders
4 and 5 w.r.t. good 1, by taking two units from bidder j1 = 3. The resulting allocation is:
X = ( (49, 73), (51, 27), (46, 2), (2, 2), (2, 46) )
and has welfare 2V + v, which approaches half of the optimal welfare (as v becomes vanishingly small). Lemma 1 for this example guarantees at least 1/3 of the optimal welfare, if
736

fiMechanisms for Multi-unit Combinatorial Auctions

the algorithm A used within the Dobzinski-Nisan method is a welfare-optimizing exhaustive
search procedure. On the other hand notice that, for this particular example and t = 2,
1 = 2 = 100, the allocation Y = ( (49, 73), (51, 27), (48, 2), 0, (2, 48) ) has almost optimal welfare, 4V , and is round according to Definition 3. Thus, the Dobzinski-Nisan
method will examine Y and it will return an allocation at least as good.
Let us explain how to find an optimal allocation of multi-unit bundles of goods (i.e.,
bundles of identical units) to bidders in [n] \ T , in step 2.1.3 of the algorithm (Figure 1).
We use dynamic programming. By re-indexing the bidders appropriately, assume that
T = {n  t + 1, . . . ,n}, thus [n]\ T = {1, . . . , n  t}. For every i = 1, . . . , n  t and for every
2
q = (q1 , . . . , qm )  m
i=1 [2n ] , define V(i, q) = V(i, (q1 , . . . , qm )) to be the maximum value
of welfare that can be obtained by allocating at most q` equi-sized bundles (of units) from
each good ` = 1, . . . , m to bidders 1, . . . , i. Each entry V(i, q) of the dynamic programming
table can be computed using:


0
0
0
V(i, q) = max
v
(q

b
,
.
.
.
,
q

b
)
+
V(i

1,
q

q
)
,
i
1
m
1
m
0
q q

where q0  q is taken component-wise; i.e., maximization occurs over all vectors q0 such
that q 0 (`)  q(`) for each ` = 1, . . . , m.
5.1 Simple Application: Multi-minded Bidders
The generalized Dobzinski-Nisan method for multiple distinct goods can be applied immediately in the setting of multi-minded bidders, to yield a Ptas that respects fully the supply
constraints of the goods. For m = O(1) goods and for any constant number of t bidders the
optimum assignment can be found exhaustively in polynomial time in log s` , ` = 1, . . . , s,
and m. In particular, if every bidders demand sets contains at most k demands, there are
exactly O(k t ) cases to be examined exhaustively, so that the optimum is found. Plugging
this algorithm in the procedure of Figure 1, yields a Ptas that, complementarily to the developments of the previous section, approximates the optimum Social Welfare within factor
(1 + ) and respects the supply constraints.
5.2 Submodular Valuation Functions
We consider submodular valuation functions over multisets in U, as defined by Kapralov,
Post, and Vondrak (2013):
Definition 4 For any ` = 1, . . . , m let e` be the unary vector with e` (`) = 1 and e` (j) = 0,
for j 6= `. Let x and y denote two multisets from U, so that x  y, where  holds
component-wise. Then, a non-decreasing function v : U 7 R+ is submodular if v(x + e` ) 
v(x)  v(y + e` )  v(y).
We assume that these valuation functions, being exponentially large to describe, are
accessed by the algorithm through value queries; i.e., that the algorithm asks the bidders
for their value, for each particular multiset that it needs to process.
We will design the MiR approximation algorithm A, needed by the method. The range
we consider for this setting is an extension of the one considered by Dobzinski and Nisan
737

fiKrysta, Telelis, & Ventre

(2010). For any  > 0, define  = 1 + ; we will be assigning to bidders multi-unit bundles
of each good `  [m], that have cardinality equal to an integral power of . For every good
`  [m], one of the n bidders (possibly a different bidder per good) will always obtain the
remaining units of the specific good. We show that optimization over this range provides a
good approximation of the unrestricted optimum Social Welfare; also, optimizing over this
range yields a Fptas for a constant number n of bidders. This, used within the generalized
Dobzinksi-Nisan method will yield a Ptas for any number of bidders.
Lemma

m 2 An optimum assignment within the defined range recovers at least a factor
2
of the socially optimal welfare.
2+2
Proof. Let X = (x1 , . . . , xn ) denote the welfare maximizing assignment. We will round
iteratively  for a particular good `  [m] in each iteration  the assignment of units to each
bidder in X , to an integral power of . Let X[`] be the assignment after rounding with
respect to the `-th good. The final assignment X  X[m] will approximate the welfare of
X[0]  X .
In the beginning of `-th iteration we process the assignment X[`1] , by rounding the
[`1]
[`1]
assignment of multi-unit bundles of good `. Assume w.l.o.g. that x1 (`)  x2 (`) 
[`1]
    xn (`). Also w.l.o.g., we assume that every bidder except for bidder 1 receives an
integral power of  units of good `; bidder 1 receives the remaining units. Let the set of
bidders be partitioned as [n] = O  E where O contains the odd indices of bidders and E
the even ones. We will consider two cases:
X  [`1]  X  [`1] 
X  [`1]  X  [`1] 
v i xi

vi x i
and
v i xi
<
vi xi
.
(3)
iO

iE

iO

iE
[`1]

For the first case, for every i  O \ {1} we will round xi
(`) up to the closest integral
[`1]
power of , while obtaining the extra units to do so by rounding xi1 (`), i  1  E, down
[`]

[`1]

to the nearest appropriately chosen integral power of . We obtain xi (`)    xi
[`1]

[`1]

[`1]

xi1 (`) = xi1 (`)  (  1)xi
[`1]

[`1]

(`) and:

[`1]

(`)  xi1 (`)  (  1)xi1 (`)

[`1]

thus, xi1 (`)  (2  )xi1 (`). To ensure that for bidder i  1 we obtain an integral power
[`1]

[`]

[`1]

of , we may need to divide xi1 (`) at most by , thus: xi1 (`)  1 xi1 (`) =
The welfare of the emerging assignment X[`] is:

 X   X   X  
[`]
[`]
[`]
SW X[`] =
vi xi =
v i xi +
vi xi
iO

i[n]



X



[`1]

vi x i



iE

2   X  [`] 
+
v i xi

iE

iO


 X 

2
[`1]
=
vi
+
SW X[`1] 
v i xi

iO
iO


2  2 X  [`1]  2  
v i xi
+
SW X[`1]
=


X



[`1]
xi



iO

738

!

2 [`1]
 xi1 (`).

fiMechanisms for Multi-unit Combinatorial Auctions




 2




1
1
SW X[`1] +
SW X[`1] =
SW X[`1]


1+
[`]

[`1]

The second line follows by submodularity; for any `  [m], we have xi1 (`)  2
 xi1 (`),




[`]
[`1]
2
so vi1 xi1   vi xi1 . For the last inequality, recall that we are examining the
P
[`1]
left-hand side case of (3), thus, we use the that:
)  12 SW (X[`1] ).
iO vi (xi



P
P
[`1]
[`1]
Consider now the second case in (3), where iO vi xi
< iE vi xi
. For
[`1]

i  E \ {2} we round up xi
(`) to the closest integral power of ; the extra units for
[`1]
this we will obtain from i  1  O, by rounding xi1 (`) down to an appropriately chosen
[`1]

closest integral power of . x2 (`) will be rounded down to closest integral power of 
[`1]
[`1]
[`]
(contrary to the rest of xi
(`), i  E), i.e., x2 (`)  1 x2 (`). For i  E \ {2} it will be
[`]

[`1]

xi (`)    xi

(`) and then we take:
[`]

xi1 (`) 

 2
1  [`1]
[`1]
[`1]
xi1 (`)  (  1)xi
(`) 
xi1 (`)



(4)

Then, for the Social Welfare of X[`] we have:
 X   X   X  

[`]
[`]
[`]
v i xi =
v i xi +
v i xi
SW X[`] =
i[n]

iO

iE

 
X
2   X  [`1]  1  [`1] 
[`]

v i xi
+ v 2 x2
+
v i xi


iO
iE\{2}
!
 X 




X
1  [`1] 
2
[`1]
[`1]
SW X[`1] 
v i xi
+ v 2 x2
+
v i xi
=


iE
iE\{2}






X
2
2  2
1
[`1]
[`1]
v2 x2
+
SW X[`1]
=
vi xi
+



iE\{2}


 1 
 2

1 X
[`1]
[`1]
>
vi x i
+
v 2 x2
+
SW X[`1]



iE\{2}

 2




1
2

SW X[`1] +
SW X[`1] =
SW X[`1]
2

2 + 2
The second line of this derivation is again due to submodularity: the factors on the sum
[`1]
[`]
[`1]
over odd-indexed bidders and on v2 (x2 ) follow by (4) and because x2 (`)  1 x2 (`).
For the last inequality, we used the fact that we are examining the right-hand side case
P
[`1]
of (3); then, iE vi ( xi
)  12 SW (X[`1] ).
Thus, for any  > 0, there is an assignment
range that approximates
 within
p  thedescribed
q
2
1
the optimum Social Welfare within factor 2+2  1+ , for some integers p, q, such that
p + q = m. The result follows by

1
1+



2
2+2 .

We obtain the following (intermediate) result:
739

2

fiKrysta, Telelis, & Ventre

Theorem 3 For multi-unit combinatorial auctions with n = O(1) submodular bidders, and
m = O(1) distinct goods, each good `  [m] available in an arbitrary supply, there exists a
truthful deterministic Fptas that, for any   1, approximates the optimum Social Welfare
within factor (1 + ).
Proof. For any fixed  > 0 we can search the specified range exhaustively in polynomial
time; to find the allocation with maximum Social Welfare, we have to try O(log s` ) cases
for each of n  1 bidders, given a fixed bidder for assigning the remaining units. Thus
the time required for trying all possible bundle assignments ofa specific good ` and for
all possible choices of a remainders bidder is O n(log s` )n1 . Because for every fixed
allocation of a specific good we need to try all possible allocations for the remaining m  1
goods, the overall complexity is in total O nm (log max` s` )(n1)m , which is polynomially
bounded for constant m and n. Also notice that, for   1 we obtain a Fptas, because:
log max s` = (log2 (1 + ))1  (log2 max s` )
`

`

and log21 (1 + )  1 .

2

Using Theorem 3 within the general Dobzinski-Nisan method, we obtain:
Corollary 1 There exists a truthful Ptas for multi-unit combinatorial auctions with constant number of distinct goods and submodular valuation functions.

6. General Valuation Functions
Interestingly, the direct generalization of the Dobzinski-Nisan method for a constant number of multiple goods does not immediately yield, for general valuation functions, a result
comparable to the one shown by Dobzinski and Nisan (2010) for a single good; for m = 1 a
truthful 2-approximation mechanism was obtained (and this factor was shown to be optimal). When m = 1, the relevant MiR algorithm A involved in Theorem 2 solves optimally
the case of t = 1 bidder, by allocating all units of all goods to him. The monotonicity of the
valuation functions guarantees that this allocation is optimal for t = 1 bidder. The factor 2
approximation follows. For m > 1 goods however, Theorem 2 appears to require a different
algorithm A (for, possibly, t > 1 bidders), to yield a comparable (constant approximation)
result. Instead, a constant (m+1)-approximation for the case of general valuation functions
accessed by value queries can be obtained, by simple modification of the direct approach
that was given by Dobzinski and Nisan, for general valuation functions.
We describe from scratch an MiR allocation algorithm. The algorithm splits for every
good the number of units into n2 equi-sized bundles of size b` = b ns`2 c; it also creates a single
extra bundle (per distinct good, `), containing the remaining units r` , so that n2 b` +r` = s` .
The algorithm allocates optimally whole bundles of units from each good to the n bidders.
First we show that this range approximates by a factor (m + 1) the optimum Social
Welfare. Let X = (x1 , . . . , xn ) denote the socially optimal allocation. Beginning with
X , we produce an allocation in the range within which the algorithm optimizes, that
approximates SW (X ) within factor (m + 1). Assume w.l.o.g. that all items are allocated
in X (by the monotonicity of valuation functions) and, for each good ` = 1, . . . , m, let
740

fiMechanisms for Multi-unit Combinatorial Auctions

j` = arg maxi xi (`). Then xj` (`) 
here.
Either:

m
X

vj` (xj` )

`=1

m

s`
n.

X

Define L = {j1 , . . . , jm }. We consider two cases

vi (xi ),

i6L

or:

m
X
`=1

vj` (xj` ) < m

X

vi (xi ).

i6L

For the first case, let us denote by Y` = (y1` , y2` , . . . , yn` )  for each ` = 1, . . . , m  the
allocation which assigns all bundles of all goods to bidder j`  L (thus, yi` = (0, . . . , 0), for
every i 6= j` ). Of these m allocations, consider Y = arg maxY` vj` (yj`` ). Then, SW (Y) 
P
1 Pm


i6L vi (xi ). Putting these inequalities together
`=1 vj` (xj` ), thus, also: SW (Y) 
m
1
yields SW (Y)  m+1 SW (X ). Notice that the allocation Y will be examined by the MiR
algorithm. For the second case we build an allocation X, by rounding up  separately for
each good `  the (optimal) allocation of bidders i 6 L to the nearest multiple of b` . The
units needed for this purpose we find  for each good `  from the corresponding bidder
j`  L, who may not obtain any unit in X. This is possible because we add at most n  ns`2 =
s`

X that gives
n  xj` (`) units in total by this rounding. This way we make up an allocation P
all multi-unit bundles of each good to bidders in [n] \ L and satisfies SW (X)  i6L vi (xi ),
1 Pm
1


thus, also: SW (X) > m
`=1 vj` (xj` ). Then, we deduce SW (X)  m+1 SW (X ). Notice
that the allocation X is also examined by the MiR algorithm. Thus, there exists a solution
within the range, that approximates SW (X ) within constant factor, at most (m + 1).
To complete our analysis, we show how to compute a MiR allocation for the described
range, using dynamic programming. Let r = (r1 , . . . , rm ) denote the vector of amounts that
correspond to bundles of remainders per good as described above. Given L  2{1,...,m}
we denote by r[L] the projection of r on indices in L; the remaining coordinates are set
to 0. Let b = (b1 , . . . , bm ). For any subset L  2{1,...,m} , define V L (i, q), q = (q1 , . . . , qm )
as the maximum welfare achievable when allocating at most q` multi-unit bundles for each
good ` = 1, . . . , m among bidders 1, . . . , i and the remainders bundle for each of the goods
`  L. We compute each V L (i, q) as follows:
n
o

0
0
0
L\L0
0
V L (i, q) = max
max
v
(q

b
,
.
.
.
,
q

b
)
+
r[L
]
+
V
(i

1,
q

q
)
i
1
m
1
m
0
0
0 q
L L q1 q1 ,...,qm
m

Because m = O(1), the entries of the dynamic programming table can be computed in
polynomial time. Thus:
Theorem 4 There exists a truthful polynomial-time mechanism for multi-unit Combinatorial Auctions with a constant number of distinct goods, m, and general valuation functions
that, using value queries, approximates the welfare of a socially optimal assignment within
constant factor, (m + 1).

7. Conclusions
In this paper we analyzed deterministic mechanisms for multi-unit Combinatorial Auctions
with a constant number of distinct goods, each in limited supply. We analyzed in particular
Maximal-in-Range allocation algorithms (Nisan & Ronen, 2007), for optimizing the Social
Welfare in this multi-unit combinatorial setting that, paired with VCG payments, yield
741

fiKrysta, Telelis, & Ventre

truthful auctions. Our main results include (i) a truthful Fptas for multi-minded bidders,
that approximates the supply constraints within factor (1 + ) and optimizes the Social
Welfare; (ii) a deterministic truthful Ptas for submodular bidders, that approximates the
Social Welfare within factor (1 + ) without violating the supply constraints. For achieving
(ii), we used a direct generalization of a single-good multi-unit allocation method proposed
by Dobzinski and Nisan (2010). All of the discussed developments are best possible in
terms of time-efficient approximation, as follows by relevant hardness results. Finally, we
showed how to treat general (unrestricted) valuation functions in our setting, by appropriately adjusting an analysis by Dobzinski and Nisan (2010). Closing the gap between a
communication complexity lower bound of 2 (for a single good) of Dobzinski and Nisan and
our (m + 1)-approximation result for m = O(1) goods, requires further understanding of
the communication complexity of our more general setting.

Acknowledgments
We thank three anonymous reviewers for helping us significantly in improving the presentation of this work. We also thank Jinshan Zhang, for pointing out a technical inconsistency
in an earlier version of this paper, and Fabrizio Grandoni and Stefano Leonardi, for very
useful discussions in the early stage of this work.
Piotr Krysta acknowledges support by the EPSRC grant EP/K01000X/1.
Carmine Ventre acknowledges support by the EPSRC grant EP/M018113/1.
Orestis Telelis acknowledges support by the research project DDCOD(PE6-213), implemented within the framework of the Action Supporting Postdoctoral Researchers of
the Operational Program Education and Lifelong Learning (Actions Beneficiary: General Secretariat for Research and Technology), and is co-financed by the European Union
(European Social Fund  ESF) and the Greek State.

References
Archer, A., Papadimitriou, C. H., Talwar, K., & Tardos, E. (2003). An Approximate Truthful Mechanism for Combinatorial Auctions with Single Parameter Agents. Internet
Mathematics, 1 (2), 129150.
Ausubel, L. M., & Milgrom, P. (2010). The Lovely but Lonely Vickrey Auction. In Cramton,
P., Shoham, Y., Smith, V. L., & Steinberg, R. (Eds.), Combinatorial Auctions, pp.
1740. MIT Press.
Bartal, Y., Gonen, R., & Nisan, N. (2003). Incentive Compatible Multi Unit Combinatorial Auctions. In Halpern, J. Y., & Tennenholtz, M. (Eds.), Proceedings of the 9th
Conference on Theoretical Aspects of Rationality and Knowledge (TARK-2003), pp.
7287. ACM.
Bikhchandani, S., de Vries, S., Schummer, J., & Vohra, R. (2011). An Ascending Vickrey
Auction for Selling Bases of a Matroid. Operations Research, 59 (2), 400413.
742

fiMechanisms for Multi-unit Combinatorial Auctions

Blumrosen, L., & Nisan, N. (2007). Combinatorial Auctions. In Nisan, N., Roughgarden,
T., Tardos, E., & Vazirani, V. V. (Eds.), Algorithmic Game Theory, pp. 267299.
Cambridge University Press.
Briest, P., Krysta, P., & Vocking, B. (2011). Approximation Techniques for Utilitarian
Mechanism Design. SIAM Journal on Computing, 40 (6), 15871622.
Chekuri, C., & Khanna, S. (2005). A Polynomial Time Approximation Scheme for the
Multiple Knapsack Problem. SIAM Journal on Computing, 35 (3), 713728.
Clarke, E. (1971). Multipart Pricing of Public Goods. Public Choice, 11 (1), 1733.
Daniely, A., Schapira, M., & Shahaf, G. (2015). Inapproximability of Truthful Mechanisms
via Generalizations of the VC Dimension. In Servedio, R. A., & Rubinfeld, R. (Eds.),
Proceedings of the 47th Annual ACM on Symposium on Theory of Computing, STOC
2015, Portland, OR, USA, June 14-17, 2015, pp. 401408. ACM.
Dobzinski, S., & Dughmi, S. (2013). On the Power of Randomization in Algorithmic Mechanism Design. SIAM Journal on Computing, 42 (6), 22872304.
Dobzinski, S., & Nisan, N. (2010). Mechanisms for Multi-Unit Auctions. Journal of Artificial
Intelligence Research, 37, 8598.
Dobzinski, S., Nisan, N., & Schapira, M. (2010). Approximation Algorithms for Combinatorial Auctions with Complement-Free Bidders. Mathematics of Operations Research,
35 (1), 113.
Grandoni, F., Krysta, P., Leonardi, S., & Ventre, C. (2014). Utilitarian Mechanism Design
for Multi-Objective Optimization. SIAM Journal on Computing, 43 (4), 12631290.
Groves, T. (1973). Incentives in Teams. Econometrica, 41 (4), 617631.
Hastad, J. (1996). Clique is Hard to Approximate Within n1 . In 37th Annual Symposium on Foundations of Computer Science (FOCS96), pp. 627636. IEEE Computer
Society.
Holzman, R., Kfir-Dahav, N. E., Monderer, D., & Tennenholtz, M. (2004). Bundling Equilibrium in Combinatorial Auctions. Games and Economic Behavior, 47 (1), 104123.
Kapralov, M., Post, I., & Vondrak, J. (2013). Online Submodular Welfare Maximization:
Greedy is Optimal. In Khanna, S. (Ed.), Proceedings of the Twenty-Fourth Annual
ACM-SIAM Symposium on Discrete Algorithms (SODA13), pp. 12161225. SIAM.
Kelly, T. (2004). Generalized Knapsack Solvers for Multi-Unit Combinatorial Auctions:
Analysis and Application to Computational Resource Allocation. In Faratin, P., &
Rodrguez-Aguilar, J. A. (Eds.), Agent-Mediated Electronic Commerce VI, Theories
for and Engineering of Distributed Mechanisms and Systems (AAMAS 2004 Workshop, AMEC 2004), Vol. 3435 of LNCS.
Khot, S., Lipton, R. J., Markakis, E., & Mehta, A. (2008). Inapproximability Results for
Combinatorial Auctions with Submodular Utility Functions. Algorithmica, 52 (1),
318.
Kothari, A., Parkes, D. C., & Suri, S. (2005). Approximately-Strategyproof and Tractable
Multiunit Auctions. Decision Support Systems, 39 (1), 105121.
743

fiKrysta, Telelis, & Ventre

Lavi, R., & Swamy, C. (2011). Truthful and Near-Optimal Mechanism Design via Linear
Programming. Journal of the ACM, 58 (6), 25.
Lehmann, B., Lehmann, D. J., & Nisan, N. (2006). Combinatorial Auctions with Decreasing
Marginal Utilities. Games and Economic Behavior, 55 (2), 270296.
Lehmann, D., Muller, R., & Sandholm, T. (2010). The Winner Determination Problem.
In Cramton, P., Shoham, Y., Smith, V. L., & Steinberg, R. (Eds.), Combinatorial
Auctions. MIT Press.
Lehmann, D. J., OCallaghan, L., & Shoham, Y. (2002). Truth Revelation in Approximately
Efficient Combinatorial Auctions. Journal of the ACM, 49 (5), 577602.
Milgrom, P. (2004). Putting Auction Theory to Work. Cambridge University Press.
Mirrokni, V. S., Schapira, M., & Vondrak, J. (2008). Tight Information-Theoretic lower
bounds for Welfare Maximization in Combinatorial Auctions. In Proceedings 9th
ACM Conference on Electronic Commerce (ACM EC), pp. 7077.
Mualem, A., & Nisan, N. (2002). Truthful Approximation Mechanisms for Restricted
Combinatorial Auctions. In Dechter, R., & Sutton, R. S. (Eds.), Proceedings of the
Eighteenth National Conference on Artificial Intelligence and Fourteenth Conference
on Innovative Applications of Artificial Intelligence (AAAI/IAAI 2002), pp. 379384.
AAAI Press / MIT Press.
Mualem, A., & Nisan, N. (2008). Truthful Approximation Mechanisms for Restricted
Combinatorial Auctions. Games and Economic Behavior, 64 (2), 612631.
Nisan, N. (2014). Algorithmic Mechanism Design, Through the Lens of Multi-Unit Auctions.
In Aumann, R., & Hart, S. (Eds.), Handbook of Game Theory, Vol. IV. Elsevier NorthHolland.
Nisan, N., & Ronen, A. (2007). Computationally Feasible VCG Mechanisms. Journal of
Artificial Intelligence Research, 29, 1947.
Nisan, N., & Segal, I. (2006). The Communication Requirements of Efficient Allocations
and Supporting Prices. Journal of Economic Theory, 129 (1), 192224.
Nisan, N., & Ronen, A. (2001). Algorithmic Mechanism Design. Games and Economic
Behavior, 35 (1-2), 166196.
Sandholm, T. (2010). Optimal Winner Determination Algorithms. In Cramton, P., Shoham,
Y., Smith, V. L., & Steinberg, R. (Eds.), Combinatorial Auctions. MIT Press.
Vazirani, V. V. (2003). Approximation Algorithms. Springer-Verlag.
Vickrey, W. (1961). Counterspeculation, Auctions, and Competitive Sealed Tenders. Journal of Finance, 16 (1), 837.
Vocking, B. (2012). A Universally Truthful Approximation Scheme for Multi-Unit Auctions.
In Rabani, Y. (Ed.), Proceedings of the Twenty-Third Annual ACM-SIAM Symposium
on Discrete Algorithms (SODA12), pp. 846855. SIAM.

744

fiJournal of Artificial Intelligence Research 53 (2015) 779-824

Submitted 05/15; published 08/15

Belief Change with Uncertain Action Histories
Aaron Hunter

aaron hunter@bcit.ca

British Columbia Institute of Technology
Burnaby, BC, Canada

James P. Delgrande

jim@cs.sfu.ca

Simon Fraser University
Burnaby, BC, Canada

Abstract
We consider the iterated belief change that occurs following an alternating sequence
of actions and observations. At each instant, an agent has beliefs about the actions that
have occurred as well as beliefs about the resulting state of the world. We represent such
problems by a sequence of ranking functions, so an agent assigns a quantitative plausibility
value to every action and every state at each point in time. The resulting formalism is able
to represent fallible belief, erroneous perception, exogenous actions, and failed actions.
We illustrate that our framework is a generalization of several existing approaches to belief
change, and it appropriately captures the non-elementary interaction between belief update
and belief revision.

1. Introduction
Many formal approaches have been introduced for reasoning about belief change in the context of actions and observations (Jin & Thielscher, 2004; Shapiro, Pagnucco, Lesperance,
& Levesque, 2011; Delgrande & Levesque, 2012). In general, the underlying assumption
is that agents perform belief update following actions and belief revision following observations. Existing formalisms, for the most part, have treated actions and observations
independently, with little explicit discussion about the interaction between the two. In this
paper, we consider the belief change that occurs due to an alternating sequence of actions
and observations. We are interested in action domains where an agent may have erroneous
beliefs, both about the state of the world as well as the action history.
Let K denote the beliefs of an agent, given by a set of possible worlds. For 1  i  n,
let Ai denote an action and let Oi denote an observation. Informally, we are interested in
sequences of the form
K  A1  O1      An  On
(1)
where  is an update operator and  is a revision operator. Our interpretation of this
expression is flexible in that the actions may be understood to represent actions executed
by a particular agent, or they may be exogenous. Note that such sequences may contain
conflicting information. For example, the observation On may not be possible following the
actions A1 , . . . , An . In this case, there are two options.
1. Reject On .
2. Accept On , and modify A1 , . . . , An .
c
2015
AI Access Foundation. All rights reserved.

fiHunter & Delgrande

In order to determine which option is preferable for a specific problem, an agent needs to
be able to compare the plausibility of On with the plausibility of each Ai .
Expressions of the form (1) have previously been addressed, under the assumption that
ontic action histories are infallible and recent observations take precedence over older observations (Hunter & Delgrande, 2011). Clearly, there are action domains in which these
assumptions are not reasonable. In this paper, we propose a more flexible approach in
which actions and observations are both represented by Spohn-style ranking functions. We
take this approach because it gives a uniform treatment of the plausibility of beliefs, observations, and actions. When presented with conflicting information, an agent can simply
compare the relative plausibility of each action and observation; a uniform representation
of all events makes it straightforward to determine the most plausible sequence. Moreover, with quantitative plausibility values, we can encode a variety of distinct scenarios by
manipulating the magnitudes of the plausibility values for alternative events.
This paper makes several contributions to existing work on epistemic action effects.
The main contribution is a formal mechanism for reasoning about incorrect or weakly held
beliefs related to action histories. Existing formalisms are generally unable to compare
the plausibility of an action occurrence with the plausibility of a state of the world. This
is a problem because, in practical reasoning problems, agents are often put in a position
where they can either believe a certain fact holds or they can believe that some action has
occurred. By using the same formal tool to represent beliefs about actions and states, we
explicitly address the manner in which prior action occurrences are postulated or retracted
in response to new observations.
A second contribution of this work is a flexible treatment of weak or unreliable observations. While some revision operators incorporate each new observation, it is obvious that
this is not a desirable feature in many reasoning domains. In many cases, it is preferable to
discard an unreliable observation if it conflicts with current, strongly held beliefs. However,
our approach does not simply allow an unreliable observation to be discarded. Since we
use ranking functions to represent observations, each observation also includes plausible
alternatives. As such, each observation can actually provide evidence of several different
states, and to differing degrees. We will see that this is useful in representing observations
of states with similar appearance. Similarly, ranking functions can give a natural representation of addititive evidence, in the form of observations that must occur several times
before changing an agents beliefs.
We formulate all of our results in a simple transition system framework that makes
our treatment of action effects explicit and easy to compare with more elaborate action
formalisms. However, our fundamental approach to dealing with uncertainty does not
actually require transition systems to be used for action effects. We do not intend for
the methodology described here to compete with alternative formalisms for representing
epistemic action effects; rather, we provide a high-level approach to dealing with uncertain
action histories that focuses specifically on the interaction between actions and observations.
The significant feature of this work is that it demonstrates how a single representation of
plausibility can be used to model iterated beliefs about actions and states simultaneously;
the basic approach could be employed in other action formalisms. At a practical level, we
demonstrate the utility of our work by giving a series of examples involving belief change
in which the relative weight given to actions and observations varies. We suggest that our
780

fiBelief Change with Uncertain Action Histories

work advances existing work on reasoning about epistemic action effects in that we provide
a flexible, elaboration tolerant approach for capturing several different kinds of belief change
that can occur when there is uncertainty about action occurrences. Note that this paper is
an extended version of the work presented by Hunter and Delgrande (2006).
We proceed as follows. In Section 2, we introduce the formal preliminaries. We then
define a general class of plausibility functions in Section 3, and we show how a sequence
of plausibility functions can be used to represent an uncertain sequence of actions and
observations. We refer to such a sequence as a graded world view, and we demonstrate
how an agents beliefs about event histories can captured by graded world views through
taking aggregates of the constituent plausibility functions. In Section 4, we demonstrate
that graded world views can be seen as epistemic states, and we use our basic framework
to define belief change in domains involving uncertainty about the actions and observations
that have occurred. We compare our approach with related work in Section 5, and we
discuss limitations and advantages in Section 6. In Section 7, we offer some concluding
remarks.

2. Preliminaries
In this section, we introduce some standard formal machinery for modelling belief change,
and for reasoning about action effects. We also introduce a simple motivating example to
illustrate the sort of problem we would like to address.
2.1 Belief Revision
Belief revision refers to the process in which an agent incorporates new information along
with some pre-existing beliefs. The most influential approach to belief revision is the AGM
approach (Alchourron, Gardenfors, & Makinson, 1985). Let F denote a finite set of fluent
symbols, which represent binary properties of the world that may change over time. For
example, there may be a fluent symbol Raining  F that is true just in case it is raining.
A state is a propositional interpretation of F, indicating which fluents are true and which
are false. In the AGM approach to belief revision, the beliefs of an agent are represented
by a belief set, which is a deductively closed set of formulas over F. Since F is finite, we
can equivalently define the beliefs of an agent to be represented by a single formula.
Informally, a belief revision operator is a function that takes a belief set  and a formula for revision  as input and returns a new formula that represents a new belief set
incorporating . An AGM revision operator is a binary function  that satisfies the AGM
postulates. The following reformulation of the AGM postulates is due to Katsuno and
Mendelzon (1991). In the postulates,  denotes logical equivalence.
[R1]
[R2]
[R3]
[R4]
[R5]
[R6]

   implies .
If    is satisfiable, then       .
If  is satisfiable, then    is satisfiable.
If 1  2 and 1  2 , then 1  1  2  2 .
(  )   implies   (  ).
If (  )   is satisfiable, then   (  ) implies (  )  .

781

fiHunter & Delgrande

The class of AGM revision operators can be captured semantically by introducing a
formal notion of plausibility. For a particular agent, we say that a state of the world s
is more plausible than another state s0 if that agent is more likely to abandon belief in
s0 when presented with new information. It turns out that every AGM revision operator
can be characterized by a class of plausibility orderings over the set of states. This has
been proved for several different representations of plausibility, including total pre-orders
over states (Katsuno & Mendelzon, 1991), systems of spheres (Grove, 1988), and ordinal
conditional functions (Spohn, 1988).
A belief state is a set of states, informally the set of states that an agent considers
possible. An observation  is also a set of states, which informally represents some peice of
information that an agent receives that provides evidence that the actual state is in . In
this paper, we are primarily interested in belief change as a process on states rather than
formulas; it is clear that the AGM approach can equivalently be defined on belief states.
While we have restricted the set of fluents and the set of actions to be finite, this is
really just for convenience when discussing examples. Our formal model will be based
on transition systems and ranking functions over sets. It would certainly be possible to
define transition relations and ranking functions over infinite sets. Since our definition of
an aggregate is very general, this would introduce no formal complications. However, from
this point on, we will maintain the restriction to finite domains in order to simplify the
discussion.
2.2 Transition Systems
We are interested in action domains that can be described by supplementing the set F of
fluent symbols with a finite set A of action symbols and a transition system describing the
effects of actions (Gelfond & Lifschitz, 1998). A pair (F, A) is called an action signature.
While we have defined a state to be an interpretation over F, it is often convenient to
identify a state s with the set of fluent symbols true in s. We use this convention in the
following definition, and throughout the rest of the paper.
Definition 1 A transition system is a pair hS, Ri where S  2F , R  S  A  S.
We restrict attention to deterministic transition systems, i.e. we assume that hs, A, s0 i  R
and hs, A, s00 i  R implies s0 = s00 . We also assume that A always contains a distinguished
null action symbol denoted by .
Belief update is the belief change that occurs when an agent becomes aware of a change
in the state of the world. Note that this is a distinct process from belief revision, which
is typically understood to capture the belief change that occurs when an agent has obtained new information about an unchanged world. One highly influential approach to
belief update is the Katsuno-Mendelzon approach (Katsuno & Mendelzon, 1992), which
is superficially similar to AGM revision in that the new information to be incorporated
is encoded as a propositional formula. By contrast, in this paper, we define update with
respect to an action with effects given by an underlying transition system. In other words,
we define belief update operators that take a belief state and an action as input, and return
a new belief state that represents an agents new beliefs after the action has been executed.
782

fiBelief Change with Uncertain Action Histories

Definition 2 Let T = hS, Ri be a transition system. The update function  : 2S  A  2S
is given by   A = {s | hs0 , A, si  R for some s0  }.
We remark that this notion of belief change can also be called belief progression or action
progression. Note that it is possible in the general case for the result of this update to be
empty, in the case where the transition system does not include any outgoing edge labelled A
from a state in . In order to avoid this problem, it is often convenient to restrict attention
to transition systems where every action has an outcome in every state. In practice, this
can be achieved by assuming a self-loop for the action A at any state where no outcome
state is given.
2.3 Belief Evolution
In previous work, we introduced so-called belief evolution operators to reason about alternating sequences of updates and revisions (Hunter & Delgrande, 2011). A belief evolution
operator is defined with respect to a fixed AGM revision operator  and a fixed update
operator . For any action A and observation , let 1 (A) denote the set of all s such that
{s}  A  . Note that this set may be empty. The belief evolution operator  is defined
such that, for any belief state ,
  hA, i = (  1 (A))  A.
The complete definition of  actually defines the belief change for a sequence of actions and
observations; but the details of this definition are not required at present. The important
point is that the way each observation is incorporated depends on the preceding actions.
The intuition behind belief evolution is that the final state must be a possible effect of
the most recently executed action A. This intuition is satisfied in the original definition by
revising the initial belief set prior to computing the effects of A. As a result, belief evolution
operators have a non-Markovian character; an observation can not be incorporated just by
considering the current state of the world. Instead, an agent incorporates a new observation
by looking back at the original state together with the complete history of actions. However,
this operation can also be understood in a Markovian manner if we allow the current belief
state to include some representation of the states that can be ruled out based on the
actions that have previously occurred. In this manner, we can define a Markovian form of
belief revision that is equivalent to belief evolution (Hunter, 2014).
Belief evolution provides a reasonable model for problems like Moores litmus paper
problem (Moore, 1985). In this problem, an agent dips a piece of litmus paper in a beaker
to determine if the beaker contains an acid or a base. Hence, the agent is performing an
action first and then observing the results. It seems, however, that the observed results
should affect the agents initial belief state. That is, after the litmus paper turns red, the
agent is likely to conclude that the beaker contained an acid even before dipping.
Belief evolution operators satisfy the following properties. In these properties, A  A,
A denotes a sequence of action symbols of indeterminate length, and 2F denotes the set
of all states over F. The symbols  and  both range over 2F , though we think of  as
a belief state and we think of  as an observation. Following the standard convention for
such postulates, we implicitly quantify universally over all variables.
783

fiHunter & Delgrande

1. If (2F  A)   6= , then (  A)    
2. If (2F  A)   = , then (  A)   =   A
3. (  A)    (  A)  
4. If (  A)   6= , then (  A)    (  A)  
5. (  A)    2F  A
Informally, these properties assert that an agent should respect the history of actions executed when incorporating a new observation. Note that we are assuming that  is an AGM
revision operator, which would appear to make properties (1) and (3) trivial. However,
they are included as properties of belief evolution to emphasize the fact that the actions
preceding the obervation need to be considered in all cases where revision occurs (Hunter
& Delgrande, 2011).
One of the main limitations of belief evolution is that it is not possible to represent
erroneous action histories; it is assumed that the action history is always correct. This is a
reasonable assumption in action domains where there is a single agent executing actions that
can not fail. However, if exogenous or failed actions are permitted, then this assumption is
difficult to support. In general, an agent may have incorrect beliefs about the actions that
have occurred in the past. One of our aims in this paper is to generalize our previous work
to allow erroneous action histories.
2.4 Motivating Example
We introduce a common-sense example in which an agent needs to compare the plausibility
of certain actions with the plausibility of observations. We will return to this example
periodically as we introduce our formal machinery.
Consider a simple action domain involving four agents: Bob, Alice, Eve, and Trent. Bob
places a chocolate chip cookie on his desk and then leaves the room; he believes that no one
is likely to eat his cookie while he is gone. At Time 1, Bob knows that Alice is at his desk.
At Time 2, Bob knows that Eve is at his desk. After Eve leaves his desk, Trent comes and
tells Bob that a bite has been taken from the cookie.
Given the preceding information, Bob can draw three reasonable conclusions: Alice
bit the cookie, Eve bit the cookie, or Trent gave him poor information. If Bob has no
additional information about the world, then each conclusion is equally plausible. However,
we suppose that Bob does have some additional information. In particular, suppose that
Alice is a close friend of Bob and they have shared cookies in the past. Moreover, suppose
that Bob believes that Trent is always honest. Bobs additional information about Alice
and Trent provides a sufficient basis for determining which of the three possible conclusions
is the most plausible.
Informally, prior to Trents report, Bob believes that his cookie was unbitten at all
earlier points in time. After Trent tells him the cookie is bitten, he must determine the most
plausible world history consistent with this information. In this case, the most plausible
solution is to conclude that Alice bit the cookie. Note that this conclusion requires Bob
to alter his subjective view of the action history. There is a non-monotonic character to
784

fiBelief Change with Uncertain Action Histories

belief change in this context, because Bob may be forced to postulate and retract actions
over time in response to new observations. The consequences of changing the action history
are determined by the underlying transition system. In order to represent this kind of
reasoning, we need to be able to compare the plausibility of action occurrences at different
points in time.
This example illustrates the kind of action scenario that we would like to capture,
because it requires an agent to determine the most plausible history given some a priori
notion of plausibility for actions and observations. For instance, it is intuitively clear that
this problem can not be resolved without comparing the plausibility of Trents report being
accurate versus the plausibility of Alice biting the cookie. By addressing this issue in a
straightforward manner, we demonstrate how existing actions formalisms can be extended
and employed to handle similar situations.
There is possibly one contentious issue in our discussion of this example. The information provided by Trent is not really an observation in the usual sense of the word; instead,
it is a report of information from an external source. However, in the sections that follow,
we will treat reports and observations in the same manner. Specifically, we will capture
both by a ranking function over states that indicates which states are supported by the
observation/report and to what degree. Of course, in reality, observations and reports are
quite different in that the manner in which a report is incorporated depends on trust in
the reporting agent. The relationship between trust and belief change is a topic of current
interest, which is generally handled by introducing some extra formal machinery to encode
the trust held in another agent (Lorini, Jiang, & Perrussel, 2014; Hunter & Booth, 2015).
It would certainly be possible to follow this approach in the present paper, using distinct
formal tools to capture trust as a phenomenon that is distinct from the perceived accuracy
of sensing. However, mathematically, we would like to end up with a single ranking over
states nevertheless. As such, for the present paper, it is more convenient to represent reported information through a single combined ranking that captures the final plausibility
attached to each state based on all considerations an agent might make.

3. Ranking Functions over Actions and States
The approach presented in this paper is based on a simple notion: when an agent is uncertain
about some action or observation, resolving this uncertainty generally involves comparing
the relative likelihood of possible alternatives. Our work is distinguished by the fact that we
use sequences of ranking functions to represent uncertainty over both actions and states. We
will see that sequences of ranking functions can capture many natural reasoning problems.
By developing high-level representations of these problems, we can see that some notion of
magnitude of likelihood is useful for reasoning about belief change with uncertain action
histories. This allows us to compare our work with existing formalisms for reasoning about
epistemic action effects, particularly those in which the representation of uncertainty is
limited to orderings over states. Our fundamental goal is to demonstrate that, in some
cases, sequences of quantitative plausibility orderings have a expressive advantage that
is significant from the perspective of knowledge representation. The aim is to develop
our approach at a high-level, in a manner that can easily be translated into other action
formalisms.
785

fiHunter & Delgrande

3.1 Plausibility Functions
We are interested in action domains where an agents beliefs about an action history may
be incorrect. In this context, the action that is believed to be executed at any given point
in time can be represented by a total pre-order over possible actions. The minimal elements
of such a pre-order represent the actions that were most likely executed, and moving higher
in the ordering gives increasingly implausible possibilities. Representing actions in this
manner allows an agent to determine plausible alternative actions in the face of conflicting
evidence. Similarly, an agent needs a mechanism for ordering states in order to represent
fallible observations and fallible beliefs. Moreover, we would like to be able to compare
orderings over actions with orderings over states. One natural way to create mutually
comparable orderings is by assigning quantitative plausibility values to every action and
state at every point in time. Towards this end, we define plausibility functions.
Definition 3 Let X be a non-empty set. A plausibility function over X is a function
r : X  N.
If r is a plausibility function and r(x)  r(y), then we say that x is at least as plausible as
y. We will only be interested in plausibility functions over finite sets, where there is always
a non-empty set of minimally ranked elements.
Plausibility functions are inspired by Spohns ordinal conditional functions (Spohn,
1988), but there are some important differences. First, we allow plausibility functions
over an arbitrary set X, rather than restricting attention to propositional interpretations.
This allows us to treat actions in the same manner that we treat observations. Another
important difference is that ordinal conditional functions must always assign rank 0 to a
non-empty subset of elements of the domain. Plausibility functions are not restricted in this
manner; the minimal rank for a given plausibility function may be greater than 0. We have
defined plausibility functions in this manner because we will be interested in taking sums
over plausibility functions, and we need to ensure that such sums also define plausibility
functions.
We remark that Darwiche and Pearl also consider ranking functions that do not necessarily assign rank 0 to any states (Darwiche & Pearl, 1997). However, Darwiche and Pearl
define the belief state associated with r to be the set of states that are assigned rank 0.
Under this convention, ranking functions that never assign rank 0 are associated with the
empty belief state. By contrast, we associate a non-empty belief state with every plausibility
function.
We introduce some useful terminology and notation. Let r be a plausibility function
over X. The minimum and maximum values obtained by r are denoted by minr and maxr ,
respectively. We define Bel(r) to be the set {s | r(s) = minr }. This notation is intended to
suggest that Bel(r) is the set of actions or states that are believed. To be clear, when we
say that a state s is believed, we mean that as far as the agent is concerned, the actual world
could be described by s. On the other hand, when we say that an action A is believed, we
mean that the agent views A to be the action that was executed. Note that minr is always
defined, because any non-empty set of natural numbers has a minimum element. On the
other hand, maxr is only guaranteed to be defined when X is finite. However, we will be
only concerned with plausibility functions over states and actions; these are both finite sets
according to our original definitions.
786

fiBelief Change with Uncertain Action Histories

For   X, we define r() to be the minimum value obtained by r for some s  . The
degree of strength of a plausibility function r is the least n such that minr +n = r(s) for
some s 6 Bel(r). Hence, the degree of strength of r is the span between the plausibility
of the minimally ranked elements and the non-minimally ranked elements. If r has degree
of strength n, this means that every s 6 Bel(r) has a plausibility value that is at least n
higher than r(Bel(r)). There are two natural interpretations of the degree of strength of a
plausibility function r over a set of states. If we think of r as an initial epistemic state, then
the degree of strength is an indication of how strongly it is believed that the actual state
is in Bel(r). If we think of r as an observation, then the degree of strength is a measure
of the subjective reliability of r. In the case where X is a set of states, we use the terms
degree of strength and degree of belief interchangeably.
The notion of degree of strength is of crucial importance in our approach. If we use
a total pre-order to represent plausibility, then there is really no corresponding notion of
strength to distinguish a situation where the most plausible belief is very strongly held
versus one where it is weakly held. But this kind of distinction is essential when we have
to sort out a sequence of events; we need some notion of strength of belief to decide which
state or action is the hardest to give up. It is possible to represent this kind of information
in an ordering with empty levels, of course. However, our focus will be on aggregating
sequences of belief states and actions. In order to make this as simple as possible, we suggest
that it is better to use the compact representation given by a quantitative ranking function
that lends itself naturally to arithmetical combinations.
Note that Spohn (1988) defines the degree of strength of a subset of X, rather than the
degree of strength of a ranking function. Our definition coincides with Spohns definition
if we identify the degree of strength of r with Spohns degree of strength of the set Bel(r).
Hence, we use the same conception of degree of strength, but we are only interested in the
strength of belief in the minimally ranked elements.
In order to illustrate the application of plausibility functions over different domains, we
continue our simple example.
Example (contd) Let F = {B iteT aken} and let A = {B iteAlice, BiteEve}. Both
actions have the same effect, namely they both make the fluent B iteT aken become true.
We represent the problem with 3 plausibility functions: a1 , a2 , and o2 .
1. a1 is a plausibility function over actions at Time 1
2. a2 is a plausibility function over actions at Time 2
3. o2 is a plausibility function over states at Time 2
Informally, each function should obtain a minimum value at the event that Bob considers
the most plausible at the given point in time. Since Bob initially believes that no one will
eat his cookie, both a1 and a2 should obtain a minimum value at the null action . Trents
report that the cookie has been bitten at Time 2 is represented as a plausibility function
over states, by defining o2 with a minimum at the set of worlds where the cookie has a bite
out of it. Note that we will generally treat reported information in this manner; the degree
of strength of a report is an indication of trust in the agent providing the report. The
787

fiHunter & Delgrande

additional soft constraints about Bobs relationships are used to determine the magnitude
of the values for each event. Define a1 and a2 by the values in the following table.
a1
a2


0
0

B iteAlice
1
10

B iteEve
10
3

The columns of the table give the plausibilities of each action at each time. In particular,
a1 encodes that the plausibility  occurs is 0, whereas the plausibility of B iteAlice and
B iteEve are 1 and 10, respectively. The degree of strength of a1 is 1, whereas the degree
of strength of a2 is 3. The fact that Alice is more likely to bite the cookie is represented by
assigning a low plausibility value to B iteAlice at Time 1.
We define o2 as follows, so give the plausibility values for all states at time 2.
o2


9

{B iteT aken}
0

Hence, the observation {B iteT aken} is assigned the minimum plausibility value, and
the only alternative observation is assigned a very high plausibility value. The degree of
strength of o2 is 9. This reflects the fact that Trents report is understood to be stronger
than the assumption that Alice and Eve do not bite the cookie.
Note that the degree of strength of a1 is less than the degree of strength of a2 and o2 .
This gives an indication that Bob has comparatively less confidence in his beliefs about the
action at Time 1.
Note that, in this example, we have explicitly referred to points in time. We do not,
however, include any formal representation of time in the our framework; any reference
to time should be interpreted as an informal explanatory device. We are concerned with
reasoning about scenarios that involve sequences of actions where each action is instantanenous, and all actions are executed consecutively. Nevertheless, since we allow observations
following null actions, we can imagine that actions are being executed in accordance with
a bounded global clock that allows one action per tick.
3.2 Graded World Views
We define a graded world view to be an alternating sequence of plausibility functions over
2F and plausibility functions over A. Hence, at each time i, we use a plausibility function
over 2F to represent an agents beliefs about the state of the world and we use a plausibility
function over A to represent an agents beliefs about the action that occurs. Informally,
a graded world view represents an agents subjective view of the evolution of the world in
the context of imperfectly known action histories. The rationale behind using a sequence of
ranking functions is to eventually make it possible to compare the likelihood of actions and
observations at different points in the sequence, to arrive at the most plausible sequence of
events. We have the following formal definition.
Definition 4 A graded world view of length n is a (2n + 1)-tuple
hOBS0 , ACT1 , OBS1 , . . . , ACTn , OBSn i
788

fiBelief Change with Uncertain Action Histories

where each OBSi is a plausibility function over 2F and each ACTi is a plausibility function
over A.
At time i, the most plausible actions are the minimally ranked actions of ACTi and the
most plausible states are the minimally ranked states of OBSi . We take OBS0 to represent the initial belief state, and each subsequent OBSi to represent a new observation. If
ACT = hACT1 , . . . , ACTn i and OBS = hOBS0 , . . . , OBSn i, then we write hACT, OBSi
as a shorthand for the graded world view hOBS0 , ACT1 , OBS1 , . . . , ACTn , OBSn i. Informally, a graded world view represents an agents subjective view of the history of actions
and observations.
We remark briefly on the intuition behind graded world views. We are interested in
action domains involving actions that are both partially observable and fallible. However,
for the moment we do not consider failed actions; we address this issue briefly in 4.6. The
plausibility of an action A represents an agents confidence that A was successfully executed
at a given instant. Hence, the lowest plausibility values will be assigned to actions that an
agent has executed, or actions that an agent has observed directly. Higher plausibility
values will be assigned to exogenous actions that are assumed to be unlikely, or action
occurrences that are only believed based on external reports. In 3.4, we provide some
additional motivation for plausibility values by illustrating a correspondence with subjective
probability functions.
Note that graded world views essentially represent the initial belief state as an observation at time 0. The underlying assumption in a graded world view is that the initial belief
state is no different than any subsequent observation; there is no reason to automatically
prefer the initial beliefs over new information, nor is there any reason to automatically
disregard the initial beliefs given new information. In the definition of a graded world view,
we have used indices in a manner that is not symmetric in order to emphasize the unique
stature of OBS0 . In particular, note that there is no ACT0 in the definition. This simple
notational convention is intended to highlight the fact that OBS0 has a slightly different
stature at an informal level.
3.3 Aggregate Plausibility Functions
Given a graded world view hACT, OBSi, we would like to be able to determine the most
plausible history of the world. We formally define the notion of a history over a transition
system.
Definition 5 Let T = hS, Ri be a transition system. A history of length n is a tuple
hs0 , A1 , . . . , An , sn i where for each i:
1. si  S,
2. Ai  A, and
3. hsi , Ai+1 , si+1 i  R.
Let HISTn denote the set of histories of length n.
Note that HISTn  S  (A  S)n . Ideally, we would like to use graded world views
to assign plausibility values to histories. However, a graded world view does not provide
789

fiHunter & Delgrande

sufficient information to define a unique plausibility function over histories. For example, a
graded world view does not indicate the relative weight of recent information versus initial
information. In order to determine the most plausible history, we need some mechanism for
combining a sequence of plausibility functions.
Although a graded world view does not define a unique plausibility function over histories, we can define a general notion of consistency between graded world views and plausibility functions on histories. Let r0 , . . . , rn be plausibility functions over X0 , . . . , Xn ,
respectively. Let r be a plausibility function over X0      Xn . We say that r is consistent
with hr0 , . . . , rn i if, for every i and every xi , x0i  Xi
ri (xi ) < ri (x0i )

r(hx0 , . . . , xi , . . . , xn i) < r(hx0 , . . . , x0i , . . . , xn i)
So r is consistent with hACT, OBSi just in case r increases monotonically with respect
to each component of hACT, OBSi. Any plausibility function r that is consistent with
hACT, OBSi provides a potential candidate ranking over histories.
Define an aggregate plausibility function to be a function that maps every graded world
view of length n to a plausibility function over HISTn . We are interested in aggregate
plausibility functions in which the output is always consistent with the input. Hence, we
say that an aggregate plausibility function agg is admissible if, for every hACT, OBSi, the
function agg(hACT, OBSi) is consistent with hACT, OBSi.
We provide some examples. Note that aggregate plausibility functions return a function
as a value; we can specify the behaviour of an aggregate by specifying a plausibility value
for each pair consisting of a graded world view and a history. Let h = hs0 , A1 , . . . , An , sn i.
One admissible aggregate is obtained by taking the sum of plausibility values.
sum(hACT, OBSi)(h) =

n
X

ACTi (Ai ) +

i=1

n
X

OBSi (si )

i=0

A weighted sum can be used to reflect the relative importance of different time points. For
each i, let bi be a positive integer.
sums (hACT, OBSi)(h) =

n
X

ACTi (Ai ) +

i=1

n
X

bi  OBSi (si ).

i=0

By setting bi = 2i , the aggregate function sums can be used to represent a strict preference
for recent observations. This is a standard assumption in many approaches to belief revision.
We could add a similar weight to the action histories as well, which would give another
distinct aggregate. The functions sum and sums are just two simple examples; many more
examples can be defined by specifying aggregate functions that increase monotonically with
each component.
We return to the cookie example to illustrate how the reasoning involved can be captured
with graded world views and aggregate plausibility functions.
Example (contd) We have already defined plausibility functions a1 , a2 and o2 . In order
to give a complete graded world view, we need to define two more plausibility functions over
790

fiBelief Change with Uncertain Action Histories

states. In particular, we need to give a plausibility function o0 representing Bobs initial
beliefs and we need to give a plausibility function o1 representing the null observation that
Bob makes at Time 1.
First, we reiterate the description of a1 and a2 in the following table.

a1
a2


0
0

B iteAlice
1
10

B iteEve
10
3

The fact that Alice is more likely to bite the cookie is represented by assigning a lower
plausibility value to B iteAlice at Time 1.
The plausibility function o0 should assign a minimum value to the state where the cookie
is unbitten. The plausibility function o1 should assign the same value to every state. The
plausibility function o2 (given previously) represents Trents report that the cookie has been
bitten. As noted previously, we treat reported information as an observation, and we use
the degree of strength of the reported information as an indication of the reliability of the
source. In this case, the degree of strength of o2 is an indication of trust in Trent. We define
o0 , o1 , o2 in the next table.

o0
o1
o2


0
0
9

{B iteT aken}
9
0
0

Note that the degree of strength of o2 is higher than the degree of strength of a1 or a2 .
This reflects the fact that Trents report is understood to supersede the assumption that
Alice and Eve do not bite the cookie. Graded world views have been defined precisely for
this kind of comparison between action plausibilities and state plausibilities.
If we use the aggregate function sum, then we are interested in finding the minimal sum
of plausibilities over ho0 , a1 , o1 , a2 , o2 i. By inspection, we find that the minimum plausibility
is obtained by the following history:
h = h, B iteAlice, B iteT aken, , B iteT akeni.
This history represents the sequence of events in which Alice bites the cookie at time 1.
Intuitively, this is the correct solution: given the choice between Alice and Eve, Bob believes
that Alice is the more plausible culprit.
We remark that graded world views bear a resemblance to the generalized belief change
framework proposed by Liberatore and Schaerf (2000). However, the Liberatore-Schaerf
approach associates a penalty with state change, which is minimized when determining
plausible models. As such, it is difficult to represent problems where non-null actions are
strictly more plausible than null actions. By contrast, graded world views have no implicit
preference for null actions. Moreover, our approach differs in that we allow actions with
conditional effects given by a transition system.
791

fiHunter & Delgrande

3.4 Subjective Probabilities
One issue that arises from our definition of a graded world view is the fact that it is not clear
how plausibility values should be assigned in practical problems. We address this problem
by illustrating a correspondence between plausibility functions and probability functions.
We simplify the discussion by restricting attention to rational-valued probability functions
as follows.
Definition 6 Let X be a non-empty set. A probability function over X is a function
P r : X  Q such that
 for all x  X, 0  P r(x)  1
P

xX P r(x) = 1.
We do not need any other axioms of probability theory for our present purposes. At a
common-sense level, it is clear what it means to say that action A occurred at time t with
probability p. By contrast, the problem with plausibility values is that there is no obvious
sense of scale; it is difficult to assign numerical plausibility values, because the numbers have
no clear meaning. We illustrate how probability functions can be translated uniformly into
plausibility functions, thereby giving a sense of scale and meaning to plausibility values.
Let P r be a probability function over a finite set X. Let Q denote the least common
denominator of all rational numbers pq such that P r(x) = pq for some x  X. Define the
plausibility function r as follows.
1. If P r(x) is minimal, set r(x) = Q.
2. Otherwise, if P r(x) =

p
Q,

then set r(x) = Q  p.

Hence, every probability function can be translated into a plausibility function.
Example (contd) Consider the following probability functions for the cookie example.

P ra1
P ra2


.5
.5

B iteAlice
.45
.15

B iteEve
.05
.35


.9
.5
.1

P ro0
P ro1
P ro2

{B iteT aken}
.1
.5
.9

The corresponding plausibility functions are given in the following tables.

a01
a02


10
10

B iteAlice
11
20

B iteEve
20
13

o00
o01
o02


1
2
10

{B iteT aken}
10
2
1

It is easy to see that these plausibility functions are obtained from the plausibility
functions given earlier by adding a constant to each value. In 4.2, we illustrate that adding
a constant in this manner does not affect the class of minimally ranked histories.
Connecting plausibility functions with subjective probabilities provides some justification for the use of the aggregate function sum. In particular, if we assume that the subjective
792

fiBelief Change with Uncertain Action Histories

probability functions are independent, then the probability of a given sequence of events is
determined by taking a product. In the cookie example, we can compare the probability of
Alice biting the cookie versus Eve biting the cookie:
1. P r(h, B iteAlice, B iteT aken, , B iteT akeni)
= .9  .45  .5  .5  .9 = .091125
2. P r(h, , , B iteEve, B iteT akeni)
= .9  .5  .5  .35  .9 = .070875
It is easy to check that the history where Alice bites the cookie is actually the most probable history. So, in this example, the minimally ranked history according to the aggregate
function sum is also the most probable history according to the sequence of probability
functions. This is a general property of our translation: maximizing probability over independent probability functions corresponds to minimizing the sum over plausibility values.
This follows simply from the fact that the more probable events have been assigned minimal plausibility values, and the fact that summation over natural numbers is an increasing
function, whereas multiplication on fractions less than 1 is decreasing.
The correspondence descibed in this section relies on the assumption that sequences of
actions are independent. It is worth noting, however, that this very often is not the case
in practice. Instead, it is often the case that actions occur in sequences with some clear
dependence between the individual actions. However, this is not our concern at present. The
only reason for considering probability functions is to provide some intuition or motivation
for the way that plausibility values can be assigned.
3.5 The Summation Convention
In order to ground the discussion, it is useful to choose a fixed aggregate function for assigning plausibility values to histories. As such, unless otherwise indicated, we will assume
that plausibility values are assigned to histories by the aggregate function sum. Although
this is not the only approach to combining plausibility functions, it provides a simple admissible aggregate function that is appropriate in many cases. In particular, we saw in the
previous section that sum is appropriate for domains where the plausibility functions have
been obtained from independent subjective probabilities.
We introduce some notation that will simplify the results in the next few sections.
Recall that sum(hACT, OBSi) is a plausibility function on histories. When the underlying
graded world view is clear from the context, we will write plaus(h) as a shorthand for
sum(hACT, OBSi)(h).
It is useful to introduce an operator that maps a graded world view to the most plausible
histories.
Definition 7 Let W Vn denote the set of graded world views of length n for a fixed action
signature. Define  : W Vn  2HISTn as follows:
(hACT, OBSi) = {h | plaus(h)  plaus(g) for all g  HISTn }.
We have the following obvious equivalence
(hACT, OBSi) = Bel(sum(hACT, OBSi)).
793

fiHunter & Delgrande

It is also useful to use plaus to define a plausibility function over states.
Definition 8 Let hACT, OBSi be a graded world view. For any state s, define
plaus state(hACT, OBSi)(s)
to be the least n such that plaus(hACT, OBSi)(h) = n for some history h with final state
s.
So the plausibility of the state s is the rank of the most plausible history ending with s.
When the underlying graded world view is clear from context, we simply write plaus state(s)
for the plausibility of the state s. We extend the operator Bel() to graded world views
by defining Bel(hACT, OBSi) to be Bel(plaus state(hACT, OBSi)). Hence, Bel takes a
graded world view as an argument and returns the most plausible set of terminal states.
3.6 A Uniform Representation
Before considering applications and formal results, we briefly discuss the most novel and
significant features of our approach. The notion of plausibility and the relation with theory
change has been explored extensively in the literature. Work on nonmonotonic consequence
operators, for example, has been informed by notions of preferred models (Kraus, Lehmann,
& Magidor, 1990) and conditional knowledge (Lehmann & Magidor, 1992). There has also
been a great deal of research on different representations of plausibility, ranging from orderings (Baltag & Smets, 2006, 2008; Britz & Varzinczak, 2013), to quantitative approaches
to possibility (Benferhat, Dubois, & Prade, 1999), to variations on probabilistic models
(Friedman & Halpern, 2001). We take a moment to position our work in this context.
As we have stated several times, what sets our work apart is that we apply the same
notion of plausibility uniformly to both states and actions. This contrasts with most existing
work, where a model of plausibility is used for representing initial beliefs, but then actions
are handled with different formal machinery. In many cases, actions are atomic; belief
change due to action is concerned with how a complicated plausibility structure changes in
response to a distinct action occurence.
In our model, an action occurrence is just another plausibility function. This has several
advantages, which we show in the next sections. First, it allows us to easily compare
strength of belief in a state with strength of belief in an action occurence. For example, if
I believe the lamp is on but I also believe that I toggled the switch: what should I believe?
Situations of this form are common, and they require a comparison between two different
forms of likelihood. In general, there is no natural logical aparatus that can help resolve this
problem; we need some extra-logical information about which belief is stronger, the belief
in the state or the belief in the action. There is a natural intuition that we actually need
to be given some rankings to make this determination; that is exactly what we do here.
The second advantage of using a uniform representation of plausibility for actions is
that it allows us to consider alternative actions. Looking at the literature on belief change
and preferential models, it is clear that we actually need orderings over states in order to
perform many kinds of reasoning. The reason we need such orderings is because we need
to be able to specify the best alternatives to the things we believed to be true. We argue
that the same is true for actions. When we find out that an action did not occur, we need to
794

fiBelief Change with Uncertain Action Histories

have some mechanism for determining the next best alternative. We will see in the next
section that there are many practical examples where this is important, and these examples
can all be captured in a straightforward way by plausibility functions.
While it is tempting to look at our model and try to position it in the context of
alternative models of plausibility, we argue that this is not the best way to look at this
work. In fact, we could propose an approach very similar to the one in this paper based
on sequences of probability functions with suitable aggregates. The fact that we are using
Spohn-style ranking functions in particular is not the most important point. What is
important is that we are using a measure of plausibility uniformly for observations and
actions, and the plausibility measure has the feature that differences in plausibility have
a magnitude. This allows us to determine which alternatives to our current beliefs about
states and actions should be abandoned, because we can appeal to our single notion of
magnitude to determine some notion of minimal change.

4. Using Graded World Views
In this section, we consider the most basic approach that one can use to try to find a
minimally ranked history.
4.1 Pointwise Minima
Let W = hACT, OBSi be a graded world view where
ACT = hACT1 , . . . , ACTn i
and
OBS = hOBS0 , . . . , OBSn i.
The easiest way to determine a minimally ranked history is to simply take the most plausible
actions and the most plausible worlds at each point in time. The following definition makes
this notion more precise.
Definition 9 Given a history h = hs0 , A1 , . . . , An , sn i, we say h is a pointwise minimum
for hACT, OBSi if, for all i,
1. for all A  A, ACTi (Ai )  ACTi (A), and
2. for all s  2F , OBSi (si )  OBSi (s).
The following proposition states that, if a graded world view has any pointwise minima,
then those will be the most plausible histories.
Proposition 1 Let W = hACT, OBSi be a graded world view and let M be the set of
pointwise minima for W . If M 6= , then (W ) = M .
Proof It is sufficient to note that, for h  M , plaus(h)  plaus(g) for all histories g. 
Note, however, that histories are restricted in that each world must be the outcome of the
preceding action. As such, it is possible that a graded world view will have no pointwise
minimum. This is why the preceding proposition starts with the assumption that the set
795

fiHunter & Delgrande

of pointwise minima is non-empty. In cases where the there are no pointwise minima, there
will still be minimally plausible histories.
Finding pointwise minima is not easy in the general case.
Proposition 2 Determining if a graded world view has a pointwise minimum is N P complete.
Proof Given a history, it is clear that checking if it is a pointwise minimum can be
done in linear time; so the problem lies in N P . Assume a fixed set F of fluent symbols.
Let W = hACT, OBSi where OBS has length 2|F| , and assume that each OBSi obtains a
minimum at a distinct interpretation of F. Then a pointwise minimum for W will correspond
to a Hamiltonian path for the underlying transition system. The result follows since finding
a Hamiltonian path is NP-complete. 
The fact that it is already intractable to find a pointwise minimum suggests that finding
minimal histories over a complicated aggregate function is likely to be computationally
difficult. This is not a major concern for our present purposes however, as we view graded
world views as a high-level tool to capture a variety of belief change scenarios. If one is
interested in modelling concrete action domains, then it is important to choose aggregate
functions that are well understood and computationally easy to minimize.
4.2 Equivalence
Clearly it is possible for two distinct graded world views to have the same set of minimally
ranked world histories. In fact, it is possible for two distinct graded world views to induce
the same preference ordering over histories. In this section, we define a natural equivalence
relation over graded world views with an eye towards categorical representations. We start
by defining a relation on plausibility functions.
Definition 10 Let r1 and r2 be plausibility functions over a set X. We say that r1 
= r2
if, for every x, y  X,
r1 (x)  r1 (y) = r2 (x)  r2 (y).
It is easy to verify the following: 
= is an equivalence relation, r1 and r2 have the same degree
of strength, and r1 
r
implies
that Bel(r1 ) = Bel(r2 ). Essentially, the only difference
= 2
between equivalent plausibility functions is the minimum value; we make this idea more
precise by defining a notion of normalization for plausibility functions.
Let r be a plausibility function. For any integer z   minr , the translation of r by
z is the plausibility function x 7 r(x) + z. It is easy to prove that r 
= r0 if and only if
0
r is a translation of r. We define the normalization of r to be the translation by  minr .
The normalization of r is the unique plausibility function equivalent to r that obtains a
minimum of 0.
We can extend the notion of equivalence to graded world views.
Definition 11 Let W1 and W2 be graded world views over histories for a fixed action signature with a given underlying transition system. We say that W1 
= W2 if, for every pair
of histories g and h,
sum(W1 )(g)  sum(W1 )(h) = sum(W2 )(g)  sum(W2 )(h).
796

fiBelief Change with Uncertain Action Histories

Unlike plausibility functions, it is possible to construct equivalent pairs of graded world
views that are not obtained by translations.
The following proposition illustrates that every graded world view is equivalent to a
graded world view consisting of normalized plausibility functions.
Proposition 3 Let hACT, OBSi be a graded world view. If hACT 0 , OBS 0 i is obtained by
normalizing each component of ACT and OBS, then
hACT, OBSi 
= hACT 0 , OBS 0 i.
Proof
Let g, h be histories. For ease of readability, let plaus1 and plaus2 denote
sum(hACT, OBSi) and sum(hACT 0 , OBS 0 i), respectively. Hence, plaus1 and plaus2 are
functions over histories, obtained by minimization of the total sum. As such, each of plaus1
and plaus2 must have a minimum value, corresponding to the lowest possible sum over
terms. Then the following equalities are immediate:
plaus2 (g)  plaus2 (h) = plaus1 (g)  min plaus1 (h) + min
plaus1

plaus1

= plaus1 (g)  plaus1 (h).

Hence, although we allow plausibility functions with minimum values larger than 0 in a
graded world view, we can always pass to an equivalent graded world view consisting of
normalized plausibility functions. We remark, however, that a graded world view defined
by a sequence of normalized plausibility functions need not obtain a minimum of 0. In this
case, the minimum will be 0 if and only if the graded world view has a pointwise minimum.
It is also important to note that Proposition 3 only holds under the aggregate plausibility
function sum.
4.3 Representing Belief States
Plausibility functions can be defined that simply pick out a distinguished set of elements of
the domain. If   X and c is an integer, let   c denote the function from X to the set of
integers that is defined as follows:

0 if s  
  c (s) =
c otherwise
If c is a positive integer, then   c denotes a plausibility function in which the elements of 
are the most plausible, and everything else is equally implausible. Plausibility functions of
the form   c will be called simple. If X is a set of states, then simple plausibility functions
correspond to belief states; if X is a set of actions, then simple plausibility functions pick out
the actions that are believed to have occurred. Using the terminology introduced earlier,
we say that  is held with degree of belief c.
If c > 0, then   c does not actually define a plausibility function. However, allowing
negative values leads to a simple symmetry in our notation. In the following proposition, 
denotes the complement of  under set difference. So if  is a set of states, then  = 2F  .
797

fiHunter & Delgrande

Proposition 4 For any set  and positive integer c
c 
=   c.
Proof Let s, t be states. By definition, we have

if s  , t 6 
 c
c if s 6 , t  
  c (s)    c (t) =

0
otherwise.
and

 (c)
c
  c (s)    c (t) =

0

if s 6 , t  
if s  , t 6 
otherwise.

Clearly, the right hand sides of each equality are the same. 
Suppose that
ACT = hACT1 , . . . , ACTn i
and
OBS = hOBS0 , . . . , OBSn i
where each ACTi and OBSi is simple, with the same maximum plausibility c. This means
that each ACTi and OBSi assigns either the value 1 or the value c to every element in
their respective domains. In this case, we essentially have belief states with no plausibility
ordering. In this case, it is easy to show that
hs0 , A1 , . . . , An , sn i  (hACT, OBSi)
if and only if the cardinality of
{Ai | Ai  ACTi }  {si | si  OBSi }
is maximal among all histories. In other words, the most plausible histories are those
that agree with hACT, OBSi at the highest number of components. This is a reasonable
approach to take in the trivial case where we have no prior ranking over states or actions.
We emphasize that this is just a special case when the plausibility functions are simple
and they share the same degree of strength. In the next section, we define belief change
operations in terms of a general concatenation on plausibility functions. If we do not restrict
the plausiblity functions as we have here, then we get a much wider range of possible belief
change operations.
4.4 Graded World Views as Epistemic States
An epistemic state is a representation of an agents belief state that defines a total pre-order
 over all states (Darwiche & Pearl, 1997). If s  t, then the underlying agent believes
that it is more likely that the actual state of the world is s than t. The current belief
state is given by the set of -minimal states. Recall also that a graded world view defines
798

fiBelief Change with Uncertain Action Histories

a plausibility function plaus state over states. So a graded world view clearly defines an
ordering over states, and we can think of a graded world view as defining an epistemic
state. The worlds that receive minimal rank in a graded world view are the worlds that
are supported by the most reliable observations and actions. Using this ranking to define
a plausibility ordering is tantamount to assuming that the plausibility of s is completely
determined by the reliability of the source reporting that s occurs.
By viewing graded world views as epistemic states, we can define belief change operations
in a more familiar manner. In particular, we can define belief change through a simple
concatenation operator  on graded world views. Given a sequence of plausibility functions
r = hr1 , . . . , rn i and a plausibility function r, we let r  r denote the sequence hr1 , . . . , rn , ri.
Let hACT, OBSi be a graded world view, let rA be a plausibility function over actions and
let rS be a plausibility function over states. Define  as follows:
hACT, OBSi  hrA , rS i = hACT  rA , OBS  rS i.
In this context the initial epistemic state is given by hACT, OBSi, which represents an
agents a priori beliefs about the history of observed actions and states. New actions and
observations are incorporated by simply concatenating the new plausibility functions on
to the initial graded world view. The new graded world view can be used to define a new
ordering on histories through some aggregate function. For example, if are using sum as the
default aggregate, then the new ordering is immediate. Note that the new graded world view
obtained in this manner also includes all historical information required for future belief
change. As a special case of this simple concatenation operation, we get a new approach
to update. For any set X, let 0 denote the plausibility function that uniformly assigns 0
to every element of X. We can identify the update hACT, OBSi  rA with the following
operation:
hACT, OBSi  hrA , 0i.
We can also define a natural approach to revision in this manner. Let null denote a
plausibility function that assigns plausibility 0 to the null action , and assigns everything
else a plausibility larger than the maximum value obtained by sum(hACT, OBSi). We
identify the revision hACT, OBSi  rS with the following operation:
hACT, OBSi  hnull, rS i.
Using plausibility functions to represent observations allows us to represent some natural
problem domains that can not be easily represented if we restrict observations to sets
of possible worlds. In particular, consider an action domain in which observations have
varying degrees of reliability. In such domains, when an agent makes an observation that is
inconsistent with the current belief state, there are two factors that should be considered:
the strength of belief in the current belief state and the reliability of the observation. There
is an obvious conflict that arises if we attempt to address both factors simultaneously. For
example, suppose that the underlying agent strongly believes that s is a possible state of
the world. Now suppose that the agent makes two observations.
1. One observation suggests that s is possible, but comes from an unreliable source.
799

fiHunter & Delgrande

2. Another observation suggests that s is not possible, and it comes from a very reliable
source.
It can be difficult to determine the appropriate belief change in this scenario, particularly
if strength of belief and observational reliability are treated independently. By quantifying
the reliability of every observation, graded world views make it easy to resolve this kind of
issue. We remark that problems of this form have also been addressed through the use of
prioritized merging operators (Delgrande, Dubois, & Lang, 2006).
Note that there is an asymmetry in our definition of revision and update through the
 operator. In the case of update, we assume that the final observation assigns the same
plausibility to every state. The symmetric definition for a single observation would be
defined as follows:
hACT, OBSi  h0, rS i.
However, this definition allows an arbitrary action to occur immediately before the observation. If we want to assume that the graded world view hACT, OBSi gives a complete picture
of the world at the time of the observation, then we need to assume that any intermediary
action is null. Hence, the asymmetry is not due to any significant difference between actions
and observations; the asymmetry is simply due to the fact that graded world views involve
alternating sequences of actions and observations, with actions occurring first by default.
In this section we have illustrated that a graded world view defines an epistemic state.
If we take an epistemic state to be a total pre-order on states, then the converse is clearly
false: an ordering on states does not provide enough information to define numerical ranking
functions over states. The move from epistemic states to graded world views is motivated
by the same kind of concern that motivates the move from belief states to epistemic states.
In particular, belief states in AGM revision can be understood to represent the minimal
elements in some ordering of states. Hence, a belief state can provide a partial description
of an ordering, and an ordering can in turn provide a partial description of a graded world
view. A belief state is sufficient for single-shot revision, provided that an ordering is implicit
in the revision operator. However, a belief state is not sufficient if we need to explicitly
reason about the way plausibility orderings are modified. Similarly, orderings on states are
sufficient for reasoning about preferences over states, but they are not sufficient if we need
to explicitly reason about action histories. In the next section, we clarify this point through
practical examples.
4.5 Representing Natural Action Domains
We illustrate how some interesting phenomena can be represented by graded world views.
The simplest examples involve graded world views of length 1. In particular, we initially
focus on graded world views of the form
hIN IT i  hrA , rS i.
In this context, IN IT represents the initial belief state of an agent, rA represents an agents
beliefs about the action that has been executed, and rS represents the observed state of
the world. Previously we have used OBS0 for the initial plausibility function over states;
the change in notation is just to emphasize that IN IT is some a priori initial ordering
800

fiBelief Change with Uncertain Action Histories

over states. To be clear, IN IT , rA , and rS are all plausibility functions. As such, we can
define the degree of strength of each. To facilitate the exposition, we denote the degrees
of strength by deg(IN IT ), deg(rA ), and deg(rS ) respectively. Varying the magnitudes of
these values allows us to capture several different underlying assumptions.
1. Fallible initial beliefs: deg(IN IT ) < deg(rA ) and deg(IN IT ) < deg(rS ).
2. Erroneous perception: deg(rS ) < deg(IN IT ) and deg(rS ) < deg(rA ).
3. Fallible action history: deg(rA ) < deg(IN IT ) and deg(rA ) < deg(rS ).
As a simple example, suppose that an agent believes a certain lamp is initially on, then
the power switch is toggled, and then the agent observes that the lamp is actually still
on. Clearly this sequence of events can not consistently be believed by a rational agent.
Manipulating the degrees of strength of IN IT , rA and rS gives an agent some mechanism
for resolving such conflicts. In case (1), the agent is not completely certain that the lamp
was initially on. As such, the easiest way to incorporate the new information is to change
the initial belief state. By contrast, in case (2), the agent is not completely certain that
the lamp is still on after toggling the switch. In this case, since the agent is confident the
lamp was initially on and the switch was toggled, it is natural to reject the observation
and believe that the lamp is now off. The distinction between these two cases cannot be
captured without some notion of reliability. In case (3), the agent would resolve the conflict
by believing that the attempt to toggle the power switch had failed.
The special case in which the degree of strength is 0 also captures some important
phenomena. Note that a plausibility function r has degree of strength 0 just in case there
is some constant c such that r(x) = c for all x. As such, a degree of 0 indicates that every
element of the domain receives minimal rank. We consider the informal interpretation of a
degree 0 for each plausibility function in our schematic example.
1. If deg(IN IT ) = 0, then every initial state is equally plausible. The agent has no
contingent a priori beliefs about the state of the world.
2. If deg(rO ) = 0, then rO represents a null observation. The observation OBS does not
provide evidence for any particular state.
3. If deg(rA ) = 0, then every action is equally likely. So the agent is completely ignorant
about the action that has occurred, and we can think of rA as an exogenous action
beyond the agents control.
These are relatively crude distinctions, but they still capture important classes of problems.
Roughly speaking, the problems that we have addressed thus far can be captured by a
plausibility ordering over sequences of the form
  A1  1      An  n
where  is a belief state, each Ai is an action symbol, and each i is an observation.
For the purpose of comparison, we remark that belief evolution operators defined in
Section 2.3 are only useful for problems in which the underlying plausibility ordering is
801

fiHunter & Delgrande

given as follows, for some permutation p1 , . . . , pn of 1, . . . , n.

A1 

..
.   p1  p2      pn

An
By contrast, graded world views are suitable for any total pre-order over A1 , 1 , . . . , An , n .
But this is not the entire class of problems representable by graded world views. By using a
ranking function for each event, we are able to draw two additional distinctions that can not
be represented by a simple ordering. First, we are able to represent changes in plausibility
that do not affect the ordering of states. This is useful for representing action domains
where an agent must observe a single piece of evidence multiple times before believing it is
correct. Second, we are able to represent graded evidence; we use the term graded evidence
to describe situations were an observation actually supports several different conclusions
with different degrees of confidence. We conclude this section with two examples illustrating
action domains that are difficult to represent if we only have an ordering over the plausibility
of events.
Example (Additive Evidence) Bob believes that he turned the lamp off in his office, but
he is not completely certain. As he is leaving the building, he talks first to Alice and then
to Eve. If only Alice tells him his lamp is still on, then he will believe that she is mistaken.
Similarly, if only Eve tells him his lamp is still on, then he will believe that she is mistaken.
However, if both Alice and Eve tell Bob that his lamp is still on, then he will believe that
it is in fact still on.
This example can easily be represented by a graded world view as follows. We assume
that the underlying action signature contains, among others, a fluent symbol LampOn and
an action symbol T urnLampOff . The underlying transition system defines the effects of
turning the lamp off in the obvious manner. Let ON denote the set of states in which
LampOn is true. The following plausibility functions define a graded world view that
represents this action domain.
1. OBS0 = ON  10
2. ACT1 = {T urnLampOff }  3
3. OBS1 = ON  2
4. ACT2 =   10
5. OBS2 = ON  2
Note that (hOBS0 , ACT1 , OBS1 i) consists of all histories where the lamp is turned off at
time 1. However, (hOBS0 , ACT1 , OBS1 , ACT2 , OBS2 i) consists of all histories where the
lamp is not turned off at time 1. Two observations of ON are required to make Bob believe
that he did not turn the lamp off.

Example (Graded Evidence) Bob receives a gift that he estimates to be worth approximately $7. He is curious about the price, so he tries to glance quickly at the receipt without
802

fiBelief Change with Uncertain Action Histories

anyone noticing. He believes that the receipt says the price is $3. This is far too low to be
believable, so Bob concludes that he must have mis-read the receipt. Since a 3 looks very
similar to an 8, he concludes that the price on the receipt must actually have been $8.
To represent this example, we first define ACT1 =   10 because Bob believes that no ontic actions have occurred. We assume that there are fluent symbols Cost1, Cost2, . . . , Cost9
interpreted to represent the cost of the gift. We define a plausibility function OBS0 representing Bobs initial beliefs.

 0 if s = {Cost7}
1 if s = {Cost6} or s = {Cost8}
OBS0 (s) =

3 otherwise
Note that Bob initially believes that the cost is $7, but it is comparatively plausible that this
cost is one dollar more or less. Finally, we define a plausibility function OBS1 representing
the observation of the receipt.

 0 if s = {Cost3}
1 if s = {Cost8}
OBS1 (s) =

3 otherwise
Bob believes that the observed digit was most likely a 3, with the most plausible alternative being the visually similar digit 8.
Given these plausibility functions, the most plausible state of the world is the state in
which the price is $8. In order to draw this conclusion, Bob needs observations that provide
graded evidence about states of the world and he needs to be able to weight this information
against his initial beliefs.
The preceding examples illustrate that there are natural common-sense reasoning problems in which an agent needs to consider aggregate plausibilities over a sequence of actions
and observations. Graded world views are well-suited for reasoning about such problems.
4.6 Non-deterministic and Failed Actions
In this section, we consider actions with non-deterministic effects. We remark, in particular,
that fallible actions can be understood as actions with non-deterministic effects; fallible
actions are just actions where some possible outcomes are considered to be failures. In the
simplest case, for example, a failed action might be one which leaves the state of the world
unchanged. Hence, by addressing non-deterministic effects, we are also handling actions
that might fail. Our basic approach is the following. We introduce some new machinery
for the representation of non-deterministic actions, and then we demonstrate that the new
machinery is unnecessary when we use summation to determine the plausibility of histories.
As such, we can reasonably restrict attention to deterministic actions when proving formal
expressibility results for graded world views.
Given a non-deterministic transition system T = hS, Ri and a graded world view W ,
it is not clear how we should choose the effects of each action in the most plausible world
histories. This problem can be solved by attaching a plausibility value to the possible effects
of each action (Boutilier, 1995). For each action A and state s, let EF F (A, s) denote the
803

fiHunter & Delgrande

set of states s0 such that (s, A, s0 )  R. Hence EF F (A, s) is the set of states that may
result, given that action A is executed in state s.
Definition 12 An effect ranking function is a function  that maps every action-state pair
(A, s) to a plausibility function over EF F (A, s).
Informally, an effect ranking function gives the plausibility of each possible effect for each
action. For instance, if we want to model a coin flipping action, then the corresponding effect
ranking function would be constant: both outcomes of the coin flip would be considered
equally plausible.
A non-deterministic graded world view is a pair hW, i where W is a graded world view
and  is an effect ranking function. We illustrate with an example.
Example Consider an action domain involving a single fluent symbol LampOn indicating
whether or not a certain lamp is turned on. There are two action symbols P ress and
T hrowP aper respectively representing the acts of pressing on the light switch, or throwing
a ball of paper at the light switch. Informally, throwing a ball of paper at the light switch
is not likely to turn on the lamp. But suppose that an agent has reason to believe that a
piece of paper was thrown at the lamp and, moreover, the lamp has been turned on. We
illustrate how non-deterministic graded world views can provide a representation of this
problem.
Both actions have non-deterministic effects in that both may cause LampOn to become
true, but both may also fail to do so. We define a graded world view hACT, OBSi of length
1. First, we define ACT so that T hrowP aper is the most likely action at time 1.

ACT1


10

P ress
2

T hrowP aper
1

Next we define OBS so that initially the light is off, and then the light is on.

OBS0
OBS1


10
0

LampOn
0
10

Finally, we define an effect ranking function  that represents the fact that pressing is more
likely to turn the light on.

(P ress, {LampOn})
(P ress, )
(T hrowP aper, {LampOn})
(T hrowP aper, )


0
10
9
0

{LampOn}
10
0
0
9

Lines 1 and 2 in the table indicate that pressing the switch is very likely to change the
state of the lamp. Lines 3 and 4 indicate that throwing paper at the switch is most likely
804

fiBelief Change with Uncertain Action Histories

to have no effect, but a change caused by the paper ball is seen as more plausible than a
failed effect when the button is pressed.
In the preceding example, there are two possible solutions: either a plausible event occurs
with an unlikely outcome, or a less plausible event occurs with an expected outcome. There
is no a priori preference given to occurrence plausibilities or to effect plausibilities; the
framework is flexible enough to represent either possibility.
Introducing effect ranking functions makes the distinction between action occurrences
and action effects explicit, which in turn gives a straightforward treatment of failed actions. However, we need to introduce some extra machinery in order to determine the
most plausible action history. The most general approach is to extend the definition of an
aggregate plausibility function: a non-deterministic aggregate plausibility function takes a
non-deterministic graded world view as an argument, and it returns a plausibility function
over histories. An admissible non-deterministic aggregate plausibility function is one that
increases monotonically with respect to the given graded world view, as well as the given
effect ranking function.
We have been using the function sum as our standard aggregate plausibility function.
The natural extension of sum to non-deterministic graded world views is the following. For
any history h = s0 , A1 , . . . , An , sn , define
X
sum(hACT, OBSi, )(h) =
OBSi (si ) + ACTi (Ai ) + (Ai , si1 )(si ).
i

It is easy to see that this is an admissible non-deterministic aggregate function. Returning
to the lamp example, there are two minimally ranked histories under this function: one in
which the lamp was turned on by pressing on the switch and one in which the lamp was
turned on by throwing a piece of paper at the switch.
In the remainder of this section, we will assume that sum is the default aggregate
function for non-deterministic world views. Under this assumption, we demonstrate that
non-deterministic graded world views can be translated into graded world views in an
extended action signature.
Let T = hS, Ri be a non-deterministic transition system over the action signature hA, Fi.
Let hhACT, OBSi, i be a non-deterministic graded world view. We extend the action
signature to a new action signature A0 where every edge in T corresponds to a unique
action symbol. In particular, let A0 = {A(s,A,t) | (s, A, t)  R}. Let T 0 = hS, R0 i where R0
is the closure of the set {hs, A(s,A,t) , ti | s, t  S}. Suppose that ACT = ACT1 , . . . , ACTn .
Define ACT 0 = ACT10 , . . . , ACTn0 where, for each i, ACTi0 (A(s,A,t) ) = ACTi (A) + (A, s)(t).
Proposition 5 For any non-deterministic transition system T , a history
h = s0 , A1 , . . . , An , sn
obtains the same rank in hhACT, OBSi, i as
h0 = s0 , A(s0 ,A1 ,s2 ) , . . . , A(sn1 ,An ,sn ) , sn
obtains in hACT 0 , OBSi.
805

fiHunter & Delgrande

Proof The plausibility of h is obtained by taking the sum
X
OBSi (si ) + ACTi (Ai ) + (Ai , si1 )(si ),
i

which is clearly the same sum taken to determine the plausibility of h0 . 
Hence non-deterministic actions and failed actions can be represented in a graded world
view, simply by setting up the plausibility functions carefully.
We remark that there is a conceptually interesting distinction that is lost in this translation. Informally, there is a distinction between an action that fails to occur and an action
that occurs, but fails to produce an expected effect. This distinction is clear if we consider
the difference between failing to drop a glass on the ground, and dropping a glass that fails
to break when it hits the ground. In the first case, the agent executes the drop action
but it fails to occur; perhaps the glass sticks to the agents hand. In the second case, the
glass is successfully dropped without breaking. In our framework, both of these events are
represented by a dropping action with the null effect. We suggest that this is an acceptable
treatment, because in both cases the sequence of actions and states is identical. As such, we
cannot distinguish between these scenarios based on our definition of a history. However,
we may be able to distinguish indirectly based on the values of other fluents. For instance,
the location of the glass is only going to change in the case where it is successfully dropped.

5. Comparison with Related Formalisms
Belief change due to actions and observations has been addressed previously in the literature.
In this section, we consider how our approach is related to existing work in the area.
5.1 Representing Single-Shot Belief Change
While our focus thus far has been on the use of graded world views for the representation
of iterated belief change due to actions and observations, the simplest scenario involves
just a single ontic or epistemic action. It is important, therefore, to verify that the singleshot belief change operators induced by a graded world view are reasonable with respect
to existing work in the area. Recall that we defined  and  on graded world views as
shorthand notation for the associated concatenation operations. Based on the results in
this section, it will be clear that this shorthand is natural and appropriate.
We first consider the case of a single ontic action.
Proposition 6 Let hACT, OBSi be a graded world view. For any plausibility function r
over A,
Bel(hACT, OBSi  hr, 0i) = Bel(hACT, OBSi)  Bel(r).
Proof Since every action is always executable, s  Bel(hACT, OBSi  hr, 0i) if and only
if there is some s0  Bel(hACT, OBSi) such that s0  A = s for some A  Bel(r). Hence
s  Bel(hACT, OBSi  hr, 0i) if and only if s  Bel(hACT, OBSi)  Bel(r). 
Proposition 6 is important if we are primarily interested in belief states and ontic actions.
Basically, in this case, graded world views are unnecessary. The most plausible final belief
state can be determined by simply looking at the belief state associated with the initial
graded world view.
806

fiBelief Change with Uncertain Action Histories

We now consider the case of a single observation. At present, we are primarily interested
in comparing the expressive power of graded world views with AGM revision operators.
There is one sense in which graded world views are clearly more expressive than AGM
operators. In particular, a new observation need not be incorporated into an agents beliefs
if the observation does not come from a reliable source. We will demonstrate that, in the
context of a single observation, this is essentially the only difference between a graded world
view and an AGM revision operator. More specifically, we will see that the belief change
defined by concatenating a single observation onto a graded world view can be captured by
an AGM operator, provided that the observation has degree of strength higher than some
fixed threshold.
We start by proving that every plausibility function defines a system of spheres, as
defined by Grove (1988). We first review the definition of a system of spheres. Let ML
denote a set of consistent, complete theories over L. A set of subsets S of ML is a system
of spheres centered on X where X  ML , if it satisfies the conditions:
S1. S is totally ordered by 
S2. X is the minimum of S under 
S3. ML  S
S4. For any formula  with || =
6 , there is a least sphere c() such that c()  || =
6 
and U  || =
6  implies that c()  U for every U  S
We picture a system of spheres as a series of concentric circles, with innermost circle X.
Let r be a plausibility function over X with minimum value minr . For any n, let r[n]
denote the set of complete, consistent theories that are satisfied by some interpretation I
with r(I)  n.
Proposition 7 Let r be a plausibility function over a finite action signature. The collection
R = {r[n] | n  minr } is a system of spheres centered on r[minr ].
Proof Clearly, for each n, r(n)  r(n + 1). Hence R is totally ordered by .
If T  r[minr ], then T is satisfied by some I with r(I)  minr . But then, for any n, T
is satisfied by some I with r(I)  n. Hence r[minr ]  r[n] for all r[n].
Since the action signature is finite, there are only finitely many states. Hence there is
a state that is assigned a maximum plausibility, say maxr . Therefore, r[maxr ] is the set of
complete, consistent theories.
Let  be a consistent formula. Since there are only finitely many states, there must be
a state s   such that r(s)  r(t) for all t  . Let n = r(s). Clearly r(n)   6= . Now
suppose that U  S and U   6= . Suppose that U = r(m), so U is the set of complete,
consistent theories satisfied by some I with r(I)  m. Since some elements of U are also in
, it follows m  n. Therefore r[n]  U , and r[n] is the least sphere intersecting . 
Using this result, we can show that single-shot revision under graded world views can be
captured by AGM revision operators. We make this claim precise in the next proposition.
807

fiHunter & Delgrande

Proposition 8 Let hACT, OBSi be a graded world view. There is an AGM revision function  and a natural number n such that, for any plausibility function r over states with
degree of strength larger than n,
Bel(hACT, OBSi  h  n, ri) = Bel(hACT, OBSi)  Bel(r).
Proof Recall that plaus is a plausibility function over histories that is defined by minimizing sums over hACT, OBSi, and plaus state is the corresponding plausibility function
over final states.
Let n be a natural number such that n > plaus(h) for every history h. Let r be a
plausibility function with rank n. It follows that s  Bel(hACT, OBSih  n, ri) if and only
if the following conditions hold:
1. s  Bel(r)
2. plaus state(s) is minimal among all states satisfying 1.
By Proposition 7, plaus state defines a system of spheres centered on Bel(plaus state).
It follows from Groves representation result (Grove, 1988) that there is an AGM revision
function  such that, for any observation , s  Bel(plaus state)   if and only if the
following conditions hold:
1. s  
2. plaus state(s) is minimal among all states satisfying 1.
Setting  = Bel(r) gives the desired result. 
Proposition 8 illustrates that, for a single observation, the most plausible worlds can be
determined without considering the history of actions and observations. We can determine
the most plausible worlds following an observation by simply abstracting a belief state from
a graded world view, then performing AGM revision. It is easy to show that the converse is
also true: every AGM revision operator can be represented by a graded world view. More
precisely, we have the following result.
Proposition 9 Let  be an AGM revision operator and let  be a belief state. There is a
graded world view hACT, OBSi with Bel(hACT, OBSi) =  and a natural number n such
that, for every non-empty observation ,
   = Bel(hACT, OBSi  h  n, ri)
where r is any plausibility function over states where the minimal ranked elements  have
degree larger than n.
Proof By Groves representation result,  can be captured by a system of spheres S.
Define the graded world view hACT, OBSi such that S is the system of spheres given by
Proposition 7. Set n such that n > plaus(h) for every history h. The result is immediate.

Taken together, Propositions 8 and 9 illustrate that graded world views are equivalent to
808

fiBelief Change with Uncertain Action Histories

AGM revision if we restrict attention to a single observation with a sufficiently high degree
of reliability. Hence, for single-shot belief change, the full expressive power of graded world
views is unnecessary. For both ontic actions and observations, we can define the same
belief change operations if we start with just a belief state. There is also a correspondence
here with Nayaks work on iterated revision (Nayak, 1994); if an observation is sufficiently
plausible, then every state in that observation ends up being strictly more plausible than
every other state.
5.2 Representing Conditionalization
Spohn uses ranking functions to define a different form of single-shot belief change called
conditionalization (Spohn, 1988). The idea is that new evidence is presented as a pair
(, m), where  is a set of states and m  0; the value of m is an indication of the strength
of the observation . Informally, the conditionalization of r is a new function where the
minimally ranked -worlds receive rank 0 and the non- worlds are all shifted up by m.
In this section, we illustrate how conditionalization can be defined in terms of graded world
views.
First, we define conditionalization formally. Let r be a plausibility function with minr =
0 and let  be a subset of the domain of r. Let min() denote the minimum value r(s) for
s  . Spohn defines the the plausibility function r(|) over  as follows:
r(s|) = r(s)  min().
We call r(s|) the -part of r. The conditionalization of r, written r(,m) , is the following
plausibility function.

r(s|)
if s  
r(,m) (s) =
m + r(s|) if s 
6 
So the conditionalization of r is the -part of R together with the -part shifted appropriately.
We show that conditionalization can easily be represented by taking minimal sums over
plausibility functions.
Definition 13 Let r be a plausibility function over 2F , let  be a non-empty subset of 2F ,
and let m be a natural number. Define rC (, m) as follows:

0
if s  
rC (, m) (s) =
m + min() if s 6 
We refer to rC (, m) as the conditionalizer of r with respect to  and m. The following
proposition illustrates how we can define the conditionalization of a plausibility function by
taking an appropriate sum.
Proposition 10 Let r be a plausibility function with minr = 0. For any , m, the normalization of r + rC (, m) is the conditionalization r(,m) .
Proof If s  , then
r(s) + rC (, m)(s) = r(s) + 0 = r(s).
809

fiHunter & Delgrande

If s 6 , then
r(s) + rC (, m)(s) = r(s) + m + min().
Since r(s)  0 and m  0, it follows that the minimum value obtained by r + rC (, m) is
min(). Hence, the normalization of r + rC (, m) is the plausibility function r0 defined as
follows.
r0 (s) = r(s) + rC (, m)(s)  min()
This is equal to r(,m) , which is what we wanted to show. 
Proposition 10 illustrates that the conditionalization of r by (, m) can be defined
by taking a minimal sum over two plausibility functions. We have restricted attention to
plausibility functions with minimum 0 because this class coincides more closely with Spohns
ranking functions. However, we can define the conditionalizer in the same manner for
plausibility functions with non-zero minimums. We can also define the conditionalization
of a graded world view. Informally, we simply conditionalize the associated plausibility
function on states. Hence, we identify the conditionalization with respect to h, mi with
the following operation:
hACT, OBSi  h, plaus stateC (, m)i.
It is straightforward to show that this gives the desired result.
5.3 Representing Belief Evolution Operators
As noted previously, we have previously defined so-called belief evolution operators to capture iterated belief change due to actions and observations (Hunter & Delgrande, 2011). In
this section, we show that graded world views actually extend this approach, by verifying
that every belief evolution operator can be captured by a graded world view.
There are two underlying assumptions in the definition of belief evolution:
1. The plausibility of an observation is determined by some ordering, recency by default.
2. The action history is assumed to be correct.
Both of these assumptions can be represented in a graded world view by setting up the
plausibility functions appropriately.
Belief evolution operators can be defined with respect to metric transition systems. A
metric transition system to be a transition system, along with a metric d that gives a distance
between states. The metric d defines a belief revision operator as follows (Delgrande, 2004):
   = {w   | v1   such that for all v2  , v3  K
we have d(w, v1 )  d(v2 , v3 )}.
Assume that we have a fixed initial belief state I , along with a metric transition system
defining a revision operator  and an update operator . Let  be the belief evolution
operator obtained from  and . Let
A = hA1 , . . . , An i
810

fiBelief Change with Uncertain Action Histories

be an action trajectory, and let
 = h1 , . . . , n i
be an observation trajectory. We want to construct a graded world view Wev that assigns
minimal plausibility value to all histories corresponding to I  hA, i.
We define Wev = hACT, OBSi presently. By combining I with the metric d given by
the metric transition system, we can define a plausibility function BASE that represents
the initial ordering of states implicit in . In particular, for any s, set
BASE(s) = min({d(s, k) | k  I }).
Using this plausibility function, we can define the observation trajectory OBS. Let max
denote the maximum value obtained by BASE.

BASE
if i = 0
OBSi =
i  (2i + max) otherwise
By incrementing the plausibility of false observations exponentially, we can assure that
recent observations will be given greater credence.
Informally, each action symbol Ai is translated into a plausibility function that obtains
the minimum value on the set {Ai }. Formally, we have the following, for 1  i  n:
ACTi = Ai  (2n+1 + max).
Proposition 11 If I  hA, i = h0 , . . . , n i, then
h  (Wev )

h = hs0 , A1 , . . . , An , sn i where si  i for each i.
Proof Assume for the moment that hA, i is consistent. Let h = hv0 , B1 , . . . , Bn , vn i. By
definition h  (Wev ) if and only if the sum
n
X

ACTi (Bi ) +

i=1

n
X

OBSi (vi )

(2)

i=0

is minimal. Since hA, i is consistent, there exist histories hs0 , A1 , . . . , An , sn i where each
si  i . For such histories, the sum (2) becomes
n
X

ACTi (Ai ) +

i=1

n
X

OBSi (si ) = 0 + OBS0 (s0 )

i=0

We remark that this sum is less than any sum that can be obtained by a history where
there is some i such that either Bi  Ai or si 6 i . Therefore h  (Wev ) if and only if the
following three conditions hold:
1. Bi = Ai for each i > 0
811

fiHunter & Delgrande

2. vi  i for each i > 0
3. OBS0 (v0 ) is minimal among states satisfying 1 and 2.
In order to satisfy condition 2, it must be the case that v0 is in the set
\
V =
i1 (Ai ).
i

In order to simultaneously satisfy condition 3, it must also be the case that v0 is minimally
distant from I according to the metric d. In other words, v0  I  V . Therefore, h 
(Wev ) if and only if each Bi = Ai and the following conditions hold:
T
1. v0  I  i i1 (Ai ), and
2. vi = v0  Ai .
This is the definition of I  hA, i, so this completes the proof.
The case where hA, i is inconsistent is similar. The only difference is that we need
to notice that the degree of strength of each observation increases by a power of 2. We
use the fact that, for any natural number p, 2p is larger than every sum of terms 2i with
i < p. As such, in order to minimize the sum (2), we need to work backwards through the
observations, keeping each observation if it is consistent with the observations that followed.
This is just an equivalent specification of  (Wev )  increasing powers exponentially forces
a strict preference for recent observations. 
Proposition 11 demonstrates that graded world views can represent any belief evolution
operator defined with respect to a distance function. From the perspective of graded world
views, the assumption that action histories are infallible is essentially just a restriction on
the admissible plausibility functions.
We conclude this section with some brief remarks about the use of orderings to resolve
inconsistency in iterated belief change. The well-known Darwiche-Pearl postulates for iterated belief change (Darwiche & Pearl, 1997) are only satisfied when we assume that the
most recent observation takes precedence over previous observations. By contrast, Papini
illustrates an alternative approach to iterated revision in which earlier observations take
precedence over later observations (Papini, 2001). More generally, we defined belief evolution operators with respect to an arbitrary total ordering over the observations. The most
natural extension of belief evolution would extend the ordering to include all observations
and actions. Using the techniques in this section, it is easy to see that this extended conception of belief evolution corresponds to the class of graded world views with an arbitrary
initial observation followed by plausibility functions of the form   2i , where each i is distinct. Hence, even the most general extension of belief evolution can be represented by a
relatively restricted class of graded world views.
5.4 Relation with the Situation Calculus
The Situation Calculus (SitCalc) is a well-establishing framework for reasoning about the
effects of actions. Action descriptions in the SitCalc are formulated in many-sorted firstorder logic along with a single second-order induction axiom. Briefly, variables can range
812

fiBelief Change with Uncertain Action Histories

over situations, entities, and actions. Situations are understood to represent the current
state of the world, including a complete history of all actions that have occurred. There is
a distinguished constant symbol S0 that denotes the initial situation and a distinguished
function symbol do that maps an action A and a situation s to the situation that results
from executing A in situation s. Predicate symbols that take a situation argument are
called fluents. For a detailed introduction, we refer the reader to the foundational summary
presented by Levesque, Pirri and Reiter (1998).
The epistemic extension of the SitCalc introduces sensing actions, alternative initial
situations, an accessibility relation, and a plausibility ordering over situations (Shapiro
et al., 2011). The situations that an agent believes possible are the minimal accessible
situations, and the effect of a sensing action changes the set of accessible states. This
results in a form of belief revision that satisfies most of the AGM postulates. We have
previously proved that belief revision in the SitCalc can be captured by a belief evolution
operator under a natural translation (Hunter & Delgrande, 2011); it therefore follows from
Proposition 11 that the belief change operators defined in the standard SitCalc approach
can be captured by graded world views.
A similar conclusion can be drawn regarding our representation of uncertain actions.
Bacchus et. al. (1999) extend the SitCalc to include noisy sensors and non-deterministic
actions. Roughly, the idea is to define complex actions in terms of a set of primitive
actions. In this manner, a non-deterministic action is represented by a probability function
over the set of primitive actions. We have a similar result in Proposition 5, which proves
that we can capture non-deterministic actions by introducing ranking functions over action
effects. If we think of each action effect as a primitive action, then we essentially have the
same representation as Bacchus et. al. except that we use ranking functions rather than
probabilities. Moreover, in section 3.4 we demonstrated that a translation from probabilities
to plausibilities is straightforward.
While our approach appears to be sufficiently general to capture belief change and
uncertainty over actions as defined in the given epistemic extensions of the SitCalc, it is
important to note that there are respects in which our approach is actually more expressive.
In particular, we allow revision by arbitrary rankings over formulas; we do not require
a fixed set of sensing actions. Our approach also allows for a flexible representation of
the interaction between uncertainty over actions and observations, and it is elaboration
tolerant in the sense that we need only modify aggregates over plausibility functions to
capture different phenomena. On the other hand, the SitCalc provides a more rigorous and
precise treatment of actions effects and ontic change. For example, the SitCalc treatment
of property persistence (Kelly & Pearce, 2010) captures a natural feature of certain world
properties that can only be captured by a graded world view with an ad hoc restriction on
plausibility functions. Overall, we see our approach as a high-level model of belief change
that can capture the dynamics of belief in the SitCalc, but it does so while losing some of
the advantages of the SitCalc model of action effects.
5.5 Relation with Dynamic Epistemic Logic
The notion of belief change due to reported information has been addressed in Dynamic
Epistemic Logic (DEL) (van Ditmarsch, van der Hoek, & Kooi, 2007). In this context, belief
813

fiHunter & Delgrande

change is captured through plausibility models. A plausibility model is a Kripke structure,
where we associate a well-ordering over possible states with each state; this ordering indicates which states are seen as the most plausible at each state in the structure. Belief update
can be captured in this setting through state-changing actions that allow different agents to
have different awareness of the action that has been executed (Baltag & Smets, 2008); belief
revision can be captured by a mapping on plausibility models that is syntactically defined
through suitable modal operators (Baltag & Smets, 2006; Van Benthem, 2007). Although
it is common to use an ordering to represent plausibility in DEL, there are also variants
in which plausibility is captured with quantitative values. For example, Laverny and Lang
(2005) define a logic that incorporates actions and uses ranking functions over N  {} to
model strength of belief for formulas without nested modalities.
There are two main distinctions between our work and related work in the DEL tradition. The first distinction is superficial, and it is related to our high-level perspective on new
information. In DEL, there is an external perspective on new information, in which structures explicitly model the way that each agent in the system views the new information.
This is why DEL is so expressive with respect to nested multi-agent beliefs: we are able to
be explicit about the perspective of each agent. As presented in this paper, we can think of
graded world views as taking an internal perspective for a single agent. When we provide a
ranking function that represents an observation, we have no external knowledge about the
actual state of the world. The advantage of this approach is that every observation (resp.
action) is always possible. By contrast, with the external perspective of DEL, there may be
situations where certain information simply cannot be provided as an information update.
For example, if the underlying Kripke structure only contains states where  is true, then
there is no information update that can be provided to make an agent believe  is false.
The second distinction between our work and existing work in DEL is related to our
uniform representation of observations and actions. In DEL, a distinction is often drawn
between so-called hard updates and soft updates (Van Benthem, 2007). A hard update
occurs when the information is definite in some sense, and it is incorporated by modifying
the accessible states. This might occur, for example, when it is common knowledge that a
certain state-changing action has occurred. A soft update corresponds to a belief revision,
where the information is incorporated by modifying the plausbility ordering. However, in a
practical setting, it is often the case that one can not distinguish which kind of change should
actually occur. This kind of ambiguity does not occur in the case of graded world views,
where all belief change operations are handled in a uniform manner by taking aggregates over
plausibility functions. Morever, in both hard update and soft update, the update is typically
by a single piece of information with a single level of plausibility. Hence, observations and
actions are not represented in a graded manner. This is one of the distinguishing features of
the present work: we allow observations (and actions) to provide different levels of evidence
for several different states simultaneously.
Despite the differences, it is important to emphasize that we actually do not see graded
world views as an alternative to DEL. Instead, we see the approach presented here as a tool
that could be used as the basis for belief change in a suitably defined DEL with quantitative
plausibility rankings. This is analogous to the way that formal belief change operators have
been incorporated in modal logics. Iterated revision operators such as lexicographic revision
(Nayak, 1994) and natural revision (Boutilier, 1996) were originally defined in a general
814

fiBelief Change with Uncertain Action Histories

setting of orderings, before they were used as motivating examples for information update
in DEL. Similarly, we suggest that a modal logic could be defined in which the semantics
of iterated propositional updates was based on aggregates over plausibility functions. Such
a logic would be able to model all of the practical domains discussed in this paper, and it
would also be able to capture complex multi-agent situations involving nested belief.
5.6 Representations of Plausibility
Following Spohn (1988), we have taken quantitative ranking functions as our representation
of plausibility. However, the literature on belief change and defeasible reasoning contains
a great deal of work on different representations of plausibility. It this section, we briefly
consider alternative approaches from the literature, and we argue that our approach is more
suitable for the kind of reasoning we intend to capture.
The most basic representation of plausibility in the literature is a total pre-order on
states, as we often see in AGM-inspired work. We normally think of such an ordering in
terms of a series of levels of plausibility. The obvious limitation of this approach is that
there can be no notion of magnitude with respect to differences in plausibility, although
this is can be introduced by allowing empty levels. In any event, it is clear that ranking
functions are more expressive than total pre-orders with respect to modelling plausibility.
A more interesting distinction occurs between our notion of plausibility, and the orderings
used in the DEL tradition. As noted above, plausibility in this context is typically captured
by a well-ordering (Van Benthem, 2007). This is necessary because, in this setting, we
do not want all states to be comparable; we want certain states to be innaccessible from
others. One might ask if our ranking functions are limited in any sense with respect to the
well-orderings used in plausibility models. Our choice of ranking functions is motivated by
the internal-external distinction discussed in the previous section. An individual agent need
not consider impossible states in order to modify their beliefs appropriately, they need
only consider highly unlikely states.
Note that our approach is superficially similar to related work on defeasible reasoning
featuring some form of preferential ordering on states. It is possible, for example, to reason
about typicality by supplementing a Kripke structure with a single ordering over states
(Britz & Varzinczak, 2013). An alternative model in the setting of description logics has also
been proposed, in which a function is introduced to map each concept to a set of elements
that are deemed to be most typical (Giordano, Olivetti, Gliozzi, & Pozzato, 2013). However,
despite the superficial similarities, it is important to be clear that our initial plausibility
function is not intended to be a representation of typicality or normality in an absolute
sense. Our model of plausibility corresponds more closely to a pointed plausibility model
that includes a distinguished actual state.
Alternative quantitative models of plausibility have also been explored in the literature.
One of the most influential such approaches is based on possibility measures, which are
functions that map states to [0, 1] (Benferhat et al., 1999). The main difference between a
possibility measure and a plausibility function is really that the possibility measure 0 actually corresponds to impossibility, and there is no corresponding plausibility value. However,
we have already shown that plausibility functions can be used to capture rational-valued
815

fiHunter & Delgrande

functions over [0,1]; it would be straightforward to modify our proof to deal with real number
values.
One of the most general measures of plausibility in the literature is a plausibility space,
which is analogous to a probability space except that the range of plausibility values can
be any set (Friedman & Halpern, 2001). Plausibility spaces are intended to be a very
general model that subsumes a great deal of existing work on reasoning about plausibility.
Indeed, plausibility functions are clearly just a special case of a plausibility space; so our
representation of belief can be seen as a special case of this more general approach.
In this section, we have been restricting the discussion to alternative representations of
plausibility on states. As noted previously, our approach to the representation of plausibility
over states is actually not new and it is therefore easy to position our work in this context.
The novel feature of our approach is that we use the same notion of plausibiility over actions,
in order to develop a novel approach to reasoning about iterated belief change due to actions
and observations

6. Limitations and Advantages
Our focus in previous sections has been on establishing the expressive power of graded
world views, as compared with existing frameworks for reasoning about belief change. As
a result, we have focused on problems involving an a priori graded world view, along with
some new information. However, the restriction to new information is artificial. In the
general case, there is no reason to restrict attention to problems in which an agent only
receives information about actions and observations occurring at the most recent point in
time. An agent could certainly receive new information about earlier events and actions.
Hence, a more general problem involves an agent with an underlying graded world view,
together with a set of constraints on the most plausible histories. In this section, we consider
the representation of problems that have this more general form.
6.1 Constrained World Views
Suppose that hACT, OBSi is a graded world view of length n. An action constraint is
a pair (A, i) where A is an action symbol and i  n. Define (hACT, OBSi)  (A, i)
to be the set of histories with minimal plausibility, subject to the restriction that the ith
action executed is A. We define observation constraints in the analogous manner, and we
let (hACT, OBSi)  (, i) be the set of minimally ranked histories where the ith state
is in . If  is a set of constraints, then we define (hACT, OBSi)   to be the set of
minimally ranked histories satisfying every constraint in . We will refer to such histories
as constrained histories and we will refer to a graded world view together with a set of
constraints as a constrained world view.
We have presented constrained world views to illustrate that graded world views are
useful for many problems beyond those that are normally considered to be in the realm of
a standard belief change operator. For example, suppose that Bob sends an encrypted
email message to Alice, inviting her to a party at his house. Bob is aware that Eve is
the system administrator, and that she could potentially manipulate the message before
delivering it. When Alice does not show up, Bob concludes that Eve did not deliver the
message. Bob is concerned that Eve read the message and had hurt feelings that she was
816

fiBelief Change with Uncertain Action Histories

not invited. However, looking at every possible action Eve could take, Bob concludes that
Eve could not have decrypted the message.
In the preceding example, Bob needs to consider all possible actions that Eve could have
executed. The conclusion that Bob draws is that Eves knowledge of the party is invariant
with respect to her actions. We can formally define invariance as follows.
Definition 14 Let hACT, OBSi be a graded world view. We say that a set of states  is an
i-invariant of hACT, OBSi if and only if, for every A  A, Bel(hACT, OBSi  (A, i))  .
The intuition behind i-invariance is that, regardless of the action at time i, the underlying
agent will always believe that the actual world is in . Reasoning about invariant properties
is essential if an agent is trying to ensure some property must hold in an action domain
involving exogenous actions. This is required, for example, in reasoning about cryptographic
protocols.
Reasoning about invariance is just one new kind of problem that can be addressed by
constrained world views. We suggest that constraints can also be used to provide natural
representations of hypothetical reasoning and abductive reasoning.
6.2 Belief Extrapolation
Constrained world views are similar to belief extrapolation operators (Dupin de Saint-Cyr
& Lang, 2011). Briefly, a belief extrapolation operator l takes a sequence of formulas,
called a scenario as input, and it outputs another scenario. The intuition is that the output
gives the most general sequence of formulas that can possibly be true, given the input and
the assumption that fluents tend to be inertial. We give a brief description of the basic
construction.
A trajectory is a sequence  of interpretations over some fixed signature. Let  (i) denote
the ith interpretation in the trajectory  . Given a scenario , let Traj () denote the set of
trajectories that satisfy each formula in  on a point-by-point basis. Every ordering  on
the class of trajectories defines an extrapolation operator l as follows:
|( l (t))| = { (t) |   Min(, Traj ())}
Hence,  l picks out the minimal trajectories satisfying .
For our present purposes, the most important feature of a belief extrapolation operator
is that it is defined with respect to an ordering over histories. Given an ordering over
histories together with a sequence of formulas, a belief extrapolation operator returns the
most plausible sequences of states. In the case of constrained world views, we essentially
do the same thing. The given graded world view defines an ordering over states, and the
constraints give a sequence of conditions that need to be satisfied. It is easy to show
that, using the summation aggregate, every graded world view can be captured by a belief
extrapolation operator. In this section, we consider the converse problem: Can every belief
extrapolation operator be represented by a suitable graded world view?
The key observation here is that the mapping from graded world views to orderings
on histories is not surjective; there are orderings on histories that can not be described
by a graded world view. For example, a graded world view can not explicitly capture
plausibilities of the form if A1 occurs at time i, then A2 is likely to occur at time i + 1.
817

fiHunter & Delgrande

This is easy to see, because a graded world view ranks histories by combining a sequence of
rankings at each instant. Of course, it might be possible to devise a sequence of rankings
along with an aggregate function that happened to support a conditional plausibility for a
fixed action signature. But this would be a domain-specific property, because the temporal
relation between actions can not be expressed with two independent plausibility functions.
Informally, a graded world view can only represent domains where the ordering on histories
is built up in a pointwise manner by the plausibilities at each point in time. In this section,
we use this limitation to establish a difference in expressive power between constrained
world views and belief extrapolation operators.
First, we need to formalize the problem that we would like to address more precisely.
Given a belief extrapolation operator, we would like to be able to find a graded world view
that captures the same information.
Definition 15 Let l be a belief extrapolation operator. We say that l is representable if
there is a graded world view hACT, OBSi such that, for every scenario  of length n,
Traj ( l) = (hACT, OBSi)  .
If l is representable, then the behaviour of l can be simulated with a graded world view.
We remark that we have abused notation in the definition in that Traj ( l) is a collection
of sequences of states, whereas (hACT, OBSi)   is a collection of histories. We interpret
the equality to mean that the two collections are equal if we ignore the action symbols in
the latter.
The following proposition indicates that belief extrapolation operators have an expressive advantage.
Proposition 12 There is a belief extrapolation operator l that is not representable.
Proof Let  be an ordering in which the following trajectories are minimal.
1. h{a, b}, {a, b}, {a, b}i
2. h{a, b}, {a, b}, {a, b}i
3. h{a, b}, {a, b}, {a, b}i
Let l be the associated belief extrapolation operator. We will show that l is not representable.
Let  = ha, a  b, bi. Note that  is satisfied by all three minimal trajectories. Therefore
T raj( l) is precisely the set of minimal trajectories.
Now suppose that hACT, OBSi is a graded world view such that
hACT, OBSi  
assigns minimal plausibility to 1, 2, and 3. Hence, there exist actions A1 , A2 , A3 , B1 , B2 , B3
such that the following sums all obtain the minimum possible rank:
1. OBS0 ({a, b}) + ACT1 (A1 ) + OBS1 ({a, b}) + ACT2 (B1 ) + OBS2 ({a, b})
2. OBS0 ({a, b}) + ACT1 (A2 ) + OBS1 ({a, b}) + ACT2 (B2 ) + OBS2 ({a, b})
818

fiBelief Change with Uncertain Action Histories

3. OBS0 ({a, b}) + ACT1 (A3 ) + OBS1 ({a, b}) + ACT2 (B3 ) + OBS2 ({a, b})
It must be the case that ACT1 (A1 ) = ACT1 (A2 ), because otherwise either 1 or 2 could
be reduced by changing the first action. Similarly, it must be the case that ACT2 (B1 ) =
ACT2 (B3 ), because otherwise either 1 or 3 would not be minimal. So, we can rewrite the
sums as follows:
1. OBS0 ({a, b}) + ACT1 (A1 ) + OBS1 ({a, b}) + ACT2 (B1 ) + OBS2 ({a, b})
2. OBS0 ({a, b}) + ACT1 (A1 ) + OBS1 ({a, b}) + ACT2 (B2 ) + OBS2 ({a, b})
3. OBS0 ({a, b}) + ACT1 (A3 ) + OBS1 ({a, b}) + ACT2 (B1 ) + OBS2 ({a, b})
From 1 and 2, it follows from basic algebra that
ACT2 (B1 ) + OBS2 ({a, b}) = ACT2 (B2 ) + OBS2 ({a, b}).
Substituting this in 3 gives another minimal sum:
OBS0 ({a, b}) + ACT1 (A3 ) + OBS1 ({a, b}) + ACT2 (B2 ) + OBS2 ({a, b}).
This corresponds to the trajectory
h{a, b}, {a, b}, {a, b}i.
Hence, any graded world view assigning minimum plausibility to 1-3, must also assign
minimum plausibility to this fourth trajectory. Informally, if 1-3 are preferred trajectories
according to a graded world view, then we are forced to accept another preferred trajectory.
But we already saw that T raj( l) consists only of 1-3. Therefore l is not representable.

Note that the proof of Proposition 12 is constructive and it demonstrates that there is a
simple, concrete, extrapolation operator that is not representable.
Informally, Proposition 12 follows from the fact that some orderings on histories can not
be defined by a graded world view. This is particularly important in applications where
an agent has preferences over the order in which events occur. In such applications, it can
be useful to assign plausibilities to certain sequences of actions. We suggest, however, that
the class of orderings definable by graded world views is a natural class of orderings. In
particular, there are many action domains where an agent has no preconceived assumptions
about the order that exogenous actions will occur. Graded world views provide a reasonable tool for the representation of such action domains. Graded world views also have an
expressive advantage in that they can explicitly represent fallible actions as well as other
forms of uncertainty over action effects.
We have seen that we can not represent every belief extrapolation operator, but it
would be a mistake to conclude that graded world views are strictly less expressive. Belief
extrapolation really says nothing explicit about actions, which leads to several differences
from our approach. Given an ordering over histories of length n, it is not clear how to use
belief extrapoloation to incorporate a new action followed by a new observation; there is
no fixed method for extending orderings over n-tuples to orderings over n + 1 tuples. In
819

fiHunter & Delgrande

the case of a graded world view, however, it is clear how the new ordering is defined when
more actions are performed. As such, graded world views are more appropriate for the
representation of epistemic action domains where we expect new observations and actions
to occur. Of course, in our approach, we do have a fixed length for the initial graded world
view. Since we can have null actions, however, the value n is really a maximum length for the
sequence of actions that occurred. As such, graded world views are most sensible in domains
where we can imagine the maximum number of actions that have previously occurred is
bounded. Again, we suggest that the class of domains that satisfies this constraint is both
large and natural.
6.3 Beliefs about Action Effects
Despite our flexible representation of observations and actions, there is one aspect of our
representation that is quite rigid. Specifically, we have a single underlying transition system
that gives the effects of actions; there is no mechanism for changing this transition system
in response to new information. This has been addressed by Varzinczak (2010) in a modal
setting, by introducing a formal mechanism that allows the effects of actions to change in
response to new information. Combining this idea with the notion of plausibility in a graded
world view would give an even more complete picture of the way that agents reason about
the effects of actions with uncertain effects. We leave a treatment of this idea for future
work.
A related problem is the fact that the underlying transition system in our framework
is essentially known to the agent. In other words, given perfect information about the
current state of the world and the action that has been executed, then the agent knows
the resulting state. An alternative approach due to Eiter et al. (2010) is to start with
an action description that specifies some actions effects. We can then introduce suitable
information update mechanisms based only on this (partial) description of effects, rather
than by assuming a definite underlying transition system. This is, however, only a superficial
distinction. By using effect ranking functions, we can essentially give a partial description of
action effects that corresponds to an action description. In this manner, it is straightforward
to define so-called Action Description Update problems with graded world views.

7. Conclusion
We have introduced a formalism for reasoning about sequences of actions and observations.
The formalism uses Spohn-style ranking functions at each instant to determine the most
plausible action or observation, and determines the most plausible histories by an aggregate
function over all instants. We have proved that the formalism subsumes belief revision,
belief evolution, and conditionalization. Moreover, it is suitable for the representation of
fallible beliefs, erroneous perception, exogenous actions, and failed actions. We have used
transition systems for the representation of actions in order to facilitate comparison with a
wide range of action formalisms. In future work, we will be interested in axiomatizing the
belief change that is permitted by the class of admissible aggregate functions.
The main advantage of graded world views over related formalisms is that graded world
views provide a uniform mechanism for dealing with imperfect information about actions
and observations. As such, graded world views allow observations that need not be incor820

fiBelief Change with Uncertain Action Histories

porated because the agent is convinced a certain action occurred previously. On the other
hand, graded world views also allow an agent to retract the belief that an action occurred if
they have a strong belief in some observation. Graded world views provide a tool for representing these opposing conclusions, simply by considering the magnitudes of plausibilities,
as well as some underlying aggregate.
In existing work, belief change caused by actions is often represented by starting with
an action formalism and then adding revision operators. One problem with this approach
is that it does not allow beliefs about action occurrences. In a sense, graded world views
take the opposite approach. We start with ranking functions, which were originally defined
for reasoning about belief change in a static environment, and then we plug in actions. An
agents beliefs about the actions that occur are independent of the formal representation
of action effects. As such, although we have presented graded world views in terms of
transition systems, it would certainly be possible to use a different action formalism. The
key point is that, by using ranking functions to represent uncertainty about states and
actions, we can define a framework for reasoning about epistemic action effects in which
primary importance is placed on the evolution of an agents beliefs.
We conclude with a brief remark about the overall approach taken in our framework. A
distinction is commonly drawn between update and revision in the belief change literature,
despite the fact that it can be difficult in practice to determine which is the appropriate
operation given a particular piece of information. In many cases, it simply is not clear if
conflicting information is the result of some unseen action or if it is the result of erroneous
beliefs. One solution to this problem is to define a single, general belief change operation
that subsumes both (Kern-Isberner, 2008). By contrasat, we maintain an explicit distinction
between the way beliefs change due to actions and observations, but we use a uniform model
of uncertainty at the agent level that focuses on finding the most plausible sequence of events
to explain all available evidence. This seems like a natural approach in many applications,
where an agents perception of past events is likely to be influenced by their conviction with
respect to their senses and beliefs.

References
Alchourron, C., Gardenfors, P., & Makinson, D. (1985). On the logic of theory change:
Partial meet functions for contraction and revision. Journal of Symbolic Logic, 50 (2),
510530.
Bacchus, F., Halpern, J., & Levesque, H. (1999). Reasoning about noisy sensors and effectors
in the situation calculus. Artificial Intelligence, 111 (1-2), 171208.
Baltag, A., & Smets, S. (2006). Dynamic belief revision over multi-agent plausibility models.
In Proceedings of Logic and the Foundations of Game and Decision Theory (LOFT),
pp. 1124.
Baltag, A., & Smets, S. (2008). A qualitative theory of dynamic interactive belief revision.
In Proceedings of Logic and the Foundations of Game and Decision Theory (LOFT).
Benferhat, S., Dubois, D., & Prade, H. (1999). Possibilistic and standard probabilistic
semantics of conditional knowledge bases. Journal of Logic and Computation, 9 (6),
873895.
821

fiHunter & Delgrande

Boutilier, C. (1995). Generalized update: Belief change in dynamic settings. In Proceedings
of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI
1995), pp. 15501556.
Boutilier, C. (1996). Iterated revision and minimal change of conditional beliefs. Journal
of Philosophical Logic, 25, 263305.
Britz, K., & Varzinczak, I. (2013). Defeasible modalities. In Proceedings of the 14th Conference on Theoretical Aspects of Rationality and Knowledge (TARK), pp. 4960.
Darwiche, A., & Pearl, J. (1997). On the logic of iterated belief revision. Artificial Intelligence, 89 (1-2), 129.
Delgrande, J. (2004). Preliminary considerations on the modelling of belief change operators by metric spaces. In Proceedings of the 10th International Workshop on NonMonotonic Reasoning (NMR 2004), pp. 118125.
Delgrande, J., Dubois, D., & Lang, J. (2006). Iterated revision as prioritized merging. In
Proceedings of the 10th International Conference on Principles of Knowledge Representation and Reasoning (KR2006).
Delgrande, J., & Levesque, H. (2012). Belief revision with sensing and fallible actions. In
The Thirteenth International Conference on Principles of Knowledge Representation
and Reasoning (KR2012), pp. 148157.
Dupin de Saint-Cyr, F., & Lang, J. (2011). Belief extrapolation (or how to reason about
observations and unpredicted change). Artificial Intelligence, 2, 760790.
Eiter, T., Erdem, E., Fink, M., & Senko, J. (2010). Updating action domain descriptions.
Artificial Intelligence, 174 (15), 11721221.
Friedman, N., & Halpern, J. (2001). Plausibility measures and default reasoning. Journal
of the ACM, 48 (4), 648685.
Gelfond, M., & Lifschitz, V. (1998). Action languages. Linkoping Electronic Articles in
Computer and Information Science, 3 (16), 116.
Giordano, L., Olivetti, N., Gliozzi, V., & Pozzato, G. (2013). A non-monotonic description
logic for reasoning about typicality. Artificial Intelligence, 195, 165202.
Grove, A. (1988). Two modellings for theory change. Journal of Philosophical Logic, 17,
157170.
Hunter, A. (2014). Belief change and non-deterministic actions. In Proceedings of the
Canadian Conference on Artificial Intelligence, pp. 289294.
Hunter, A., & Booth, R. (2015). Trust-sensitive belief revision. In Proceedings of the
International Joint Conference on Artificial Intelligence (IJCAI15).
Hunter, A., & Delgrande, J. (2006). Belief change in the context of fallible actions and observations. In Proceedings of the National Conference on Artificial Intelligence(AAAI06),
pp. 257262.
Hunter, A., & Delgrande, J. (2011). Iterated belief change due to actions and observations.
Journal of Artificial Intelligence Research (JAIR), 40, 269304.
822

fiBelief Change with Uncertain Action Histories

Jin, Y., & Thielscher, M. (2004). Representing beliefs in the fluent calculus. In Proceedings
of the European Conference on Artificial Intelligence(ECAI04).
Katsuno, H., & Mendelzon, A. (1991). On the difference between updating a knowledge base
and revising it. In Proceedings of the Second International Conference on Principles
of Knowledge Representation and Reasoning (KR 1991), pp. 387394.
Katsuno, H., & Mendelzon, A. (1992). On the difference between updating a knowledge
base and revising it. In G ardenfors, P. (Ed.), Belief Revision, pp. 183203. Cambridge
University Press.
Kelly, R., & Pearce, A. (2010). Property persistence in the situation calculus. Artificial
Intelligence, 174 (12-13), 865888.
Kern-Isberner, G. (2008). Linking iterated belief change operations to nonmonotonic reasoning. In Proceedings of the Eleventh International Conference on Principles of
Knowledge Representation and Reasoning (KR08), pp. 166176.
Kraus, S., Lehmann, D., & Magidor, M. (1990). Nonmonotonic reasoning, preferential
models and cumulative logics. Artificial Intelligence, 4, 167207.
Laverny, N., & Lang, J. (2005). From knowledge-based programs to graded belief-based
programs, part i: On-line reasoning. Synthese, 147 (2), 277321.
Lehmann, D., & Magidor, M. (1992). What does a conditional knowledge base entail?.
Artificial Intelligence, 55, 160.
Levesque, H., Pirri, F., & Reiter, R. (1998). Foundations for the situation calculus.
Linkoping Electronic Articles in Computer and Information Science, 3 (18), 118.
Liberatore, P., & Schaerf, M. (2000). BReLS: A system for the integration of knowledge
bases. In Proceedings of KR2000, pp. 145152. Morgan Kaufmann Publishers.
Lorini, E., Jiang, G., & Perrussel, L. (2014). Trust-based belief change. In ECAI 2014 21st European Conference on Artificial Intelligence, pp. 549554.
Moore, R. (1985). A formal theory of knowledge and action. In Hobbs, J., & Moore, R.
(Eds.), Formal Theories of the Commonsense World, pp. 319358. Ablex Publishing.
Nayak, A. (1994). Iterated belief change based on epistemic entrenchment. Erkenntnis, 41,
353390.
Papini, O. (2001). Iterated revision operations stemming from the history of an agents
observations. In Rott, H., & Williams, M. (Eds.), Frontiers in Belief Revision, pp.
279301. Kluwer Academic Publishers.
Shapiro, S., Pagnucco, M., Lesperance, Y., & Levesque, H. (2011). Iterated belief change
in the situation calculus. Artificial Intelligence, 175 (1), 165192.
Spohn, W. (1988). Ordinal conditional functions. A dynamic theory of epistemic states. In
Harper, W., & Skyrms, B. (Eds.), Causation in Decision, Belief Change, and Statistics, vol. II, pp. 105134. Kluwer Academic Publishers.
Van Benthem, J. (2007). Dynamic logic for belief revision. Journal of applied non-classical
logics, 17 (2), 129155.
823

fiHunter & Delgrande

van Ditmarsch, H., van der Hoek, W., & Kooi, B. (2007). Dynamic Epistemic Logic.
Springer.
Varzinczak, I. (2010). On action theory change. Journal of Artificial Intelligence Research
(JAIR), 37, 189246.

824

fiJournal of Artificial Intelligence Research 53 (2015) 375-438

Submitted 01/15; published 07/15

Approximate Value Iteration with
Temporally Extended Actions
Timothy A. Mann
Shie Mannor

mann@ee.technion.ac.il
shie@ee.technion.ac.il

Electrical Engineering
The Technion - Israel Institute of Technology,
Haifa, Israel

Doina Precup

dprecup@cs.mcgill.ca

School of Computer Science
McGill University,
Montreal, QC, H3A2A7, Canada

Abstract
Temporally extended actions have proven useful for reinforcement learning, but their
duration also makes them valuable for efficient planning. The options framework provides
a concrete way to implement and reason about temporally extended actions. Existing
literature has demonstrated the value of planning with options empirically, but there is a
lack of theoretical analysis formalizing when planning with options is more efficient than
planning with primitive actions. We provide a general analysis of the convergence rate of a
popular Approximate Value Iteration (AVI) algorithm called Fitted Value Iteration (FVI)
with options. Our analysis reveals that longer duration options and a pessimistic estimate
of the value function both lead to faster convergence. Furthermore, options can improve
convergence even when they are suboptimal and sparsely distributed throughout the statespace. Next we consider the problem of generating useful options for planning based on a
subset of landmark states. This suggests a new algorithm, Landmark-based AVI (LAVI),
that represents the value function only at the landmark states. We analyze both FVI and
LAVI using the proposed landmark-based options and compare the two algorithms. Our
experimental results in three different domains demonstrate the key properties from the
analysis. Our theoretical and experimental results demonstrate that options can play an
important role in AVI by decreasing approximation error and inducing fast convergence.

1. Introduction
We consider the problem of planning in Markov Decision Processes (MDPs; Puterman,
1994, see Section 2) with large or even infinite state-spaces. In this setting, traditional
planning algorithms, such as Value Iteration (VI) and Policy Iteration (PI), are intractable
because the computational and memory complexities at each iteration scale (polynomially
and linearly, respectively; Littman, Dean, & Kaelbling, 1995) with the number of states in
the target MDP. Approximate Value Iteration (AVI) algorithms are more scalable than VI,
because they compactly represent the value function (Bertsekas & Tsitsiklis, 1996). This
allows AVI algorithms to achieve per iteration computational and memory complexities
that are independent of the size of the state-space. However, there are many challenges to
c
2015
AI Access Foundation. All rights reserved.

fiMann, Mannor, & Precup

using AVI algorithms in practice. AVI and VI often need many iterations to solve the MDP
(Munos & Szepesvari, 2008). It turns out that temporally extended actions can play an
important role in reducing the number of iterations.
The options framework defines a unified abstraction for representing both temporally
extended actions and primitive actions (Sutton, Precup, & Singh, 1999). When an option
is initialized, it immediately selects a primitive action (or lower-level option) to execute but
does not return control to the agent. Then, on each following timestep, the option tests
whether it should return control to the agent that called the option or continue by selecting
another primitive action (or lower-level option). Because they can represent temporally
extended actions, options provide a valuable tool for efficient planning (Sutton et al., 1999;
Silver & Ciosek, 2012). Under most analyses of AVI, one iteration corresponds to planning
one additional timestep into the future. On the other hand, by performing a single iteration
of AVI with temporally extended actions, one iteration could instead correspond to planning
several timesteps into the future. We derive bounds that help us reason about when AVI
with temporally extended actions converges faster than AVI with only primitive actions.
The options framework is appealing for investigating planning with temporally extended
actions. For one thing, the class of options includes both primitive actions as well as a wide
range of temporally extended actions, and many of the well-known properties of Markov
Decision Processes generalize when arbitrary options are added (e.g., Value Iteration and
Policy Iteration still converge, Precup & Sutton, 1997; Precup, Sutton, & Singh, 1998;
Sutton et al., 1999). In addition, much effort has gone into algorithms that learn good
options for exploration (Iba, 1989; Stolle & Precup, 2002; Mannor, Menache, Hoze, & Klein,
2004; Konidaris & Barto, 2007). These algorithms may produce options that are also useful
for planning. Lastly, options allow for greater flexibility when modeling problems where
actions do not have the same temporal resolution. For example, in inventory management
problems where placing orders may not occur at regular intervals (Minner, 2003) or in the
RoboCup Keepaway domain where agents only make decisions when they have control of a
soccer ball (Stone, Sutton, & Kuhlmann, 2005). Thus, options are an important candidate
for investigating planning with temporally extended actions.
1.1 Motivation
Ultimately we care about the time it takes to solve an MDP. However, we focus on analyzing the convergence rate of AVI with options, because a faster convergence rate implies
a solution with fewer iterations. Using the convergence rate we can determine the total
computational cost of planning by bounding the computational cost at each iteration. If
the total computational cost with options is smaller than with primitive actions, planning
with options is faster than planning with primitive actions.
We focus on the convergence rate because it can provide valuable insight about when
planning with options is faster than planning with primitive actions. However, we do not
present the full computational complexity of planning with options because the computational cost per iteration is highly domain dependent. Therefore, the convergence rate of
planning with options gives us insight about when planning with options may be faster than
planning with primitive actions but without getting bogged down in domain specific details. For example, the computational cost of an iteration depends on (1) the computational
376

fiApproximate Value Iteration with Temporally Extended Actions

complexity of simulating an option, (2) the computational complexity of the value function
approximation method, and (3) the number primitive actions and temporally extended
actions that can be initialized at each state.
For the sake of clarity, we discuss how each of these factors can impact the computational
complexity of AVI.
1.1.1 The Cost of Simulating Actions and Options
The computational cost of simulating an option depends on the simulator. We assume
for simplicity that all primitive actions can be simulated with approximately the same
computational cost. The main question is: What is the compuational cost of simulating
temporally extended actions compared to the cost of simulating a primitive actions?
In general, a simulator for primitive actions can be used to simulate options by executing
the sequence of primitive actions prescribed by the option. The computational cost of this
is equal to the cost of simulating the sequence of primitive actions. However, when the
simulator is inexpensive, the simulation costs may be outweighed by the cost of fitting the
data with a function approximator. This can be seen in our experiments in section 5.
In some cases, specialized simulators can be constructed so that temporally extended
actions have approximately the same cost as primitive actions. For discrete state MDPs,
this can be accomplished through a preprocessing step by composing options from primitive
actions (Silver & Ciosek, 2012). In large- or continuous-state MDPs, the linear options
framework enables the construction of option models by composing models for primitive
actions (Sorg & Singh, 2010). In addition, some existing simulators are carefully designed
for simulating actions at both long and short timescales (Chassin, Fuller, & Djilali, 2014).
1.1.2 The Cost of Value Function Approximation
The choice of function approximation architecture can have drastic implications on the
computational cost of each iteration. Ridge Regression, LASSO, SVR, Neural Networks,
etc. all have computational costs that scale with the number of features at varying rates.
In some cases, the cost of training a suitable function approximation architecture may
be significantly more expensive than the cost of querying the simulator. In these cases,
decreasing the number of iterations can result in significant overall computational savings
even if options require more queries to the simulator.
1.1.3 The Number of Actions and Options
At each iteration, AVI samples all actions from a collection of states. If there are a large (or
even infinite) number of primitive actions, planning can be made both more computationally
efficient and sample efficient by planning instead with a smaller number of options.
1.1.4 The Cost of Acquiring Options
One final consideration is the computational cost of acquiring options. If the options are
designed in advance by experts, then there is no additional cost. However, if the options
are discovered or generated, then this cost should be factored into the total cost of the
algorithm. The landmark-based option generation approach proposed in section 5.3.2 has
377

fiMann, Mannor, & Precup

almost no overhead, given a set of landmark states. However, more computationally
expensive methods for acquiring options (Simsek & Barto, 2004; Mannor et al., 2004) could
be justified if the options are reused for planning in many tasks (Fernandez & Veloso,
2006).
1.2 Contributions
The main contributions of this paper are the following:
 We propose the Options Fitted Value Iteration (OFVI) algorithm, which is a variant of the popular Fitted Value (or Q-) Iteration (FVI, Riedmiller, 2005; Munos &
Szepesvari, 2008; Shantia, Begue, & Wiering, 2011) algorithm with samples generated
by options.
 We analyze OFVI in Theorem 1, characterizing the asymptotic loss and the convergence behavior of planning with a given set of options. We give two corollaries
specifying how the bound simplifies when: (1) all the options have a minimum duration d > 1 (Corollary 1) and (2) the option set contains some long duration options
and primitive actions (Corollary 2).
 We introduce a novel method for generating options, based on landmark states.
This suggests a new algorithm, Landmark-based Approximate Value Iteration (LAVI),
that only needs to model the value of a finite set of states rather than the whole value
function.
 We analyze the asymptotic loss and convergence behavior of LAVI in Theorem 2 and
OFVI with landmark options in Theorem 3. Comparing the bounds of LAVI and
OFVI suggests that LAVI may converge faster than OFVI. However, their asymptotic
losses are not directly comparable.
 We provide a detailed experimental comparison of FVI with primitive actions, OFVI
with hand-coded options, OFVI with landmark options, and LAVI. Our experiments
in a domain with realistic pinball-like physics and a complex inventory management
problem, demonstrate that LAVI achieves a favorable performance versus time tradeoff.
The rest of this paper is organized as follows. Section 2 introduces background on
Markov Decision Processes, Dynamic Programming, previous analysis of FVI, Semi-Markov
Decision Processes, and options. Section 3 defines the Options Fitted Value Iteration
(OFVI) algorithm and compares it to Primitive Actions Fitted Value Iteration (PFVI)
considered in previous work. Section 3.2 provides a detailed discussion of the convergence
properties of OFVI under different conditions. Section 4 introduces landmark options and
explains how landmarks can be used to generate useful options for planning. This section also provides analyses the convergence rates of LAVI and OFVI with landmark-based
options. Section 5 provides experiments and results comparing PFVI to OFVI in three
different domains. Section 6 discusses the relationship between the results presented here
and previous work, as well as, extensions and directions for future work.
378

fiApproximate Value Iteration with Temporally Extended Actions

2. Background
Let X be a subset of d-dimensional Euclidean space, M (X) be the set of probability measures on X, and f : X  R be a function from vectors in X to the real numbers. The
max-norm kf k = supxX |f (x)|. For p  1 and   M (X), the (p, )-norm is defined by
1/p
R
kf kp, =
(x)kf (x)kp dx
.
An MDP is defined by a 5-tuple hX, A, P, R, i (Puterman, 1994) where X is a set
of states, A is a set of primitive actions, P maps from state-action pairs to probability
distributions over states, R is a mapping from state-action pairs to reward distributions
bound to the interval [RMAX , RMAX ], and   [0, 1) is a discount factor. Let B(X; VMAX )
denote the set of functions with domain X and range bounded by [VMAX , VMAX ] where
MAX
VMAX  R1
. Throughout this paper we will consider MDPs where X is a bounded subset
of a d-dimensional Euclidean space and A is a finite (non-empty) set of actions.
A policy  : X  A is a mapping from states to actions. We denote the set of deterministic, stationary Markov policies by . The standard objective of planning in an MDP
is to derive a policy    that maximizes
"
#
X

t
V (x) = E
(1)
 Rt (xt , (xt ))|x0 = x,  ,
t=0

where x is the long-term value of following  starting in state x. The function V  is called
the value function with respect to policy  and it is well known that it can be written
recursively as the solution of
Z
 
T V , E [R(x, (x))] +  P (y|x, (x))V  (y)dy,
(2)
where T  is the Bellman operator with respect to  and V  is its unique fixed point. Given
a vector V  B(X; VMAX ), the greedy policy  with respect to V is defined by
Z
(x) = arg max E [R(x, a)] +  P (y|x, (x))V (y)dy .
(3)
aA

We denote the optimal value function by V  = max V  .
Definition 1. A policy   is optimal if its corresponding value function is V  . A policy
 is -optimal if V  (x)  V  (x)   for all x  X.
The Bellman optimality operator T is defined by


Z
(T V )(x) = max E [R(x, a)] +  P (y|x, a)V (y)dy ,
aA

(4)

y

where V  B(X; VMAX ), which is known to have fixed point V  . Value Iteration (VI), a
popular planning algorithm for MDPs, is defined by repeatedly applying (4). The algorithm
produces a series of value function estimates V0 , V1 , V2 , . . . , VK and the greedy policy K is
constructed based on VK . Since VI converges only in the limit, the policy K may not be
optimal. However, we would still like to measure the quality of K compared to   .
To measure the quality of a policy we need to define a notion of loss. The following
defines loss of a policy with respect to a set of states and loss of a policy with respect to a
probability distribution.
379

fiMann, Mannor, & Precup

Definition 2. Let x  X. The subset-loss of a policy  with respect to a set of states
Y  X is defined by
LY () = max (V  (x)  V  (x)) ,
(5)
xY

and we denote the special case where Y  X by L (). Let p  1 and   M (X). The
loss of a policy with respect to a distribution over states  is defined by
Lp, () = kV   V  kp, .

(6)

VI operates on the entire state space. This is how it is able to decrease the L error,
but VI is computationally intractable in MDPs with extremely large or continuous state
spaces. Thus approximate forms of VI generally seeks to decrease the loss with respect to
a probability distribution over the state space.
2.1 Approximate Value Iteration (AVI)
Approximate Value Iteration (AVI) is the family of algorithms that estimate the optimal
(action-)value function by iteratively applying an approximation of the Bellman optimality
operator. There are many possible relaxations of VI. Which states are backed up according to T , the representation of value function estimates, the number of times to sample
from the simulator, etc. all impact the loss of the resulting policy.
One popular family of AVI algorithms are the Fitted Value Iteration (FVI) algorithms.
These algorithms use a function approximator to represent value function estimates at each
iteration. Primitive action Fitted Value Iteration (PFVI) is a generalization of VI to handle
large or continuous state spaces. PFVI runs iteratively producing a sequence of K  1
estimates {Vk }K
k=1 of the optimal value function and returns a policy K that is greedy
with respect to the final estimate VK . During each iteration k, the algorithm computes a
set of empirical estimates Vk of T Vk1 for n states, and then fits a function approximator
to Vk . To generate Vk , n states {xi }ni=1 are sampled from a distribution   M (X). For
a }m and rewards
each sampled state xi and each primitive action a  A, m next states {yi,j
j=1
a }m
th iteration, the estimates of
{ri,j
j=1 are sampled from the MDP simulator S. For the k
the Bellman backups are computed by
m

1 X a
a
Vk (xi ) = max
ri,j + Vk1 (yi,j
) ,
aA m

(7)

j=1

where V0 is the initial estimate of the optimal value function given as an argument to PFVI.
The k th estimate of the optimal value function is obtained by applying a supervised learning
algorithm A, that produces
n fi
fip
X
fi
fi
Vk = arg min
fif (xi )  Vk (xi )fi ,
f F

(8)

i=1

where p  1 and F  B(X; VMAX ) is the hypothesis space of the supervised learning
algorithm.
The work of Munos and Szepesvari (2008) presented a full finite-sample, finite-iteration
analysis of PFVI with guarantees dependent on the Lp -norm rather than the much more
380

fiApproximate Value Iteration with Temporally Extended Actions

conservative infinity/max norm. This enabled analysis of instances of PFVI that use one
of the many supervised learning algorithms minimizing L1 or L2 norm. A key assumption
needed for their analysis is the notion of discounted-average concentrability of future state
distributions.
Assumption 1. [A1(, )] [Discounted-Average Concentrability of Future-State
Distributions] (Munos, 2005; Munos & Szepesvari, 2008) Given two distributions
 and  defined over the state space X, m  1, and m arbitrary policies 1 , 2 , . . . , m , we
assume that P 1 P 2 . . . P m is absolutely continuous with respect to  implying that


 d(P 1 P 2 . . . P m ) 
def
 < + ,
c(m) =
sup 
(9)


d
1 ,2 ,...,m

and we assume that

def

C, = (1  )2

X

m m1 c(m) < +

m1

is the discounted average concentrability coefficient, where P  denotes the transition kernel
induced by executing the action prescribed by the policy .
Intuitively, this assumption prevents too much transition probability mass from landing
on a small number of states. The condition that C, is finite depends on c(m) growing
at most subexponentially. See the work of Munos (2005) for a more complete discussion
of Assumption 1. We note that the work of Farahmand, Munos, and Szepesvari (2010)
presents a refined analysis using the expectation in (9) rather than a supremum. This
results in tighter bounds but the bounds are more difficult to interpret due to a blowup in
notation.
The work of Munos and Szepesvari (2008) shows that given an MDP, if we select probability distributions ,   M (X), a positive integer p, a supervised learning algorithm
A over a bounded function space F that returns the function f  F that minimizes the
empirical p-norm error, V0  F an initial estimate of the optimal value function, and  > 0
and   (0, 1]. Then for any K  1, with probability at least 1  , there exist positive
integers n, m, K such that the policy K returned by PFVI satisfies




2
1/p
K+1 1/p 2 kV  V0 k
Lp, (K ) 
C bp, (T F, F) +  + 
,
(10)
(1  )2 ,
(1  )2
where bp, (T F, F) = sup inf kT f  gkp, is the inherent Bellman error of F with respect
f F gF
T .1 The

to Bellman operator
inherent Bellman error is a measure of how well the chosen
hypothesis space F can represent Vk at each iteration. The first term in (10) is called
the approximation error and corresponds to the error introduced by the inability of the
supervised learning algorithm to exactly capture Vk at each iteration, while the second
term, the estimation error, is due to using a finite number of samples to estimate Vk . The
last term is controlled by the number of iterations K of the algorithm. By increasing K the
1. In this paper, we consider the multi-sample variant of PFVI that uses fresh samples at each iteration.
The bound for the single-sample variant of PFVI, which uses the same batch of samples at each iteration,
is almost identical to (10). See the work of Munos and Szepesvari (2008) for details.

381

fiMann, Mannor, & Precup

last term shrinks exponentially fast. This last term characterizes the convergence rate of the
algorithm. The size of the discount factor  controls the rate of convergence. Convergence
is faster when  is smaller. Unfortunately,  is part of the problem definition. However,
because options execute for multiple timesteps, an option can have an effective discount
factor that is smaller than .
2.2 Semi-Markov Decision Processes
Semi-Markov Decision Processes (SMDPs) are a generalization of the Markov Decision
Process (MDP) model that incorporates temporally extended actions. Temporally extended
actions have primarily been applied to direct exploration in reinforcement learning (Iba,
1989; Mannor et al., 2004; Konidaris & Barto, 2007; Jong & Stone, 2008). However, they
may also play an important role in planning (Precup & Sutton, 1997; Precup et al., 1998;
Sutton et al., 1999; Silver & Ciosek, 2012). For example, the popular dynamic programming
algorithms VI and PI still converge when applied to SMDPs (Puterman, 1994). The work of
Precup et al. (1998) shows that options and an MDP form an SMDP. The works of Sutton
et al. (1999) and Silver and Ciosek (2012) provide experimental results demonstrating that
options can speed up planning in finite state MDPs. However, these works did not apply
options to tasks with continuous state spaces and there is no theoretical analysis of the
convergence rate of planning with options compared to planning with primitive actions. We
will use the SMDP framework to investigate planning with temporally extended actions.
An MDP paired with a set of temporally extended actions called options, denoted by
O, forms an SMDP.
Definition 3. (Sutton et al., 1999) An option o is defined by a 3-tuple hIo , o , o i where
Io is the set of states that o can be initialized from, o is the stationary policy defined
over primitive actions followed during the lifetime of o, and o : X  [0, 1] determines the
probability that o will terminate while in a given state.
For each state x  X, we denote the set of options that can be initialized from x by
Ox = {o  O | x  Io }. Options are a generalization of actions. In fact they encompass,
not only primitive actions and temporally extended actions, but also stationary policies and
other control structures. Here we take actions to be options that always terminate after
only a finite number of timesteps. Policies on the other hand never terminate. For example,
a stationary policy can be represented by an option by setting the termination probabilities
to (x) = 0 for all states.
For an option o = hIo , o , o i, we denote the probability that o is initialized from a state
o
x and terminates in a subset of states Y  X in exactly t timesteps by P
t (Y |x) and the
P
t o
discounted termination state probability distribution of o by Peo (Y |x) = 
t=1  Pt (Y |x).
For a state-option pair (x, o), the discounted cumulative reward distribution during the
e o).
options execution is denoted by R(x,
The objective of planning with options is to derive a policy  : X  O from states to
options that maximizes
h
i Z
e (x)) + Pe(x) (y|x)V  (y)dy .
V  (x) = E R(x,
382

(11)

fiApproximate Value Iteration with Temporally Extended Actions

The Bellman operator for an SMDP is defined by
 h

i Z
o
e o) + Pe (y|x)V (y)dy ,
(TV )(x) = max E R(x,
oOx

(12)

where T is defined over the set of options O instead of primitive actions A. The differences
between (4) and (12) could potentially lead to widely different results when embedded in
the FVI algorithm.

3. Options Fitted Value Iteration
Algorithm 1 Options Fitted Value Iteration (OFVI)
Require: Collection of options O, an SMDP simulator S, state distribution , function
space F, initial iterate V0  F, n the number of states to sample, m the number of
samples to obtain from each state-option pair, K the number of times to iterate before
returning
1: for k = 1, 2, . . . , K do {Generate K iterates V1 , V2 , . . . VK .}
2:
{Collect new batch of samples.}
3:
for i = 1, 2, . . . , n do {Sample N states.}
4:
x   {Sample a state from distribution .}
5:
for o  Ox do
6:
for j = 1, 2, . . ., m do
o , ro ,  o
7:
yi,j
i,j i,j  S(x, o) {Query the simulator for a terminal state, discounted
cumulative reward, and duration of executing (x, o).}
8:
end for
9:
end for
10:
end for
11:
{Estimate Bellman
Backups.}
h
i
o
1 Pm
o
o )
12:
V  m j=1 ri,j +  i,j Vk1 (yi,j
{Find the best fitting approximation to V .}
14:
Vk = arg inf f F kf  V kn
15: end for
16: return K {Return the greedy policy wrt VK .}
13:

Algorithm 1 is a generalization of the multisample FVI algorithm to the case where
samples are generated by options (with primitive actions as a special case). The algorithm,
Options Fitted Value Iteration (OFVI), takes as arguments positive integers n, m, K,  
M (X), an initial value function estimate V0  F, and a simulator S. At each iteration
k = 1, 2, . . . , K, states xi   for i = 1, 2, . . . , n are sampled, and for each option o  Oxi ,
o , r o ,  o i  S(x , o) are sampled
m next states, rewards, and option execution times hyi,j
i
i,j i,j
for j = 1, 2, . . . , m. Then the update resulting from applying the Bellman operator to the
previous iterate Vk1 is estimated by
m
i
o
1 Xh o
o
Vk (xi )  max
ri,j +  i,j Vk1 (yi,j
) ,
(13)
oOxi m
j=1

383

fiMann, Mannor, & Precup

and we apply a supervised learning algorithm to obtain the best fit according to (8). The
given simulator S differs from the simulator for PFVI. It returns the state where the option
returned control to the agent, the total cumulative, discounted reward received during its
execution, and the duration or number of timesteps that the option executed. This additional information is needed to compute (13). Otherwise the differences between PFVI and
OFVI are minor and it is natural to ask if OFVI has similar finite-sample and convergence
behavior compared to PFVI.
3.1 Simple Analysis
Notice that PFVI is a special case of OFVI where the given options contain only the
primitive action set (i.e., O  A). Therefore, we cannot expect OFVI to always outperform
PFVI. Instead, we aim to show that OFVI does not converge more slowly than PFVI and
identify cases where OFVI converges more quickly than PFVI. The following proposition
provides a general upper bound on the loss of the policy derived by OFVI when O contains
A. This bound can be compared to bounds on the loss of the policy derived by PFVI.
Proposition 1. For any S ,  > 0 and K  1. Fix p  1. Let O be a set of options that
contains the set of primitive actions A. Given an initial state distribution   M (X), a
sampling distribution   M (X), and V0  B(X, VMAX ), if A1(, ) (Assumption 1) holds,
then there exists positive integers n and m such that when OFVI is executed,




2
1/p
K+1 1/p 2 kV  V0 k
Lp, (K ) 
C bp, (TF, F) + S + 
(14)
(1  )2 ,
(1  )2
holds with probability at least 1  .
A proof of Proposition 1 as well as sufficient values for n and m are given in the appendix. Proposition 1 suggests that as long as O contains A, OFVI has performance at least
comparable to PFVI (if not better). There are two main differences between the bound in
Proposition 1 and in the work of Munos and Szepesvari (2008, Thm. 2). First, the inherent
Bellman error in Proposition 1 may be larger than the inherent Bellman error with only
primitive actions. Second, the convergence rate of OFVI tracks the convergence rate of the
SMDP Bellman operator T rather than the MDP Bellman operator T .
Proposition 1 implies that OFVI converges approximately as fast as PFVI when O
contains A. However, the two algorithms may converge to different value functions due to
the larger inherent Bellman error of OFVI.
Proposition 1 has two limitations. First it only considers the case where O contains
A. Second, it does not describe when OFVI converges more quickly than PFVI. In the
following section, we will investigate both of these possibilities.
3.2 General Analysis
There are two perspectives that explain how applying options to AVI can decrease the
number of iterations needed to find a near-optimal policy.
In the first perspective, options increase information flow between otherwise temporally
disparate states facilitating fast propagation of value throughout the state-space. For example, if it takes many primitive actions to transition from a state x to a state y, then planning
384

fiApproximate Value Iteration with Temporally Extended Actions

with primitive actions will require many iterations before information can be propagated
from y back to x. However, given an option that when initialized in state x, terminates in
y, value from y is propagated back to x at every iteration.
In the second perspective, options with long duration can cause rapid contraction toward the optimal or a near-optimal value function. For the discounted and average reward
objectives, the proof that VI converges is based on a contraction argument (for details,
see Puterman, 1994). It turns out that options with long duration can induce a faster
contraction than primitive actions (or faster than options with shorter durations).
How these options influence the convergence rate of AVI depends critically on the agents
objective. In this paper, we only analyze the discounted reward objective. However, to put
our results into context, in this section, we comment on the finite horizon and average
reward objectives as well.
 Undiscounted, Finite Horizon: The agent maximizes the sum of rewards received
over H  1 timesteps. Here options can short circuit the number of iterations needed
to propagate reward back H steps. This effect is more naturally described as increasing
the information flow between temporally disparate states.
 Discounted Reward (our analysis): Our analysis uses a contraction argument to
show faster convergence and an information flow argument to show that fast contraction can occur even when the temporally extended actions are sparsely distributed in
the state-space.
 Average Reward: The agent maximizes the average of an infinite sequence of rewards. While we only consider the discounted reward setting, similar contraction
arguments could provably be applied to show how options can produce a closer approximation of the optimal value function with fewer iterations.
Our approach is based on a contraction mapping argument. By applying the MDP
Bellman operator T to V  B(X, VMAX ), we obtain the following contraction mapping
kV   T V k  kV   V k

(15)

where  (the discount factor) serves as the contraction coefficient. Since  < 1, the left hand
side is strictly smaller than kV   V k . Smaller values of  imply a faster convergence rate,
but the discount factor  is part of the problem description and cannot be changed. However,
if we apply the MDP Bellman operator T ,  > 1 times, then we obtain a contraction
mapping where the contraction coefficient is   < . Temporally extended options have a
similar effect. Options can speed up the convergence rate of the SMDP Bellman operator T
by inducing a smaller contraction coefficient that depends on the number of timesteps that
the option executes for.
Intuitively, options with a long duration are desirable for planning because options
that execute for many timesteps enable OFVI to look far into the future during a single
iteration. However, the duration depends on both an option and the state where the option
is initialized. The following definition makes the notion of an options duration precise.

385

fiMann, Mannor, & Precup

Definition 4. Let x  X be a state and o  Ox be an option. The duration of executing
option o from state x is the number of timesteps that o executes before terminating (i.e.,
o
returning control to the option policy). We denote by Dx,Y
the random variable representing
the duration of initializing option o from state x and terminating in Y  X. For
h a set
i of
o
options O, we define the minimum duration to be dmin = min inf Y X E Dx,Y .
xX,oOx

First notice that the duration of an option is a random variable that depends on the
state where the option was initialized. This complicates the analysis compared to assuming
that all temporally extended actions terminate after a fixed number of timesteps, but it
allows for much greater flexibility when selecting options to use for planning.
Similar to the analysis of PFVI, the analysis of OFVI depends on the concentrability
of future state distributions. We introduce the following assumption on the future state
distributions of MDPs with a set of options. The given set of options O may or may not
contain the entire set of primitive actions A from the underlying MDP.
Assumption 2. [A2(, )] [Option-Policy Discounted-Average Concentrability of
Future-State Distributions] Given two distributions  and  defined over the state
space X, m  1, t  m, and m arbitrary option policies 1 , 2 , . . . , m , we assume that
Pt1 2 ...m is absolutely continuous with respect to  implying that


 d (Pt1 2 ...m ) 

 < + ,
ct (m) =
sup
(16)


d
1 ,2 ,...,m

and we assume that
C, = (1  )

2


X

t t1

t=1

max

m{1,2,...,t}

ct (m) < +

(17)

is the option discounted average concentrability coefficient, where Pt1 2 ...m (y) assigns
probability mass according to the event that a sequence of m options will terminate in a
state y exactly t timesteps after an initial state is sampled from  and a sequence of m
options are executed where the ith option in the sequence is chosen according to i .
Assumption 2 is analogous to Assumption 1. Despite the fact that options are a more
general framework than the set of primitive actions, Assumption 2 results in a smaller
concentrability coefficient than Assumption 1.
Lemma 1. Let ,   M (X). Assumption A1(, ) implies Assumption A2(, ) (i.e.,
C,  C, ).
Proof. First notice that since any t timestep sequence of actions generated by a sequence
1 , 2 , . . . , m of m  t option policies can be expressed by a sequence of t primitive policies
1 , 2 , . . . , t . Thus
   ...m 
 d(Pt 1 2
)

ct (m) = sup1 ,2 ,...,m 
d


  


t 
1
2

 sup1 ,2 ,...,t  d(P Pd ...P ) 


= c(t) .

386

fiApproximate Value Iteration with Temporally Extended Actions

Since ct (m)  c(t) for all m  1 and t  m, then
P
t1 max
C, = (1  )2 
m{1,2,...,t} ct (m)
t=1 t
P

2
t1
 (1  )
t
c(t)
t=1
= C, .

Lemma 1 implies that Assumption 2 holds for any set of options whenever Assumption
1 holds. The main reason is because a sequence of m options that executes for t timesteps
has fewer degrees of freedom than a sequence of t primitive policies. Furthermore, the proof
of Lemma 1 tells us that the important property of the discounted concentrability of future
states is not the number of options executed in a sequence but the number of timesteps
that the sequence of options executes for.
In our analysis of the convergence rates of OFVI, we will report bounds containing the
coefficient C, rather than C, . This is because, in cases where the option set contains
mostly temporally extended actions, C, may be smaller than C, . However, Lemma 1
tells us that we can replace C, in the bounds with C, for the purposes of comparing
with (10) and (14).
The important properties of temporally extended actions that cause faster convergence
are (1) the quality of the policy they follow, and (2) how long the action executes for (or its
duration). The following definition describes the set of states where there exists an option
that follows a near-optimal policy and has sufficient duration.
Definition 5. Let X be the set of states in an MDP with option set O,   0, and d  dmin .
The (, d)-omega set defined by


 o 


,d  x  X | oOx s.t. inf E Dx,Y  d  Q (x, o)  V (x)   ,
(18)
Y X

is the set of states where there is an -optimal temporally extended action with duration
longer than d and where  is the optimal option policy.
The states in ,d have particularly long duration and follow an -optimal policy. However, the states outside of ,d do not. At these other states, either the available options are
not sufficiently temporally extended or they follow a suboptimal policy. To obtain faster
convergence, we need a way of connecting the convergence rates of the states outside of ,d
with the convergence rates of the states in ,d .
Assumption 3. [A3(, d, , , j)] Let , , j  0, d  dmin , and   M (X). For any m  0
option policies 1 , 2 , . . . , m , let  = P 1 P 2 . . . P m . There exists an -optimal option
policy  such that either (1) Prx [x  ,d ]  1   or (2) i{1,2,...,j} Pryi [y  ,d ] 
i
1   where i = P 1 P 2 . . . P m P  for i = 1, 2, . . . , j.
Assumption 3 points out three key features that impact planning performance with
options:
1. Quality of the option set controlled by ,
387

fiMann, Mannor, & Precup

2. Duration of options specified by d, and
3. Sparsity of ,d in the state-space characterized by j and .
We refer to the policy  as the bridge policy, because it bridges the gap between the
states in ,d and other states. Notice that we do not assume that the planner has any
knowledge of . It is enough that such a policy exists. Assumption 3 says that no matter
what policies are followed, either (1) the agent will end up in ,d with high probability or
(2) there exists a near-optimal option policy that will transport the agent to ,d in at most
j timesteps with high probability. This enables us to account for problems where only a few
states have temporally extended actions, but these states can be reached quickly without
following a policy that is too suboptimal.
The following theorem provides a comprehensive description of the convergence behavior
of OFVI (with PFVI as a special case where O = A).
Theorem 1. Let S ,  > 0, , , j  0, K, p  1, d  dmin , 0  Z  K, and ,   M (X).
Suppose that A2(, ) (Assumption 2) and A3(, d, , , j) (Assumption 3) hold. Given
V0  B(X, VMAX ), if the first Z iterates {Vk }Z
k=0 produced by the algorithm are pessimistic

(i.e., Vk (x)  V  (x) for all x  X), then there exists positive integers n and m such that
when OFVI is executed,
Lp, (K )  Lp, ( ) +

2 dmin 1/p
C (bp, (TF, F) + ) + S
(1  )2 ,

1/p
+  dmin (K+1)+(1)(ddmin )bZ/jc

 
 !
2 V   V0 
(19)
(1  )2

holds with probability at least 1   where  is the optimal option policy with respect to the
given options O and j = j + 1.
Theorem 1 bounds the loss of the option policy K returned after performing K  1
iterations of value iteration with respect to a (p, )-norm. The distribution  can be thought
of as an initial state distribution. It places more probability mass on the regions of the
state space where we want the policy K to have the best performance. The value of p  1
is generally determined by the function approximation procedure. For p = 1, the function
approximation procedure minimizes the L1 -norm and for p = 2, the function approximation
procedure minimizes the L2 -norm.
The right hand side of (19) contains four terms.
1. The first term bounds the abstraction loss, which is the loss between the optimal
policy over primitive actions and the optimal option policy.
2. The second term bounds the approximation error, which is the error caused by the
inability of the function approximation architecture to exactly fit Vk (xi ) during each
iteration and  which shows up in this term is due to bootstrapping off options that
 dmin
follow -optimal policies to gain faster convergence. Notice that (1)
2 shrinks as
dmin grows. Thus option sets with longer minimum duration shrink the approximation
error.
388

fiApproximate Value Iteration with Temporally Extended Actions

3. The third term S is the sample error, which is controlled by the number of samples
taken at each iteration.
4. The last term controls the convergence error. Notice that , the discount factor, is in
[0, 1) and therefore the last term shrinks rapidly as its exponent grows. While OFVI
does not actually converge in the sense that the loss may never go to zero, this last
term goes to zero as K  . In the worst case, the convergence rate is controlled by
 dmin (K+1) , but the convergence rate can be significantly faster if Z and d are large
and j is small.
An iterate V : X  R is pessimistic if


yX V (y)  V  (y) ,
where  is the optimal policy defined over option set O. Whether iterates are pessimistic
(or not) has a critical impact on the convergence rate of OFVI. To understand why, suppose
that q  Ox is an option that can be initialized from a state x  X where q is -optimal with


respect to  (i.e., Q (x, q)  V  (x)  ) and has a long duration (at least d timesteps).
If V is pessimistic, then the Bellman optimality operator performs an update
 h

i Z
o
e
e
(TV )(x) = max E R(x, o) + P (y|x)V (y)dy ,
By definition (12).
oOx
h
i Z
e q) + Peq (y|x)V (y)dy .
E R(x,
The update with option q.



Since T is known to be a monotone operator, V  (x)  (TV )(x). Taken together, these
facts imply that even if an option other than q was selected for the update, (TV )(x) is at

least as close to V  (x) as if q was selected. This allows us to prove that when the iterates
are pessimistic, the convergence rate of OFVI is rapid (depending on d). Unfortunately,
when the iterates are not pessimistic, this reasoning no longer holds and convergence may
depend on options with duration dmin instead.
On each of the Z iterations where the estimate of the value function is pessimistic, OFVI
exploits the options with duration d rather than dmin . However, it can only get samples
of these options from states in ,d . The states outside of ,d can benefit from the rapid
convergence of the states in ,d but only after j additional iterations. The main reason is
because it can take j steps to propagate value from states in ,d back to the other states.
Using Theorem 1 it is possible to consider the convergence rates of OFVI on a wide range
of planning problems. In the following subsections, we examine special cases of Theorem
1. First, we consider what happens when dmin is greater than 1 ignoring the possibility of
exploiting options with longer duration. Second, we consider what happens when we mix
primitive actions with temporally extended actions.
3.2.1 Abstraction
An important case involves planning where only temporally extended actions are available.
The main advantage in this case is that we can guarantee an upper bound on the convergence
389

fiMann, Mannor, & Precup

rate of the algorithm is strictly faster than the upper bound for PFVI. However, the solution
that OFVI converges to may be inferior to the solution converged to by PFVI if the best
policy with respect to the given set of options is poor.
Corollary 1. Let S ,  > 0, K, p, dmin  1, and ,   M (X). Given V0  B(X, VMAX ),
if A2(, ) (Assumption 2) and A3( = 0, d  dmin ,  = 0, , j = 0) (Assumption 3) hold,
then there exist positive integers n and m such that when OFVI is executed
Lp, (K )  Lp, ( ) +

2 dmin 1/p
C bp, (TF, F) + S
(1  )2 ,

1/p
+  dmin (K+1)

 
 !
2 V   V0 
(20)
(1  )2

holds with probability at least 1  , where  is the optimal option policy with respect to
the given options O.
First notice that in Corollary 1, the upper bound is with respect to the loss of an optimal
policy   (over primitive actions). The bound on the loss in Corollary 1 depends on four
terms,
1. the first term is controlled by the error between the optimal policy   and the best
policy  with respect to the given options O,
2. the second term is controlled by the option policy future state concentrability coefficient C, and inherent bellman error bp, (TF, F),
3. the third term is simply the estimation error term  (which is controlled by the amount
of sampling done by OFVI), and

4. the last term is the convergence error controlled by  dmin (K+1) .
When dmin > 1 the convergence rate of OFVI can be significantly faster than PFVI, but
the loss term Lp, ( ) may be large if the given option set O cannot represent a sufficiently
good policy.
Although the abstraction setting has a fast convergence rate, the quality of the policies
produced depends on the best possible option policy derived from the given set of options.
If this policy is poor, then the policy produced by OFVI will also be poor. In the next
subsection, we try to overcome this limitation by augmenting the set of primitive actions
instead of discarding them.
3.2.2 Augmentation with Sparsely Scattered Temporally Extended Actions
Experimental results have demonstrated that a few well placed temporally extended actions often improve the convergence rate of planning (Precup et al., 1998). We would like
to describe conditions where sparsely scattered temporally extended actions cause faster
convergence.
The following theorem gives a bound for OFVI in environments with sparsely distributed
temporally extended actions.
390

fiApproximate Value Iteration with Temporally Extended Actions

Corollary 2. Let S ,  > 0, ,   0, K, p, d, j  1, 0  Z  K, and ,   M (X).
Suppose that A2(, ) (Assumption 2) and A3(, d, , , j) (Assumption 3) hold. Given
V0  B(X, VMAX ), if the first Z iterates {Vk }Z
k=0 produced by the algorithm are pessimistic
(i.e., Vk (x)  V  (x) for all x  X), then there exist positive integers n and m such that
when OFVI is executed,
Lp, (K ) 

2
C 1/p (bp, (TF, F) + ) + S
(1  )2 ,

1/p  2 kV   V k 
0 
K+1+(1)(d1)bZ/jc
+ 
(21)
(1  )2

holds with probability at least 1   and j = j + 1.
 =
Notice that the abstraction loss Lp, ( ) disappears because in this special case VM
VM . The improvement in convergence rate is only on the first Z pessimistic iterates and
a small penalty  appears in the approximation error term due to our exploitation of optimal temporally extended actions. The convergence rate of Corollary 2 is driven by
 K+1+(1)(d1)bZ/jc   K+1 , demonstrating that OFVI can converge faster than PFVI.
Notice that when j is large, meaning that it can take more timesteps to visit ,d , the
convergence rate is slower than when j is small. This means that convergence improvement
may be less dramatic when the temporally extended actions are too sparse. When j = 1,
the convergence rate is controlled by  K+1+(1)(d1)Z   K+1 .


4. Generating Options via Landmarks
One limitation of planning with options is that options typically need to be designed by
an expert. In this section, we consider one approach to generating options automatically.
Our approach is similar in spirit to the successful FF-Replan algorithm (Yoon, Fern, &
Givan, 2007), which plans on a deterministic projection of the target MDP. The algorithm
replans whenever the agent enters a state that is not part of the current plan. However,
unlike FF-Replan, our approach is more scalable as it does not plan globally over the entire
system.
More specifically, we assume access to a simulator SM for the target MDP M =
c
b
hX, A, P, R, i and a simulator SM
c for a relaxed MDP M = hX, A, P , R, i with deterministic transition dynamics. Given a state x and option o  Ox , a simulator S returns
the discounted cumulative reward R of executing the option, the duration of the options
execution  , and the termination state y.
c has deterministic transition probabilities, its dynamics are captured by a diSince M
rected graph G = hX, Pbi. Furthermore, R specifies the reward associated with each edge.
If two or more actions transition from a state x to the same state y, the maximum reward
of these actions is associated with the edge (x, y) in G. We denote a maximum reward path
from x  X to g  X by pG (x, g) and the length of the maximum reward path by |pG (x, g)|.
Throughout this section we will assume that the rewards are all non-positive (i.e. bound to
[RMAX , 0]). The reason for this is because stochastic shortest path problems are undefined
when the MDP contains positive reward cycles, however, in our experiments we relax this
assumption.
391

fiMann, Mannor, & Precup

^
M
x

l4

l6
l5

Optimal Trajectory
Optimal Landmark Trajectory
Landmark Trajectory
w/Local Planner

l3

l2

l1
(a)

l4

M
x

l2

l7

g
l8

l5

l6

l7

g
l8

Optimal Trajectory
Noise-Free Landmark Trajectory
Landmark Trajectory
w/Local Planner

l3

l1
(b)

Figure 1: A trajectory from state x to state g. Error is introduced by planning a policy over
landmark options rather than following the optimal policy. (a) In the determinc, errors are caused by the landmarks not being exactly on the
istic relaxation M
optimal trajectory and the local planner not taking the maximum reward path
from one landmark to another. (b) In the stochastic target problem M , errors
are introduced by noise, which causes the agent to only reach states nearby the
landmark states on its path to the goal.

392

fiApproximate Value Iteration with Temporally Extended Actions

c is that computationally efficient planning algorithms
The purpose of introducing M
are known for minimum cost path problems (Dijkstra, 1959; Hart, Nilsson, & Raphael,
1968), which are equivalent to maximum reward path planning problems provided there
c is a reasonable approximation for M , then we
are no positive reward cycles. Thus, if M
c. We assume an efficient local
can dynamically generate options for M by planning on M
planner P exists for G.
However, in very large directed graphs, even so called efficient algorithms can be
computationally expensive. Thus, we assume that P has a given maximum planning horizon
d+  1.
Recent work on finding minimum cost paths in very large graphs has shown that paths
can be found more efficiently by introducing landmarks (Sanders & Schultes, 2005), an
intuition that has been used in robotic control for a long time (Lazanas & Latombe, 1992).
Definition 6. A landmark set L is a finite, non-empty subset of the state-space.
Each landmark is a single state, and a landmark set induces a directed graph over the
state-space. Obtaining a provably good landmark set is generally a hard problem (Peleg
& Schaffer, 1989). Here we assume that the landmark states are given, but they could be
c as in the work of Simsek and Barto (2004) or
acquired through analyzing the dynamics of M
from demonstrations as in the work of Konidaris, Kuindersma, Barto, and Grupen (2010).
Given a set of landmarks, we can define a corresponding set of options, as follows.
Definition 7. Let   0 and  be a metric over the state-space. For landmark set L
and local planner P, the set of landmark options, denoted by O, contains one option
ol = hIl , l , l i for each landmark state l  L, where
1. Il = {x  X||P(x, l)|  d+ } is the initialization set,
2. l (x) = P(x, l) is the policy for x  X, and

1 if (x, l)    x 
/ Il
3. l (x) =
defines termination probabilities for each state
0 otherwise
x  X.
c,
In other words, landmark options result from planning on the deterministic MDP M
and they terminate once the vicinity of the landmark has been reached. A landmark ls
option can only be executed from states where reaching l in the graph would take less
than d+ timesteps. Once discovered, these options will be executed in the target MDP M .
We denote the number of valid landmark-option pairs by L. Note that in principle, some
landmarks might not be reachable within the given planning horizon.
The idea is to plan using only the set of landmark options. To achieve this, we require
that the local planner can derive a path to at least one landmark state from every state in
X:
Assumption 4. For all x  X there exists l  L such that |P(x, l)|  d+ .
In a deterministic MDP, starting in a landmark state, it is possible to avoid visiting
non-landmark states (Figure 1a). In this case, it is possible to ignore the other states and
393

fiMann, Mannor, & Precup

plan entirely based on the landmark states. However, in stochastic MDPs, landmark options
will not always terminate in landmark states. We solve this problem by allowing landmark
options to terminate near landmark states (Figure 1b).
Landmark-based options can be used directly with OFVI or alternatively can be used
to create a new AVI algorithm that only maintains estimates of the value function at the
finite number of landmark states and therefore avoids explicit function approximation. In
this section, we discuss both of these approaches and analyze their convergence properties.
4.1 Landmark-Based Approximate Value Iteration
Landmark-based Approximate Value Iteration (LAVI), Algorithm 2, belongs to the family
of AVI algorithms. It takes as arguments: (1) K the number of iterations to perform, (2)

a landmark set L, (3) an initial guess V0 of the value function V  for states in L, (4)
the number of times to sample each landmark-option pair during updates m, and (5) a
simulator S. As output, the algorithm produces value estimates for the landmark states L.
Algorithm 2 Landmark-based AVI
Require: K, L, V0 , m, S
1: for k = 1, 2, . . . , K do
2:
for l  L do
3:
for ol  Ol do
(j)
4:
(Rl,o ,  (j) , y (j) )  S(l, ol ) for j = 1, 2, . . . , m
5:
end for
(j)
1 Pm
 (j) (V
(j)
6:
Vk (l)  max m
k1 , y )
j=1 Rl,o + 
oOl

end for
8: end for
9: return VK
7:

Unlike basic VI, LAVI scales to large or infinite MDPs because it only estimates values
for the landmark states, while at the same time avoiding the use of complicated function
approximation algorithms.
If the target MDP M has deterministic dynamics, then we can ensure that options will
always terminate in landmark states. So we can construct backups directly with V (y) where
y  L. However, when M has stochastic dynamics, it may be impossible to guarantee that
all options terminate in landmark states. A less restrictive requirement is to assume that
options terminate near a landmark state with high probability. This notion of closeness
requires that we have a metric  : X  X  [0, ). For some small positive constant  and
a state x  X, we define
L (x) = {l  L | (x, l) < }

(22)

to be the set of landmark states that are closer than  to x  X. The function

(V, x) =

maxlL (x) V (l) if L (x) 6=  ,
0
otherwise
394

(23)

fiApproximate Value Iteration with Temporally Extended Actions

takes into consideration the fact that options do not necessarily terminate in landmark
states. If the distance between the termination state and some landmark l is less than ,
then we plug in V (l). Otherwise, we assume a value of 0.
After Algorithm 2 returns its K th estimate of the landmark values VK , we define the
greedy policy for LAVI to be
!
 Z
X
K (x) = arg max Rxo +
 t Pto (y|x)(VK , y)dy .
(24)
oOx

t=1

4.1.1 Analysis
We provide a theoretical analysis of LAVI along two dimensions. (1) We bound the loss
associated with policies returned by LAVI compared to the optimal policy over primitive
actions, and (2) we analyze the convergence rate of LAVI. To save space, the proofs are
deferred to Appendix C.
c. Thus, no error is introduced by stochasticity in the
For deterministic MDPs, M = M
environment. However, the selection of landmark states and the local planner can both
introduce error.
Definition 8. (Landmark Error) Given a landmark set L, the smallest L such that for all
x  X and some l  {l0  L | dbmin  |P(x, l0 )|  dmax }



|pG (x,l)| V  (l)   ,

VM
(x)

R
+

(25)
L
pG (x,l)
c
c
M
 is the optimal value function for M
c and Rp (x,l)
is called the landmark error where V c
G
M
c.
is the discounted reward of the optimal path from x to l in M

The landmark error quantifies how well the chosen landmark states preserve maximum
reward paths. In Figure 1a, the landmark error is represented by the distance of the
landmarks from the optimal trajectory. This definition assumes that our local planner is
optimal, however, it may be convenient to use a suboptimal local planner.
Definition 9. (Local Planning Error) Given a local planner P and landmark set L, the
smallest P such that for all x  X and l  L where P(x, l) < d+ , the path P(x, l) generated
by P satisfies

 



|P(x,l)| 
RpG (x,l) +  |pG (x,l)| VM
(l)

R
+

V
(l)
 P ,
(26)
P(x,l)
c
c
M
is called the local planning error.
The local planning error quantifies the loss due to using the local planner P instead
of a planner that returns the maximum reward path from x to a nearby landmark state.
In Figure 1a, the local planning error is represented by the trajectory (dashed line) that
transitions from landmark to landmark, but does not follow the shortest path between
landmarks.
So far, we have only considered factors that impact planning error when the environment
is deterministic. When M contains stochastic dynamics, we need a way to bound the error
395

fiValue

Mann, Mannor, & Precup




V*
State-Space

Figure 2: Optimal value function over a one dimensional state space with landmarks depicted by black circles. The gray hourglass shapes around the landmarks depict
 to change slowly around the
landmark error. Assumption 5 only requires VM
landmarks. The value function may change rapidly at regions with no landmarks.

c. When M is stochastic, a landmark option may
of following a policy in M planned in M
have trouble reaching a particular state. Thus, we need to relax the condition that an
option always terminates at a landmark state.
Assumption 5. (Locally Lipschitz around Landmarks) We are given a metric  over the
 (l)  V  (x)(x, l)
state-space X such that for all l  L and x  X, if (x, l) < , then VM
M
for some   0.
 does not change too dramatically
Assumption 5 says that the optimal value function VM
for states that are close to landmark states. If this assumption is violated, options terminating arbitrarily close to a landmark state may have unboundedly lower value with respect to
 . This would lead to unboundedly suboptimal landmark policies. Thus, Assumption 5 is
VM
critical to obtain meaningful bounds on the quality of landmark policies. Notice, however,
that Assumption 5 only applies near the landmarks. Figure 2 depicts a value function for a
 can change rapidly in regions
one-dimensional state-space that illustrates the fact that VM
of the state-space that are not too close to a landmark.
Assumption 5 allows us to treat the hypersphere with radius  around a landmark state
all as one state. However, treating all of these states the same introduces the following
error.

Definition 10. (Local Lipschitz Error) We define the local Lipschitz error bound by
H = .
The local Lipschitz error is the largest possible difference between the value at a landmark l and the value at a state within the -radius hypersphere centered at l. This is
essentially the error introduced by allowing landmark options to terminate at any state
that is within a distance of  of l.
Now we need to define the error caused by following a policy whose options were planned
c in M .
on M

396

fiApproximate Value Iteration with Temporally Extended Actions

Definition 11. (Stochastic Plan Failure) Let  and  be as in Assumption 5. The stochastic planning failure  is the smallest value such that
Pr
(R, ,y)SM (x,ol )

[(y, l) > ]  

for all x  X and ol  Ox where l is the landmark associated with ol .
The stochastic plan failure bounds the probability that a path to a landmark state
c will terminate in a state that is far from the desired landmark state when
planned on M
executed in M .
c is for M . It
We also need a way of characterizing how good of an approximation M
turns out that we can characterize this relationship in a simple way.
Definition 12. (Relaxation Error) The relaxation error is





b 
 
 
 

R = max VM
 VM

V

V
,
0
,

 M
c
c 
M




b  is the optimal option policy in M
c.
where  is the optimal option policy in M and 
Surprisingly, the relaxation error only depends on the difference between the optimal
c. If these
policies over primitive actions and the optimal policies over options in M and M
c can be a good approximation for M , even if the dynamics
policies have similar values then M
of M are very noisy.
Finally, the sampling error S is controlled by m in Algorithm 2. Increasing m corresponds to collecting more samples which consequently decreases S .
Theorem 2. (LAVI Convergence) Let S > 0,   (0, 1]. There exists



1
LK
m=O
ln

(S (1  )2 (1   dmin ))2
such that with probability greater than 1  , if Algorithm 2 is executed for K  1 iterations,
the greedy policy K derived from VK satisfies
!
 
 !
V  V0 
2(L + P )
M
(K+1)dmin
L
L1, (K ) 
+ R +  + S + 
,
(27)
dmin
bmin
d
1


1
 d


dmin
 min
where  = 1
1 + (1)
(VMAX + (1  )H ) and dbmin and dmin are the mindmin
d
1 min
c and M , respectively.
imum duration of any landmark-option pair in M
The proof of Theorem 2 appears in Appendix C. Surprisingly, Eq. (27) holds for the
initial state distribution even though LAVI only maintains value estimates for states in L.
Although this bound is not directly comparable to bounds derived for FVI, it has many
of the same characteristics as the bounds found in the works of Farahmand et al. (2010)
and Mann and Mannor (2014). For example, the first three terms on the right hand side
of Eq. (27) correspond to the approximation error, S is the estimation error controlled
by the level of sampling, and the last term characterizes the convergence behavior of the
397

fiMann, Mannor, & Precup

algorithm. For FVI the approximation error is caused by choosing a function approximation
architecture that is not rich enough to represent the estimates of the value function at each
iteration. For LAVI the approximation error is caused by choosing a landmark set that is
not sufficiently rich or using a planner that cannot reliably reach the vicinity of landmark
states.
The first four terms on the right hand side of (27) describe the worst case loss of the
policy derived by LAVI as K  . The first term corresponds to the error associated with
the choice of landmarks and using a suboptimal local planner. If LAVI uses an optimal
local planner, such as A , then P = 0. The second term is the relaxation error (discussed
more below).  is controlled by the stochastic plan failure  and local Lipschitz error H .
If both,  and H are small then  will be small. In addition, longer duration options (i.e.,
larger dmin ) decreases . The sample error S is decreased by increasing m.
The last term corresponds to LAVIs convergence rate and is one of the keys to LAVIs
speed. The convergence rate  dmin is faster than , the convergence rate of FVI with
primitive actions (Munos & Szepesvari, 2008; Mann & Mannor, 2014). The minimum
duration dmin is controlled by the minimum time between landmark regions. So convergence
is faster when the landmarks provide greater mobility throughout the state-space.
A closer


  ,

look at the last term in Eq. (27) shows that the convergence error depends on V0  VM
L
which is a max-norm only over the landmark states L. This last term represents the fact
that LAVI only needs to estimate the value function at the landmark states.
c is
The relaxation error term R in (27) determines how good of an approximation M
for M given the set of landmark options. One naive bound for R is in terms of a bound
on the transition dynamics with respect to the primitive actions




D = max P (|s, a)  Pb(|s, a)
1

(s,a)SA

where k  k1 is the L1 norm, P are the transition probabilities for M , and Pb are the state
transitions (degenerate probability distributions with all mass on a single next state) for
c. It is not difficult to show that R  2D . However, this bound is generally extremely
M
1
c whereas
conservative. R only depends on the loss of the best option policies in M and M
D is influenced by primitive actions and states that may never be visited by these option
policies.
The total number of samples used by LAVI is LKm, where L is the number of valid
landmark-option pairs, K is the number of iterations, and m is the number of landmarkoption samples. On the other hand, the number of samples used in the analysis of Fitted Value Iteration depends on the complexity of the function approximation architecture
(Munos & Szepesvari, 2008). In MDPs with complex value functions, such as the Pinball
domain (see Section 5.2), complex function approximation schemes are necessary to get
FVI to work. On the other hand, with an appropriate landmark set, LAVI can simply skip
over complex regions of the value function.
Notice that executing the policy derived from the output of LAVI requires sampling from
the simulator. The number of samples needed depends on the discount factor . When 
is close to 1, more samples are needed to ensure that the policy behaves near-optimally.
However, this is an acceptable cost when the simulator is relatively inexpensive compared
to the overall cost of planning.
398

fiApproximate Value Iteration with Temporally Extended Actions

4.2 Landmark-based Options Fitted Value Iteration
It is also possible to consider using landmark-based options with OFVI. We refer to the
resulting case of the algorithm as Landmark-based Options Fitted Value Iteration (LOFVI).
Theorem 3. (LOFVI Convergence) Let S > 0,   (0, 1] and O be a set of landmark-based
options. If assumption A2(, ) (Assumption 2) and A3( = 0, d = dmin ,  = 0, , j = 0)
(Assumption 3) hold, then there exists n and m, such that with probability greater than
1  , if Algorithm 1 is executed for K  1 iterations, the greedy policy K derived from VK
satisfies
Lp, (K ) 

2(L + P )
1   dbmin

!
+ R

+

2 dmin 1/p
C bp, (TF, F) + S
(1  )2 ,


+  dmin (K+1)

1/p

 
 !
2 VM
 V0 
, (28)
(1  )2

c and M ,
where dbmin and dmin are the minimum duration of any landmark-option pair in M
respectively.
Theorem 3 is comparable to Theorem 2 but provides a (p, )-norm bound rather than
a (1, )-norm bound. If we consider the case where p = 1, then we can compare Theorem 3
to Theorem 2. Both theorems share the same abstraction loss. Although their convergence
rates are similar, LAVIs convergence term (Theorem 2) is significantly smaller than the
convergence term for OFVI. However, LOFVIs convergence depends on an explicit representation over the state-space while LAVIs convergence depends only on values maintained
at the landmark states.
The main advantage of LOFVI over LAVI is its potential for lower sample complexity
when querying the policy. Although not shown here, it is easy to extend the analysis
of OFVI to produce action-value functions rather than value functions. With an explicit
action-value function querying the policy does not require any additional samples. This
may be an important consideration if the simulator is computationally expensive.
4.3 Additional Considerations
One barrier to applying landmark-based options is that we need access to a deterministic
relaxation of the target MDP for local planning. In many domains, a deterministic model
may have already been created by domain experts. However, if this relaxation is unavailable,
we might wonder how one could be acquired.
One simple strategy for obtaining a deterministic MDP from the target MDP simulator
would be to use the most frequently sampled next state. Algorithm 3 demonstrates one
possible implementation for this strategy. The algorithm builds a deterministic model
H : X  O  R  X as new state-action pairs are requested. It takes as arguments
the target MDP simulator SM a state-action pair (x, a), the number of samples m  1,
and the partial deterministic model H and returns a 3-tuple containing a reward r and
terminal state y. Furthermore, Algorithm 3 can easily be extended to the continuous state
setting by matching states that are close together. The cost of Algorithm 3 depends on
399

fiMann, Mannor, & Precup

Algorithm 3 DREX (Deterministic RElaXation)
Require: SM , x  X, a  A, m  N, H : X  O  R  X
1: if H(x, a) 6=  then {Model has no entry for (x, a).}
2:
for i = 1, 2, . . . , m do
3:
(ri , , yi )  SM (x, a)
4:
end for
P
5:
y  arg maxi=1,2...,m m
j=1 I{yi = yj } {Assign most frequent next state.}
P
m
1
6:
rm
I{y
=
y
}r
i i {Average reward for next state y.}
i=1
7:
H(x, o)  (r, y)
8: end if
9: return H(x, o)

the cost of sampling a primitive action m times, where m can be chosen to ensure that
the highest probability terminal state is chosen with high probability. This approach only
makes sense when there exists a most probable next state (region) for each state-action
pair. Nevertheless, this may capture a wide range of real-world domains.
While we have used the example of a deterministic relaxation for local planning. Landmark options could be implemented using alternative local planning algorithms, for example,
UCT (Kocsis & Szepesvari, 2006), Episodic Natural Actor Critic (Peters & Schaal, 2008),
etc. These approaches have the advantage that they can be applied directly on the target
MDP simulator. The theoretical guarantees provided for deterministic planners can easily
be adapted to other black box planners whenever the local planning error can be bounded.
However, we focus our analysis on deterministic local planners for brevity and clarity.

5. Experiments and Results
We compared PFVI and OFVI in three different tasks: (1) the optimal replacement problem
(Munos & Szepesvari, 2008), (2) the pinball domain (Konidaris & Barto, 2009), and (3) an
eight commodity inventory management problem (Mann & Mannor, 2014).
Our theoretical analysis from the previous sections characterizes convergence rates.
However, we are also interested in the trade-off between the planning effort and performance (i.e., cumulative reward) of the resulting policy. While it is possible to compare the
time-to-solution, this requires setting a potentially arbitrary performance threshold. Choosing a performance threshold can unfairly bias our judgment about which algorithm achieves
the best overall performance-time trade-off. We measure this trade-off by introducing the
following statistic:
V k (x)
(x, k) = Pk
,
(29)
i=1 ti
where x is the start state used for evaluation and k refers to the number of iterations
performed by the algorithm so far. For i = 1, 2, . . . , k the value ti is the time in seconds of
the ith iteration. Higher values are more desirable because they imply more performance
for less time spent planning.
In all of our experiments, we simulated options by simulating individual primitive actions, until the selected option terminates or a maximum number of timesteps (100 in our
400

fiApproximate Value Iteration with Temporally Extended Actions

18
16

k =1

||V  Vk||1

14
12

k =2
k =3

10
84

3

2

1

0

1

Deviation from x

2

3

4

Figure 3: Optimal Replacement: Expected loss of iterates V1 , V2 , and V3 of OFVI given
the primitive actions and a single option of varying quality. Error bars represent
1 standard deviation. Results were averaged over 20 trials.

experiments) occurs. This potentially places options-based planning methods at a disadvantage. Nevertheless, our experiments provide strong evidence that options can speed up
the convergence rate of planning, which leads to a smaller time-to-solution.
All experiments were implemented in Java and executed using OpenJDK 1.7 on a desktop computer running Ubuntu 12.04 64-bit with an 8 core Intel Core i7-3370 CPU 3.40GHz
and 8 gigabytes of memory.
5.1 Optimal Replacement Task
In the optimal replacement problem, we only compare PFVI and OFVI with hand crafted
options. Due to the simplicity of the task, option generation is unnecessary and we include
this task for comparison with previous work. In this problem, the agent selects from one of
two actions K and R, whether to maintain a product (action K) at a maintenance cost c(x)
that depends on the products condition x or replace (action R) the product with a new one
for a fixed cost C. This problem is easy to visualize because it has only a single dimension,
and the optimal value function and optimal policy can be derived in closed form (Munos &
Szepesvari, 2008) so that we can compare PFVI and OFVI directly with the optimal policy.
We used parameter values  = 0.6,  = 0.5, C = 30 and c(x) = 4x (identical to those
used in the work of Munos & Szepesvari, 2008) where  is the inverse of the mean of an
exponential distribution driving the transition dynamics of the task. Similar to the work
of Munos and Szepesvari (2008), we used polynomials to approximate the value function.
All results presented here used fourth degree polynomials. The optimal policy keeps the
product up to a point x and replaces the product once the state equals or exceeds x.
For the OFVI condition, we introduced a single option that keeps the product up to
a point x = x +  and terminates once the state equals or exceeds x. By modifying ,
we controlled the optimality of the given option. As predicted by our analysis, adjusting
 away from 0 (i.e., reducing the option quality), resulted in slower convergence when the
401

fiMann, Mannor, & Precup

Time (s) per Iteration

0.006

0.005

0.004

0.003

0.002

0.001

0.000

PFVI

OFVI

Figure 4: Optimal Replacement: Time in seconds per iteration for PFVI and OFVI on the
optimal replacement task averaged over 20 trials.

Optimal Replacement Task

Convergence w/V0 =0

4

6

8

10

2

10

50
40
30
20
10
00

2

||V  Vk ||1

PFVI
OFVI

4

6

8

10

4

6

8

10

||V  Vk ||

2

||V  Vk ||

50
40
30
20
10
00

Convergence w/V0 = 75

50
40
30
20
10
00

PFVI
OFVI

||V  Vk ||1

50
40
30
20
10
00

2

4

6

Iteration # (k)

8

(a) Optimistic (V0 > V  )

Iteration # (k)

(b) Pessimistic (V0  V  )

Figure 5: Optimal Replacement: Convergence rates of PFVI and OFVI in the Optimal
Replacement Task. (a) When the initial value function estimate is optimistic,
there is no difference between the convergence rates of PFVI and OFVI. (b)
However, when the value function estimate is pessimistic, OFVI converges faster
than PFVI. Results were averaged over 20 trials.

initial value function was pessimistic (see Figure 3). For an optimistic initial value function,
the behavior of PFVI and OFVI was almost identical.
402

fiPFVI

Approximate Value Iteration with Temporally Extended Actions

20
40

OFVI

0

PFVI

4

6

8

10

20
40
0

OFVI

2

V

20
30
40
50
60
200
30
40
50
60
0

2

V

4

6

k=2

V

2

4

4

6

k=2

40

40

0

2

4

6

8

10

0

20

20

40

40

4

6

8

10

10

0

2

4

6

8

10

8

(a) Optimistic (V0 > V  )
20
30
40
50
60
10 200 2 4 6 8 10
30
40
50
60
10
0 2 4 6 8 10

20
30
40
50
60
200
30
40
50
60
0

2

4

6

8

10

2

4

6

8

10

8

0

2

4

2

10

V

2

20

8



6

20

6

k=5

8

k=5

k=10

k=10

(b) Pessimistic (V0  V  )
Figure 6: Optimal Replacement: Average iterates Vk (k = 2, 5, and 10) for PFVI and
OFVI for both (a) optimistic and (b) pessimistic initial value functions. With a
pessimistic value function OFVI converges significantly faster than PFVI.

Figure 4 shows that OFVI takes slightly longer per iteration than PFVI, because OFVI
considers both primitive and temporally extended actions. Figure 5a shows the average
convergence rates of PFVI and OFVI (with  = 0), when the initial value function estimate
is optimistic for both max-norm and L1 -norm error. In both cases the value functions
converge at almost identical rates as predicted by our analysis. Figure 5b shows the average
convergence rates of PFVI and OFVI, when the initial value function estimate is pessimistic.
With a pessimistic initial value function, OFVI converges significantly faster than PFVI as
predicted by our analysis.
Figure 6 compares the average iterates Vk of OFVI to PFVI for k = 2, 5, and 10 with
optimistic (Figure 6a) and pessimistic (Figure 6b) initial value function estimates. The solid
black line depicts the optimal value function V  . With an optimistic initial value function
the behavior of PFVI and OFVI is qualitatively identical. However, with a pessimistic
initial value function, OFVIs second iterate is qualitatively similar to PFVIs fifth iterate.
With a pessimistic initial value function estimate, even suboptimal options were able to
improve convergence rates, though to a lesser degree than when  = 0.
403

fiMann, Mannor, & Precup

Figure 7: Instance of the pinball domain used in our experiments. Black polygons are
obstacles. The large red circle is the target, and the smaller blue circle is the
controlled ball.

5.2 Pinball
In the Pinball domain (Konidaris & Barto, 2009) the agent applies forces to control a ball
on a 2-dimensional surface containing polygonal obstacles. The agents goal is to direct the
ball to a goal region. Figure 7 depicts the instance of the Pinball domain used in our experiments. The state-space consists of four continuous dimensions (x, y, x, y) corresponding to
the coordinates (x, y) of the ball and its velocity (x, y). Similar to the work of Tamar, Castro, and Mannor (2013), we added zero-mean Gaussian noise to the velocities with standard
deviation 0.03. The discount factor was  = 0.95.
The pinball domain contains five primitive actions: (1) accelerate along the X-axis, (2)
decelerate along the X-axis, (3) accelerate along the Y-axis, (4) decelerate along the Y-axis,
and (5) leave the velocities unchanged. Since it is unclear how to create useful hand-coded
options, we decided to only compare PFVI against the LOFVI and LAVI where the options
are generated. We experimented with randomly placed landmarks and landmarks placed
in a grid. In both cases, one landmark was manually placed at the goal state. Randomly
placed landmarks were uniformly sampled over the coordinates of the state-space. Grid
landmarks were placed in a two-dimensional grid over the X and Y coordinates the statespace. All landmarks corresponded to states where the ball was at zero velocity. If a
sampled landmark fell inside of an obstacle, then a new landmark was sampled so that all
landmarks corresponded to valid states of the task.
The metric used to determine the distance between two states ~x = (x, y, x, y) and
~x0 = (x0 , y 0 , x0 , y 0 ) is given by
p
p
(~x, ~x0 ) = (x  x0 )2 + (y  y 0 )2 +  (x  x0 ))2 + (y  y 0 )2 ,
(30)
404

fiApproximate Value Iteration with Temporally Extended Actions

90
15000

Time (s) per Iteration

80

5000

0

5000

20

Iteration #

25

30

35

(a)

)
96
(1

20

96
)

VI

0
15

LA
VI

10

30

I(1

5

40

FV

0

50

LO

15000

60

10

LAVI(196)
LOFVI(196)
PFVI

10000

70

PF

Performance

10000

(b)

Figure 8: Pinball: Comparison of planning with PFVI, LOFVI, and LAVI with 196 landmarks (+1 at the goal) arranged in a grid in the Pinball domain. (a) Performance
over policies derived from each iteration of PFVI, LOFVI, and LAVI. Shaded regions represent 1 standard deviation. (b) Time in seconds to compute each
iteration of PFVI, LOFVI, and LAVI. Results were averaged over 20 trials.

where , which places less emphasis on the differences in velocity than the differences in
coordinates. We chose  = 0.01 through experimentation.
For PFVI and LOFVI, we tried many different function approximation architectures
including Radial Basis Function Networks (RBF), Cerebellar Model Arithmetic Computer
(CMAC), linear regression with various features, but we found through experimentation
that nearest neighbor approximation was both fast and able to capture the complexity of
the value function. For LOFVI, we used one-nearest neighbor approximation and N = 1, 000
states were sampled at each iteration. For PFVI, we averaged the value of states within a 0.1
radius of the queried state and N = 30, 000 states were sampled at each iteration. Without
30, 000 samples, PFVI either failed to solve the task or produced a policy that solved the
task unreliably. Both PFVI and LOFVI used L = 5 samples for each state-option pair.
We chose these settings because they resulted in the strongest performance for PFVI and
LOFVI.
For the landmark options, we experimented with different numbers of landmarks. For
simplicity we selected landmarks that formed a uniform grid over the pinball domains Xand Y -coordinates. By choosing grid sizes of 10  10, 12  12, and 14  14, the number of
landmarks were 100, 144, 196, respectively. With fewer than 100 landmarks performance of
LOFVI and LAVI degraded significantly. The radius of the hypercube around landmarks
was set to  = 0.03, and the landmark options available at a state corresponded to the
landmarks that were at a distance less than 0.2 from the balls current state, which approximates a local planning horizon d+ . For brevity, we consider the results for landmark
options arranged in a grid. Randomly selected landmarks gave qualitatively similar results
with slightly higher variance.
Figure 8a compares the performance of PFVI, LOFVI, and LAVI in the pinball domain
with 196 landmarks (+1 landmark at the goal). After about six iterations, LOFVI and
405

fiMann, Mannor, & Precup

Perf. / Cumulative Time

103

LAVI(196)
LOFVI(196)
PFVI

102

101

100

101

102

0

5

10

15

Iteration #

20

25

30

Figure 9: Pinball: Performance over cumulative time in seconds received by policies from
each iteration. Higher is better. Results were averaged over 20 trials.

Time-to-Solution (in Minutes)

30

25

20

15

10

5

0

PFVI

LOFVI

LAVI

Figure 10: Pinball: Cumulative time-to-solution in minutes for PFVI, LOFVI, and LAVI
averaged over 20 trials. LAVI has the smallest time-to-solution.

406

fiApproximate Value Iteration with Temporally Extended Actions

LAVI are able to solve the task. However, PFVI takes about 25 iterations to solve the
task. Figure 8b compares the time per iteration of PFVI, LOFVI, and LAVI in the pinball
domain. PFVI has the highest time per iteration cost. This is because we needed to use a
lot more samples per iteration for PFVI to solve the task. Notice that LAVI is less expensive
than LOFVI. This is due to the fact that LAVI only needs to sample from landmark states,
whereas LOFVI samples from a larger number of states sampled at each iteration (although
less than PFVI).
Figure 9 compares the performance over cumulative time spent planning. PFVI has a
poor performance over time trade-off because each iteration is takes more time than LOFVI
and LAVI, and it takes many iterations to achieve high performance. LOFVI and LAVI
achieve similar performance, but LAVI has a higher score due to the fact that it spends less
time planning per iteration. Figure 10 compares the time in minutes before PFVI, LOFVI,
and LAVI produce a policy that achieves performance greater than 8,000. PFVI takes a
longer than LOFVI and LAVI, because it converges slowly and uses an expensive function
approximation step at each iteration. Despite the fact that LOFVI and LAVI both use the
same landmark options, LAVI is faster than LOFVI, because LAVI only approximates the
value function around landmark states.
5.3 Inventory Management Task
In a basic inventory management task, the objective is to maintain stock of one or more
commodities to meet customer demand while at the same time minimizing ordering costs
and storage costs (Scarf, 1959; Sethi & Cheng, 1997). At each time period, the agent is
given the opportunity to order shipments of commodities to resupply its warehouse.
We created an inventory management problem where the agent restocks a warehouse
with n = 8 different commodities (Mann, 2014). The warehouse has limited storage (500
units in our experiments). Demand for each commodity is stochastic and depends on the
time of year. Orders can be placed twice each month for a total of 24 order periods per
demand cycle.
The state h, xi of the inventory management problem is a vector specifying the time
of year  and a vector x specifying the quantity of each commodity (denoted xi for the ith
commodity) stored in the warehouse. During each timestep, a demand vector  is drawn by
sampling the demand for each commodity independently from a normal distribution where
the mean depended on the time of year (see Table 1 for parameters used in our experiments).
The demand vector is then subtracted from the quantity of each commodity stored in the
warehouse. If any of the commodities were negative after subtracting the demand vector,
the agent receives an unmet demand penalty

P
Pn
ub + us ni=1 [xi  i ] if
i=1 [xi  i ] < 0 ,
pud (x  ) =
(31)
0
otherwise ,
where ub = 2 represents the base
 unmet demand cost, us = 10 represents the per unit
x if x < 0
unmet demand cost, and [x] =
.
0 otherwise
Once the demand is subtracted, the agent is given the opportunity to either resupply its
warehouse or order nothing. The set of possible primitive actions is n501 = 8501 . Searching
over this set would be intractable. Therefore, we designed a smaller set of primitive actions.
407

fiMann, Mannor, & Precup

Table 1: Commodity Properties
Commodity Index
1
2
3
4
5
Unit Cost (os,i )
1
3
1
2 0.5
Demand Peak (Month)
1
3
7 10 8.5
Demand Std. Deviation
2
1
2
3
2
Max. Expected Demand 16 10 20 4 10

6
1
12
2
9

7
1
1
1
20

8
1
5.5
2
16

The primitive actions available to the agent were the ability to order nothing or to order
any single commodity in quantities of 25 up to the maximum size of the warehouse. This
resulted in (8  (500/25)) + 1 = 161 primitive actions. An action a = hi, qi is defined by a
commodity index i and a quantity q. The cost of an order is defined by

0
if q = 0 ,
poc (i, q) =
(32)
ob + os,i q otherwise ,
where ob = 8 is the base ordering cost and os,i (see Table 1) is the commodity dependent
unit cost. The new state steps forward half a month into the future and the quantities
in the inventory are updated to remove the purchased commodities and add the ordered
commodities (if any). If the agent orders more than will fit in the inventory, then only the
portion of the order that fits in the warehouse will be kept (but the agent will be charged
for the complete order). At the end of each decision step, the agent receives a negative
reward (i.e., cost)
R(x, a) =  (pud (x  ) + poc (i, q)) ,
(33)
which is the negative of the sum of the unmet demands and the order costs depending
on the inventory levels x, the demand , and the action a = hi, qi. There is no storage
cost, but the limit on the inventory forces the agent to make careful decision about which
commodities to order. The discount factor was  = 0.9.
The high dimensionality of the state space (1 dimension for each commodity and 1 dimension for time) required a function approximation architecture with good generalizability.
We tried many different function approximation architectures before settling on Radial Basis Function networks (RBFs) with a grid of 1-dimensional radial bases. By limiting the
dimensionality of the radial bases we were able to achieve good generalization performance
with few samples. We divided the state space into 24 time periods, so that the value function approximation was implemented by 24 RBFs. The the number of bases per dimension
was 25 and the basis widths were controlled by  = 0.1. Throughout the experiments we
sampled n = 1000 states each iteration and sampled each option m = 20 times.
5.3.1 Hand-crafted Options
It is difficult to design a good policy for this problem by hand. Inventory management
has received a lot of attention from the operations research community. One of the main
findings is that the optimal strategy for a large class of inventory management problems
belong to a simple family of policies called (s, S)-policies (Scarf, 1959). For a problem with
a stationary demand distribution and a single commodity, an (s, S)-policy orders enough
stock to bring the inventory level up to S whenever the inventory level falls below s and
408

fiApproximate Value Iteration with Temporally Extended Actions

Inventory Management Task
8000

0

LOFVI(100)
OFVI
PFVI

7000

1000

6000

2000

Value

Value

5000
4000
3000

3000
4000

2000

5000

1000

6000

0

7000

1000

0

5

10

Iteration #

15

8000

20

(a) Optimistic (V0 > V  )

LOFVI(100)
OFVI
PFVI

0

5

10

Iteration #

15

20

(b) Pessimistic (V0  V  )

Figure 11: Inventory Management: Average value of iterates produced by LOFVI, OFVI
and PFVI. Results were averaged over 20 trials. Shaded regions represent 1
standard deviation.

orders no new stock otherwise. In an inventory management task with Markov demand,
(s, S)-policies are optimal for each demand state (Sethi & Cheng, 1997).
While the problem described here does not cleanly fit into the Markov demand setting,
the notion of (s, S)-policies provides a potential idea for a temporally extended action.
Since there is a high base order cost (Eq. (32) and ob = 8) while storage is free, a reasonable
policy should prefer to make large orders whenever possible and maximize the number of
timesteps where nothing is ordered. One way to encode this prior knowledge is to provide
temporally extended actions that follow the policy order nothing until some threshold is
met. In addition to the primitive actions, we provided OFVI with 20 temporally extended
actions for each commodity. The policy followed by all of these temporally extended actions
was to order nothing and terminate once the inventory level for a particular commodity fell
below a constant level (one of 20 levels spanning from 0 to the maximum storage of the
warehouse).
Since we do not know the optimal value function for this problem, we cannot compare
the iterates of PFVI, OFVI, and LOFVI to a ground truth. However, we can still examine
their iterates. Figure 11a shows the average iterates produced by PFVI, OFVI, and LOFVI
with optimistic V0 . In this case, we can see that PFVI, OFVI, and LOFVI appear to
decrease at similar rates. Figure 11b shows the average iterates produced by PFVI, OFVI,
and LOFVI with pessimistic V0 . Here we see that OFVI and LOFVI increase their values
(toward V  ) more quickly than PFVI. LOFVI appears to converge to a different solution
than PFVI and OFVI, which is probably due to the fact that the landmark options used in
this experiment may be more powerful than the actions available to PFVI and OFVI. Note
that a comparison to the value function produced by LAVI is not straightforward because
it does not produce an approximation for all states (only the landmark states).
409

fiMann, Mannor, & Precup

7000

7000

6000

6000

Discounted
Cumulative Reward

Discounted
Cumulative Reward

Inventory Management Task

5000

4000

3000

LOFVI(100)
LAVI(100)
OFVI
PFVI
1-Step Greedy
Rand

2000

1000

0

5

10

Iteration #

15

5000

4000

3000

LOFVI(100)
LAVI(100)
OFVI
PFVI
1-Step Greedy
Rand

2000

1000

20

0

(a) Optimistic (V0 > V  )

5

10

Iteration #

15

20

(b) Pessimistic (V0  V  )

Figure 12: Inventory Management: Performance of policies at each iteration of LAVI,
LOFVI, OFVI and PFVI starting from a state with no inventory. LAVI settles on a near-optimal policy after a single iteration. Results were averaged over
20 trials. Shaded regions represent 1 standard deviation.

We considered the performance of the policies derived from iterates of PFVI and OFVI.
When V0 is initialized pessimistically, Figure 12b shows that OFVI quickly converges to a
better policy than PFVI. It takes PFVI several more iterations before an equally successful
policy is found. We compared these policies with a policy that selected uniformly at random
from the available primitive actions (denoted Random) and a policy that selects the action
that has the immediate lowest cost (denoted 1-Step Greedy). The initial state was set
to the beginning of the year with zero inventory. For the case where V0 is optimistically
initialized, Figure 12 shows that the performance of PFVI and OFVI quickly improve beyond
the Random and 1-Step Greedy policies, but their performance is similar at each iterate.
5.3.2 Landmark-Based Options
For LAVI, we set m = 20 (where m controls the number of samples per state-option pair)
after experimentation showed that this value works reasonably well. Since the state-space of
the problem was too large to use a generic graph-based planner, we constructed a heuristic
local planner that used a deterministic instance of the problem to transition as close as
possible to landmark states. Given the definition of the inventory management task, it is
easy to define a deterministic model by replacing samples from the Gaussian distributions
with the expectation of those distributions. Given a landmark l, if the current state was
lower than l for the ith commodity, then it would order the amount needed to reach the ith
commoditys quantity in l plus the expected demand for the ith commodity. If the current
state was higher than l for the ith commodity, it would place no order for that commodity.
Notice that this local planner is efficient and is able to make use of the entire set of primitive
actions. We used Euclidean distance and set  = 0.05  500 where 500 was the maximum
410

fi30

6000

25

VI

0

FV

O

PF

00
)2
(1

LO

LA
VI

LA
VI

(1

)2

00
)

0

)1
00

I(1
00

FV

LO

FV
I(1

LO

O

FV
I2

0

VI
2

FV
O

PF

an
R

1

0
0

0

I1

5

d

1000

(a)

)

10

00

2000

15

LA
VI
(1

3000

20

00
)

4000

I(1

5000

FV
I

Time (s) per Iteration

7000

PF
VI
1

Performance

Approximate Value Iteration with Temporally Extended Actions

(b)

Figure 13: Inventory Management: (a) Comparison of performance of the first and last
policies derived by PFVI, OFVI, and LAVI. (b) Comparison of time per iteration in seconds. Results were averaged over 20 trials. Error bars represent 1
standard deviation.

Perf. / Cumulative Time

104

LAVI(100)
LOFVI(100)
OFVI
PFVI
103

102

101

0

5

10

Iteration #

15

20

Figure 14: Inventory Management: Comparison of performance over cumulative time in
seconds (higher is better). LAVI achieves higher performance for time invested
even compared to LOFVI which also uses landmark options. Results were averaged over 20 trials.

inventory level and d+ = . The reason we set d+ =  was because successfully managing
inventory requires making large jumps in the state-space (e.g., going from 0 inventory to
maximum inventory levels) in a single timestep.
Figure 13a compares the performance of a policy that selects primitive actions uniformly
at random and policies derived from the first and last iterates of PFVI, OFVI, LOFVI, and
411

fiMann, Mannor, & Precup

Time-to-Solution (s)

100

80

60

40

20

)
00
(1

I(1
FV
LO

LA
VI

00
)

I
FV
O

PF

VI

0

Figure 15: Cumulative time-to-solution (with performance threshold 5,500) averaged over
10 independent trials for PFVI, OFVI, LOFVI with 100 landmarks, and LAVI
with 100 landmarks. LAVI has the smallest time-to-solution.

LAVI. In this task, LAVI outperforms PFVI and OFVI after on its first iteration, while
LOFVI ultimately has higher performance. Figure 13b compares the time per iteration in
seconds of PFVI, OFVI, LOFVI, and LAVI. In this task, LAVI is significantly faster than
PFVI, OFVI, and LOFVI. Figure 14 shows that LAVI achieves a better performance-time
trade-off on all iterations.
Figure 15 shows the cumulative time in seconds to derive a policy that achieves performance above 5,500 averaged over 10 independent trials. OFVI, which uses a mixture
of options and primitive actions, has a slightly faster time-to-solution than PFVI, despite
having to evaluate both primitive and temporally extended actions at each iteration. Thus,
OFVIs faster time-to-solution is due to its faster convergence rate. LOFVI and LAVI both
have a smaller time-to-solution than PFVI and OFVI. Although there are approximately
the same number of primitive actions and landmark options that can be initialized at each
state, LOFVI and LAVI are faster than PFVI because they converge faster. Finally, LAVI is
faster than LOFVI because it uses a computationally efficient estimate of the value function
based only at the landmark states, whereas LOFVI (1) uses a potentially more expensive
function approximation architecture and (2) does not make explicit use of the landmark
state locations.
5.4 Experimental Domains versus Theoretical Assumptions
It is a useful exercise to consider how our theoretical assumptions map onto our experimental
domains.
First, we consider the concentrability coefficient (Assumptions 1 and 2). Unfortunately,
we cannot estimate the concentrability coefficients for our domains because they depend
on a supremum norm over sequences of policies. However, the concentrability coefficients
412

fiApproximate Value Iteration with Temporally Extended Actions

are generally smaller in stochastic domains, where every policy has a broad future state
distribution (meaning that the long-term value of a state depends a little bit on lots of
states). Along this line of reasoning, we should expect that the optimal replacement and
inventory management problems might have smaller concentrability coefficients than the
pinball domain. Consistent with this hypothesis, we found that all of our algorithms were
much more sensitive to the sampling distribution in the pinball domain than the other two
domains.
In addition, Lemma 1 suggests that with options the concentrability coefficient is always
less than (or equal to) the concentrability coefficient for primitive actions. This is also supported by our experiments with the pinball domain, where PFVI was much more sensitive
to the sampling distribution than LOFVI or LAVI.
Second, let us consider Assumption 3 with respect to our experimental domains. Assumption 3 deals with the sparseness of options in the state-space. Informally, it says
that nearly-optimal temporally extended actions are abundant in the state-space. With
landmark options, this holds true for the pinball and inventory management domains by
definition, and as a result LAVI and LOFVI achieve fast convergence. Our hand-crafted
options in the optimal replacement and inventory management domains, on the other hand,
may terminate immediately in some states and have a long duration in others. This may
account for why landmark-based options resulted in faster convergence in our experiments.
Now we will consider the assumptions and definitions from the analysis of LAVI. The
locally Lipschitz assumption probably holds for the inventory management domain because
the high stochasticity generally smooths the value function. In the pinball domain, there
are regions of the state-space that are probably not Lipschitz due to the complex obstacles.
Two states on the opposite side of an obstacle can be relatively close but have extremely
different values. However, LAVI only needs the locally Lipschitz assumption to hold around
landmark states. Since the majority of the state-space in the pinball domain is probably
smooth, the local Lipschitz assumption likely holds for most landmark configurations from
our experiments.
Landmark error decreases when we add more landmark states. In the pinball and inventory management domains the task could not be solved with too few landmarks. However,
we were surprised that in the pinball and inventory management domains only 100 landmarks were needed to learn reasonable solutions. Furthermore, in the pinball domain, how
landmark states were chosen did not have a large effect on the performance. Thus a gridbased layout of landmarks only produced slightly better policies than uniformly sampled
landmark states. This suggests that landmark error could be made small in our experimental domains with a small number of landmarks.
Local planning error is the error due to using an imperfect deterministic planner. For
the inventory management task, the local planning error was 0, because we were able to
use the deterministic model to specify what to order to get to a landmark state. In the
pinball domain, the local planning error may be large because we used a greedy algorithm
to select actions that move the agent in a straight line toward the landmark. This local
planner will fail when the ball needs to be pushed around a corner, but it worked well in our
experiments. This suggests that the local planning error may have been small on average.
Stochastic plan failure occurs when noise in the environment prevents a landmark option
from terminating sufficiently close to the designated landmark state. In our analysis of
413

fiMann, Mannor, & Precup

LAVI, even a small probability  of a stochastic plan failure caused a large increase in the
approximation error. This is due to the possibility that failing to reach a landmark may leave
the agent in a non-recoverable state. However, in the pinball (Konidaris & Barto, 2009) and
inventory management (Mann & Mannor, 2014) domains, the agent can eventually recover
from any mistake. So stochastic plan failure generally has a much smaller consequence than
what is predicted by (27).
The relaxation error quantifies how much worse the best landmark option policy is in
the target MDP than in the deterministic relaxation. Despite the fact that the pinball
domain and the inventory management domain have significant stochasticity, LAVI and
LOFVI were able to derive good policies. Unfortunately, it is not clear how to measure the
relaxation error.

6. Discussion
We have proposed and analyzed Options Fitted Value Iteration and Landmark-based Approximate Value Iteration. For both algorithms longer temporally extended actions result
in faster convergence and smaller approximation error. For OFVI, our analysis shows that
when the value function estimate is pessimistic with respect to the optimal value function,
the convergence rate of OFVI can take advantage of temporally extended actions that have
a smaller effective discount factor than the options with minimum duration. Furthermore,
options can improve convergence even when they are suboptimal and spread throughout the
environment. In fact, LAVI and LOFVI both converge faster as landmark-based options
are spread out further in the environment.
Approximate Modified Policy Iteration (Scherrer, Ghavamzadeh, Gabillon, & Geist,
2012) is related to planning with options in the sense that modified policy iteration performs
backups from d-step rollouts (rather than 1-step rollouts) of the greedy policy. However,
planning with options is more flexible because the options can have termination conditions
that depend on state and time. Furthermore, the analysis from the work of Scherrer et al.
(2012) does not point to any improvement in convergence rates by increasing the length of
the rollouts used to perform backups.
Special representations such as factorization of a tasks state and action spaces can
be exploited to achieve faster planning (Hoey, St-Aubin, Hu, & Boutilier, 1999; Barry,
Kaelbling, & Lozano-Prez, 2011). However, for many problems a simulator already exists
or simulators are a more convenient way to represent the task. In fact, the work of Dietterich,
Taleghan, and Crowley (2013) presents an example where the simulator representing the
task is computationally efficient, but exact inference on the factored representation of the
task is computationally intractable. Therefore, the ability to plan on black box simulators
is more generally applicable than requiring a problem to be in some special representation.
This is why we focus on planning with a black box simulator.
Option discovery has been investigated extensively, and many approaches explore heuristics related to finding useful subgoals (McGovern & Barto, 2001; Simsek & Barto, 2004;
Stolle & Precup, 2002; Wolfe & Barto, 2005), which is similar in spirit to finding landmarks.
In all of these approaches, however, the emphasis is on finding only useful subgoals. Our
analysis provides instead a way to use any arbitrary set of landmarks, and quantify the
quality of the obtained policy. Because of this less careful approach in selecting landmarks,
414

fiApproximate Value Iteration with Temporally Extended Actions

and because of the use of local planning on a deterministic problem, the scalability of LAVI
is significantly better, especially in high-dimensional problems.
Given a collection of policies, the works of Comanici and Precup (2010) and Mankowitz,
Mann, and Mannor (2014) have investigated creating useful options by applying option
interruption. Both of these methods rely on being given a collection of policies. Here
we make use of a deterministic local planner instead, which gives LAVI and LOFVI more
flexibility since they are not restricted to a few predefined policies.
For clarity, we have focused on learning a good approximation of the optimal value
function and then showed that the resulting greedy policy has bounded loss. However, in
practice we cannot directly obtain the greedy policy from a value function. It must be
approximated with samples. However, our results can easily be extended to handle this
by the same arguments used to prove Thm. 3 in the work of Munos & Szepesvari, 2008
or by approximating the action-value function (Farahmand, Ghavamzadeh, Szepesvari, &
Mannor, 2008).
For brevity and generality, we have presented an analysis of the convergence behavior
of AVI algorithms (not computational complexity). It is possible using the bounds in
Theorem 1 and Theorem 2 to obtain bounds on computational complexity. However, there
are two critical decisions needed to determine the computational complexity. The first is
the computational complexity of sampling options. For example, the smart grid simulator
Gridlab-d can efficiently simulate actions at multiple timescales (Chassin et al., 2014).
On the other hand, some simulators may require sampling the outcome of a sequence of
primitive actions. The second decision involves the choice of function approximation, which
can vary widely.
Planning with options is an important setting because options are a more natural model
for settings where decisions are made at irregular time intervals. Furthermore, algorithms
that plan with options can potentially make use of the many algorithms proposed for learning options from data (Iba, 1989; Mannor et al., 2004). However, which algorithms produce
good options for planning is an open question, since the majority of previous research
has considered generating options for exploration. Our analysis of landmark-based options
helps to address this question because landmark-options are similar in spirit to many existing techniques for option generation, such as skill chaining (Konidaris & Barto, 2009) and
bottleneck discovery (McGovern & Barto, 2001; Simsek & Barto, 2004).
Options may have other benefits for planning besides improving the convergence rate
(and thus the overall speed of planning). For example, options may enable a planning
algorithm to skip over regions of the state space with highly complex dynamics without
impacting the quality of the planned policy. In particular, LAVI only models the value
function around the landmark states, which allows it to perform well in tasks where the
value function is highly nonlinear (such as the Pinball domain in Section 5.2). In partially
observable environments, options may be exploited to decrease uncertainty about the hidden
state by skipping over regions of the state space where there is large observation variance,
or testing hypotheses about the hidden state. Options may also play an important role
in robust optimization, where the dynamics of temporally extended actions are known
with greater certainty than the dynamics of primitive actions. In fact, macro-actions have
already been used for planning in partially observable environments with some success (He,
415

fiMann, Mannor, & Precup

Brunskill, & Roy, 2011). However, these results only consider a very narrow definition of
temporally extended actions that excludes closed loop policies such as options.
We have focused on generalizations of value iteration, but there are many other algorithms where planning can benefit from options. For example, approximate policy iteration
(Lazaric, Ghavamzadeh, & Munos, 2010; Scherrer et al., 2012) may also exploit options to
speed up convergence. Another interesting family of planning algorithms is the sparse sampling framework, which estimates the value at a single state using either breadth-first-like
search (Kearns, Mansour, & Ng, 2002) or rollouts (Kocsis & Szepesvari, 2006). Options
may enable sparse sampling algorithms to derive higher quality policies with a smaller
dependence on the horizon.
For option generation, we assumed the existence of an efficient local planner. For many
applications it may be much easier to create and/or learn an efficient local planner than a
global planner. This is especially true in domains where the local dynamics tend to remain
similar throughout large regions of the environment (Brunskill, Leffler, Li, Littman, & Roy,
2008).

Acknowledgments
This work was funded in part by the NSERC Discovery grant program and the European
Research Council under the European Unions Seventh Framework Programme (FP/20072013) / ERC Grant Agreement n.306638.

Appendix A. Proof of Proposition 1
Proof. (of Proposition 1) This proposition follows from Theorem 1. To see why, consider
any Z  0, there is at least one optimal policy   defined over primitive actions that satisfies
Assumption 3 with values  = 0, d = 1,  = 0, arbitrary   M (X), and j = 0. In this
case, Theorem 1 gives us the following high probability (> 1  ) bound with  = 0:
||V   V K ||p,


+


1/p
2
C (bp, (TF, F) + ) + 
(1)2 ,


1/p 
Z
2kV  V0 k
 (1)d+ +  KZ+1
(1)2 
1/p 2kV  V0 k 
1/p
2
K+1

C
b
(TF,
F)
+

+

(1)2 , p,
(1)2

,

where we replace C, with C, since Lemma 1 tells us that C,  C, .

Appendix B. Proof of Theorem 1 and Supporting Lemmas
In this appendix, we prove Theorem 1 and provide sufficient values for the arguments n
and m, where n controls the number of states sampled at each iteration and m controls the
number of samples simulated at state-action pairs.
The proof of Theorem 1 is similar in structure to Thm. 2 in the work of Munos &
Szepesvari, 2008 with several changes due to the differences between options and primitive
actions. The proof of Theorem 1 has the following structure:
416

fiApproximate Value Iteration with Temporally Extended Actions

1. In Appendix B.1, we derive Lemma 2, which bounds the number of states n and
the number of samples m from each state-option pair that are necessary to achieve
a high-probability bound on the error of a single iteration of AVI. Lemma 2 is used
directly in the proof of Theorem 1 but not in any of the other supporting lemmas.
2. In Appendix B.2, we derive Lemma 6, which provides a pointwise bound on the loss
of the policy produced by OFVI after K  1 iterations. We start by deriving an
upper bound for the policys pointwise loss based on the value functions pointwise
error (Lemma 3). To use Lemma 3, we need bounds on the value function estimates
pointwise error. So we derive upper and lower bounds on its pointwise error after K
iterations (Lemma 4). Lemma 6 puts Lemmas 3 and 4 together and exploits options
that follow a near-optimal policy to get a tighter bound when the estimate of the
value function is pessimistic. For technical reasons, it is important in the next part
of the proof (Appendix B.3) that the coefficients in the pointwise bound sum to 1.
Therefore, we introduce coefficients k for k = 0, 1, 2, . . . , K and show that they do
indeed sum to 1 (Lemma 5).
3. In Appendix B.3, we convert the pointwise bound derived in Appendix B.2 into an
Lp -norm bound, as well as, deriving the convergence behavior of OFVI (Lemma 8).
Lemma 7 shows how the concentrability assumption (Assumption 2) allows us to
replace the error according to the future state distribution with the error according
to our sampling distribution. We use Lemma 7, as well as, Assumption 3 to prove
Lemma 8.
4. Appendix B.4 proves Theorem 1. The proof uses Lemma 2 to select the number of
samples needed to ensure that the error at all K  1 iterations is low with high
probability. Then we apply Lemma 8 to bound the error after K iterations.
Before moving onto the proofs, we first introduce some additional notation. In contrast
to the discounted termination state probability density Pe, we denote the undiscounted
probability that an option o executed from a state x  X will terminate in a subset of
states Y  X by

X
o
P (Y |x) =
Pto (Y |x) .
(34)
t=dmin

R
R
Notice that because (34) is undiscounted Peo (y|x)dy < P o (y|x)dy = 1. For an option
policy  : X  O, we will denote by Pe discounted termination state probability distribution for executing  once at each state (executing each option until termination) and the
undiscounted termination state probability distribution P  analogously. Notice that for an
option policy, we also have

X
P  (Y |x) =
Pt (Y |x)
(35)
t=dmin

for all Y  X and x  X.
417

fiMann, Mannor, & Precup

Notice that if f is an option, an option policy, or a policy over primitive actions we can
write the discounted termination state probability density by
Pef (Y |x) =


X
t=dmin

 t Ptf (Y |x)

(36)

for all Y  X and x  X. When we compose options o1 , o2 , . . . om , we write Peo1 o2 ...om =
Peo1 Peo2 . . . Peom , and we can write
Peo1 o2 ...om (Y |x) =


X
t=mdmin

 t (P o1 P o2 . . . P om )t (Y |x)

(37)

for all Y  X and x  X.
We will assume throughout this supplementary material that when we refer to an optimal
policy   , it is a policy over primitive actions. When O contains the set of primitive actions
A, the fixed point of the SMDP Bellman operator T and the MDP Bellman operator T is


the optimal value function V  . Thus T is equivalent to T  .
B.1 Bounding the Number of Samples
The following lemma is used in Theorem 1 to select sufficient values for parameters n and
m to ensure that the per iteration error is less than some  > 0 with probability at least
1  .
Lemma 2. Let M
p be an SMDP with option set O, F  B(X; VMAX ) be a bounded function
space with 81 4 , p -covering number bounded by N , V  F, p be a fixed positive integer,
and V 0 be the result of a single iteration of OFVI derived from (13) followed by (8). For
any ,  > 0,
 0

V  TV   bp, (TV, F) + 
p,
holds with probability at least 1   provided that


8VMAX 2p
n > 128
(log(1/) + log(32N ))


(38)

and

8(RMAX + VMAX )2
(log(1/) + log(8n|O|)) .
(39)
2
The proof of Lemma 2 follows from the proof of Lemma 1 from the work of Munos &
Szepesvari, 2008 simply by replacing the MDP Bellman operator with the SMDP Bellman
operator T everywhere it occurs, and noting that we must sample from |O| options rather
than only |A| primitive actions. We omit the proof here for brevity.
m>

B.2 Bounding the Pointwise Propagation Error
We are interested in bounding the loss due to following the policy K derived by OFVI
rather than following the optimal policy   . We will use the fact that


 





kV   V K kp,  V   V   + V   V K 
(40)
p,

418

p,

fiApproximate Value Iteration with Temporally Extended Actions



by the triangle inequality and focus on bounding kV   V K kp, , which is the loss due to
following the policy K produced by OFVI instead of the optimal option policy  .
Because OFVI is a value-based method, it does not directly improve the policy at each
iteration. Instead performing more iterations improves the estimate
of the
 
 optimal option




K
policys value function V . Thus, we need to relate the loss V  V p, to the quality
of the final value function estimate VK produced by the OFVI algorithm. The following


lemma develops a pointwise relationship between the V   V K and V   VK .
Lemma 3. Suppose OFVI is executed for K iterations with iterates Vk for k = 0, 1, 2, . . . , K.
Let  be the optimal policy with respect to the given options O and K be the greedy option
policy with respect to the K th and final iterate VK , then
 
 


V   V K  (I  PeK )1 Pe  PeK V   VK ,
(41)
where I is the identity matrix.




Proof. Since TV  = V  and TK V K = V K , we get


V   V K

=
=
=
=

=
=
=



TV   TK V K



TV   T VK + T VK  TK V K




Pe V   VK + T VK  TK V K




Pe V   VK + T VK  TVK + TVK  TK V K



Pe V   VK + TVK  TK V K



Pe V   VK + TK VK  TK V K



Pe V   VK + PeK (VK  V K )






Pe V   VK + PeK VK  V  + V   V K ,


where the initial equality is based on the fact that V  is the fixed point for T and V K is


the fixed point for TK . The first step is obtained by inserting (T VK +T VK ) = 0. The


second step pulls out the discounted transition probability kernel Pe by subtracting T VK

from TV  . Since the backups are performed by the
same policy  , the immediate reward



terms are canceled, leaving only Pe V   V K . The third step inserts (TVK +TVK ) =


0. Since T VK  TVK , we obtain the fourth step by dropping the terms T VK  TVK ,
which is a vector whose elements are less than zero. We obtain the fifth step by noticing
that since K is the greedy policy with respect to VK , TVK = TK VK . The sixth step pulls


out PeK by subtracting TK V K from TK VK . The seventh step inserts (V  +V  ) = 0.
We can manipulate the above inequality



e V   VK + PeK VK  V  + V   V K
V   V K  P








V   V K 
Pe  PeK V   VK + PeK V   V K
 







V   V K  PeK V   V K

Pe  PeK V   VK


 





I  PeK V   V K

Pe  PeK V   VK ,


where I is the identity matrix, so that the V   V K terms are all on the left hand side.
Since (I  PeK ) is invertible and its inverse is a monotone operator, we get
 
 


V   V K  (I  PeK )1 Pe  PeK V   VK ,
419

fiMann, Mannor, & Precup





which relates (V   V K ) to (V   VK ).
Now that Lemma 3 provides us with a relationship between the quality of estimates of
the value function and quality of the resulting policy, we need to bound the quality of value
function estimates. Each iteration k = 1, 2, . . . , K of OFVI results in some error
k = TVk1  Vk ,

(42)

which is induced by the fitting process. One of the main issues in the proof of Theorem 1
is to determine how these fitting errors propagate through the iterations.

The following lemma helps to bound the error between V  and VK by developing

pointwise upper and lower bounds for V   VK that show how error propagates recursively
with each iteration.
Lemma 4. Suppose  is the optimal policy with respect to the options O, OFVI is executed
for K iterations with iterates Vk for k = 0, 1, 2, . . . , K and iteration errors k for k =
1, 2, . . . , K as defined by (42), then we have the following upper bound
V



 VK 

K 
X


Pe

Kk

k=1

  K
k + Pe
(V   V0 ) ,

(43)

and the following lower bound


V   V K  K +

K1
X
k=1



 

PeK1 PeK2 . . . Pek k + PeK1 PeK2 . . . Pe0 V   V0 .
(44)


Proof. First we derive an upper bound for V   VK . By equation (42), we have


V   Vk =
=

=



TV   TVk1 + k




T V   T Vk1 + T Vk1  TVk1 + k


TV   T Vk1 + k



Pe V   Vk1 + k .

By recursing on this inequality, we obtain an upper bound


V   VK 

K 
X


Pe

Kk

k=1

  K
k + Pe
(V   V0 ) .



Now we will derive a lower bound for V   VK . Let k denote the greedy policy with
respect to Vk . By (42), we have


V   Vk =
=

=



TV   TVk1 + k



TV   Tk1 V  + Tk1 V   TVk1 + k

Tk1 V   TVk1 + k


Pek1 V   Vk1 + k .
420

fiApproximate Value Iteration with Temporally Extended Actions

By recursing on this inequality, we obtain a lower bound


V   VK  K +

K1
X
k=1



 

PeK1 PeK2 . . . Pek k + PeK1 PeK2 . . . Pe0 V   V0 .

Lemma 3 gives a relationship between the quality of value function estimates and the
quality of the resulting greedy policy, while Lemma 4 gives upper and lower bounds on value
function estimates. The next step is to combine the results from these lemmas to derive a

pointwise error bound for V   V K .
We will make use of the following definition in deriving the point-wise error bound. The
lambda values are used to simplify the notation, but we also use the fact that they are
carefully designed so that they sum to 1.
Definition 13. For t = 1, 2, . . . , , let
k =

1
 (Kk)
1   K+1

(45)

for k = 0, 1, . . . , K.
The following lemma shows that the k values sum to 1.
P
Lemma 5. The  values defined by (45) satisfy K
k=0 k = 1 .
Proof. We have
PK

k=0 k

PK
1
(Kk)
=
k=0 1 K+1 
P
K
1
k
= 1
K+1
k=0 
P
k
= 11K+1 K
k=0 (1  )
= 11K+1 (1   K+1 )
= 1 .


Now we are ready to derive the point-wise error bound for V   V K .
Lemma 6. Let Z  {0, 1, 2, . . . , K}, k be the greedy policy with respect to the k th iterate
Vk derived by OFVI,  be an option policy such that Q (x, (x))  V  (x)   for all x  X,
and  =  dmin . If A3(, d, , , j) (Assumption 3) is true and the first Z iterates of OFVI

are pessimistic (i.e., for all x  X and k  {0, 1, 2, . . . , Z}, V  (x)  Vk (x)), then the

difference between V  and the value of the option policy K returned by OFVI is bounded
by
 X
K
X

V   V K  
k Pk,t |k | ,
t=1 k=0

where the k s are defined by (45),

=

2(1   K+1 )
(1  )3
421


,

fiMann, Mannor, & Precup

Pk,t

for t  1, and

i
 h


 KZ P  Zk

P
0kZ

 h
t
i

 Kk
1
=
P t
+ (P K1 P K2 . . . P k )t Z < k < K
2



1
k=K
 
 V  V0 k = 0
k =
 +
1kZ
 k
k
Z<kK

.

Proof. We can place an upper bound (43) and a lower bound (44) on the relationship

between VK and V  . Then we can use this information to bound the difference between

V K and V  . However, in this lemma, we will exploit the pessimism of the first Z iterates
and the option policy  to achieve a more informative bound.

When an iterate Vk is pessimistic V   Vk is lower bounded by 0. For an upper bound,
we have




V   Vk = V   T V  + T V   Vk

  + T V   Vk

=  + T V   TVk1 + k

=  + T V   T Vk1 + T Vk1  TVk1 + k

  + T V   T Vk1 + k
 Pe (V   Vk1 ) + (k + ) ,




where the initial inequality inserts the term (T V  + T V  ) = 0. The first step follows
from the fact that following  for a single decision and then following  produces an


-optimal policy, so V   T V   . The second step is due to the definition of k
from (42). The third step inserts (T Vk1 + T Vk1 ) = 0. The fourth step removes
T Vk1  TVk1 because the sum of those two terms is less than or equal to zero (since T
updates using the max operator, while T updates using the policy ). The fifth and final
step pulls out the discounted transition probability kernel Pe .
By recursing on this inequality Z  0 times we obtain
 
Z=0
 V
  V0 
  

Zj
Z

PZ
V   VZ 
. (46)

e
(j + ) + Pe
V   V0 1  Z  K

j=1 P
By combining our upper bound recursion from (43) with (46), we obtain terms
 

 Z 


 KZ


e

P
Pe
V   V0 k = 0



 


 Zk 
 KZ

e
uk k =
P
Pe
(k + ) k = 1, 2, . . . , Z








 Kk


Pe
k
k = Z + 1, Z + 2, . . . , K

such that


V   VK 
422

K
X
k=0

uk k

fiApproximate Value Iteration with Temporally Extended Actions



upper bounds the difference between V  and the final iterate derived by OFVI, VK .

Now, since 0 lower bounds the difference between V  and the first Z iterates of OFVI,
we can use 0 as our lower bound for the first Z iterations and fill in the rest of the iterates
with (44). This gives us the terms

0kZ

 h0
i
PeK1 PeK2 . . . Pek k Z < k < K  1 ,
lk k =


K
K
such that


V   VK 
lower bounds
the difference between V
P
VK |  K
(u
k=0 k  lk )k .
By Lemma 3, we have



K
X

lk k

k=0


and the final iterate VK . This implies that |V  


 P
 
K
(u

l
)
 (I  PeK )1 Pe  PeK
k k
fi
fi P k=0 k


fi
fi
K
1



(u
+
l
)|
|
 (I  Pe K ) fiPe  Pe K fi
k
k
k=0 k
P

K

1
 (I  Pe K )
(uk + lk )|k |
 k=0

P
P
K

eK )i
(u
+
l
)|
|
= 
(
P
k
k
k=0 k
i=0

P i  PK
 
i=0 
k=0 (uk + lk )|k |

P i  PK
(u
+
l
)|
|

 
k
k
k
k=0
i=0
 PK
 1
(u
+
lk )|k | ,
k=0 k



V   V K

where for the first step we have taken the absolute value
of both fisides of the inequality, and
fi
fi e
fi
d
min
for the second step we used the fact that 
 fiP  PeK fi. In the remainder of the
proof we will denote  dmin by .
For k = 0, we have

=
=

2
2






(u0 + l0 )|0 |
1


1(u0|0 |)

2
1



1
2




Pe

KZ 

 

Pe

Z 

Z 

|0 |



 KZ
Pe
|0 |
Pe

P

2

 K P0,t |0 |
 (1) K+1t=1
P 

2(1
)

1
K P | |
=

0,t 0
t=1 1 K+1
(1)2

P
 
0 P0,t |0 | .




2
1

t=1

For k = 1, 2, . . . , Z, we have


(uk + lk )|k |
1
423

fiMann, Mannor, & Precup

2
2

=
=


=




1(uk|k |)


 Zk 
 KZ

e
P
Pe
|k |

   KZ  Zk 
2
Pe
Pe
|k |
1
P


2
 Kk Pk,t |k |
(1)
t=1

  

2(1 K+1 ) P
1
Kk P | |

k,t k
(1)2
1 K+1


2
1

 




P

t=1



1
2

t=1

k Pk,t |k | .

For k = Z + 1, Z + 2, . . . , K, we have

(uk + lk )|k |
1
2
2




1

(uk + lk ) |k |
i
   Kk h
2
1




=
+ Pe K1 Pe K2 . . . Pe k |k |
Pe
1 2

P

2

 Kk Pk,t |k |
(1)
t=1

  

2(1 K+1 ) P
1
Kk P | |
=

k,t k
2
K+1
(1)
1

=



= 


P

t=1

t=1

k Pk,t |k | .


By plugging
in the results from these three inequalities, we obtain V   V K 
P
 PK
 t=1 k=0 k Pk,t |k |.
B.3 From Pointwise to Lp -norm Propagation Error
Lemma 6 gives us a pointwise bound on the loss of the policy K derived by OFVI compared
to following the optimal option policy  , but the most common function approximation architectures minimize an Lp -norm (not pointwise loss). In this subsection, we derive Lemma
8 that transforms our pointwise bound to an Lp -norm bound weighted by an arbitrary distribution   M(X). The key to this transformation is based on A2(, ) (Assumption 2),
which allows us to bound the pointwise transition probability kernels Pk,t from Lemma 6 by
c() from Assumption 2 at each iteration k  {1, 2, . . . , K}. The following lemma provides
the first step in this transformation.
Lemma 7. Suppose that A2(, ) (Assumption 2) holds, then
Pk,t 

max

m{1,2,...,i+t}

ct (m) ,

where ,   M (X).
Proof. We have two cases to consider (case 1) 1  k  Z and (case 2) Z < k  K.
424

(47)

fiApproximate Value Iteration with Temporally Extended Actions

For case 1, we have
Pk,t = 

h



P

KZ

(P  )Zk

 ct (K  k) .

i
t

For case 2, we have
h
i
 Kk
Pk,t =  21 P  t
+ (P K1 P K2 . . . P k )t
h
i
 Kk
+  (P K1 P K2 . . . P k )t
= 12  P  t
 12 [ct (K  k) + ct (K  k)]
= ct (K  k) .
To derive the Lp -norm bound, we need the following additional notation to represent
the set of options that can be initialized from state x  X and have duration longer than
some d  1.
Definition 14. Let d  1, x  X be a state, and O be a set of options. The set Ox,d
denotes the
h subset
i of options o  O that can be initialized from the state x, such that
o
inf Y X E Dx,Y  d.
Notice that by assumption Ox,dmin  Ox .
Lemma 8. Let K  1,  > 0, and Z  {0, 1, 2, . . . , K}. Suppose that Assumption 2 and
Assumption 3 are true and that the first Z iterates of OFVI are pessimistic, then
 !

1/p 2 V   V0 

2

kV   V K kp, 
C1/p (+)+  dmin (K+1)+(1)(ddmin )bZ/jc
(1  )2 ,
(1  )2
(48)
holds, provided that the approximation errors k satisfy kk kp,   for all k = 1, 2, . . . , K.
Proof. First note that

(x) =


arg maxoOx,d Q (x, o) if x  ,d
 (x)
otherwise

.



is a policy such that Q (x, (x))  V  (x)   for all x  X. Therefore, by Lemma 6, we
have
 X
K
X

V   V K  
k Pk,t |k | .
t=1 k=0

Now, we have
 

V  V K p
p,

fi 
fip
(x) fiV  (x)  V K (x)fi dx
  K
p
R
P P

(x) 
k Pk,t |k |(x) dx
t=1 k=0


p
 P
K
R
P

p

  (x)
k Pk,t |k + | + 0 P0,t |V  V0 | (x) dx ,
=

R

t=1 k=1

425

fiMann, Mannor, & Precup



where the initial equality is due to the definition of kkp, . The first step replaces V  V K
P PK
with  
t=1
k=0 k Pk,t |k |. The last step pulls k = 0 out of the sum and moves  outside
of the integral.
K
P
Recall by Lemma 5 that
k = 1. We apply Jensens inequality twice; once with
k=0

convex function |  |p and parameters P
k for k = 0, 1, . . . , K, and once with parameters
determined by the stochastic operators 
t=1 Pk,t , to obtain
#
Z "X
 X
K
 
p

 

k Pk,t |k + |p + 0 P0,t |V   V0 |p (x)dx .
V  V K   p 
p,

t=1 k=1





Noticing that |V   V0 | is bounded by kV   V0 k , we obtain

 

P PK
p
p
V  V K p
 
t=1
k=1 k Pk,t |k + | +
p,

R

(x)0 P0,t kV   V0 kp dx .
By Assumption 2 and Lemma 7, we have that
Pk,t 

max

m{1,2,...,Kk+t0 1}

cKk+t0 1 (m) ,

where t0 = t  (K  k) + 1. Thus we have
K P

P
k=1 t=1

k Pk,t |k + |p 

K P

P
k=1

t0 =1

1
 Kk 
1 K+1

max

m{1,2,...,Kk+t0 1}




K P
P

k=1 t0 =1

t

0 1

0
1
 Kk  t 1 
(1 K+1 )

max

m{1,2,...,Kk+t0 1}



(1)2
1 K+1

K P

P

k=1 t0 =0

max

m{1,2,...,k+t0 }




cKk+t0 1 (m)kk + kpp,

cKk+t0 1 (m)kk + kpp,
0

 k+t 

ck+t0 (m)kk + kpp,


P
1
(1  )2
t t1 maxm{1,2,...,t} ct (m)kk
1 K+1
t=1
1
C ( + )p ,
1 K+1 ,

+ kpp,

where C, is the SMDP discounted average concentrability coefficient from Assumption 2.
P PK
p
By replacing 
t=1
k=1 k Pk,t |k + | , we get

p 
 

2(1 K+1 )
1
V  V K p
C ( + )p +

(1)2
1 K+1 ,
p,

(49)
P R

p

(x)
P
kV

V
k
dx
.

0
0,t
0
t=1
426

fiApproximate Value Iteration with Temporally Extended Actions

Consider the second term in the last step of (49). By replacing P0,t with its definition,
we get




Z 
R
R
P
P
 KZ
(1)
K


(x)
0 P0,t dx =
(x)

dx
(P )
P
1 K+1
K+t1
t=1
t=1




Z 
R
P
 KZ
(1)
K
t1


=
(x)
 
(P )
dx
P
 t1 (1 K+1 )
K+t1
t=1




Z

(1)2 R
 1
(x) (Pe )KZ Pe
dx
K+1


(1)2 dmin K+(1)(ddmin )bZ/jc

1 K+1

,

(50)
where the initial equality is due to expanding P0,t by its definition. The first step simplifies

KZ
and drops the dependence on (1  )(I  PeK )1  1. The final step replaces PeK
 Z
 Z
d
(KZ)

(1)dbZ/
jc+d
Z
e
min
min
with 
and P
with 
. Under any case, Pe
  dmin K .
Under Assumption 3 with probability (1  ) either (a) the state transitioned to is in ,d ,
in which case the effective discount factor is  d , or (b) following the bridge policy  from
the current state reaches a state in ,d in no more than j timesteps. On the timesteps that
the agent is not in ,d the effective discount factor is  dmin , but with probability (1) this
 Z
can only happen Z  bZ/jc times during. Thus Pe
  (1+)dmin Z+(1)(ddmin )bZ/jc 
 dmin K+(1)(ddmin )bZ/jc .
By replacing the second term from (49) with (50), we get
p 


 
2(1 K+1 )
1
V  V K p
C ( + )p +

(1)2
1 K+1 ,
p,

(1)2 dmin K+(1)(ddmin )bZ/jc

kV 
1 K+1

Since 1   K+1
 

V  V K p
p,

p 



1
1 K+1





p h

2
(1)2



V0 kp


.

 1, then


C, ( + )p + (1  )2  dmin K+(1)(ddmin )bZ/jc kV   V0 kp

Thus, we have
 

V  V K 
p,



1/p
2
C (
(1)2 ,


1/p 


2kV  V0 k
+ ) +  dmin (K+1)+(1)(ddmin )bZ/jc
.
2
(1)

B.4 Proof of Theorem 1
Proof. (of Theorem 1) We use Lemma 2 to select appropriate values for n and m, when
1/p
0 = (1  )2 /(2C, ) and  0  K .
Since the iterates V1 , V2 , . . . , VK are random objects, we cannot directly apply Lemma
2 to bound the error at each iteration. However, this problem was resolved in the proof of
427

i

.

fiMann, Mannor, & Precup

Thm. 2 from the work of Munos & Szepesvari, 2008 by using the fact that the algorithm
collects independent samples at each iteration.
The iterate Vk+1 depends on the random variable Vk and the random samples Sk containing the n  m  |O| next states, rewards, and trajectory lengths. Let the function

	
f (Sk , Vk ) = I kVk+1 (Vk , Sk )  TVk kp,  dp, (TVk , F) + 0  (1   0 ) ,
where we have written Vk+1 (Vk , Sk ) to emphasize Vk+1 s dependence on both random variables Vk and Sk . Notice that Vk and Sk are independent because Sk was not used to
generate Vk and the simulator S generates independent samples. Because Vk and Sk are independent random variables, we can apply Lemma 5 from the work of Munos & Szepesvari,
2008. This lemma tells us that E [f (Sk , Vk ) | Vk ]  0 provided that E [f (Sk , v)]  0 for
allv  F. For any v  F, by Lemma 2, and by our choice of n and m, we have that
P kVk+1 (v, Sk )  Tvkp,  dp, (Tv, F) + 0  1   0 . This implies that E [f (Sk , v)]  0.
By Lemma 5 from the work of Munos & Szepesvari, 2008, we have
 that E [f (Sk , Vk ) | Vk ]  0.
0
Thus we have P kVk+1 (Vk , Sk )  TVk kp,  dp, (Tv, F) +   1   0 . By the union
bound, this ensures that kkp,   for all K iterations with probability at least 1  K 0 =
1  K(/K) = 1  .
The result follows by applying Lemma 8 with kk kp,  dp, (TVk , F) + 0 .
kV   V K kp,


 


 V   V  p, + V   V K p,
1/p

2
0
 Lp, ( ) + (1)
2 C, (bp, (TF, F) +  +  )

1/p 


2kV  V0 k
+  dmin (K+1)+(1)(ddmin )bZ/jc
2
(1)


1/p
2
2 /(2C1/p )
C
b
(TF,
F)
+

+
(1

)
= Lp, ( ) + (1)
,
,
p,
2


1/p 


2kV V0 k
+  dmin (K+1)+(1)(ddmin )bZ/jc
(1)2
1/p

2
= Lp, ( ) + (1)
2 C, (bp, (TF, F) + ) + 

1/p 


2kV  V0 k
+  dmin (K+1)+(1)(ddmin )bZ/jc
.
(1)2

Appendix C. Proof of Theorems 2, and 3
In this appendix, we prove Theorems 2, and 3. First we will analyze LAVI by proving
Theorem 2. Then we use one of the lemmas developed for analyzing LAVI to analyze OFVI
with landmark-based options and prove Theorem 3. Throughout this appendix we assume
that rewards are bound to [RMAX , 0], so that stochastic shortest path problems are well
defined.
Each iteration Algorithm 2 performs the operator Tbm defined by


m


1 X  e(j)
(j)
b
Tm V (x) = max
Rx,o +   (V, y (j) )
oOx m
j=0

428

(51)

fiApproximate Value Iteration with Temporally Extended Actions

Table 2: Errors Impacting Landmark-based VI
Error Name
Landmark Error
Loc. Planning Error
Loc. Lipschitz Error

Symbol
L
P
H

Stoc. Plan Failure



Relaxation Error
Sampling Error

R
S

Due to . . .
selected landmarks
c
Ps sub-optimality in M
terminate in y where
(y, l) > 0
prob. terminating far from
L
increased cost in M
finite # samples

at each state x  X where

(V, y) =

maxlL (y) V (l) if L (y) 6= 
0
otherwise

.

If we consider the limit of Tbm as m  , then we obtain T defined by


Z

X
e

(T V ) (x) = max R
 t Pto (y|x)(V, y)dy 
x,o +
oOx

(52)

(53)

t=1 yX

for each state x  X.
However, we would like to compare T to the Bellman optimality operator T defined by


 Z
X
e

(TV ) (x) = max R
 t Pto (y|x)V (y)dy  ,
(54)
x,o +
oOx

t=1 yX

for which it is well known to converge to the optimal value function with respect to option
set O.
Throughout this analysis we will work with vectors with dimension |X| but we mostly
focus on |L| elements. For vectors V and V 0 in [VMAX , 0]|X| , we define the max-norm with
respect to the subset of states in L  X by


fi
fi
V  V 0  = max fiV (l)  V 0 (l)fi ,
(55)
L
lL

which measures the difference between V and V 0 only at the states in L.
Table 2 provides an overview of the errors that contribute to the sub-optimality of
policies derived from LAVI.
C.1 Proof of Theorem 2
We proceed by bounding the value estimation error, which we use to bound the loss of the
derived policy. Next we bound the error due to using a deterministic local planner. Finally,
we use these results to prove Theorem 2.
429

fiMann, Mannor, & Precup

C.1.1 Bounding the Value Estimation Error
Lemma 9. (Bound Value Estimation Error with Tbm ) Let 1 > 0,   (0, 1], K  1,
V  [VMAX , 0]|S| , L be the set of landmark states, O the set of landmark options, and 
be the optimal policy on M with respect to O. If


2LK
1
ln
,
(56)
m>
2(1 (1  ))2

all landmark-option pairs have a duration of at least dmin , and Assumption 5 holds, then
with probability at least 1  /K



 

b

 
 
Tm V  VM
   dmin VMAX + (1  ) + (1  ) V  VM
 + 1
L

L

where  is the stochastic plan failure,  is a distance threshold, and  is the Lipschitz
coefficient from Assumption 5. By recursing on this inequality K times, we obtain



K  
 dmin (VMAX + (1  )) + 1 

 

 
dmin
+
(1

)
V

V
VK  VM
 


0
d
min
1
L
L

(57)

with probability at least 1  .
Proof. Notice that for all l  L
fi
fi
fi
fi
fi b
 (l)fi = fi(T
 )(l)fi
fi(Tm V )(l)  VM
fi
fi bm V )(l)  (TVM
fi
fi
fi

fi b
 )(l) + (T V  )(l)  (TV  )(l)fi
= fi(Tm V )(l)  (T VM
fi
M
fi
fi fi M
fi



fi b
fi
 )(l) + fi(T V  )(l)  (TV  )(l)fi
 fi(Tm V )(l)  (T VM
fi
M
M


(58)



 (l) = (TV  )(l), the first step inserts
where the initial equality is due to the fact that VM
M




(T VM )(l) + (T VM )(l) = 0, and the
last
step
uses
the
triangle
inequality.
fi
fi

fi b
fi
Now let X denote the event that fi Tm V (l)  (T V ) (l)fi  1 . If event X occurs, then

the first term in the last step of inequality (58) is

!
fi
fi
fi
 R
fi
P

fi b
fi
 )(l)
el,o +
 t Pto (y|x)(V, y)dy 
fi(Tm V )(l)  (T VM
fi = fifi max R
oOl
t=1 yX
!fi
 R
fi
P

 , y)dy fi + 
el,o +
max R
 t Pto (y|x)(VM
1
fi
oOl

 max

 R
P

oOl t=1 yX

  dmin max

t=1 yX

fi
fi
 , y)fi dy + 
 t Pto (y|x) fi(V, y)  (VM
1

P

R

oOl t=d
min yX

fi
fi
 , y)fi dy +  ,
Pto (y|x) fi(V, y)  (VM
1

where the last step in the previous inequality is due to the fact that all landmark-option
pairs execute for at least dmin timesteps (meaning that they have effective discount factor
less than or equal to  dmin ). By our choice of m and using Hoeffdings inequality it can
easily be shown that event X occurs for a landmark-option pair with probability at least
430

fiApproximate Value Iteration with Temporally Extended Actions

1  /LK. Since there are L =

P
lL

|Ol | total landmark-option pairs, then using the union

bound we have
P that event X holds for all of these landmark-options pairs with probability
at least 1  L
i=1 /LK = 1  L (/LK) = 1  /K.
 , y)| = |0  0| = 0. On the other hand, if
Now, if L (y) = , then |(V, y)  (VM
L (y) 6= , then we have
fi
fi
fi
fi
fi
fi
fi(V, y)  (V  , y)fi = fi max V (l0 )  max V  (l0 )fi
M
M
fil0 L (y)
fi
l0 L (y) fi
fi
 0
0

fi
fi
 max V (l )  VM (l )
l0 L (y)


 

 V  VM
,
L
fi
fi


fi
 )(l)fi   dmin (1  ) V  V   +  holds for all
which implies that fi(Tbm V )(l)  (T VM
fi
1
M L
landmarks with probability at least 1  /K.
The second term in the last step of inequality (58) is
!
fi
 R
fi
fi
fi
P



 , y)dy 
el,o +
fi(T V  )(l)  (TV  )(l)fi = fi max R
 t Pto (y|x)(VM
M
M
fi oOl
t=1 yX
!fi
 R
fi
P

 (y)dy fi
el,o +
max R
 t Pto (y|x)VM
fi
oOl
t=1 yX
 R
fi
fi
P
 , y)  V  (y)fi dy
 max
 t Pto (y|x) fi(VM
M
oOl t=1 yX

  dmin max


P

R

oOl t=d
min yX

fi
fi
 , y)  V  (y)fi dy
Pto (y|x) fi(VM
M

where the last step in the previous inequality is due to the fact that all landmark-option
pairs execute for at least dmin timesteps.
 , y)  V  (y)|  |0  V  (y)|  V
With probability at most , we have |(VM
MAX and
M
M
 , y)  V  (y)|  | max 0
 (l0 )  V  (y)|  .
with probability 1   we have |(VM
V
l L (y)
M
fi
fi


Thus, fi(T V  )(l)  (TV  )(l)fi   dmin (VMAX + (1  )).
the two terms in the last step

 of inequality
  (58) we obtain our result
 By replacing
T V  V     dmin VMAX + (1  )  + V  V   +1 with probability at least
M L
L
1  /K.
C.1.2 Bounding the Policy Error
Lemma 10. (Bound Policy Error) Let 2 > 0, O be a set of landmark options and dmin  1
|L|

be the minimum duration of all state-options pairs,
MAX , 0] ,   0, and  be

 V  [V

    and
the optimal policy with respect to O. Suppose that V  VM
L


 Z
X
e

(x) = arg max R
Pto (y|x) max (V, y)dy 
x,o +
oOx

t=1 yX

lL

is the greedy policy with respect to V . If Assumption 5 holds, then
 

 dmin ((1  ) ( + ) + VMAX )
 

VM  VM  
1   dmin

431

(59)

fiMann, Mannor, & Precup

holds.

Proof. Let x  X, o = (x), and o =  (x). Since (x) = o, then

ex,o +
R

 Z
X
t=1 yX



ex,o +
 t Pto (y|x)(V, y)dy  R

 Z
X

 t Pto (y|x)(V, y)dy .

(60)

t=1 yX

Let G = {y  X | L (y) 6= } and G = X\G. The set G contains all states that are closer
than  to at least one landmark state. By rearranging we obtain

ex,o  R
ex,o 
R

 R
P
t=1 yX
 R
P




 t Pto (y|x)  Pto (y|x) (V, y)dy




 t Pto (y|x)  Pto (y|x) (V, y)dy
t=1 yG

R t o

+
 Pt (y|x)  Pto (y|x) (V, y)dy

ex,o  R
ex,o 
R

ex,o  R
ex,o 
R
ex,o  R
ex,o 
R
ex,o  R
ex,o 
R

yG

P

R

t=1 yG




 t Pto (y|x)  Pto (y|x) max V (l0 )dy


P

R

t=1

yG


P
t=1

l0 L (y)




 t Pto (y|x)  Pto (y|x)







!

max VM (l0 ) +  dy
!


R t o


 (y) +  +  dy
 Pt (y|x)  Pto (y|x) VM
,
l0 L (y)

yG

where the initial inequality rearranges (60) to isolate the reward terms. The first step is
obtained by dividing the sum over states into states where L (y) 6=  and states where
L (y) = . The third step replaces (V, y) by its definition. Since (V, y) = 0 for all
y where L (y) = , the second term on the right hand side disappears. The fourth step
 (l0 ) + , and the final step uses Assumption 5 to replace V  (l0 ) with
replaces V (l0 )  VM
M
 (y) + .
VM
432

fiApproximate Value Iteration with Temporally Extended Actions

Now we have
 R
 

 (y)  P o (y|x)V  (y) dy
 (x)  V  (x) = R
ex,o + P
ex,o  R
 t Pto (y|x)VM
VM
t
M
M
t=1 yX

ex,o  R
ex,o
= R


P R t  o
 (y)  P o (y|x)V  (y) dy
+
 Pt (y|x)VM
t
M
+

t=1 yG
 R
P

t=1 yG

 

 (y)  P o (y|x)V  (y) dy
 t Pto (y|x)VM
t
M

ex,o  R
ex,o
 R


P R t  o
 (y)  P o (y|x)V  (y) dy
+
 Pt (y|x)VM
t
M
t=1 yG

+  dmin VMAX
!
 R
 o
 

P

t
o

 Pt (y|x)  Pt (y|x) VM (y) +  +  dy
t=1 yG
!
 R

 o
P


t

o
+
 Pt (y|x)VM (y)  Pt (y|x)VM (y) dy
t=1 yG

+ 
 dmin VMAX
 R
 

P
 (y)  P o (y|x) V  (y) +  + 
 t Pto (y|x)VM
=
t
M
t=1 yG



 (y) +  +   P o (y|x)V  (y) dy +  dmin V
o
+ Pt (y|x) VM
MAX
t
M
!
 R 

P
 (y)  V  (y) +  +  dy +  dmin V
Pto (y|x) VM
  dmin
MAX
M
t=1 yG



  V   +  +  +  dmin V
  dmin (1  ) VM
MAX .
M 
By recursing on this inequality, we obtain

 
 

VM  VM 





 dmin ((1  )( + ) + VMAX )
.
1  d

C.1.3 Bounding Error in the Deterministic Relaxation
b  be the optimal policy over options in M
c, then
Lemma 11. Let 


b 
 


V
VM
c 
c
M





2(L + P )
1   d

holds.
Proof. For any l  L, we have


b



eP(l,l0 ) +  P(l,l0 ) V b  (l0 ) .
VM
(l)

V
(l)

V
(l)

max
R
c
c
c
c
0
M
M
M
l Ll

433

fiMann, Mannor, & Precup

By the definition of landmark error, we have




b


ep (l,l0 ) +  |pG (l,l0 )| V  (l0 )  max R
eP(l,l0 ) +  |P(l,l0 )| V b  (l0 ) + L ,
VM
(l)

V
(l)

max
R
c
c
c
c
G
0
0
M
M
M
l Ls

l Ll

and by the definition of planning error, we have




b


eP(l,l0 ) +  |P(l,l0 )| V b  (l0 ) max R
eP(l,l0 ) +  |P(l,l0 )| V  (l0 ) +L +P .
VM
(l)V
(l)

max
R
c
c
c
c
0
0
M
M
M
l Ll

l Ll

Now if we take the max over the set of valid landmark destinations from l, we get


b
 (l)  V 
eP(l,l0 )   |P(l,l0 )| V  (l0 ) + L + P
eP(l,l0 ) +  |P(l,l0 )| V b  (l0 )  R
(l)

max
R
Vc
c
c
c
M
M
M
M
l0 Ll


0

b
 (l0 )  V 
0
 |P(l,l )| V c
 max
c (l ) + L + P .
0
M

l Ll

M


Since all
 landmarks are separated by paths of length at least d while planning with P, we
 
b 
L +P

obtain V c
 Vc
.
  1
d
M
M L
Therefore by similar reasoning as above, we can see that for any state x  X
b



VM
c(x)  VM
c (x)  L + P +

 (L + P )
2 (L + P )

.

d
1
1   d

C.1.4 Proof of Theorem 2
Proof. (of Theorem 2)
We apply Lemma 9 with 1 =

S (1 dmin )2
(1) dmin

and   (0, 1] and Lemma 10 to obtain



 

 dmin
 
K 
VMAX + (1  ) +
VM  VM  
1   dmin
1,



  
1
 dmin

dmin K  
[VMAX + (1  )] +
+ (1  )
.
VM  V0 
d
d
min
min
1
1
L
(61)
By replacing  with H and 1 , we get
 

 
K 
V

V
 M
M 

1,



 dmin
1   dmin


1+

(1  ) dmin
1   dmin



(VMAX + (1  )H ) +
 
 !
V  V0 
M
L
S + (1  )2  (K+1)dmin
(62)
1   dmin

after rearranging terms.
Due to the definition of relaxation error
 

V  V  
M
M 1, 


(Definition 12),


 
b 

V

V
+ R
 M
c
c 
M
2(L +P )
b
1 dmin

434

1,

+ R

(63)

fiApproximate Value Iteration with Temporally Extended Actions

where the last step is due to Lemma 11.
By combining (62) and (63), we obtain


kV  V

K k

1,




where  =



 dmin
1 dmin


1+

2(L +P )
1 dmin
b

2(L +P )
1 dmin
b

+ R +  + S + (1 
+ R +  + S +

(1) dmin
1 dmin





 

VM V0 

)2  (K+1)dmin

 (K+1)dmin

1 dmin



 

VM V0 
1 dmin

!
L

!
L

(VMAX + (1  )H ).

C.2 Proof of Theorem 3
Proof. (of Theorem 3)
By Corollary 1, we have that
Lp, (K )  Lp, ( ) +

1/p

2 dmin 1/p
dmin (K+1)
C
b
(TF,
F)
+

+

p,
S
(1  )2 ,



2 kV   V0 k
(1  )2


.
(64)

By (63), we have that
Lp, ( ) 

2(L + P )
1   dbmin

+ R .

The result follows by replacing Lp, ( ) in (64) with the right hand side from the previous
inequality.

References
Barry, J. L., Kaelbling, L. P., & Lozano-Prez, T. (2011). DetH*: Approximate Hierarchical
Solution of Large Markov Decision Processes. In International Joint Conference on
Artificial Intelligence.
Bertsekas, D. P., & Tsitsiklis, J. (1996). Neuro-dynamic programming. Athena Scientific.
Brunskill, E., Leffler, B. R., Li, L., Littman, M. L., & Roy, N. (2008). CORL: A ContinuousState Offset-Dynamics Reinforcement Learner. In Proceedings of the 24 th Conference
on Uncertainty in Artificial Intelligence (UAI-08).
Chassin, D. P., Fuller, J. C., & Djilali, N. (2014). GridLAB-D: An agent-based simulation
framework for smart grids. Journal of Applied Mathematics, 2014.
Comanici, G., & Precup, D. (2010). Optimal policy switching algorithms for reinforcement
learning. In Proceedings of the 9 th International Conference on Autonomous Agents
and Multiagent Systems, pp. 709714.
Dietterich, T. G., Taleghan, M. A., & Crowley, M. (2013). PAC optimal planning for invasive
species management: Improved exploration for reinforcement learning from simulatordefined MDPs. In Proceedings of the National Conference on Artificial Intelligence.
Dijkstra, E. (1959). A note on two problems in connexion with graphs. Numerische Mathematik, 1 (1), 269271.
435

fiMann, Mannor, & Precup

Farahmand, A., Ghavamzadeh, M., Szepesvari, C., & Mannor, S. (2008). Regularized fitted
Q-iteration: Application to planning. In Recent Advances in Reinforcement Learning,
pp. 5568. Springer.
Farahmand, A., Munos, R., & Szepesvari, C. (2010). Error propagation for approximate
policy and value iteration. In Advances in Neural Information Processing Systems.
Fernandez, F., & Veloso, M. (2006). Probabilistic policy reuse in a reinforcement learning
agent. In Proceedings of the 5th International Joint Conference on Autonomous Agents
and Multiagent Systems, pp. 720727.
Hart, P., Nilsson, N., & Raphael, B. (1968). A formal basis for the heuristic determination
of minimum cost paths. Systems Science and Cybernetics, IEEE Transactions on,
4 (2), 100107.
He, R., Brunskill, E., & Roy, N. (2011). Efficient planning under uncertainty with macroactions. Journal of Artificial Intelligence Research, 40, 523570.
Hoey, J., St-Aubin, R., Hu, A. J., & Boutilier, C. (1999). SPUDD: Stochastic Planning
Using Decision Diagrams. In Proceedings of Uncertainty in Artificial Intelligence,
Stockholm, Sweden.
Iba, G. A. (1989). A heuristic approach to the discovery of macro-operators. Machine
Learning, 3, 285317.
Jong, N. K., & Stone, P. (2008). Hierarchical model-based reinforcement learning: Rmax +
MAXQ. In Proceedings of the 25th International Conference on Machine Learning.
Kearns, M., Mansour, Y., & Ng, A. Y. (2002). A sparse sampling algorithm for near-optimal
planning in large Markov decision processes. Machine Learning, 49 (2-3), 193208.
Kocsis, L., & Szepesvari, C. (2006). Bandit based Monte-Carlo Planning. In Machine
Learning: ECML2006, pp. 282293. Springer.
Konidaris, G., & Barto, A. (2009). Skill discovery in continuous reinforcement learning
domains using skill chaining. In Advances in Neural Information Processing Systems
22, pp. 10151023.
Konidaris, G., & Barto, A. G. (2007). Building portable options: Skill transfer in reinforcement learning.. In Proceedings of the International Joint Conference on Artificial
Intelligence, Vol. 7, pp. 895900.
Konidaris, G., Kuindersma, S., Barto, A., & Grupen, R. (2010). Constructing skill trees for
reinforcement learning agents from demonstration trajectories. In Advances in Neural
Information Processing Systems, pp. 11621170.
Lazanas, A., & Latombe, J.-C. (1992). Landmark-based robot navigation. Tech. rep.,
Stanford University.
Lazaric, A., Ghavamzadeh, M., & Munos, R. (2010). Analysis of a classification-based policy
iteration algorithm. In Proceedings of the 27th International Conference on Machine
Learning.
Littman, M. L., Dean, T. L., & Kaelbling, L. P. (1995). On the complexity of solving
Markov decision problems. In Proceedings of the 11th conference on Uncertainty in
artificial intelligence, pp. 394402.
436

fiApproximate Value Iteration with Temporally Extended Actions

Mankowitz, D. J., Mann, T. A., & Mannor, S. (2014). Time-regularized interrupting options.
In Proceedings of the 31st International Conference on Machine Learning.
Mann,
T.
A.
(2014).
Cyclic
Inventory
Management
https://code.google.com/p/rddlsim/source/browse/trunk/files/
rddl2/examples/cim.rddl2. Accessed: 2015-06-29.

(CIM).

Mann, T. A., & Mannor, S. (2014). Scaling up approximate value iteration with options:
Better policies with fewer iterations. In Proceedings of the 31 st International Conference on Machine Learning.
Mannor, S., Menache, I., Hoze, A., & Klein, U. (2004). Dynamic abstraction in reinforcement
learning via clustering. In Proceedings of the 21st International Conference on Machine
learning, ICML 04, pp. 71, New York, NY, USA. ACM.
McGovern, A., & Barto, A. G. (2001). Automatic discovery of subgoals in reinforcement
learning using diverse density. In Proceedings of the 18th International Conference on
Machine Learning, pp. 361  368, San Fransisco, USA.
Minner, S. (2003). Multiple-supplier inventory models in supply chain management: A
review. International Journal of Production Economics, 8182, 265279. Proceedings
of the 11th International Symposium on Inventories.
Munos, R. (2005). Error bounds for approximate value iteration. In Proceedings of the
National Conference on Artificial Intelligence.
Munos, R., & Szepesvari, C. (2008). Finite-time bounds for fitted value iteration. Journal
of Machine Learning Research, 9, 815857.
Peleg, D., & Schaffer, A. A. (1989). Graph spanners. Journal of Graph Theory, 13 (1),
99116.
Peters, J., & Schaal, S. (2008). Reinforcement learning of motor skills with policy gradients.
Neural Networks, 21, 682691.
Precup, D., & Sutton, R. S. (1997). Multi-time models for temporally abstract planning.
In Advances in Neural Information Processing Systems 10.
Precup, D., Sutton, R. S., & Singh, S. (1998). Theoretical results on reinforcement learning
with temporally abstract options. In Machine Learning: ECML1998, pp. 382393.
Springer.
Puterman, M. L. (1994). Markov Decision Processes - Discrete Stochastic Dynamic Programming. John Wiley & Sons, Inc.
Riedmiller, M. (2005). Neural fitted Q iterationfirst experiences with a data efficient
neural reinforcement learning method. In Machine Learning: ECML2005, pp. 317
328. Springer.
Sanders, P., & Schultes, D. (2005). Highway hierarchies hasten exact shortest path queries.
In Brodal, G., & Leonardi, S. (Eds.), Algorithms: ESA2005, Vol. 3669 of Lecture
Notes in Computer Science, pp. 568579. Springer Berlin Heidelberg.
Scarf, H. (1959). The optimality of (s,S) policies in the dynamic inventory problem. Tech.
rep. NR-047-019, Office of Naval Research.
437

fiMann, Mannor, & Precup

Scherrer, B., Ghavamzadeh, M., Gabillon, V., & Geist, M. (2012). Approximate Modified
Policy Iteration. In Proceedings of the 29th International Conference on Machine
Learning, Edinburgh, United Kingdom.
Sethi, S. P., & Cheng, F. (1997). Optimality of (s,S) policies in inventory models with
markovian demand. Operations Research, 45 (6), 931939.
Shantia, A., Begue, E., & Wiering, M. (2011). Connectionist reinforcement learning for
intelligent unit micro management in starcraft. In Proceedings of the International
Joint Conference on Neural Networks, pp. 17941801. IEEE.
Silver, D., & Ciosek, K. (2012). Compositional planning using optimal option models. In
Proceedings of the 29th International Conference on Machine Learning, Edinburgh.
Simsek, O., & Barto, A. G. (2004). Using relative novelty to identify useful temporal abstractions in reinforcement learning. In Proceedings of the 21st International Conference
on Machine Learning, pp. 95102, New York, NY, USA. ACM.
Sorg, J., & Singh, S. (2010). Linear options. In Proceedings of the 9th International Conference on Autonomous Agents and Multiagent Systems, pp. 3138.
Stolle, M., & Precup, D. (2002). Learning options in reinforcement learning. In Abstraction,
Reformulation, and Approximation, pp. 212223. Springer.
Stone, P., Sutton, R. S., & Kuhlmann, G. (2005). Reinforcement learning for robocup soccer
keepaway. Adaptive Behavior, 13 (3), 165188.
Sutton, R. S., Precup, D., & Singh, S. (1999). Between MDPs and semi-MDPs: A framework
for temporal abstraction in reinforcement learning. Artificial Intelligence, 112 (1), 181
211.
Tamar, A., Castro, D. D., & Mannor, S. (2013). TD methods for the variance of the rewardto-go. In Proceedings of the 30 th International Conference on Machine Learning.
Wolfe, A. P., & Barto, A. G. (2005). Identifying useful subgoals in reinforcement learning
by local graph partitioning. In Proceedings of the 22nd International Conference on
Machine Learning, pp. 816823.
Yoon, S. W., Fern, A., & Givan, R. (2007). FF-Replan: A Baseline for Probabilistic Planning. In Proceedings of the International Conference on Automated Planning and
Scheduling, Vol. 7, pp. 352359.

438

fiJournal of Artificial Intelligence Research 53 (2015) 91-126

Submitted 12/14; published 05/15

The Ceteris Paribus Structure of Logics of Game Forms
Davide Grossi

d.grossi@liverpool.ac.uk

Department of Computer Science
University of Liverpool

Emiliano Lorini

emiliano.lorini@irit.fr

IRIT-CNRS
Universite Paul Sabatier

Francois Schwarzentruber

francois.schwarzentruber@ens-rennes.fr

ENS Rennes - IRISA

Abstract
The article introduces a ceteris paribus modal logic, called CP, interpreted on the
equivalence classes induced by finite sets of propositional atoms. This logic is studied
and then used to embed three logics of strategic interaction, namely atemporal STIT, the
coalition logic of propositional control (CLPC) and the starless fragment of the dynamic
logic of propositional assignments (DLPA). The embeddings highlight a common ceteris
paribus structure underpinning the key operators of all these apparently very different logics
and show, we argue, remarkable similarities behind some of the most influential formalisms
for reasoning about strategic interaction.

1. Introduction
The logical analysis of agency and gamesfor an expository introduction to the field see van
der Hoek and Paulys overview paper (2007)has boomed in the last two decades giving
rise to a plethora of different logics in particular within the multi-agent systems field.1 At
the heart of these logics are always representations of the possible choices (or actions) of
groups of players (or agents) and their powers to force specific outcomes of the game. Some
logics take the former as primitives, like STIT (the logic of seeing to it that, Belnap, Perloff,
& Xu, 2001; Horty, 2001), some take the latter like CL (coalition logic, Pauly, 2002; Goranko,
Jamroga, & Turrini, 2013) and ATL (alternating-time temporal logic, Alur, Henzinger, &
Kupferman, 2002).
In these formalisms the power of players is modeled in terms of the notion of effectivity.
In a strategic game, the -effectivity of a group of players consists of those sets of outcomes
of the game for which the players have some collective action which forces the outcome of
the game to end up in that set, no matter what the other players do (Moulin & Peleg,
1982). So, if a set of outcomes X belongs to the -effectivity of a set of players J, there
exists an individual action for each agent in J such that, for all actions of the other players,
the outcome of the game will be contained in X. If we keep the actions of the other agents
1. The richness of this logical landscape was the object of the IJCAI13 invited talk by A. Herzig Logics
for Multi-Agent Systems: a Critical Overview.
c
2015
AI Access Foundation. All rights reserved.

fiGrossi, Lorini, & Schwarzentruber

fixed, then the selection of an individual action for each agent in J corresponds to a choice
of J under the assumption that the other agents stick to their choices.
It was already observed by van Benthem, Girard, and Roy (2009) that this formalization
of choice and power in games is of an all other things being equal, or ceteris paribus, nature.
Considering which outcomes of a game are possible for a set of players J once the other
players have fixed their actions, amounts to considering what may be the case under the
ceteris paribus condition all actions of the agents not in J being equal (to their current
ones). In the aforementioned work van Benthem et al. also show how this intuition can
be used, for instance, to give a modal formulation of Nash equilibria of one-shot games.2
In the current paper we leverage this intuition further and show how it can provide a novel
systematization of many of the most influential formalisms in the field of logic and games.
1.1 Scientific Context
Formal relationships linking the logics (or fragments thereof) we mentioned above have
been object of several publications. Notable examples are: the embedding of CL into the
next-time fragment of ATL (Goranko, 2001) and the embedding of CL into NCL (normal
coalition logic, Broersen, Herzig, & Troquard, 2007; Balbiani, Gasquet, Herzig, Schwarzentruber, & Troquard, 2008a), the embedding of CL and ATL into STIT (Broersen, Herzig,
& Troquard, 2005, 2006). Earlier contributions have also attempted more comprehensive
systematizations of the field of logic and games. Two in particular are worth mentioning:
Goranko and Jamrogas work (2004), which compares game logics based on the computation tree abstraction like ATL and its variants; and Herzigs work (2014), which provides
a conceptual and syntax-basedwhile we favor here semantic methodscomparison of all
the main formalisms in the literature.
1.2 Aim of the Paper
The aim of the paper is to provide a technical contribution towards a unification of the field
of logic and games. We set out to develop a series of embeddings which highlight a common
structure in the representation of choice and power, which underpins the semantics of all
the logics mentioned above.
We focus on the components of the semantics of those logics that have directly to do
with the representation of choice and power, and we abstract away from the representation
of time and repeated interaction. So the logics we will be working with are: the atemporal
fragment of STIT, logic CLPC (coalition logic of propositional control, van der Hoek &
Wooldridge, 2005) and the starless fragment of DLPA (dynamic logic of propositional
assignments, van Eijck, 2000; Balbiani, Herzig, & Troquard, 2013). These logics cover,
arguably, a large spectrum of the most influential existing formalisms.3 Logic STIT is often
considered a standard in the literature, as it embeds both CL and ATL (Broersen et al., 2005,
2. We refer the reader to Osborne and Rubinsteins textbook (1994) for an introduction to the basic notions
of game theory.
3. It is worth stressing that we focus here on logics of choice and power (that is, on the notion of effectivity) and not on formalisms incorporating also an explicit representation of how choice and power are
implemented (that is, an explicit notion of strategy), such as for instance ATL with explicit strategies
(Walther, van der Hoek, & Wooldridge, 2007).

92

fiThe Ceteris Paribus Structure of Logics of Game Forms

CLPC

DLPA

CP
one
nti
al

S5

bo

un
de

d

m

od

el
s

exp

STIT

restricted languages

NCL

Figure 1: Summary of the embeddings established in the paper and known from the literature
An arrow indicates that each formula of the source logic is satisfiable if and only
if a suitable translation of that formula is satisfiable in the target logic. DLPA
denotes the starless version of dynamic logic of propositional assignments, NCL
and STIT denote the atemporal version of, respectively, normal coalition logic
and the seeing-to-it logic. S5 denotes the normal modal logic of equivalence relations. Dotted lines indicate embeddings known in the literature: from CLPC to
DLPA (Balbiani et al., 2013) and from STIT to NCL (and vice versa) with respect to fragments of the respective languages (Lorini & Schwarzentruber, 2011).
The embedding from STIT to CP assumes a bound on the STIT-models. All
embeddings are polynomial except for the one from CP to S5.

2006), so we use it as a natural starting point. Logic CLPC is an influential extension of
CL and has strong formal ties (Dunne, van der Hoek, Kraus, & Wooldridge, 2008) with the,
equally influential, Boolean games model (Harrenstein, van der Hoek, Meyer, & Witteveen,
2001) in multi-agent systems. Finally, logic DLPA is an extension of PDL (propositional
dynamic logic, Harel, Kozen, & Tiuryn, 2000), which has recently been proposed as a new
standard for the representation of choice and power (Herzig, Lorini, Moisan, & Troquard,
2011; Balbiani et al., 2013).
To articulate our analysis, whose main technical tool consists of satisfiability-preserving
embeddings, the paper introduces and studiesin its axiomatization and complexitya
simple ceteris paribus logic based on propositional equivalence, which we call CP. Such
logic is the yardstick allowing us to compare and unify STIT, CLPC and DLPA.
1.3 Outline and Summary of Results
Section 2 introduces logic CP. The logic will be compared to S5 and axiomatized.
Section 3 provides a study of the relationship between the atemporal version of STIT and
CP. We show that CP embeds atemporal group STITthe fragment of atemporal STIT
in which actions of both individuals and groups are representedunder the assumption
that the agents choices are bounded. We call the latter atemporal bounded group STIT.
Moreover, we show that CP embeds atemporal individual STITthe variant of atemporal
93

fiGrossi, Lorini, & Schwarzentruber

STIT in which only the actions of individuals are represented. The former embedding is
used to transfer complexity results to CP. We also present an embedding in CP of a variant
of atemporal group STIT in which groups are nested (i.e., given two sets of agents J and J 0
either J  J 0 or viceversa).
Section 4 provides an embedding of the coalition logic of propositional control into atemporal
bounded group STITand therefore, indirectly, into CPas well as a direct embedding
of CLPC into CP.
Section 5 provides an embedding of the starless fragment of DLPA into CP as well as an
embedding of CP into DLPA and therefore, indirectly, of STIT (on bounded models) and
CLPC into DLPA.
Finally, in Section 6 we discuss the obtained results, put them in perspective with related
work and draw some general implications for the field. We conclude in Section 7. Longer
proofs are collected in a technical appendix at the end of the paper.
Figure 1 gives a graphical presentation of the embeddings established in the paper
as well as relevant ones already known in the literature. Two embeddings are known for
the above logics: the embedding of CLPC into DLPA (Balbiani et al., 2013), and the
embedding of STIT into NCL, and vice versa, when the language of STIT (and of NCL) are
restricted to a fragment which does not allow nesting modalities.4

2. A Ceteris Paribus Logic Based on Propositional Equivalence
In this section we introduce and study the logic that will be used as target logic in all the
embeddings we will present. The section starts with the definition of equivalence modulo
a set of atoms. Then we present our ceteric paribus logic CP whose semantics is based on
these equivalence relations. The section finishes with an exponentially embedding of ceteris
paribus logic CP into S5 proving that the CP-satisfiability problem is decidable.
2.1 Equivalence Modulo a Set of Atoms
Consider a structure (W, V ) where W is a set of states, and V : P  2W a valuation
function from a countable set of atomic propositions P to subsets of W .5
Definition 1. (Equivalence modulo X) Given a pair (W, V ), X  P and |X| < , the
relation VX  W 2 is defined as:

w VX w0  p  X : w  V (p)  w0  V (p)
When X is a singleton (e.g. p), we will often write Vp instead of V{p} . Also, in order to
avoid clutter, we will often drop the reference to V in VX when clear from the context.
Intuitively, two states w and w0 are equivalent up to set X, or X-equivalent, if and only
if they satisfy the same atoms in X (according to a given valuation V ). The finiteness of
4. The reader is referred to Lorini and Schwarzentrubers paper (2011) for the BNF of this language.
5. In the literature game logics are sometimes defined over a countable set of atoms (e.g., Balbiani et al.,
2013) and sometimes over a finite set of atoms (e.g., van der Hoek & Wooldridge, 2005). Here we opt for
generality and define the language of CP over a countable set. Under the assumption of a finite supply
of atoms some of the results we present later would trivialize (for instance the CP satisfiability problem
would be in PTIME) and would therefore hide some of the interesting technical features of CP.

94

fiThe Ceteris Paribus Structure of Logics of Game Forms

X is clearly not essential in the definition. It is assumed because, as we will see, each set
X will be taken to model a set of actions of some agent in a game form and sets of actions
are always assumed to be finite.
We state the following simple fact without proof. It highlights some interesting features
of the notion of propositional equivalence modulo subsets of P, some of which will be of use
later on in the paper.
Fact 1. (Properties of P ) The following holds for any set of states W , valuation V :
P  2W and finite sets X, Y  P:
(i) X is reflexive, transitive and symmetric;
(ii) if X  Y then Y  X ;
(iii) if X is a singleton, X induces a partition of W with at most 2 cells;
(iv) X  Y = XY ;
(v)  = W 2 .
Intuitively: (i) states that each X is an equivalence relation; (ii) states that the larger
the set of atoms, the more refined is the equivalence relation indexed by it; (iii) states that
if the set of atom is a singleton, then its equivalence relation would induce a partition of
one (if the proposition in the singleton is globally true or globally false in the model) or two
cells (otherwise); (iv) states that the relation indexed by the union of two sets of atoms is
the relation one obtains by intersecting the relations of the two sets; finally (v) states that
the relation of the empty set of atoms is the global relation.
2.2 A Modal Logic of the X Relation
In this section we consider a simple modal language interpreted on relations X and axiomatize its logic on the class of structures (W, V ). The key modal operator of the language
will be hXi, whose intuitive meaning is  is the case in some state which is X-equivalent to
the current one or, to stress a ceteris paribus reading,  is possible all things expressed in
X being equal. We call the resulting logic propositional ceteris paribus logic, CP in short.
2.2.1 Syntax of CP.
Let P be a countable set of atomic propositions. The language LCP (P) is defined by the
following BNF:
LCP (P) :  ::= p |  | (  ) | hXi
where p ranges over P and X is a finite subset of atomic propositions (X  P and X finite).
Note that as the set of finite subsets of atomic propositions is countable, the language
LCP (P) is also countable. The Boolean connectives >, , ,  and the dual operators [X]
are defined as usual. Although we have taken diamond operators as primitive, we will for
convenience also make use of box operators to state some results in later sections.
The set SF () of subformulas of a formula  is defined inductively as follows:
 SF (p) = {p};
95

fiGrossi, Lorini, & Schwarzentruber

 SF () = {}  SF ();
 SF (  ) = {  }  SF ()  SF ();
 SF (hXi) = {hXi}  SF ().
We say that a signature X appears in  if there exists a formula  such that hXi  SF ().
2.2.2 Semantics of CP
This is the class of models we will be working with:
Definition 2. (CP-models) Given a countable set P, a CP-model for LCP (P) is a tuple
M = (W, V ) where:
 W is a non-empty set of states;
 V : P  2W is a valuation function.
A CP-model is called universal if W = 2P and V is s.t. V (p) = {w | p  w}. It is called
non-redundant if P is the identity relation in W 2 .
Intuitively, a CP-model consists just of a state-space and a valuation function for a given
set of atoms. The satisfaction relation is defined as follows:
Definition 3. (Satisfaction for CP-models) Let M = (W, V ) be an CP-model for LCP (P),
w  W and ,   LCP (P):
M, w |=CP p  w  V (p);
M, w |=CP   M, w 6|=CP ;
M, w |=CP     M, w |=CP  and M, w |= ;
M, w |=CP hXi  w0  W : w VX w0 and M, w0 |=CP 
Formula  is CP-satisfiable, if and only if there exists a model M and a state w such that
M, w |=CP . Formula  is valid in M, noted M |=CP , if and only if for all w  W ,
M, w |=CP . Finally,  is CP-valid, noted |=CP , if and only if it is valid in all CP-models.
The logical consequence of formula  from a set of formulae, noted  |=CP , is defined as
usual.
So, modal operators are interpreted on the equivalence relations X induced by the
valuation of the model. It is worth observing that the logic of this class of models is not
closed under uniform substitution,6 that is, logic CP is not uniform.7 To witness that, notice
that formula [{p}]p  [{p}]p is valid, whereas [{p}]  [{p}] is not.
Let us give a simple illustration of the above semantics.
Example 1. Let us consider the following model M made up of 5 states w, x, u, y, z:
6. For the definition of uniform substitution the reader is referred to the textbook by Blackburn, de Rijke,
and Venema (2001, Def. 1.18).
7. The terminology comes from Goldblatts work (1992).

96

fiThe Ceteris Paribus Structure of Logics of Game Forms

w : p, q

x:p

u : p, q, r

z:q

y

For instance, we have M, w |=CP h{p, q}ir and M, z |=CP [{p}]r.
The following lemmas state simple facts concerning the relation of logic CP with logic
S5 and isolate an interesting class of CP-models.
Lemma 1. Let LCP (P) the set of formulae   LCP (P) containing only hi operators. The
set of formulae of LCP (P) which are CP-valid is the modal logic of Kripke frames (W, W 2 ),
i.e., logic S5.
Proof. It follows directly from Fact 1 item (v).
In other words, the hi operator of LCP is nothing but the global modality (Blackburn
et al., 2001, pp. 367370). The next lemma states that CP is actually the logic of the class
of relevant CP-models.
Lemma 2. Every satisfiable CP-formula is satisfiable on a non-redundant model.
Proof. Assume M, w |= . We show that MP , |w|P |=  of M where MP is the quotient
of M by the equivalence relation P (defined in the natural way) and |w|P is the set
of states which are P -equivalent to w. We proceed by induction on the structure of .
The propositional and Boolean cases are obvious. Let  = hXi with X  P. From the
assumption and the semantics of CP operators we have that there exists v such that w X v
and M, v |= . By construction we directly have that |w|P X |v|P . By IH MP , |v|P |= ,
and therefore MP , |w|P |= hXi.
2.2.3 Axiomatics of CP
We can obtain an axiom system for CP by a standard reduction technique exploiting Lemma
1. The axiom system is given in Figure 2. The first thing to notice is that the system consists
of the usual S5 axioms plus the Reduce axiom. Logic S5 is known to be sound and strongly
complete for the class of models where the accessibility relation is the total relation W 2
(Blackburn et al., 2001), and modality hi can therefore be axiomatized as the (dual of)
the global modality.
Having said this, soundness and strong completeness of the above system are easy to
establish.
Theorem 1. The axiom system given in Figure 2 is sound and strongly complete for the
class of CP-models.
Proof. Soundness It suffices to show that Reduce is CP-valid, which follows straightforwardly from Definition 1. Completeness To obtain completeness we proceed as customary
in DEL (van Ditmarsch, Kooi, & van der Hoek, 2007; Wang & Cao, 2013). We first fix a
97

fiGrossi, Lorini, & Schwarzentruber

(P)

all tautologies of propositional calculus

(K)

[](  )  ([]  [])

(T)

  hi

(4)

hihi  hi

(5)

hi  []hi





^
^
^
^
^

p  
p  [] 
p
[X] 
p

(Reduce)

Y X

(MP)
(N)

pY

pY

pX\Y

pX\Y

if `CP  and `CP    then `CP 
if `CP  then `CP []

Figure 2: Axiom schemas and rules of CP. X, Y range over finite elements of 2P , , 
over LCP (P), and p over P. As usual, `CP means that there exists a sequence of
formulae each of which are either an axiom or are obtained from previous formulae
through the application of an inference rule.

translation tr0 : LCP (P)  LCP (P) as follows:
tr0 (p) = p
tr0 () = tr0 ()
tr0 (  ) = tr0 ()  tr0 ()





^
^
^
^
^

tr0 ([X]) =
p
p  [] 
p
p  tr0 ()
Y X

pY

pY

pX\Y

pX\Y

We also write tr0 () for {tr0 () |   }. Notice that the translation removes occurrences
of hXi and [X] operators from formulae where X 6=  and it has the same structure of axiom
Reduce. Consider then the following rule of substitution of provable equivalents (REP):
(REP)

if `CP   0 then `CP   [/0 ]

where [/0 ] is the formula that results from  by replacing zero or more occurrences of
, in , by 0 . We have that rule REP is derivable in the axiom system of Figure 2 (). The
proof of this claim is provided in Appendix A. By using axiom Reduce and rule REP we
obtain by () that, for any   LCP (P), `CP   tr0 () (). We can then proceed as follows:
if  |=CP  then tr0 () |=CP tr0 () by (); by Lemma 1 and the strong completeness of
S5 we thus obtain tr0 () `S5 tr0 () and therefore tr0 () `CP tr0 (); finally by () again it
follows that  `CP , which proves strong completeness.
98

fiThe Ceteris Paribus Structure of Logics of Game Forms

The crux of the above reduction argument lies in the use of axiom Reduce. What the
axiom does is to enable the reduction of [X]-formulae by taking care of all the possible truth
V
V
value combinations of the atoms in X. If a given combination, e.g.,
p

p
,
pY
pX\Y
is true at a given state (for some Y ), then in all accessible states, if that combination is
true, then what occurs in the scope of [X] is also true.
We opted for the above axiomatization in virtue of its simplicity, but alternative systems
are of course possible. One in particular is worth mentioning. It first reduces hpi operators
by axiom:
hpi  ((p  hi(p  ))  (p  hi(p  )))

(1)

This states that hpi is equivalent to either the case in which the current state satisfies p
and there exists a (possibly different) p-state where  is true, or the case where p is true
and there exists a (possibly different) p-state where  is true (recall property (iii) in Fact
1). Given the above reduction, one can then use axioms to enforce the appropriate behavior
of X relations where X consists of more than one atom. To this aim, axioms can be used
that are known to be canonical for properties (ii) and (iv) of Fact 1, namely:
hX  Y i  hXi

(2)

hXii  hY ii  hX  Y ii

(3)

where i ranges over a set of nominals. A complete system could then be obtained by
axiomatizing the behavior of nominalsthrough axioms and rules used in hybrid logic
(Areces & Ten Cate, 2006). From that system, a named canonical model could be built
(i.e., a canonical model where all maximal consistent sets contain exactly one nominal)
where the axioms in Formulae 1-3 would enforce the desirable properties on the canonical
relations.
2.3 Exponentially Embedding CP into S5
The property expressed by axiom Reduce enables a truth-preserving translation of CP into
S5 via the translation tr0 provided in the proof of Theorem 1. This translation is, however,
such that the length of the translated formula grows exponentially by a tower of exponents
of height equal to the modal depth of the original formula.
In this section we propose a translation that is single exponential and preserves satisfiability. Take the standard modal language L (P) with one modal operator  defined on
the set of atoms P. S5-models are structures M = (W, V ) where W is a set of states, and
V : P  2W a valuation function. Given an S5-model M = (W, V ) and a state w  W ,
the truth conditions are defined as follows:
M, w |=S5   u  W : M, u |=S5 
S5-satisfiability is defined as usual. It is possible to define an exponential truth-preserving
reduction tr : LCP (P)  L (P) as follows:
^
tr(0 ) = p0 
(p  tr1 ())
SF (0 )

99

fiGrossi, Lorini, & Schwarzentruber

where p are fresh atomic proposition (note that p0 is p when  is the formula 0 itself,
which is also a subformula of 0 )8 ,  ranging over SF (0 ) and tr1 is defined as follows:
tr1 (p) = p

for p  P

tr1 () = tr1 ()
tr1 (  ) = tr1 ()  tr1 ()
tr1 ([]) = p



tr1 ([X]) =

^

^


Y X

pY

p

^





p   

^
pY

pX\Y

p

^



p  p 

pX\Y

Intuitively, the translation is designed to operate like axiom Reduce but avoiding exponential blow-up to pile up with the modal depth of the formula. The atomic propositions
p in tr1 ([X]) avoid the non-elementary size of tr(0 ). The definition of tr1 ([]) corresponds to the degenerated case of tr1 ([X]) where X = .The following theorem states the
satisfiability preservation. The proof is given in Appendix B.
Theorem 2. (tr preserves satisfiability) Let 0 be a CP-formula. The two following statements are equivalent: 0 is CP-satisfiable; tr(0 ) is S5-satisfiable.
As a consequence, we also obtain the following result.
Corollary 1. (Decidability) The satisfiability problem for CP is decidable and in NEXPTIME.
Proof. The satisfiability problem for S5 is decidable and in NP (Blackburn et al., 2001, Ch.
6). The result follows from Theorem 2 and a decision procedure may work as follows: in
order to check that  is satisfiable we compute the formula tr() and we apply a NP-decision
procedure to check whether tr() is S5-satisfiable or not.
Notice that if the cardinality of each X that appears in operators [X] of  is bounded
by a fixed integer, then the translation tr becomes polynomial in the size of . Thus,
as S5-satisfiability problem is NP-complete, the CP-satisfiability problem with a bounded
cardinality restrictions over set of atomic propositions in modal operators is in NP. As it is
trivially NP-hard, it is NP-complete.
In Section 3, we will embed the atemporal version of STIT (the logic of seeing to it that)
into CP thereby obtaining lower bounds results.

3. The Ceteris Paribus Structure of STIT Logic
In this section, we investigate the possibility of embedding the logic of agency STIT into
CP. STIT logic (the logic of seeing to it that, Belnap et al., 2001; Horty, 2001) is one of
the most prominent logical accounts of agency. It is the logic of constructions of the form
agent i (or group J) sees to it that . STIT has a non-standard modal semantics based on
the concepts of moment and history. However, as shown by Balbiani, Herzig, and Troquard
(2008b) and Herzig and Schwarzentruber (2008), the basic STIT language without temporal
operators can be simulated in a standard Kripke semantics.
8. Such a use of fresh atomic propositions to obtain more efficient satisfiability preserving translations is
based on the propositional logic technique known as Tseitin transformation (Tseitin, 1968).

100

fiThe Ceteris Paribus Structure of Logics of Game Forms

3.1 Atemporal Group STIT
First let us recall the syntax and the semantics of atemporal group STIT. The language of
this logic is built from a countable set of atomic propositions P and a finite set of agents
AGT = {1, . . . , n} and is defined by the following BNF:
LGSTIT (P, AGT ) :  ::= p |  | (  ) | [J : stit]
where p ranges over P and J ranges over 2AGT . The construction [J : stit] is read group
J sees to it that  is true regardless of what the other agents choose. We define the dual
def
operator hJ : stiti = [J : stit]. When J = , the construction [ : stit] is read 
is true regardless of what every agent chooses or simply  is necessarily true.
Definition 4 (STIT-Kripke model, Herzig & Schwarzentruber, 2008). A STIT-Kripke model
M = (W, {RJ }JAGT , V ) is a 3-tuple where:
 W is a non-empty set of worlds;
 for all J  AGT , RJ is an equivalence relation such that:
i) RJ  R ;
T
ii) RJ = jJ R{j} ;
iii) T
for all w  W and (w1 , . . . , wn )  W n , if u1  R{1} (w), . . . , un  R{n} (w) then
1jn R{j} (uj ) 6= ;
 V : P  2W is a valuation function for atomic propositions;
with RJ (w) = {u  W : (w, u)  RJ } for any J  2AGT .
The partition induced by the equivalence relation RJ is the set of possible choices of
the group J.9 Indeed, in STIT a choice of a group J at a given world w is identified with
the set of possible worlds RJ (w). We call RJ (w) the set of possible outcomes of group
Js choice at world w, in the sense that group Js current choice at w forces the possible
worlds to be in RJ (w). The set R (w) is simply the set of possible outcomes at w, or said
differently, the set of outcomes of the current game at w. According to Condition (i), the
set of possible outcomes of a group Js choice is a subset of the set of possible outcomes.
Condition (ii), called additivity, means that the choices of the agents in a group J is made
up of the choices of each individual agent and no more. Condition (iii) corresponds to the
property of independence of agents: whatever each agent decides to do, the set of outcomes
corresponding to the joint action of all agents is non-empty. More intuitively, this means
that agents can never be deprived of choices due to the choices made by other agents. In
Lorini and Schwarzentrubers work (2011) determinism for the group AGT was assumed.
That is to say that the set of outcomes corresponding to a joint action of all agents is a
singleton. Hortys group STIT logic (Horty, 2001) does not suppose this. Here we deal with
Hortys version of STIT. So a STIT model is a game form in which a joint action of all
agents might determine more than one outcome.
9. One can also see the partition induced by the equivalence relation Rj as the set of actions that agent j
can try, where the notion of trying corresponds to the notion of volition studied in philosophy of action
(e.g., OShaughnessy, 1974; McCann, 1974).

101

fiGrossi, Lorini, & Schwarzentruber

w

u

v

r

s

t

z

R{1}
R{2}

Figure 3: The STIT-model M
Example 2. The tuple M = (W, R , R{1} , R{2} , R{1,2} , V ) defined by:
 W = {w, u, v, r, s, t, z};
 R = W  W ;
 R{1} = {w, u, v}2  {r, s}2  {t, z}2 ;
 R{2} = {w, r, t}2  {u, v, s, z}2 ;
 R{1,2} = {(w, w), (r, r), (s, s), (t, t), (z, z), (u, u), (v, v), (u, v), (v, u)};
 for all p  P, V (p) = .
is a STIT-Kripke model. Figure 3 shows the model M. The equivalence classes induced by
the equivalence relation R{1} are represented by ellipses and correspond to the choices of
agent 1. The equivalence classes induced by the equivalence relation R{2} are represented
by rectangles and correspond to the choices of agent 2. The choice of group {1, 2} at a
given world is determined by the intersection of the choice of agent 1 and the choice of
agent 2 at this world. For example, the choice of agent 1 at world u is {w, u, v} whereas
the choice of agent 2 at world u is {u, v, s, z}. The choice of group {1, 2} at u is {u, v}.
Note that Condition (iii) of Definition 4 ensures that for any choice of agent 1 and for any
choice of agent 2 the intersection between these two choices is non-empty. That is, for any
equivalence class induced by the relation R{1} and for any equivalence class induced by the
relation R{2} , the intersection between these two equivalence classes is non-empty.
Given a STIT-Kripke model M = (W, {RJ }JAGT , V ) and a world w in M, the truth
conditions of STIT formulae are the following:
M, w |=STIT p  w  V (p);
M, w |=STIT   M, w 6|=STIT ;
M, w |=STIT     M, w |=STIT  and M, w |=STIT ;
M, w |=STIT [J : stit]  v  RJ (w) : M, v |=STIT 
where RJ (w) = {u  W | (w, u)  RJ }.
102

fiThe Ceteris Paribus Structure of Logics of Game Forms

3.2 Embedding Atemporal STIT into CP
We are not able to embed group STIT into CP because of many reasons. The first one is
that the group STIT satisfiability problem is undecidable if there are more than 3 agents
(Herzig & Schwarzentruber, 2008).10 The second one is that group STIT does not have
the finite model property. Indeed Herzig and Schwarzentruber (2008) provide a translation
from the product logic S5n to group STIT logic, and as S5n does not have the finite model
property (Gabbay, Kurucz, Wolter, & Zakharyaschev, 2003), so atemporal group STIT will
not have it. On the contrary CP inherits the finite model property from S5. Indeed, if a
formula  is CP-satisfiable, Theorem 2 says that tr() is S5-satisfiable. But as S5 has the
polynomial model property, there exists a polynomial-sized S5-model for tr() in the size
of tr(). In other words, there exists an exponential S5-model for tr() in the size of .
Theorem 2 ensures that there exists an exponential CP-model for  in the size of .
We will nevertheless embed a variant of group STIT under the assumption that every
agent has a finite and bounded number of actions in his repertoire. For every agent j, a
Rj -equivalence class Rj (u) corresponds to an action of agent j. We say that agent j has kj
actions in a STIT model if and only if there are exactly kj Rj -equivalence classes in M.
The game structure in STIT-models should be enforced in CP-models. That is why
we introduce special atomic propositions to encode the game structure. Without loss of
generality, we assume that the set P contains special atomic propositions rep j1 , rep j2 , . . . for
all agents j which are used to represent the actions of the agents. Let k be the maximal
number of actions: k = maxjAGT kj . For every agent, we represent its actions by numbers
` in {0, . . . , k  1} and some atomic propositions encode the binary representation of `.
Let m be an integer that represents the number of digits we need to represent an action.
For instance let m = dlog2 ke (the ceiling of the logarithm of k). For a given agent j,
Rjm = {rep j1 , . . . , rep jm } is the set atomic propositions that represent the binary digits of an
action of agent j. We suppose that if j 6= i then Rjm  Rim = .
Example 3. For example, in the model of Example 2, agent 1 has k1 = 3 actions and agent
2 has k2 = 2 actions. So k = 3 and m = dlog2 3e = 2. We have R1m = {rep 11 , rep 12 } and
R2m = {rep 21 , rep 22 }. Then for instance, we may represent the action of agent 1 corresponding
to R{1} (w) = {w, u, v} by the valuation rep 11  rep 12 , the action of agent 1 corresponding
to {r, s} by rep 11  rep 12 , the action of agent 1 corresponding to {t, z} by rep 11  rep 12 ,
the action of agent 2 corresponding to {w, r, t} by rep 21  rep 22 and the action of agent 2
corresponding to {u, v, s, z} by rep 21  rep 22 .
S
Let Rm = jAGT Rjm be the set of all atomic propositions used to denote actions. Let
us define the following CP formula:
def

GRIDm =

V

xRm []((x

 hRm \ {x}ix)  (x  hRm \ {x}ix))

(4)

This formula enforces a CP model to be universal (over Rm ), that is, to contain all possible
valuations over Rm (recall Definition 2). A model that satisfies GRIDm is then interpreted
as a game form where each valuation of Rjm represents an action of player j.
10. See Lorini and Schwarzentrubers paper (2011) for a study of some decidable fragments of group STIT.

103

fiGrossi, Lorini, & Schwarzentruber

Example 4. For instance, if some world of a CP model M0 satisfies GRID2 , then the
skeleton of M0 should have the following form (we intentionally draw the skeleton of the
model M0 so that it looks like the model M):






|

{z

}

worlds where
rep 21 and rep 22
are true

|

{z

worlds where
rep 11 and rep 12 are true
worlds where
rep 11 and rep 12 are true
worlds where
rep 11 and rep 12 are true

}

worlds where
rep 21 and rep 22
are true

R{1}
R{2}

We now define a translation from LGSTIT to LCP (P) as follows:
tr2 (p) = p

for p  P

tr2 () = tr2 ()
tr2 (  ) = tr2 ()  tr2 ()
[
tr2 ([J : stit]) = [
Rjm ]tr2 ()
jJ

The translation tr2 should be parameterized by m. For notational convenience, in what
follows we write tr2 instead of tr2m leaving implicit the parameter m.
S
The set jJ Rjm represents all the atomic propositions used to represented actions of
the coalition J.
Example 5. For instance, with m = 2,
tr2 ([{1} : stit][{1, 2} : stit]p) = [{rep 11 , rep 12 }][{rep 11 , rep 12 , rep 21 , rep 22 }]p.
We then obtained the desired satisfiability-preservation result. The proof is given in
Appendix C.
Theorem 3. Let us consider a group STIT formula . Let m be an integer. Then the
following items are equivalent:
1.  is STIT-satisfiable in a STIT-model where each agent has at most 2m actions;
2.  is STIT-satisfiable in a STIT-model where each agent has exactly 2m actions;
3. GRIDm  tr2 () is CP-satisfiable.
104

fiThe Ceteris Paribus Structure of Logics of Game Forms

3.3 Atemporal Individual STIT
In this subsection, we consider the following fragment of STIT called atemporal individual
STIT 11 :
LISTIT (P, AGT ) :  ::= p |  | (  ) | [{j} : stit]
where p ranges over P and j ranges over AGT .
This fragment of STIT, axiomatized by Xu (1998), has the exponential finite model
property (see Lemma 7 in Balbiani et al., 2008b). Moreover, as the following theorem
highlights, it can be embedded in the logic CP.
Theorem 4. Let us consider a STIT formula  of the individual STIT fragment. Let m be
the length of . Then the following three items are equivalent:
1.  is STIT-satisfiable
2.  is STIT-satisfiable in a model where each agent has at most 2m actions;
3. GRIDm  tr2 () is CP-satisfiable.
Proof. 1  2 Consider a STIT formula  of the individual STIT fragment. If  is STITsatisfiable and m is the length of , then  is STIT-satisfiable in a model where there are
at most 2m worlds (see Lemma 7 in Balbiani et al., 2008b). This implies that there are at
most 2m actions in that model. The implications 2  3 and 3  1 come from Theorem
3.
Thanks to Theorem 4, we reduce the NEXPTIME-complete satisfiability problem of
individual STIT (Balbiani et al., 2008b) to the CP-satisfiability problem. As the reduction
is polynomial, we obtain the following lower bound complexity result for the CP-satisfiability
problem.
Corollary 2. The CP-satisfiability problem is NEXPTIME-hard.
3.4 Group STIT where Coalitions Are Nested
In this subsection we address the satisfiability problem of the fragment of CP consisting of
formulae  of LCP such that the sets of atomic propositions that appear in any operator [X]
occurring in  form a linear set of sets of atomic propositions. More formally, if [X] and [X 0 ]
are two operators occurring in  then either X  X 0 or X 0  X. For instance, the formula
[{p, q}](  [{p}][{p, q, r, s}]) belongs to the fragment because {p}  {p, q}  {p, q, r, s}.
On the contrary, the formula [{p}]p  [{q}]p is not an element of this fragment of CP.
We call the satisfiability problem of this fragment of CP the CP-nested satisfiability
problem. Due to the embedding proposed in Theorem 3 of STIT into CP, we provide the
following lower bound complexity result for the CP-nested satisfiability problem. The proof
is given in Appendix D.
Theorem 5. The CP-nested satisfiability problem is PSPACE-hard.
11. Some authors (e.g., Broersen, 2008; Wansing, 2006) use the term multi-agent STIT to designate the
logic where operators are of the form [{j} : stit]. Here we prefer to use the more explicit term individual
STIT as in Herzig and Schwarzentrubers work (2008).

105

fiGrossi, Lorini, & Schwarzentruber

The following theorem provides an upper bound complexity result for this fragment of
CP. The proof is given in Appendix E.
Theorem 6. The CP-nested satisfiability problem is in PSPACE.
This concludes our analysis of STIT logics via CP. In the next section we move to normal
coalition logic.
3.5 Normal Coalition Logic
We conclude this section on STIT by briefly mentioning a related system, normal coalition
logic. Normal coalition logic NCL was introduced by Broersen et al. (2007) to provide
an embedding in a normal modal logic of the influentialand non-normalcoalition logic
(Pauly, 2002). The embedding was based on a general simulation technique developed
by Gasquet and Herzig (1994) and showed for the first time how coalition logicwhich
had already been recognized as the fragment of ATL containing only the next operator
(Goranko, 2001)could actually be interpreted on very traditional structures such as Kripke
frames based on equivalence relations. NCL was further studied by Balbiani et al. (2008a).
Also NCL has a known atemporal variant, introduced and studied by Balbiani et al. (2008a)
and Lorini and Schwarzentruber (2011).
Two results on atemporal NCL from the literature are worth mentioning in this context.
First, Balbiani et al. (2008a, Thm. 38) show that the satisfiability problem for atemporal
NCL (when |AGT |  2) is NEXPTIME-complete, like CP; second, Lorini and Schwarzentruber (2011, Prop. 1) show that when the |AGT |  2, then atemporal STIT is embeddable
in atemporal NCL and vice versa, and that an embedding (in both directions) in the general
case is possible only by considerably restricting the syntax of LSTIT .

4. The Ceteris Paribus Structure of Coalition Logic of Propositional
Control
In this section we study the relationships between CP, atemporal bounded group STIT,
and another well-known game logic, the logic CLPC (coalition logic of propositional control ).12 Specifically, we show that CLPC can be embedded, preserving satisfiability, into
atemporal bounded group STIT and, by the fact that atemporal bounded group STIT
can be embedded into CP (Section 3.1), we indirectly show that CLPC can be embedded
into CP. To complete the picture we also provide a direct embedding from CLPC to CP.
This latter embedding is of particular interest to highlight the striking similarities between
the models of CP and of CLPC.
CLPC was introduced by van der Hoek and Wooldridge (2005) as a formal language
for reasoning about capabilities of agents and coalitions in multiagent environments. In this
logic the notion of capability is modeled by means of the concept of control. In particular, it
is assumed that each agent i is associated with a specific finite subset Pi of the finite set of
all propositions P. Pi is the set of propositions controlled by the agent i. That is, the agent
i has the ability to assign a (truth) value to each proposition Pi but cannot affect the truth
12. In Gerbrandys work (2006) generalizations of some of the assumptions underlying CLPC have been
studied. Here we only consider the original version of CLPC proposed by van der Hoek and Wooldridge.

106

fiThe Ceteris Paribus Structure of Logics of Game Forms

values of the propositions in P \ Pi . In the variant of CLPC studied by van der Hoek and
Wooldridge (2005) it is also assumed that control over propositions is exclusive, that is, two
agents cannot control the same proposition (i.e., if i 6= j then Pi  Pj = ). Moreover, it is
assumed that control over propositions is complete, that is, every proposition is controlled
by at least one agent (i.e., for every p  P there exists an agent i such that p  Pi ).
The preceding concepts and assumptions are precisely formulated in the following section, which illustrates the syntax and the formal semantics of CLPC.
4.1 Syntax and Semantics of CLPC
The language of CLPC is built from a finite set of atomic propositions P and a finite set
of agents AGT = {1, . . . , n}, and is defined by the following BNF:
LCLPC (P, AGT ) :  ::= p |  | (  ) | J 
where p ranges over P and J ranges over 2AGT . Operator J is called cooperation modality,
and the construction J  means that group J has the contigent ability to achieve .
Definition 5 (CLPC model). A model for CLPC is a tuple M = (P1 , . . . , Pn , X) where:
 P1 , . . . , Pn is a partition of P among the agents in AGT ;
 X  P is the set of propositions which are true in the initial state.
S
For every group of agents J  AGT , let PJ = iJ Pi be the set of atomic propositions
controlled by the group J. Moreover, for every group J  AGT and for every set of atomic
propositions X  P, let XJ = X  PJ be the set of atomic propositions in X controlled by
the group J. Sets XJ are called J-valuations.
Given a CLPC model M = (P1 , . . . , Pn , X), the truth conditions of CLPC formulae
are the following:
M |=CLPC p  p  X;
M |=CLPC   M 6|=CLPC ;
M |=CLPC     M |=CLPC  and M |=CLPC ;
M
M |=CLPC J   XJ0  PJ : M
XJ0 |=CLPC 
where M

L

XJ0 is the CLPC model (P1 , . . . , Pn , X 00 ) such that:
00
XAGT
\J = XAGT \J

XJ00 = XJ0
That is, J  is true at a given model M if and only if, the coalition J can change the truth
values of the atoms that it controls in such a way that  will be true afterwards (i.e., given
the actual truth-value combination of the atoms which are not controlled by J, there exists
a truth-value combination of the atoms controlled by J which ensures ).
Let us illustrate the CLPC semantics with an example.
107

fiGrossi, Lorini, & Schwarzentruber

Example 6. Let AGT = {1, 2, 3}, P = {p, q, r}, P1 = {p}, P2 = {q} and P3 = {r}.
Consider the CLPC model M = (P1 , P2 , , P3 , {r}). We have that:
M |=CLPC {1,2} ((p  q  r)  (p  q  r)).
0
Indeed, there exists a set of atoms X{1,2}
 P{1,2} controlled by {1, 2} such that
L 0
0
M X{1,2} |=CLPC ((p  q  r)  (p  q  r)). An example is X{1,2}
= {p}  P{1,2} , from
which,
(P
,
P
,
P
,
{p,
r})
|=
((p

q

r)

(p

q

r))
where
(P
1
2
3
1 , P2 , P3 , {p, r}) =
CLPC
L
M {p}.

4.2 Embedding CLPC into STIT
The aim of this section is to provide an embedding of CLPC into the variant of atemporal group STIT with bounded choices (atemporal bounded group STIT) that have been
presented in Section 3.1.
Let us provide the following STIT formulae which catpure four basic assumptions of
CLPC:
^
^
def
EXC + =
(h : stiti[{i} : stit]p  h : stiti[{j} : stit]p)
(5)
pP i,jAGT :i6=j

EXC

 def

=

^

^

(h : stiti[{i} : stit]p  h : stiti[{j} : stit]p)

(6)

pP i,jAGT :i6=j
def

COMPL =

^

_

[ : stit]([{i} : stit]p  [{i} : stit]p)

(7)

pP iAGT


def

GRID  =

^

h : stiti 

XP


^

p

pX

^

p

(8)

pP\X

Formulae EXC + and EXC  mean that control over atomic propositions in P is exclusive
(i.e., there is no proposition in P which can be forced to be true or false by more than one
agent), whereas formula COMPL means that exercise of control over atomic propositions in
P is complete (i.e., for every proposition in P there exists at least one agent who either forces
it to be true or forces it to be false). Finally, formula GRID  means that all the possible
truth-value combinations of the atomic propositions in P are possible. Note that EXC + ,
EXC  , COMPL and GRID  are well-formed STIT formulae because of the assumption that
the set P is finite.13
We define the following translation from LCLPC (P, AGT ) to LSTIT (P, AGT ):
tr3 (p) = p

for p  P

tr3 () = tr3 ()
tr3 (  ) = tr3 ()  tr3 ()
tr3 (J ) = hAGT \ J : stititr3 ()
The following theorem highlights that bounded group STIT embeds CLPC. The proof
is given in Appendix F.
13. This assumption is also made by van der Hoek and Wooldridge (2005).

108

fiThe Ceteris Paribus Structure of Logics of Game Forms

Theorem 7. Let m = |P|. Then, a CLPC formula  is CLPC-satisfiable if and only if
(EXC +  EXC   COMPL  GRID  )  tr3 () is satisfiable in a STIT model where each
agent has at most 2m actions.
As CP embeds atemporal bounded group STIT (Theorem 3 in Section 3.1), from Theorem 7 it follows that CP also embeds CLPC. Indeed, given a CLPC-satisfiable formula
, one can use the translation tr2 given in Section 3.1 in order to find a corresponding STIT
formula which is STIT-satisfiable. Then, one uses the preceding translation tr3 in order to
find a corresponding CP formula which is CP-satisfiable.
Corollary 3. Let m = |P|. Then, a CLPC formula  is CLPC-satisfiable if and only if
GRID m  tr2 ((EXC +  EXC   COMPL  GRID  )  tr3 ()) is CP-satisfiable.
4.3 Directly Embedding CLPC into CP
To complete the picture, we study here a direct embedding of CLPC into CP.
Definition 6 (From CLPC to CP models). Let M = (P1 , . . . , Pn , X) be a CLPC-model.
Define MCP = (W, V ) as follows:
 W = 2P ;
 V is such that V (p) = {w | p  w} for all p  P.
Intuitively, V is such that any truth-assignment on P is witnessed by exactly one
w  W and wX is the witness of the truth assignment represented by X (i.e., makes
all atoms in X true and the rest false). So MCP is a non-redundant universal CP model and
MCP , X is a pointed CP-model (Definition 2). Now define the following translation from
LCLPC (P, AGT ), and a partition P1 , . . . , Pn of P, to LCP (P):
tr4 (p) = p

for p  P

tr4 () = tr4 ()
tr4 (  ) = tr4 ()  tr4 ()
tr4 (J ) = hYJ itr4 ()
where YJ = 

S

jAGT

Pj (i.e., the atoms that are not controlled by anybody in J).

Theorem 8. Let M = (P1 , . . . , Pn , X) be a CLPC-model and   LCLPC (P, AGT ):
M |=CLPC   MCP , X |=CP tr4 ()
Proof. We proceed by induction on the syntax of . Base Trivial by the construction of
MCP (Definition 6). Step The cases for the Boolean connectives are straightforward. We
focus on the modal case:
M |=CLPC J   MCP , X |=CP hYJ itr4 ()
109

fiGrossi, Lorini, & Schwarzentruber

where YJ = 

S

jN

Pj . The case is proven by the following series of equivalences:

M |=CLPC J   XJ0  PJ : M

L

XJ0 |=CLPC 

Semantics of J
 XJ0  PJ : (P1 , . . . , Pn , XJ0  XAGT \J ) |=CLPC 
L
Definition of
 XJ0  PJ : MCP , XJ0  XAGT \J |=CP tr4 ()
Definition 6 and IH
 Y YJ X and MCP , Y |=CP tr4 ()
Definition 1
 MCP , X |=CP hYJ itr4 ()
Definition 3

This completes the proof.

5. The Ceteris Paribus Structure of Dynamic Logic of Propositional
Assignments
The dynamic logic of propositional assignments (DLPA) is the concrete variant of propositional dynamic logic (PDL) (Harel et al., 2000) in which atomic programs are assignments of
propositional variables to true or to false.14 The complexities of the model checking and of
the satisfiability problem for DLPA have been recently studied by Balbiani et al. (2013).
The starless version of DLPA was previously studied by van Eijck (2000) and recently
put to use by Herzig et al. (2011), who have shown that it embeds CLPC. In the next
section we study the relationship between CP and DLPA. Specifically, we provide a truthpreserving embedding of starless DLPA into CP as well as a truth-preserving embedding
of CP into DLPA.
DLPA, it has been argued by Herzig et al. (2011), represents a very general and
because of its direct link with PDLnatural formalism for reasoning about agency. The
results in this section, we argue, point to a similar status for CLPC, modulo the use of the
Kleene star about which we will comment in Section 7.

14. Programs in standard PDL are abstract as they are just letters a, b, . . . from some alphabet.

110

fiThe Ceteris Paribus Structure of Logics of Game Forms

5.1 Syntax and Semantics of DLPA
The language of DLPA is built from a finite set of atomic propositions P and is defined
by the following BNF:
 ::= +p | p | ;  |    |   | ?
LDLPA (P) :  ::= p |  | (  ) | hi
We will use p to denote (+p  p).
Definition 7 (DLPA model). A DLPA-model is a set X  P.
That is, a DLPA model is a propositional valuation.
The semantics is given by induction as follows:
 J+pK = {(X, X 0 ) | X 0 = X  {p}};

 JpK = {(X, X 0 ) | X 0 = X  {p}};

 J;  0 K = JK  J 0 K;

 J   0 K = JK  J 0 K;
S
 J  K = kN JKk ;
 J?K = {(X, X) | X  JK};
 JpK = {X | p  X};
 JK = 2P  JK;
 J  K = JK  JK;

 JhiK = {X | there exists X 0 s.th. (X, X 0 )  JK and X 0  JK}.

We write X |=DLPA  for X  JK. We will refer to the fragment of DLPA without 
operator as starless DLPA.
5.2 Some Properties of DLPA
Like in PDL, program constructors ; ,  and ? are eliminable:
Fact 2. The following are DLPA validities:
h;  0 i  hih 0 i
h   0 i  hi  h 0 i
h?i    
However, unlike in PDL, the  operator is also eliminable in DLPA:
Fact 3 (Balbiani et al., 2013). For every   LCLPC (P) there exists 0  LCLPC (P) such
that   0 is DLPA valid.
111

fiGrossi, Lorini, & Schwarzentruber

5.3 Embedding Starless DLPA into CP
By a direct adaptation of Definition 6 above, a DLPA model X can be translated to the
pointed CP model MCP , X defined in Definition 6. Now fix the following translation:15
tr5 (p) = p

for p  P

tr5 () = tr5 ()
tr5 (  ) = tr5 ()  tr5 ()
tr5 (h+pi) = hP \ {p}i(p  tr5 ())
tr5 (hpi) = hP \ {p}i(p  tr5 ())
Notice that the translation from starless DLPA to CP does not need to include the cases
for sequential composition (;), nondeterministic choice () and test (?) since they are
eliminable in DLPA. Therefore, it guarantees that in CP we could do the same kind of
reasoning as in starless DLPA:
Theorem 9. Let X be a DLPA model and  belong to the language of starless DLPA:
X |=DLPA   MCP , X |=CP tr5 ()
Proof. We proceed by induction on the syntax of . Base Trivial by the construction of
MCP (Definition 6). Step The cases for the Boolean connectives are straightforward. We
focus on the modal case:
X |=DLPA h+pi  MCP , X |=CP hP \ {p}itr5 ()
The case is proven by the following series of equivalences:
X |=DLPA h+pi  X  {p} |=DLPA 

Semantics of h+pi

 Y P{p} X : MCP , Y |=CP tr5 () Definition 1 and IH
 MCP , X |=CP hP \ {p}itr5 ()

Definition 3

The case for hpi is identical.
5.4 Embedding of CP into Starless DLPA
The above subsection has shown that the semantics of CP and DLPA are closely related.
However, DLPA has a built-in assumption to the effect that any valuation (i.e., set of
atoms) is feasible. From the point of view of CP this means that DLPA actually works with
universal models (cf. Definition 2). Here, we establish an embedding from CP interpreted
on universal models,16 to starless DLPA. Consider the following translation from LCP (P)
15. It must be observed that for this translation to work P should be finite. If not, then {p} is co-finite
and {p} would not belong to LCP .
16. This is the class of models one can axiomatize by extending the axiom system of Figure 2 with axioms
of form hi where  ranges over propositional formulae encoding one single valuation.

112

fiThe Ceteris Paribus Structure of Logics of Game Forms

into LDLPA (P):
tr7 (p) = p

for p  P

tr7 () = tr7 ()
tr7 (  ) = tr7 ()  tr7 ()
tr7 (hXi) = hp1 i . . . hpn itr7 ()
where p1 , . . . , pn is an enumeration of the atoms in P \ X.17 We have the following result:
Theorem 10. Let M be an CP-model and   LCP (P):
M, w |=CP   w |=DLPA tr7 ()
Proof. We proceed by induction on the syntax of . Base Trivial by the construction of
MCP (Definition 6). Step The cases for the Boolean connectives are straightforward. We
focus on the modal case:
M, w |=CP hXi  w |=DLPA hp1 i . . . hpn itr7 ()
where p1 , . . . , pn is an enumeration of the atoms in P\X. The case is proven by the following
series of equivalences:
M, w |=CP hXi  w0 X w s.t. M, w0 |=CP 

Definition 1

 w0 X w s.t. w0 |=DLPA tr7 ()

IH

 w0 s.t. w0 = (. . . (wF{p1 })F . . .)F{pn }
and w0 |=DLPA tr7 ()
 w |=DLPA hp1 i . . . hpn itr7 ()

Definition 1
Semantics of DLPA

where F  {, }, and p1 , . . . , pn is an enumeration of the atoms in P  X.
From Theorem 10 we can obtain as corollary a satisfiability-preserving embedding of CP
in DLPA. Fix the formula18


^
^
^
def
GRID =
hi 
p
p
(9)
XP

pX

pP\X

which, it is easy to see, forces a CP-model to contain all propositional valuations from P.
We then have:
Corollary 4. Let   LCP (P). Then, tr7 () is DLPA satisfiable iff GRID   is CP
satisfiable.
This concludes the presentation of the embeddings of STIT, CLPC and DLPA into
CP (Figure 1). In the following section we take stock commenting on the technical results
presented and drawing links with related work.
17. Again, it is crucial that P be finite.
18. Cf. Formula (8).

113

fiGrossi, Lorini, & Schwarzentruber

6. Discussion and Related Work
In this section we provide a summary of the results presented and discuss their implications.
We also position our work with respect to existing contributions in the literature on logic
and games.
6.1 Discussion
The paper has introduced a modal logic that arises naturally by interpreting modal operators on the equivalence relations induced by finite sets of propositional atoms. This logic,
called CP, has been axiomatized and embedded (exponentially) into S5. CP has then been
used as a tool to compare three logics of one-shot strategic interactionatemporal STIT,
the coalitional logic of propositional control CLPC and the dynamic logic of propositional
assignments DLPA. All these logics have been embedded into CP.
These embeddings (recall Figure 1) put us in the position to to draw the following
general remarks.
 It appears to be justified to talk about a common ceteris paribus structure underpinning several of the main logics of game forms as they are all embeddable into CP.
This illustrates a striking uniformity in the logical tools needed for expressing choice
and effectivity of games in logical languages, and CP appears to offer a well-suited
abstraction for systematizing existing formalisms.
 Furthermore, all these logics are embeddable in S5 (either directly or via CP), highlighting the fact that in order to reason about choice and effectivity in games one
essentially reasons over suitably defined partitions of the state space.
 New interesting and so far unexplored embeddings are obtainable as corollaries. In
particular, it follows from our results that atemporal STIT on bounded models can be
embedded into starless DLPA via CP.
 Via logic S5, one can easily show that embeddings in the other directions are also
possible (albeit at exponential cost), so that all arrows in Figure 1 may actually
be made symmetric. S5 embeds CP, but it is also directly embeddable in all the
mentioned logic, as they all contain the universal modality, in the following forms: hi
in CP, hAGT \  : stiti in atemporal STIT, AGT in CLPC and hp1 i . . . hpn i in
DLPA (where p1 , . . . , pn is an enumeration of P).
All in all our results unveil a strikingand to some extent unexpecteduniformity
underpinning all the formalisms we considered.
6.2 Related Work
We review two sets of related contributions.
6.2.1 CP and Modal Ceteris Paribus Logics
There are two logics in the modal logic literature that are strictly related to CP: release
logic, and the logic of ceteris paribus preference.
114

fiThe Ceteris Paribus Structure of Logics of Game Forms

Release logic is a relatively less known formalism in the landscape of modal logics for
artificial intelligence. It has been introduced and studied by Krabbendam and Meyer (2003,
2000) in order to provide a modal logic characterization of a general notion of irrelevancy.
Modal operators in release logic are S5 operators indexed by subsets of a finite set Iss of
abstract elements denoting the issues that are taken to be irrelevant, or that can be released,
while evaluating the formula in the scope of the operator. A release model is therefore a tuple
(W, {rX }XIss , V ) where all rX are equivalence relations with the additional constraint
that if X  Y then rX rY , that is, by releasing more issues one obtains a more coarse
equivalence relation. Formally, this is the semantics of release operators:
M, w |= X   w0  W : w rX w0 and M, w0 |= 
where X  Iss and M = (W, {rX }XIss , V ).
One can easily observe that, by Fact 1 (clause (ii)), CP models are release models where
Iss = P and where the release relation rX =X . Vice versa, for Iss = P, not all release
models are CP models. As a consequence, the logic of hXi operators in CP is a conservative
extension of the logic of X release operators.
Preference logic has also long been concerned with so-called ceteris paribus preferences,
that is, preferences incorporating an all other things being equal condition. A first logical
analysis of such preferences dates back to Von Wrights work (1963), where dyadic modal
operators are studied representing statements like  is preferred to , ceteris paribus. More
recently, van Benthem et al. (2009) studied a modal logic of ceteris paribus preferences based
on standard unary modal operators. Leaving the preferential component of such logic aside,
its ceteris paribus fragment concerns sentences of the form hi whose intuitive meaning
is there exists a state which is equivalent to the current (evaluation) state with respect to
all the formulae in the (finite) set  and which satisfies , where the formulae in  are not
only atoms but formulae from the full language. Logic CP is, therefore, a fragment of the
ceteris paribus logic studied by van Benthem et al. where  is allowed to consist only of a
finite set of atoms.
6.2.2 Other Contributions to a Systematization of Game Logics
Despite the wealth of approaches that can be found in the literature on game logics, only very
few papers have attempted some form of comparison spanning across several formalisms,
and attempting some kind of systematization. Two in particular are worth mentioning here.
The most recent one is Herzigs work (2014), which provides a very comprehensive
analysis of the field by classifying the existing logics depending on what aspects of agency
(e.g., whether they capture strategic interaction or not, whether they handle uncertainty
and epistemic attitudes) they capture in their languages. The logics we considered in this
paper, for instance, would fall into the strategic and no uncertainty categories according to
the terminology used by Herzig. This analysis is conceptual and predominantly driven by
syntactic features of the logics, that is, by the theorems about agency that they enable.
An earlier work which is methodologically closer to ours in its focus on semantics is
Goranko and Jamrogas work (2004). That paper compares ATL, its epistemic variant ATEL
(epistemic ATL, van der Hoek & Wooldridge, 2003) and ECL (extended CL,19 Pauly, 2001)
19. This is CL extended with a Kleene star operator.

115

fiGrossi, Lorini, & Schwarzentruber

providing constructive transformations between their models and establishing, in particular,
that ATL subsumes ECL and that ATEL can be embedded into ATL preserving satisfiability.

7. Conclusions and Future Work
The paper has provided a unification of the, to date, most influential logics for the representation of one-shot strategic interactionatemporal STIT, CLPC and starless DLPA
under the ceteris paribus abstraction formalized in logic CP.
One natural future research direction presents itself, which consists in extending logic
CP with a Kleene star operator, in analogy with DLPA. We conjecture DLPA to be
embeddable into CP with Kleene star and it remains to be investigated whether such new
logic could play the same unifying role for logics of extensive form games, that we show it
plays in the atemporal case. This will complete the systematization program initiated by
the current paper.
Related to the above question, but in a somewhat more technical vein, we have shown
in this paper that CP and atemporal individual STIT have the same high complexity of
the satisfiability problem when we consider the whole languages. The study of efficient
syntactic fragments is then important and we intend to pursue this study in parallel both
for CP and for atemporal individual STIT. We expect that several complexity results about
fragments of atemporal STIT may be transferred to fragments of CP and viceversa.

Acknowledgments
The authors wish to thank the anonymous reviewers of JAIR for their thorough and helpful
comments. The paper was greatly improved thanks to their feedback.

Appendix A. Proof of Claim () in Theorem 1
Proof. One can show that REP is derivable for every operator [X] as follows: first one shows
that each [X] operator satisfies the Axiom K and the rule of necessitation N.
We provide here below the syntactic proofs of these two claims. For notational convenience we use the following abbreviation:


def
Yb = 


^

p

pY

116

^
pX\Y

p

fiThe Ceteris Paribus Structure of Logics of Game Forms

1.

Derivation of K for [X]:


^ 
`CP [X](  ) 
Yb  [] Yb  (  )
Y X

2.

by Reduce




`CP Yb  (  )  (Yb  )  (Yb  )
by P

3.

^ 

`CP





^ 
Yb  [] Yb  (  ) 
Yb  [] (Yb  )  (Yb  )

Y X

Y X

4.

by P, 2 and rule RM for [] (i.e., if `    then ` []  [])




^ 
^ 
`CP
Yb  [] (Yb  )  (Yb  ) 
Yb  [](Yb  )  [](Yb  )

5.

by K and P





^ 
^ 
Yb  [](Yb  )  [](Yb  )  
Yb  [] Yb  
`CP

Y X

Y X

Y X

Y X




^ 



Yb  [] Yb  




Y X

by P
6.

^ 

`CP (





^ 
Yb  [] Yb   
Yb  [] Yb   )  ([X]  [X])

Y X

Y X

by Reduce
7.

`CP [X](  )  ([X]  [X])
from 1 and 3-6
Derivation of N for [X]:

1.

`CP 
hypothesis

2.

`CP []

3.

from 1 by N for []


^
`CP
[] Yb  
Y X

4.

from 2 by the S5 theorem []  [](  )


^ 
`CP
Yb  [] Yb  
Y X

from 3 by P
5.

`CP [X]
from 4 by Reduce and MP

117

fiGrossi, Lorini, & Schwarzentruber

Then one proves that REP is derivable by an induction routine analogous to the one used
by Chellas (1980, Thm. 4.7).

Appendix B. Proof of Theorem 2
Let 0 be a CP-formula. We have equivalence between 0 is CP-satisfiable and tr(0 ) is
S5-satisfiable.
Proof.  Suppose that there exists a CP-model M = (W, V ) and a world w  W such
that M, w |=CP 0 . Let V 0 be the valuation V modified such that p is true in exactly all
worlds u such that M, u |=CP . Let M0 be the S5-model defined as (W, V 0 ). A standard
induction provides that M0 , w |=S5 tr(0 ). More precisely, let us prove by induction that
for all   SF (0 ), we have M, u |=CP  iff M0 , u |=S5 tr1 () for all u  W .
 Propositional case: for all atomic propositions p, we have M, u |=CP p iff u  V (p) iff
u  V 0 (p) iff M0 , u |=S5 tr1 (p).
 Negation: M, u |=CP  iff M, u 6|=CP  iff M0 , u 6|=S5 tr1 () iff M0 , u |=S5 tr1 ().
 Conjunction: M, u |=CP    iff M, u |=CP  and M, u |=CP  iff M0 , u |=S5 tr1 ()
and M0 , u |=S5 tr1 () iff M0 , u |=S5 tr1 (  ).
 Case of a formula of the form [X]:
M, u |=CP [X]
iff for all v  W , u VX v implies M, v |=CP 
iff for all v  W , u VX v implies M0 , v |=S5 p
(by construction of V 0 )
iff M0 , u |=S5 tr1 ([X])
V
By construction of V 0 , we have M0 , w |=S5 SF (0 ) (p  tr1 ()). As M, w |=CP 0 we
have M0 , w |=S5 tr1 (0 ) thus M0 , w |=S5 p0 by construction of V 0 . As a result, M0 , w |=S5
tr(0 ).
 Suppose that there exists a S5 model M0 = (W, V ) and a world w  W such that
0
M , w |=S5 tr(0 ). We define the relations X where X  P as in the Definition 1. Let
M be the CP-model equal to (W, V ). A standard induction provides that M, w |=CP 0 .
More precisely, let us prove by induction that for all   SF (0 ), we have M, u |=CP  iff
M0 , u |=S5 tr1 () for all u  W .
 Propositional case: for all atomic propositions p, we have M, u |=CP p iff u  V (p) iff
u  V 0 (p) iff M0 , u |=S5 tr1 (p).
 Negation: M, u |=CP  iff M, u 6|=CP  iff M0 , u 6|=S5  iff M0 , u |=S5 .
 Conjunction: M, u |=CP    iff M, u |=CP  and M, u |=CP  iff M0 , u |=S5 tr1 ()
and M0 , u |=S5 tr1 () iff M0 , u |=S5 tr1 (  ).
118

fiThe Ceteris Paribus Structure of Logics of Game Forms

 Case of a formula of the form [X]:
M, u |=CP [X]
iff for all v  W , u VX v implies M, v |=CP 
iff for all v  W , u VX v implies M0 , v |=S5 tr1 ()
(by induction)
iff for all v  W , u VX v implies M0 , v |=S5 p
(because, as M0 , w |=S5 tr(0 ) we have that
for all v  W , M0 , v |=S5 (p  tr1 ()))
iff M0 , u |=S5 tr1 ([X])
As M0 , w |=S5 tr(0 ), we have that M0 , w |=S5 (p0  tr1 (0 )) and M0 , w |=S5 p0 . Thus,
M0 , w |=S5 tr1 (0 ). Hence M, w |=CP 0 .

Appendix C. Proof of Theorem 3
Let us consider a group STIT formula . Let m be an integer. Then the following items
are equivalent:
1.  is satisfiable in a model where each agent has at most 2m actions;
2.  is satisfiable in a model where each agent has exactly 2m actions;
3. GRIDm  tr2 () is CP-satisfiable.
Proof. 1  2 Let M0 = (W 0 , {RJ0 }JAGT , V 0 ) be a STIT-model with at most 2m actions
per agent and w  W 0 such that M0 , w |=STIT . We construct a sequence of models
Mj = (W j , {RJj }JAGT , V j ) such that all agents j 0  {1, . . . , j} have exactly 2m actions
in Mj and such that Mj is bisimilar to Mj1 . We construct Mj from Mj1 as follows.
j1
j1
j1
Let R{j}
(w1 ), . . . , R{j}
(wk ) be an enumeration of R{j}
- classes (that is, actions for agents
j1
j), where k  2m . Let (Copy` )`{k+1,...,2m } be a family of disjoint copies of R{j}
(w1 ).
We write uCv to say that u = v or v is a copy of u or u is a copy of v. The model
Mj = (W j , {RJj }JAGT , V j ) is defined as follows:
S
 W j = W j1  `{k+1,...,2m } Copy` ;
j
j1
 R{j}
= R{j}


S

`{k+1,...,2m } {(u, v)

| u, v  Copy` }

j
j1
0
 R{j
0 } = C  R{j 0 }  C for all j 6= j;

 V j (p) = {v  W j | vCu and u  V j1 (p)}.
119

fiGrossi, Lorini, & Schwarzentruber

This construction makes that Mj and Mj1 are bisimilar and by induction we have that
all agents j 0  {1, . . . , j} have exactly 2m actions in Mj . Finally, we have Mn , w |=STIT 
and each agent has exactly 2m actions in Mn .
23
Let
us
consider
a
STIT
model
m
M = (W, {RJ }JAGT , V ) in which each agent has exactly 2 actions. Let w  W be
such that M, w |=STIT . For all j  AGT , let R{j} (wj,1 ), . . . , R{j} (wj,2m ) be an enumeration of all R{j} -classes in M. Let us extend V such that in all worlds of R{j} (wj,i ) the
valuations of the atomic propositions in Rj correspond to the binary digits in the binary
representation of i. For all d  {1, . . . , m}:
[
R{j} (wj,i )
(10)
V (rep jd ) =
i=1..2m | the dth digit of i is 1

Independence of agents in M ensures that M, w |=CP GRIDm . We prove that M, u |=CP
tr2 () iff M, u |=STIT  by induction over all subformulae  of .
3  1 Let M = (W, V ) be a CP-model and w  W such that M, w |=CP GRIDm 
tr2 (). We define RJ =SjJ Rj . The resulting Kripke-model M0 = (W, {RJ }JAGT , V )
is a STIT-model where each agent has exactly 2m actions. In particular, it satisfies the
independence of agents because M, w |=CP GRIDm . We prove that M, u |=CP tr2 () iff
M0 , u |=STIT  by induction over all subformulae  of .

Appendix D. Proof of Theorem 5
The CP-nested satisfiability problem is PSPACE-hard.
Proof. We reduce the satisfiability problem of STIT-formulae where coalitions are taken
from a linear set of coalitions, which is PSPACE-complete (Schwarzentruber, 2012) to the
CP-nested satisfiability problem: we use the translation tr2 of Subsection 3.1. Let  be a
STIT-formula. We have  is STIT-satisfiable iff tr2 () is CP-satisfiable.
 As it stated by Schwarzentruber (2012), the STIT where coalitions are taken from
a linear set of coalitions has the exponential model property. So the result of Theorem 3 is
true. Hence if  is STIT-satisfiable then GRIDm  tr2 () is CP-satisfiable (where m is the
length of ). Hence tr2 () is CP-satisfiable.
 Suppose that there exists a CP-model M = (W, V ) and w  W such that M, w |=
tr2 (). We define RJ =SjJ . Then the STIT model M0 = (W, (RJ )J , V ) is such that
M0 , w |= . Remark that we do not need to specify all the relations RJ for all J. As long
as RJ is specified for all coalitions J that appear in  and that RJ  RJ 0 if J 0  J, we can
extend the Kripke model M0 to a completely specified STIT-model also satisfying .20

Appendix E. Proof of Theorem 6
The CP-nested satisfiability problem is in PSPACE.
Proof. We reduce the CP-nested satisfiability problem to the satisfiability problem of STIT
where coalitions are taken from a linear set of coalitions. We define AX = {jp such that p  X}
20. See Schwarzentrubers paper (2012) for more details about this construnction.

120

fiThe Ceteris Paribus Structure of Logics of Game Forms

where jp is a fresh agent corresponding to the atomic proposition p. Let us define the following translation:
 tr(p) = p;
 tr() = tr();
 tr(  ) = tr()  tr0 ();
 tr([X]) = [AX : stit]tr().
Let us consider a fixed CP-formula . We recall that a signature X appears in  if there
exists a formula  such that hXi  SF (). We have also to define the following formula
V
CON T ROL = [ : stit] X appearing in 
V
pX (p  [AX : stit]p)  (p  [AX : stit]p).
tr()  CON T ROL is a STIT-formula which is computable in polynomial time and which
satisfies the condition of nesting over groups (i.e., for any two operators [J : stit] and
[J 0 : stit] occurring in the formula either J  J 0 or J 0  J). We also have that  is
CP-satisfiable iff tr()  CON T ROL is satisfiable in a STIT-model.
 Suppose that there exists an CP-model M = (W, V ) and w  W such that
M, w |=CP . We define RAX =X . Then the STIT model M0 = (W, (RAX )X , V ) is
such that M0 , w |=STIT tr()  CON T ROL. Remark that we do not need to specify all
the relations RJ for all J. As long as RJ is specified for all coalitions J that appear in
tr()  CON T ROL and that RJ  RJ 0 if J 0  J, we can extend the Kripke model M0 to
a completely specified STIT-model also satisfying tr()  CON T ROL.21

Suppose
that
there
exists
a
STIT-model
M0 = (W, (RAX )X , V ) and a world w  W such that M0 , w |=STIT tr()  CON T ROL.
As M0 , w |= CON T ROL, we have X = RAX . This is the reason why we define M =
(W, {X }X2P , V ). Consequently, we have M, w |=CP .

Appendix F. Proof of Theorem 7
Let m = |P|. Then, a CLPC formula  is CLPC satisfiable if and only if (EXC + 
EXC   COMPL  GRID  )  tr3 () is satisfiable in a STIT model where each agent has at
most 2m actions.
Proof. Let us suppose |P| = m.
 Let M = (P1 , . . . , Pn , X  ) be a CLPC model such that M |=CLPC , where
P1 , . . . , Pn is a partition of P among the agents in AGT .
We build the STIT
model M = (W, {RJ }JAGT , V ) as follows:
 W = {X : X  P},
 for all J  AGT and for all X, X 0  W , (X, X 0 )  RJ0 if and only if XJ = XJ0 ,
21. Again see Schwarzentrubers paper for more details about this construction.

121

fiGrossi, Lorini, & Schwarzentruber

 for all p  P and for all X  W , X  V (p) if and only if p  X,
S
where for any X  P and for any J  AGT , XJ = X  PJ (with PJ = iJ Pi ). The size
of M is 2m . It follows that the number of RAGT -equivalence classes (alias joint actions) is
equal or lower than 2m . Consequently, the number of actions for every agent is bounded by
2m .
It is straightforward to prove that for all X  W we have M, X |=STIT EXC + 
EXC   COMPL  GRID  . Moreover, by induction on the structure of , we prove that
M, X  |=STIT tr3 (). The only interesting case is  = J :
M |=CLPC J  iff there exists XJ  PJ s.t. M

M

XJ |=CLPC 

iff there exists XJ  PJ s.t.

M, XJ  XAGT
\J |=STIT tr3 () (by I.H.)

iff M, X  |=STIT hAGT \ J : stititr3 ()
 Let M = (W, {RJ }JAGT , V ) be a STIT model where the number of actions for
every agent is bounded by 2m and w0  W such that M, w0 |=STIT (EXC +  EXC  
COMPL  GRID  )  tr3 ().
For any i  AGT , let



M, v |= [{i} : stit]p or
Ctrl i = p  P : v  W,


M, v |= [{i} : stit]p







be the set of atoms in P controlled by agent i. For any J  AGT , let Ctrl J =

S

iJ

Ctrli .

Lemma 3. For all J  AGT , X  P, X  X and w  W we have:
(i) if CtrlJ = X then CtrlAGT \J = P \ X,
V
V
(ii) if M, w |=STIT p+ p  p p and CtrlJ = X then, for all v  RJ (w), we have
X V
X
V
M, v |=STIT p+ p  p p,
X

X

0
(iii) if CtrlJ = X then, for all P\X
 P\X, there exists v  RJ (w) such that M, v |=STIT
V
V
p

p.
0+
0
p
p
P\X

P\X

+

where for any X  P and for any X  X, X
= X and X
= X \ X .

Proof. (i) Let us suppose that p 6 CtrlJ . We are going to prove that p  CtrlAGT \J .
From p 6 CtrlJ it follows that for all w  W we have M, v |=STIT p for some v  RJ (w).
This implies that for all i  J and for all w  W we have M, w |=STIT [{i} : stit]p 
[{i} : stit]p. From M, w0 |=STIT COMPL it follows that there is i  AGT \ J such
that M, w |=STIT [{i} : stit]p  [{i} : stit]p for all w  W . The latter implies that
p  CtrlAGT \J . The other direction (i.e., p  CtrlJ implies p 6 CtrlAGT \J ) follows from
M, w0 |=STIT EXC +  EXC  .
122

fiThe Ceteris Paribus Structure of Logics of Game Forms

(ii) Let us suppose that M, w |=

V

+
pX

p

V


pX

p and CtrlJ = X. By the fact

+
that relations RJ are reflexive, it follows that, for all p  X
, there exists i  J such that

M, w |=STIT [{i} : stit]p and for all p  X there exists i  J such that M, v |=STIT [{i} :
+
stit]p. From the latter it follows that for all p  X
we have M, w |=STIT [J : stit]p and

for all p  V
we
have
M,
v
|=
[J
:
stit]p.
Therefore,
for all v  RJ (w), we have
STIT
X
V
M, v |=STIT p+ p  p p.
X

X

0
 P\X
(iii) Let us suppose that CtrlJ = X and let us consider an arbitrary P\X

and w  W .VFrom M, wV
|=
GRID
it
follows
that
there
exists
v

W
such
that
0
STIT
M, v |=STIT p0+ p  p0 p. By item (ii), the latter implies that there exists
P\X
P\X
V
V
v  W such that M, v |=STIT [AGT \J : stit]( p0+ p p0 p). From the constraint
P\X

P\X

of
V independence
V of agents it follows that there exists v  RJ (w) such that M, v |=STIT
p

0+
p
p 0 p.
P\X

P\X

We transform the STIT model M in a CLPC model M = (P1 , . . . , Pn , X  ) as follows:
 for all p  P, p  X  if and only if w0  V (p),
 for all p  P and for all i  AGT , p  Pi if and only if p  Ctrli .
By the item (i) of Lemma 3 it is easy to check that M is indeed a CLPC model. In
particular, P1 , . . . , Pn is a partition of P among the agents in AGT .
By induction on the structure of  and by using Lemma 3 it is straightforward to prove
that M |=CP . The only interesting case is  = J :
M, w0 |=STIT hAGT \ J : stititr3 ()
iff M, v |=STIT tr3 () for some v  RAGT \J (w0 )
iff there exists XJ  PJ s.t.

(P1 , . . . , Pn , XJ  XAGT
\J ) |=CP 

(by I.H., and items (ii) and (iii)
of Lemma 3)
iff M |=CP J 
This completes the proof.

References
Alur, R., Henzinger, T., & Kupferman, O. (2002). Alternating-time temporal logic. Journal
of the ACM, 49, 672713.
Areces, C., & Ten Cate, B. (2006). Hybrid logics. In Blackburn, P., van Benthem, J., &
Wolter, F. (Eds.), Handbook of Modal Logic, pp. 821868. Elsevier.
Balbiani, P., Gasquet, O., Herzig, A., Schwarzentruber, F., & Troquard, N. (2008a). Coalition games over kripke semantics. In Degremont, C., Keiff, L., & Ruckert, H. (Eds.),
Festschrift in Honour of Shahid Rahman, pp. 112. College Publications.
123

fiGrossi, Lorini, & Schwarzentruber

Balbiani, P., Herzig, A., & Troquard, N. (2008b). Alternative axiomatics and complexity
of deliberative stit theories. Journal of Philosophical Logic, 37 (4), 387406.
Balbiani, P., Herzig, A., & Troquard, N. (2013). Dynamic logic of propositional assignments:
A well-behaved variant of PDL. In Proceedings of the 28th ACM/IEEE Symposium
on Logic in Computer Science (LICS 2013), pp. 143152. IEEE Computer Society.
Belnap, N., Perloff, M., & Xu, M. (2001). Facing the future: agents and choices in our
indeterminist world. Oxford University Press, USA.
Blackburn, P., de Rijke, M., & Venema, Y. (2001). Modal Logic. Cambridge University
Press, Cambridge.
Broersen, J. (2008). A complete STIT logic for knowledge and action, and some of its
applications. In Proceedings of the 6th International Workshop on Declarative Agent
Languages and Technologies (DALT 2008), Vol. 5397 of LNCS, pp. 4759. SpringerVerlag.
Broersen, J., Herzig, A., & Troquard, N. (2005). From coalition logic to STIT. In Lomuscio,
A., de Vink, E., & Wooldridge, M. (Eds.), Proceedings of the Third International
Workshop on Logic and Communication in Multi-agent Systems (LCMAS05), pp.
2335.
Broersen, J., Herzig, A., & Troquard, N. (2006). Embedding alternating-time temporal logic
in strategic STIT logic of agency. Journal of Logic and Computation, 16 (5), 559578.
Broersen, J., Herzig, A., & Troquard, N. a. (2007). A normal simulation of coalition logic
and an epistemic extension. In Samet, D. (Ed.), Proceedings TARK07, pp. 92101.
ACM Press.
Chellas, B. F. (1980). Modal Logic. An Introduction. Cambridge University Press, Cambridge.
Dunne, P., van der Hoek, W., Kraus, S., & Wooldridge, M. (2008). Cooperative boolean
games. In Proceedings of AAMAS 2008, pp. 10151022. ACM.
Gabbay, D. M., Kurucz, A., Wolter, F., & Zakharyaschev, M. (2003). Many-dimensional
modal logics: theory and applications. Elsevier.
Gasquet, O., & Herzig, A. (1994). Translating non-normal modal logics into normal modal
logics.. In Jones, A., & Sergot, M. (Eds.), Proceedings of the International Workshop
on Deontic Logic in Computer Science (DEON94).
Gerbrandy, J. (2006). Logics of propositional control. In Proceedings of AAMAS06, pp.
193200. ACM.
Goldblatt, R. (1992). Logics of Time and Computation. CSLI.
Goranko, V. (2001). Coalition games and alternating temporal logics. In Proceedings of
the 8th conference on theoretical aspects of rationality and knowledge (TARK01), pp.
259272.
Goranko, V., & Jamroga, W. (2004). Comparing semantics of logics for multi-agent systems.
Synthese, 139, 241280.
124

fiThe Ceteris Paribus Structure of Logics of Game Forms

Goranko, V., Jamroga, W., & Turrini, P. (2013). Strategic games and truly playable effectivity functions. Journal of Autonomous Agents and Multi-Agent Systems, pp.
288314.
Harel, D., Kozen, D., & Tiuryn, J. (2000). Dynamic Logic. MIT Press.
Harrenstein, P., van der Hoek, W., Meyer, J., & Witteveen, C. (2001). Boolean games. In
van Benthem, J. (Ed.), Proceedings of TARK01, pp. 287298. Morgan Kaufmann.
Herzig, A. (2014). Logics of knowledge and action: A critical analysis and challenges. Journal
of Autonomous Agents and Multi-Agent Systems, DOI: 10.1007/s10458-014-9267-z.
Herzig, A., Lorini, E., Moisan, F., & Troquard, N. (2011). A dynamic logic of normative
systems. In Walsh, T. (Ed.), Proceedings of the Twenty-Second International Joint
Conference on Artificial Intelligence (IJCAI 2011), pp. 228233. AAAI Press.
Herzig, A., & Schwarzentruber, F. (2008). Properties of logics of individual and group
agency. Advances in modal logic, 7, 133149.
Horty, J. F. (2001). Agency and Deontic Logic. Oxford University Press, Oxford.
Krabbendam, J., & Meyer, J. (2003). Contextual deontic logics. In McNamara, P., &
Prakken, H. (Eds.), Norms, Logics and Information Systems, pp. 347362, Amsterdam. IOS Press.
Krabbendam, J., & Meyer, J. (2000). Release logics for temporalizing dynamic logic, orthogonalising modal logics. In Barringer, M., Fisher, M., Gabbay, D., & Gough, G.
(Eds.), Advances in Temporal Logic, pp. 2145. Kluwer Academic Publisher.
Lorini, E., & Schwarzentruber, F. (2011). A logic for reasoning about counterfactual emotions. Artificial Intelligence, 175 (3-4), 814847.
McCann, H. J. (1974). Volition and basic action. The Philosophical Review, 83, 451473.
Moulin, H., & Peleg, B. (1982). Cores of effectivity functions and implementation theory.
Journal of Mathematical Economics, 10, 115145.
Osborne, M. J., & Rubinstein, A. (1994). A Course in Game Theory. MIT Press.
OShaughnessy, B. (1974). Trying (as the mental pineal gland). The Journal of Philosophy,
70, 365386.
Pauly, M. (2001). A logical framework for coalitional effectivity in dynamic procedures.
Bulletin of Economic Research, 53 (4), 305324.
Pauly, M. (2002). A modal logic for coalitional power in games. Journal of Logic and
Computation, 12 (1), 149166.
Schwarzentruber, F. (2012). Complexity results of STIT fragments. Studia logica, 100 (5).
Tseitin, G. (1968). On the complexity of derivation in propositional calculus.. In Structures
in Constructive Mathematics and Mathematical Logic, Part II, Seminars in Mathematics (translated from Russian). Steklov Mathematical Institute.
van Benthem, J., Girard, P., & Roy, O. (2009). Everything else being equal: A modal logic
for ceteris paribus preferences. Journal of Philosophical Logic, 38, 83125.
125

fiGrossi, Lorini, & Schwarzentruber

van der Hoek, W., & Pauly, M. (2007). Modal logic for games and information. In Blackburn,
P., van Benthem, J., & Wolter, F. (Eds.), Handbook of Modal Logic, pp. 10771146.
Elsevier.
van der Hoek, W., & Wooldridge, M. (2003). Cooperation, knowledge and time: Alternatingtime temporal epistemic logic and its applications. Studia logica, 75 (1), 125157.
van der Hoek, W., & Wooldridge, M. (2005). On the logic of cooperation and propositional
control. Artificial Intelligence, 164, 81119.
van Ditmarsch, H., Kooi, B., & van der Hoek, W. (2007). Dynamic Epistemic Logic, Vol.
337 of Synthese Library Series. Springer.
van Eijck, J. (2000). Making things happen. Studia logica, 66 (1), 4158.
Von Wright, G. H. (1963). The Logic of Preference. Edinburgh University Press.
Walther, D., van der Hoek, W., & Wooldridge, M. (2007). Alternating-time temporal logic
with explicit strategies. In Proceedings of the 11th conference on Theoretical Aspects
of Rationality and Knowledge, pp. 269278. ACM Press.
Wang, Y., & Cao, Q. (2013). On axiomatizations of public announcement logic. Synthese,
190, 103134.
Wansing, H. (2006). Tableaux for multi-agent deliberative-STIT logic. In Governatori, G.,
Hodkinson, I., & Venema, Y. (Eds.), Advances in Modal Logic, Volume 6, pp. 503520.
Kings College Publications.
Xu, M. (1998). Axioms for deliberative STIT. Journal of Philosophical Logic, 27, 505552.

126

fiJournal of Artificial Intelligence Research 53 (2015) 223-270

Submitted 12/14; published 06/15

Probabilistic Inference Techniques for Scalable
Multiagent Decision Making
Akshat Kumar

akshatkumar@smu.edu.sg

School of Information Systems
Singapore Management University, Singapore

Shlomo Zilberstein

shlomo@cs.umass.edu

College of Information and Computer Sciences
University of Massachusetts, Amherst, USA

Marc Toussaint

marc.toussaint@informatik.uni-stuttgart.de

Department of Computer Science
University of Stuttgart, Germany

Abstract
Decentralized POMDPs provide an expressive framework for multiagent sequential decision making. However, the complexity of these modelsNEXP-Complete even for two
agentshas limited their scalability. We present a promising new class of approximation algorithms by developing novel connections between multiagent planning and machine
learning. We show how the multiagent planning problem can be reformulated as inference
in a mixture of dynamic Bayesian networks (DBNs). This planning-as-inference approach
paves the way for the application of efficient inference techniques in DBNs to multiagent
decision making. To further improve scalability, we identify certain conditions that are sufficient to extend the approach to multiagent systems with dozens of agents. Specifically, we
show that the necessary inference within the expectation-maximization framework can be
decomposed into processes that often involve a small subset of agents, thereby facilitating
scalability. We further show that a number of existing multiagent planning models satisfy
these conditions. Experiments on large planning benchmarks confirm the benefits of our
approach in terms of runtime and scalability with respect to existing techniques.

1. Introduction
Decentralized partially observable MDPs (Dec-POMDPs) have emerged in recent years as
a prominent framework for modeling sequential decision making by a team of collaborating agents (Bernstein, Givan, Immerman, & Zilberstein, 2002). Their expressive power
makes it possible to tackle coordination problems in which agents must act based on different partial information about the environment and about each other so as to maximize a
global reward function. Applications of Dec-POMDPs include coordinating the operation of
planetary exploration rovers (Becker, Zilberstein, Lesser, & Goldman, 2004), coordinating
firefighting robots (Oliehoek, Spaan, & Vlassis, 2008), target tracking by a team of sensor
agents (Nair, Varakantham, Tambe, & Yokoo, 2005) and improving throughput in wireless
networks (Pajarinen, Hottinen, & Peltonen, 2014).
While there has been rapid progress with exact algorithms for Dec-POMDPs (Oliehoek,
Spaan, Amato, & Whiteson, 2013), such optimal solutions can only be obtained for relac
2015
AI Access Foundation. All rights reserved.

fiKumar, Zilberstein, & Toussaint

tively smaller problems. In terms of computational complexity, optimally solving a finitehorizon Dec-POMDP is NEXP-Complete (Bernstein et al., 2002). In contrast, finite-horizon
POMDPs are PSPACE-complete (Mundhenk, Goldsmith, Lusena, & Allender, 2000), a
strictly lower complexity class that highlights the difficulty of solving Dec-POMDPs.
1.1 Related Work
For the finite-horizon case, a substantial number of promising point-based approximate
algorithms have been developed (Kumar & Zilberstein, 2010b; Wu, Zilberstein, & Chen,
2010; Dibangoye, Mouaddib, & Chaib-draa, 2009; Kumar & Zilberstein, 2009a; Seuken &
Zilberstein, 2007). However, unlike their point-based counterparts for POMDPs (Pineau,
Gordon, & Thrun, 2006; Smith & Simmons, 2004), they cannot be easily adopted for the
infinite-horizon case due to a variety of reasons. For example, POMDP algorithms represent
the policy as compact -vectors, whereas most Dec-POMDP algorithms explicitly store the
policy as a mapping from observation sequences to actions, making them unsuitable for
the infinite-horizon case. This has motivated the development of alternative approaches,
approximating factored finite-horizon Dec-POMDPs by a series of collaborative graphical
Bayesian games (Oliehoek, Whiteson, & Spaan, 2013) or using genetic algorithms (Eker &
Akin, 2013).
Recently, a number of approaches have been developed that transform a Dec-POMDP
into a continuous-state MDP and then use techniques from the POMDP literature to solve
the continuous-state MDP (Dibangoye, Amato, Doniec, & Charpillet, 2013a; Dibangoye,
Amato, Buffet, & Charpillet, 2013b). The state in such a continuous MDP reformulation of a
Dec-POMDP, also called occupancy state, is the probability distribution over the world state
and the history of observations each agent has received. Despite the adoption of efficient
POMDP techniques to such a reformulation, a drawback of the approach is that the size of
observation histories increases exponentially with respect to the plan horizon. In contrast,
our approach uses the notion of finite-state controllers (FSCs) that summarize relevant
features of the planning problem. Often, compact FSCs can provide a good approximation
for large planning problems. The occupancy state based formulation has also been applied
to infinite-horizon problems by converting an infinite-horizon problem to an approximate
finite-horizon version. This is done by using the future reward discount factor to derive
some finite-horizon H after which the remaining rewards have a negligible contribution
to the overall value function (Dibangoye, Buffet, & Charpillet, 2014). A drawback of the
truncated horizon approach is that, for high discount factors, the required horizon H can
be prohibitively large.
In terms of solution representation, most algorithms for infinite-horizon problems represent agent policies as finite-state controllers (Amato, Bernstein, & Zilberstein, 2010; Bernstein, Amato, Hansen, & Zilberstein, 2009), unlike algorithms for finite-horizon problems
that often use policy trees (Hansen, Bernstein, & Zilberstein, 2004). The resulting solution
is approximate because of the limited memory of the controllers and because optimizing
the action selection and transition parameters is extremely hard. Previous approaches to
optimize finite-state controller based policies include decentralized bounded policy iteration (DEC-BPI) (Bernstein et al., 2009) and a technique based on non-linear programming
(NLP) (Amato et al., 2010). The DEC-BPI algorithm uses a linear programming formula224

fiProbabilistic Inference for Multiagent Decision Making

tion to improve the parameters of one node of one finite-state controller at a time. That
is, it fixes the parameters of all the nodes of all the controllers, except a single node of a
particular agent. Then it uses a linear program to find better action selection and transition
parameters for that particular node. This LP guarantees that the policy value is increased
for every belief state. A major drawback of this scheme is that it is not designed to optimize the value of a particular belief state. Therefore, to produce a good policy, DEC-BPI
may need a large number of controller nodes, which reduces the effectiveness of the LP
formulation.
In contrast to the DEC-BPI approach, the NLP formulation of Amato et al. (2010) can
optimize the controllers for a given initial belief state. This formulation has a linear objective
function. However, the Bellman constraints, which involve additional variables representing
the value of each node, are nonlinear and non-convex in all the variables. This can cause the
NLP solver to get stuck in a local optimum. Furthermore, empirically we observed that the
performance of the state-of-the-art NLP solvers such as SNOPT (Gill, Murray, & Saunders,
2002) degrades quickly even with a moderate increase in the number of nonlinear constraints.
This highlights the challenges presented by scaling the NLP approach for larger ( 2 agents)
problems. A complementary research direction has been to investigate the kind of structure
in a controller that can enable better quality solutions. For example, layered controllers
can be developed for POMDPs and Dec-POMDPs, that are then optimized by point based
approaches (Pajarinen & Peltonen, 2011b). The EM based planning algorithms we develop
in our work can also take advantage of such controller structures. There are other approaches
to compute policies for infinite-horizon Dec-POMDPs that are not based on a controller
representation of the joint-policy (MacDermed & Isbell, 2013). However, a key advantage
of policies based on finite-state controllers is their ease of execution in resource constrained
environments (Grzes, Poupart, & Hoey, 2013; Grzes, Poupart, Yang, & Hoey, 2015), without
any expensive belief update operations required in other approaches. Furthermore, policies
represented as finite-state controllers can carry more semantic information, where each
controller node summarizes some relevant aspects of the observation history.
Generalizing Dec-POMDP algorithms to more than two agents has been a persistent
challenge from both problem representation and algorithmic viewpoints. Many recent attempts to increase the scalability of planners with respect to the number of agents impose
restrictions on the form of interaction among the agents. Examples of such restrictions include transition independence (Becker et al., 2004; Nair et al., 2005), weak-coupling among
agents that is limited to certain states (Varakantham, Kwak, Taylor, Marecki, Scerri, &
Tambe, 2009), and directional transition dependence (Witwicki & Durfee, 2010). One of
the earliest models that demonstrated scalability by limiting interactions among agents was
transition-independent Dec-MDP (TI-Dec-MDP) (Becker, Zilberstein, Lesser, & Goldman,
2003). Agents in these models can fully affect and observe their local state, but they cannot
affect the local states or observations of other agents. The dependence among agents is limited to the joint reward function. This restricted model has been shown to be NP-Complete
by Goldman and Zilberstein (2004), who also conducted a detailed complexity analysis of
different subclasses of Dec-POMDPs with various limitations on agent interactions. An extension of TI-Dec-MDP was proposed by Becker, Zilberstein, and Lesser (2004), introducing
structured transition dependencies.
225

fiKumar, Zilberstein, & Toussaint

The TI-Dec-MDP model was extended to handle partial observability in a problem
instance by Nair et al. (2005), resulting in a model called network-distributed POMDP
(ND-POMDP). It was shown that the value function of ND-POMDPs factorizes among
smaller sub-groups of agents based on immediate reward decomposability. This property
was used to develop a number of algorithms that scale relatively well with respect to the
number of agents (Nair et al., 2005; Varakantham, Marecki, Yabu, Tambe, & Yokoo, 2007;
Marecki, Gupta, Varakantham, Tambe, & Yokoo, 2008). Another restricted class of models
is the transition-decoupled POMDP (TD-POMDP) (Witwicki & Durfee, 2010). This model
explicitly differentiates between an agents private state that can only be affected by the
agents local action and shared states that can be affected by other agents. Structured
interactions have also been used for deciding how to communicate among agents (Mostafa
& Lesser, 2009, 2011). Within the framework of multiagent MDPs (MMDPs), the submodularity property of the value function for a class of sensor network planning problems has
been exploited by Kumar and Zilberstein (2009b).
By and large, the above models try to identify restrictions on agent interactions that
facilitate scalable planning. With the exception of the work of Witwicki and Durfee (2011),
there has not been much work towards a general characterization of conditions under which
multiagent planning can be made scalable. Witwicki and Durfee provide a characterization
of weak-coupling among agents similar to our work. However, a significant difference is
that we propose a concrete algorithm that can efficiently exploit such weak-coupling among
agents to enable scalability to large multiagent systems. The algorithmic outline presented
by Witwicki and Durfee to exploit weak interactions among agents by mapping the policy
optimization problem to constraint optimization is not scalable as it involves enumerating
the policy space of agents.
This article extends two conference papers published at UAI10 (Kumar & Zilberstein,
2010a) and IJCAI11 (Kumar, Zilberstein, & Toussaint, 2011) with more detailed background, new theorems and proofs, simplified EM derivations, and detailed derivation of the
EM updates for the ND-POMDP model. The EM based policy optimization approach that
we developed is the foundation of several other efforts that explore different aspects of multiagent planning. For example, Wu, Zilberstein, and Jennings (2013) developed samplingbased E-step inference techniques. This technique is particularly useful in the model-free
setting in which the underlying model is not known. Pajarinen and Peltonen (2011a) extend our EM approach to factored Dec-POMDPs. They present an approximation of the
E-step using a factored representation of the forward and backward messages as defined in
Section 4.4. Their approach increases the scalability of the EM algorithm w.r.t. the number of agents and other problem parameters at the expense of making the EM algorithm
approximate. That is, the E and M steps are approximate and may lead to non-monotonic
improvement in the policy value. Pajarinen and Peltonen (2013) further extend the EM
approach to average reward Dec-POMDPs.
The techniques we present in this article differ from such previous approaches in that
rather than approximating the computationally challenging inference for larger multiagent
systems, we explore some natural ways to decompose the inference process and produce an
exact, yet scalable EM algorithm. We further show that the necessary conditions for such
value factorization based decomposition are satisfied in several existing planning models,
confirming the generality of the developed framework.
226

fiProbabilistic Inference for Multiagent Decision Making

1.2 Summary of Contributions
We present a promising new class of algorithms that combines planning with probabilistic inference and opens the door to the application of rich inference techniques to solving
infinite-horizon Dec-POMDPs. Our approach is based on Toussaint et. al.s approach of
transforming a single-agent planning problem into its equivalent mixture of dynamic Bayes
nets (DBNs) and using likelihood maximization in this framework to optimize the policy
value (Toussaint & Storkey, 2006; Toussaint, Harmeling, & Storkey, 2006). Such approaches
have been successful in solving MDPs and POMDPs (Toussaint et al., 2006) and they easily extend to factored or hierarchical structures (Toussaint, Charlin, & Poupart, 2008).
Furthermore, they can handle continuous action and state spaces thanks to advanced probabilistic inference techniques (Hoffman, Kueck, de Freitas, & Doucet, 2009b). We show
how Dec-POMDPs, which are much harder to solve than MDPs or POMDPs, can also be
reformulated as a mixture of DBNs. We then present an Expectation Maximization (EM)
algorithm (Dempster, Laird, & Rubin, 1977) to maximize the reward likelihood and the
policy value in this framework. This approach offers an attractive anytime algorithm because EM improves the likelihoodand hence the policy valuewith each iteration. Our
experiments on benchmark domains show that EM compares favorably against previous
approaches to optimize FSCs, DEC-BPI and NLP-based optimization.
To address scalability with respect to the number of agents, we identify conditions
based on value function factorization that are sufficient to make planning amenable to a
scalable approximation. This is achieved by constructing a graphical model that exploits the
locality of interactions among the agents. We show how to efficiently extend the likelihood
maximization paradigm to this generalized graphical model. Furthermore, we show that
the necessary inference can be decomposed into processes that involve a small subset of
agentsaccording to the interaction graphthereby facilitating scalability. We derive a
global update rule that combines these local inferences to monotonically increase the overall
solution quality. Several existing multiagent planning models are shown to satisfy these
conditions, thereby validating the generality of the value factorization property. A further
benefit of our approach is that it is amenable to a massively parallel implementation, since
it relies on local computations and message-passing among neighboring agents.
Experiments on a large multiagent planning benchmark, with joint state and joint action
spaces up to 522 , 320 respectively, confirm that our approach is scalable with respect to the
number of agents and can provide good quality solutions for planning problems for up to 20
agents, which cannot be handled by the best existing approaches. For smaller multiagent
systems, our approach provides better solution quality and is about an order-of-magnitude
faster than the previous best nonlinear programming approach for optimizing FSCs.

2. Decentralized POMDP Model
The Dec-POMDP model is a natural extension of MDPs and POMDPs. It can be defined
by a tuple hI, S, {Ai }, P, R, {Y i }, O, i, where:
 I denotes a finite set of n agents
 S denotes a finite set of states with designated initial state distribution 0
227

fiKumar, Zilberstein, & Toussaint

 Ai denotes a finite set of actions for each agent i
 P denotes state transition probabilities: P (s0 |s, ~a), the probability of transitioning
from state s to s0 when the joint-action ~a is taken by the agents
 R denotes the reward function: R(s, ~a) is the immediate reward for being in state s
and joint-action taken as ~a
 Y i denotes a finite set of observations for each agent i
 O denotes the observation probabilities: O(~y |s0 , ~a) is the probability of receiving the
joint-observation ~y when the last joint-action taken was ~a that resulted in the environment state being s0
  denotes the reward discounting factor
An agent is policy,  i : Y i  Ai , maps the set of all possible observation histories Y i
to actions. Solving a Dec-POMDP entails finding the joint-policy  = h 1 , . . . ,  n i that
maximizes the total expected reward.
X



t
E
 R st , ~at ; 

(1)

t=0

where  denotes the joint-policy and subscript t denotes the dependence on time. In a
Dec-POMDP, agents are acting under uncertainty not only about the underlying environment state but also about each other. Although the joint-observation received by agents
may be correlated, each agent only observes its own component of the joint-observation.
This makes the coordination problem particularly challenging. In Section 2.1, we show a
compact representation of the policy as finite-state controllers, rather than long sequences
of observations.
When there are two agents in a given multiagent system, we adopt a simplified notation
as follows. The action set of agent 1 is denoted by a  A and of agent 2 by b  B.
The state transition probability P (s0 |s, a, b) depends upon the actions of both the agents.
Upon taking the joint-action ha, bi in state s, agents receive the joint-reward R(s, a, b). Y
is the finite set of observations for agent 1 and Z for agent 2. O(yz | s, a, b) denotes the
probability P (y, z|s, a, b) of agent 1 observing y  Y and agent 2 observing z  Z when the
joint-action ha, bi was taken and resulted in state s. For infinite-horizon Dec-POMDPs, a
reward discounting factor  < 1 is used.
2.1 Finite State Controllers
In the case of infinite-horizon Dec-POMDPs, agents operate continuously with the immediate reward R being discounted by a factor  < 1. Representing agents local policies using
an explicit tree structured representation is not feasible in this case. Therefore, agents
policies are represented as cyclic finite-state controllers (FSC).
We represent each agents policy as a bounded, finite state controller (FSC). This approach has been used successfully for both POMDPs (Poupart & Boutilier, 2003; Amato,
Bernstein, & Zilberstein, 2007) and Dec-POMDPs (Amato et al., 2010). In this case, each
228

fiProbabilistic Inference for Multiagent Decision Making

y1

z1

p2

q2
y2

y2

z2

z2
(  )

p1 (a, p)

q1
z1

y1
Agent 1

Agent 2

Figure 1: A two-agent infinite-horizon joint-policy represented using finite-state controllers.
Each node is a memory state. Edges represent the node transition function. Agent
1 has two observations, y1 and y2. Agent 2 also has two observations, z1 and z2.
agent i has a finite internal memory state, q i  Qi , which summarizes the crucial information obtained from past observations to support efficient action selection. The size of
the set Qi determines the expressiveness of the FSC based policy. For POMDPs, FSCs are
beneficial due to their compactness and relative ease of policy execution compared to the
full belief over world states. In Dec-POMDPs, belief over world states cannot be maintained
during the execution time due to lack of availability of joint-observations. Therefore, FSCs
are particularly useful as executing FSC-based policies does not require maintaining belief
over world states.
The FSC of the ith agent is parameterized by i = ( i , i ,  i ) as explained below.
 An agent chooses actions depending on its internal state q: P (a|q; ) = a,q .
 The internal state is updated with each new observation, by the node transition function:
P (q 0 |q, y; ) = q0 ,q,y .
 Finally, q0 is the initial node distribution P (q0 ) for each agent.

Figure 1 shows the structure of such controllers for two agents. Both the action selection
parameter  and the node transition parameter  could be deterministic or stochastic.
We optimize stochastic controllers in this work because they can generally produce higher
values (Poupart & Boutilier, 2003).
Figure 2 shows the complete DDN representation of a two-agent Dec-POMDP depicting
how the environment and agents policies interact with each other. We use the convention
that subscripts denote time and superscripts, if any, identify agents.
2.2 Objective Function
Considering again the two-agent case, we denote the controller nodes for agent 1 by p and
agent 2 by q. The value for starting the controllers in nodes hp, qi at state s is given by:
X
V (p, q, s) =
a,p b,q
a,b

h
i
X
X
X
R(s, a, b) + 
P (s0 | s, a, b)
O(yz | s0 , a, b)
p0 ,p,y q0 ,q,z V (p0 , q 0 , s0 )
s0

y,z

229

p0 q 0

fiAgent 1

Kumar, Zilberstein, & Toussaint

p1

p0
a0
s0

b0
Agent 2

y1

s1

a0
s0

z1
q1

q0

p1

p0

y1

s1
b0

p2
a1
s2

z1

b1

q1

q0

r0

Figure 2: A two-time slice dynamic decision network (DDN) representation
of a two-agent
p1
Dec-POMDP. All nodes are random variables. Square nodes represent decisions;
diamond nodes represent the reward; p and q represent the states of policy for
y1
agent 0 and 1 respectively; subscripts denote time. a0
The goal is to set the parameters h, , i of the agents controllers
(of some given size)
s1
that maximize the expected discounted reward for the initial belief 0 :
V (0 ) =

X

p q 0 (s)V (p, q, s) b0

z1

p,q,s

3. Dec-POMDPs as Mixture of DBNs

q1

In this section, we describe how Dec-POMDPs can be reformulated as a mixture of DBNs,
such that maximizing the reward likelihood defined below is equivalent to optimizing the
joint-policy. Our approach is based on the framework proposed by Toussaint et al. (2006)
and Toussaint and Storkey (2006) to solve Markovian planning problems using probabilistic
inference. In this section, we develop the planning-as-inference strategy for two-agent DecPOMDPs and later extend it to multiple ( 2) agents. The previous approach of Toussaint
et al. (2006) and Toussaint and Storkey (2006) focused on single agent MDPs and POMDPs.
In our work, we develop new mixture models that allow us to extend the planning-asinference paradigm to multiple agents, a significant generalization of the single agent case.
First, we briefly describe the intuition behind this reformulation and then we describe in
detail the necessary modifications required for Dec-POMDPs.
A Dec-POMDP can be described using a single DBN where the reward is emitted at
each time step, such as the unrolled DBN corresponding to the one shown in Figure 2.
However, in our approach, it is described by an infinite mixture of a special type of DBNs
where the reward is emitted only at the end. There is one mixture component for each
time period from T = 0 to . Furthermore, to simulate the discounting of rewards, the
probability of T , which acts as mixture weight, is set as P (T = t) =  t (1  ). This also
230

y2

z2
q2

fiProbabilistic Inference for Multiagent Decision Making

p0

a0
s0

T =0

r
b0

q0
p1

a0
s0

y1

s1
b0

Agent2

a0

p1

p0

z1

q1

q0
r0

Mixture of finite-time Dec-POMDPs

Agent1

p0

s0

y1

s1
b0

z1

q1

p0

p1

a0

y1

s1
b0

b1

a1

p2

pT

y2

yT

s2
z1

q1

q0

T =1

r

q0

s0

a1

b1

aT
r

sT
z2

zT

q2

qT

bT

Figure 3: The time-dependent DBN mixture (right) corresponding to the two-agent DecPOMDP (left). The first DBN component in the mixture corresponds to the
reward for time step 1, second DBN corresponds to the reward for time step 2.
The last DBN in the mixture shows the general structure of a T -step DBN.

231

fiKumar, Zilberstein, & Toussaint

P
ensures that the mixing weights are normalized or 
t=0 P (T = t) = 1. We now describe the
structure of each mixture component of a single T -step DBN.
The first DBN in the DBN mixture model shown in Figure 3 describes the DBN for
time T = 0. The key intuition is that for the reward emitted at any time step T , we have
a separate DBN with the general structure as shown in the last T -step DBN shown in
Figure 3. Such a DBN shows how the reward obtained at time step T depends on policy
parameters and the underlying Dec-POMDP model.
The random variable r shown in the DBN mixture of Figure 3 is a binary variable
with its conditional distribution (for any time T ) described using the normalized immediate
reward as:
Rsab = P (r = 1|sT = s, aT = a, bT = b) = (Rsab  Rmin )/(Rmax  Rmin ).
The parameter Rmax is the maximum reward for any state action pair for the given DecPOMDP instance and Rmin denotes the minimum reward. This scaling of the reward is
the key to transforming the optimization problem from the realm of planning to likelihood
maximization as stated below. Let  denote the joint parameters h, , i for each agents
controller.
Theorem 1. By choosing the conditional probability of binary rewards r such that Rsab 
Rsab and introducing the discounting time prior P (T ) =  T (1  ), the joint-policy value V 
relates linearly to the likelihood L of observing the reward variable as 1:
V =

(Rmax  Rmin )L
Rmin
+
(1  )
1

Proof. We have the value function defined as:
V



X



t
=E
 R st , ~at ; 

(2)

t=0

Consider the T -step DBN in the mixture of Figure 3. We define the likelihood for this time
step T DBN as follows:
LT = P (r = 1|T ; )

(3)

For the full mixture corresponding to a Dec-POMDP, we have:
L =

X
T

P (T )LT = (1  )

X

 T P (r = 1|T ; )

(4)

T

We already chose P (r = 1|sT = s, aT = a, bT = b) = (Rsab  Rmin )/(Rmax  Rmin ). Therefore,
using the construction of the T -step DBN, we have:


E R(sT , ~aT )  Rmin
P (r = 1|T ; ) =
(5)
Rmax  Rmin
232

fiProbabilistic Inference for Multiagent Decision Making

Substituting back the above result in the expression for L , we get:


X E R(sT , ~aT  Rmin

T
L = (1  )

Rmax  Rmin
=

T
)V 

(1 
 Rmin
Rmax  Rmin

(6)
(7)

where we have used linearity of expectation such that
X



X T

 T E R(sT , ~aT = E
 R(sT , ~aT

T

T

Combining the above result with the definition of the value function in Eq. (2), the theorem
is proved.
Using the above result, we establish the following result.
Lemma 1. Maximizing the likelihood L = P (r = 1; ) in the mixture of DBNs is equivalent
to optimizing the Dec-POMDP policy.
Theorem 1 and Lemma 1 show that the policy optimization problem can be reformulated
as a parameter learning problem in a suitable DBN mixture. This immediately suggests
using machine learning approaches to maximize the likelihood. Furthermore, this reformulation is lossless in the sense that optimizing the likelihood would exactly optimize the
joint-policy value. We note that we never explicitly create the mixture of DBNs. All the
computations on this DBN mixture can be implemented as message-passing over the original Dec-POMDP DDN of Figure 2. The only additional computational requirement is that
of scaling the rewards using Rmax and Rmin , which can be done easily in linear time.
We next present the Expectation-Maximization (EM), a well known technique to maximize the likelihood.

4. Policy Optimization via Expectation Maximization
This section describes our application of the EM algorithm (Dempster et al., 1977) for
maximizing the reward likelihood in the mixture of DBNs representing a Dec-POMDP.
EM also possesses the desirable anytime characteristic as the likelihood (and the policy
value which is proportional to the likelihood) is guaranteed to increase per iteration until
convergence. We note that EM is not guaranteed to converge to a global optimum. However,
in the experiments we show that EM almost always achieves similar values as the NLP based
solver to optimize FSCs (Amato et al., 2010) and much better than DEC-BPI (Bernstein
et al., 2009). Key potential advantages of using EM lie in its ability to easily generalize to
much richer representations than currently possible for Dec-POMDPs such as hierarchical
controllers (Toussaint et al., 2008), and continuous state and action spaces (Hoffman et al.,
2009b). Another important advantage is the ability to generalize the solver to larger multiagent systems with more than 2 agents by exploiting the relative independence among
agents, as we will show in later sections.
233

fiKumar, Zilberstein, & Toussaint

l(; x)

Q( 2 , )

Q( 1 , )

2
1 

Parameter space

Figure 4: The coordinate ascent strategy of the EM algorithm
4.1 Overview of the EM Algorithm
We first provide a high level overview of the EM algorithm followed by detailing its adaptation to planning in Dec-POMDPs. The EM algorithm is a general approach to the problem
of maximum likelihood (ML) parameter estimation in models with latent variables. In the
given latent variable model, let X denote the observable variables and Z denote the hidden
variables. Let  denote the model parameters. The ML problem is to solve the following
optimization problem:
X
max l(; x) = max log
p(x, z; )
(8)




z

It is hard to optimize the above problem as the summation is inside the log. Furthermore,
maximizing the log-likelihood l(; x) is generally a non-convex optimization problem as
shown in Figure 4. Therefore, the EM algorithm iteratively performs coordinate ascent in
the parameter space. First, the EM algorithm computes a lower bound for the function
l(; x) such that this lower bound touches l(; x), say at point 1 . This lower bound is
denoted as Q(1 , ), and is defined as:
X
X
Q(1 , ) =
p(z|x; 1 ) log p(x, z; ) 
p(z|x; 1 ) log p(z|x; 1 )
(9)
z

z

The last term in the above expression is the entropy of the variable Z|x. Figure 4 shows the
lower bound Q(1 , ) by a blue curve. The function Q(1 , ) is also called expected complete
log-likelihood. Importantly, the lower bound Q is concave in parameters  and thus, can be
optimized globally to provide a better parameter estimate 2 . This process is also shown
in Figure 4. Such coordinate ascent continues iteratively by defining a new lower bound
Q(2 , ) at the point 2 as also shown in Figure 4, and then optimizing it to yield the next
better parameter estimate until convergence.
To connect such an iterative maximization strategy with planning in Dec-POMDPs, we
note that the likelihood function is directly proportional to the joint-policy value for policy
234

fiProbabilistic Inference for Multiagent Decision Making

parameters  (see Theorem 1). Therefore, for adapting the EM approach to Dec-POMDPs,
we need to perform the following steps:
1. (E-step) Formulate the expected complete log-likelihood Q(i , ) for the DBN mixture
model in Figure 3 for iteration i
2. (M-step) Maximize Q(i , ) w.r.t.  to yield a better policy parameters i+1
3. Repeat steps 1 and 2 until convergence, i.e., until i  i+1
We next explain how steps 1 and 2 can be implemented specifically for Dec-POMDPs.
4.2 Step 1: Formulating the Expected Log-Likelihood Q(,  ? )
In the DBN mixture of Figure 3, every variable is hidden and the only observed data
is the binary reward variable r = 1. For a particular DBN for time step T , such as
the last DBN in the mixture of Figure 3, let L = (P, Q, A, B, S, Y, Z) denote the latent
variables for a T-step DBN, where each variable denotes a sequence of length T . That is,
P = p0:T , denotes the T -step long sequence of controller state pt of agent 1. EM maximizes
the following expected complete log-likelihood for the Dec-POMDP DBN mixture. Let 
denote the previous iterations parameters and  ? denotes new parameters. The expected
log-likelihood (ignoring the entropy term independent of ? ) is given as:
Q(,  ? ) =

 X
X

P (r = 1, L, T ; ) log P (r = 1, L, T ;  ? )

(10)

T =0 L

In the rest of the section, all the derivations refer to the general T -step DBN structure as
shown in Figure 3. We also adopt the convention that for any random variable v, v 0 refers
to the next time slice and v refers to the previous time slice. For any group of variables v,
Pt (v, v0 ) refers to P (vt = v, vt+1 = v0 ). The joint probability of all the variables is:
T
Y




P (r = 1, L, T ; ) = P (T ) Rsab t=T
ap bq Pssab Oyzsab ppy qqz t
t=1



 ap bq p q 0 (s) t=0

(11)



where brackets indicate the time slices, i.e., Rsab t=T = R(sT , aT , bT ). We also used the
shorthand Pssab = P (s|s, a, b); similarly for Oyzsab . Taking the log, we get:
log P (r = 1, L, T ) = . . . +

+

T
X
t=0
T
X

log at pt +

T
X

log bt qt

t=0

log pt pt1 yt +

t=1

T
X

log qt qt1 zt + log p0 + log q0

(12)

t=1

where the missing terms represent the quantities independent of , and they do not affect
the maximization of . As all the policy parameters h, , i get separated out for each
agent in the log above, the expression for expected log-likelihood can also be formulated
235

fiKumar, Zilberstein, & Toussaint

separately for action parameters, controller node transition parameters and initial node
distribution. For action parameters of an agent, we have the simplified expression as:
Qa,p (,  ? ) =


X

P (T )

T X
X

?
P (r = 1, at = a, pt = p|T ; ) log ap

(13)

t=0 a,p

T =0

P
The above expression was derived by substituting Tt=0 log at pt for log P (r = 1, L, T ;  ? )
in Eq. (10) and marginalizing out the remaining variables (See derivation in Appendix A).
Analogous expected log-likelihood expressions can be written for node transition  and
initial node distribution  parameters.
4.3 Step 2: Maximizing the Expected Log-Likelihood Q(,  ? )
As highlighted in section 4.1, once we have formulated the expected log-likelihood, the next
step is to maximize it to get a better policy ? . We show such a maximization first for
action updates. The expected log-likelihood for action updates is given as:
Qa,p (,  ? ) =


X

P (T )

T X
X

?
P (r = 1, at = a, pt = p|T ; ) log ap

(14)

t=0 a,p

T =0

The maximization step (M-step) involves solving the following convex optimization problem:
max Qa,p (,  ? )

(15)

? }
{ap

subject to:

X
a

?
ap
= 1 p

(16)

The above optimization problem can be easily solved analytically by solving for the KarushKuhn-Tucker (KKT) conditions (Boyd & Vandenberghe, 2004). The resulting update for
the controller action parameters is given as:
?
ap

=
=

P

T =0 P (T )

PT

t=0 P (r = 1, at = a, pt = p|T ; )

Cp
E [r = 1, a, p]
Cp

(17)
(18)

where Cp is a normalization constant.
All parameter updates: Analogous to the action updates above, we can write the controller transition  as well as initial node distribution updates  as follows:
E [r = 1, a, p]
Cp
E [r = 1, p, p, y]
?ppy =
Cpy
E [r = 1, p]
p? =
C
?
ap
=

236

(19)

fiProbabilistic Inference for Multiagent Decision Making

where we have:
E [r = 1, p, p, y] =
E [r = 1, p] =


X
T =1

X

P (T )

T
X

P (r = 1, pt = p, pt1 = p, yt = y|T ; )

(20)

t=1

P (T )P (r = 1, p0 = p|T ; )

(21)

T =0

Sections 4.2 and 4.3 summarize the two main steps that form the core of the EM algorithm. These two steps are applied iteratively until convergence and result in monotonic
improvement of solution quality.
4.4 Inference for Parameter Updates
We now detail the inference procedure that computes different expectations E required for
parameter updates shown in Eq. (19). Computing such expectations also forms the E-step of
the EM algorithm. In this step, for the fixed parameter , forward messages  and backward
messages  are propagated. Such forward-backward message passing is very similar to the
message passing in the Baum-Welch algorithm for parameter estimation in hidden Markov
models (Koller & Friedman, 2009). First, we define the following Markovian transitions on
the (p, q, s) state in a T -step DBN of Figure 3. These transitions are independent of the
time t due to the stationary joint-policy.
X
P (p0, q 0, s0 |p, q, s) =
p0 py0 q0 qz 0 Oy0 z 0 abs0 ap bq Ps0 sab
(22)
aby 0 z 0

Definition 1. The forward message t is defined to be the probability that the FSC of agent
1 is in state p, FSC of agent 2 is in state q and the world is in state s at time t
t (p, q, s) = P (pt = p, qt = q, st = s; ).
It might appear that we need to propagate  messages for each DBN in the DBN mixture
separately, but as pointed out by Toussaint et al. (2006), only one sweep is required as the
head of the DBN is shared among all the mixture components. That is, 2 is the same for
all the T-step DBNs with T  2. We will omit using  as long as it is unambiguous.
0 (p, q, s) = p q 0 (s)
X
t (p0 , q 0 , s0 ) =
P (p0 , q 0 , s0 |p, q, s)t1 (p, q, s)

(23)
(24)

p,q,s

The  messages are propagated backwards and are defined as Pt (r = 1|p, q, s; ). However,
this particular definition would require separate inference for each DBN as for T and T 0
step DBN, t will be different due to difference in the time-to-go (T  t and T 0  t). To
circumvent this problem,  messages are indexed backward in time and defined as follows.
Definition 2. The backward message  is defined to be the probability of variable r = 1
given that the FSC of agent 1 is in state p, FSC of agent 2 is in state q and the world is in
state s and there are  steps-to-go in a T -step DBN:
 (p, q, s) = P(r = 1|pT  = p, qT  = q, sT  = s)
237

fiKumar, Zilberstein, & Toussaint

where  = 0 denotes the last time slice t = T . As the tails of all the DBNs in the mixture
of Figure 3 is exactly the same, such backward indexing avoids performing separate inference
in each DBN. For example, 3 is same for all the DBNs with length T  3 as 3 computes
the probability of reward variable being one given the state of the Markov chain is (p, q, s)
when there are 3 more time steps to go. The recursive definition is as follows:
X
0 (p, q, s) =
Rsab ap bq
(25)
ab

X

 (p, q, s) =

p0 ,q 0 ,s0

 1 (p0 , q 0 , s0 )P (p0 , q 0 , s0 |p, q, s)

(26)

Based on the  and  messages we compute two more quantities:
(p, q, s) =


X

P (T = t)t (p, q, s)

(27)

P (T =  ) (p, q, s)

(28)

t=0

(p, q, s) =


X
 =0

These quantities are used in the M-step. The cut-off time for message propagation can either
be fixed a priori or be more flexible based on the likelihood accumulation. If  messages
are propagated for t-steps and -messages for  steps, then the likelihood for T = t +  is
given as:
X
Lt+ = P (r = 1|T = t +  ; ) =
P (r = 1, pt = p, qt = q, st = s|T = t +  )
(29)
p,q,s

=

X

P (r = 1|pt = p, qt = q, st = s, T = t +  )P (pt = p, qt = q, st = s|T = t +  )

(30)

t (p, q, s) (p, q, s)

(31)

p,q,s

=

X
p,q,s

P
T 
If both  and  messages are propagated for k steps and L2k  2k1
T =0  LT , then the
message propagation can be stopped. This criterion simply means that when the expected
value (or likelihood) for time step T = 2k is too small when compared to the already
accumulated value until time step 2k  1, then the message propagation can be stopped.
Once we have computed such  and  messages, we can compute the expectations required
for different updates in Section 4.3. The derivations of such updates are provided in the
Appendix A.
Definition 3. Based on computing quantities  and  in Eq. (27)- (28), the parameters of
an agents controller are updated after each EMs iteration as follows:
?
ap
=

X
ap X

(p, q, s)
Rsab bq +
Cp qs
1
b

X

0

0

0

(p , q , s )

p0 q 0 s 0 y 0 z 0

X
ppy X
(p, q, s)(p, q, s)qqz
Oyzsab Pssab ap bq
Cpy sqsqz
ab
p X
p? =
(p, q, s)q Ps 0 (s)
C qs

?ppy =

238

p0 py 0



q 0 qz 0

X
b


O

y 0 z 0 s0 ab

bq P

s0 sab

fiProbabilistic Inference for Multiagent Decision Making

4.5 Complexity
Calculating the Markov transitions on the (p, q, s) chain in Section 4.4 has the complexity
O(|Q|4 |S|2 |A|2 |Y |2 ), where |Q| is the maximum number of nodes for a controller. The
message propagation has complexity O(Tmax |Q|4 |S|2 ). Techniques to effectively reduce this
complexity without sacrificing accuracy will be discussed in the experiments section.
The complexity of updating all action parameters is O(|Q|4 |S|2 |A||Y |2 ). Updating node
transitions requires O(|Q|4 |S|2 |Y |2 + |Q|2 |S|2 |Y |2 |A|2 ). This is relatively high when compared to a single agent POMDP updates requiring O(|Q|2 |S|2 |A||Y |) mainly due to the
scale of the interactions present in Dec-POMDPs.
In our experimental settings, we observed that having a relatively small sized controller
(|Q|  5) suffices to yield good quality solutions. The main contributor to the complexity
is the factor S 2 as we experimented with large domains having nearly 250 states. The
good news is that the structure of the E and M-step equations provides a way to effectively
reduce this complexity by a significant factor without sacrificing accuracy. For a given state
s, joint-action ha, bi and joint-observation hy, zi, the possible next states can be calculated
as follows: succ(s, a, b, y, z) = {s0 |P (s0 |s, a, b)O(y, z|s0 , a, b) > 0}. For most of the problems,
the size of this set is typically a constant k < 10. Such simple reachability analysis and
other techniques speed up the EM algorithm by more than an order of magnitude for large
problems. The effective complexity reduces to O(|Q|4 |S||A||Y |2 k) for the action updates
and O(|Q|4 |S||Y |2 k + |Q|2 |S||Y |2 |A|2 k) for node transitions.
4.6 The Intuition Behind the EM Update Strategy
In this section, we provide some insights about the update strategies of EM. The action
parameter update according to Section 4.3 is given as:


E r = 1, a, p
?
ap =
(32)
Cp
where Cp is a normalization constant. To understand Eq. (32) more clearly, consider the
following iterative algorithm for optimizing the controller. First, we fix every controller
nodes parameters for every agent except for the parameters for a single controller node for
a particular agent. Now, we deterministically try every action setting for the particular node
and greedily set the action parameter for the node to be the action that results in maximum
joint value. Clearly, this strategy will monotonically improve the policy value until it reaches
a local optima. Such a strategy has been used in other decision making models such as
influence diagrams and is referred to as Single Policy Update (SPU) algorithm (Lauritzen
& Nilsson, 2001).
The updates of the EM algorithm are essentially a soft version of the above greedy and
deterministic rule. To understand this, let a? denote the action with maximum expectation:


a? = arg max E r = 1, a, p
(33)
aA

Now consider applying the update rule of Eq. (32) infinitely many times without recomputing the E-step. Clearly in the limit, we will have a?? p = 1 and the rest of the action
parameters will converge to zero. This is essentially the SPU algorithm.
239

fiKumar, Zilberstein, & Toussaint

The above connection also provides insight as to why the soft-max approach of EM
may be a better strategy than the greedy deterministic update rule. First, the greedy
update rule computes deterministic controllers for both the agents. It has been already
shown that stochastic controllers can achieve better solution quality than deterministic
controllers (Poupart & Boutilier, 2003). EM updates can provide stochastic controllers,
which is an advantage. Second, it has been already known in the graphical models community that the greedy update rule, also referred to as Hard-Assignment EM (Koller &
Friedman, 2009, ch. 19) and the EM algorithm optimize different objectives. They can in
general lead to different solutions, for example, in situations when stochastic controllers are
preferred to deterministic ones. The hard-assignment EM traverses a combinatorial path
and needs to fix all the parameters except one. The soft-assignment EM, on the other
hand, can simultaneously change the parameters of multiple nodes. Thus, the moves in the
parameter space of the soft-assignment EM are more sophisticated and in general, infeasible for the hard-assignment EM, which cannot simultaneously change multiple parameters.
Thus, soft-assignment EM can converge to a better policy. Therefore, using the soft-max
based update strategy of the EM algorithm can be more advantageous than the greedy
deterministic rule.
4.7 Discussion
We presented a new approach to solve Dec-POMDPs using inference in a mixture of DBNs.
The main benefit of the EM approach is that it opens up the possibility of using powerful
probabilistic inference techniques to solve decentralized planning problems. Using a graphical DBN structure, EM can easily extend to larger multi-agent systems with more than 2
agents, as will be shown in the following sections. Furthermore, the planning-as-inference
viewpoint can help to generalize to richer representations such as factored or hierarchical controllers (Toussaint et al., 2008), or continuous state and action spaces (Hoffman,
de Freitas, Doucet, & Peters, 2009a).
Incidentally, there is an increasing interest in applying variational inference techniques
from the machine learning and graphical models literature, such as marginal MAP inference
and belief propagation (Liu & Ihler, 2013, 2012), to planning under uncertainty. Particularly
related to our work is the use of such marginal MAP (MMAP) based inference to single agent
POMDP planning (Kiselev & Poupart, 2014a, 2014b). In such MMAP based planning, a
single dynamic Bayesian network is developed by introducing additional binary variables
similar to the binary reward variable r in our case. Once a single graphical model is
developed for the POMDP model, it is shown that maximizing the likelihood of observing
one such special binary variable is equal to optimizing the joint policy.
The key difference between our approach and that of Kiselev and Poupart is the use
of different graphical models to represent the underlying planning problem. The approach
of Kiselev and Poupart uses two additional binary variables per time step that represent the
reward discounting and the value function, which our approach does not need. Furthermore,
in our approach, the time indexing of backward message propagation does not need to be
fixed a priori as shown in Section 4.4, whereas a fixed horizon of the DBN needs to prespecified in Kiselev and Pouparts (2014b) method. Nonetheless, using MMAP inference
for planning represents another variational inference-based approach that can be applied to
240

fiProbabilistic Inference for Multiagent Decision Making

planning under uncertainty problems. Similar to the development of the EM in our case,
one can also develop an EM algorithm for policy optimization in the single DBN model
of Kiselev and Poupart. It remains to be seen, however, how the approach of Kiselev and
Poupart (2014a, 2014b) gets translated to the multiagent setting w.r.t. to scalability and
solution quality.

5. Achieving Scalability Under Restricted Models
In previous sections, we developed probabilistic inference based approximate algorithms
that can efficiently solve 2-agent Dec-POMDPs. However, scaling even such approximate
algorithms for more than 2-agents is a non-trivial task. In fact, a naive extension leads to
exponential increase in complexity with the number of agents. Therefore, in the following
sections, we present a general characterization of interactions among agents that when
present in a multiagent planning model leads to relatively scalable approximate algorithms.
In this section, we identify conditions that are sufficient to make multiagent planning
amenable to a scalable approximation w.r.t. the number of agents. This is achieved by
constructing a graphical model that exploits such restricted interactions among agents.
We again illustrate a close relationship with machine learning by showing that likelihood
maximization in such a graphical model is equivalent to policy optimization. Using the
Expectation-Maximization framework for likelihood maximization, we show that the necessary inference can be decomposed into processes that often involve a small subset of agents,
thereby facilitating scalability. We derive a global update rule that combines these local
inferences to monotonically increase the overall solution quality. Furthermore, our approach
is easily parallelizable and takes the form of message-passing among agents, ideally suited
for large multiagent systems.
Experiments on a large multiagent planning benchmark, with joint state and action
spaces up to 522 , 320 respectively, confirm that our approach is scalable w.r.t. the number
of agents and can provide good quality solutions for planning problems for up to 20 agents,
which cannot be handled by previous best approaches. For smaller multiagent systems, our
approach provides better solution quality and is about an order-of-magnitude faster than
the previous best nonlinear programming approach for optimizing FSCs.
5.1 The Value Factorization Framework
The value factorization framework leads to efficient inference for large multiagent systems
and is general enough to subsume several existing planning models. As before, we represent
each agents policy as a bounded, finite state controller (FSC). We assume that the state
space S is factored s.t. S = (S 1  ...  S M )a common assumption in multiagent planning
models such as ND-POMDPs (Nair et al., 2005) and TD-POMDPs (Witwicki & Durfee,
2010). Without making further (conditional independence) assumptions on the problem
structure, a general Dec-POMDP requires exact inference in the full corresponding (finitetime) DBNs, which would be exponential in the number of state variables and agents. Our
approach relies on a general simplifying property of agent interaction, which we later show
to be consistent with many of the existing multiagent planning models.
241

fi s
f

f F

s#f

r
f F


q!

t
a

Kumar, Zilberstein, & Toussaint
q!

t

y!

y!

a

ai

qi#

qi

s#i

si

ai
i  Af

si

s#i

x

#

r
f F

1

yi#

x

(a)

(b)

i  Af

2
r f F

3

4

(c)

Figure 5: Value factorization property in different models: (a) Plate notation for transition
independent Dec-MDPs; (b) Plate notation for ND-POMDPs; (c) Agent interaction digraph for TD-POMDPs

Definition 4. A value factor f defines a subset Af {1, .., N } of agents and a subset
Sf  {1, .., M } of state variables.
Definition 5. A multiagent planning problem satisfies value factorization if the jointpolicy value function can be decomposed into a sum over value factors:
X
V (, s) =
Vf (f , sf ) ,
(34)
f F

where F is a set of value factors, f  Af is the collection of parameters of the agents of
factor f , and sf  sSf is the collection of state variables of this factor.
Even when the value factorization property holds, planning in such models is still highly
coupled because factors may overlap. That is, an agent can appear in multiple factors as
can state variables. Therefore, a value factor cannot be optimized independently. But, as we
show later, it leads to an efficient Expectation Maximization algorithm. Such additive value
functions have also been used to solve large factored MDPs (Koller & Parr, 1999). Witwicki
and Durfee (2011) use such factored value functions to analyze the complexity of solving
multiagent planning problems. Our work, in contrast, uses such value factorization property in conjunction with graphical models and probabilistic inference to develop a scalable
likelihood maximization based algorithm. We require that each value factor Vf can be evaluated using the DBN mixture based approach of Section 3. However, this does not limit
generality, as the DBN-based approach can model arbitrary Markovian planning problems.
Theorem 2. The worst case complexity of optimally solving a multiagent planning problem
satisfying the value factorization property is NEXP-Hard.
The proof of the above theorem is straightforwardany two agent finite-horizon DecPOMDP is NEXP-Complete and also satisfies the value factorization property as there is
only a single factor involving two agents. In fact, in previous sections we precisely addressed
this issue using the EM framework (see Section 4). Next, we investigate when this property
holds and when it is computationally advantageous, and establish the following result.
242

fiProbabilistic Inference for Multiagent Decision Making

Theorem 3. The value factorization property holds in Transition-Independent Dec-MDPs
(Becker et al., 2004), Network-Distributed POMDPs (Nair et al., 2005) and TransitionDecoupled POMDPs (Witwicki & Durfee, 2010).
Proof. The joint value is shown to be factorized based on the immediate-reward factorization
in transition independent Dec-MDPs (Becker et al., 2004) and ND-POMDPs (Nair et al.,
2005). Figure 5 shows the plate notation for our value factor representation for both of
these models. The outer plate shows a factor f and the inner plate depicts the interaction
among agent parameters which include state, action and observation variables. In both these
models, a key assumption that leads to scalability is that only a few agents are involved in
a single reward function. Thus, each value factor is small and leads to efficient inference.
Our approach can also model Transition-Decoupled POMDPs (TD-POMDPs) (Witwicki
& Durfee, 2010). In this case, agents have local parameters (factored local state and rewards). However, certain features of the local state can depend on other agents actions.
Such features are called nonlocal features for an agent i. This dependency among agents
is described using an agent interaction digraph (Witwicki, 2011, Section 3.5.1.2). There is
a node for each agent i. There is a directed edge from node i to node j if agent i affects
a nonlocal feature of agent j. Let (i) denote all the ancestors of a node i in the agent
interaction digraph for a TD-POMDP. As shown by Witwicki (2011, Thm. 3.33), the joint
value function for a TD-POMDP defined over N agents can be factored as:
V () =

N
X
i=1

Vi i , hj | j  (i)i



(35)

The state variables involved in each factor Vi are the local states for each agent in {i}(i).
Furthermore, each value factor Vi can be evaluated by constructing a DBN mixture involving
only the agents {i}(i). Therefore, the TD-POMDP model satisfies the value factorization
property. Consider for example the agent interaction digraph in Figure 5(c). The joint value
factorizes as V () = V1 (1 ) + V2 (2 , 1 ) + V3 (3 , 1 ) + V4 (4 , 1 , 3 ).
Furthermore, TD-POMDPs are mainly useful for weakly-coupled planning problems
(Witwicki, 2011). This implies that the number of agents involved in a single value factor
should be small compared to the total number of agents, potentially leading to computational savings in approaches that can exploit the structure of such smaller value factors.
We note that the value factorization property of Eq. (34) is trivially satisfied when all
the agent and state variables are included in a single factor. Obviously, the computational
advantages of our approach are limited to settings where each factor is sparse, involving
much fewer agents than the entire team. This allows for efficient inference in the respective
DBNs (inference can still be efficient for special cases such as TD-POMDPs that have larger
factors). In the general case, the additive value function may include components depending
on all states and agent parameters. This is analogous to the factored HMMs (Ghahramani
& Jordan, 1995) where, conditioned on the observations, all Markov chains become coupled
and the exact E-step of EM becomes infeasible. While this is beyond the scope of this paper, a promising approach for the general case is using variational methods to approximate
the posterior P (s1:M
1:T | r = 1) (minimizing the KL-divergence between the factored representation and the true posterior) (Ghahramani & Jordan, 1995). Given such an approximate
243

fiKumar, Zilberstein, & Toussaint

posterior, the M-step updates can be used to realize an approximate EM scheme, as also
shown by Pajarinen and Peltonen (2011a).
In the next section, we describe a new DBN mixture model for the value factorization
framework. Before that, we highlight the key result behind the scalability of the EM
algorithm
Theorem 4. In models satisfying the value factorization property, the inferences required
for the E-step of the EM algorithm can be performed independently for each value factor f .
For e.g., for action updates for an agent j, we have
X f
E [r = 1, aj , q j ]
E [r = 1, aj , q j ] =
f F (j)

where j denotes a particular agent, F (j) denotes the set of value factors the agent j is
involved in, (aj , q j ) denote the action and controller state of agent j, Ef [] (to be defined
precisely in Section 5.4.1) denotes inference only in the DBN mixture corresponding to the
valued factor f . A constructive proof of the above result is provided below. This result
highlights the generality and scalability of our approach, whichunlike previous works
does not require any further independence assumptions.
In the next section, we define a latent variable model, again based on a mixture of
DBNs, such that likelihood maximization (LM) in such a mixture model is equivalent to
joint-policy optimization. Once we have established such a relationship between LM and
policy optimization, we show how to perform different steps of EM in this mixture model
as outlined in section 4.1.
5.2 DBN Mixture for Value Factors
Figure 6 shows a new problem-independent DBN mixture, also called value factor mixture,
which models Eq. (34). It consists of two mixture variables: F and T . F ranges from 1
to |F |, the total number of value factors. Intuitively, F = i denotes the time dependent
DBN mixture for value factor i. A zoomed-in view of this DBN mixture is provided in
Figure 6(b). The mixture variable F has a fixed, uniform distribution (= 1/|F |). The
distribution of the variable T is set as in Theorem 1.
This model relies on the fact that in our representation, each value factor can be represented and evaluated using a time dependent DBN mixture of Figure 6(b) and the binary
reward variable r, as also shown in the Section 3. This DBN mixture for a particular value
factor f will contain all the variables for agents involved in factor f : actions, controller
nodes and observations, and the state variables Sf . The valuation Vf (f ) can be calculated by finding the likelihood Lf (f ; r = 1) = P (r = 1; f ) of observing the binary reward
variable as r = 1 in the DBN mixture for value factor f . Using Theorem 1, we have the
following result:
Vf (f ) = kLf (f ; r = 1) + kf

(36)

where k and kf are constants, with k being the same for all value factors. This can be easily
ensured by making all the original rewards positive by adding a suitable constant. Next we
state one of our main results. We are also assuming that the policy is being optimized for
244

fiProbabilistic Inference for Multiagent Decision Making

x0

y0
r

x0

y0

x1

y1
r

F =1

F =2

F = |F |

Figure 6: (a) Value factor mixture; (b) Zoomed-in view of each mixture component (x, y
are generic placeholders for random variables).

an initial belief 0 . We will also use a notational convenience that
P
f F .

P|F |

F =1

is equivalent to

Theorem 5. Maximizing the likelihood L(; r = 1) of observing r = 1 in the value factor
mixture (Figure 6(a)) is equivalent to optimizing the global policy .
Proof. The overall likelihood is given by:
L(; r = 1) = P (r = 1; ) =

X 1
Lf (f ; r = 1)
|F |

(37)

f F

The theorem follows by substituting the value of each L(f ; r = 1) in the previous equation
from Eq. (36) and the joint-policy value decomposition assumption of Eq. (34).
5.3 Step 1: Formulating the Expected Log-Likelihood Q(,  ? )
We now formulate the expected log-likelihood Q(, ? ) for the DBN mixture of Figure 6(a).
Only r = 1 is the observed data, the rest of the variables are latent. Note that our derivations
differ markedly by Toussaint and Storkey (2006) as they focus on a single-agent problem or
from the EM approach for 2-agent Dec-POMDPs (see Section 4). Our focus is on scalability
w.r.t. the number of agents and generality.
An assignment to the mixture variables T and F = f denotes a T-step DBN for the
value factor f . For example, assume that the time dependent DBN mixture in Figure 6(b)
is for value factor f . Then F = f and T = 1 denote the 1-step DBN (the second DBN) in
Figure 6(b). Let Z f denote a complete assignment to all the variables from slice 0 to T in
the T-step DBN for factor f . We assume for simplicity that each value factor f involves k
agents. The full joint for the mixture is:
k Y
T
k Y
T
Y



 Y


P (r = 1, Z f , T, F = f ) = P (T )P (F = f ) Rsf af t=T
fi (a, q) t
fi (q, q, y) t
i=1 t=0

k
Y
i=1

[fi (q)]0 P (Z f \(Af , Qf )|T, F = f )
245

i=1 t=1

(38)

fiKumar, Zilberstein, & Toussaint

where the index fi denotes the respective parameters for agent i involved in factor f . The
square brackets denote dependence upon time: [fi (a, q)]t = fi (at = a, qt = q). We also use
[P (v, v)]t to denote P (vt = v, vt1 = v).
Let Z f \(Af , Qf ) denote all the variables in this DBN except the action and controller
nodes of all the agents. Importantly, the structure of this previous equation is model independent as by the conditional independence of policy parameters ((a, q) = P (a|q)), the
first part of the equation (the product terms) can always be written this way. By model independent, we imply that the structure of previous equation remains the same for different
models with value factorization property. Since EM maximizes the expected log-likelihood,
we take the log of the above to get:
log P (r = 1, Z f , T, F = f ) =

k X
T
X


k X
T
X



log fi (a, q) t +
log fi (q, q, y) t

i=1 t=0

i=1 t=1

+

k
X

i=1


log fi (q) 0 + hother termsi

(39)

where hother termsi denote terms independent of policy parameters . EM maximizes the
following expected log-likelihood:
?

Q(,  ) =

 X
XX
f F T =0 Z f

?

P (r = 1, Z f , T, F = f ; f ) log P (r = 1, Z f , T, F = f ; f )

(40)

where  denotes the previous iterations joint-policy and  ? is the current iterations policy
to be determined by maximization. The structure of the log term (Eq. (39)) is particularly
advantageous as it allows us to perform maximization for each parameter of each agent
independently. This does not imply complete problem decoupling as all the parameters still
depend on the previous iterations parameters for all other agents.
5.4 Step 2: Maximizing the Expected Log-Likelihood Q(,  ? )
We first derive the update for action parameters of an agent j. P (F) can be ignored as it
is a constant. The expected log-likelihood for action updates, Qja,q (,  ? ), is given by:
Qja,q (,  ? )

=

X X

P (T )

f F (j) T

=

X X
f F (j) T

X
Zf

P (T )

X

T


?
P (r = 1, Z |T, f ;  )
log j (a, q) t
f

f

(41)

t=0

T X
X
t=0 a,q

P (r = 1, at = a, qt = q, |T, f ; f ) log j? (a, q)

(42)

where F (j) is the set of value factors that involve agent j. The M-step involves solving the
following convex optimization problem:
max

Qja,q (,  ? )

(43)

X

j? (a, q) = 1 q

(44)

{j? (a,q) a,q}

subject to:

a

246

fiProbabilistic Inference for Multiagent Decision Making

f12

f23

f34

f12


Ag1

f23

f34


Ag2

Ag3

Ag4

Ag1

(a)

Ag2

Ag3

Ag4

(b)

Figure 7: Message passing on the value factor graph: (a) shows the message direction for
E-step; (b) shows the M-step.

The above optimization problem can be easily solved analytically by solving for the KarushKuhn-Tucker (KKT) conditions resulting in following action parameter updates:
P
P
PT
f
f F (j)
T =0 P (T )
t=0 P (r = 1, at = a, qt = q|T, f ;  )
?
j (a, q) =
(45)
Cq
P
f
f F (j) E [r = 1, a, q]
=
(46)
Cq
where Cq is the normalization constant.
5.4.1 All Parameter Updates
Analogous to the action updates in the previous section, we can write the controller
sition  as well as initial node distribution updates  for the agent j as follows:
P
f
f F (j) E [r = 1, a, q]
?
j (a, q) =
Cq
P
f
f F (j) E [r = 1, q, q, y]
?j (q, q, y) =
Cqy
P
f
f F (j) E [r = 1, q]
j? (q) =
C

tran-

(47)
(48)
(49)

We have omitted the superscript j above to denote the action and controller variable for
agent j to avoid clutter. In the next two equations too, all action, controller variables belong
to agent j.
Ef [r

= 1, q, q, y] =
Ef [r = 1, q] =


X
T =1

X

P (T )

T
X

P (r = 1, qt = q, qt1 = q, yt = y|T, f ; f )

(50)

t=1

P (T )P (r = 1, q0 = q|T, f ; f )

(51)

T =0

5.5 Scalability and Message Passing Implementation
The parameter updates in Section 5.4.1 highlight the high scalability of EM for DecPOMDPs. Even though the planning problem is highly coupled with each agent and state
247

fiKumar, Zilberstein, & Toussaint

variables allowed to participate in multiple value factors (see Eq. (34)), updating policy
parameters requires separate local inference, Ef [r = 1, , ], in each value factor.
The global update rules (Eq. (47)(49)) combine such local inferences to provide a
monotonic increase in the overall solution quality. Each local inference is model dependent
and can be computed using standard probabilistic techniques. As our problem setting
includes sparse factors, such local inference will be computationally much simpler than
performing it on the complete planning model.
Furthermore, the E and M-steps can be implemented using a parallel, distributed messagepassing on a bipartite value-factor graph G = (U, V, E). The set U contains a node uj for
each agent j. The set V contains a node vf for each factor f  F . An edge e = (uj , vf )
is created if agent j is involved in factor f . Figure 7 shows such a graph with three value
factors in the black squares (the set V ) and 4 agents (the set U ).
During the E-step, each factor node vf computes and sends the message f j = Ef [r =
1, , ] to each node uj that is connected to vf . Figure 7(a) shows this step. An agent node
uj upon receiving all the  messages from each factor node connected to it, updates its
parameters as in Eq. (47)(49) and sends the updated policy parameters ? back to each
factor node it is connected to (see Figure 7(b)). This procedure repeats until convergence.
Based on this message-passing interpretation of EM, we can state the following result:
Theorem 6. Each iteration of the EM algorithm has linear complexity in the number of
edges in the value factor graph and exponential complexity with respect to the maximum
number of agents involved in a single value factor.
As stated earlier, when the value factors are sparse, EM algorithm will provide significant
computational savings over an approach that is oblivious to the underlying interaction
among agents. Another significant advantage of the EM algorithm is that all messages
can be computed in parallel by each factor node. Our experiments, using an 8-core CPU
resulted in near linear speedup over a sequential version. These characteristics of the EM
algorithm significantly enhance its scalability for large, but sparse planning problems.
5.5.1 Nature of Local Optima
Variants of the Dec-POMDP model are often solved by fixing the policies of a group of agents
and then finding the best response policy of the agent that interacts with the group (Nair,
Tambe, Yokoo, Pynadath, Marsella, Nair, & Tambe, 2003; Witwicki & Durfee, 2010). EM
offers significant advantages over such strategy. While both find local optima, EM is more
stringent and satisfies the Karush-Kuhn-Tucker conditions (Bertsekas, 1999), which is the
norm in nonlinear optimization. The local optima of the EM refers to the stationary point
of the likelihood function (or the value function) l(; x) in Figure 4. The parameter space
 in this figure includes both the discrete parameters that can be found by local optimal
algorithms such as any best response strategy and the continuous parameters that are found
by algorithms such as EM.
The local optima of any best response strategy simply refers to the fact that the particular algorithm cannot improve the policy using the best-response strategy. Such algorithm
specific local optima may or not be the stationary point of the value function in Figure 4.
If such a solution is not the stationary point, then EM would be able to improve upon
248

fiProbabilistic Inference for Multiagent Decision Making

this solution given that EM is guaranteed to converge to a stationary point of the value
function. Thus, the local optima provided by EM satisfies the more stringent guarantee of
being a stationary point of the value function. The best-response strategy provides no such
guarantees.
5.6 Discussion
Despite rapid progress in multiagent planning, the scalability of the prevailing formal models
has been limited. We developed a new approach to multiagent planning by identifying
the general property of value factorization that facilitates the development of a scalable
approximate algorithm using probabilistic inference. We show that several existing classes
of Dec-POMDPs satisfy this property. In contrast to previous approaches, our framework
does not impose any further restrictions on agent interaction beyond this property, thus
providing a general solution for value factorization based multiagent planning.
The key result that supports the scalability of our approach is that, within the EM
framework, the inference process can be decomposed into separate components that are
much smaller than the complete model, thus avoiding an exponential complexity in the
number of agents. Additionally, the EM algorithm allows for distributed planning using
message-passing along the edges of the value-factor graph, and is amenable to parallelization. Results on large sensor network problems confirm the scalability of our approach.
Empirically, our approach, which has linear complexity in the number of edges in the agent
interaction graph could scale up to 20 agents, whereas the previous best approach based
on nonlinear programming could only scale up to 5 agents due to increase in the number of
nonlinear constraints.
We also highlight the key differences in our approach against the previous approach
of Toussaint et al. (2006, 2008) for single-agent MDPs and POMDPs. Although the idea
of decomposing the planning problem into a time-dependent DBN mixture remains the
same in our approach, the key differences lie in the structure of the DBNs for two agent
Dec-POMDPs and value factored Dec-POMDPs, the derivation of the lower bound function
Q(, ? ) and the maximization of this Q function for Dec-POMDPs as required within the
EM framework (see Section 4.1), and computing the required inferences over the underlying
Markov chain of DBNs as shown in Section 4.4.
The interactions between the FSC-based policy and the environment present in the
DBN for MDPs and POMDPs are relatively simple when compared against the interactions
present in a Dec-POMDPs. In Dec-POMDPs, agents not only interact with the environment, but also with each other. Such inter-agent interactions leads to a much different
DBN structure, and planning challenges in Dec-POMDPs. For example, the DBN mixture for our value factorization model is nested as shown in Figure 6. This is a unique
feature required in our formulation of the DBN approach, and not present in POMDPs.
Due to such differences in the structure of a single DBN as well as the mixture model, our
adaptation of EM requires different formulation of alpha and beta messages to account for
inter-agent influences as shown in Section 4.4. Similarly, the structure of Q(, ) function
is different owing to inter-agent influences (see Eq. (40)), and leads to different updates
that provide scalability and inter-agent message-passing structure in multiagent systems as
shown in Sections 5.4.1 and 5.5. Therefore, while adapting the EM approach to multiagent
249

fiKumar, Zilberstein, & Toussaint

Size
1
2
3
4

DEC-BPI
4.687
4.068
8.637
7.857

NLP
9.1
9.1
9.1
9.1

EM
9.05
9.05
9.05
9.05

DEC-BPI
< 1s
< 1s
2s
5s

EM
< 1s
< 1s
1.7s
4.62s

Table 1: Broadcast channel: Policy value, execution time

planning, one of our key contributions has been to deliberately investigate and exploit the
independencies that are present in a multiagent system to translate into a scalable algorithmic structure. Furthermore, we have also shown general applicability of our approach to
previous multiagent planning models in Theorem 3.

6. Empirical Evaluation
We begin the empirical evaluation with experiments conducted with 2-agent problems. This
is followed by experiments with larger problems involving up to 20 agents.
6.1 Two Agents Dec-POMDPs
We experimented with several standard 2-agent Dec-POMDP benchmarks with discounting factor  = 0.9. We compare our EM algorithm with the decentralized bounded policy
iteration (DEC-BPI) algorithm (Bernstein et al., 2009) and a non-linear, non-convex optimization solver (NLP) (Amato et al., 2010). The DEC-BPI algorithm iteratively improves
the parameters of a node using a linear program while keeping the other nodes parameters fixed. The NLP approach, which has been the state-of-the-art for infinite-horizon
Dec-POMDPs, recasts the policy optimization problem as a non-linear program and uses
an off-the-shelf solver, Snopt (Gill et al., 2002), to obtain a solution. We implemented the
EM algorithm in JAVA. All our experiments were on a Mac with 4GB RAM and 2.4GHz
CPU. Each data point for every algorithm tested and parameter setting is an average of 10
runs with random initial controller parameters. In terms of solution quality, EM is always
better than DEC-BPI and it achieves similar or higher solution quality than NLP. We note
that the NLP solver (Gill et al., 2002) is an optimized package and therefore for larger
two agent problems is currently faster than the EM approach. For the EM algorithm, we
did not implement optimizations such as parallel execution using multithreading, that can
decrease the runtime significantly.
Table 1 shows results for the broadcast channel problem, which has 4 states, 2 actions per
agent and 5 observations. This is a networking problem where agents must decide whether
or not to send a message on a shared channel and must avoid collision to get a reward. We
tested with different controller sizes shown in the first column. On this problem, all the
algorithms compare reasonably well, with EM being better than DEC-BPI and very close
in value to NLP. The time for NLP is also  1s.
Figure 8(a) compares the solution quality of the EM approach against DEC-BPI and
NLP for varying controller sizes on the recycling robots problem. In this problem, two robots
250

fi65

8

60

7

55

6

50

Time (sec)

Policy Value

Probabilistic Inference for Multiagent Decision Making

45
40
EM(2)
EM(4)
NLP(2)
NLP(4)
DEC-BPI(2)
DEC-BPI(4)

35
30
25
20
0

50

100

150 200
Iteration

250

5
4
3
2
1

300

0
350

EM(2)
EM(4)
0

50

100

(a) |S| = 3, |A| = 3, |Y | = 2

150
200
Iteration

250

300

350

300

350

(b)

8

60

7

50

5

Time (sec)

Policy Value

6

4
3
2

EM(2)
EM(3)
NLP(2)
NLP(3)

1
0
0

50

100

150
200
Iteration

250

40
30
20
10

300

0
350

(c) |S| = 16, |A| = 5, |Y | = 4

EM(2)
EM(3)
0

50

100

150 200
Iteration

250

(d)

Figure 8: Solution quality and runtime for recycling robots (a) & (b) and meeting on a
grid (c) & (d)

have the task of picking up cans in an office building. They can search for a small can, a
big can or recharge the battery. The large item is only retrievable by the joint action of the
two robots. Their goal is to coordinate their actions to maximize the joint reward. EM(2)
and NLP(2) show the results with controller size 2 for both agents in Figure 8(a). For this
problem, EM works much better than both DEC-BPI and the NLP approach. EM achieves
a value of  62 for all controller sizes, providing nearly 12% improvement over DEC-BPI
(= 55) and 20% improvement over NLP (= 51). Figure 8(b) shows the time comparisons
for EM with different controller sizes. Both the NLP and DEC-BPI take nearly 1s to
converge. EM with controller size 2 has comparable performance, but as expected, EM
with 4-node controllers takes longer as the complexity of EM is proportional to O(|P|4 ),
where |P| denotes the controller size.
Figure 8(c) compares the solution quality of EM on the meeting on a grid problem.
In this problem, agents start diagonally across in a 2  2 grid and their goal is to take
actions such that they meet each other (i.e., share the same square) as much as possible.
As the figure shows, EM provides much better solution quality than the NLP approach. EM
achieves a value of  7, which nearly doubles the solution quality achieved by NLP (= 3.3).
DEC-BPI results are not plotted as it performs much worse and achieves a solution quality
of 0, essentially unable to improve the policy at all even for large controllers. Both DECBPI and NLP take around 1s to converge. Figure 8(d) shows the time comparison for EM
versions. EM with 2-node controllers is very fast and takes < 1s to converge (50 iterations).
251

fiKumar, Zilberstein, & Toussaint

0

0.85

-10

0.75
Likelihood

Policy Value

0.8

-20
-30

EM(2)
EM(4)
EM(10)
NLP(2)
NLP(5)
NLP(10)

-40
-50
0

100

200
Iteration

300

0.7
0.65
0.6
0.55
0.5
0.45

400

(a) |S| = 2, |A| = 3, |Y | = 2

0.4

EM(2)
EM(10)
0

100

200
Iteration

300

400

(b)

Figure 9: Solution quality (a) and likelihood (b) for multiagent tiger

Again, because of EMs quartic complexity in the controller size |P|, the time required for
larger controllers is higher. Also note that in both the cases, EM could run with much
larger controller sizes ( 10), but the increase in size did not provide tangible improvement
in solution quality.
Figure 9 shows the results for the multi-agent tiger problem, involving two doors with
a tiger behind one door and a treasure behind the other. Agents should coordinate to
open the door leading to the treasure (Amato et al., 2010). Figure 9(a) shows the quality
comparisons. EM does not perform well in this case; even after increasing the controller
size, it achieves a value of 19. NLP works better with large controller sizes. However, this
experiment presents an interesting insight into the workings of EM as related to the scaling
of the rewards. Recalling the relation between the likelihood and the policy value from
Theorem 1, the equation for this problem is: V  = 1210L  1004.5. For EM to achieve the
same solution as the best NLP setting (= 3), the likelihood should be .827. Figure 9(b)
shows that the likelihood EM converges to is .813. Therefore, from EMs perspective, it
is finding a really good solution. Thus, the scaling of rewards has a significant impact (in
this case, adverse) on the policy value. This is a potential drawback of the EM approach,
which applies to other Markovian planning problems when using the technique of Toussaint
et al. (2006). Incidently, DEC-BPI performs much worse and gets a quality of 77.
Figure 10 shows the results for the two largest Dec-POMDP domainsBox pushing and
Mars rovers. In the box pushing domain, agents need to coordinate and push boxes into a
goal area. In the Mars rovers domain, agents need to coordinate their actions to perform
experiments at multiple sites. Figure 10(a) shows that EM performs much better than DECBPI for every controller size. For controller size 2, EM achieves better quality than NLP
with comparable runtime (Figure 10(b), 500 iterations). However, for the larger controller
size (= 3), it achieves slightly lower quality than NLP. For the largest Mars rovers domain
(Figure 10(c)), EM achieves better solution quality (= 9.9) than NLP (= 8.1). However,
EM also takes many more iterations to converge than for previous problems and hence,
requires more time than NLP. EM is also much better than DEC-BPI, which achieves a
quality of 1.18 and takes even longer to converge (Figure 10(d)). For this Mars rover
domain, the NLP solver was not able to run for larger controller sizes due to the size of the
nonlinear program.
252

fiProbabilistic Inference for Multiagent Decision Making

50

10000

Time (sec, logscale)

40

Policy Value

30
20
10
0

EM(2)
EM(3)
NLP(2)
NLP(3)
DEC-BPI(2)
DEC-BPI(3)

-10
-20
-30
0

200

400

1000
100
10

0.1

600 800 1000 1200 1400
Iteration

EM(2)
EM(3)
NLP(2)
NLP(3)
DEC-BPI(2)

1

0

200 400

(a) |S| = 100, |A| = 4, |Y | = 5

(b)
100000

10
Time (sec, logscale)

Policy Value

5
0
-5
-10
EM(2)
NLP(2)
DEC-BPI(2)

-15
-20

600 800 1000 1200 1400
Iteration

0

1000

2000
3000
Iteration

4000

10000
1000
100
10
1

5000

(c) |S| = 256, |A| = 6, |Y | = 8

EM(2)
NLP(2)
DEC-BPI(2)
0

1000

2000
3000
Iteration

4000

5000

(d)

Figure 10: Solution quality and runtime for box pushing (a) & (b) and Mars rovers
(c) & (d)

To summarise, a simple implementation of the EM approach was competitive with an
industrial strength off-the-shelf nonlinear programming solver. Our algorithm provided
similar or better solution quality than the current best NLP approach. The main benefit
of the EM approach lies in the fact that it opens up the possibility of using powerful
probabilistic inference techniques to solve decentralised planning problems. As we shall see
in the next section, EM scales significantly better than the NLP approach for larger ( 2)
multiagent benchmarks where the NLP approach fails due to the large size of the resulting
nonlinear programs.
6.2 Larger Multiagent Benchmarks
We experimented on a target tracking application in sensor networks modeled as an NDPOMDP (Nair et al., 2005). Figure 11 shows four sensor topologies: 5P, 11H and 15-3D
by Marecki et al. (2008) and the largest 20D by Kumar and Zilberstein (2009b).
We describe and develop a significantly enriched variant of this application to better
test our approach, originally introduced by Nair et al. (2005). Each node in these graphs
is a sensor agent and edges are locations where targets can move. To track a target and
gain reward (= +80), two sensors must scan the target location simultaneously, otherwise a
penalty (= 1) is given. All targets have independent, stochastic trajectories and their all
possible locations form the external state-space, implying target movement is not affected
253

fiKumar, Zilberstein, & Toussaint

Figure 11: Benchmarks 20D (left), 15-3D, 5P and 11H (right)
by the sensors actions. Sensors have an internal state, an indication of the battery level
of the sensor. Each scan action depletes the battery. In addition to scanning, sensors
have two additional actionssensor off and recharge. The sensor off action allows sensors
to conserve energy by remaining idle. When the battery is completely depleted, a sensor
must perform recharge action, which has a cost (= 1). Each sensor has three observation:
target present, target absent and idle. False positives/negatives are allowed for the first
two observations. At runtime, sensors operate in a decentralized manner without a central
controller.
Note that our formulation of sensor network application is much richer and challenging
than the previously used benchmarks (Marecki et al., 2008). Earlier benchmarks did not
include any internal states and sensors were assumed to have an unbounded battery life. In
the current formulation, planning is much more complex as sensors must reason not only
about scanning, but also about conserving their energy, as they have a limited battery.
The EM algorithm was implemented in JAVA. All our experiments were done on an
8-core iMac with 2GB RAM. Our implementation used multithreading to parallelize EM
as highlighted in Section 5.5 and utilized all 8-cores. Each datapoint is the average of 10
runs. To speed up EMs convergence, we used a greedy variant of the M-step presented
by Toussaint et al. (2008). Such a step positively affects the rate of convergence of EM
with relatively little affect on the solution quality. Such an M-step is essentially a softer
version of the hard assignment EM (see Eq.(33)) and follows the same design (Toussaint
et al., 2008). We next describe problem sizes for different instances.
The 5P domain has 2 targets, 11H has 3 targets, 15-3D has 5 targets, and 20D has
6 targets. All these problems have very large state-spaces: 6  55 for 5P, 64  511 for 11H,
144  515 for 15-3D and 2700  520 for 20D. Evidently, EM efficiently exploits the factored
representation of the state and action spaces and is highly scalable with linear complexity
w.r.t. the number of edges in the graph. We also note that even solving the underlying
factored MMDP optimally is not feasible due to large state and action spaces.
Figure 12 shows the solution quality EM achieves for each of these benchmarks with
different controller sizes. A notable observation from these graphs is that EM converges
quickly with the greedy M-step of Toussaint et al. (2008) , taking fewer than 200 iterations even for such large multiagent planning problems. The solution quality, as expected,
increases monotonically with each iteration highlighting the anytime property of EM. In
254

fiProbabilistic Inference for Multiagent Decision Making

1400

1600
1400

1200

1200
1000

1000

800

800

600

600
400

400
200
0

200

EM(2)
EM(3)
EM(4)
EM(5)
0

20

40

60

EM(2)
EM(3)
EM(4)
EM(5)

0
-200

80 100 120 140 160 180 200

0

20

(a) 5P

40

60

80 100 120 140 160 180 200

(b) 11H

3500

4500
4000

3000

3500
2500

3000

2000

2500

1500

2000
1500

1000
500
0

1000

EM(2)
EM(3)
EM(4)
EM(5)
0

20

40

60

EM(2)
EM(3)
EM(4)
EM(5)

500
0

80 100 120 140 160 180 200

0

(c) 15-3D

20

40

60

80

100

120

140

160

(d) 20D

Figure 12: Solution quality achieved by EM (y-axis denotes quality and x-axis denotes the
iteration number).

Instance\Size

2-Node

3-Node

4-Node

5-Node

5P
11H
15-3D
20D

.232
1.29
1.17
5.03

1.07
6.07
5.39
22.01

3.22
18.90
16.69
67.85

7.74
45.23
40.47
171.26

Table 2: Time in seconds per iteration of EM

general, the solution quality increased with the number of controller nodes. For example,
for 20D, a 2-node controller achieves a quality of 3585.06 and a 5 node controller achieves
4154.04. However, for 5P and 15-3D, we did not observe a significant increase in quality by
increasing controller size, possibly due to the relative simplicity of these configurations.
Table 2 shows the runtime per iteration of EM for different instances and varying controller sizes. Encouragingly, the runtime is fairly smallparticularly for smaller controller
sizeseven for large problems such as 20D. To further decrease the runtime for larger
controllers, we plan to use Monte-Carlo sampling techniques in the future.
Table 3 shows the solution quality comparison of EM with random controllers and a loose
upper bound. The upper bound was computed by assuming that each target is detected
at every time step including the battery recharge cost. Against random controllers, EM
255

fiKumar, Zilberstein, & Toussaint

Instance/Value

5P
11H
15-3D
20D

EM

1250.89
1509.27
3094.05
4154.04

(44.3%)
(35.6%)
(43.8%)
(49.1%)

U.B.

Random

2820
4230
7050
8460

61.23
8.41
104.2
31.67

Table 3: Quality comparisons with a loose upper bound and random controllers for all
instances

N
2
3
4

Internal State = 2
EM
NLP
670.8/3.8
79/5.4
670.8/13.02 140.4/14.5
710.4/35.8
140.4/139.4

Internal
EM
972.5/8.9
1053.16/35.8
1062.4/107.4

State = 3
NLP
905.7/17.8
887.2/139
1024.8/1078.1

Table 4: Solution quality/time comparison of EM (100 iterations) with NLP for the 5P
domain, N denotes controller size, Time in seconds

always achieves much better solution quality. When compared against the upper bound, we
can see that EM performs quite well. Despite being a very loose bound, EM still achieves
a quality within 49.1% of this bound for the largest 20D domaina solid performance.
Previously, no algorithm could solve infinite-horizon ND-POMDPs (>2-agents). To
further assess EMs performance, we developed a nonlinear programming (NLP) formulation
of the problem and used a state-of-the-art NLP solver called Snopt (Gill et al., 2002). Snopt
could only solve the smallest 5P domain and could not scale beyond controller size 4 and
internal state 3 as it ran out of memory (=2GB). Table 4 shows the solution quality and
time comparisons. For internal state size of 2, Snopt gets stuck in a poor local optimum
compared to EM. It provides more competitive solutions for internal state 3, but EM is still
better in solution quality. Furthermore, the runtime of Snopt degrades quickly with the
increase in nonlinear constraints. This makes Snopt about an order-of-magnitude slower
for controller size 4 and internal state 3. These results further highlight the scalability of
EM, which could scale up to controller size 10 and internal state 5 within 2GB RAM and
 4 hours for 100 iterations.
Table 5 shows a comparison of EM against handcrafted controllers designed to take into
account the target trajectories and partial observation in the 11H benchmark (Figure 11).
To simplify the problem for the handcrafted solution, all penalties were zero and the reward
for detecting a target was 1. This allowed continuous scan by sensors without worrying
about miscoordination penalty. The first row in this table shows this no penalty case. We
see that EM is competitive with the handcrafted controller. The second row shows the
results when there was a cost to charge batteries. In this case, sensors need to decide when
to become idle to conserve power. The handcrafted controller cannot learn this behavior
and hence EM produces much better quality in this case.
256

fiProbabilistic Inference for Multiagent Decision Making

Version \ FSC Size Handcrafted 2 (EM) 3 (EM) 4 (EM)

No penalty
Penalty (.25)

13.92
-3.36

13.95
5.27

15.48
5.27

15.7
5.27

Table 5: Quality of handcrafted controllers vs. EM (11H)
Version \ FSC Size
Serial
Parallel

2
41.05
5.03

3
177.54
22.01

4
543.52
67.85

5
1308.20
171.26

Table 6: Serial vs. parallel execution times per EM iteration in 20D.
Finally, Table 6 highlights the significant opportunities that EM provides for parallel
computation. We consistently obtained almost linear speedup when using multithreading
on an 8-core CPU (total possible parallel threads in the largest domain 20D is 60). By
using a massively parallel platform such a Googles MapReduce,we could easily scale to
much larger team decision problems than currently possible.

7. Conclusion
Despite the rapid progress in multiagent planning, the scalability of the prevailing formal
models and algorithms has been limited. We presented a new approach to multiagent planning by developing novel connections between multiagent planning and machine learning.
We showed how the multiagent planning problem can be reformulated as inference in a mixture of dynamic Bayesian networks. By viewing multiagent planning through the lens of
probabilistic inference, we open the door to the application of efficient inference techniques
to multiagent decision making.
To further improve scalability to large multiagent systems, we identified a general condition called value factorization that facilitated the development of a scalable approximate
algorithm using probabilistic inference. We showed that several existing classes of DecPOMDPs satisfy this property. In contrast to previous approaches, our framework does not
impose any further restrictions on agent interaction beyond this property, thus providing
a general solution for value factorization based structured multiagent planning. The key
result that supports the scalability of our approach is that, within the EM framework, the
inference process can be decomposed into separate components that are much smaller than
the complete model, thus avoiding an exponential complexity.
Empirically, we experimented with several standard and large multiagent planning
benchmarks. Our inference-based approach was competitive with previous best approaches
based on nonlinear and linear programming. Our approach, which has linear complexity in
the number of edges in the agent interaction graph could scale up to 20 agents, whereas the
previous best approach based on nonlinear programming could only scale up to 5 agents
due to increase in the number of nonlinear constraints.
Our theoretical and empirical results show that exploring methods that overlap machine learning and planning has a great potential to overcome the practical limitations of
existing multiagent planning algorithms. In future work, we plan to explore several such
257

fiKumar, Zilberstein, & Toussaint

directions. We are interested in exploring the overlap of stochastic control theory and multiagent planning in continuous action and state space models similar to the work of Hoffman
et al. (2009a, 2009b). We also plan to further explore ways to overcome the effect of local
optima on the solution quality achieved by the EM algorithm. We specifically plan to investigate strategies to escape local optima (Poupart, Lang, & Toussaint, 2011) and to adapt
them to the multiagent setting.
Another key issue with planning-as-inference strategy using the EM algorithm is the
lack of any optimality guarantee or upper bounds on the controller based joint-policy. An
interesting recent research direction in the graphical models and machine learning literature
is the development of multiple inference strategies for marginal MAP (Liu & Ihler, 2013). It
has been shown that the EM algorithm can be used as one such inference approach to solve
the marginal MAP problem (Liu & Ihler, 2013). Planning under uncertainty problems can
be categorized as an instance of the marginal MAP inference (Cheng, Liu, Chen, & Ihler,
2013). Therefore, developing graphical models that can exploit new inference approaches for
the marginal MAP problem and provide quality guarantees is an interesting new direction
to explore at the frontier of planning and machine learning.

Acknowledgments
This work was funded in part by the National Science Foundation under grants IIS-0812149
and IIS-1116917, the Air Force Office of Scientific Research under grant FA9550-08-1-0181,
and the EU 3rdHand project.

Appendix A. Proof of EM Update Equations in Definition 3
We provide a constructive proof, showing the derivations of the relevant update equations.

A.1 Proof of Eq. (13)
Q(,  ? ) =

=

 X
X

T
X

P (r = 1, L, T ; )
log a?t pt

(52)

t=0

T =0 L

X

T X
X

T =0

t=0 L

P (T )

P (r = 1, L|T ; ) log a?t pt

(53)

In the above equation, the variable L includes all the hidden variables in the DBN of length
T . We can simplify the above summation using marginalization over variables other than
{at , pt }. We also use the fact that the policy is stationary to simplify as:
=

=


X

P (T )

T X
X

T =0

t=0 a,p


X

T X
X

T =0

P (T )

?
log ap

X
L\{at ,pt }

P (r = 1, at = a, pt = p, L\{at , pt }|T ; )

?
log ap
P (r = 1, at = a, pt = p|T ; )

t=0 a,p

258

(54)

(55)

fiProbabilistic Inference for Multiagent Decision Making

A.2 Action Parameter Updates
The expectation required for action updates as given in Section 4.3 is given as:
E [r = 1, a, p] =


X

T
X


P (T )
P (r = 1, a, p|T ; ) t

(56)

t=0

T =0

By breaking the above summation between t = T and t = 0 to T  1, we get
E [r = 1, a, p] =


X


X
X
Rsab ap bq T (p, q, s)+
P (T )
P (T )

T =0

T =0

qbs
T
1
X

X

t=0

p 0 q 0 s0

T t1 (p0 , q 0 , s0 )Pt (a, p, p0 , q 0 , s0 )

(57)

In the above equation, we marginalized the last time slice over the variables (q, b, s). For
the intermediate time slice t, we condition upon the variables (p0 , q 0 , s0 ) in the next time
slice t + 1. We now use the definition of  and move the summation over time T inside
for the last time slice and further marginalize over the remaining variables (q, s) in the
intermediate slice t:
E [r = 1, a, p] =

X

Rsab ap bq (p, q, s) +

X

t=0

p0 q 0 s0 sq

P (T )

T =0

q,b,s
T
1
X


X

T t1 (p0 , q 0 , s0 )ap P (p0 , q 0 , s0 |a, p, q, s)t (p, q, s)

(58)

Upon further marginalizing over the joint observations y 0 z 0 and simplifying we get:
E [r = 1, a, p] = ap

XX
qs

X

Rsab bq (p, q, s) +


X

P (T )

p0 q 0 s0 y 0 z 0 T =0

b

T
1
X
t=0

T t1 (p0 , q 0 , s0 )


P (s |a, q, s)p0 py0 q0 qz 0 P (y z |a, q, s )t (p, q, s)
0

0 0

0

(59)

We resolve the above time summation, as Toussaint et al. (2006), based on the fact that:
 T
1
X
X
T =0 t=0

f (T  t  1)g(t) =

and then setting  = T  t  1 to get
Finally we get:

 X

X
t=0 T =t+1

P

t=0 g(t)

f (T  t  1)g(t)

P

 =0 f ( ).

X


1


qs
b

X
0 0 0
0
0 0
0
(p , q , s )p0 py0 q0 qz 0 P (s |a, q, s)P (y z |a, q, s )

E [r = 1, a, p] = ap

X

(p, q, s)

p0 q 0 s0 y 0 z 0

259

Rsab bq +

(60)

fiKumar, Zilberstein, & Toussaint

The product P (s0 |a, q, s)P (y 0 z 0 |a, q, s0 ) can be further simplified by marginalizing out over
actions b of agent 2 as follows:
E [r = 1, a, p] = ap

X

X
(p, q, s)
Rsab bq +

qs

X

0

0

b

0

(p , q , s )p0 py0 q0 qz 0

p 0 q 0 s0 y 0 z 0

X


1


Oy0 z 0 s0 ab bq Ps0 sab

b

The expectation E [r = 1, b, q] for the other agent can be found similarly by the analogue
of the above equation.
A.3 Controller Node Transition Parameter Updates
The expectation required for the controller node transition parameters is as follows:
E [r = 1, p, p, y] =


X

T
X

P (T )

P (r = 1, pt = p, pt1 = p, yt = y|T ; )

(61)

t=1

T =1

By marginalizing over the variables (q, s) for the current time slice t, we get

T X
X
X
E [r = 1, p, p, y] =
P (T )
T t (p, q, s)Pt (p, p, y, s, q|T ; )

(62)

t=1 sq

T =1

By further marginalizing over the variables (s, q) for the previous time slice of t and over
the observations z of the other agent, we get
E [r = 1, p, p, y] = ppy


X

P (T )

T X
X
t=1 sqsqz

T =1

T t (p, q, s)qqz

P (yz|p, q, s)P (s|p, q, s)t1 (p, q, s)

(63)

The above equation can be further simplified by marginalizing the product
P (yz|p, q, s)P (s|p, q, s) over actions a and b of both the agents as follows:
E [r = 1, p, p, y] = ppy


X

P (T )

T X
X

T =1

t=1 sqsqz

T t (p, q, s)qqz t1 (p, q, s)

X

Oyzsab Pssab ap bq

ab

(64)
Upon resolving the time summation as before, we get the final expression as:
E [r = 1, p, p, y] = ppy

X

(p, q, s)(p, q, s)qqz

sqsqz

X

Oyzsab Pssab ap bq

(65)

ab

The expectation E [r = 1, q, q, z] for the other agent can be found in an analogous way.
260

fiProbabilistic Inference for Multiagent Decision Making

A.4 Initial Node Distribution Update
The expectation for the initial node distribution update is given as:
E [r = 1, p] =


X

P (T )P (r = 1, p0 = p|T ; )

(66)

T =0

The above expression can be computed as follows:
E [r = 1, p] =
=
=


X
T =0

X
T =0

X

P (T )[P (r = 1, p|T ; )]t=0
P (T )

P0 (r = 1|p, q, s, T ; )P0 (p, q, s)

(68)

T (p, s, q)p q 0 (s)

(69)

qs

P (T )

T =0

= p

X

(67)

X

X
qs

(p, s, q)q 0 (s)

(70)

qs

Appendix B. EM Derivation for the ND-POMDP Model
An n-agent ND-POMDP has the following parameters:
S = 1in Si  Su , where Si is the local state of agent i; Su is a set of uncontrollable
or external states that are independent of the agents actions. In the sensor network
example, Si is the battery level, while Su corresponds to the set of locations where
targets can be present.
A = 1in Ai where Ai is the set of actions for agent i. For the sensor network,
Ai ={l1 , . . . , lk ,Off, Recharge}, where {l1 , . . . , lk } represents the edges in the graph which
can be scanned by the given sensor agent.
Y = 1in Yi is the joint observation set. For the sensor network case, Yi = {target
present, absent, sensor idle}. We assume that sensor i can observe its internal state Si .
This is a realistic assumption as sensors can normally monitor their own battery level.
The noisy component of the observation set corresponds to target locations.
Q
P P (s0 |s, a) = Pu (s0u |su ) ni=1 Pi (s0i |si , su , ai ) is the transition model, where a = ha1 , . . . , an i
is the joint action taken in joint state s = hsu , s1 , . . . , sn i resulting in joint state s0 =
hs0u , s01 , . . . , s0n i. The model relies on conditional (on external state) transition independence among the agents.
Q
O O(y|s, a) = ni=1 Pi (yi |ai , su , si , ), where y is the joint observation after taking joint
action a and transitioning to joint state s. This relies on conditional observation independence.
P
R R(s, a) = l Rl (su , sl , al ) is the reward function, which is decomposable along subgroups
of agents defined by a set of links l. If k agents i1 , . . . , ik are members of a particular
261

fiKumar, Zilberstein, & Toussaint

p0

p1

a0

a1

y2

a2

p3

pT

y3

yT

aT

u0

u1

u2

uT

s0

s1

s2

sT

v0

v1

v2

vT

b0

q0

y1

p2

z1

q1

b1

z2

b2

q2

z3

zT

q3

qT

r

bT

Figure 13: A T-step DBN for a link involving two agents. There is one such DBN for each
link l and every time step T . In
top three layers, p denotes first agents (on
1
link l) controller nodes, a denotes action and u denotes internal state. The layer
si denotes the external state. In bottom three layers, q denotes second agents
controller nodes, b denotes action and v denotes the internal state

subgroup l, then sl = hsi1 , . . . , sik i denotes the internal states of these agents. Similarly,
al = hai1 , . . . , aik i. In the sensor network 5P in Figure 11, the reward is decomposed
among five subgroups, one per each edge. The reward function thus induces an interaction hypergraph in which hyperlink l connects the subset of agents which form the
reward component Rl .
bo bo = (buQ, b1 , . . . , bn ) is the initial belief for joint state s = hsu , s1 , . . . , sn i and b(s) =
bu (su )  ni=1 bi (si ).

The joint-value function in ND-POMDPs satisfies the value factorization property as follows (Nair et al., 2005):
X
V (, s) =
Vl (l , su , sl ).
l

There is one value factor Vl for each link l. We next present a T-step DBN for a factor l
that involves two agents. This DBN is the basis for the time-dependent DBN mixture for
value factor l. For ease of exposition, the nodes of the controller of one agent are denoted
by p and those of the other agent by q. The internal states, actions, observations of the
first agent are denoted by u, a, y respectively and v, b, z denote the same for the second
agent. The external state is denoted by s. As shown in Section 5.4.1, the E step of the
EM algorithm requires separate inference, one for each value factor Vl . Therefore, we only
derive the inference required in the time dependent mixture corresponding to the DBN in
Figure 13. Notice that it differs from the inference for a two-agent general Dec-POMDP
due to the presence of conditional transition and observation independence. This property
will be exploited during the E-step. The policy parameters to be optimized are defined for
262

fiProbabilistic Inference for Multiagent Decision Making

agent 1 (analogously for agent 2 as well) as:
apu = P (a|p, u)
0

p0 py = P (p |p, y)

p = P (pt=0 = p)

(71)
(72)
(73)

Notice that we also included the internal state u in the action parameter apu . This represents more expressive and accurate policy as agents have full observability of the internal
state. Next, similar to Section 4.4, we define the following Markovian chain in the DBN of
Figure 13:
P (p0,q 0,s0,u0,v 0|p, q, s, u, v) = P (p0,u0|p, u, s)P (q 0,v 0|q, v, s)P (s0|s)
X
P (p0,u0 |p, u, s) =
p0 py Pu0 au Pyasu apu

(74)
(75)

a,y

P (q 0,v 0|q, v, s) can be expressed similarly. These transitions are independent of time as we
use a stationary policy. Based on these transitions, we define the forward  messages as
follows: t = Pt (p, q, s, u, v; ). Intuitively, t represents the probability that controllers of
agents on the link are in state (p, q), their internal state is (u, v) and the external state is s
at time t. These messages are defined as:
0 (p, q, s, u, v) = p q bo (s, u, v)
X
t (p0,q 0,s0,u0,v 0 ) =
P (p0,q 0,s0,u0,v 0|p, q, s, u, v)t1 (p, q, s, u, v)

(76)
(77)

p,q,s,u,v

Similarly backward  messages are defined as follows:  = PT  (r=1|p, q, s, u, v; ), with
 = 0 representing the tail end of each DBN. As shown by Toussaint et al. (2006), thanks to
this notation, all the DBNs share the same tail. Hence we only need one sweep to compute
the  messages. We get:
X
0 (p, q, s, u, v)=
Rsuvab apu bqv

(78)

a,b

X
 (p, q, s, u, v)=  1 (p0,q 0,s0,u0,v 0 )P (p0,q 0,s0,u0,v 0|p, q, s, u, v)

(79)

p0,q 0,s0,u0,v 0

 represents the normalized expected reward for the link when the controllers of the agents
are in state (p, q), their internal state is (u, v) and the external state is s at time T   ,
given the policy parameters . Based on  and  messages, we also calculate two more
quantities:
(p, q, s, u, v) =
(p, q, s, u, v) =


X
t=0

X

P (T = t)t (p, q, s, u, v),

(80)

P (T =  ) (p, q, s, u, v).

(81)

 =0

263

fiKumar, Zilberstein, & Toussaint

The cutoff time for  and  message propagation can be fixed as shown in Section 4.4. As
shown in Section 5.4.1, the expectation required for action updates is the following in the
time dependent DBN mixture for the link l:
El [r = 1, a, p, u] =


X

P (T )

T
X

P (r = 1, at = a, pt = p, ut = u|T ; l )

(82)

t=0

T =0

Breaking the inner summation for the last time step T and the remainder, we get:
=

=


X

P (T )PT (r = 1, a, p, u)+

T =0
 X
X


T
1
X
X
P (T )
Pt (r = 1, a, p, u)
t=0

T =0


T
1 X
X
X
Rsuvab apu bqv T (p, q, s, u, v)+ P (T )
T t1 (p0,q 0,s0,u0,v 0 )Pt (a,p,u, p0,q 0,s0,u0,v 0 )

T =0 svqb

t=0 p0 q 0 s0 u0 v 0

T =0

For the last equality, we marginalized over the variables in the intermediate time slice. By
moving the summation over time T inside for the last time slice, and further marginalizing
the intermediate time slice t over (q, s, v), we get:
= apu
0

X

Rsuvab bqv (p, q, s, u, v) +


X

P (T )

X
T t1 (p0 , q 0 , s0 , u0 , v 0 )P (p0 , u0 |a, p, u, s)

t=0 qvsp0 q 0 s0 u0 v 0

T =0

svqb
0

T
1
X

P (q , v |q, v, s)Ps0 s apu t (p, q, s, u, v)
Upon resolving the time summation for the second part of the equation (Toussaint et al.,
2006), we get the final expression:

X
p0 q 0 s0 u0 v 0

X


1
qsv
b

0 0 0 0 0
0 0
0 0
0
(p , q , s , u , v )P (p , u |a, p, u, s)P (q , v |q, v, s)Ps s

El [r = 1, a, p, u]

= apu

X

(p, q, s, u, v)

Rsuvab bqv +

The expectation El [r = 1, b, q, v] for the other agent can be calculated in an analogous
manner.
We now derive the expectation for controller node transition update:
El [r = 1, p, p, y] =


X

P (T )

T =1

T
X

P (r = 1, pt = p, pt1 = p, yt = y|T ; l )

t=1

By simplifying the above equation, we get:
=


X
T =1

P (T )

T X
X
t=1 qsuv

T t (p, q, s, u, v)Pt (p, q, s, u, v, p, y)
264

(83)

fiProbabilistic Inference for Multiagent Decision Making

Upon further marginalizing over the (q, s, u, v), we get:
=


X

P (T )

T
X

X

t=1 qsuvqsuv

T =1


T t (p, q, s, u, v)Pt (p, q, s, u, v|p, q, s, u, v, y) 

P (y|p, u, s)t1 (p, q, s, u, v)

=


X

P (T )

T =1

T
X
t=1

X

ppy

qsuvqsuv


T t (p, q, s, u, v)t1 (p, q, s, u, v)P (y|p, u, s) 

P (u|p, u, y)Pss P (q, v|q, v, s)

= ppy

X

(p, q, s, u, v)(p, q, s, u, v)P (u, y|p, u, s)Pss P (q, v|q, v, s)

qsuvqsuv

where P (u, y|p, u, s), P (q, v|q, v, s) are defined as:
X
P (u, y|p, u, s) =
Puau Pyasu apu

(84)

a

P (q, v|q, v, s) =

X

qqz Pvbv Pz bsv bqv

(85)

bz

The final equation for El [r = 1, p, p, y] is given by:
X
El [r = 1, p, p, y] = ppy
(p, q, s, u, v)(p, q, s, u, v)P (u, y|p, u, s)Pss P (q, v|q, v, s)
qsuvqsuv

(86)

References
Amato, C., Bernstein, D. S., & Zilberstein, S. (2007). Solving POMDPs using quadratically constrained linear programs. In International Joint Conference on Artificial
Intelligence, pp. 24182424.
Amato, C., Bernstein, D. S., & Zilberstein, S. (2010). Optimizing fixed-size stochastic
controllers for POMDPs and decentralized POMDPs. Autonomous Agents and MultiAgent Systems, 21 (3), 293320.
Becker, R., Zilberstein, S., & Lesser, V. (2004). Decentralized Markov decision processes
with event-driven interactions. In Proceedings of the 3rd International Conference on
Autonomous Agents and Multiagent Systems, pp. 302309.
Becker, R., Zilberstein, S., Lesser, V., & Goldman, C. V. (2003). Transition-independent
decentralized Markov decision processes. In Proceedings of the 2nd International Conference on Autonomous Agents and Multi Agent Systems, pp. 4148.
Becker, R., Zilberstein, S., Lesser, V., & Goldman, C. V. (2004). Solving transition independent decentralized Markov decision processes. Journal of Artificial Intelligence
Research, 22, 423455.
265

fiKumar, Zilberstein, & Toussaint

Bernstein, D. S., Amato, C., Hansen, E. A., & Zilberstein, S. (2009). Policy iteration for
decentralized control of Markov decision processes. Journal of Artificial Intelligence
Research, 34, 89132.
Bernstein, D. S., Givan, R., Immerman, N., & Zilberstein, S. (2002). The complexity
of decentralized control of Markov decision processes. Mathematics of Operations
Research, 27, 819840.
Bertsekas, D. P. (1999). Nonlinear Programming (2nd edition). Athena Scientific.
Boyd, S., & Vandenberghe, L. (2004). Convex Optimization. Cambridge University Press,
New York, NY, USA.
Cheng, Q., Liu, Q., Chen, F., & Ihler, A. (2013). Variational planning for graph-based
MDPs. In Advances in Neural Information Processing Systems, pp. 29762984.
Dempster, A. P., Laird, N. M., & Rubin, D. B. (1977). Maximum likelihood from incomplete
data via the EM algorithm. Journal of the Royal Statistical society, Series B, 39 (1),
138.
Dibangoye, J. S., Amato, C., Doniec, A., & Charpillet, F. (2013a). Producing efficient errorbounded solutions for transition independent decentralized MDPs. In Proceedings of
the International Conference on Autonomous Agents and Multi-agent Systems, pp.
539546.
Dibangoye, J. S., Amato, C., Buffet, O., & Charpillet, F. (2013b). Optimally solving DecPOMDPs as continuous-state MDPs. In Proceedings of the 23rd International Joint
Conference on Artificial Intelligence, pp. 9096.
Dibangoye, J. S., Buffet, O., & Charpillet, F. (2014). Error-bounded approximations for
infinite-horizon discounted decentralized POMDPs. In Proceedings of the European
Conference on Machine Learning and Knowledge Discovery in Databases, pp. 338353.
Dibangoye, J. S., Mouaddib, A.-I., & Chaib-draa, B. (2009). Point-based incremental pruning heuristic for solving finite-horizon DEC-POMDPs. In Proceedings of the 8th International Joint Conference on Autonomous Agents and Multiagent Systems, pp.
569576.
Eker, B., & Akin, H. L. (2013). Solving decentralized POMDP problems using genetic
algorithms. Autonomous Agents and Multi-Agent Systems, 27 (1), 161196.
Ghahramani, Z., & Jordan, M. I. (1995). Factorial hidden Markov models. In Advances in
Neural Information Processing Systems, Vol. 8, pp. 472478.
Gill, P. E., Murray, W., & Saunders, M. A. (2002). SNOPT: An SQP algorithm for largescale constrained optimization. SIAM Journal on Optimization, 12 (4), 9791006.
Goldman, C. V., & Zilberstein, S. (2004). Decentralized control of cooperative systems:
Categorization and complexity analysis. Journal of Artificial Intelligence Research,
22, 143174.
Grzes, M., Poupart, P., & Hoey, J. (2013). Controller compilation and compression for resource constrained applications. In International Conference on Algorithmic Decision
Theory, pp. 193207.
266

fiProbabilistic Inference for Multiagent Decision Making

Grzes, M., Poupart, P., Yang, X., & Hoey, J. (2015). Energy efficient execution of POMDP
policies. IEEE Transactions on Cybernetics, preprint.
Hansen, E. A., Bernstein, D. S., & Zilberstein, S. (2004). Dynamic programming for partially
observable stochastic games. In Proceedings of the 19th AAAI Conference on Artificial
Intelligence, pp. 709715.
Hoffman, M., de Freitas, N., Doucet, A., & Peters, J. (2009a). An expectation maximization algorithm for continuous Markov decision processes with arbitrary rewards. In
Proceedings of the International Conference on Artificial Intelligence and Statistics,
pp. 232239.
Hoffman, M., Kueck, H., de Freitas, N., & Doucet, A. (2009b). New inference strategies for
solving Markov decision processes using reversible jump MCMC. In Proceedings of
the International Conference on Uncertainty in Artificial Intelligence, pp. 223231.
Kiselev, I., & Poupart, P. (2014a). Policy optimization by marginal-map probabilistic inference in generative models. In Proceedings of the International Conference on Autonomous Agents and Multi-agent Systems, pp. 16111612.
Kiselev, I., & Poupart, P. (2014b). POMDP planning by marginal-MAP probabilistic inference in generative models. In Proceedings of the 2014 AAMAS Workshop on Adaptive
Learning Agents.
Koller, D., & Parr, R. (1999). Computing factored value functions for policies in structured MDPs. In Proceedings of the 16th International Joint Conference on Artificial
Intelligence, pp. 13321339.
Koller, D., & Friedman, N. (2009). Probabilistic Graphical Models: Principles and Techniques. MIT Press.
Kumar, A., & Zilberstein, S. (2009a). Constraint-based dynamic programming for decentralized POMDPs with structured interactions. In Proceedings of the Eighth International
Conference on Autonomous Agents and Multiagent Systems, pp. 561568.
Kumar, A., & Zilberstein, S. (2009b). Event-detecting multi-agent MDPs: Complexity
and constant-factor approximation. In Proceedings of the 21st International Joint
Conference on Artificial Intelligence, pp. 201207.
Kumar, A., & Zilberstein, S. (2010a). Anytime planning for decentralized POMDPs using expectation maximization. In Proceedings of the Conference on Uncertainty in
Artificial Intelligence, pp. 294301.
Kumar, A., & Zilberstein, S. (2010b). Point-based backup for decentralized POMDPs:
Complexity and new algorithms. In Proceedings of the 9th International Conference
on Autonomous Agents and Multiagent Systems, pp. 13151322.
Kumar, A., Zilberstein, S., & Toussaint, M. (2011). Scalable multiagent planning using
probabilistic inference. In Proceedings of the 22nd International Joint Conference on
Artificial Intelligence, pp. 21402146.
Lauritzen, S. L., & Nilsson, D. (2001). Representing and solving decision problems with
limited information. Management Science, 47, 12351251.
267

fiKumar, Zilberstein, & Toussaint

Liu, Q., & Ihler, A. T. (2012). Belief propagation for structured decision making. In
Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence,
Catalina Island, CA, USA, August 14-18, 2012, pp. 523532.
Liu, Q., & Ihler, A. T. (2013). Variational algorithms for marginal MAP. Journal of
Machine Learning Research, 14 (1), 31653200.
MacDermed, L. C., & Isbell, C. (2013). Point based value iteration with optimal belief compression for Dec-POMDPs. In Advances in Neural Information Processing Systems,
pp. 100108.
Marecki, J., Gupta, T., Varakantham, P., Tambe, M., & Yokoo, M. (2008). Not all agents
are equal: Scaling up distributed POMDPs for agent networks. In Proceedings of the
7th International Joint Conference on Autonomous Agents and Multiagent Systems,
pp. 485492.
Mostafa, H., & Lesser, V. R. (2009). Offline planning for communication by exploiting structured interactions in decentralized MDPs. In International Conference on Intelligent
Agent Technology, pp. 193200.
Mostafa, H., & Lesser, V. R. (2011). Compact mathematical programs for DEC-MDPs
with structured agent interactions. In International Conference on Uncertainty in
Artificial Intelligence, pp. 523530.
Mundhenk, M., Goldsmith, J., Lusena, C., & Allender, E. (2000). Complexity of finitehorizon Markov decision process problems. J. ACM, 47 (4), 681720.
Nair, R., Varakantham, P., Tambe, M., & Yokoo, M. (2005). Networked distributed
POMDPs: A synthesis of distributed constraint optimization and POMDPs. In Proceedings of the 20th AAAI Conference on Artificial Intelligence, pp. 133139.
Nair, R., Tambe, M., Yokoo, M., Pynadath, D., Marsella, S., Nair, R., & Tambe, M. (2003).
Taming decentralized POMDPs: Towards efficient policy computation for multiagent
settings. In Proceedings of the 18th International Joint Conference on Artificial Intelligence, pp. 705711.
Oliehoek, F. A., Spaan, M. T. J., Amato, C., & Whiteson, S. (2013). Incremental clustering
and expansion for faster optimal planning in Dec-POMDPs. Journal of Artificial
Intelligence Research, 46, 449509.
Oliehoek, F. A., Spaan, M. T. J., & Vlassis, N. A. (2008). Optimal and approximate Q-value
functions for decentralized POMDPs. Journal of Artificial Intelligence Research, 32,
289353.
Oliehoek, F. A., Whiteson, S., & Spaan, M. T. J. (2013). Approximate solutions for factored
dec-pomdps with many agents. In Proceedings of the 12th International Conference
on Autonomous Agents and Multiagent Systems, pp. 563570.
Pajarinen, J., Hottinen, A., & Peltonen, J. (2014). Optimizing spatial and temporal reuse
in wireless networks by decentralized partially observable Markov decision processes.
IEEE Transactions on Mobile Computing, 13 (4), 866879.
Pajarinen, J., & Peltonen, J. (2011a). Efficient planning for factored infinite-horizon DECPOMDPs. In Proceedings of the 22nd International Joint Conference on Artificial
Intelligence, pp. 325331.
268

fiProbabilistic Inference for Multiagent Decision Making

Pajarinen, J., & Peltonen, J. (2011b). Periodic finite state controllers for efficient POMDP
and DEC-POMDP planning. In Advances in Neural Information Processing Systems,
pp. 26362644.
Pajarinen, J., & Peltonen, J. (2013). Expectation maximization for average reward decentralized POMDPs. In Proceedings of the European Conference on Machine Learning,
pp. 129144.
Pineau, J., Gordon, G., & Thrun, S. (2006). Anytime point-based approximations for large
POMDPs. Journal of Artificial Intelligence Research, 27, 335380.
Poupart, P., & Boutilier, C. (2003). Bounded finite state controllers. In Advances in Neural
Information Processing Systems.
Poupart, P., Lang, T., & Toussaint, M. (2011). Analyzing and escaping local optima in
planning as inference for partially observable domains. In European Conference on
Machine Learning, pp. 613628.
Seuken, S., & Zilberstein, S. (2007). Memory-bounded dynamic programming for DECPOMDPs. In Proceedings of the 20th International Joint Conference on Artificial
Intelligences, pp. 20092015.
Smith, T., & Simmons, R. (2004). Heuristic search value iteration for POMDPs. In International Conference on Uncertainty in Artificial Intelligence, pp. 520527.
Toussaint, M., Harmeling, S., & Storkey, A. (2006). Probabilistic inference for solving
(PO)MDPs. Tech. rep. EDIINF-RR-0934, University of Edinburgh, School of Informatics.
Toussaint, M., Charlin, L., & Poupart, P. (2008). Hierarchical POMDP controller optimization by likelihood maximization. In International Conference on Uncertainty in
Artificial Intelligence, pp. 562570.
Toussaint, M., & Storkey, A. J. (2006). Probabilistic inference for solving discrete and
continuous state Markov decision processes. In International Conference on Machine
Learning, pp. 945952.
Varakantham, P., Marecki, J., Yabu, Y., Tambe, M., & Yokoo, M. (2007). Letting loose
a SPIDER on a network of POMDPs: Generating quality guaranteed policies. In
Proceedings of the 6th International Joint Conference on Autonomous Agents and
Multiagent Systems, pp. 18.
Varakantham, P., Kwak, J., Taylor, M. E., Marecki, J., Scerri, P., & Tambe, M. (2009).
Exploiting coordination locales in distributed POMDPs via social model shaping. In
Proceedings of the 19th International Conference on Automated Planning and Scheduling, pp. 313320.
Witwicki, S. J. (2011). Abstracting Influences for Efficient Multiagent Coordination Under
Uncertainty. Ph.D. thesis, Department of Computer Science, University of Michigan,
Ann Arbor.
Witwicki, S. J., & Durfee, E. H. (2010). Influence-based policy abstraction for weaklycoupled Dec-POMDPs. In Proceedings of the 20th International Conference on Automated Planning and Scheduling, pp. 185192.
269

fiKumar, Zilberstein, & Toussaint

Witwicki, S. J., & Durfee, E. H. (2011). Towards a unifying characterization for quantifying
weak coupling in Dec-POMDPs. In Proceedings of the 10th International Conference
on Autonomous Agents and Multiagent Systems, pp. 2936.
Wu, F., Zilberstein, S., & Chen, X. (2010). Trial-based dynamic programming for multiagent planning. In Proceedings of the 24th AAAI Conference on Artificial Intelligence,
pp. 908914.
Wu, F., Zilberstein, S., & Jennings, N. R. (2013). Monte-Carlo expectation maximization
for decentralized POMDPs. In Proceedings of the 23rd International Joint Conference
on Artificial Intelligence, pp. 397403.

270

fiJournal of Artificial Intelligence Research 53 (2015) 541-632

Submitted 01/15; published 07/15

ITSAT: An Efficient SAT-Based Temporal Planner
Masood Feyzbakhsh Rankooh
Gholamreza Ghassem-Sani

feyzbakhsh@ce.sharif.edu
sani@sharif.edu

Computer Engineering Department,
Sharif University of Technology,
Azadi ave., Tehran, Iran

Abstract
Planning as satisfiability is known as an efficient approach to deal with many types of
planning problems. However, this approach has not been competitive with the state-space
based methods in temporal planning. This paper describes ITSAT as an efficient SAT-based
(satisfiability based) temporal planner capable of temporally expressive planning. The
novelty of ITSAT lies in the way it handles temporal constraints of given problems without
getting involved in the difficulties of introducing continuous variables into the corresponding
satisfiability problems. We also show how, as in SAT-based classical planning, carefully
devised preprocessing and encoding schemata can considerably improve the efficiency of
SAT-based temporal planning. We present two preprocessing methods for mutex relation
extraction and action compression. We also show that the separation of causal and temporal
reasoning enables us to employ compact encodings that are based on the concept of parallel
execution semantics. Although such encodings have been shown to be quite effective in
classical planning, ITSAT is the first temporal planner utilizing this type of encoding. Our
empirical results show that not only does ITSAT outperform the state-of-the-art temporally
expressive planners, it is also competitive with the fast temporal planners that cannot
handle required concurrency.

1. Introduction
Temporal planning is an extension of classical planning where actions are durative rather
than being instantaneous. The introduction of durative actions adds a new dimension to
solving planning problems, namely reasoning about time. Temporal reasoning is per se very
different from causal reasoning, because time is a real-valued quantity, whereas the causal
aspects of planning are normally represented by propositions.
The current standard language for defining temporal planning problems is PDDL2.1
(Fox & Long, 2003). Although PDDL+ (Fox & Long, 2002) has been introduced to the
planning community as a more expressive language for defining temporal and numerical
planning problems, throughout this paper, we will focus on PDDL2.1, because the planning
problems that we have tackled do not need the expressive power of PDDL+. In PDDL2.1,
actions can have separate preconditions and effects upon starting and ending. Each temporal
action can also have some invariants, which must be preserved during the execution of that
action. An important subset of problems defined by PDDL2.1 are problems for which every
valid plan includes the concurrent execution of two or more actions. This subset is called the
problems with required concurrency. It has been shown that the concurrent execution of two
actions may be necessary for solving some temporal problems (Halsey, Long, & Fox, 2004;
Cushing, Kambhampati, Mausam, & Weld, 2007). For instance, in some temporal planning
c
2015
AI Access Foundation. All rights reserved.

fiRankooh & Ghassem-Sani

problems, actions may require a proposition that is only available during the execution of
another action. In such cases, these two actions must be executed concurrently. A more
specific example is given in Section 2, where we describe the Driverlogshift domain. A
common approach in many planners that are not temporally expressive is to eliminate such
cases by compressing all temporal actions to create non-durative classical actions.
In this paper, we describe ITSAT, a temporally expressive SAT-based (i.e., satisfiability
based) planner. ITSAT uses an approach that takes advantage of parallel execution semantics without rendering it incomplete for the problems with required concurrency. In this
approach, the durations of all actions of a given problem are first abstracted out. This is
done by breaking each temporal action into two starting and ending instantaneous events.
Then the obtained temporally abstract problem is encoded into a SAT formula using novel
-step and -step semantics for causally valid plans. We show how these semantics can be
used to encode a given temporal planning problem into a SAT formula. Classical -step
and -step encoding methods have been introduced before (Rintanen, Heljanko, & Niemela,
2006). In addition to extending these methods to the temporal planning context, we also
introduce a new encoding method based on -step semantics for causally valid plans. We
show that our new encoding often results in a significant reduction of the number of required
steps.
After generating a causally valid plan, ITSAT performs a scheduling phase. During this
phase, ITSAT tries to satisfy those temporal constraints that are imposed by considering the
durations of actions. This is done by solving a Simple Temporal Problem (STP) (Dechter,
Meiri, & Pearl, 1991). However, for problems with required concurrency, the posed STP
may be inconsistent. In such cases, the cause of inconsistency, which manifests itself as a
negative cycle in the corresponding Simple Temporal Network (STN), is detected. ITSAT
then generates a number of clauses that if added to the SAT formula, collectively prevent
the reoccurrence of the particular negative cycle that has occurred and other similar cycles.
This process is repeated until a temporally valid plan is found.
Similar to other SAT-based planners, ITSAT takes advantage of a preprocessing phase
to extract some information about the structure of problems. Such information is used
throughout the encoding phase to produce a formula whose satisfiability can be checked
more efficiently by the SAT solver. In Section 3, we describe the preprocessing phase of
ITSAT, which includes the reasoning about mutual exclusion and the so-called safe action
compression (Coles, Coles, Fox, & Long, 2009). Two propositions are regarded as mutually
exclusive if they can never be jointly true in the same state of a valid temporal plan. Here, we
show that one can detect the mutually exclusive propositions of temporal problems by using
planning graph analysis (Blum & Furst, 1997). It is known that employing mutual exclusion
reasoning can significantly improve the performance of SAT-based planners (Gerevini &
Schubert, 1998).
As we mentioned earlier, ITSAT breaks down each temporal action into two starting
and ending instantaneous events. Although in some cases such breaking down might be
necessary for producing concurrent plans, there are situations where this is not necessary
for finding valid plans. In Section 3, we show that by using the mutual exclusion information,
one can identify those temporal actions that can safely be compressed into a classical action
without falsifying the validity of temporal plans. This analysis results in a smaller number
of distinct events and therefore a simpler planning problem.
542

fiITSAT: An Efficient SAT-Based Temporal Planner

We empirically show that by taking advantage of its preprocessing, encoding, and
scheduling phases, not only does ITSAT significantly outperform state-of-the-art temporally expressive planners, it is also competitive with the best temporally simple planners
which are incapable of solving problems with required concurrency property. The components of ITSAT are shown in Figure 1. In this figure, the processing components of
ITSAT system are shown by rectangular blocks, while links represents the data produced
and received by these components.

Figure 1. The block diagram of ITSAT

543

fiRankooh & Ghassem-Sani

1.1 Motivation
As we mentioned earlier, temporal planners need to reason about time, which is a continuous
quantity. Nevertheless, causal structures of problems in temporal planning are still very
similar to those in classical planning. The existence of abundant temporal planning domains
that also have classical versions can be regarded as an evidence to this claim. This suggests
that temporal planners can benefit from using approaches that have been previously shown
to be effective in dealing with classical problems.
The usage of Boolean satisfiability checking is a well-known paradigm in tackling classical
planning problems (Kautz & Selman, 1992). In this approach, a given planning problem is
translated into a formula in propositional logic. Each variable of the SAT formula typically
represents the occurrence of the corresponding action or proposition in a certain place of
potential plan. The causal constraints of the planning problem are represented by a number
of ground clauses. The output plan is assumed to have a finite number of steps. Each step
may include one or more actions. The original SAT-based planner allowed only one action
per step (Kautz & Selman, 1992). However, the previously introduced SAT-based planners
allow multiple occurrence of actions in each step. The produced formula is given as the
input to an off-the-shelf SAT solver, which tries to find a model for it. If such a model is
found, a plan is extracted from it. Otherwise, the number of steps in the output plan is
increased by one and the corresponding SAT formula is given again to the SAT solver. This
process is repeated until a valid plan is extracted or some predefined termination condition
is reached. In order to obtain an efficient SAT-based planner, one important issue that
should be considered is how to encode the given planning problem into a SAT formula.
SAT-based planning was originally used to find optimal plans, i.e., plans with minimum
number of actions (Kautz & Selman, 1992). To guarantee the optimality of the output plan,
the formula must include certain clauses to ban each step from containing more than one
action. However, in the so-called satisficing planning, in which optimality is not the main
objective, forcing single-action steps is not necessary. An alternative approach is to consider
actions that can be executed in parallel in each step of the output plan (Ernst, Millstein,
& Weld, 1997). Exploiting such parallelism can result in a smaller number of steps in the
SAT formula. Another important benefit of producing compact formulae is lower memory
requirements. Several encoding methods have been introduced to take advantage of action
parallelism. These encoding methods are based on the so-called -step and -step semantics
of valid plans (Rintanen et al., 2006).
The -step and -step semantics are different in the extent of action parallelism that they
allow to occur in each step. The -step semantics allows a set of actions to be executed in
parallel, only if those actions can be executed in every possible ordering without affecting the
validity of the plan. The -step semantics, on the other hand, imposes a weaker restriction:
for each step of a plan, there must exist at least one possible ordering in which the actions
can be executed without falsifying the validity of the plan. It should be clear that the -step
semantics potentially allows more parallelism than is permitted by the -step semantics. In
fact, by taking advantage of -step semantics, the most efficient SAT-based classical planner,
i.e., Mp (Rintanen, 2012), is competitive with the state-of-the-art state-space planners. In
this paper, we show that the separation of causal and temporal reasoning phases of temporal
planning enables us to employ these compact encodings for efficient temporal planning.
544

fiITSAT: An Efficient SAT-Based Temporal Planner

1.2 Related Work
Previous research in the field of temporal planning has benefited enormously from employing
well-developed classical planning strategies. For instance, many successful temporal planners have utilized the ideas of partial order planning, e.g. VHPOP (Younes & Simmons,
2003) and CPT (Vidal & Geffner, 2006). Planning graph analysis has also been adopted by
temporal planners such as TGP (Smith & Weld, 1999) and TPSYS (Garrido, Fox, & Long,
2002). Some other temporal planners have embedded temporal reasoning into heuristic
state space search. TFD (Eyerich, Mattmuller, & Roger, 2009), LPG-td (Gerevini, Saetti,
& Serina, 2006), and POPF (Coles, Coles, Fox, & Long, 2010) are the successful instances
of this latter approach.
The usage of Boolean satisfiability checking is one of the well-known paradigms in tackling classical planning problems (Kautz & Selman, 1992). In order to obtain an efficient
SAT-based planner, one important issue that should be considered is how to encode the
given planning problem into a SAT formula. In fact, devising efficient encoding methods
has been an important research trend in the field of SAT-based planning. Examples of
efficient encodings are: the split action representation (Kautz & Selman, 1996; Ernst et al.,
1997; Robinson, Gretton, Pham, & Sattar, 2009), the SAS+ based encoding (Huang, Chen,
& Zhang, 2012), and the compact mutual exclusion representation (Rintanen, 2006). Based
on parallel semantics of plans, another effective encoding method has been introduced (Rintanen et al., 2006). This latter encoding method is of particular interest in this paper.
Satisfiability checking has also been employed in the field of temporal planning. However,
SAT-based temporal planners do encounter a major challenge: representing temporal aspects of problems. Since time is a continuous quantity, it cannot be treated in the exact same
way in which discrete causality is handled. To tackle this problem, STEP (Huang, Chen, &
Zhang, 2009), SCP2 (Lu, Huang, Chen, Xu, Zhang, & Chen, 2013), and T-SATPLAN (Mali
& Liu, 2006) use a discrete representation of time. These planners assign explicit discrete
time labels to each step of the encoding. Generally speaking, in this approach, each step i
is exactly one time unit ahead of step i + 1. As a result, if an action with duration d starts
in step i, it is forced to end in step i + d. One immediate outcome of such an approach
is the introduction of an enormous number of steps into the encoding, many of which will
not contribute to the output plan. This drawback of the explicit time representation causes
STEP, SCP2, and T-SATPLAN to be inefficient in terms of both speed and memory usage. To obtain a better performance, SCP2 uses -step semantic to allow causal relations
between actions in each time point (Lu et al., 2013).
TM-LPSAT (Shin & Davis, 2005), which has been designed to solve planning problems
defined in PDDL+ (Fox & Long, 2002), is another SAT-based planner capable of handling
temporal planning problems. Similar to STEP and T-SATPLAN, TM-LPSAT attaches
time labels to each step. However, in TM-LPSAT, these labels are not predefined discrete
numbers. Instead, each label is a numeric variable whose value will be determined after
the problem is solved by an SMT solver (Armando & Giunchiglia, 1993). This approach
can result in encodings which are more compact than those produced by STEP and TSATPLAN.
A major disadvantage of assigning a time label to each step of the formula is that the parallelism mentioned above cannot be exploited effectively. That is because when two events
545

fiRankooh & Ghassem-Sani

are to happen in a certain step of the plan, their time labels must be the same, and thus,
they must be simultaneous when the final plan is executed. This compulsory simultaneity
is a restriction that reduces the number of events that can happen in each step of the final
plan, which in turn, increases the number of steps needed for solving input problems. This
implies that all the efficiency gain one could obtain from using parallel execution semantics
will be sacrificed to achieve an easy way to deal with temporal constraints. However, in the
majority of current temporal planning problems, satisfying temporal constrains is not the
hardest task in finding a valid plan. It was shown that for the problems without required
concurrency, one can omit the temporal constraints altogether, find a causally valid plan,
and then, by considering temporal constraints only in a postproccessing step, schedule the
actions of that plan to find a temporally valid plan (Cushing et al., 2007). This approach
has actually been used by many previous temporal planners including YAHSP3-mt (Vidal,
2014), the winner of the temporal satisficing track of IPC 2014. Despite being efficient in
solving many temporal problems, such planners are incomplete, as they are incapable of
solving problems with required concurrency.
In addition to classical planning problems, SAT-based methods have also been used to
deal with other categories of planning problems. Examples are planning under uncertainty
(Castellini, Giunchiglia, & Tacchella, 2003), cost-optimal planning (Robinson, Gretton,
Pham, & Sattar, 2010) and numerical planning (Hoffmann, Gomes, Selman, & Kautz,
2007).

2. Preliminaries
The standard language used for defining temporal planning problems is PDDL2.1 (Fox &
Long, 2003). Figure 2 presents an example of the PDDL2.1 representation of a temporal
planning domain. This domain, which is a simplified version of Driverlogshift (Halsey, 2004),
will be referred to several times throughout this paper. As Figure 2 shows, in PDDL2.1,
each action can have separate conditions and effects upon starting and ending. The starting
and ending conditions (or effects) of an action are specified by the at start and at end
tokens, respectively. Each action may also have some conditions that need to be preserved
during execution. These conditions are specified using the over all token. Moreover, the
duration of each action is defined by an (= ?duration x) statement, where x is a rational
number or a function specifying the actual duration of that action.
Driverlogshift is the temporal version of the Driverlog domain from IPC3. As in its
classical counterpart, in Driverlogshift, the objective is to transfer several objects from their
original places to their destinations. Each object can be loaded into and unloaded from a
certain truck by using the LOAD and UNLOAD operators, respectively. A truck can move
between locations using the MOVE operator. The main difference between Driverlogshift and
Driverlog is that the trucks here must be rested during the intervals between their working
shifts. Each working shift is defined by the WORK operator, which produces the (working
truck) proposition upon starting, and deletes that proposition upon ending. LOAD, UNLOAD,
and MOVE have (working truck) as their invariant. Once (working truck) is deleted by
the ending of WORK, it may be reproduced by the REST operator, which defines the resting
shift of a certain truck.
546

fiITSAT: An Efficient SAT-Based Temporal Planner

(define (domain driverlogshift)
(:requirements :typing :durative-actions)
(:types
location locatable - object
truck obj - locatable)
(:predicates
(at ?obj - locatable ?loc - location)
(in ?obj1 - obj ?obj - truck)
(link ?x ?y - location)
(working ?t - truck)
(need-rest t - truck)
(rested t - truck))
(:durative-action WORK
:parameters
(?truck - truck)
:duration (= ?duration 100)
:condition (and
(at start (rested ?truck)))
:effect (and (at start (working ?truck))
(at end (not (working ?truck)))
(at start (not (rested ?truck)))
(at end (need-rest ?truck))))
(:durative-action REST
:parameters
(?truck - truck)
:duration (= ?duration 20)
:condition (and
(at start (need_rest ?truck)))
:effect (and
(at start (not (need_rest ?truck)))
(at end (rested ?truck))))
(:durative-action LOAD
:parameters
(?obj - obj
?truck - truck
?loc - location)
:duration (= ?duration 10)
:condition (and
(over all (at ?truck ?loc))
(over all (working ?truck))
(at start (at ?obj ?loc)))
:effect (and
(at start (not (at ?obj ?loc)))
(at end (in ?obj ?truck))))

(:durative-action UNLOAD
:parameters
(?obj - obj
?truck - truck
?loc - location)
:duration (= ?duration 10)
:condition (and
(over all (at ?truck ?loc))
(over all (working ?truck))
(at start (in ?obj ?truck)))
:effect (and
(at start (not (in ?obj ?truck)))
(at end (at ?obj ?loc))))
(:durative-action MOVE
:parameters
(?truck - truck
?loc-from - location
?loc-to - location)
:duration (= ?duration 50)
:condition (and
(at start (at ?truck ?loc-from))
(at start (link ?loc-from ?loc-to))
(over all (working ?truck)))
:effect (and
(at start (not (at ?truck ?loc-from)))
(at end (at ?truck ?loc-to)))))

Figure 2. PDDL2.1 description of Driverlogshift domain

Note that the version of Driverlogshift described in Figure 2 is slightly different from
its original version (Halsey, 2004), in which there are drivers that can walk to, board, and
disembark trucks. Furthermore, the REST and WORK actions are performed by the drivers
rather than trucks. However, in order to make the examples simpler, we have merged drivers
and trucks into a single entity of just trucks.
A simple example of problems in Driverlogshift is shown in Figure 3. In this problem,
there are three locations (s0, s1, and s2), one truck (truck1), and one object (package1).
In the initial state, truck1 and package1 are at s0. The objective of the problem is to
transfer package1 to s2.
2.1 Formalism of PDDL2.1
Now we present our formalism of the specifications of PDDL2.1. Our formalism is devised in
a way that simplifies the description of the preprocessing, encoding, and scheduling phases
of ITSAT. We should mention that our formalism has some limitations compared to the
full specifications of PDDL2.1. These limitations will be discussed in details in Section 2.2.
547

fiRankooh & Ghassem-Sani

(define (problem DLOG)
(:domain driverlogshift)
(:objects
truck1
- truck
package1 - obj
s0 s1 s2 - location)
(:init
(rested truck1)
(at truck1 s0)
(at package1 s0)
(link s0 s1)
(link s1 s0)
(link s2 s1)
(link s1 s2)
)
(:goal (and
(at package1 s2))))

Figure 3. PDDL2.1 description of a problem in Driverlogshift domain

Definition 1 (events). An event, e, is a triple (pre(e), add(e), del(e)), where pre(e),
add(e), and del(e) are three sets of atomic propositions (facts) representing preconditions,
positive effects, and negative effects of e, respectively.
Definition 2 (temporal actions) A temporal action, a, is a quadruple (start(a), end(a),
inv(a), dur(a)), where start(a) and end(a) are two events denoting the starting and ending
events of a, inv(a) is a set of atomic propositions representing the invariants of a, and
dur(a) is a positive rational number specifying the duration of a.
Example 1. Figure 4 shows a temporal action a = LOAD(package1, truck1, s0), which
is an instance of the LOAD operator defined in Figure 2. In Figure 4, a is depicted by a rectangular box. Conditions and effects of a are written above and below the box, respectively.
The at start conditions and effects of a are placed at the left hand side of the box, and
the at end conditions and effects are placed at the right hand side of the box. The over
all conditions of a are placed at the middle of the box. Here, start(a) and end(a) are
two events, where pre(start(a)) = {(at package1 s0)}, add(start(a)) = , del(start(a)) =
{(at package1 s0)}, pre(end(a)) = , add(end(a)) = {(at package1 truck1)}, and also
del(end(a)) = . Moreover, we have inv(a) = {(at truck1 s0), (working truck1)}, and
dur(a) = 10.
548

fiITSAT: An Efficient SAT-Based Temporal Planner

Figure 4. A temporal action

Definition 3 (temporal states). A temporal state, s, is a pair (state(s), agenda(s)),
where state(s) is a classical planning state represented by a set of atomic propositions, and
agenda(s) contains a finite set of open actions (i.e., actions started prior to s and not yet
ended).
Definition 4 (applicability). The starting event e of an action a is applicable in state s,
if the following conditions hold:
(1) state(s) contains all the preconditions of e and
S all the invariants of a (except for those
invariants of a that are added by e): pre(e) (inv(a)  add(e))  state(s)
(2) a is not already open in s: a 
/ agenda(s)
(3) eSdoes not delete theTinvariants of any open action of s:

del(e) = 
a agenda(s) inv(a )
The ending event e of an action a is applicable in state s, if the following conditions hold:
(1) state(s) contains all the preconditions of e: pre(e)  state(s)
(2) a is open in s: a  agenda(s)
(3) S
e does not delete the invariants
of any open action of s (other than a):
T
)
inv(a
del(e)
=
a agenda(s){a}
Definition 5 (successors). If the starting event e of action a is applicable in state s, it
will change s to the unique state s satisfying the following conditions:


 The set of open
S actions of s is equal to the set of open actions of s with a: agenda(s ) =
agenda(s) {a}

 All positive and negative effects
S of e are respectively added to and deleted from s :

state(s ) = (state(s)  del(e)) add(e)

If the ending event e of action a is applicable in state s, it will change s to the unique state
s satisfying the following conditions:
549

fiRankooh & Ghassem-Sani

 The set of open actions of s is equal to the set of open actions of s without a:
agenda(s ) = agenda(s)  {a}
 All positive and negative
effects of e are respectively added and deleted in s : state(s ) =
S
(state(s)  del(e)) add(e)
From now on, we may use succ(s, e) to represent the successor state s obtained by applying
e to s.
Definition 4 and Definition 5 can be easily extended to also cover any sequence of events:
succ(s, he1 , ..., en i) = succ(succ(s, he1 , ..., en1 i), en ), and succ(s, hi) = s. A sequence of
events he1 , ..., en i is applicable in a temporal state s, if succ(s, he1 , ..., en i) is defined.
Example 2. Let s be a temporal state such that
state(s) = {(at package1 s0), (at truck1 s0), (working truck1), (link s0 s1)}
and
agenda(s) = .
Let a = LOAD(package1, truck1, s0) and a = MOVE(truck1, s0, s1) be two temporal actions, which are respectively instances of the LOAD and MOVE operators presented in Figure
2. the event start(a) is applicable in s and it changes s to s such that
state(s ) = {(at truck1 s0), (working truck1), (link s0 s1)}
and
agenda(s ) = {LOAD(package1, truck1, s0)}.
Now, start(a) is not applicable in s because a is already open in s . Nor isstart(a ) applicable in s , as it deletes (at truck1 s0) that is an invariant of a, which is still open in s .
However, end(a) is applicable in s and changes it to s such that
state(s ) = {(at package1 truck1), (at truck1 s0), (working truck1), (link s0 s1)}
and
agenda(s ) = .

Definition 6 (temporal problems). A temporal problem, P, is a triple (I, G, A), where
I, representing the initial state, is a temporal state such that agenda(I) = . G is a set
of atomic propositions denoting the goal conditions, and A is the finite set of all possible
temporal actions of P.
Definition 7 (causally valid plans). Let P = (I, G, A) be a temporal problem and
 = he1 , ..., en i be a sequence of events where for each i, ei is a starting or an ending event of
an action in A.  is a causally valid plan for P, if it is applicable in I, G  state(succ(s, )),
and agenda(succ(s, )) = .
550

fiITSAT: An Efficient SAT-Based Temporal Planner

Definition 8 (pairing events). Let  = he1 , ..., en i be a causally valid plan for problem P = (I, G, A). Assume that ei and ej are respectively the starting and ending events of
a certain action a  A and i < j. If for all k such that i < k < j, ek is neither the starting
nor the ending event of a, we say that ei (ej ) is the pairing event of ej (ei ) in . In other
words, ei and ej are pairing events in  if they are related to the same occurrence of a in .
Definition 9 (valid temporal plans and makespan). Let  = he1 , ..., en i be a causally
valid plan for P = (I, G, A), and  : {1, ..., n}  Q be a scheduling function for , where Q
is the set of rational numbers. (,  ) is a valid temporal plan for P if  has the following
properties:
 For all i,  (i) <  (i + 1).
 For each a  A, if start(a) = ei , and ej is the pairing event of ei , then  (j) =
 (i) + dur(a).
The maximum value assigned by  to the events of  is called the makespan of .
Example 3. Consider the problem P = (I, G, A) depicted in Figure 3, where state(I)
and G contain the propositions listed after the labels :init and :goal, respectively, and A
is the set of all possible instantiations of operators presented in Figure 2 with the objects
listed after label :objects of Figure 3. Let
 = hstart(WORK(truck1)),
start(LOAD(truck1, package1, s0)),
end (LOAD(truck1, package1, s0)),
start(MOVE(truck1, s0, s1)),
end (MOVE(truck1, s0, s1)),
start(MOVE(truck1, s1, s2)),
end (MOVE(truck1, s1, s2)),
start(UNLOAD(truck1, package1, s2)),
end (UNLOAD(truck1, package1, s2)),
end (WORK(truck1))i
A schematic representation of  is depicted in Figure 5. A straightforward checking shows
that  is a causally valid plan for P. However,  is not a valid temporal plan because the
duration of WORK(truck1) is 100,  requires the serial execution of two MOVE actions, one
LOAD action, and one UNLOAD action, with the total duration of 120, while WORK(truck1)
is still open. In other words, one single working shift of truck1 is not sufficient to transfer
package1 from s0 to s2. Therefore, no scheduling function  with the properties of
Definition 9 exists for . A valid temporal plan for P is depicted in Figure 6. In this
plan, two working shifts of truck1 are used.
551

fiRankooh & Ghassem-Sani

Figure 5. A causally valid plan

Figure 6. A valid plan

2.2 Limitations
We end this section by describing the differences between our formalism of valid temporal
plans and that of PDDL2.1. The main limitations of our formalism are listed below:
 According to Definition 4, the starting event of an action a is applicable in the state
s only if a is not already open in s. This means that, similar to many previous
temporal planners, we do not permit two versions of the same action to overlap.
Consequently, the current implementation of ITSAT does not allow self-overlapping
actions. However, the specification of PDDL2.1 allows plans to have such actions,
which have been shown to be necessary for solving certain temporal problems (Fox &
Long, 2007). Our experimental results indicate that this restriction does not render
ITSAT incapable of solving current benchmark problems. Nevertheless, it has been
shown that, in theory, having self-overlapping actions may cause the complexity of
temporal planning to become EXPSPACE-hard rather than PSPACE-hard (Rintanen,
2007).
 Our formalism does not allow two or more events to be simultaneously applied to any
state. As an example of cases where such a simultaneity is required, consider two
temporal actions a and b, such that the starting event of a adds an invariant of b,
552

fiITSAT: An Efficient SAT-Based Temporal Planner

and the starting event of b adds an invariant of a. In this case, it might be necessary
to simultaneously apply the starting event of both actions to a given state. It is not
clear from the specification of PDDL2.1 whether such a simultaneity is permitted or
not. On the other hand, it has been shown that almost none of current benchmark
problems require such a simultaneity for being solvable (Rankooh & Ghassem-Sani,
2013).
 PDDL2.1 allows the usage of numerical variables. This is not supported by ITSAT.
PDDL2.1 also allows duration dependent effects and state dependent durations for
actions in numerical planning problems. These features are not supported by ITSAT
either; because ITSAT does not currently handle numerical fluents.
 According to our formalism, the duration of a temporal action is defined by an (=
?duration x) assignment, where x is a rational number or a function specifying
the actual duration of that action. PDDL2.1, on the other hand, also allows using
inequalities such as ( ?duration x) and ( ?duration x) to define a range for
the duration of any temporal action. Nevertheless, the current benchmark problems
do not include such inequalities. Although the current implementation of ITSAT does
not support these inequalities, it is quite easy to include this feature, as these kinds
of constraints on the duration of actions are handled by Simple Temporal Problems
(Dechter et al., 1991).

3. Preprocessing Phase
Preprocessing is an important phase in many planners. The main objective of this phase
is to extract certain information from the problem. This information can later be used to
enhance search performance. One important issue that should be addressed when devising
any preprocessing method is the correctness of extracted information. In other words, the
constraints inferred during the preprocessing phase must be correct in the sense that, it
does not cause the planner to become incapable of finding valid plans. Moreover, for a
preprocessing method to be effective, it is required to be performed in at most polynomial
time. In this section, we explain two different preprocessing methods used by ITSAT: mutual
exclusion analysis and action compression. We also formally prove that these methods are
both correct and can be performed in polynomial time.
3.1 Mutual Exclusion Analysis
Mutual exclusion analysis is a preprocessing method to find pairs of propositions that cannot be mutually true in any state of a valid plan. SAT-based planners typically add an
explicit clause to their SAT formula for each pair of mutually exclusive propositions. Such
clauses prevent mutually exclusive pairs of propositions from being true true at the same
time. Although such information can be obtained through the search phase itself, by acquiring it beforehand, one can prune the search tree of the SAT solver and thereby improve
performance.
Polynomial time mutual exclusion analysis for classical planning problems was originally performed by constructing planning graphs, a data structure which was introduced
553

fiRankooh & Ghassem-Sani

in GRAPHPLAN (Blum & Furst, 1997). It has been shown that the mutual exclusion
information obtained from planning graphs can be quite effective in improving the performance of SAT-based planners (Gerevini & Schubert, 1998). Other methods have also been
introduced to compute n-way mutexes (instead of the pairwise mutexes computed by the
planning graphs). The hn heuristic (Haslum & Geffner, 2000), which analyzes the reachability of any set of n propositions from the initial state, is an example of such methods.
It has been shown that a generalization of the hn heuristic can be efficiently computed by
using a syntactic regression operation (Rintanen & Gretton, 2013).
The method used by ITSAT for finding mutual exclusion relations is based on the
planning graph analysis. A classical planning graph is a layered structure. The first layer
includes all the propositions that are present in the initial state of the problem. In each
layer of planning graph, mutual exclusion (mutex ) relations between pairs of proposition
are computed. Two propositions are non-mutex in the first layer if and only if they are
both present in the initial state. An action is applicable in a layer if all its preconditions
are non-mutex in that layer. Two different actions are mutex in layer i, if at least one of
the following conditions holds: 1) they have interference with each other (i.e., one action
deletes any effect of the other action), 2) they have conflict with each other (i.e., one action
deletes any precondition of the other action), or 3) their preconditions are mutex in layer i.
Layer i + 1 includes all the effects of the actions applicable in layer i. Two propositions that
are mutex in layer i become non-mutex in layer i + 1 if they are produced by non-mutex
actions of layer i. To transfer propositions from one layer to the next layer, there exists a
special noopp action for each proposition p that both requires and adds p. The construction
of planning graph may continue until no change take places between two consecutive layers.
In that case, we say the graph has leveled off.
Planning graphs have previously been employed to tackle temporal planning problems
(Smith & Weld, 1999). In fact, the first completely domain-independent temporal planner
called TGP, was an extension of GRAPHPLAN (Blum & Furst, 1997). TGP requires all
preconditions of each temporal action to be preserved throughout the time that the action
is open, and also does not allow actions to have effects upon starting. As a result, TGP
is not compatible with the requirements of PDDL2.1. TPSYS (Garrido et al., 2002), an
extension of TGP, is another planning graph based temporal planner that can produce
plans in domains with required concurrency. Similar to GRAPHPLAN, in addition to the
construction of a planning graph, both TPSYS and TGP perform a backward search for a
valid temporal plan.
LPGP (Long & Fox, 2003) is another planning graph based temporal planner. In LPGP,
the mutex relations between proposition and actions are computed by considering only
the causal constraints of the problem; whereas the temporal constraints are taken into
account later while a plan is being extracted by solving a Linear Programming (LP) problem.
Omitting the temporal constraints of the problem is done by converting the given temporal
problem into a classical problem. As a result, the graph construction of LPGP is very
similar to that of GRAPHPLAN.
As mentioned earlier, in ITSAT, the temporal constraints of the problem are considered
only after a causally valid plan is produced. Therefore, those constraints are not needed to
be dealt with in the planning graph construction phase. This makes the graph structure of
LPGP suitable for ITSAT. Here, we explain the graph construction phase of LPGP. The
554

fiITSAT: An Efficient SAT-Based Temporal Planner

correctness of mutual exclusion information obtained by this method is essential for the
correctness of our action compression and SAT encoding methods. However, the description
of LPGP was not accompanied by any formal proof of correctness. Therefore, here, we
formally prove the correctness and tractability of this preprocessing method.
Definition 10 (causal abstraction of temporal problems). Let P = (I, G, A) be a
temporal planning problem and Ac be a set of classical actions such that for each a  A
there are exactly three classical actions as , ai , and ae in Ac , with the following properties:
 pre(as ) = pre(start(a))  (inv(a)  add(a))
 add(as ) = add(start(a))  {opena }, where opena is a new proposition specifying that
a is started but not yet finished
 del(as ) = del(start(a))  add(start(a))
 pre(ai ) = inv(a)  {opena }
 add(ai ) = inv(a)  {opena }
 del(ai ) = 
 pre(ae ) = pre(end(a))  {opena }
 add(ae ) = add(end(a))
 del(ae ) = (del(end(a))  add(end(a)))  {opena }
The causal abstraction of P is the classical problem P c = (state(I), G, Ac ).
In fact, by Definition 10, to produce a causal abstraction of a given temporal planning problem, we split any temporal action a into three classical actions as , ai , and ae .
Actions as and ae correspond respectively to the starting and ending events of a. In addition to their normal effects and preconditions, as adds a special proposition named opena ,
which is required and deleted by ae . The action ai is called the invariant checking action
of a, and requires all invariants of a plus opena as its preconditions, and produces opena as
its effect.
For a given temporal planning problem P = (I, G, A), ITSAT produces
c
P = (state(I), G, Ac ) i.e., the causal abstraction of P. ITSAT then constructs a classical planning graph for P c .
The planning graph in ITSAT is very similar to that of GRAPHPLAN. There is only one
difference between the planning graphs of these two planners. In GRAPHPLAN, as mentioned earlier, all propositions are propagated through layers by the so-called noop actions.
However, in ITSAT, there is an exception to this usage of noop actions: the new proposition
of form opena introduced by our causal abstraction of action a. This particular proposition
is propagated by ai , the invariant checking action of a. Therefore, ai can be seen as a new
kind of noop action used to cover the invariants during the reasoning about mutex relations.

555

fiRankooh & Ghassem-Sani

Theorem 1. Let P = (I, G, A) be a temporal planning problem and P c = (state(I), G, Ac )
be the causal abstraction of P. Let  = he1 , ..., en i be any finite sequence of events that is
applicable in I, and sn = succ(I, ). Then the following conditions must hold:
 If two propositions p and q are both members of state(sn ), then p and q are non-mutex
in the layer n of the planning graph of P c .
 If proposition p is a member of state(sn ), and action a is a member of agenda(sn ),
then p and opena are non-mutex in layer n of the planning graph of P c .
Proof. See Appendix A.
After a planning graph has been leveled off, all the subsequent extensions of the graph
has no effect on the new layers. Therefore, if two propositions are mutex in the last layer of
a leveled-off graph, they will remain mutex in all subsequently produced layers. In this case,
Theorem 1 implies that such pairs of propositions can never appear at the same temporal
state during the execution of a valid temporal plan. The only matter that remains is to
show that the mutual exclusion analysis of ITSAT can be performed in polynomial time.
Let P be a temporal planning problem, and P c be the causal abstraction of P. It can be
deduced from Definition 10, that the size of P c is greater than that of P only by a constant
factor. The process of constructing the planning graph of P c can be obtained by modifying
the construction process of planning graphs in GRAPHPLAN planner, in such a way that
for any temporal action a, noopopena is never used. GRAPHPLAN constructs its planning
graphs in polynomial time (Blum & Furst, 1997). Therefore, the overall time needed for
the mutual exclusion analysis of ITSAT is also polynomial in the size of any given temporal
planning problem.
3.2 Action Compression
Temporal actions can have a variety of temporal relations with one another. A popular
model for representing temporal relations between actions was initially introduced by James
Allen (1984). The model included 13 possible temporal relations between any two actions.
Some of Allens temporal relations require the starting and/or ending events of actions to
be executed simultaneously. As it was mentioned in Section 2.2, none of the temporal plans
produced by ITSAT can necessitate such a simultaneity. As a result, the set of temporal
relations between any two temporal actions will be confined to a proper subset of all Allens
temporal relations. These possible temporal relations are depicted in Figure 7. As it is
shown in Figure 7, in 4 out of 6 types of these relations, the actions are concurrent, i.e.,
there exists a time in which the two actions are both being executed. Such a concurrency is
unnecessary for solving some temporal planning problems. If we know that two actions are
not required to be concurrently executed, in order to find a valid plan, checking only the
two temporal relations depicted in Figure 7-(c) is sufficient in the searching phase of any
planner. However, if all valid plans include concurrent executions of two or more actions,
restricting the temporal relations of actions to just the two relations depicted in Figure
7-(c) will render the planner incomplete.

556

fiITSAT: An Efficient SAT-Based Temporal Planner

Figure 7. Temporal relations between two PDDL2.1 actions

Definition 11 (compression-safe sets of actions and compressed plans). Let
P = (I, G, A) be a temporal planning problem for which there exists at least one valid
temporal plan, and A be a subset of A. We say A is compression-safe for P, if there exists
a causally valid plan for P that is compressed with respect to A . A causally valid plan
 = he1 , ..., en i is compressed with respect to A if it has the following property:
 For each k, if ek is the starting event of action a  A , then ek+1 is the ending event
of a.
According to Definition 11, the starting and ending events of all members of A are
assumed to be executed consecutively in at least one causally valid plan. Therefore, while
that plan is being executed, no other event is causally needed to happen between the starting
and ending of any member of A . This suggests that members of A can be regarded as a
single event in the environment, rather than having two separate starting and ending events.
557

fiRankooh & Ghassem-Sani

In other words, for each member of A , we can compress the starting and ending events
into a single event without rendering the problem unsolvable. As an example, consider the
DRIVERLOGSHIFT temporal planning problem presented in Example 3. The plan  presented
in Example 3 shows that the set of all LOAD, MOVE, and UNLOAD actions is a compressionsafe set of actions for that problem. A straightforward analysis of this example shows that
neither of WORK actions presented in Example 3 can be a member of any compression-safe
subset of actions.
Note that, according to Definition 11, only a causally valid plan can be regarded as a
compressed sequence of events. Although the concept of compression can be extended to
cover even those sequences of events that do not lead to any goal state, for the sake of
simplicity, we have focused our attention to only those sequences that are causally valid
plans, and defined compression-safe actions only for solvable temporal planning problems.
As we explain later in Section 4, the information obtained by our compression-safety analysis
is incorporated into the encoding of the problem by adding some extra SAT formulae, which
makes the problem at hand tighter. In other words, this information is only used to prune
the search space of the SAT solver. As a result, our handling of compression-safety can
never cause the planner to produce any (invalid) plan for an unsolvable planning problem.
Safe action compression has been employed before in the field of temporal planning
(Coles et al., 2009). It has been shown that in the temporal problems that do not possess the
property of required concurrency, all temporal actions can be safely compressed into classical
actions (Cushing et al., 2007). A temporal problem is said to have required concurrency, if
its every valid temporal plan includes at least one action whose execution overlaps with the
execution of some other action. In the problems without required concurrency, all temporal
actions can be compressed into classical actions. In this case, the problem is transformed
into a classical planning problem. This phenomenon is completely consistent with the
semantics of Definition 11, as it can be easily shown that in the problems without required
concurrency, the set of all actions is indeed a compression-safe set of actions. However,
as it is the case in Example 3, even when the problem does have the required concurrency
property, there may still exist a non-empty compression-safe set of actions.
CRIKEY3 and its successor, POPF, are two state-space based temporal planners that
detect the compression-safe actions as a preprocessing task (Coles et al., 2009). However,
the concept of compression-safety in those planners is different from what we presented
in Definition 11. CRIKEY3 does not assume that the ending event of a compression-safe
action must be executed immediately after its corresponding starting event. Instead, once
the starting event of a compression-safe action is applied to a state, using a simple inference
method, CRIKEY3 can determine when to apply the corresponding ending event. This
method can reduce the branching factor of the search space in state-space based temporal
planning. Here, we show that by using the idea of detecting compression-safe actions,
one can significantly reduce the search space of the satisfiability checking based temporal
planning. As it is later explained in Section 4.4, for each compression-safe action a, we add
a clause to the SAT formula to guarantee that the starting event of a is present in a step
if and only if the ending event of a is present in the same step. These clauses can be used
to prune the search tree when the SAT solver is checking the satisfiability of the produced
formula.
CRIKEY3 considers action a to be compression-safe if the following two conditions hold:
558

fiITSAT: An Efficient SAT-Based Temporal Planner

 pre(end(a))  inv(a)
 del(end(a)) = 
Figure 8-(a) shows a temporal plan that is executed to reach proposition q. In this example the ending event of action b does not have any precondition or delete effect. Therefore,
CRIKEY3 considers b to be compression safe. However, if our goal is to produce q, the
singleton A = {b} is not a compression-safe set by Definition 11. In fact, the method used
by CRIKEY3 has been specifically devised for the state-space based temporal planners, and
cannot be easily employed by the SAT-based planners such as ITSAT. In contrast, as it is
later shown, our method can be easily used by both state-space based temporal planners
and SAT-based planners.
There are also cases where the method used by CRIKEY3 cannot detect actions that
are compression-safe according to Definition 11. Consider the plan depicted in Figure 8(b). Suppose that proposition p is the only member of the initial state, and the goal is
to produce proposition g. In this plan, actions a and b must be executed consecutively to
produce g. That is because p and q, which are respectively the overall conditions of a and
b are mutually exclusive, and can never be true together. However, neither a nor b has
the second property required by CRIKEY3 to be regarded as a compression-safe action. In
this section we show how the mutex information can be used for detecting compression-safe
actions.
Definition 12 (swappable events). Let a and a be two different temporal actions, e be
the starting or ending event of a, and e be the starting or ending event of a . We say e and
e are swappable if all the following conditions hold:
 e and e do not have interference with each other: add(e)  del(e ) =  and add(e ) 
del(e) = .
 e and e do not have conflict with each other: del(e)  (pre(e )  inv(a )) =  and
del(e )  (pre(e)  inv(a)) = .
 e and e are not supporting each other: add(e)  (pre(e )  (inv(a )  add(e ))) = 
and add(e )  (pre(e)  (inv(a)  add(e))) = .
According to Definition 12, two events are swappable if there is no causal relation
between them. This means in any causally valid plan  = he1 , ..., e, e , ..., en i, we can swap
e and e to reach another causally valid plan  = he1 , ..., e , e, ..., en i. We can use such
swapping to reorder the events of a given causally valid plan without falsifying it.
Consider a causally valid plan  = he1 , ..., en i. Let ei and ej be the starting and
ending event of the same action. If all other events of this plan are swappable with ej ,
then, by repeatedly swapping, one can reorder  to produce another causally valid plan
  = he1 , ..., ei , ej , ei+1 , ..., ej1 , ej+1 , ..., en i, in which ei and ej are two consecutive events.
Therefore, here {a} is a compression-safe set. In this case, we say that a is compressed
towards its start. Similarly, if every event of the plan other than ei and ej is swappable
with ei , then, by repeatedly swapping, one can reorder  to produce the causally valid
plan   = he1 , ..., ei1 , ei+1 , ..., ej1 , ei , ej , ..., en i. Once again, we can conclude that {a} is
a compression-safe set. In this latter case, we say that a is compressed towards its end.
559

fiRankooh & Ghassem-Sani

Figure 8. Temporal actions that are not regarded as compressible by ITSAT (a) and CRIKEY3 (b)

To find out whether it is safe to compress a given action a, there is no need to check if
all events are swappable with the starting and/or ending events of a. In fact, by considering
the mutex relations obtained from the planning graph of the problem, we already know that
some events can never be executed while a is open. This information can be effectively used
to find out if a given set of actions is compression-safe.
Definition 13 (compressible actions). Let P = (I, G, A) be a temporal planning problem, and a  A be a particular temporal action. We say that a is compressible towards its
start, if for every event e such that e is the starting or ending event of a  A  {a}, at least
one of the following conditions holds:
 A precondition or add effect of e is mutex with opena in the last layer of the leveled-off
planning graph of the causal abstraction of P.
 e is swappable with end(a).
560

fiITSAT: An Efficient SAT-Based Temporal Planner

Similarly, we say that a is compressible towards its end, if for every event e such that e is
the starting or ending event of a  A  {a}, at least one of the following conditions holds:
 A precondition or add effect of e is mutex with opena in the last layer of the leveled-off
planning graph of the causal abstraction of P.
 e is swappable with start(a).
Theorem 2. Let P = (I, G, A) be a solvable temporal planning problem. Let A be the set
of every member of A that is either compressible towards its start or compressible towards
its end. A is compression-safe for P.
Proof. See Appendix A.
We now give an example for further clarification of this matter.
Example 4. Let P = (I, G, A) be a temporal planning problem, where A is the set of
three temporal actions a, b, and c. Consider the hypothetical causally valid plan depicted
in Figure (9-a), where the execution of action a includes the execution of action b that in
turn includes the execution of action c. Assume that a is compressible towards its start,
and b is compressible towards its end. We show how this plan can be converted to another
causally valid plan in which a, b, and c are being executed sequentially. Figures (9-b) and
(9-c) show the results of doing two consecutive swaps by which b is compressed towards its
end. The starting event of b has been swapped with the starting event of c to transform
the plan of Figure (9-a) into the plan of Figure (9-b). Since b is compressible towards its
end, this swapping cannot result in an invalid plan. Similarly, the starting event of b has
been swapped with the ending event of c to transform the plan of Figure (9-b) into the
plan of Figure (9-c). Figures (9-d) to (9-g) show the results of doing four consecutive swaps
by which a is compressed towards its start. As a result of doing these swaps, the fully
sequential plan shown in figure (9-g) is produced. This implies that even if a planner does
not allow the execution of any event while a or b is open, it will still be capable of producing
the temporally valid plan of Figure (9-g).
For any given problem P = (I, G, A), ITSAT computes the compression-safe set A
of Theorem 2. To check the first condition of Definition 13, ITSAT needs to construct
a planning graph for the causal abstraction of P which, as we showed in the previous
subsection, can be done in polynomial time. For the second condition of Definition 13, it
suffices to check every possible pair of events to see if they are swappable. Since this can
be done for each pair in constant time, the total time will be O(|A|2 ). We conclude that
finding A can be performed in polynomial time.
The method described here for finding compression-safe actions can be used by statespace temporal planners, too. State-space temporal planners can be divided into two categories. The first category includes the planners that are based on the so-called decision
epoch planning method (Cushing et al., 2007). Examples of decision epoch planners are
TP4 (Haslum, 2006), SAPA (Do & Kambhampati, 2003), and TFD (Eyerich et al., 2009).
561

fiRankooh & Ghassem-Sani

Figure 9. Action compression

In this method, the start of each action is restricted to be immediately after the start or
end of another action. Each state has an explicit time-stamp. When an action is applied to
a state, the starting time of the action will be set to time-stamp of that state. As a result,
once the starting event of an action is added to the plan, the time of its corresponding
ending event will be exactly known. When searching for a valid plan, in each state, the
562

fiITSAT: An Efficient SAT-Based Temporal Planner

planner has to make a decision between either advancing to the time of the ending event of
an open action, or to open a new action. However, if we know that an action is compressionsafe, the planner can advance the time to the ending of that action and thereby prune the
search space. Plans produced in this way might have larger makespans in comparison to
those produced without pruning the search space. Nevertheless, the produced plans can
be rescheduled to find plans with improved makespans by the method we explain later in
Section 6.
An alternative approach for the state space search is the so-called temporally lifted
progression planning, which has been proved to be complete for PDDL2.1 (Fox & Long,
2003). CRIKEY3 and POPF are examples of the planners that are using this approach.
Each state in the temporally lifted progression planning represents a permutation of a
number of events. At each state, the consistency of temporal constraints imposed by the
sequence of events in that state is checked by solving a Simple Temporal Problem (STP).
Similar to the decision epoch planning, in each state, there may exist two possible choices: to
add the ending event of an open action, or to open a new action. However, for compressionsafe actions, the ending event of actions can be applied immediately after the starting event,
which in turn reduces the future choices of the planner. We will show in Section 6 that by
taking advantage of compression-safe actions in this manner, the planner can still visit all
the STPs of all causally valid permutations of events.
Table 1 shows the comparison between the average percentage of actions regarded as
compression-safe by our new method and the method used in CRIKEY3 and POPF, in various temporal planning domains. We will explain more information regarding our benchmark
domains and problems later in Section 6. At it can be seen in Table 1, our compression
method can detect significantly more compressible actions in a number of benchmark domain.

4. Encoding Phase
In this section, we explain how the abstract causal problem associated with a given temporal
problem is encoded into a SAT formula. As in classical planning, there exist more than one
way to translate a particular planning problem into its corresponding SAT formula. Previous
investigations in the field of classical planning show that the choice of the encoding method
can have a major impact on the efficiency of a SAT-based planner. As mentioned earlier,
the most successful SAT-based classical planners have used special encoding methods that
are based on the so-called -step and -step semantics of valid plans (Rintanen et al., 2006).
In this section, we define the temporal versions of the classical -step and -step plans.
We also show how exactly these semantics can be used to translate a given temporal planning
problem into a SAT formula. We introduce a -step encoding and two different types of
-step encodings in temporal planning. The -step and the first -step encoding methods
are temporal versions of the classical -step and -step encodings. Similar to their classical
versions, in these new encodings, a few restrictive simplifying assumptions are assumed to
hold. Our second type of the -step encoding, however, is obtained by relaxing one of these
assumptions. As we later show, this new -step encoding often requires fewer steps than the
other one. Besides, as our experimental results show, among these new encoding methods,
the second -step encoding results in the best performance of ITSAT in terms of both speed
563

fiRankooh & Ghassem-Sani

domain
zenotravel
rovers
depots
airport
pegsol
crewplanning
openstacks
elevators
sokoban
parcprinter
driverlog
floortile
mapanalyser
matchcellar
parking
rtam
satellite
storage
turnandopen
tms
driverlogshift
matchlift

CRIKEY3
12
85
100
0
100
100
100
100
100
100
100
100
13
96
100
88
100
100
95
73
98
95

ITSAT
100
100
100
95
100
100
100
100
100
100
98
100
96
96
100
95
98
99
99
75
98
95

Table 1: Average Percentage of Compressed Actions
and memory usage of the planner. The necessary proofs of soundness and completeness of
our encoding methods are also given in this section.
4.1 Parallel Semantics for Causally Valid Plans
As mentioned earlier, the classical -step semantics permits the parallel execution of more
than one action in each step, only if the validity of the plan does not depend on the execution
order of those actions. This can simply be guaranteed by adding a particular clause for
each pair of mutually exclusive actions to ensure that such actions will not be included in
the same step. However, this strategy does not work in temporal planning. In temporal
planning, because of the temporal constraints imposed on the starting and ending events
of the actions, the validity of a particular ordering of events in a certain step, also depends
on the ordering of events in the other steps. Nevertheless, in ITSAT this problem has been
tackled by separating the causal and temporal reasoning phases. In general, if we focus only
on finding causally valid plans, and postpone the scheduling phase, the mentioned problem,
about checking the feasibility of imposing different orderings of events in each step, will no
longer exist. We next introduce our semantics for causally valid -step and -step temporal
plans.

564

fiITSAT: An Efficient SAT-Based Temporal Planner

Definition 14 (temporal -steps and -steps). Let E = {e1 , ..., en } be a set of events,
and s1 and s2 be two temporal states. S is a temporal -step from s1 to s2 only if for all
one-to-one ordering functions O : {1, ..., n}  {1, ..., n} (i.e., all permutations of events),
we have: s2 = succ(s1 , heO(1) , ..., eO(n) i). S is a temporal -step from s1 to s2 only if there
exist at least a one-to-one ordering function O : {1, ..., n}  {1, ..., n} (i.e., at least one
permutation of events), such that: s2 = succ(s1 , heO(1) , ..., eO(n) i).
Definition 15 (causally valid -step and -step plans). Let P = (I, G, A) be a temporal planning problem. Suppose s0 , ..., sn is a sequence of temporal states such that s0 = I,
G  state(sn ), and agenda(sn ) = . If for each 1  i  n, Stepi is a -step (-step) from
si1 to si , then we call the sequence  = hStep1 , ..., Stepn i, a causally valid -step (-step)
plan for P. We say that hs0 , ..., sn i is the state transition sequence of .
Classical -step and -step encodings (Rintanen et al., 2006) are based on the -step
and -step semantics for classical valid plans, respectively. However, in the -step encoding,
for the sake of improving the efficiency of the planner, the following restrictive rules have
been also enforced on the semantics.
 Rule 1: Instead of accepting all possible orderings among the actions of each step,
only a fixed arbitrary ordering is allowed. As a result, by this rule, the execution of a
step necessitates the execution of its actions according to this fixed ordering.
 Rule 2: Preconditions of all actions of each step must be members of the state immediately before that step. Similarly, the effects of all actions of each step must be
consistent with the state reached immediately after that step.
In this section, we present one -step and two -step encodings for planning in causal
abstractions of temporal planning. Our encodings are based on the -step and -step
semantics for causally valid plans (Definition 15). By considering events, instead of actions,
both of the above rules can be applied to temporal planning, too. While in our first -step
encoding, we respect both rules, in our second -step encoding, the second restrictive rule
is relaxed.
In fact, the second rule imposes some serious restrictions on the applicability of actions
in each step. For instance, it prevents a proposition from being both produced and used
in the same step of a plan. Neither does it allow the deletion and production of any
particular proposition in the same step. By relaxing these restrictions, the encoding can be
more compact, i.e., the relaxation permits more events to occur in each step. In classical
planning, a less relaxed form of Rule 2 has been introduced so that the effects of actions
in each step can be used by other actions in that step (Wehrle & Rintanen, 2007). Here,
however, we totally relax Rule 2 and allow each proposition to be required, added, and
deleted many times in the same step.
Before explaining our SAT encodings, we first define SAT variables and auxilary clauses
commonly used in our three encoding methods. Let  = hStep1 , ..., Stepn i be a causally
valid -step (or -step) plan for a given temporal planning problem P = (I, G, A), and
hs0 , ..., sn i be the state transition sequence of . In order to encode P into a SAT formula
whose model can be translated back into , we use the following SAT variables:
565

fiRankooh & Ghassem-Sani

 For each proposition p, and each t such that 0  t  n, we define a SAT variable pt .
Assigning true (f alse) to pt implies that p is (is not) a member of state(st ).
 For each action a  A, and each t such that 0  t  n, we define a SAT variable at .
Assigning true (f alse) to at implies that a is (is not) a member of agenda(st ).
 For each event e such that e is the starting or ending event of an action in A, and
each t such that 1  t  n, we define a SAT variable et . Assigning true (f alse) to et
implies that e is (is not) a member of Stept .
If a SAT formula is satisfiable there exists a model for it. This model is a binary function
that assigns a value of true or f alse to each variable of the formula in such a way that the
formula is satisfied. For each of our encoding methods, if the produced formula has a
model M , one can easily translate M to a corresponding causally valid -step (or -step)
plan, using the description given above about the variables of the formula. We denote the
resulting plan by plan(M ). For showing the correctness of a particular encoding method,
two issues must be addressed. First, we must show that if there exists a causally valid
plan for the temporal problem P, then the encoding of P has a model. We call this the
completeness of our encoding method. Second, we must show that if the encoding of P has
a model M , then plan(M ) is a causally valid plan for P. This is called the soundness of
our encoding method.
Note that here, we prove the finite-horizon completeness and not the -completeness for
our encodings. In other words, we prove that if there exists a -step (or -step) plan  with
l steps for a given problem, then the problem can be translated by our -step (or -step)
encoding into a satisfiable SAT formula with at most l steps, so that the model of the formula
can be translated back into . On the other hand, proof of -completeness would need the
value of l to be determined. Our proofs of the finite-horizon completeness could have implied
the -completeness if at least an upper bound on the value of l had been determined. Recent
research in the field of classical planning has shown that in some classical planning domains,
tight upper bounds on the length of optimal plans can be determined (Rintanen & Gretton,
2013). However, determining such upper bounds in temporal planning is beyond the scope
of the current work. To find a causally valid plan, ITSAT starts from an encoding with only
one step, and sequentially produces and tries to satisfy formulae with increasing number
of steps, until a satisfiable formula will be encountered or a predefined time limit will be
reached.
In classical SAT-based planning, in order to produce linear-size encodings for -step
and -step semantics of valid plans, special sets of clauses, named chains, have been used
(Rintanen et al., 2006). Since we have also used these chains in ITSAT, the formal definition
of their temporal version is given here. Let e1 , ..., en be an arbitrary fixed ordering of all
events, E and R be two sets of events, k be a natural number, and m be special symbol
that assigns a unique name to the chain at hand. We define chain(e1 , ..., en ; E; R; k; m) by
the conjunction of formulae (C-1) to (C-3) stated below.
(C-1)

V

{eki  bkj,m |i < j, ei  E, ej  R, {ei+1 , ..., ej1 }  R = }

(C-2)

V

{bki,m  bkj,m |i < j, {ei , ej }  R, {ei+1 , ..., ej1 }  R = }
566

fiITSAT: An Efficient SAT-Based Temporal Planner

(C-3)

V

{bki,m  eki |ei  R}

The above formulae in fact encodes a message passing strategy. The symbol m specifies
the name of the message and is used to distinguish the SAT variables of a certain chain
from those of other chains. The number k specifies the step whose variables are affected
by the message to be produced. The message may be produced by any member of E. The
receivers of the message are the members of R. If bki,m is true, it means that message is
received by the i-th event of the k-th step of the formula. If ei is a member of E, and eki
is true, then a message will be produced and sent to ej , which is the first member of R
located after ei in the fixed ordering. This is represented in chain(e1 , ..., en ; E; R; k; m) by
the clauses of the form eki  bkj,m in formula (C-1). Once the message is produced, it will be
transmitted forward according to the fixed ordering by the clauses of the form bki,m  bkj,m
in formula (C-2). If an event ei receives the message, its corresponding SAT variable will
bef alse by the clauses of the form bki,m  eki in formula (C-3). In fact, the members of R
that receive the message will certainly be excluded from the final plan.
We now present some examples to show how these chains are practically used to guarantee particular characteristics that the output plan will have.
Example 5. Assume that e1 , e2 , e3 , e4 are four events. Suppose that proposition x is
required by e1 and e4 , deleted by e2 , and added by e3 . Also assume that four propositions
p1 , ..., p4 are respectively added by e1 , ..., e4 . Consider the following two cases:
 Case 1: we want to prevent proposition x from being both required and deleted in the
same step, say k, of the final plan. For this purpose, we can add the conjunction of
chain(e1 , ..., e4 ; E; R; k; mx1 ) and chain(e4 , ..., e1 ; E; R; k; mx2 ) to the formula, where E
is the set of all events that delete x (i.e., E = {e2 }), and R is the set of all events that
require x (i.e., R = {e1 , e4 }). Note that mx1 and mx2 are two symbols that enable us to
distinguish between the SAT variables used in these two different chains. In this case,
adding chain(e1 , ..., e4 ; E; R; k; mx1 ) will add the following formulae to the encoding of
the problem:
 ek2  bk4,mx
1



bk1,mx
1



bk4,mx
1

 bk1,mx  ek1
1

 bk4,mx  ek4
1

Assume that there exists a model M for the produced SAT encoding such that
M (ek2 ) = true. In this case, since M satisfies ek2  bk4,mx , we have M (bk4,mx ) = true.
1
1
Consequently, since M satisfies bk4,mx  ek4 , we have M (ek4 ) = f alse. In other words,
1
if e2 is a member of step k, then e4 cannot be a member of the same step. Similarly,
adding chain(e4 , ..., e1 ; E; R; k; mx2 ) will add the following formulae to the encoding of
the problem:
 ek2  bk1,mx
2

567

fiRankooh & Ghassem-Sani

 bk4,mx  bk1,mx
2



bk4,mx
2

2



ek4

 bk1,mx  ek1
2

An argument similar to the one given for chain(e1 , ..., e4 ; E; R; k; mx1 ) shows that after
adding chain(e4 , ..., e1 ; E; R; k; x2 ), if e2 is a member of step k, then e1 cannot be a
member of the same step. As a result, by adding both mentioned chains to the SAT
formula, if the execution of step k produces p2 , it then cannot produce p1 or p4 . This
is actually how the occurrence of conflicting actions in each step of the final plan is
avoided by the linear-size classical -step encoding (Rintanen et al., 2006).
 Case 2: we allow proposition x to be both required and deleted in a particular step
k only if the deleting event does not precede the requiring event in the fixed ordering
he1 , e2 , e3 , s4 i. For this purpose, we only need to add chain(e1 , ..., en ; E; R; k; mx ) to
the formula, where E and R are the same as E and R in case 1. In this case, if the
execution of step k produces p2 , it can also produce p1 , but not p4 . This strategy, too,
was initially introduced for the linear-size classical -step encoding (Rintanen et al.,
2006).
Note that if one admits the second restrictive rule mentioned above, which is the case in
classical -step and -step encodings, no proposition can be added by an event in any step
while being deleted by another event in the same step. As a result, if the execution of step
k produces p2 , it cannot produce p3 in any of the above cases.

4.2 Temporal Versions of Classical -step and -step Encodings
We first present the temporal versions of the classical -step and -step encodings. Similar
to their classical forms, in the temporal versions of these encodings, we assume an arbitrary
but fixed ordering e1 , ..., en for all events of the given temporal problem P = (I, G, A).
We also assume that the output plan will have a fixed number of steps, denoted by l.
Let  = hStep1 , ..., Stepl i be an output plan for P, and hs0 , ..., sl i be the state transition
sequence of . For each event e, let action(e) be the member of A whose starting or
ending event is equal to e. Let P be the set of all propositions of P. For each proposition
p  P , let Ep = {e|p  del(e)}, Ep+ = {e|p  add(e)} and Rp = {e|p  pre(e)}  {e|p 
inv(action(e))  add(e)}. Moreover, assume that there are two dummy events e0 and en+1 ,
which do not have any precondition, add effect, or delete effect.
4.2.1 The -step Encoding
Given the temporal problem P = (I, G, A), we produce the SAT-formula l , which is based
on the -step semantics of causally valid plans, for P by the conjunction of all formulae
described below.
V
(-1) {p0 |p  state(I)}  {p0 |p 
/ state(I)}
V l
(-2) {p |p  G}
568

fiITSAT: An Efficient SAT-Based Temporal Planner

(-3)

V

{a0 |a  A}

(-4)

V

{al |a  A}

(-5)

V

{ek  pk1 |0 < k  l, p  P, e  Rp }

(-6)

V

{ek  pk |0 < k  l, p  P, e  Ep+ }

(-7)

V

(-9)

V

(-10)

V

(-11)

V

(-12)

V

{ek  ak1  ak |0 < k  l, a  A, e = start(a)}

(-13)

V

{ek  ak1  ak |0 < k  l, a  A, e = end(a)}

{ek  pk |0 < k  l, p  P, e  Ep }
V
W
(-8) {pk1  pk  eEp+ ek |0 < k  l, p  P }
{pk1  pk 

W

eEp

ek |0 < k  l, p  P }

{chain(e1 , ..., en+1 ; Ep ; Rp  {en+1 }; k; mp1 )|0 < k  l, p  P } 
{(bkn+1,mp  ak )|0 < k  l, p  inv(a)}
1

{chain(en , ..., e0 ; Ep ; Rp  {e0 }; k; mp2 )|0 < k  l, p  P } 
{(bk0,mp  ak1 )|0 < k  l, p  inv(a)}
2

Formula (-1) indicates that any member of state(s0 ) is true iff it is present in the initial state. Similarly, formula (-2) states that all members of the goal state must be true in
state(sl ). Formulae (-3) and (-4) imply that agenda(s0 ) and agenda(sl ) are both empty.
Formulae (-5) to (-7) show that when an event is applied to step k, its preconditions must
be present in state(sk1 ), and its effects must be consistent with state(sk ). Formulae (8) and (-9) are responsible for encoding the so-called explanatory frame axioms: formula
(-8) implies that if p is present after but not before step k, then there must exist at least
one event in step k that has p in its add effects. Similarly, formula (-9) implies that if p is
present before but not after step k, then there must exist at least one event in step k that
deletes p. Formulae (-10) and (-11) are added to guarantee that the events of each step
can be executed in any possible ordering. Formula (-10) implies that if p is deleted by an
event ei in step k, then p cannot be required by any other event ej of step k such that j > i.
It also implies that if p is deleted by any event in step k, and action a has p as an invariant,
then a cannot be a member of agenda(sk ). Note that in chain(e1 , ..., en+1 ; Ep ; Rp ; k; mp1 )
used in formula (-10), the value of bn+1,mp1 indicates whether or not p is deleted by any
event in step k. The reason why we are using the dummy event en+1 is to have such an
indicator. Analogously, formula (-11) implies that if p is deleted by an event ei in step k,
then p cannot be needed by any other event ej of step k such that j < i. Formula (-11)
also implies that if p is deleted by any event in step k, and action a has p as an invariant,
then a cannot be a member of agenda(sk1 ). Formulae (-12) and (-13) are responsible for
applying appropriate changes in the agendas of the states that are located before and after
each step of the final plan. Formula (-12) implies that if the starting event of an action a
is a member of the step k of the output plan, then a must be a member of agenda(sk ) but
not agenda(sk1 ). Similarly, formula (-13) implies that if the ending event of action a is a
569

fiRankooh & Ghassem-Sani

member of step k of the plan, then a must be a member of agenda(sk1 ) but not agenda(sk ).
Theorem 3 (completeness of temporal -step encoding). Let P = (I, G, A) be a
solvable temporal planning problem, {e1 , ..., en } be the set of all events of P, and  =
hStep1 , ..., Stepl i be a causally valid -step plan for P. There exists a model M for l such
that  = plan(M ).
Proof. See Appendix A.
Theorem 4 (soundness of -step encoding). Let P = (I, G, A) be a temporal planning
problem, {e1 , ..., en } be the set of all events of P, and l be the -step encoding for P. If
l has a model M , then plan(M ) is a causally valid -step plan for P.
Proof. See Appendix A.
4.2.2 The -step Encoding
In this part, we present the SAT-formula l , which is based on the -step semantics of
causally valid plans. By considering the two restrictive rules stated above, our -step
encoding is very similar to the -step encoding described previously in this section. However,
there are two major differences between these two kinds of encoding. First, our -step
encoding allows each proposition to be both required and deleted in each step, provided
that the deleting event does not precede the requiring event in the fixed ordering he1 , ..., sn i.
This is in contrast with our -step encoding, where a proposition could not be both deleted
and required in the same step of the final plan. Second, in our -step encoding, each
step may also contain both the starting and ending event of the same action. Given the
temporal problem P = (I, G, A), we produce the SAT-formula l , which is based on the
-step semantics of causally valid plans, for P by the conjunction of all formulae described
below.
V
(-1) {p0 |p  state(I)}  {p0 |p 
/ state(I)}
V l
(-2) {p |p  G}
V
(-3) {a0 |a  A}
V
(-4) {al |a  A}
V
(-5) {ek  pk1 |0 < k  l, p  P, e  Rp }
V
(-6) {ek  pk |0 < k  l, p  P, e  Ep+ }
V
(-7) {ek  pk |0 < k  l, p  P, e  Ep }
V
W
(-8) {pk1  pk  eEp+ ek |0 < k  l, p  P }
(-9)

V

{pk1  pk 

W

(-10)

V

{chain(e1 , ..., en+1 ; Ep ; Rp  {en+1 }; k; mp1 )|0 < k  l, p  P }  {(bkn+1,mp 

eEp

ek |0 < k  l, p  P }
1

ak )|0 < k  l, p  inv(a)}
570

fiITSAT: An Efficient SAT-Based Temporal Planner

(-11)

V

(-12)

V

(-13)

V

(-14)

V

(-15)

V

(-16)

V

(-17)

V

(-18)

V

(-19)

V

(-20)

V

{eki  ak1 |0 < k  l, a  A, ei = start(a), ej = end(a), i < j}
{eki  ak  ekj |0 < k  l, a  A, ei = start(a), ej = end(a), i < j}
{ekj  ak |0 < k  l, a  A, ei = start(a), ej = end(a), i < j}
{ekj  ak1  eki |0 < k  l, a  A, ei = start(a), ej = end(a), i < j}
{eki  ak1  ekj |0 < k  l, a  A, ei = start(a), ej = end(a), j < i}
{eki  ak |0 < k  l, a  A, ei = start(a), ej = end(a), j < i}
{ekj  ak  eki |0 < k  l, a  A, ei = start(a), ej = end(a), j < i}
{ekj  ak1 |0 < k  l, a  A, ei = start(a), ej = end(a), j < i}
{ak1  ak  eki |0 < k  l, a  A, ei = start(a)}
{ak1  ak  ekj |0 < k  l, a  A, ej = end(a)}

Note that formulae (-1) to (-9) are exactly the same as formulae (-1) to (-9). Similar
to our -step encoding, these formulae are responsible for the validity of the initial state, goal
state, conditions and effects of events, and also for the explanatory frame axioms explained
before. Moreover, notice that while formula (-10) is also present in our -step encoding as
formula (-10), formula (-11) is not present in l . This results in the first major difference
stated above between our -step encoding and -step encoding. Formulae (-11)to (-20)
enforce appropriate changes to agenda(sk1 ) and agenda(sk ), which are the agendas of the
states immediately before and after step k of the final plan. According to their definitions,
formulae (-11) to (-14) are added for each action a with the property that start(a) is
located before end(a) in the fixed ordering he1 , ..., en i. Formula (-11) ensures that a can
be started in step k, only if it is not open in sk1 . Formula (-12) guarantees that if a is
started but not ended in step k, it must be open in sk . Formula (-13) ensures that if a
is ended in step k, it will not be open in sk . Formula (-14) implies that if a is ended but
not started in step k, then it must be open in sk1 . Analogously, formulae (-15) to (-18)
guarantee similar properties for each action a with the property that start(a) is located
after end(a) in the fixed ordering he1 , ..., en i. Formula (-19) ensures that if a is a member
of agenda(sk ) but not agenda(sk1 ), it must be started in step k. Similarly, formula (-20)
ensures that if a is a member of agenda(sk1 ) but not agenda(sk ), it must be ended in step
k.
Since our -step encoding conforms to the two restrictive rules stated earlier in this
section, there may exist a -step causally valid plan with l steps for a given problem while
l would be unsatisfiable for the same problem. This is also the case in the linear size step encoding of the classical planning problems (Rintanen et al., 2006). However, since we
showed by Theorem 3 that our -step encoding is complete, the completeness of our -step
encoding can be proved by showing that the satisfiability of l entails the satisfiability of l .
Theorem 5 (completeness of -step encoding). Let P = (I, G, A) be a solvable temporal planning problem, {e1 , ..., en } be the set of all events of P, and  = hStep1 , ..., Stepl i
be a causally valid -step plan for P. There exists a model M for l such that  = plan(M ).
571

fiRankooh & Ghassem-Sani

Proof. See Appendix A.
Theorem 5 also shows that in our -step encoding, the number of required steps to
solve a temporal planning problem is less than (or equal to) what is required by our -step
encoding. In other words, our -step encoding is more compact than its -step counterpart.
Theorem 6 (soundness of -step encoding). Let P = (I, G, A) be a temporal planning
problem, {e1 , ..., en } be the set of all events of P, and l be the -step encoding for P. If
l has a model M , then plan(M ) is a causally valid -step plan P.
Proof. See Appendix A.
4.3 The Relaxed -step Encoding
As we mentioned in Section 4.2.2, our -step encoding allows each proposition to be both
required and deleted by any two events of the same step, only if the deleting event does not
precede the requiring event in the fixed ordering he1 , ..., en i. Besides, since formulae (-5)
and (-6) are present in both of our -step and -step encodings, no proposition can be
both added and deleted in the same step of these encodings. These restrictions, which are
also present in the classical -step and -step encodings (Rintanen et al., 2006), are lifted in
our new relaxed version of -step encoding. As a result, each proposition can be required,
added, and deleted in any step as many times as it is needed. This property has not been
previously examined in classical -step encoding, and consequently, the chaining mechanism
explained in Section 4.1 is not compatible with it. Here, we introduce a generalized version
of the chains and explain the conceptual difference with those used in classical encodings.
We also present new kinds of chains to be used specially in temporal planning for preserving
the invariants of temporal actions while the plan is being produced. Note that, similar to
our non-relaxed -step encoding, here we assume that the events of each step are executed
according to a fixed ordering he1 , ..., en i.
Let k be a natural number and e1 , ..., en be a fixed ordering of all events. For some reasons to be discussed later, we assume that if ei is the starting event of an action, then ei+1
is the ending event of that action. In other words, we assume that the ending event of each
action is located immediately after its starting event in the fixed ordering. Note that here,
we do not demand the end of an action to immediately follow its start in the final plan. We
only put this constraint on the fixed ordering. This cannot compromise the completeness of
ITSAT: the SAT solver can still choose the start of an action a from step k, choose whatever
actions are needed from steps k to k + m for an arbitrary m, and then choose the end of a
from step k +m. Moreover, suppose that there are two dummy events e0 and en+1 , which do
not have any precondition, add-effect, or delete-effect. Let P be the set of all propositions of
P. For each proposition p  P , let Ep = {e|p  del(e)}, Ep+ = {e|p  add(e)}, Op = {e|p 
inv(action(e))}  {e0 , en+1 }, and Rp = {e|p  pre(e)}  {e|p  inv(action(e))  add(e)}.
We define chain (e0 , ..., en+1 ; Ep+ ; Ep ; Rp ; k; mp ) by the conjunction of formulae (C -1) to
(C -8) stated below. Note that mp is a symbol used for distinguishing the SAT varibales
used in the formula chain (e0 , ..., en+1 ; Ep+ ; Ep ; Rp ; k; mp ) from other variables used in other
formulae.

572

fiITSAT: An Efficient SAT-Based Temporal Planner

(C -1)

V

{eki  bkj,mp |i < j, ei  Ep+ , ej  Rp  Ep , {ei+1 , ..., ej1 }  (Rp  Ep ) = }

(C -2)

V

{eki  bkj,mp |i < j, ei  Ep , ej  Rp  Ep+ , {ei+1 , ..., ej1 }  (Rp  Ep+ ) = }

V

{bki,mp  bkj,mp |i < j, ei  Rp  (Ep+  Ep ), ej  R  Ep+  Ep , {ei+1 , ..., ej1 } 
(Rp  Ep+  Ep ) = }
V
(C -4) {(bki,mp  eki )  bkj,mp |i < j, {ei , ej }  Rp  Ep+  Ep , {ei+1 , ..., ej1 }  (Rp  Ep+ 
Ep ) = }
V
(C -5) {(bki,mp  eki )  bkj,mp |i < j, {ei , ej }  Rp  Ep+  Ep , {ei+1 , ..., ej1 }  (Rp 
Ep+  Ep ) = }
V
(C -6) {bki,mp  eki |ei  Rp }

(C -3)

(C -7) bk0,mp  pk1
(C -8) bkn+1,mp  pk
In fact, chain (e0 , ..., en+1 ; Ep+ ; Ep ; Rp ; k; mp ) encodes a message passing method that is
different from that of chain(e1 , ..., en ; E; R; k; m). In chain (e0 , ..., en+1 ; Ep+ ; Ep ; Rp ; k; mp ),
the conveyed message is in fact the value of proposition p, and is therefore either true or
f alse. Similar to the message passing strategy of chain(e1 , ..., en ; E; R; k; m), the received
message is transferred only in the forward direction of the fixed ordering e1 , ..., en . Each
event in Ep+ , Ep , or Rp receives a message from its previous event in the fixed ordering.
Every event may or may not change the value of the received message. In either cases, the
message is then passed to the next event. The events in Ep+ can only change the value
of the received message to true, as these events have p in their add-effects. Similarly, the
events in Ep can only change the value of the received message to f alse. The formulae
(C -1) and (C -2) impose such changes on the value of a received message. If an event is
not a member of Ep+ or Ep , it neither adds nor deletes p, and thus, it will pass the received
message without altering its value. This is enforced by (C -3). (C -4) and (C -5) ensure
that received messages are passed without being changed by those events that are not to
be chosen for Stepk of the output plan. According to (C -6), if an event in Rp receives a
message with the value of f alse, the event cannot be chosen as a member of Stepk . That
is because members of Rp require p, which necessitates their received messages to have a
value of true. (C -7) implies that the initial value of the message produced in Stepk is equal
to the value of p in the state immediately before the execution of Stepk . Similarly, (C -8)
implies that the value of p in the state immediately after the execution of Stepk will be
equal to the final value of the message in Stepk .
Example 6. Consider the same events given in Example 5. Let E + be the set of events
that add x (i.e., E + = {a3 }), E  be the set of events that delete x (i.e., E  = {a2 }),
and R be the set of events that require x (i.e., R = {a1 , a4 }). Moreover, suppose that
there are two dummy events e0 and e5 , which do not have any precondition, add-effect, or
delete-effect. Assume that we have added chain (e0 , ..., e5 ; E + ; E  ; R  {e0 , e5 }; k; mx ) to
573

fiRankooh & Ghassem-Sani

the SAT formula. According to formulae (C -1) to (C -8), this chain is the conjunction of
the following formulae:
 ek3  bk4,mp
 ek2  bk3,mp
 bk0,mp  bk1,mp
 bk1,mp  bk2,mp
 bk4,mp  bk5,mp
 bk0,mp  ek0  bk1,mp
 bk1,mp  ek1  bk2,mp
 bk2,mp  ek2  bk3,mp
 bk3,mp  ek3  bk4,mp
 bk4,mp  ek4  bk5,mp
 bk0,mp  ek0  bk1,mp
 bk1,mp  ek1  bk2,mp
 bk2,mp  ek2  bk3,mp
 bk3,mp  ek3  bk4,mp
 bk4,mp  ek4  bk5,mp
 bk0,mp  ek0
 bk1,mp  ek1
 bk4,mp  ek4
 bk5,mp  ek5
 bk0,mp  xk1
 bk5,mp  xk
574

fiITSAT: An Efficient SAT-Based Temporal Planner

A straightforward examination shows that we can have a model M for the chain mentioned above such that M (ek0 ) = M (ek1 ) = M (ek2 ) = M (ek3 ) = M (ek4 ) = M (ek5 ) = true,
M (bk0,mp ) = M (bk1,mp ) = M (bk2,mp ) = M (bk4,mp ) = M (bk5,mp ) = true, and M (bk3,mp ) = f alse.
In other words, if x is deleted by e2 in Stepk of the final plan, it can later be produced again
by e3 , and as a result, e4 can appear in Stepk , too. Here, M (bk3,mp ) = f alse represents the
fact that x has been deleted after the execution of e2 . In this example, all four propositions
p1 , p2 , p3 , and p4 can be produced by a single step of the final plan. Note that in neither
of the cases of Example 5, producing all these propositions by only one step was possible.
This is an example of how our new -step encoding, which employs the generalized message
passing strategy, can permit more parallelism than is allowed by the temporal versions of
classical -step and -step encodings.
In addition to chain (e0 , ..., en+1 ; Ep+ ; Ep ; Rp ; k; mp ), which is responsible for tracking
the value of p inside Stepk , we also need some extra formulae to prevent p from being deleted
whenever p is an invariant of an open temporal action. Therefore, we introduce two new
ob

ob
chain formulae: chainof (e1 , ..., en+1 ; Ep ; Op ; k; mof
p ) and chain (e0 , ..., en ; Ep ; Op ; k; mp ).
Formula chainof (e1 , ..., en+1 ; Ep ; Op ; k; mof
p ) is produced by the conjunction of formulae
of
of
(C -1) to (C -4). Similar to the chains explained before, mof
p is a symbol used to distinguish the SAT varibales of this chain from those of other formulae.
V
(Cof -1) {eki  bk of |i < j, ei  Ep , ej  Op , {ei+1 , ..., ej1 }  Op = }
j,mp

(Cof -2)

V

{bk

(Cof -3)

V

{(bk

 ekj )  eki |ei  Op , ei = start(a), ej = end(a)}

(Cof -4)

V

{(bk

 ak )  eki |a  A, ei = start(a), ei  Op }

i,mof
p

 bk

j,mof
p

j,mof
p

n+1,mof
p

|i < j, {ei , ej }  Op , {ei+1 , ..., ej1 }  Op = }

Similar to chain(e1 , ..., en ; Ep ; Rp ; k; mp ), chainof (e1 , ..., en ; Ep ; Op ; k; mof
p ) represents a
message that is produced and sent in the forward direction of the fixed ordering, whenever a
proposition p is deleted by an event. (Cof -1) and (Cof -2) are responsible for the production
and propagation of the mentioned message, respectively. By (Cof -3), if the ending event of
an action a with p as its invariant receives this message in step k, then step k must also
include the starting event of a. In these cases, (Cof -3) prevents a from being open when p
is deleted. That is because we assume that in the fixed ordering, the ending event of each
action is located immediately after its starting event. (Cof -4) guarantees that if p is deleted
somewhere in step k, and an action a with p as its invariant is open after step k, then step
k must also include the starting event of a (otherwise, a is open everywhere in step k, and
thus, p, which is an invariant of a, is deleted while a is open).
In chainof (e1 , ..., en+1 ; Ep ; Op ; k; mof
p ), the message that indicates p is deleted is only sent
forward. Thus, it cannot help preserving the invariants of those members of Op that
are started prior to the deletion of p. To tackle this problem, we add another chain,
namely chainob (e0 , ..., en ; Ep ; Op ; k; mob
p ), to the formula. This chain is quite analogous to
of
of

chain (e1 , ..., en+1 ; Ep ; Op ; k; mp ), that whenever p is deleted by an event, the chain sends
575

fiRankooh & Ghassem-Sani

the message backward according to the fixed ordering. chainob (e0 , ..., en ; Ep ; Op ; k; mob
p ) is
ob
ob
produced by the conjunction of formulae (C -1) to (C -4).
(Cob -1)

V

{eki  bkj,mob |j < i, ei  Ep , ej  Op , {ej+1 , ..., ei1 }  Op = }

(Cob -2)

V

{bki,mob  bkj,mob |j < i, {ei , ej }  Op , {ej+1 , ..., ei1 }  Op = }

(Cob -3)

V

{(bki,mob  eki )  ekj |ei  Op , ei = start(a), ej = end(a)}

(Cob -4)

V

{(bk0,mob  ak1 )  ekj |a  A, ej = end(a), ej  Op }

p

p

p

p

p



We now present the SAT-formula l , which represents our relaxed -step encoding and

is based on the -step semantics of causally valid plans. l is produced by the conjunction
of all formulae described below.
V
( -1) {p0 |p  state(I)}  {p0 |p 
/ state(I)}
V
( -2) {pl |p  G}
V
( -3) {a0 |a  A}
V
( -4) {al |a  A}
V
( -5) {chain (e0 , e1 , ..., en+1 ; Ep+ ; Ep ; Rp  {e0 , en+1 }; k; mp )|0 < k  l, p  P }
( -6)

V

{chainof (e1 , ..., en+1 ; Ep ; Op ; k; mof
p )|0 < k  l, p  P }

( -7)

V

{chainob (e0 , ..., en ; Ep ; Op ; k; mob
p )|0 < k  l, p  P }

( -8)

V

( -9)

V

{eki  ak1 |0 < k  l, a  A, ei = start(a)}

( -10)

V

{ekj  ak |0 < k  l, a  A, ej = end(a)}

( -11)

V

{ekj  ak1  eki |0 < k  l, a  A, ei = start(a), ej = end(a)}

( -12)

V

( -13)

V

{ak1  ak  eki |0 < k  l, a  A, ei = start(a)}

{eki  ak  ekj |0 < k  l, a  A, ei = start(a), ej = end(a)}

{ak1  ak  ekj |0 < k  l, a  A, ej = end(a)}

( -1) ensures that any member of state(s0 ) is true if and only if that member is present in
the initial state. Similarly, ( -2) guarantees that all members of the goal state are true in
state(sl ). ( -3) and ( -4) imply that agenda(s0 ) and agenda(sl ) are both empty. ( -5),
as explained before, is responsible for imposing appropriate changes in the value of the SAT
variables, whenever proposition p is added or deleted by an event in a certain step of the
output plan. ( -6) and ( -7) prevent the invariants of an action a from being deleted
while a is open. ( -8) to ( -13) are responsible for enforcing the appropriate changes to
agenda(sk1 ) and agenda(sk ), which are agendas of the states immediately before and after
576

fiITSAT: An Efficient SAT-Based Temporal Planner

step k of the output plan. ( -8) ensures that a can be started in step k only if it is not
open in sk1 . ( -9) indicates that if a is started but not ended in step k, then it has to be
open in sk . ( -10) ensures that if a is ended in step k, it will not be open in sk . ( -11)
implies that if a is ended but not started in step k, then it has to be open in sk1 . ( -12)
ensures that if a is a member of agenda(sk ) but not a member of agenda(sk1 ), then a is
started in step k. Similarly, ( -13) ensures that if a is a member of agenda(sk1 ) but not
agenda(sk ), then a is ended in step k.
By Theorem 5, we know that if a temporal planning problem P is satisfiable, then there
exists a positive number l, such that the non-relaxed -step encoding of P with l steps
(i.e., l ) is satisfiable. Accordingly, the completeness of our relaxed -step encoding can be

proved by showing that if l is satisfiable then l will also be satisfiable.
Theorem 7 (completeness of the relaxed -step encoding). Let P = (I, G, A) be a

temporal planning problem and formulae l and l be two -step encodings of P explained

above. If M is a model for l , then l has a model M  such that plan(M  ) = plan(M ).
Proof. See Appendix A.
Theorem 7 also shows that, in our  -step encoding, the number of required steps to
solve a temporal planning problem is less than (or equal to) what is required by our -step
encoding, provided that the same fixed ordering has been used in these two encodings. In
other words, our  -step encoding is more compact than our -step encoding.
Theorem 8 (soundness of the relaxed -step encoding). Let P = (I, G, A) be a

temporal planning problem, {e1 , ..., en } be the set of all events of P, and l be the relaxed

-step encoding for P. If l has a model M , then plan(M ) is a causally valid -step plan
P.
Proof. See Appendix A.
4.4 Mutual Exclusion Relations and Action Compression
As we mentioned earlier in Section 3, the performance of any SAT-based temporal planner
can be improved by mutual exclusion analysis and action compression. In this section,
we show how the information obtained by these tasks has been utilized in ITSAT. Let
P = (I, G, A) be a temporal planning problem, MU T be the set of all mutually exclusive
pairs of propositions of P, and COM be the set of all compression-safe actions of P (see

Section 3). Let l be the encoding of P, which can be any of l , l , or l . For taking
advantage of mutual exclusion relations, we add an extra formula mut
to l , where mut
=
l
l
V
k
k
{p  q |(p, q)  MU T , 1  k  l}. As Theorem 1 shows, two mutually exclusive
propositions p and q can never be both true in any state that is achieved by the execution
of a valid temporal plan starting from I. As a result, adding mut
to the encoding cannot
l
render the planner incapable of finding valid plans.
Let a  A be a compression-safe action. As it was showed in Section 3.2, it is safe
to assume that in any causally valid plan, the ending event of a occurs immediately after
its starting event. One way to impose such constraint is to add some extra clauses to the
577

fiRankooh & Ghassem-Sani

encoding to guarantee that both starting and ending events of a are always included in the
same step. However, these two events may have conflicting effects, in which case l and
l will not allow such events to be both present at any step. Therefore, the information

regarding compression-safe actions is only added
to our relexed -step encoding, l . This
V

is done by adding com
to l , where com
= {eki  eki+1 |a  COM, ei = start(a), 1 
l
l


k  l}. Note that in l , if ei is the starting event of an action, ei+1 denotes its ending
event.

5. Scheduling Phase
In this section, we describe how a causally valid plan is augmented by temporal information
to produce a valid temporal plan. Let  = he1 , ..., en i be a causally valid plan produced by
our planner. The scheduling of  is done by defining the scheduling function  of Definition
9, which assigns a rational number to each event of  as its execution time. Suppose that
we have given different names to different occurrences of the same action in this plan, so
all the events e1 , ..., en are unique. We can assume that for each i,  (i) <  (i + 1), and
thereby satisfy the first condition of Definition 9. However, this can lead to the plans with
unnecessarily large makespans. Alternatively, for obtaining plans with improved quality,
we impose a more relaxed set of constraints on the function  .
Definition 16 (relaxed scheduling functions). Let  be a causally valid plan. The
scheduling function  is a relaxed scheduling function if it has the following properties:
(S-1) For all i and j such that ei is located before ej in , and ei is not swappable with ej
(cf. Definition 12), we require that  (i) <  (j).
(S-2) For all i and j, if ei is the starting event of a particular action a, and ej is the pairing
event of ei in  (cf. Definition 8), we require that  (j) =  (i) + dur(a).
Theorem 9. Let P = (I, G, A) be a temporal planning problem,  = he1 , ..., en i be a
causally valid plan for P, and  : {1, ..., n}  Q be a relaxed scheduling function for .
There exists a valid temporal plan for P.
Proof. See Appendix A.
Theorem 9 shows that whenever a relaxed scheduling function exists for a causally valid
plan of P, a valid temporal plan can be produced for P. To prove that our scheduling
method does not render ITSAT incomplete, we also need to show that if P is solvable,
our planner will be able to produce a causally valid plan  and a scheduling function 
such that  is a relaxed scheduling function for . Let (,  ) be a valid temporal plan
for P. Every causally valid plan can also be regarded as a causally valid -step plan with
singleton steps. Therefore, by Theorem 3, l has a model M such that  = plan(M ).

By Theorem 5, M also satisfies l . Moreover, by Theorem 7, l has a model M  such
that  = plan(M  ) = plan(M ). Therefore, if any of our encoding methods are used for
translating P to a SAT formula, the resulting formula has a model that will be translated
to . On the other hand, according to Definition 9,  satisfies all constraints of the form
(S-1) and (S-2), and therefore, is a relaxed scheduling function of . However, as mentioned
578

fiITSAT: An Efficient SAT-Based Temporal Planner

in Section 4.2.4, we may add certain clauses to the encoding to ensure that the produced
causally valid plan is always compressed (Definition 11). We now show that for each solvable temporal plan, there exists a compressed causally valid plan that can be scheduled to
a valid temporal plan by a relaxed scheduling function.
Theorem 10. Let P = (I, G, A) be a solvable temporal planning problem, and COM
be the set of every member of A that is either compressible towards its start or compressible towards its end (Definition 13). There exists a valid temporal plan (,  ) for P such
that  is a causally valid plan for P,  is compressed with respect to COM, and  is a
relaxed scheduling function for .
Proof. See Appendix A.
To check the existence of the function  with the properties stated above, we solve an
instance of a Simple Temporal Problem (STP) (Dechter et al., 1991). Each STP is associated
with a weighted graph named a Simple Temporal Network (STN). We construct an STN in
which node xi corresponds to the event ei in the causally valid plan . Let  be an arbitrary
small rational number. For each constraint of the form  (i) <  (j), we add an edge with
the weight  from xi to xj . For each constraint of the form  (j) =  (i) + dur(a), we
add an edge with the weight -dur(a) from xi to xj , and another edge with weight dur(a)
from xj to xi . We also add a reference node x0 to the constructed STN. x0 has an edge
with the weight 0 to every other node. A solution of any STP can be found by computing
the length of the shortest path form x0 to all other nodes (Dechter et al., 1991). Suppose
that such shortest paths exist and the length of the shortest from x0 to xi is shown by
distance(x0 , xi ). For each event ei , we define  (i) to be equal to distance(x0 , xi ). In
this case, Theorem 9 guarantees that the resulting plan has all the specifications of a valid
temporal plan.
To see the intuition behind the explained method of defining the function  , suppose that in the constructed STN, there is an edge with the weight  from xi to xj .
This means that distance(x0 , xj )  distance(x0 , xi )  , which implies distance(x0 , xi ) 
distance(x0 , xj )  . This, in turn, implies that distance(x0 , xi ) < distance(x0 , xj ),
and  (i) <  (j). Similarly, it can be easily shown that if there exists an edge with the
weight -dur(a) from xi to xj , and another edge with weight dur(a) from xj to xi , then
we will have:  (j) =  (i) + dur(a). Bellman-Ford algorithm (Cormen, Leiserson, Rivest,
& Stein, 2009) can be used to find all single source shortest paths of any weighted graph
in polynomial time. Besides, the number of the nodes of the produced STN is equal to
the number of events of the causally valid plan. Therefore, we can conclude that, if the
mentioned shortest paths exist,  (i) can be computed in polynomial time.
However, there are situations in which such shortest paths do not exist. It happens when
the corresponding STN has a negative cycle. In these situations, the STP is inconsistent
and consequently, all the temporal constraints cannot be satisfied at the same time. An
example of such a case is depicted in Figure 10.
In Figure 10, action a adds propositions p and g by its starting and ending events,
respectively. a needs proposition q as a precondition for its ending event. Action b requires
p upon starting and adds q upon ending. Durations of actions a and b, are 5 and 10,
respectively. The goal of planning is reaching fact g. For this problem,  = has , bs , be , ae i
579

fiRankooh & Ghassem-Sani

Figure 10. Negative Cycles

is a causally valid plan, where as = start(a), ae = end(a), bs = start(b), be = end(b). This
plan is depicted in Figure 10-(a). In this plan, the execution of action b must be entirely
inside that of action a (i.e., b is started after starting a and is ended before ending a).
However, this is impossible considering the fact that the duration of a is less than that of
b. The invalidity of this plan is caused by the fact that while producing this causally valid
plan, all the durations are abstracted out. The STN constructed for the plan of Figure
10-(a) is depicted in Figure 10-(b). as bs be ae as is a negative cycle with the total weight of
5  2.
5.1 Negative Cycle Prevention
If the STN of a causally valid plan includes a negative cycle, the plan cannot be transformed
into a valid temporal plan. In such cases, the SAT solver will be forced to find a different
solution. This can be done by adding an extra clause such that at least one of the events
of the current negative cycle will be prevented from occurring in its current step. However,
after adding such a blocking clause, the planner can still produce new plans that are basically
equivalent to the previous plan. For instance, consider the example given in Figure 10.
Suppose as , bs , be , and ae are members of steps 1 to 4, respectively. Assume that the
output plan is to have 5 steps. If we forbid the exact occurrence of the negative cycle, a
580

fiITSAT: An Efficient SAT-Based Temporal Planner

new causally valid plan can still be produced by shifting ae to layer 5 and maintaining all
other events in their current steps. The new solution will have the same negative cycle and
therefore cannot be transformed to a valid temporal plan. In fact, here the cause of the
invalidity of the plan has not changed. We now show how by exploiting the simple structure
of negative cycles, one can prevent the reoccurrence of such cycles more effectively.
From the discussion given above, it should be clear that the main reason that such
negative cycles are encountered in the STN of a particular causally valid plan, is some
specific order of events in that plan. In fact, if the events of any negative cycle reoccur with
the same order in a new causally valid plan, the new plan will include the same negative
cycle, too.
For any temporal planning problem P, P
we can regard the set of all possible sequences
of events as a language over the alphabet
= {e1 , ..., en }, where n is the number of all
events of P. The set of all sequences of events in which certain P
events have appeared in
a particular order can also be regarded as another language over . It is straightforward
to show that this latter language is in fact a regular language and can be accepted by a
Finite State Machine (FSM). Figure 10-(c) shows a Finite State Machine that detects the
sequences of events in which as , ae , bs , and be appear according to the order has , bs , be , ae i.
Note that, for the sake of clarity, the self-loop transitions of the Finite State Machine are
not shown in Figure 10-(c).
P
Definition 17 (FSMs). AP
Finite State Machine  is a 5-tuple (S , , T , xP
0 , A ), where
S is a finite set of states,
is a finite set of alphabet symbols, T : S 
 S is a
mapping defining all transitions of , x0  S is the starting state, and A  S is a set of
accepting states.
We now show how by adding certain formulae to our SAT encodings, one can avoid
the members of a given regular language
from being produced as causally valid plans. Let
P
P be a temporal problem,P
and
= {e1 , ..., en } be the set of all events of P. Let L be
any regular
. Assume that L is accepted P
by the Finite State Machine
P langaue over
out
|T (xi , e) = xj , i 6= j} and
 = (S , ,P
T , A ). For each xi  S , let Ei = {e 
Eiin = {e 
|T (xj , e) = xi , i 6= j}. Assume that there are two dummy events e0 and
en+1 , which do not have any precondition, add effect, or delete effect. We define the SAT
variable xk,i for 1  k  l, 0  i  n + 1, and x  S . Assigning a value of true to xk,i
means that  will be in state x, after operating on the sequence of all events of the steps 1
to k  1 and the events of step k with the indices less than i of the final plan. We construct
the formula 
l by the conjunction of formulae (-1) to (-6) stated below:
(-1)

V

k,j
out  E in  {e
{eki  xk,i
s  xt |1  k  l, i < j, T (xs , ei ) = xt , ej  Et
n+1 },
t
in
out
{ei+1 , ..., ej1 }  (Et  Et ) = }

(-2)

V

(-3)

V

k,j
in
out
out
{eki  xk,i
s  xs |1  k  l, i < j, ei  Es , ej  Es  Es  {en+1 },
in
out
{ei+1 , ..., ej1 }  (Es  Es ) = }

k,i
in
out
{xk,0
s  xs |1  k  l, 1  i  n, xs  S , ei  Es  Es ,
in
out
{e1 , ..., ei1 }  (Es  Es ) = }

581

fiRankooh & Ghassem-Sani

(-4)

V

{xk,n+1
 xk+1,0
|1  k < l, xs  S }
s
s

(-5)

V

{xk,i
0 |1  k  l, 1  i  n}

(-6)

V

{xk,i |x  A , 1  k  l, 1  i  n + 1}

Adding 
l to the encoding of a problem makes the SAT solver to somehow simulate
the behavior of , while finding a model that represents a causally valid plan. Assume that
observing ei causes  to make a transition from xs to xt . Moreover, let ej be the first event
after ei (according to the fixed ordering e1 , ..., en ) that may cause a transition to or from
xt . Formula (-1) guarantees that if ei is a member of Stepk , and  is at state xs at the
time of observing ei , then the state of  will be changed to xt , and the next relevant event
of xt (i.e., ej ) will become aware of this transition. (-2) implies that if ei is not a member
of Stepk , and  is at state xs at the time of observing ei , then  will remain at xs , and
the next relevant event of xs will become aware of the current state of . (-3) causes the
information regarding the state of  at the start of each step to be propagated to the first
relevant event of that step. (-4) propagates the information regarding the state of  at
the end of each step to the next step. (-5) ensures that  can be at its starting state
at any place of the final plan. This means that the simulation of  can be started from
anywhere in the plan that is being produced. This enables the SAT solver to detect not
only the strings accepted by , but also strings that have some subsequences accepted by
. Finally, (-6) guarantees that  will never be at one of its accepting states.

Example 7. Let  be the Finite State Machine depicted in Figure 10-(c). This Finite
State Machine detects the sequences of events in which as , ae , bs , and be appear according
to the order has , bs , be , ae i. Assume that we have four events: e1 = as , e2 = ae , e3 = bs , and
e4 = be . Also assume that there are two dummy events e0 and e5 . For the sake of simplicity,
suppose that the problem does not have any events other than e0 to e5 , and the encoding has
only two steps. Consider a boolean assignment M , such that M (e11 ) = M (e13 ) = M (e14 ) =
M (e22 ) = true, and M (e12 ) = f alse. In other words, M is choosing as , bs , and be from the
first step, and ae from the second step. In fact, we have plan(M ) = has , bs , be , ae i. In this
example, we have E0in = {e2 }, because e2 = ae is the only event that causes a transition to
state s0 of . Similarly, we have: E0out = {e1 }, E1in = {e1 }, E1out = {e2 , e3 }, E2in = {e3 },
E2out = {e2 , e4 }, E3in = {e4 }, E3out = {e2 }, E4in = {e2 }, and E4out = . We show that if we
use the formulae (-1) to (-6) stated above to encode , then M cannot be a model for
the produced SAT formula. We show this by contradiction. Assume that M is a model for
the produced SAT formula.
 s0 is the starting state of . Hence, according to (-5), we have M (s01,1 ) = true,
which means that  is in state s0 , prior to checking whether e1 is present in the first
step of the final plan.
1
 s1,2
 According to (-1), we have e11  s1,1
1 . Since we have M (e1 ) = true and
0
1,2
M (s1,1
0 ) = true, we must also have M (s1 ) = true. In other words,  verifies that e1
is present in the first step of the final plan, which causes the current state of  to be

582

fiITSAT: An Efficient SAT-Based Temporal Planner

changed from s0 to s1 . M (s1,2
1 ) = true implies that  is in state s1 , prior to checking
whether e2 is present in the first step of the final plan.
1,3
1
 According to (-2), we have e12  s1,2
1  s1 . Since we have M (e2 ) = f alse and
1,3
1,2
M (s1 ) = true, we must also have M (s1 ) = true. In other words,  verifies that
e2 is not present in the first step of the final plan, which causes the state of  to
remain s1 . M (s1,3
1 ) = true implies that  is in state s1 , prior to checking whether e3
is present in the first step of the final plan.
1
 According to (-1), we have e13  s1,3
 s1,4
1
2 . Since we have M (e3 ) = true and
1,4
1,3
M (s1 ) = true, we must have M (s2 ) = true. In other words,  verifies that e3 is
present in the first step of the final plan, which causes the state of  to be changed
from s1 to s2 . M (s1,4
2 ) = true implies that  is in state s2 , prior to checking whether
e4 is present in the first step of the final plan.
1
 s1,5
 According to (-1), we have e14  s1,4
3 . Since we have M (e4 ) = true and
2
1,5
M (s1,4
2 ) = true, we must have M (s3 ) = true. In other words,  finds out that e4 is
present in the first step of the final plan, which causes the state of  to be changed
from s2 to s3 . M (s1,5
3 ) = true implies that  is in state s3 , after visiting all events of
the first step of the final plan.
1,5
2,0
 According to (-4), we have s1,5
3  s3 . Since we have M (s3 ) = true, we must also
have M (s2,0
3 ) = true, which implies that  is in state s3 , prior to visiting any event
of the second step of the final plan.
2,0
2,2
 According to (-3), we have s2,0
3  s3 . Since we have M (s3 ) = true, we must also
have M (s2,2
3 ) = true, which implies that  is in state s3 , prior to checking whether e2
is present in the second step of the final plan.
2
 s2,5
 According to (-1), we have e22  s2,2
4 . Since we have M (e2 ) = true and
3
2,5
M (s2,2
3 ) = true, we must also have M (s4 ) = true. In other words,  verifies that
e2 is present in the second step of the final plan, which causes the state of  to be
changed from s3 to s4 . M (s2,5
4 ) = true implies that  is in state s4 , after visiting all
events of the first two steps of the final plan. On the other hand, s4 is an accepting
state of . Hence, according to (-6), we have M (s2,5
4 ) = f alse. This is clearly a
contradiction. Therefore, we can conclude that M cannot be a model for the produced
SAT formula.

We now prove that adding 
l to the encoding of the given problem, prevents the planner
from producing those causally valid plans with a subsequence of events that is equivalent to
any string accepted by . This means that once a negative cycle has been translated into
an FSM, the reoccurrence of the negative cycle can be avoided by translating that FSM
into a SAT formula, and adding the formula to the encoding of the problem.
P
Theorem 11. Let P = (I, G, A) be a temporal planning problem,
= {e1 , ..., en } be the




set of all events of P, l be any of the three formulae l , l , l (defined in Section
P 4), and 
be a non-empty causally valid plan for P obtained by solving l . Let  = (S , , T , x0 , A )
583

fiRankooh & Ghassem-Sani

be an FSM that accepts a subsequence   = he1 , ..., em i of , and 
l be the encoding of
 presented by (-1) to (-6). There does not exist any model M for l  
l such that
 = plan(M ).
Proof. See Appendix A.
We also need to show that adding 
l to the encoding will not render our planner incapable of producing those plans that do not contain any subsequence accepted by .
P
Theorem 12. Let P = (I, G, A) be a temporal planning problem,
= {e1 , ..., en } be

the set of all events of P, and l be any of the three formulae l , l , l (defined in
Section 4). Let M be a model that satisfies l , and  = he1 , ..., em i = plan(M ). Let
P
 = (S , , T , x0 , A ) be an FSM that does not accept any subsequence of , and 
l be


the encoding of  composed of (-1) to (-6). There exists a model M for l  l such
that  = plan(M  ).
Proof. See Appendix A.
We now explain how a sequence of events that introduce a negative cycle in the STN of a
causally valid plan can be used to prevent similar negative cycles from reoccurringP
in future
plans produced for the problem at hand. Let P be a temporal planning problem,
be the
set of all events of P, and  = e1 , ..., en be a causally valid plan for P. Assume that the STN
representing the scheduling function of  has a negative cycle N with nodes xi1 , ..., xim . Note
that xik is the node corresponding to event eik of . Without loss of generality, we assume
that i1 < ... < im , i.e., the events of the negative cycle are ordered by the same order that
that are started but not finished
they have in . Let Oik be the set of all temporal actions P
{e|action(e)  Oik }  {eik }.
before reaching eik in the sequence ei1 , ..., eim , and P
ik =
Consider the regular language LN over the alphabet
defined by LN = ei1 i2 ei2 ...im eim ,
where ik denotes any string of the symbols in ik . In fact, in the strings of LN , events other
than those already present in the current negative cycle can be inserted in the sequence in
such a way that the temporal constraints among ei1 , ..., eim remain unchanged. To see why
we exclude the events of open actions from ik , consider two hypothetical events eij and eij 
that are respectively the starting event and the ending event of an action a. Therefore, there
is a temporal constraint on the scheduling function  in the form of  (ij  ) (ij ) = dur(a).
Here, if we insert another copy of the ending event of a between these two events, then a is
ended before the execution of eij  and, as a result, eij  will no longer be the pairing event
of eij , and the mentioned constraint will no longer exist.
Theorem 13. Let N = xi1 , ..., xim be a negative cycle in the STN corresponding to a
causally valid plan  = e1 , ..., en of a temporal problem P, where xik is the node corresponding to event eik of . Let   be another causally valid plan for P. If a subsequence of
  is a member of LN (defined above), the corresponding STN of   will also have N as a
negative cycle.
Proof. See Appendix A.
584

fiITSAT: An Efficient SAT-Based Temporal Planner

Consrtucting an FSM that accepts LN is straightforward. Let  be that FSM. Theorem
13 shows that if  is added to the encoding of the input problem, ITSAT will still be
capable of finding a valid temporal plan, provided that such a plan exists.

6. Empirical Results
In this section, we show how our preprocessing, encoding, and scheduling methods contribute to the overall performance of ITSAT. Since the contribution of the preprocessing
part can be investigated only when the encoding is fixed, we first analyze the performance of
the three encodings explained in Section 4. We also compare the performance of ITSAT with
several state-of-the-art temporal planners on all non-numerical temporal planning problems
of the previous International Planning Competitions.
In Section 4, we theoretically showed that our novel relaxed -step encoding is at least as
compact as the temporal versions of the -step and -step encodings for fixed ordering (i.e.,
the number of steps needed by our relaxed -step encoding to solve a given problem is less
than or equal to that of the temporal versions of the -step and -step encodings). Here, we
empirically show that our relaxed -step often needs a significantly smaller number of steps,
compared to the -step and -step encodings. We also show that the mentioned compactness
causes our relaxed -step to significantly outperform -step and -step encodings in the
benchmark problems in terms of both memory and speed.
In Section 3, we explained two preprocessing methods, namely mutual exclusion analysis
and action compression. In this section we show how each of these methods contribute to the
overall performance of ITSAT on the benchmark problems. For this purpose, we compare
four versions of ITSAT: 1) ITSAT without preprocessing, 2) ITSAT with mutual exclusion
analysis, 3) ITSAT with action compression, and 4) ITSAT with both mutual exclusion
analysis and action compression. Our experimental results show that each of these methods
separately enhance the performance of ITSAT.
In Section 5, we discussed that by adding certain blocking clauses to the encoding of
the problem, one can prevent negative cycles from reoccurring in the STNs of the produced
causally valid plans. We also introduced a more elaborate method for preventing such negative cycles, by adding some extra clauses that are based on certain Finite State Machines.
Here, we empirically show that our FSM-based method is crucial for the efficiency of ITSAT
in the problems with required concurrency.
Finally, we compare the performance of ITSAT with that of the state-of-the-art temporal planners, namely OPTIC (Coles et al., 2010), LPG-td (Gerevini et al., 2006), and
TFD (Eyerich et al., 2009). OPTIC and TFD have different degrees of temporal expressivity, whereas LPG-td is not temporally expressive at all (i.e., it is not capable of solving
the problems with required concurrency). We show that ITSAT significantly outperforms
OPTIC and TFD, while it is competitive with LPG-td in many domains.
6.1 Implementation Details
In order to parse the planning problems and domain, and also validating output plans
produced by ITSAT, we have used VAL, which is a plan validation tool developed by the
organizers of in IPC 2011. The schematic operators of a given domain are instantiated by the
objects of the input problem to produce all possible valid ground temporal actions. ITSAT
585

fiRankooh & Ghassem-Sani

then performs a polynomial reachability analysis to recognize those actions and prepositions
that are relevant to the given problem. For this purpose, all the goal conditions are initially
assumed to be relevant propositions. Any action that produces a relevant proposition upon
starting or ending is considered to be a relevant action. ITSAT then adds all preconditions of
starting and ending events of all relevant actions to the current set of relevant propositions.
The invariants of relevant actions will be added to this set, too. Updating the sets of relevant
propositions and actions are repeated until no further changes will occur in these sets. We
then update the set of relevant propositions by omitting some of relevant propositions that
are present in the initial state of the given problem. The omitted propositions are those that
are not deleted by any relevant action. These propositions will also be omitted from the
at-start, at-end, and invariants of all relevant actions. Mutual exclusion analysis and action
compression methods described in Section 3, are then performed on the sets of relevant
actions and propositions.
As we mentioned in Section 4, in all our encoding methods we assume that there exists a
predefined fixed ordering on all events of the given problem. In the current implementation
of ITSAT, the ordering in which the events are produced while constructing ground actions,
is taken as the presumed fixed ordering of events. The starting event of each action is
placed immediately before its corresponding ending event in the mentioned ordering. More
elaborate heuristic methods for producing such an ordering may result in a more compact
encoding (Rintanen et al., 2006). Investigating such methods is beyond the scope of this
paper and is left for future research.
In the current version of ITSAT, we use P recosat (Biere, 2009), which is a free off-theshelf system, as our SAT solver. We have also examined two other SAT solvers, namely
M inisat (Een & Biere, 2005) and Lingeling (Biere, 2013) for satisfying the formulae.
However, precosat had the best overall performance among these three SAT solvers; though
Lingeling had a better performance in terms of memory usage.
Since P recosat accepts formulae only in the Conjunctive Normal Form (CNF), all formulae described throughout this paper had to be translated to their equivalent CNF formulae. This has been performed simply by using logical equivalence relations such as
(1  2  1  2 ) and ((1  2 )  1  2 ).
For each problem, we start with a formula with just one step. We set a time limit
of three minutes for precosat to find a model for the formula. In the case of the failure,
we add three more steps to the formula and repeat this process until either a model is
found or a predetermined maximum time of 30 minutes is reached. In the case of success
in finding a model, a causally valid plan is extracted from the model. The plan is then
given to a scheduling process to find a valid temporal plan. If the scheduling function
fails, an appropriate FSM is generated and encoded in the problem formula (see Section
5) without increasing the number of steps. The new formula is again given to P recosat
to find a new model. Although parallel solving of formulae with different number of steps
was shown to be more effective than a nave sequential approach (Rintanen et al., 2006;
Streeter & Smith, 2007), our empirical results show that even our simple sequential method
is sufficient to outperform current temporal planners in many planning domains. We leave
the investigation regarding the effect of using such parallelism for our future research.
All the experiments explained in this section have been conducted on a 3.1GHz corei5
CPU with 4GB main memory. As our benchmark problems, we have used the problem
586

fiITSAT: An Efficient SAT-Based Temporal Planner

sets of all previous IPCs. These problems are from different planning domains including
zenotravel, rovers, and depots of IPC 2004, airport of IPC 2006, pegsol, crewplanning,
openstacks, elevators, sokoban, and parcprinter of IPC 2011, and driverlog, f loortile,
matchcellar, mapanalyser, parking, rtam, satellite, storage, turnandopen, and tms of
IPC 2014. Note that some of these domains have been used in different IPCs. For such
domains, we have chosen the problem set of the most recent competition with those domains.
That is why no problem set of IPC 2008 is present in our experiments.
Among the domains used in previous IPCs, only matchcellar, turnandopen, and tms
include problems with the required concurrency. These are the problem sets in which only
temporally expressive planners are capable of producing valid plans. In order to achieve
a better assessment of ITSAT in problems with required concurrency, we have used two
extra domains driverlogshift and matchlift (Halsey, 2004). We have also performed our
experiments on time-window variants of satellite and airport domains. These domains,
which have been used in IPC 2004 and do have required concurrency, are referred throughout
this section by satellite-tw and airport-tw, respectively. The mentioned domains with the
required concurrency are explained in more details in Section 6.4.
6.2 The Impact of Different Encoding Methods
To evaluate our -step, -step, and relaxed -step encodings we have produced three different versions of ITSAT, namely, ITSAT-, ITSAT-, and ITSAT- , respectively. In all
these versions, the formula mut , which encodes the mutex relations, is also added to the
encoding. None of these versions take advantage of action compression. The negative cycle
prevention method described in Section 5 has been used in all the three versions of ITSAT.
Table 2 shows a comparison in each domain among these versions with regard to the number
of solved problems.
As it can be seen in Table 2, ITSAT- has the best performance among the three
versions. In fact, ITSAT- has been able to solve 65 problems more than ITSAT-, and
103 problems more than ITSAT-. Furthermore, almost all problems solved by ITSAT-
or ITSAT- were also solved by ITSAT- . This means that our relaxed -step encoding
is significantly more efficient than the temporal versions of the classical -step and -step
encodings.
Table 3, shows a more detailed comparison among the mentioned encodings. The different columns of Table 3 represent the following items: the name of the domain, the problem
number, the used encoding method, the number of steps in the encoding, the result of
P recosat in terms of satisfiability or unsatisfiability of the formula, the number of clauses
and variables divided by 1000, the amount of time taken by P recosat to determine the
result, and the amount of memory needed for solving the formula. For each problem and
each encoding method, the results are presented for two cases: unsatisfiable formula with
the highest number of steps, and satisfiable formula with the lowest number of steps. Note
that to produce these results, we have increased the number of steps by one when a formula
was unsatisfiable. Symbol  is used in the time column for those cases in which P recosat
has failed to find a model for the formula in 1800 seconds. The results are presented only for
those domains in which at least one of the problems has been solved by at least two of the
planners. Accordingly, openstacks, elevators, matchcellar, and rtam have been omitted
587

fiRankooh & Ghassem-Sani

domain
zenotravel
rovers
depots
satellite-tw
airport-tw
airport
pegsol
crewplanning
openstacks
elevators
sokoban
parcprinter
driverlog
floortile
mapanalyser
matchcellar
parking
rtam
satellite
storage
turnandopen
tms
driverlogshift
matchlift
total

problems
20
20
22
36
50
50
20
20
20
20
20
20
20
20
20
20
20
20
20
20
20
20
10
14
542

ITSAT-
13
18
13
3
19
20
20
8
0
0
2
15
0
10
14
0
10
0
0
0
1
18
10
14
208

solved
ITSAT-
16
18
17
3
21
21
20
8
0
0
3
16
2
16
19
0
10
0
3
9
2
18
10
14
246

ITSAT-
16
20
19
3
21
38
20
20
9
0
2
17
3
20
19
18
10
0
3
9
2
18
10
14
311

Table 2: Overall Comparison of Different Encoding Methods
from Table 3. Moreover, in satellite and storage, the results are only presented for the
 -step and -step encodings.
In each domain, Table 3 presents only the results for the hardest problem (i.e., the
problem with the greatest number of propositions). In our experiments, we have observed a
pattern similar to that of the chosen problems for other problems of each domain. Note that
the results presented in Table 3 are only for finding the first causally valid plan. Therefore,
these results do not include the information regarding FSM encoding method described in
Section 5. We will explain the impact of our FSM-based negative cycle prevention later in
this section.

588

fiITSAT: An Efficient SAT-Based Temporal Planner

domain

zenotravel

rovers

depots

satellite-tw

airport-tw

airport

prob encoding steps

10

4

13

3

11

5

4

26

25

18

2

27

26

3

10

6

17

3

11

7


4

12

12

3

5

13

13


6

72

33

19

7

73

34


8

70

31
20

5

71

32


6
Continued on next page

result
F
F
F
T
T
T
F
F
F
T
T
T
F
F
F
T
T
T
F
F
F
T
T
T
F
F
F
T
T
T
F
F
F
T
T
T

589

C
1000

V
1000

474
133
69
521
167
92
5467
4444
375
5673
4618
511
7950
3256
1008
8747
3802
1336
33
26
7
36
28
8
5415
1811
243
5573
1817
270
9261
3189
430
9392
3290
506

136
33
23
150
42
32
560
412
15
581
428
24
1259
455
236
1387
534
326
9
7
3
10
8
3
1516
412
44
1541
439
49
1381
361
37
1401
373
44

time
(s)
25
0.4
0.38
33
0.54
0.56


0.14
89
47
0.36
6.6
2.2
1.39
8.9
3
2.1
1.2
1.4
0.1
0.7
0.7
0.1
24
3.7
0.43
19
4.1
0.5
27
3.5
0.16
21
4.3
0.62

mem
(MB)
131
18
10
146
20
17
413
244
12
353
220
12
598
284
129
1001
301
141
3
2
1
4
4
1
912
271
22
900
283
28
837
257
31
844
263
36

fiRankooh & Ghassem-Sani

domain

prob encoding steps

12

12

pegsol
20

5

13

13


6

69

69

crewplanning 8

4

70

70

5

19

13

sokoban
4

6

20

14

7

42

31

parcprinter
20

8

43

32


9

7

driverlog
2

5

8

6

22

11

floortile
10

6

23

12

7

17

9

mapanalyser 15

2
Continued on next page

result
F
F
F
T
T
T
F
F
F
T
T
T
F
F
F
T
T
T
F
F
F
T
T
T
F
F
T
T
F
F
F
T
T
T
F
F
F

590

C
1000

V
1000

34
25
10
37
27
12
112
102
3
114
104
3
542
274
122
571
295
142
2325
1436
345
2380
1482
385
2634
2803
5248
3291
201
74
31
211
80
36
187009
101893
29988

9
6
3
10
7
4
34
29
1
35
30
1
143
64
37
150
69
43
382
198
51
391
205
58
258
184
294
221
21
11
6
22
12
7
1005
534
122

time
(s)


0.05
22.4
7.3
0.3


0
30
15
0

12
3.9
74
40
1.5

30
1.1
93
25
0.7
6.2
14
23
13.9
55
7
0.14
20
0.62
0.16
18
10
5.4

mem
(MB)
28
26
1
14
11
3
40
49
1
44
46
1
163
78
23
190
125
20
319
119
24
289
128
26
165
70
280
131
48
16
5
49
13
5
1196
519
75

fiITSAT: An Efficient SAT-Based Temporal Planner

time
domain
prob encoding steps result
(s)

18
T
197433 1064
26

10
T
112123 593
11


3
T
40062
181
6.7

4
F
1769
150
2.5

2
F
605
75
0.5

parking
11

1
F
238
38
0.4

5
T
1868
187
4.2

3
T
911
112
2.5


2
T
470
75
2

11
F
870
221


satellite
3

5
F
294
91


12
T
950
244
157

6
T
352
110
4.9

11
F
1523
125


storage
9

8
F
633
84
48

12
T
1662
136
5.9


9
T
710
94
9.4

42
F
2416
203


22
F
944
106
33

turnandopen 1

10
F
371
53
1.1

43
T
2474
207
176

23
T
987
111
37

11
T
406
58
1.3

9
F
491
51
2.3

7
F
257
39
0.5

tms
18

3
F
91
18
0.1

10
T
546
56
1.4

8
T
284
43
0.5


4
T
115
23
0.3

18
F
373
65
1.5

15
F
235
37
0

driverlogshift 11

9
F
54
17
0

19
T
411
69
0.6

16
T
260
39
0.3


10
T
57
20
0.3
Table 3: Detailed Comparison of the Encoding Methods
C
1000

591

V
1000

mem
(MB)
1177
538
128
177
70
34
297
87
72
431
91
440
95
198
69
149
72
454
112
41
465
106
42
65
42
14
67
42
17
35
19
7
39
21
9

fiRankooh & Ghassem-Sani

Figure 11. Speed Comparison Between ITSAT- and ITSAT-

In Section 4, we theoretically showed that in order to solve a given planning problem,
our relaxed -step encoding requires fewer steps than the temporal versions of classical
-step and -step encodings when the ordering is fixed. Table 3 shows that ITSAT-
often needs a considerably smaller number of steps. This phenomenon is most prominent
in airport, crewplanning, and mapanalyser. Moreover, in openstacks and matchcellar,
neither ITSAT- nor ITSAT- was able to solve any problem due to the very large number
of steps that had been required. This suggests a correlation between the performance of
the planner, and the compactness of the encoding. Generally speaking, when a relatively
high number of steps is needed for -step encoding to solve a problem, we can deduce that
there is a strong causal connection between the actions of the produced plan. On the other
hand, our  -step encoding has been devised to take advantage of such causal connections.
Therefore, the  -step encoding is expected to have more advantage over the -step encoding
in such domains. This phenomenon is most visible in airport, crewplanning, openstacks,
and matchcellar domains, as the numbers of steps required by the -step encoding in these
domains are exceptionally high. Table 3 also shows that our relaxed -step encoding results
in a significant improvement of the planner in terms of the memory usage.
592

fiITSAT: An Efficient SAT-Based Temporal Planner

Figure 12. Speed Comparison Between ITSAT- and ITSAT-

We have also compared the speed of ITSAT- with that of ITSAT- and ITSAT- in
solving all the benchmark problems. The results are depicted in Figure 11 and Figure 12.
As it can be seen, ITSAT- has outperformed ITSAT- and ITSAT- in almost all the
problems.
6.3 The Impact of the Mutual Exclusion Analysis and Action Compression
In Section 3, we explained how mutual exclusion analysis and action compression are performed as preprocessing components of ITSAT. Here, we empirically show that these components are both quite effective in enhancing the performance of our planner. As we showed

before,  is the encoding that results the best performance of ITSAT. We have fixed this
formula as the base of comparison, and produced three other formulae to investigate the

impact of each preprocessing method. These three formulae are   mut (the base encod
ing plus the mutual exclusion information),   com (the base encoding plus the action

compression information), and   mut  com (the base encoding plus both mutual exclusion and action compression information). Table 4 shows the number of problems solved
by each of the mentioned versions of ITSAT.
593

fiRankooh & Ghassem-Sani

domain
zenotravel
rovers
depots
satellite-tw
airport-tw
airport
pegsol
crewplanning
openstacks
elevators
sokoban
parcprinter
driverlog
floortile
mapanalyser
matchcellar
parking
rtam
satellite
storage
turnandpen
tms
driverlogshift
matchlift
total

problems
20
20
22
36
50
50
20
20
20
20
20
20
20
20
20
20
20
20
20
20
20
20
10
14
542




13
20
11
3
21
38
20
20
1
0
1
17
0
3
14
0
3
0
0
0
2
0
10
14
211



  mut
16
20
19
3
21
38
20
20
9
0
2
17
3
20
19
18
10
0
3
9
2
18
10
14
311



  com
14
20
14
3
21
38
20
20
5
0
3
19
1
20
19
20
7
0
0
0
3
18
10
14
289



  mut  com
18
20
20
3
21
39
20
20
13
0
8
20
4
20
20
20
17
0
16
20
9
18
10
14
370

Table 4: The Impact of the Mutual Exclusion Analysis and Action Compression
As it can be seen in Table 4, these preprocessing methods result in a significant improvement in terms of overall coverage. In fact, the version of ITSAT that uses both methods
solves 159 problems more than the base planner. Besides, the version that uses both methods even considerably outperforms any of the two versions that only use one preprocessing
method. This suggest that both preprocessing components are necessary for producing the
best performance of ITSAT.
To investigate the effectiveness of our action compression method in the domains that
ITSAT compresses considerably more actions than CRIKEY, we have performed another
experiment. We compressed only the actions that ITSAT considers compression safe but
CRIKEY3 does not. In this new version of ITSAT, we also used the mutual exclusion
information. This version of ITSAT solves six problems more than the version where only
594

fiITSAT: An Efficient SAT-Based Temporal Planner

mutual exclusion information was used: of these six problems, four problems are from
zenotravel, one is from airport, and one is from mapanalyser. The results does not change
much in other domains. Note that the three mentioned domains are those in which ITSAT
compresses considerably more actions than CRIKEY.
6.4 The Impact of the FSM-Based Negative Cycle Detection
As mentioned earlier, among the domains used to evaluate ITSAT, matchcellar, turnandopen, tms, driverlogshift, matchlift, and time-window versions of airport and satellite can
have problems with required concurrency. In fact, in these domains, it may be impossible
to schedule a causally valid plan produced by solving a SAT formula, into a valid temporal
plan. Here, we briefly explain why the problems in each of these domains may require concurrency, and how this may introduce some negative cycles into the STN associated with a
causally valid plan.
In matchcellar and matchlift, there exists an action for lighting a match. This action
produces light for a certain amount of time. The objective is to mend some fuses. The
actions for mending a fuse can only be executed if there is light. As a result, the actions of
lighting a match and mending a fuse must be executed concurrent. However, in a causally
valid plan, since the planner does not consider the durations of actions, it may assume that
any match can remain lit until all the fuses are mended. As it was discussed in Section 5,
this can introduce a negative cycle into the STN of a produced causally valid plan.
In tms, the objective is to produce a certain number of ceramic structures. These
structures need several preparations that can be done only while a furnace is producing
heat. It should be clear that this domain is very similar to matchcellar and matchlift, and
requires concurrency in a similar way.
A simplified version of driverlogshift was introduced in Section 2. The only difference
between that simplified version and the one used in this section to evaluate the planners, is
that here, there are drivers that can walk to, board, and disembark trucks. Furthermore,
the REST and WORK actions are performed by the drivers rather than trucks. In this domain,
the working shifts of the drivers are analogous to the action of lighting a match in the
matchcellar domain.
In turnandopen, there exists a robot that needs to move between a number of rooms.
There are doors between each pair of adjacent rooms. All these doors, which are closed in
the initial state, should be opened by the robot. The robot can open a door only while it
is turning the doorknob. In this domain, the actions of turning the doorknob and opening
the door must be executed concurrently. However, the duration of the action for turning
the knob is 3, whereas that of opening the door is 2. This enables ITSAT to schedule every
causally valid plan into a valid temporal plan. Therefore, preventing negative cycles is not
necessary in this domain.
In time-window versions of airport and satellite, there is a specific time by which all
goals must be obtained. Such a deadline is introduced into a problem by using a specific
frame action with the duration equal to the time of that deadline. All other actions can
be executed only when this frame action is being executed. In other words, all the actions
must be concurrent with the is frame action. However, in a causally valid plan, since the
planner does not consider the durations of actions, it may assume that the frame action can
595

fiRankooh & Ghassem-Sani

domain
problem
satellite-tw
3
airport-tw
21
matchcellar
20
tms
18
driverlogshift
10
matchlift
14

restarts
121
1
34
3
43
9

C
1000


12
552
44
121
68
72

FSM
1450
5
408
12
930
180

V
1000


3
65
22
23
26
15

FSM
318
2
203
10
280
112

memory (MB)

FSM
1
382
41
1
9
77
16
2
13
107
31
98

Table 5: The Collective Size of the SAT Encodings of FSMs
be arbitrarily long, and thereby neglect to meet the deadline for achieving the goals. This
can introduce negative cycles into the STNs of produced causally valid plans.
As we explained in Section 5, if the STN of a causally valid plan includes a negative
cycle, we must force the SAT solver to find a different solution. This can be done simply
by adding an extra blocking clause to the current SAT formula to prevent at least one of
the events of that negative cycle from reoccurring in its current step. Alternatively, we
introduced a more elaborate method for doing the same thing, by adding the encoding of
certain FSMs into the encoding. In this method, when the STN of a causally valid plan with
k steps includes a negative cycle, an FSM that detects that negative cycle will be encoded
into a SAT formula, and the solver will be restarted. In order to decrease the number of
restarts, whenever a sequence of events corresponding to a negative cycle is found, ITSAT
tries to find other potential negative cycles by replacing the actions of current sequence by
other actions of the problem and checking the STN of the resulting sequence for negative
cycles.
Table 5 shows the collective size of the SAT encodings of the FSMs required for solving
the problems of the above domains. As our base encoding, we have used the relaxed
-step encoding with both mutual exclusion analysis and action compression. For each
domain, the results are shown in Table 5 only for the hardest problem solved by ITSAT. The
turnandopen domain has been excluded from Table 5, as no negative cycle is encountered
when solving problems of this domain. The different columns of Table 5 represent the
following items: the name of the domain, the problem number, the number of clauses and
variables divided by 1000, and the amount of memory needed to produce the formula. The
results for the number of causes, number of variables, and the used memory are presented
with separated columns for the base encoding and the encoding of the FSMs.
As it can be seen in Table 5, our negative cycle prevention method helps ITSAT solve
a considerable number of problems with required concurrency. Nevertheless, the SAT encoding of the required FSMs is significantly larger than the base encoding in the domains
where the number of restarts are relatively high. On the other hand, as the number of the
restarts increase, the speed of ITSAT declines. That is because after each restart, the SAT
solver must verify the satisfiability of the formula, from scratch. In fact, these numerous
restarts are the main reason for the poor performance of ITSAT in the time-window version
of satellite.
596

fiITSAT: An Efficient SAT-Based Temporal Planner

6.5 ITSAT Versus the State-of-the-art Temporal Planners
We now compare ITSAT with three efficient temporal planners, namely, OPTIC (Benton,
Coles, & Coles, 2012), TFD (Eyerich et al., 2009), and LPG-td (Gerevini et al., 2006).
Because of the similarities between the approach used in ITSAT and that of SCP2 (Lu
et al., 2013), we have also included the results of this planner in our experimental results.
OPTIC is the newest version of POPF (Coles et al., 2010). It is a heuristic state-space
temporal planner based on the so-called temporally-lifted progression planning (Cushing
et al., 2007). Using this approach of planning enables OPTIC to solve problems with
required concurrency. Besides, OPTIC handles self-overlapping actions, which makes it
more expressive than ITSAT. Although handling self-overlapping actions is hardly necessary
for solving non-numerical temporal planning problems (Fox & Long, 2007), among the
current benchmark domains, zenotravel, rovers, and airport do permit such actions due to
some modeling errors. For a fair comparison between ITSAT and OPTIC, we have used the
corrected versions of these three domains1 in our evaluations. For guiding its search, OPTIC
benefits from a heuristic function that is based on the relaxed planning graph (Hoffmann
& Nebel, 2001).
TFD is another heuristic state-space temporal planner. TFD is based on the so-called
decision epoch planning (Cushing et al., 2007). The planners that use this approach are
not as temporally expressive as the planners that are based on temporally lifted progression planning. In other words, in theory, there are temporal planning problems defined in
PDDL2.1 that can be solved by ITSAT and OPTIC but not by TFD. However, all the current benchmark problems can potentially be solved by using decision epoch planning. For
guiding its search, TFD benefits from a temporal version of the so-called Context-enhanced
Additive Heuristic (Helmert & Geffner, 2008).
LPG-td is a very fast temporal planner, which is not temporally expressive. In fact,
LPG-td at first generates a sequential plan for a given problem, and then tries to reschedule
the plan to produce one with an improved quality. This renders LPG-td incapable of
solving any problem in matchcellar, turnandopen, tms, driverlogshift, or matchlift. Similar
to OPTIC, LPG-td benefits from a heuristic based on the relaxed planning graph. However,
instead of searching in the state space of the problem, LPG-td performs its search by making
some local improvements to a structure that is similar to partial plans, which is called Linear
Action Graph. Two different configurations of LPG-td can be used based on whether we
prefer speed of the planner or the quality of produced plans. Here, we only present the
results of the quality configuration of LPG-td, as it produced better results than the speed
configuration in our experiments.
SCP2 (Lu et al., 2013), is a SAT-based temporal planner that uses a discrete representation of time. This planner assigns explicit discrete time labels to each step of the encoding.
In this approach, each step i is exactly one time unit ahead of step i + 1. As a result, if an
action with duration d starts in step i, it is forced to end in step i + d. This means that the
number of layers required for producing a plan  is greater than or equal to the makespan
of . SPC2 starts with a formula with only one step, an increases the number of steps by
one, every time the formula is unsatisfiable. This enables SPC2 to find the optimal plan for
1. The corrected version of mentioned domains can be downloaded from the official website of POPF
planner.

597

fiRankooh & Ghassem-Sani

a number of given problems. To obtain a better performance, SCP2 uses -step semantic
to allow causal relations between actions in each time point.
We have compared ITSAT with the above planners based on the number of problems
they can solve in each domain and also by the total score given to each planner using the
scoring strategy of recent IPCs; that is, if a planner cannot solve a problem, it will get a
score of 0 for that problem; Otherwise, its score will be equal to the makespan of the best
produced plan divided by the makespan of the plan found by this planner. The results are
presented in Table 6.
As it can be seen in Table 6, ITSAT significantly outperforms OPTIC, TFD, and SCP2.
In fact, ITSAT solves 162 problems more than OPTIC, 145 problems more than TFD, and
282 problems more than SCP2. ITSAT also solves 64 problems more than LPG-td. However,
this is mainly because LPG-td is incapable of solving problems with required concurrency.
If we exclude satellite-tw, airport-tw, matchcellar, turnandopen, tms, driverlogshift, and
matchlift that are the domains in which LPG-td cannot solve any problem, ITSAT solves
only 31 problems less than LPG-td. This shows that ITSAT is quite competitive with
LPG-td even in solving the problems without required concurrency.
As it is shown in Table 6, OPTIC solves zero problems in parcprinter, driverlog, floortile,
mapanalyser, matchcellar, rtam, storage, and tms. In all of these domains, the main reason
of poor performance of OPTIC is that it runs out of memory, early during its search. TFD
solves zero problems in satellite-tw, airport-tw, parcprinter, driverlog, floortile, rtam, storage,
and tms. Except for parcprinter, in which TFD runs out of memory, in other domains
TFD performs poorly because it is unable to find a plan within 1800 seconds. As we
mentioned before, LPG-td solves zero problems in the domains with required concurrency.
The performance of SCP2 is rather poor in many of the benchmark domains. The reason of
the poor performance of SCP2 is that, for many of the benchmark problems, the makespan
of the optimal plan is relatively large. As a result, in most of the problems, SCP2 is unable
to check the satisfiability of all the formulae with numbers of steps less than the makespan
of the optimal plan, within the 1800 seconds time limit.
To compare the quality of the plans produced by ITSAT with those of other competing
planners, consider Table 7. The numbers presented in Table 7 are the average makespan
ratio of plans mutually solved by the corresponding planner and ITSAT in the corresponding
domain. Ratios less than one indicate better average quality of the solutions produced by
ITSAT in comparison with the competing planners. For those cases that neither ITSAT nor
the competing planner has been able to solve any problem of a domain, the corresponding
cell of Table 7 has remained blank.
We have also performed some experiments based on a number of two planners portfolios
of different pairs of the above planners. The portfolios enabled us to combine the advantages
of two planners. To do this, the 30 minutes time limit is divided equally between each pair
of planners. The results of running these portfolios are presented in Table 8. The results
show that the best configuration has been obtained by combining ITSAT and LPG-td. The
resulting planner was capable of solving 423 out of 542 benchmark problems. Moreover,
each of these planners has produced its best results when it was combined with ITSAT.

598

fiITSAT: An Efficient SAT-Based Temporal Planner

solved

domain

N

zenotravel

20

18

rovers

20

depots

ITSAT

OPTIC

SCP2

IPC

score

ITSAT

OPTIC

TFD

LPG-td

SCP2

TFD

LPG-td

12

12

20

1

11.41

10.56

11.32

17.68

1

20

20

20

20

4

18.25

18.35

18.56

16.88

4

22

20

7

5

21

6

9.52

4.21

3.04

19.43

6

satellite-tw

36

3

4

0

0

0

3

4

0

0

0

airport-tw

50

21

7

0

0

0

21

7

0

0

0

airport

50

39

24

20

43

0

35.2

23.35

18.11

39.68

0

pegsol

20

20

19

18

20

20

19.36

18.02

17.24

18.98

20

crewplanning

20

20

20

14

9

0

18.37

20

11.94

7.82

0

openstacks

20

13

20

20

20

0

7.26

17.01

19.83

15.23

0

elevators

20

0

1

3

9

0

0

1

3

7.44

0

sokoban

20

8

2

3

5

1

7.64

1.72

3

3.26

1

parcprinter

20

20

0

0

7

0

20

0

0

5.72

0

driverlog

20

4

0

0

14

0

3.89

0

0

13.38

0

floortile

20

20

0

0

20

10

17.05

0

0

16.51

10

mapanalyser

20

20

0

19

20

0

18.48

0

15.62

14.81

0

matchcellar

20

20

20

20

0

0

20

20

16

0

0

parking

20

17

16

20

20

6

5.05

13.98

18.72

19.02

6

rtam

20

0

0

0

20

0

0

0

0

20

0

satellite

20

16

4

13

20

0

2.61

3.55

7.05

20

0

storage

20

20

0

0

18

20

18.77

0

0

1.45

20

turnandopen

20

9

9

18

0

1

9

6.52

13.34

0

1

tms

20

18

0

0

0

0

18

0

0

0

0

driverlogshift

10

10

10

6

0

9

8.01

9.48

5.03

0

9

matchlift

14

14

13

14

0

13

14

13

14

0

13

total

542 370

208

225

306

88

305.87 191.75 195.8

Table 6: ITSAT Versus State-of-the-art Temporal Planners

599

256.29 88

fiRankooh & Ghassem-Sani

domain
zenotravel
rovers
depots
satellite-tw
airport-tw
airport
pegsol
crewplanning
openstacks
elevators
sokoban
parcprinter
driverlog
floortile
mapanalyser
matchcellar
parking
rtam
satellite
storage
turnandopen
tms
driverlogshift
matchlift

OPTIC

TFD

LPG-td

SCP2

1.47
0.99
1.24
1
1
1.06
0.98
1.11
1.34

0.86




1
2.94

6.72

0.72

1.14
1

1.50
1.03
1.41


0.95
0.98
0.89
1.90

1.18



0.86
0.80
2.98

4.45

0.48

0.98
1

1.41
0.89
2.02


0.91
0.96
0.88
1.31

0.69
0.81
1.03
0.97
0.79

3.01

7.35
0.11





1
1.27
2.11



1.02



1


1.05


3.15


1.05
1

1.19
1

Table 7: Average Makespan Ratio

ITSAT
OPTIC
TFD
LPG-td
SCP2

ITSAT

375
385
423
348

OPTIC
375

262
350
237

TFD
385
262

360
256

Table 8: 2-planners Portfolio

600

LPG-td
423
350
360
302

SCP2
348
237
256
302


fiITSAT: An Efficient SAT-Based Temporal Planner

Figure 13. Speed Comparison Between ITSAT and OPTIC

Although ITSAT is quite competitive with the state-of-the-art temporal planners, our
empirical results reveal a few drawbacks of the planner. We have compared the speed of
ITSAT with that of OPTIC, TFD, LPG-td, and SCP2 on all of our benchmark problems.
The results are presented in Figure 13, Figure 14, Figure 15, and Figure 16, respectively.
In these figures, the results for the required concurrency domains have been separated
from that of other domains by using different symbols in the scatterplots: the star symbol
represents the problems with required concurrency, and the diamond symbol represents
other problems. As it can be seen, ITSAT is slower than OPTIC, TFD, and LPG-td in a
number of the benchmark problems. A major cause of this drawback is that in ITSAT, the
SAT solver spends too much time refuting several formulae before it will finally find the
first satisfiable formula. As it has been shown in the case of classical planning, the speed of
SAT-based planners can be significantly improved by checking the satisfiability of several
formulae with different number of steps in parallel. We discuss this in more detail in Section
7 as our future research.
Another observation is that, ITSAT performs rather slowly in solving a number of
problems with required concurrency that are quickly solved by OPTIC and TFD. This is
mainly due to restarting the SAT solver whenever negative cycles are encountered. As we
601

fiRankooh & Ghassem-Sani

Figure 14. Speed Comparison Between ITSAT and TFD

explained earlier, when the STN of a causally valid plan with k steps includes a negative
cycle, an FSM detecting that negative cycle will be encoded into a SAT formula, and the
solver will try to satisfy the new formula with k steps, from scratch. In the domains where
these negative cycles are abundant, the performance of ITSAT can be significantly affected
by the numerous restarts of the SAT solver.
The performance of ITSAT is particularly poor in three domains, namely elevators,
driverlog, and rtam. In these domains, the number of ground actions is higher than
that of other domains. A linear increase in the number of ground actions may cause an
exponential growth in the size of the search space of the problem. To tackle this problem,
state-space based planners take advantage of heuristic functions that are devised specially
for pruning the search space of planning problems. It has also been shown that using SAT
solvers tailored for solving planning problems can result in a significant improvement of the
performance of SAT-based classical planning (Rintanen, 2012). We think that employing
this idea can also improve the speed of ITSAT in the mentioned domains.
Another drawback of ITSAT is the poor quality of its produced plans in some benchmark
domains. Most notably, although ITSAT solves most of the problems in depots, openstacks,
parking, and satellite, the quality of its plans is rather low in these domains, according to
602

fiITSAT: An Efficient SAT-Based Temporal Planner

Figure 15. Speed Comparison Between ITSAT and LPG-td

Table 7. This is mainly due to the fact that ITSAT abstracts out the duration of actions,
and thus, its SAT solver lacks the competency for evaluating the quality of the plans that
are being produced. Nevertheless, the quality of the plans produced by ITSAT is generally
comparable to that of other planners in most benchmark domains. In Section 7, we explain
an idea for improving the quality of the plans produced by ITSAT.

7. Conclusions and Future Research
In this paper, we described ITSAT, a temporally expressive SAT-based planner. ITSAT is
based on an approach that takes advantage of parallel encodings. In this approach, at first,
the durations of all actions of a given problem are abstracted out. Then the abstract problem
is encoded into a SAT formula using -step and -step semantics for causally valid plans.
After generating a causally valid plan, ITSAT performs a scheduling process. During this
process, ITSAT tries to satisfy those temporal constraints that are imposed by considering
the durations of actions. This is done by solving a Simple Temporal Problem (STP). In the
cases with an inconsistent STP, the cause, which is a negative cycle in the corresponding
Simple Temporal Network (STN), is detected. ITSAT then adds certain clauses to the SAT
formula at hand to prevent the reoccurrence of such negative cycles. This process is then
603

fiRankooh & Ghassem-Sani

Figure 16. Speed Comparison Between ITSAT and SCP2

repeated until a temporally valid plan will be produced, or a predefined time limit will be
reached.
The main contributions of this paper can be summarized as follows:
 We introduced a novel method to detect temporal actions that can be compressed into
classical ones. The new compression technique is performed as a preprocessing task
and thus is independent from the planning algorithm and can be used by any other
temporal planner. This makes our compression technique more general than that of
POPF, which is specifically tailored for that planner. Our empirical results showed
that such action compression results in an improved performance for ITSAT. We also
empirically showed that our method is capable of detecting more compression-safe
temporal actions than the only previous action compression method, used by POPF.
 We introduced three new encoding methods based on the concept of parallel plans for
SAT-based temporal planning. While two of these methods have been adopted from
classical planning, our third method, which produces more compact formulae, has
been employed by ITSAT for the first time. Our empirical results show that this new
encoding can significantly enhance the performance of SAT-based temporal planning.
604

fiITSAT: An Efficient SAT-Based Temporal Planner

 We introduced a method to avoid producing the plans that are members of a given
regular language over the set of all events. This was done by embedding the SAT
encoding of a particular FSM that accepts the language into the SAT encoding of
the input problem. We used this method for preventing the temporal inconsistencies
of the produced causally valid plans from reoccurring in subsequent causally valid
plans. Our experiments showed that this method contributed considerably to the
performance of ITSAT in current benchmark problems with required concurrency.
According to our empirical results, by taking advantage of these new approaches, not
only does ITSAT outperform the state-of-the-art temporally expressive planners, it is also
competitive with the most efficient temporal planners that do not handle required concurrency. Nevertheless, we believe that the performance of ITSAT can be improved in several
ways, which are discussed below.
In the current version of ITSAT, the satisfiability of formulae with different number of
steps are checked in a sequential manner, starting with a formula encoding one step. This
means that the SAT solver has to refute several formulae until it finds the first satisfiable
formula. If the time required for checking the satisfiability of formulae increased with the
number of steps, this policy would result in the best performance of ITSAT. However, this
is almost never the case. As it has been shown in the case of classical planning, for a fixed
planning problem, the time needed for finding a model for a satisfiable formula is usually
considerably less than the time needed for refuting an unsatisfiable formula (Rintanen et al.,
2006). Based on our experiments, the same phenomenon happens in the case of temporal
planning, too. Similar to the SAT-based classical planning, one can take advantage of this
phenomenon by checking for the satisfiability of formulae with different numbers of steps
in parallel. The applicability of such parallelisms can be very sensistive to the amount
of memory required for saving the formulae. As it was shown in Section 6, our newly
introduced  -step encoding is considerably more efficient than the temporal version of the
classical -step encoding in terms of the memory usage. This suggests that our  -step
encoding is more suitable for employing such parallelism.
As in linear sized classical -step and -step encodings, in all our encoding methods,
we assume that there exists a predefined fixed ordering on all events of a given problem.
This ordering can have a great impact on the number of steps needed for solving the input
problem. For example, consider a sequential plan in which no ground action is applied more
than once. If this potential plan is a subsequence of the mentioned fixed ordering, then only
one step will be sufficient for finding the plan. On the other hand, if in this case we reversed
this fixed ordering, the number of steps that would be required to find the model might be as
large as the size of the plan itself. In the current implementation of ITSAT, the ordering in
which the events are produced while constructing ground actions, is taken as the predefined
fixed ordering of events. However, considering the causal relationships among the actions
of a given problem, one might be able to find more effective orderings that would result
in a fewer steps for solving the problem. We believe that this enhancement would result
in an improved version of ITSAT, which will be more efficient in terms of both speed and
memory usage.
The current version of ITSAT uses off-the-shelf general-purpose SAT solvers. This
means that any advancement in designing such solvers can also improve the performance of
ITSAT. Recent investigations in the field of SAT-based classical planning have shown that
605

fiRankooh & Ghassem-Sani

designing a SAT solver tailored for solving planning problems can result in a much improved
performance of SAT-based planners. In particular, the most efficient SAT-based classical
planner, Mp (Rintanen, 2012), has been able to be competitive with the sate-of-the-art
state-space based planners by employing this idea. Since the causal structures of temporal
planning problems are generally very similar to that of classical planning problems, we
believe that ITSAT can benefit enormously from employing a planning-oriented SAT solver.
As mentioned in Section 6, one of the main drawbacks of ITSAT is the poor quality of
its produced plans in some of the benchmark domains. This is mainly due to the fact that
ITSAT abstracts out the duration of actions, and thus, the SAT solver does not have the
needed resources for evaluating the quality of the plans that are being produced. Alternatively, one can add an explicit representation of the time into the encoding (Shin & Davis,
2005). This can be done by using SMT solvers (Armando & Giunchiglia, 1993), which handle continuous variables. However, as it was discussed in Section 1, this solution may result
in a considerably slower search. We think that ITSAT can benefit from a combination of
these two approaches: after that the first plan is produced by ITSAT, it can proceed by
introducing appropriate numerical constraint to the SAT formula at hand, and then use an
SMT solver to produce improved plans. This is the subject of our ongoing research.
Finally, we should mention that some of the components of ITSAT, can also be used in
other fields of AI planning. Most notably, our  -step encoding can also be employed in
SAT-based classical planning. Our empirical results show that this encoding method can
be quite effective in reducing the number of steps needed to produce valid plans in several
temporal planning domains that also have a classical version. We think that the improved
performance of the  -step encoding in comparison to the -step encoding can be achieved
in classical planning, too. Moreover, in Section 5, we showed how to prevent members of
any given regular language on the events of the input problem from being produced as the
output plan. We used this method to prevent ITSAT from producing temporally invalid
plans. The same method can be employed to enforce a variety of constraints on the plan
that is being produced. For example, consider the case where we require certain actions
to be executed only in a specific order. It must be clear that the set of all plans violating
this constraint can be regarded as a regular language over the set of all actions. Therefore,
such a constraint can be introduced to the encoding of the problem by the same method
discussed in Section 5.

Acknowledgments
The authors would like to thank the handling editor, Jorg Hoffmann, and the anonymous
reviewers for their invaluable contributions to the quality of this paper.

Appendix A. Proofs
Theorem 1. Let P = (I, G, A) be a temporal planning problem and P c = (state(I), G, Ac )
be the causal abstraction of P. Assume that  = he1 , ..., en i is a sequence of events that is
applicable in I, and sn = succ(I, ). Then the following conditions must hold:
 If two propositions p and q are both members of state(sn ), then p and q are non-mutex
in the layer n of the planning graph of P c .
606

fiITSAT: An Efficient SAT-Based Temporal Planner

 If proposition p is a member of state(sn ), and action a is a member of agenda(sn ),
then p and opena are non-mutex in layer n of the planning graph of P c .
Proof. We give the proof by induction on n (the length of ). For n = 0, i.e., when no
event is applied to I, the conclusions obviously hold because every member of state(I) is
present in the first layer of the graph, we have no mutex in the first layer, and we have
agenda(I) =  by Definition 6. Now suppose the conclusions hold for n = k  1. We show
that they will also hold for n = k. Assume that  = he1 , ..., ek i is a sequence of events that
is applicable in I, sk = succ(I, ), and sk1 = succ(I, he1 , ..., ek1 i).
 Let p and q be two members of state(sk ). There are three possible cases:
Case 1: if both p and q are members of sk1 , then by the induction hypothesis, p and
q are non-mutex in layer k  1 of the planning graph of P c and thus noopp and noopq
are non-mutex in layer k  1. Hence, p and q will not be mutex in layer k.
Case 2: if neither p nor q are members of state(sk ), then by Definition 5, p and q
must be both members of add(ek ). Assume that ek is the ending event of action a
(the case where ek is the starting event of a is analogous and thus is omitted here).
Since ek is applicable in state sk1 , by Definition 4, all members of pre(ek ) must be
also members of state(sk1 ), and a must be a member of agenda(sk1 ). Therefore, by
the induction hypothesis, all members of pre(ae ) are non-mutex in layer k  1. As a
result, p and q, which based on Definition 10 are both added by ae , will be non-mutex
in layer k.
Case 3: if only p is a member of state(sk1 ), then by Definition 5, q must be a member
of add(ek ), and p cannot be a member del(ek ). Assume that ek is the ending event of
action a (again, the case where ek is the starting event of a is analogous and thus is
omitted here). By the induction hypothesis, all members of pre(ae ) are non-mutex in
layer k  1, and by Definition 10, ae does not delete p. Therefore, ae will be present
in layer k  1 and it cannot be mutex with noopp . As a result, p and q are non-mutex
in layer k.
 Let p be a member of state(sk ), and action a be a member of agenda(sk ). There are
two possible cases:
Case 1: if a is also a member of agenda(sk1 ), then a is started but not yet ended
before reaching sk1 , and all invariants of a have to be members of state(sk1 ). By
the induction hypothesis, these invariants must be non-mutex in layer k 1. Hence, ai
is present in layer k  1. By Definition 10, ai adds opena . Now, we must show that p
can be added by an action that is not mutex with ai in layer k  1. If p is a member of
state(sk1 ), then by the induction hypothesis, p is present in layer k  1. Therefore,
noopp , which is not mutex with ai , is applicable in layer k  1 and, as a result, p
and opena are not mutex in layer k. On the other hand, if p is not a member of
state(sk1 ), it must be added by ek . Since ek is applicable in sk1 and a is a member
of agenda(sk1 ), by Definition 4, ek cannot delete any invariant of a. Assume that ek
is the ending event of action b (the case where ek the starting event of b is analogous
and thus is omitted here). By the induction hypothesis, be , which is not mutex with
607

fiRankooh & Ghassem-Sani

ai must be applicable in layer k  1. This means that p and opena will not be mutex
in layer k.
Case 2: if a is not a member of agenda(sk1 ), then by Definition 5, ek must be the
starting event of a, and a does not delete p. Moreover, by Definition 4, all starting and
invariants of a must be present in state(sk1 ). Therefore, by the induction hypothesis,
as is applicable in layer k  1. If p is not present in state(sk1 ), then it must be added
by ek . By Definition 10, all propositions added by the starting event of a are also
added by as . Since as also adds opena , then p and opena cannot be mutex in layer k.
On the other hand, if p is a member of state(sk1 ), then by the induction hypothesis,
p is present in layer k  1. Therefore, noopp , which is not mutex with as , is applicable
in layer k  1. This means that p and opena will not be mutex in layer k.

Theorem 2. Let P = (I, G, A) be a solvable temporal planning problem. Let A be the set
of every member of A that is either compressible towards its start or compressible towards
its end. A is compression-safe for P.
Proof. Let 0 be a causally valid plan for P (Such a plan must exist because P is solvable).
Starting from 0 , we produce a sequence of causally valid plans by swapping the events
that are next to each other in the plan at hand. Assume an arbitrary order ha1 , ..., an i
on all members of A . Without loss of generality, we assume that no action is repeated in
0 (otherwise, different names can be given to the different occurrences of the same action
to eliminate such a repetition). For producing the causally valid plan i , we consider the
causally valid plan i1 . If ai is compressible towards its start, we keep swapping the ending
event of ai with its previous event in i1 until that previous event becomes the starting
event of ai . In fact, doing these swaps collectively cause ai to become compressed towards
its start. Doing such swaps can never falsify the causally valid plan at hand: assume that
e is the event immediately prior to the ending event of ai in a causally valid plan, and e
is not the starting event of ai . Then each precondition and effect of e must be present
in at least one state whose agenda includes ai . Thus, by Theorem 1, no precondition or
effect of e can be mutex with openai in the last layer of the levelled-off planning graph of
the causal abstraction of P. Then by Definition 13, e must be swappable with the ending
event ai . Therefore, i is a causally valid plan in which the starting and ending events of
ai are located next to each other. Similarly, if ai is compressible towards its end, we keep
swapping the starting event of ai with its next event in i1 until that next event becomes
the ending event of ai . As a result, in n , the ending event of all members of A are located
next to their corresponding starting events, and therefore, according to Definition 11, A is
compression-safe for P.
Lemma 1. Let S = {e1 , ..., en } be a set of events, E and R, be two subsets of S. Assume
that S  is a subset of S such that if ei  S   E, we have ej 
/ S   R for all j > i. Let M
be the function that is defined by the following rules and assigns a value of true or f alse
to each SAT variable of chain(e1 , ..., en ; E; R; k; m):
608

fiITSAT: An Efficient SAT-Based Temporal Planner

 For each i, M (eki ) = true if and only if ei is a member of S  .
 For each i, M (bki,m ) = true if and only if there is ej  S   E such that j < i.
Then M satisfies chain(e1 , ..., en ; E; R; k; m).
Proof. We show that M satisfies formulae (C-1) to (C-3), and therefore it satisfies
chain(e1 , ..., en ; E; R; k; m).
/ S  then
(C-1) Consider an arbitrary formula eki  bkj,m from the formula (C-1). If ei 
M (eki ) = f alse, and thus the formula is trivially satisfied. Now consider the case
where ei  S  . By the definition of formula (C-1), we know that i < j and ei  E.
Therefore, according to the definition of M , we must have M (bkj,m ) = true, and thus
again the formula is satisfied.
(C-2) Consider an arbitrary formula bki,m  bkj,m from the formula (C-2). If M (bki,m ) =
f alse, the formula is trivially satisfied. On the other hand, if M (bki,m ) = true, then
there must exist an l, such that l < i and el  S   E. Since i < j, we must have
l < j, and therefore, we have M (bkj,m ) = true, and hence the formula is satisfied.
(C-3) Consider an arbitrary formula bki,m  eki from the formula (C-3). If M (bki,m ) =
f alse, the formula is trivially satisfied. On the other hand, if M (bki,m ) = true,
then there must exist an l, such that l < i and el  S   E. Then according to
the properties of S  , we must have ei 
/ S   R. However, by the definition of
chain(e1 , ..., en ; E; R; t; m), we have ei  R and, as a result, ei 
/ S  . Therefore,
k
M (ei ) = f alse and the formula is satisfied.

Lemma 2. Let S = {e1 , ..., en } be a set of events, and E and R be two subsets of S. Assume
that M is a model for chain(e1 , ..., en ; E; R; k; m). If ei  E and M (eki ) = true, then for all
j > i such that ej  R, we have M (bkj,m ) = true, and consequently M (ekj ) = f alse.
Proof. Suppose that in sequence e1 , ..., en , event ej is the first event after ei such that
ej  R. Since in the formula (C-1) of chain(e1 , ..., en ; E; R; k; m), we have eki  bkj,m , then
M (bkj,m ) = true. Similarly, if in sequence e1 , ..., en , event ej  is the first event after ej
such that ej   R, then we must have the formula bkj,m  bkj ,m in (C-2), which implies
that M (bkj ,m ) = true. By repeating the argument of the latter case, we infer that for
all j > i such that ej  R, we have M (bkj,m ) = true, and according to (C-3), we have
M (ekj ) = f alse.
Theorem 3 (completeness of temporal -step encoding). Let P = (I, G, A) be a
solvable temporal planning problem, {e1 , ..., en } be the set of all events of P, and  =
hStep1 , ..., Stepl i be a causally valid -step plan for P. There exists a model M for l such
that  = plan(M ).
609

fiRankooh & Ghassem-Sani

Proof. We construct a function M that assigns true or f alse to all SAT variables of the
formula l . Let hs0 , ..., sl i be the state transition sequence of . M is defined by the
following rules:
 For each proposition p, and each k such that 0  k  l, M (pk ) = true iff p is a
member of state(sk ).
 For each action a  A, and each k such that 0  k  l, M (ak ) = true iff a is a member
of agenda(sk ).
 For each each i such that 1  i  n, and each k such that 1  k  l, M (eki ) = true
iff ei is a member of Stepk . Moreover, for each k such that 1  k  l, M (ek0 ) =
M (ekn+1 ) = f alse.
 For each proposition p, each i such that 0  i  n + 1, and each k such that 0  k  l,
M (bki,mp ) = true iff there exists and event ej , such that j < i, ej  Ep , and ej  Stepk .
1

 For each proposition p, each i such that 0  i  n + 1, and each k such that 0  k  l,
M (bki,mp ) = true iff there exists and event ej , such that j > i, ej  Ep , and ej  Stepk .
2

We show that M satisfies all formulae (-1) to (-13), and therefore is a model for l . Note
that by the way M is constructed, we directly have  = plan(M ).
(-1) According to Definition 15, we have s0 = I, and thus formula (-1) is clearly satisfied.
(-2) According to Definition 15, we have G  state(sl ), and thus formula (-2) is clearly
satisfied.
(-3) According to Definition 6, we have agenda(I) = , and thus formula (-3) is clearly
satisfied.
(-4) According to Definition 15, we have agenda(sl ) = , and thus formula (-4) is clearly
satisfied.
(-5) Let p be an arbitrary proposition, and e be an event such that e  Rp . If e 
/ Stepk ,
then M (ek ) = f alse and, therefore, formula (-5) is trivially satisfied. Consider the
case where e  Stepk . According to Definition 15, Stepk must be a -step from sk1
to sk . Thus, by Definition 14, for any possible ordering on the events of Stepk , we
must be able to execute these events according to the ordering, starting from state
sk1 . One possible ordering can be an specific ordering that puts e in the front of
all other events. Therefore, e must be applicable to sk1 . Then, by Definition 4, we
have p  state(sk1 ). This implies that M (pk1 ) = true, from which the satisfaction
of formula (-5) easily follows.
/ Stepk ,
(-6) Let p be an arbitrary proposition, and e be an event such that e  Ep+ . If e 
then M (ek ) = f alse and, therefore, formula (-6) is trivially satisfied. Consider the
case where e  Stepk . Similar to the previous case, starting from state sk1 , we
must be able to execute the events of Stepk by any possible ordering and reach sk .
610

fiITSAT: An Efficient SAT-Based Temporal Planner

One such possible ordering is one that puts e after all other events. Therefore, addeffects of e must be members of state(sj ), and by Definition 5, we have p  state(sk ).
This implies that M (pk ) = true, from which the satisfaction of formula (-6) easily
follows.
/ Stepk
(-7) Let p be an arbitrary proposition, and e be an event such that e  Ep . If e 
k
then M (e ) = f alse and, therefore, formula (-7) is trivially satisfied. Otherwise,
by the same argument given case (-6), we have p 
/ state(sk ), and the satisfaction
of formula (-7) easily follows.
(-8) Let p be an arbitrary proposition. Consider the nontrivial case where p is a member
of state(sk ) but not state(sk1 ). It can be easily derived from Definition 5, that if p
is added by the application of a sequence of events, then at least one of those events
must add p. This implies the satisfaction of formula (-8).
(-9) This case is analogous to case (-8)
(-10) Lemma 1 is used to prove that M satisfies chain(e1 , ..., en+1 ; Ep ; Rp  {en+1 }; k; mp1 ).
It is straightforward to see that M has all the properties of the model M in Lemma
1. We only need to show that for any proposition p, provided that ei  Stepk  Ep
and j > i, then ej 
/ Stepk  Rp . Suppose that ej  Stepk . By Definition 14, for any
possible ordering on the events of Stepk , we must be able to execute those events
according to the ordering, starting from the state sk1 . One such possible ordering
can be one that puts ei immediately before ej . Notice that ei deletes p, because
ei  Ep . Thus, ej cannot have p as its precondition, and ej 
/ Rp . We can infer
that ej 
/ Stepk  Rp . Therefore, by Lemma 1, M satisfies chain(e1 , ..., en+1 ; Ep ; Rp 
{en+1 }; k; mp1 ). Now, we show that M also satisfies bkn+1,mp  ak . Consider the
1

nontrivial case, where M (bkn+1,mp ) = true. Based on the way we construct M , at
1
least one of e1 , ..., en , say ej , must delete p. Again, since Stepk is a -step from sk1
to sk , we must be able to execute the events of Stepk by any possible ordering and
reach sk . Consider an specific ordering that puts ej after all other events. Now,
if a  agenda(sk ), according to Definition 5, ej cannot be the ending event of a.
Therefore, by Definition 4, a is also a member of the agenda of the state to which
ej is applied. This clearly contradicts the applicability of ej , because ej deletes p,
which is an invariant of a. Therefore, M (ak ) = f alse, which implies that M satisfies
bkn+1,mp  ak .
1

(-11) In this case, too, Lemma 1 is used to prove that M satisfies chain(en , ..., e0 ; Ep ; Rp 
{e0 }; k; mp1 ). Similar to the previous case, it is straightforward to confirm that M
has all the properties of Lemma 1. Since the ordering in chain(e1 , ..., en+1 ; Ep ; Rp 
{en+1 }; k; mp1 ) is reversed in chain(en , ..., e0 ; Ep ; Rp  {e0 }; k; mp1 ), we need to show
that for any proposition p, provided that ei  Stepk  Ep and j < i, then ej 
/
Stepk  Rp . Suppose that ej  Stepk . By Definition 14, for any possible ordering
on the events of Stepk , we must be able to execute these events according to the
ordering, starting from state sk1 . One such possible ordering can be one that
puts ei immediately before ej . By the same argument as the one given in case
611

fiRankooh & Ghassem-Sani

(-10), we infer that ej 
/ Stepk  Rp and, therefore, by Lemma 1, M satisfies
chain(en , ..., e0 ; Ep ; Rp  {e0 }; k; mp1 ). Now, we show that M also satisfies bk0,mp 
2

ak1 . Consider the nontrivial case, where M (bk0,mp ) = true. Based on the way we
2
construct M , at least one of e1 , ..., en , say ej , must delete p. Again, because Stepk is
a -step from sk1 to sk , starting from the state sk1 , we must be able to execute the
events of Stepk by any possible ordering. Consider the specific ordering that puts ej
before all other events. By Definition 4, since ej deletes p, which is an invariant of
a, a cannot be a member of agenda(sk1 ). Therefore, M (ak ) = f alse, which implies
that M satisfies bk0,mp  ak1 .
2

(-12) Assume that e is the starting event of action a. Because Stepk is a -step from sk1
to sk , starting from the state sk1 , we must be able to execute the events of Stepk
by any possible ordering. Consider the specific ordering that puts e before all other
events. Therefore, e must be applicable to sk1 . Then, by Definition 4, a cannot be
a member of agenda(sk1 ), and thus M (ak1 ) = f alse. To see that M (ak ) = true,
consider the specific ordering of events that puts e after all other events. If the events
are executed by this ordering, the results of applying e will appear in state sk , and
thus a must be a member of agenda(sk ). Hence, M satisfies ek  ak1  ak .
(-13) Analogous to case (-12), assume that e is the ending event of action a. Since Stepk
is a -step from sk1 to sk , starting from state sk1 , we must be able to execute the
events of Stepk by any possible ordering. Consider a specific ordering that puts e
before all other events. Therefore, e must be applicable to sk1 . Then, by Definition
4, a must be a member of agenda(sk1 ), and thus M (ak1 ) = true. To see that
M (ak ) = f alse, consider the specific ordering of events that puts e after all other
events. If the events are executed by this ordering, the results of applying e will
appear in state sk , and thus a cannot be a member of agenda(sk ). We conclude that
M satisfies ek  ak1  ak .

Theorem 4 (soundness of -step encoding). Let P = (I, G, A) be a temporal planning
problem, {e1 , ..., en } be the set of all events of P, and l be the -step encoding for P. If
l has a model M , then plan(M ) is a causally valid -step plan for P.
Proof. We can obtain plan(M ) as follows. For each k such that 1  k  l, let Stepk
be the set of all events e for which M (ek ) = true. For each k such that 0  k  l, let
sk be a temporal state. Assume that state(sk ) is the set of all propositions p for which
M (pk ) = true, and agenda(sk ) is the set of all actions a for which M (ak ) = true. We now
show that  = plan(M ) = hStep1 , ..., Stepl i is a causally valid -step plan for P with the
state transition sequence hs0 , ..., sl i.
From formula (-1), it immediately follows that I = state(s0 ). Formula (-2) implies
that G  state(sl ). Formulae (-3) and (-4) respectively imply that agenda(s0 ) and
agenda(sn ) are empty sets. Now, we only need to show that for each k such that 1  k  l,
Stepk = {e1 , ..., em }  {e1 , ..., en } is a -step from sk1 to sk . We first show that for any
612

fiITSAT: An Efficient SAT-Based Temporal Planner

proposition p, Stepk cannot include two different events ei and ej such that ei  Rp , and
ej  Ep . If j < i, since M satisfies formula (-10), by Lemma 2, we can infer M (eki ) and
M (ekj ) cannot be both equal to true. On the other hand, if i < j, since M satisfies formula
(-11), again by Lemma 2, we can infer M (eki ) and M (ekj ) cannot be both equal to true.
Thus, ei and ej cannot be both members of Stepk . Let O : {1, ..., m}  {1, ..., m} be an
arbitrary ordering function. We now show by induction on k  , where k   m, the sequence
heO(1) , ..., eO(k ) i is applicable to sk1 . For k  = 0 (i.e., the case where no event is to be
applied to sk1 ), the conclusion trivially holds. As the induction hypothesis, let sk be the
temporal state resulting from applying the sequence heO(1) , ..., eO(k ) i to sk1 . Let eO(k +1)
be the starting event of action a (we omit the very similar case where eO(k +1) is the ending
event of a). We show that conditions (1) to (3) of Definition 4 hold and thereby eO(k +1) is
applicable to sk . As a result, heO(1) , ..., eO(k +1) i will be applicable to sk1 .
(1) From formula (-5), it easily follows that all preconditions of eO(k +1) and all invariants of a (except for those invariants of a that are added by eO(k +1) ) are members
if state(sk1 ). As mentioned above, neither of these propositions can be deleted by
another member of Stepk . Thus, these propositions are also members of state(sk ).
(2) From formula (-7), it easily follows that a is not a member agenda(sk1 ). Notice that
according to Definition 5, the starting event of a, i.e., eO(k +1) , is the only event that
can add a to the agenda of any state. Therefore, a cannot be a member of agenda(sk ),
either.
(3) Let a be any action with an invariant p such that p  del(eO(k +1) ). Clearly we have
start(a )  Rp , and thus, as it was argued above, start(a ) and eO(k+1) cannot be both
members of Stepk . Hence, start(a ) 
/ Stepk . On the other hand, since p is deleted in
step k, and M satisfies chain(en , ..., e0 ; Ep ; Rp {e0 }; k; mp2 ), then according to Lemma 2,
we have: M (b0,mp2 ) = true. Therefore, by formula (-11), we can infer that M (ak1 ) =
f alse, and thus, a 
/ agenda(sk1 ). start(a ) 
/ Stepk and a 
/ agenda(sk1 ) jointly


imply that a 
/ agenda(sk ).
We now show that sm , which is the result of applying heO(1) , ..., eO(m) i to sk1 , is equal to
sk .
 Let p be an arbitrary member of state(sm ). From formulae (-6) and (-7) it follows
that if p is deleted by any member of Stepk , it cannot be added by any other member
 ). Therefore, p is not deleted by
of Stepk and thus, p cannot be a member
m
Wof state(s
k
any member of Stepk , and the formula eEp e is not satisfied by M . Besides, if p
is not added by any member of Stepk , it must be a member of state(sk1 ), and thus
M (pk1 ) = true. Now, by formula (-9), we can infer that M (pk ) = true, and hence
p  state(sk ). On the other hand, if p is added by a member of Stepk , from formula
(-6), we can deduce that M (pk ) = true, and again we have p  state(sk ). Therefore,
state(sm )  state(sk ).
 Let p be an arbitrary member of state(sk ). According to formula (-7), p cannot be
deleted by any member of Stepk . Besides, by formula (-8), p is either a member of
613

fiRankooh & Ghassem-Sani

state(sk1 ) or is added by a member of Stepk . In both cases, Definition 5 implies that
p  state(sm ). Therefore, state(sk )  state(sm ).
 Let a be an arbitrary member of agenda(sm ). From formulae (-12) and (-13), it
follows that the starting and ending events of no single action can be both members of
Stepk . If a  agenda(sk1 ), then because a is still open in state sm , we can infer that
end(a) 
/ Stepk . Therefore, according to formula (-13), M (ak ) = true, and a must
be a member of agenda(sk ). On the other hand, if a 
/ agenda(sk1 ), then start(a)
must be a member of Stepk . Then, by formula (-12), we have: M (ak ) = true, and
again, a must be a member of agenda(sk ). Therefore, agenda(sm )  agenda(sk ).
 Let a be an arbitrary member of agenda(sk ). According to formula (-13), start(a)
cannot be a member of Stepk . Besides, by formula (-12), a is either a member of
agenda(sk1 ) or start(a) is a member of Stepk . In both cases, Definition 5 implies
that a  agenda(sm ). Therefore, agenda(sk )  agenda(sm ).
Above argument shows that state(sk ) = state(sm ) and agenda(sk ) = agenda(sm ). Hence,
sk = sm = succ(sk1 , heO(1) , ..., eO(m) i). Therefore, Stepk is a -step from sk1 to sk .
Theorem 5 (completeness of -step encoding). Let P = (I, G, A) be a solvable temporal planning problem, {e1 , ..., en } be the set of all events of P, and  = hStep1 , ..., Stepl i
be a causally valid -step plan for P. There exists a model M for l such that  = plan(M ).
Proof. By Theorem 3, there exists a model M for l such that  = plan(M ). We show that
M can be translated into a model for l . Since formulae (1) to (10) are shared between
l and l , then M also satisfies all these formulae. We now show that M also satisfies
all formulae (-11) to (-20), and therefore can be translated into a model for l . In the
following cases, a is an arbitrary temporal action, ei is the starting event of a, and ej is the
ending event of a.
(-11) If M (eki ) = f alse, then formula (-11) is trivially satisfied. If M (eki ) = true, then by
formula (-12) we have: M (ak1 ) = f alse, and therefore formula (-11) is satisfied.
(-12) If M (eki ) = f alse, then formula (-12) is trivially satisfied. If M (eki ) = true, then
by formula (-12) we have: M (ak ) = true, and therefore formula (-12) is satisfied.
(-13) If M (ekj ) = f alse, then formula (-13) is trivially satisfied. If M (ekj ) = true, then by
formula (-13) we have: M (ak ) = f alse, and therefore formula (-13) is satisfied.
(-14) If M (ekj ) = f alse, then formula (-14) is trivially satisfied. If M (ekj ) = true, then by
formula (-13) we have: M (ak1 ) = true, and therefore formula (-14) is satisfied.
(-15) Exactly the same as case (-11).
(-16) Exactly the same as case (-12).
(-17) Exactly the same as case (-13).
(-18) Exactly the same as case (-14).
614

fiITSAT: An Efficient SAT-Based Temporal Planner

(-19) Follows immediately from the fact that M satisfies formula (-12).
(-20) Follows immediately from the fact that M satisfies formula (-13).

Theorem 6 (soundness of -step encoding). Let P = (I, G, A) be a temporal planning
problem, {e1 , ..., en } be the set of all events of P, and l be the -step encoding for P. If
l has a model M , then plan(M ) is a causally valid -step plan P.
Proof. We can obtain plan(M ) as follows. For each k such that 1  k  l, let Stepk be
the set of all events e for which we have M (ek ) = true. Moreover, for each k such that
0  k  l, let sk be a temporal state. Assume that state(sk ) is the set of all propositions p
such that M (pk ) = true and agenda(sk ) is the set of all actions a such that M (ak ) = true.
We construct  = plan(M ) = hStep1 , ..., Stepl i and show that  is a causally valid -step
plan for P with state transition sequence hs0 , ..., sl i.
From formula (-1), it immediately follows that I = state(s0 ). Formula (-2) implies
that G  state(sl ). Formulae (-3) and (-4) imply that agenda(s0 ) and agenda(sn )
are both empty sets. Now, we only need to show that for each k such that 1  k  l,
Stepk = {e1 , ..., em }  {e1 , ..., en } is a -step from sk1 to sk . Without loss of generality,
assume that the sequence he1 , ..., em i is ordered by the fixed ordering he1 , ..., en i. Note that
since M satisfies formula (-10), by Lemma 2, for any proposition p, Stepk cannot include
any two events ei and ej such that ei  Rp , ej  Ep , and j < i. By induction on k  , we show
that for every k   m, the sequence he1 , ..., ek i is applicable to sk1 . For k  = 0 (i.e., the
case where no event is applied to sk1 ), the conclusion obviously holds. As the induction
hypothesis, let sk be the temporal state resulting from applying the sequence he1 , ..., ek i
to sk1 . Assume that ek +1 is the starting event of action a (we omit the very similar case
where ek +1 is the ending event of a). We show that conditions (1) to (3) of Definition 4
hold and thereby ek +1 is too applicable to sk .
(1) From formula (-5), it clearly results that all preconditions of ek +1 and all invariants of a
(except for those invariants of a that are added by eO(k +1) ) are members of state(sk1 ).
As we stated before, neither of these propositions can be deleted by ei for i < k  . Thus,
these propositions are also members of state(sk ).
(2) There are two possible cases. Consider the first case, where according to the fixed
ordering he1 , ..., en i, the ending event of a is located after ek +1 . Formula (-11) implies
that a cannot be a member agenda(sk1 ). Notice that according to Definition 5, the
starting event of a, i.e., ek +1 , is the only event that can add a to the agenda of any
state. Therefore, a cannot be a member of agenda(sk ). In the other case, where the
ending event of a is located before ek +1 , formula (-15) implies that either a is not
a member of agenda(sk1 ), or end(a) is a member of Stepk . However, if end(a) is a
member of Stepk , it will certainly remove a from the agenda of its resulting state. Since
ek +1 is the only event that can add a to the agenda of any state, we can conclude that
a cannot be a member of agenda(sk ). Therefore, in neither of these two cases, a is a
member of agenda(sk ).
615

fiRankooh & Ghassem-Sani

(3) Let a be an action that has p  del(ek +1 ) as an invariant. Since p is deleted in step k,
and M satisfies chain(e1 , ..., en ; Ep ; Rp {en+1 }; k; mp1 ), then according to Lemma 2, we
have M (bn+1,np1 ) = true. Therefore, by formula (-10), we have a 
/ agenda(sk ). On
the other hand, we clearly have end(a )  Rp , and thus, as we argued before, if end(a )
is a member of Stepk , it cannot be located after ek +1 in the fixed ordering he1 , ..., en i.
Hence, if a is not a member of agenda(sk ), it cannot be a member of agenda(sk ),
either. Therefore, we can infer that a 
/ agenda(sk ).
We now show that sm , which is the result of applying he1 , ..., em i to sk1 , is equal to sk .
 By the same argument given in the proof of Theorem 4, we have state(sm )  state(sk )
and state(sk )  state(sm ), hence, state(sm ) = state(sk ).
 Let a be an arbitrary member of agenda(sm ). Let ei and ej be the starting and ending
events of a, respectively. There are three possible cases. Consider the first case where
a  agenda(sk1 ), ei 
/ Stepk , and ej 
/ Stepk (i.e., a is open immediately before step
k and is neither started nor ended in step k). In this case, since M satisfies (-20), we
have M (ak ) = true, and therefore, a  agenda(sk ). Consider the second case where
a  agenda(sk1 ), ei  Stepk , ej  Stepk , and j < i (i.e., a is open immediately
before step k, and is first ended and then started again in step k). In this case, since
M satisfies formula (-16), we have M (ak ) = true, and therefore, a  agenda(sk ).
Finally, consider the third case where a 
/ agenda(sk1 ), ei  Stepk , and ej 
/ Stepk
(i.e., a is not open immediately before step k, and it is started but not ended in
step k). In this case, if j < i, then M must satisfy formula (-16) and we have
M (ak ) = true. On the other hand, if i < j, then M must satisfy formula (-12) and,
since M (ekj ) = f alse, we must have M (ak ) = true, and therefore, a  agenda(sk ).
Consequently, in all these three cases, a must be a member of agenda(sk ); hence
agenda(sm )  agenda(sk ).
 Let a be an arbitrary member of agenda(sk ), i.e., M (ak ) = true. There are two
possible cases. Case 1) a is not a member of agenda(sk1 ), and hence M (ak1 ) =
f alse. By formula (-19), we have: M (eki ) = true. This means that a is started in
step k. Now, if j < i, the ending event of a cannot happen after its starting event,
and therefore, a must remain open after the execution of step k, i.e., a  agenda(sm ).
On the other hand, if i < j, by formula (-14) we have: M (ekj ) = f alse. This
means that a is started but not ended in step k, and therefore a must remain open
after the execution of step k, i.e., a  agenda(sm ). Now consider case 2) a is a
member of agenda(sk1 ), and hence M (ak1 ) = true. If i < j, by formula (-11)
we have M (eki ) = f alse, and by formula (-13) we have M (ekj ) = f alse. This means
that a is open immediately before the execution of step k, and is neither started nor
ended in step k. Therefore, a must also be open after the execution of step k, i.e.,
a  agenda(sm ). On the other hand, if j < i, since both M (ak ) and M (ak1 ) are
false, formulae (-15) and (-17) can be combined to form the formula (eki  ekj ).
This means that a is ended in step k if and only if it is later started again in the same
step. Therefore, again a must be open after the execution of step k, and we have
a  agenda(sm ). Therefore, agenda(sk )  agenda(sm ).
616

fiITSAT: An Efficient SAT-Based Temporal Planner

Above arguments show that state(sk ) = state(sm ) and agenda(sk ) = agenda(sm ). Hence,
sk = sm = succ(sk1 , he1 , ..., em i). Therefore, for the ordering functions O : {1, ..., m} 
{1, ..., m}, such that O(i) = i, we have sk = succ(sk1 , heO(1) , ..., eO(m) i), and thus Stepk is
a -step from sk1 to sk .
Lemma 3. Let M be a model for chain (e1 , ..., en ; Ep+ ; Ep ; Rp ; k; mp ), ej be a member of
Rp , and EpM = {e|e  Ep+  Ep , M (ek ) = true}. M have the following properties:
 If there exists no event ei such that ei  EpM and i < j, then M (bkj,mp ) = M (pk1 ).
 If there exists an event ei such that ei  Ep+ , i < j, {ei+1 , ..., ej1 }  EpM = , and
M (eki ) = true, then M (bkj,mp ) = true.
 If there exists an event ei such that ei  Ep , i < j, {ei+1 , ..., ej1 }  EpM = , and
M (eki ) = true, then M (bkj,mp ) = f alse.
Proof.
 Assume that there exists no event ei such that ei  EpM and i < j. Consider the
case where we have M (pk1 ) = true. Let {ei0 , ..., eim } be the set of all events ei
such that ei  Rp  Ep+  Ep , and 0  i  j. Without loss of generality, we can
assume that 0 = i0 < i1 < ... < im = j. Since M must satisfy (C -7), we know that
M (bki ,m ) = true. Assume that for an arbitrary s, M (bkis ,mp ) = true. If eis  Ep+ Ep ,
0

p

then we know that M (ekis ) = f alse, and by (C -4) we have M (bki


s+1 ,mp

other hand, if eis 

Rp Ep+ Ep ,

by

(C -3),

we have

M (bki ,m )
p
s+1

) = true. On the

= true. We can infer

that for each 1  s  j, we have M (bkis ,mp ) = true, and thereby M (bkj,mp ) = M (pk1 ).
The proof for the case where we have M (pk1 ) = f alse is analogous, except that
instead of (C -4), we need to use (C -5) .
 Assume that there exists an event ei such that ei  Ep+ , i < j, {ei+1 , ..., ej1 } 
EpM = , and M (eki ) = true. Let {ei0 , ..., eim } be the set of all events ei such that
ei  Rp  Ep+  Ep , and i  i  j. Without loss of generality, we can assume that i =
i0 < i1 < ... < im = j. Since M must satisfy (C -1), we know that M (bki ,m ) = true.
1

p

Assume that for an arbitrary s  1, M (bkis ,mp ) = true. If eis  Ep+  Ep , then we
know that M (ekis ) = f alse, and by (C -4), we have M (bki
hand, if eis  Rp 

Ep+



 ) = true. On the other
s+1 ,mp
k
by
we have M (bi ,m ) = true. We can infer
p
s+1
k
have M (bis ,mp ) = true, and thereby M (bkj,mp ) = true.

Ep ,

that for each 1  s  j, we

(C -3),

 Assume that there exists an event ei such that ei  Ep , i < j, {ei+1 , ..., ej1 } 
EpM = , and M (eki ) = true. Let {ei0 , ..., eim } be the set of all events ei such that
ei  Rp  Ep+  Ep , and i  i  j. Without loss of generality, we can assume that i =
617

fiRankooh & Ghassem-Sani

i0 < i1 < ... < im = j. Since M must satisfy (C -1), we know that M (bki ,m ) = f alse.
1

p

Assume that for an arbitrary s  1, M (bkis ,mp ) = f alse. If eis  Ep+  Ep , then we
know that M (ekis ) = f alse, and by (C -5), we have M (bki
hand, if eis  Rp 

Ep+



 ) = f alse. On the other
s+1 ,mp
k
by
we have M (bi ,m ) = f alse. We can infer
p
s+1
k
have M (bis ,mp ) = f alse, and thereby M (bkj,mp ) = f alse.

Ep ,

that for each 1  s  j, we

(C -3),


Lemma 4. Let M be a model for chain(e1 , ..., en+1 ; Ep ; Rp ; k; mof
p ). Assume that ei  Ep ,
M (eki ) = true, and p  inv(a). Let ej and ej+1 be the starting event and ending event of
a, respectively. M has the following properties:

 If M (ekj+1 ) = true, and i < j, then M (ekj ) = true.
 If M (ak ) = true, then M (ekj ) = true.
Proof. Let {ei1 , ..., eim } be equal to the set {es |es  Op , i < s  n + 1}. Without loss
of generality, we can assume that i1 < i2 < ... < im = n + 1. Since M (eki ) = true, by
(Cof -1), we can infer that M (bi1 ,mof ) = true. For each s, such that M (bis ,mof ) = true, by
p

p

(Cof -2), we can deduce that M (bis+1 ,mof ) = true. Therefore, we have M (bn+1,mof ) = true.
p
p
Furthermore, if i < j, we have ej+1  {ei1 , ..., eim }, and thus M (bj+1,mof ) = true. Besides,
p

if M (ekj+1 ) = true, then by (Cof -3), we have M (ekj ) = true. On the other hand, if M (ak ) =
true, we can infer from formula (Cof -4) that M (ekj ) = true.

Lemma 5. Let M be a model for chain(e0 , ..., en ; Ep ; Rp ; k; mob
p ). Assume that ei  Ep ,
M (eki ) = true, and p  inv(a). Let ej and ej+1 be the starting event and ending event of
a, respectively. M has the following properties:

 If M (ekj ) = true, and j + 1 < i, then M (ekj+1 ) = true.
 If M (ak1 ) = true, then M (ekj+1 ) = true.
Proof. The proof is very analogous to that of Lemma 4, and thus is omitted.
Theorem 7 (completeness of the relaxed -step encoding). Let P = (I, G, A)

be a temporal planning problem and formulae l and l be two -step encodings of

P explained in Section 4. If M is a model for l , then l has a model M  such that
plan(M  ) = plan(M ).
Proof. Let M be a model for l . We construct the function M  to assign a value of true

or f alse to each binary variable of l , by using the following rules:
(R-1) For 1  i  n and 1  k  l, M  (eki ) = M (eki ).
618

fiITSAT: An Efficient SAT-Based Temporal Planner

(R-2) For 1  k  l, M  (ek0 ) = M  (ekn+1 ) = f alse.
(R-3) For 0  k  l and each proposition p, M  (pk ) = M (pk ).
(R-4) For 0  k  l and each action a, M  (ak ) = M (ak ).
(R-5) For 0  i  n + 1, 1  k  l, and each proposition p, if there exist j < i such that
M (ekj ) = true and ej  Ep+  Ep then M  (bki,mp ) = M (pk ); otherwise, M  (bki,mp ) =
M (pk1 ).
(R-6) For 1  i  n + 1, 1  k  l, and each proposition p, if there exist j < i such that
M (ekj ) = true and ej  Ep then M  (bk of ) = true; otherwise, M  (bk of ) = f alse.
i,mp

i,mp

(R-7) For 0  i  n, 1  k  l, and each proposition p, if there exist j > i such that
M (ekj ) = true and ej  Ep then M  (bki,mob ) = true; otherwise, M  (bki,mob ) = f alse.
p

p

We now show that M  satisfies all formulae ( -1) to ( -13), and therefore is a model for

l . From above rules, it should be clear that we have plan(M  ) = plan(M ).
( -1) Formula ( -1) is exactly the same as (-1). Besides, M and M  assign the same
value to each variable of this formula.
( -2) Formula ( -2) is exactly the same as (-2). Besides, M and M  assign the same
value to each variable of this formula.
( -3) Formula ( -3) is exactly the same as (-3). Besides, M and M  assign the same
value to each variable of this formula.
( -4) Formula ( -4) is exactly the same as (-4). Besides, M and M  assign the same
value to each variable of this formula.
( -5) Formula ( -4) is the conjunction of formulae (C -1) to (C -8). We show that M 
satisfies all formulae (C -1) to (C -8), and thereby, it satisfies ( -5).
 Consider an arbitrary formula eki  bkj,mp from (C -1). If M (eki ) = f alse, then
the formula is trivially satisfied. If M (eki ) = true, then by (R-5), we have
M  (bki,mp ) = M (pk ). On the other hand, because M satisfies (-6), we have
M (pk ) = true. Therefore, M  (bki,mp ) = true, and the formula is satisfied again.
 Consider an arbitrary formula eki  bkj,mp from (C -2). If M (eki ) = f alse,
then the formula is trivially satisfied. If M (eki ) = true, then by (R-5), we have
M  (bki,mp ) = M (pk ). On the other hand, because M satisfies (-7), we have
M (pk ) = f alse. Therefore, M  (bki,mp ) = f alse, and the formula is satisfied
again.
 Consider an arbitrary formula bki,mp  bkj,mp from (C -3). Since ei is not a
member of Ep+  Ep , and none of the events located between ei and ej in the
fixed ordering are members of Ep+  Ep , then by (R-5), we can easily show
that M  (bki,mp ) = M  (bkj,mp ). Thus, the formula is satisfied.
619

fiRankooh & Ghassem-Sani

 Consider an arbitrary formula bki,mp  eki  bkj,mp from (C -4). If M (eki ) =
true, then the formula is trivially satisfied. If M (eki ) = f alse, then since none
of the events located between ei and ej in the fixed ordering are members of
Ep+ Ep , then by (R-5), we can easily show that M  (bki,mp ) = M  (bkj,mp ). Thus,
the formula is satisfied.
 Consider an arbitrary formula bki,mp eki  bkj,mp from (C -5). By the same
argument as the one given for (C -4), we can infer that M  (bki,mp ) = M  (bkj,mp ).
Thus, the formula is satisfied.
 Consider an arbitrary formula bki,mp  eki from (C -6). If M  (bki,mp ) = true,
then the formula is trivially satisfied. If M  (bki,mp ) = f alse, then there exist two
possible cases. Case 1: there exists an event ej such that j < i, M (ekj ) = true,
and ej  Ep+ Ep . In this case, by (R-5), we have M (pk ) = M  (bki,mp ) = f alse.
Since M must also satisfy (-6), we have ej 
/ Ep+ , and thus ej  Ep . Besides,
M must satisfy (-10), which implies M (eki ) = f alse. By (R-1), we have
M  (eki ) = M (eki ) = f alse, and therefore, the formula is satisfied. Case 2: there
does not exist any event ej such that j < i, M (ekj ) = true, and ej  Ep+  Ep .
In this case, by (R-5), we have M (pk1 ) = M  (bki,mp ) = f alse. Now, since
M must satisfy (-5), we can infer that M (eki ) = f alse. By (R-1), we have
M  (eki ) = M (eki ) = f alse, and therefore, the formula is satisfied again.
 Consider the formula bk0,mp  pk1 from (C -6). From (R-5), it can be easily deducted that M  (bk0,mp ) is always equal to M (pk1 ). By (R-3), we have
M  (pk1 ) = M (pk1 ). As a result, M  (bk0,mp ) = M  (pk1 ), and the formula is
satisfied.
 Consider the formula bkn+1,mp  pk from (C -6). There are two possible cases.
Case 1: there exists an event e such that M (ek ) = true, and e  Ep+  Ep .
In this case, by (R-5), we have M  (bn+1,mp ) = M (pk ). Now, by (R-5), we
have M (pk ) = M  (pk ). Therefore, M  (bn+1,mp ) = M  (pk ), and the formula is
satisfied. Case 2: there does not exist any event e such that M (ek ) = true and
e  Ep+ Ep . In this case, by (R-5), we have M  (bn+1,mp ) = M (pk1 ). Besides,
since M satisfies formulae (-8) and the right hand side of (-8) becomes f alse,
the left hand side of (-8), i.e., pk1  pk , has to be f alse, too. Thus, if
M (pk1 ) = f alse, then we have M (pk ) = f alse. A similar argument about
(-9) can show that if M (pk1 ) = true, then we have M (pk ) = true. Thus,
M (pk1 ) = M (pk ), and by (R-3), we have M  (pk ) = M (pk1 ). Therefore,
M  (bn+1,mp ) = M (pk1 ) = M  (pk ), and the formula is satisfied again.
( -6) We show that M  satisfies all formulae (Cof -1) to (Cof -4), and thereby, it satisfies
( -6).
 Consider an arbitrary formula eki  bk

j,mof
p

from (Cof -1). We know from (Cof -

1) that i < j. If M  (eki ) = f alse, the formula is trivially satisfied. If M  (eki ) =
true, by (R-6), we have M  (bj,mof ) = true, and the formula is satisfied.
p

620

fiITSAT: An Efficient SAT-Based Temporal Planner

 Consider an arbitrary formula bk

i,mof
p

from (Cof -2). If M  (bk

 bk

j,mof
p

i,mof
p

= f alse, the formula is trivially satisfied. If M  (bk

i,mof
p

)

) = true, by the rule

i

(R-6), there must exist an event ei , such that
< i, M (eki ) = true, and
ei  Ep . Since we have i < j, we must also have i < j. Now, by (R-6), we
have M  (bj,mof ) = true, and the formula is satisfied.
p

 Consider an arbitrary formula bk

j,mof
p

 ekj  eki from (Cof -3). If M  (bk

j,mof
p

f alse, the formula is trivially satisfied. If M  (bk

j,mof
p

)=

) = true, by (R-6), there

must exist an event ej  such that j  < j, M (ekj ) = true, and ej   Ep . Now,
since M satisfies (-10), we can infer that M (ekj ) = f alse. By (R-1), we have
M  (ekj ) = M (ekj ) = f alse, and therefore, the formula is satisfied.
 Consider an arbitrary formula bk

n+1,mof
p

ak  eki from (Cof -4). If M  (bk

n+1,mof
p

= f alse, the formula is trivially satisfied. If M  (bk

)

) = true, by (R-6),

n+1,mof
p
k
an event ej such that M (ej ) = true and ej  Ep . Now,
(-10), we can infer that M (ak ) = f alse. By (R-4), we have

there must exist
since M satisfies
M  (ak ) = M (ak ) = f alse, and therefore, the formula is satisfied.

( -7) We show that M  satisfies all formulae (Cob -1) to (Cof -4), and thereby, it satisfies
( -7).
 Consider an arbitrary formula eki  bkj,mob from (Cob -1). We know from (Cob -1)
p

that j < i. If M  (eki ) = f alse, the formula is trivially satisfied. If M  (eki ) =
true, by (R-7), we have M  (bj,mob
) = true, and the formula is satisfied.
p
 Consider an arbitrary formula bki,mob  bkj,mob from (Cob -2). If M  (bki,mob ) =
p

p

p

f alse, the formula is trivially satisfied. If M  (bki,mob ) = true, by the rule (R-7),
p

there must exist an event ei such that i < i , M (eki ) = true, and ei  Ep .
Since we have j < i, we must also have j < i . Now, by (R-7), we have
) = true, and the formula is satisfied.
M  (bj,mob
p
 Consider an arbitrary formula bki,mob  eki  ekj from (Cob -3). If M  (bki,mob ) =
p

p

f alse or M  (eki ) = f alse, the formula is trivially satisfied. If M  (bki,mob ) = true
p

and M  (eki ) = true, by (R-7), there must exist an event ei , such that i < i ,
M (eki ) = true, and ei  Ep . Since M satisfies (-10), we can infer that
M (ak ) = f alse. However, we know that i = j  1 < j; thus M must satisfy (12). Therefore, we have M (ekj ) = true. By (R-1), we have M  (ekj ) = M (ekj ) =
true, and therefore, the formula is satisfied.
 Consider an arbitrary formula bk0,mob  ak1  ekj from (Cob -4). If M  (bk0,mob ) =
p

p

f alse or M  (ak1 ) = f alse, the formula is trivially satisfied. If M  (bk0,mob ) =
p

true and M  (ak1 ) = true, by (R-7), there must exist an event ei such that
M (eki ) = true, and ei  Ep . Since M satisfies (-10), we can infer that
M (ak ) = f alse. However, M must satisfy (-20). Therefore, we have M (ekj ) =
621

fiRankooh & Ghassem-Sani

true. By (R-1), we have M  (ekj ) = M (ekj ) = true, and therefore, the formula
is satisfied.
( -8) Consider an arbitrary formula eki  ak1 from ( -8). Let ej be the ending event
of a. We know that i = j  1 < j. Therefore, M must satisfy (-11). ( -8)
is exactly the same as (-11). Besides, M and M  assign the same value to each
variable of these formulae. Thus, ( -8) is satisfied by M  .
( -9) Consider an arbitrary formula eki  ak ekj from ( -9). We know that i = j 1 < j.
Therefore, M must satisfy formula (-12). ( -9) is exactly the same as (-12).
Besides, M and M  assign the same value to each variable of these formulae. Thus,
( -9) is satisfied by M  .
( -10) Consider an arbitrary formula ekj  ak from ( -10). Let ei be the starting event
of a. We know that i = j  1 < j. Therefore, M must satisfy (-13). ( -10)
is exactly the same as (-13). Besides, M and M  assign the same value to each
variable of these formulae. Thus,( -10) is satisfied by M  .
( -11) Consider an arbitrary formula ekj  ak1  eki from ( -11). We know that i =
j  1 < j. Therefore, M must satisfy (-14). ( -11) is exactly the same as (-14).
Besides, M and M  assign the same value to each variable of these formulae. Thus,
( -11) is satisfied by M  .
( -12) ( -12) is exactly the same as (-19). Besides, M and M  assign the same value to
each variable of these formulae. Thus, ( -12) is satisfied by M  .
( -13) ( -13) is exactly the same as (-20). Besides, M and M  assign the same value to
each variable of these formulae. Thus, ( -13) is satisfied by M  .

Theorem 8 (soundness of the relaxed -step encoding). Let P = (I, G, A) be a

temporal planning problem, {e1 , ..., en } be the set of all events of P, and l be the relaxed

-step encoding for P. If l has a model M , then plan(M ) is a causally valid -step plan
P.
Proof. We can obtain plan(M ) as follows. For each k such that 1  k  l, let Stepk be
the set of all events e for which we have M (ek ) = true. For each k such that 0  k  l,
let sk be a temporal state. Assume that state(sk ) is the set of all propositions p such that
M (pk ) = true, and agenda(sk ) is the set of all actions a such that M (ak ) = true. We
construct  = plan(M ) = hStep1 , ..., Stepl i and show that  is a causally valid -step plan
for P with state transition sequence hs0 , ..., sl i.
From ( -1), it immediately follows that I = state(s0 ). Also, ( -2) implies that G 
state(sl ). Besides, ( -3) and ( -4) imply that agenda(s0 ) and agenda(sn ) are empty sets,
respectively. Now we only need to show that for each k such that 1  k  l, Stepk =
{ei1 , ..., eim }  {e1 , ..., en } is a -step from sk1 to sk . Without loss of generality, we
622

fiITSAT: An Efficient SAT-Based Temporal Planner

assume that the sequence hei1 , ..., eim i is ordered according to the fixed ordering he1 , ..., en i,
i.e., i1 < i2 < ... < im .
By induction on k  , we can conclude that for each k   m, the sequence hei1 , ..., eik i is
applicable to sk1 . For k  = 0 (i.e., the case, where no event is applied to sk1 ), the conclusion obviously holds. Let sk be the temporal state resulting from applying hei1 , ..., eik i
to sk1 . Assume that eik +1 is the starting event of action a. We omit the very similar case
where eik +1 is an ending event of a. We show that conditions (1) to (3) of Definition 4
holds and thereby eik +1 is applicable to sk .
(1) Assume that p 
/ state(sk ), where p is either a precondition of eik +1 or an invariant
of a that is not added by eik +1 . There are two possible cases. Case 1: p is not a
member of state(sk1 ), and is not added or deleted by any member of {ei1 , ..., eik }.
In this case, we have M (pk1 ) = f alse. Moreover, there exists no event ei such that
ei  Ep+  Ep , i < k  + 1, and M (eki ) = true. Case 2: p is deleted by an event
ei  Stepk and is not added or deleted by any event ej  Stepk , such that i < j  k  .
In this case, we have ei  Ep , i < ik +1 , {ei+1 , ..., eik +1 }  EpM = , and M (eki ) = true,
where EpM = {e|e  Ep+  Ep , M (ek ) = true}. In both cases, by Lemma 3, we have
M (bkik +1 ,mp ) = f alse, which contradicts the fact that M satisfies (C -6).
(2) Since M satisfies ( -8), a is not a member of agenda(sk1 ). However, eik +1 is the only
event in Stepk that can add a to the agenda of any state. Thus, a is not a member of
agenda(sk ).
(3) Let b be any action other than a, with an invariant p  del(eik +1 ). Let ej and ej+1
be the starting and ending events of b, respectively. As mentioned earlier, we assume
that the ending event of each action is located immediately after its starting event in
the fixed ordering. We show that b cannot be a member of agenda(sk ). There are
two possible cases in which b may be a member of agenda(sk ). Case 1: b is an open
action immediately before the execution of Stepk ; and b it is not ended in Stepk until
eik +1 is executed. In this case, we have M (bk1 ) = true, and M (ekik +1 ) = true. Since
M satisfies ( -7), by Lemma 5, we have M (ekj+1 ) = true. As we just assumed that b
is not ended until the execution of eik +1 , we have ik +1 < j + 1. On the other hand,
since ej is the starting event of b, we have ik +1 6= j, and thus, ik +1 < j. Therefore,
by Lemma 4, we have M (ekj ) = true. This contradicts the fact that M satisfies ( -8),
because here we have M (bk1 ) = true, M (ekj ) = true, and ej = start(b). Case 2: b is
started in step k, and it is not ended during step k until the execution of eik +1 . In this
case, we have M (ekj ) = true, M (ekj+1 ) = f alse, j + 1 < ik +1 , and M (ekik +1 ) = true.
Since M satisfies ( -7), by Lemma 5, we must have M (ekj+1 ) = true, which again is a
contradiction.
We now show that sm , which is the result of applying hei1 , ..., eim i to sk1 , is equal to sk .
 Let p be an arbitrary proposition. If p  state(sm ), there are two possible cases.
Case 1: p is a member of state(sk1 ), and is not added or deleted by any member of
{ei1 , ..., eim }. In this case, we have M (pk1 ) = true. Moreover, there exists no event ei
such that ei  Ep+  Ep , i < n + 1, and M (eki ) = true. Case 2: p is added by an event
623

fiRankooh & Ghassem-Sani

ei  Stepk and is not added or deleted by any event ej  Stepk , such that i < n + 1.
In this case, we have ei  Ep+ , i < n + 1, {ei+1 , ..., en }  EpM = , and M (eki ) = true,
where EpM = {e|e  Ep+  Ep , M (ek ) = true}. In both cases, by Lemma 3, we
have M (bkn+1,mp ) = true. Since M satisfies (C -8), we have M (pk ) = true, and thus
p  state(sk ). Therefore, state(sm )  state(sk ).
 Let p be an arbitrary proposition. If p 
/ state(sm ), there are two possible cases. Case
1: p is not a member of state(sk1 ), and is not added or deleted by any member of
{ei1 , ..., eim }. In this case, we have M (pk1 ) = f alse. Moreover, there exists no event
ei such that ei  Ep+  Ep , i < n + 1, and M (eki ) = true. Case 2: p is deleted by
an event ei  Stepk and is not added or deleted by any event ej  Stepk , such that
i < j < n + 1. In this case, we have ei  Ep , i < n + 1, {ei+1 , ..., en }  EpM = , and
M (eki ) = true, where EpM = {e|e  Ep+ Ep , M (ek ) = true}. In both cases, by Lemma
3, we have M (bkn+1,mp ) = f alse. Since M satisfies (C -8), we have M (pk ) = f alse,
and thus p 
/ state(sk ). Therefore, state(sk )  state(sm ).
 Let a be an arbitrary action, and ei and ej be its starting event and ending event,
respectively. If a  agenda(sm ), since we assume that the ending event of each action
is located immediately after its starting event in the fixed ordering, there are only two
possible cases. Case 1: a is open immediately before step k, and is not ended during
step k. In this case, we have M (ak1 ) = true and M (ekj ) = f alse. Since M satisfies
( -13), we must have M (ak ) = true. Therefore, a  agenda(sk ). Case 2: a is started
but not ended in step k. In this case, we have M (eki ) = true and M (ekj ) = f alse. As
M satisfies ( -9), we must have M (ak ) = true. Therefore, a  agenda(sk ). Since in
both cases, we have a  agenda(sk ), we can infer that agenda(sm )  agenda(sk ).
 Let a be an arbitrary action, and ei and ej be its starting event and ending event of a,
respectively. If a 
/ agenda(sm ), since we assume that ending event of each action is
located immediately after its starting event in the fixed ordering, there are only two
possible cases. Case 1: a is not open immediately before execution of step k, and is
not started during step k. In this case, we have M (ak1 ) = f alse and M (eki ) = f alse.
Since M satisfies ( -12), we must have M (ak ) = f alse. Therefore, a 
/ agenda(sk ).
k
Case 2: a is ended in step k. In this case, we have M (ej ) = true. Since M satisfies
( -10), we must have M (ak ) = f alse. Therefore, a 
/ agenda(sk ). Because in both
cases, we have a 
/ agenda(sk ), we can infer that agenda(sk )  agenda(sm ).
The above arguments show that state(sk ) = state(sm ) and agenda(sk ) = agenda(sm ).
Hence, we have sk = sm and sk = succ(sk1 , hei1 , ..., eim i). Therefore, for the ordering functions O : {1, ..., m}  {1, ..., m}, where O(i) = i, we have sk = succ(sk1 , heO(i1 ) , ..., eO(im ) i),
and thus Stepk is a -step from sk1 to sk .

Theorem 9. Let P = (I, G, A) be a temporal planning problem,  = he1 , ..., en i be a
causally valid plan for P, and  : {1, ..., n}  Q be a relaxed scheduling function for .
There exists a valid temporal plan for P.
624

fiITSAT: An Efficient SAT-Based Temporal Planner

Proof. By using the bubble sort algorithm, we can sort the events of  in an increasing order
according the values given to them by  . This algorithm takes two consecutive members
of a sequence, and swaps them only if the value of the first one is greater than that of the
second one. It continues doing the swaps until the whole sequence is properly sorted. Let ei
and ej be two events swapped by the bubble sort in any stage of the algorithm (assume that
ei is located before ej in the sequence prior to swapping). Then, we must have  (i) >  (j).
Thus, according to (S-1), we know that ei and ej are swappable (c.f., Definition 12). As a
result, if the whole sequence was a causally valid plan prior to swapping, it would also be a
causally valid plan after that swapping. This means that sorting  according to the values
given by  will result in another causally valid plan, say   . The plan   obviously satisfies
the two conditions of Definition 9, and therefore, (  ,  ) is a valid temporal plan for P.

Theorem 10. Let P = (I, G, A) be a solvable temporal planning problem, and COM be
the set of every member of A that is either compressible towards its start or compressible
towards its end (Definition 13). There exists a valid temporal plan (,  ) for P such that
 is a causally valid plan for P,  is compressed with respect to COM, and  is a relaxed
scheduling function for .
Proof. Let 1 = e1 , ..., ei , ei+1 , ..., en be a causally valid plan for P such that ei and ei+1 are
two swappable events. Let 2 = e1 , ..., ei+1 , ei , ..., en be the result of swapping ei and ei+1
in 1 . We show that if  is a relaxed scheduling function for 1 , then it is also a relaxed
scheduling function for 2 .
 Consider any two events ej and ek such that in 2 , ej is located before ek . If j 6= i + 1
or k 6= i, then in 1 , ej is definitely located before ek , too, and therefore the property
(S-1) holds for ej and ek . On the other hand, if j = i + 1 and k = i, ej and ek are
swappable and therefore the property (S-1) trivially holds for ej and ek .
 Assume that ej is the starting event of a particular action a, and ek is the pairing
event of ej in 2 . From Definition 8, we can easily infer that if in 2 , ei is located
between ej and ek , then ei cannot be the starting or ending event of a. Similarly, if
ei+1 is located between ej and ek , then ei+1 cannot be the starting or ending event of
a. On the other hand, since ei and ei+1 are swappable, we know that they cannot be
both some events of the same action, and therefore, either j 6= i + 1 or k 6= i. Thus,
swapping ei and ei+1 cannot falsify the fact that ej and ek are pairing events. In other
words, in 1 , too, ek is the pairing event of ej . This implies that the property (S-2)
holds for ej and ek .
Let (  ,  ) be an arbitrary valid temporal plan for P. Since  is a scheduling function for
  , it can obviously be also regarded as a relaxed scheduling function for   . As we showed
in Section 3.2,   can be transformed to a causally valid plan  that is compressed with
respect to COM, by doing a series of swaps, where each swapping occurs between a pair
of consecutive swappable events. Therefore,  must also be a relaxed scheduling function
for , and (,  ) is a valid temporal plan for P, where  is the same scheduling function
as  .
625

fiRankooh & Ghassem-Sani

P
Theorem 11. Let P = (I, G, A) be a temporal planning problem,
= {e1 , ..., en } be the




set of all events of P, l be any of the three formulae l , l , l (defined in Section
P 4), and 
be a non-empty causally valid plan for P obtained by solving l . Let  = (S , , T , x0 , A )
be an FSM that accepts a subsequence   = he1 , ..., em i of , and 
l be the encoding of
 presented by (-1) to (-6). There does not exist any model M for l  
l such that
 = plan(M ).
Proof. We give the proof by contradiction. Assume that there exists a model M for l  
l
such that  = plan(M ). Let f : {1, ..., m}  {1,P
..., n} be a function such that for each i,
f (i) is equal to the index of the i-th event of   in . Moreover, let g : {1, ..., m}  {1, ..., l}
be a function such that for each i, g(i) is equal to the step number of the SAT variable in
l that corresponds to the i-th event of   . Assume x0 , ..., xm to be a sequence of states
of  such that for 0 < i  m, we have xi = T (xi1 , ef (i) ). Since  accepts   , we must
f (1),g(1)

have xm  A . As M satisfies (-5), we have M (x0
) = true. Here, two cases can be
considered. case 1: g(2) = g(1). In this case, since  = plan(M ), for f (1) < j < f (2), we
g(1)
must have M (ej ) = f alse. Now, by considering (  1) and (  2), we can infer that
g(2),f (2)

M (x1
) = true. Case 2: g(2) > g(1). In this case, by considering (  1) and (  2),
g(1),n+1
we can infer that M (x1
) = true. Then by, considering (  4), we can deduce that
g(1)+1,0
M (x1
) = true. By the same argument plus considering (  3) we can show that
g(2),f (2)
g(i),f (i)
M (x1
) = true. The whole deduction can be repeated to show that M (xi1
) = true
g(m),f (m)

for 1  i  m. Therefore, we have M (xm1
) = true. Since xm = T (xm1 , ef (m) ),
out  E in  {e
by considering (  1), we can infer that for j, such that ej  Em
n+1 } and
m
g(m),j
in
out
) = true. However, since xm  A ,
{ef (m)+1 , ..., ej1 }  (Em  Em ) = , we have M (xm
this contradicts the assumption that M satisfies (  6).
P
Theorem 12. Let P = (I, G, A) be a temporal planning problem,
= {e1 , ..., en } be



the set of all events of P, and l be any of the three formulae l , l , l (defined in
Section 4). Let M be a model that satisfies l , and  = he1 , ..., em i = plan(M ). Let
P
 = (S , , T , x0 , A ) be an FSM that does not accept any subsequence of , and 
l be


the encoding of  composed of (-1) to (-6). There exists a model M for l  l such
that  = plan(M  ).
Proof. Let us introduce a total order relation  on those SAT variables of l that correspond


to events of the input problem. For any two sat variables eki and eki , we have eki  eki if
and only if one of the following two conditions holds: 1) k < k  . 2) k = k  and i < i .
Assume that f : {1, ..., m}  {1, ...,Pn} is a function such that for each i, f (i) is equal to
the index of the i-th event of  in . Moreover, assume that g : {1, ..., m}  {1, ..., l} is
a function such that for each k, g(k) is equal to the step number of the SAT variable in l
that corresponds to the k-th event of . Let uk,i = heu , ..., et i denote a subsequence of 
with the following properties:
g(t)

 M (ef (t) ) = true.
626

fiITSAT: An Efficient SAT-Based Temporal Planner

g(t)





 For all i and k  such that ef (t)  eki  eki , we have M (eki ) = f alse.
In fact, uk,i is a substring of  that spans from the u-th event of  to the last event of 
whose corresponding SAT variable is located before eki in l . We define the model M  for
l  
l by the following rules:
(R-1) For each SAT variable v of l , M  (v) = M (v).
(R-2) For 1  k  l and 1  i  n, M  (xk,i
0 ) = true.
(R-3) For 1  k  l, 1 < i  n, and xs  S , M  (xk,i
s ) = true iff for some j, the sequence
jk,i transforms  from x0 to xs .
From (R-1), we can infer that M  satisfies l . We now show that M  also satisfies all
formulae (-1) to (-6), and thereby, it satisfies 
l .
 xk,j
be an arbitrary formula from (-1). If M  (eki ) = f alse or
(-1) Let eki  xk,i
s
t
 k,i
 k
M  (xk,i
s ) = f alse, the formula is trivially satisfied. Assume that M (ei ) = M (xs ) =
k,i
true. By (R-3), for some u, the sequence u transforms  from x0 to xs . Since
eki  ekj , by the way we defined uk,j , we can deduce that uk,j = uk,i  hei i    , where ()
denotes the concatenation operator and   is a sequence of events from {ei+1 , ..., ej1 }.
By (-1), we have T (xs , ei ) = xt , and therefore ei causes  to transit from xs to
xt . Besides, {ei+1 , ..., ej1 }  Etout = , and thus, no member of   can cause  to
transit to a state other than xt . Therefore, uk,j transforms  from x0 to xt , and
M  (xk,j
t ) = true. Hence, the formula is satisfied.
k,j
be an arbitrary formula from (-2). If M  (eki ) = true or
(-2) Let eki  xk,i
s  xs
 k
M  (xk,i
s ) = f alse, the formula is trivially satisfied. Assume that M (ei ) = f alse and
k,i
M  (xk,i
s ) = true. By (R-3), for some u, the sequence u transforms  from x0 to xs .
k,j
Since eki  ekj , by the way we defined u , we can deduce that uk,j = uk,i    , where
  is a sequence of events from {ei+1 , ..., ej1 }. Besides, {ei+1 , ..., ej1 }  Esout = ,
and thus, no member of   can cause  to transit to a state other than xs . Therefore,
uk,j transforms  from x0 to xs , and M  (xk,j
s ) = true. Hence, the formula is satisfied.
k,i
 k,0
(-3) Let xk,0
s  xs be an arbitrary formula from (-3). If M (xs ) = f alse, the formula
is trivially satisfied. Assume that M  (xk,0
s ) = true. By (R-3), for some u, the
k,0
sequence u transforms  from x0 to xs . Since ek0  eki , by the way we defined uk,i ,
we can deduce that uk,i = uk,0   , where   is a sequence of events from {e1 , ..., ei1 }.
Besides, {e1 , ..., ei1 }Esout = , and thus, no member of   can cause  to transit to a
state other than xs . Therefore, uk,i transforms  from x0 to xs , and M  (xk,i
s ) = true.
Hence, the formula is satisfied.

(-4) Let xk,n+1
 xk+1,0
be an arbitrary formula from (-4). By the way we defined
s
s
k+1,0
) =
, we can deduce that uk,n+1 = uk+1,0 for every u. Therefore, M  (xk,n+1
u
s
k+1,0

M (xs
). Hence, the formula is satisfied.
(-5) According to (R-2), any formula from (-5) is directly satisfied by M  , .
627

fiRankooh & Ghassem-Sani

(-6) Let xk,i be an arbitrary formula from (-6). Accorrding to our assumptions, uk,i
cannot cause  to transit to any of its accepting states. Since we have x  A , (R-3)
implies that M  (xk,i ) = f alse. Hence, the formula is satisfied.

Theorem 13. Let N = xi1 , ..., xim be a negative cycle in the STN corresponding to a
causally valid plan  = e1 , ..., en of a temporal problem P, where xik is the node corresponding to event eik of . Let   be another causally valid plan for P. If a subsequence of
  is a member of LN (defined in Section 5), the corresponding STN of   will also have N
as a negative cycle.
Proof. Let ei1 , e2,1 ..., e2,k2 , ei2 , ..., eim1 , em,1 , ..., em,km , eim be a subsequence of   , where
ej,1 , ..., ej,kj is a string of symbols in j , for 1 < j  m. Consider two arbitraty events
eij and eij  from this sequence, such that ij < ij  . We show that any temporal constraints
between  (eij ) and  (eij  ) is also present between  (ij ) and  (ij  ).
 If we have the constraint  (ij ) <  (ij  ), then by the scheduling constraint (S-1)
explained in Section 5, eij and eij  are not swappable. Besides, in   , eij is clearly
located before eij  . Consequently, we must have  (ij ) <  (ij  ) according to the
scheduling constraint (S-1).
 If we have the constraint  (ij  ) (ij ) = dur(a), then by the scheduling rule (S-2), eij
and eij  have to be the starting event and the ending event of a, respectively. Moreover,
for j < j  < j  , we have action(eij  ) 6= a. This indicates that for j < j   j  , a is
in Oj  , and therefore eij  
/ j  . Since ej  ,1 ..., ej  ,k  is a string of symbols in j  ,
j
we conclude that in   , a is not yet ended before reaching eij  . This means that eij
and eij  are pairing events in   . Thus, by the scheduling constraint (S-2), we have
 (ij  )   (ij ) = dur(a).
This shows that any edge between xij and xij  in the corresponding STN of  is also present
in the corresponding STN of   , and thus the latter STN has N as its negative cycle.

References
Allen, J. F. (1984). Towards a general theory of action and time. Artif. Intell., 23 (2),
123154.
Armando, A., & Giunchiglia, E. (1993). Embedding complex decision procedures inside an
interactive theorem prover. Ann. Math. Artif. Intell., 8 (3-4), 475502.
Benton, J., Coles, A. J., & Coles, A. (2012). Temporal planning with preferences and
time-dependent continuous costs. In Proceedings of the Twenty-Second International
Conference on Automated Planning and Scheduling, ICAPS 2012, Atibaia, Sao Paulo,
Brazil, June 25-19, 2012.
Biere, A. (2009). P{re,i}cosat@sc09. solver description for SAT competition 2009. In SAT
2009 Competitive Event Booklet.
628

fiITSAT: An Efficient SAT-Based Temporal Planner

Biere, A. (2013). Lingeling, Plingeling and Treengeling entering the sat competition 2013.
In Proceedings of SAT Competition 2013.
Blum, A., & Furst, M. L. (1997). Fast planning through planning graph analysis. Artif.
Intell., 90 (1-2), 281300.
Castellini, C., Giunchiglia, E., & Tacchella, A. (2003). SAT-based planning in complex
domains: Concurrency, constraints and nondeterminism. Artif. Intell., 147 (1-2), 85
117.
Coles, A. J., Coles, A., Fox, M., & Long, D. (2009). Extending the use of inference in
temporal planning as forwards search. In Proceedings of the 19th International Conference on Automated Planning and Scheduling, ICAPS 2009, Thessaloniki, Greece,
September 19-23, 2009.
Coles, A. J., Coles, A., Fox, M., & Long, D. (2010). Forward-chaining partial-order planning. In Proceedings of the 20th International Conference on Automated Planning and
Scheduling, ICAPS 2010, Toronto, Ontario, Canada, May 12-16, 2010, pp. 4249.
Cormen, T. H., Leiserson, C. E., Rivest, R. L., & Stein, C. (2009). Introduction to Algorithms
(3. ed.). MIT Press.
Cushing, W., Kambhampati, S., Mausam, & Weld, D. S. (2007). When is temporal planning
really temporal?. In IJCAI 2007, Proceedings of the 20th International Joint Conference on Artificial Intelligence, Hyderabad, India, January 6-12, 2007, pp. 18521859.
Dechter, R., Meiri, I., & Pearl, J. (1991). Temporal constraint networks. Artif. Intell.,
49 (1-3), 6195.
Do, M. B., & Kambhampati, S. (2003). Sapa: A multi-objective metric temporal planner.
J. Artif. Intell. Res. (JAIR), 20, 155194.
Een, N., & Biere, A. (2005). Effective preprocessing in SAT through variable and clause
elimination. In Theory and Applications of Satisfiability Testing, 8th International
Conference, SAT 2005, St. Andrews, UK, June 19-23, 2005, Proceedings, pp. 6175.
Ernst, M. D., Millstein, T. D., & Weld, D. S. (1997). Automatic sat-compilation of planning
problems. In Proceedings of the Fifteenth International Joint Conference on Artificial
Intelligence, IJCAI 97, Nagoya, Japan, August 23-29, 1997, 2 Volumes, pp. 1169
1177.
Eyerich, P., Mattmuller, R., & Roger, G. (2009). Using the context-enhanced additive
heuristic for temporal and numeric planning. In Proceedings of the 19th International Conference on Automated Planning and Scheduling, ICAPS 2009, Thessaloniki,
Greece, September 19-23, 2009.
Fox, M., & Long, D. (2002). PDDL+: Modelling continuous time-dependent effects. In the
Third International NASA Workshop on Planning and Scheduling for Space.
Fox, M., & Long, D. (2003). PDDL2.1: An extension to PDDL for expressing temporal
planning domains. J. Artif. Intell. Res. (JAIR), 20, 61124.
Fox, M., & Long, D. (2007). A note on concurrency and complexity in temporal planning.
In the 26th Workshop of the UK Planning and Scheduling Special Interest Group.
629

fiRankooh & Ghassem-Sani

Garrido, A., Fox, M., & Long, D. (2002). A temporal planning system for durative actions of
PDDL2.1. In Proceedings of the 15th Eureopean Conference on Artificial Intelligence,
ECAI2002, Lyon, France, July 2002, pp. 586590.
Gerevini, A., Saetti, A., & Serina, I. (2006). An approach to temporal planning and scheduling in domains with predictable exogenous events. J. Artif. Intell. Res. (JAIR), 25,
187231.
Gerevini, A., & Schubert, L. K. (1998). Inferring state constraints for domain-independent
planning. In Proceedings of the Fifteenth National Conference on Artificial Intelligence
and Tenth Innovative Applications of Artificial Intelligence Conference, AAAI 98,
IAAI 98, July 26-30, 1998, Madison, Wisconsin, USA., pp. 905912.
Halsey, K. (2004). CRIKEY! Its Co-ordination in Temporal Planning. Ph.D. thesis, University of Durham.
Halsey, K., Long, D., & Fox, M. (2004). Multiple relaxations in temporal planning. In
Proceedings of the 16th Eureopean Conference on Artificial Intelligence, ECAI2004,
including Prestigious Applicants of Intelligent Systems, PAIS 2004, Valencia, Spain,
August 22-27, 2004, pp. 10291030.
Haslum, P. (2006). Improving heuristics through relaxed search - an analysis of TP4 and
HSP*a in the 2004 planning competition. J. Artif. Intell. Res. (JAIR), 25, 233267.
Haslum, P., & Geffner, H. (2000). Admissible heuristics for optimal planning. In Proceedings of the Fifth International Conference on Artificial Intelligence Planning Systems,
Breckenridge, CO, USA, April 14-17, 2000, pp. 140149.
Helmert, M., & Geffner, H. (2008). Unifying the causal graph and additive heuristics. In
Proceedings of the Eighteenth International Conference on Automated Planning and
Scheduling, ICAPS 2008, Sydney, Australia, September 14-18, 2008, pp. 140147.
Hoffmann, J., Gomes, C. P., Selman, B., & Kautz, H. A. (2007). SAT encodings of statespace reachability problems in numeric domains. In IJCAI 2007, Proceedings of the
20th International Joint Conference on Artificial Intelligence, Hyderabad, India, January 6-12, 2007, pp. 19181923.
Hoffmann, J., & Nebel, B. (2001). The FF planning system: Fast plan generation through
heuristic search. J. Artif. Intell. Res. (JAIR), 14, 253302.
Huang, R., Chen, Y., & Zhang, W. (2009). An optimal temporally expressive planner:
Initial results and application to P2P network optimization. In Proceedings of the
19th International Conference on Automated Planning and Scheduling, ICAPS 2009,
Thessaloniki, Greece, September 19-23, 2009.
Huang, R., Chen, Y., & Zhang, W. (2012). SAS+ planning as satisfiability. J. Artif. Intell.
Res. (JAIR), 43, 293328.
Kautz, H. A., & Selman, B. (1992). Planning as satisfiability. In ECAI, pp. 359363.
Kautz, H. A., & Selman, B. (1996). Pushing the envelope: Planning, propositional logic and
stochastic search. In Proceedings of the Thirteenth National Conference on Artificial
Intelligence and Eighth Innovative Applications of Artificial Intelligence Conference,
AAAI 96, IAAI 96, Portland, Oregon, August 4-8, 1996, Volume 2., pp. 11941201.
630

fiITSAT: An Efficient SAT-Based Temporal Planner

Long, D., & Fox, M. (2003). Exploiting a graphplan framework in temporal planning. In
Proceedings of the Thirteenth International Conference on Automated Planning and
Scheduling (ICAPS 2003), June 9-13, 2003, Trento, Italy, pp. 5261.
Lu, Q., Huang, R., Chen, Y., Xu, Y., Zhang, W., & Chen, G. (2013). A SAT-based approach
to cost-sensitive temporally expressive planning. ACM TIST, 5 (1), 18.
Mali, A. D., & Liu, Y. (2006). T-satplan: a SAT-based temporal planner. International
Journal on Artificial Intelligence Tools, 15 (5), 779802.
Rankooh, M. F., & Ghassem-Sani, G. (2013). New encoding methods for sat-based temporal
planning. In Proceedings of the Twenty-Third International Conference on Automated
Planning and Scheduling, ICAPS 2013, Rome, Italy, June 10-14, 2013.
Rintanen, J. (2006). Compact representation of sets of binary constraints. In ECAI 2006,
17th European Conference on Artificial Intelligence, August 29 - September 1, 2006,
Riva del Garda, Italy, Including Prestigious Applications of Intelligent Systems (PAIS
2006), Proceedings, pp. 143147.
Rintanen, J. (2007). Complexity of concurrent temporal planning. In Proceedings of the
Seventeenth International Conference on Automated Planning and Scheduling, ICAPS
2007, Providence, Rhode Island, USA, September 22-26, 2007, pp. 280287.
Rintanen, J. (2012). Planning as satisfiability: Heuristics. Artif. Intell., 193, 4586.
Rintanen, J., & Gretton, C. O. (2013). Computing upper bounds on lengths of transition
sequences. In IJCAI 2013, Proceedings of the 23rd International Joint Conference on
Artificial Intelligence, Beijing, China, August 3-9, 2013.
Rintanen, J., Heljanko, K., & Niemela, I. (2006). Planning as satisfiability: parallel plans
and algorithms for plan search. Artif. Intell., 170 (12-13), 10311080.
Robinson, N., Gretton, C., Pham, D. N., & Sattar, A. (2009). Sat-based parallel planning
using a split representation of actions. In Proceedings of the 19th International Conference on Automated Planning and Scheduling, ICAPS 2009, Thessaloniki, Greece,
September 19-23, 2009.
Robinson, N., Gretton, C., Pham, D. N., & Sattar, A. (2010). Partial weighted MaxSAT
for optimal planning. In PRICAI 2010: Trends in Artificial Intelligence, 11th Pacific
Rim International Conference on Artificial Intelligence, Daegu, Korea, August 30September 2, 2010. Proceedings, pp. 231243.
Shin, J.-A., & Davis, E. (2005). Processes and continuous change in a SAT-based planner.
Artif. Intell., 166 (1-2), 194253.
Smith, D. E., & Weld, D. S. (1999). Temporal planning with mutual exclusion reasoning. In
Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence,
IJCAI 99, Stockholm, Sweden, July 31 - August 6, 1999. 2 Volumes, 1450 pages, pp.
326337.
Streeter, M. J., & Smith, S. F. (2007). Using decision procedures efficiently for optimization.
In Proceedings of the Seventeenth International Conference on Automated Planning
and Scheduling, ICAPS 2007, Providence, Rhode Island, USA, September 22-26, 2007,
pp. 312319.
631

fiRankooh & Ghassem-Sani

Vidal, V. (2014). Yahsp3 and Yahsp3-mt in the 8th international planning competition. In
International Planning Competition.
Vidal, V., & Geffner, H. (2006). Branching and pruning: An optimal temporal POCL
planner based on constraint programming. Artif. Intell., 170 (3), 298335.
Wehrle, M., & Rintanen, J. (2007). Planning as satisfiability with relaxed -Step plans.
In AI 2007: Advances in Artificial Intelligence, 20th Australian Joint Conference on
Artificial Intelligence, Gold Coast, Australia, December 2-6, 2007, Proceedings, pp.
244253.
Younes, H. L. S., & Simmons, R. G. (2003). VHPOP: Versatile heuristic partial order
planner. J. Artif. Intell. Res. (JAIR), 20, 405430.

632

fiJournal of Artificial Intelligence Research 53 (2015) 315374

Submitted 09/14; published 07/15

Regular Path Queries in Lightweight Description Logics:
Complexity and Algorithms
Meghyn Bienvenu

meghyn@lri.fr

Laboratoire de Recherche en Informatique,
CNRS & Universite Paris-Sud, France

Magdalena Ortiz
Mantas Simkus

ortiz@kr.tuwien.ac.at
simkus@dbai.tuwien.ac.at

Institute of Information Systems,
TU Wien, Austria

Abstract
Conjunctive regular path queries are an expressive extension of the well-known class
of conjunctive queries. Such queries have been extensively studied in the (graph) database
community, since they support a controlled form of recursion and enable sophisticated path
navigation. Somewhat surprisingly, there has been little work aimed at using such queries
in the context of description logic (DL) knowledge bases, particularly for the lightweight
DLs that are considered best suited for data-intensive applications. This paper aims to
bridge this gap by providing algorithms and tight complexity bounds for answering twoway conjunctive regular path queries over DL knowledge bases formulated in lightweight
DLs of the DL-Lite and EL families. Our results demonstrate that in data complexity,
the cost of moving to this richer query language is as low as one could wish for: the
problem is NL-complete for DL-Lite and P-complete for EL. The combined complexity of
query answering increases from NP- to PSpace-complete, but for two-way regular path
queries (without conjunction), we show that query answering is tractable even with respect
to combined complexity. Our results reveal two-way conjunctive regular path queries as
a promising language for querying data enriched by ontologies formulated in DLs of the
DL-Lite and EL families or the corresponding OWL 2 QL and EL profiles.

1. Introduction
Recent years have seen a rapidly growing interest in using description logic (DL) ontologies
to query instance data. This setting can be seen as a generalization of the related problem of
querying graph databases which, like DL instance data, are sets of ground facts using only
unary and binary predicates, i.e., node- and edge-labeled graphs (Consens & Mendelzon,
1990; Barcelo, Libkin, Lin, & Wood, 2012). The relevance of both problems lies in the
fact that in many application areas, data can be naturally represented in such form. This
applies, in particular, to XML and RDF data. In the presence of a DL ontology, the
domain knowledge expressed in the ontology is exploited when querying the data, which
can facilitate query formulation and provide users with more complete answers to their
queries. While the DL and database communities share some common research goals, the
research agendas they have pursued differ significantly. In the DL research, the focus has
been on studying the computational complexity of answering (unions of) plain conjunctive
queries (CQs) in the presence ontological constraints expressed in different DLs, and on
c
2015
AI Access Foundation. All rights reserved.

fiBienvenu, Ortiz, & Simkus

the development of efficient algorithms for this setting, see the surveys of Ortiz (2013)
and Ortiz and Simkus (2012). By contrast, the work on graph databases typically does
not consider ontological knowledge, but instead aims at supporting expressive navigational
query languages.
Regular path queries (RPQs) constitute the basic navigational query language. Formally, an RPQ is given as a regular language (represented as a regular expression or a finite
automaton) over the binary predicates in the database facts (arc labels, or roles in DL parlance), and it returns all pairs of objects that are connected by a path whose label is a word
belonging to the specified language. A crucial feature of these queries is that they allow for
controlled form of recursion that is computationally well behaved yet sufficient for expressing reachability queries and the traversal of paths of unbounded length (Florescu, Levy, &
Suciu, 1998). In two-way RPQs (2RPQs), the regular expressions may use the arc labels in
the backwards direction, which allows for more flexible path navigation. Notably, this comes
at no computational cost: answering both RPQs and 2RPQs over (plain) graph databases
is complete for NL in combined complexity (that is, when the complexity is measured in
terms of the whole input, which in this case consists of the query and the data). Conjunctive (two-way) regular path queries (C(2)RPQs), which are one of the most expressive
and popular languages for querying graph databases, simultaneously extend plain CQs and
(2)RPQs by allowing for conjunctions of atoms that can share variables in arbitrary ways,
where the atoms may contain regular expressions that navigate the arcs of the database.
If we consider the data complexity measure (in which the complexity is measured only in
terms of the size of the data, with all other inputs considered as fixed), then answering
C2RPQs is still NL-complete. In combined complexity, the C2RPQ answering problem is
NP-complete, which is in fact the lowest complexity that could be expected, given that
CQ answering is already NP-hard. We note that the navigational capabilities provided by
RPQs and their extensions have long been considered crucial for querying data on the Web.
Indeed, navigation along regular paths is at the heart of the XPath language for querying
XML data (Berglund, Boag, Chamberlin, Fernandez, Kay, Robie, & Simeon, 2007), and
SPARQL 1.1, the language recently recommended by the World Wide Web Consortium
(W3C) as the new standard for querying RDF data (Harris & Seaborne, 2013), adds to the
previous standard a feature called property paths, which roughly amounts to extending the
core of SPARQL from CQs to C2RPQs.
It comes as a surprise that, despite their advantages and relevance, RPQs and their extensions have received rather little attention in the DL literature. They were first considered
in the seminal work of Calvanese, de Giacomo, and Lenzerini (1998) on query answering
in the presence of DL ontologies. However, the vast majority of subsequent research has
targeted instance queries (IQs), conjunctive queries and unions thereof, and occasionally
positive first-order queries. Only a handful of works have considered C2RPQs. Calvanese et
al. (2014, 2007, 2009) showed that C2RPQ answering is 2ExpTime-complete in combined
complexity for the very expressive DLs1 ZIQ, ZIO, and ZOQ, which allow for regular
expressions as role constructors. The same upper bound was shown for containment of
C2RPQs in the presence of rich ontological knowledge (Calvanese et al., 2009). This is
noteworthy since it is a well-known fact that most forms of recursion make query answer1. The Z symbol was introduced as an abbreviation for ALCbSelf
reg (Calvanese et al., 2009).

316

fiRegular Path Queries in Lightweight Description Logics

ing and query containment undecidable in the presence of ontological constraints (Levy &
Rousset, 1996). Moreover, this complexity is not higher than that of other significantly
more restricted settings, such as answering plain CQs in the DL ALCI (Lutz, 2008) or positive first-order queries in ALC (Ortiz & Simkus, 2014). However, hardness for 2ExpTime
is nevertheless a prohibitively high complexity for many applications. Even in data complexity, the algorithms underlying the aforementioned results still need exponential time.
More recently, algorithms for answering C2RPQs in Horn-SHOIQ and Horn-SROIQ were
proposed (Ortiz, Rudolph, & Simkus, 2011). These algorithms run in polynomial time in
the size of the data, but may still require exponential time in the size of the ontology, which
is worst-case optimal for these logics. By contrast, for the lightweight DLs of the DL-Lite
(Calvanese, De Giacomo, Lembo, Lenzerini, & Rosati, 2007) and EL (Baader, Brandt, &
Lutz, 2005) families, which are the languages of choice for ontology-mediated query answering and notably underlie the OWL 2 QL and EL profiles (Motik, Cuenca Grau, Horrocks,
Wu, Fokoue, & Lutz, 2012), the precise complexity of answering C2RPQs was left open:
the ExpTime upper bound in combined complexity for more expressive Horn DLs does not
match the NP lower bound stemming of answering plain CQs, and it is not at all apparent
how to obtain better upper bounds for C2RPQs by adapting existing techniques. For the
DL-Lite family, the data complexity was also left open, with a gap between the NL-hardness
of RPQ answering inherited from the graph database setting and the P upper bound that
had been established for more expressive Horn DLs.
In this paper, we close these open questions by presenting algorithms and precise complexity bounds for answering (C)2RPQs in the EL and DL-Lite families of lightweight DLs.
Many of our results were first announced in the conference version of this paper (Bienvenu,
Ortiz, & Simkus, 2013), but here we provide full proofs of these results, additional examples, and a discussion of extensions of our results and their applicability to the OWL 2
profiles. We also strengthen some of the complexity lower bounds by considering more restricted classes of queries (Proposition 4.5) or more succinct representations (Theorem 4.2)
and identify interesting restrictions that lead to better combined complexity for C2RPQs
(Theorems 6.10 and 6.11). Our main contributions can be summarized as follows:
 We establish a P lower bound in combined complexity for answering 2RPQs in
DL-Lite, as well as for RPQs in DL-LiteR , which can be contrasted with the NLcompleteness of instance checking in these logics. This result improves upon a similar
lower bound from the conference version of this paper by adopting the (less succinct)
regular expression representation of queries.
 We present an algorithm for answering 2RPQs over DL-LiteR and ELH knowledge
bases that runs in polynomial time in combined complexity. This tractability result
is extended to all single-atom C2RPQs, including those with existential variables.
 We show that answering CRPQs is PSpace-hard for both DL-Lite and EL. This result
had already been shown in the conference version, but here we provide a different
proof that holds even for the structurally-restricted class of strongly acyclic CRPQs
in which the regular expressions are disjunction-free and of star-height two. This
hardness result is interesting when compared with the graph database setting, where
317

fiBienvenu, Ortiz, & Simkus

C2RPQ answering is NP-complete (and thus not harder than CQs in the worst case)
and becomes feasible in polynomial time when restricted to strongly acyclic C2RPQs.
 We develop a query rewriting procedure for answering C2RPQs in DL-LiteR and
ELH, which we use to show that the problem is feasible in PSpace for both logics.
The PSpace upper bound for ELH is especially interesting, since C2RPQs allow for
inverse roles and it is well known that adding inverse roles to EL immediately leads to
ExpTime-hardness of query answering, even for instance queries. This result demonstrates that by including inverses in the query language, rather than the ontology
language, it is possible to obtain algorithms that use only polynomial space.
 Using the same algorithm, we derive an NL upper bound in data complexity for DLLiteR and a P upper bound for ELH. In both cases, this is the lowest data complexity
that could be expected in light of existing results.
 We also identify cases where C2RPQ answering is feasible in NP, and thus not harder
than answering plain CQs. This is the case for queries whose existential variables do
not occur in joins (or only occur in joins in a restricted way), or for arbitrary C2RPQs
whenever the ontology is guaranteed to have a finite canonical model.
Our new complexity results (and relevant existing results) are summarized in Figure 1.
This paper is organized as follows. We begin in Section 2 by introducing the lightweight
description logics DL-LiteR and ELH (and relevant sublogics) and recalling some basic
notions related to regular languages and computational complexity. In Section 3, we define
the syntax and semantics of the different types of path queries considered in this paper.
Section 4 is dedicated to showing lower bounds, first for RPQs and then for CRPQs. In
both cases, we start by presenting some easy bounds that follow from known results, before
moving on to the main results. Section 5 presents algorithmic techniques and upper bounds
for 2RPQs, and then in Section 6, we show how these can be extended to handle C2RPQs.
In Section 7, we give a brief overview of related results for similar query languages and other
DLs and discuss the applicability of our results to the profiles of the OWL Web Ontology
Language. Conclusions and directions for future work are given in Section 8. To improve
the readability of the paper, one of the more technical proofs is deferred to the appendix.

2. Preliminaries
We briefly recall some basics of description logics, a few computational complexity classes
that are relevant in the paper, and some notation for regular languages.
2.1 Description Logics
We first recall the syntax and semantics of description logics, focusing on the lightweight
families of logics DL-Lite (Calvanese et al., 2007) and EL (Baader et al., 2005). We also
recall the definition of canonical model for these logics.
318

fiRegular Path Queries in Lightweight Description Logics

IQ

DL-LiteRDFS
DL-Lite(R)
EL(H)

data

combined

data

in AC0

NL-c

NL-c

NL-c

 (A)

 (A)

 Thm 5.2

in AC0

NL-c

NL-c

P-c

 (G)

 (G)

 Thm 5.9

 Thm 4.2,  Thm 5.9

P-c

P-c

P-c

P-c

 (E)

 (C)

 Thm 5.9

 Thm 5.9

CQ

DL-LiteRDFS
DL-Lite(R)
EL(H)

(2)RPQ
combined

C(2)RPQ
combined

data

combined

data

in AC0

NP-c

NL-c

NP-c

 (B)

 Thm 6.8

 Thm 6.8

in AC0

NP-c

NL-c

PSpace-c

 (D)

 (D)

 Thm 6.8

 Prop 4.5,  Thm 6.8

P-c

NP-c

P-c

PSpace-c

 (F)

 (F)

 Thm 6.8

 Prop 4.5,  Thm 6.8

Figure 1: Complexity of query answering. The c indicates completeness results, and 
and  are used for upper and lower bounds respectively. New results are marked in bold.
The remaining annotations have the following meanings:
 P-hardness for RPQs applies only to DL-LiteR
(A) Easy reduction from the NL-complete directed reachability problem
(B) Follows from NP-hardness of CQ answering over relational databases
(C) Baader et al. (2005)
(D) Calvanese et al. (2007)
(E) Calvanese, De Giacomo, Lembo, Lenzerini, and Rosati (2006)
(F) Rosati (2007), Krisnadhi and Lutz (2007), Krotzsch and Rudolph (2007)
(G) Calvanese et al. (2007), Artale, Calvanese, Kontchakov, and Zakharyaschev (2009)

2.1.1 Description Logic Syntax
As usual, we assume countably infinite, mutually disjoint sets NC , NR , and NI of concept
names, role names, and individuals, respectively. An inverse role takes the form r where



r  NR . We will use N
R to refer to NR  {r | r  NR }, and if R  NR , we use R to mean


r if R = r and r if R = r .
A description logic knowledge base (KB) K = (T , A) consists of a TBox T and an
ABox A. The former provides general domain knowledge, while the latter expresses facts
319

fiBienvenu, Ortiz, & Simkus

about particular entities. Sometimes we use the generic terms ontology and data(set) in
place of TBox and ABox.
Formally, a TBox is a finite set of inclusions, whose form depends on the DL in question.
In DL-Lite, TBoxes consist of a set of concept inclusions of the form B v C, with B and
C concepts constructed according to the following syntax:
B := A | R

C := B | B

where A  NC and R  N
R . DL-LiteR additionally allows for role inclusions of the form
R1 v ()R2 , where R1 , R2  N
R . The logic DL-LiteRDFS is obtained from DL-LiteR by
disallowing inclusions that contain negation or have existential concepts (R) on the righthand side. DL-LiteR is the basis for the OWL 2 QL profile, and DL-LiteRDFS corresponds
to the fragment of DL-LiteR that is expressible in the RDF Schema ontology language
(Brickley & Guha, 2014).
In EL, concept inclusions have the form C1 v C2 where C1 , C2 are complex concepts
constructed according to the following syntax:
C := > | A | C u C | r.C
where A  NC and r  NR . The DL ELH additionally allows for role inclusions of the form
r1 v r2 , where r1 , r2  NR . Note that in EL(H) TBoxes, inverse roles are not permitted.
We will use sig(T ) to denote the signature of a TBox T , that is, the set of all concept
and role names appearing in T . It will also prove useful to introduce the set BCT of basic
concepts for a TBox T , defined as follows: BCT = (NC sig(T )){r, r | r  NR sig(T )}
if T is a DL-LiteR TBox, and BCT = (NC  sig(T ))  {>} if T is an ELH TBox.
In all of the considered DLs, an ABox is a finite set of concept assertions of the form
A(b) and role assertions of the form r(b, c), where A  NC , r  NR , and b, c  NI . We use
Ind(A) to refer to the set of individuals appearing in the ABox A.
2.1.2 Description Logic Semantics
The semantics of DL KBs is defined in terms of interpretations, which take the form I =
(I , I ), where I is a non-empty set and I maps each individual a  NI to aI  I , each
concept name A  NC to AI  I , and each role name r  NR to rI  I  I . The
function I is extended to general concepts and roles as follows:
>I = I
(A)I = I \ AI
(R)I = (I  I ) \ RI

(r )I = {(c, d) | (d, c)  rI }
(R)I = {c | d : (c, d)  RI }
(r.C)I = {c | d : (c, d)  rI , d  C I }

An interpretation I satisfies an inclusion G v H, denoted I |= G v H, if GI  H I .
Similarly, I satisfies an assertion A(a) if aI  AI , in symbols I |= A(a); I satisfies an
assertion r(a, b) if (aI , bI )  rI , in symbols I |= r(a, b). An interpretation I is a model
of T , if I satisfies all inclusions in T ; it is a model of A if it satisfies all assertions in A;
and it is a model of (T , A) if it is a model of T and A. A KB (T , A) is satisfiable if it
possesses at least one model, else it is unsatisfiable. Note that ELH knowledge bases are
320

fiRegular Path Queries in Lightweight Description Logics

tramway:T1

sbHFT

TramwayLine

stop:cathSq

FT

b

subway:U1
SubwayLine

sb
H

Su

FT

sbSub

T
LF
T

locIn

theater:Volkstheater

sbLFT

locIn

park:huberPark
Park,FamFriendly

locIn

theater:opera

locIn

locIn

park:cityPark

locIn

trainSt:ViennaCenter
trainStation

EquipStop

playgr:cityPark
Playground

Park
locIn

Cafe:Sacher
Cafe

Theater

GLStop

stop:trainStation

Cafe:Hawelka
Cafe

Theater

GLStop

stop:cityPark

locIn

Square

EquipStop

F
sbL

TramwayLine

stop:Volkstheater

stop:opera

sb
Su
b
sb

tramway:T2

square:cathSq

Stop

sbH

sb

locIn

locIn

shopping:cityMall
ShoppingCenter

Figure 2: Example ABox Amob
always satisfiable. If G is a TBox, ABox, or KB, and  an inclusion or assertion, we say
that G entails , written G |= , if I |=  for every model I of G.
Observe that we do not make the Unique Names Assumption (UNA), as our definition
of interpretation allows distinct individuals to be mapped to the same domain element. We
remark however that all of the results in this paper hold equally well under the UNA.
Example 2.1. As a motivating example, we consider the domain of public transport and
urban mobility. A partial database for this domain is presented in Figure 2. The nodes in
the two leftmost columns (shaded in blue) correspond to public transport lines and stations.
The arrows between them represent the existing connections, and are labeled according to
the type of transport line serving them: sbSub stands for served by subway, while sbLFT
and sbHFT respectively stand for served by low-floor tramway and served by high-floor
tramway. Nodes are also labeled with classes in which they participate. In particular, the
labels GLStop and EquipStop indicate two specific kinds of public transport stops: groundlevel stops, and stops that are suitably equipped with ramps and elevators for passengers
with restricted mobility. The remaining columns contain places of interest, and the locInlabeled arrows between them represent the located in relation. Note that this is nothing
else but a graphical representation of a DL ABox, which we call Amob , where each label A
on a node b corresponds to the concept assertion A(b), and an arc labeled r from node b to
node c corresponds to the role assertion r(b, c).
In our example, we assume that the information in the first two columns is provided and
maintained by the local public transport authorities, thus it is complete and well structured.
In contrast, the remaining data is crowd-sourced, thus it is likely to be incomplete and may
not adhere to a rigid, predefined structure.
In order to better respond to user queries, this data is enriched with domain knowledge
expressed by the EL ontology in Figure 3. The ontology defines subclass relations between
concepts like stop and the more specialized accessible stop, ground-level stop, and it
defines new terms not present in the data but which may be useful at query time, such
as food services. It also enhances the possibly incomplete data by asserting the existence
of other places of interest. For example, a family-friendly location has both dining and
playground facilities.
321

fiBienvenu, Ortiz, & Simkus

(1) An accessible stop (AccStop) is a public transport stop (Stop). A ground-level stop (GLStop) or
a stop that is suitably equipped with ramps and
elevators (EquipStop) is an accessible stop.
(2) Restaurants and cafes are food services
(FoodServ).
(3) A place that is family friendly (FamFriendly)
has some food service and a playground.

AccStop v Stop

(1a)

GLStop v AccStop

(1b)

EquipStop v AccStop

(1c)

Restaurant v FoodServ

(2a)

Cafe v FoodServ

(2b)

FamFriendly v hasFacility.FoodServ

(3a)

FamFriendly v hasFacility.Playground (3b)
ShoppingCenter v hasFacility.Foodcourt

(4a)

(4) A shopping center has a supermarket and a
ShoppingCenter v hasFacility.Supermarket (4b)
food court.
Foodcourt v hasFacility.FoodServ

(5) A food court has some food service.

(5a)

Figure 3: Example DL-Lite TBox Tmob expressing domain knowledge
To keep the example compact, we have chosen to write the example ontology in EL,
but the same knowledge can be expressed in DL-LiteR by using auxiliary roles to simulate
the concept inclusions with qualified existential quantification on the right-hand-side. For
instance, the concept inclusion (3a) can be replaced by the following three inclusions in the
syntax of DL-Lite:
FamFriendly v hasFoodServ
hasFoodServ v FoodServ
hasFoodServ v hasFacility
and similarly for (3b), (4a), (4b), and (5a).
2.1.3 Normal Form for ELH TBoxes
To simplify the presentation, we will assume throughout the paper that ELH TBoxes are
in normal form, meaning that all concept inclusions are of one of the following forms:
AvB

A v r.B

A 1 u A2 v B

r.B v A

with A, A1 , A2 , B  NC {>}. The following well-known property (see (Baader et al., 2005))
shows that this assumption is without loss of generality.
Proposition 2.2. For every ELH TBox T , one can construct in polynomial time an ELH
TBox T 0 in normal form (possibly using new concept names) such that T 0 is a model
conservative extension of T , that is, every model of T 0 is a model of T , and for every model
I of T , there is a model I 0 of T 0 such that I and I 0 have the same domain and coincide on
the interpretation of all concept and role names except those in sig(T 0 ) \ sig(T ).
322

fiRegular Path Queries in Lightweight Description Logics

2.1.4 Canonical Models
Canonical models are a key technical tool used to study lightweight description logics, and
we will use them in the proofs of many of our results.
We recall the definition of the canonical model IT ,A (alternatively denoted IK ) of a
satisfiable DL-LiteR or ELH KB K = (T , A). The domain IT ,A consists of sequences of
the form aR1 C1 . . . Rn Cn (n  0), where a  Ind(A), each Ci is a concept, and each Ri is a
(possibly inverse) role. The exact definition depends on which logic we consider:
(A) When T is a DL-LiteR TBox, the domain IT ,A contains exactly those sequences
aR1 R1 . . . Rn Rn which satisfy:
 if n  1, then T , A |= R1 (a)
 for 1  i < n, T |= Ri v Ri+1 .
(B) When T is an ELH TBox,2 the domain IT ,A contains exactly those sequences
ar1 A1 . . . rn An for which each ri  NR , and:
 if n  1, then T , A |= r1 .A1 (a);
 for 1  i < n, T |= Ai v ri+1 .Ai+1 .
For elements e  IT ,A \ Ind(A), we will use the notation Tail(e) to denote the final concept
in e. The set TCT of tail concepts for a TBox T is defined as follows: TCT = {r, r | r 
NR  sig(T )} if T is a DL-LiteR TBox, and TCT = BCT = (NC  sig(T ))  {>} if T is an
ELH TBox. Clearly, if e  IT ,A \ Ind(A), then Tail(e)  TCT .
To complete the definition of IT ,A , we must fix the interpretation of the individual
names, concept names, and role names. This is done as follows:
AIT ,A = {a  Ind(A) | T , A |= A(a)}  {e  IT ,A \ Ind(A) | T |= Tail(e) v A}
rIT ,A = {(a, b) | T , A |= r(a, b)}  {(e1 , e2 ) | e2 = e1 S C and T |= S v r} 
{(e2 , e1 ) | e2 = e1 S C and T |= S v r }
aIT ,A = a

for all a  Ind(A)

Note that IT ,A is composed of a core consisting of the ABox individuals and an anonymous part consisting of (possibly infinite) trees rooted at the ABox individuals. We
will use IT ,A |e to denote the submodel of IT ,A obtained by restricting the domain to those
elements containing e as a prefix. Observe that, by construction, IT ,A |e and IT ,A |e0 are
isomorphic whenever Tail(e) = Tail(e0 ).
It can be verified that IK |= K for every satisfiable KB K. Moreover, it is well known
that the canonical model IK can be homomorphically embedded into any model of K.
Example 2.3. The canonical model of (Tmob , Amob ) from Example 2.1 is depicted in Figure 4. To satisfy the existential restrictions in the inclusions (3a)  (5a) of Tmob , the
2. Recall that throughout the paper, we assume that ELH KBs are in normal form, and for this reason, we
only need to consider existential concepts of the form r.A with A  NC  {>}.

323

fiBienvenu, Ortiz, & Simkus

tramway:T1

sbHFT

TramwayLine

stop:cathSq

b

subway:U1
SubwayLine

sb
H

FT

sbSub

T
LF

FT

TramwayLine

stop:Volkstheater

locIn

theater:Volkstheater

Park,FamFriendly

lo c

sbLFT

stop:trainStation

lo

cIn

hasFacility

e1
FoodServ

In

GLStop,AccStop,Stop

park:huberPark

hasFacility

stop:cityPark

sbL

locIn

Theater

GLStop,AccStop,Stop

EquipStop,AccStop,Stop

sb

tramway:T2

Cafe,FoodServ

stop:opera

sb
Su
b

Cafe:Hawelka

Square

FT

Su

locIn

square:cathSq

Stop

sbH

sb

locIn

theater:opera

e2
Playground

locIn

Cafe:Sacher
Cafe,FoodServ

Theater

EquipStop,AccStop,Stop

locIn

In

loc

park:cityPark

playgr:cityPark

Park

Playground

trainSt:ViennaCenter

locIn

shopping:cityMall

trainStation

ShoppingCenter
hasFacility

e3
Foodcourt

hasFacility

e4
Supermarket

hasFacility

e5
FoodServ

Figure 4: Canonical model ITmob ,Amob of the KB (Tmob , Amob )
canonical model contains the following five anonymous elements e1 , . . . e5 , which form two
tree-shaped structures rooted at the nodes park:huberPark and shopping:cityMall:
e1 = park:huberPark hasFacility FoodServ
e2 = park:huberPark hasFacility Playground
e3 = shopping:cityMall hasFacility Foodcourt
e4 = shopping:cityMall hasFacility Supermarket
e5 = shopping:cityMall hasFacility Foodcourt hasFacility FoodServ

Example 2.4. To illustrate why canonical models can be infinite, we present in Figure 5
a simple DL-LiteR knowledge base (T , A) and a depiction of its canonical model IT ,A . As
in the preceding example, we use names ei as abbreviations for the anonymous objects in
IT ,A . Note that the tree rooted at b2 is infinite, since every object that belongs to the
concept B has an r-child that also belongs to B.
2.2 Regular Languages
We assume the reader is familiar with regular languages, represented either by regular
expressions or nondeterministic finite state automata (NFAs). Regular expressions E over
alphabet  are defined by the grammar
E   |  | E  E | E  E | E
where    and  denotes the empty word (i.e., the sequence of length 0). An NFA over an
alphabet  is a tuple  = (S, , , s0 , F ), where S is a finite set of states,   S    S the
324

fiRegular Path Queries in Lightweight Description Logics

B, D
r

a

b

B v r,
B v r1 ,

r v B,
r1 v r2

e1

}
e11

A = { r(a, b), r2 (b, c), D(b) }

e2

B

r


r1 , r2

B

e12


r1 , r2

r

e111

c


r1 , r2

r

T ={

r2

e112

B

..
.

Figure 5: Example DL-LiteR knowledge base (T , A) and its canonical model IT ,A
transition relation, s0  S the initial state, and F  S the set of final states. We use L(E)
(resp. L()) to denote the language defined by the regular expression E (resp. the NFA ).
We recall that NFAs are exponentially more succinct than regular expressions, in that
there exists a polynomial translation of regular expressions into equivalent NFAs, while the
translation from NFAs to regular expressions may incur an exponential blowup (Ehrenfeucht
& Zeiger, 1974). Thus, to ensure that our complexity results hold regardless of the chosen
representation, we will prove our complexity lower bounds using the regular expression
representation, and for our upper bounds, we will adopt the NFA representation.
2.3 Computational Complexity
We assume familiarity with standard complexity classes, such as NL (problems solvable in
non-deterministic logarithmic space), P (problems solvable in polynomial time), NP (problems solvable in non-deterministic polynomial time), PSpace (problems solvable in polynomial space), and NPspace (problems solvable in non-deterministic polynomial space).
We recall that by Savitchs theorem, we have NPspace = PSpace. We shall also consider the oracle classes NLNL and NLP consisting of problems solvable in non-deterministic
logarithmic space when given access to an NL (respectively, P) oracle. It is well-known
that NLNL = NL and NLP = P. The circuit complexity class AC0 that was mentioned in
Figure 1 comprises problems that can be computed by a family of unbounded fan-in circuits
of constant depth and polynomial size. The preceding classes are ordered as follows:
AC0 ( NL  P  NP  PSpace
For precise definitions of these complexity classes, and other standard notions of computational complexity, we refer the reader to the recent textbook of Arora and Barak (2009)
and references therein.

3. Path Queries
In this section, we introduce the different query languages considered in this paper and
define the relevant computational problems.
325

fiBienvenu, Ortiz, & Simkus


q1 (x, y) = AccStop?  (sbSub  sbSub )  (sbLFT  sbLFT )  AccStop?(x, y)

q2 (x, y) = z1 , z2 . AccStop?  (sbSub  sbSub )  (sbLFT  sbLFT )  AccStop?(x, y)
 locIn  (locIn )  hasFacility  FoodServ? (x, z1 )
 locIn  (locIn )  hasFacility  Playground? (y, z2 )

Figure 6: Example queries
3.1 Syntax of Path Queries
A conjunctive (two-way) regular path query (C2RPQ) has the form q(~x) = ~y .  where ~x
and ~y are disjoint tuples of variables, and  is a conjunction of atoms of the forms:
(i) A(t), where A  NC and t  NI  ~x  ~y
(ii) (t, t0 ), where  is an NFA or regular expression defining a regular language over
0
x  ~y
N
R  {A? | A  NC }, and t, t  NI  ~
As usual, variables and individuals are called terms, the variables in ~x are called answer
variables, and the variables in ~y are called quantified variables. We use terms(q), vars(q),
avars(q), and qvars(q) to refer respectively to the sets of terms, variables, answer variables,
and quantified variables appearing in query q. A query with no answer variables is called a
Boolean query. Note that where convenient we will treat a query as its set of atoms.
Conjunctive (one-way) regular path queries (CRPQs) are obtained by disallowing symbols from N
R \ NR in atoms of type (ii), and conjunctive queries (CQs) result from only
allowing type (ii) atoms of the form r(t, t0 ) with r  NR . Two-way regular path queries
(2RPQs) consist of a single atom of type (ii) such that t and t0 are both answer variables.
Regular path queries (RPQs) are 2RPQs that do not use any symbols from N
R \NR . Finally,
instance queries (IQs) take the form A(x) with A  NC , or r(x, y) with r  NR .
Note that it will sometimes prove convenient to treat queries as sets of atoms, using the
notation   q to indicate that  is an atom of q.
Example 3.1. Figure 6 shows two example queries. The 2RPQ q1 retrieves pairs x, y
of public transport stops that have an accessible connection, that is, both x and y are
accessible stops, and there is a public transport route between them that uses only subway
and low-floor tramway. The query q2 retrieves pairs x, y of public transport stops that have
an accessible connection (as in q1 ) and such that there is a place to eat at the location of x
and a playground at the location of y.
Note that by using the Kleene star ( ), we can query for services such as restaurants and
playgrounds available at some location without having to take care of the different ways in
which places can be related. For example, a restaurant could be at the same location as a
stop, or it could be that it is in a food court inside some shopping center that is itself at
the same location as a stop. In both cases, q2 correctly identifies it as a food service at that
location. This is a very useful feature of RPQs and their extensions, particularly in cases
where the data does not comply to a rigid schema.
326

fiRegular Path Queries in Lightweight Description Logics

3.2 Semantics of Path Queries
We now proceed to define the semantics of C2RPQs. Given an interpretation I, a path from
e0 to en in I is a sequence e0 u1 e1 u2 . . . un en with n  0 such that every ei is an element
from I , every ui is a symbol from N
R  {A? | A  NC }, and for every 1  i  n:
 If ui = A?, then ei1 = ei  AI ;
I
 If ui = R  N
R , then (ei1 , ei )  R .

The label (p) of path p = e0 u1 e1 u2 . . . un en is the word u1 u2 . . . un . Note that if p = e0 ,
then we define (p) to be .
Then for every language L over N
R  {A? | A  NC }, the semantics of L w.r.t. interpretation I is defined as follows:
LI = {(e0 , en ) | there is some path p from e0 to en with (p)  L}
A match for a C2RPQ q in an interpretation I is a mapping  from the terms in q to
elements in I such that
 (c) = cI for each c  NI ,
 (t)  AI for each atom A(t) in q, and
 ((t), (t0 ))  L()I for each (t, t0 ) in q.
Given a C2RPQ q with answer variables x1 , . . . , xk , we say that a tuple of individuals
(a1 , . . . , ak ) from Ind(A) is a certain answer to q w.r.t. the KB K = (T , A) just in the case
that in every model I of K there is a match  for q such that (vi ) = aIi for every 1  i  k.
We use cert(q, K) to denote the set of certain answers to q w.r.t. the KB K. Note that if
q is a Boolean query, then either cert(q, K) = {()} (where () denotes the empty tuple) or
cert(q, K) = . In the former case, we say that q is entailed from K, and we write K |= q.
We remark that the normal form for ELH TBoxes can also be assumed without loss of
generality for query answering. Indeed, we can always assume that the fresh symbols in
the TBox T 0 in normal form do not occur in q, and it follows from Proposition 2.2 and the
definition of certain answers that cert(q, (T , A)) = cert(q, (T 0 , A)) for every C2RPQ q that
does not use any symbols in sig(T 0 ) \ sig(T ).
While the definition of certain answers involves all models of a KB, for the DLs considered in this paper, it is in fact sufficient to look for matches in the canonical model.
Lemma 3.2. For every DL-LiteR or ELH KB K = (T , A), C2RPQ q(~x) of arity k, and
k-tuple ~a of individuals from A: ~a  cert(q, K) if and only if there is a match  for q in IK
such that (~x) = ~a.
Proof sketch. It is well known that the canonical model IK can be homomorphically embedded into any model of K (Calvanese et al., 2007; Rosati, 2007; Krisnadhi & Lutz, 2007;
Krotzsch & Rudolph, 2007). It follows that whenever query matches are preserved under
homomorphisms, the existence of a match in IK implies the existence of a match in every
model. This has been often observed for CQs, and it applies equally well to C2RPQs (Calvanese et al., 2014; Ortiz et al., 2011). Since the converse is trivially true, certain answers
coincide with the answers over the canonical model.
327

fiBienvenu, Ortiz, & Simkus

D

a

s0

s0

r
r

s0

e1 s01

b

c

s0f

r

x


e2

r

s0 sf
r

s0 s2
r1

e11

e12

s0

:

s00

r1

s1

r2

s2

r

sf

r

s01

r

s0f



r

e111

:

y

r

s00

r

e112

z

D

r2

e1111

s1 e1112
..
.

Figure 7: A match witnessing a  cert(q, K) for q and K from Example 3.4
Example 3.3. In our urban mobility example, the stop at the cathedral square is not
known to be accessible (i.e., AccStop(stop:cathSq) is not entailed by (Tmob , Amob )), hence
stop:cathSq cannot participate in any match for q1 . The stop at the theater is accessible,
but it is only connected to other stops via high-floor tramway. Thus stop:Volkstheater only
participates in one mapping for q1 in ITmob ,Amob , namely (x) = (y) = stop:Volkstheater.
Indeed, the path
stop:Volkstheater AccStop? stop:Volkstheater AccStop? stop:Volkstheater

witnesses that (stop:Volkstheater, stop:Volkstheater)  LI1 for the language L1 specified in q1 ,
and there is no longer path starting or ending at stop:Volkstheater whose label belongs to L1 .
The stops stop:opera, stop:cityPark, and stop:trainStation are all accessible and mutually connected via accessible public transport (i.e., subway and low-floor tramway lines). Hence, we
can find a path between any pair of them whose label is in L1 , and all such pairs are certain
answers to q1 . Thus cert(q1 , (Tmob , Amob )) contains (stop:Volkstheater, stop:Volkstheater), and
all pairs of stops involving stop:opera, stop:cityPark, and stop:trainStation.
The certain answers to q2 are precisely those pairs (s1 , s2 ) of stops that are an answer to
q1 such that there is some food service at the location of s1 and a playground at the location
of s2 . Since in ITmob ,Amob , we find both some food service and a playground at the location of stop:Volkstheater, we have (stop:Volkstheater, stop:Volkstheater)  cert(q2 , (Tmob , Amob )).
We also find food services at the locations of stop:opera and stop:trainStation, and a playground at the location of stop:cityPark, hence cert(q2 , (Tmob , Amob )) also contains the pairs
(stop:opera, stop:cityPark) and (stop:trainStation, stop:cityPark).
Example 3.4. We also give an example of a query over the KB (T , A) in Figure 5:
q(x) = y, z. r  r1  r2  r (x, y)  r  r (y, z)  D(z)
We have cert(q, K) = {a, b}. To see why a is a certain answer, consider the mapping
(x) = a, (y) = e11 , and (z) = b. The path arbre1 re11 re111 r1 e1112 r2 e111 r e11 witnesses
that (a, e11 )  L(r  r1  r2  r )IT ,A , while the path e11 r e1 r b witnesses that (e11 , b) 
328

fiRegular Path Queries in Lightweight Description Logics

L(r  r )IT ,A . Since b  DIT ,A , the mapping  is a match for q. This match is depicted
in Figure 7. To see that b is also a certain answer, consider  0 (x) =  0 (z) = b and  0 (y) =
e11 , and observe that  0 is also a match because bre1 re11 re111 r1 e1112 r2 e111 r e11 witnesses
(b, e11 )  L(r  r1  r2  r )IT ,A .
Note that in both matches, y is mapped to an element in the anonymous part, and there
is no match mapping y to an individual. This illustrates that anonymous elements may play
a decisive role in query answering, and every complete query answering algorithm must
consider possible matches into the possibly infinite anonymous part of canonical models.
3.3 Computational Problems
In this paper, we will be interested in the problem of computing the certain answers to
C2RPQs, and more precisely, the associated decision problem of determining whether a
given tuple is a certain answer to a query. In what follows, for a query language Q 
{IQ, CQ, RPQ, CRPQ, 2RPQ, C2RPQ}, we will use the term Q answering to refer to the
problem of deciding given a KB K, tuple ~a, and query q from Q, whether ~a  cert(q, K).
There are different ways of measuring the complexity of query answering, depending on
which of the three parameters of the problem (T , A, and q) are considered as inputs and
which are considered fixed. In this work, we consider the two most commonly used measures:
combined complexity and data complexity. Combined complexity treats all three parameters
as inputs, so the complexity is measured with respect to the total size |T | + |A| + |q| (we use
|  | to denote the size of an object, e.g. the length of its string representation according to
some suitable encoding). Data complexity takes A as input and assumes T and q to be fixed,
so the complexity is measured only with respect to |A|, with |T | and |q| treated as constants.

4. Lower Bounds
In this section, we establish the required complexity lower bounds. We begin with some
lower bounds for RPQs that can be straightforwardly obtained from existing results.
Proposition 4.1. RPQ answering is
1. NL-hard in data complexity for DL-LiteRDFS ;
2. P-hard in data complexity for EL;
Proof. Statement (1) follows from the analogous result for graph databases (Consens &
Mendelzon, 1990). It can be shown by a simple reduction from the NL-complete directed
reachability problem: vertex b is reachable from vertex a in a directed graph G if and only
if (a, b) is a certain answer to the RPQ r (x, y) w.r.t. the KB (, AG ), where
AG = {r(v1 , v2 ) | there is a directed edge from v1 to v2 in G}.
Statement (2) is a direct consequence of the P-hardness in data complexity of instance
checking in EL (Calvanese et al., 2006), since the instance query A(x) can be computed
using the RPQ A?(x, y).
In the case of DL-Lite, we establish a P lower bound for 2RPQs, which contrasts with
the NL-completeness of instance checking. We remark that a similar result was given in
329

fiBienvenu, Ortiz, & Simkus

the conference version of this paper (Bienvenu et al., 2013), but the reduction required an
NFA representation of the regular language in the query. The complexity of 2RPQs using
the (less succinct) regular expression representation was left open and is resolved by the
following theorem.
Theorem 4.2. 2RPQ answering is P-hard in combined complexity for DL-Lite.
Proof. We give a reduction from the P-complete entailment problem for propositional definite Horn theories. Without loss of generality, we suppose that we are given a propositional
Horn theory  over variables v1 , . . . , vn that consists of
 a set of rules i = vi1  vi2  vi3 (1  i  m)
 a single initialization fact: v1 , with v1 6= vn
Indeed, any arbitrary propositional definite Horn theory 0 can be transformed into a
theory of the preceding form as follows: take a fresh variable v1 not appearing in 0 , add
v1 to the body of every rule in 0 , add the fact v1 , and finally perform standard syntactic
manipulations (possibly introducing additional fresh variables) to ensure that all rules other
than the initialization fact v1 contain exactly two body variables.
In what follows, we show how to construct, given a propositional Horn theory  of the
form above, a DL-Lite KB K = (T , A) and Boolean 2RPQ q such that K |= q if and only
if  |= vn . We first provide an informal description of the reduction, and then present it
formally. As is well known,  |= vn just in the case that there exists a proof tree for vn
from , which can be defined as a binary tree T where each node is labeled with a variable
from v1 , . . . , vn such that the following conditions are satisfied: (i) the root is labeled with
vn , (ii) the leaves are labeled with v1 , and (iii) for any inner node d, if d is labeled with vk ,
then there is a rule i   such that vk = vi3 and the two children of d are labeled with
vi1 and vi2 , respectively. The existence of a node-labeled proof tree T as just described
is equivalent to the existence of an edge-labeled proof tree T 0 , defined as a sibling-ordered
binary tree whose edges are labeled with rules from  as follows. First, the two edges
outgoing from the root are labeled with a rule i with vi3 = vn . For a non-root node d`
which is the `-th child of its parent d, where `  {1, 2}, if the edge (d, d` ) is labeled with i ,
then either vi` = v1 , or d` has two outgoing edges E1 and E2 that are both labeled with a
rule j such that vj3 = vi` .
We will next show how to construct K and q such that K |= q if and only if there exists
a edge-labeled proof tree T 0 for vn from . Roughly speaking, we use K to generate in
the anonymous part a tree that contains all possible edge-labeled proof trees. Since such
proof trees are based upon sibling-ordered trees, we need to distinguish between the first
and second children of a node, and so we use two roles ri,1 and ri,2 for each rule i . An
edge-labeled proof tree T 0 will thus map into a subtree of IK of the same structure, but with
label i replaced by either ri,1 or ri,2 , depending on whether the edge leads to the first or
second child of the parent node. We then use a 2RPQ q to determine whether IK actually
contains such a subtree. The intuition is that every path that witnesses the satisfaction of
q corresponds to the complete depth-first traversal of (the representation of) a valid edgelabeled proof tree that starts and ends at the root, and in which the left subtree of a node
is always visited before the right one.
330

fiRegular Path Queries in Lightweight Description Logics




E =(

[

1im

ri,1 )  

[

1im

ri,1 

[
kF1


(rk,1
 rk,2 ) 

[
kF2

[


(rk,2
(

 
ri,2
) (

1im

[


(ri,1
 ri,2 )  ))

1im

Figure 8: Regular expression used in the proof of Theorem 4.2.
The ABox A consists of a single assertion A(a), and the TBox T contains the following
concept inclusions:
 A v ri,` , where `  {1, 2} and 1  i  m with vi3 = vn

 ri,`
v rk,j , where `, j  {1, 2} and 1  i, k  m such that vi` = vk3

For `  {1, 2}, we define the set
F` = {k | 1  k  m, vk` = v1 }.
Intuitively, F` contains the index of each rule where we can turn back since its `-th variable
is the initial variable v1 , (and so the corresponding child node in the proof tree would be a
leaf). We use F1 and F2 to define the regular expression E in Figure 8, which we then use
to define the following 2RPQ:
q = E(a, a).
We now prove the correctness of the reduction.
() Suppose that K |= q. Then by Lemma 3.2, there is a match for q in the canonical
model IK of K. This means that (a, a) = (aIK , aIK )  L(E)IK , and so there exists a path
e0 1 . . . p ep in IK whose label is in L(E) such that e0 = ep = a.
Claim 1. For every 2  j  p:

1. If j = ri,1 , then there exists j 0 > j with j 0 = ri,2
.


2. If j = ri,2
, then there exists j 0 < j such that j 0 = ri,1
.

3. If j = ri,1
and i 6 F1 , then j1 = ri0 ,2 where vi03 = vi1 .

4. If j = ri,2
and i 6 F2 , then j1 = ri0 ,2 where vi03 = vi2 .

Proof of claim. For Point 1, we remark that in IK all roles are directed away from the
ABox; formally, for every role name s, if (g, g 0 )  sIK , then g 0 = gss . It follows that if

j = ri,1 , then ej = ej1 ri,1 ri,1
. Since the sequence of elements ej , . . . , ep defines a path
in IK from ej to ep = a, by continuity, there must be some j0 > j such that ej0 1 = ej

and ej0 = ej1 , in which case we must have j0 = ri,1
. Next note that the structure of E

ensures that every occurrence of ri,1 is immediately followed by ri,2 . Then repeating the

same argument, substituting ri,2 for ri,1 , we can find some j 0 > j0 such that j 0 = ri,2
.
For Point 2, we again use the fact that roles in IK are directed away from the ABox.


Thus, if j = ri,2
, then ej1 = ej ri,2 ri,2
. The sequence e0 , . . . , ej1 of elements forms a
331

fiBienvenu, Ortiz, & Simkus


path in IK from e0 = a to ej ri,2 ri,2
. Thus, by continuity, there must exist some j 0 < j

such that ej 0 = ej , ej 0 +1 = ej ri,2 ri,2
, and j 0 +1 = ri,2 . By examining the structure of E, we


can see that any occurrence of ri,2 must be immediately preceded by ri,1
, and so j 0 = ri,1
.

To show Point 3, suppose that j = ri,1
and i 6 F1 . The structure of E ensures
IK
that the preceding symbol j1 takes the form ri0 ,2 . We thus have (ej , ej1 )  ri,1
and

(ej1 , ej2 )  riI0K,2 . Once again using the fact that elements in IK do not contain inverse


role names, we obtain ej2 = ej ri,1 ri,1
ri0 ,2 ri0 ,2 . It follows that T |= ri,1
v ri0 ,2 , which
can only be the case if vi03 = vi1 .

Finally, for Point 4, suppose that j = ri,2
and i 6 F2 . Examining the structure
of E, it is clear that the preceding symbol j1 must be of the form ri0 ,2 , and so we have
IK

(ej , ej1 )  ri,2
and (ej1 , ej2 )  riI0K,2 . This means that T |= ri,2
v ri0 ,2 , and hence
vi03 = vi2 . (end proof of claim)

It is easy to see that the first symbol 1 must have the form ri,1 , and so we have
IK
IK

(e0 , e1 )  ri,1
. Since e0 = a, we have (a, ari,1 ri,1
)  ri,1
. This implies that T |= A v ri,1 ,
and hence that vi3 = vn . Applying Points 1 and 2 of the preceding claim, we can find j, k


such that j = ri,1
and k = ri,2
. To complete the proof of this direction, we establish the
following claim.

Claim 2. For every 1  j  p and `  {1, 2}, if j = ri,`
, then  |= vi` .

Proof of claim. We proceed by induction on j. For the base case, suppose that j = ri,`
,

0
and there is no j < j with j 0 = ri0 ,`0 . It follows from Claim 1 that i  F1 and ` = 1. We
thus have vi` = v1 , so  |= vi` trivially holds.

For the induction step, suppose that claim holds for all j < k, and consider k = ri,`
.
We consider three cases:

and i  F`
 Case 1: k = ri,`
Since i  F` , we have vi` = v1 , so  |= vi` follows immediately.

 Case 2: k = ri,1
and i 6 F1
By Point 3 of Claim 1, k1 = ri0 ,2 where vi03 = vi1 . Applying the induction hypothesis
to k1 , we obtain  |= vi02 . From Point 2 of Claim 2, there exists some j < k such
that j = ri0 ,1 . A second application of the induction hypothesis yields  |= vi01 . The
rule vi01  vi02  vi03 belongs to , so we must also have  |= vi03 . Then since vi03 = vi1 ,
we obtain  |= vi1 .

 Case 3: k = ri,2
and i 6 F2
We can use almost the same argument as for Case 2, except that we must use Point
4 of Claim 1, rather than Point 3. (end proof of claim)

As  contains the rule vi1  vi2  vi3 , it follows from Claim 2 that  |= vn .
() If  |= vn , then there must be exist an edge-labeled proof tree T 0 for vn from 
as described at the beginning of the proof. We define a mapping f from the nodes of T 0 to
domain elements in IT ,A as follows:
332

fiRegular Path Queries in Lightweight Description Logics

 f (d) = a for d the root of T 0 ;
 for every non-root node d, if d is the first (resp., second) child of its parent dp and


(dp , d) is labeled i , then f (d) = f (dp )ri,1 ri,1
(resp. f (d) = f (dp )ri,2 ri,2
).
We show the following claim:
Claim 3: For every non-leaf node d in T 0 , (f (d), f (d))  L(E)IT ,A .
Proof of claim. For every non-leaf node d, we consider a depth-first traversal of the subtree
of T 0 rooted at d that always visits the left subtree of a node before visiting the right one,
and then returns to the root. Let d1 , d2 , . . . , dn be the sequence of nodes visited in this
traversal, with d1 = dn = d, and for every 2  i  n, let i1,i be as follows:
 i1,i = rj,1 if di is the first child of di1 and (di1 , di ) is labeled with j ;
 i1,i = rj,2 if di is the second child of di1 and (di1 , di ) is labeled with j ;

 i1,i = rj,1
if di1 is the first child of di and (di , di1 ) is labeled with j ;

 i1,i = rj,2
if di1 is the second child of di and (di , di1 ) is labeled with j .

We now define a path pd as follows:
pd = f (d1 )1,2 f (d2 )2,3 . . . n1,n f (dn )
To show the claim, it suffices to show the following for every non-leaf node d:
() pd is a path in IT ,A with (pd )  L(E).
This can be shown by induction on the minimal distance of d to a leaf in T 0 . An important
observation is that, for `  {1, 2}, if the `th child d` of a node d of T 0 is a leaf, and the edge
from d to d` is labeled i , then by definition i  F` .
In order to be able to more easily argue that some words belong to L(E), we give names
to the relevant subexpressions of E:
[
E1 =
ri,1
1im

E2 =

[


(rk,1
 rk,2 )

kF1

E3 = (

[

 
ri,2
)

1im

E4 = (

[


(ri,1
 ri,2 )  )

1im

E5 =

[


(rk,2
(

kF2

[

 
ri,2
) (

1im

[


(ri,1
 ri,2 )  )

1im




E6 = 

[
1im

ri,1 

[
kF1


(rk,1
 rk,2 ) 

[


(rk,2
(

[

1im

kF2

333

 
ri,2
) (

[


(ri,1
 ri,2 )  ))

1im

fiBienvenu, Ortiz, & Simkus

S

Note that we have E5 = kF2 (rk,2
 E3  E4 ), E6 = (E1  E2  E5 ) , and E = E1  E6 .
Now we are ready to prove (). For the base case, when both children of d are leaves,
let i be the label of the edges from d to its children. By construction of IT ,A , f (d) has an


ri,1 -child and an ri,2 -child, so pd is a path in IT ,A with (pd ) = ri,1  ri,1
 ri,2  ri,2
. Both

children of d are leaves, so i  F1  F2 . We thus have ri,1  L(E1 ), ri,1  ri,2  L(E2 ), and
S


ri,2
 L(E5 ) = L( kF2 (rk,2
 E3  E4 )) (for the latter, observe that   L(E3 ) and   L(E4 )).



Using E6 = (E1  E2  E5 ) , we get ri,1
 ri,2  ri,2
 L(E6 ), and using E = E1  E6 , we obtain


(pd ) = ri,1  ri,1  ri,2  ri,2  L(E).
For the induction step, let dL and dR be the left and right children of d respectively,
and let i be the rule that is used to label the edges from d to dL and from d to dR . By
construction of IT ,A and f , we know that f (dL ) is an ri,1 -child of f (d) and that f (dR ) is
an ri,2 -child of f (d). We distinguish three cases:
 If neither of the children dL and dR of d is a leaf, then we know from the induction
hypothesis that pdL and pdR are paths in IT ,A which start and end at f (dL ) and
f (dR ) respectively and are such that {(pdL ), (pdR )}  L(E). It follows that pd =


f (d)ri,1 pdL ri,1
f (d)ri,2 pdR ri,2
f (d) is a path in IT ,A . We let kL (resp. kR ) be the label
L
R
linking d (resp. d ) to its two children. Note that by construction, (pdL ) (resp.
(pdL )) ends with rkL ,1 (resp. rkL ,2 ). It follows that (pdL )  L(E1  E6  E5 ) and that
S

 ri,2 )  ) of the final E5 , we must select . By
for the subexpression E4 = ( 1im (ri,1


choosing to instantiate E4 with ri,1 ri,2 instead, we can show that (pdL )  ri,1
 ri,2 

L(E6 ). A similar argument for (pdR ) can be used to show that (pdR )  ri,2
 L(E6 ).


We therefore obtain (pd ) = ri,1  (pd1 )  ri,1
 ri,2  (pd1 )  ri,2
 L(E1  E6  E6 )  L(E).
L
R
 If d is not a leaf but d is, then we can apply the induction hypothesis to infer that
pdL is a path in IT ,A that starts and ends at f (dL ) and is such that (pdL )  L(E).

By using the same reasoning as in the previous case, we obtain that (pdL )  ri,1
 ri,2 

R
L(E6 ). As d is a leaf, it follows that i  F2 , and hence ri,2  L(E5 ) (here again we
choose  to satisfy the subexpressions E3 and E4 of E5 ). Putting this together, we find


that (pd ) = ri,1 (pd1 )ri,1
ri,2 ri,2
 L(E1  E6  E5 )  L(E).
L
R
 If d is a leaf but d is not, the argument is analogous to the previous case.
(end proof of claim)
Since f (d) = a for the root d of T 0 , it follows from the preceding claim that (a, a) 
L(E)IT ,A , hence K |= q.
In DL-LiteR , we can strengthen Theorem 4.2 by using role inclusions to eliminate inverse
roles in the query.
Corollary 4.3. RPQ answering is P-hard in combined complexity in DL-LiteR .
Proof. Let q be a 2RPQ and (T , A) be a DL-LiteR KB. For each inverse role r appearing
in q, we introduce a new role name rinv . We then let q 0 be the RPQ obtained by replacing
every occurrence of r in the query by rinv , and let T 0 be the extension of T with the
role inclusions r v rinv and rinv v r for each new role name rinv . It is easy to see that
cert(q, (T , A)) = cert(q 0 , (T 0 , A)).
334

fiRegular Path Queries in Lightweight Description Logics

We leave open whether RPQ answering in DL-Lite is P-hard in combined complexity.
We next provide combined complexity lower bounds for CRPQs. As CRPQs generalize CQs, we inherit an NP lower bound from the well-known NP-hardness in combined
complexity of CQ answering in relational databases (see (Abiteboul, Hull, & Vianu, 1995)):
Proposition 4.4. CRPQ answering is NP-hard in combined complexity for DL-LiteRDFS .
For DL-Lite and EL, we show that CRPQ answering is PSpace-hard for combined
complexity, in contrast to CQ answering which is NP-complete. Interestingly, PSpacehardness holds even under strong restrictions. In particular, we consider the following
restrictions on the shape of the query and the form of the regular languages in query atoms:
 (Strong) acyclicity: a C2RPQ q is acyclic if its associated undirected graph Gq =
{{t, t0 } | L(t, t0 )  q} is acyclic. It is strongly acyclic if additionally (i) it does
not contain any atoms of the form L(t, t) and (ii) for every pair of distinct atoms
L1 (t1 , t01 ), L2 (t2 , t02 ), we have {t1 , t01 } =
6 {t2 , t02 }.
 Disjunction-freeness and star-height of regular expressions: a regular expression is
disjunction-free if it does not contain . The star-height of a regular expression is
defined as the maximum nesting depth of stars appearing in the regular expression.
We point out that in the graph database setting, (strong) acyclicity leads to tractability:
acyclic C2RPQs can be evaluated in polynomial time in combined complexity (Barcelo
et al., 2012), and for strongly C2RPQs, evaluation can even be done in linear time in the
size of the database (Barcelo, 2013). By contrast, the following result shows that strong
acyclicity has no impact on the worst-case complexity of CRPQ answering in our setting:
Theorem 4.5. CRPQ answering is PSpace-hard in combined complexity for DL-Lite and
EL. This result applies even under the restriction to strongly acyclic CRPQs whose regular
languages are given by disjunction-free regular expressions of star-height two.
Proof. We give a reduction from the problem of emptiness of the intersection of an arbitrary
number of regular languages, which is known to be PSpace-hard. This result was first shown
by Kozen (1977) for regular languages given as deterministic NFAs. More recently, Bala
(2002) proved that this problem is also PSpace-hard when the regular languages are given
as disjunction-free regular expressions of star-height two. So, let E1 , . . . , En be disjunctionfree regular expressions of star-height two over the alphabet  = {1 , . . . , m }. We will
use the symbols in  as role names, and we will also use concept names A and B. For our
reduction, we consider the following Boolean CRPQ:
q = x1 , . . . , xn , y. A(x1 )  . . . A(xn )  E1 (x1 , y)  . . .  En (xn , y)
Observe that q satisfies the restrictions in the proposition statement. For the KB, we use
the ABox A = {A(a)} and a TBox T whose form depends on the logic in question. For
DL-Lite, we use
T = {A v i | i  }  {i v j | i , j  }
335

fiBienvenu, Ortiz, & Simkus

and for EL, we use instead:
T = {A v B}  {B v i .B | i  }.
Notice that in both cases the canonical model IK of K = (T , A) consists of an infinite tree
rooted at a such that every element in the interpretation has a unique i -child for each
i   (and no other children). Thus, we can associate to every domain element in IK the
word over  given by the sequence of role names encountered along the unique path from
a, and moreover, for every word w   we can find an element ew such that the sequence
of role names on the path from a to ew is exactly w.
We claim that L(E1 )  . . .  L(En ) is non-empty if and only if K |= q. To see why, first
note that if w  L(E1 )  . . .  L(En ), then we can define a match for q in the canonical model
by mapping the variables x1 , . . . , xn to a and y to ew . Conversely, if q is entailed from K,
then there is a match  of q in IK . Since AIK = {a}, we must have (xi ) = a for every
1  i  n. It follows that the unique path from a to (y) in IK is a word that belongs to
every L(Ei ), which means that L(E1 )  . . .  L(En ) is non-empty.
We note that the proof of the preceding theorem from the conference version (Bienvenu
et al., 2013) uses a simpler query in which the variables xi are replaced by a and the atoms
A(xi ) are dropped. However, that query is not strongly acyclic. We also remark that a
similar proof had already been used to establish PSpace hardness of CQs in an extension
of ELH that allows for regular role hierarchies (Krotzsch & Rudolph, 2007).

5. Upper Bounds for 2RPQs
In the next two sections, we provide concrete query answering algorithms for the considered
classes of path queries and DLs, which we leverage to derive matching upper bounds to
the complexity lower bounds from Section 4. The technical developments are presented in
stages. We begin this section by giving a simple algorithm for answering 2RPQs in DLLiteRDFS . The remainder of the section is then devoted to showing how this algorithm can
be extended to handle DL-LiteR and ELH. Afterwards, in Section 6, we introduce a query
rewriting procedure that, when combined with the algorithms from the present section,
yields a method for answering C2RPQs.
In this and the following section, we assume that all binary atoms take the form (t, t0 ),
where  is an NFA over N
R  {A? | A  NC }. This is without loss of generality, since every
regular expression can transformed into an equivalent NFA; an examination of the standard
technique for constructing NFAs from regular expressions (Thompson, 1968) reveals that
the transformation can in fact be performed by a deterministic logspace transducer.
It will also be useful to introduce notation for NFAs that result from changing the initial
and final states of some other NFA. In what follows, given an NFA  = (S, , , s0 , F ), we
will use s,G to denote the NFA (S, , , s, G), i.e., the NFA with the same states and
transitions as  but with initial state s and final states G. When there is a single final state
s0 , we write s,s0 in place of s,{s0 } .
336

fiRegular Path Queries in Lightweight Description Logics

Algorithm BasicEval
Input: NFA  = (S, , , s0 , F ) with   N
R  {A? | A  NC }, DL-LiteRDFS KB (T , A),
(a, b)  Ind(A)  Ind(A)
1. Initialize current = (a, s0 ) and count = 0. Set max = |A|  |S|.
2. While count < max and current 6 {(b, sf ) | sf  F }
(a) Let current = (c, s).
(b) Guess a pair (d, s0 )  Ind(A)  S and a transition (s, , s0 )  
 If   N
R , then verify that T , A |= (c, d), and return no if not.
 If  = A?, then verify that c = d and T , A |= A(c), and return no if not.
(c) Set current = (d, s0 ) and increment count.
3. If current = (b, sf ) for some sf  F , return yes. Else return no.
Figure 9: Non-deterministic algorithm for 2RPQ answering in DL-LiteRDFS .
5.1 Warm-up: 2RPQ Answering in DL-LiteRDFS
A standard technique for answering 2RPQs in the absence of an ontology is to nondeterministically guess a path between a pair of individuals that is labeled by a word from
the specified regular language. Such a procedure can be made to run in logarithmic space
by only keeping a small portion of the path in memory at any time.
In Figure 9, we present a simple non-deterministic algorithm BasicEval for answering
2RPQs over DL-LiteRDFS knowledge bases that implements this idea. The algorithm takes
as input an NFA  = (S, , , s0 , F ), a DL-LiteRDFS KB K = (T , A), and a pair of individuals
(a, b) from A, and it decides whether (a, b)  cert((x, y), K). In Step 1, we initialize current
with the pair (a, s0 ) and the counter count to 0. We also compute the maximum value max
of the counter, which corresponds to the largest length of path that needs to be considered.
At every iteration of the while loop (Step 2), we start with a single pair (c, s) stored in
current and then proceed to guess a new pair (d, s0 ) together with a transition of the form
(s, , s0 ). The idea is that we would like to append d to the path guessed so far, but to
do so, we must ensure that the conditions of paths are satisfied. This is the purpose of the
entailment checks in Step 2(b). If the applicable check succeeds, then we place (d, s0 ) in
current and increment count. We exit the while loop once we have reached the maximum
counter value or the pair in count takes the form (b, sf ) with sf a final state. In the latter
case, we have managed to guess a path with the required properties, and so the algorithm
returns yes.
The use of the counter ensures that the algorithm terminates, and the following proposition proves that it always outputs the correct result.
Proposition 5.1. For every 2RPQ q = (x, y), DL-LiteRDFS KB K = (T , A), and pair of
individuals (a, b) from Ind(A): (a, b)  cert(q, K) if and only if there is some execution of
BasicEval(, K, (a, b)) that returns yes.
337

fiBienvenu, Ortiz, & Simkus

Proof. Consider a 2RPQ q = (x, y) with  = (S, , , s0 , F ), a DL-LiteRDFS KB K =
(T , A), and a pair of individuals (a, b) from Ind(A).
First suppose that (a, b)  cert(q, K). Then there is a path p = e0 u1 . . . un en in IK such
that e0 = a, en = b, and (p)  L(). We may assume without loss of generality that there
is no path of shorter length than p that satisfies these conditions. As T is a DL-LiteRDFS
TBox, we know that IK = Ind(A), so every ei  Ind(A). Since (p)  L(), we can
find a sequence of states s0 s1 . . . sn from S such that sn  F and for every 1  i  n,
(si1 , ui , si )  . Because of our minimality assumption, we know that (ei , si ) 6= (ej , sj )
for i 6= j, so the sequence of pairs (e0 , s0 )(e1 , s1 ) . . . (en , sn ) has length at most |A|  |S|. It
is easily verified that by guessing this sequence of pairs, together with the corresponding
transitions (si1 , ui , si ), we obtain an execution of BasicEval that returns yes.
For the other direction, suppose that there is some execution of BasicEval(, K, (a, b))
that returns yes, and let (c0 , s0 )(a1 , s1 ) . . . (cn , sn ) be the sequence of pairs that were guessed
by this execution. Then we must have c0 = a, cn = b, and sn  F . Moreover, for every
1  i  n, there must exist a transition (si1 , ui , si )   such that T , A |= ui (ci1 , ci )
if ui  N
R , or ci1 = ci and T , A |= A(ci ) if ui = A?. It follows that the sequence
p = c0 u1 c1 . . . un cn is a path in IK with (p)  L(), and so (c0 , cn ) = (a, b)  cert(q, K).
By analyzing the complexity of the procedure BasicEval, we obtain an NL upper bound,
which matches the NL lower bound from Proposition 4.1.
Theorem 5.2. 2RPQ answering is in NL in combined complexity for DL-LiteRDFS .
Proof. By Proposition 5.1, we know that BasicEval is a decision procedure for 2RPQ answering in DL-LiteRDFS . To see why BasicEval runs in non-deterministic logarithmic space,
we note that (i) by utilizing a binary encoding, only logarithmic space is needed to store
the value of count, the old and new values of current, and the guessed transition from , and
(ii) the entailment checks in 2(b) can be performed in non-deterministic logarithmic space.
The latter follows from the fact that instance checking is in NL in combined complexity in
the superlogic DL-LiteR (Calvanese et al., 2007).
5.2 2RPQ Answering in DL-LiteR and ELH
We now turn to the problem of answering 2RPQs over DL-LiteR and ELH knowledge bases.
The following example shows that the basic evaluation algorithm we used for DL-LiteRDFS is
incomplete for these logics. Intuitively, the problem lies in the fact that the algorithm only
considers paths along ABox individuals, whereas to satisfy the query it may be necessary
to consider paths that pass through the anonymous part.
Example 5.3. In the remaining of the paper, we will consider the KB (T , A) with T =
{B v r, r v B, B v r1 , r1 v r2 } and A = {r(a, b), r2 (b, c), D(b)} from Example 2.4. As
an example 2RPQ, we consider q 0 (x, y) = (x, y), where  = hS, , , s0 , {sf }i with
S = {s0 , s1 , s2 , sf }
 = {r, r1 , r2 , r }
 = {(s0 , r, s0 ), (s0 , r1 , s1 ), (s1 , r2 , s2 ), (s2 , r , sf )}
338

fiRegular Path Queries in Lightweight Description Logics

D

a
s0 sf

r
r

e1

c

b
s0 s2

r2

r1

r

x


s1 e2

:

s0

r1

s1

r2

s2

r

sf

y
e1111

e1112
..
.

Figure 10: A match witnessing (a, a)  cert(q, K) for q(x, y) = (x, y)
The NFA  is depicted in the upper right-hand-side of Figure 10. Observe that L() =
L(r r1 r2 r ) (i.e., the same language as in the first atom of the C2RPQ q of Example 3.4).
Note that (a, a)  cert(q, (T , A)), but this is only witnessed by the path arbr1 e2 r2 br a,
which passes by the element e2 in the anonymous part of IT ,A . On this input, the algorithm
BasicEval would start from (a, s0 ). In the first iteration of the while loop, it could guess the
pair (b, s0 ) and the transition (s0 , r, s0 ), and the entailment check in the first item of Step
2(b) would succeed since T , A |= r(a, b). However, in the next iteration the only transitions
from s0 are (s0 , r, s0 ), (s0 , r1 , s1 ), but there is no d  Ind(A) such that T , A |= r(b, d) or
T , A |= r1 (b, d). Hence there is no good pair to guess in Step 2(b) and the algorithm would
fail. Since there are no other possible guesses in the first iteration that satisfy the entailment
check, the algorithm incorrectly returns no.
Our aim is to modify the evaluation algorithm to take into account detours through the
anonymous part. We observe that any path between two ABox individuals in IK can be
decomposed into a sequence of paths of two types:
 paths whose elements all belong to the ABox
 paths that begin and end with the same ABox individual and whose intermediate
points all belong to the anonymous part
Paths of the first type are already handled by the evaluation algorithm. To handle paths of
the second type, we will show how to check whether there is a path of this form that starts
and ends at a given individual a and takes the query automaton from state s to state s0 . If
such a loop exists at the individual a, then during query evaluation, we will be allowed
to jump directly from (a, s) to (a, s0 ). By modifying the evaluation algorithm to allow such
shortcuts in addition to normal transitions, we can ensure that all possible paths in the
canonical model are taken into account.
A key observation is that to decide whether a loop is available at a given ABox individual
a it is sufficient to consider the basic concepts that hold at a. This leads us to define a
table ALoop in which the entry ALoop [s, s0 ] contains the set of all basic concepts C that
force the existence of a path of the second type whose label takes the query automaton from
state s to state s0 . In order to define ALoop , we will require a second table Loop that will
contain for each pair of states (s, s0 ), a set of tail concepts that guarantee the existence of
339

fiBienvenu, Ortiz, & Simkus

a path from an anonymous element e to itself that takes the query automaton from s to s0
while never leaving the subtree of IK rooted at e (note that we do allow e to occur multiple
times along the path).
Let us now proceed to the definition of the tables ALoop and Loop . If T is a DL-LiteR
TBox, then Loop is defined inductively using the following rules:
(L1) For every s  S: Loop [s, s] = TCT .
(L2) If C  Loop [s1 , s2 ] and C  Loop [s2 , s3 ], then C  Loop [s1 , s3 ].
(L3) If C  TCT , T |= C v A, and (s1 , A?, s2 )  , then C  Loop [s1 , s2 ].
(L4) If C  TCT , T |= C v R, T |= R v R0 , T |= R v R00 , (s1 , R0 , s2 )  , R 
Loop [s2 , s3 ], and (s3 , R00 , s4 )  , then C  Loop [s1 , s4 ].
and the table ALoop is constructed from Loop using the rule:
(L5) If C  BCT , T |= C v R, T |= R v R0 , T |= R v R00 , (s1 , R0 , s2 )  , R 
Loop [s2 , s3 ], and (s3 , R00 , s4 )  , then C  ALoop [s1 , s4 ].
For ELH, we use the same definitions, except that the rules 4 and 5 are replaced by:
(L4) If C  TCT , T |= C v r.D, T |= r v r0 , T |= r v r00 , (s1 , r0 , s2 )  , D 
Loop [s2 , s3 ], and (s3 , r00 , s4 )  , then C  Loop [s1 , s4 ].
(L5) If C  BCT , T |= C v r.D, T |= r v r0 , T |= r v r00 , (s1 , r0 , s2 )  , D 
Loop [s2 , s3 ], and (s3 , r00 , s4 )  , then C  ALoop [s1 , s4 ].
Note that since TCT = BCT in ELH, the only difference between (L4) and (L5) is that
the former adds concepts to the table Loop , while the latter adds concepts to ALoop .
The following example illustrates the construction of the tables Loop and ALoop .
Example 5.4. Observe that in our running example TCT = {r, r , r1 , r1 , r2 , r2 }
and BCT = {B}  TCT . In the first step of the loop computation for the 2RPQ q 0 from
Example 5.3, we get Loop [s, s] = TCT for every s  {s0 , s1 , s2 , sf }. We can infer that
r  Loop [s0 , s2 ] using rule L4 and the following facts:
T |= r v r1

T |= r1 v r1

(s0 , r1 , s1 )  

r1  Loop [s1 , s1 ]

T |= r1 v r2
(s1 , r2 , s2 )  

Intuitively, every element e that satisfies r has an r1 -child e0 (by T |= r v r1 ),
and e0 is in turn an r2 -child of e (by r1 v r2 ). Hence, starting from such an element e, we
can always use the transition (s0 , r1 , s1 ) to go to the child e0 and then return to e using the
transition (s1 , r2 , s2 ), i.e., there is a loop from s0 to s2 at e.
In a further step, we can infer r  Loop [s0 , s3 ] by using rule L4 together with:
T |= r v r

T |= r v r

(s0 , r, s0 )  

r  Loop [s0 , s2 ]
340

T |= r v r
(s2 , r , sf )  

fiRegular Path Queries in Lightweight Description Logics

This reflects that whenever an element satisfies r , there is a loop from s0 to s3 as follows:
we can move to some r-child (which exists by T |= r v r) staying in state s0 with
(s0 , r, s0 ), then use the previously computed loop to jump to s2 at the same element, and
then go back up to e with the transition (s2 , r , sf ).
One can verify that no further loops can be inferred with the rules, so we obtain:
Loop [si , si ] = TCT for 0  i  3

Loop [s0 , s2 ] = {r }

Loop [s0 , s3 ] = {r }

Now we compute ALoop . First we note that there are applications of L5 analogous
to the two described applications of L4, which respectively result in r  ALoop [s0 , s2 ]
and r  ALoop [s0 , s3 ]. Moreover, similar applications but using T |= B v r1 instead
of T |= r v r1 and T |= B v r instead of T |= r v r yield B  ALoop [s0 , s2 ] and
B  ALoop [s0 , s3 ]. Further applications of L5 yield no new loops, hence we obtain:
ALoop [s0 , s2 ] = {r , B}

ALoop [s0 , s3 ] = {r , B}

We observe that the tables Loop and ALoop can be constructed in polynomial time
in |T | and || since entailment of inclusions is in P for both DL-LiteR and ELH (Calvanese
et al., 2007; Baader et al., 2005). The following propositions show that Loop and ALoop
have the desired meaning:
Proposition 5.5. For every DL-LiteR or ELH KB K = (T , A) and d  IK \ Ind(A), the
following are equivalent:
1. Tail(d)  Loop [s, s0 ];
2. There is a path p = e0 u1 e1 . . . un en in IK such that (p)  L(s,s0 ), e0 = en = d, and
ei  IK |d for every 0  i  n.
Proof. Consider a KB K = (T , A) and an automaton  = (S, , , s, F ). We begin by
proving that the first statement implies the second. Fix a sequence of applications of the
rules L1, L2, L3, and L4 (or L4) which generates the full table Loop , and let k be the
length of this sequence. It then suffices to show the following claim for all 1  i  k:
Claim: If C is inserted into Loop [s, s0 ] on the i-th rule application and d  IT ,A \ Ind(A)
is such that Tail(d) = C, then there is a path p = e0 u1 e1 . . . un en in IK such that (p) 
L(s,s0 ), e0 = en = d, and ei  IK |d for every 0  i  n.
Proof of claim. The proof is by induction on i. First suppose that C is inserted into
Loop [s, s0 ] with the first rule application and d  IT ,A \ Ind(A) is such that Tail(d) = C.
Then either rule L1 or rule L3 must have been applied. In the former case, we have s = s0 ,
so the path p = d satisfies the required conditions (recall that in this case (p)  ). If
instead it was rule L3 that was applied, then we must have T |= C v A and (s, A?, s0 )  
for some concept name A. Since Tail(d) = C, we must have d  AIT ,A . It follows that
p = dA?d is a path satisfying the required conditions.
For the induction step, suppose that the statement holds for all 1  i < k, and let
d  IT ,A \ Ind(A) be such that C = Tail(d) is inserted into Loop [s, s0 ] on the k-th rule
application. The first possibility is that the k-th rule application involves rules L1 or L3,
in which case we proceed as in the base case. The next possibility is that rule L2 was
341

fiBienvenu, Ortiz, & Simkus

applied. Then there must exist s00 such that after the first k  1 rule applications, we have
C  Loop [s, s00 ] and C  Loop [s00 , s0 ]. Applying the induction hypothesis, we find paths
p1 and p2 that both begin and end with d, contain only elements from IK |d , and are such
that (p1 )  L(s,s00 ) and (p2 )  L(s00 ,s0 ). Let p3 be the path obtained by taking p1 then
adding p2 with its first occurrence of d removed. Then p3 begins and ends at d, contains
only elements from IK |d , and is such that (p3 )  L(s,s0 ).
The final possibility is that the k-th rule application involves rule L4. Here the proof
differs depending on whether T is formulated in DL-LiteR or ELH. We give the proof
only for DL-LiteR ; the proof for ELH proceeds analogously. We first note that since an
application of rule L4 leads to the insertion of C into Loop [s, s0 ] at stage k, it must be the
00 000
case that we can find R, R0 , R00  N
R and s , s  S such that
 T |= C v R, T |= R v R0 , T |= R v R00 ,
 (s, R0 , s00 )   and (s000 , R00 , s0 )  , and
 R  Loop [s00 , s000 ] (after k  1 rule applications).
As Tail(d) = C and T |= C v R, the element d0 = dRR must belong to IT ,A . Then
by applying the induction hypothesis, we can infer that there is a path p0 that begins and
ends at d0 , contains only elements from IK |d0 , and is such that (p0 )  L(s00 ,s000 ). It
follows that the path p = dR0 p0 R00 d satisfies all requirements, and in particular, is such that
(p)  L(s,s0 ). (end proof of claim)
To show the other direction, we proceed by induction on the length of the path p =
e0 u1 e1 . . . un en . The first base case is when n = 0, i.e., when (p) = . Then   L(s,s0 ),
which implies that s = s0 . By rule L1 of the definition of Loop , we must have Tail(d) 
Loop [s, s0 ]. The second base case is when n = 1, i.e., p = dA?d. Since p is a path, we
must have d  AIT ,A , which means that T |= Tail(d) v A. We also know that (p) = A? 
L(s,s0 ), which implies that (s, A?, s0 )  . We have thus shown that the conditions of rule
L3 are satisfied, and so Tail(d)  Loop [s, s0 ].
For the induction step, suppose that the second direction holds for all 0  ` < k, and
suppose that there is a path p = e0 u1 e1 . . . uk ek in IK with k > 2 such that (p)  L(s,s0 ),
e0 = ek = d, and ei  IK |d for every 0  i  k. First suppose that there exists some
ej with 0 < j < k such that ej = d. Let p1 = e0 u1 e1 . . . ej and p2 = ej uj . . . ek . We
know that (p) = (p1 )(p2 )  L(s,s0 ), so there must exist some state s00 such that
(p1 )  L(s,s00 ) and (p2 )  L(s00 ,s0 ). Applying the induction hypothesis to p1 and p2 ,
we obtain Tail(d)  Loop [s00 , s0 ] and Tail(d)  Loop [s, s00 ]. Hence, by rule L2 of the
construction of Loop , we must have Tail(d)  Loop [s, s0 ].
Now let us consider the second possibility, which is that ej 6= d for all 0 < j < k. Since
IT ,A |d is tree-shaped, and p is a path, it must be the case that e1 = ek1 = dRC  IT ,A |d .
At this point, the proof slightly differs depending on whether we are in DL-LiteR or ELH.
We present the proof for the case of ELH, in which case we have R = r  NR and T |=
Tail(d) v r.C. As (p) = u1 . . . uk  L(s,s0 ), e0 = ek = d, and e1 = ek1 = dRC, it must
be the case that
 u1  N
R and T |= r v u1 , and
342

fiRegular Path Queries in Lightweight Description Logics


 uk  N
R and T |= r v t where uk = t .

We also know that there must exist states s00 , s000  S such that
 (s, u1 , s00 )   and (s000 , uk , s0 )  , and
 u2 . . . uk1  L(s00 ,s000 ).
We can apply the induction hypothesis to p0 = e1 u1 . . . uk1 ek1 to infer that Tail(e1 ) =
C  Loop [s00 , s000 ]. We thus satisfy all of the required conditions for applying rule L4 to
obtain Tail(d)  Loop [s, s0 ]. The proof for DL-LiteR is analogous.
Proposition 5.6. For every DL-LiteR or ELH KB K = (T , A), NFA  containing states
s, s0 , and a  Ind(A), the following statements are equivalent:
1. There is a concept C  ALoop [s, s0 ] such that T , A |= C(a).
2. There is a path p = e0 u1 e1 u2 . . . un en in IK such that (p)  L(s,s0 ), e0 = en = a,
n > 1, and ei 6 Ind(A) for 0 < i < n.
Proof. Fix a DL-LiteR TBox T and an NFA  = (S, , , s, F ) (the proof for ELH is similar
and left to reader). For the first direction, suppose that there is a concept C  ALoop [s, s0 ]
such that T , A |= C(a). It follows from the definition of ALoop that there exists roles
00 000
0

R, R0 , R00  N
R and states s , s  S such that T |= C v R, T |= R v R , T |= R v
00
000
000
00
0
00
0
00

R , (s, R , s )  , R  Loop [s , s ], and (s , R , s )  . Since T , A |= C(a) and
T |= C v R, it follows from the definition of IK that aRR  IK . By Proposition 5.5,
R  Loop [s00 , s000 ] implies that there is a path p = e0 u0 . . . un en in IK |aRR with e0 =
en = aRR and (p)  L(s00 ,s000 ). Then it can be verified that by taking p0 = aR0 pR00 a,
we obtain a path in IK that satisfies the conditions of the second statement. In particular,
since (s, R0 , s00 )  , (p)  L(s00 ,s000 ), and (s000 , R00 , s0 )  , we have (p0 )  L(s,s0 ).
For the second direction, suppose that there is a path p = e0 u1 e1 u2 . . . un en in IK such
that (p)  L(s,s0 ), e0 = en = a, n > 1, and ei 6 Ind(A) for 0 < i < n. Since n > 1,
e1 6 Ind(A), and p is a path, it must be the case that e1 = aRR for some R  N
R.
Moreover, since IK |a is a tree, we must have en1 = e1 . Then by the definition of paths, it
must be the case that
 u1  N
R and T |= R v u1 , and

 un  N
R and T |= R v un .

We also know that there must exist states s00 , s000  S such that:
 (s, u1 , s00 )   and (s000 , un , s0 )  
 u2 . . . un1  L(s00 ,s000 )
By applying Proposition 5.5 to p0 = e1 u1 . . . un1 en1 , we can infer that R  Loop [s00 , s000 ].
Since aRR  IK , the definition of IK ensures that there is some C  BCT such that a 
C IK and T |= C v R. The conditions of Rule 5 are thus satisfied, so C  ALoop [s, s0 ].
343

fiBienvenu, Ortiz, & Simkus

Algorithm EvalAtom
Input: NFA  = (S, , , s0 , F ) with   N
R  {A? | A  NC }, DL-LiteR or ELH KB
(T , A), (a, b)  Ind(A)  Ind(A)
1. Test whether (T , A) is satisfiable, output yes if not.
2. Initialize current = (a, s0 ) and count = 0. Set max = |A|  |S| + 1.
3. While count < max and current 6 {(b, sf ) | sf  F }
(a) Let current = (c, s).
(b) Guess a pair (d, s0 )  Ind(A)  S together with either (s, , s0 )   or B 
ALoop [s, s0 ].
(c) If (s, , s0 ) was guessed
 If   N
R , then verify that T , A |= (c, d), and return no if not.
 If  = A?, then verify that c = d and T , A |= A(c), and return no if not.
(d) If B was guessed, then verify that c = d and T , A |= B(c), and return no if not.
(e) Set current = (d, s0 ) and increment count.
4. If current = (b, sf ) for some sf  F , return yes. Else return no.
Figure 11: Non-deterministic algorithm for 2RPQ answering in DL-LiteR and ELH.
Now that we have a means to determine which loops through the anonymous part are
available from a given ABox individual, we are ready to present the extended evaluation
algorithm EvalAtom in Figure 11 that handles DL-LiteR and ELH KBs. The algorithm
EvalAtom differs from BasicEval in two respects. First, because DL-LiteR KBs may contain
contradictions, there is an initial consistency check in Step 1 to determine whether the input
KB is satisfiable (this step can be skipped for ELH KBs, which are always satisfiable). If
the KB is shown to be unsatisfiable, then the query trivially holds, so the algorithm outputs
yes. The second difference occurs in Step 3(b) within the while loop, where we now have
the choice between guessing a pair (d, s0 )  Ind(A)  S (as before) and guessing a concept
B  ALoop [s, s0 ]. The first option corresponds to taking a step in the ABox, whereas the
second corresponds to a shortcut through the anonymous part. If we choose the second
option, then we must check that the selected concept is entailed at the current individual.
The exit conditions for the while loop and the criterion for outputting yes in Step 4 remain
unchanged.
Example 5.7. Algorithm EvalAtom correctly returns yes on the input (a, a) together with
our example query and KB, in contrast to BasicEval (as was shown in Example 5.3). Indeed,
a successful execution (illustrated pictorially in Figure 12) starts from (a, s0 ) and guesses
the pair (b, s0 ) and the transition (s0 , r, s0 ) in the first iteration of the while loop. The
checks in Step 3(c) succeed as T , A |= r(a, b). In the next iteration, it can guess (b, s2 ) and,
as r  ALoop [s0 , s2 ], it can also guess the concept r . The checks in Step 3(d) succeed
because T , A |= r (b). In the third iteration, it guesses the pair (a, sf ) and the transition
344

fiRegular Path Queries in Lightweight Description Logics

r

D

a
s0 sf

r

s0 s2

r

e1

c

b



x
:

r2

r1

(s0 , s2 )-loop



s1 e2

s0

r1

s1

r2

s2

r

sf



r  ALoop [s0 , s2 ]

y
e1111

e1112
..
.

Figure 12: Establishing (a, a)  cert(q, K) using EvalAtom for q(x, y) = (x, y)
(s2 , r , sf ). As T , A |= r (b, a), the checks in Step 3(c) succeed again. Since sf  F , this
is the last iteration, and in Step 4 the algorithm returns yes.
Proposition 5.8. For every 2RPQ q = (x, y), DL-LiteR or ELH KB K = (T , A), and
pair of individuals (a, b) from Ind(A): (a, b)  cert(q, K) if and only if there is some execution
of EvalAtom(, K, (a, b)) that returns yes.
Proof. Consider a 2RPQ q = (x, y) with  = (S, , , s0 , F ), a DL-LiteR or ELH KB
K = (T , A), and a pair of individuals (a, b) from Ind(A).
For the first direction, suppose that (a, b)  cert(q, K). Then there is a path p =
e0 u1 . . . un en in IK such that e0 = a, en = b, and (p)  L(). We may assume without
loss of generality that there is no shorter path with these properties. Since (p)  L(), we
can find a sequence of states s0 s1 . . . sn from S such that sn  F and for every 1  i  n,
(si1 , ui , si )  . Let j1 < . . . < jm be all of the indices i such that ei  Ind(A), and
consider the sequence of pairs  = (ej1 , sj1 )(ej2 , sj2 ) . . . (ejm , sjm ). Observe that j1 = 0 and
jm = n, so we have ej1 = a, ejm = b, and sjm  F . Also observe that we must have
(ej` , sj` ) 6= (ejk , sjk ) whenever ` 6= k, since otherwise, we could construct a shorter path
with the same properties as p, contradicting our minimality assumption. It follows that the
sequence  contains at most |A|  |S| pairs. Thus, to prove that the sequence  leads to an
execution of EvalAtom that returns yes, it only remains to show that it is always possible to
guess a transition or concept in 3(b) such that the checks in 3(c) and 3(d) succeed. Thus,
let us suppose that current = (ej` , sj` ) and we guess the pair (ej`+1 , sj`+1 ) in 3(b). There are
three cases to consider:
 Case 1: j`+1 = j` +1 and uj`+1 = R  N
R . From earlier, we have that (sj` , R, sj`+1 )  ,
and since p is a path, we know that (ej` , ej`+1 )  RIK . The latter implies that
T , A |= R(ej` , ej`+1 ), so by choosing the transition (sj` , R, sj`+1 ) in 3(b), we can ensure
that the entailment check will succeed in 3(c).
 Case 2: j`+1 = j` + 1 and uj`+1 = A?. From earlier, we have that (sj` , A?, sj`+1 )  ,
and since p is a path, we know that ej` = ej`+1  AIK . By choosing the transition
(sj` , A?, sj`+1 ), we can ensure that the conditions of 3(c) are satisfied.
 Case 3: j`+1 > j` + 1. In this case, the path p0 = ej` uj` +1 . . . uj`+1 ej`+1 is such that
(p0 )  L(sj` ,sj`+1 ), ej` = ej`+1  Ind(A), and for every j` < i < j`+1 , we have
345

fiBienvenu, Ortiz, & Simkus

ei 6 Ind(A). We can thus apply Lemma 5.6 to find a concept C  ALoop [sj` , sj`+1 ]
such that T , A |= C(ej` ). By choosing C in 3(b), we can be sure that the entailment
check in 3(d) will succeed.
For the other direction, consider some execution of EvalAtom(, K, (a, b)) that returns
yes, and let (c0 , s0 )(a1 , s1 ) . . . (cn , sn ) be the sequence of pairs that were guessed by this
execution. Then we must have c0 = a, cn = b, and sn  F . To complete the proof, it
suffices to establish the following claim:
Claim: For every 0  i  n, there is a path pi from c0 to ci in IK such that (pi )  L(s0 ,si ).
Proof of claim. The proof is by induction on i. For the base case (i = 0), we can simply
take the path p0 = c0 since (p0 ) =  and   L(s0 ,s0 ). For the induction step, we
suppose that pk1 is a path from c0 to ck1 such that (pk1 )  L(s0 ,sk1 ), and we show
how to construct a path pk with the required properties. There are three cases to consider
depending on which transition or concept was guessed together with (ck , sk ) in Step 3(b).
 Case 1: the transition (sk1 , R, sk )   was guessed. Since the check in 3(c) succeeded,
we have T , A |= R(ck1 , ck ). Then pk = pk1 Rck is a path in IK from c0 to ck such
that (pk )  L(s0 ,sk ).
 Case 2: the transition (sk1 , A?, sk )   was guessed. Then since the check in 3(c)
succeeded, we have ck = ck1 and T , A |= A(ck1 ). Then pk = pk1 A?ck is a path in
IK from c0 to ck such that (pk )  L(s0 ,sk ).
 Case 3: the concept B  ALoop [sk1 , sk ]
succeeded, we have ck = ck1 and T , A |=
can find a path p0 = e0 u1 e1 u2 . . . un en in IK
en = ck1 . Then pk = pk1 u1 e1 u2 . . . un en
(pk1 )(p0 )  L(s0 ,sk ).

was guessed. Since the check in 3(d)
B(ck1 ). By applying Lemma 5.6, we
such that (p0 )  L(sk1 ,sk ) and e0 =
is a path from c0 to ck with (pk ) =

By analyzing the complexity of our modified evaluation procedure, we obtain upper
bounds that match the lower bounds from Section 4.
Theorem 5.9. 2RPQ answering is in
1. NL in data complexity for DL-LiteR ;
2. P in combined complexity for DL-LiteR ;
3. P in combined and data complexity for ELH.
Proof. The procedure EvalAtom involves three different types of checks: the consistency
check performed in Step 1 and the entailment and loop checks that take place in Step 3.
The cost of these checks depends on the choice of the DL and the complexity measure.
Aside from these checks, the base procedure runs in non-deterministic logarithmic space
in combined complexity, as only logarithmic space is needed to keep track of the value of
count, the old and new values of current, and the guesses of transitions and concepts.
For DL-LiteR , we know from existing results (Calvanese et al., 2007) that the consistency
and entailment checks can be performed in non-deterministic logarithmic space in combined
346

fiRegular Path Queries in Lightweight Description Logics

complexity (and hence also for data complexity). We have also seen that the table ALoop
can be constructed in polynomial time in |T | and , so the loop checks can be performed
in constant time in |A| and in polynomial time w.r.t. the whole input. It follows that
EvalAtom runs in non-deterministic logarithmic space w.r.t. |A|, yielding Statement (1).
Regarding Statement (2), we note that EvalAtom can be viewed as an NL procedure that
uses a P oracle to handle the loop checks. Since NLP = P, this yields a P upper bound on
the combined complexity of 2RPQ answering in DL-LiteR . The proof of Statement (3) is
similar. We simply note that when the input TBox is formulated in ELH, all three types
of checks can be performed in polynomial time w.r.t. the whole input (Baader et al., 2005).
Using NLP = P, we may conclude that EvalAtom provides a polynomial-time procedure for
2RPQ answering in ELH.

6. Upper Bounds for C2RPQs
The main objective of this section is to define a procedure for deciding, given a KB K =
(T , A), C2RPQ q(~x) of arity k, and k-tuple ~a of individuals from A, whether there is
a match  for q in IK such that (~x) = ~a. A nave approach might consist in guessing a
mapping  from the query variables to the individuals in the core of IT ,A and then checking
that  is a match by running the EvalAtom algorithm on ((t), (t0 )) for every query atom
(t, t0 ). Such an algorithm would properly take into account paths between individuals that
pass by the anonymous part, but because it does not consider matches that send variables
to anonymous objects, it would still be incomplete. In particular, such a procedure would
not return a or b as answers in Example 3.4. To regain completeness, one could instead
guess matches into the entire domain of IT ,A , but this would not yield a decision procedure
since the latter may be infinite. Moreover, since matches of C2RPQs can involve domain
elements that are arbitrarily far apart, it is not apparent how to identify a suitable finite
subset of the domain that is guaranteed to contain a match for the query if one exists.
To address these challenges, the procedure we propose in this section comprises two main
steps. As a first step, we rewrite the input query q into a set Q of C2RPQs such that there
is a match  for q in IT ,A with (~x) = ~a if and only if there is a match  0 for some q 0  Q
in IT ,A with  0 (~x) = ~a. The advantage of the rewritten queries is that we will only need
to consider matches  0 which map query variables to Ind(A). The second step decides the
existence of such restricted matches for the rewritten queries using the EvalAtom procedure
defined in Section 5.
For the purposes of this section, it will prove convenient to work with DL-LiteR TBoxes
that satisfy the following condition: for every role name r  sig(T ), there exists concept


names Ar , A
r such that T contains the inclusions Ar v r, r v Ar , Ar v r , r v
Ar , and these are the only inclusions in T involving the concept names Ar and Ar . Note
that if T does not satisfy this condition, then we can simply choose fresh concepts Ar , Ar
for each role name r  sig(T ) and add the corresponding inclusions to T . The resulting
TBox T 0 is a model conservative of extension of T , hence for every ABox A and every
C2RPQ q with sig(q)  (sig(T 0 ) \ sig(T )) = , we have cert(q, (T , A)) = cert(q, (T 0 , A)). We
may therefore assume without any loss of generality that all DL-LiteR TBoxes considered
in this section satisfy this syntactic condition. In what follows, when we use a concept name
AR , with R  N
R , we will assume the TBox contains the inclusions AR v R and R v AR .
347

fiBienvenu, Ortiz, & Simkus

6.1 Query Rewriting
Our aim is to rewrite our query in such a way that we do not need to map any variables
to the anonymous part of the model. We draw our inspiration from a query rewriting
procedure for Horn-SHIQ introduced by Eiter, Ortiz, Simkus, Tran, and Xiao (2012). The
main intuition is as follows. Suppose we have a match  for q which maps some variable
y to the anonymous part, and no other variable is mapped below (y). Then we modify q
in such a way that the resulting query q 0 has a match  0 that is the same as  except that
those variables mapped to (y) by  are now mapped by  0 to the (unique) parent of (y)
in IT ,A . The delicate point is that we must split atoms of the form (t, t0 ) with y  {t, t0 }
into the parts which are satisfied in the subtree IT ,A |(y) , and those which occur above
(y), whose satisfaction still needs to be determined and thus must be incorporated into
the new query. With each iteration of the rewriting procedure, we obtain a query which
has a match which maps variables closer to the core of IT ,A , until eventually we obtain
a query that has a match which maps all terms to Ind(A).
In Figure 13, we implement this intuition by defining an algorithm OneStep that performs a single (non-deterministic) rewriting step. We illustrate the functioning of OneStep
in the following examples.
Example 6.1. Recall the query q(x) = y, z. r  r1  r2  r (x, y)  r  r (y, z)  D(z) from
Example 3.4, and the KB (T , A) from Example 2.4.
To illustrate the rewriting algorithm, we first disregard the first atom and consider the
simpler Boolean query q1 = y, z. (y, z)  D(z), where  is the NFA with language r  r
depicted in Figure 14. The figure shows a match  for q1 with (y) = e11 and (z) = b.
Since y is a leaf of the image of q1 under , we want to modify q1 into a query q10 that
has a match  0 which only differs from  in having  0 (y) = e1 , where e1 is the parent of
(y) = e11 . Intuitively, this is done using OneStep by choosing Leaf = {y} as the only
variable to be moved up. Then we choose the concept B because if it holds at e1 , then
this enforces the existence of an r role from a child of e1 (namely, e11 ) to e1 . Hence, by
checking that B holds at e1 , we implicitly check that the first r needed to satisfy  holds
below e1 . This rewriting step is illustrated in the upper part of Figure 14. Formally, in
Step 1 we choose Leaf = {y}. Then in Step 2 we choose r (since, intuitively, this is
the tail concept that causes everything that holds at e11 ). In Step 3, we simply take the
final state s0f , so the atom (z, y) remains the same, and in Step 4 we do nothing. These
two steps are so simple in this example because there are no complex paths in our query
match that need to be deeper than e11 . Next, in Step 5, we choose B, since T |= r, that
is, B enforces the existence of a node that satisfies the tail concept r that we guessed
above. In Step 5b we only need to take care of the atom (y, z). We choose s01 and in
Step 6, we replace  by s01 ,s0f . In Step 7, we add the atom B(y), and we output the query
B(y)  s01 ,s0f (y, z)  D(z).
The lower part of the figure illustrates a successive application of OneStep in which we
proceed analogously, dropping the second r of , and adding again an atom B(y). This
results in the query q100 = y, z. B(y)s0f ,s0f (y, z)D(z), which has a match  00 (z) =  00 (y) = b
ranging over individuals only. Note that L(s0f ,s0f ) = {} so this query is equivalent to
y.B(y)  D(y).
348

fiRegular Path Queries in Lightweight Description Logics

Algorithm OneStep
Input: C2RPQ q with all binary atoms specified by NFAs, DL-LiteR or ELH TBox T
1. Guess a non-empty set Leaf  qvars(q) and y  Leaf.
Rename all variables in Leaf to y.
2. Guess C  TCT such that T |= C v B for every atom B(y)  q. Drop all such atoms
from q.
3. For each atom (t, t0 ) where  = (S, , , s, F ) is an NFA and y  {t, t0 },
 Guess a sequence s1 , . . . , sn1 of distinct states from S and a state sn  F .
 Replace (t, t0 ) by the atoms s,s1 (t, y), s1 ,s2 (y, y), . . . , sn2 ,sn1 (y, y),
sn1 ,sn (y, t0 ).
4. Drop all atoms s,s0 (y, y) such that C  Loop [s, s0 ].
5. Guess some D  BCT and R  N
R such that:
(a) C = R and T |= D v R
R  NR and T |= D v R.C

(if T is a DL-LiteR TBox)
(if T is an ELH TBox)

(b) For each atom (y, t) with  = (S, , , s, F ), there exists s0  S and U  N
R
with (s, U  , s0 )   and T |= R v U .
(c) For each atom (t, y) with  = (S, , , s, F ), there exists s00  S, sf  F , and
00
U  N
R with (s , U, sf )   and T |= R v U .
For atoms of the form (y, y), conditions (b) and (c) must both be satisfied.
6. Replace
 each atom (y, t) with t 6= y by s0 ,F (y, t)
 each atom (t, y) with t 6= y by s,s00 (t, y)
 each atom (y, y) by s0 ,s00 (y, y)
with s, s0 , s00 , F as in Step 5.
7. If D  NC is the concept chosen in Step 5, add D(y) to q. If D = P  was the chosen
concept, add AP  (y) to q. Output q.
Figure 13: Non-deterministic query rewriting algorithm OneStep.

Example 6.2. Now we illustrate two rewriting steps for q(x) = y, z. r  r1  r2  r (x, y) 
r  r (y, z)  D(z) (see Figure 15). First, recall the match  from Figure 7, which is also
reproduced in the top part of Figure 15. Observe that (y) = e11 is a leaf in the image of
. To move the match up one step, we make the same initial choices as in Example 6.1,
setting Leaf = {y} in Step 1 and selecting the tail concept r that characterises e11 in
349

fiBienvenu, Ortiz, & Simkus

D

a

b

D

r

e1 s01

c

s0f

a
y

e2

B



B v r



r

e11 s00
..
.

b

e12

z

D

r

s00

e1 s01

e2

B

s0 ,s0
1

e11

e12

s01

r

s0f

s01

s01 ,s0f :

z

D

y

B

f

b
e1 s01

r

s0f

B, D

D

a

c

s0f

a
y

r

e2

s0 ,s0

e12

z

b

B

1

e11

y

r

..
.
:

B

c

s0f

D

f

B

B v r



..
.

c

s0f

e1

e2

s0 ,s0
f

e11

e12

z

f

D

..
.
s01 ,s0f :

s01

r

s0f

s0f ,s0f :

s0f

Figure 14: Two successive applications of OneStep to q1 = y, z. r  r (y, z)  D(z)
Step 2. However, because the path that satisfies  goes deeper than (y) = e11 , in Step 3,
we will need to separate the path into the parts that occur below e11 and those that occur
above it. We choose the sequence of states s0 , sf as these correspond to the states of the
automata when it visits e11 , and we replace (x, y) by s0 ,s0 (x, y) and s0 ,sf (y, y). Note
that this intermediate query is illustrated at the top of Figure 15. Since r  Loop [s0 , sf ]
(see Example 5.4), we drop the second atom in Step 4. Now we can move up y similarly
as above, but considering simultaneously atoms s0 ,s0 (x, y) and (y, z). In Step 5, we again
choose the concept B, since it satisfies T |= B v r. For the atom (y, z) we choose s01 and
U = r (Step 5(b)) and for s0 ,s0 (x, y), we choose s0 and U = r (Step 5(c)). In Step 6, we
replace the two query atoms by s0 ,s0 (x, y) and s01 ,s0f (y, z), and in Step 7, we add the atom
B(y) and output the resulting query s0 ,s0 (x, y)  B(y)  s01 ,s0f (y, z), which is displayed in
the middle of Figure 15. The lower part of this figure depicts a second, similar application
of OneStep that outputs a query that has a match ranging over the individuals only.
Slightly abusing notation, we will use OneStep(q, T ) to denote the set of queries that are
output by some execution of OneStep on input (q,T ). We then consider the set Rewrite(q, T )
consisting of all queries that can be obtained from (q,T ) by zero or more applications of
OneStep. Formally, we define Rewrite(q, T ) as the smallest set that contains the initial
query q and is closed under applications of OneStep, i.e., if q 0  Rewrite(q, T ) and q 00 
OneStep(q 0 , T ), then q 00  Rewrite(q, T ).
The next proposition shows that using Rewrite(q, T ), we can reduce the problem of
finding an arbitrary query match to finding a match involving only ABox individuals.

350

fiRegular Path Queries in Lightweight Description Logics

r
D

a

s0

b

s0

r
r

r

e1 s01

s0

c

s0f

x

r

s0 sf
r

s0 s2

e11

r
s0 ,sf :


s00

:

e111

r1

s0

s1

r2

s2

r

sf

e12

r

r1

s0 ,sf

y

s00

s0

s0 ,s0

e2

r
(s0 , sf )-loop

s0 ,so :

e112

z

r

s01

r

s0f

D

r2

e1111

s1 e1112
..
.



r  Loop [s0 , s0f ], B v r,
r

x
D

s0

a
r
r

b

s0

r

e1 s01

s0

c

s0f

s0 ,so :

s0

s01 ,s0f :

s01

s0 ,s0



y

e2

r

s0f

B

B

e11

s0 ,s0
1

e12

..
.

z



f

D

B v r
r

x

B, D

s0

a

s0

r

e1

b

c

s0f
e2

s0 ,so :

s0

s0f ,s0f :

s0f

s0 ,s0
y

..
.

B

s0 ,s0
f

z

f

D

Figure 15: Two rewriting steps for q(x) = y, z. r  r1  r2  r (x, y)  r  r (y, z)  D(z)
Proposition 6.3. For every satisfiable DL-LiteR or ELH KB (T , A) and C2RPQ q:
~a  cert(q, (T , A)) if and only if there exists a query q 0 (~x)  Rewrite(q, T ) and a match
 for q 0 in IT ,A such that (~x) = ~a and (z)  Ind(A) for every variable z in q 0 .
We split Proposition 6.3 into two lemmas, the first showing completeness of Rewrite,
and the second showing its correctness.
Lemma 6.4. If ~a  cert(q, (T , A)), then there exists a query q 0 (~x)  Rewrite(q, T ) and a
match  for q 0 in IT ,A such that (~x) = ~a and (z)  Ind(A) for every variable z in q 0 .
351

fiBienvenu, Ortiz, & Simkus

Proof. Consider a knowledge base (T , A) and its canonical model IT ,A . For every element e  IT ,A , we define the distance dist(e) of e from the core of IT ,A as follows:
dist(aR1 C1 . . . Rn Cn ) = n. Observe that dist(e) = 0 implies e  Ind(A). Using this notion of
distance, we can define the cost of a match  of some query in IT ,A as vdom() distT ,A ((v)),
where dom() the domain of . We remark that a match has cost equal to zero just in the
case that it maps all query variables to ABox individuals.
Suppose that ~a  cert(q, (T , A)), and  is a match for q in IT ,A such that (~x) = ~a. If
 maps all variables in q to Ind(A), then we are done. Otherwise, there must exist some
variable y such that (y) = eSC and there is no z  vars(q) such that (y) is a proper
prefix of (z). We aim to construct a match  0 for some query q 0  OneStep(q, T ) such that
(c1)  0 (t) = (t) for every t  terms(q) such that (t) 6= (y); and
(c2)  0 (t) = e for every t  terms(q) with (t) = (y).
In other words,  0 is essentially the same as  except that it maps each t with (t) = (y)
one step closer to the ABox. Observe that (c1) and (c2) together ensure that the cost of
 0 is strictly inferior to the cost of . Thus, by repeatedly applying this operation, we will
eventually obtain a query q   Rewrite(q, T ) and a match   for q  that has cost zero, i.e.,
such that   (z)  Ind(A) for every variable z in q  .
We now show how to obtain a query q 0  OneStep(q, T ) and match  0 with properties
(c1) and (c2). In Step 1, we set Leaf = {t  terms(q) | (t) = (y) = eSC}. Note
that Leaf  qvars(q) since eSC 6 Ind(A). We define a function  as follows: (t) = t if
(t) 6= (y), else (t) = y. At the end of Step 1, we have the query:
{B((t)) | B(t)  q0 }  {((t), (t0 )) | (t, t0 )  q0 }
In Step 2, we choose the concept C (note that C  TCT because (y) = eSC). Consider
some atom B(y) that is present at the end of Step 1. The existence of atom B(y) at the
end of this step means that there must have existed an atom B(v)  q with v  Leaf. Since
 is a match for q, we know that (v) = (y)  B IT ,A . As (y) = eSC, it follows from the
definition of IT ,A that T |= C v B, as required by Step 2.
Next we show how to select states s1 , . . . , sn in Step 3. Consider an atom (t1 , t2 ) which
is present in the query at the start of Step 3, such that y  {t1 , t2 } and  = (S, , , s0 , F ).
Then we know that there is an atom (t01 , t02 ) in the input query q such that t1 = (t01 ) and
t2 = (t02 ). Since  is a match for q with (t1 ) = (t01 ) and (t2 ) = (t02 ), we know that
((t1 ), (t2 ))  L()IT ,A . It follows that we can find a path p = e0 u1 e1 . . . um em such that
e0 = (t1 ), em = (t2 ), and (p)  L(). We assume without loss of generality that m is
minimal, i.e., we cannot find a path satisfying the same properties but of shorter length.
Now let j1 < . . . < jn1 be all of the indices 0 < ` < m such that e` = (y), and consider
the following paths:
 p1 = e0 u1 . . . uj1 ej1
 pi = eji1 uji1 +1 . . . uji eji for 1 < i < n
 pn = ejn1 ujn1 +1 . . . um em
352

fiRegular Path Queries in Lightweight Description Logics

We also define a sequence of states s1 , . . . , sn such that
 (p1 )  L(s0 ,s1 ),
 (pi )  L(si1 ,si ) for 1 < i  n, and
 sn  F .
Note that such a sequence of states must exist since (p) = (p1 )(p2 ) . . . (pn )  L() and
 has start state s0 and final states F . Using the fact that eji = (y) for 1  i < n, we
have the following:
 ((t1 ), (y)) = ((t01 ), (y))  L(s0 ,s1 )IT ,A .
 ((y), (y))  L(si1 ,si )IT ,A , for 2  i < n.

(?)

 ((y), (t2 )) = ((y), (t02 ))  L(sn1 ,sn )IT ,A .
We aim to show that si 6= sj for every 1  k < ` < n. Suppose for a contradiction that
sk = s` for some 1  k < ` < n, and consider the path p0 = e0 u1 . . . ujk ejk uj` +1 ej` +1 . . . um em .
As (p0 ) = u1 . . . ujk uj` +1 . . . um , u1 . . . ujk  L(s0 ,sk ), uj` +1 . . . um  L(s` ,sn ) = L(sk ,sn ),
and sn  F , it follows that (p0 )  L(). However, this means that p0 satisfies the same conditions as p but has strictly shorter length, contradicting our minimality assumption. Hence
all states in the sequence s1 , . . . , sn1 must be distinct. We can thus choose this sequence
of states in Step 3, and replace the atom (t1 , t2 ) = ((t01 ), (t02 )) with the atoms:
s0 ,s1 ((t01 ), y), s1 ,s2 (y, y), sn2 ,sn1 (y, y), sn1 ,sn (y, (t02 )).
The final choices to be made occur in Step 5, where we must choose a concept D  BCT
and role R  N
R such that conditions (a), (b), and (c) are satisfied. We set R = S (recall
that (y) = eSC). If e 6 Ind(A), then we let D be the unique concept such that e = e0 P D.
Note that if we are in DL-LiteR , then D = P  . It follows from the definition of canonical
models that T |= D v S (if we are in DL-LiteR ) or T |= D v S.C (for ELH), so
condition (a) holds. If instead we have e  Ind, then the definition of canonical models,
together with our normal form for ELH TBoxes, guarantees that there is some D  BCT
such that e  DIT ,A and T |= D v S (if we are in DL-LiteR ) or T |= D v S.C (for
ELH). Note that in the case of DL-LiteR , T , A |= S(e) implies that one of the following
holds: (i) there is a concept assertion A(e)  A such that T |= A v S, (ii) there is a role
assertion S 0 (e, e0 )  A such that T |= S 0 v S, or (iii) there is a role assertion S 0 (e0 , e)  A
such that T |= (S 0 ) v S. Thus, it is always possible to choose D such that A |= D(e),
and we will assume in what follows that D has this property.
It remains to show that conditions (b) and (c) are verified when R = S. For (b),
consider some binary atom (y, t) which belongs to the query at the start of Step 5. Then
we know that there must exist an atom (t01 , t02 )  q with  = (S, , , s0 , F ) such that
(y, t) is equal to one of the following atoms which replaced ((t01 ), (t02 )) during Step 3:
s0 ,s1 ((t01 ), y), s1 ,s2 (y, y), . . . , sn2 ,sn1 (y, y), sn1 ,sn (y, (t02 )). Thus, we have an atom
of the form si1 ,si (y, t). Using property (?) and considering the different possible values
for t, we can infer that ((y), (t))  L(si1 ,si )IT ,A , as witnessed by the path pi =
353

fiBienvenu, Ortiz, & Simkus

eji1 uji1 +1 . . . uji eji . We also know that e` 6= (y) for every ji1 < ` < ji . In particular,
this means that either the path pi is entirely contained in the subtree rooted at (y) or
it never visits any element below (y). The former option cannot hold, since it would
imply that t = y and that C  Loop [si , si+1 ], so the atom would have been removed in
Step 4. Thus, it must be the case that the first step in the path pi goes from (y) to its
IT ,A . Since
parent e. It follows that uji1 +1 = U  for some U  N
R such that (e, (y))  U
(y) = eSC, we must have T |= S v U . Since (pi )  L(si1 ,si ), there must exist a state
s0  S such that (si1 , U  , s0 )   and uji1 +2 . . . uji  L(s0 ,si ). This shows that condition
(b) is satisfied, and also that (e, (t))  L(s0 ,s )IT ,A
We now consider condition (c). Take some atom of the form (t, y) which appears
in the query at the start of Step 5. Then we know from earlier that we can find some
atom (t01 , t02 )  q (where  = (S, , , s0 , F )) such that (t, y) is equal to one of the
following atoms that replaced ((t01 ), (t02 )) during Step 3: s0 ,s1 ((t01 ), y), s1 ,s2 (y, y),
. . . , sn2 ,sn1 (y, y), sn1 ,sn (y, (t02 )). It follows that (t, y) has the form si ,si+1 (t, y).
Using property (?), and considering the two possible values for t, we can deduce that
((t), (y))  L(si ,si+1 )IT ,A , as witnessed by the path pi = eji1 uji1 +1 . . . uji eji . Arguing
as we did for (b), we can show that the path pi is entirely contained in the subtree rooted at
(y) or the path never visits any element below (y). The former option would imply that
t = y and that C  Loop [si , si+1 ], so the atom would have been removed in Step 5. Thus,
it must be the case that the last step in the path is from e to (y), so uji = U for some
IT ,A . Since (y) = eSC, we must have T |= S v U . Since
U  N
R such that (e, (y))  U
(pi )  L(si ,si+1 ), we also know that there must exist a state s00 such that (s00 , U, si+1 )  
and uji1 +1 . . . uji 1  L(si ,s00 ). This shows that condition (c) is satisfied, and also that
((t), e)  L(si ,s00 )IT ,A . It is also important to note that if t = y, then we can apply the
arguments for conditions (b) and (c) together to show that (e, e)  L(s0 ,s00 )IT ,A (with state
s0 as in (b), and s00 as required for (c)).
Now let q 0 be the query we obtain at the end of Step 7 when all non-deterministic choices
are made in the manner that we have described. Consider the mapping  0 defined as follows:
  0 (t) = (t) for every t  terms(q) with (t) 6= (y).
  0 (t) = e for every t  terms(q) with (t) = (y).
Note that the mapping  0 satisfies the properties (c1) and (c2). It remains to show that  0
is a match for q 0 .
Consider first a concept atom B(t)  q 0 . There are two possibilities. Either B(t) appears
in q and t 6 Leaf, or B(t) was introduced in Step 7. In the former case, we know that 
satisfies B(t), and since  0 (t) = (t) (since t 6 Leaf), the same is true of  0 . In the latter
case, we must have t = y and either B = D if D  NC or B = AP  if D = P  . If B = D,
then we can use the fact that  0 (t) = e and D was chosen so that e  DIT ,A to infer that
 0 satisfies B(t). If B = AP  , then we have that e  (P  )IT ,A . Since IT ,A is a model of
I
T and P  v AP   T , we also have e  APT,A , which means that  0 satisfies B(t).
Now consider an atom (t0 , t00 )  q 0 . If y 6 {t0 , t00 }, then (t0 , t00 )  q. As  is a
match for q in IT ,A , it must be the case that ((t0 ), (t00 ))  L()IT ,A . Since  0 (t0 ) = (t0 )
and  0 (t00 ) = (t00 ), the same holds for  0 , and so the atom (t0 , t00 ) is satisfied by  0 . Next
354

fiRegular Path Queries in Lightweight Description Logics

suppose that y  {t0 , t00 }. An examination of Rewrite shows that (t0 , t00 ) must have replaced
some atom during Step 6. We distinguish three cases:
 Case 1: (t0 , t00 ) replaces si ,si+1 (y, t) with t 6= y. Then (t0 , t00 ) must have the form
s0 ,si+1 (y, t), where s0 is the state that was chosen to ensure condition (b) in Step 5.
We recall that s0 is such that (e, (t))  L(s0 ,si+1 )IT ,A . Since t 6= y, we know that
t 6 Leaf, and so (t) =  0 (t). It follows that ( 0 (y),  0 (t))  L(s0 ,si+1 )IT ,A , so the
atom (t0 , t00 ) is satisfied by  0 .
 Case 2: (t0 , t00 ) replaces si ,si+1 (t, y) with t 6= y. Then (t0 , t00 ) must have the form
si ,s00 (t, y), where s00 is the state that was used in condition 5(c). We showed earlier
when examining condition (c) that ((t), e)  L(si ,s00 )IT ,A . Using the fact that
 0 (t) = (t) and  0 (y) = e, we can infer that ( 0 (t),  0 (y))  L(si ,s00 )IT ,A , hence  0
satisfies the atom (t0 , t00 ).
 Case 3: (t0 , t00 ) replaces si ,si+1 (y, y). Then (t0 , t00 ) must have the form s0 ,s00 (y, y),
where s0 is the state from 5(b) and s00 is the state from 5(c). We have ( 0 (y),  0 (y)) =
(e, e)  L(s0 ,s00 )IT ,A , which means that  0 satisfies the atom (t0 , t00 ).

0

As we have shown that every atom in q 0 is satisfied by the mapping  0 , it follows that
is a match for q 0 in IT ,A , which completes the proof.

Lemma 6.5. If ~a  cert(q 0 , (T , A)) for some q 0  Rewrite(q, T ), then ~a  cert(q, (T , A)).
Proof. It is sufficient to show that if q 0  OneStep(q, T ) and ~a  cert(q 0 , (T , A)), then ~a 
cert(q, (T , A)). Fix a C2RPQ q and a DL-LiteR or ELH TBox T , and let q 0  OneStep(q, T )
be such that ~a  cert(q 0 , (T , A)). By Lemma 3.2, there exists a match  0 for q 0 in IT ,A such
that  0 (~x) = ~a, with ~x the answer variables of q 0 .
Consider the execution of OneStep(q, T ) that leads to the query q 0 being output. Let Leaf
be the non-empty subset of qvars(q) that was selected in Step 1, let y be the variable from
Leaf chosen in Step 1, let C  TCT be the concept selected in Step 2, and let D  BCT and
R be the concept and role selected in Step 5. In Step 7, if D  NC , then D(y) was added, and
if D = P  , then AP  (y) was added. In the former case, we know that  0 (y)  DIT ,A and
that either (i) T |= D v R and C = R , or (ii) T |= D v R.C, hence there must exist
I
an R-child e of  0 (y) in IT ,A with Tail(e) = C. In the latter case, we have  0 (y)  APT,A . By
our assumption on DL-LiteR TBoxes, T must contain the inclusion AP  v P  . Since IT ,A
is a model of T , this yields  0 (y)  (P  )IT ,A , and hence  0 (y)  DIT ,A . We then use the
fact that T |= D v R where C = R to find an R-child e of  0 (y) in IT ,A with Tail(e) = C.
We define a mapping  : terms(q)  IT ,A by setting (t) = e for every t  Leaf and
setting (t) =  0 (t) for every t  terms(q 0 ) \ {y}. This mapping is well-defined since every
term in q either belongs to Leaf or appears in q 0 . Observe that (~x) = ~a since  0 (~x) = ~a and
Leaf does not contain answer variables. We aim to show that  is a match for q in IT ,A .
To this end, consider some concept atom B(t)  q. First suppose that t  Leaf. Then we
know that the concept C selected in Step 2 is such that T |= C v B. We then use the fact
that since t  Leaf, we have (t) = e  C IT ,A . If t 6 Leaf, then B(t)  q 0 . As  0 is a match
for q 0 , we have  0 (t)  B IT ,A . Since  0 (t) = (t), we get (t)  B IT ,A .
355

fiBienvenu, Ortiz, & Simkus

Now consider some atom of the form (t, t0 )  q, where  = (S, , , s0 , F ). If both
t 6 Leaf and t0 6 Leaf, then it can be verified that (t, t0 )  q 0 . As  0 is a match for
q 0 in IT ,A , it must be the case that ( 0 (t),  0 (t0 ))  L()IT ,A . Since  0 (t) = (t) and
 0 (t) = (t), the same holds for . Let us next consider the more interesting case in which
{t, t0 }  Leaf 6= . In Step 3, we have a query containing ((t), (t0 )), where the mapping
 is defined as follows: (t00 ) = t00 for t00 6 Leaf and (t00 ) = y for t00  Leaf. Note that
since {t, t0 }  Leaf 6= , at least one of (t) and (t0 ) must be y. It follows that in Step 3,
we will guess a sequence s1 , . . . , sn1 of distinct states from S and a state sn  F , and
we will replace ((t), (t0 )) by the atoms: s0 ,s1 ((t), y), s1 ,s2 (y, y), . . . , sn2 ,sn1 (y, y),
sn1 ,sn (y, (t0 )). Let us denote this set of atoms by Q . Slightly abusing terminology,
we use the phrase match for Q to refer to a match for the Boolean query given by the
conjunctions of all atoms in Q . We now establish the following claim:
Claim 1. If  is a match for Q in IT ,A , then  is a match for (t, t0 ) in IT ,A .
Proof of claim. Suppose that  is a match for the atoms in Q in IT ,A . This means that
 (((t)), (y))  L(s0 ,s1 )IT ,A ,
 ((y), (y))  L(si ,si+1 )IT ,A for every 1  i < n  1, and
 ((y), ((t0 )))  L(sn1 ,sn )IT ,A .
We then remark that the language consisting of all words w1 . . . wn such that w1  L(s0 ,s1 ),
wi  L(si ,si+1 ) for every 1  i < n  1, and wn  L(sn1 ,sn ) is a subset of the language
L(s0 ,sn ), and hence of L(). Thus, by composing the paths witnessing each of the statements in the preceding list, we can show that (((t), ((t0 ))  L()IT ,A . Then to complete
the proof, we simply note that ((t)) = (t) and ((t0 )) = (t), because of the way we
defined  and . (end proof of claim)
Because of Claim 1, to complete the proof that  is a match for q in IT ,A , it is sufficient
to show that  is a match for Q , which is established by the following claim:
Claim 2. For every s,s0 (u, u0 )  Q : ((u), (u0 ))  L(s,s0 )IT ,A .
Proof of claim. Take some s,s0 (u, u0 )  Q . We start with the case where u = u0 = y
and C  Loop [s, s0 ]. As (y) = e and Tail(e) = C, we have Tail((y)) = C. We can
thus apply Proposition 5.5 to infer that ((y), (y))  L(s,s0 )IT ,A , which yields the desired
result given that u = u0 = y. Next suppose that either u 6= y, u0 6= y, or C 6 Loop [s, s0 ].
Then we will not remove s,s0 (u, u0 ) in Step 4, so it will still be present in Step 5. There
are three cases depending on which of u and u0 equals y. We treat each case separately:
 Case 1: u = y and u0 6= y. It follows that (u) = e and (u0 ) =  0 (u0 ). In Step 6, we
will replace s,s0 (u, u0 ) with s00 ,s0 (u, u0 ) where s00  S is such that (s, U  , s00 )   for
0
IT ,A , (u) = e, T |=
some U  N
R with T |= R v U . Using the facts that ( (y), e)  R
R v U , and (s, U  , s00 )  , we can infer that ((u),  0 (y))  L(s,s00 )IT ,A . We also
know that the atom s00 ,s0 (u, u0 ) belongs to q 0 , and so it must be satisfied by  0 , which
yields( 0 (u),  0 (u0 ))  L(s00 ,s0 )IT ,A . By combining ((u),  0 (y))  L(s,s00 )IT ,A and
( 0 (u),  0 (u0 ))  L(s00 ,s0 )IT ,A , and using the fact that ( 0 (u),  0 (u0 )) = ( 0 (y), (u0 )),
we can infer that ((u), (u0 ))  L(s,s0 )IT ,A .
356

fiRegular Path Queries in Lightweight Description Logics

 Case 2: u 6= y and u0 = y. It follows that (u) =  0 (u) and (u0 ) = e. In Step 6, we will
replace s,s0 (u, u0 ) with an atom s,s00 (u, u0 ) where s00  S is such that (s00 , U, s0 )  
for some U  N
R with T |= R v U . Using similar arguments to Case 1, we can show
that ((u),  0 (y))  L(s,s00 )IT ,A and ( 0 (y), (u0 ))  L(s00 ,s0 )IT ,A , from which we can
deduce that ((u), (u0 ))  L(s,s0 )IT ,A .
 Case 3: u = u0 = y. It follows that (u) = (u0 ) = e. In Step 6, we will replace
s,s0 (u, u0 ) with an atom s00 ,s000 (u, u0 ) where s00 , s000 are such that (s, U  , s00 )   and
0
(s00 , U 0 , s0 )   for some roles U, U 0  N
R with T |= R v U and T |= R v U . By applying similar reasoning to that used in Cases 1 and 2, we can show that ((u),  0 (y)) 
L(s,s00 )IT ,A , ( 0 (y),  0 (y))  L(s00 ,s000 )IT ,A , and ( 0 (y), (u0 ))  L(s000 ,s0 )IT ,A . From
this, we can infer that ((u), (u0 ))  L(s,s0 )IT ,A .
Together, Lemmas 6.4 and 6.5 establish Proposition 6.3. We remark that the number
of possible atoms appearing in the queries in Rewrite(q, T ) is polynomially bounded by
|q| + |T |. This is the key property used to show the following:
Proposition 6.6. There are only exponentially many queries in Rewrite(q, T ), each having
size polynomial in |q| + |T |.
Proof. Consider a DL-LiteR or ELH TBox T , a C2RPQ q, and q 0  Rewrite(q, T ). We first
note that OneStep never introduces any fresh variables, so vars(q 0 )  vars(q). We next note
that OneStep does not introduce any fresh concept names, and it only introduces binary
atoms whose NFAs are obtained from one of the original NFAs by changing the initial and
finite states. Thus, every atom in q 0 takes one of the following forms:
 A(v), where A  NC  sig(T ) and v  vars(q)
 s,s0 (v, v 0 ), where  appears in q, s, s0 are states of , and v, v 0  vars(q)
It is easy to see that the number of such atoms is bounded polynomially in |T | + |q|, and
thus, there are at most single-exponentially many distinct queries in Rewrite(q, T ).
6.2 Query Evaluation
In Figure 16, we present a non-deterministic algorithm EvalQuery for C2RPQ answering in
DL-LiteR and ELH. The algorithm starts by checking whether the input KB is satisfiable.
If the check succeeds, then the algorithm guesses a query from Rewrite(q, T ) and a variable
assignment and calls the evaluation algorithm3 EvalAtom from Section 5 to check whether
this assignment yields a match for this query in IK . The following proposition establishes
the correctness of EvalQuery.
Proposition 6.7. For every C2RPQ q, DL-LiteR or ELH KB K = (T , A), and tuple of
individuals ~a from Ind(A) of same arity as q: ~a  cert(q, K) if and only if there is some
execution of EvalQuery(q, K, ~a) that returns yes.
3. As we check KB satisfiability in the first step of EvalQuery, we may skip the satisfiability checks in the
calls to EvalAtom.

357

fiBienvenu, Ortiz, & Simkus

Algorithm EvalQuery
Input: C2RPQ q(x1 , . . . , xk ), DL-LiteR or ELH KB K = hT , Ai, tuple ~a  Ind(A)k
1. Test whether K is satisfiable, output yes if not.
2. Guess some q 0  Rewrite(q, T ) and an assignment ~b of individuals to qvars(q 0 ).
(a) Let q 00 be the query obtained by substituting ~a for (x1 , . . . , xk ) and ~b for qvars(q 0 ),
then replacing each atom of the form B(a) by the atom B? (a) where B? is an
NFA with L(B? ) = {B?} consisting on an initial state sB
0 , a single final state
B
B
B
sf , and a single transition (s0 , B?, sf ).
(b) For every atom (a, b) in q 00
check if EvalAtom(, K, (a, b)) = yes
(c) If all checks succeed, return yes.
3. Return no.
Figure 16: Non-deterministic C2RPQ answering algorithm EvalQuery.
Proof. Let ~x be the set of answer variables of q.
To show the first direction, consider an execution of EvalQuery on input (q, K, ~a) that
returns yes. If the algorithm returns yes in Step 1, then K is unsatisfiable, so we trivially
have ~a  cert(q, K). Otherwise, in Step 2, the algorithm will guess a query q 0  Rewrite(q, T )
and assignment ~b for the quantified variables ~y of q 0 . Let q 00 be the query obtained by
substituting ~a for ~x and ~b for ~y , and writing all atoms in the form (a, b) with  an NFA.
In Step 2(c), for every atom (a, b) in q 00 , we call EvalAtom on input (, K, (a, b)). Since
the algorithm returns yes in Step 3, it must be the case that all of these calls return yes,
and so by Proposition 5.8, (a, b)  cert(q, K) for every atom (a, b) in q 00 . It follows that
the mapping  sending ~x to ~a and ~y to ~b defines a match for q 0 in IK , so ~a  cert(q 0 , K).
Applying Proposition 6.3, we obtain ~a  cert(q, K).
Next suppose that ~a  cert(q, K). If K is unsatisfiable, then the algorithm will return
yes in Step 1. Otherwise, we know from Proposition 6.3 that there exists a query q 0 
Rewrite(q, T ) and a match  for q 0 in IT ,A that maps all variables to Ind(A) and is such
that (~x) = ~a. In Step 2, we choose the query q 0 and tuple (~y ), where ~y is the set of
quantified variables in q 0 . Let q 00 be the query obtained by substituting ~a for ~x and ~b for ~y ,
and writing all atoms in the form (a, b) with  an NFA. Because of the match  we know
that (a, b)  cert(q, K) for every atom (a, b) in q 00 . It follows then by Proposition 5.8 that
all of the calls to EvalAtom will return yes, and EvalQuery will return yes in Step 2(c).
By analyzing the complexity of the algorithm EvalQuery, we obtain the following upper
bounds for C2RPQ answering, which match the lower bounds given in Section 4.
Theorem 6.8. C2RPQ answering is in
1. NL in data complexity for DL-LiteR and DL-LiteRDFS ;
358

fiRegular Path Queries in Lightweight Description Logics

2. P in data complexity for ELH;
3. NP in combined complexity for DL-LiteRDFS ;
4. PSpace in combined complexity for DL-LiteR and ELH.
Proof. For Statement (1), we consider the resources required to run EvalQuery on input
(T , A, q), where T is formulated in DL-LiteR . The consistency check in Step 1 can be
performed in non-deterministic logarithmic space in |A| (Calvanese et al., 2007). If T and q
are treated as fixed, then computing Rewrite(q, T ) requires only constant time (and space)
in |A|. It follows that the guessed query q 0 and tuple ~b and the set of atoms at the end of
Step 2(a) can be stored using logarithmic space in |A|. In Step 2(b), we call EvalAtom on
each of the stored atoms. By Theorem 5.9, EvalAtom runs in non-deterministic logarithmic
space in |A|. Since NLNL = NL, we obtain the desired NL upper bound in data complexity.
To show Statement 2, consider what happens if the input TBox T is formulated in ELH.
In this case, the consistency check in Step 1 takes constant time (since every ELH KB is
satisfiable), and by Theorem 5.9, EvalAtom runs in polynomial time in |A|. We thus have a
decision procedure for C2RPQ answering in ELH that runs in non-deterministic logarithmic
space in |A| with access to a P oracle. Since NLP = P, this yields membership in P for
data complexity.
To establish Statement (3), we first note that if T is a DL-LiteRDFS TBox, then the
query cannot be rewritten, i.e., Rewrite(q, T ) = {q} (we cannot choose a D as required in
Step 5 of Algorithm OneStep because T does not entail any inclusions of the form D v R).
Thus, in Step 2 of EvalQuery, we only need to guess a tuple ~b whose size is polynomial in |A|
and |q|. The calls to EvalAtom in Step 2(b) run in polynomial time in the input (Theorem
5.2), so the overall procedure runs in NP.
For Statement (4), instead of computing the whole set Rewrite(q, T ), which can contain
exponentially many queries, we generate a single q 0  Rewrite(q, T ) non-deterministically.
By Proposition 6.6, every query in Rewrite(q, T ) can be generated after at most exponentially many steps, so we can use a polynomial-size counter to check when we have reached
this limit. Since each rewritten query is of polynomial size (Proposition 6.6), and we keep
only one query in memory at a time, the generation of a single query in Rewrite(q, T ) requires
only polynomial space. We can then proceed as for statement 3, guessing a (polynomialsize) tuple ~b and performing a polynomial number of polynomial-time evaluation checks
(Theorem 5.9). This yields a non-deterministic polynomial space procedure for deciding
~a  cert(q, (T , A)). Using the well-known fact that NPSpace = PSpace, we obtain the
desired PSpace upper bound.
6.3 Cases with Lower Complexity
Given the substantial jump in combined complexity  from NP to PSpace  when moving
from CQs to C(2)RPQs, it is natural to look for interesting subcases that offer lower complexity. We pinpoint two such subcases, the first obtained by restricting the query language,
and the second obtained by restricting the class of KBs.
359

fiBienvenu, Ortiz, & Simkus

Let us recall that 2RPQs are single-atom C2RPQs that do not contain quantified variables. The following theorem shows that this restriction is inessential, as our complexity
results for 2RPQs hold also for single-atom queries with quantified variables.4
Theorem 6.9. The results in Theorem 5.9 hold also for single-atom C2RPQs.
Proof. Fix a DL-LiteR or ELH KB K = (T , A). There are six types of single-atom queries
to consider: y. (a, y) (with a an individual), y. (x, y) (with x an answer variable),
y. (y, a) (with a an individual), y. (y, x) (with x an answer variable), x, y. (x, y)
(with x 6= y), and y. (y, y). For the first five types of queries, there are simple reductions
to 2RPQs. When q = x, y. (x, y) (with x 6= y), we can simply replace q by the 2RPQ
q 0 (x, y) = 0 (x, y), where



L(0 ) = (N
R  sig(T ))  L()  (NR  sig(T )) .

The following claim establishes the correctness of this reduction.
Claim. K |= q if and only if cert(q 0 , K) 6= .
Proof of claim. First suppose that K |= q. If K is unsatisfiable, then we trivially have
cert(q 0 , K) 6= . Otherwise, there is a match  for q in IK . This means that ((x), (y)) 
L()IK , and so there must exist some path p0 from (x) to (y) with (p0 )  L(). Let
a be such that (x) is either equal to a or begins by a, and let p1 be any path from a to
(x). Since (y) is reachable from (x), it must also be reachable from a, and so we can
find a path p2 from (y) to a. We may choose p1 and p2 so that (p1 ) and (p2 ) belong

to (N
R  sig(T )) . By combining the paths p1 , p0 and p2 (in that order), we obtain a path



from a to a whose label belongs to (N
R  sig(T ))  L()  (NR  sig(T )) . It follows that
0
(a, a)  cert(q , K).
Suppose next that (a, b)  cert(q 0 , K) for two (not necessarily distinct) individuals a, b. If
K is unsatisfiable or   L(), then we trivially have K |= q. Otherwise, there is a path p =



e0 u1 e1 u2 . . . un en in IK with e0 = a, en = b, and (p)  (N
R sig(T )) L()(NR sig(T )) .
Since  6 L(), there exists 0 < i < j  n such that ui . . . uj1  L(). By setting
(x) = ei1 and (y)  ej , we obtain a match for q in IK . (end proof of claim)
The other four types of queries containing distinct terms can be handled similarly:
 For y. (x, y), we use the 2RPQ q 00 (x, y) = 00 (x, y), where L(00 ) = L()  (N
R 
sig(T )) . Arguing as in the claim, we can show that b  cert(y. (x, y), K) iff (b, c) 
cert(q 00 (x, y), K) for some c  Ind(A).
 For y. (a, y), we have K |= y. (a, y) iff a  cert(y. (x, y), K), so we can reuse
the 2RPQ from the preceding point.

 For y. (y, x), we use the 2RPQ q 000 (x, y) = 000 (y, x) with L(000 ) = (N
R  sig(T )) 
L(). Using a similar argument to that used in the preceding claim, we can show that
b  cert(y. (y, x), K) iff (b, c)  cert(q 000 (x, y), K) for some c  Ind(A).

4. In the preliminary version of this paper, we in fact used the more general notion of single-atom queries
(possibly with quantified variables) as the definition of 2RPQs (Bienvenu et al., 2013).

360

fiRegular Path Queries in Lightweight Description Logics

 For y. (y, a), we have K |= y. (y, a) iff a  cert(y. (y, x), K), so we can reuse
the 2RPQ from the preceding point.
For queries of the form x. (x, x), the proof is more involved and passes by the definition
of an alternative rewriting procedure for 2RPQs, which is similar in spirit to the algorithm
Rewrite but guaranteed to run in polynomial time. Details are given in the appendix.
More interestingly, we can adapt the techniques from the preceding proof in order to
provide an NP upper bound for the class of C2RPQ s that do not contain any existential-join
variables, i.e., existentially quantified variables that occur more than once in a query.
Theorem 6.10. C2RPQ answering is in NP in combined complexity for DL-LiteR and
ELH knowledge bases when restricted to C2RPQs without existential-join variables.
Proof. Consider a DL-LiteR or ELH KB K = (T , A) and a C2RPQ q with answer variables
~x and existential variables ~y such that every variable in ~y occurs exactly once in q. We let
q 0 be the C2RPQ obtained from q as follows:
 Replace every atom (y1 , y2 ) such that y1 , y2 are both existential variables by the



atom 0 (y1 , y2 ) where L(0 ) = (N
R  sig(T ))  L()  (NR  sig(T )) .
 Replace every atom (t, y) such that y is an existential variable and t is an individual

or answer variable by the atom 00 (t, y) where L(00 ) = L()  (N
R  sig(T )) .
 Replace every atom (y, t) such that y is an existential variable and t is an individual

or answer variable by the atom 000 (y, t) where L(000 ) = (N
R  sig(T ))  L().
 Add ~y to the set of answer variables.
Clearly, it takes only polynomial time to construct the C2RPQ q 0 . By exploiting the fact
that every existential variable in q occurs at most once, and applying similar reasoning to
that used in the proof of Theorem 6.9, we can show that ~a  cert(q, K) iff (~a, ~b)  cert(q 0 , K)
for some tuple of individuals ~b of the same arity as ~y . To decide whether the latter holds,
we non-deterministically guess a tuple ~b and let  be the variable assignment that maps ~x
to ~a and ~y to ~b. We then use EvalAtom to verify that ((t1 ), (2 ))  cert((t1 , t2 ), K) for
every atom (t1 , t2 )  q 0 , and we return yes if this is the case. Correctness of the described
procedure follows from the correctness of EvalAtom (Proposition 5.8). Since EvalAtom can
be implemented so as to run in polynomial time for both DL-LiteR and ELH (Theorem 5.9),
we obtain an NP procedure for answering C2RPQs without existential-join variables.
The preceding result can be extended a bit further to allow for simple chains of existential variables. Indeed, we remark that if a C2RPQ q contains atoms 1 (x, y) and 2 (y, z)
with z an existential-join variable and y an existential variable appearing only in these two
atoms, then we can replace {1 (x, y), 2 (y, z)} by 3 (x, z) where L(3 ) = {w1 w2 | w1 
L(1 ), w2  L(2 )} (such an NFA 3 is easily constructed from 1 , 2 in polynomial time).
By performing this and similar polynomial-time equivalence-preserving transformations, we
can eliminate some existential-join variables and thereby enlarge the class of C2RPQs that
can be handled using an NP procedure.
361

fiBienvenu, Ortiz, & Simkus

Finding further interesting classes of computationally well-behaved queries will likely
prove difficult, given that the PSpace lower bound in Section 4 was shown to hold even
under strong structural restrictions on C2RPQs. This suggests that it may be more fruitful to consider restrictions on knowledge bases. The next proposition identifies a natural
restriction on knowledge bases that leads to an improved NP upper bound.
Theorem 6.11. C2RPQ answering is in NP in combined complexity for DL-LiteR and
ELH knowledge bases whose canonical models have finite domains.
Proof. Let K = (T , A) be a satisfiable DL-LiteR or ELH knowledge base whose canonical
model IK contains only finitely many elements (note that if K is unsatisfiable, then it is
trivial to perform query answering). It follows the construction of IK and the fact that IK
is finite that every element aR1 C1 . . . Rn Cn  IK is such that Ci 6= Cj for i 6= j, which
implies in particular that n  |TCT | (indeed, if Ci = Cj for i < j, then the domain of IT ,A
would contain the element aR1 C1 . . . Rj Cj (Ri+1 Ci+1 . . . Rj Cj )m for every m  0). We rely
on this property to devise a non-deterministic algorithm for deciding ~a  cert(q, K) that
runs in polynomial time in the combined size of the inputs.
Without loss of generality, we may suppose that the input query q contains only binary
atoms of the form (t, t0 ) with  an NFA. In a first step, we guess a mapping  from the terms
in q to sequences of the form aR1 C1 . . . Rn Cn where each Ri is a role, each Ci is a concept
from TCT , and 0  n  |TCT |. In a second step, we verify that  is indeed a match for q.
First, we check that (b) = b for every individual b in q, and that (~x) = ~a where ~x is the
tuple of answer variables of q. Next, we check whether (z)  IT ,A for every z  qvars(q).
This can be done with a polynomial number of (polynomial-time) entailment checks that
verify conditions (A) and (B) of the definition of canonical models in Section 2.1.4. It only
remains to check that all of the query atoms are satisfied under the mapping . To this end,
we construct a new ABox A as follows. Let E be the set containing each aR1 C1 . . . Rj Cj
with 1  j  n such that there is some aR1 C1 . . . Rj Cj . . . Rn Cn in the range of . We
introduce a fresh individual name be for each e  E , and let A be the ABox obtained by
adding to A the following assertions:
 R(a, be ) if R  NR and R (be , a) if R  NR for each e = aRC  E where a  NI
 R(be , be0 ) if R  NR and R (be0 , be ) if R  NR , for each e0 = eRC  E where e  E
 C(be0 ) for each e0 = eRC  E where e  E  NI and C  NC
Define the mapping  0 from terms in q to individuals in A as follows:  0 (t) = (t) if
(t)  Ind(A) and  0 (t) = b(t) otherwise. For every atom (t, t0 ) in q, we call EvalAtom on
input (, (T , A ), ( 0 (t),  0 (t0 ))); by Theorem 5.9, each of these calls needs only polynomial
time. We output yes just in the case that every call to EvalAtom returns yes.
It should be clear that the algorithm just described runs in non-deterministic polynomial
time. To show that the algorithm is sound, consider an execution of the algorithm that
returns yes, and let  be the mapping that was guessed. Since the algorithm returned
yes, we know that for every atom (t, t0 ) in q, there is a successful execution of EvalAtom
on input (, (T , A ), ( 0 (t),  0 (t0 ))). Since EvalAtom is known to be correct (Theorem 6.7),
this shows  0 is a match for q in IT ,A . It follows from the way we defined A and the
construction of canonical models that IT ,A can be homomorphically embedded into IT ,A .
Moreover, we can choose the homomorphism h such that h(be ) = e for each of the new
362

fiRegular Path Queries in Lightweight Description Logics

individuals be in A . Since matches of C2RPQs are preserved under homomorphisms, it
follows that  is a match for q in IT ,A with (~x) = ~a, so ~a  cert(q, (T , A)).
To show completeness, suppose that  is a match for q in IT ,A . By construction, there is
a homomorphism h from IT ,A to IT ,A that maps every individual a  Ind(A) to itself and
every e  E to the individual be . Using the fact that query matches are preserved under
homomorphisms, we have that  0 is a match for q in IT ,A . It follows that for every atom
(t, t0 ) in q, there is an execution of EvalAtom on (, (T , A ), ( 0 (t),  0 (t0 ))) that returns
yes, and so the algorithm returns yes after guessing the mapping .
We point out that the class of knowledge bases considered in Theorem 6.11 is of practical relevance. Indeed, several important large-scale ELH terminologies, like the medical
ontology SNOMED5 , are acyclic terminologies (see, e.g., (Haase & Lutz, 2008)), which
are guaranteed to have finite canonical models. Moreover, it has been recently argued
that real-world DL-LiteR ontologies often yield canonical models of shallow depth (Kikot,
Kontchakov, Podolskii, & Zakharyaschev, 2013).

7. Beyond C2RPQs and Lightweight DLs
In this section, we discuss some of the implications of our results and give a brief overview
of related results for similar settings.
7.1 Extensions of C2RPQs
We have argued in this paper that C2RPQs provide significantly more expressiveness than
plain CQs as languages for querying ontologies, at a moderate computational cost. However,
C2RPQs also have many limitations, and several application domains seem to call for even
more expressive query languages. We now discuss some extensions to C2RPQs.
7.1.1 Complex Labels
In a preliminary version of the present work, we added to C2RPQs the ability to talk about
combinations of concepts and roles that appear along a path (Bienvenu, Ortiz, & Simkus,
2012). In that language, which we called C2RPQs with complex labels, one can use, for
example, the expression (sbLFT  sbLFSub) to find paths between stations that are served
by both low-floor tramway and subway. Complex labels provide a more concise and flexible syntax for many queries, and in fact they increase the expressiveness of C2RPQs. In
particular in DLs that do not support role conjunction, like the ones considered here, with
standard C2RPQs we can query for pairs of stations that are connected by two different
means of transport, but we cannot require the existence of one route between them that is
fully served by both. All the algorithms described in this paper can be extended straightforwardly to C2RPQs with complex labels, and the given complexity results apply also to
that more expressive query language. However, the extension to complex labels causes a
significant overhead in the notation and the technicalities of the algorithms. Hence, for the
sake of readability, we decided not to include such an extension in this paper.
5. http://www.ihtsdo.org/snomed-ct/

363

fiBienvenu, Ortiz, & Simkus

7.1.2 RPQs with Nesting
Recent works in the database field advocate the extension of RPQs with nesting, allowing
one to require that objects along a path satisfy complex conditions, in turn expressed
through (nested) 2RPQs, in line with the XML query language XPath. RPQs with nesting
were proposed as the basic component of the navigational language nSPARQL for RDF
(Perez, Arenas, & Gutierrez, 2010) and they have received some attention in the setting of
graph databases (Reutter, 2013; Barcelo, Perez, & Reutter, 2012).
Building on the conference version of this work, we recently studied the query answering
problem for nested (C)2RPQs in the presence of DL ontologies (Bienvenu, Calvanese, Ortiz,
& Simkus, 2014). We establish tight complexity bounds in data and combined complexity
for a variety of DLs, ranging from the lightweight DLs DL-Lite and EL considered in the
present paper, up to highly expressive DLs. We show that adding nesting to (C)2RPQs does
not increase the worst-case data complexity of query answering, but it leads to ExpTimehardness in combined complexity, even for (non-conjunctive) 2RPQs and the lightweight
DLs DL-Lite or EL. This contrasts sharply with the tractability result we obtained in this
paper for the same setting but without nesting.
Other authors have considered nested navigational queries over DL KBs. Stefanoni,
Motik, Krotzsch, and Rudolph (2014) studied the complexity of answering certain types
of nested path queries over knowledge bases formulated in OWL 2 EL, which extends
ELH with a number of constructs, most notably complex role inclusions. They establish
PSpace membership for a query language that roughly corresponds to the nested (C)RPQs
mentioned earlier extended with unary complex labels, and they show P membership for
the non-conjunctive fragment. These results demonstrate that nesting is computationally
simpler when inverse roles are allowed neither in queries, nor in the ontology language.
Kostylev, Reutter, and Vrgoc (2015) have recently investigated the complexity of the socalled DLXPath family of query languages over knowledge bases expressed in lightweight
DLs, with a particular emphasis on the connection to propositional dynamic logic (PDL)
and the effects of negation. The most expressive variant of DLXPath can be used to test
Boolean conditions on nodes and edges (and thus fully captures the complex labels of
the previous subsection), but unfortunately, answering such queries is undecidable even in
the simplest of settings. Disallowing negation of binary relations restores decidability, but
query answering remains coNP-hard in data complexity if negation of unary expressions is
permitted. We finally note that Bourhis, Krotzsch, and Rudolph (2014) have recently explored several highly expressive extensions of RPQs with nesting for which query answering
remains decidable in the presence of DL ontologies.
7.1.3 Path Variables and Path Relations
In the area of graph databases, it has been argued that C2RPQs are sometimes too weak
since they can neither output the witnessing paths, nor talk about the relationships holding
between different paths. This has motivated the recent extension of C2RPQs with path
variables and relations among tuples of paths (Barcelo et al., 2012; Barcelo & Munoz,
2014). The introduction of path variables makes it possible to refer to the specific paths
(or more precisely, the labels of those paths) that are used to witness the satisfaction of
query atoms. By using a path variable in multiple atoms, one can enforce that paths with
364

fiRegular Path Queries in Lightweight Description Logics

the same label are used to connect different pairs of points. Moreover, path variables can
appear as answer variables in the query, in which case a compact representation of the
labels of witnessing paths is given as output (Barcelo et al., 2012). A further extension
allows queries to enforce that a tuple of path labels belongs to a given relation, either via
regular relations such as prefix or equal length (Barcelo et al., 2012), or using some common
non-regular relations like subword and subsequence (Barcelo & Munoz, 2014).
An in-depth study of these extensions in the presence of DL ontologies is ongoing work,
but our preliminary results suggest that the addition of ontological knowledge makes things
significantly harder. For instance, even in the presence of very simple DL-Lite ontologies,
the labels of paths witnessing a query answer may form a non-regular language, as illustrated
by the following example.
Example 7.1. Consider the DL-Lite KB K consisting of the TBox {A v R, R v R}
and ABox {A(a)}, and let q be the 2RPQ E(x, y) with E = r  (r ) . There are infinitely
many paths witnessing that (a, a) is an answer to q in IK , each obtained by taking n steps
away from a via r, then n steps back up to a via r . It follows that the set of labels of the
witnessing paths forms the non-regular language {rn  (r )n | n  0}.
The previous example crucially uses inverse roles, but non-regular (indeed, non-contextfree!) languages can also be enforced using shared path variables:
Example 7.2. Consider the EL KB K consisting of the TBox {A v r.A, A v s.A}
and ABox {A(a)}. Let q be the Boolean CRPQ x, y, z. E(x, y)  E(y, z)  E(x, z) with
E = (r  s) , and further suppose that we use path variables to require that the label of
the path from x to y is the same as the label of the path from y to z. Then every triple
of paths (pxy , pyz , pxz ) witnessing the satisfaction of q is such that (pxy ) = (pyz ) and
(pxz ) = (pxy )(pyz ). It follows that the set of labels of witnessing paths for the third
atom yields the language {ww | w  L((r  s) )}, which is neither regular nor context-free.
While these examples may seem artificial, they highlight the difficulties that arise when
combining path variables and ontologies. In particular, they demonstrate that we cannot
use NFAs as a compact representation of path labels (as was the case for graph databases),
and even if all path variables are existentially quantified, the sharing of path variables will
likely require significant modification of our query answering algorithms. Interestingly, it
seems that some of the techniques based on word equations with regular constraints that
were recently explored for handling non-regular path relations (Barcelo & Munoz, 2014) may
be relevant in the presence of ontologies already for answering queries with (existentially
quantified) path variables, and possibly also simple regular relations.
7.2 Other DLs
Our rewriting algorithm for C2RPQs is inspired by a technique first proposed for answering
CQs in the DL Horn-SHIQ (Eiter et al., 2012), and it can be extended to all the constructs
in that logic. In fact, a similar algorithm for nested C2RPQs was already developed for
the DL ELHI  which subsumes both DL-LiteR and ELH (Bienvenu et al., 2014). ELHI 
contains many of the constructors of Horn-SHIQ, and they behave similarly in terms of
computational complexity. We point out that transitive roles, which are often problematic
365

fiBienvenu, Ortiz, & Simkus

for query answering algorithms, are not a major issue in this setting. They can be easily accommodated in the ELHI  algorithm by combining the known techniques for axiomatizing
in the TBox the transitivity of roles, and the machinery for handling the transitive closure
constructor in the queries. The extension of our algorithm to ELHI  or Horn-SHIQ runs
in polynomial time in the size of the data, and it is thus worst-case optimal in data complexity. Naturally, in combined complexity, it may require exponential time in some cases,
but it always runs in single-exponential time, which again is worst-case optimal for these
DLs. Moreover, we conjecture that if implemented smartly, this exponential behavior will
rarely occur for real-world ontologies.
7.3 OWL 2 Profiles and Query Answering for the Semantic Web
The Web Ontology Language is a family of languages for specifying ontologies, endorsed as
a standard by the W3C. The current version of the standard, called OWL 2 (OWL Working
Group, 2009), features three profiles (Motik et al., 2012) or sublanguages that restrict the
expressivity in such a way that logical inference over ontologies can be achieved by efficient
algorithms. The three profiles provide different modeling capabilities, making them suitable
for different applications: the EL profile is the preferred language for life science ontologies,
the QL profile is geared towards applications that enrich relational data with ontological
information, and the RL profile is used mostly for reasoning with Web data. For more
information about the profiles, their modeling capabilities and supported inference services,
we refer the reader to the introductory text by Krotzsch and references therein (Krotzsch,
2012). The EL and DL-Lite families of lightweight description logics studied in the present
paper provide the logical underpinnings of the EL and QL profiles (the third profile, RL,
is based upon Datalog). As a consequence, our results are immediately relevant to the
problem of answering regular path queries over OWL 2 knowledge bases formulated using
the QL and EL profiles. In particular, our algorithms can be adapted for querying datasets,
such as RDF triplestores, enriched with ontological knowledge expressed in the fragments
of the profiles that correspond to DL-LiteR and ELH.

8. Conclusion and Future Work
In this paper, we have provided algorithms and tight complexity bounds for answering various forms of regular path queries over knowledge bases formulated in lightweight DLs from
the DL-Lite and EL families. Our results demonstrate that the query answering problem
for these richer query languages is often not much harder than for the CQs and IQs typically considered. Indeed, for both DL-LiteR and ELH, query answering remains tractable
in data complexity and in PSPACE in combined complexity for the highly expressive class
of C2RPQs, and for 2RPQs, we even retain tractability in combined complexity. This
computational price does not seem too high, particularly if we consider that the rich navigational features of these queries can partially compensate for the limited expressiveness
of lightweight ontology languages. We thus believe that C2RPQs constitute a promising
language for ontology-mediated query answering.
An important challenge for future work is to implement and experimentally evaluate
the developed algorithms. Although C2RPQs is the natural query language to aim at, we
believe that it makes more sense to start with a prototype implementation of the algorithm
366

fiRegular Path Queries in Lightweight Description Logics

for (2)RPQs. Indeed, not only does the algorithm for (2)RPQs have a significantly lower
worst-case complexity, but it is also considerably simpler than the rewriting-based approach
for C2RPQs, and we are confident that it can be easily translated into a practical procedure.
By contrast, the rewriting algorithm used to establish the PSpace upper bound for C2RPQs
involves a considerable amount of non-determinism, and a nave implementation can be
expected to perform poorly. We nonetheless believe that the proposed rewriting approach,
when suitably modified to avoid unnecessary non-deterministic guesses, provides a good
basis for the development of practical methods for C2RPQ answering. Finally, identifying
further restrictions on queries and ontologies that lead to lower combined complexity is
another interesting problem for future study.

Acknowledgments
The authors would like to thank the anonymous reviewers for their very careful reading
of the paper and their many helpful comments. This work was supported by the French
National Research Agency (ANR) project PAGODA 12-JS02-007-01, the Austrian Science
Fund (FWF) project T515, the FWF project P25518 and the Vienna Science and Technology Fund (WWTF) project ICT12-015.

Appendix A. Proof of Theorem 6.9
To complete the proof of Theorem 6.9, we must show how to handle queries of the form
x. (x, x). As there does not seem to be a simple reduction to 2RPQs for queries of this
form, we propose instead an approach based upon query rewriting.
To define our new query rewriting algorithm and prove its correctness, we will require
the following notion. Given two concepts C, D from BCT , we say that C causes D w.r.t.
a TBox T , denoted C T D, if for every ABox A and any model I of T and A, we have
that C I 6=  implies DI 6= . It is not difficult to see that checking C T D is feasible in
polynomial time:
Lemma A.1. The following problem is in P: given a DL-LiteR or ELH TBox T and
concepts C, D  BCT , decide whether C T D.
Proof. We start with the case where T is a DL-LiteR TBox. It is easy to see that C T D
iff T |= C v D or there exists a sequence of role names R1 , . . . , Rn such that
 T |= C v R1 ,
 T |= Rn v D, and
 for 1  i < n, T |= Ri v Ri+1 .
We then observe that the existence of a sequence R1 , . . . , Rn satisfying these conditions can
be decided in polynomial time by (i) initializing a set Reach with all roles R1 such that
T |= C v R1 , (ii) saturating Reach by adding role S to Reach whenever T |= U  v S
for some U  Reach, and (iii) checking whether there is some U  Reach such that T |=
U  v D. Since TBox reasoning is tractable for DL-LiteR , the procedure we have just
described can be performed in polynomial time.
Assume T is an ELH TBox. We have that C T D iff T |= C v D or there exists a
sequence of concepts r1 .A1 , . . . , rn .An , where {A1 , . . . , An }  NC , such that
367

fiBienvenu, Ortiz, & Simkus

 T |= C v r1 .A1 ,
 T |= An v D, and
 for 1  i < n, T |= Ai v ri+1 .Ai+1 .
The existence of a sequence r1 .A1 , . . . , rn .An satisfying the above conditions can be decided in polynomial time, using a saturation procedure analogus to the one used for DLLiteR (recall that TBox reasoning is tractable for ELH).
In Figure 17, we present a deterministic query rewriting algorithm PolyRewrite that takes
as input an NFA  and a DL-LiteR or ELH TBox T and outputs a set of queries, which will
be denoted PolyRewrite(, T ). As was the case in Section 6, the purpose of query rewriting
is to ensure that we only need to consider query matches that map answer variables to
ABox individuals. Since we are only interested in queries of the form x. (x, x), it turns
out that aside from the trivial rewriting (x, x), it is sufficient to consider rewritings of the
forms C(x) or C(x)  s1 ,s2 (x, x) in which C is a basic concept and s1 and s2 are states in
. In Step 1, we initialize Frontier with all tuples (C, s0 , sf ) such that C is a basic concept,
s0 is the initial state of , and sf is a final state. Then, at each iteration of the while loop,
we remove a tuple (C, s1 , s2 ) from Frontier and add it to Visited to record that it has already
been examined. If C  Loop [s1 , s2 ], then the corresponding query C(x)  s1 ,s2 (x, x) is
equivalent (under T ) to the simpler query C(x), and so we add to Q all queries D(x) that
ensure that x. C(x) holds. If C 6 Loop [s1 , s2 ], then we add the corresponding query
C(x)  s1 ,s2 (x, x) to Q. We next add to Frontier all those unvisited tuples (D, s5 , s6 ) such
that a match for D(x)  s5 ,s6 (x, x) that maps x to e implies the existence of a match
for C(x)  s1 ,s2 (x, x) that maps x to a child of e in the anonymous part. This operation
intuitively moves the query match one step closer to the ABox and can be viewed as the
analogue of Steps 6 and 7 in the algorithm Rewrite from Section 6.
A simple inspection of the algorithm PolyRewrite reveals that each tuple in BCT  S  S
is examined at most once, and so there can be at most |BCT |  |S|2 iterations of the while
loop in Step 2. Since we know that all of the loop, entailment, and causation checks can be
carried out in polynomial time, we obtain the following:
Lemma A.2. The algorithm PolyRewrite runs in polynomial time in || and |T |, and hence
PolyRewrite(, T ) contains a polynomial number of queries.
The next two lemmas establish the correctness of the rewriting procedure.
Lemma A.3. If T , A |= x. (x, x), then there exists a query q(x)  PolyRewrite(, T )
which has a match  in IT ,A with (x)  Ind(A).
Proof. Suppose that T , A |= x. (x, x), and let  be a match for x. (x, x) in IT ,A . If
(x)  Ind(A), then the statement trivially holds since (x, x) is added to Q in Step 1.
Thus, suppose that (x) 6 Ind(A). We start by proving the following claim, which captures
how query matches in the anonymous part of the canonical model can be moved up closer
to individuals:
Claim: Suppose that (C, s1 , s2 ) is added to Frontier at some point during the execution of
PolyRewrite on input (, T ). Further suppose that C 6 Loop [s1 , s2 ], and there is a match
 for s1 ,s2 (x, x) in IT ,A such that  (x) = dRC for some d  IT ,A . Then there is a tuple
368

fiRegular Path Queries in Lightweight Description Logics

Algorithm PolyRewrite(, T )
Input: NFA  = (S, , , s0 , F ), DL-LiteR or ELH TBox T
1. Set Q = {(x, x)}, Visited = , and Frontier = {(C, s0 , sf ) | C  BCT , sf  F }.
2. While Frontier 6= 
(a) Move a tuple (C, s1 , s2 ) from Frontier to Visiteda .
(b) If C  Loop [s1 , s2 ], then
 For every D  BCT such that D

T

C, add D(x) to Q.

(c) If C 6 Loop [s1 , s2 ], then
 Add C(x)  s1 ,s2 (x, x) to Q.
4
 For every tuple (D, R, s3 , s4 , s5 , s6 )  BCT  N
R  S such that
 C  Loop [s1 , s3 ] and C  Loop [s4 , s2 ],
 T |= D v R and C = R [DL-LiteR ] or T |= D v R.C [ELH ],
 there exist roles R0 , R00 with T |= R v R0 , T |= R v R00 , (s3 , R0 , s5 )  ,
and (s6 , R00 , s4 )  , and
 (D, s5 , s6 ) 6 (Frontier  Visited),
add (D, s5 , s6 ) to Frontier.
3. Output Q.
a. We choose the least tuple in Frontier according to some arbitrary lexicographic ordering on BCT SS.
We note however that the particular choice of a tuple does not affect the output of the procedure.

Figure 17: Query rewriting algorithm PolyRewrite.

(D, s5 , s6 ) that is added to Frontier at some point such that the query D(x)  s5 ,s6 (x, x)
has a match  0 such that  0 (x) = d, T |= D v R.C, and either d  Ind(A) or Tail(d) = D.
Proof of claim. Suppose that (C, s1 , s2 ) and  satisfy the conditions of the claim. Since
 (x) = dRC, it follows from the definition of canonical models that there is a concept
D  BCT such that d  DIT ,A and either T |= D v R and C = R (if T is formulated
in DL-LiteR ), or T |= D v R.C (for T an ELH TBox). Moreover, we may choose D
such that D = Tail(d) if d 6 Ind(A). We also know that  is a match for s1 ,s2 (x, x), so
there exists a path p = e0 u1 e1 u2 . . . un en with e0 = en =  (x) whose label (p) belongs to
L(s1 ,s2 ). Since (p)  L(s1 ,s2 ), we can find a sequence of states s00 s01 . . . s0n with s00 = s1
and s0n = s2 such that for every 1  i  n, (s0i1 , ui , s0i )  . Because C 6 Loop [s1 , s2 ], we
know that the match  is not fully contained within IT ,A |(x) . Thus, there must be at least
one occurrence of the parent d of (x) in the path p. Let ej and ek respectively be the first
and last occurrences of d in p (if there is a single occurrence of d, then j = k). Observe
that ej1 = ek+1 = (x). Set s3 = s0j1 , s4 = s0k+1 , s5 = s0j , and s6 = s0k . The paths
e0 u1 e1 . . . uj1 ej1 and ek+1 uk+2 ek+2 . . . un en witness that ((x), (x))  L(s1 ,s3 )IT ,A and
((x), (x))  L(s4 ,s2 )IT ,A . As the paths e0 . . . ej1 and ek+1 . . . en begin and end at (x)
369

fiBienvenu, Ortiz, & Simkus

and are fully contained within IT ,A |(x) , we obtain C  Loop [s1 , s3 ] and C  Loop [s4 , s2 ].
Finally, we know from the definiton of paths and the construction of the canonical model
that there must exist roles R0 , R00 with T |= R v R0 , T |= R v R00 , (s3 , R0 , s5 )  , and
(s6 , R00 , s4 )  .
By assumption, the tuple (C, s1 , s2 ) is added to Frontier at some point, and it will
eventually be selected during Step 2. Moreover, since C 6 Loop [s1 , s2 ], we will enter 2(c)
when examining (C, s1 , s2 ). From what we have shown above, we know that the tuple
(D, R, s3 , s4 , s5 , s6 ) satisfies the first three requirements of the for-loop in Step 2(c). If the
fourth requirement also holds, then this means that (D, s5 , s6 ) is added to Frontier, and
if it fails, then this is because this triple has already been added to Frontier earlier in the
execution of the algorithm. To complete the proof of the claim, we remark that the path
ej uj+1 . . . uk ek witnesses that (d, d)  L(s5 ,s6 )IT ,A . Moreover, we have seen that d  DIT ,A .
It follows that by setting  0 (x) = d, we obtain a match for the query D(x)  s5 ,s6 (x, x)
with the required properties. (end proof of claim)
Observe that if  0 as described in the claim exists, then  0 (x) is the parent of  (x) in
the canonical model IT ,A . We can now finalize the proof. Indeed, since (x) 6 Ind(A), we
have (x) = dRC for some R  N
R and C  BCT . As  is a match for (x, x), it must also
be a match for s0 ,sf (x, x) for some sf  F . In Step 1, the tuple (C, s0 , sf ) will be added to
Frontier. Repeated applications of the claim either yield a query q(x)  PolyRewrite(, T )
having a match  mapping x to the ABox, or result in the insertion of a tuple (D, s1 , s2 ) into
Frontier such that D(x)s1 ,s2 (x, x) has a match in IT ,A and D  Loop [s1 , s2 ]. In the latter
case, let  be a match for D(x)  s1 ,s2 (x, x), and let a  Ind(A) be such that  (x)  IT ,A |a .
Then it follows from the definition of canonical models that there is some E  BCT such
that E T D and a  E IT ,A . Thus, there is a match for E(x)  PolyRewrite(, T ) that
maps x to a  Ind(A).
Lemma A.4. If q(x)  PolyRewrite(, T ) and  is a match for q(x) in IT ,A with (x) 
Ind(A), then T , A |= x. (x, x).
Proof. To simplify presentation we introduce the notion of containment of Boolean queries
w.r.t. a TBox. Given a TBox T and two Boolean queries q1 , q2 , we write q1 T q2 if for
every ABox A we have that T , A |= q1 implies T , A |= q2 . With a slight abuse of notation
we allow q1 and q2 to contain atoms of the form R(x). Each occurrence of R(x) in a
query q is a shorthand for R(x, y) where y is a variable that occurs once in q.
We start by establishing the following claim:
Claim: If (C, s1 , s2 ) is added to Frontier at some point during the execution of PolyRewrite
on input (, T ), then x. C(x)  s1 ,s2 (x, x) T x. (x, x).
Proof of claim. The proof is by induction on the precedence relation obtained by setting
(C, s, s0 )  (D, s00 , s000 ) if the tuple (D, s00 , s000 ) is added to Frontier during the examination
of tuple (C, s, s0 ). For the base case, we have the tuples (C, s0 , sf ) which are inserted in
Step 1. Every such tuple has the form (C, s0 , sf ), where sf  F , and thus we trivially
have x. C(x)  s0 ,sf (x, x) T x. (x, x). Next suppose that we have already shown the
property for (C, s1 , s2 ), and let (D, s5 , s6 ) be such that (C, s1 , s2 )  (D, s5 , s6 ). Further
suppose that there is a match  for D(x)  s5 ,s6 (x, x) in IT ,A . Then we can find a path
p0 = e0 u1 e1 u2 . . . un en with e0 = en = (x) such that (p)  L(s5 ,s6 ). It follows that
370

fiRegular Path Queries in Lightweight Description Logics

there must exist a sequence of states s00 s01 . . . s0n with s00 = s5 and s0n = s6 such that for
every 1  i  n, (s0i1 , ui , s0i )  . Because (C, s1 , s2 )  (D, s5 , s6 ), there must exist
a tuple (D, R, s3 , s4 , s5 , s6 ) such that (D, s5 , s6 ) was added to Frontier when examining
(D, R, s3 , s4 , s5 , s6 ). We know that this tuple must have satisfied the four conditions, and
so we must have
 T |= D v R and C = R [DL-LiteR ] or T |= D v R.C [ELH ],
 C  Loop [s1 , s3 ] and C  Loop [s4 , s2 ], and
 there exist roles R0 , R00 with T |= R v R0 , T |= R v R00 , (s3 , R0 , s5 )  , and
(s6 , R00 , s4 )  .
By the first point, the element (x)RC belongs to the canonical model, and by the second
point, we can find paths p1 = e00 . . . u0m e0m and p2 = e000 . . . u00` e00` such that e00 = e000 = e0m =
e00` = (x)RC, (p1 )  L(s1 ,s3 ), and (p2 )  L(s4 ,s2 ). Using this and the third point, we
can show that the path p = p1 R0 p0 R00 p2 is such that (p )  L(s1 ,s2 ). Since p begins
and ends at (x)RC and (x)RC  C IT ,A , it follows that T , A |= x. C(x)  s1 ,s2 (x, x).
By the induction hypothesis, x. C(x)  s1 ,s2 (x, x) T x. (x, x), so we must also have
T , A |= x. (x, x). This establishes the desired containment x. D(x)  s5 ,s6 (x, x) T
x. (x, x). (end proof of claim)
Now suppose that  is a match for q(x)  PolyRewrite(, T ) in IT ,A such that (x) 
Ind(A). There are three possibilities. The first is that q(x) = (x, x), in which case we
trivially have T , A |= x. (x, x). The next possibility is that q(x) = x. C(x)  s,s0 (x, x),
in which case we can apply the preceding claim to show that T , A |= x. (x, x). The final
possibility is that q(x) = D(x), in which case there must have been some (C, s, s0 ) 
Frontier such that C  Loop [s, s0 ] and x. D(x) T x. C(x). In this case, we have
x. D(x) T x. C(x), x. C(x) T x. C(x)  s,s0 (x, x) (since C  Loop [s, s0 ]), and
x. C(x)  s,s0 (x, x) T x. (x, x) (by the claim). Putting these statements together, we
obtain x. D(x) T x. (x, x), which yields T , A |= x. (x, x).
To complete the argument, we observe that the queries output by PolyRewrite are either
2RPQs or take the form C(x) or C(x)s,s0 (x, x), and queries of the latter forms are trivially
transformed into 2RPQs. It follows that we can answer a query of the form x. (x, x) by
(i) computing the set PolyRewrite(, T ), and (ii) using EvalAtom to check, for each 2RPQ
0 (x, x) obtained from PolyRewrite(, T ), whether cert(0 (x, x), (T , A)) 6= . Correctness of
this procedure follows from Proposition 5.8 and Lemmas A.3 and A.4, and the complexity
bounds then follow from Lemma A.2 and Theorem 5.9.

References
Abiteboul, S., Hull, R., & Vianu, V. (1995). Foundations of Databases. Addison-Wesley.
Arora, S., & Barak, B. (2009). Computational Complexity - A Modern Approach. Cambridge
University Press.
Artale, A., Calvanese, D., Kontchakov, R., & Zakharyaschev, M. (2009). The DL-Lite family
and relations. Journal of Artificial Intelligence Research (JAIR), 36, 169.
371

fiBienvenu, Ortiz, & Simkus

Baader, F., Brandt, S., & Lutz, C. (2005). Pushing the EL envelope. In Proceedings of the
Nineteenth International Joint Conference on Artificial Intelligence (IJCAI 2005).
Bala, S. (2002). Intersection of regular languages and star hierarchy. In Proceedings of the
Twenty-Ninth International Colloquium on Automata, Languages and Programming
(ICALP 2002).
Barcelo, P. (2013). Querying graph databases. In Proceedings of the Thirty-Second Symposium on Principles of Database Systems (PODS 2013).
Barcelo, P., Libkin, L., Lin, A. W., & Wood, P. T. (2012). Expressive languages for path
queries over graph-structured data. ACM Transactions on Database Systems (TODS),
37 (4), 31.
Barcelo, P., & Munoz, P. (2014). Graph logics with rational relations: The role of word
combinatorics. In Proceedings of the Twenty-Ninth Annual ACM/IEEE Symposium
on Logic in Computer Science (LICS 2014).
Barcelo, P., Perez, J., & Reutter, J. L. (2012). Relative expressiveness of nested regular
expressions. In Proceedings of the Sixth Alberto Mendelzon International Workshop
on Foundations of Data Management (AMW 2012).
Berglund, A., Boag, S., Chamberlin, D., Fernandez, M. F., Kay, M., Robie, J., & Simeon,
J. (2007). XML Path Language (XPath) 2.0. W3C Recommendation. Available at
http://www.w3.org/TR/xpath20/.
Bienvenu, M., Calvanese, D., Ortiz, M., & Simkus, M. (2014). Nested regular path queries
in description logics. In Proceedings of the Fourteenth International Conference on
the Principles of Knowledge Representation and Reasoning (KR 2014).
Bienvenu, M., Ortiz, M., & Simkus, M. (2012). Answering expressive path queries over
lightweight DL knowledge bases. In Proceedings of the Twenty-Fifth International
Workshop on Description Logics (DL 2012).
Bienvenu, M., Ortiz, M., & Simkus, M. (2013). Conjunctive regular path queries in
lightweight description logics. In Proceedings of the Twenty-Third International Joint
Conference on Artificial Intelligence (IJCAI 2013).
Bourhis, P., Krotzsch, M., & Rudolph, S. (2014). How to best nest regular path queries.
In Proceedings of the Twenty-Seventh International Workshop on Description Logics
(DL 2014).
Brickley, D., & Guha, R. (2014). RDF Schema 1.1. W3C Recommendation. Available at
http://www.w3.org/TR/rdf-schema/.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2006). Data
complexity of query answering in description logics. In Proceedings of the Tenth
International Conference on Principles of Knowledge Representation and Reasoning
(KR 2006).
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2007). Tractable
reasoning and efficient query answering in description logics: The DL-Lite family.
Journal of Automated Reasoning, 39 (3), 385429.
372

fiRegular Path Queries in Lightweight Description Logics

Calvanese, D., De Giacomo, G., & Lenzerini, M. (1998). On the decidability of query containment under constraints. In Proceedings of the Seventeenth Symposium on Principles of Database Systems (PODS 1998).
Calvanese, D., Eiter, T., & Ortiz, M. (2007). Answering regular path queries in expressive
description logics: An automata-theoretic approach. In Proceedings of the TwentySecond AAAI Conference on Artificial Intelligence (AAAI 2007).
Calvanese, D., Eiter, T., & Ortiz, M. (2009). Regular path queries in expressive description logics with nominals. In Proceedings of the Twenty-First International Joint
Conference on Artificial Intelligence (IJCAI 2009).
Calvanese, D., Eiter, T., & Ortiz, M. (2014). Answering regular path queries in expressive
description logics via alternating tree-automata. Information and Computation, 237,
1255.
Consens, M. P., & Mendelzon, A. O. (1990). GraphLog: A visual formalism for real life
recursion. In Proceedings of the Ninth Symposium on Principles of Database Systems
(PODS 1990).
Ehrenfeucht, A., & Zeiger, P. (1974). Complexity measures for regular expressions. In
Proceedings of the Sixth Annual ACM Symposium on Theory of Computing (STOC
1974).
Eiter, T., Ortiz, M., Simkus, M., Tran, T., & Xiao, G. (2012). Query rewriting for HornSHIQ plus rules. In Proceedings of the Twenty-Sixth AAAI Conference on Artificial
Intelligence (AAAI 2012).
Florescu, D., Levy, A., & Suciu, D. (1998). Query containment for conjunctive queries with
regular expressions. In Proceedings of the Seventeenth Symposium on Principles of
Database Systems (PODS 1998).
Haase, C., & Lutz, C. (2008). Complexity of subsumption in the EL family of description logics: Acyclic and cyclic TBoxes. In Proceedings of the Eighteenth European
Conference on Artificial Intelligence (ECAI 2008).
Harris, S., & Seaborne, A. (2013). SPARQL 1.1 Query Language. W3C Recommendation.
Available at http://www.w3.org/TR/sparql11-query/.
Kikot, S., Kontchakov, R., Podolskii, V. V., & Zakharyaschev, M. (2013). Query rewriting
over shallow ontologies. In Proceedings of the Twenty-Sixth International Workshop
on Description Logics (DL 2013).
Kostylev, E. V., Reutter, J. L., & Vrgoc, D. (2015). XPath for DL ontologies. In Proceedings
of the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI 2015).
Kozen, D. (1977). Lower bounds for natural proof systems. In Proceedings of the Eighteenth
Annual Symposium on Foundations of Computer Science (SFCS 1977).
Krisnadhi, A., & Lutz, C. (2007). Data complexity in the EL family of DLs. In Proceedings
of the Twentieth International Workshop on Description Logics (DL 2007).
Krotzsch, M. (2012). OWL 2 Profiles: An introduction to lightweight ontology languages.
In Proceedings of the Eighth Reasoning Web Summer School (RW 2012).
373

fiBienvenu, Ortiz, & Simkus

Krotzsch, M., & Rudolph, S. (2007). Conjunctive queries for EL with composition of roles.
In Proceedings of the Twentieth International Workshop on Description Logics (DL
2007).
Levy, A. Y., & Rousset, M. (1996). The limits on combining recursive Horn rules with
description logics. In Proceedings of the Thirteenth National Conference on Artificial
Intelligence and Eighth Innovative Applications of Artificial Intelligence Conference
(AAAI 96).
Lutz, C. (2008). The complexity of conjunctive query answering in expressive description
logics. In Proceedings of the Fourth Joint Conference on Automated Reasoning (IJCAR
2008).
Motik, B., Cuenca Grau, B., Horrocks, I., Wu, Z., Fokoue, A., & Lutz, C. (2012). OWL 2
Web Ontology Language Profiles. W3C Recommendation. Available at http://www.
w3.org/TR/owl2-profiles/.
Ortiz, M. (2013). Ontology based query answering: The story so far. In Proceedings of the
Seventh Alberto Mendelzon International Workshop on Foundations of Data Management (AMW 2013).
Ortiz, M., Rudolph, S., & Simkus, M. (2011). Query answering in the Horn fragments of
the description logics SHOIQ and SROIQ. In Proceedings of the Twenty-Second
International Joint Conference on Artificial Intelligence (IJCAI 2011).
Ortiz, M., & Simkus, M. (2012). Reasoning and query answering in description logics. In
Proceedings of the Eighth Reasoning Web Summer School (RW 2012).
Ortiz, M., & Simkus, M. (2014). Revisiting the hardness of query answering in expressive
description logics. In Proceedings of the Eighth International Conference on Web
Reasoning and Rule Systems (RR 2014).
OWL Working Group, W. (2009). OWL 2 Web Ontology Language: Document Overview.
W3C Recommendation. Available at http://www.w3.org/TR/owl2-overview/.
Perez, J., Arenas, M., & Gutierrez, C. (2010). nSPARQL: A navigational language for RDF.
Journal of Web Semantics, 8 (4), 255270.
Reutter, J. L. (2013). Containment of nested regular expressions. CoRR, abs/1304.2637.
Rosati, R. (2007). On conjunctive query answering in EL. In Proceedings of the Twentieth
International Workshop on Description Logics (DL 2007).
Stefanoni, G., Motik, B., Krotzsch, M., & Rudolph, S. (2014). The complexity of answering
conjunctive and navigational queries over OWL 2 EL knowledge bases. Journal of
Artificial Intelligence Research (JAIR), 51, 645705.
Thompson, K. (1968). Regular expression search algorithm. Communications of the ACM,
11 (6), 419422.

374

fiJournal of Artificial Intelligence Research 53 (2015) 127-168

Submitted 1/15; published 6/15

Clause Elimination for SAT and QSAT
Marijn Heule

MARIJN @ CS . UTEXAS . EDU

Department of Computer Science,
The University of Texas at Austin, USA

Matti Jarvisalo

MATTI . JARVISALO @ CS . HELSINKI . FI

HIIT, Department of Computer Science,
University of Helsinki, Finland

Florian Lonsing

FLORIAN . LONSING @ TUWIEN . AC . AT

Institute of Information Systems,
Vienna University of Technology, Austria

Martina Seidl
Armin Biere

MARTINA . SEIDL @ JKU . AT
BIERE @ JKU . AT

Institute for Formal Models and Verification,
Johannes Kepler University Linz, Austria

Abstract
The famous archetypical NP-complete problem of Boolean satisfiability (SAT) and its PSPACEcomplete generalization of quantified Boolean satisfiability (QSAT) have become central declarative programming paradigms through which real-world instances of various computationally hard
problems can be efficiently solved. This success has been achieved through several breakthroughs
in practical implementations of decision procedures for SAT and QSAT, that is, in SAT and QSAT
solvers. Here, simplification techniques for conjunctive normal form (CNF) for SAT and for
prenex conjunctive normal form (PCNF) for QSATthe standard input formats of SAT and QSAT
solvershave recently proven very effective in increasing solver efficiency when applied before
(i.e., in preprocessing) or during (i.e., in inprocessing) satisfiability search.
In this article, we develop and analyze clause elimination procedures for pre- and inprocessing.
Clause elimination procedures form a family of (P)CNF formula simplification techniques which
remove clauses that have specific (in practice polynomial-time) redundancy properties while maintaining the satisfiability status of the formulas. Extending known procedures such as tautology,
subsumption, and blocked clause elimination, we introduce novel elimination procedures based
on asymmetric variants of these techniques, and also develop a novel family of so-called covered
clause elimination procedures, as well as natural liftings of the CNF-level procedures to PCNF. We
analyze the considered clause elimination procedures from various perspectives. Furthermore, for
the variants not preserving logical equivalence under clause elimination, we show how to reconstruct solutions to original CNFs from satisfying assignments to simplified CNFs, which is important for practical applications for the procedures. Complementing the more theoretical analysis, we
present results on an empirical evaluation on the practical importance of the clause elimination procedures in terms of the effect on solver runtimes on standard real-world application benchmarks. It
turns out that the importance of applying the clause elimination procedures developed in this work
is empirically emphasized in the context of state-of-the-art QSAT solving.

c
2015
AI Access Foundation. All rights reserved.

fiH EULE , J ARVISALO , L ONSING , S EIDL , & B IERE

1. Introduction
Boolean satisfiability (SAT) is the problem of determining whether a given propositional logic formula has a solution. SAT has become an important declarative approach to formulate and solve
various NP-hard problemsa general coverage of modern satisfiability research is provided by
Biere, Heule, van Maaren, and Walsh (2009). Contrasting the classical worst-case view on NPcompleteness as intractability (Cook, 1971; Garey & Johnson, 1979), central to the success of the
SAT-based approach are major advances in robust implementations of decision procedures for SAT,
i.e., SAT solvers. Modern SAT solvers are routinely used in a vast number of different industrial
and artificial intelligence applications (Claessen, Een, Sheeran, & Sorensson, 2008; Marques-Silva,
2008), giving rise to a high demand for new techniques for further improving the robustness and
efficiency of current state-of-the-art SAT solvers.
As SAT is the archetypical problem for NP, the quantified Boolean satisfiability (QSAT) problem
of evaluating quantified Boolean formulas (QBF), the well-known extension of SAT, is archetypical for PSPACE, offering a powerful framework for modelling a very large range of important
computational problems in artificial intelligence, knowledge representation, verification, and synthesis (Benedetti & Mangassarian, 2008). During the last decade, much effort has been spent in
the development of efficient QSAT solvers. Despite several success stories, much research effort is
needed for QSAT solving to reach the level of maturity of modern SAT solvers. Due to the wide
range of possible QSAT applications, developing more efficient QSAT solver technology is indeed
an important on-going quest. A major part of this quest is to lift techniques proven effective in SAT
solving to the more general framework of QSAT solving and to analyze their impact.
Simplification techniques applied both before (i.e., in preprocessing) and during search have
proven integral in enabling efficient conjunctive normal form (CNF) level SAT solving for realworld application domains. Indeed, there is a large body of work on preprocessing CNF formulas (Freeman, 1995; Le Berre, 2001; Lynce & Marques-Silva, 2001; Bacchus, 2002; Ostrowski,
Gregoire, Mazure, & Sas, 2002; Brafman, 2004; Subbarayan & Pradhan, 2005; Gershman & Strichman, 2005; Een & Biere, 2005; Van Gelder, 2005; Fourdrinoy, Gregoire, Mazure, & Sas, 2007a,
2007b; Jin & Somenzi, 2005; Han & Somenzi, 2007; Piette, Hamadi, & Sas, 2008; Jarvisalo, Biere,
& Heule, 2010; Manthey, Heule, & Biere, 2013; Heule, Jarvisalo, & Biere, 2013b) based on, for
examples, variable elimination and equivalence reasoning. Further, while many SAT solvers rely
mainly on Boolean constraint propagation (that is, unit propagation) during search, it is possible to
improve solving efficiency by applying additional simplification techniques also during search. This
dynamic interplay between simplification and search is captured by the inprocessing SAT solving
paradigm (Jarvisalo, Heule, & Biere, 2012b). Inprocessing SAT solvers have been recently shown
to push further the efficiency of SAT solving, as witnessed for example by L INGELING (Biere,
2013), one of the most successful SAT solvers in the recent SAT Competitions (Jarvisalo, Le Berre,
Roussel, & Simon, 2012; SAT Competitions Organizing Committee, 2014). Importantly, when
scheduling combinations of simplification techniques during search, even quite simple ideas, such
as removal of subsumed clauses, can bring additional gains by enabling further simplifications by
other techniques.
Motivated by the impact of preprocessing in SAT, some preprocessors for QSAT have started to
emerge, and have proven advantageous for the evaluation of representative QSAT benchmarks (Samulowitz, Davies, & Bacchus, 2006; Bubeck & Kleine Buning, 2007; Giunchiglia, Marin, & Narizzano, 2010; Mangassarian, Le, Goultiaeva, Veneris, & Bacchus, 2010; Pigorsch & Scholl, 2010). In
128

fiC LAUSE E LIMINATION

FOR

SAT

AND

QSAT

fact, there is high promise for achieving further advances to the efficiency of QSAT solvers through
adding stronger simplification techniques to the solving flow. Intuitively, this is due to the fact
that, in light of simplification and preprocessing techniques, real-world SAT and QSAT instances
tend to notably differ in their size characteristics. Real-world application SAT instances still solvable with state-of-the-art SAT solvers today can contain up to tens of millions of variables and
clauses (Jarvisalo et al., 2012), which restricts the use of theoretically interesting polynomial-time
simplification techniques in practical applications due to the shear size of the input CNF formulas
the solvers must be able to cope with. In contrast, QSAT instances are often relatively small, because the language of QBF enables more succinct encodings via quantification. Despite their small
size, QBFs can be very challenging for state-of-the-art solvers to solve. Hence there is more room
for successful applications of computationally intensive (but still polynomial-time) simplification
rules. Inprocessing for QSAT has hardly been considered so far; the solver S TRU Q S (Pulina &
Tacchella, 2009) combines search-based solving with variable elimination what may be considered
as a step in this direction.
The focus of this article is on preprocessing and simplification techniques for SAT and QSAT
solving. This work is motivated on one hand by the possibilities of improving SAT and QSAT
solving efficiency further by integrating additional simplification techniques to the solving process
before and/or during search, and on the other hand by understanding the relationships between
different simplification techniques. Especially, we concentrate on developing and analyzing clause
elimination procedures for CNF (for SAT) and PCNF formulas (for QSAT)the standard input
formats of SAT and QSAT solvers.
Clause elimination procedures form a specific family of simplification techniques which focus
on removing redundant clauseswith respect to specific redundancy propertiesfrom CNF formulas in a satisfiability-preserving way. More precisely, a clause elimination procedure based on
a redundancy property P is a procedure which, given a CNF (or PCNF) formula F , removes iteratively until fixpoint from F clauses which have P . For any well-defined redundancy property
P , it holds that for any clause C which has P in a (P)CNF formula F , F and F without C are
satisfiability-equivalent. In other words, F is satisfiable whenever F without C is satisfiable.
However, the most general redundancy property, simply requiring satisfiability-equivalence under clause elimination, is not applicable in practice, since checking whether a clause is C redundant
under this property is co-NP-complete (Liberatore, 2005). In connection to practically relevant
clause elimination procedures, in the context of this work of specific interest are clause elimination procedures that are based on polynomial-time checkable redundancy properties. As simple
examples in the context of SAT, two such well-known redundancy properties are tautology and
subsumption. The corresponding clause elimination procedures are tautology elimination and subsumption elimination (Een & Biere, 2005). The more sophisticated redundancy property of blocked
clauses (Kullmann, 1999) allows for blocked clause elimination (Ostrowski et al., 2002; Jarvisalo
et al., 2010).
As extensions of the known procedures, in this work we introduce novel elimination procedures
based on asymmetric variants of the techniques. In the asymmetric variants, a clause in a CNF is
first augmented with certain literals so that the satisfiability of the CNF is preserved. The original
clause is replaced by the augmented one. If the augmented clause turns out to have a redundancy
property, then it is eliminated from the CNF. Otherwise, the original clause is restored. We also
develop a novel family of so-called covered clause elimination procedures. For applications to the
129

fiH EULE , J ARVISALO , L ONSING , S EIDL , & B IERE

more general setting of QSAT, we develop natural liftings of the CNF-level procedures to PCNF,
which turn outnaturallyto be somewhat more involved.
We analyze the resulting clause elimination procedures from various perspectives. One property is reduction power, that is, the ability to remove clauses and thus reduce the size of the CNF
formula. The relative reduction power of two clause elimination procedures reveals the potential
strengths of the procedures, subject to practical realizations of the more powerful procedures being
fast enough to speed up the total solving time. Another orthogonal property we consider is BCPpreservance, that is, the ability to preserve all possible unit propagations that can also be done on
the original CNF. BCP-preservance amounts to the question of whether different clause elimination procedures maintain arc consistency on the clausal level w.r.t. the original CNF formula. A
third property we consider, confluence, implies that a procedure has a unique fixpoint; for practical realizations, knowledge of whether a simplification procedure is confluent is of interest. For
non-confluent procedures, well-working elimination-ordering heuristics have to be developed. The
fourth property we consider is whether the procedures maintain logical equivalence with respect to
the original CNF, that is, preserve the set of satisfying assignments. Maintaining logical equivalence
is most often not necessary for applications in which only a single solution is sought for. However,
for simplification techniques which maintain only satisfiability but not logical equivalence, it is important to develop algorithms for fast reconstruction of a satisfying assignment to the original CNF
from an assignment to the simplified instance. Motivated by this, for the variants which do not preserve logical equivalence, we show how to efficiently reconstruct solutions to original CNFs from
satisfying assignments to simplified CNFs.
Complementing the analysis of the properties and relationships between the considered clause
elimination procedures, we also provide empirical results on the practical implications of the clause
elimination procedures in terms of runtime improvements with state-of-the-art SAT and QSAT
solvers on real-world application benchmarks. The empirical results show that the clause elimination procedures developed in this work have a clear positive effect on the performance of various
state-of-the-art QSAT solvers, while the impact on the performance of inprocessing SAT solving is
less announced.
The rest of this article is organized as follows. After preliminaries on SAT, QSAT, and related necessary concepts (Section 2), we present an overview of the results on the properties of
clause elimination procedures (Section 3). Technical analysis of the clause elimination procedures
for SAT and QSAT are presented in Sections 46, followed by a section on solution reconstruction (Section 7). Before concluding, results from an empirical evaluation of the procedures are
presented in Section 8.
This article extends and thoroughly revises work presented earlier at the 17th International Conference on Logic for Programming, Artificial Intelligence and Reasoning (LPAR 2010) (Heule,
Jarvisalo, & Biere, 2010, 2013a) and at the 23rd International Conference on Automated Deduction (CADE 2011) (Biere, Lonsing, & Seidl, 2011). The variants of quantified covered clause
elimination (in Section 6) have not been published previously in detail. Further, model reconstruction for the variants of covered clause elimination (in Section 7) are new. Compared to the earlier
publications, the empirical evaluation presented in this article is extended and updated with more
recent state-of-the-art solvers and benchmarks. Definitions of some of the clause elimination procedures, and the related analysis, have been updated to better reflect the current insights into these
procedures. Furthermore, discussions, examples, and background have been extended.
130

fiC LAUSE E LIMINATION

FOR

SAT

AND

QSAT

2. Preliminaries
In this section we review necessary background concepts: Boolean satisfiability, resolution, Boolean
constraint propagation, as well as their counterpart in the more general context of quantified Boolean
formulas.
2.1 Boolean Satisfiability
For a Boolean variable x, there are two literals, the positive literal, denoted by x, and the negative
literal, denoted by x. A clause is a disjunction of literals and a CNF formula is a conjunction of
clauses. A clause can be seen as a finite set of literals and a CNF formula as a finite set of clauses.
The set of literals occurring in a CNF formula F is denoted by lits(F ). A unit clause contains
exactly one literal. A clause is a tautology if it contains both x and x for some variable x. Given a
CNF formula F , a clause C1  F subsumes (another) clause C2  F in F if and only if C1  C2 .
Then C2 is subsumed by C1 .
A truth assignment for a CNF formula F is a function  that maps variables in F to {t, f}. If
 (x) = v, then  (x) = v, where t = f and f = t. A clause C is satisfied by  if  (l) = t for
some l  C. An assignment satisfies F if it satisfies every clause in F . An assignment falsifies a
clause C if it assigns all literals that occur in C to f.
Two CNF formulas are logically equivalent if they have the same set of satisfying assignments
over the common variables.
2.1.1 R ESOLUTION

AND

BCP

The classical resolution proof system (Robinson, 1965) for CNF formulas consists of the resolution
rule, which states that, given two clauses C1 and C2 with l  C1 and l  C2 , the clause C =
(C1 \ {l})  (C2 \ {l}), called the resolvent of C1 and C2 , can be inferred by resolving on the literal
l. This is denoted by C = C1 l C2 . The resolution rule not only forms a complete proof system
for SAT, but it is also important as an inference rule used in preprocessing CNF formulas.
Boolean constraint propagation (BCP) or unit propagation is based on applying unit resolution,
i.e., the special case of the resolution rule in which one of the clauses C1 and C2 is a unit clause.
BCP is a central propagation mechanism applied within the typical DPLL and CDCL-based SAT
solvers. For a CNF formula F , BCP propagates all unit clauses, that is, repeats the following until
fixpoint:
If there is a unit clause (l)  F , remove from F \ {(l)} all clauses that contain the
literal l, and remove the literal l from all clauses in F .
The resulting formula is referred to as BCP(F ). It is easy to see that BCP has a unique fixpoint for
any CNF formula. In other words, BCP is confluent.
If (l)  BCP(F ) for some unit clause (l) 
/ F , we say that BCP of F assigns the literal l to t


(and the literal l to f). If (l), (l)  BCP(F ) for some literal l 
/ F (or, equivalently,   BCP(F )),
we say that BCP derives a conflict on F . For notational convenience, for a partial assignment 
over the variables in F , let BCP(F,  ) := BCP(F  T  F ), where T = {(x) |  (x) = t} and
F = {(x) |  (x) = f}. In words, BCP(F,  ) denotes the formula obtained by adding to F unit
clauses corresponding to the variable assignments in  .
131

fiH EULE , J ARVISALO , L ONSING , S EIDL , & B IERE

2.2 Quantified Boolean Formulas
A quantified Boolean formula G in prenex conjunctive normal form (PCNF) has the structure .F
with quantifier prefix  and propositional matrix F in conjunctive normal form. The quantifier
prefix  is an ordered partition Q1 . . . Qn of the variables in F . The size of the quantifier prefix
 = Q1 . . . Qn , denoted by ||, is |Q1 | + . . . + |Qn |. An element Qi of  is called a scope or
quantifier block. The function quant(Qi ) assigns either a universal quantifier  or an existential
quantifier  to scope Qi in such a way that quant(Qi ) 6= quant(Qi+1 ). For convenience we also
write Qx1 , . . . , xn for a scope S = {x1 , . . . , xn } with quant(S) = Q and Q  {, }. The
quantifier level of a variable x with x  Qi is i, i.e., one plus the number of the preceding scopes.
In the following, we assume that each variable of F occurs exactly once in the prefix. We say that a
variable x is universal (existential) in QBF .F if x  S in  and quant(S) =  (quant(S) = ).
The notions of literals, clauses, and tautologies follow those for SAT. The function var(l) returns x
if l is of the form x or x. If l = x then l = x else l = x. For a literal l with var(l)  S, quant(l) =
quant(S). For a clause C, its existential and its universal literals are given by LQ (C) = {l  C |
quant(l) = Q} with Q  {, }. For literals l, l with var(l)  Qi and var(l )  Qj , l  l if i  j.
Let G = .F be a QBF and l a literal. Then G[l] denotes the QBF which is obtained from G by
deleting each clause C with l  C, by removing each occurrence of l, and by substituting the scope
Qi with var(l)  Qi by Qi \{var(l)}.
The truth value of a QBF G = .F is recursively defined as follows.
 If F =  then G is satisfiable, if   F then G is unsatisfiable.
 If quant(Q1 ) =  and x  Q1 , then G is satisfiable iff G[x] and G[x] is satisfiable.
 If quant(Q1 ) =  and x  Q1 , then G is satisfiable iff G[x] or G[x] is satisfiable.
This definition of QBF semantics indicates that the ordering of the variables in the prefix impacts
the truth value of a formula. The prefix ordering introduces an ordering between the variables such
that a variable x has to be assigned before a variable y if x < y. This restriction is specific to
QBF and is does not apply to propositional logic where variables can be assigned in any order. The
following example illustrates the consequences of swapping quantifiers.
Example 1. The QBF G = xy.((x  y)  (x  y)) is satisfiable, whereas the QBF G =
yx.((x  y)  (x  y)) obtained from G by swapping x and y in the prefix is unsatisfiable.
Intuitively, the semantics of QBF can also be considered as a two-player game (Schaefer, 1978)
with an existential player and a universal player. The former controls the existentially quantified
variables with the goal to satisfy the formula and the latter controls the universally quantified variables with the goal to falsify the formula. The moves are performed according to the order of the
variables in the quantifier prefix from the left to the right. Obviously, the universal player takes
advantage of the CNF structure, because conflicts can be easily detected. To reduce this bias while
preserving the benefits of the CNF, approaches realizing duality-aware reasoning have been presented (Zhang, 2006; Klieber, Sapra, Gao, & Clarke, 2010; Goultiaeva & Bacchus, 2013; Goultiaeva, Seidl, & Biere, 2013; Sabharwal, Ansotegui, Gomes, Hart, & Selman, 2006).
2.2.1 QBF M ODELS
In the context of QSAT there are different definitions of models (satisfying assignments). The
choice of one particular definition is motivated by the actual application and underlying formal
132

fiC LAUSE E LIMINATION

FOR

SAT

AND

QSAT

framework. In a theoretical setting, satisfiability models of QBFs were presented as sets of Skolem
functions (Kleine Buning & Bubeck, 2009). A Skolem function fy models the values an existential
variable y can take to satisfy the matrix with respect to the universal variables on which y depends.
In general, a Skolem function fy for one particular y is not unique. Replacing y by fy produces a
semantically equivalent formula. This definition of satisfiability models is the theoretical foundation of certificate extraction for QBFs from resolution proofs (Balabanov & Jiang, 2011; Niemetz,
Preiner, Lonsing, Seidl, & Biere, 2012). In the context of the game-based view on the QBF semantics, a similar approach for extracting winning strategies was introduced by Goultiaeva, Van Gelder,
and Bacchus (2011). An approach to directly produce Skolem functions by symbolic skolemization (Benedetti, 2005a) was implemented to verify results of the solver S K IZZO (Benedetti, 2005b).
Skolem function extraction from so-called QRAT proofs was recently proposed by Heule, Seidl,
and Biere (2014b).
In certain applications related to preprocessing in QSAT, it is necessary to explicitly distinguish
variable assignments which satisfy the matrix. Recursive semantics and satisfiability models are
too coarse and hence not suitable for that purpose. Instead, tree-like models for QBFs can be
applied (Samer, 2008; Samulowitz et al., 2006). In a tree-like model, every path from the root
of the tree to a leaf comprises a variable assignment which satisfies the matrix. Assignments to
universal variables are reflected by branches in the tree. Tree-like models are the formal foundation
of preprocessing techniques in QSAT such as hyper binary resolution (Samulowitz et al., 2006) and
failed literal detection (Van Gelder, Wood, & Lonsing, 2012) and are also relevant for theoretical
work such as dependency schemes (Samer, 2008).
The different notions of models give rise to different definitions of equivalence in the theory of
QSAT. In analogy with SAT, equivalence of two QBFs G1 and G2 can be checked by comparing
the sets of tree-like models of G1 and G2 , respectively. Such explicit comparison is impossible if
recursive semantics is applied. For example, the definition of recursive semantics (Kleine Buning
& Bubeck, 2009) distinguishes only between different assignments to the free variables in the QBF,
i.e., variables which occur in the matrix but which are not explicitly quantified in the prefix. In this
paper, we only consider closed QBFs without free variables.
Two QBFs G and G are satisfiability-equivalent if and only if the following holds: G is satisfiable if and only if G is satisfiable. For simplicity, in the context of QSAT we write equivalent
instead of satisfiability-equivalent.
2.2.2 Q-R ESOLUTION
Many techniques used in SAT can be transferred to QSAT with certain adaptions to preserve soundness. In the following, we introduce some of these techniques which are important for the rest of
the paper.
For defining Q-resolution (Kleine Buning, Karpinski, & Flogel, 1995), a lifting of the resolution
rule to QSAT, we first review the concept of universal reduction (UR) (Kleine Buning et al., 1995;
Cadoli, Giovanardi, & Schaerf, 1998).
Definition 1. A universally reduced clause C  is obtained from a clause C by applying universal
reduction until fixpoint, i.e.,
C := C\{l  C | quant(l) = , and there is no l  C with quant(l ) =  and l < l }.
133

fiH EULE , J ARVISALO , L ONSING , S EIDL , & B IERE

The removal of a universally quantified literal l from a clause which does not contain any existentially quantified literals with a higher level than l is called universal reduction.
It can be easily shown that the application of universal reduction is confluent and preserves
satisfiability of a formula, provided that it is applied to non-tautological clauses only. Based on
the universal reduction rule, Q-resolution is a combination of resolution for propositional logic and
universal reduction.
Definition 2. The Q-resolvent C1 l C2 of two non-tautological clauses C1 and C2 with l  C1 ,
l  C2 , and quant(l) =  is defined as (C  \ {l})  (C  \ {l}) where C  and C  are the universally
1
2
1
2
reduced clauses obtained from C1 and C2 , respectively. The literal l is called the pivot element.
The construction rule of Q-resolvents, enhanced with the universal reduction rule, forms the
quantified resolution calculus which is sound and refutationally-complete for QSAT (Kleine Buning
et al., 1995): a QBF G is unsatisfiable if and only if the empty clause can be derived from G by
Q-resolution and universal reduction. When combining universal reduction (as in Definition 1) and
Q-resolution (as in Definition 2), the restriction of Q-resolution to non-tautological clauses is crucial
for soundness, as the following example shows.
Example 2. Consider the satisfiable QBF G = ax.C1  C2 , where C1 = (a  a  x) and
C2 = (x). Universal reduction cannot reduce literals from C1 and C2 . Furthermore, C1 and C2 do
not have a Q-resolvent since C1 is tautological. If the restriction of Q-resolution to non-tautological
clauses is ignored, then a Q-resolvent of C1 and C2 is C = C1 x C2 = (aa). Universal reduction
reduces C to the empty clause, which erroneously determines G as unsatisfiable.
2.2.3 U NIT

AND

P URE L ITERALS

FOR

QSAT

The unit literal rule for QSAT, which is part of the definition of BCP for QSAT (QBCP) given in
the following, is obtained by extending from SAT to QSAT as follows. The variable of a unit literal
is required to be existentially quantified, because universal reduction immediately reduces a clause
containing only a universal literal to the empty clause. By taking universal reduction into account,
we arrive at the following rule for unit propagations in QSAT.
Definition 3. An existentially quantified literal l is unit in QBF G = .F if {l, l1 , . . . , lm }  F
with quant(li ) =  and l < li . If l is unit in G, then G is equivalent to G[l].
Obviously, if a QBF G contains a non-tautological clause with universally quantified literals
only, then G is unsatisfiable. Note that unit literal elimination allows to ignore the quantifier ordering during the evaluation, i.e., to assign a variable that is not a member of the outermost quantifier
block.
Another important rule allowing to assign a variable not occurring in the outermost quantifier
block is quantified pure literal elimination.
S
S
Definition 4. A literal l is pure in a QBF G = Q1 . . . Qn .F if l  CF and l 6 CF . Then G is
equivalent to G[l] if quant(l) =  and equivalent to G[l] if quant(l) = .
In addition to unit propagation through which BCP for SAT is defined, the QSAT-specific variant of BCP (QBCP) includes quantified pure literal elimination and universal reduction (UR) applied until fixpoint. Both the elimination of universal pure literals and universal reduction increase
134

fiC LAUSE E LIMINATION

FOR

SAT

AND

QSAT

the deductive power of QBCP. In other words, the successive application of these rules together with
unit propagation allows to identify unit clauses which would be missed if only unit propagation was
applied.

3. Overview of Contributions
In this section, we give an overview of our main results. For this overview, we present several
important definitions of basic clause elimination techniques. Further, we introduce several measures
of comparison, such as the notion of relative reduction power, which allow a detailed analysis of the
various techniques.
3.1 Clause Elimination Procedures for SAT and QSAT
Generally speaking, a clause elimination procedure based on a redundancy property P is a procedure which, given a CNF (or PCNF) formula F , removes iteratively until fixpoint from F clauses
which have P . For any well-defined redundancy property P , it should hold that for any clause
C that has P in a (P)CNF formula F , F and F \ {C} are satisfiability-equivalent. In connection
to practically relevant clause elimination procedures, in the context of this work of specific interest are clause elimination procedures which are based on polynomial-time checkable redundancy
properties. As simple examples in the context of SAT, two such well-known redundancy properties are tautology and subsumption, which give the corresponding clause elimination procedures of
tautology elimination and subsumption elimination.
Definition 5 (Tautology Elimination). For a given formula F , tautology elimination (TE) repeats
the following until fixpoint: If there is a tautological clause C  F , let F := F \ {C}. The CNF
formula resulting from applying TE on F is denoted by TE(F ).
Definition 6 (Subsumption Elimination). For a given formula F , subsumption elimination (SE)
repeats the following until fixpoint: If there is a subsumed clause C  F , let F := F \ {C}. The
CNF formula resulting from applying SE on F is denoted by SE(F ).
A third earlier definedbut somewhat more involvedpolynomial-time checkable redundancy
property is that of a clause being blocked1 (Kullmann, 1999), which gives the corresponding technique of blocked clause elimination (BCE). Blocked clause elimination has been recently shown to
be surprisingly effective in simulating various structure-based simplification mechanisms purely on
the CNF-level (Jarvisalo, Biere, & Heule, 2012a), further motivating the technique both from the
theoretical and the practical perspectives.
Definition 7 (Blocked Clause Elimination for SAT). Given a CNF formula F , a clause C, and a
literal l  C, the literal l blocks C w.r.t. F if for each clause C   F with l  C  , C  (C  \ {l}) is
a tautology. Given a CNF formula F , a clause C is blocked w.r.t. F if there is a literal that blocks
C w.r.t. F . For a CNF formula F , blocked clause elimination (BCE) repeats the following until
fixpoint: If there is a blocked clause C  F w.r.t. F , let F := F \ {C}. The CNF formula resulting
from applying BCE on F is denoted by BCE(F ).
1. Kullmann defines a blocking literal l  C as a literal for which it holds that all resolvents C l D with l  D are
tautologies. Our definition is slightly different in that it implies that binary tautologies are blocked clauses, in contrast
to Kullmanns definition. Apart from that detail, the two definitions are equivalent.

135

fiH EULE , J ARVISALO , L ONSING , S EIDL , & B IERE

Example 3. Consider the following CNF formula, having a structure that is often observed in CNF
encodings of graph coloring problems. The formula encodes a graph with two vertices v and w and
an edge between them using three colors. The variable vi (or wi ) has the interpretation that vertex
v (or w) gets color i.
FBCE = (v1  v2  v3 )  (w1  w2  w3 )  (v1  w1 )  (v2  w2 )  (v3  w3 ) 
(v1  v2 )  (v1  v3 )  (v2  v3 )  (w1  w2 )  (w1  w3 )  (w2  w3 ).
The first two clauses encode that v and w have at least one color. The next three clauses force that v
and w cannot have the same color. The last six clauses denote that v and w have at most one color.
It is easy to check that these last six clauses are blocked in FBCE , since for each of these clauses,
both of the two literals block the clause. Thus BCE will remove these last six binary clauses from
FBCE . Thus the formula BCE(FBCE ) does not include the at-most-one-color constraints over the
nodes v and w, and hence, in contrast to FBCE , BCE(FBCE ) has satisfying assignments in which
v or w are assigned multiple colors. However, given such a satisfying assignment, there is a simple
linear-time algorithm (see Section 7 for details) for reconstructing a satisfying assignment to FBCE ,
i.e., an assignment in which v and w are both assigned only a single color.
In this work, we focus on a total of eight different clause elimination procedures for CNF formulas as well as for PCNF formulas, based on clause elimination techniques that remove tautological,
subsumed, blocked, and covered clauses. For each of these elimination techniques, we consider
the plain as well as what we call the asymmetric variant. For (plain) tautology elimination (TE),
we introduce asymmetric tautology elimination (ATE). For (plain) subsumption elimination (SE),
we have the asymmetric variant ASE, and for (plain) blocked clause elimination (BCE), the asymmetric variant ABCE, respectively. Additionally, we develop a novel family (including the plain
(CCE) and asymmetric (ACCE) variants) of so-called covered clause elimination procedures. For
the context of QSAT, we propose natural liftings of these CNF-level clause elimination procedures.
While for the redundancy properties tautology and subsumed the corresponding CNF-level clause
elimination procedures are directly applicable by ignoring the quantifier prefix, the PCNF-level procedures based on the properties blocked and covered require more care as it is necessary to take the
quantifier prefix into account.
Due to somewhat involved definitions, we postpone the definitions for clause elimination procedures based on the covered property until Section 6 in which we analyze there procedures in detail.
Similarly, liftings of the blocked and covered clause elimination procedures to QSAT are defined in
Sections 5 and 6, respectively. However, let us already here define the concept of asymmetric clause
elimination procedures, generalizing any (plain) clause elimination procedure. This is motivated by
the fact that, as will be shown, the asymmetric variants can achieve more simplification than the
plain procedures.
The asymmetric variant of a clause elimination technique relies on the clause extension rule of
asymmetric literal addition (ALA).
Definition 8 (Asymmetric Literal Addition). Given a clause C and a CNF formula F , literal l is
an asymmetric literal for clause C if there exist a clause C   Fl \ {C} such that C  \ {l} subsumes
C. For a clause C and a CNF formula F , ALA(F, C) denotes the unique clause resulting from
repeating the following until fixpoint: If l1 , . . . , lk  C and there is a clause (l1  . . .  lk  l) 
F \ {C} for some literal l, let C := C  {l}.
136

fiC LAUSE E LIMINATION

FOR

SAT

AND

QSAT

 ALA(F, (a  b  c))
Example 4. Consider the formula F = (a  b  c)  (a  b  d)  (a  c  d).
first adds the asymmetric literals d and d using (a  b  d) and (a  c  d), respectively. Afterwards,
 The fixpoint of
it can add the asymmetric literals a and b using (a  b  d) and c using (a  c  d).

ALA(F, (a  b  c)) is (a  a  b  b  c  c  d  d).
It is easy to show that the replacement of a clause C occurring in a CNF F by ALA(F, C)
preserves logical equivalence regardless of whether F is a quantifier free propositional formula or
the matrix of a QBF. In other words, ALA is agnostic of the quantifier prefix.
As concrete examples, asymmetric tautology elimination, asymmetric subsumption elimination,
and asymmetric blocked clause elimination are defined as follows.
Definition 9. A clause C is an asymmetric tautology if and only if ALA(F, C) is a tautology.
Asymmetric tautology elimination (ATE) repeats the following until fixpoint: If there is a clause
C  F for which ALA(F, C) is a tautology, let F := F \ {C}.
 from Example 4. For
Example 5. Consider the formula F = (a  b  c)  (a  b  d)  (a  c  d)

this F , ALA(F, (a  b  c)) = (a  a  b  b  c  c  d  d) is a tautology, and hence removed by
ATE from F .
As stated by the following lemma, ATE performs what could be called asymmetric branching on clausesreferred to as UP-redundancy by Fourdrinoy et al. (2007a, 2007a) and Piette
et al. (2008)which is used for example in the technique of clause distillation (Jin & Somenzi,
2005). This gives an alternative characterization of ATE in terms of Boolean constraint propagation.
S
Lemma 1. ALA(F, C) is a tautology if and only if BCP on (F \ {C})  lC {(l)}) derives a
conflict.
The definitions of asymmetric subsumption elimination and asymmetric blocked clause elimination are analogous to that of asymmetric tautology elimination.
Definition 10. Asymmetric subsumption elimination (ASE) repeats the following until fixpoint: If
there is a clause C  F for which ALA(F, C) is subsumed in F , let F := F \ {C}.
 ASE can remove
Example 6. Consider the formula FASE = (a  b  c)  (a  b  d)  (a  c  d).
 is subsumed by (a  b  d)
(a  b  c) from F , because ALA(FASE , (a  b  c)) = (a  b  c  d  d)

or (a  c  d).
Definition 11. For a given CNF formula F , a clause C  F is asymmetric(ally) blocked if
ALA(F, C) is blocked w.r.t. F . Asymmetric blocked clause elimination (ABCE) repeats the following until fixpoint: If there is an asymmetric blocked clause C  F for which ALA(F, C) is
blocked w.r.t. F , let F := F \ {C}.
  (a  d)  (b  d)
  (c  d).

Example 7. Consider the formula FABCE = (a  b  c)  (b  c  d)
ABCE can eliminate (a  b  c), because ALA(FABCE , (a  b  c)) = (a  b  c  d) which b and
c are blocking literals. Also, ABCE can remove all clauses in FABCE .
It turns out that clause elimination procedures that preserve logical equivalence in the case of
SAT, do not have to consider any information on quantifier ordering in the case of QSAT, i.e., these
137

fiH EULE , J ARVISALO , L ONSING , S EIDL , & B IERE

rules are the same for SAT and QSAT. Procedures which preserve only satisfiability equivalence in
SAT, however, would become unsound if the quantifier ordering were ignored and, therefore, the
restrictions imposed by the prefix have to be considered in the definition of such procedures. This
results in the quantified variants of blocked clause elimination (QBCE) and asymmetric blocked
clause elimination (AQBCE), both detailed in Section 5, as well as the quantified variants of covered clause elimination (QCCE) and asymmetric covered clause elimination (AQCCE) detailed in
Section 6).
3.2 Analyzing Clause Elimination Procedures
We will present detailed analysis on the relationships between the considered clause elimination
procedures, in terms of the achieved level of simplification (relative reduction power), the level of
equivalence maintained by the procedures (in terms of the sets of models of original and simplified
formulas), confluence (i.e., whether the procedures have a unique fixpoint), as well as the level
of constraint propagation maintained when applying the procedures. We will now formally define
these concepts and give an overview of the results. Detailed proofs for these results are presented
in Sections 46. Analysis of the procedures in terms of these properties are later (in Section 8)
complemented with an empirical evaluation on the effect of the clause elimination procedures on
runtimes of state-of-the-art SAT and QSAT solvers.
A relevant aspect of simplification techniques is the question of how much a specific technique
reduces the size of CNF (and PCNF) formulas. In this paper we analyze the relative reduction power
of the considered clause elimination procedures based on the clauses removed by the procedures.
For this we apply the following natural definition of reduction power.
Definition 12 (Relative reduction power). Assume two clause elimination procedures S1 and S2
that take as input an arbitrary CNF formula F and each produce as output a CNF formula that
consists of a subset of F that is satisfiability-equivalent to F .
 S1 at least as powerful as S2 if, for any F and any output S1 (F ) and S2 (F ) of S1 and S2 on
input F , respectively, we have that S1 (F )  S2 (F );
 S2 is not as powerful as S1 if there is an F for which there are outputs S1 (F ) and S2 (F ) of
S1 and S2 , respectively, such that S1 (F )  S2 (F );
 S1 is more powerful than S2 if
(i) S1 is at least as powerful as S2 , and
(ii) S2 is not as powerful as S1 .
Our definition of relative reduction power takes into account non-confluent elimination procedures, that is, procedures that do not generally have a unique fixpoint and that may thus have more
than one possible output for a given input. It should be noted that the result of a non-confluent
simplification procedure can be very unpredictable due to the non-uniqueness of results.
The definition of relative reduction power extends naturally to QSAT by considering the size
reduction of the matrix of a QBF. Hence, for natural liftings2 QS1 and QS2 of two CNF-level clause
elimination procedures S1 and S2 to QBFs, it will hold that if S1 is more powerful than S2 , then
QS1 is more powerful than QS2 .
2. A lifting QS1 of S1 is considered natural if QS1 behaves exactly like S1 when restricted to QBFs without universally
quantified variables.

138

fiC LAUSE E LIMINATION

ASE

FOR

SAT

AND

A(Q)BCE

ATE

QSAT

A(Q)CCE

logical equivalence
preserving

satisfiability-equivalence
preserving

SE

(Q)BCE

TE

(Q)CCE

SE: Subsumption elimination (same for SAT and QSAT).
TE: Tautology elimination (same for SAT and QSAT).
ASE: Asymmetric subsumption elimination (same for SAT and QSAT).
ATE: Asymmetric tautology elimination (same for SAT and QSAT).
(Q)BCE: (Quantified) blocked clause elimination.
(Q)CCE: (Quantified) covered clause elimination.
A(Q)BCE: Asymmetric (quantified) blocked clause elimination.
A(Q)CCE: Asymmetric (quantified) covered clause elimination.
Figure 1: Relative reduction power hierarchy of clause elimination procedures. An edge from X to
Y means that X is more powerful than Y. A solid edge means no corner cases. A dashed
edge means that the property does not hold for the corner case that the formula contains
only tautologies. A dotted edge means that the property does not hold for the corner case
that the formula contains the empty clause. A missing edge from X to Y means that X
is not as powerful as Y. However, notice that transitive edges are missing from the figure
for clarity. The Q in the prefix of a clause elimination technique indicates that there are
differences in the QSAT and SAT variant.

Our analysis results in a relative reduction power hierarchy (Figure 1) for the considered elimination procedures. For example, we show that for each of the known plain techniques, the asymmetric variants are more powerful. In this sense, the novel variants are proper generalizations of
the known plain techniques. It also turns out that the most powerful technique is the asymmetric
variant of covered clause elimination. The figure is slightly different from earlier work (Heule et al.,
2010). The changes are based on our renewed view of subsumption: a tautology is subsumed by
any clause.3 This view is justified by the fact that if C  F subsumes another clause C   F in
a CNF F due to C  C  , then C  is logically entailed by F . Since a tautological clause C   F
is trivially entailed by F , we regard C  to be subsumed by any other clause C  F . We note that
there is only one single corner case: if a formula contains only tautologies, then tautology elimination can remove all tautologies, while subsumption elimination can remove all, but one. Using this
definition, subsumption elimination techniques are as powerful as tautology elimination techniques.
Additionally, we consider the properties listed in Table 1 for further analysing the clause elimination procedures for SAT. It is easy to see that TE and SE are confluent and BCP-preserving, and
also that for any CNF formula F , TE(F ) and SE(F ) are logically equivalent to F . Furthermore,
for any QBF .F , tautology elimination and subsumption elimination, as well as their asymmetric
3. Donald Knuth convinced us of this view in personal communication on July 21, 2014.

139

fiH EULE , J ARVISALO , L ONSING , S EIDL , & B IERE

Table 1: Properties of clause elimination procedures for SAT.
Preserves logical eq. BCP-preserving Confluent
Subsumption-based
SE
yes
yes
yes
yes
no
no
ASE
Tautology-based
TE
yes
yes
yes
yes
no
no
ATE
Blocked clause
BCE
no
no
yes
ABCE
no
no
no
Covered clause
CCE
no
no
yes
no
no
no
ACCE
variants, do not use any information on the variable ordering, i.e., S(.F ) := .S(F ) for any
S  {TE, SE, ATE, ASE}. It also holds that BCE is confluent (Jarvisalo et al., 2010).
While each of the techniques preserves satisfiability (and are thus sound), it turns out that the
variants of blocked clause elimination and covered clause elimination do not preserve logical equivalence; this is the motivation for demonstrating in Section 7 how one can efficiently reconstruct
original solutions based on satisfying assignments for CNFs simplified using these variants. A
further property of simplification techniques is BCP-preservance, which implies that relevant unit
propagation (restricted to the remaining variables in the simplified CNF formula) possible in the
original CNF is also possible in the simplified CNF under any partial assignment. This property is
solver-related and very much practically relevant, since BCP is an integral part of a vast majority
of SAT solvers today.
Definition 13 (BCP-preserving). For a formula F , a preprocessing procedure S preserves BCP
on F if under any partial assignment  over the variables in F and for any formula S(F ) resulting
from applying S on F , we have that
(i) for any literal l occurring in S(F ), (l)  BCP(F,  ) implies (l)  BCP(S(F ),  )
(ii)   BCP(F,  ) implies   BCP(S(F ),  ) (the empty clause is obtained, that is, BCP
derives a conflict).
S is BCP-preserving if S preserves BCP on every CNF formula.
Notice that our definition is similar to deductive power as defined by Han and Somenzi (2007).
Also notice that BCP-preserving implies that logical equivalence is also preserved. Interestingly, it
turns out that BCP-preserving is quite a strict property, as only the plain SE and TE have it.
We note that if a procedure S is not BCP-preserving, and a procedure S  is more powerful
than S, then we immediately have that S  is not BCP-preserving. Similarly, if S does not preserve
logical equivalence, then S  does not preserve logical equivalence either. Furthermore, by viewing
CNF formulas as PCNF formulas where all variables are existentially quantified, showing that a
CNF-level procedure S is not BCP-preserving or is not confluent typically directly implies the same
140

fiC LAUSE E LIMINATION

FOR

SAT

AND

QSAT

negative result for the lifting of S to PCNF formulas. Indeed, this is the case for the procedures
considered in this article. Furthermore, since the quantifier structure does not impose restrictions on
the behavior of S  {TE, SE, ATE, ASE}, the positive results on BCP-preservance and confluence
also directly translate to the level of PCNF formulas. Hence we focus the analysis of the procedures
from these perspectives on the CNF-level.
In the following, we proceed by giving detailed analysis of each of the variants of tautology, subsumption, blocked clause, and covered clause based elimination procedures by considering equivalence preserving techniques first followed by a discussion of satisfiability preserving techniques.
Finally, experimental results on the practical effectiveness of the procedures are presented in Section 8.

4. Logical Equivalence Preserving Clause Elimination Techniques
We start our analysis by shortly considering clause elimination procedures which preserve logical
equivalence, namely, the well-known tautology and subsumption elimination, and their asymmetric
variants.
Lemma 2. ATE is more powerful than TE.
Proof. ATE is at least as powerful as TE due to C  ALA(F, C): if C is a tautology, so is
ALA(F, C). Moreover, let F = (ab)(bc)(ac). Since ALA(F, (ac)) = (aabbcc),
ATE can remove (a  c) from F , in contrast to TE.
Proposition 1. ATE is not confluent.
Proof. Consider the formula F = (ab)(ac)(ac)(bc)(bc). Now, ALA(F, (ab)) =
ALA(F, (a  c)) = ALA(F, (b  c)) = (a  a  b  b  c  c). ATE can remove either (a  b) or
both (a  c), (b  c).
Proposition 2. For any CNF formula F , ATE(F ) is logically equivalent to F .
S
Proof. For any clause C removed by ATE, (F \ {C})  lC {(l)} is unsatisfiable. This implies
that F \ {C} |= C, that is, F \ {C} logically entails C.
Proposition 3. ATE is not BCP-preserving.
Proof. Consider the standard CNF translation of x = If-Then-Else(c, t, e) as the formula
(x  c  t)  (x  c  t)  (x  c  e)  (x  c  e)  (x  e  t)  (x  e  t).
Notice that ATE can remove (x  e  t) and (x  e  t). However, after removing these clauses,
BCP will no longer assign x to t under the truth assignment  (e) =  (t) = f. Also, BCP will no
longer assign x to f under the truth assignment  (e) =  (t) = t.
Next we consider the asymmetric variants of tautology elimination and subsumption elimination.
Proposition 4. ASE is more powerful than SE.
141

fiH EULE , J ARVISALO , L ONSING , S EIDL , & B IERE

Proof. ASE is at least as powerful as SE since for any CNF formula F , (i) for every clause C  F ,
C  ALA(F, C), and (ii) if C is subsumed then any clause C   C is subsumed. Moreover,
consider the formula F = (a  b  c)  (a  b  d)  (b  c). In contrast to SE, ASE can remove
(a  b  d), because ALA(F, (a  b  d)) = (a  b  c  d) is subsumed by (a  b  c).
In connection with Proposition 4, we note that, under the UP-redundancy terminology, it was
equivalently observed by Fourdrinoy et al. (2007a) that, essentially, removing asymmetric tautologies until fixpoint produces a formula which is closed under subsumption elimination.
Lemma 3. ATE is at least as powerful as ASE, except in the corner case that the formula contains
the empty clause.
Proof. Consider the corner case that a formula contains the empty clause. In the case, ASE can
remove all clauses other than the empty clause. However, let F :=   C such that C is not a
tautology. ATE cannot remove C, while ASE can.
To see that ATE is at least as powerful as ASE in the other cases, consider the following. If
there is a clause C  F for which ALA(F, C) is subsumed by C   F \ {C}, then ALA(F, C)
is a tautology: say ALA(F, C) is subsumed by C  = (l1  . . .  lk ). By the definition of ALA,
l1 , . . . , lk  ALA(F, C).
Lemma 4. ASE is at least as powerful as ATE, except in the corner case that the formula consists
of only tautologies.
Proof. Consider the corner case that a formula consists of only tautologies. In this case, ATE can
remove all clauses. However, ASE can remove at most all but one clause.
To see that ASE is at least as powerful as ATE, consider the following. Any tautology is
subsumed by any other clause. That also holds for asymmetric tautologies. Hence as long as there
is at least one clause available for subsumption (which is the case if the formula contains at least
one non-tautological clause), ASE is at least as powerful as ATE.
Proposition 5. ASE is not confluent.
Proof. By replacing ATE with ASE in the proof of Proposition 1.
Proposition 6. For any CNF formula F , ASE(F ) is logically equivalent to F .
S
Proof. For any clause C removed by ASE, (F \ {C})  lC {(l)} is unsatisfiable. This implies
that F \ {C} |= C, that is, F \ {C} logically entails C.
By replacing ATE with ASE in the proof of Lemma 3 we have the following.
Proposition 7. ASE is not BCP-preserving.

5. Clause Elimination Procedures based on Blocked Clauses
As a third family of clause elimination procedures considered in this paper, we now analyze procedures that eliminate blocked clauses (Kullmann, 1999) and present generalizations thereof to QSAT.
142

fiC LAUSE E LIMINATION

FOR

SAT

AND

QSAT

5.1 Blocked Clause Elimination for SAT
We start with the plain variant of blocked clause elimination, BCE.
Proposition 8. BCE is more powerful than TE.
Proof. To see that BCE is at least as powerful as TE, notice that for any tautology C, D  C is
also a tautology. In this case D = C  C  \ {l} with C  and l from Definition 7 of BCE. Moreover,
consider the formula F := (a). BCE can remove (a) from F , in contrast to TE.
Proposition 9. For some CNF formula F , BCE(F ) is not logically equivalent to F .
Proof. Recall the formula FBCE from Example 3. Consider the truth assignment  with  (v1 ) =
 (v2 ) =  (w3 ) = t and  (v3 ) =  (w1 ) =  (w2 ) = f. Although  satisfies BCE(FBCE ), the clause
(v1  v2 ) in FBCE is falsified by  .
Proposition 10. BCE(F ) is not BCP-preserving.
Proof. Follows from the fact that BCE does not preserve logical equivalence (Proposition 9).
We now turn to the asymmetric variant of blocked clause elimination which turns out to be more
powerful that BCE.
Proposition 11. Removal of an asymmetric blocked clause preserves satisfiability.
Proof. Follows from the facts that F is logically equivalent to (F \ {C})  {ALA(F, C)} and that
BCE preserves satisfiability.
Lemma 5. ABCE is more powerful than (i) BCE, and (ii) ATE.
Proof. ABCE is at least as powerful as BCE due to C  ALA(F, C): if C is a tautology, then
ALA(F, C) is a tautology. ABCE is at least as powerful as ATE since tautologies are blocked
clauses. Moreover, recall that in Example 7, ABCE could eliminate all clauses from FABCE . Neither BCE nor ATE can remove any clause from FABCE .
Proposition 12. ABCE is not confluent.
  (b  d)  (c  d). F contains four asymmetric
Proof. Let F = (a  b)  (a  c)  (a  d)
 with blocking literal b, ALA(F, (a  c)) =
blocked clauses: ALA(F, (a  b)) = (a  b  c  d)

(a  b  c  d) with blocking literal c, ALA(F, (b  d)) = (a  b  c  d) with blocking literal b,
and ALA(F, (c  d)) = (a  b  c  d) with blocking literal c. ABCE removes either (a  b) and
(b  d), or (a  c) or (c  d) from F .
Replacing BCE with ABCE in the proof of Proposition 9, we have the following.
Proposition 13. For some CNF formula F , ABCE(F ) is not logically equivalent to F .
143

fiH EULE , J ARVISALO , L ONSING , S EIDL , & B IERE

5.2 Quantified Blocked Clause Elimination
In the following, we generalize the notion of blocked clauses and blocked clause elimination (BCE)
for QSAT. We prove that blocked clauses can be removed also in the case of QSAT, and discuss
differences to propositional logic.
Definition 14. A literal l with quant(l) =  in a clause C  F of a QBF G = Q1 . . . Qn .F is called
a quantified blocking literal if for all C   F with l  C  , a literal l with l  l exists such that
l , l  C  (C  \ {l}). A clause is quantified blocked if it contains a quantified blocking literal.
For a QBF G in PCNF, quantified blocked clause elimination repeats the removal of quantified
blocked clauses from G until fixpoint. The resulting QBF is denoted by QBCE(G). Our definition
of quantified blocking literals slightly differs from the original definition (Biere et al., 2011) which
uses l , l  C l C  instead of l , l  C  (C  \ {l}). Our definition includes the case that either C
or C  is a tautology, i.e., when C l C  is undefined. Apart from this, the definitions are equivalent.
In contrast to propositional logic, there are two restrictions on the selection of quantified blocking literals. A blocking literal has to be existential and the literals responsible for the tautology in a
resolvent have to be of smaller level than the blocking literal. Without these restrictions, quantified
blocked clause elimination would not be sound, as illustrated by the following examples.
Example 8. Both clauses in the satisfiable QBF G = xy.((x y)(xy)) are quantified blocked
clauses since the existential literals y and y are quantified blocking literals in the first and second
clause, respectively. Note that x < y in the prefix ordering. Let G = xy.((x  y)  (x  y)) be
obtained from G by changing the quantifiers of x and y in the prefix. The QBF G is unsatisfiable.
No clause in G is quantified blocked since x is existential and x < y. If we ignored the condition on
the level of a quantified blocking literal, then erroneously both clauses in G would be considered
as quantified blocked, and removing any clause of G results in a satisfiable QBF.
Example 9. Consider the unsatisfiable QBF yxz.((y  x  z)  (y  x  z)  (y)  (z)).
By definition, literals of the universal variable x cannot be quantified blocking. If we ignored the
quantifier type of x then erroneously x and x in the first and second clause, respectively, would
be considered as quantified blocking literals. Note that the condition on the quantifier level holds,
that is, y < x where y is responsible for the tautological resolvent of the two clauses containing
x. However, removing the second clause, which is erroneously considered as quantified blocked,
results in a satisfiable QBF.
As the following theorem shows, quantified blocked clauses contain redundant information only,
and may therefore be removed from the formula.
Theorem 1. Let G = Q1 . . . Qn .(F  {C}) be a QBF and let C be a quantified blocked clause in
G with blocking literal l. Then G and Q1 . . . Qn .F are equivalent.
Proof. Let C be a quantified blocked clause with the quantified blocking literal l with var(l)  Qi ,
i  n. The direction G  Q1 . . . Qn .F trivially holds. We show Q1 . . . Qn .F  G by induction
over q = |Q1 . . . Qi1 |.
In the base case, we have q = 0, i.e., var(l)  Q1 with quant(Q1 ) = . The same argument
as in SAT (Kullmann, 1999) applies: let  be a satisfying assignment for F , i.e., for each C   F
there exists a literal l such that  (l ) = t. If  satisfies C, the implication Q1 .F  G holds,
144

fiC LAUSE E LIMINATION

FOR

SAT

AND

QSAT

otherwise we construct a satisfying assignment   for F  {C} as follows. Let   (l ) =  (l ) for
l 6= l and   (l) = t. Then   satisfies not only C but also all other clauses C   F . If l  C  , there
exists a literal l 6= l such that l  C and l  C  , with  (l ) =  (C) =   (l ) = f and thus
  (C  ) =   (l ) = t. Note that l  Q1 due to the restriction l  l.
For the induction step, assume q > 0. Let h be a literal with var(h) = y and y  Q1 . Note
that var(l) 6= y. We show that Q1 \{y} . . . Qn .F [h]  G[h]. The rest follows from lifting the
implication over the conjunction that defines the semantics of universal quantification if quant(Q1 )
= , and, respectively, over the disjunction that defines the semantics of the existential quantification
if quant(Q1 ) = . Three cases have to be considered for showing that C[h] is a blocked clause or
removed in F [h].
1. h  C. Then C is removed from G[h].
2. h 6 C and h 6 C. Consequently, C[h] = C. Furthermore, C is still a quantified blocked
clause in G[h], since h was not used to make a resolvent on l tautological. Then the induction
hypothesis is applicable.
3. h  C. Consequently, C[h] = C\{h} which is a quantified blocked clause in G[h], because
each clause C  with h, h  C l C  is removed from G[h], and other clauses C  with k, k 
C l C  and y 6= var(k) still produce tautological resolvents with C on l. Note that l  C[h]
since l 6= h.
Theorem 2. The application of QBCE(G) on a QBF G is confluent.
Proof. The argument is similar as for propositional logic (see Section 5).
For the soundness of quantified blocked clause elimination for QSAT, the level of the blocking
literal must be equal or higher than the level of the literal making the resolvent tautological, as the
following example illustrates.
Example 10. An extended example related to Example 8 where universal reduction is not applicable
to any clause is given by the unsatisfiable QBF
G = xyz.((x  z)  (x  z)  (y  z)  (y  z)).
The first two clauses in G encode that x and z are equivalent, and the last two clauses encode
that y and z are equivalent. The variable z prohibits the application of universal reduction. In the
first two clauses x and x, respectively, are not quantified blocking literals because x < z. If these
clauses were erroneously considered to be blocked and removed, then the resulting QBF would be
satisfiable.
Quantified blocked clauses may be eliminated from a formula without changing its truth value,
because they contain redundant information only. Hence, quantified blocked clause elimination is
applied in order to remove clauses from a QBF which may reduce the number of variables occurring
in the formula too. The following properties established for SAT (Kullmann, 1999; Heule et al.,
2010), also hold for QSAT. For the sake of compactness, we omit the prefix quantified if no
confusion arises.
145

fiH EULE , J ARVISALO , L ONSING , S EIDL , & B IERE

1. Formulas which are smaller with respect to their number of clauses potentially contain more
blocked clauses. If the matrix of a QBF G1 is a subset of the matrix of the QBF G2 then
there might be clauses which are blocked in G1 , but not in G2 . If there is a clause C which is
blocked in G2 , but not in G1 , then C 6 G1 .
2. From the statement above, it follows immediately that QBCE has a unique fixpoint. If a
clause C is blocked in a QBF G, then any clause C  with C 6= C  blocked in G is also
blocked in G\{C}.
3. If a clause C is subsumed by a blocked clause C  , i.e., C   C, then C is also a blocked
clause. Obviously, the other direction does not hold.
4. Clauses containing an existential pure literal are blocked. The pure literal is the blocking
literal. In fact, QBCE may be considered as a generalization of pure literal elimination rule
for existentially quantified variables. However, the elimination of pure literals which are
universally quantified is not simulated by QBCE. In general, pure literal elimination of
universal literals does not eliminate whole clauses, but only single literals.
5. If the clauses C1 . . . Cn are the only clauses of a QBF G which contain the literal l, then
a clause C with l  C is blocked if for each clause Ci , the clause C contains a literal li
with li  Ci and li < l. In particular, if a QBF G contains an equivalence of the form
(l, l1 , . . . , ln ), (l, l1 ), . . . , (l, ln ) and l occurs in no other than these clauses, then the equivalence may be removed due to QBCE.
The fifth property indicates that QBCE eliminates equivalences under certain conditions. In
fact, like BCE in SAT (Jarvisalo et al., 2012a), QBCE achieves structure-based simplifications
defined for circuit-based representations purely on the PCNF-level, without explicit knowledge on
the structure of the original representation.

6. Covered Clause Elimination Procedures
As the final family of clause elimination procedures considered in this paper, we now introduce and
analyze CNF and PCNF-level procedures that eliminate what we call covered clauses.
Covered clause elimination is based on successively adding certain literals to a clause C in a
CNF F  = F  {C} in a satisfiability-preserving way. Adding a literal l to C produces the extended
clause C  = C  {l} which replaces C in F  to obtain F  = F  {C  }. If C  becomes blocked due
to adding a literal l then C  can be removed from F  by BCE, effectively eliminating one clause of
F  . The literals added to a clause C by extension steps are called covered literals. These literals are
determined by inspecting all the clauses which, when resolved with C, result in a non-tautological
resolvent. A clause that becomes blocked by adding covered literals is called a covered clause. For
the application of covered clause elimination in practice, a clause C is extended by covered literals
only tentatively. If the extended clause C  does not become blocked eventually, then the added
literals are discarded and the original clause C is restored.
In the following, we formally define the set of covered literals, which can safely be used to
extend clauses in a CNF. Then we introduce covered clause elimination for SAT and QSAT and
analyze its properties. Similar to BCE, covered clause elimination does not preserve logical equivalence. Therefore, in Section 7 we present an algorithm to reconstruct solutions to CNFs where
covered clauses have been eliminated.
146

fiC LAUSE E LIMINATION

FOR

SAT

AND

QSAT

6.1 Covered Clause Elimination Procedures for SAT
Given a CNF formula F , a clause C, and a literal l  C, the set of resolution candidates of C w.r.t. l
is
RC(F, C, l) := {C  | C   Fl and C l C  is not a tautology}.
In words, the set RC(F, C, l) of resolution candidates consists of clauses C  that contain the opposite
literal of l, meaning that these clauses can be resolved with C on the literal l. Notice that every clause
in RC(F, C, l) contains the literal 
l. If RC(F, C, l) = , then C is blocked w.r.t. F . The literals

apart from l which occur in all clauses of RC(F, C, l) form the resolution intersection RI(F, C, l)
of l and C w.r.t. F , formally defined as
\

RI(F, C, l) :=
RC(F, C, l) \{l}.
In words, the resolution intersection RI(F, C, l) is the set of literals (apart from l) which occur in
each clause in the resolution candidate set RC(F, C, l). Given a CNF formula F , a clause C  F ,
and a literal l  C, we say that l covers the literals in RI(F, C, l) (w.r.t. F and C). A literal l is
covered by l  C if l  RI(F, C, l). A literal l  C is covering w.r.t. F and C if l covers at least
one literal, that is, RI(F, C, l) 6= .
Example 11. Consider the formula
  (a  b  c)  (a  b  d)
  (a  c  d)
FCLA = (a  b  c)  (a  b  d)  (a  c  d)
which is also visualized as a resolution graph in Figure 2. The resolution graph is constructed as
follows. The clauses are the vertices and vertices are connected if and only if resolution between
the two clauses results in a non-tautological resolvent. In other words, vertices are only connected
if they have exactly one clashing literal. The edges have a label which shows the corresponding
variable of the clashing literal pair.
BCE cannot remove a clause FCLA because for each literal occurrence in FCLA there exists a
non-tautological resolvent. Figure 2 illustrates this by having for each literal of each clause at least
one edge. Now, RC(FCLA , (a  b  c), b) = {(a, b, d)}, so RI(FCLA , (a  b  c), b) = {a, d}. In
other words, literal b in (a  b  c) covers the literals a and d. As discussed in the following, adding
d to (a  b  c) preserves satisfiability. 4
After the addition of d to (a  b  c), several edges disappear. It no longer holds that each
literal occurrence has a corresponding edge. All literals that do not have an edge, (for example, c
 have become blocking literals.
in (a  c  d)),
Lemma 6. For any CNF formula F , clause C  F , and literal l  C, it holds that replacing C by
C  RI(F, C, l) in F preserves satisfiability.
Proof. For any literal l  C it holds that VE(F, l) = VE((F \ {C})  {C  RI(F, C, l)}, l), where
VE(F, l) denotes the CNF formula resulting from variable eliminating the variable of the literal l
from F (more formally, VE(F, l) = (Fl  Fl )  (F \ (Fl  Fl )), where Fl and Fl consist of the
clauses in F that contain l and l, respectively, and Fl  Fl = {C l C  | C  Fl , C   Fl , and
C l C  is not a tautology}).
4. In fact, based on the same reasoning, one could also eliminate literal a from (a  b  c).

147

fiH EULE , J ARVISALO , L ONSING , S EIDL , & B IERE

abc
a

b

a  b  d
c

a
a

a  c  d

a  b  d

abcd
a

a  c  d

a
c

d

d

a

a

b

a  b  c

b

a  b  d

a

d

a  c  d

a  c  d

a
c

d
a  b  d

a
b

a
a  b  c

Figure 2: Two resolution graphs of FCLA : both graphs have for each clause in FCLA one vertex
and vertices are connected with an edge if the have exactly one pair of complementary
literals (hence the resolvent in non-tautological). The edges are labeled with the variable
of the complementary literals. The top figure shows FCLA before adding covered literal
d to (a  b  c) and the bottom figure shows FCLA after the addition. Notice that in the
top figure there is an edge for each literal. Literals in the bottom figure that have no edge
associated with them, such as c and d in (a  b  c  d) are blocking literals.

For a given clause C in a CNF formula F , we denote by (covered literal addition) CLA(F, C) the
clause resulting from repeating the following until fixpoint:
If there is a literal l  C such that RI(F, C, l) \ C 6= , let C := C  RI(F, C, l).
Lemma 7. Replacing a clause C  F by CLA(F, C) preserves satisfiability.
Proof. The clause CLA(F, C) is obtained by iteratively applying Lemma 6 on clause C.
Lemma 8. Assume two clauses C, D with l  C  D and two sets of clauses F, G with F 
G. Further assume that D is not blocked w.r.t. F and hence C is not blocked w.r.t. G. Then
RC(G, C, l)  RC(F, D, l) 6=  and hence RI(G, C, l)  RI(F, D, l).
Proof. Monotonicity of RC w.r.t. its first argument and anti-monotonicity w.r.t. its second argument
follows directly from its definition. For RI, note that intersection is anti-monotonic for non-empty
sets of sets.
148

fiC LAUSE E LIMINATION

FOR

SAT

AND

QSAT

Theorem 3. Given a CNF formula F and a clause C  F , CLA(F, C) is blocked or uniquely
defined.
Proof. Assume C is not blocked w.r.t. F and contains two literals l1 , l2 , which cover the literals
Li = RI(F, C, li ) respectively. Consider the clauses C1 = C  L1 and C2 = C  L2 . Now assume
that both of C1 , C2 are not blocked w.r.t. F . Then all clauses D  RC(F, C1 , l2 )  RC(F, C, l2 )
contain all literals in L2 . Since C1 is not blocked and thus RC(F, C1 , l2 ) is not empty, we obtain
L2  RI(F, C1 , l2 ). The case where the indices are exchanged (i.e., L1  RI(F, C2 , l1 )) is symmetric. Thus as long clauses do not become blocked, covered literals can be added independently.
The case that both of C1 , C2 are blocked is trivial.
What remains (by symmetry) is the case that C2 is blocked but C1 is not. Again, we get L2 
RI(F, C1 , l2 ). For C1 = C1 RI(F, C1 , l2 ) we have C1 = C L1 RI(F, C1 , l2 )  L1 (C L2 ) 
C2 which is also blocked. This generalizes to the following observation: For any non-deterministic
choice of adding covered literals to C, the literal l2 remains covering. Further, if in this process the
clause did not become blocked, it will eventually become blocked if the covered literals of l2 are
added.
With the needed preliminaries in place, we are ready to introduce covered clause elimination
procedures. The first one is the plain variant, simply called covered clause elimination.
Definition 15. Given a CNF formula F , a clause C  F is covered if CLA(F, C) is blocked
w.r.t. F .
Example 12. Back to Example 11. Recall that RI(FCLA , (abc), b) = {a, d}. Also, RI(FCLA , (a
 Therefore, depending on the order of addition, CLA(FCLA , (a  b  c)) is eib  c), c) = {a, d}.
 when starting with
ther (a  b  c  d) when starting with covering literal b, or (a  b  c  d)
covering literal c. In both cases CLA(F CLA , (a  b  c)) is blocked. After replacing (a  b  c)
by (a  b  c  d), the truth assignment  with  (a) =  (b) =  (c) = f and  (d) = t satisfies
the new formula, while falsifying (a  b  c)  FCLA . In fact, FCLA witnesses the fact that none
of the clause elimination procedures based on covered clauses, as introduced next, preserve logical
equivalence in general.
An illustration of the above sis show in Figure 2. It shows the resolution graph of FCLA before
and after adding a covered literal.
Lemma 9. Removal of an arbitrary covered clause preserves satisfiability.
Proof. Clause C can be replaced by CLA(F, C) (Lemma 7), and C can be removed as CLA(F, C)
is blocked.
Definition 16 (Covered Clause Elimination). For a given formula F , covered clause elimination
(CCE) repeats the following until fixpoint: If there is a covered clause C  F , let F := F \ {C}.
The resulting unique formula is denoted by CCE(F ).
Confluence of CCE follows from the following lemma.
Lemma 10. The following holds for any CNF formula F , clause C  F , and set of clauses S  F
such that C 6 S. If C is covered w.r.t. F , then C is covered w.r.t. F \ S.
149

fiH EULE , J ARVISALO , L ONSING , S EIDL , & B IERE

Proof. Let CLA(F, C) = Ck , where C0 := C, and Ci+1 := Ci RI(F, Ci , li ) for each i = 0..k 1
and li  Ci . Now define D0 := C and, for each i = 0..k1, Di+1 := Di if Di is blocked w.r.t. F \S
and Di+1 := Di  RI(F \ S, Di , li ) otherwise. Using Lemma 8, one can show by induction that
for each i we have either (i) Di is blocked w.r.t. F \ S, or (ii) RI(F \ S, Di , li )  RI(F, Ci , li ). If
(i) holds for some i, then CLA(F \ S, C) is blocked w.r.t. F \ C. If Di is not blocked w.r.t. F \ S
for any i, then CLA(F \ S, C)  CLA(F, C).
Theorem 4. CCE is confluent.
Proof. Follows directly from Lemma 10: if two clauses C and D are both covered w.r.t. F , then C
is covered w.r.t. F \ {D}.
Lemma 11. CCE is more powerful than BCE.
Proof. CCE is at least as powerful as BCE follows from the fact that C  CLA(C): if C is
blocked, so is CLA(C). Moreover, in FCLA no clause is blocked. However, all clauses are covered.
Hence BCE will not remove a single clause, while CCE removes all of them.
In fact, in cases it is possible to add covered literals to ALA(F, CLA(F, C)): there are cases
when adding asymmetric literals to a clause can increase |RI(F, C, l)| for a non-asymmetric literal
l  C.
Example 13. Consider the CNF formula F = (a  b)  (b  c)  (a  c)  (a  d). Notice that
CLA(F, (a  b), a) = (a  b). Literal c is asymmetric w.r.t. (a  b) due to (b  c). After adding c,
a in the extended clause (a  b  c) covers d.
This observation motivates the following definition for asymmetric covered clauses, based on
extending clauses iteratively by both CLA and ALA until a fixpoint is reached. More formally, for a
given CNF formula F , a clause C  F is asymmetric covered if the clause resulting from repeating
1. C := CLA(F, C).
2. C := ALA(F, C).
until fixpoint is blocked w.r.t. F . Based on this definition, we arrive at asymmetric covered clause
elimination, ACCE.
Definition 17. Asymmetric covered clause elimination (ACCE) repeats the following until fixpoint:
if there is asymmetric covered clause C in F , let F := F \ {C}.
Lemma 12. Removal of an arbitrary asymmetric covered clause preserves satisfiability.
Proof. Follows from the facts that (i) F is satisfiability-equivalent to (F \ {C})  {CLA(F, C)};
(ii) F is satisfiability-equivalent to (F \{C}){ALA(F, C)}; and (iii) BCE preserves satisfiability.

Lemma 13. ACCE is more powerful than (i) ABCE, and (ii) CCE.
150

fiC LAUSE E LIMINATION

FOR

SAT

AND

QSAT

Proof. (i) By replacing CCE and BCE by ACCE and ABCE in the proof of Lemma 11. (ii) Consider the formula
FACCE = (a  b  c)  (a  b  c)  (a  b  c)  (a  b  c) 
(a  b  c)  (a  b  c)  (a  b  c)  (a  b  c) 
  (a  b  d)  (a  b  d).

(a  b  d)  (a  b  d)
CLA cannot add literals to any clause in FACCE . However, ALA can add d and d to (a  b  c)
 and (a  b  d), respectively. After ALA, (a  b  c) is a tautology, so ACCE can
using (a  b  d)
remove it.
6.2 Quantified Covered Clause Elimination
We now introduce a lifting of covered clause elimination for QSAT. Thereby, covered clauses are
clauses which are blocked when they are enriched with literals contained in any resolvent with pivot
element l, the covering literal. As with QBCE, the prefix ordering has to be taken into account.
Definition 18. Let QRC denote the set of resolution candidates with
QRC(G, C, l) := {C  | C   F, l  C  , 6 l : {l , l }  C l C  },
where G is a QBF with matrix F , C  F , l  C. Then the resolution intersection QRI(G, C, l) of l
and C w.r.t. G is given by

\
{C  | C   QRC(G, C, l), C   C  , l  C  : l  l} \{l}.
QRI(G, C, l) :=
A literal l is called covering literal if QRI(G, C, l) 6= , i.e., l covers the literals in QRI(G, C, l).
Lemma 14. The replacement of a clause C in a QBF G by C  QRI(G, C, l) preserves unsatisfiability.
Proof. We will show that for each QBF G = .(F  {C}) it holds that if G is unsatisfiable, so is
G = .(F  QRI(G, C, l)). Assume that G is unsatisfiable, but G is not. Then there has to be at
least one assignment  such that  (C) = f and  (QRI(G, C, l)) = t. In consequence, there is at
least one l  QRI(G, C, l) with  (l ) = t. Due to the construction of QRI, it holds that C   F
with l  C  ,  (C  ) = t. This means that at latest, after assigning all variables v where v < l with
value  (v), l is pure. This means that F is satisfiable by an assignment   with   (k) =  (k) for
k 6= l and   (l) = ( (l)), which is in contradiction with the assumption that G is unsatisfiable.
In the following, QRI(G, C) denotes the clause C extended with all quantified covered literals,
i.e., for all l  QRI(G, C) it holds that QRI(G, C, l)  QRI(G, C).
Lemma 15 (Quantified Covered Literal Addition). The replacement of a clause C in a QBF G by
QRI(G, C) preserves unsatisfiability.
Proof. Iterative application of Lemma 14.
Definition 19 (Quantified Covered Clause). A clause C in a QBF G is covered if QRI(G, C) is
blocked w.r.t. G.
151

fiH EULE , J ARVISALO , L ONSING , S EIDL , & B IERE

Theorem 5. The removal of a covered clause preserves unsatisfiability.
Proof. According to Lemma 15, each clause may be replaced by the clause QRI(G, C). If this
clause is blocked, it may be removed according to Theorem 1. If it is a tautology, it can be removed
due to standard rewriting rules.
Example 14. In the QBF a, b, c x, y.((x  a)  (x  y  b)  (x  y  c)  (y  a)) the literal x
of the clause (x  a) covers the literal y. Therefore, we can replace this clause by (x  a  y) which
is blocked due to the blocking literal y. Consequently, the clause (x  a  y) can be eliminated.
The restriction that the literals in the resolution candidates are of smaller level than the according
pivot is necessary for the correctness of quantified covered clause elimination, as shown in the
following example.
Example 15. Consider the unsatisfiable QBF xay.((xy)(xa)(y  a)). If we do not give
any restriction on the selection of the resolution candidates, we would obtain the clause (x  y  a)
which is blocked with blocking literal y. If we remove this clause, the QBF becomes satisfiable.
Lemma 16. Covered clause elimination for QSAT is confluent.
Proof. Assume we have to add literal l to a clause C in order to make C a covered clause which is
then removed by blocked clause elimination. Assume that l  QRI(C, l ) for a literal l  C. We
have to show that if clause C  with l  C  is removed due to QCCE, then either l may be still added
to C or C is blocked and hence may be removed. If C  was the only clause containing l, then there
is no other clause C  with l  C  . Then l is pure and C may be removed. Otherwise, l is in the
intersection of the resolvents with pivot literal l and hence l may be added to C.

7. Reconstructing Solutions
Since the elimination procedures based on blocked clauses and covered clauses do not preserve
logical equivalence, a truth assignment  satisfying, for example, BCE(F ) may not satisfy F . In
this section we shown how solutions to the original CNF formulas can be reconstructed based on
solutions to the CNF formulas resulting from applying variations of blocked clause and covered
clause elimination.
7.1 Procedures Based on Blocked Clauses
Jarvisalo and Biere (2010) showed how, given any CNF formula F and truth assignment  that
satisfies BCE(F ), one can construct a satisfying assignment for F : Add the clauses C  F \
BCE(F ) back in the opposite order of their elimination. In case C is satisfied by  , do nothing.
Otherwise, assuming that l  C is blocking C, flip the truth value of l in  to t. After all clauses
have been added, the modified  satisfies F .
We now show that this procedure can be used to reconstruct solutions for formulas simplified
using ABCE.
Lemma 17. Given a clause C  F , if ALA(F, C) is blocked and not a tautology, then there is a
literal l  C blocking it.
152

fiC LAUSE E LIMINATION

FOR

SAT

AND

QSAT

Proof. By construction, for each literal l  ALA(F, C) \ C, here is a clause C   F that contains l
and C  \{l}  ALA(F, C). Therefore, because ALA(F, C) is not a tautology, C  l ALA(F, C) =
ALA(F, C) \ {l} is not a tautology either. Hence l is not blocking ALA(F, C).
Lemma 18. Given a CNF formula F and a truth assignment  satisfying F , if C 
/ F is falsified
by  , then ALA(F, C) is falsified by  .
Proof. From Lemma 1 follows that F {ALA(F, C)} is logically equivalent to F {C}. Therefore,
ALA(F, C) is satisfied by  if and only if  satisfies C.
Lemma 19. Given a CNF formula F and a truth assignment  satisfying F , if C 
/ F is falsified
by  and ALA(F, C) is blocked w.r.t. F with blocking literal l  C, then  satisfies at least two
literals in each clause C   F with 
l  C .
Proof. First, such C   F contain a literal l which is satisfied by  . Second, because l is blocking,
each clause C  must contain one more literal l 6= l such that l  ALA(F, C). Since all literals in
ALA(F, C) are falsified by  , l must be satisfied by  .
Combining these three lemmas, we can reconstruct a solution for F if we have a satisfying
assignment  for any ABCE(F ). The clauses C  F \ ABCE(F ) are added back in reverse order
of elimination to ensure that ALA(F, C) is blocked. If C is satisfied by F do nothing. Otherwise,
we know that there is a literal l  C blocking ALA(F, C); recall Lemma 17. Furthermore, all
literals in ALA(F, C) are falsified; recall Lemma 18. However, any C   F containing l has two
satisfied literals; recall Lemma 19. Therefore, by flipping the truth assignment for l to t, C becomes
satisfied, while no such C  becomes falsified.
Theorem 6. The following holds for an arbitrary CNF formula F and truth assignment  satisfying
F . For any clause C 
/ F for which C, ALA(F, C) is blocked w.r.t. F with blocking literal l, either
(i)  satisfies F  {C}, or (ii)   , which is a copy of  except for   (l) = t, satisfies F  {C}.
The reconstruction proof provides several useful elements that can be used to implement ABCE
more efficiently. First, since only original literals l  C can be blocking ALA(F, C), we can avoid a
blocking literal check for all literals l  ALA(F, C) \ C. Second, it is enough to save each removed
original clause C. None of the additional literals in the extended clause ALA(F, C) not occurring
in C have to be flipped.
We implemented reconstruction as follows. When a clause C is eliminated by a procedure
based on blocking literals (BCE and ABCE), C together with a blocking literal l  C is pushed
on a reconstruction stack S: i.e., S := S, hl:Ci. During reconstruction, we examine the eliminated
clauses in reverse order. If the clause on the top of the stack is falsified, the truth value of the
blocking literal is flipped. Figure 3 shows the pseudo-code of the algorithm.
7.2 Procedures Based on Covered Clauses
When covered clauses are eliminated, reconstruction of solutions becomes more tricky. This is due
to the following. As shown in the previous section, given any  satisfying a formula F , and a clause
C which is falsified by  , by Lemma 18 ALA(F, C) is also falsified by  . However, the analogous
claim for CLA(F, C) is not true in general.
153

fiH EULE , J ARVISALO , L ONSING , S EIDL , & B IERE

reconstruction (CNF formula F , truth assignment  , elimination sequence S)
while S is not empty
let hl:Ci := S.pop()
if C is falsified by  then
flip the truth value of l in  to t
F := F  {C}
return 

Figure 3: Pseudo-code for reconstructing a solution with F a reduced formula,  a satisfying assignment for F and S a set of eliminated clauses ordered by last eliminated.

Proposition 14. There is a CNF formula F and a satisfying truth assignment  for F such that the
following holds. There is a clause C  F that is falsified by  , while CLA(F, C) is satisfied by  .
Proof. Let F = FCLA \ {(a  b  c)} and assume that the truth assignment  assigns  (a) =  (b) =

 (c) =  (d) = f. Let C = (a  b  c). Now,  satisfies F , but falsifies C. Since c  C covers d,

satisfies CLA(F, C) = (a  b  c  d).
Additionally, for any formula F and clause C  F , if ALA(F, C) is blocked w.r.t. F , then there
is a literal l  C blocking it by definition. However, in case CLA(F, C) is blocked, there might be
a literal in CLA(F, C) \ C blocking it.
Proposition 15. There is a CNF formula F for which the following holds. There exist a clause
C  F such that C is not blocked w.r.t. F but CLA(F, C) is blocked (due to a blocking literal
l  CLA(F, C) \ C).
Proof. Recall FCLA from Section 6. In FCLA , (a  b  c) is not blocked. However, the extended
 is blocked with blocking literal d.

clause CLA(FCLA , C) = (a  b  c  d)
Due to the properties stated as Propositions 14 and 15, given a truth assignment  satisfying a
formula F and a covered clause C  F , one may be required to flip the truth values of multiple
variables in  in order to construct a satisfying assignments for F  {C} based on  . We will next
show how this can be achieved.
Theorem 7. Given a CNF formula F and a truth assignment  satisfying F . Let clause C 
/ F be
falsified by  , while there is a literal l  C such that  satisfies C  RI(F, C, l), then   , being a
copy of  with l assigned to t, satisfies F and C.
Proof. We need to show that after flipping l to t, all clauses in Fl are still satisfied. There are two
cases. First, consider a clause C   Fl such that C l C  is a tautology. Then there must be a
x  C such that x  C  . Since x is falsified by  , because C is falsified by  , x is satisfied by
 and so is C  . The second case is C   Fl with C l C  not being a tautology. By definition,
RI(F, C, l)  C  . Since C is falsified by  while C  RI(F, C, l) is satisfied by  , RI(F, C, l) is
satisfied by  and so is C  .
154

fiC LAUSE E LIMINATION

FOR

SAT

AND

QSAT

Given a CNF formula F , C 
/ F with CLA(F, C) being blocked w.r.t. F , and a truth assignment
 satisfying F . We can use the observation in Theorem 7 to compute, given a truth assignment  
that satisfies F  {C}. In case  satisfies C it is trivial (  =  ). Otherwise, there is a sequence
C0 , C1 , . . . , Cc such that C0 := C, Cc := CLA(F, C), and Ci+1 := Ci  RI(F, Ci , li ) with li  Ci .
Now, let the reconstruction stack S contain the sequence hl0 :C0 i, hl1 :C1 i, . . . , hlc :Cc i. Applying
  = reconstruction(F, , S) using the algorithm in Figure 3 produces   that satisfies F and C.
7.3 Solution Reconstruction for QSAT
We briefly review approaches to obtaining satisfiability models from preprocessed formulas in the
context of QSAT. These approaches can be classified into two categories depending on whether full
or partial satisfiability models are generated.
First, for some QSAT applications, it is sufficient to extract a partial model of a formula out of
a preprocessed one. Often only the values of the leftmost existential block of variables in a QBF
are of interest. In this case, the partial model represents a single assignment to these variables,
that is a Skolem function of zero arity. To generate partial models in practice, the preprocessor is
restricted to apply only those preprocessing rules which do not affect any variables from the leftmost
quantifier block. These variables are declared as dont touch variables (Seidl & Konighofer, 2014).
This approach originates from incremental bounded model checking based on SAT (Kupferschmid,
Lewis, Schubert, & Becker, 2011) and QSAT (Marin, Miller, & Becker, 2012).
As an alternative to dont touch variables, the preprocessor can be equipped with partial tracing
capabilities (Heyman, Smith, Mahajan, Leong, & Abu-Haimed, 2014). Thereby, the application
of the preprocessing rules is not restricted. Instead, information necessary to reconstruct a partial
model in terms of an assignment to the leftmost existential variables is collected during preprocessing.
The second category comprises methods to extract full satisfiability models. Reconstruction
steps for common preprocessing rules except the expansion of universal variables have been presented by Janota, Grigore, and Marques-Silva (2013). The effect of universal expansion cannot be
expressed solely by Q-resolution, in contrast to equivalence literal substitution, for example, and
hence causes complications. The QRAT proof system (Heule, Seidl, & Biere, 2014a; Heule et al.,
2014b) is the first framework to allow the extraction of full satisfiability models from formulas preprocessed using all currently implemented preprocessing techniques, including universal expansion.

8. Experimental Evaluation
Complementing the more theoretical analysis on the relationships and properties of the considered
clause elimination procedures, we now present results on an empirical evaluation of the effect of
applying clause elimination on the runtimes of state-of-the-art SAT and QSAT solvers. As benchmarks, we used standard competition benchmark sets from the most recent SAT Competition (SAT
Competitions Organizing Committee, 2014) and QSAT solver evaluation (Jordan & Seidl, 2014),
focusing on real-world application instances. As an overview of the results, it turned out that the
clause elimination procedures developed in this work, applied within preprocessing, have a clear
positive effect on the performance of various state-of-the-art QSAT solvers. On pure CNF SAT formulas, the effectswhile still positive or non-negative on the wholeare more modest, both when
applying clause elimination as preprocessing, as well as within inprocessing. This difference can
partly be explained as follows: SAT benchmarks are typically large and relatively easy considering
155

fiH EULE , J ARVISALO , L ONSING , S EIDL , & B IERE

their size, whiledue to the more succinct representation form enabled with the use of quantifiers
QSAT benchmarks are relatively small in terms of how hard they are to solve in practice. Although
the presented clause elimination techniques require polynomial timewhile the solving procedures
are exponentialthey can be at times expensive in practice, especially on very large CNF formulas
with millions of variables and clauses.5
All experiments were performed on a cluster of 2.8-GHz Intel Core 2 Quad machines each
equipped with 8-GB memory and running Ubuntu 9.04.
8.1 Effectiveness of Clause Elimination in the Context of SAT
We evaluated the effectiveness of clause elimination procedures in the context of state-of-the-art
SAT solvers, more specifically L INGELING version aqw (Biere, 2013), which won the application track in the SAT competition 2013. L INGELING heavily relies on the concept of inprocessing (Jarvisalo et al., 2012b). As already discussed in the introduction, inprocessing is based on
the idea of interleaving preprocessing, including clause elimination procedures, with search. The
inprocessing paradigm enables the use of facts learned during search, such as learned unit clauses,
in subsequent inprocessing phases, and the then simplified clauses again during search. Beside this
synergistic effect, inprocessing allows preprocessing algorithms to be pre-empted by search and
then resumed in the next inprocessing phase, to avoid getting stuck in a too-costly preprocessing
stages.
There is a difference in cost, in terms of running time, of specific inprocessing algorithms, as
well as the issue that certain simplification steps can actually be achieved with different inprocessing
techniques. As a consequence, it is extremely difficult to evaluate the effect of individual inprocessing techniques in isolation precisely. As an alternative, we investigated how the performance of the
competition version of L INGELING is affected when disabling (i) all clause elimination procedures,
(ii) all pre- and inprocessing techniques other than clause elimination procedures, and (iii) all preand inprocessing techniques. As the benchmark set we used the same instances as in the application
track of the SAT 2013 competition, with the time limit of 5000 seconds per benchmark, running
on hardware with almost identical speed as in the competition. In essence, the results show how
L INGELING would have performed in the competition without clause elimination in comparison to
using other or none of the pre- and inprocessing techniques applied within the solver.
We note that we conducted the same experiment also using the application track instances from
the SAT Competition 2014. While modest improvements (as shown in the following) on the 2013
instances can be observed, on the 2014 instances clause elimination procedures did not noticeably
improve (nor degrade) the performance of L INGELING.6 Hence here we present more details on
the results for the 2013 instances. In general, it appears that clause elimination procedures provide
only modest improvements for SAT solvers, but much more substantial improvement for QSAT
solversas we will demonstrate in the following.
5. In fact, in terms of worst-case complexity, it has recently been shown that, conditional to the so-called strong exponential time hypothesis (SETH) (Impagliazzo, Paturi, & Zane, 2001; Calabro, Impagliazzo, & Paturi, 2009) being
true, checking whether a given CNF formula contains a clause having AT cannot be done in sub-quadratic time, even
when restricting to Horn-3-CNF formulas (Jarvisalo & Korhonen, 2014).
6. We suspect that the differences in the 2013 and 2014 benchmarks are due to the benchmark selection procedure
applied by the competition organizers, which to an extent balance out performance differences between a set of topperforming solvers from the previous year. This benchmark selection procedure used in the main SAT competitions
during 20122014 is described by Balint, Belov, Jarvisalo, and Sinz (2015).

156

fiC LAUSE E LIMINATION

configuration
pre- & inprocessing disabled
only clause elimination enabled
base line without clause elimination
L INGELING version aqw (base line)

FOR

#sat
108
112
111
119

SAT

AND

#unsat
83
86
112
113

QSAT

#total
191
198
223
232

avg
1254
1209
1111
1124

total
784440
749324
632852
600691

 over solved formulas
 over all formulas

Table 2: L INGELING configurations on application instances from SAT Competition 2013.
Among other inprocessing algorithms this competition version of L INGELING implements the
following clause elimination procedures. A separate BCE inprocessor is scheduled during inprocessing if bounded variable elimination (VE) (Een & Biere, 2005) was run-to-completion at least
once. Note that BCE can be implemented much faster than variable elimination. However, since
the latter has often a more pronounced effect, we run VE until completion first, before applying
BCE. Further, as proposed by Han and Somenzi (2007), BCE can partially be performed on-the-fly
during VE. In the configuration without clause elimination we disable both variants of BCE.
The special case of on-the-fly subsumption of on-the-fly strengthening during conflict clause
learning (Han & Somenzi, 2009), can also be considered as a clause elimination procedure, but is
performed during search. Thus we keep it enabled in each of the configurations of L INGELING used
in this experiment. The same applies to subsumption elimination (SE) during VE (Een & Biere,
2005) and subsumed clauses found during lazy hyper binary resolution (Heule et al., 2013b). Another separate inprocessor performs transitive reduction of the binary implication graph. In practice,
transitive reduction is actually quite fast, but unfortunately in our experience does not give much
benefit in terms of solving times.
L INGELING aqw also contains an implementation of ACCE, which is rather costly and usually
never runs until completion. There is also another simple and partial variant of ATE, called basic
ATE (BATE). It is computationally inexpensive to detect some asymmetric tautologies (AT) during
(two-sided) literal probing (Le Berre, 2001), which is used as basic probing technique in various
inprocessing algorithms.
In the default configuration of L INGELING aqw, all the discussed clause elimination procedures not only wait until VE was completed at least once, as BCE does, but also require BCE
to be completed at least once (except for BCE obviously). Concretely, the configuration of L IN GELING with all the clause elimination procedures disabled is called with the following command
line options: --no-bate --no-block --no-cce --no-transred. By specifying --plain
we further compare a configuration where pre- and inprocessing is disabled with a configuration which only uses clause elimination procedures during pre- and inprocessing, using --plain
--bate --block --cce=3 --transred, which first disables all preprocessing and then selectively enables all clause elimination procedure, combined with --batewait=0 --blockwait=0
--ccewait=1, which makes sure that CCE is only started after BCE has run until completion (as
in the default configuration) and BCE is not delayed.
Results from the experiment are shown in Figure 4 and Table 2. The net result of this evaluation
is that the default configuration of L INGELING, as entered to the competition, with all clause elimination procedures enabled, solves 9 more instances; base line version solves 232 benchmarks but
only 223 without clause elimination. Out of the nine instances, eight are satisfiable, which seems to
157

fiH EULE , J ARVISALO , L ONSING , S EIDL , & B IERE

5000
4500

pre- & inprocessing disabled
only clause elimination enabled
base line without clause elimination
Lingeling version aqw (base line)

4000
Runtime (sec)

3500
3000
2500
2000
1500
1000
500
0
0

50

100

150

200

250

Number of solved formulas
Figure 4: L INGELING version aqw without clause elimination procedures solves 9 instances less
on the application track benchmark set of the SAT 2013 Competition with a time limit of
5000 seconds.

suggest that additionally applying clause elimination is beneficial especially on satisfiable instances.
The results suggest that there are in cases benefits to using clause elimination procedures, but the
improvements are not on the same scale as enabling or disabling VE (Een & Biere, 2005). Here
we note that, as explained in detail in (Jarvisalo et al., 2012a), while VE and BCE are to some
extent orthogonal in terms of the simplifications achieved on CNF formulas, they can perform various types of similar simplifications on their own. These include, for examples, various circuit-level
optimizations, such as cone-of-influence and monotone input gate reductions, as well as CNF level
techniques such as pure literal eliminations.
8.2 Effectiveness of Clause Elimination in the Context of QSAT
In the following, we empirically investigate the impact of PCNF-level clause elimination procedures
when applied as preprocessing in QSAT. To this end, we implemented those techniques in the QSAT
preprocessor BLOQQER (version 35) (Seidl & Biere, 2015). The core of BLOQQER is based on the
concept of resolve and expand realized in the QSAT solver Q UANTOR (Biere, 2005). Basically,
Q UANTOR is a complete solver using variable elimination to remove existential variables from
the innermost quantifier block and universal expansion to remove variables from the innermost
universal quantifier block. Depending on the benchmark family, this approach proved to be either
extremely efficient such that formulas hard to other solvers could be solved within a few seconds or
to be extremely memory-consuming. To overcome this limitation, we developed the preprocessor
158

fiC LAUSE E LIMINATION

900

SAT

AND

QSAT

GhostQ
bloqqer + GhostQ
DepQBF
bloqqer + DepQBF
QuBE
bloqqer + QuBE
RAReQS
bloqqer + RAReQS

800
700
Runtime (sec)

FOR

600
500
400
300
200
100
0
0

50

100

150

200

250

Number of solved formulas
Figure 5: Runtimes on the QBFLib Track Benchmarks of QBF Gallery 2014.
BLOQQER that applies the resolve and expand approach in a bounded manner. In BLOQQER ,
the preprocessed formula is rewritten in a way that a complete solver can then benefit from the
careful application of variable elimination and universal expansion which are repeatedly applied
until the formula either does not change any more or until some limits are reached. (Note that a
formula might already be solved in the preprocessing phase.) We integrated the clause elimination
techniques in BLOQQER such that they are applied during each cycle of variable elimination and
universal expansion.
For evaluating our implementation, we considered the benchmarks of the QBFLib track and of
the Application track of the QBF Gallery 2014 (Jordan & Seidl, 2014). The formulas included in
the QBFLib track are a selection of the QBFLib, the QBF community platform. This set contains
345 formulas from various benchmark families. In the competitive evaluation of the QBF Gallery
2014 only a subset of 276 formulas not directly solved by preprocessors was used. The benchmark
set of the Application track consists of 735 formulas from recently presented encodings to QSAT.
None of these formulas is directly solved by BLOQQER.
The time and memory limits were set to 900 seconds and 7 GB, respectively. Time spent on
preprocessing is included in the time limit for experiments that involve QSAT solvers. If the preprocessor does not terminate after 900 seconds, then preprocessing is aborted and the formula is
considered to be unsolved. We consider four participants of the QBF Gallery, which are publicly available: the CDCL-based solver D EP QBF (Lonsing & Biere, 2010; Egly, Lonsing, & Widl,
2013); the CEGAR-based solver RAR E QS (Janota, Klieber, Marques-Silva, & Clarke, 2012); the
G HOST Q solver (Klieber et al., 2010; Klieber, 2014) implementing a CEGAR-based approach in
combination with so-called ghost variables, allowing for duality-aware reasoning on the CNF level;
and the solver Q U BE that includes the preprocessor S Q UEEZE BF (Giunchiglia et al., 2010). This

159

fiH EULE , J ARVISALO , L ONSING , S EIDL , & B IERE

900
800

Runtime (sec)

700
600
500

GhostQ
bloqqer + GhostQ
DepQBF
bloqqer + DepQBF
QuBE
bloqqer + QuBE
RAReQS
bloqqer + RAReQS

400
300
200
100
0
0

100

200

300

400

500

600

Number of solved formulas
Figure 6: Runtimes on the Application Track Benchmarks of QBF Gallery 2014.
configuration
no preprocessing
no VE/Expansion
no QBCE
no QCCE
no asymmetric CE
full preprocessing

#sat
45
70 (2)
76 (27)
98 (32)
98 (28)
99 (33)

#unsat
74
72 (8)
91 (34)
98 (36)
94 (33)
98 (37)

#total
119
142 (10)
167 (61)
196 (68)
192 (61)
197 (70)

avg
56
66
47
42
53
38

total
210K
192K
168K
142K
148K
141K

vars
32925
32928
33306
33342
33310
33381

cls.
77710
36863
31776
28012
31642
27858

 average runtime over solved formulas
 total runtime over all formulas

Table 3: Different BLOQQER configurations with solver D EP QBF on QBFLib benchmarks.
preprocessor implements several techniques also included in BLOQQER as well as a special kind of
equivalence substitution, but no clause elimination procedures like BCE and CCE.
Each of these four solvers was run in its standard configuration with and without BLOQQER.
The results are shown in Figure 5 and Figure 6. For all solvers except G HOST Q preprocessing
was beneficial. While in the Application track preprocessing hardly had any effect on the runtime
of G HOST Q, in the QBFLib track the performance of G HOST Q was decreased by preprocessing.
G HOST Q relies on structural patterns in the CNF and the application of the preprocessor seems to
destroy these patterns.
Since BLOQQER implements many preprocessing techniques, the just-described experiment
does not directly indicate the power of PCNF-level clause elimination procedures. In order to
evaluate the impact of the various techniques, we run different configurations of BLOQQER in com160

fiC LAUSE E LIMINATION

configuration
no preprocessing
no VE/Expansion
no QBCE
no QCCE
no asymmetric CE
full preprocessing

#sat
155
174
189
196
175
192

#unsat
128
219
202
213
213
217

FOR

#total
283
393
391
409
388
409

SAT

AND

avg
62
36
60
69
41
70

QSAT

total
424K
322K
333K
322K
328K
322K

vars
11262
11283
11340
11356
11342
11358

cls.
227312
173487
186860
173454
174992
173402

 average runtime over solved formulas
 total runtime over all formulas

Table 4: Different BLOQQER configurations with solver D EP QBF on Application benchmarks.
bination with our solver D EP QBF. The results are summarized in Table 3 and Table 4. The tables
give the number of solved satisfiable and unsatisfiable formulas and well as average runtimes for
the solved formulas and the total runtime for the complete benchmark set. The last two columns
show the average number of variables and clauses of the original formulas for the no preprocessing configuration. For the other configurations, the average number of variables and clauses of
the preprocessed formulas is given. Table 3 additionally contains information about the number of
formulas directly solved by BLOQQER (the number of solved formulas in brackets). We ran the
following configurations: (i) no preprocessing, i.e., only the solver D EP QBF, (ii) variable elimination and expansion turned off but the clause elimination techniques turned on, (iii) blocked clause
elimination turned off, (iv) covered clause elimination turned off, (v) asymmetric clause elimination
techniques turned off, and (vi) all preprocessing techniques turned on.
For both benchmark sets, we observe that the application of BLOQQER is very beneficial for D E P QBF. The best performance is achieved when applying all preprocessing techniques. By turning
off variable elimination and universal expansion, we see that using only the clause elimination techniques is already beneficial for the solver. The results for different BLOQQER configurations show
that the clause elimination techniques considerably improve the runtimes and the number of solved
formulas, especially in the case of the QBFLib track benchmarks. In average, the application of
BLOQQER increases the number of variables. This is mainly due to universal expansion. However,
also for the configuration where expansion is disabled, we observe a modest increase of the number
of variables. This is because BLOQQER splits large clauses into smaller ones what turned out to be
beneficial for D EP QBF. Especially for the QBFLib track benchmarks, the application of BLOQQER
drastically decreases the number of clauses (more than 50 percent for most configurations).

9. Conclusions
Preprocessing and inprocessing (generally, formula simplification) techniques have proven important in speeding up state-of-the-art SAT and QSAT solving. Understanding the effects of and relationships between various simplification procedures is important for gaining a better understanding of the procedures. In this article, we focused on a specific type of preprocessing techniques,
clause elimination procedures that remove clauses from CNF and PCNF formulas based on different polynomial-time checkable redundancy properties. We introduced novel clause elimination
procedures for both CNF and PCNF formulas as asymmetric variants of the known techniques of
tautology, subsumption, and blocked clause elimination procedures, and additionally developed a
161

fiH EULE , J ARVISALO , L ONSING , S EIDL , & B IERE

novel family (including the plain and asymmetric variants) of so-called covered clause elimination procedures. We analyzed all of the variants from various perspectivesrelative effectiveness,
BCP-preserving, confluence, logical equivalencehighlighting intricate differences between the
procedures. This also resulted in a relative power hierarchy, reflecting the relative strengths of the
procedures in removing clauses. In terms of relative power, the asymmetric variant of covered
clause elimination dominates the plain procedures, and the novel covered clause elimination procedures are the most powerful ones among the considered procedures. Complementing the more
theoretical analysis, we presented results of an empirical evaluation on the practical effectiveness of
the procedures in speeding-up the overall solving runtime of state-of-the-art SAT and QSAT solvers
on real-world benchmark instances. The results show that, while the effects on the SAT-level are
modest, applying the clause elimination procedures is clearly beneficial in the context of QSAT
solving.
Many of the SAT-level clause elimination procedures have already been integrated as inprocessing techniques in a state-of-the-art SAT solver. An important aspect of future work would be to
integrate these procedures as inprocessing techniques into a QSAT solver. The motivation for doing
so is similar to that of the current state-of-the-art inprocessing SAT solvers: to speed up the satisfiability search further via interleaving applications of preprocessing techniques with the core search
routine. For example, clauses may become blocked during the solving process and then removed.
An additional question to investigate is whether it is possible to loosen the blocking criterion by taking variable dependencies into account. It would also be interesting to perform a thorough in-depth
study of clause elimination in the context of other generalizations of and formalisms related to SAT,
such as maximum satisfiability (MaxSAT) and the extraction of minimally unsatisfiable subsets
(MUSes) of CNF formulas. While there is some recent work looking into possibilities of applying
SAT-based preprocessing in the contexts of MUS and MaxSAT (Belov, Jarvisalo, & Marques-Silva,
2013a; Belov, Morgado, & Marques-Silva, 2013b; Berg, Saikko, & Jarvisalo, 2015)including the
use of BCEwe believe such directions have not yet been fully explored.

Acknowledgments
The authors would like to thank Donald Knuth for his comments that helped to improve the article.
The authors gratefully acknowledge financial support from DARPA contract number N66001-102-4087 (MH); Academy of Finland under grants 251170 COIN Centre of Excellence in Computational Inference Research, 276412, and 284591 (MJ); Austrian Science Foundation (FWF) NFN
Grants S11408-N23 RiSE (AB) and S11409-N23 RiSE (FL); and Vienna Science and Technology
Fund (WWTF) under grant ICT10-018 (MS).

References
Bacchus, F. (2002). Enhancing Davis Putnam with extended binary clause reasoning. In Dechter, R.,
& Sutton, R. S. (Eds.), Proceedings of the 18th National Conference on Artificial Intelligence
(AAAI 2002), pp. 613619. AAAI Press.
Balabanov, V., & Jiang, J.-H. R. (2011). Resolution proofs and Skolem functions in QBF evaluation and applications. In Gopalakrishnan, G., & Qadeer, S. (Eds.), Proceedings of the 23rd
162

fiC LAUSE E LIMINATION

FOR

SAT

AND

QSAT

International Conference on Computer Aided Verification (CAV 2011), Vol. 6806 of Lecture
Notes in Computer Science, pp. 149164. Springer.
Balint, A., Belov, A., Jarvisalo, M., & Sinz, C. (2015). Overview and analysis of the SAT Challenge
2012 solver competition. Artificial Intelligence, 223, 120155.
Belov, A., Jarvisalo, M., & Marques-Silva, J. (2013a). Formula preprocessing in MUS extraction.
In Piterman, N., & Smolka, S. A. (Eds.), Proceedings of the 19th International Conference on
Tools and Algorithms for the Construction and Analysis of Systems (TACAS 2013), Vol. 7795
of Lecture Notes in Computer Science, pp. 108123. Springer.
Belov, A., Morgado, A., & Marques-Silva, J. (2013b). SAT-based preprocessing for MaxSAT. In
McMillan, K. L., Middeldorp, A., & Voronkov, A. (Eds.), Proceedings of the 19th International Conference on Logic for Programming, Artificial Intelligence, and Reasoning (LPAR19), Vol. 8312 of Lecture Notes in Computer Science, pp. 96111. Springer.
Benedetti, M. (2005a). Extracting certificates from quantified boolean formulas. In Kaelbling, L. P.,
& Saffiotti, A. (Eds.), Proceedings of the 19th International Joint Conference on Artificial
Intelligence (IJCAI 2005), pp. 4753. Professional Book Center.
Benedetti, M. (2005b). sKizzo: A Suite to Evaluate and Certify QBFs. In Nieuwenhuis, R. (Ed.),
Proceedings of the 20th International Conference on Automated Deduction (CADE-20), Vol.
3632 of Lecture Notes in Computer Science, pp. 369376. Springer.
Benedetti, M., & Mangassarian, H. (2008). QBF-based formal verification: Experience and perspectives. Journal of Satisfiability, Boolean Modeling and Computation, 5(1-4), 133191.
Berg, J., Saikko, P., & Jarvisalo, M. (2015). Improving the effectiveness of SAT-based preprocessing for MaxSAT. In Proceedings of the 24th International Joint Conference on Artificial
Intelligence (IJCAI 2015). AAAI Press.
Biere, A. (2005). Resolve and expand. In Hoos, H. H., & Mitchell, D. G. (Eds.), Revised Selected Papers of the 7th International Conference on Theory and Applications of Satisfiability
Testing (SAT 2004), Vol. 3542 of Lecture Notes in Computer Science, pp. 5970. Springer.
Biere, A. (2013). Lingeling, Plingeling and Treengeling entering the SAT Competition 2013. In
Balint, A., Belov, A., Heule, M., & Jarvisalo, M. (Eds.), Proceedings of SAT Competition
2013, Vol. B-2013-1 of Department of Computer Science Series of Publications B, pp. 51
52. University of Helsinki.
Biere, A., Heule, M., van Maaren, H., & Walsh, T. (Eds.). (2009). Handbook of Satisfiability, Vol.
185 of Frontiers in Artificial Intelligence and Applications. IOS Press.
Biere, A., Lonsing, F., & Seidl, M. (2011). Blocked clause elimination for QBF. In Bjrner, N.,
& Sofronie-Stokkermans, V. (Eds.), Proceedings of the 23rd International Conference on
Automated Deduction (CADE 2011), Vol. 6803 of Lecture Notes in Computer Science, pp.
101115. Springer.
Brafman, R. I. (2004). A simplifier for propositional formulas with many binary clauses. IEEE
Transactions on Systems, Man, and Cybernetics, Part B, 34(1), 5259.
Bubeck, U., & Kleine Buning, H. (2007). Bounded universal expansion for preprocessing QBF. In
Marques-Silva, J., & Sakallah, K. A. (Eds.), Proceedings of the 10th International Conference
163

fiH EULE , J ARVISALO , L ONSING , S EIDL , & B IERE

on Theory and Applications of Satisfiability Testing (SAT 2007), Vol. 4501 of Lecture Notes
in Computer Science, pp. 244257. Springer.
Cadoli, M., Giovanardi, A., & Schaerf, M. (1998). An algorithm to evaluate quantified boolean
formulae. In Mostow, J., & Rich, C. (Eds.), Proceedings of the 15th National Conference on
Artificial Intelligence (AAAI 1998), pp. 262267. AAAI Press / The MIT Press.
Calabro, C., Impagliazzo, R., & Paturi, R. (2009). The complexity of satisfiability of small depth
circuits. In Chen, J., & Fomin, F. V. (Eds.), Revised Selected Paper of the 4th International
Workshop on Parameterized and Exact Computation (IWPEC 2009), Vol. 5917 of Lecture
Notes in Computer Science, pp. 7585. Springer.
Claessen, K., Een, N., Sheeran, M., & Sorensson, N. (2008). SAT-solving in practice. In Proceedings of the 9th International Workshop on Discrete Event Systems (WODES 2008), pp. 6167.
IEEE.
Cook, S. A. (1971). The complexity of theorem-proving procedures. In Harrison, M. A., Banerji,
R. B., & Ullman, J. D. (Eds.), Proceedings of the 3rd Annual ACM Symposium on Theory of
Computing (STOC 1971), pp. 151158. ACM.
Een, N., & Biere, A. (2005). Effective preprocessing in SAT through variable and clause elimination.
In Bacchus, F., & Walsh, T. (Eds.), Proceedings of 8th International Conference on Theory
and Applications of Satisfiability Testing (SAT 2005), Vol. 3569 of Lecture Notes in Computer
Science, pp. 6175. Springer.
Egly, U., Lonsing, F., & Widl, M. (2013). Long-distance resolution: Proof generation and strategy extraction in search-based QBF solving. In McMillan, K., Middeldorp, A., & Voronkov,
A. (Eds.), Proceedings of the 19th International Conference on Logic for Programming, Artificial Intelligence, and Reasoning (LPAR 2013), Vol. 8312 of Lecture Notes in Computer
Science, pp. 291308. Springer.
Fourdrinoy, O., Gregoire, E., Mazure, B., & Sas, L. (2007a). Eliminating redundant clauses in SAT
instances. In Hentenryck, P. V., & Wolsey, L. A. (Eds.), Proceedings of the 4th International
Conference on Integration of AI and OR Techniques in Constraint Programming (CPAIOR
2007), Vol. 4510 of Lecture Notes in Computer Science, pp. 7183. Springer.
Fourdrinoy, O., Gregoire, E., Mazure, B., & Sas, L. (2007b). Reducing hard SAT instances to
polynomial ones. In Proceedings of the 8th IEEE International Conference on Information
Reuse and Integration (IRI 2007), pp. 1823. IEEE.
Freeman, J. (1995). Improvements to propositional satisfiability search algorithms. Ph.D. thesis,
University of Pennsylvania.
Garey, M. R., & Johnson, D. S. (1979). Computers and Intractability: A Guide to the Theory of
NP-Completeness. W. H. Freeman.
Gershman, R., & Strichman, O. (2005). Cost-effective hyper-resolution for preprocessing CNF
formulas. In Bacchus, F., & Walsh, T. (Eds.), Proceedings of the 8th International Conference
on Theory and Applications of Satisfiability Testing (SAT 2005), Vol. 3569 of Lecture Notes
in Computer Science, pp. 423429. Springer.
Giunchiglia, E., Marin, P., & Narizzano, M. (2010). sQueezeBF: An effective preprocessor for
QBFs based on equivalence reasoning. In Strichman, O., & Szeider, S. (Eds.), Proceedings of
164

fiC LAUSE E LIMINATION

FOR

SAT

AND

QSAT

the 13th International Conference on Theory and Applications of Satisfiability Testing (SAT
2010), Vol. 6175 of Lecture Notes in Computer Science, pp. 8598. Springer.
Goultiaeva, A., & Bacchus, F. (2013). Recovering and utilizing partial duality in QBF. In Jarvisalo,
M., & Gelder, A. V. (Eds.), Proceedings of the 16th International Conference on Theory and
Applications of Satisfiability Testing (SAT 2013), Vol. 7962 of Lecture Notes in Computer
Science, pp. 8399. Springer.
Goultiaeva, A., Seidl, M., & Biere, A. (2013). Bridging the gap between dual propagation and
CNF-based QBF solving. In Macii, E. (Ed.), Proceedings of Design, Automation and Test in
Europe Conference & Exhibition (DATE 2013), pp. 811814. EDA Consortium / ACM DL.
Goultiaeva, A., Van Gelder, A., & Bacchus, F. (2011). A uniform approach for generating proofs
and strategies for both true and false QBF formulas. In Walsh, T. (Ed.), Proceedings of the
22nd International Joint Conference on Artificial Intelligence (IJCAI 2011), pp. 546553.
IJCAI/AAAI Press.
Han, H., & Somenzi, F. (2007). Alembic: An efficient algorithm for CNF preprocessing. In Proceedings of the 44th Design Automation Conference (DAC 2007), pp. 582587. IEEE.
Han, H., & Somenzi, F. (2009). On-the-fly clause improvement. In Kullmann, O. (Ed.), Proceedings
of the 12th International Conference on Theory and Applications of Satisfiability Testing (SAT
2009), Vol. 5584 of Lecture Notes in Computer Science, pp. 209222. Springer.
Heule, M., Jarvisalo, M., & Biere, A. (2010). Clause elimination procedures for CNF formulas.
In Fermuller, C., & Voronkov, A. (Eds.), Proceedings of the 17th International Conference
on Logic for Programming, Artificial Intelligence and Reasoning (LPAR-17), Vol. 6397 of
Lecture Notes in Computer Science, pp. 357371. Springer.
Heule, M., Jarvisalo, M., & Biere, A. (2013a). Covered clause elimination. In Fermuller, C., &
Voronkov, A. (Eds.), Short Paper Proceedings of the 17th International Conference on Logic
for Programming, Artificial Intelligence and Reasoning (LPAR-17), Vol. 13 of EasyChair
Proceedings in Computing, pp. 4146.
Heule, M., Jarvisalo, M., & Biere, A. (2013b). Revisiting hyper binary resolution. In Gomes, C. P.,
& Sellmann, M. (Eds.), Proceedings of the 10th International Conference on Integration of
AI and OR Techniques in Constraint Programming for Combinatorial Optimization Problems
(CPAIOR 2013), Vol. 7874 of Lecture Notes in Computer Science, pp. 7793. Springer.
Heule, M., Seidl, M., & Biere, A. (2014a). A Unified Proof System for QBF Preprocessing. In
Demri, S., Kapur, D., & Weidenbach, C. (Eds.), Proceedings of the 7th International Joint
Conference on Automated Reasoning (IJCAR 2014), Vol. 8562 of Lecture Notes in Computer
Science, pp. 91106. Springer.
Heule, M., Seidl, M., & Biere, A. (2014b). Efficient extraction of Skolem functions from QRAT
proofs. In Claessen, K., & Kuncak, V. (Eds.), Proceedings of the 14th International Conference on Formal Methods in Computer-Aided Design (FMCAD 2014), pp. 107114. IEEE.
Heyman, T., Smith, D., Mahajan, Y., Leong, L., & Abu-Haimed, H. (2014). Dominant controllability
check using QBF-solver and netlist optimizer. In Sinz, C., & Egly, U. (Eds.), Proceedings of
the 17th International Conference on Theory and Applications of Satisfiability Testing (SAT
2014), Vol. 8561 of Lecture Notes in Computer Science, pp. 227242. Springer.
165

fiH EULE , J ARVISALO , L ONSING , S EIDL , & B IERE

Impagliazzo, R., Paturi, R., & Zane, F. (2001). Which problems have strongly exponential complexity?. Journal of Computer and System Sciences, 63(4), 512530.
Janota, M., Grigore, R., & Marques-Silva, J. (2013). On QBF proofs and preprocessing. In McMillan, K. L., Middeldorp, A., & Voronkov, A. (Eds.), Proceedings of the 19th International
Conference on Logic for Programming, Artificial Intelligence, and Reasoning (LPAR-19),
Vol. 8312 of Lecture Notes in Computer Science, pp. 473489. Springer.
Janota, M., Klieber, W., Marques-Silva, J., & Clarke, E. (2012). Solving QBF with counterexample
guided refinement. In Cimatti, A., & Sebastiani, R. (Eds.), Proceedings of the 15th International Conference on Theory and Applications of Satisfiability Testing (SAT 2012), Vol. 7317
of Lecture Notes in Computer Science, pp. 114128. Springer.
Jarvisalo, M., & Biere, A. (2010). Reconstructing solutions after blocked clause elimination. In
Strichman, O., & Szeider, S. (Eds.), Proceedings of the 13th International Conference on
Theory and Applications of Satisfiability Testing (SAT 2010), Vol. 6175 of Lecture Notes in
Computer Science, pp. 340345. Springer.
Jarvisalo, M., Biere, A., & Heule, M. (2010). Blocked clause elimination. In Esparza, J., & Majumdar, R. (Eds.), Proceedings of the 16th International Conference on Tools and Algorithms
for the Construction and Analysis of Systems (TACAS 2010), Vol. 6015 of Lecture Notes in
Computer Science, pp. 129144. Springer.
Jarvisalo, M., Biere, A., & Heule, M. (2012a). Simulating circuit-level simplifications on CNF.
Journal of Automated Reasoning, 49(4), 583619.
Jarvisalo, M., Heule, M., & Biere, A. (2012b). Inprocessing rules. In Gramlich, B., Miller, D.,
& Sattler, U. (Eds.), Proceedings of the 6th International Joint Conference on Automated
Reasoning (IJCAR 2012), Vol. 7364 of Lecture Notes in Computer Science, pp. 355370.
Springer.
Jarvisalo, M., & Korhonen, J. H. (2014). Conditional lower bounds for failed literals and related
techniques. In Sinz, C., & Egly, U. (Eds.), Proceedings of the 17th International Conference
on Theory and Applications of Satisfiability Testing (SAT 2014), Vol. 8561 of Lecture Notes
in Computer Science, pp. 7584. Springer.
Jarvisalo, M., Le Berre, D., Roussel, O., & Simon, L. (2012). The international SAT solver competitions. AI Magazine, 33(1), 8992.
Jin, H., & Somenzi, F. (2005). An incremental algorithm to check satisfiability for bounded model
checking. Electronic Notes in Theoretical Computer Science, 119(2), 5165.
Jordan, C., & Seidl, M. (2014). QBF Gallery 2014. http://qbf.satisfiability.org/
gallery/.
Kleine Buning, H., & Bubeck, U. (2009). Theory of quantified Boolean formulas. In Biere, A.,
Heule, M., van Maaren, H., & Walsh, T. (Eds.), Handbook of Satisfiability, Vol. 185 of Frontiers in Artificial Intelligence and Applications, pp. 735760. IOS Press.
Kleine Buning, H., Karpinski, M., & Flogel, A. (1995). Resolution for Quantified Boolean Formulas. Information and Computation, 117(1), 1218.
Klieber, W. (2014).
Formal Verification Using Quantified Boolean Formulas
(QBF).
Ph.D. thesis, Carnegie Mellon University, available at http://reportsarchive.adm.cs.cmu.edu/anon/2014/CMU-CS-14-117.pdf.
166

fiC LAUSE E LIMINATION

FOR

SAT

AND

QSAT

Klieber, W., Sapra, S., Gao, S., & Clarke, E. M. (2010). A non-prenex, non-clausal QBF solver
with game-state learning. In Strichman, O., & Szeider, S. (Eds.), Proceedings of the 13th
International Conference on Theory and Applications of Satisfiability Testing (SAT 2010),
Vol. 6175 of Lecture Notes in Computer Science, pp. 128142. Springer.
Kullmann, O. (1999). On a generalization of extended resolution. Discrete Applied Mathematics,
9697, 149176.
Kupferschmid, S., Lewis, M. D. T., Schubert, T., & Becker, B. (2011). Incremental preprocessing
methods for use in BMC. Formal Methods in System Design, 39(2), 185204.
Le Berre, D. (2001). Exploiting the real power of unit propagation lookahead. Electronic Notes in
Discrete Mathematics, 9, 5980.
Liberatore, P. (2005). Redundancy in logic I: CNF propositional formulae. Artificial Intelligence,
163(2), 203232.
Lonsing, F., & Biere, A. (2010). DepQBF: A dependency-aware QBF solver. Journal of Satisfiability, Boolean Modeling and Computation, 7(2-3), 7176.
Lynce, I., & Marques-Silva, J. (2001). The interaction between simplification and search in propositional satisfiability. In CP01 Workshop on Modeling and Problem Formulation.
Mangassarian, H., Le, B., Goultiaeva, A., Veneris, A. G., & Bacchus, F. (2010). Leveraging dominators for preprocessing QBF. In Design, Automation and Test in Europe (DATE 2010), pp.
16951700. IEEE.
Manthey, N., Heule, M., & Biere, A. (2013). Automated reencoding of boolean formulas. In Biere,
A., Nahir, A., & Vos, T. E. J. (Eds.), Revised Selected Papers of the 8th International Haifa
Verification Conference (HVC 2012), Vol. 7857 of Lecture Notes in Computer Science, pp.
102117. Springer.
Marin, P., Miller, C., & Becker, B. (2012). Incremental QBF preprocessing for partial design verification - (poster presentation). In Cimatti, A., & Sebastiani, R. (Eds.), Proceedings of the 15th
International Conference on Theory and Applications of Satisfiability Testing (SAT 2012),
Vol. 7317 of Lecture Notes in Computer Science, pp. 473474. Springer.
Marques-Silva, J. (2008). Practical applications of Boolean satisfiability. In Proceedings of the 9th
International Workshop on Discrete Event Systems (WODES 2008), pp. 7480. IEEE.
Niemetz, A., Preiner, M., Lonsing, F., Seidl, M., & Biere, A. (2012). Resolution-based certificate
extraction for QBF - (tool presentation). In Cimatti, A., & Sebastiani, R. (Eds.), Proceedings
of the 15th International Conference on Theory and Applications of Satisfiability Testing (SAT
2012), Vol. 7317 of Lecture Notes in Computer Science, pp. 430435. Springer.
Ostrowski, R., Gregoire, E., Mazure, B., & Sas, L. (2002). Recovering and exploiting structural
knowledge from CNF formulas. In Hentenryck, P. V. (Ed.), Proceedings of the 8th International Conference on Principles and Practice of Constraint Programming (CP 2002), Vol.
2470 of Lecture Notes in Computer Science, pp. 185199. Springer.
Piette, C., Hamadi, Y., & Sas, L. (2008). Vivifying propositional clausal formulae. In Ghallab,
M., Spyropoulos, C. D., Fakotakis, N., & Avouris, N. M. (Eds.), Proceedings of the 18th European Conference on Artificial Intelligence (ECAI 2008), Vol. 178 of Frontiers in Artificial
Intelligence and Applications, pp. 525529. IOS Press.
167

fiH EULE , J ARVISALO , L ONSING , S EIDL , & B IERE

Pigorsch, F., & Scholl, C. (2010). An AIG-based QBF-solver using SAT for preprocessing. In
Sapatnekar, S. S. (Ed.), Proceedings of the 47th Design Automation Conference (DAC 2010),
pp. 170175. ACM.
Pulina, L., & Tacchella, A. (2009). A structural approach to reasoning with quantified boolean
formulas. In Boutilier, C. (Ed.), Proceedings of the 21st International Joint Conference on
Artificial Intelligence (IJCAI 2009), pp. 596602.
Robinson, J. A. (1965). A machine-oriented logic based on the resolution principle. Journal of the
ACM, 12(1), 2341.
Sabharwal, A., Ansotegui, C., Gomes, C. P., Hart, J. W., & Selman, B. (2006). QBF Modeling: Exploiting Player Symmetry for Simplicity and Efficiency. In Biere, A., & Gomes, C. P. (Eds.),
Proceedings of the 9th International Conference on Theory and Applications of Satisfiability
Testing (SAT 2006), Vol. 4121 of Lecture Notes in Computer Science, pp. 382395. Springer.
Samer, M. (2008). Variable dependencies of quantified CSPs. In Cervesato, I., Veith, H., &
Voronkov, A. (Eds.), Proceedings of the 15th International Conference on Logic for Programming, Artificial Intelligence, and Reasoning (LPAR 2008), Vol. 5330 of Lecture Notes in
Computer Science, pp. 512527. Springer.
Samulowitz, H., Davies, J., & Bacchus, F. (2006). Preprocessing QBF. In Benhamou, F. (Ed.),
Proceedings of the 12th International Conference on Principles and Practice of Constraint
Programming (CP 2006), Vol. 4204 of Lecture Notes in Computer Science, pp. 514529.
Springer.
SAT Competitions Organizing Committee (2014). The international SAT Competitions web page.
http://satcompetition.org/.
Schaefer, T. J. (1978). On the complexity of some two-person perfect-information games. Journal
of Computer and System Sciences, 16(2), 185225.
Seidl, M., & Biere, A. (2015). Bloqqer. http://fmv.jku.at/bloqqer.
Seidl, M., & Konighofer, R. (2014). Partial witnesses from preprocessed quantified Boolean formulas. In Proceedings of Design, Automation & Test in Europe Conference & Exhibition (DATE
2014), pp. 16. IEEE.
Subbarayan, S., & Pradhan, D. K. (2005). NiVER: Non-increasing variable elimination resolution
for preprocessing SAT instances. In Hoos, H. H., & Mitchell, D. G. (Eds.), Proceedings of the
7th International Conference on Theory and Applications of Satisfiability Testing (SAT 2004),
Vol. 3542 of Lecture Notes in Computer Science, pp. 276291. Springer.
Van Gelder, A. (2005). Toward leaner binary-clause reasoning in a satisfiability solver. Annals of
Mathematics and Artificial Intelligence, 43(1), 239253.
Van Gelder, A., Wood, S. B., & Lonsing, F. (2012). Extended failed-literal preprocessing for quantified boolean formulas. In Cimatti, A., & Sebastiani, R. (Eds.), Proceedings of the 15th
International Conference on Theory and Applications of Satisfiability Testing (SAT 2012),
Vol. 7317 of Lecture Notes in Computer Science, pp. 8699. Springer.
Zhang, L. (2006). Solving QBF by combining conjunctive and disjunctive normal forms. In Gil, Y.,
& Mooney, R. J. (Eds.), Proceedings of the 21st National Conference on Artificial Intelligence
(AAAI 2006), pp. 143150. AAAI Press.

168

fiJournal of Artificial Intelligence Research 53 (2015) 41-90

Submitted 03/14; published 05/15

Learning Relational Event Models from Video
Krishna S. R. Dubba
Anthony G. Cohn
David C. Hogg

krishna.dubba@gmail.com
a.g.cohn@leeds.ac.uk
d.c.hogg@leeds.ac.uk

School of Computing, University of Leeds,
Leeds, UK. LS2 9JT

Mehul Bhatt
Frank Dylla

bhatt@informatik.uni-bremen.de
dylla@informatik.uni-bremen.de

Cognitive Systems, SFB/TR 8 Spatial Cognition
University of Bremen, Bremen 28334, Germany

Abstract
Event models obtained automatically from video can be used in applications ranging
from abnormal event detection to content based video retrieval. When multiple agents are
involved in the events, characterizing events naturally suggests encoding interactions as
relations. Learning event models from this kind of relational spatio-temporal data using
relational learning techniques such as Inductive Logic Programming (ILP) hold promise,
but have not been successfully applied to very large datasets which result from video data.
In this paper, we present a novel framework remind (Relational Event Model INDuction)
for supervised relational learning of event models from large video datasets using ILP.
Efficiency is achieved through the learning from interpretations setting and using a typing
system that exploits the type hierarchy of objects in a domain. The use of types also
helps prevent over generalization. Furthermore, we also present a type-refining operator
and prove that it is optimal. The learned models can be used for recognizing events from
previously unseen videos. We also present an extension to the framework by integrating
an abduction step that improves the learning performance when there is noise in the input
data. The experimental results on several hours of video data from two challenging real
world domains (an airport domain and a physical action verbs domain) suggest that the
techniques are suitable to real world scenarios.

1. Introduction
With the advent of digital technology and wide availability of cameras and video recorders,
the quantity of video data has increased enormously over recent years, e.g., YouTube users
upload about 100 hours of video to the site every minute (YouTube, 2015). This data is
semantically rich but there is a lack of algorithms to process and utilize this data effectively.
There are a number of applications that demand video processing, especially event modelling
and recognition, such as content based video search, robotics, automatic description of
activities, video surveillance etc. The main objective of our work is to provide a supervised
relational learning framework to learn high level human understandable event models and
use them to recognize events in video. Supervised learning is the machine learning task of
inferring a model from labelled training data.
c
2015
AI Access Foundation. All rights reserved.

fiDubba, Cohn, Hogg, Bhatt & Dylla

Video is considered as a sequence of images and the area of video analysis poses several
challenges. The most interesting aspect of video when compared to images is that objects
(or parts of objects) in video can be perceived to move in space over time. These changes of
state in the space dimension are interesting and we call them events if they satisfy certain
properties such as being sufficiently frequent and having sufficiently well defined boundaries
etc. An event can be a change of state of a single object, such as moving some parts of its
body (for example people waving their hands) or it can be an interaction between multiple
objects. By interactions, in this context, we mean the movement of all the objects relative
to their surroundings as well as relative to each other. For example, an interaction between
two objects might be both objects moving towards each other or one at rest and the other
moving away from it. Before events can be recognized, we assume that the objects involved
have been detected and tracked from the source video. This is not a requirement in general
since some approaches (Laptev, 2005) do not require detection of objects prior to event
detection.
In events involving multiple objects, interactions between the objects become a distinguishing factor in recognizing the event instance. Capturing these interactions is the crux of
event modelling and recognition as it is our hypothesis that each event can be distinguished
by the interactions between the objects involved. Some events might have more than one
interaction pattern that identifies the event. One way to capture these interactions is to
abstract the interactions into relations between the objects. In order to represent the interactions of objects in an abstract form, we can use relations between the objects that depend
on the spatial configuration or motion pattern of objects over a period of time. We call
these spatio-temporal relations and in this paper we focus purely on the use of qualitative
spatial relations since these abstract away from the metric details of particular object trajectories and thus facilitate the recognition of interactions as being instances of some event
class (Cohn et al., 2006). There is no unique way to represent interactions using qualitative
spatio-temporal relations, and the best set of relations to use depends on the domain, kind
of data available (speed, orientation, size of moving objects, etc.) and the objectives of the
task.
Though each event class has distinguishing interaction patterns, there are two particular
challenges in event learning from examples expressed as qualitative spatio-temporal relations. Firstly, automatic object detection and tracking from video is not perfect and will
introduce errors into the relations. Secondly, the same event may be performed in different
ways.
1.1 Overview of the Framework
We follow the relational learning approach to the cognitive vision task of learning event
models from videos and using them for recognition (Cohn et al., 2006; Dubba, Cohn, &
Hogg, 2010). The video data (sequence of images with pixel data) is converted to relational
facts involving qualitative spatio-temporal relations using the tracking data of the objects
involved in the scenes. We use several qualitative spatial calculi to represent the video
data in relational form. Event instances are annotated temporally and spatially though the
objects involved in each event are not delineated separately and these annotations are used
for obtaining the positive and negative examples of events. The learning procedure as well
42

fiLearning Relational Event Models from Video

as the extension of this procedure using abduction (explained in later sections) is applied
on this relational data to obtain the event models. These event models are in the form of
Prolog rules that can be used as queries in the relational data from an unseen video. From
the answer substitutions we extract the spatial and temporal extensions of recognized event
instances.
The main contributions1 of the paper are:
 a novel supervised relational learning framework remind for learning event models
from video and recognizing event instances using these models.
 an optimal Type Refinement operator for upward refinement of hypotheses that exploits a type hierarchy in a domain for finding better event models.
 an extended framework to integrate induction and abduction in an interleaved fashion
with an embedded spatial theory for improving the learning of event models.
 an evaluation of the framework on two real world video data sets (aircraft turn-arounds
where the events include aircraft arrival, luggage loading and human interactions
where the events are common action verbs such as exchange, follow, dig etc).
Though we concentrate on relational data obtained from tracking objects from video,
the principles and techniques in this work equally apply to spatio-temporal relational data
acquired from non-visual sources (e.g. laser mapping, GPS tracks, textual descriptions etc).

2. Related Work
Much of the work in event analysis (Ivanov & Bobick, 2000; Medioni, Cohen, Bremond,
Hongeng, & Nevatia, 2001; Vu, Bremond, & Thonnat, 2003; Albanese, Moscato, Picariello,
Subrahmanian, & Udrea, 2007; Ryoo & Aggarwal, 2009, 2011; Morariu & Davis, 2011), does
not involve learning of the models used. Instead high level event models are hand-coded
using different representations (Nevatia, Hobbs, & Bolles, 2004; Hakeem, Sheikh, & Shah,
2004).
Techniques that are based on a similarity based metric in a space of low level pixel based
features such as local space-time features (Laptev, 2005) are frequently used for modelling
and recognizing events. These are generally more suitable for single agent events like human
activities based on motion. These kind of activities generally include a particular motion
signature with which an event can be recognized such as running, jumping, waving hands
etc. In some event recognition systems, hand-coded high level event models are used on top
of the learned low level human activity models (Ivanov & Bobick, 2000; Ryoo & Aggarwal,
2009, 2011).
One of the best performances to date in event recognition using low level pixel-based
features is obtained by the Stack convolutional Independent Subspace Analysis (ScISA) (Le,
Zou, Yeung, & Ng, 2011) algorithm. ScISA is based on pixel level flow based features
which are then used to model events using a hierarchical representation using deep learning
techniques (Bengio, 2009). The authors present an extension of Independent Subspace
1. This paper is an extended version of the work by Dubba et al. (2010, 2012).

43

fiDubba, Cohn, Hogg, Bhatt & Dylla

Analysis to learn invariant spatio-temporal features in an unsupervised fashion instead of
using predefined features.
If events are considered as a sequence of primitive states or events, state-space models
are useful in representing the event models. It is also easy to hand-code the structure of the
state space models, though the parameters are better if learned than encoded by hand. They
provide a more robust statistical event model than hand-coded models and event recognition
is done using inference on these models. Bayesian Networks are not very popular in event
modelling as they lack the temporal aspect though other state space models such as Hidden
Markov Models (HMM) (Rabiner, 1989) and Dynamic Bayesian Networks (DBN) (Ghahramani, 1998) are extensively used in event modelling and recognition. A simple HMM is not
very effective in modelling complex events. Several extensions of HMM are used to suit the
context and the type of event models. Hoogs and Perera (2008) proposed a DBN for jointly
solving event recognition and broken tracks linking problems. The event model is a set of
discrete states which expresses how the actors in the event interact over time. They assume
the states are strictly ordered and this may limit learning some events that involve complex
temporal relations such as during, overlaps etc.
The main problem with state space models is that it is difficult to encode high-level
temporal relations such as during, overlaps etc. The states or sub-events in an event are
assumed to be in a sequential order which is not the case in many domains. Also the
states are propositional in nature and hence are semantically less complex than a relational
representation.
Veeraraghavan et al. (2007) learn Stochastic Context Free Grammar based models from
traffic videos using predefined regions in the image. Each event model is a spatio-temporal
pattern of primitive actions expressed as a string, S = a1 , a2 , . . . , an . The event learning
algorithm aims to find a grammar that can generate the corresponding pattern for an event.
The primitive actions are sequentially arranged, hence Allens temporal relations are not
used to connect the primitive actions. Gupta et al. (2009) claim that the fixed structure of
the DBNs poses serious limitations for modelling events if there are many variations in the
way an event can happen. Instead they use AND-OR graphs for modelling event models.
The order of the nodes imposes the causal relationship among the nodes. Because of this,
some Allen relationships such as during, overlaps etc. cannot be modelled which limits its
application since modelling these relations is very important in many domains.
Though low-level features and state space models are popular for simple motion patterns,
it is possible to build high-level event recognition systems through several layers of reasoning.
These systems use simple pattern recognition techniques to detect primitive events and then
use a temporal structure to reason about complex events. The main motivation for using
a high level temporal structure is that the low level features (like bag-of-features) discard
most of the information regarding the relations between different entities in the data and
thus makes it hard to recognize events involving complex interactions between multiple
objects.
Moyle and Muggleton demonstrated using a simple blocks world that domain specific
axioms can be learned from temporal observations using an ILP framework (Moyle & Muggleton, 1997). In work by Needham et al. (2005), the Progol system (Muggleton, 1995)
was used to learn the protocols of table top games from real sensory data from a video camera and microphone. A key aspect of this work is a method for spatio-temporal attention
44

fiLearning Relational Event Models from Video

applied to the sensor data from audio and video devices. This identifies subsets of the sensor
data relating to discrete concepts. Symbolic description of the continuous data is obtained
by clustering within continuous feature spaces from processed sensor data. The Progol
ILP system is subsequently used to learn symbolic models of the temporal protocols present
in the presence of noise and over-representation in the symbolic input data. The framework
is based on time points and used only the successor temporal relation.
Konik and Laird (2006) proposed a learning by observation framework to learn an agent
program that mimics a human experts behaviour in domains such as games. The learned
concepts are used to generate behaviour rather than classification. They applied ILP techniques on artificially created examples from expert behaviour traces and goal annotations.
The relational data used is simple as each predicate is valid in a situation (an abstract time
point) and hence concepts with sophisticated temporal relations such as Allens interval
algebra (Allen, 1983) that use intervals cannot be learned. This limits the real world applicability of the framework where there are different events occurring in parallel and hence
requires using Allens interval algebra to model them. The framework uses only positive
examples and the negative examples are generated randomly in a controlled fashion.
Fern, Givan and Siskind (2002) introduced a system, leonard, that learns event definitions from videos by following a standard specific-to-general learning approach from only
positive data. There are seven simple event types that are learned in this system namely
pick up, put down, stack, unstack, move, assemble and disassemble. The relational data is
obtained by tracking objects in indoor scenarios. No negative examples are supplied and
the event models are found by computing the least-general covering formula (LGCF) of
each positive example and then computing the least-general generalisation (LGG) of all
these resulting formulae. When computing the LGCF of each example, the resulting LGCF
will not have any interval information. Hence the model can only support before and equal
temporal relations between states.
The important aspect to note for the above review is that most of the work in this area
has been done on either artificial or simulated data (Moyle & Muggleton, 1997; Konik &
Laird, 2006) or very simple real world data (Fern et al., 2002; Needham et al., 2005) that
involves few objects, the events are of short duration and all the objects in the scene are
involved in the events. In our case, the tracked data from videos is very large and at the
same time more complex and noisy and contains more objects.
Several attempts were made in the literature for integrating induction and abduction for
learning better theories. It was pointed out by Tammaddoni-Nezhad et al. (2006) that abduction and induction are integrated in general when two conditions hold: the background
knowledge is incomplete and the hypothesis language is disjoint from the observation language. The setting in which the latter condition holds is called non Observation Predicate
Learning (non-OPL) setting (i.e. in the OPL setting, the examples and hypotheses define
the same predicate). They assume the existence of a theory that connects the hypothesis
language and the observation language to start with. Since this theory is not learned, it
can be considered as the background theory. The general strategy in this case is to abduce (Kakas & Riguzzi, 2000) the missing observations using background theory and use
this abduced data for inducing new theories. Muggleton and Bryant (2000) proposed Theory Completion using Inverse Entailment (TCIE) in the non-OPL setting. TCIE abduces
and adds facts called the Start Set that connect the target predicate with observable pred45

fiDubba, Cohn, Hogg, Bhatt & Dylla

icates to the observation data and generalizes this data. In our case, the missing facts are
because of noise in the observed data and the set of target predicates is the same as the set
of observables whereas in TCIE, it is because the target predicate is not observable and the
set of target predicates and the set of observables are disjoint.
Moyle (2003) introduces an ILP system (alecto) that combines abduction and induction to learn theories for robot navigation. One limitation of this system is that it is
restricted to positive observations only learning. The integration is not interleaved in nature as abduction is first used to generate explanations for each example and induction is
applied on this set of explanations. This means the abduction phase does not take into
consideration the concepts learned in the induction phase and dealing with noise in data
was left as future work.

3. Relational Representation of Scenes in Video
To represent interactions of objects by relational data, we use spatial and temporal relations.
Since the input in this work is from video, the spatial relations are defined either in the
image plane or in the ground plane (if a homography is used to map the image plane to
the ground plane). The spatial relations are necessary to encode the state a particular pair
of objects is in. These states between two objects change as time progresses, hence we
need temporal relations to connect these states. In this section, we explain how objects
interactions are converted to relational data.
Notation: We use a first-order typed language (L) with the following alphabet: {, , ,
, , , ,R}. Let R = {r1 , r2 , . . . , rm } denote a set of m qualitative spatial relationships
in an arbitrary qualitative spatial calculus. There are sorts (and corresponding variables)
as given below (upper case letter denotes a set and lower case letter denotes a set element):
time points

T  t

time intervals

 

spatial objects

O o

events

E  

temporal relations

A 

object types

 

The special event-predicate tran(ri , ok , ol , tm )  E denotes a transition from a spatial
relation ri between objects ok and ol at time point tm . Note that in this work,  can only
take values from the set of 13 Allens base relations (Allen, 1983) i.e. A = {before, after,
meets, met by, overlaps, overlapped by, during, contains, equals, finishes, finished by, starts,
started by}. We say two intervals are disjoint if the Allen relation between them is from
the set {before, after, meets, met by}.
3.1 Spatial Relations
In order to get a high level description of the interactions of objects in videos, we need relations that can encode the interactions of objects without loss of essential information (Cohn
et al., 2006). There are several possibilities on what kind of relations we have to choose.
Since the interactions of objects in video take place in spatial dimensions, it is natural to
46

fiLearning Relational Event Models from Video

  

?







?





?









  

Figure 1: Qualitative Trajectory Calculus (QT CL1 ) (Van de Weghe et al., 2006): Each
blob is a possible QT CL1 spatial relation. In a blob, an asterix (left object) or circle (right
object) represent objects in motion while a star and black dot represent objects at rest and
the direction of the arrow shows the direction of motion of the object. For example, the
top-left ellipse is interpreted as two objects moving towards each other and the bottom-left
ellipse is interpreted as the right object is moving away from the left object while the left
object is moving towards the right object (i.e the left object is chasing the right object).
Though nine relations are possible in QT CL1 as shown in the figure, in practice we can
reduce them to six exploiting symmetry in some relations. When only one object changes
its motion state (note that an object cannot change direction without going through the
rest state), the QTC relation changes along a thick line connecting two relations. When
both objects change motion state instantaneously, the relation changes along a dotted line.

use qualitative spatial calculi to model the interactions. These interactions also have a
temporal dimension as they occur over a period of time, so we extend the spatial relations
with arguments modelling the temporal dimension. When we say interactions of objects we
mean the interactions of the bounding boxes2 (aligned to the axes) of these objects that we
get from tracking the objects using computer vision algorithms (Yilmaz, Javed, & Shah,
2006). There are different kinds of spatial calculi that target different aspects of object
interactions like topology, orientation, direction, trajectories etc. and which calculi to use
is a domain dependent choice (Chen, Cohn, Liu, Wang, Ouyang, & Yu, 2015). We primarily use three spatial relations that encode the object interactions at the topological level:
dc (Discrete) when the intersection of pixels in the bounding boxes of two objects is empty,
in (Inside) when the intersection of pixels is the same as the pixels in the bounding box of
one of the objects and touch in every other case. This set of simple topological relations
is an abstracted version3 of RCC-8 (Randell, Cui, & Cohn, 1992) spatial calculus, reduced
for practical purposes without loss of essential information for event analysis. We also use
QT CL1 (Van de Weghe et al., 2006) (Fig.1) and domain specific relations as primitives to
represent the interactions of objects in the videos.
2. In principle, other shape abstractions could be used as well, e.g. convex hulls, silhouettes, bounding ovals
etc.
3. The other two relations in this version of RCC called RCC-5 are equal and contains (inverse of in). The
relation equal rarely occurs in our experiments and we do not use contains as we can convert it to in by
reversing the arguments.

47

fiDubba, Cohn, Hogg, Bhatt & Dylla

o1

o1

o1
o2

o2

1

3

2

dc

o2

touch

in

dc(o1 , o2 , 1 )
touch(o1 , o2 , 2 )
in(o2 , o1 , 3 )
meets(1 , 2 )
meets(2 , 3 )
before(1 , 3 )

Figure 2: Converting interactions of objects to relational data.
3.2 Temporal Interval Relations
We can define temporal relations between time intervals based on Allens interval algebra.
We use start and end frames of an interval to represent the intervals. An advantage of this
approach is, we do not have to precalculate temporal relations and store them beforehand in
the database for inference. Instead, Prolog rules that calculate temporal relations given start
and end time points of two intervals are used. In order to incorporate temporal information
in describing a scenario, we extend the spatial relations with a temporal interval as an extra
argument.
3.2.1 Temporally Extending a Spatial Relation
The state where a spatial relation r between objects o1 and o2 holds throughout an interval 
is represented as r(o1 , o2 , ) where r  R , o1 , o2  O and   . Grounding this expression
with objects and intervals from a database will provide us with spatio-temporal facts.
A temporal relation between two spatio-temporal facts is the Allen relation between the
intervals in the spatio-temporal facts.
3.3 Representing an Event Class
An event class is represented by a set of Horn clauses where the head predicate is the same
as the event name under consideration and the body is a non empty conjunction of atoms
consisting of spatial and temporal predicates.
The structure of each clause in an event model for an event class  is as follows:
() :  1 , . . . , i , . . . , n
where each i is either of the form r(o1 , o2 , ) where r  R, o1 , o2  O and   , or is
of the form (1 , 2 ) where   A and 1 , 2  .
1
48

fiLearning Relational Event Models from Video

4. Deictic Supervision
For supervised learning, we need positive and preferably negative examples too of event
instances. One major problem in supervised learning is collecting the labelled training
data. Because of the general ambiguity in defining the spatial and in particular the temporal
extent of an event (i.e. where do events precisely start and finish), it is difficult to annotate
videos with event labels. A possible approach is to annotate the objects involved in each
event and give the events temporal extent. But annotating objects is tedious and prone
to human error and for some events there may be uncertainty in the objects involved. We
can avoid this by using Deictic Supervision (Dubba et al., 2010). Instead of annotating the
exact objects involved in the training event instances, we only give a bounding spatial and
temporal extent of each event instance which may contain other objects. The spatial extent
is a region indicating where the event is happening in the video. The temporal extent is
an interval which includes the actual temporal extent of the event, but may be deliberately
longer in order to avoid accidentally truncating state changes relevant for the event. This
makes preparation of training data easier and the learning process more robust and less
biased to the labelling and the learning algorithm should be able to induce reasonable
models even with this data.
Delineating spatio-temporal volumes in videos from which to learn feature-based representations of actions such as hand gestures is not without precedent in the computer vision
literature (Laptev & Perez, 2007), but our use here extends it to multiple simultaneous actors and relational descriptions and resilience to perturbations in the placement of cuboids
provided events are fully enclosed.
4.1 Deictic Interval and Region
In this work, a deictic spatial region is a rectangle on the image plane indicating where
the event happened and the deictic interval is a time interval indicating when the event
happened. A deictic spatial region is obtained by hand-delimiting the event in the image
plane with a rectangle4 , hence can be represented using a coordinate point (top-left corner
vertex), height and width of the rectangle (x, y, h, w). A deictic temporal interval is provided
by specifying the start and end time points of the interval. Together they define a space-time
cuboid which delimits the spatial and temporal extension of an event.
A deictic cuboid defines a set of spatial facts and temporal relations between them;
an event instance is a subset of these facts and corresponds to a positive example in the
learning from interpretations setting. Obtaining positive and negative examples for learning
using event annotations in the form of deictic spatial regions and deictic temporal intervals
is explained in the following sections. Note that the deictic interval and region are regarded
as any other interval and object in  and O respectively and the spatial relations are
computed accordingly. After the positive and negative examples are computed, the spatial
relations involving the deictic regions as one of the objects are discarded from the database
as they are of no further use.
4. If the tracking data is on the ground plane then we can back-project this rectangle automatically into a
minimum enclosing rectangle on the ground plane using homography (Hartley & Zisserman, 2004).

49

fiDubba, Cohn, Hogg, Bhatt & Dylla

4.2 Herbrand Interpretation for an Event
Let si and i be the deictic spatial region and deictic temporal interval for an instance i of
an event class  in video v. Let v be the set of spatio-temporal facts present in v, Ov be
the set of objects in v and v be the set of all time intervals in v. The set of facts Ei  v
is the Herbrand Interpretation for the event i over v iff all the facts contained in it are
entailed by v , whose temporal intervals are not disjoint with the deictic interval and whose
objects have relation touch or in within the deictic region.

Ei = {r(o1 , o2 , ) : v  r(o1 , o2 , ) 
v  (i , ) where  
/ {before,after,meets,metby} 
1 [v  r1 (si , o1 , 1 ) where r1  {touch,in}
v  1 (i , 1 ) where 1 
/ {before,after,meets,metby}]
2 [v  r2 (si , o2 , 2 ) where r2  {touch,in}
v  2 (i , 2 ) where 2 
/ {before,after,meets,metby}]
o1 , o2 , si  Ov  r  R  1 , 2  v
}

An example interpretation for an event instance of AFT Bulk LoadUnload in the Airport
domain is illustrated in Fig.3. The interpretation includes all those spatial facts involving
objects that have the relation touch or in with the deictic region and which lie within
the two vertical dashed lines (the deictic interval). The set of Herbrand Interpretations
corresponding to the set of deictic regions and intervals for an event form the positive
examples for the learning phase. The rest of the relational facts in each video form the
negative example where if an event model fires an instance in the database, it is considered
as a false positive. When a Herbrand Interpretation is extracted from a set of spatiotemporal facts of a video, this interpretation is independent of all the other facts in the
spatio-temporal database5 for the video and hence other facts can be assumed false from
this interpretations point of view.
Note that by definition the spatio-temporal facts that do not spatio-temporally overlap
the deictic region and interval of an event instance are not relevant to the event. Considering
facts outside the indicated event occurrence not only increases the size of the training data
but also makes the example instances for different event classes less distinct.
One limitation of using a cuboid shaped deictic region for delineating an event instance
is that it is not possible to differentiate among multiple co-occurring instances of the same
event type involving different objects in a region. One way to overcome this limitation is to
use more than one cuboid to enclose an event instance allowing the elimination of unwanted
facts.
5. This spatio-temporal database is itself a subset of the Herbrand Base of the video obtained using the
predicates (spatio-temporal relations) and the constants (objects and time intervals) from the video.

50

fiLearning Relational Event Models from Video

Figure 3: An example interpretation for the event AFT Bulk LoadUnload in the Airport
domain. The vertical black lines are the start and end of the deictic interval. Each row
represents the interactions of two of the objects present in the deictic region during the
deictic interval in the video. The colours of the lines represent the spatial relations between
the pairs of objects at that point of time. This figure does not show the effect of a deictic
spatial region, but this would correspond to the elimination of certain rows (where the
objects do not have a spatial relation of touch or in whilst in the deictic spatial region
during the deictic temporal interval.
4.3 Herbrand Interpretation for a Non-event Interval (Negative Example)
In our framework, the negative examples are not explicitly labelled. The negative example
for a given event in a video is the set of spatio-temporal facts from the database of the
video that are not present in the positive examples of the event in that video. Note that
the negative example will in general contain data that might be in positive examples of
other event classes in that video. Another alternative is to use labelled positive examples
of other events as negative examples for the event we are learning. This is convenient for
classification purposes but not in recognition tasks as this will miss the background data
that might be useful to minimize detections in the background regions.
Let v be the union of all spatio-temporal facts from all Herbrand Interpretations of
event  in video v. The set of facts NIv  v is the Herbrand Interpretation for a negative
example for event  in video v iff it contains all the facts in v which are not in v , i.e.,
NIv = v  v .

5. Typed ILP
In any event learning and recognition system, low level image processing and computer
vision techniques may introduce noise into the system. One kind of noise, in particular
when video quality is bad as in videos from some CCTVs, is that the wrong type may be
assigned by the tracker to an object history. An object detector is typically trained with
51

fiDubba, Cohn, Hogg, Bhatt & Dylla

Person
Aircraft
GPU
Transporter

Object

Light Vehicle
Push Back
Service Vehicle
Mobile Stairs
Vehicle
Loader
Conveyor Belt
Passenger Boarding Bridge
Heavy Vehicle
Container
Catering
Tanker
Bulk Loader

Figure 4: Tree-structured object type hierarchy in the Airport domain.
many example images of objects to be detected. Even though many example images are
given for training, it is not possible to capture all the possible ways an object can appear
because of lighting, viewing direction, size, shape, etc. (Lowe, 2004). This may result in
correctly localized objects but with the wrong categories of the objects, especially those
that look visually similar in low contrast images.
When the input data is huge and noisy, there are several problems an ILP system can
face. One of these is that hypothesis evaluation can take a lot of time because of the size of
the data. Also the noise will tend to make the hypothesis over specific as the system learns
more rules to cover the inconsistent examples. Using a typed ILP system can speed up
evaluation because of typed arguments in the hypothesis (Walther, 1985; Cohn, 1989) and
also reduce the number of false positives through avoiding certain cases where the types of
the arguments do not match. Any event model that has objects with a specialized type will
fail to recognize some event instances where the object appears with a different type. In
contrast, if the event model does not have any type system and uses a very generic type for
all the objects such as object, thing etc., then this approach will have many false positives
as it cannot differentiate between events with same structure but involving different types
of objects. One possible approach is to find an appropriate type generalization instead of
using one of the two extremes: most generic type and most specialized type.
In most ILP systems, the type hierarchy of objects is not integrated into the learning
process. For example, in Progol, types of the objects are used only in mode declarations
and since it assumes a flat type hierarchy of the domain, the search procedure cannot
take any type hierarchy into consideration. For example, if the tracking system sometimes
confuses two types of objects (1 , 2 ) such that some objects of type 1 are misclassified as
52

fiLearning Relational Event Models from Video

s2
s1

s4
s3
s5

Figure 5: Tree-structured example object type hierarchy. s1 is the most general type and
s2 , s4 , s5 are the most specific types.
type 2 , then Progol generates two rules, one with 1 and another with 2 . Even if we
are not dealing with a vision system that introduces noise into the high level learning and
reasoning system, in some cases an event might involve objects from a particular sub-group
of objects. In this case, instead of using a very generic type like object or very particular
types like the type of the object itself, it is more efficient to use an intermediate generic type
that represents this sub-group. A variable without type restrictions will be satisfied by any
type of object when instantiating the Horn clause. However, an appropriate generalization
can be enforced by the learning system with a variable of type 1 t2 from the type hierarchy,
which is satisfied6 only by objects of type 1 or 2 , thereby reducing false positives.
5.1 Representing a Typed Hierarchy
If we wish to use an existing Prolog engine for hypothesis evaluation then some way of
encoding type using terms must be found. There are several ways of doing this depending
on whether the structure of the object type hierarchy is a tree or a lattice. We use the
type representation proposed by Bundy, Byrd and Mellish (1985) that can deal with tree
structured type hierarchies; we then develop a refinement operator by incorporating this
representation in the hypothesis search procedure. An advantage of using this representation
is that ordinary unification can be used to determine whether two types are compatible.
We will write i < j , if i is a subtype of j and i 6= j . Every object o of type n
in a hypothesis can be represented by the term 1 (2 (. . . n (o) . . .)) where 1 , . . . , n is the
maximal sequence of types such that n < . . . < 2 < 1 . We denote this representation
function by . Note that we need a constraint, i.e. a tree structured type hierarchy, in
order to guarantee the uniqueness of the sequence 1 , . . . , n .
For example, let s1 , s2 , s3 , s4 , s5 be types such that s4 < s3 < s1 , s5 < s3 < s1 and
s2 < s1 as shown in Fig.5. Then any object o of type s4 can be represented as follows:
(o) = s1 (s3 (s4 (o)))

An object oi is not compatible with an object oj in a hypothesis if (oi ) is not unifiable
with (oj ). For example: s1 (s3 (o1 )) will not unify with s1 (s2 (o2 )) but will unify with
s1 (s3 (s4 (o3 ))) and s1 (s3 (s5 (o4 ))), hence they are compatible.
5.1.1 Example of Representing a Type Hierarchy
An object type hierarchy that occurs in one of the two domains used in the evaluation
section of this work is shown in Fig.4. The hierarchy from Fig.4 is hand defined based
6. A variable of type 1 t 2 can unify with a term of type 1 or 2 .

53

fiDubba, Cohn, Hogg, Bhatt & Dylla

on observed errors in object classification of the tracking data from the Airport domain.
In the airport domain, the ground power unit (GPU), transporter and push back vehicle
are small vehicles that look similar as the videos from CCTV cameras in the airport have
low resolution contrast without much colour or sharp edges. This makes it challenging to
train an object detector and use it for detecting objects in these videos. The objects from
the Verbs domain present no particular challenges from an automatic classification point
of view but some events involve objects from a particular subset of objects, for example,
the throw event involves balls of different types like small ball, basket ball, etc. Hence using
a type hierarchy based on utility is expected to help find event models that have better
performance in detecting events in unseen videos.
A vehicle V of type GPU will be represented as obj(veh(light veh(gpu(V ))))7
while V of type light veh is represented as obj(veh(light veh(V ))). Note that
obj(veh(light veh(V ))) unifies with vehicles of type GPU and vehicles of type Transporter. So using obj(veh(light veh(V ))) in a model can cover examples that either
involve a GPU or a Transporter and hence can handle the noise from an object detector if
it confuses these vehicles by outputting GPU in place of Transporter or vice versa.
5.2 Type Refinement Operator
A refinement operator is used to traverse through the hypothesis lattice. There are two
types of refinement operators: upward and downward (Nienhuys-Cheng & De Wolf, 1997).
We write Hg  Hs if Hg is a more generic8 hypothesis than Hs . If we assume that the
top most element of the hypothesis lattice is the most generic hypothesis and the bottom
most hypothesis is the most specific hypothesis, then the upward refinement operator can be
defined as follows (the downward refinement operator can be defined in a similar fashion):
Let L be the set of all possible hypotheses. An (upward) refinement operator  is defined
such that for a hypothesis H,  produces only generalizations of H, (H) = {Hg | Hg 
H, Hg  L}.
We define the (upward) Type Refinement operator t as an operator that generalizes
the object types of H. Apart from object types, the structure of H and members of t (H)
is identical.
We define a type generalizing operator as follows:
generalize type(1 (2 (. . . n1 (n (o)) . . .))) = 1 (2 (. . . n1 (o) . . .))
The Type Refinement operator, t , applies the generalize type operator to a selected
object type present in a hypothesis, resulting in a more generic hypothesis by moving up
exactly one level in the type hierarchy.
Though the specific current representation of type hierarchy using functors requires a
tree structured hierarchy, having a tree structured hierarchy is beneficial from a computational viewpoint in limiting the type generalizations, i.e., there are no multiple ancestors.
A tree-like type hierarchy is very natural in many domains though some domains might not
7. Note the short forms used for Object, Vehicle and Light Vehicle.
8. There are several possible generality orders, most important are subsumption and logical implication (Nienhuys-Cheng & De Wolf, 1997).

54

fiLearning Relational Event Models from Video

have a well defined tree-like object type hierarchy. In such cases, a lattice structured type
hierarchy is more suitable though this will increase the size of the search space since the
number of possible refinements is increased, in particular in a tree structure type generalization is deterministic whilst this is not the case in a lattice structure.
5.2.1 Optimality of the Type Refinement Operator
Refinement operators can be ideal or optimal (Nienhuys-Cheng & De Wolf, 1997)9 . An
optimal refinement operator generates any hypothesis in the hypothesis lattice only once
and there is a unique way to produce each hypothesis. This kind of refinement operator is
desirable in complete search algorithms as duplicate generation of hypotheses will increase
the cost of the search procedure. The optimality of the Type Refinement operator is proved
in Appendix A.

6. Learning from Interpretations Setting for Learning Event Models
The result of deictic supervision gives us examples that are sets of spatio-temporal facts.
Though these examples (sets of facts) come from the same or different videos, they are
independent of each other, i.e., the mapping of each example to a class is independent of
other examples. For this kind of learning setting where each example is independent of
each other and each example is a set of facts, the learning from interpretations setting is
an apt choice (Blockeel, De Raedt, Jacobs, & Demoen, 1999). The setting can be specified
formally thus:
Given:
 a set of classes C (each class label c is a nullary predicate).
 a set of classified examples E (each element of E is of the form (Ei , c) with Ei a set
of facts and c a class label)
 and a background theory B,
Find : a hypothesis H (a set of Horn clauses), such that for all (Ei , c)  E:
 H  Ei  B  c, and
 c0  C  {c} : H  Ei  B 2 c0
In the current event learning problem, the above setting is applied for each event class
where in each case the set of classes has only two elements, the event class and the background class. Background class represents the negative examples and each class label c is a
nullary predicate.
9. An ideal refinement operator is proper and complete whereas an optimal refinement operator is weaklycomplete and non-redundant. See Appendix A for formal definitions.

55

fiDubba, Cohn, Hogg, Bhatt & Dylla

6.1 Traversing the Search Space
The search process for a hypothesis starts with an initial hypothesis which has a nullary
predicate as head and an empty body. The hypothesis lattice is traversed using the Progol
and Type Refinement operators in an interleaved fashion. The Progol refinement operator
is a specialization operator that adds atoms from the bottom clause to a hypothesis. A
specialization operator moves from the top (empty clause) to bottom in the hypothesis
space which is a lattice bounded by the bottom clause from the bottom. Adding atoms
from the bottom clause makes the hypothesis more specialized because the body of the
hypothesis is a conjunction of atoms and each atom can be considered as a constraint.
Adding more atoms to the body increases the constraints it has to satisfy to become true.
The Progol refinement operator that we use here is based on the bottom clause also
called the most-specific clause and is non-redundant though it is not weakly-complete with
respect to the general subsumption order (Tamaddoni-Nezhad & Muggleton, 2009). The
most-specific clause that the Progol refinement operator uses can be computed from training examples, mode declarations and the background knowledge (Muggleton, 1995). Mode
declarations are user defined syntactic biases in the form of predicates that specify what
predicates from the background knowledge are expected in the target hypothesis and also
the nature of the variables (input, output, or constant). The selection of atoms to be
added to the hypothesis from bottom clause is done in a controlled manner. The atoms
are only considered starting from left and moving to the right and each atom can be added
only once (Tamaddoni-Nezhad & Muggleton, 2009). These constraints on the selection of
atoms makes the refinement process non-redundant, i.e., a hypothesis is not generated twice.
There is an additional refinement operator that refines by unifying two variables arbitrarily
selected from the hypothesis or by substituting a variable with a constant. We do not use
this operator as unifying two variables needs checking the hypothesis for consistency with
respect to the underlying spatial theory and there are no fixed constants (apart from frame
numbers) in the domain as constants in any example are independent from constants from
other examples. For example, consider the three relations from Section 4.1 for the spatial
theory and Allens relations for the temporal relations: we cannot unify two arguments of
a predicate (spatial or temporal) as this violates the semantics of these relations.
The Type Refinement operator generalizes a hypothesis by generalizing the type of an
object in the hypothesis (Fig.6). There are two possible approaches to apply the Type
Refinement operator: type first approach is to select a type from the set of types for a
hypothesis and generalize the type of the variables that belong to the selected type and
variable first approach is to select a variable from the hypothesis and generalize the type
of all occurrences of this variable in the hypothesis. The type first approach generalizes the
selected type throughout the hypothesis and this may involve several variables while the
variable first approach only generalizes the type of one variable. In our work, we use the
type first approach as this has a fewer number of refined hypotheses and a smaller search
space while the variable first approach has a larger number of choices and hence a larger
search space. One more reason to use the type first approach is that the computer vision
algorithm might confuse the type of a whole group of objects that belong to a particular
type rather than a single object in a video because of an inaccurate object detector.
56

fiLearning Relational Event Models from Video

Figure 6: Type refinement operator (generalization).
6.2 Searching the Event Model
Once the most-specific clause is computed, the sub-lattice bounded from below by the mostspecific clause is searched using a best-first search for the hypothesis that has a maximum
score calculated based on a combination of (1) the number of positive examples covered, (2)
the number of answer substitutions in the negative examples, (3) the length of hypothesis
and (4) the number of distinct variables in the hypothesis subject to the given constraints
(discussed in the next subsection).
score(H) =   p  (%  n + l + v)
where
 = weight to positive examples
p = number of positive examples covered
% = weight to answer substitutions in negative examples
n = number of answer substitutions in negative examples
l = length of the hypothesis
v = number of distinct variables in the hypothesis
An answer substitution  for an example e is a substitution that grounds the hypothesis,
h  b1 , . . . , bn , and the query  b1 , . . . , bn  succeeds in the database of e. Note that in
the learning from interpretations setting each positive example is a separate database and
when a hypothesis is used as a Prolog query in each database, it might result in multiple
answer substitutions. An example is considered covered by the hypothesis if there are one
or more answer substitutions. While testing the hypothesis in a test database, each answer
substitution is considered as one recognized event instance. If the recognized event interval
falls outside the event ground truth in a test video, it is considered as false positive. In the
event recognition domain, the hypothesis is used for recognizing events in unseen videos
57

fiDubba, Cohn, Hogg, Bhatt & Dylla

instead of classifying videos, a hypothesis with fewer false positives is desirable. Hence
hypotheses are penalized using the number of answer substitutions10 in negative examples.
If the number of positive and negative examples are disproportionate in numbers, giving
more weight to the positive examples and negative examples using  and % will result in an
hypothesis that has better performance in test data.
Since the starting hypothesis is empty and completely generic, it will cover all the
negative examples. As the hypothesis is specialized by Progols refinement operator, the
number of false positives decreases. When the score of the hypothesis no longer increases, the
Type Refinement operator is used to generalize the types thereby increasing the generality
of the hypothesis with a possible increase of positive examples covered (as well as false
positives). This process of interleaved application of both operators is continued until the
hypothesis score no longer increases.
Once a satisfactory hypothesis is found, an argument representing temporal information
in the form of a list of time intervals formed using the time intervals in the body of the
event model can be introduced into the head in order to explicitly represent when the event
occurs  this is useful when using the hypothesis for event monitoring  it allows the interval
during which the event occurs to be explicitly flagged when viewing the video.
The learning algorithm uses a set covering method (Quinlan, 1990) to learn an event
model that is a set of clauses interpreted as a disjunction. The covering method starts with
an empty model and learns a clause using the provided positive and negative examples and
adds this clause to the model. It repeats this procedure but now with positive examples
that are not covered by the earlier clause. This process is repeated until all the positive
examples are covered.
6.3 Constraining the Search Space
The size of the search space depends on the size of the bottom clause (Muggleton, 1995).
Thus, in the event learning domain, it depends on the number of spatial relations being
used and the number of objects in the event instances used as positive examples. The size
of the bottom clause increases with the number of Allens temporal relations as each interval
in the atoms with spatial predicates is temporally connected to every other interval in other
atoms with spatial predicates. This creates many atoms with temporal predicates in the
bottom clause.
In order to decrease the size of the search space, the algorithm makes use of domaindependent and domain-independent constraints on the structure of the hypothesis. The
constraints the algorithm uses such as restrictions on the hypothesis length and the number
of variables in the hypothesis etc. are domain-independent structural constraints as they
do not depend on the predicates used or any domain knowledge. The following are the two
domain-dependent constraints that reduce the search space and time thereby making the
learning process more efficient.
 Upper bounds on the number of atoms in the body of a rule.
10. Counting the number of answer substitutions instead of number of examples covered is a heuristic used
in the FOIL system (Quinlan & Cameron-Jones, 1993).

58

fiLearning Relational Event Models from Video

 Any interval in an atom with spatial (temporal) predicate should appear in an atom
with a temporal (spatial) predicate. Any hypothesis with atoms that do not satisfy
this criteria is not semantically meaningful since it might be satisfied by facts not at
all related to the event in question.
However the constraints listed above are domain dependent constraints rather than application specific constraints, i.e., these constraints that involve the spatial and temporal
predicates should be applicable in most, if not all event learning scenarios. Note that some
of the constraints are hard (i.e. inviolate). If a hypothesis violates any such constraint, then
it is discarded without scoring or refining. For example, the domain-independent constraints
and the first domain-dependent constraint mentioned above are hard. In contrast, if a hypothesis violates a constraint that is not hard, for example, the second domain-dependent
constraint listed above, it is not scored and discarded only after generating refined hypotheses from it. This is because discarding such hypotheses without refining might obstruct the
traversal of the lattice. For example, in the current work, the algorithm starts the search
process with an empty hypothesis and the initial hypotheses obtained by refining the empty
hypothesis do violate the second domain-dependent constraint listed above (since they contain exactly one predicate and therefore cannot contain both a spatial and a temporal
predicate).
6.4 Event Recognition
The learned event models are used for event recognition in unseen videos. For this purpose,
the test video is converted to relational data and used as a database and the event models
are used as Prolog queries. The querying is done in the whole database and the intervals
extracted from the answer substitutions from these queries give the temporal extent of the
recognized instances of the events. In order to record when the event takes place, we change
the arity of the event predicate (the rule head) to be monadic such that the argument is a
list of all interval variables occurring in the body. Note that it would also be possible to
introduce a second argument to record which objects are involved in the event (i.e. a list
of all variables of type object - or equivalently that occur as the first two arguments of any
spatial predicate in the body).
An issue that arises is exactly when an event occurs given that it consists of multiple
overlapping temporal intervals from the instantiated predicates in any given answer substitution. Given the list of all intervals  occurring in the instantiated body of the hypothesis,
various possibilities present themselves. One could take the maximal interval which exactly
spans all intervals in . Or one could take the interval which exactly spans the interval from
the first transition (i.e. pair of meeting intervals involving the same pair of objects) to the
last such transition. Clearly there are other possibilities too. Ultimately this is probably a
domain dependent decision. For our experiments, we take the list of all intervals in  and
the temporal extension of the event is obtained by taking the minimum and maximum of
the time points in .
Note that there may be several rules for an event class, each rule capturing a variation
in which an event can happen. These rules do not have weights specifying how important
or reliable a rule is for recognizing events. When recognizing events, all the rules of an event
class are used and this may result in multiple answer substitutions.
59

fiDubba, Cohn, Hogg, Bhatt & Dylla

7. Interleaved Induction and Abduction (IIA)
In the previous section we showed how ILP can be applied to learn rule-based relational
event/activity models, given an observation dataset, positive and negative examples of
events whose models have to be learnt. However, data from visual and other sensors tend
to be noisy with high variability in the sample space. This leads to over-fitted models (i.e.,
more rules), as the model has to cover some of the examples that are corrupt because of
the sensor noise. A model with more rules can result in many false positives when used for
event-recognition in test data.
In this section, we show how well-fitted, semantically meaningful event models can be
learned from noisy data by interleaving induction and abduction. This acquires significance
in cases where training data is scarce and noisy. We apply the Typed ILP system presented
in the previous section to learn event-based models and using these models as the domain
theory, we explain the examples/observations not covered by the induced theory using
abduction. The uncovered examples are either noisy, or are examples for the same event
that in reality happened in a different way. Using the explanation we rectify the errors in the
noisy examples corrupted by tracking errors and thus reduce the requirement for additional
rules. In our framework, the examples themselves are noisy (i.e. incorrect) thereby requiring
observation data revision in a manner that is consistent with the initially learned theory,
and general common-sense knowledge about space, spatial change, and the dynamics of the
domain. Note that many ILP approaches discard examples considering them as noisy by
using a heuristic stopping criteria. This is not acceptable in cases where there is scarcity of
training data, where learning from every example is potentially important.
7.1 Domain-Independent Spatial Theory
In order to pursue our goal, an Axiomatic Characterisation of the Spatial Theory is necessary. Many spatial calculi exist, each corresponding to a different aspect of space. Here, it
suffices to focus on one spatial domain, e.g., topology, with a corresponding mereotopological axiomatization by way of the binary relationships of the RCC-8 fragment Rrcc8 . From an
axiomatic viewpoint, a spatial calculus defined on R has some general properties (P1P5),
which can be assumed to be known apriori. To realize a domain-independent spatial theory
that can be used for reasoning (e.g., spatio-temporal abduction) across dynamic domains,
it is necessary to formalize a domain-independent spatial theory (space ) which preserves
the high-level axiomatic semantics of these generic properties. For reasons of space we only
sketch the properties P1P5 and neglect the formal axiomatization.
(P1P2) The Basic Calculus Properties (cp ) describe the jointly exhaustive & pairwise disjoint (JEPD) property, i.e., for any two entities in O, one and only one spatial
relationship from R holds in a given situation. The jointly exhaustive property of n = |R|
base relations can be axiomatized by n ordinary state constraints and, similarly, the pairwise disjoint property can be axiomatized by [n(n  1)/2)] constraints. Other miscellaneous
properties such as symmetry and asymmetry can be expressed in the same manner.
(P3) The primitive relationships in R have a continuity structure, referred to as its Conceptual Neighbourhood (cn ) (CND) (Freksa, 1991), which determines the direct, continuous changes in the quality space (e.g., by deformation and/or translational motion).
(P4) From an axiomatic viewpoint, a spatial calculus defined on R is (primarily) based
60

fiLearning Relational Event Models from Video

on the derivation of a set of Composition Theorems (ct ) between the JEPD set R.
In general, for a calculus consisting of n JEPD relationships, [n  n] compositions are
precomputed. Each of these composition theorems is equivalent to an ordinary state constraint, which every n-clique spatial situation description should satisfy.
(P5) Additionally, Axioms of Interaction (ai ) are necessary when more than one spatial calculus is modelled in a non-integrated manner (i.e., with independent composition
theorems). These axioms explicitly characterize the relative entailments between interdependent aspects of space, e.g., topology and size.
Now, let space def [cp  cn  ct  ai ] denote a domain-independent spatial theory
that is based on the axiomatizations encompassing (P1P5).
7.2 Physically Plausible Scenarios
Corresponding to each spatial situation (e.g., within a hypothetical situation space), there
exists a situation description that characterizes the spatial state of the system. It is necessary that the spatial component of such a state be a complete specification, possibly with
disjunctive information. For k spatial calculi being modelled, the initial situation description involving m domain objects requires a complete n-clique specification with [m(m 1)/2]
spatial relationships for each calculus. Therefore, we need to define a scene description to
be C-Consistent, i.e., compositionally consistent, if the n-clique state or spatial situation
description corresponding to the situation satisfies all the composition constraints of every
spatial domain (e.g., topology, orientation, size) being modelled. If more than one calculus
is modelled the inter-dependent constraints (P5) must hold as well.
From the viewpoint of model elimination of narrative descriptions during an (abductive)
explanation process, C-Consistency of scenario descriptions is a key (contributing) factor
determining the commonsensical notion of the physically realizability of the (abduced) scenario completions. Bhatt and Loke (2008) show that a standard completion semantics with
causal minimization in the presence of frame assumptions and ramification constraints preserves this notion of C-Consistency for space within a logic programming framework, as
well as with arbitrary basic action theories.
7.3 The Inductive-Abductive Framework
We interleave inductive and abductive commonsense reasoning about space, events and
change within a logic programming framework. Induction is used as a means to learn event
models by generalizing from sensory data, whereas abductive reasoning is used for noisy
data correction by scenario and narrative completion, thereby improving the learning.
7.3.1 Explanation by Abduction
Diametrically opposite to projection and planning is the task of post-dictum or explanation (Poole, Goebel, & Aleliunas, 1987), where given a set of time-stamped observations
or snap-shots, the objective is to explain which events and/or actions may have caused
the observed state-of-affairs. Explanation problems demand the inclusion of a narrative
description, which is essentially a distinguished course of actual events about which we may
have incomplete information (Miller & Shanahan, 1994). Narrative descriptions are typi61

fiDubba, Cohn, Hogg, Bhatt & Dylla

cally available as sensory observations from the real execution of a system or process. Given
narratives, the objective is often to assimilate/explain them with respect to an underlying
process model.
The abductive explanation problem can be stated as follows (Kakas, Kowalski, & Toni,
1992):
Given: Theory T and observations G, find an explanation 4 such that:
 T

S

4G

 T

S

4 is consistent

i.e., the observation follows logically and non trivially from the theory extended given the
explanation. Abductive explanations are usually restricted to ground literals with predicates that are undefined in theory, namely the abducibles. Abductive explanations are
derived by trying to prove the observation from the initial theory alone: whenever a literal is encountered for which there is no clause to resolve with, the literal is added to the
explanation.
The abduction procedure results in many valid explanations. In order to reduce the
number of explanations, several restrictions as listed below can be used (Kakas et al., 1992):
Explanations should be basic This means one explanation should not explain another
explanation. This is enforced by not allowing abducibles in the head of any rule.
Explanations should be minimal This means one explanation should not subsume another explanation.
Explanations should satisfy all integrity constraints With this restriction, we obtain explanations that are valid in the domain under consideration. In our work,
explanations should satisfy all the spatial constraints of the underlying spatial theory.
7.3.2 Scenario and Narrative Completion
It is easy to intuitively infer the general structure of narrative completion by abductive
explanation. Consider the illustration in Fig.7 for a hypothetical situation space that characterizes the complete evolution of a system. In Fig.7 the situation-based history given
by the solid arrows represents one path, corresponding to an actual time-line discretized
into intervals h0 , 1 , . . . , m i, within the overall branching-tree structured situation space.
Given incomplete narrative descriptions, e.g., corresponding to only some ordered intervals
in terms of high-level spatial (e.g., topological, orientation) and occurrence information, the
objective of explanation is to derive one or more paths from the branching situation space,
that could best-fit the available narrative information. Formally:


1  touch(a, c, 1 )












dc(a,
c,

)

in(b,
a,

)

dc(b,
c,

)


2
4
4
4








[



]
|=

,
where


space
1
2


  ( i , j ).[ meets(1 , i )  bef ore(i , 4 )  dc(b, a, i )






 touch(a, c, i )  dc(b, c, i )]











[
meets(
,

)

meets(
,

)

touch(b,
a,

)


i
j
j
4
j






 touch(a, c, j )  dc(b, c, j )]
62

(7.1)

fiLearning Relational Event Models from Video

a
a

c

b
a

c
a

c

b

a

c
a

b
c

ab

c

b
c
a

b

c

Figure 7: Branching/Hypothetical Situation Space. Only a few possibilities are shown.
There are clearly more paths from the initial scenario to the target scenario. There are also
more possible states.

In (7.1), 1 denotes the initial situation and 2 denotes the final situation represented in
terms of spatial relations (RCC-5) among the objects present in the scene. The abductive
derivation of , that explains how the scene changed from situation 1 to situation 2 ,
primarily involves non-monotonic reasoning in the form of minimizing change, in addition to
making the default assumptions about inertia, and an appropriate treatment of ramification
constraints (Bhatt & Loke, 2008).
7.4 IIA Algorithm
Most ILP systems use a covering algorithm to learn models from examples. The search
ranges over a hypothesis lattice and each hypothesis is evaluated based on the number of
positive and negative examples it covers. If it is selected as a suitable hypothesis based on
some scoring function, this hypothesis (rule) is added to the model, the covered examples are
removed and this process is repeated until all the positive examples are covered. Examples
can be corrupted by noise resulting in missing or incorrect facts. In such cases, more
rules are learned than necessary in order to cover all the examples. As the number of
rules for a concept increases, this may result in many false positives when the rules are
used for classification/recognition in test examples. In order to avoid learning from corrupt
examples, the framework identifies examples as being corrupted by explaining them through
abduction using the already induced model and the background theory (Dubba et al., 2012).
63

fiDubba, Cohn, Hogg, Bhatt & Dylla

The main assumption we make here is that the noise in the examples is not consistent. If
the noise is consistent (i.e., present in most of the examples in a similar fashion) then it
becomes part of the pattern that defines the concept and might be learned by the learning
algorithm.
The pseudo algorithm is given in Algorithm 1. The induction algorithm induces an
initial hypothesis based on the score function explained in previous sections. The positive
+
examples covered (ERule
) by this hypothesis are removed from the list of positive examples
yet to be covered. The induced theory along with background knowledge is used to explain
the uncovered examples treating each example as a narrative. Abduction gives several
possible explanations each with different cost (based on the nature and the number of facts
in the explanation). The explanations are rejected if they have a cost more than a specified
threshold. Furthermore, given the formulation of the spatial theory space , C-Consistency of
+
abduced explanations is ensured. The examples (E4
) that have an explanation whose cost
is less than the specified threshold are removed from the positive examples list that are yet
to be covered, as they are now considered to be covered by the already induced model. This
process of induction and abduction is repeated until all the positive examples are covered.
Apart from the constraints enforced by the spatial theory to filter abduced explanations,
several heuristics can be used to give a score to each explanation so that a low cost consistent
explanation can be selected by the system. One of the several possible heuristics is to prefer
explanations where the number of transitions in spatial relations is minimal (Hazarika &
Cohn, 2002). This heuristic is a direct consequence of McCarthys Common Sense Law of
Inertia (McCarthy, 1986) which states that change is abnormal and persistence is to be
preferred in the absence of data. In a spatio-temporal domain, each explanation abduced in
the absence of data is a set of spatio-temporal facts and there are three ways to add them to
the explanation: (i) Extend the current relation between two objects (can be done in both
directions of the timeline if the situation permits) (ii) change the current relation between
two objects to its neighbouring relation in a CND (iii) introduce a new object (hypothetical)
into the scene and its spatial relations with other objects in the scene as well. The cost of
each explanation is based on the type of spatio-temporal fact chosen and is calculated as
explained below.
7.4.1 Cost of an Abduced Explanation
Let 4 be an explanation from the abduction procedure where 4 is a set of grounded spatiotemporal facts of the form r(oi , oj , k ) denoted as fijrk , Ep+ is the current positive example
(an interpretation, i.e., a set of facts) and let r be from the set of spatial relations R in a
spatial calculus. Let O be the set of objects in Ep+ . Let cfijrk be the cost of abducing fijrk .
X
The total cost of 4 denoted as C4 is
cfijrk .
fijrk 4

The cost of

 ,
,
cfl =

  n,

abducing fijrk is calculated as follows:
if there exists a fijrm in Ep+ such that k and m are disjoint
if there exists a fijsm in Ep+ such that k and m are disjoint and r =
6 s
where n is the number of hypothetical objects (objects not in O) in fijrk

where  <  < .
64

fiLearning Relational Event Models from Video

The first case in the cost function occurs when the system abduces a fact that extends
a relation between two objects in the temporal dimension. This does not count as a spatial
transition and hence has a very low cost. In contrast, the second case occurs when the
system abduces a fact that extends the existence of two objects in temporal dimension with
a different relation (the new spatial relation must be a neighbour to the existing relation
in the CND) than the one that already exists between them. This counts as a spatial
transition and has a cost more than the first case. The third case occurs if it is necessary
to have a hypothetical object to satisfy the hypothesis in Ep+ . This case is used when an
object involved in an event is completely missed by the object tracker while first two cases
are used in scenarios where an object is not detected in some temporal slice in its life time.
Note that the first case is clearly the most preferred if the abduction procedure has to find a
low cost explanation and the third case which is the most expensive applies when an object
is completely missed by the object tracker. Though it is possible to avoid transitions to
reduce the score, sometimes it is mandatory to consider transitions. For example, consider a
scenario where two objects have a dc relation and in the final state they have an in relation.
In this case, the algorithm has to abduce facts where there are two transitions (one when
the dc relation changes to touch and another when touch changes to in relation). Note that
it is not necessary to abduce temporal relational facts as the Prolog definitions for temporal
relations in the background theory can be used to compute them when needed. This can
be achieved by not including temporal predicates in the list of abducibles.
The abduction procedure uses the existing constants in the database and one issue with
this is that though the number of relations and objects is small, the number of possible
intervals is very large if not constrained. In order to constrain the possible explanations,
we introduce intervals with predefined duration into the database so that abduction uses
only these intervals for abducing explanations. Note that abduction as we have defined
it only adds missing spatio-temporal information and cannot be used to retract corrupted
data resulting from noise.

Algorithm 1 Interleaved Induction and Abduction algorithm (IIA)
procedure IIA(E + , E  , B) . training sets and background knowledge (includes spatial
theory)
H
4
while E + 6=  do
Rule  S
Induce(B, E + , E  )
H  H {Rule}
+
E +  E +  ERule
4  Abduce(B, H, E + )
+
E +  E +  E4
end while
return H
. Learned theory
end procedure

65

fiDubba, Cohn, Hogg, Bhatt & Dylla

Figure 8: Airport domain: The videos are recorded using 6 static cameras looking at the
same scene from different angles.

8. Experimental Results
In this section, we present an evaluation of remind, as well as the extension presented in
Section 7. For the experiments, we used two real world video datasets that are different from
each other in many aspects. The videos from these datasets are shot in outdoor settings and
in different weather and light conditions (rainy, cloudy, sunny, night). These variations in
the videos present various challenges to the vision system and subsequently to the learning
system both in the training phase and the event recognition phase.
The two datasets used in this work for evaluation are from airport logistics and verb
videos. The datasets from these domains differ in many aspects such as number of objects
in the video, length of video, duration of events, background structures, the number of
cameras used to capture the events and also the plane (image plane or ground plane) in
which the tracking data is made available. We view the differences in the datasets as a
positive aspect - the framework shown to work in two very different kinds of scenarios.
remind11 is implemented in Python and for speed, some modules are implemented
in Cython; SWI-Prolog is used as the underlying Prolog engine for storing and querying
relational facts and background knowledge.
8.1 Airport Logistics
For experiments in the airport logistics domain, 15 turn-arounds12 were used where each
turn-around was shot using 6 cameras from different angles (Fig.8) and each video is on
average one hour long (15 frames per sec).
The following are informal descriptions of the International Air Transport Association (IATA) events we aim to learn models for:
11. Available on request from the first author and will be made public in the near future.
12. A turn-around is the duration of a plane entering and leaving the apron area.

66

fiLearning Relational Event Models from Video

Aircraft Arrival

Aircraft comes into the apron

Aircraft Departure

Aircraft moves away from its position on the apron

GPU Positioning

Ground power unit comes and positions in its zone

Left Refuelling

Fuel truck arrives on the left side of aircraft for refuelling

PB Positioning

Push-back vehicle positioning in front of the aircraft

PBB Positioning

Passenger Boarding Bridge attaches itself to the aircraft

PBB Removing

Passenger Boarding Bridge detaches itself from the aircraft

FWD CN LoadUnload

Container Loading/Unloading at the front end of the aircraft

AFT CN LoadUnload

Container Loading/Unloading at the rear end of the aircraft

AFT Bulk LoadUnload

Baggage Loading/Unloading at the rear end of the aircraft

FWD Bulk LoadUnload

Catering Loading/Unloading at the front end of the aircraft

Within each event, there is high variability because of noise in tracking and also because
of objects extraneous to the event entering the event scene. Note that some events might
not be present or may occur multiple times in some turn-arounds. The scenes involve
interactions of vehicles and people with zones on the apron. These zones are specified
on the ground plane according to the IATA specifications and the position of the zones
depends on the type of aircraft. These zones are used for parking and steering vehicles
for different operations carried out in a turn-around. Note that these zones are static
throughout the video and do not change size or position, unlike the bounding boxes of the
vehicles obtained through tracking. Hence zones are not included in the type hierarchy used
for the domain (Fig.4) since they do not suffer from visual noise. The main reason to use
zones is that the RCC-5 spatial relations between bounding boxes of vehicles and people
in the ground plane rarely touch, hence most of the interactions are encoded as dc if zones
are not used. It is important to use the zones as most of these interactions happen in the
zones. According to the IATA specifications, a vehicle transition through the zones and the
position of the vehicle in particular zones is important to determine the events.
We use the object tracks provided by a partner in the Co-Friend project (Ferryman,
Borg, Thirde, Fusier, Valentin, Bremond, Thonnat, Aguilera, & Kampel, 2005); certain
details in some events are not detectable with this tracking system such as the direction of
baggage on the rail of the loader vehicle and whether the trolleys are empty when they arrive
at the scene. The Load/Unload events obtained from IATA events differ in such details (if
the trolleys are loaded when they arrive at the scene and the baggage is moving towards
the aircraft, then the event is loading and if the trolleys are empty when they arrive at the
scene and the baggage is moving away from the aircraft, the event is unloading). Apart from
these details, they are semantically similar and hence are regarded as the same events (for
example, FWD CN Load and FWD CN Unload are regarded as the same events and named
FWD CN LoadUnload, the same strategy is followed with other Load/Unload events).
8.1.1 Tracking and Obtaining Relational Data
The apron scene area is too large to be covered by a single static camera. The events
on the apron occur on both sides of the aircraft which is very difficult to cover with a
67

fiDubba, Cohn, Hogg, Bhatt & Dylla

single camera and because of the size of the aircraft it is possible to have many occluded
objects in the scene. In order to solve these problems, six cameras are used to shoot the
scene from different angles so that the entire area is covered and the number of occluded
objects is minimized. Working on the ground plane data results in learned models that are
independent of the camera view and the airport as these models can be readily applied at
different airports with different camera configurations.
The tracking data is obtained for each of the videos from six cameras of a turn-around
and fused together to get 3D data on the ground plane (Ferryman et al., 2005). The tracking
data is noisy because of low quality, bad light and weather conditions and low contrast
of CCTV videos. The noise can be the presence of phantom objects, missing objects,
wrong types of vehicles, inaccurate bounding boxes, broken trajectories, object identity,
inconsistencies etc. which are typical problems in any computer vision tracking system.
Each turn-around is separately processed to get relational data that consists of a set of
spatial relations among the vehicles and zones on the apron. Prolog rules that decide the
temporal relationships among intervals are considered as background information in the ILP
system. The data for each video has between 250 and 500 spatial relational facts (excluding
temporal relational facts) depending on the number of objects and the interactions between
these objects.
Note that an event requires at least one change in the state (here, spatio-temporal
relations between pairs of objects) of the objects. If the relation between any two objects
is dc and does not change during the life span of the objects, it signifies that the objects
are not interacting and the relational fact is discarded as these spatio-temporal facts do not
contain relevant information in defining event models. The tracking data also consists of
bounding boxes for people in the scenes, but these are discarded as people are not germane
in the semantics of the events and also these increase the size of the relational data.
8.1.2 Annotation of Events
For supervised learning we need positive and preferably negative example instances of
events. In the airport domain, the temporal extent of the events is provided by individuals who have expertise in the IATA protocols and apron activities, by specifying the
start and end frame numbers of the event instance in that video. The spatial extent was
obtained by using a tool with which a polygon can be drawn on one of the image planes
and the corresponding ground plane region is obtained using an homography (it was easier
for a human annotator to watch an actual video to provide the spatial annotation rather
than view a 3D visualization in the ground plane, which being the fusion of the imperfectly
tracked data that does not always show all relevant objects). This region gives a spatial
extent for the event instance.
8.2 Physical Action Verbs Dataset
The Action Verbs dataset13 is a corpus of video vignettes (Fig.9) that portray motion verbs
such as approach, exchange, jump, collide, etc. enacted in natural environments like parks,
13. This dataset (Minds Eye Year 1 recognition task videos) is provided by DARPA and publicly available
from http://www.visint.org/datasets

68

fiLearning Relational Event Models from Video

(a) Approach event with tracked objects

(b) Snatch event with tracked objects

Figure 9: Example event instances for Approach and Snatch in Action Verbs dataset
urban places, etc. These vignettes are short in duration when compared to videos from the
airport domain, a few tens of seconds. The full list of verbs is given in Table 3.
Though each vignette was shot to portray a single verb action, other verbs are inevitably
present as well, sometimes overlapping in time. This is primarily unavoidable, for example, a
vignette that portrays the verb carry, will automatically include walk if a person is carrying
some object in their hand. This aspect is taken into consideration while annotating the
vignettes. We were able to obtain tracked data from an external source (Morariu, Harwood,
& Davis, 2013) including object type information (Fig.10).
The new challenge in using this dataset is the different ways a verb can be enacted.
There are 48 verbs in the dataset and a total of 1615 vignettes are used for training and
2348 vignettes are used for testing.
8.2.1 Tracking and Obtaining Relational Data
The tracking data available to us often suffers from errors, e.g., a bouncing ball is often only
tracked once it is being held and fast moving objects such as a running person are missed.
We used Qualitative Trajectory Calculus relations (QT CL1 ) (Van de Weghe et al., 2006)
as the primitive spatio-temporal relations. We did not choose RCC for this dataset as it
seemed unlikely that a purely topological representation would be sufficient. In contrast,
the QT CL1 relations capture the typical movements in the verbs dataset like moving away,
approaching, follow etc. For example, in a chase event where one object is following another
object the relation is dc, using RCC, which is also the same for two objects standing still
with some distance in between.
It is difficult to model motion patterns of objects like run, walk, raise, bend etc. using
only relational data without referring to parts of a person. The verbs dataset contains some
events that involve such motion patterns and to recognize these, pixel based models are
more appropriate. The primitive events were recognized in all the videos using the method
proposed by Jiang, Lin and Davis (2010), where an action is represented as a sequence
of joint HOG-flow descriptors (Dalal & Triggs, 2005) extracted independently from each
frame. Instead of applying this approach to the entire frame and video as proposed by
Jiang et al. (2010), the input was restricted to sliding temporal windows along the spatio69

fiDubba, Cohn, Hogg, Bhatt & Dylla

Person
Object

Vehicle
Other

Figure 10: Tree-structured object type hierarchy in the Action Verbs domain.
temporal volume defined by a persons bounding box. These primitive events14 in addition
to QT CL1 relations provide the relational data in the verbs domain.
8.2.2 Annotation of Events
The ground truth for events in the verbs dataset vignettes is of a different nature to the
ground truth in the airport domain. The development and the test set were annotated
by 10 people using Amazon Mechanical Turk (AMT). Each vignette was presented to the
annotator and 48 questions were presented in the form: Is verb X present in this vignette?
Only verbs annotated by more than 50% of the annotators are considered as events present
in the vignette. For the development set, the annotations were extended by providing for
each event instance its temporal extent.
8.3 Experimental Results and Evaluation for the Typed ILP Framework
Sample rules15 learned for Aircraft Arrival and AFT Bulk LoadUnload events are given
below. For example, the Aircraft Arrival rule can be interpreted as: an aircraft arrives
when the aircraft bounding box has the in relation with the right AFT Bulk TS Zone and
then moves forward thereby changing the relation to touch. This happens when the aircraft
arrives and is moving to its position. The rule also correctly identifies that this bounding
box should belong to an object of type aircraft. The goals in the rule are ordered such that
spatial predicates come before (to the left of) the temporal predicates since there are more
temporal facts16 compared to spatial facts and this ordering speeds up the query execution.
aircraft arrival([intv(T1,T2), intv(T3,T4)]) :in(obj(aircraft(V)), right AFT Bulk TS Zone, intv(T1,T2)),
touch(right AFT Bulk TS Zone, obj(aircraft(V)), intv(T3,T4)),
meets(intv(T1,T2), intv(T3,T4)).
aft bulk loadunload([intv(T1,T2), intv(T3,T4)]) :touch(left TK Zone, obj(veh(heavy veh(V1))), intv(T1,T2)),
touch(obj(veh(V2)), left TK Zone, intv(T3,T4)),
meets(intv(T3,T4), intv(T1,T2)).
We followed the standard leave-one-out methodology for testing performance in the
airport domain. All turn-arounds except one are used for training and the remaining one is
used as a test case. This process is iterated until each turn-around is used as the test case
14. This data was provided by Vlad Morariu from University of Maryland.
15. A temporal interval is represented as intv(T1 , T2 ) for programming convenience where T1 and T2 are the
starting and ending frame numbers of the interval.
16. As already noted, temporal facts are not explicitly stored but are computed via background knowledge
rules.

70

fiLearning Relational Event Models from Video

Event

#Examples

Without Type Generalization

With Type Generalization

FWD CN LoadUnload

7

0.86

0.06

0.11

0.86

0.08

0.15

GPU Positioning

16

0.4

0.03

0.05

0.27

0.02

0.04

Aircraft Arrival

15

0.43

0.01

0.02

0.36

0.01

0.02

AFT Bulk LoadUnload

29

0.72

0.20

0.31

0.72

0.20

0.31

PBB Removing

15

0.43

0.06

0.10

0.36

0.12

0.18

Left Refuelling

8

0.25

0.03

0.05

0.12

0.10

0.11

PB Positioning

14

0.28

0.04

0.07

0.14

0.06

0.08

Aircraft Departure

12

0.41

0.11

0.17

0.33

0.19

0.24

AFT CN LoadUnload

15

0.80

0.05

0.09

0.67

0.07

0.13

PBB Positioning

15

0.73

0.16

0.26

0.67

0.34

0.45

FWD Bulk LoadUnload

3

1.00

0.24

0.39

1.00

1.00

1.00

Weighted Average

0.15

0.20

Table 1: Performance comparison of models obtained without and with using types using
RCC-5 primitives in the airport domain. The first, second and third columns for each
category are recall, precision and f1 respectively. The best f1 value in each case is presented
in bold. It is clear from the table that using types improves the overall performance. By
without type generalization we mean, type information from tracker is ignored (all objects
have the same type) and type generalization is not performed during learning.

exactly once. The results of our experiments are summarised in Table 1. The third and
fourth columns show the recall and precision without using types in the 15 turn-arounds
(i.e., type information from the tracker is ignored, hence all objects have the same type and
type generalization is not performed during learning). The fifth and sixth columns show
the recall and precision using the type hierarchy. From the tables it is clear that using type
information can increase the accuracy of event recognition. Also the combined execution
time for all the experiments using type generalization was reduced by roughly 30% when
compared to the execution time of experiments without type generalization.
The detailed recognition results (temporal localization) for events in a turn-around is
shown in Fig. 11 (best seen in colour). The plot shows a turn-around with one subplot showing ground truth of event instances and another subplot showing the recognized instances
by the Typed ILP system in that turn-around. Each event is colour coded for comparing
ground truth with the recognized instance intervals. Note that a recognized event instance
is considered a true positive if it overlaps at least 20% with the corresponding event ground
truth interval (Oh et al., 2011). In some cases the temporal extent of recognized event in71

fiDubba, Cohn, Hogg, Bhatt & Dylla

Figure 11: Recognition of events in turn-around 1 in the airport domain (best viewed in
colour).

stances is long because some spatial relations that are important in the event extend beyond
the deictic interval of the event.
8.3.1 Evaluating Learned Event Models Against Hand-Coded Event Models
The learned models are also evaluated by comparing them with hand-coded models. The
hand-coded models were provided by domain experts using a set of domain-dependent
spatial relations (Ferryman et al., 2005). In order to directly compare performance without
any change in underlying representation, rather than using the RCC-5, we recomputed
relational data for remind using the same domain-dependent primitives. The comparisons
are given in Table 2. It is clear from the table that learned models have a better performance
in all the event categories when compared to the performance of hand-coded models. The
hand-coded models were single primitives rather than a set of primitives connected by
temporal relations. These kind of models with only one single predicate have far more
false positives when compared to models that have a set of spatial relations connected by
72

fiLearning Relational Event Models from Video

Event

#Examples

Learned (RCC-5)

Learned (d-d)

Hand-coded (d-d)

FWD CN LoadUnload

7

0.86

0.08

0.15

0.14

0.50

0.22

0.71

0.04

0.07

GPU Positioning

16

0.27

0.02

0.04

0.44

0.03

0.05

0.00

1.00

0.00

Aircraft Arrival

15

0.36

0.01

0.02

0.07

0.01

0.01

0.07

0.05

0.06

AFT Bulk LoadUnload

29

0.72

0.20

0.31

0.59

0.27

0.37

0.03

0.05

0.04

PBB Removing

15

0.36

0.12

0.18

0.26

0.14

0.18

0.00

1.00

0.00

Left Refuelling

8

0.12

0.10

0.11

0.38

0.23

0.28

0.00

1.00

0.00

PB Positioning

14

0.14

0.06

0.08

0.07

0.08

0.07

0.21

0.09

0.12

Aircraft Departure

12

0.33

0.19

0.24

0.00

1.00

0.00

0.00

1.00

0.00

AFT CN LoadUnload

15

0.67

0.07

0.13

0.33

0.05

0.08

0.47

0.07

0.12

PBB Positioning

15

0.67

0.34

0.45

0.26

0.20

0.22

0.40

0.07

0.12

FWD Bulk LoadUnload

3

1.00

1.00

1.00

0.00

1.00

0.00

1.00

0.02

0.04

Weighted Average

0.20

0.16

0.05

Table 2: Table comparing learned (RCC-5), learned (domain-dependent) and hand-coded
models performance (domain-dependent). The first, second and third columns for each
category are recall, precision and f1 respectively. The best f1 value in each case is presented
in bold.
temporal relations. Also the hand-coded models use a very specific vehicle type in the event
models which affects the performance by reducing the true positives as there is noise in the
object type detection, whereas the learned models use an appropriate generalized object
type to cover these instances.
8.3.2 Evaluating Learned Event Models with Different Spatial Relations
We also performed an evaluation to investigate the effects of different spatial relations.
For comparison we used RCC-5 and domain specific relations in the airport domain. We
did not use QTC relations for this domain as there were very few examples to learn from
and because of the many spatial relations in the QTC spatial calculus, the patterns for
events do not emerge. The results are given in Table 2. From the table it is clear that
models learned using RCC-5 have a better recognition performance (mean f1: 0.25) when
compared to the models learned using domain-dependent relations (mean f1: 0.13). One
reason might be that RCC-5 has a better representation granularity when compared to the
domain-dependent primitives. Also RCC-5 has the JEPD (jointly exhaustive and pair-wise
disjoint) property while the domain-dependent primitives in the airport domain does not
(it lacks the pair-wise disjoint property).
8.3.3 Evaluating on the Verbs Dataset
The framework that uses the type generalization has also been applied to the verbs dataset
for the 48 verbs. Table 3 shows the precision, recall and f1 scores for the classification task.
For each video in the test set, all the event models are used as queries and if an event model
73

fiDubba, Cohn, Hogg, Bhatt & Dylla

succeeds, that particular verb is considered to be present in the video (and the variable
bindings give the time of the occurrence and what objects are involved). This is compared
with the ground truth to obtain the precision and recall values.
Below we provide sample rules learned for the events Approach and Snatch which cover
the instances shown in Fig.9. The QT CL1 relations moto (short form for moving towards a
stationary object), static and depart (short form for moving away from a stationary object)
corresponds to the relations in blobs in row 2 and column 1, row 2 and column 2, row 2
and column 3 respectively in Fig.1. Also note that unlike the models learned in the Airport
Dataset, there is no list of temporal intervals as an argument in the head of the rules here.
This is because we just want to recognize the events in the videos in the Action Verbs
dataset and the videos are too short to find the temporal extent of an event.
approach() :moto(obj(vehicle(J)), obj(person(K)), intv(V 32,V 33)),
static(obj(vehicle(J)), obj(person(K)), intv(V 34,V 35)),
meets(intv(V 32,V 33), int(V 34,V 35)).
snatch() :static(obj(person(J)), obj(person(K)), intv(V 24,V 25)),
moto(obj(other(L)), obj(person(J)), intv(V 40,V 41)),
depart(obj(other(L)), obj(person(K)), intv(V 18,V 51)),
overlaps(intv(V 40,V 41), int(V 18,V 51)),
during(intv(V 40,V 41), intv(V 24,V 25)),
during(intv(V 18,V 51), intv(V 24,V 25)).
The proposed framework, was compared with other existing systems and the results
are presented in Tables 4-7. One of the systems that we compared with, the RedVine
system, is a supervised learning version of the framework proposed by Sridhar, Cohn and
Hogg (2010). It is based on a graphical representation of relational facts, where an event
is represented by a histogram of graphemes (small graphs that represent spatio-temporal
interactions of the objects involved in the event) mapped into a vector space to facilitate
classification. The Stack convolutional Independent Subspace Analysis (ScISA) (Le et al.,
2011)17 is based on pixel level flow based features which are then used to model events
using a neural network. The spatio-temporal features used in this algorithm are learned
in unsupervised fashion instead of using predefined features such as SIFT (Lowe, 2004),
HoG (Dalal & Triggs, 2005), etc.
The evaluation dataset as provided by DARPA has a total of 2348 vignettes. It was
found that some vignettes (1294) in the training set also appeared in the evaluation set.
We call the dataset with 2348 vignettes the Verb Evaluation Dataset-1 (VED1) and the
remaining vignettes after discarding the 1294 vignettes that appeared in training dataset
VED2. Evaluation on VED1 gives interesting insights into overfitting and underfitting
in the different learning frameworks that are compared. We chose two different average
mechanisms (macro and micro)18 to get an overall f1 and Matthews correlation coefficient
(MCC) scores over all the verbs and vignettes. True Negatives do not play a role in f1
17. Results using this system have been provided by Tuyen Huynh of SRI.
18. Macro-average is calculated by first calculating precision and recall for each category and then taking
the average of these values, while micro-average is calculated by constructing a global contingency table
and then calculating precision and recall using these sums.

74

fiLearning Relational Event Models from Video

Verb

precision recall

approach
arrive
attach
bounce
bury
carry
catch
chase
close
collide
dig
drop
enter
exchange
exit
fall

0.36
0.28
0.08
0.11
0.05
0.14
0.05
0.04
0.07
0.14
0.02
0.08
0.17
0.06
0.15
0.10

0.78
0.73
0.49
0.71
0.57
0.53
0.58
0.44
0.29
0.83
0.36
0.47
0.74
0.56
0.71
0.62

f1

Verb

0.49
0.40
0.14
0.19
0.10
0.22
0.10
0.07
0.11
0.24
0.04
0.14
0.28
0.11
0.25
0.17

flee
fly
follow
get
give
go
hand
haul
have
hit
hold
jump
kick
leave
lift
move

precision recall
0.07
0.06
0.09
0.15
0.11
0.52
0.10
0.09
0.46
0.14
0.47
0.06
0.07
0.29
1.00
0.76

0.82
0.42
0.62
0.50
0.66
0.79
0.66
0.46
0.60
0.64
0.69
0.25
0.38
0.76
0.00
0.74

f1
0.14
0.11
0.16
0.23
0.18
0.63
0.17
0.15
0.52
0.23
0.56
0.09
0.11
0.42
0.00
0.75

Verb
open
pass
pickup
push
putdown
raise
receive
replace
run
snatch
stop
take
throw
touch
turn
walk

precision recall
0.10
0.22
1.00
0.16
1.00
0.33
0.15
0.07
0.10
0.11
0.39
0.24
0.06
0.64
0.47
0.32

0.77
0.39
0.00
0.82
0.00
0.63
0.70
0.77
0.82
0.51
0.78
0.62
0.39
0.53
0.53
0.80

f1
0.17
0.28
0.00
0.27
0.00
0.44
0.25
0.13
0.18
0.19
0.52
0.35
0.10
0.58
0.50
0.45

Table 3: Classification results per verb in the physical action verbs domain.

scores but have a considerable effect on MCC scores as MCC does not differentiate between
positive and negative classes. MCC will give the same scores even if the class labels are
interchanged while f1 scores change. Note that much of the work in the literature on activity
recognition use f1 scores.
From the Tables 4-7, it is clear that ScISA has better MCC scores in all cases while
remind has a better f1 score on the VED2, though it has lower MCC scores than the
MCC scores of the other two algorithms. Also note the drop of performance of ScISA
in VED2 set when compared to VED1, whereas remind and RedVine have almost the
same performance indicating ScISA is overfitting the data while remind and RedVine are
underfitting the data. The reason for the very high f1 but very low MCC score in remind
is because of few True Negatives.
ScISA performs quite well (w.r.t. MCC score) but does not have the modelling capability
our framework has since it underutilizes the temporal domain. While we do not outperform
the state-of-the-art for all evaluation measures, the proposed scheme is still general, i.e.,
(i) it gives good interpretations of activities in video scenes; (ii) does take the temporal
domain into account unlike the ScISA and therefore provides better modelling capabilities;
(iii) gives high recall while precision can be improved with further post-processing and
(iv) provides (elegant) logical rules which can be easily interpreted by a human observer.
One major drawback of ScISA is the lack of spatio-temporal localization of the recognized
event. It is suitable only for the event classification tasks (verbs dataset) but not for the
event recognition tasks (airport domain). Although we do not report on localization here
(owing to the short videos in the data set), deriving localization (or position) information
from remind is trivial once the event is recognized since the intervals and objects involved
are explicitly identified in the rule body.
75

fiDubba, Cohn, Hogg, Bhatt & Dylla

Method
remind
RedVine
ScISA

avg-prec
0.24
0.32
0.91

avg-rec
0.56
0.24
0.54

f1
0.34
0.28
0.68

MCC
0.05
0.16
0.67

Table 4: Performance in the verbs domain: VED1, macro-average per verb.

Method
remind
RedVine
ScISA

total prec
0.21
0.37
0.92

total rec
0.59
0.24
0.60

f1
0.31
0.30
0.72

MCC
0.0
0.19
0.70

Table 5: Performance in the verbs domain: VED1, micro-average (total detection classification)

Method
remind
RedVine
ScISA

avg-prec
0.25
0.35
0.49

avg-rec
0.59
0.25
0.20

f1
0.35
0.29
0.29

MCC
0.04
0.17
0.21

Table 6: Performance in the verbs domain: VED2, macro-average per verb

Method
remind
RedVine
ScISA

total prec
0.21
0.40
0.59

total rec
0.63
0.26
0.29

f1
0.32
0.31
0.39

MCC
0.08
0.20
0.33

Table 7: Performance in the verbs domain: VED2, micro-average (total detection classification)

8.4 Experimental Results and Evaluation for IIA
The IIA framework has been evaluated on both the airport and verb datasets. We use
Hyprolog, a logic programming framework capable of abductive inference (Christiansen &
Dahl, 2005).
8.4.1 Embedding Spatial Theory for the Airport Domain
For the airport domain, we have encoded the RCC-5 spatial theory space into the framework that contains the conceptual neighbourhood graph, the JEPD relationships and the
composition theorems of the spatial relations used as follows:
76

fiLearning Relational Event Models from Video

% Sample
dc(X, Y,
dc(X, Y,
dc(X, Y,
dc(X, Y,
dc(X, Y,

JEPD
T) ,
T1),
T1),
T1),
T1),

constraints
touch(X, Y,
touch(X, Y,
touch(X, Y,
touch(X, Y,
touch(X, Y,

(P1 - P2) for RCC-5
T)
T2), during(T1, T2)
T2), during(T2, T1)
T2), overlaps(T1, T2)
T2), overlaps(T2, T1)

<=>
<=>
<=>
<=>
<=>

fail.
fail.
fail.
fail.
fail.

% Conceptual Neighbourhood constraints (P3) for RCC-5
dc(X, Y, T1), in(X, Y, T2), meets(T1, T2) <=> fail.
in(X, Y, T1), dc(X, Y, T2), meets(T1, T2) <=> fail.
% Sample Composition Theorem (P4) for RCC-5
in(X, Y, T1), dc(Y, Z, T2), touch(X, Z, T3), during(T2, T1),
during(T3, T2) <=> fail.

The JEPD and the CND property constraints forbid the abduction of facts which contradict the spatial theory thus avoiding physically impossible scenarios and this also helps
abduction to complete in reasonable time.
To explain our approach, consider the following fragments of actually occurring datasets
(Ex:1 - Ex:4) for the event Aircraft Arrival :

Ex:1

dc(arr zone,obj(aircraft(obj45)),intv(6661,7137))
touch(arr zone,obj(aircraft(obj45)),intv(7138,29114))
touch(arr zone,obj(veh(light veh(gpu(obj54)))),intv(7154,8161))
dc(arr zone,obj(veh(heavy veh(loader(obj2)))),intv(749,30380))

Ex:2

dc(arr zone,obj(aircraft(obj68)),intv(2342,2663))
touch(arr zone,obj(aircraft(obj68)),intv(2664,29524))

Ex:3

dc(arr zone,obj(veh(light veh(trolley(obj0)))),intv(285,21494))
touch(arr zone,obj(aircraft(obj41)),intv(4458,32404))
touch(arr zone,obj(veh(light veh(trolley(obj2)))),intv(1712,32405))

Ex:4

dc(arr zone,obj(aircraft(obj33)),intv(2435,6987))
touch(arr zone,obj(veh(heavy veh(loader(obj27)))),intv(2197,2310))
dc(arr zone,obj(veh(heavy veh(loader(obj27)))),intv(2311,2645))

We obtain the following model for Aircraft Arrival event learned by the ILP approach from
the first two examples of the given examples with arr zone denoting a specific zone on the
apron and any Ti denotes a time point. Each fact has two time points indicating the start
and end of an interval in which the spatio-temporal fact holds.

aircraft arrival([intv(T1,T2), intv(T3,T4)]) :dc(arr zone, obj(aircraft(V)), intv(T1,T2)),
touch(arr zone, obj(aircraft(V)), intv(T3,T4)),
meets(intv(T1,T2), intv(T3,T4)).
77

fiDubba, Cohn, Hogg, Bhatt & Dylla

obj

obj

Z1
Z2

Z1
Z2
Z3

Z3

arr_zone

Z4

Z5

arr_zone

Z4

Z5

time

aircraft_arrival(T1,T2)
dis(arr_zone,obj(aircraft(V)),T1,T)
con(arr_zone,obj(aircraft(V)),T+1,T2)
(a) Spatial primitive based event modelling

966

966

Z1
Z2
Z3
Z4

Z1
Z2
Z3

arr_zone
Z5

Z4

arr_zone
Z5

time

aircraft_arrival(5338,16868)
rel?(arr_zone,obj?,5338,16630)
con(arr_zone,obj(aircraft(obj996)),16631,16868)
(b) Narrative completion (of data video) and previously learned model

Figure 12: IIA Scenario and Narrative Completion; E.g., aircraft arrival
This rule states that an aircraft arrival takes place if there is some interval in which an
aircraft is disconnected from arr zone directly followed by an interval, i.e., meets, where
the same aircraft is connected to arr zone. This model does not cover any other examples
apart from Ex:1 and Ex:2. Ex:3 has a missing dc relation related to the aircraft whereas
Ex:4 has a missing touch relation (Fig. 12b). These represent the typical data corruption
at a higher level because of tracking error at lower level video processing at different stages
in the video.
8.4.2 Narrative Completion in the Airport Domain
Multiple explanations are interesting as they give several possible scenarios that are all consistent with the narrative. For example, consider Ex:4 where a touch fact related to aircraft
arrival event is missing in the narrative. This happens when the vision algorithm fails to
detect the aircraft when it is coming towards the parking zone as such a big object changes
the light conditions in the scene. The abduction system comes up with two explanations (as
shown in the following sample interactive run of the system), one filling the missed fact that
is consistent with the narrative and the background knowledge and constraints. Another
explanation is using a hypothetical object ( G41673) that is not present in the database.
78

fiLearning Relational Event Models from Video

This explanation is more expensive than the first explanation, so the system chose the first
explanation.
%A small narrative with three observations (The touch fact
%is missing, this happens, when the vision algorithm fails
%to detect the aicraft when it is close: Approximate
%interval can be specified for aircraft-arrival query
%dc(arr_zone,obj(aircraft(obj33)),intv(2435,6987))
%touch(arr_zone,obj(veh(heavy_veh(loader(obj7)))),intv(2197,2310))
%dc(arr_zone,obj(veh(heavy_veh(loader(obj7)))),intv(2311,2645))
?- aircraft_arrival(intv(2000,12000)).
touch(arr_zone,obj(aircraft(obj33)),intv(6988,7988))
true ;
dc(arr_zone,obj(aircraft(_G41673)),intv(2435,6987))
touch(arr_zone,obj(aircraft(_G41673)),intv(6988,7988))
true ; false.
With narrative completion, it is possible to cover all the examples given above, with one
single Aircraft Arrival model learned. This avoids learning spurious rules to cover these
corrupted examples thus giving us compact and semantically meaningful models.
To evaluate our approach, we compare the rules learned using only induction and rules
learned using the IIA algorithm. The first column in Table 8 shows the events that we
considered for the experiments, the second column shows the number of instances of that
particular event in the 15 turnarounds. The third column shows the number of rules learned
by using only ILP while the fourth column shows the results using the IIA algorithm while
fifth column shows the number of examples that were not covered by the induced rules
but were explained using abduction and hence no rules learned from them. By interleaving
induction and abduction, we were able to avoid learning spurious rules as shown by the
results. For most classes, the number of rules are reduced by about 50% and the overall
performance is also increased. We also observed that the rules which had been previously
learned for the examples now covered by abduction did not semantically correspond to the
events.
8.4.3 Evaluating IIA on the Verbs Dataset
For the verbs domain, we encoded the spatial theory for QT CL1 spatial calculi. Though
we also used domain-dependent primitive events for this domain besides QT CL1 , we did
not encode any spatial theory for these relations as it is not well defined. For example, the
domain-dependent primitives for this domain are neither jointly exhaustive nor pair-wise
disjoint. We also avoided abducing explanations with these relations by not including these
relations in the list of abducibles. 10-fold cross-validation was used for evaluation for verbs
dataset. Since each video is short in duration with around 200 frames, we used classification
instead of recognition. From Table 9 it is clear that using abduction reduces the number
of rules in event model thereby giving a more compact model. In the verbs dataset results,
there is no considerable change in performance as this is a classification task rather than a
recognition task. The main performance increase with IIA during inference comes when
79

fiDubba, Cohn, Hogg, Bhatt & Dylla

Airport Events

#pos

FWD CN LoadUnload
5
GPU Positioning
15
Aircraft Arrival
15
Aircraft Departure
15
AFT Bulk LoadUnload
12
Left Refuelling
6
PB Positioning
15
AFT CN LoadUnload
7
PBB Positioning
15
PBB Removing
15
FWD Bulk LoadUnload
3
Num of rules with only Induction 2

2
5
5
5
5
2
4
3
4
5
2
Num of



2

RoI

PoI

RIA

PIA

1
2
0.8
0.3
0.8
3
4
1
0.2
1
2
5
0.38
0.26
0.33
2
7
0.8
0.15
0.71
2
4
0.63
0.43
0.63
1
2
0.66
0.5
0.66
3
2
0.33
0.34
0.33
1
3
0.57
0.4
0.57
3
2
1
0.57
1
2
5
0.54
0.23
0.54
1
1
1
1
1
rules with IIA  avg num of examples covered

0.4
0.4
0.32
0.26
0.65
0.55
0.42
0.51
0.62
0.31
1
by abd

Table 8: Airport domain IIA results averaged from all iterations in leave-one-out testing.
RoI, PoI - recall and precision with only induction: RIA, PIA - recall and precision using
IIA.
Verb Events

#pos

2



RoI

Approach
584
12
5
45
0.73
Arrive
8
2
1
2
0.50
Attach
48
6
3
12
1.00
Bounce
22
2
2
0
0.95
Catch
201
7
4
31
0.59
Chase
108
11
7
19
0.59
Collide
101
6
4
14
0.98
Dig
140
10
7
21
0.96
Drop
44
2
2
0
1.00
Exchange
18
6
3
4
0.40
Fall
134
8
5
18
0.92
Give
552
27
20
54
0.94
Jump
150
6
4
14
0.98
Kick
48
4
3
6
1.00
Leave
116
10
4
34
0.67
Lift
78
8
5
17
0.67
Pass
76
8
4
13
0.87
Pickup
40
6
4
8
0.81
Run
76
7
5
7
0.57
Throw
26
3
2
5
0.67
Num of rules with only Induction 2 Num of rules with IIA  avg

PoI

RIA

0.12
0.74
0.05
0.50
0.14
1.00
0.06
0.95
0.11
0.56
0.08
0.57
0.16
0.98
0.38
0.96
0.16
1.00
0.03
0.40
0.35
0.90
0.56
0.94
0.13
0.98
0.15
1.00
0.20
0.67
0.24
0.67
0.10
0.87
0.13
0.81
0.12
0.57
0.11
0.67
num of examples covered

PIA
0.12
0.05
0.17
0.08
0.11
0.08
0.18
0.39
0.16
0.03
0.35
0.60
0.13
0.15
0.22
0.24
0.12
0.16
0.12
0.11
by abd

Table 9: Verbs dataset IIA results averaged from all iterations in 10-fold cross-validation
testing. RoI, PoI - recall and precision with only induction: RIA, PIA - recall and precision
using IIA.

there is reduction of false positives because of fewer rules and in recognition there is a high
possibility of multiple rules firing in the test data thereby giving many false positives. In
classification, this is not the case, as once a vignette is classified as being a member of a
particular event class by a rule, the classification by other rules of the event class does not
affect the overall outcome for that vignette.

80

fiLearning Relational Event Models from Video

9. Limitations and Future Work
The models used by remind are local, i.e., without the context of a wider activity model
that could be used to filter out recognized instances thereby increasing performance. For
example, in some turn-arounds in the airport domain, the Aircraft Departure event is
sometimes recognized even before Aircraft Arrival is recognized resulting in false positives
for Aircraft Departure. Another limitation of the learned models is the lack of representation
of duration for events. Many recognized event instances are rejected by the system because
the temporal extent of the recognized instances is long and fails our criteria of 20% overlap
with the ground truth. These can be reduced by learning a global model (Greenall, Cohn, &
Hogg, 2011) that constrains the ordering of events such that Aircraft Departure detections
can happen only after Aircraft Arrival. The activity models can also represent expected
duration of events, the temporal separation of events and the number of occurrences.
The framework is sensitive to the initial example selected to start the learning procedure.
The induction system used is based on the algorithm that uses a bottom clause (Muggleton,
1995) constructed from the selected example to guide the refinement of the hypothesis while
searching in the lattice. Hence it is possible it might select a corrupted example initially and
this might affect the whole induction process. This is a typical problem in machine learning
and there are several ways to avoid this. One promising approach that we followed in this
work is to repeat the learning with different examples chosen randomly as the starting point
then selecting the iteration that gives the minimum number of rules.
Another limitation of the framework is its dependency on the tracking of objects as it
uses the interactions of objects to model events. Challenging scenarios for object tracking
pose limitations for the current framework. The current framework is not probabilistic, i.e.,
neither the input data nor the learned models are probabilistic. One direction of future work
is to extend the framework using statistical relational learning so that it can use soft evidence
and learn more robust probabilistic relational models. Since the current framework does
not handle hierarchies of events, the framework could be extended to handle hierarchical
composition of events. One possible approach for this is to learn models of events at a
particular layer using the events at lower layers as primitives.

10. Conclusion
In this paper, we have proposed a supervised relational learning framework, and an extension using abduction, to learn event models from complex videos. Such event models can
be used to recognize event instances from unseen videos. We presented a Type Refinement
operator that exploits the object type hierarchy in the domain to search for better hypotheses and also proved that it is an optimal refinement operator. We presented an empirical
evaluation of the proposed framework on two real world video data sets and the results are
encouraging, showing that the framework can be effectively used in real world systems for
event recognition in various domains. We also showed that the proposed framework has
better generalization capabilities and performance when compared to the state-of-the-art
systems in event modelling. Finally, we note that although we have focused on learning
from video data here, in fact the approach would also be suitable for learning from other
data sources which provide tracks of interacting moving objects (e.g. from GPS streams).
81

fiDubba, Cohn, Hogg, Bhatt & Dylla

Acknowledgements
We thank colleagues in the CO-FRIEND, RACE, STRANDS and VIGIL projects consortia
for their valuable inputs to this research, and the respective funding from EU Framework
7 (FP7-ICT-214975, FP7-ICT-27752, FP7-ICT-600623) and DARPA (W911NF-10-C-0083).
Also financial support by the Deutsche Forschungsgemeinschaft in the Transregional Collaborative Research Center SFB/TR 8 Spatial Cognition project R3-[Q-Shape] is gratefully
acknowledged.

Appendix A. Proof of Optimality of Type Refinement Operator 
Let T be the type hierarchy tree with set of nodes TV , and set of leaf nodes TL , i.e. most
specific types (TL  TV ) and r be the root of the tree (most generic type). A type at a
parent node  in T is more generic than the types i at its children nodes and we write
 = i .
Let g be a function, g : TV  TV , that maps a child node to its immediate parent. The
function g can be considered as a generalizing operator that generalizes a type to its nearest
generic type. g can be applied as long as i 6= r
Let Si be an ordered (from most-specific to most general) set of all possible generalizations of i including i .
Si = {i , g(i ), g(g(i )), . . . , r }
For any set of types {1 , 2 , . . . , n }, we can define corresponding sets S1 , S2 , . . . , Sn .
Let {h1 , h2 , . . . , hm } be the set of types19 in the clause C where {h1 , h2 , . . . , hm }  TV . For
{h1 , h2 , . . . , hm }, we can define Sh1 , Sh2 , . . . , Shm . We can make the set {h1 , h2 , . . . , hm }
more generic by applying g (one or more times) on any arbitrarily selected subset of types
one type at a time.
The Cartesian product Sh1  Sh2  . . .  Shm is the set of tuples where each tuple is a
possible generalization of {h1 , h2 , . . . , hm }.
Let l be a function mapping a non-leaf type node to an integer that specifies how many
times g was applied to the original leaf node to obtain the non-leaf node20 in T , l : TV  N .
Using l, we can generate a new set Nhi from Shi by replacing i by l(i ). Now we can
generate a new Cartesian product Nh1  Nh2  . . .  Nhm .
Example .1. Let (1 , 2 , 3 ) be the set of types from a clause C and let the type hierarchy
be as given in Fig.13. Then we can define S1 , S2 , S3 and N1 , N2 , N3 as follows and the
tree representation of the Cartesian products S1  S2  S3 and N1  N2  N3 are given
in Fig.14 and Fig.15 respectively.
19. If we consider the list of types from a clause C where some types may be repeated because of some
arguments have the same type, the results in this appendix are still valid.
20. Note that in general, a non-leaf node can be obtained from any of the leaf nodes that are its descendants
but a unique leaf node can be obtained if we store the original leaf node that is generalized using g to
get the non-leaf type node.

82

fiLearning Relational Event Models from Video

r

1 g(2 ), g(3 )

2 3

Figure 13: An example type hierarchy.

S1 = {1 , g(1 )}
S2 = {2 , g(2 ), g(g(2 ))}
S3 = {3 , g(3 ), g(g(3 ))}
N1 = {0, 1}
N2 = {0, 1, 2}
N3 = {0, 1, 2}

1
2

g(2 )

g(1 )
2

g(g(2 ))

g(2 )

g(g(2 ))

3 g(3 ) g(g(3 )) 3 g(3 ) g(g(3 )) 3 g(3 ) g(g(3 )) 3 g(3 ) g(g(3 )) 3 g(3 ) g(g(3 )) 3 g(3 ) g(g(3 ))

Figure 14: Representing the Cartesian product S1  S2  S3 as a tree. The root is empty
and the next layer corresponds to S1 and so on. Note that g(1 ) = g(g(2 )) = g(g(3 )) = r .
Each path in the tree to a leaf is a possible generalization of (1 , 2 , 3 )  the leftmost path
being the null generalization, i.e. (1 , 2 , 3 ).
Definition .2. (Type Substitution,  ) A type substitution  is a set
{h1 /1 , h2 /2 , . . . , hn /n } where each hi is the type of a subset of variables in the
clause C and i is the immediate generic type of hi (parent node of hi in the tree T ). We
say i is substituted for hi in the clause. The set {h1 , h2 , . . . , hn } is called the domain of
 , denoted dom( ) and the set (1 , 2 , . . . , n ) is called range of  , denoted rng( ).
A type substitution is used to generalize the type of a subset of variables in the clause.
Definition .3. (Most Generic Type Substitution, r ) A most generic type substitution is
a type substitution whose range is the set {r } where r the root of type hierarchy tree T .
A most generic type substitution is used to check if two clauses are structurally equivalent
(Def:4) by substituting the types of all variable by r . Note that every clause C has a unique
most generic type substitution, Cr , whose domain is the set of all types in C and range is
the set {r }.
83

fiDubba, Cohn, Hogg, Bhatt & Dylla



0

0

1

1

2

0

1

2

0 1 2 0 1 2 0 1 2 0 1 2 0 1 2 0 1 2

Figure 15: Representing the Cartesian product N1  N2  N3 as a tree. The root is empty
and the next layer corresponds to N1 and so on. Each path to a leaf represents a possible
generalization of (1 , 2 , 3 ). Each generalization is obtained by following the path from
root to leaf and generalizing the type at each layer the number of times indicated by the
node value. For example, the highlighted sequence (1,2,1) corresponds to the generalization
(g(1 ), g(g(2 )), g(3 )). This can be obtained by generalizing 1 once and generalizing 2
twice and generalizing 3 once. If a top to bottom order (left to right in case of tuples)
is followed, this is the only unique way to achieve the generalization (g(1 ), g(g(2 )), g(3 ))
from (1 , 2 , 3 ).
Definition .4. (Structurally Equivalent, ) Two clauses, C and C 0 are structurally equiv0
alent, denoted C  C 0 , if CCr  C 0 Cr .
A clause C is structurally equivalent to all the clauses obtained by replacing a subset of
types of variables in C by any of their generalizations.
Definition .5. (Generic Order w.r.t. types,  ) A clause C is said to be more general
w.r.t. type than another clause C 0 , denoted C  C 0 , iff C  C 0 and the set of types of C
are correspondingly more generic than the set of types of C 0 .
Definition .6. (Type-Refinement Operator) Let  be a clausal language, T a type hierarchy
and C a clause in . CT is a subset of  defined over T and C where a clause C 0  CT is
structurally equivalent to C, i.e., C  C 0 . Let  be the subsumption order as defined above.
The Type-Refinement operator  for hCT ,  i is a function such that  (C)  {D|D  C}.
 A One-step type refinement of C is defined when  is applied only once, i.e. 1 =
 (C). An n-step type refinement can be defined similarly, i.e. n = {D | E, E 
n1
(C) such that D   (E)}. The set of all type refinements on C is given by

 (C) = 1 (C)  2 (C)  . . ..
  is locally finite if for every C  ,  (C) is finite and computable.
  is proper if for every C  ,  (C)  {D|D  C}.
  is complete if for every C, D   such that D  C, there is an E   (C) such
that D  E (i.e. D and E are equivalent in the  order).
84

fiLearning Relational Event Models from Video

  is weakly complete for hCT ,  i if  (C) = CT .
  is non-redundant if for every C, D, E  , E   (C) and E   (D) implies
C   (D) or D   (C)
  is ideal if it is locally finite, proper and complete.
  is optimal if it is locally finite, non-redundant and weakly complete.
The type refinement operator selects a type hi from the set {h1 , h2 , . . . , hm } for C and applies the type generalizing operator to the type and the resulting set {h1 , h2 , . . . , g(hi ), . . . , hm }
is used for substitution in C to obtain a more generic clause C 0 with respect to type, i.e.,
C 0  C . The type refinement operator follows a left to right order in generalizing types
to avoid generating redundant clauses, i.e., if a type at position i is generalized then in the
next step of refinement of h0 no type at position j, j < i is selected for generalizing.
Theorem .7.  is locally finite
Proof. Let T be the type hierarchy tree and TV be the set of all nodes in T and TC =
{h1 , h2 , . . . , hm } be the set of types in the clause C where TC  TV . Let g be the type
generalizing operator and  be the type refinement operator.  operates on C by selecting
a type from the set {h1 , h2 , . . . , hm } and generalizing it by applying g. There are only |TC |
possibilities to select from and for each possible type selected there is only one possible
generalization, as a type only has one parent in the tree T . Also each type can only be
generalized finite number of times (i.e., until it becomes r ). Hence the number of possible
refinements, i.e., | (C)| is finite making  locally finite.
Theorem .8.  is weakly complete
Proof. For any given clause C with set of types {h1 , h2 , . . . , hm } and T as defined above.
1 ,  1 , . . .} obtained from one-step type refinement
Let X1 be the set of substitutions {,1
,2
1 and C 0  1 (C). Let X = X  X  . . . where X is the set of
such that Ci0 = C,i
1
2
2

i
1 ,  1 , . . . ,  1 } be
substitutions obtained from two-step type refinement and so on and {i1
i2
im
1 ).
the rng(,i
1 ,  1 , . . . ,  1 ), . . . , ( 1 ,  1 , . . . ,  1 ), . . .}, i.e. 1 is the
Let 1 be the set of tuples {(11
12
1m
i1 i2
im
set of tuples where each tuple represents a possible type refinement of {h1 , h2 , . . . , hm }
through one-step type refinement and let  = 1  2  . . ..
It is easy to observe that any tuple P   is also a tuple in the Cartesian product
Sh1  Sh2  . . .  Shm . In fact there is an exact one to one matching for the members of 
and members of Sh1  Sh2  . . .  Shm . It is easy to obtain each member tuple, say P where
P = (1 , 2 , . . . , m ) of the Cartesian product by generalizing {h1 , h2 , . . . , hm }. A type hi
is generalized until it is equal to i before moving to the next type on the immediate right
of hi . In this way all possible type generalizations of {h1 , h2 , . . . , hm } are reachable from
{h1 , h2 , . . . , hm }, i.e., all possible type generalized clauses are reachable from C, hence 
is weakly complete.
Theorem .9.  is non-redundant
85

fiDubba, Cohn, Hogg, Bhatt & Dylla

Proof. Let Nh1  Nh2  . . .  Nhm be as defined previously for a set of types {h1 , h2 , . . . , hm }
from a clause C. This Cartesian product can be represented as a tree where the root is
empty and the next level is the elements from Nh1 and so on. Each path to a leaf represents
a possible generalization of {h1 , h2 , . . . , hm }. Each generalization is obtained by following
the path from root to leaf and generalizing the type at each layer the number of times
indicated by the node value. If a top to bottom order (left to right in case of tuples) is
followed, there is a unique way to obtain the generalization that path generates. Hence 
is non-redundant.
Example .10. For example in Fig.15, the sequence (1,2,1) in bold corresponds to the generalization (g(1 ), g(g(2 )), g(3 )). This can obtained by generalizing 1 once and generalizing
2 twice and generalizing 3 once. If a top to bottom order is followed, this is the only
unique way to achieve the generalization (g(1 ), g(g(2 )), g(3 )) from (1 , 2 , 3 ).
Theorem .11.  is optimal
Proof. Since the type refinement operator is locally finite, weakly complete and non-redundant,
it is optimal.

References
Albanese, M., Moscato, V., Picariello, A., Subrahmanian, V., & Udrea, O. (2007). Detecting
stochastically scheduled activities in video. In Proceedings of the International Joint
Conference on Aritificial Intelligence (IJCAI), pp. 18021807.
Allen, J. F. (1983). Maintaining knowledge about temporal intervals. Communications of
the ACM, 26, 832843.
Bengio, Y. (2009). Learning deep architectures for AI. Foundations and Trends in Machine
Learning, 2 (1), 1127.
Bhatt, M., & Loke, S. (2008). Modelling dynamic spatial systems in the situation calculus.
Spatial Cognition & Computation, 8 (1-2), 86130.
Blockeel, H., De Raedt, L., Jacobs, N., & Demoen, B. (1999). Scaling up Inductive Logic
Programming by learning from interpretations. Data Mining and Knowledge Discovery, 3 (1), 5993.
Bundy, A., Byrd, L., & Mellish, C. (1985). Special-purpose, but domain-independent, inference mechanisms. In Progress in Artificial Intelligence, pp. 93111. London: Ellis
Horwood.
Chen, J., Cohn, A. G., Liu, D., Wang, S., Ouyang, J., & Yu, Q. (2015). A survey of
qualitative spatial representations. The Knowledge Engineering Review, 30, 106136.
Christiansen, H., & Dahl, V. (2005). HYPROLOG: A new logic programming language
with assumptions and abduction. Logic Programming, 159173.
Cohn, A. G. (1989). Taxonomic reasoning with many-sorted logics. Artificial Intelligence
Review, 3 (2), 89128.
86

fiLearning Relational Event Models from Video

Cohn, A. G., Hogg, D. C., Bennett, B., Devin, V., Galata, A., Magee, D. R., Needham, C.,
& Santos, P. (2006). Cognitive vision: integrating symbolic qualitative representations
with computer vision.. Vol. 3948 of LNCS, chap. 14, pp. 221246. Springer.
Dalal, N., & Triggs, B. (2005). Histograms of oriented gradients for human detection. In
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Vol. 1, pp.
886893.
Dubba, K. S., Bhatt, M., Dylla, F., Hogg, D. C., & Cohn, A. G. (2012). Interleaved
inductive-abductive reasoning for learning complex event models. In Inductive Logic
Programming, pp. 113129. Springer.
Dubba, K. S., Cohn, A. G., & Hogg, D. C. (2010). Event model learning from complex
videos using ILP. In Proceedings of the European Conference on Artificial Intelligence
(ECAI), Vol. 215, pp. 9398.
Fern, A., Givan, R., & Siskind, J. (2002). Specific-to-general learning for temporal events
with application to learning event definitions from video. Journal of Artificial Intelligence Research, 17, 379449.
Ferryman, J., Borg, M., Thirde, D., Fusier, F., Valentin, V., Bremond, F., Thonnat, M.,
Aguilera, J., & Kampel, M. (2005). Automated scene understanding for airport aprons.
LNCS-3809, Springer Verlag, 3809, 593.
Freksa, C. (1991). Conceptual neighborhood and its role in temporal and spatial reasoning.
In Singh, M., & Trave-Massuyes, L. (Eds.), Decision Support Systems and Qualitative
Reasoning, pp. 181187. North-Holland, Amsterdam.
Ghahramani, Z. (1998). Learning Dynamic Bayesian networks. Adaptive Processing of
Sequences and Data Structures, 168197.
Greenall, J., Cohn, A. G., & Hogg, D. C. (2011). Temporal structure models for event
recognition. British Machine Vision Conference (BMVC).
Gupta, A., Srinivasan, P., Shi, J., & Davis, L. (2009). Understanding videos, constructing
plots learning a visually grounded storyline model from annotated videos. In IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 20042011.
Hakeem, A., Sheikh, Y., & Shah, M. (2004). CASEE : A hierarchical event representation
for the analysis of videos. In Proceeding of the National Conference on Artificial
Intelligence (AAAI), pp. 263268.
Hartley, R., & Zisserman, A. (2004). Multiple View Geometry in Computer Vision (Second
edition). Cambridge University Press.
Hazarika, S. M., & Cohn, A. G. (2002). Abducing qualitative spatio-temporal histories
from partial observations. In International Conference on Principles Of Knowledge
Representation And Reasoning, pp. 1425.
Hoogs, A., & Perera, A. G. A. (2008). Video activity recognition in the real world. In
Proceedings of the National Conference on Artificial Intelligence (AAAI), pp. 1551
1554.
87

fiDubba, Cohn, Hogg, Bhatt & Dylla

Ivanov, Y., & Bobick, A. (2000). Recognition of visual activities and interactions by stochastic parsing. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI),
22 (8).
Jiang, Z., Lin, Z., & Davis, L. S. (2010). A tree-based approach to integrated action localization, recognition and segmentation. Third Workshop on Human Motion (in
conjuntion with ECCV).
Kakas, A., Kowalski, R., & Toni, F. (1992). Abductive logic programming. Journal of Logic
and Computation, 2 (6), 719.
Kakas, A., & Riguzzi, F. (2000). Abductive concept learning. New Generation Computing,
18 (3), 243294.
Konik, T., & Laird, J. (2006). Learning goal hierarchies from structured observations and
expert annotations. Machine Learning, 64 (1), 263287.
Laptev, I. (2005). On space-time interest points. International Journal of Computer Vision,
64 (2), 107123.
Laptev, I., & Perez, P. (2007). Retrieving actions in movies. In IEEE International Conference on Computer Vision (ICCV), pp. 18.
Le, Q., Zou, W., Yeung, S., & Ng, A. (2011). Learning hierarchical invariant spatio-temporal
features for action recognition with independent subspace analysis. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 33613368. IEEE.
Lowe, D. (2004). Distinctive image features from scale-invariant keypoints. International
Journal of Computer Vision, 60 (2), 91110.
McCarthy, J. (1986). Applications of circumscription to formalizing common-sense knowledge. Artificial Intelligence, 28 (1), 89116.
Medioni, G., Cohen, I., Bremond, F., Hongeng, S., & Nevatia, R. (2001). Event detection
and analysis from video streams. IEEE Transactions on Pattern Analysis and Machine
Intelligence (PAMI), 23 (8), 873889.
Miller, R., & Shanahan, M. (1994). Narratives in the Situation Calculus. Journal of Logic
and Computation, 4 (5), 513530.
Morariu, V. I., & Davis, L. S. (2011). Multi-agent event recognition in structured scenarios..
In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3289
3296.
Morariu, V. I., Harwood, D., & Davis, L. S. (2013). Tracking peoples hands and feet
using mixed network and/or search.. In IEEE Transactions on Pattern Analysis and
Machine Intelligence (PAMI).
Moyle, S. (2003). Using theory completion to learn a robot navigation control program.
Proceedings of the International Conference on ILP, 182197.
Moyle, S., & Muggleton, S. (1997). Learning programs in the Event Calculus. LNAI-1297,
Springer-Verlag, 205212.
Muggleton, S. (1995). Inverse entailment and Progol. New Generation Computing, 13 (3&4),
245286.
88

fiLearning Relational Event Models from Video

Muggleton, S., & Bryant, C. H. (2000). Theory completion using inverse entailment. In Proceedings of the International Conference on ILP, pp. 130146, UK. Springer-Verlag.
Needham, C., Santos, P., Magee, D., Devin, V., Hogg, D., & Cohn, A. (2005). Protocols
from perceptual observations. Artificial Intelligence, 167 (1-2), 103136.
Nevatia, R., Hobbs, J., & Bolles, B. (2004). An ontology for video event representation.
In Computer Vision and Pattern Recognition Workshop (CVPRW-04), pp. 119119.
IEEE.
Nienhuys-Cheng, S., & De Wolf, R. (1997). Foundations of Inductive Logic Programming,
Vol. 1228. Springer Verlag.
Oh, S., Hoogs, A., Perera, et al. (2011). A large-scale benchmark dataset for event recognition in surveillance video. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 31533160.
Poole, D., Goebel, R., & Aleliunas, R. (1987). Theorist: A logical reasoning system for
defaults and diagnosis. In The Knowledge Frontier, pp. 331352.
Quinlan, J., & Cameron-Jones, R. (1993). FOIL: A midterm report. In Proceedings of the
European Conference on Machine Learning (ECML), pp. 120.
Quinlan, J. (1990). Learning logical definitions from relations. Machine Learning, 5 (3),
239266.
Rabiner, L. (1989). A tutorial on Hidden Markov models and selected applications in speech
recognition. Proceedings of the IEEE, 77 (2), 257286.
Randell, D. A., Cui, Z., & Cohn, A. (1992). A spatial logic based on regions and connection. In Proceedings of the International Conference on Knowledge Representation
and Reasoning, pp. 165176. Morgan Kaufmann.
Ryoo, M. S., & Aggarwal, J. K. (2009). Semantic representation and recognition of continued
and recursive human activities. International Journal of Computer Vision, 82 (1), 1
24.
Ryoo, M., & Aggarwal, J. (2011). Stochastic representation and recognition of high-level
group activities. International Journal of Computer Vision, 93 (2), 183200.
Sridhar, M., Cohn, A. G., & Hogg, D. C. (2010). Unsupervised learning of event classes from
video. In Proceedings of the National Conference on Artificial Intelligence (AAAI),
pp. 16311638.
Tamaddoni-Nezhad, A., Chaleil, R., Kakas, A., & Muggleton, S. (2006). Application of
abductive ILP to learning metabolic network inhibition from temporal data. Machine
Learning, 64 (1), 209230.
Tamaddoni-Nezhad, A., & Muggleton, S. (2009). The lattice structure and refinement
operators for the hypothesis space bounded by a bottom clause. Machine learning,
76 (1), 3772.
Van de Weghe, N., Cohn, A., De Tre, G., & De Maeyer, P. (2006). A qualitative trajectory calculus as a basis for representing moving objects in geographical information
systems. Control and Cybernetics, 35 (1), 97.
89

fiDubba, Cohn, Hogg, Bhatt & Dylla

Veeraraghavan, H., Papanikolopoulos, N., & Schrater, P. (2007). Learning dynamic event
descriptions in image sequences. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 16.
Vu, V.-T., Bremond, F., & Thonnat, M. (2003). Automatic video interpretation: A novel
algorithm for temporal scenario recognition. In Proceedings of the International Joint
Conference on Artifical Intelligence (IJCAI), Vol. 3, pp. 12951300.
Walther, C. (1985). A mechanical solution of Schuberts Steamroller by many-sorted resolution. Artificial Intelligence, 26 (2), 217224.
Yilmaz, A., Javed, O., & Shah, M. (2006). Object tracking: A survey. ACM Computing
Surveys (CSUR), 38 (4), 13.
YouTube (2015) http://www.youtube.com/yt/press/statistics.html. Accessed January 25, 2015.

90

fi